{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport os\nimport json\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\n\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgbm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.metrics import make_scorer\nfrom sklearn.model_selection import cross_val_score\nimport json\nimport string\n\n%matplotlib inline\n\nfrom plotly import tools\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n\nfrom sklearn import model_selection, preprocessing, metrics, ensemble, naive_bayes, linear_model\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nimport lightgbm as lgb\n\nfrom sklearn import model_selection, preprocessing, metrics, ensemble, naive_bayes,linear_model\nfrom sklearn.ensemble import RandomForestClassifier\n\nimport re\nfrom keras.preprocessing.text import text_to_word_sequence\nfrom keras.preprocessing.text import Tokenizer  \nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras import models\nfrom keras import layers\nfrom keras import losses\nfrom keras import metrics\nfrom keras import optimizers\nfrom keras.utils import plot_model\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, Flatten, Dense\nfrom keras.callbacks import EarlyStopping, LearningRateScheduler, ReduceLROnPlateau, ModelCheckpoint\nfrom keras.layers import Embedding, Flatten, Dense, GRU\nfrom keras.utils import to_categorical\nimport matplotlib.pyplot as plt\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\n\nfrom collections import Counter\nfrom pathlib import Path\nimport os\nimport numpy as np\nimport re\nimport string\nimport nltk\nnltk.download('punkt')\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nnltk.download('wordnet')\nfrom nltk.corpus import wordnet\nimport unicodedata\nimport html\nstop_words = stopwords.words('english')\n\n\nimport warnings\nimport numpy as np \nimport pandas as pd \nimport os\nimport re\nimport nltk\nfrom nltk.corpus import abc\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem.porter import *\nfrom nltk.stem.snowball import *\nfrom nltk.util import ngrams\nimport string\nimport spacy\nfrom spacy import displacy\nfrom sklearn import preprocessing\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.decomposition import PCA\nfrom wordcloud import WordCloud, STOPWORDS\nimport gensim\nfrom gensim.models.word2vec import Text8Corpus\nfrom gensim.models import KeyedVectors\nfrom gensim.scripts.glove2word2vec import glove2word2vec\nfrom nltk.sentiment import SentimentIntensityAnalyzer\nfrom textblob import TextBlob\n#from spacytextblob.spacytextblob import SpacyTextBlob\nfrom IPython.core.display import HTML\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport html\n%matplotlib inline\n\npd.options.mode.chained_assignment = None\npd.options.display.max_columns = 999\n\nrand_seed = 13579\nnp.random.seed(rand_seed)\n\nsns.set(style=\"darkgrid\", context=\"notebook\")\n\n\nimport cv2\nimport pandas as pd\nimport numpy as np\nimport os\nimport json\nfrom tqdm import tqdm, tqdm_notebook\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Flatten, Activation\nfrom keras.layers import Dropout\nfrom keras.layers.convolutional import Conv2D, MaxPooling2D\nfrom keras.utils import np_utils\nfrom keras.optimizers import SGD, RMSprop, Adam\nimport matplotlib.pyplot as plt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","id":"AtJFizu37UYL","outputId":"1f99fb2e-5092-47ad-b48c-dc5fab29b7fb","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv('../input/petfinder-adoption-prediction/train/train.csv')","metadata":{"id":"FELpwszu7UYa","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv('../input/petfinder-adoption-prediction/test/test.csv')","metadata":{"id":"JnpDczRD7UYb","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This breed of `154` has no images so we will drop it as it is not found in the test data as well.","metadata":{}},{"cell_type":"code","source":" train_df.iloc[1672,:]","metadata":{"id":"2GJ8xsOvfPnE","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.drop(1672, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Text processing ","metadata":{"id":"fyVZLdsl8ThO"}},{"cell_type":"code","source":"from wordcloud import WordCloud, STOPWORDS\n\n# Thanks : https://www.kaggle.com/aashita/word-clouds-of-various-shapes ##\ndef plot_wordcloud(text, mask=None, max_words=200, max_font_size=100, figure_size=(24.0,16.0), \n                   title = None, title_size=40, image_color=False):\n    stopwords = set(STOPWORDS)\n    \n\n    wordcloud = WordCloud(background_color='black',\n                    stopwords = stopwords,\n                    max_words = max_words,\n                    max_font_size = max_font_size, \n                    random_state = 42,\n                    width=800, \n                    height=400,\n                    mask = mask)\n    wordcloud.generate(str(text))\n    \n    plt.figure(figsize=figure_size)\n    if image_color:\n        image_colors = ImageColorGenerator(mask);\n        plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\");\n        plt.title(title, fontdict={'size': title_size,  \n                                  'verticalalignment': 'bottom'})\n    else:\n        plt.imshow(wordcloud);\n        plt.title(title, fontdict={'size': title_size, 'color': 'black', \n                                  'verticalalignment': 'bottom'})\n    plt.axis('off');\n    plt.tight_layout()  \n    \nplot_wordcloud(train_df[\"Description\"], title=\"Word Cloud of Train\")","metadata":{"id":"2QjUg6Hf8f8z","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from wordcloud import WordCloud, STOPWORDS\n\n\ndef plot_wordcloud(text, mask=None, max_words=200, max_font_size=100, figure_size=(24.0,16.0), \n                   title = None, title_size=40, image_color=False):\n    stopwords = set(STOPWORDS)\n    \n\n    wordcloud = WordCloud(background_color='black',\n                    stopwords = stopwords,\n                    max_words = max_words,\n                    max_font_size = max_font_size, \n                    random_state = 42,\n                    width=800, \n                    height=400,\n                    mask = mask)\n    wordcloud.generate(str(text))\n    \n    plt.figure(figsize=figure_size)\n    if image_color:\n        image_colors = ImageColorGenerator(mask);\n        plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\");\n        plt.title(title, fontdict={'size': title_size,  \n                                  'verticalalignment': 'bottom'})\n    else:\n        plt.imshow(wordcloud);\n        plt.title(title, fontdict={'size': title_size, 'color': 'black', \n                                  'verticalalignment': 'bottom'})\n    plt.axis('off');\n    plt.tight_layout()  \n    \nplot_wordcloud(test_df[\"Description\"], title=\"Word Cloud of Test\")","metadata":{"id":"gIDDNIZF9KfA","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df[\"Description\"].fillna(\"unknown\",inplace=True)","metadata":{"id":"4Xi7J0Vm913O","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df[\"Description\"].fillna(\"unknown\",inplace=True)","metadata":{"id":"vJhuXidE95ad","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_special_chars(text):\n    re1 = re.compile(r'  +')\n    x1 = text.lower().replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n        'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n        '<br />', \"\\n\").replace('\\\\\"', '\"').replace('<unk>', 'u_n').replace(' @.@ ', '.').replace(\n        ' @-@ ', '-').replace('\\\\', ' \\\\ ')\n    return re1.sub(' ', html.unescape(x1))\n\n\ndef remove_non_ascii(text):\n    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n    return unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n\n\ndef to_lowercase(text):\n    return text.lower()\n\n\n\ndef remove_punctuation(text):\n    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n    translator = str.maketrans('', '', string.punctuation)\n    return text.translate(translator)\n\n\ndef replace_numbers(text):\n    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n    return re.sub(r'\\d+', '', text)\n\n\ndef remove_whitespaces(text):\n    return text.strip()\n\n\ndef remove_stopwords(words, stop_words):\n    \"\"\"\n    :param words:\n    :type words:\n    :param stop_words: from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n    or\n    from spacy.lang.en.stop_words import STOP_WORDS\n    :type stop_words:\n    :return:\n    :rtype:\n    \"\"\"\n    return [word for word in words if word not in stop_words]\n\n\ndef stem_words(words):\n    \"\"\"Stem words in text\"\"\"\n    stemmer = PorterStemmer()\n    return [stemmer.stem(word) for word in words]\n\ndef lemmatize_words(words):\n    \"\"\"Lemmatize words in text\"\"\"\n\n    lemmatizer = WordNetLemmatizer()\n    return [lemmatizer.lemmatize(word) for word in words]\n\ndef lemmatize_verbs(words):\n    \"\"\"Lemmatize verbs in text\"\"\"\n\n    lemmatizer = WordNetLemmatizer()\n    return ' '.join([lemmatizer.lemmatize(word, pos='v') for word in words])\n\ndef text2words(text):\n  return word_tokenize(text)\n\ndef normalize_text( text):\n    text = remove_special_chars(text)\n    text = remove_non_ascii(text)\n    text = remove_punctuation(text)\n    text = to_lowercase(text)\n    text = replace_numbers(text)\n    words = text2words(text)\n    words = remove_stopwords(words, stop_words)\n    #words = stem_words(words)# Either stem ovocar lemmatize\n    words = lemmatize_words(words)\n    words = lemmatize_verbs(words)\n\n    return ''.join(words)","metadata":{"id":"sWUfGtfy9-13","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def normalize_corpus(corpus):\n  return [normalize_text(t) for t in corpus]","metadata":{"id":"_Kt47xJF-CIf","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df_text = normalize_corpus(train_df[\"Description\"])","metadata":{"id":"QAgZc7Cf-EAC","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in train_df_text[:10]:\n    print(i,\"\\n\")","metadata":{"id":"iwImSBLH-F5b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_text = train_df['AdoptionSpeed']\ny_text = to_categorical(y_text) #One hot encoding","metadata":{"id":"C8qWoEI4-IS9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df_text = normalize_corpus(test_df[\"Description\"])\ntexts = train_df_text + test_df_text\n","metadata":{"id":"Uc_MPnG2-Mq3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab_sz = 10000\ntok = Tokenizer(num_words=vocab_sz, oov_token='UNK')","metadata":{"id":"y8Tg5tzY-QrU","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_text.shape","metadata":{"id":"5i8HDQw6-Slw","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" tok.fit_on_texts(texts)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## GloVe","metadata":{}},{"cell_type":"code","source":"!wget http://nlp.stanford.edu/data/glove.6B.zip\n!unzip glove.6B.zip","metadata":{"id":"8kvb8ewC-mK2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_index = {}","metadata":{"id":"kZ68is56-9Bn","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f = open(os.path.join('./glove.6B.200d.txt'), encoding='utf8')\n\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embedding_index[word] = coefs\nf.close()","metadata":{"id":"7czcziU2-4VE","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_index = tok.word_index","metadata":{"id":"vXa0-FIl-7Dz","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max([len(x.split()) for x in train_df_text])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.hist([len(x.split()) for x in train_df_text], bins=100);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"maxlen = 150\nmax_words = 10000\nembedding_dim = 200\nembedding_matrix = np.zeros((max_words, embedding_dim))\n\nfor word, i in word_index.items():\n    if i < max_words:\n        embedding_vector = embedding_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector","metadata":{"id":"7FMatqYK-_oW","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sequences = tok.texts_to_sequences(train_df_text)\ny_train_text = np.asanyarray(y_text)","metadata":{"id":"-ETNHACn_B6P","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sequences","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train_text = pad_sequences(sequences, maxlen=maxlen)","metadata":{"id":"8AQQvC89_EXi","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train_text, x_val_text, y_train_text, y_val_text = train_test_split(x_train_text, y_train_text, test_size=0.3, random_state=41)","metadata":{"id":"uqj1luh9_Gei","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Image processing","metadata":{"id":"TVqmvcFD8aiK"}},{"cell_type":"code","source":"img_size = 128","metadata":{"id":"buzFTqF582sI","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def resize_to_square(im):\n    old_size = im.shape[:2]\n    ratio = float(img_size)/max(old_size)\n    new_size = tuple([int(x*ratio) for x in old_size])\n    im = cv2.resize(im, (new_size[1], new_size[0]))\n    delta_w = img_size - new_size[1]\n    delta_h = img_size - new_size[0]\n    top, bottom = delta_h//2, delta_h-(delta_h//2)\n    left, right = delta_w//2, delta_w-(delta_w//2)\n    color = [0, 0, 0]\n    new_im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT,value=color)\n    return new_im","metadata":{"id":"ZwxYBpO784PL","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_image(path, pet_id):\n    image = cv2.imread(f'{path}{pet_id}-1.jpg')\n    new_image = resize_to_square(image)\n    return new_image","metadata":{"id":"xCSYT7XK840a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pet_ids = train_df['PetID'].values","metadata":{"id":"ZxJkTi_U9Aui","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nX_img = []\nY_img = []\nfor pet_id in tqdm_notebook(pet_ids):\n    try:\n        im = load_image(\"../input/petfinder-adoption-prediction/train_images/\", pet_id)\n        X_img.append(im)        \n        ads = train_df[train_df['PetID'] == pet_id]['AdoptionSpeed'].values[0]\n        Y_img.append(ads)\n    except:\n#         print(pet_id)\n        breed = train_df[train_df['PetID'] == pet_id]['Breed1'].values[0]\n        idd = train_df[(train_df['Breed1']==breed) &(train_df['PhotoAmt']!=0)]['PetID'].sample().values[0]\n        im = load_image(\"../input/petfinder-adoption-prediction/train_images/\", idd)\n        X_img.append(im)\n        ads = train_df[train_df['PetID'] == pet_id]['AdoptionSpeed'].values[0]\n        Y_img.append(ads)","metadata":{"id":"iK0fzvff_yeW","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_img = np.asarray(X_img)\nX_img.shape","metadata":{"id":"h3HDwPvTAMkC","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_tr_img, X_tst_img, y_tr_img, y_tst_img = train_test_split(X_img, Y_img, test_size=0.3, random_state=41)","metadata":{"id":"_vDqu1wkAZgj","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_tr_img = X_tr_img.astype('float32')\nX_tst_img = X_tst_img.astype('float32')\nX_tr_img /= 255\nX_tst_img /= 255","metadata":{"id":"darAazpWAcb-","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tabular data and sentiment analysis","metadata":{"id":"NZfFbNpF8Wah"}},{"cell_type":"code","source":"train_tab = train_df.copy()\ntest_tab = test_df.copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.setdiff1d(test_tab.Breed1.unique(),train_tab.Breed1.unique())","metadata":{"id":"44DDhZCR7UYj","outputId":"a2d67b66-1b5e-4a79-87d2-d188e2c283a7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.setdiff1d(train_tab.Breed1.unique(),test_tab.Breed1.unique())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tabular_df = pd.concat((train_tab, test_tab), axis=0, ignore_index=True)","metadata":{"id":"v6goOjpj7UYl","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('../input/petfinder-adoption-prediction/train_sentiment/0008c5398.json','r') as f:\n    data = json.load(f)\n    \ntype(data)","metadata":{"id":"h8OVAyo27UYp","outputId":"58d7308f-fe9e-48c6-89be-d532cac91abd","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sent_list = []\nfor filename in os.listdir('../input/petfinder-adoption-prediction/train_sentiment'):\n    sent_list.append(filename.split('.')[0])\nlen(sent_list)","metadata":{"id":"YhuJugFe7UYq","outputId":"069db8a4-f9ad-44af-c850-6335566a8465","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"missing sentimenst for some pets","metadata":{"id":"hVbnxC0a7UYq"}},{"cell_type":"code","source":"tabular_df[['sentiment_score', 'sentiment_magnitude']] = 0\ntabular_df['sentiment_lang'] = 'en'\ntabular_df.set_index('PetID', inplace=True)","metadata":{"id":"OJUMXupn7UYr","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(tabular_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"idx =  tabular_df.index\nfor filename in os.listdir('../input/petfinder-adoption-prediction/train_sentiment'):\n    with open('../input/petfinder-adoption-prediction/train_sentiment/' + filename, 'r') as f:\n        sentiment = json.load(f)\n    pet_id = filename.split('.')[0]\n    if pet_id in idx:\n        tabular_df.loc[pet_id,'sentiment_score'] = sentiment['documentSentiment']['magnitude']\n        tabular_df.loc[pet_id,'sentiment_magnitude'] = sentiment['documentSentiment']['score']\n        tabular_df.loc[pet_id,'sentiment_lang'] = sentiment['language']","metadata":{"id":"Nh3i3GsU7UYs","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(tabular_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for filename in os.listdir('../input/petfinder-adoption-prediction/test_sentiment'):\n    with open('../input/petfinder-adoption-prediction/test_sentiment/' + filename, 'r') as f:\n        sentiment = json.load(f)\n    pet_id = filename.split('.')[0]\n    if pet_id in idx:\n        tabular_df.loc[pet_id,'sentiment_score'] = sentiment['documentSentiment']['magnitude']\n        tabular_df.loc[pet_id,'sentiment_magnitude'] = sentiment['documentSentiment']['score']\n        tabular_df.loc[pet_id,'sentiment_lang'] = sentiment['language']","metadata":{"id":"ndwcWCgj7UYt","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(tabular_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tabular_df.to_pickle('tabular_sentiment.pkl')","metadata":{"id":"mo1D6YTg7UYu","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_tab = tabular_df[~tabular_df['AdoptionSpeed'].isna()].AdoptionSpeed.values\ny_tab","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tabular_df.State.unique()","metadata":{"id":"NOUVoYht7UYw","outputId":"7e549f22-b112-44fb-f752-5c02babb04d9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tabular_df['Breed1'].unique()","metadata":{"id":"Oim5tW3z7UY2","outputId":"e3c71091-212f-4ba0-8659-46c7d97a9581","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tabular_df.drop(['RescuerID','Name','Description'], axis=1, inplace=True)","metadata":{"id":"sqtIsPP77UY8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tabular_df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"col_lab_transform_avg = ['Type','Breed1','Breed2','Gender','Color1','Color2','Color3','MaturitySize','FurLength','Vaccinated','Dewormed','Sterilized','Health','State','sentiment_lang']\ncol_lab_transform_rnk = ['Type','Breed1','Breed2','Gender','Color1','Color2','Color3','State','sentiment_lang']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tabular_df_encoded = tabular_df.copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# label_encoders_avg = {}\n# for col in col_lab_transform_avg:\n#     idx = pd.DataFrame(tabular_df.groupby(col).mean()['AdoptionSpeed'].sort_values(ascending=True).reset_index()).reset_index()\n# #     label_encoders[col] = idx.set_index(col).to_dict()['index']\n#     label_encoders_avg[col] = idx.set_index(col).to_dict()['AdoptionSpeed']  \n#     new_name = col + '_avg'\n#     tabular_df_encoded[new_name] = tabular_df[col].apply(lambda x : label_encoders_avg[col][x])","metadata":{"id":"8jcMCxEy7UY_","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label_encoders_rnk = {}\nfor col in col_lab_transform_rnk:\n    idx = pd.DataFrame(tabular_df.groupby(col).mean()['AdoptionSpeed'].sort_values(ascending=True).reset_index()).reset_index()\n    label_encoders_rnk[col] = idx.set_index(col).to_dict()['index']\n#     label_encoders[col] = idx.set_index(col).to_dict()['AdoptionSpeed']  \n    new_name = col + '_rank'\n    tabular_df_encoded[new_name] = tabular_df[col].apply(lambda x : label_encoders_rnk[col][x])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tabular_df_encoded['Breed1_Color1_avg'] = tabular_df_encoded['Breed1_avg'] * tabular_df_encoded['Color1_avg']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tabular_df_encoded.info()","metadata":{"id":"_XaVi39b7UZB","outputId":"659a0327-d5c9-4fe2-d5d4-6a14ef1570d6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tabular_df_encoded.fillna(0,inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for f in ['Breed1', 'Color1', 'Gender', 'sentiment_lang', 'Type']:\n#     df_all_dummy = pd.get_dummies(tabular_df_encoded[f], prefix=f)\n#     tabular_df_encoded = pd.concat((tabular_df_encoded, df_all_dummy), axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tabular_df_encoded = tabular_df_encoded.drop(col_lab_transform_rnk, axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tabular_df_encoded","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\nX = scaler.fit_transform(tabular_df_encoded.drop('AdoptionSpeed',1))","metadata":{"id":"dPjNCR8h7UZF","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tabular_df_encoded.info()","metadata":{"id":"hi8zJo367UZG","outputId":"8322856a-c409-4174-d0c3-08b900580028","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_tab = X[:len(train_tab)]\ntest_tab = X[len(train_tab):]","metadata":{"id":"0zgkJhMT7UZM","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras import models\nfrom keras import losses\nfrom keras import metrics\nfrom keras import optimizers\nfrom keras.utils import plot_model\nfrom keras.utils import to_categorical\n\nimport matplotlib.pyplot as plt\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers","metadata":{"id":"1GoNYXMl7UZN","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train_tab, X_val_tab, y_train_tab, y_val_tab = train_test_split(train_tab, y_tab, test_size=0.3, random_state=41)","metadata":{"id":"tnaidDIu7UZO","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train = to_categorical(y_train_tab)\ny_val = to_categorical(y_val_tab)","metadata":{"id":"oY-4OZt-7UZP","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modelling","metadata":{"id":"ZYu54-28Ah0m"}},{"cell_type":"code","source":"import gc\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"maxlen = 150\nmax_words = 10000\nembedding_dim = 200\nembedding_matrix = np.zeros((max_words, embedding_dim))\n\nimg_size = 128\ntabular_shape=X_train_tab.shape[1]","metadata":{"id":"8UqmvK0lOAxN","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_input = keras.Input(shape=(maxlen,), name=\"text\")\nembedding_layer  = layers.Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=maxlen, weights=[embedding_matrix], trainable=False)(text_input)\ntxt = layers.GRU(124, return_sequences=True)(embedding_layer)\ntxt = layers.GRU(64, return_sequences=True)(txt)\ntxt = layers.GRU(64)(txt)\ntxt = layers.Flatten()(txt)\ntxt = layers.Dense(512, activation='relu')(txt)\ntxt = layers.Dense(64, activation='relu')(txt)\ntxt_output = layers.Dense(5, activation='softmax')(txt)\n\n\nimage_input = keras.Input(shape=(img_size, img_size, 3), name=\"img\")\nimg = layers.Conv2D(64, 3, activation=\"relu\")(image_input)\nimg = layers.Conv2D(64, 3, activation=\"relu\")(img)\nimg = layers.MaxPooling2D(2)(img)\nimg = layers.Dropout(.25)(img)\nimg = layers.Conv2D(64, 3, activation=\"relu\")(img)\nimg = layers.Conv2D(img_size*2, 3, activation=\"relu\")(img)\nimg = layers.MaxPooling2D(2)(img)\nimg = layers.Dropout(.25)(img)\nimg = layers.Flatten()(img)\nimg = layers.Dense(64, activation='relu')(img)\nimg_output = layers.Dense(5, activation='softmax')(img)\n\n\n# tabular_input = keras.Input(shape=(tabular_shape,), name=\"tabular\")\n# tab = layers.Dense(2048, activation='relu')(tabular_input)\n# tab = layers.Dropout(.3)(tab)\n\n# tab = layers.Dense(1028, activation='relu')(tab)\n# tab = layers.Dropout(.3)(tab)\n\n# tab = layers.Dense(512, activation='relu')(tab)\n# tab = layers.Dropout(.3)(tab)\n\n# tab = layers.Dense(512, activation='relu')(tab)\n# tab = layers.Dense(128, activation='relu')(tab)\n# tab_output = layers.Dense(32, activation='relu')(tab)\n\n\ntabular_input = keras.Input(shape=(tabular_shape,), name=\"tabular\")\ntab = layers.Dense(2048, activation='relu')(tabular_input)\ntab = layers.Dense(1028, activation='relu')(tab)\ntab = layers.Dense(512, activation='relu')(tab)\ntab = layers.Dropout(.3)(tab)\ntab = layers.Dense(128, activation='relu')(tab)\ntab = layers.Dense(64, activation='relu')(tab)\ntab_output = layers.Dense(5, activation='softmax')(tab)\n\nl1 = layers.Add()([txt_output, img_output, tab_output])\npred = layers.Dense(5, activation='softmax', name=\"pred\")(l1)\n# l1 = layers.concatenate([txt_output, img_output, tab_output])\n\n# l1 = layers.Average()([txt_output, img_output, tab_output])\n\n# l2 = layers.Dense(64, activation='relu')(l1)\n# drop_l2 = layers.Dropout(.3)(l2)\n# batch_l2 = layers.BatchNormalization()(drop_l2)\n\n# # l3 = layers.Dense(256, activation='relu', kernel_regularizer= keras.regularizers.l1_l2(l1=1e-3, l2=1e-2),\n# #                   bias_regularizer=keras.regularizers.l2(1e-3), activity_regularizer=keras.regularizers.l2(1e-3))(batch_l2)\n# # drop_l3 = layers.Dropout(.3)(l3)\n# # batch_l3 = layers.BatchNormalization()(drop_l3)\n\n# l4 = layers.Dense(32, activation='relu', kernel_regularizer= keras.regularizers.l1_l2(l1=1e-1, l2=1e-2),\n#                   bias_regularizer=keras.regularizers.l2(1e-3), activity_regularizer=keras.regularizers.l2(1e-3))(batch_l2)\n\n# l5 = layers.Dense(32, activation='relu', kernel_regularizer= keras.regularizers.l1_l2(l1=1e-3, l2=1e-2),\n#                   bias_regularizer=keras.regularizers.l2(1e-3), activity_regularizer=keras.regularizers.l2(1e-3))(l4)\n\n# pred = layers.Dense(5, activation='softmax', name=\"pred\")(l5)\n\nmodel = keras.Model(\n    inputs=[text_input, image_input, tabular_input],\n    outputs=[pred],\n)","metadata":{"id":"9DhtjfNkAhiq","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(\n    optimizer=keras.optimizers.Adam(),\n    loss='categorical_crossentropy',\n    metrics=[\"acc\"],\n)","metadata":{"id":"XOAzVs0UMs1q","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"keras.utils.plot_model(model, show_shapes=True)","metadata":{"id":"zFgmTmKa7UZS","outputId":"41bf75ed-f410-440b-88d2-ffab4ca9ef38","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit({\"text\": x_train_text, \"img\": X_tr_img, \"tabular\": X_train_tab},\n                   {\"pred\": y_train},\n                   epochs=20,\n                   batch_size=32,\n                   validation_data=([x_val_text, X_tst_img, X_val_tab], y_val),)","metadata":{"id":"oUTEnddP7UZT","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class WeightedSum(layers.Layer):\n    \"\"\"A custom keras layer to learn a weighted sum of tensors\"\"\"\n\n    def __init__(self, **kwargs):\n        super(WeightedSum, self).__init__(**kwargs)\n\n    def build(self, input_shape=1):\n        self.a = self.add_weight(\n            name='alpha',\n            shape=(5),\n            initializer='ones',\n            dtype='float32',\n            trainable=True,\n        )\n        self.b = self.add_weight(\n            name='beta',\n            shape=(5),\n            initializer='ones',\n            dtype='float32',\n            trainable=True,\n        )\n        self.c = self.add_weight(\n            name='ceta',\n            shape=(5),\n            initializer='ones',\n            dtype='float32',\n            trainable=True,\n        )\n        super(WeightedSum, self).build(input_shape)\n\n    def call(self, model_outputs):\n        return self.a * model_outputs[0] + (self.b) * model_outputs[1] + (self.c) * model_outputs[2]\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[0]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def quadratic_kappa(actuals, preds, N=5):\n    \"\"\"This function calculates the Quadratic Kappa Metric used for Evaluation in the PetFinder competition\n    at Kaggle. It returns the Quadratic Weighted Kappa metric score between the actual and the predicted values \n    of adoption rating.\"\"\"\n    w = np.zeros((N,N))\n    O = confusion_matrix(actuals, preds)\n    for i in range(len(w)): \n        for j in range(len(w)):\n            w[i][j] = float(((i-j)**2)/(N-1)**2)\n    \n    act_hist=np.zeros([N])\n    for item in actuals: \n        act_hist[item]+=1\n    \n    pred_hist=np.zeros([N])\n    for item in preds: \n        pred_hist[item]+=1\n                         \n    E = np.outer(act_hist, pred_hist);\n    E = E/E.sum();\n    O = O/O.sum();\n    \n    num=0\n    den=0\n    for i in range(len(w)):\n        for j in range(len(w)):\n            num+=w[i][j]*O[i][j]\n            den+=w[i][j]*E[i][j]\n    return (1 - (num/den))","metadata":{"id":"zcZhgtHt7UZU","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pre=model.predict([x_val_text, X_tst_img, X_val_tab])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nquadratic_kappa(Y_VAL,Y_PRE)","metadata":{"id":"ets5OmFk7UZU","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_PRE = np.argmax(pre,axis=1)\n","metadata":{"id":"CDCB52rj7UZV","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_VAL = np.argmax(y_val,axis=1)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}