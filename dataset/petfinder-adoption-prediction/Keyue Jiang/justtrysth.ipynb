{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Pet Finder\n在阅读这个kernel的时候，之前，请先查看一下我们的报告。报告中将大部分关键信息都概要在内，能更直观地了解我们的project\n\n## 综述\n### 问题背景\n\n当今，越来越多的流浪宠物被抛弃在街头，遭受着疾苦，如果没有人领养，他们的最终归途大多都是被安乐死。PetFinder.my是一个动物爱好组织的动物领养平台，他们与全球各地的动物爱好者、媒体、企业密切合作，以求带给动物们福利。\n\n### 问题目标\n动物的领养率和网站提供的动物元数据，比如描述性文本与照片特征，息息相关。PetFinder.my提供给我们大约19000只宠物的数据库，数据包括表格型数据、图片数据与描述性文本数据。其中大约15000项将被作为训练数据，其余作为测试数据。我们将用这些数据，训练出一个用于预测流浪宠物被收养时间的模型。\n\n"},{"metadata":{},"cell_type":"markdown","source":"### 数据概览\n因为官方提供的数据是以训练集与测试集分开的形式提供的，因此在接下来的数据分析当中，均以训练集为例。（除了Adoptation speed之外，其他数据两集类似。）\n\n**目标数据**：\nAdoptation speed也就是领养时间。我们将最终的领养时间氛围。领养时间是一个Categorical数据，分了0-4五类。\n\n* 0 - 宠物被挂出时当天被领养\n* 1 - 宠物被挂出后1-7天被领养\n* 2 - 宠物被挂出后8-30天被领养\n* 3 - 宠物被挂出后31-90天被领养\n* 4 - 100天以内未被领养\n\n**表格数据：**\n表格类数据主要分为四类，分别为数值型（Numerical）、类别型（Categorical）object型（文本、string等）和目标数据（target）。\n\n其中，可以直接送入模型进行训练的数据为数值型与类别型数据。而object类数据，如果是文本则和之后的text data一起处理，如果是其他无意义的数据（RescuerID, Name等）则经过简单的处理（如求长度、累计出现次数等）后添加为一个新的列特征。当然还有一些用于辨别的数据，如PetID，在训练时直接删除即可。\n\n"},{"metadata":{},"cell_type":"markdown","source":"**metadata数据：**\n\nMetadata数据为官方通过Google API对image（metadata.json）和description(sentiment.json)进行处理后得到的数据，需要提取后使用。metadata.json包括数字量 `annots_score`,`color_score`，`color_pixelfrac`, `crop_conf`, `crop_importance` 和文本`annots_top_desc`共6个特征变量。`sentiment.json`包括数字量 `magnitude_sum`, `score_sum`, `magnitude_mean`, `score_mean`, `magnitude_var`, `score_var`共6个特征变量。拥有metadata的样本个数为18150，占比0.976，拥有sentiment的样本个数为18307，占比0.965。\n\n**图片数据：**\n\n在我们的18965个训练数据当中，18330个数据拥有相对应的图片数据，均为.jpg格式, 占比96.5%。单个样本可能不止有一张对应的图片，最高多达10张，图片总共有58370张。图片尺寸也不固定\n\n**评价标准：**\n\n我们整个输出的结果将会通过quadratic weighted kappa来计算。我们在报告中对kappa系数进行了介绍。\n"},{"metadata":{},"cell_type":"markdown","source":"先读入必要的library，并做初始化"},{"metadata":{"_uuid":"a194102854a9eaec85445c81d0621d55f1476560","collapsed":true,"trusted":false},"cell_type":"code","source":"import gc\nimport glob\nimport os\nimport json\nimport matplotlib.pyplot as plt\nimport pprint\n\nimport numpy as np\nimport pandas as pd\n\nfrom joblib import Parallel, delayed\nfrom tqdm import tqdm\nfrom PIL import Image\n\n%matplotlib inline\n\npd.options.display.max_rows = 128\npd.options.display.max_columns = 128\n\nimport tensorflow as tf\nimport keras.backend.tensorflow_backend as KTF\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"在正式处理之前，我们先对tensorflow进行资源限制。因为TF如果不加以限制，它会自动申请调所有的GPU，这是我们不想见到的"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_session(gpu_fraction=0.6):\n    # 设置允许TF使用的GPU为60%\n    num_threads = os.environ.get('OMP_NUM_THREADS')\n    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=gpu_fraction)\n\n    if num_threads:\n        return tf.Session(config=tf.ConfigProto(\n            gpu_options=gpu_options, intra_op_parallelism_threads=num_threads))\n    else:\n        return tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\nKTF.set_session(get_session())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6911a2be5a3b016869b7a94c16bfeb82cd0a7342","collapsed":true,"trusted":false},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (12, 9)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"os.listdir('../input')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f16a86529d54b5fbc3d24569da81077abab9ba21"},"cell_type":"markdown","source":"### 加载核心 DataFrames (train and test):"},{"metadata":{"_uuid":"8473644b13e590420cdc349f7fb27a541fb6644b","collapsed":true,"trusted":false},"cell_type":"code","source":"os.listdir('../input/petfinder-adoption-prediction/test/')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"54985b02f752cd86dd2a39ac261281573ca9e81d","collapsed":true,"scrolled":true,"trusted":false},"cell_type":"code","source":"train = pd.read_csv('../input/petfinder-adoption-prediction/train/train.csv')\ntest = pd.read_csv('../input/petfinder-adoption-prediction/test/test.csv')\nsample_submission = pd.read_csv('../input/petfinder-adoption-prediction/test/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"将state这一无用信息赋予意义，给予GDP、人口等数据。这些数据比单纯一个state的分类有用的多。\nThis is from state GDP: https://en.wikipedia.org/wiki/List_of_Malaysian_states_by_GDP"},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"\nstate_gdp = {\n    41336: 116.679,\n    41325: 40.596,\n    41367: 23.02,\n    41401: 190.075,\n    41415: 5.984,\n    41324: 37.274,\n    41332: 42.389,\n    41335: 52.452,\n    41330: 67.629,\n    41380: 5.642,\n    41327: 81.284,\n    41345: 80.167,\n    41342: 121.414,\n    41326: 280.698,\n    41361: 32.270\n}\n\n# 州人口: https://en.wikipedia.org/wiki/Malaysia\nstate_population = {\n    41336: 33.48283,\n    41325: 19.47651,\n    41367: 15.39601,\n    41401: 16.74621,\n    41415: 0.86908,\n    41324: 8.21110,\n    41332: 10.21064,\n    41335: 15.00817,\n    41330: 23.52743,\n    41380: 2.31541,\n    41327: 15.61383,\n    41345: 32.06742,\n    41342: 24.71140,\n    41326: 54.62141,\n    41361: 10.35977\n}\n\ntrain[\"state_gdp\"] = train['State'].map(state_gdp)\ntrain[\"state_population\"] = train['State'].map(state_population)\ntest[\"state_gdp\"] = test['State'].map(state_gdp)\ntest[\"state_population\"] = test['State'].map(state_population)\ntrain[\"gdp_vs_population\"] = train[\"state_gdp\"] / train[\"state_population\"]\ntest[\"gdp_vs_population\"] = test[\"state_gdp\"] / test[\"state_population\"]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"43025d0dbd7886a7e3d7168a45bec6790307ff44"},"cell_type":"markdown","source":"### 加载遍历所需要的字典"},{"metadata":{"_uuid":"cdb36fcca79a9c8db7f139d7266d8c8e4ad021f3","collapsed":true,"trusted":false},"cell_type":"code","source":"labels_breed = pd.read_csv('../input/petfinder-adoption-prediction/breed_labels.csv')\nlabels_state = pd.read_csv('../input/petfinder-adoption-prediction/color_labels.csv')\nlabels_color = pd.read_csv('../input/petfinder-adoption-prediction/state_labels.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cd40d640362e85d9818b98e51b0be3aec443762b"},"cell_type":"markdown","source":"### 数据探析:\n\n除了表格类数据之外，我们还有以下三类数据\n\n- 图片\n- 元数据\n- 文本类\n\n这一类数据，可以很大程度上提高我们对于预测的准确率。因为图片、文本数据才是人们在领养已知宠物时会最优先考虑的数据。\n\n首先现粗略统计需要操作的数据量"},{"metadata":{"_uuid":"876557740f8c8ccc2b4c0080acee005fb2eff73f","collapsed":true,"trusted":false},"cell_type":"code","source":"train_image_files = sorted(glob.glob('../input/petfinder-adoption-prediction/train_images/*.jpg'))\ntrain_metadata_files = sorted(glob.glob('../input/petfinder-adoption-prediction/train_metadata/*.json'))\ntrain_sentiment_files = sorted(glob.glob('../input/petfinder-adoption-prediction/train_sentiment/*.json'))\n\nprint('num of train images files: {}'.format(len(train_image_files)))\nprint('num of train metadata files: {}'.format(len(train_metadata_files)))\nprint('num of train sentiment files: {}'.format(len(train_sentiment_files)))\n\n\ntest_image_files = sorted(glob.glob('../input/petfinder-adoption-prediction/test_images/*.jpg'))\ntest_metadata_files = sorted(glob.glob('../input/petfinder-adoption-prediction/test_metadata/*.json'))\ntest_sentiment_files = sorted(glob.glob('../input/petfinder-adoption-prediction/test_sentiment/*.json'))\n\nprint('num of test images files: {}'.format(len(test_image_files)))\nprint('num of test metadata files: {}'.format(len(test_metadata_files)))\nprint('num of test sentiment files: {}'.format(len(test_sentiment_files)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f0faa84d0f6382e59e704680a0291687022c99e"},"cell_type":"markdown","source":"### 训练分析：\n\n统计数据量，查看有多少数据缺失"},{"metadata":{"_uuid":"96053ab161520c2d8d40a60e0a2fdf649f65bb23","collapsed":true,"scrolled":true,"trusted":false},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (12, 9)\nplt.style.use('ggplot')\n\n\n# Images:\ntrain_df_ids = train[['PetID']]\nprint(train_df_ids.shape)\n\ntrain_df_imgs = pd.DataFrame(train_image_files)\ntrain_df_imgs.columns = ['image_filename']\ntrain_imgs_pets = train_df_imgs['image_filename'].apply(lambda x: x.split('/')[-1].split('-')[0])\ntrain_df_imgs = train_df_imgs.assign(PetID=train_imgs_pets)\nprint(len(train_imgs_pets.unique()))\n\npets_with_images = len(np.intersect1d(train_imgs_pets.unique(), train_df_ids['PetID'].unique()))\nprint('fraction of pets with images: {:.3f}'.format(pets_with_images / train_df_ids.shape[0]))\n\n# Metadata:\ntrain_df_ids = train[['PetID']]\ntrain_df_metadata = pd.DataFrame(train_metadata_files)\ntrain_df_metadata.columns = ['metadata_filename']\ntrain_metadata_pets = train_df_metadata['metadata_filename'].apply(lambda x: x.split('/')[-1].split('-')[0])\ntrain_df_metadata = train_df_metadata.assign(PetID=train_metadata_pets)\nprint(len(train_metadata_pets.unique()))\n\npets_with_metadatas = len(np.intersect1d(train_metadata_pets.unique(), train_df_ids['PetID'].unique()))\nprint('fraction of pets with metadata: {:.3f}'.format(pets_with_metadatas / train_df_ids.shape[0]))\n\n# Sentiment:\ntrain_df_ids = train[['PetID']]\ntrain_df_sentiment = pd.DataFrame(train_sentiment_files)\ntrain_df_sentiment.columns = ['sentiment_filename']\ntrain_sentiment_pets = train_df_sentiment['sentiment_filename'].apply(lambda x: x.split('/')[-1].split('.')[0])\ntrain_df_sentiment = train_df_sentiment.assign(PetID=train_sentiment_pets)\nprint(len(train_sentiment_pets.unique()))\n\npets_with_sentiments = len(np.intersect1d(train_sentiment_pets.unique(), train_df_ids['PetID'].unique()))\nprint('fraction of pets with sentiment: {:.3f}'.format(pets_with_sentiments / train_df_ids.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2d6e48743d5bc42015320b367cc6e11393233264","collapsed":true,"scrolled":true,"trusted":false},"cell_type":"code","source":"# Images:\ntest_df_ids = test[['PetID']]\nprint(test_df_ids.shape)\n\ntest_df_imgs = pd.DataFrame(test_image_files)\ntest_df_imgs.columns = ['image_filename']\ntest_imgs_pets = test_df_imgs['image_filename'].apply(lambda x: x.split('/')[-1].split('-')[0])\ntest_df_imgs = test_df_imgs.assign(PetID=test_imgs_pets)\nprint(len(test_imgs_pets.unique()))\n\npets_with_images = len(np.intersect1d(test_imgs_pets.unique(), test_df_ids['PetID'].unique()))\nprint('fraction of pets with images: {:.3f}'.format(pets_with_images / test_df_ids.shape[0]))\n\n\n# Metadata:\ntest_df_ids = test[['PetID']]\ntest_df_metadata = pd.DataFrame(test_metadata_files)\ntest_df_metadata.columns = ['metadata_filename']\ntest_metadata_pets = test_df_metadata['metadata_filename'].apply(lambda x: x.split('/')[-1].split('-')[0])\ntest_df_metadata = test_df_metadata.assign(PetID=test_metadata_pets)\nprint(len(test_metadata_pets.unique()))\n\npets_with_metadatas = len(np.intersect1d(test_metadata_pets.unique(), test_df_ids['PetID'].unique()))\nprint('fraction of pets with metadata: {:.3f}'.format(pets_with_metadatas / test_df_ids.shape[0]))\n\n\n\n# Sentiment:\ntest_df_ids = test[['PetID']]\ntest_df_sentiment = pd.DataFrame(test_sentiment_files)\ntest_df_sentiment.columns = ['sentiment_filename']\ntest_sentiment_pets = test_df_sentiment['sentiment_filename'].apply(lambda x: x.split('/')[-1].split('.')[0])\ntest_df_sentiment = test_df_sentiment.assign(PetID=test_sentiment_pets)\nprint(len(test_sentiment_pets.unique()))\n\npets_with_sentiments = len(np.intersect1d(test_sentiment_pets.unique(), test_df_ids['PetID'].unique()))\nprint('fraction of pets with sentiment: {:.3f}'.format(pets_with_sentiments / test_df_ids.shape[0]))\n\n\n# are distributions the same?\nprint('images and metadata distributions the same? {}'.format(\n    np.all(test_metadata_pets == test_imgs_pets)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"18f435d349d1fc384c645b41233d2c5a22860903"},"cell_type":"markdown","source":"### 数据获取与特征提取\n可以发现，我们的大部分文本特征都存储在json文件中，但有一小部分是来自表格类数据中（description列）\n\n\n在了解了数据结构之后，我们就明白了需要如何读取数据，提取特征，并和主要的表格类数据融合(merge)并构建起新的DataFrame。\n\n数据读取这一块来自baseline model:https://www.kaggle.com/wrosinski/baselinemodeling\n\n具体每个函数的功能请具体看我们的代码注释"},{"metadata":{"_kg_hide-input":true,"_uuid":"1db107527d3ef5ba55560679334d16f1d82fb663","collapsed":true,"trusted":false},"cell_type":"code","source":"class PetFinderParser(object):\n    def __init__(self, debug=False):\n        self.debug = debug\n        self.sentence_sep = '; '\n        self.extract_sentiment_text = False\n        \n    def open_metadata_file(self, filename):\n        \"\"\"\n        加载元数据文件\n        \"\"\"\n        with open(filename, 'r') as f:\n            metadata_file = json.load(f)\n        return metadata_file\n            \n    def open_sentiment_file(self, filename):\n        \"\"\"\n        加载情感数据文件\n        \"\"\"\n        with open(filename, 'r') as f:\n            sentiment_file = json.load(f)\n        return sentiment_file\n            \n    def open_image_file(self, filename):\n        \"\"\"\n        加载图片文件\n        \"\"\"\n        image = np.asarray(Image.open(filename))\n        return image\n        \n    def parse_sentiment_file(self, file):\n        \"\"\"\n        爬取情感数据文件，简单来讲，就是将数据通过Pandas读入一个DataFrame，然后将整一个dataframe作为返回值\n        \"\"\"\n        # documentSentiment\n        ret_val = {}\n        ret_val['doc_mag'] = file['documentSentiment']['magnitude']\n        ret_val['doc_score']= file['documentSentiment']['score']\n        ret_val['doc_language'] = file['language']\n        ret_val['doc_stcs_len'] = len(file['sentences'])\n        if ret_val['doc_stcs_len']>0:\n            ret_val['doc_first_score'] = file['sentences'][0]['sentiment']['score']\n            ret_val['doc_first_mag'] = file['sentences'][0]['sentiment']['magnitude']\n            ret_val['doc_last_score'] = file['sentences'][-1]['sentiment']['score']\n            ret_val['doc_last_mag'] = file['sentences'][-1]['sentiment']['magnitude']\n        else:\n            ret_val['doc_first_score'] = np.nan\n            ret_val['doc_first_mag'] = np.nan\n            ret_val['doc_last_score'] = np.nan\n            ret_val['doc_last_mag'] = np.nan\n        ret_val['doc_ent_num'] = len(file['entities'])\n        \n        # sentence score\n        mags, scores = [], []\n        for s in file['sentences']:\n            mags.append(s['sentiment']['magnitude'])\n            scores.append(s['sentiment']['score'])\n        \n        if len(scores)==0:\n            ret_val['doc_score_sum'] = np.nan\n            ret_val['doc_mag_sum'] = np.nan\n            ret_val['doc_score_mena'] = np.nan\n            ret_val['doc_mag_mean'] = np.nan\n            ret_val['doc_score_max'] = np.nan\n            ret_val['doc_mag_max'] = np.nan\n            ret_val['doc_score_min'] = np.nan\n            ret_val['doc_mag_min'] = np.nan\n            ret_val['doc_score_std'] = np.nan\n            ret_val['doc_mag_std'] = np.nan\n        else:\n            ret_val['doc_score_sum'] = np.sum(scores)\n            ret_val['doc_mag_sum'] = np.sum(mags)\n            ret_val['doc_score_mena'] = np.mean(scores)\n            ret_val['doc_mag_mean'] = np.mean(mags)\n            ret_val['doc_score_max'] = np.max(scores)\n            ret_val['doc_mag_max'] = np.max(mags)\n            ret_val['doc_score_min'] = np.min(scores)\n            ret_val['doc_mag_min'] = np.min(mags)\n            ret_val['doc_score_std'] = np.std(scores)\n            ret_val['doc_mag_std'] = np.std(mags)\n\n        # entity type\n        ret_val['sentiment_entities'] = []\n        ret_val['doc_ent_person_count'] = 0\n        ret_val['doc_ent_location_count'] = 0\n        ret_val['doc_ent_org_count'] = 0\n        ret_val['doc_ent_event_count'] = 0\n        ret_val['doc_ent_woa_count'] = 0\n        ret_val['doc_ent_good_count'] = 0\n        ret_val['doc_ent_other_count'] = 0\n        key_mapper = {\n            'PERSON':'doc_ent_person_count',\n            'LOCATION':'doc_ent_location_count',\n            'ORGANIZATION':'doc_ent_org_count',\n            'EVENT':'doc_ent_event_count',\n            'WORK_OF_ART':'doc_ent_woa_count',\n            'CONSUMER_GOOD':'doc_ent_good_count',\n            'OTHER':'doc_ent_other_count'\n        }\n        for e in file['entities']:\n            ret_val['sentiment_entities'].append(e['name'])\n            if e['type'] in key_mapper:\n                ret_val[key_mapper[e['type']]]+=1\n        \n        ret_val['sentiment_entities'] = ' '.join(ret_val['sentiment_entities'])\n        return ret_val\n    \n    def parse_metadata_file(self, file, img):\n        \"\"\"\n        爬取元数据文件，返回一个df\n        \"\"\"\n        file_keys = list(file.keys())\n        if 'textAnnotations' in file_keys:\n#             textanno = 1\n            textblock_num = len(file['textAnnotations'])\n            textlen = np.sum([len(text['description']) for text in file['textAnnotations']])\n        else:\n#             textanno = 0\n            textblock_num = 0\n            textlen = 0\n        if 'faceAnnotations' in file_keys:\n            faceanno = 1\n        else:\n            faceanno = 0\n        if 'labelAnnotations' in file_keys:\n            file_annots = file['labelAnnotations']#[:len(file['labelAnnotations'])]\n            if len(file_annots)==0:\n                file_label_score_mean = np.nan\n                file_label_score_max = np.nan\n                file_label_score_min = np.nan\n            else:\n                temp = np.asarray([x['score'] for x in file_annots])\n                file_label_score_mean = temp.mean()\n                file_label_score_max = temp.max()\n                file_label_score_min = temp.min()\n            file_top_desc = [x['description'] for x in file_annots]\n        else:\n            file_label_score_mean = np.nan\n            file_label_score_max = np.nan\n            file_label_score_min = np.nan\n            file_top_desc = ['']\n        \n        file_colors = file['imagePropertiesAnnotation']['dominantColors']['colors']\n        file_crops = file['cropHintsAnnotation']['cropHints']\n        if len(file_colors)==0:\n            file_color_score = np.nan\n            file_color_pixelfrac = np.nan\n            color_red_mean = np.nan\n            color_green_mean = np.nan\n            color_blue_mean = np.nan\n            color_red_std = np.nan\n            color_green_std = np.nan\n            color_blue_std = np.nan\n        else:\n            file_color_score = np.asarray([x['score'] for x in file_colors]).mean()\n            file_color_pixelfrac = np.asarray([x['pixelFraction'] for x in file_colors]).mean()\n            file_color_red = np.asarray([x['color']['red'] if 'red' in x['color'] else 0 for x in file_colors])\n            file_color_green = np.asarray([x['color']['green'] if 'green' in x['color'] else 0for x in file_colors])\n            file_color_blue = np.asarray([x['color']['blue'] if 'blue' in x['color'] else 0 for x in file_colors])\n            color_red_mean = file_color_red.mean()\n            color_green_mean = file_color_green.mean()\n            color_blue_mean = file_color_blue.mean()\n            color_red_std = file_color_red.std()\n            color_green_std = file_color_green.std()\n            color_blue_std = file_color_blue.std()\n        \n        if len(file_crops)==0:\n            file_crop_conf=np.nan\n            file_crop_importance = np.nan\n            file_crop_fraction_mean = np.nan\n            file_crop_fraction_sum = np.nan\n            file_crop_fraction_std = np.nan\n            file_crop_num = 0\n        else:\n            file_crop_conf = np.asarray([x['confidence'] for x in file_crops]).mean()\n            file_crop_num = len(file_crops)\n            if 'importanceFraction' in file_crops[0].keys():\n                file_crop_importance = np.asarray([x['importanceFraction'] for x in file_crops]).mean()\n            else:\n                file_crop_importance = np.nan\n            crop_areas = []\n            image_area = img.shape[0]*img.shape[1]\n            for crophint in file_crops:\n                v_x, v_y = [], []\n                for vertices in crophint['boundingPoly']['vertices']:\n                    if 'x' not in vertices:\n                        v_x.append(0)\n                    else:\n                        v_x.append(vertices['x'])\n                    if 'y' not in vertices:\n                        v_y.append(0)\n                    else:\n                        v_y.append(vertices['y'])\n                crop_areas.append((max(v_x)-min(v_x))*(max(v_y)-min(v_y))/image_area)\n            file_crop_fraction_mean = np.mean(crop_areas)\n            file_crop_fraction_sum = np.sum(crop_areas)\n            file_crop_fraction_std = np.std(crop_areas)\n\n        df_metadata = {\n            'label_score_mean': file_label_score_mean,\n            'label_score_max': file_label_score_max,\n            'label_score_min': file_label_score_min,\n            'color_score': file_color_score,\n            'color_pixelfrac': file_color_pixelfrac,\n            'crop_conf': file_crop_conf,\n            'crop_importance': file_crop_importance,\n            'color_red_mean':color_red_mean,\n            'color_green_mean':color_green_mean,\n            'color_blue_mean':color_blue_mean,\n            'color_red_std':color_red_std,\n            'color_green_std':color_green_std,\n            'color_blue_std':color_blue_std,\n#             'crop_area_mean':file_crop_fraction_mean,\n            'crop_area_sum':file_crop_fraction_sum,\n#             'crop_area_std':file_crop_fraction_std,\n            'annots_top_desc': self.sentence_sep.join(file_top_desc),\n            'img_aratio':img.shape[0]/img.shape[1],\n#             'text_annotation':textanno,\n            'text_len':textlen,\n            'textblock_num':textblock_num,\n            'face_annotation':faceanno\n        }\n        \n        return df_metadata\n    \n# 读取文件的函数\ndef extract_additional_features(pet_id, mode='train'):\n    sentiment_filename = '../input/petfinder-adoption-prediction/{}_sentiment/{}.json'.format(mode, pet_id)\n    try:\n        sentiment_file = pet_parser.open_sentiment_file(sentiment_filename)\n        df_sentiment = pet_parser.parse_sentiment_file(sentiment_file)\n        df_sentiment['PetID'] = pet_id\n    except FileNotFoundError:\n        df_sentiment = None\n\n    dfs_metadata = []\n    for ind in range(1,200):\n        metadata_filename = '../input/petfinder-adoption-prediction/{}_metadata/{}-{}.json'.format(mode, pet_id, ind)\n        image_filename = '../input/petfinder-adoption-prediction/{}_images/{}-{}.jpg'.format(mode, pet_id, ind)\n        try:\n            image = cv2.imread(image_filename)\n            metadata_file = pet_parser.open_metadata_file(metadata_filename)\n            df_metadata = pet_parser.parse_metadata_file(metadata_file, image)\n            df_metadata['PetID'] = pet_id\n            dfs_metadata.append(df_metadata)\n        except FileNotFoundError:\n            break\n    return [df_sentiment, dfs_metadata]\n    \npet_parser = PetFinderParser()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"确认显存资源："},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"import pynvml\n# 在进入图片处理前，确认一眼我们分配的显存\npynvml.nvmlInit()\n# 这里的0是GPU id\nhandle = pynvml.nvmlDeviceGetHandleByIndex(0)\nmeminfo = pynvml.nvmlDeviceGetMemoryInfo(handle)\nprint(meminfo.total) #第一块显卡总的显存大小\nprint(meminfo.used)#这里是字节bytes，所以要想得到以兆M为单位就需要除以1024**2\nprint(meminfo.free) #第一块显卡剩余显存大小\nprint(pynvml.nvmlDeviceGetCount())#显示有几块GPU","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true},"cell_type":"markdown","source":"## 读取json文件\n这一步从json文件中读取了三类数据，包括metadata(元数据)，sentiment(情感评分)， text（文本数据）"},{"metadata":{"_uuid":"1b8c6eedb089ac6df143592de6455c1978b460f6","collapsed":true,"trusted":false},"cell_type":"code","source":"import cv2\nimport pandas as pd\nimport numpy as np\nimport os\nfrom tqdm import tqdm, tqdm_notebook\nfrom keras.applications.densenet import preprocess_input, DenseNet121\nimport itertools\n# 读取宠物的PetID（Unique）\ndebug = False\ntrain_pet_ids = train.PetID.unique()\ntest_pet_ids = test.PetID.unique()\n\nif debug:\n    train_pet_ids = train_pet_ids[:1000]\n    test_pet_ids = test_pet_ids[:500]\n\nn_jobs = 6\n# Train set:\n# 并行数据读取\ndfs_train = Parallel(n_jobs=n_jobs, verbose=1)(\n    delayed(extract_additional_features)(i, mode='train') for i in train_pet_ids)\n\n# 将读取得到的信息特征提取，并转换为dataframe\ntrain_dicts_sentiment = [x[0] for x in dfs_train if x[0] is not None]\ntrain_dfs_metadata = [x[1] for x in dfs_train if len([x[1]])>0]\n\ntrain_dfs_sentiment = pd.DataFrame(train_dicts_sentiment)\ntrain_dfs_metadata = list(itertools.chain.from_iterable(train_dfs_metadata))\ntrain_dfs_metadata = pd.DataFrame(train_dfs_metadata)\n\nprint(train_dfs_sentiment.shape, train_dfs_metadata.shape)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"# 测试集读取：\ndfs_test = Parallel(n_jobs=n_jobs, verbose=1)(\n    delayed(extract_additional_features)(i, mode='test') for i in test_pet_ids)\n\ntest_dicts_sentiment = [x[0] for x in dfs_test if x[0] is not None]\ntest_dfs_metadata = [x[1] for x in dfs_test if len(x[1])>0]\n\ntest_dfs_sentiment = pd.DataFrame(test_dicts_sentiment)\ntest_dfs_metadata = list(itertools.chain.from_iterable(test_dfs_metadata))\ntest_dfs_metadata = pd.DataFrame(test_dfs_metadata)\n\nprint(test_dfs_sentiment.shape, test_dfs_metadata.shape)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"# 再确认一次GPU\nhandle = pynvml.nvmlDeviceGetHandleByIndex(0)\nmeminfo = pynvml.nvmlDeviceGetMemoryInfo(handle)\nprint(meminfo.total) #第一块显卡总的显存大小\nprint(meminfo.used)#这里是字节bytes，所以要想得到以兆M为单位就需要除以1024**2\nprint(meminfo.free) #第一块显卡剩余显存大小\nprint(pynvml.nvmlDeviceGetCount())#显示有几块GPU","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"76489c2957389009c1b55ec2142ebf5dd9d7a923"},"cell_type":"markdown","source":"### 根据PetID，提取特征，并合并特征"},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"# 分别处理metadata和sentiment，最终会在merge部分和main DFs融合。\nmeta_df = pd.concat([train_dfs_metadata, test_dfs_metadata], sort=False).reset_index(drop=True)\nsenti_df = pd.concat([train_dfs_sentiment, test_dfs_sentiment], sort=False).reset_index(drop=True)\n\n# 根据排序读取文件\nmetadata_desc = meta_df.groupby(['PetID'])['annots_top_desc'].unique().reset_index()\nmetadata_desc['meta_annots_top_desc'] = metadata_desc['annots_top_desc'].apply(lambda x: '; '.join(x))\nmetadata_desc.drop('annots_top_desc', axis=1, inplace=True)\n\npossible_annots = set()\nfor i in range(len(meta_df)):\n    possible_annots = possible_annots.union(set(meta_df['annots_top_desc'].iloc[i].split('; ')))\nannot_mapper = {}\nfor idx, a in enumerate(possible_annots):\n    annot_mapper[a] = str(idx)\nmetadata_desc['meta_desc'] = metadata_desc['meta_annots_top_desc'].apply(lambda x: ' '.join(annot_mapper[i] for i in x.split('; ')))","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"# 情感特征，包括词长，内容等\nsenti_df['sentiment_entities'].fillna('', inplace=True)\nsenti_df['sentiment_entities'] = senti_df['sentiment_entities'].str.lower()\nsenti_df['sentiment_len'] = senti_df['sentiment_entities'].apply(lambda x:len(x))\nsenti_df['sentiment_word_len'] = senti_df['sentiment_entities'].apply(lambda x: len(x.replace(';',' ').split(' ')))\nsenti_df['sentiment_word_unique'] = senti_df['sentiment_entities'].apply(lambda x: len(set(x.replace(';',' ').split(' '))))\nsenti_df['doc_language'] = pd.factorize(senti_df['doc_language'])[0]\n\n","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"senti_df.head()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"# metadata, 因为每一个宠物都有着很多个json文件，也就对应着大量的数据。我们需要通过聚合，将特征通过均值、最大值、最小值或者平均值反映出来。\naggregates = {\n    'color_blue_mean':['mean','std'],\n    'color_blue_std':['mean'],\n    'color_green_mean':['mean','std'], \n    'color_green_std':['mean'],\n    'color_pixelfrac':['mean','std'],\n    'color_red_mean':['mean','std'],\n    'color_red_std':['mean'],\n    'color_score':['mean','max'], \n    'crop_area_sum':['mean','std','min'], \n    'crop_conf':['mean','std','max'],\n    'crop_importance':['mean','std'],\n    'label_score_max':['mean','std','max'],\n    'label_score_mean':['mean','max','std'],\n    'label_score_min':['mean','max','std'],\n    'img_aratio':['nunique','std','max','min'],\n    'textblock_num':['mean','max'],\n    'face_annotation':['mean','nunique']\n}\n\n# Train\nmetadata_gr = meta_df.drop(['annots_top_desc'], axis=1)\nfor i in metadata_gr.columns:\n    if 'PetID' not in i:\n        metadata_gr[i] = metadata_gr[i].astype(float)\nmetadata_gr = metadata_gr.groupby(['PetID']).agg(aggregates)\nmetadata_gr.columns = pd.Index(['{}_{}_{}'.format('meta', c[0], c[1].upper()) for c in metadata_gr.columns.tolist()])\nmetadata_gr = metadata_gr.reset_index()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"meta_df = metadata_desc.merge(metadata_gr, on='PetID', how='left')\n\n# annotation feature\nmeta_df['meta_annots_top_desc'].fillna(' ', inplace=True)\n\nmeta_df[meta_df['meta_textblock_num_MEAN']>0.8].head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"132f5680377aa571312a860adb1506cb05ef37b7"},"cell_type":"markdown","source":"### 拷贝一个新的train_proc用于做后续处理\n防止误操作"},{"metadata":{"_uuid":"947ecbc24175147fb4808f7d85068a851d39cd57","collapsed":true,"trusted":false},"cell_type":"code","source":"# Train merges:\ntrain_proc = train.copy()\n# Test merges:\ntest_proc = test.copy()\n\n\nprint(train_proc.shape, test_proc.shape)\nassert train_proc.shape[0] == train.shape[0]\nassert test_proc.shape[0] == test.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"train_proc.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dc1c92dca3d8e7d3bee4e7c42c24def791e9df28"},"cell_type":"markdown","source":"### 添加breed（品种）遍历\n\n在原数据中，breed仅仅只是一个编号，而官方提供给我们breed label来遍历breed,这样我们还可以通过breed_rating这一数据来添加额外数据"},{"metadata":{"_uuid":"a44dda1090936393134d00e5024b74c56935b4ba","collapsed":true,"trusted":false},"cell_type":"code","source":"# 以breed1为主breed\ntrain_breed_main = train_proc[['Breed1']].merge(\n    labels_breed, how='left',\n    left_on='Breed1', right_on='BreedID',\n    suffixes=('', '_main_breed'))\n\ntrain_breed_main = train_breed_main.iloc[:, 2:]\ntrain_breed_main = train_breed_main.add_prefix('main_breed_')\n# 第二品种\ntrain_breed_second = train_proc[['Breed2']].merge(\n    labels_breed, how='left',\n    left_on='Breed2', right_on='BreedID',\n    suffixes=('', '_second_breed'))\n\ntrain_breed_second = train_breed_second.iloc[:, 2:]\ntrain_breed_second = train_breed_second.add_prefix('second_breed_')\n\n# 添加两列用来记录breed\ntrain_proc = pd.concat(\n    [train_proc, train_breed_main, train_breed_second], axis=1)\n\n# 测试集做同样操作\ntest_breed_main = test_proc[['Breed1']].merge(\n    labels_breed, how='left',\n    left_on='Breed1', right_on='BreedID',\n    suffixes=('', '_main_breed'))\n\ntest_breed_main = test_breed_main.iloc[:, 2:]\ntest_breed_main = test_breed_main.add_prefix('main_breed_')\n\ntest_breed_second = test_proc[['Breed2']].merge(\n    labels_breed, how='left',\n    left_on='Breed2', right_on='BreedID',\n    suffixes=('', '_second_breed'))\n\ntest_breed_second = test_breed_second.iloc[:, 2:]\ntest_breed_second = test_breed_second.add_prefix('second_breed_')\n\n\ntest_proc = pd.concat(\n    [test_proc, test_breed_main, test_breed_second], axis=1)\n\nprint(train_proc.shape, test_proc.shape)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"train_proc.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Image Features\n\n我们最终仅通过DenseNet进行图片特征提取，且由于每一个宠物的多张图片特征值相似，于是每个宠物仅提取第一张图片。"},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"# FROM: https://www.kaggle.com/myltykritik/simple-lgbm-image-features\n# 图片处理需要定义图片的大小以及每一次打包的文件大小。\nimg_size = 256\nbatch_size = 16","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"pet_ids = train['PetID'].values\nn_batches = len(pet_ids) // batch_size + 1","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"def resize_to_square(im):\n    # 图片标准化，将读取的图片信息重新编成256*256*3的RGB图片。\n    old_size = im.shape[:2] # old_size is in (height, width) format\n    ratio = float(img_size)/max(old_size)\n    new_size = tuple([int(x*ratio) for x in old_size])\n    im = cv2.resize(im, (new_size[1], new_size[0]))\n    delta_w = img_size - new_size[1]\n    delta_h = img_size - new_size[0]\n    top, bottom = delta_h//2, delta_h-(delta_h//2)\n    left, right = delta_w//2, delta_w-(delta_w//2)\n    color = [0, 0, 0]\n    new_im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT,value=color)\n    return new_im\n\ndef load_image(path, pet_id,i=1):\n    # 读取图片的函数。我们将i的默认值设为1，是为了在仅读取一个petID的一张图片时用\n    image = cv2.imread(f'{path}{pet_id}-{i}.jpg')\n    new_image = resize_to_square(image)\n    new_image = preprocess_input(new_image)\n    return new_image","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"# 通过使用别人训练好的DenseNet分类器分类特征\nfrom keras.models import Model\nfrom keras.layers import GlobalAveragePooling2D, Input, Lambda, AveragePooling1D\nimport keras.backend as K\ninp = Input((256,256,3))\nbackbone = DenseNet121(input_tensor = inp, \n                       weights=\"../input/densenet/DenseNet-BC-121-32-no-top.h5\",\n                       include_top = False)\nx = backbone.output\nx = GlobalAveragePooling2D()(x)\nx = Lambda(lambda x: K.expand_dims(x,axis = -1))(x)\nx = AveragePooling1D(4)(x)\nout = Lambda(lambda x: x[:,:,0])(x)\n\nm = Model(inp,out)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"# 图片特征提取，前面注释的部分是遍历每一张图片，最终我们只遍历每一个PetID的第一张图片。\n# 在此隆重介绍装逼神奇tqdm!这个可以让长循环生成进度条的工具实在是太舒服了。。。\n\"\"\"\nfeatures = {}\nfor b in tqdm_notebook(range(n_batches)):\n    start = b*batch_size\n    end = (b+1)*batch_size\n    batch_pets = pet_ids_list[start:end]\n    batch_images = np.zeros((len(batch_pets),img_size,img_size,3))\n    for i,pet_id in enumerate(batch_pets):\n        for j in range(int(train[train.PetID==pet_id]['PhotoAmt'])):\n            try:\n                batch_images[i] = load_image(\"../input/petfinder-adoption-prediction/train_images/\", pet_id,j)\n            except:\n                pass\n    batch_preds = m.predict(batch_images)\n    for i,pet_id in enumerate(batch_pets):\n        if pet_id in features.keys():\n            sum += batch_preds[i]\n            count += 1\n        else:\n            sum = batch_preds[i]\n            count = 0\n        if count == 0:\n            count = 0.1\n        features[pet_id] = sum/count\n        \n\"\"\"\nfeatures = {}\n# 将工作分为n个batch。不知道有什么用。。。可能是为了让进度条看起来好看一点\nfor b in tqdm_notebook(range(n_batches)):\n    start = b*batch_size\n    end = (b+1)*batch_size\n    batch_pets = pet_ids[start:end]\n    batch_images = np.zeros((len(batch_pets),img_size,img_size,3))\n    # 对每个petID进行特征提取\n    for i,pet_id in enumerate(batch_pets):\n        try:\n            batch_images[i] = load_image(\"../input/petfinder-adoption-prediction/train_images/\", pet_id)\n        except:\n            pass\n    batch_preds = m.predict(batch_images)\n    for i,pet_id in enumerate(batch_pets):\n        features[pet_id] = batch_preds[i]\n\n","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"train_feats = pd.DataFrame.from_dict(features, orient='index')\ntrain_feats.columns = ['pic_'+str(i) for i in range(train_feats.shape[1])]","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"# 测试集遍历\n\"\"\"\nsum = 0\npet_ids = test_proc['PetID'].values\npet_ids_list = []\nfor k in range(len(pet_ids)):\n    sum += int(test[test.PetID==pet_ids[k]]['PhotoAmt'])\n    for j in range(int(test[test.PetID==pet_ids[k]]['PhotoAmt'])):\n        pet_ids_list.append(pet_ids[k])\nn_batches = sum // batch_size + 1\nprint(pet_ids_list[0:20])\n\n\nfeatures = {}\nfor b in tqdm_notebook(range(n_batches)):\n    start = b*batch_size\n    end = (b+1)*batch_size\n    batch_pets = pet_ids_list[start:end]\n    batch_images = np.zeros((len(batch_pets),img_size,img_size,3))\n    for i,pet_id in enumerate(batch_pets):\n        for j in range(int(test[test.PetID==pet_id]['PhotoAmt'])):\n            try:\n                batch_images[i] = load_image(\"../input/petfinder-adoption-prediction/test_images/\", pet_id,j)\n            except:\n                pass\n    batch_preds = m.predict(batch_images)\n    for i,pet_id in enumerate(batch_pets):\n        if pet_id in features.keys():\n            sum += batch_preds[i]\n            count += 1\n        else:\n            sum = batch_preds[i]\n            count = 0\n        if count == 0:\n            count = 0.1\n        features[pet_id] = sum/count\n        \n\"\"\"\npet_ids = test['PetID'].values\nn_batches = len(pet_ids) // batch_size + 1\n\nfeatures = {}\nfor b in tqdm_notebook(range(n_batches)):\n    start = b*batch_size\n    end = (b+1)*batch_size\n    batch_pets = pet_ids[start:end]\n    batch_images = np.zeros((len(batch_pets),img_size,img_size,3))\n    for i,pet_id in enumerate(batch_pets):\n        try:\n            batch_images[i] = load_image(\"../input/petfinder-adoption-prediction/test_images/\", pet_id)\n        except:\n            pass\n    batch_preds = m.predict(batch_images)\n    for i,pet_id in enumerate(batch_pets):\n        features[pet_id] = batch_preds[i]\n","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"# 再一次查看GPU\n\npynvml.nvmlInit()\n# 这里的1是GPU id\nhandle = pynvml.nvmlDeviceGetHandleByIndex(0)\nmeminfo = pynvml.nvmlDeviceGetMemoryInfo(handle)\nprint(meminfo.total) #第1块显卡总的显存大小\nprint(meminfo.used)#这里是字节bytes，所以要想得到以兆M为单位就需要除以1024**2\nprint(meminfo.free) #第1块显卡剩余显存大小\nprint(pynvml.nvmlDeviceGetCount())#显示有几块GPU","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 生成特征列\n\n这里我们将不同类型的特征列区分开来，以便后续操作（例如统一处理文本数据等）。"},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"test_feats = pd.DataFrame.from_dict(features, orient='index')\ntest_feats.columns = ['pic_'+str(i) for i in range(test_feats.shape[1])]","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"test_feats = test_feats.reset_index()\ntest_feats.rename({'index': 'PetID'}, axis='columns', inplace=True)\n\ntrain_feats = train_feats.reset_index()\ntrain_feats.rename({'index': 'PetID'}, axis='columns', inplace=True)\n\ntrain = pd.merge(train, train_feats, how='left', on='PetID')\ntest = pd.merge(test, test_feats, how='left', on='PetID')","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"train_proc = pd.merge(train_proc, train_feats, how='left', on='PetID')\ntest_proc = pd.merge(test_proc, test_feats, how='left', on='PetID')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"509754ac9784439024c118860ceef27edbe4ce84"},"cell_type":"markdown","source":"### 链接训练集与测试集\n\n将训练集与测试集链接起来，统一处理。\n观察数据集当中的NaN（空数据）:\n其中，`AdoptionSpeed` 是目标数据，在Test中必然是空集"},{"metadata":{"_uuid":"a5561dc64fb58b47da60da90a8fa1aaa35349cce","collapsed":true,"scrolled":true,"trusted":false},"cell_type":"code","source":"X = pd.concat([train_proc, test_proc], ignore_index=True, sort=False)\nprint('NaN structure:\\n{}'.format(np.sum(pd.isnull(X))))","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"X.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fb503cf4d614ea3f7edd103f5d0cc570986ad0a2"},"cell_type":"markdown","source":"### 观察不同类型的数据:\n\n- integer的列一般都是categorical的特征，不需要做过多操作\n- float的列一般都是数值型特征\n- object的列比较特别，包括文本和string，这些是后面需要文本处理的列"},{"metadata":{"_uuid":"3528d7e8036c8b8bb0c5da254d9cd18d2019607d","collapsed":true,"scrolled":true,"trusted":false},"cell_type":"code","source":"column_types = X.dtypes\n\nint_cols = column_types[column_types == 'int']\nfloat_cols = column_types[column_types == 'float']\ncat_cols = column_types[column_types == 'object']\n\nprint('\\tinteger columns:\\n{}'.format(int_cols))\nprint('\\n\\tfloat columns:\\n{}'.format(float_cols))\nprint('\\n\\tto encode categorical columns:\\n{}'.format(cat_cols))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a424da889496d55188c3c8478f0035c9fa8a7555"},"cell_type":"markdown","source":"### feature engineering:"},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"# 首先，我们先定义一个to_drop_columns，这个list用于保存不需要进入模型训练的数据列名\nto_drop_columns = ['PetID', 'Name', 'RescuerID']\n# 创建X_temp作为特征工程的操作列\n# 这样可以避免一些误操作\nX_temp = X.copy()\nrescuer_ids = X_temp['RescuerID'].values\n\n# 通过将3列color和2列breed合并，重新编码进行降维\nX_temp['Breed_full'] = X_temp['Breed1'].astype(str)+'_'+X_temp['Breed2'].astype(str)\nX_temp['Color_full'] = X_temp['Color1'].astype(str)+'_'+X_temp['Color2'].astype(str)+'_'+X_temp['Color3'].astype(str)\nX_temp['Breed_full'],_ = pd.factorize(X_temp['Breed_full'])\nX_temp['Color_full'],_ = pd.factorize(X_temp['Color_full'])\n\nto_drop_columns.extend(['Breed1','Breed2','Color1','Color2','Color3'])\n","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"# 添加姓名特征，因为姓名本身的意义并不大，但是可以通过操作来得到一些有用的特征\nimport re\npattern = re.compile(r\"[0-9\\.:!]\")\nX_temp['empty_name'] = X_temp['Name'].isnull().astype(np.int8)\nX_temp['Name'] =X_temp['Name'].fillna('')\nX_temp['name_len'] = X_temp['Name'].apply(lambda x: len(x))\nX_temp['strange_name'] = X_temp['Name'].apply(lambda x: len(pattern.findall(x))>0).astype(np.int8)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"# 重新编码Vacinated, Dewormed, Sterilized等特征。目的也是降维\n\nX_temp['hard_interaction'] = X_temp['Type'].astype(str)+X_temp['Gender'].astype(str)+ \\\n                              X_temp['Vaccinated'].astype(str)+'_'+ \\\n                              X_temp['Dewormed'].astype(str)+'_'+X_temp['Sterilized'].astype(str)\nX_temp['hard_interaction'],_ = pd.factorize(X_temp['hard_interaction'])\n\nX_temp['MaturitySize'] = X_temp['MaturitySize'].replace(0, np.nan)\nX_temp['FurLength'] = X_temp['FurLength'].replace(0, np.nan)\n\nX_temp['Vaccinated'] = X_temp['Vaccinated'].replace(3, np.nan)\nX_temp['Vaccinated'] = X_temp['Vaccinated'].replace(2, 0)\n\nX_temp['Dewormed'] = X_temp['Dewormed'].replace(3, np.nan)\nX_temp['Dewormed'] = X_temp['Dewormed'].replace(2, 0)\n\nX_temp['Sterilized'] = X_temp['Sterilized'].replace(3, np.nan)\nX_temp['Sterilized'] = X_temp['Sterilized'].replace(2, 0)\n\n\nX_temp['Health'] = X_temp['Health'].replace(0, np.nan)\nto_drop_columns.extend","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"to_drop_columns.extend(['Type','Gender','Vaccinated','Dewormed','Sterilized'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 文本处理\n\n接下来通过一些操作对文本类数据（如Description）进行处理。\n\n文本处理参考：https://www.kaggle.com/bminixhofer/6th-place-solution-code"},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"# nltk是常用的自然语言处理包\n\nfrom nltk.tokenize import TweetTokenizer\nimport nltk\nisascii = lambda s: len(s) == len(s.encode())\n# Tokenizer是一个分词器，他可以将文本转换为向量形式。\ntknzr = TweetTokenizer()\n# jieba是一个中文分词器（不要问我为什么还有中文，这个组织是马来西亚的）\nimport jieba\n\n# PorterStemmer和SnowballStemmer两者都是提取词干的算法\nfrom nltk.stem import PorterStemmer\n\nps = PorterStemmer()\nfrom nltk.stem.lancaster import LancasterStemmer\nlc = LancasterStemmer()\nfrom nltk.stem import SnowballStemmer\nsb = SnowballStemmer(\"english\")\n\ndef custom_tokenizer(text):\n    init_doc = tknzr.tokenize(text)\n    retval = []\n    for t in init_doc:\n        if isascii(t): \n            retval.append(t)\n        else:\n            for w in t:\n                retval.append(w)\n    return retval","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"X_temp['Description'] = X_temp['Description'].fillna(' ')\n\n# 分别对中文和英文进行处理\nenglish_desc, chinese_desc = [], []\ntokens = set()\nword_dict = {}\npos_count, word_count = 1, 1 \npos_dict = {}\neng_sequences = []\npos_sequences = []\nfor i in range(len(X_temp)):\n    e_d, c_d, eng_seq, pos_seq = [], [], [], []\n    doc = custom_tokenizer(X_temp['Description'].iloc[i])\n    for token in doc:\n        if not isascii(token):\n            c_d.append(token)\n        else:\n            e_d.append(token)\n            if token not in word_dict:\n                word_dict[token] = word_count\n                word_count +=1\n    english_desc.append(' '.join(e_d))\n    chinese_desc.append(' '.join(c_d))\n    pos_seq = nltk.pos_tag(e_d)\n    for t in pos_seq:\n        if t[1] not in pos_dict:\n            pos_dict[t[1]] = pos_count\n            pos_count += 1\n    pos_seq = [pos_dict[t[1]] for t in pos_seq]\n    eng_seq = [word_dict[t] for t in e_d]\n    if len(eng_seq)==0:\n        eng_seq.append(0)\n        pos_seq.append(0)\n    eng_sequences.append(eng_seq)\n    pos_sequences.append(pos_seq)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"# 从所提取的文本信息中提取特征，特征主要有文本长度，单词长度，单词数量等等。\n# 这些特征不一定有用，我们最后会判断这些特征是否有用，这里先提取出来即可。\n\nX_temp['English_desc'] = english_desc\nX_temp['Chinese_desc'] = chinese_desc\n\nX_temp['e_description_len'] = X_temp['English_desc'].apply(lambda x:len(x))\nX_temp['e_description_word_len'] = X_temp['English_desc'].apply(lambda x: len(x.split(' ')))\nX_temp['e_description_word_unique'] = X_temp['English_desc'].apply(lambda x: len(set(x.split(' '))))\n\nX_temp['c_description_len'] = X_temp['Chinese_desc'].apply(lambda x:len(x))\nX_temp['c_description_word_len'] = X_temp['Chinese_desc'].apply(lambda x:len(x.split(' ')))\nX_temp['c_description_word_unique'] = X_temp['Chinese_desc'].apply(lambda x: len(set(x)))\n\nX_temp['description_len'] = X_temp['Description'].apply(lambda x:len(x))\nX_temp['description_word_len'] = X_temp['Description'].apply(lambda x: len(x.split(' ')))\n\nto_drop_columns.extend(['English_desc','Description','Chinese_desc'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3a220d5a9cd973e97c894d57b1ef8f6c6e95fdc5","collapsed":true,"trusted":false},"cell_type":"code","source":"# 选择需要处理的列\ntext_columns = ['Description','English_desc','Chinese_desc']\ncategorical_columns = ['main_breed_BreedName', 'second_breed_BreedName']\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"68d8f68c17adb43e1c5c86a773058b09c208d721","collapsed":true,"scrolled":true,"trusted":false},"cell_type":"code","source":"# 和name和PetID都该被处理， RescuerID被处理后也会被丢掉\n\n# 计算出现次数\nrescuer_count = X.groupby(['RescuerID'])['PetID'].count().reset_index()\nrescuer_count.columns = ['RescuerID', 'RescuerID_COUNT']\n\n# 添加特征到目标DF\nX_temp = X_temp.merge(rescuer_count, how='left', on='RescuerID')","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"feat_df = X_temp[['PetID','Color1','Breed1','State','RescuerID','Name','Breed_full','Color_full','hard_interaction']]","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"# 继续处理其他特征\nagg = {\n    'Fee':['mean','std','max'],\n    'Breed1':['nunique'],\n    #'Gender':['nunique'],\n    'Age':['mean','std','max'], #,'min'\n    'Quantity':['std'],#'mean',,'min','max'\n    'PetID':['nunique']\n}\n# 处理颜色特征\nfeat = X_temp.groupby('Color1').agg(agg)\nfeat.columns = pd.Index(['COLOR_' + e[0] + \"_\" + e[1].upper() for e in feat.columns.tolist()])\nfeat.reset_index(inplace=True)\nfeat_df = feat_df.merge(feat, on='Color1', how='left')\n\nagg = {\n    'Fee':['mean','std','max'],\n    'Breed_full':['nunique'],\n    'Quantity':['sum'],\n}\nfeat = X_temp.groupby('Color_full').agg(agg)\nfeat.columns = pd.Index(['COLORfull_' + e[0] + \"_\" + e[1].upper() for e in feat.columns.tolist()])\nfeat.reset_index(inplace=True)\nfeat_df = feat_df.merge(feat, on='Color_full', how='left')","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"# Breed feature\nagg = {\n    'Color_full':['nunique'],\n    'Breed2':['nunique'],\n    'FurLength':['nunique'],\n    'Fee':['mean','max'],#,'min'\n    'Age':['mean','std','min','max'],\n    'Quantity':['mean','std','max','sum'],#'min'\n    'PetID':['nunique'],\n    'FurLength':['mean'],\n    'Health':['mean'],\n    'MaturitySize':['mean','std','min','max'],\n    'Vaccinated':['mean'],\n    'Dewormed':['mean'],\n    'Sterilized':['mean']\n}\nfeat = X_temp.groupby('Breed1').agg(agg)\nfeat.columns = pd.Index(['BREED1_' + e[0] + \"_\" + e[1].upper() for e in feat.columns.tolist()])\nfeat.reset_index(inplace=True)\nfeat_df = feat_df.merge(feat, on='Breed1', how='left')\n\n# Breed 特征\nagg = {\n    'Color_full':['nunique'],\n    'Fee':['mean','min','max'],\n    'Quantity':['sum'],\n    'PetID':['nunique']\n}\nfeat = X_temp.groupby('Breed_full').agg(agg)\nfeat.columns = pd.Index(['BREEDfull_' + e[0] + \"_\" + e[1].upper() for e in feat.columns.tolist()])\nfeat.reset_index(inplace=True)\nfeat_df = feat_df.merge(feat, on='Breed_full', how='left')","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"# State 特征\nagg = {\n    'Color_full':['nunique'],\n    'Breed_full':['nunique'],\n    'PetID':['nunique'],\n    'RescuerID':['nunique'],\n    'Fee':['mean','max'],\n    'Age':['mean','std','max'],\n    'Quantity':['mean','std','max'],#,'min','sum'\n    'FurLength':['mean','std'],\n    'Health':['mean'],\n    'MaturitySize':['mean','std'],\n    'Vaccinated':['mean'],\n    'Dewormed':['mean'],\n    'Sterilized':['mean'],\n    'VideoAmt':['mean','std'],\n    'PhotoAmt':['mean','std']\n}\nfeat = X_temp.groupby('State').agg(agg)\nfeat.columns = pd.Index(['STATE_' + e[0] + \"_\" + e[1].upper() for e in feat.columns.tolist()])\nfeat.reset_index(inplace=True)\nfeat_df = feat_df.merge(feat, on='State', how='left')","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"agg = {\n    'Fee':['mean','min','max'],\n    'Age':['mean','std','min','max'],\n    'Quantity':['mean','std','sum'],\n    'PetID':['nunique'],\n    'FurLength':['mean'],\n    'Health':['mean'],\n    'MaturitySize':['mean','std'],\n    'Vaccinated':['mean'],\n    'Dewormed':['mean'],\n    'Sterilized':['mean']\n}\nfeat = X_temp.groupby(['State','Breed1','Color1']).agg(agg)\nfeat.columns = pd.Index(['MULTI_' + e[0] + \"_\" + e[1].upper() for e in feat.columns.tolist()])\nfeat.reset_index(inplace=True)\nfeat_df = feat_df.merge(feat, on=['State','Breed1','Color1'], how='left')\n\nagg = {\n    'Fee':['mean','min','max'],\n    'Age':['mean','std','min','max'],\n    'Quantity':['mean','std','sum'],\n    'PetID':['nunique'],\n}\nfeat = X_temp.groupby(['State','Breed_full','Color_full']).agg(agg)\nfeat.columns = pd.Index(['MULTI2_' + e[0] + \"_\" + e[1].upper() for e in feat.columns.tolist()])\nfeat.reset_index(inplace=True)\nfeat_df = feat_df.merge(feat, on=['State','Breed_full','Color_full'], how='left')","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"# name feature\nfeat = X_temp.groupby('Name')['PetID'].agg({'name_count':'nunique'}).reset_index()\nfeat_df = feat_df.merge(feat, on='Name', how='left')","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"# 继续添加特征\nagg = {\n\n    'PetID':['nunique'],\n    'Breed_full':['nunique'],\n    'VideoAmt':['mean','std'],\n    'PhotoAmt':['mean','std'],\n    'Sterilized':['mean'],\n    'Dewormed':['mean'],\n    'Vaccinated':['mean']\n}\nrescuer_count = X_temp.groupby(['RescuerID']).agg(agg)\nrescuer_count.columns = pd.Index(['RESCUER_' + e[0] + \"_\" + e[1].upper() for e in rescuer_count.columns.tolist()])\nrescuer_count.reset_index(inplace=True)\nfeat_df = feat_df.merge(rescuer_count, how='left', on='RescuerID')","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"# State 特征\nagg = {\n    'Fee':['mean','min','max']\n}\nfeat = X_temp.groupby('hard_interaction').agg(agg)\nfeat.columns = pd.Index(['INTERACTION_' + e[0] + \"_\" + e[1].upper() for e in feat.columns.tolist()])\nfeat.reset_index(inplace=True)\nfeat_df = feat_df.merge(feat, on='hard_interaction', how='left')","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"feat_df.drop(['Color1','Breed1','State','RescuerID','Name','Breed_full','Color_full','hard_interaction'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a228cc25a0b400527671a077471cde07068cc975","collapsed":true,"trusted":false},"cell_type":"code","source":"# 将特征型的列重新编码\nfor i in categorical_columns:\n    X_temp.loc[:, i] = pd.factorize(X_temp.loc[:, i])[0]","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"X_temp.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c5e1043f2ba2ca95566ccfeb60ea04a470041c44","collapsed":true,"trusted":false},"cell_type":"code","source":"# 建立text数据子集\nX_text = X_temp[text_columns]\n\nfor i in X_text.columns:\n    X_text.loc[:, i] = X_text.loc[:, i].fillna('<MISSING>')","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"X_text = X_text.drop(['Chinese_desc'],axis =1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0f5d0c82ad86bf1010cda2db2f35c1e32f4406fa","collapsed":true,"scrolled":true,"trusted":false},"cell_type":"code","source":"# 通过TFIDF处理文本特征\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import SparsePCA, TruncatedSVD, LatentDirichletAllocation, NMF\n\nn_components = 5\ntext_features = []\n\n# 之前也提到了，其实TFIDF就是文本特征\n# 生成文本特征\nfor i in X_text.columns:\n    \n    # Initialize decomposition methods:\n    print('generating features from: {}'.format(i))\n    svd_ = TruncatedSVD(\n        n_components=n_components, random_state=1337)\n    nmf_ = NMF(\n        n_components=n_components, random_state=1337)\n    \n    tfidf_col = TfidfVectorizer().fit_transform(X_text.loc[:, i].values)\n    svd_col = svd_.fit_transform(tfidf_col)\n    svd_col = pd.DataFrame(svd_col)\n    svd_col = svd_col.add_prefix('SVD_{}_'.format(i))\n    \n    nmf_col = nmf_.fit_transform(tfidf_col)\n    nmf_col = pd.DataFrame(nmf_col)\n    nmf_col = nmf_col.add_prefix('NMF_{}_'.format(i))\n    \n    text_features.append(svd_col)\n    text_features.append(nmf_col)\n\n    \n# 结合所有文本数据\ntext_features = pd.concat(text_features, axis=1)\n\n# 接合文本数据到主DF\nX_temp = pd.concat([X_temp, text_features], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3ba80726173d872ca31e1c13e10256e05db0ff40","collapsed":true,"trusted":false},"cell_type":"code","source":"# 这一步本来是为了删除一些不必要的列的。。后来没删，但是后面的代码懒得改了，于是就多出来了一个X_proc\nX_proc = X_temp.copy()\n#for item in to_drop_columns:\n#    X_temp = X_temp.drop([item], axis=1)\n\n# Check final df shape:\n#print('X shape: {}'.format(X_temp.shape))","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"# 在merge之前观察一下我们的列\ncolumn_types = X.dtypes\n\nint_cols = column_types[column_types == 'int']\nfloat_cols = column_types[column_types == 'float']\ncat_cols = column_types[column_types == 'object']\n\nprint('\\tinteger columns:\\n{}'.format(int_cols))\nprint('\\n\\tfloat columns:\\n{}'.format(float_cols))\nprint('\\n\\tto encode categorical columns:\\n{}'.format(cat_cols))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# merge features"},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"# merge\nX_temp = X_temp.merge(senti_df, how='left', on='PetID')\nX_temp = X_temp.merge(meta_df, how='left', on='PetID')\nX_temp = X_temp.merge(feat_df, how='left', on='PetID')\n\nprint(X_temp.shape)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"X_temp.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5acc8411e12eb4f741956b1ae608017e10950040"},"cell_type":"markdown","source":"# train/test split:"},{"metadata":{"_uuid":"ef5a8fdbe2b786bbc3ffdc797cfa3ca02262b70e","collapsed":true,"trusted":false},"cell_type":"code","source":"# 将训练集和测试集分开\nX_train = X_temp.loc[np.isfinite(X_temp.AdoptionSpeed), :]\nX_test = X_temp.loc[~np.isfinite(X_temp.AdoptionSpeed), :]\n\n# 将测试集中的AdoptionSpeed列删除（理论上全部都是NaN）\nX_test = X_test.drop(['AdoptionSpeed'], axis=1)\n\n\nprint('X_train shape: {}'.format(X_train.shape))\nprint('X_test shape: {}'.format(X_test.shape))\n\nassert X_train.shape[0] == train.shape[0]\nassert X_test.shape[0] == test.shape[0]\n\n\n# 确认长度，保证分割正确性\ntrain_cols = X_train.columns.tolist()\ntrain_cols.remove('AdoptionSpeed')\n\ntest_cols = X_test.columns.tolist()\n\nassert np.all(train_cols == test_cols)\nrescuer_ids = rescuer_ids[:len(X_train)]\nassert len(rescuer_ids) == len(X_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0e0d21d076fc196facd083d6e3ae7cdbb1d575a3"},"cell_type":"markdown","source":"### NaN数据填补\n理论上不应该出现NaN数据了，因为之前都做过。但是以防万一再做一次（反正不亏）"},{"metadata":{"_uuid":"0d232ef1ddd004597ac58f171697d122d17e63d8","collapsed":true,"scrolled":true,"trusted":false},"cell_type":"code","source":"np.sum(pd.isnull(X_train))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ec6ee11a3b7d0ca97d3faf0cccd06bab9dd17637","collapsed":true,"scrolled":true,"trusted":false},"cell_type":"code","source":"np.sum(pd.isnull(X_test))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a87c86df9fd01e6c1b744a7d21b40d816ca66106"},"cell_type":"markdown","source":"### model training:\n\n在模型训练之前，我们需要定义几个函数。这几个函数是模型训练过程中用于判断模型训练是否良好的标准。"},{"metadata":{"_uuid":"7302b59080f81bde834d10e386dd35cdad394e12","collapsed":true,"trusted":false},"cell_type":"code","source":"import scipy as sp\n\nfrom collections import Counter\nfrom functools import partial\nfrom math import sqrt\n\nfrom sklearn.metrics import cohen_kappa_score, mean_squared_error\nfrom sklearn.metrics import confusion_matrix as sk_cmatrix\n\n# 这以下三个函数都是拿来计算QWK的。QWK的计算过程我们在报告中有写，可以参考。\n# taken from Ben Hamner's github repository： https://github.com/benhamner/Metrics\ndef confusion_matrix(rater_a, rater_b, min_rating=None, max_rating=None):\n    \"\"\"\n    返回两个rater产生的混淆矩阵\n    \"\"\"\n    assert(len(rater_a) == len(rater_b))\n    if min_rating is None:\n        min_rating = min(rater_a + rater_b)\n    if max_rating is None:\n        max_rating = max(rater_a + rater_b)\n    num_ratings = int(max_rating - min_rating + 1)\n    conf_mat = [[0 for i in range(num_ratings)]\n                for j in range(num_ratings)]\n    for a, b in zip(rater_a, rater_b):\n        conf_mat[a - min_rating][b - min_rating] += 1\n    return conf_mat\n\n\ndef histogram(ratings, min_rating=None, max_rating=None):\n    \"\"\"\n    直方图统计\n    \"\"\"\n    if min_rating is None:\n        min_rating = min(ratings)\n    if max_rating is None:\n        max_rating = max(ratings)\n    num_ratings = int(max_rating - min_rating + 1)\n    hist_ratings = [0 for x in range(num_ratings)]\n    for r in ratings:\n        hist_ratings[r - min_rating] += 1\n    return hist_ratings\n\n\ndef quadratic_weighted_kappa(y, y_pred):\n    # 计算并返回QWK\n    rater_a = y\n    rater_b = y_pred\n    min_rating=None\n    max_rating=None\n    rater_a = np.array(rater_a, dtype=int)\n    rater_b = np.array(rater_b, dtype=int)\n    assert(len(rater_a) == len(rater_b))\n    if min_rating is None:\n        min_rating = min(min(rater_a), min(rater_b))\n    if max_rating is None:\n        max_rating = max(max(rater_a), max(rater_b))\n    conf_mat = confusion_matrix(rater_a, rater_b,\n                                min_rating, max_rating)\n    num_ratings = len(conf_mat)\n    num_scored_items = float(len(rater_a))\n\n    hist_rater_a = histogram(rater_a, min_rating, max_rating)\n    hist_rater_b = histogram(rater_b, min_rating, max_rating)\n\n    numerator = 0.0\n    denominator = 0.0\n\n    for i in range(num_ratings):\n        for j in range(num_ratings):\n            expected_count = (hist_rater_a[i] * hist_rater_b[j]\n                              / num_scored_items)\n            d = pow(i - j, 2.0) / pow(num_ratings - 1, 2.0)\n            numerator += d * conf_mat[i][j] / num_scored_items\n            denominator += d * expected_count / num_scored_items\n\n    return (1.0 - numerator / denominator)\n\n\ndef to_bins(x, borders):\n    # 封装函数\n    for i in range(len(borders)):\n        if x <= borders[i]:\n            return i\n    return len(borders)\n\nclass OptimizedRounder(object):\n    # 这个类是在模型输出结果的时候用的。\n    # 每个模型训练后，都会输出一个numpy的矩阵，而这几个矩阵通过这个函数结合起来后，可以得到我们最终的stacking结果\n    # 这个函数可以在单个模型中用，也可以在stacking后的模型用\n    def __init__(self):\n        self.coef_ = 0\n\n    def _loss(self, coef, X, y, idx):\n        # 定义损失函数\n        X_p = np.array([to_bins(pred, coef) for pred in X])\n        ll = -quadratic_weighted_kappa(y, X_p)\n        return ll\n    \n    def _kappa_loss(self, coef, X, y):\n        # 定义卡帕损失系数\n        X_p = np.copy(X)\n        for i, pred in enumerate(X_p):\n            if pred < coef[0]:\n                X_p[i] = 0\n            elif pred >= coef[0] and pred < coef[1]:\n                X_p[i] = 1\n            elif pred >= coef[1] and pred < coef[2]:\n                X_p[i] = 2\n            elif pred >= coef[2] and pred < coef[3]:\n                X_p[i] = 3\n            else:\n                X_p[i] = 4\n\n        ll = quadratic_weighted_kappa(y, X_p)\n        return -ll\n\n    def fit(self, X, y):\n        # 定义拟合\n        coef = [1.5, 2.0, 2.5, 3.0]\n        golden1 = 0.618\n        golden2 = 1 - golden1\n        ab_start = [(1, 2), (1.5, 2.5), (2, 3), (2.5, 3.5)]\n        for it1 in range(10):\n            for idx in range(4):\n                # golden section search\n                a, b = ab_start[idx]\n                # calc losses\n                coef[idx] = a\n                la = self._loss(coef, X, y, idx)\n                coef[idx] = b\n                lb = self._loss(coef, X, y, idx)\n                for it in range(20):\n                    # choose value\n                    if la > lb:\n                        a = b - (b - a) * golden1\n                        coef[idx] = a\n                        la = self._loss(coef, X, y, idx)\n                    else:\n                        b = b - (b - a) * golden2\n                        coef[idx] = b\n                        lb = self._loss(coef, X, y, idx)\n        self.coef_ = {'x': coef}\n\n    def predict(self, X, coef):\n        X_p = np.array([to_bins(pred, coef) for pred in X])\n        return X_p\n\n    def coefficients(self):\n        return self.coef_['x']\n    \ndef rmse(actual, predicted):\n    return sqrt(mean_squared_error(actual, predicted))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 计算优化效果"},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"# 以下几个函数，是用来调参的时候查看输出结果的。\n\ndef val_kappa(preds, train_data):\n    labels = train_data.get_label()\n    preds = np.argmax(preds.reshape((-1,5)), axis=1)\n    \n    return 'qwk', quadratic_weighted_kappa(labels, preds), True\n\ndef val_kappa_reg(preds, train_data, cdf):\n    labels = train_data.get_label()\n    preds = getTestScore2(preds, cdf)\n    return 'qwk', quadratic_weighted_kappa(labels, preds), True\n\ndef get_cdf(hist):\n    return np.cumsum(hist/np.sum(hist))\n\ndef getScore(pred, cdf, valid=False):\n    num = pred.shape[0]\n    output = np.asarray([4]*num, dtype=int)\n    rank = pred.argsort()\n    output[rank[:int(num*cdf[0]-1)]] = 0\n    output[rank[int(num*cdf[0]):int(num*cdf[1]-1)]] = 1\n    output[rank[int(num*cdf[1]):int(num*cdf[2]-1)]] = 2\n    output[rank[int(num*cdf[2]):int(num*cdf[3]-1)]] = 3\n    if valid:\n        cutoff = [ pred[rank[int(num*cdf[i]-1)]] for i in range(4) ]\n        return output, cutoff\n    return output\n\ndef getTestScore(pred, cutoff):\n    num = pred.shape[0]\n    output = np.asarray([4]*num, dtype=int)\n    for i in range(num):\n        if pred[i] <= cutoff[0]:\n            output[i] = 0\n        elif pred[i] <= cutoff[1]:\n            output[i] = 1\n        elif pred[i] <= cutoff[2]:\n            output[i] = 2\n        elif pred[i] <= cutoff[3]:\n            output[i] = 3\n    return output\n\ndef getTestScore2(pred, cdf):\n    num = pred.shape[0]\n    rank = pred.argsort()\n    output = np.asarray([4]*num, dtype=int)\n    output[rank[:int(num*cdf[0]-1)]] = 0\n    output[rank[int(num*cdf[0]):int(num*cdf[1]-1)]] = 1\n    output[rank[int(num*cdf[1]):int(num*cdf[2]-1)]] = 2\n    output[rank[int(num*cdf[2]):int(num*cdf[3]-1)]] = 3\n    return output\n\ndef rmse(actual, predicted):\n    return sqrt(mean_squared_error(actual, predicted))\n\nisascii = lambda s: len(s) == len(s.encode())\n\ndef custom_tokenizer(text):\n    init_doc = tknzr.tokenize(text)\n    retval = []\n    for t in init_doc:\n        if isascii(t): \n            retval.append(t)\n        else:\n            for w in t:\n                retval.append(w)\n    return retval\n\ndef build_emb_matrix(word_dict, emb_dict):\n    embed_size = 300\n    nb_words = len(word_dict)+1000\n    nb_oov = 0\n    embedding_matrix = np.zeros((nb_words, embed_size), dtype=np.float32)\n    unknown_vector = np.zeros((embed_size,), dtype=np.float32) - 1.\n    for key in tqdm(word_dict):\n        word = key\n        embedding_vector = emb_dict.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[word_dict[key]] = embedding_vector\n            continue\n        word = key.lower()\n        embedding_vector = emb_dict.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[word_dict[key]] = embedding_vector\n            continue\n        word = key.upper()\n        embedding_vector = emb_dict.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[word_dict[key]] = embedding_vector\n            continue\n        word = key.capitalize()\n        embedding_vector = emb_dict.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[word_dict[key]] = embedding_vector\n            continue\n        word = ps.stem(key)\n        embedding_vector = emb_dict.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[word_dict[key]] = embedding_vector\n            continue\n        word = lc.stem(key)\n        embedding_vector = emb_dict.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[word_dict[key]] = embedding_vector\n            continue\n        word = sb.stem(key)\n        embedding_vector = emb_dict.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[word_dict[key]] = embedding_vector\n            continue\n        nb_oov+=1\n        embedding_matrix[word_dict[key]] = unknown_vector                    \n    return embedding_matrix, nb_words, nb_oov\n\ndef _init_esim_weights(module):\n    if isinstance(module, nn.Linear):\n        nn.init.kaiming_normal_(module.weight.data)\n        nn.init.constant_(module.bias.data, 0.0)\n\n    elif isinstance(module, nn.LSTM) or isinstance(module, nn.GRU):\n        if isinstance(module, nn.LSTM):\n            hidden_size = module.bias_hh_l0.data.shape[0] // 4\n        else:\n            hidden_size = module.bias_hh_l0.data.shape[0] // 3\n        for name, param in module.named_parameters():\n            if 'bias' in name:\n                nn.init.constant_(param, 0.0)\n            elif '_ih_' in name:\n                nn.init.xavier_normal_(param)\n            elif '_hh_' in name:\n                nn.init.orthogonal_(param)\n                param.data[hidden_size:(2 * hidden_size)] = 1.0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### K cross validation"},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"# K交叉验证算法需要的函数\n\ndef stratified_group_k_fold(X, y, groups, k, seed=None):\n    labels_num = np.max(y) + 1\n    y_counts_per_group = defaultdict(lambda: np.zeros(labels_num))\n    y_distr = Counter()\n    for label, g in zip(y, groups):\n        y_counts_per_group[g][label] += 1\n        y_distr[label] += 1\n\n    y_counts_per_fold = defaultdict(lambda: np.zeros(labels_num))\n    groups_per_fold = defaultdict(set)\n\n    def eval_y_counts_per_fold(y_counts, fold):\n        y_counts_per_fold[fold] += y_counts\n        std_per_label = []\n        for label in range(labels_num):\n            label_std = np.std([y_counts_per_fold[i][label] / y_distr[label] for i in range(k)])\n            std_per_label.append(label_std)\n        y_counts_per_fold[fold] -= y_counts\n        return np.mean(std_per_label)\n    \n    groups_and_y_counts = list(y_counts_per_group.items())\n    random.Random(seed).shuffle(groups_and_y_counts)\n\n    for g, y_counts in sorted(groups_and_y_counts, key=lambda x: -np.std(x[1])):\n        best_fold = None\n        min_eval = None\n        for i in range(k):\n            fold_eval = eval_y_counts_per_fold(y_counts, i)\n            if min_eval is None or fold_eval < min_eval:\n                min_eval = fold_eval\n                best_fold = i\n        y_counts_per_fold[best_fold] += y_counts\n        groups_per_fold[best_fold].add(g)\n\n    all_groups = set(groups)\n    for i in range(k):\n        train_groups = all_groups - groups_per_fold[i]\n        test_groups = groups_per_fold[i]\n\n        train_indices = [i for i, g in enumerate(groups) if g in train_groups]\n        test_indices = [i for i, g in enumerate(groups) if g in test_groups]\n\n        yield train_indices, test_indices","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"def get_oof(clf, X, y, X_test, groups):   \n    # 将训练的预测结果转换成可以stacking的函数\n    oof_train = np.zeros((ntrain,))\n    oof_test = np.zeros((ntest,))\n    oof_test_skf = np.empty((NFOLDS, ntest))\n\n    for i, (train_index, test_index) in enumerate(stratified_group_k_fold(X, y, rescuer_ids, NFOLDS, 1337)):\n        print('Training for fold: ', i + 1)\n        \n        x_tr = X.iloc[train_index, :]\n        y_tr = y[train_index]\n        x_te = X.iloc[test_index, :]\n        y_te = y[test_index]\n\n        clf.train(x_tr, y_tr, x_val=x_te, y_val=y_te)\n\n        oof_train[test_index] = clf.predict(x_te)\n        oof_test_skf[i, :] = clf.predict(X_test)\n\n    oof_test[:] = oof_test_skf.mean(axis=0)\n    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"# 这一块是几个模型训练定义的类。当然，最后我们并没有用到全部的模型，例如sklearn和xgb我们就没有用。\n\nclass SklearnWrapper(object):\n    # Sklearn类，包含constructor,两个函数：训练和预测函数\n    def __init__(self, clf, seed=0, params=None):\n        params['random_state'] = seed\n        self.clf = clf(**params)\n\n    def train(self, x_train, y_train, **kwargs):\n        self.clf.fit(x_train, y_train)\n\n    def predict(self, x):\n        return self.clf.predict(x)\n    \n    \nclass XgbWrapper(object):\n    # XGB类，同样包含constructor,两个函数：训练和预测函数\n    def __init__(self, params=None):\n        self.param = params\n        self.nrounds = params.pop('nrounds', 60000)\n        self.early_stop_rounds = params.pop('early_stop_rounds', 2000)\n\n    def train(self, x_train, y_train, **kwargs):\n        dtrain = xgb.DMatrix(x_train, label=y_train)\n        dvalid = xgb.DMatrix(data=kwargs['x_val'], label=kwargs['y_val'])\n        \n        watchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n        \n        self.model = xgb.train(dtrain=dtrain, num_boost_round=self.nrounds, evals=watchlist, early_stopping_rounds=self.early_stop_rounds, \n                               verbose_eval=1000, params=self.param)\n\n    def predict(self, x):\n        return self.model.predict(xgb.DMatrix(x), ntree_limit=self.model.best_ntree_limit)\n\n    \nclass LGBWrapper(object):\n    # LGB类，同样包含constructor,两个函数：训练和预测函数，但不同的是多了一个importance函数，用于输出特征的重要性\n    def __init__(self, params=None):\n        self.param = params\n        self.num_rounds = params.pop('nrounds', 60000)\n        self.early_stop_rounds = params.pop('early_stop_rounds', 2000)\n\n    def train(self, x_train, y_train, **kwargs):\n        dtrain = lgb.Dataset(x_train, label=y_train)\n        dvalid = lgb.Dataset(kwargs['x_val'], label=kwargs['y_val'])\n\n        watchlist = [dtrain, dvalid]\n        \n        print('training LightGBM with params: ', self.param)\n        self.model = lgb.train(\n                  self.param,\n                  train_set=dtrain,\n                  num_boost_round=self.num_rounds,\n                  valid_sets=watchlist,\n                  verbose_eval=1000,\n                  early_stopping_rounds=self.early_stop_rounds\n        )\n\n    def predict(self, x):\n        return self.model.predict(x, num_iteration=self.model.best_iteration)\n    \n    def importance(self):\n        lgb.plot_importance(self.model, max_num_features=300)\n        return 0\n    ","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"from contextlib import contextmanager\nimport time\n@contextmanager\n# 定义一个计时器，用于计算我们训练的时间\ndef timer(task_name=\"timer\"):\n    # a timer cm from https://www.kaggle.com/lopuhin/mercari-golf-0-3875-cv-in-75-loc-1900-s\n    print(\"----{} started\".format(task_name))\n    t0 = time.time()\n    yield\n    print(\"----{} done in {:.0f} seconds\".format(task_name, time.time() - t0))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Importance"},{"metadata":{"_uuid":"af567e8fd28c1ce85578d21427e9c5936ad8e2e0","collapsed":true,"trusted":false},"cell_type":"code","source":"# 这一段预先训练一次lightGBM，是为了使用LGB中的feature importance函数来确定哪些特征可以被剔除\n# 当然在结束之后为了节省资源，我们就不需要再用这个了，注释掉即可\nfrom collections import defaultdict\nimport random\n# LightGBM\n\"\"\"\n\nlgbm_params = {\n    'task': 'train',\n    'boosting_type': 'gbdt',\n    'objective': 'regression',\n    'metric': 'rmse',\n    'nrounds': 50000,\n    'early_stop_rounds': 2000,\n    # trainable params\n    'max_depth': 9,\n    'num_leaves': 70,\n    'feature_fraction': 0.6,\n    'bagging_fraction': 0.9,\n    'bagging_freq': 8,\n    'learning_rate': 0.019,\n    'verbose': 0\n}\nlgb_wrapper = LGBWrapper(lgbm_params)\n\nwith timer('Training LightGBM'):\n    lgb_oof_train, lgb_oof_test = get_oof(lgb_wrapper, X_train_non_null.drop(['AdoptionSpeed'], axis=1), X_train_non_null['AdoptionSpeed'].values.astype(int), X_test_non_null, groups=rescuer_ids)\n\nlgb_wrapper.importance()\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"\"\"\"\nimportance = lgb_wrapper.model.feature_importance()\nnames = lgb_wrapper.model.feature_name()\nfeature_importance = pd.DataFrame({'feature_name':names,'importance':importance} )\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"# feature_importance","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"# feature_no_result = feature_importance[feature_importance['importance']<130]","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"# list_column=feature_no_result['feature_name'].tolist()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"X_train_drop = X_train.fillna(-1)\nX_test_drop = X_test.fillna(-1)\n# for index in list_column:\n#     X_train_drop = X_train_drop.drop(columns=[index])","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"X_train_drop.to_csv('X_train_drop.csv', index = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 正式训练"},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold, GroupKFold\n\n# 通过KCV，将训练集分为5份，分别进行训练。\nn_splits = 5\n# kfold = GroupKFold(n_splits=n_splits)\nsplit_index = []\n# for train_idx, valid_idx in kfold.split(train, train['AdoptionSpeed'], train['RescuerID']):\n#     split_index.append((train_idx, valid_idx))\n\nkfold = StratifiedKFold(n_splits=n_splits, random_state=1991)\nfor train_idx, valid_idx in kfold.split(X_train_drop, X_train_drop['AdoptionSpeed']):\n    split_index.append((train_idx, valid_idx))","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"# 定义一部分几个模型通用的超参数\nearly_stop = 300\nverbose_eval = 100\nnum_rounds = 10000\n\n# 再添加一部分不需要被训练的列，\ndrop_columns = ['PetID', 'Name', 'RescuerID', 'AdoptionSpeed',  \n                   'main_breed_Type', 'main_breed_BreedName', 'second_breed_Type', 'second_breed_BreedName',\n                   'Description', 'sentiment_entities', 'meta_annots_top_desc','meta_desc',\n                   'Chinese_desc', 'English_desc']\n\nto_drop_columns.extend(drop_columns)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"# 去除重复的列\nto_drop_columns = list(set(to_drop_columns))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# RNN model\n\nRNN模型是一种对GloVe和TfIdf提取出来特征比较友好的模型。训练代码如下\n\n参考 https://www.kaggle.com/wuyhbb/final-small"},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"# torch imports\nfrom torch import nn\nimport torch\nfrom torch.nn import functional as F\nfrom torchvision.models import resnet50, resnet34, densenet201, densenet121\nfrom torch.optim.lr_scheduler import StepLR\nfrom torch.autograd import Variable\nfrom torch.utils.data import TensorDataset\n\ntorch.manual_seed(1991)\ntorch.cuda.manual_seed(1991)\ntorch.backends.cudnn.deterministic = True\n","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"# 在NN模型中需要预处理的列\nfm_cols = ['Type','Age','Breed1','Breed2','Gender','Color1','Color2','Color3','MaturitySize',\n           'FurLength','Vaccinated','Dewormed','Sterilized','Health','State','Breed_full',\n           'Color_full', 'hard_interaction']\nfm_data = X_temp[fm_cols]\nfm_values = []\nfor c in fm_cols:\n    fm_data.loc[:,c] = fm_data[c].fillna(0)\n    fm_data.loc[:,c] = c+'_'+fm_data[c].astype(str)\n    fm_values+=fm_data[c].unique().tolist()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nlbe = LabelEncoder()\nlbe.fit(fm_values)\nfor c in fm_cols:\n    fm_data.loc[:,c] = lbe.transform(fm_data[c])","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"numerical_cols = [x for x in X_train_drop.columns if x not in to_drop_columns+fm_cols]","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"# 将numerical feature单独拿出\nnumerical_feats = []\nfor c in numerical_cols:\n    numerical_feats.append(X_temp[c].fillna(0))\n    \n# for c in range(1920):\n#     numerical_feats.append(raw_img_features['resnet50_%d'%c].fillna(0))\n\nnumerical_feats = np.vstack(numerical_feats).T\n# numerical_feats = stdscaler.fit_transform(numerical_feats)\n","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"numerical_feats.shape","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\n\nMAX_LEN = 400\nclass PetDesDataset(Dataset):\n    def __init__(self, sentences, pos, fm_data, numerical_feat,\n                 mode='train', target=None):\n        super(PetDesDataset, self).__init__()\n        self.data = sentences\n        self.pos = pos\n        self.target = target\n        self.mode = mode\n        self.fm_data = fm_data\n        self.fm_dim = fm_data.shape[1]\n        self.numerical_feat = numerical_feat\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, index):\n        if index not in range(0, self.__len__()):\n            return self.__getitem__(np.random.randint(0, self.__len__()))\n        sentence_len = min(MAX_LEN, len(self.data[index]))\n        sentence = torch.tensor(self.data[index][:sentence_len])\n        fm_data = self.fm_data[index,:]\n        pos = torch.tensor(self.pos[index][:sentence_len])\n\n        if self.mode != 'test':  # , pos, tag\n            return sentence, pos, sentence_len, fm_data, self.numerical_feat[index], self.target[index]  # , clf_label\n        else:\n            return sentence, pos, sentence_len, fm_data, self.numerical_feat[index]\n        \ndef nn_collate(batch):\n    has_label = len(batch[0]) == 6\n    if has_label:\n        sentences, poses, lengths, fm_data, numerical_feats, label = zip(*batch)\n        sentences = nn.utils.rnn.pad_sequence(sentences, batch_first=True).type(torch.LongTensor)\n        poses = nn.utils.rnn.pad_sequence(poses, batch_first=True).type(torch.LongTensor)\n        lengths = torch.LongTensor(lengths)\n        fm_data = torch.LongTensor(fm_data)\n        numerical_feats = torch.FloatTensor(numerical_feats)\n        label = torch.FloatTensor(label)\n        return sentences, poses, lengths, fm_data, numerical_feats, label\n    else:\n        sentences, poses, lengths, fm_data, numerical_feats = zip(*batch)\n        sentences = nn.utils.rnn.pad_sequence(sentences, batch_first=True).type(torch.LongTensor)\n        poses = nn.utils.rnn.pad_sequence(poses, batch_first=True).type(torch.LongTensor)\n        lengths = torch.LongTensor(lengths)\n        fm_data = torch.LongTensor(fm_data)\n        numerical_feats = torch.FloatTensor(numerical_feats)\n        return sentences, poses, lengths, fm_data, numerical_feats\n","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"\ndef get_mask(sequences_batch, sequences_lengths, cpu=False):\n    batch_size = sequences_batch.size()[0]\n    max_length = torch.max(sequences_lengths)\n    mask = torch.ones(batch_size, max_length, dtype=torch.float)\n    mask[sequences_batch[:, :max_length] == 0] = 0.0\n    if cpu:\n        return mask\n    else:\n        return mask.cuda()\nclass Attention(nn.Module):\n    def __init__(self, feature_dim, bias=True, head_num=1, **kwargs):\n        super(Attention, self).__init__(**kwargs)\n        self.supports_masking = True\n        self.bias = bias\n        self.feature_dim = feature_dim\n        self.head_num = head_num\n        weight = torch.zeros(feature_dim, self.head_num)\n        bias = torch.zeros((1, 1, self.head_num))\n        nn.init.xavier_uniform_(weight)\n        self.weight = nn.Parameter(weight)\n        self.b = nn.Parameter(bias)\n\n    def forward(self, x, mask=None):\n        batch_size, step_dim, feature_dim = x.size()\n        eij = torch.mm(\n            x.contiguous().view(-1, feature_dim),  # B*L*H\n            self.weight  # B*H*1\n        ).view(-1, step_dim, self.head_num)  # B*L*head\n        if self.bias:\n            eij = eij + self.b\n        eij = torch.tanh(eij)\n        if mask is not None:\n            eij = eij * mask - 99999.9 * (1 - mask)\n        a = torch.softmax(eij, dim=1)\n\n        weighted_input = torch.bmm(x.permute((0,2,1)),\n                                   a).view(batch_size, -1)\n        return weighted_input\n","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"\nembed_size = 300\nclass FM(nn.Module):\n\n    def __init__(self, max_features, feat_len, embed_size):\n        super(FM, self).__init__()\n        self.bias_emb = nn.Embedding(max_features, 1)\n        self.fm_emb = nn.Embedding(max_features, embed_size)\n        self.feat_len = feat_len\n        self.embed_size = embed_size\n\n    def forward(self, x):\n        bias = self.bias_emb(x)\n        bias = torch.sum(bias,1) # N * 1\n\n        # second order term\n        # square of sum\n        emb = self.fm_emb(x)\n        sum_feature_emb = torch.sum(emb, 1) # N * k\n        square_sum_feature_emb = sum_feature_emb*sum_feature_emb\n\n        # sum of square\n        square_feature_emb = emb * emb\n        sum_square_feature_emb = torch.sum(square_feature_emb, 1) # N * k\n\n        second_order = 0.5*(square_sum_feature_emb-sum_square_feature_emb) # N *k\n        return bias+second_order, emb.view(-1, self.feat_len*self.embed_size)\n\n","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"\nclass FmNlpModel(nn.Module):\n    def turn_on_embedding(self):\n        self.embedding.weight.requires_grad = True\n\n    def __init__(self, hidden_size=64, init_embedding=None, head_num=3,\n                 fm_embed_size=8, fm_feat_len=10, fm_max_feature = 300, numerical_dim = 300,\n                 nb_word = 40000, nb_pos = 200, pos_emb_size = 10):\n        super(FmNlpModel, self).__init__()\n        self.hidden_size = hidden_size\n        self.embedding = nn.Embedding(nb_word, 300, padding_idx=0)\n        self.pos_embedding = nn.Embedding(nb_pos+100, pos_emb_size, padding_idx=0)\n        \n        if init_embedding is not None:\n            self.embedding.weight.data.copy_(torch.from_numpy(init_embedding))\n        self.embedding.weight.requires_grad = False\n        \n        self.fm = FM(fm_max_feature, fm_feat_len, fm_embed_size)\n\n        self.dropout = nn.Dropout(0.1)\n        self.attention_gru = Attention(feature_dim=self.hidden_size * 2, head_num=head_num)\n        self.gru = nn.GRU(embed_size+pos_emb_size, hidden_size, bidirectional=True, batch_first=True) #\n        self.gru2 = nn.GRU(hidden_size * 2, hidden_size, bidirectional=True, batch_first=True)\n        \n        self.dnn = nn.Sequential(\n            nn.BatchNorm1d(numerical_dim),\n            nn.Dropout(0.1),\n            nn.Linear(numerical_dim, 256),\n            nn.ELU(inplace=True),\n            nn.Dropout(0.1),\n            nn.Linear(256, 128),\n            nn.ELU(inplace=True),\n        )\n        \n        self.rnn_dnn = nn.Sequential(\n            nn.BatchNorm1d(fm_embed_size+fm_feat_len*fm_embed_size +2*head_num * hidden_size+128), #\n            nn.Dropout(0.3),\n            nn.Linear(fm_embed_size+fm_feat_len*fm_embed_size+2*head_num * hidden_size+128, 32),\n            nn.ELU(inplace=True),\n        )\n        self.logit = nn.Sequential(\n            nn.Linear(32,1)\n        )\n#         self.apply(_init_esim_weights)\n\n    def forward(self, x, pos_x, len_x, fm_x, numerical_x):\n        \n        fm_result, fm_embed = self.fm(fm_x)\n        \n        sentence_mask = get_mask(x, len_x)\n        x = x * sentence_mask.long()\n        sentence_mask = torch.unsqueeze(sentence_mask, -1)\n\n        h_embedding = self.embedding(x)\n        h_pos_embedding = self.pos_embedding(pos_x)\n        h_embedding = torch.cat([h_embedding, h_pos_embedding],2)\n        \n        h_embedding = self.dropout(h_embedding)\n        \n        sorted_seq_lengths, indices = torch.sort(len_x, descending=True)\n        # 排序前的顺序是对下标再排一次序\n        _, desorted_indices = torch.sort(indices, descending=False)\n        h_embedding = h_embedding[indices]\n        packed_inputs = nn.utils.rnn.pack_padded_sequence(h_embedding, sorted_seq_lengths, batch_first=True)\n        \n        h_gru, _ = self.gru(packed_inputs)\n        h_gru2, _ = self.gru2(h_gru)  # sentence_mask.expand_as(h_lstm)\n        \n        h_gru2, _ = nn.utils.rnn.pad_packed_sequence(h_gru2, batch_first=True)\n        h_gru2 = h_gru2[desorted_indices]\n        att_pool_gru = self.attention_gru(h_gru2, sentence_mask)\n        \n        numerical_x = self.dnn(numerical_x)\n\n        x = torch.cat([att_pool_gru,fm_result,fm_embed,numerical_x],1) # \n        feat = self.rnn_dnn(x)\n        out = self.logit(feat)\n\n        return out, feat\n","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"\nX_train_numerical = numerical_feats[0:len(train)]\nX_test_numerical = numerical_feats[len(train):]\n\nX_train_seq = pd.Series(eng_sequences[0:len(train)])\nX_test_seq = pd.Series(eng_sequences[len(train):])\n\nX_train_pos_seq = pd.Series(pos_sequences[0:len(train)])\nX_test_pos_seq = pd.Series(pos_sequences[len(train):])\n\nX_train_fm = fm_data.iloc[0:len(train)].values\nX_test_fm = fm_data.iloc[len(train):].values\n\nY_train = X_temp.iloc[0:len(train)]['AdoptionSpeed'].values\n","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"# 通过Glove建立文本模型\ndef load_glove():\n    EMBEDDING_FILE = '../input/glove840b300dtxt/glove.840B.300d.txt'\n\n    def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n\n    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in (open(EMBEDDING_FILE)))\n    return embeddings_index\n\nglove_emb = load_glove()\n\nembedding_matrix, nb_words, nb_oov = build_emb_matrix(word_dict, glove_emb)\nprint(nb_words, nb_oov)\ndel glove_emb\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"nb_pos = len(pos_dict)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"# 训练\ntrain_epochs = 6\nloss_fn = torch.nn.MSELoss().cuda()\noof_train_nlp = np.zeros((X_train_drop.shape[0], 32+1))\noof_test_nlp = []\n\ntest_set = PetDesDataset(X_test_seq.tolist(), X_test_pos_seq.tolist(), X_test_fm, X_test_numerical, mode='test')\ntest_loader = DataLoader(test_set, batch_size=512, shuffle=False, num_workers=1, pin_memory=True,\n                                collate_fn=nn_collate)\nqwks = []\nrmses = []\n\nfor n_fold, (train_idx, valid_idx) in enumerate(split_index): \n        \n    print('fold:', n_fold)\n    hist = histogram(Y_train[train_idx].astype(int), \n                     int(np.min(X_train_drop['AdoptionSpeed'])), \n                     int(np.max(X_train_drop['AdoptionSpeed'])))\n    tr_cdf = get_cdf(hist)\n    \n    training_set = PetDesDataset(X_train_seq[train_idx].tolist(), \n                                 X_train_pos_seq[train_idx].tolist(),\n                                 X_train_fm[train_idx], \n                                 X_train_numerical[train_idx], target = Y_train[train_idx])\n    \n    validation_set = PetDesDataset(X_train_seq[valid_idx].tolist(), \n                                   X_train_pos_seq[valid_idx].tolist(),\n                                   X_train_fm[valid_idx], \n                                  X_train_numerical[valid_idx],target = Y_train[valid_idx])\n    \n    training_loader = DataLoader(training_set, batch_size=512, shuffle=True, num_workers=1,\n                                collate_fn=nn_collate)\n    validation_loader = DataLoader(validation_set, batch_size=512, shuffle=False, num_workers=1,\n                                collate_fn=nn_collate)\n    \n    model = FmNlpModel(hidden_size=48, init_embedding=embedding_matrix, head_num=10, \n                      fm_embed_size=10, fm_feat_len=X_train_fm.shape[1], fm_max_feature=len(fm_values),\n                      numerical_dim=X_train_numerical.shape[1],\n                      nb_word=nb_words, nb_pos=nb_pos, pos_emb_size=10)\n    model.cuda()\n    \n    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.01)\n#     scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=4, gamma=0.5)\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=4, eta_min=0.0001)\n    \n    iteration = 0\n    min_val_loss = 100\n    since = time.time()\n    \n    for epoch in range(train_epochs):       \n        scheduler.step()\n        model.train()\n        for sentences, poses, lengths, x_fm, x_numerical, labels in training_loader:\n            iteration += 1\n            sentences = sentences.cuda()\n            poses = poses.cuda()\n            lengths = lengths.cuda()\n            x_fm = x_fm.cuda()\n            x_numerical = x_numerical.cuda()\n            labels = labels.type(torch.FloatTensor).cuda().view(-1, 1)\n\n            pred,_ = model(sentences, poses, lengths, x_fm, x_numerical)\n            loss = loss_fn(pred, labels)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n        model.eval()\n        val_predicts = []\n        val_feats = []\n        with torch.no_grad():\n            for sentences, poses, lengths, x_fm, x_numerical, labels in validation_loader:\n                sentences = sentences.cuda()\n                poses = poses.cuda()\n                lengths = lengths.cuda()\n                x_fm = x_fm.cuda()\n                x_numerical = x_numerical.cuda()\n                labels = labels.type(torch.FloatTensor).cuda()#.view(-1, 1)\n                v_pred, v_feat = model(sentences, poses, lengths, x_fm, x_numerical)\n                val_predicts.append(v_pred.cpu().numpy())\n                val_feats.append(v_feat.cpu().numpy())\n\n        val_predicts = np.concatenate(val_predicts)\n        val_feats = np.vstack(val_feats)\n        val_loss = rmse(Y_train[valid_idx], val_predicts)\n        if val_loss<min_val_loss:\n            min_val_loss = val_loss\n            oof_train_nlp[valid_idx,:] = np.hstack([val_feats, val_predicts])\n            test_feats = []\n            test_preds = []\n            with torch.no_grad():\n                for sentences, poses, lengths, x_fm, x_numerical in test_loader:\n                    sentences = sentences.cuda()\n                    poses = poses.cuda()\n                    lengths = lengths.cuda()\n                    x_fm = x_fm.cuda()\n                    x_numerical = x_numerical.cuda()\n                    v_pred, feat = model(sentences, poses, lengths, x_fm, x_numerical)\n                    test_preds.append(v_pred.cpu().numpy())\n                    test_feats.append(feat.cpu().numpy())\n            test_feats = np.hstack([np.vstack(test_feats), np.concatenate(test_preds)])\n#             pred_test_y_k = getTestScore2(val_predicts.flatten(), tr_cdf)\n#             qwk = quadratic_weighted_kappa(Y_train[valid_idx], pred_test_y_k)\n#             print(epoch, \"val loss:\", val_loss, \"val QWK_2 = \", qwk, \"elapsed time:\", time.time()-since)\n    oof_test_nlp.append(test_feats)\n    del model\n    del training_set\n    del validation_set \n    del sentences\n    del lengths\n    del x_fm\n    del x_numerical\n    del poses\n    gc.collect()\n    torch.cuda.empty_cache()\n    \n#     qwks.append(qwk)\n#     rmses.append(min_val_loss)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"pynvml.nvmlInit()\n# 这里的1是GPU id\nhandle = pynvml.nvmlDeviceGetHandleByIndex(0)\nmeminfo = pynvml.nvmlDeviceGetMemoryInfo(handle)\nprint(meminfo.total) #第二块显卡总的显存大小\nprint(meminfo.used)#这里是字节bytes，所以要想得到以兆M为单位就需要除以1024**2\nprint(meminfo.free) #第二块显卡剩余显存大小\nprint(pynvml.nvmlDeviceGetCount())#显示有几块GPU","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"oof_test_nlp = np.mean(oof_test_nlp, axis=0)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"print(oof_train_nlp)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"oof_test_nlp[:,-1]","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"#for item in to_drop_columns:\n#    X_temp = X_temp.drop([item], axis=1)\n\n# Check final df shape:\nprint('X shape: {}'.format(X_temp.shape))","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"# 再一次将训练集和测试集分开\nX_train = X_temp.loc[np.isfinite(X_temp.AdoptionSpeed), :]\nX_test = X_temp.loc[~np.isfinite(X_temp.AdoptionSpeed), :]\n\n# 剔除AdoptionSpeed列\nX_test = X_test.drop(['AdoptionSpeed'], axis=1)\n\n\nprint('X_train shape: {}'.format(X_train.shape))\nprint('X_test shape: {}'.format(X_test.shape))\n\nassert X_train.shape[0] == train.shape[0]\nassert X_test.shape[0] == test.shape[0]\n\n\n# 确认两者长度是否一致\ntrain_cols = X_train.columns.tolist()\ntrain_cols.remove('AdoptionSpeed')\n\ntest_cols = X_test.columns.tolist()\n\nassert np.all(train_cols == test_cols)\nrescuer_ids = rescuer_ids[:len(X_train)]\nassert len(rescuer_ids) == len(X_train)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"X_train_drop = X_train.fillna(-1)\nX_test_drop = X_test.fillna(-1)\nntrain = X_train_drop.shape[0]\nntest = X_test_drop.shape[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# LGB model\n\nLGB是一种基于GBDT的集成模型。其较XGB的优势在于速度较快，精确度较高。但是调参比较繁琐"},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"import lightgbm as lgb\nimport xgboost as xgb","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"features = [x for x in X_train_drop.columns if x not in to_drop_columns]","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"# features.remove('AdoptionSpeed')","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"# Additional parameters:\nearly_stop = 300\nverbose_eval = 100\nnum_rounds = 10000","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"# LGB训练，这一段在report中有介绍，可以看那边\nparams = {'application': 'regression',\n          'boosting': 'gbdt',\n          'metric': 'rmse',\n          'num_leaves': 37,\n          'max_depth': 6,\n          'learning_rate': 0.01,\n          'subsample': 0.85,\n          'feature_fraction': 0.7,\n          'lambda_l1':0.01,\n          'verbosity': -1,\n         }\n\noof_train_lgb = np.zeros((X_train_drop.shape[0]))\noof_test_lgb = []\nqwks = []\nrmses = []\n\nfor n_fold, (train_idx, valid_idx) in enumerate(split_index):\n    since = time.time()\n    X_tr = X_train_drop.iloc[train_idx]\n    X_val = X_train_drop.iloc[valid_idx]\n\n    y_tr = X_tr['AdoptionSpeed'].values    \n    y_val = X_val['AdoptionSpeed'].values\n        \n    d_train = lgb.Dataset(X_tr[features], label=y_tr,\n#                          categorical_feature=['Breed1','Color1','Breed2','State','Breed_full','Color_full']\n                         )\n    d_valid = lgb.Dataset(X_val[features], label=y_val, reference=d_train)\n    watchlist = [d_valid]\n    \n    print('training LGB:')\n    lgb_model = lgb.train(params,\n                      train_set=d_train,\n                      num_boost_round=num_rounds,\n                      valid_sets=watchlist,\n                      verbose_eval=500,\n                      early_stopping_rounds=100,\n                     )\n    \n    val_pred = lgb_model.predict(X_val[features])\n    test_pred = lgb_model.predict(X_test_drop[features])\n    train_pred = lgb_model.predict(X_tr[features])\n    \n    oof_train_lgb[valid_idx] = val_pred\n    oof_test_lgb.append(test_pred)\n               \n    hist = histogram(X_tr['AdoptionSpeed'].astype(int), \n                    int(np.min(X_train['AdoptionSpeed'])), \n                    int(np.max(X_train['AdoptionSpeed'])))\n    tr_cdf = get_cdf(hist)\n    _, cutoff = getScore(train_pred, tr_cdf, True)\n    pred_test_y_k = getTestScore(val_pred, cutoff)\n    qwk = quadratic_weighted_kappa(X_val['AdoptionSpeed'].values, pred_test_y_k)\n    print(\"QWK_1 = \", qwk)\n    qwks.append(qwk)\n    rmses.append(rmse(X_val['AdoptionSpeed'].values, val_pred))\n    \n#     pred_test_y_k = getTestScore2(val_pred, tr_cdf)\n#     qwk = quadratic_weighted_kappa(X_val['AdoptionSpeed'].values, pred_test_y_k)\n#     qwks.append(qwk)\n#     rmses.append(rmse(X_val['AdoptionSpeed'].values, val_pred))\n#     print(\"QWK_2 = \", qwk,'elapsed time:',time.time()-since)\n\nprint('overall rmse: %.5f'%rmse(oof_train_lgb, X_train_drop['AdoptionSpeed']))\nprint('mean rmse =', np.mean(rmses), 'rmse std =', np.std(rmses))\nprint('mean QWK =', np.mean(qwks), 'std QWK =', np.std(qwks))","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true},"cell_type":"markdown","source":"### 调参\n\n下面这一段是利用gridsearchCV调参。调整之后就可以注释掉，减少训练时间。"},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"\"\"\"\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\n\nfeatures.extend(['AdoptionSpeed'])\ntrain_data = X_train_drop[features]  # 读取数据\ny = train_data.pop('AdoptionSpeed').values   # 用pop方式将训练数据中的标签值y取出来，作为训练目标，这里的‘30’是标签的列名\n\ncol = train_data.columns\nx = train_data[col].values  # 剩下的列作为训练数据\ntrain_x, valid_x, train_y, valid_y = train_test_split(x, y, test_size=0.2, random_state=0)   # 分训练集和验证集\ntrain = lgb.Dataset(train_x, train_y)\nvalid = lgb.Dataset(valid_x, valid_y, reference=train)\n\n# since = time.time()\n# X_tr = train_data.iloc[train_idx]\n# X_val = train_data.iloc[valid_idx]\n\n# y_tr = X_tr['AdoptionSpeed'].values    \n# y_val = X_val['AdoptionSpeed'].values\n\nwatchlist = [valid]\n\noof_train_lgb = np.zeros((X_train_drop.shape[0]))\noof_test_lgb = []\nqwks = []\nrmses = []\n\nparams = {'application': 'regression',\n          'boosting': 'gbdt',\n          'metric': 'rmse',\n          'num_leaves': 70,\n          'max_depth': 9,\n          'learning_rate': 0.01,\n          'subsample': 0.85,\n          'feature_fraction': 0.7,\n          'lambda_l1':0.01,\n          'verbosity': -1,\n         }\n\n\n\nparameters = {\n              'max_depth': [15, 20, 25, 30, 35],\n              'num_leaves':[20, 30, 40, 50, 60],\n              'learning_rate': [0.01, 0.02, 0.05, 0.1, 0.15],\n              'feature_fraction': [0.6, 0.7, 0.8, 0.9, 0.95],\n              'bagging_fraction': [0.6, 0.7, 0.8, 0.9, 0.95],\n              'bagging_freq': [2, 4, 5, 6, 8],\n              'lambda_l1': [0, 0.1, 0.4, 0.5, 0.6],\n              'lambda_l2': [0, 10, 15, 35, 40],\n              'cat_smooth': [1, 10, 15, 20, 35]\n}\n\ngbm = lgb.train(params,\n                      train_set=train,\n                      num_boost_round=num_rounds,\n                      valid_sets=watchlist,\n                      verbose_eval=500,\n                      early_stopping_rounds=100,\n                     )\n\n# 有了gridsearch我们便不需要fit函数\n\ngsearch = GridSearchCV(gbm, param_grid=parameters, scoring='neg_mean_squared_error', cv=5, verbose=1, n_jobs=4)\ngsearch.fit(train_x, train_y)\n\nprint(\"Best score: %0.3f\" % gsearch.best_score_)\nprint(\"Best parameters set:\")\nbest_parameters = gsearch.best_estimator_.get_params()\nfor param_name in sorted(parameters.keys()):\n    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 贝叶斯调参\n\n进一步通过贝叶斯调餐搜索最优参数"},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"\"\"\"\nfrom hyperopt import fmin, tpe, hp, partial\nfrom sklearn.model_selection import train_test_split\n\nfeatures.extend(['AdoptionSpeed'])\ntrain_data = X_train_drop[features]  # 读取数据\ny = train_data.pop('AdoptionSpeed').values   # 用pop方式将训练数据中的标签值y取出来，作为训练目标，这里的‘30’是标签的列名\n\ncol = train_data.columns\nx = train_data[col].values  # 剩下的列作为训练数据\ntrain_x, valid_x, train_y, valid_y = train_test_split(x, y, test_size=0.2, random_state=0)   # 分训练集和验证集\ntrain = lgb.Dataset(train_x, train_y)\nvalid = lgb.Dataset(valid_x, valid_y, reference=train)\nwatchlist = [valid]\n\noof_train_lgb = np.zeros((X_train_drop.shape[0]))\noof_test_lgb = []\nqwks = []\nrmses = []\n# 自定义hyperopt的参数空间\nspace = {\"max_depth\": hp.randint(\"max_depth\", 15),\n         'learning_rate': hp.uniform('learning_rate', 1e-3, 5e-1),\n         \"bagging_fraction\": hp.randint(\"bagging_fraction\", 5),\n         \"num_leaves\": hp.randint(\"num_leaves\", 6),\n         }\n\ndef argsDict_tranform(argsDict, isPrint=False):\n    argsDict[\"max_depth\"] = argsDict[\"max_depth\"] + 5\n    argsDict[\"learning_rate\"] = argsDict[\"learning_rate\"] * 0.02 + 0.05\n    argsDict[\"bagging_fraction\"] = argsDict[\"bagging_fraction\"] * 0.1 + 0.5\n    argsDict[\"num_leaves\"] = argsDict[\"num_leaves\"] * 3 + 10\n    if isPrint:\n        print(argsDict)\n    else:\n        pass\n\n    return argsDict\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"\"\"\"\nfrom sklearn.metrics import mean_squared_error\n\ndef lightgbm_factory(argsDict):\n    argsDict = argsDict_tranform(argsDict)\n\n    params = {'max_depth': argsDict['max_depth'],  # 最大深度\n              'learning_rate': argsDict['learning_rate'],  # 学习率\n              'bagging_fraction': argsDict['bagging_fraction'],  # bagging采样数\n              'num_leaves': argsDict['num_leaves'],  # 终点节点最小样本占比的和\n              'application': 'regression',\n              'subsample': 0.85,\n              'feature_fraction': 0.7,  # 样本列采样\n              'lambda_l1': 0.01,  # L1 正则化\n              'bagging_seed': 100,  # 随机种子,light中默认为100\n              }\n    #rmse\n    params['metric'] = ['rmse']\n\n    model_lgb = lgb.train(params,\n                      train_set=train,\n                      num_boost_round=num_rounds,\n                      valid_sets=watchlist,\n                      verbose_eval=500,\n                      early_stopping_rounds=100,\n                     )\n\n    return get_tranformer_score(model_lgb)\n\ndef get_tranformer_score(tranformer):\n\n    model = tranformer\n    prediction = model.predict(valid_x, num_iteration=model.best_iteration)\n\n    return mean_squared_error(valid_y, prediction)\n    \n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"# 开始使用hyperopt进行自动调参\n# algo = partial(tpe.suggest, n_startup_jobs=1)\n# best = fmin(lightgbm_factory, space, algo=algo, max_evals=20, pass_expr_memo_ctrl=None)\n\n# print('best :', best)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"# oof_train_lgb","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"\"\"\"\nhist = histogram(X_tr['AdoptionSpeed'].astype(int), \n                int(np.min(X_train['AdoptionSpeed'])), \n                int(np.max(X_train['AdoptionSpeed'])))\ntr_cdf = get_cdf(hist)\n_, cutoff = getScore(train_pred, tr_cdf, True)\npred_test_y_k = getTestScore(val_pred, cutoff)\nqwk = quadratic_weighted_kappa(X_val['AdoptionSpeed'].values, pred_test_y_k)\nprint(\"QWK_1 = \", qwk)\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# catboost\n\nCatBoost并不是一种对所有数据集精确度很高的模型，但是对于特定编码的categorical模型，他的准确度特别高。而这正好可以用于我们的训练。另一点很关键的是，他真的很快！"},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"# 接下来训练catboost\nfrom catboost import CatBoostRegressor, Pool\nimport time\n\nfeatures = [x for x in X_train_drop.columns if x not in to_drop_columns]\n\ncat_index = []\nfor idx, c in enumerate(features):\n    if c in ['Type','Breed1','Breed2','Gender','Color1','Color2','Color3','State','Breed_full',\n           'Color_full', 'hard_interaction','img_CLUSTER_0']:\n        cat_index.append(idx)\n        \noof_train_cat = np.zeros((X_train_drop.shape[0]))\noof_test_cat = []\nqwks = []\nrmses = []\n\nfor n_fold, (train_idx, valid_idx) in enumerate(split_index):\n    since = time.time()\n    X_tr = X_train_drop.iloc[train_idx]\n    X_val = X_train_drop.iloc[valid_idx]\n\n    y_tr = X_tr['AdoptionSpeed'].values \n    y_val = X_val['AdoptionSpeed'].values \n        \n    \n    eval_dataset = Pool(X_val[features].values,\n                    y_val,\n                   cat_index)\n    print('training Catboost:')\n    model = CatBoostRegressor(learning_rate=0.01,  depth=6, task_type = \"GPU\", l2_leaf_reg=2)\n    model.fit(X_tr[features].values,\n              y_tr,\n              eval_set=eval_dataset,\n              cat_features= cat_index,\n              verbose=False)\n    \n    val_pred = model.predict(eval_dataset)\n    test_pred = model.predict(X_test_drop[features])\n    \n    oof_train_cat[valid_idx] = val_pred\n    oof_test_cat.append(test_pred)\n               \n    hist = histogram(X_tr['AdoptionSpeed'].astype(int), \n                      int(np.min(X_train['AdoptionSpeed'])), \n                      int(np.max(X_train['AdoptionSpeed'])))\n    tr_cdf = get_cdf(hist)\n    \n    pred_test_y_k = getTestScore2(val_pred, tr_cdf)\n    qwk = quadratic_weighted_kappa(X_val['AdoptionSpeed'].values, pred_test_y_k)\n    qwks.append(qwk)\n    rmses.append(rmse(X_val['AdoptionSpeed'].values, val_pred))\n    print('rmse=',rmses[-1],\"QWK_2 = \", qwk,'elapsed time:',time.time()-since)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"print('overall rmse: %.5f'%rmse(oof_train_cat, X_train_drop['AdoptionSpeed']))\nprint('mean rmse =', np.mean(rmses), 'rmse std =', np.std(rmses))\nprint('mean QWK =', np.mean(qwks), 'std QWK =', np.std(qwks))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# XGB model"},{"metadata":{},"cell_type":"markdown","source":"XGB这一块，最后被我们删除了。因为XGB最终的训练效果并不好。"},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"features = [x for x in X_train_drop.columns if x not in to_drop_columns]\n# features.remove('AdoptionSpeed')\nxgb_features = features","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"\"\"\"\nparams = {\n        'objective': 'reg:linear', #huber\n        'eval_metric':'rmse',\n        'eta': 0.01,\n        'tree_method':'gpu_hist',\n        'max_depth': 9,  \n        'subsample': 0.85,  \n        'colsample_bytree': 0.7,     \n        'alpha': 0.01,  \n    } \n\noof_train_xgb = np.zeros((X_train_drop.shape[0]))\noof_test_xgb = []\nqwks = []\n\ni = 0\ntest_set = xgb.DMatrix(X_test_drop[xgb_features])\n\nfor n_fold, (train_idx, valid_idx) in enumerate(split_index):  \n    X_tr = X_train_drop.iloc[train_idx]\n    X_val = X_train_drop.iloc[valid_idx]\n    \n    y_tr = X_tr['AdoptionSpeed'].values    \n    y_val = X_val['AdoptionSpeed'].values\n        \n    d_train = xgb.DMatrix(X_tr[xgb_features], y_tr)\n    d_valid = xgb.DMatrix(X_val[xgb_features], y_val)\n    watchlist = [d_valid]\n    since = time.time()\n    print('training XGB:')\n    model = xgb.train(params, d_train, num_boost_round = 10000, evals=[(d_valid,'val')],\n                     early_stopping_rounds=100, #feval=xgb_eval_kappa,\n                     verbose_eval=500)\n    \n    val_pred = model.predict(d_valid)\n    test_pred = model.predict(test_set)\n    \n    oof_train_xgb[valid_idx] = val_pred\n    oof_test_xgb.append(test_pred)\n    \n#     hist = histogram(X_tr['AdoptionSpeed'].astype(int), \n#                      int(np.min(X_train['AdoptionSpeed'])), \n#                      int(np.max(X_train['AdoptionSpeed'])))\n#     tr_cdf = get_cdf(hist)\n    \n#     pred_test_y_k = getTestScore2(val_pred, tr_cdf)\n#     qwk = quadratic_weighted_kappa(X_val['AdoptionSpeed'].values, pred_test_y_k)\n#     qwks.append(qwk)\n#     print(\"QWK_2 = \", qwk,'elapsed time:',time.time()-since)\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"\"\"\"\nimport pandas as pd\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\n\ntrain_data = X_train_drop  # 读取数据\ny = train_data.pop('AdoptionSpeed').values   # 用pop方式将训练数据中的标签值y取出来，作为训练目标\n\ncol = train_data.columns   \nx = train_data[col].values  # 剩下的列作为训练数据\ntrain_x, valid_x, train_y, valid_y = train_test_split(x, y, test_size=0.333, random_state=0)   # 分训练集和验证集\n\nparameters = {\n              'max_depth': [5, 10, 15, 20, 25],\n              'learning_rate': [0.01, 0.02, 0.05, 0.1, 0.15],\n              'n_estimators': [500, 1000, 2000, 3000, 5000],\n              'min_child_weight': [0, 2, 5, 10, 20],\n              'max_delta_step': [0, 0.2, 0.6, 1, 2],\n              'subsample': [0.6, 0.7, 0.8, 0.85, 0.95],\n              'colsample_bytree': [0.5, 0.6, 0.7, 0.8, 0.9],\n              'reg_alpha': [0, 0.25, 0.5, 0.75, 1],\n              'reg_lambda': [0.2, 0.4, 0.6, 0.8, 1],\n              'scale_pos_weight': [0.2, 0.4, 0.6, 0.8, 1]\n\n}\n\nxlf = xgb.XGBRegressor(\n                    objective = 'reg:linear',\n                    eval_metric='rmse',\n                    tree_method='gpu_hist',\n                    device='gpu',\n                    silent= 1,\n                    # seed= 1337,\n                    num_boost_round= 10000,\n                    verbose_eval=100,\n                    # trainable params\n                    subsample= 0.85,\n                    colsample_bytree= 0.6,\n                    gamma= 0.65,\n                    eta = 0.01,\n                    max_depth= 4,\n                    min_child_weight= 5.0,\n                    n_estimators= 1000,)\n            \n# 有了gridsearch我们便不需要fit函数\ngsearch = GridSearchCV(xlf, param_grid=parameters, scoring='accuracy', cv=3)\ngsearch.fit(train_x, train_y)\n\nprint(\"Best score: %0.3f\" % gsearch.best_score_)\nprint(\"Best parameters set:\")\nbest_parameters = gsearch.best_estimator_.get_params()\nfor param_name in sorted(parameters.keys()):\n    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"np.corrcoef([np.mean(oof_test_lgb,axis=0), \n             #np.mean(oof_test_lgb2,axis=0),\n             np.mean(oof_test_cat,axis=0),\n             #np.mean(oof_test_xgb,axis=0),\n             oof_test_nlp[:,-1]\n            ])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Stacking"},{"metadata":{},"cell_type":"markdown","source":"通过岭回归进行模型融合。"},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"from sklearn.linear_model import Ridge\n\nY_train = X_train_drop.iloc[0:len(train)]['AdoptionSpeed'].values\n# 生成训练融合矩阵\nX_train_stacking = np.vstack([oof_train_lgb,  \n                              oof_train_cat,\n                             oof_train_nlp[:,-1]\n                             ]).T\n#  生成测试融合矩阵\nX_test_stacking = np.vstack([np.mean(oof_test_lgb, axis=0),\n\n                             np.mean(oof_test_cat,axis=0),\n                            oof_test_nlp[:,-1]\n                            ]).T\n\nstacking_train = np.zeros((X_train_drop.shape[0]))\nstacking_test = []\nrmses, qwks = [], []\n# 对每一个fold分别进行岭回归训练\nfor n_fold, (train_idx, valid_idx) in enumerate(split_index):\n    \n    X_tr = X_train_stacking[train_idx]\n    X_val = X_train_stacking[valid_idx]\n    \n    y_tr = X_train_drop.iloc[train_idx]['AdoptionSpeed'].values    \n    y_val = X_train_drop.iloc[valid_idx]['AdoptionSpeed'].values\n        \n    since = time.time()\n    \n    print('training Ridge:')\n    model = Ridge(alpha=1)\n    model.fit(X_tr, y_tr)\n    print(model.coef_)\n    \n    val_pred = model.predict(X_val)\n    test_pred = model.predict(X_test_stacking)\n    \n    stacking_train[valid_idx] = val_pred\n    stacking_test.append(test_pred)\n    loss = rmse(Y_train[valid_idx], val_pred)\n    hist = histogram(y_tr.astype(int), \n                     int(np.min(X_train['AdoptionSpeed'])), \n                     int(np.max(X_train['AdoptionSpeed'])))\n    tr_cdf = get_cdf(hist)\n    \n    pred_test_y_k = getTestScore2(val_pred, tr_cdf)\n    qwk = quadratic_weighted_kappa(y_val, pred_test_y_k)\n    qwks.append(qwk)\n    rmses.append(loss)\n    print(\"RMSE=\",loss, \"QWK_2 = \", qwk,'elapsed time:',time.time()-since)\nstacking_test = np.mean(stacking_test, axis=0)\nprint('mean rmse:',np.mean(rmses), 'rmse std:', np.std(rmses))\nprint('mean qwk:', np.mean(qwks), 'qwk std:', np.std(qwks))","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"# 计算融合之后的QWK\nhist = histogram(X_train['AdoptionSpeed'].astype(int), \n                 int(np.min(X_train['AdoptionSpeed'])), \n                 int(np.max(X_train['AdoptionSpeed'])))\ntr_cdf = get_cdf(hist)\ntrain_predictions = getTestScore2(stacking_train, tr_cdf)\ntest_predictions = getTestScore2(stacking_test, tr_cdf)\nqwk = quadratic_weighted_kappa(X_train['AdoptionSpeed'].values, train_predictions)\nprint(\"QWK = \", qwk)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"# 最终生成结果文件，上传\n\nsubmission = pd.DataFrame({'PetID': test['PetID'].values, 'AdoptionSpeed': test_predictions.astype(np.int32)})\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":1}