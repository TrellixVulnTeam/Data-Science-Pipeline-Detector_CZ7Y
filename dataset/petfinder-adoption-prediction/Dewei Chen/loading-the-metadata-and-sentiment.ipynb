{"cells":[{"metadata":{},"cell_type":"markdown","source":"Hi, these kernel is forked by **BaselineModeling** And just copy the part of extra Meta-images and sentiment featuers.\nAnd save it to .csv"},{"metadata":{"trusted":true,"_uuid":"a194102854a9eaec85445c81d0621d55f1476560"},"cell_type":"code","source":"import gc\nimport glob\nimport os\nimport json\nimport matplotlib.pyplot as plt\nimport pprint\n\nimport numpy as np\nimport pandas as pd\n\nfrom joblib import Parallel, delayed\nfrom tqdm import tqdm\nfrom PIL import Image\n\n%matplotlib inline\n\npd.options.display.max_rows = 128\npd.options.display.max_columns = 128","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6911a2be5a3b016869b7a94c16bfeb82cd0a7342"},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (12, 9)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f16a86529d54b5fbc3d24569da81077abab9ba21"},"cell_type":"markdown","source":"### load core DFs (train and test):"},{"metadata":{"trusted":true,"_uuid":"8473644b13e590420cdc349f7fb27a541fb6644b"},"cell_type":"code","source":"os.listdir('../input/test/')","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"54985b02f752cd86dd2a39ac261281573ca9e81d"},"cell_type":"code","source":"train = pd.read_csv('../input/train/train.csv')\ntest = pd.read_csv('../input/test/test.csv')\nsample_submission = pd.read_csv('../input/test/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"43025d0dbd7886a7e3d7168a45bec6790307ff44"},"cell_type":"markdown","source":"### load mapping dictionaries:"},{"metadata":{"trusted":true,"_uuid":"cdb36fcca79a9c8db7f139d7266d8c8e4ad021f3"},"cell_type":"code","source":"labels_breed = pd.read_csv('../input/breed_labels.csv')\nlabels_state = pd.read_csv('../input/color_labels.csv')\nlabels_color = pd.read_csv('../input/state_labels.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cd40d640362e85d9818b98e51b0be3aec443762b"},"cell_type":"markdown","source":"### additional data:\n\nWe have also additional information about pets available in form of:\n\n- images\n- metadata\n- sentiment\n\nIntegration of those will enable us to possibly improve the score.\nInformation derived from example from images should be very important, as picture of a pet influences the way we look at an animal in a significant way."},{"metadata":{"trusted":true,"_uuid":"876557740f8c8ccc2b4c0080acee005fb2eff73f"},"cell_type":"code","source":"train_image_files = sorted(glob.glob('../input/train_images/*.jpg'))\ntrain_metadata_files = sorted(glob.glob('../input/train_metadata/*.json'))\ntrain_sentiment_files = sorted(glob.glob('../input/train_sentiment/*.json'))\n\nprint('num of train images files: {}'.format(len(train_image_files)))\nprint('num of train metadata files: {}'.format(len(train_metadata_files)))\nprint('num of train sentiment files: {}'.format(len(train_sentiment_files)))\n\n\ntest_image_files = sorted(glob.glob('../input/test_images/*.jpg'))\ntest_metadata_files = sorted(glob.glob('../input/test_metadata/*.json'))\ntest_sentiment_files = sorted(glob.glob('../input/test_sentiment/*.json'))\n\nprint('num of test images files: {}'.format(len(test_image_files)))\nprint('num of test metadata files: {}'.format(len(test_metadata_files)))\nprint('num of test sentiment files: {}'.format(len(test_sentiment_files)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f0faa84d0f6382e59e704680a0291687022c99e"},"cell_type":"markdown","source":"### train analysis:"},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"96053ab161520c2d8d40a60e0a2fdf649f65bb23"},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (12, 9)\nplt.style.use('ggplot')\n\n\n# Images:\ntrain_df_ids = train[['PetID']]\nprint(train_df_ids.shape)\n\ntrain_df_imgs = pd.DataFrame(train_image_files)\ntrain_df_imgs.columns = ['image_filename']\ntrain_imgs_pets = train_df_imgs['image_filename'].apply(lambda x: x.split('/')[-1].split('-')[0])\ntrain_df_imgs = train_df_imgs.assign(PetID=train_imgs_pets)\nprint(len(train_imgs_pets.unique()))\n\npets_with_images = len(np.intersect1d(train_imgs_pets.unique(), train_df_ids['PetID'].unique()))\nprint('fraction of pets with images: {:.3f}'.format(pets_with_images / train_df_ids.shape[0]))\n\n# Metadata:\ntrain_df_ids = train[['PetID']]\ntrain_df_metadata = pd.DataFrame(train_metadata_files)\ntrain_df_metadata.columns = ['metadata_filename']\ntrain_metadata_pets = train_df_metadata['metadata_filename'].apply(lambda x: x.split('/')[-1].split('-')[0])\ntrain_df_metadata = train_df_metadata.assign(PetID=train_metadata_pets)\nprint(len(train_metadata_pets.unique()))\n\npets_with_metadatas = len(np.intersect1d(train_metadata_pets.unique(), train_df_ids['PetID'].unique()))\nprint('fraction of pets with metadata: {:.3f}'.format(pets_with_metadatas / train_df_ids.shape[0]))\n\n# Sentiment:\ntrain_df_ids = train[['PetID']]\ntrain_df_sentiment = pd.DataFrame(train_sentiment_files)\ntrain_df_sentiment.columns = ['sentiment_filename']\ntrain_sentiment_pets = train_df_sentiment['sentiment_filename'].apply(lambda x: x.split('/')[-1].split('.')[0])\ntrain_df_sentiment = train_df_sentiment.assign(PetID=train_sentiment_pets)\nprint(len(train_sentiment_pets.unique()))\n\npets_with_sentiments = len(np.intersect1d(train_sentiment_pets.unique(), train_df_ids['PetID'].unique()))\nprint('fraction of pets with sentiment: {:.3f}'.format(pets_with_sentiments / train_df_ids.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"2d6e48743d5bc42015320b367cc6e11393233264"},"cell_type":"code","source":"# Images:\ntest_df_ids = test[['PetID']]\nprint(test_df_ids.shape)\n\ntest_df_imgs = pd.DataFrame(test_image_files)\ntest_df_imgs.columns = ['image_filename']\ntest_imgs_pets = test_df_imgs['image_filename'].apply(lambda x: x.split('/')[-1].split('-')[0])\ntest_df_imgs = test_df_imgs.assign(PetID=test_imgs_pets)\nprint(len(test_imgs_pets.unique()))\n\npets_with_images = len(np.intersect1d(test_imgs_pets.unique(), test_df_ids['PetID'].unique()))\nprint('fraction of pets with images: {:.3f}'.format(pets_with_images / test_df_ids.shape[0]))\n\n\n# Metadata:\ntest_df_ids = test[['PetID']]\ntest_df_metadata = pd.DataFrame(test_metadata_files)\ntest_df_metadata.columns = ['metadata_filename']\ntest_metadata_pets = test_df_metadata['metadata_filename'].apply(lambda x: x.split('/')[-1].split('-')[0])\ntest_df_metadata = test_df_metadata.assign(PetID=test_metadata_pets)\nprint(len(test_metadata_pets.unique()))\n\npets_with_metadatas = len(np.intersect1d(test_metadata_pets.unique(), test_df_ids['PetID'].unique()))\nprint('fraction of pets with metadata: {:.3f}'.format(pets_with_metadatas / test_df_ids.shape[0]))\n\n\n\n# Sentiment:\ntest_df_ids = test[['PetID']]\ntest_df_sentiment = pd.DataFrame(test_sentiment_files)\ntest_df_sentiment.columns = ['sentiment_filename']\ntest_sentiment_pets = test_df_sentiment['sentiment_filename'].apply(lambda x: x.split('/')[-1].split('.')[0])\ntest_df_sentiment = test_df_sentiment.assign(PetID=test_sentiment_pets)\nprint(len(test_sentiment_pets.unique()))\n\npets_with_sentiments = len(np.intersect1d(test_sentiment_pets.unique(), test_df_ids['PetID'].unique()))\nprint('fraction of pets with sentiment: {:.3f}'.format(pets_with_sentiments / test_df_ids.shape[0]))\n\n\n# are distributions the same?\nprint('images and metadata distributions the same? {}'.format(\n    np.all(test_metadata_pets == test_imgs_pets)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"18f435d349d1fc384c645b41233d2c5a22860903"},"cell_type":"markdown","source":"### data parsing & feature extraction:\n\nAfter taking a look at the data, we know its structure and can use it to extract additional features and concatenate them with basic train/test DFs."},{"metadata":{"trusted":true,"_uuid":"1db107527d3ef5ba55560679334d16f1d82fb663"},"cell_type":"code","source":"class PetFinderParser(object):\n    \n    def __init__(self, debug=False):\n        \n        self.debug = debug\n        self.sentence_sep = ' '\n        \n        # Does not have to be extracted because main DF already contains description\n        self.extract_sentiment_text = False\n        \n        \n    def open_metadata_file(self, filename):\n        \"\"\"\n        Load metadata file.\n        \"\"\"\n        with open(filename, 'r') as f:\n            metadata_file = json.load(f)\n        return metadata_file\n            \n    def open_sentiment_file(self, filename):\n        \"\"\"\n        Load sentiment file.\n        \"\"\"\n        with open(filename, 'r') as f:\n            sentiment_file = json.load(f)\n        return sentiment_file\n            \n    def open_image_file(self, filename):\n        \"\"\"\n        Load image file.\n        \"\"\"\n        image = np.asarray(Image.open(filename))\n        return image\n        \n    def parse_sentiment_file(self, file):\n        \"\"\"\n        Parse sentiment file. Output DF with sentiment features.\n        \"\"\"\n        \n        file_sentiment = file['documentSentiment']\n        file_entities = [x['name'] for x in file['entities']]\n        file_entities = self.sentence_sep.join(file_entities)\n\n        if self.extract_sentiment_text:\n            file_sentences_text = [x['text']['content'] for x in file['sentences']]\n            file_sentences_text = self.sentence_sep.join(file_sentences_text)\n        file_sentences_sentiment = [x['sentiment'] for x in file['sentences']]\n        \n        file_sentences_sentiment = pd.DataFrame.from_dict(\n            file_sentences_sentiment, orient='columns').sum()\n        file_sentences_sentiment = file_sentences_sentiment.add_prefix('document_').to_dict()\n        \n        file_sentiment.update(file_sentences_sentiment)\n        \n        df_sentiment = pd.DataFrame.from_dict(file_sentiment, orient='index').T\n        if self.extract_sentiment_text:\n            df_sentiment['text'] = file_sentences_text\n            \n        df_sentiment['entities'] = file_entities\n        df_sentiment = df_sentiment.add_prefix('sentiment_')\n        \n        return df_sentiment\n    \n    def parse_metadata_file(self, file):\n        \"\"\"\n        Parse metadata file. Output DF with metadata features.\n        \"\"\"\n        \n        file_keys = list(file.keys())\n        \n        if 'labelAnnotations' in file_keys:\n            file_annots = file['labelAnnotations'][:int(len(file['labelAnnotations']) * 0.3)]\n            file_top_score = np.asarray([x['score'] for x in file_annots]).mean()\n            file_top_desc = [x['description'] for x in file_annots]\n        else:\n            file_top_score = np.nan\n            file_top_desc = ['']\n        \n        file_colors = file['imagePropertiesAnnotation']['dominantColors']['colors']\n        file_crops = file['cropHintsAnnotation']['cropHints']\n\n        file_color_score = np.asarray([x['score'] for x in file_colors]).mean()\n        file_color_pixelfrac = np.asarray([x['pixelFraction'] for x in file_colors]).mean()\n\n        file_crop_conf = np.asarray([x['confidence'] for x in file_crops]).mean()\n        \n        if 'importanceFraction' in file_crops[0].keys():\n            file_crop_importance = np.asarray([x['importanceFraction'] for x in file_crops]).mean()\n        else:\n            file_crop_importance = np.nan\n\n        df_metadata = {\n            'annots_score': file_top_score,\n            'color_score': file_color_score,\n            'color_pixelfrac': file_color_pixelfrac,\n            'crop_conf': file_crop_conf,\n            'crop_importance': file_crop_importance,\n            'annots_top_desc': self.sentence_sep.join(file_top_desc)\n        }\n        \n        df_metadata = pd.DataFrame.from_dict(df_metadata, orient='index').T\n        df_metadata = df_metadata.add_prefix('metadata_')\n        \n        return df_metadata\n    \n\n# Helper function for parallel data processing:\ndef extract_additional_features(pet_id, mode='train'):\n    \n    sentiment_filename = '../input/{}_sentiment/{}.json'.format(mode, pet_id)\n    try:\n        sentiment_file = pet_parser.open_sentiment_file(sentiment_filename)\n        df_sentiment = pet_parser.parse_sentiment_file(sentiment_file)\n        df_sentiment['PetID'] = pet_id\n    except FileNotFoundError:\n        df_sentiment = []\n\n    dfs_metadata = []\n    metadata_filenames = sorted(glob.glob('../input/{}_metadata/{}*.json'.format(mode, pet_id)))\n    if len(metadata_filenames) > 0:\n        for f in metadata_filenames:\n            metadata_file = pet_parser.open_metadata_file(f)\n            df_metadata = pet_parser.parse_metadata_file(metadata_file)\n            df_metadata['PetID'] = pet_id\n            dfs_metadata.append(df_metadata)\n        dfs_metadata = pd.concat(dfs_metadata, ignore_index=True, sort=False)\n    dfs = [df_sentiment, dfs_metadata]\n    \n    return dfs\n\n\npet_parser = PetFinderParser()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"1b8c6eedb089ac6df143592de6455c1978b460f6"},"cell_type":"code","source":"# Unique IDs from train and test:\ndebug = False\ntrain_pet_ids = train.PetID.unique()\ntest_pet_ids = test.PetID.unique()\n\nif debug:\n    train_pet_ids = train_pet_ids[:1000]\n    test_pet_ids = test_pet_ids[:500]\n\n\n# Train set:\n# Parallel processing of data:\ndfs_train = Parallel(n_jobs=6, verbose=1)(\n    delayed(extract_additional_features)(i, mode='train') for i in train_pet_ids)\n\n# Extract processed data and format them as DFs:\ntrain_dfs_sentiment = [x[0] for x in dfs_train if isinstance(x[0], pd.DataFrame)]\ntrain_dfs_metadata = [x[1] for x in dfs_train if isinstance(x[1], pd.DataFrame)]\n\ntrain_dfs_sentiment = pd.concat(train_dfs_sentiment, ignore_index=True, sort=False)\ntrain_dfs_metadata = pd.concat(train_dfs_metadata, ignore_index=True, sort=False)\n\nprint(train_dfs_sentiment.shape, train_dfs_metadata.shape)\n\n\n# Test set:\n# Parallel processing of data:\ndfs_test = Parallel(n_jobs=6, verbose=1)(\n    delayed(extract_additional_features)(i, mode='test') for i in test_pet_ids)\n\n# Extract processed data and format them as DFs:\ntest_dfs_sentiment = [x[0] for x in dfs_test if isinstance(x[0], pd.DataFrame)]\ntest_dfs_metadata = [x[1] for x in dfs_test if isinstance(x[1], pd.DataFrame)]\n\ntest_dfs_sentiment = pd.concat(test_dfs_sentiment, ignore_index=True, sort=False)\ntest_dfs_metadata = pd.concat(test_dfs_metadata, ignore_index=True, sort=False)\n\nprint(test_dfs_sentiment.shape, test_dfs_metadata.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"76489c2957389009c1b55ec2142ebf5dd9d7a923"},"cell_type":"markdown","source":"### group extracted features by PetID:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extend aggregates and improve column naming\naggregates = ['mean', 'sum', 'var']\n\n\n# Train\ntrain_metadata_desc = train_dfs_metadata.groupby(['PetID'])['metadata_annots_top_desc'].unique()\ntrain_metadata_desc = train_metadata_desc.reset_index()\ntrain_metadata_desc[\n    'metadata_annots_top_desc'] = train_metadata_desc[\n    'metadata_annots_top_desc'].apply(lambda x: ' '.join(x))\n\nprefix = 'metadata'\ntrain_metadata_gr = train_dfs_metadata.drop(['metadata_annots_top_desc'], axis=1)\nfor i in train_metadata_gr.columns:\n    if 'PetID' not in i:\n        train_metadata_gr[i] = train_metadata_gr[i].astype(float)\ntrain_metadata_gr = train_metadata_gr.groupby(['PetID']).agg(aggregates)\ntrain_metadata_gr.columns = pd.Index(['{}_{}_{}'.format(\n            prefix, c[0], c[1].upper()) for c in train_metadata_gr.columns.tolist()])\ntrain_metadata_gr = train_metadata_gr.reset_index()\n\n\ntrain_sentiment_desc = train_dfs_sentiment.groupby(['PetID'])['sentiment_entities'].unique()\ntrain_sentiment_desc = train_sentiment_desc.reset_index()\ntrain_sentiment_desc[\n    'sentiment_entities'] = train_sentiment_desc[\n    'sentiment_entities'].apply(lambda x: ' '.join(x))\n\nprefix = 'sentiment'\ntrain_sentiment_gr = train_dfs_sentiment.drop(['sentiment_entities'], axis=1)\nfor i in train_sentiment_gr.columns:\n    if 'PetID' not in i:\n        train_sentiment_gr[i] = train_sentiment_gr[i].astype(float)\ntrain_sentiment_gr = train_sentiment_gr.groupby(['PetID']).agg(aggregates)\ntrain_sentiment_gr.columns = pd.Index(['{}_{}_{}'.format(\n            prefix, c[0], c[1].upper()) for c in train_sentiment_gr.columns.tolist()])\ntrain_sentiment_gr = train_sentiment_gr.reset_index()\n\n\n# Test\ntest_metadata_desc = test_dfs_metadata.groupby(['PetID'])['metadata_annots_top_desc'].unique()\ntest_metadata_desc = test_metadata_desc.reset_index()\ntest_metadata_desc[\n    'metadata_annots_top_desc'] = test_metadata_desc[\n    'metadata_annots_top_desc'].apply(lambda x: ' '.join(x))\n\nprefix = 'metadata'\ntest_metadata_gr = test_dfs_metadata.drop(['metadata_annots_top_desc'], axis=1)\nfor i in test_metadata_gr.columns:\n    if 'PetID' not in i:\n        test_metadata_gr[i] = test_metadata_gr[i].astype(float)\ntest_metadata_gr = test_metadata_gr.groupby(['PetID']).agg(aggregates)\ntest_metadata_gr.columns = pd.Index(['{}_{}_{}'.format(\n            prefix, c[0], c[1].upper()) for c in test_metadata_gr.columns.tolist()])\ntest_metadata_gr = test_metadata_gr.reset_index()\n\n\ntest_sentiment_desc = test_dfs_sentiment.groupby(['PetID'])['sentiment_entities'].unique()\ntest_sentiment_desc = test_sentiment_desc.reset_index()\ntest_sentiment_desc[\n    'sentiment_entities'] = test_sentiment_desc[\n    'sentiment_entities'].apply(lambda x: ' '.join(x))\n\nprefix = 'sentiment'\ntest_sentiment_gr = test_dfs_sentiment.drop(['sentiment_entities'], axis=1)\nfor i in test_sentiment_gr.columns:\n    if 'PetID' not in i:\n        test_sentiment_gr[i] = test_sentiment_gr[i].astype(float)\ntest_sentiment_gr = test_sentiment_gr.groupby(['PetID']).agg(aggregates)\ntest_sentiment_gr.columns = pd.Index(['{}_{}_{}'.format(\n            prefix, c[0], c[1].upper()) for c in test_sentiment_gr.columns.tolist()])\ntest_sentiment_gr = test_sentiment_gr.reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_metadata_gr.shape, test_metadata_gr.shape)\nprint(\"sentiment\", train_sentiment_gr.shape, test_sentiment_gr.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ntrain_metadata_gr = train_metadata_gr.merge(\n    train_metadata_desc, how='left', on='PetID')\nprint(\"Train_metadata_gr\",train_metadata_gr.shape)\n\ntrain_sentiment_gr = train_sentiment_gr.merge(\n    train_sentiment_desc, how='left', on='PetID')\nprint(\"Train_sentiment_gr\",train_sentiment_gr.shape)\n\ntest_metadata_gr = test_metadata_gr.merge(\n    test_metadata_desc, how='left', on='PetID')\nprint(\"Test_metadata_gr\",test_metadata_gr.shape)\n\ntest_sentiment_gr = test_sentiment_gr.merge(\n    test_sentiment_desc, how='left', on='PetID')\nprint(\"Test_sentiment_gr\",test_sentiment_gr.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"558824a494ce6b6c5821d2cb85420393bf728338"},"cell_type":"code","source":"train_metadata_gr.to_csv('train_dfs_metadata.csv', index=False)\ntrain_sentiment_gr.to_csv('train_dfs_sentiment.csv', index=False)\ntest_metadata_gr.to_csv('test_dfs_metadata.csv', index=False)\ntest_sentiment_gr.to_csv('test_dfs_sentiment_gr.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"132f5680377aa571312a860adb1506cb05ef37b7"},"cell_type":"markdown","source":""},{"metadata":{"trusted":true,"_uuid":"947ecbc24175147fb4808f7d85068a851d39cd57"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dc1c92dca3d8e7d3bee4e7c42c24def791e9df28"},"cell_type":"markdown","source":""},{"metadata":{"trusted":true,"_uuid":"a44dda1090936393134d00e5024b74c56935b4ba"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"509754ac9784439024c118860ceef27edbe4ce84"},"cell_type":"markdown","source":""},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"a5561dc64fb58b47da60da90a8fa1aaa35349cce"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"_uuid":"707623a2bc031be931b9793fa3a462f491e938cf"},"cell_type":"raw","source":""},{"metadata":{"_uuid":"fb503cf4d614ea3f7edd103f5d0cc570986ad0a2"},"cell_type":"markdown","source":""},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"3528d7e8036c8b8bb0c5da254d9cd18d2019607d"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a424da889496d55188c3c8478f0035c9fa8a7555"},"cell_type":"markdown","source":""},{"metadata":{"trusted":true,"_uuid":"3a220d5a9cd973e97c894d57b1ef8f6c6e95fdc5"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"68d8f68c17adb43e1c5c86a773058b09c208d721"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a228cc25a0b400527671a077471cde07068cc975"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c5e1043f2ba2ca95566ccfeb60ea04a470041c44"},"cell_type":"code","source":"\n","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"0f5d0c82ad86bf1010cda2db2f35c1e32f4406fa"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3ba80726173d872ca31e1c13e10256e05db0ff40"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5acc8411e12eb4f741956b1ae608017e10950040"},"cell_type":"markdown","source":""},{"metadata":{"trusted":true,"_uuid":"ef5a8fdbe2b786bbc3ffdc797cfa3ca02262b70e"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0e0d21d076fc196facd083d6e3ae7cdbb1d575a3"},"cell_type":"markdown","source":""},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"0d232ef1ddd004597ac58f171697d122d17e63d8"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"ec6ee11a3b7d0ca97d3faf0cccd06bab9dd17637"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a87c86df9fd01e6c1b744a7d21b40d816ca66106"},"cell_type":"markdown","source":""},{"metadata":{"trusted":true,"_uuid":"7302b59080f81bde834d10e386dd35cdad394e12"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"af567e8fd28c1ce85578d21427e9c5936ad8e2e0"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"58ab4de93b0345c8c251e81f7de173de375a7953"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6d645345cb8ab39676376d10f6ce5aa0bc58d369","scrolled":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"18b9db420430b00b0a466cbde7b662ada81a95cd"},"cell_type":"markdown","source":""},{"metadata":{"trusted":true,"_uuid":"476da82fb0a09d8ae23afb03859e05b70a87b44d"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d2ccebb74d117385e00ab3d0a0df008f2c430b82"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"03ded9b8f81d756c3dc128d0d9a517d36a30c073"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"49f62f5776c7f47f561541a928231d750d09d3ce"},"cell_type":"code","source":"\n\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}