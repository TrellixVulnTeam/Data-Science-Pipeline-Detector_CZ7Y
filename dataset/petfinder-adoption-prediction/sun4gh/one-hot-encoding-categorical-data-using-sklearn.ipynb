{"cells":[{"metadata":{"_uuid":"9b1572bf909d8bf830788909d88a5afcca2ebc91"},"cell_type":"markdown","source":"Including one-hot encoded categorigal data along with numerical data, when using a DecisionTreeClassifier."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\nimport pandas as pd\nimport numpy as np\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aa117b0cbec6c77dc09f19d03c575120f52ddc11"},"cell_type":"markdown","source":"Bring in as panda dataframes\n* the labels\n* the train data as orig_train_df (because we will split them into *training* and *validation* data\n* the test data\n\nCombine the original training and testing data into one dataframe, so that the transformations we make apply to all of them. (First mark all the original training and test data as train and test accordingly so we can separate them later.)"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"breeds_df = pd.read_csv('../input/breed_labels.csv')\n\norig_train_df = pd.read_csv('../input/train/train.csv')\ntest_df = pd.read_csv('../input/test/test.csv')\n\norig_train_df['dataset_type'] = 'train'\ntest_df['dataset_type'] = 'test'\nall_data_df = pd.concat([orig_train_df, test_df])\n\nprint (\"all_data_df shape: \", all_data_df.shape)\nprint (\"orig_train_df shape: \", orig_train_df.shape)\nprint (\"test_df shape: \", test_df.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a1adac2542e9dcc1448fdba9c544dafb0053be85"},"cell_type":"markdown","source":"Before manipulating any data, let's extract the series (column) of the PetID from the test data.  Toward the end of this kernel, we will use the sample submission file to submit the results.  Before submitting, we can verify that the order of PetIDs for which we have results is the same as the order of PetIDs in the sample submission file.  (This will make more sense when we get to that point; for now we only have to obtain that series.)"},{"metadata":{"trusted":true,"_uuid":"11ff60e051a12bcc84c48bd87973f32d3bd465f5"},"cell_type":"code","source":"print (list(test_df.columns))\nthe_test_df_pet_ids = test_df[\"PetID\"]\nprint(the_test_df_pet_ids.head())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"75e5816afdadd4723633e40e1af9943685c5d3be"},"cell_type":"markdown","source":"We will need \n* a list of the numerical fields we will use\n* a list of the categorical fields we will use, and \n* a list of the categorical fields we will not use (so we can drop them from the dataframe prior to training the model.)"},{"metadata":{"trusted":true,"_uuid":"2208e3aaf7536c82b9a3a19e03af11b1f6cbf8b0"},"cell_type":"code","source":"list_of_num_fields = [ 'Age', 'MaturitySize', 'Quantity', 'Fee', 'VideoAmt', 'PhotoAmt', ]\nlist_of_categ_fields = ['Breed1']\nlist_to_drop = [ \"RescuerID\", \"Name\", \"Description\", \"PetID\", 'dataset_type']\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f2fd3184b2f0a42654eb82416ffee8b82b0930e4"},"cell_type":"markdown","source":"# Explore the Breeds1 data\n\nThe Breeds1 column in the all_data dataframe holds a *numerical* value representing Breed1, rather than holding the breed name in *text*.\n\nTo see the breed name in text, we can use the lookup dictionary which we already imported into breeds_df. The length of that dataframe is 307 rows, yet the last row (Tuxedo cat) has the numerical  value 306. That's because 307 stands for Mixed Breed and is tucked between the alphabetically-listed dog breeds, and all the cat breeds.\n\nImportant data science: I used to have a Category 304 cat and she was the love of my life.  Smartest person I ever met ðŸ¤”ðŸ˜»\n\n"},{"metadata":{"trusted":true,"_uuid":"ddc98a8bae3bae58aaae79e79312a5362848cc66"},"cell_type":"code","source":"print(\"What the head() of Breeds1 column looks like in the all_data dataframe :\",all_data_df[\"Breed1\"].head())\n\nprint(\"\\nLength of breeds_df.BreedID: \", len(breeds_df.BreedID))\nprint(\"\\nWhat the head() Breeds1 lookup table looks like:\\n\", breeds_df.head())\nprint(\"\\nWhat the tail() Breeds1 lookup table looks like:\\n\", breeds_df.tail())\nprint(\"\\nThe numerical values are of type: \", type(breeds_df.BreedID[3]))\n\nif 307 in list(breeds_df.BreedID):\n    print (\"Category 307 (Mixed Breed) is in the dictionary, at position: \" , \n           list(breeds_df.BreedID).index(307), \n           \". It is wedged between all the dog breeds and all the cat breeds:\") \n    print (breeds_df[238:243])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"69a9fa0bf2bcee1fa271226814bfbf9f9b341ee5"},"cell_type":"markdown","source":"# One-Hot Encode the Chosen Columns\n\nBreed1 is the only column we are one-hot encoding."},{"metadata":{"_uuid":"775773411cc168ddbfea977a3d5c02c9e9480ef2"},"cell_type":"markdown","source":"Testing out how to get 307 columns for one-hot encoding using get_dummies, for the head of the all_data_df.Breed1 column."},{"metadata":{"trusted":true,"_uuid":"691c54fc8e3a8c91fb6f39dd28d63965335cedd9"},"cell_type":"code","source":"the_one_hot_encodings = pd.get_dummies(all_data_df.Breed1.head(5), prefix = 'Breed1')\nprint (the_one_hot_encodings.head(5))\nprint(\"\\nThe columns are: \", the_one_hot_encodings.columns)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f46d09420b975b06032b824b5c9e5dfa9e4317da"},"cell_type":"markdown","source":"Because only at most five categories would be used in the first five rows, we get new columns for only those categories. For example, the first time I run the above, there were only 3 columns created and the .columns statement lists them as 265, 299, and 307:\n\n```Index(['Breed1_265', 'Breed1_299', 'Breed1_307'], dtype='object')```\n\nUsing the above method we run the risk of missing columns, if any categories do not exist in the training data (which we will extract again shortly) but do show up in the given test data or future test data.\n\nA better way to handle this would be to provide a complete list of potential categories to the dataframe, so that when creating the dummies all possible columns would be created."},{"metadata":{"trusted":true,"_uuid":"1e044f42adb2ad08a135f4e65b29616c4bc7c30b"},"cell_type":"code","source":"all_data_df[\"Breed1\"] = all_data_df[\"Breed1\"].astype('category', categories = list(breeds_df.BreedID) )\nthe_one_hot_encodings = pd.get_dummies(all_data_df.Breed1.head(), prefix = 'Breed1')\nprint(the_one_hot_encodings.head(5))\nprint(the_one_hot_encodings.columns)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a7f13b61ea73c6cb3a3c19d2cbebee03cece9520"},"cell_type":"markdown","source":"Concatenate, as columns, the two dataframes to get one wider dataframe, then split back to the original training data and test data."},{"metadata":{"trusted":true,"_uuid":"733616f63a41dcf10950462735749631ed762365"},"cell_type":"code","source":"all_data_df = pd.concat( [all_data_df, pd.get_dummies(all_data_df.Breed1, prefix = 'Breed1') ] , axis = 1)\nprint(\"all_data_df shape: \", all_data_df.shape)\nprint(\"Now extract from that the following:\")\n#Split them back into the original train vs. test data\norig_train_df = all_data_df[all_data_df.dataset_type == \"train\"]\ntest_df = all_data_df[all_data_df.dataset_type == \"test\"]\nprint(\"orig_train_df shape:\", orig_train_df.shape)\nprint(\"test_df shape:\", test_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1fff9ef75f600848283547b640ef79ddaa7e0e39"},"cell_type":"markdown","source":"Furthermore, split the original training data into training data and validation data, at 80-20 split.\nsklearn provides an easy way of doing this"},{"metadata":{"trusted":true,"_uuid":"d0404fe79595ea73a2e696cd5565f2abca8dc36c"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_df, val_df = train_test_split(orig_train_df, test_size = .2)\n\nprint (\"train_df shape:\", train_df.shape)\nprint (\"val_df shape:\", val_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8d67aa0af9212b123eb14c87fe294894e9152b33"},"cell_type":"markdown","source":"Drop the unneeded columns"},{"metadata":{"trusted":true,"_uuid":"5b3037d5cef7006cfc05815f6e0fa737dc10b47f"},"cell_type":"code","source":"train_targets = train_df[\"AdoptionSpeed\"]\ntrain_df.drop( [\"AdoptionSpeed\"] + list_to_drop + [\"Breed1\"], axis =1, inplace = True)\nval_targets = val_df[\"AdoptionSpeed\"]\nval_df.drop([\"AdoptionSpeed\"] + list_to_drop + [\"Breed1\"], axis = 1, inplace = True)\n\ntest_df.drop([\"AdoptionSpeed\"] + list_to_drop + [\"Breed1\"], axis = 1, inplace = True)\n\ntrain_df.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"53b1ab71821f5005ea5d0856afb02a1d0906feb3"},"cell_type":"code","source":"print (\"\\tall_data_df has been: \", all_data_df.shape)\nprint (\"From which we extracted :\")\nprint (\"\\torig_train_df\", orig_train_df.shape)\nprint (\"\\ttest_df\", test_df.shape)\nprint (\"\\nAnd furthermore, we split the orig_train_df into:\")\nprint (\"\\ttrain_df\", train_df.shape)\nprint (\"\\tval_df\", val_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"70c0f381e2cc2531963a2a1ec88d95925684df91"},"cell_type":"markdown","source":"# Create and Train the Model"},{"metadata":{"trusted":true,"_uuid":"e62d574332b8c5fef00bd3e588059df64ba4a29c"},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import cohen_kappa_score\n\nclf = DecisionTreeClassifier(random_state=0, max_leaf_nodes=22)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5c2523a7f8b9a5529fbba75099da149504107c4d"},"cell_type":"markdown","source":"Train the model"},{"metadata":{"trusted":true,"_uuid":"b0da24c00b99d3ae97a157f63a0128a356a79221"},"cell_type":"code","source":"clf.fit(train_df, train_targets)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d0e07e932bb8e179db19415f1abdf94e9a6c58b8"},"cell_type":"markdown","source":"Make a single prediction.\n(Note that the prediction comes out as a *float*, which we need to remember when later on we submit the results.)"},{"metadata":{"trusted":true,"_uuid":"2398ab63f0963de223ff6c7b4640bf9d6de5e2ec"},"cell_type":"code","source":"clf.predict(test_df.iloc[3:4])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e51cd17fd516f713ff18043c2314e4c709f66508"},"cell_type":"markdown","source":"Make predictions on the validation set, and calculate the CKS"},{"metadata":{"trusted":true,"_uuid":"ef2a3758fcb6d788f55f729756e5d5e72e19d8b8"},"cell_type":"code","source":"val_preds = clf.predict(val_df)\ncohen_kappa_score(val_preds, val_targets, weights = \"quadratic\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a2618ec1995b1c6dd3c31c11b615115a82ceba93"},"cell_type":"markdown","source":"\n# Prepare and Submit the Results"},{"metadata":{"trusted":true,"_uuid":"eafaf7c109a6bcf44daf687d6b5ca6f25907de92"},"cell_type":"code","source":"test_preds = clf.predict(test_df)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"98f38f500d292c6ea6713756aa7343b8c65434ed"},"cell_type":"markdown","source":"Create a sub_df dataframe to hold the submission of the results, by reading the sample_submission.csv .\nEnsure that the order of pets in  prediction set is the same as the order of pets in the sub_df, and if so, simply replace the \"AdoptionSpeed\" of the sub_df with our prediction results - but convert the floats to integers first."},{"metadata":{"trusted":true,"_uuid":"59bbcc3d7c4f51c8a0cd827d2d10be5baef258d3"},"cell_type":"code","source":"sub_df = pd.read_csv('../input/test/sample_submission.csv')\nprint(sub_df.PetID.head())\nprint()\nprint(the_test_df_pet_ids.head())\n\nprint(\"Are the two series types comparable:\", type(sub_df.PetID) == type(the_test_df_pet_ids))\nprint(\"The two series are identical: \", sub_df[\"PetID\"].equals(the_test_df_pet_ids))\n\nsub_df['AdoptionSpeed'] = test_preds.astype(int)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"262e66c8e021f08169743208c185e15092d595a8"},"cell_type":"markdown","source":"Convert the _df to a csv file and write it.\n(After writing it we have to manually submit it, outside of this kernel.)"},{"metadata":{"trusted":true,"_uuid":"b98bbb50adabac7b7002265c763a2c3c2cbc3cbc"},"cell_type":"code","source":"sub_df.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e5fdb399e0f4c733a4b716d10a2977e71fcae7b1"},"cell_type":"markdown","source":"# Visually Inspect the Decision Tree"},{"metadata":{"trusted":true,"_uuid":"1a1dbdf450fbb6f7b8036038dfc46c685aa4be25"},"cell_type":"code","source":"from sklearn import tree\nimport graphviz\n\noutfile = 'ourtree.dot'\ndot_data = tree.export_graphviz(clf, out_file=outfile,\n                                feature_names=train_df.columns,  \n                                class_names=['0','1','2','3','4'],  \n                                filled=True, rounded=True, max_depth=4, \n                                special_characters=True) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"333f5a883bf45967251b03cefae79dbf84d23a35"},"cell_type":"code","source":"#Efforts to create a png\n#!ls\n#!rm ourtree.png\n#!ls\n#!dot -Tpng ourtree.dot -o ourtree.png\n#!ls\n#%matplotlib inline\n#import matplotlib.pyplot as plt\n#import matplotlib.image as mpimg\n#img = mpimg.imread('ourtree.png')\n#plt.imshow(img)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"94a2f45c155052d41282106269c784f4599f04d9"},"cell_type":"code","source":"dot_data = tree.export_graphviz(clf, out_file=None,\n                                feature_names=train_df.columns,  \n                                class_names=['0','1','2','3','4'],  \n                                filled=True, rounded=True, max_depth=4, \n                                special_characters=True) \n\ngraph = graphviz.Source(dot_data)  # you can't have these two lines when trying out_file (for some reason)\ngraph ","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}