{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!ls ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport seaborn as sns\nsns.set()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport zipfile\nzf = zipfile.ZipFile('../input/web-traffic-time-series-forecasting/train_1.csv.zip') # having First.csv zipped file.\ndf = pd.read_csv(zf.open('train_1.csv'))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ndata_start_date = df.columns[1]\ndata_end_date = df.columns[-1]\nprint('Data ranges from %s to %s' % (data_start_date, data_end_date))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_random_series(df, n_series):\n    \n    sample = df.sample(n_series, random_state=8)\n    page_labels = sample['Page'].tolist()\n    series_samples = sample.loc[:,data_start_date:data_end_date]\n    \n    plt.figure(figsize=(10,6))\n    \n    for i in range(series_samples.shape[0]):\n        np.log1p(pd.Series(series_samples.iloc[i]).astype(np.float64)).plot(linewidth=1.5)\n        #pd.Series(series_samples.iloc[i]).plot(linewidth=1.5)\n    \n    plt.title('Randomly Selected Wikipedia Page Daily Views Over Time (Log(views) + 1)')\n    plt.legend(page_labels)\n    \nplot_random_series(df, 6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from datetime import timedelta\n\npred_steps = 14\npred_length=timedelta(pred_steps)\n\nfirst_day = pd.to_datetime(data_start_date) \nlast_day = pd.to_datetime(data_end_date)\n\nval_pred_start = last_day - pred_length + timedelta(1)\nval_pred_end = last_day\n\ntrain_pred_start = val_pred_start - pred_length\ntrain_pred_end = val_pred_start - timedelta(days=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"enc_length = train_pred_start - first_day\n\ntrain_enc_start = first_day\ntrain_enc_end = train_enc_start + enc_length - timedelta(1)\n\nval_enc_start = train_enc_start + pred_length\nval_enc_end = val_enc_start + enc_length - timedelta(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Train encoding:', train_enc_start, '-', train_enc_end)\nprint('Train prediction:', train_pred_start, '-', train_pred_end, '\\n')\nprint('Val encoding:', val_enc_start, '-', val_enc_end)\nprint('Val prediction:', val_pred_start, '-', val_pred_end)\n\nprint('\\nEncoding interval:', enc_length.days)\nprint('Prediction interval:', pred_length.days)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"date_to_index = pd.Series(index=pd.Index([pd.to_datetime(c) for c in df.columns[1:]]),\n                          data=[i for i in range(len(df.columns[1:]))])\n\nseries_array = df[df.columns[1:]].values\n\ndef get_time_block_series(series_array, date_to_index, start_date, end_date):\n    \n    inds = date_to_index[start_date:end_date]\n    return series_array[:,inds]\n\ndef transform_series_encode(series_array):\n    \n    series_array = np.log1p(np.nan_to_num(series_array)) # filling NaN with 0\n    series_mean = series_array.mean(axis=1).reshape(-1,1) \n    series_array = series_array - series_mean\n    series_array = series_array.reshape((series_array.shape[0],series_array.shape[1], 1))\n    \n    return series_array, series_mean\n\ndef transform_series_decode(series_array, encode_series_mean):\n    \n    series_array = np.log1p(np.nan_to_num(series_array)) # filling NaN with 0\n    series_array = series_array - encode_series_mean\n    series_array = series_array.reshape((series_array.shape[0],series_array.shape[1], 1))\n    \n    return series_array","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Model\nfrom keras.layers import Input, LSTM, Dense\nfrom keras.optimizers import Adam\n\nlatent_dim = 50 # LSTM hidden units\ndropout = .20 \n\n# Define an input series and encode it with an LSTM. \nencoder_inputs = Input(shape=(None, 1)) \nencoder = LSTM(latent_dim, dropout=dropout, return_state=True)\nencoder_outputs, state_h, state_c = encoder(encoder_inputs)\n\n# We discard `encoder_outputs` and only keep the final states. These represent the \"context\"\n# vector that we use as the basis for decoding.\nencoder_states = [state_h, state_c]\n\n# Set up the decoder, using `encoder_states` as initial state.\n# This is where teacher forcing inputs are fed in.\ndecoder_inputs = Input(shape=(None, 1)) \n\n# We set up our decoder using `encoder_states` as initial state.  \n# We return full output sequences and return internal states as well. \n# We don't use the return states in the training model, but we will use them in inference.\ndecoder_lstm = LSTM(latent_dim, dropout=dropout, return_sequences=True, return_state=True)\ndecoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n                                     initial_state=encoder_states)\n\ndecoder_dense = Dense(1) # 1 continuous output at each timestep\ndecoder_outputs = decoder_dense(decoder_outputs)\n\n# Define the model that will turn\n# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\nmodel = Model([encoder_inputs, decoder_inputs], decoder_outputs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nmodel.summary()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"first_n_samples = 20000\nbatch_size = 2**11\nepochs = 100\n\n# sample of series from train_enc_start to train_enc_end  \nencoder_input_data = get_time_block_series(series_array, date_to_index, \n                                           train_enc_start, train_enc_end)[:first_n_samples]\nencoder_input_data, encode_series_mean = transform_series_encode(encoder_input_data)\n\n# sample of series from train_pred_start to train_pred_end \ndecoder_target_data = get_time_block_series(series_array, date_to_index, \n                                            train_pred_start, train_pred_end)[:first_n_samples]\ndecoder_target_data = transform_series_decode(decoder_target_data, encode_series_mean)\n\n# lagged target series for teacher forcing\ndecoder_input_data = np.zeros(decoder_target_data.shape)\ndecoder_input_data[:,1:,0] = decoder_target_data[:,:-1,0]\ndecoder_input_data[:,0,0] = encoder_input_data[:,-1,0]\n\nmodel.compile(Adam(), loss='mean_absolute_error')\nhistory = model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n                     batch_size=batch_size,\n                     epochs=epochs,\n                     validation_split=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n# Import Data\n#df = pd.read_csv('https://github.com/selva86/datasets/raw/master/AirPassengers.csv')\n# Draw Plot\nll=df.iloc[133].tolist()\nll.pop(0)\nll\n\nfig, (ax1) = plt.subplots(1, 1,figsize=(16,6), dpi= 80)\nplot_acf(ll, ax=ax1, lags=400)\n#plot_pacf(ll, ax=ax2, lags=400)\n# Decorate\n# lighten the borders\nax1.spines[\"top\"].set_alpha(.3); ax2.spines[\"top\"].set_alpha(.3)\nax1.spines[\"bottom\"].set_alpha(.3); ax2.spines[\"bottom\"].set_alpha(.3)\nax1.spines[\"right\"].set_alpha(.3); ax2.spines[\"right\"].set_alpha(.3)\nax1.spines[\"left\"].set_alpha(.3); ax2.spines[\"left\"].set_alpha(.3)\n# font size of tick labels\nax1.tick_params(axis='both', labelsize=12)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\n\nplt.xlabel('Epoch')\nplt.ylabel('Mean Absolute Error Loss')\nplt.title('Loss Over Time')\nplt.legend(['Train','Valid'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from our previous model - mapping encoder sequence to state vectors\nencoder_model = Model(encoder_inputs, encoder_states)\n\n# A modified version of the decoding stage that takes in predicted target inputs\n# and encoded state vectors, returning predicted target outputs and decoder state vectors.\n# We need to hang onto these state vectors to run the next step of the inference loop.\ndecoder_state_input_h = Input(shape=(latent_dim,))\ndecoder_state_input_c = Input(shape=(latent_dim,))\ndecoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n\ndecoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\ndecoder_states = [state_h, state_c]\n\ndecoder_outputs = decoder_dense(decoder_outputs)\ndecoder_model = Model([decoder_inputs] + decoder_states_inputs,\n                      [decoder_outputs] + decoder_states)\n\ndef decode_sequence(input_seq):\n    \n    # Encode the input as state vectors.\n    states_value = encoder_model.predict(input_seq)\n\n    # Generate empty target sequence of length 1.\n    target_seq = np.zeros((1, 1, 1))\n    \n    # Populate the first target sequence with end of encoding series pageviews\n    target_seq[0, 0, 0] = input_seq[0, -1, 0]\n\n    # Sampling loop for a batch of sequences - we will fill decoded_seq with predictions\n    # (to simplify, here we assume a batch of size 1).\n\n    decoded_seq = np.zeros((1,pred_steps,1))\n    \n    for i in range(pred_steps):\n        \n        output, h, c = decoder_model.predict([target_seq] + states_value)\n        \n        decoded_seq[0,i,0] = output[0,0,0]\n\n        # Update the target sequence (of length 1).\n        target_seq = np.zeros((1, 1, 1))\n        target_seq[0, 0, 0] = output[0,0,0]\n\n        # Update states\n        states_value = [h, c]\n\n    return decoded_seq","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder_input_data = get_time_block_series(series_array, date_to_index, val_enc_start, val_enc_end)\nencoder_input_data, encode_series_mean = transform_series_encode(encoder_input_data)\n\ndecoder_target_data = get_time_block_series(series_array, date_to_index, val_pred_start, val_pred_end)\ndecoder_target_data = transform_series_decode(decoder_target_data, encode_series_mean)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict_and_plot(encoder_input_data, decoder_target_data, sample_ind, enc_tail_len=50):\n\n    encode_series = encoder_input_data[sample_ind:sample_ind+1,:,:] \n    pred_series = decode_sequence(encode_series)\n    \n    encode_series = encode_series.reshape(-1,1)\n    pred_series = pred_series.reshape(-1,1)   \n    target_series = decoder_target_data[sample_ind,:,:1].reshape(-1,1) \n    \n    encode_series_tail = np.concatenate([encode_series[-enc_tail_len:],target_series[:1]])\n    x_encode = encode_series_tail.shape[0]\n    \n    plt.figure(figsize=(10,6))   \n    \n    plt.plot(range(1,x_encode+1),encode_series_tail)\n    plt.plot(range(x_encode,x_encode+pred_steps),target_series,color='orange')\n    plt.plot(range(x_encode,x_encode+pred_steps),pred_series,color='teal',linestyle='--')\n    \n    plt.title('Encoder Series Tail of Length %d, Target Series, and Predictions' % enc_tail_len)\n    plt.legend(['Encoding Series','Target Series','Predictions'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict_and_plot(encoder_input_data, decoder_target_data, 300)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}