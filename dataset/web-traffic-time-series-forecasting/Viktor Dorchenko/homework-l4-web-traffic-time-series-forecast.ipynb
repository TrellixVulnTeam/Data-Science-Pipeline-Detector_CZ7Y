{"cells":[{"metadata":{"_uuid":"b9e54ae2e59bbe01fae9ae8538f9051a7cc5523f","_cell_guid":"a2b0c508-7688-4ed9-9acf-b95b4a9f2dd3","collapsed":true},"cell_type":"markdown","source":"# Web Traffic Time Series Forecasting (Experimenting with different method)\n\nBy Lai Yiu Ming, Tom\n\n1. Introduction\n    1. Competition details\n    2. Load libraries and data files, file structure and content\n    3. Missing values\n    4. Data visualization\n    5. Extreme data\n2. Data transformation and helper functions\n    1. Article names and metadata\n    2. Split into train and validation dataset\n3. Forecast methods\n    1. SMAPE, the measurement\n    2. Simple median model\n    3. Median model - weekday, weekend and holiday\n    4. ARIMA model\n    5. Facebook prophet model\n    6. Sample series analysis (For script reconciliation)\n4. Selected model performance (validation score) over train dataset\n    1. Simple median model\n    2. Median model - weekday, weekend, holiday\n    3. ARIMA model\n    4. Facebook model\n    5. mixed model"},{"metadata":{"_uuid":"466341ff90ee4d39548c2f90174d6d5933c2c62d","_cell_guid":"80d89790-42b4-4117-82c0-4452210f13f7"},"cell_type":"markdown","source":"# 1. Introduction \n\n## A. Competition details\n\n### First stage\n* Training data from 2015-07-01 to 2016-12-29\n* Testing data from 2017-01-01 to 2017-03-01\n* Length of training vs length of testing = 547 days vs 59 days\n* Predict interval is ~10.7% of the training interval\n\n### Second stage\n* Training data from 2015-07-01 to 2017-09-01\n* Testing data from 2017-09-10 to 2017-11-10\n* Length of training vs length of testing = 793 days vs 61 days\n* Predict interval is ~7.7% of the training interval"},{"metadata":{"_uuid":"10592e734dd2568886858d69960a1e2c8f3f5ed9","_cell_guid":"323c4a34-6421-4288-ac99-2916900dd11e"},"cell_type":"markdown","source":"## B. Load libraries and data files, file structure and content"},{"metadata":{"_uuid":"28610d78b73a0eb79868e208dd46f91045c7bb75","_cell_guid":"102debc2-87d8-41c4-bfac-82508f003abb","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\npd.set_option(\"max_columns\", 500)\npd.set_option(\"max_rows\", 500)\nfrom fbprophet import Prophet\nimport matplotlib.pyplot as plt\nimport math as math\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9d9e2a33b5f7e70d1e3f080c01ef8b4f28a14bc6","_cell_guid":"a7bd9791-f58e-4b4b-ad28-dab934014da8","trusted":true},"cell_type":"code","source":"# Load the data\ntrain = pd.read_csv(\"../input/web-traffic-time-series-forecasting/train_1.csv.zip\", compression=\"zip\")\nkeys = pd.read_csv(\"../input/web-traffic-time-series-forecasting/key_1.csv.zip\", compression=\"zip\")\nss = pd.read_csv(\"../input/web-traffic-time-series-forecasting/sample_submission_1.csv.zip\", compression=\"zip\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f340420a80c636df3d8c6e45f07a418bd0d57583","_cell_guid":"7cf073a9-c545-4199-865e-8ffd911cb53b","trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9ffedd5f603ceafcd6e6b192f813f64b22fdc9e9","_cell_guid":"9c72cdb3-e2ce-4ad5-8fbd-4cf501441145"},"cell_type":"markdown","source":"## C. Missing values"},{"metadata":{"_uuid":"c1be90c632f06a95e31a19a6305d4b204fe0c3dd","_cell_guid":"b897047e-daa0-49e7-8f60-2bb46487b9a6","trusted":true},"cell_type":"code","source":"# Check the data\nprint(\"Check the number of records\")\nprint(\"Number of records:\", train.shape[0], \"\\n\")\n\nprint(\"Null analysis\")\nempty_sample = train[train.isnull().any(axis=1)]\nprint(\"Number of records contain 1+ null:\", empty_sample.shape[0], \"\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Первые и последние 10 строк с null-ами"},{"metadata":{"_uuid":"7ff43c83dacae8bbdea49114e98c73808628dba5","scrolled":false,"_cell_guid":"e47c83b5-c349-45c0-be65-a333871f14fc","trusted":true},"cell_type":"code","source":"empty_sample.iloc[np.r_[0:10, len(empty_sample)-10:len(empty_sample)]]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5375f461500b9994ec73ee54bbb50f9dfa566708","_cell_guid":"d6370b56-3177-46af-a1f3-58c1d2c8eb6a"},"cell_type":"markdown","source":"## D. Data visualization"},{"metadata":{"_uuid":"73b03e85ebe22e512ed840f44f9c87fbed17a2b7","_cell_guid":"d5a55e88-28ab-4d0d-82a4-d912295cf3a3","trusted":true},"cell_type":"code","source":"# plot 3 the time series\ndef plot_time_series(df, row_num, start_col =1, ax=None):\n    if ax is None:\n            fig = plt.figure(facecolor=\"w\", figsize=[10, 6])\n            ax = fig.add_subplot(111)\n    else:\n        fig = ax.get_figure()\n\n    series_title = df.iloc[row_num, 0]\n    sample_series = df.iloc[row_num, start_col:]\n    sample_series.plot(style=\".\", ax=ax)\n    ax.set_title(\"Series: %s\" % series_title)\n\nfig, axs  = plt.subplots(4,1,figsize=(12,12))\nplot_time_series(empty_sample, 1, ax=axs[0])\nplot_time_series(empty_sample, 10, ax=axs[1])\nplot_time_series(empty_sample, 100, ax=axs[2])\nplot_time_series(empty_sample, 1005, ax=axs[3])\n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b05fff7f45fcd175a98d9fbfc51c1129f92ab752","_cell_guid":"731719c5-3a82-46cb-8e95-a72f342e97cb"},"cell_type":"markdown","source":"## E. Extreme data"},{"metadata":{"_uuid":"53dc5fd82f094abb5775b8eab7ac320a502b5a8d","_cell_guid":"9c2b896d-d160-4097-9ff3-0202896cd15f","trusted":true},"cell_type":"code","source":"# series with all NaN\nempty_sample.iloc[1000:1010]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e8471f75073027f67fdf8bd9d2427d19451af98b","_cell_guid":"6feda8d9-9b89-4c3a-ac5a-4c15ea2b1f8a"},"cell_type":"markdown","source":"# 2. Data transformation and helper functions\n## A. Article names and metadata"},{"metadata":{"_uuid":"89747bd76d2f750f36e7335f2012262f5926edd0","_cell_guid":"8682bf5e-f60f-45d3-8df3-88a0996f4031","trusted":true},"cell_type":"code","source":"import re\n\ndef breakdown_topic(str):\n    m = re.search(\"(.*)\\_(.*).wikipedia.org\\_(.*)\\_(.*)\", str)\n    if m is not None:\n        return m.group(1), m.group(2), m.group(3), m.group(4)\n    else:\n        return \"\", \"\", \"\", \"\"\n\nprint(breakdown_topic(\"Рудова,_Наталья_Александровна_ru.wikipedia.org_all-access_spider\"))\nprint(breakdown_topic(\"台灣災難列表_zh.wikipedia.org_all-access_spider\"))\nprint(breakdown_topic(\"File:Memphis_Blues_Tour_2010.jpg_commons.wikimedia.org_mobile-web_all-agents\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Достаем параметры всех wiki-страниц"},{"metadata":{"_uuid":"8558f96774bc5dcb78a31e936862c3933d254336","_cell_guid":"da27ba91-e49b-4a0f-bdb5-3cb649b69f80","trusted":true},"cell_type":"code","source":"page_details = train.Page.str.extract(r\"(?P<topic>.*)\\_(?P<lang>.*).wikipedia.org\\_(?P<access>.*)\\_(?P<type>.*)\")\n\npage_details[0:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Кол-во уникальных статей"},{"metadata":{"_uuid":"540494ecf1658f7649e0a9043b64c6ba274643e5","_cell_guid":"949a7a7c-010c-4792-af07-66016a4a6f86","trusted":true},"cell_type":"code","source":"unique_topic = page_details[\"topic\"].unique()\nprint(unique_topic)\nprint(\"Number of distinct topics:\", unique_topic.shape[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Распределения по типам параметров"},{"metadata":{"_uuid":"04fb2f9495a2b53e554af13c9b5cc30dd5d731e0","_cell_guid":"93e6ec07-6165-410f-a753-a83780c8a5c5","trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(3, 1, figsize=[12, 12])\n\npage_details[\"lang\"].value_counts().sort_index().plot.bar(ax=axs[0])\naxs[0].set_title(\"Language - distribution\")\n\npage_details[\"access\"].value_counts().sort_index().plot.bar(ax=axs[1])\naxs[1].set_title(\"Access - distribution\")\n\npage_details[\"type\"].value_counts().sort_index().plot.bar(ax=axs[2])\naxs[2].set_title(\"Type - distribution\")\n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"35d2c27cfe8db4b0881b403d57773156eccc69c2","_cell_guid":"06e8a5ac-b792-4abc-978d-fef1c9c6d8f4"},"cell_type":"markdown","source":"## B. Split into train and validation dataset"},{"metadata":{"_uuid":"aceb98a79ee58d03db63929da691a08fe8ccfe5e","_cell_guid":"8b2a5205-893c-4a57-87d9-f7117915d46c","trusted":true},"cell_type":"code","source":"# Generate train and validate dataset\ntrain_df = pd.concat([page_details, train], axis=1)\n\ndef get_train_validate_set(train_df, test_percent):\n    train_end = math.floor((train_df.shape[1] - 5) * (1 - test_percent))\n    train_ds = train_df.iloc[:, np.r_[0, 1, 2, 3, 4, 5:train_end]]\n    test_ds = train_df.iloc[:, np.r_[0, 1, 2, 3, 4, train_end:train_df.shape[1]]]\n\n    return train_ds, test_ds\n\nX_train, y_train = get_train_validate_set(train_df, 0.1)\n\nprint(\"The training set sample:\")\nprint(X_train[0:10])\nprint(\"The validation set sample:\")\nprint(y_train[0:10])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f11bf9c1d853839b7ee1f1d1403e70a57904ad65","_cell_guid":"d1756260-6638-41c6-b6b8-09d7574d948a"},"cell_type":"markdown","source":"# 3 Forecast methods\n\nВ этой секции рассмотрим наиболее популярные методы работы с временными рядами"},{"metadata":{"_uuid":"005f5e7d99ebfc8fe6ebc11921d031974b4f03b7","_cell_guid":"1dd91743-c36a-454e-9d48-520653ade0eb","trusted":true},"cell_type":"code","source":"def extract_series(df, row_num, start_idx):\n    y = df.iloc[row_num, start_idx:]\n    df = pd.DataFrame({\"ds\": y.index, \"y\": y.values})\n    return df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e02a22b21b2ed36585d4c5c84ffde3b21e7e3fcd","_cell_guid":"4287b753-9bfd-4a16-aa70-272e4bf2050e"},"cell_type":"markdown","source":"## A. SMAPE, the measurement\n\nSMAPE is harsh when the series is near zero.\nA notebook https://www.kaggle.com/cpmpml/smape-weirdness give a very good visualization of the SMAPE function.\n\nAfter you find that there is no way to further improve quality of the result, you may consider doing a little bit hacking on SMAPE to give you better score."},{"metadata":{"_uuid":"1b9849d8bdc4e7d2877acfbc3e84d512a6f573ee","_cell_guid":"39c51bf7-2449-4bbc-a787-5404a144dbe2","trusted":true},"cell_type":"code","source":"def smape(predict, actual, debug=False):\n    \"\"\"\n    predict and actual is a panda series.\n    In this implementation I will skip all the datapoint with actual is null\n    \"\"\"\n    actual = actual.fillna(0)\n    data = pd.concat([predict, actual], axis=1, keys=[\"predict\", \"actual\"])\n    data = data[data[\"actual\"].notnull()]\n    if debug:\n        print(\"debug\", data)\n\n    evals = abs(data[\"predict\"] - data[\"actual\"]) * 1.0 / (abs(data[\"predict\"]) + abs(data[\"actual\"])) * 2\n    evals[evals.isnull()] = 0\n    #print(np.sum(evals), len(data), np.sum(evals) * 1.0 / len(data))\n\n    result = np.sum(evals) / len(data) * 100.0\n\n    return result\n\n# create testing series\ntesting_series_1 = X_train.iloc[0, 5:494]\ntesting_series_2 = X_train.iloc[0, 5:494].shift(-1)\ntesting_series_3 = X_train.iloc[1, 5:494]\ntesting_series_4 = pd.Series([0, 0, 0, 0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testing_series_1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.repeat(3, 500)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### конструируем несколько типов baseline"},{"metadata":{"_uuid":"26889456ad3d2f4f2566a9b5318565546974db77","scrolled":true,"_cell_guid":"fc669ec6-3568-4c10-b17f-16c159663852","trusted":true},"cell_type":"code","source":"random_series_1 = pd.Series(np.repeat(3, 500))\nrandom_series_2 = pd.Series(np.random.normal(3, 1, 500))\nrandom_series_3 = pd.Series(np.random.normal(500, 20, 500))\nrandom_series_4 = pd.Series(np.repeat(500, 500))\n\n# testing 1 same series\nprint(\"\\nSMAPE score to predict a constant array of 3\")\nprint(\"Score (same series): %.3f\" % smape(random_series_1, random_series_1))\nprint(\"Score (same series - 1) %.3f\" % smape(random_series_1, random_series_1 - 1))\nprint(\"Score (same series + 1) %.3f\" % smape(random_series_1, random_series_1 + 1))\n\n# testing 2 same series shift by one\nprint(\"\\nSMAPE score to predict a array of normal distribution around 3\")\nprint(\"Score (random vs mean) %.3f\" % smape(random_series_2, random_series_1))\nprint(\"Score (random vs mean-1) %.3f\" % smape(random_series_2, random_series_2 - 1))\nprint(\"Score (random vs mean+1) %.3f\" % smape(random_series_2, random_series_2 + 1))\nprint(\"Score (random vs mean*0.9) %.3f\" % smape(random_series_2, random_series_2 * 0.9))\nprint(\"Score (random vs mean*1.1) %.3f\" % smape(random_series_2, random_series_2 * 1.1))\n\n# testing 3 totally different series\nprint(\"\\nSMAPE score to predict a array of normal distribution around 500\")\nprint(\"Score (random vs mean) %.3f\" % smape(random_series_3, random_series_4))\nprint(\"Score (random vs mean-20) %.3f\" % smape(random_series_3, random_series_3 - 20))\nprint(\"Score (random vs mean+20) %.3f\" % smape(random_series_3, random_series_3 + 20))\nprint(\"Score (random vs mean*0.9) %.3f\" % smape(random_series_3, random_series_3 * 0.9))\nprint(\"Score (random vs mean*1.1) %.3f\" % smape(random_series_3, random_series_3 * 1.1))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"07b01ca5365bd2537a9cc83827e5b1b974c5fefb","_cell_guid":"74171c24-cc33-4229-bbe7-9af4fa486a1e","trusted":true},"cell_type":"code","source":"y_true_1 = pd.Series(np.random.normal(1, 1, 500))\ny_true_2 = pd.Series(np.random.normal(2, 1, 500))\ny_true_3 = pd.Series(np.random.normal(3, 1, 500))\ny_pred = pd.Series(np.ones(500))\nx = np.linspace(0,10,1000)\nres_1 = list([smape(y_true_1, i * y_pred) for i in x])\nres_2 = list([smape(y_true_2, i * y_pred) for i in x])\nres_3 = list([smape(y_true_3, i * y_pred) for i in x])\nplt.plot(x, res_1, color='b')\nplt.plot(x, res_2, color='r')\nplt.plot(x, res_3, color='g')\nplt.axvline(x=1, color='k')\nplt.axvline(x=2, color='k')\nplt.axvline(x=3, color='k')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2e92732c900a79072586caeb9a275319f879dc77","_cell_guid":"d803e04a-a1f1-486c-a437-7ef813fa95c4"},"cell_type":"markdown","source":"## B. Simple median model"},{"metadata":{"_uuid":"f9df34726d19532211e00897c5a342902a5013e1","_cell_guid":"c345d9f2-5bb1-40ca-b3f8-d97c57ad1ba2","trusted":true},"cell_type":"code","source":"def plot_prediction_and_actual_2(train, forecast, actual, xlim=None, ylim=None, figSize=None, title=None):\n    fig, ax  = plt.subplots(1,1,figsize=figSize)\n    ax.plot(pd.to_datetime(train.index), train.values, \"k.\")\n    ax.plot(pd.to_datetime(actual.index), actual.values, \"r.\")\n    ax.plot(pd.to_datetime(forecast.index), forecast.values, \"b-\")\n    ax.set_title(title)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8b8f71ccfaa3830f44e3f99d0c1e526ad4b65e3c","_cell_guid":"f4d0da8a-7064-4401-86aa-bdaa20160eb1","trusted":true},"cell_type":"code","source":"def median_model(df_train, df_actual, p, review=False, figSize=[12, 4]):\n\n    def nanmedian_zero(a):\n        return np.nan_to_num(np.nanmedian(a))\n\n    df_train[\"y\"] = df_train[\"y\"].astype(np.float64)\n    df_actual[\"y\"] = df_actual[\"y\"].astype(np.float64)\n    visits = nanmedian_zero(df_train[\"y\"].values[-p:])\n    train_series = df_train[\"y\"]\n    train_series.index = df_train[\"ds\"]\n\n    idx = np.arange(p) + np.arange(len(df_train) - p + 1)[:, None]\n    b = [row[row >= 0] for row in df_train[\"y\"].values[idx]]\n    pre_forecast = pd.Series(np.append(([float(\"nan\")] * (p - 1)), list(map(nanmedian_zero, b))))\n    pre_forecast.index = df_train[\"ds\"]\n\n    forecast_series = pd.Series(np.repeat(visits, len(df_actual)))\n    forecast_series.index = df_actual[\"ds\"]\n\n    forecast_series = pre_forecast.append(forecast_series)\n\n    actual_series = df_actual[\"y\"]\n    actual_series.index = df_actual[\"ds\"]\n\n    if(review):\n        plot_prediction_and_actual_2(train_series, forecast_series, actual_series, figSize=figSize, title=\"Median model\")\n\n    return smape(forecast_series, actual_series)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"38904abe75b4ccce7f9a481841bc7ef67703aea3","_cell_guid":"e0b6ed0f-c108-48b6-9a91-c1dcec6e5ec5","trusted":true},"cell_type":"code","source":"# This is to demo the median model\nprint(train.iloc[[2]])\n\ndf_train = extract_series(X_train, 2, 5)\ndf_actual = extract_series(y_train, 2, 5)\nlang = X_train.iloc[2, 1]\nscore = median_model(df_train.copy(), df_actual.copy(), 15, review=True)\nprint(\"The SMAPE score is : %.5f\" % score)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7894159ebfcaced527c078a8e209530625f4df40","_cell_guid":"0f97bda6-75b5-41c2-a21e-acd7eaa7abe7"},"cell_type":"markdown","source":"## C. Median model - weekday, weekend and holiday"},{"metadata":{"_uuid":"7b9cb56a8fee5548340ce23db7f7446f3e83c3ed","_cell_guid":"d8710782-db8d-46bc-9442-796cec9550d7","trusted":true},"cell_type":"code","source":"# holiday variable\n#holiday_en = ['2015-01-01', '2015-01-19', '2015-04-03', '2015-05-04', '2015-05-25', '2015-07-01', '2015-07-03', '2015-09-07', '2015-11-26', '2015-11-27', '2015-12-25', '2015-12-26', '2015-12-28', '2016-01-01', '2016-01-18', '2016-03-25', '2016-05-02', '2016-05-30', '2016-07-01', '2016-07-04', '2016-09-05', '2016-11-11', '2016-11-24', '2016-12-25', '2016-12-26', '2016-12-27', '2017-01-01', '2017-01-02', '2017-01-16', '2017-04-14', '2017-05-01', '2017-05-29', '2017-07-01', '2017-07-03', '2017-07-04', '2017-09-04', '2017-11-10', '2017-11-23', '2017-12-25', '2017-12-26']\n\nholiday_en_us = ['2015-01-01', '2015-01-19', '2015-05-25', '2015-07-03', '2015-09-07', '2015-11-26', '2015-11-27', '2015-12-25', '2016-01-01', '2016-01-18', '2016-05-30', '2016-07-04', '2016-09-05', '2016-11-11', '2016-11-24', '2016-12-26', '2017-01-01', '2017-01-02', '2017-01-16', '2017-05-29', '2017-07-04', '2017-09-04', '2017-11-10', '2017-11-23', '2017-12-25']\nholiday_en_uk = ['2015-01-01', '2015-04-03', '2015-05-04', '2015-05-25', '2015-12-25', '2015-12-26', '2015-12-28', '2016-01-01', '2016-03-25', '2016-05-02', '2016-05-30', '2016-12-26', '2016-12-27', '2017-01-01', '2017-04-14', '2017-05-01', '2017-05-29', '2017-12-25', '2017-12-26']\nholiday_en_canada = ['2015-01-01', '2015-07-01', '2015-09-07', '2015-12-25', '2016-01-01', '2016-07-01', '2016-09-05', '2016-12-25', '2017-01-01', '2017-07-01', '2017-07-03', '2017-09-04', '2017-12-25']\n\nholiday_ru_russia = ['2015-01-01', '2015-01-02', '2015-01-05', '2015-01-06', '2015-01-07', '2015-01-08', '2015-01-09', '2015-02-23', '2015-03-09', '2015-05-01', '2015-05-04', '2015-05-09', '2015-05-11', '2015-06-12', '2015-11-04', '2016-01-01', '2016-01-04', '2016-01-05', '2016-01-06', '2016-01-07', '2016-02-22', '2016-02-23', '2016-03-08', '2016-05-01', '2016-05-09', '2016-06-12', '2016-06-13', '2016-11-04', '2017-01-01', '2017-01-02', '2017-01-03', '2017-01-04', '2017-01-05', '2017-01-06', '2017-01-07', '2017-02-23', '2017-02-24', '2017-03-08', '2017-05-01', '2017-05-08', '2017-05-09', '2017-06-12', '2017-11-04', '2017-11-06']\n#holiday_es = ['2015-01-01', '2015-01-06', '2015-01-12', '2015-02-02', '2015-03-16', '2015-03-23', '2015-04-02', '2015-04-03', '2015-05-01', '2015-05-18', '2015-06-08', '2015-06-15', '2015-06-29', '2015-07-20', '2015-08-07', '2015-08-17', '2015-09-16', '2015-10-12', '2015-11-01', '2015-11-02', '2015-11-16', '2015-12-06', '2015-12-08', '2015-12-12', '2015-12-25', '2016-01-01', '2016-01-06', '2016-01-11', '2016-02-01', '2016-03-21', '2016-03-24', '2016-03-25', '2016-05-01', '2016-05-09', '2016-05-30', '2016-06-06', '2016-07-04', '2016-07-20', '2016-08-07', '2016-08-15', '2016-09-16', '2016-10-12', '2016-10-17', '2016-11-01', '2016-11-02', '2016-11-07', '2016-11-14', '2016-11-21', '2016-12-06', '2016-12-08', '2016-12-12', '2016-12-25', '2016-12-26', '2017-01-01', '2017-01-02', '2017-01-06', '2017-01-09', '2017-02-06', '2017-03-20', '2017-04-13', '2017-04-14', '2017-05-01', '2017-05-29', '2017-06-19', '2017-06-26', '2017-07-03', '2017-07-20', '2017-08-07', '2017-08-15', '2017-09-16', '2017-10-12', '2017-10-16', '2017-11-01', '2017-11-02', '2017-11-06', '2017-11-13', '2017-11-20', '2017-12-06', '2017-12-08', '2017-12-12', '2017-12-25']\n\nholiday_es_mexico = ['2015-01-01', '2015-02-02', '2015-03-16', '2015-04-02', '2015-04-03', '2015-05-01', '2015-09-16', '2015-10-12', '2015-11-02', '2015-11-16', '2015-12-12', '2015-12-25', '2016-01-01', '2016-02-01', '2016-03-21', '2016-03-24', '2016-03-25', '2016-05-01', '2016-09-16', '2016-10-12', '2016-11-02', '2016-11-21', '2016-12-12', '2016-12-25', '2016-12-26', '2017-01-01', '2017-01-02', '2017-02-06', '2017-03-20', '2017-04-13', '2017-04-14', '2017-05-01', '2017-09-16', '2017-10-12', '2017-11-02', '2017-11-20', '2017-12-12', '2017-12-25']\nholiday_es_spain = ['2017-01-01', '2017-01-06', '2017-04-14', '2017-05-01', '2017-08-15', '2017-10-12', '2017-11-01', '2017-12-06', '2017-12-08', '2017-12-25', '2016-01-01', '2016-01-06', '2016-03-25', '2016-05-01', '2016-08-15', '2016-10-12', '2016-11-01', '2016-12-06', '2016-12-08', '2016-12-25', '2015-01-01', '2015-01-06', '2015-04-03', '2015-05-01', '2015-10-12', '2015-11-01', '2015-12-06', '2015-12-08', '2015-12-25']\nholiday_es_colombia = ['2015-01-01', '2015-01-12', '2015-03-23', '2015-04-02', '2015-04-03', '2015-05-01', '2015-05-18', '2015-06-08', '2015-06-15', '2015-06-29', '2015-07-20', '2015-08-07', '2015-08-17', '2015-10-12', '2015-11-02', '2015-11-16', '2015-12-08', '2015-12-25', '2016-01-01', '2016-01-11', '2016-03-21', '2016-03-24', '2016-03-25', '2016-05-01', '2016-05-09', '2016-05-30', '2016-06-06', '2016-07-04', '2016-07-20', '2016-08-07', '2016-08-15', '2016-10-17', '2016-11-07', '2016-11-14', '2016-12-08', '2016-12-25', '2017-01-01', '2017-01-09', '2017-03-20', '2017-04-13', '2017-04-14', '2017-05-01', '2017-05-29', '2017-06-19', '2017-06-26', '2017-07-03', '2017-07-20', '2017-08-07', '2017-08-15', '2017-10-16', '2017-11-06', '2017-11-13', '2017-12-08', '2017-12-25']\n\nholiday_fr_france = ['2015-01-01', '2015-04-06', '2015-05-01', '2015-05-08', '2015-05-14', '2015-05-25', '2015-07-14', '2015-08-15', '2015-11-01', '2015-11-11', '2015-12-25', '2016-01-01', '2016-03-28', '2016-05-01', '2016-05-05', '2016-05-08', '2016-05-16', '2016-07-14', '2016-08-15', '2016-11-01', '2016-11-11', '2016-12-25', '2017-01-01', '2017-04-17', '2017-05-01', '2017-05-08', '2017-05-25', '2017-06-05', '2017-07-14', '2017-08-15', '2017-11-01', '2017-11-11', '2017-12-25']\nholiday_jp_japan = ['2015-01-01', '2015-01-12', '2015-02-11', '2015-03-21', '2015-04-29', '2015-05-03', '2015-05-04', '2015-05-05', '2015-05-06', '2015-07-20', '2015-09-21', '2015-09-22', '2015-09-23', '2015-10-12', '2015-11-03', '2015-11-23', '2015-12-23', '2016-01-01', '2016-01-11', '2016-02-11', '2016-03-21', '2016-04-29', '2016-05-03', '2016-05-04', '2016-05-05', '2016-07-18', '2016-08-11', '2016-09-19', '2016-09-22', '2016-10-10', '2016-11-03', '2016-11-23', '2016-12-23', '2017-01-01', '2017-01-09', '2017-02-11', '2017-03-20', '2017-04-29', '2017-05-03', '2017-05-04', '2017-05-05', '2017-07-17', '2017-08-11', '2017-09-18', '2017-09-22', '2017-10-09', '2017-11-03', '2017-11-23', '2017-12-23']\n\n#holiday_de = ['2015-01-01', '2015-01-06', '2015-04-03', '2015-04-06', '2015-05-01', '2015-05-14', '2015-05-25', '2015-06-04', '2015-08-01', '2015-08-15', '2015-10-03', '2015-10-26', '2015-11-01', '2015-12-08', '2015-12-25', '2015-12-26', '2016-01-01', '2016-01-06', '2016-03-25', '2016-03-28', '2016-05-01', '2016-05-05', '2016-05-16', '2016-05-26', '2016-08-01', '2016-08-15', '2016-10-03', '2016-10-26', '2016-11-01', '2016-12-08', '2016-12-25', '2016-12-26', '2017-01-01', '2017-01-06', '2017-04-14', '2017-04-17', '2017-05-01', '2017-05-25', '2017-06-05', '2017-06-15', '2017-08-01', '2017-08-15', '2017-10-03', '2017-10-26', '2017-10-31', '2017-11-01', '2017-12-08', '2017-12-25', '2017-12-26']\n\nholiday_de_germany = ['2015-01-01', '2015-04-03', '2015-04-06', '2015-05-01', '2015-05-14', '2015-05-14', '2015-05-25', '2015-10-03', '2015-12-25', '2015-12-26', '2016-01-01', '2016-03-25', '2016-03-28', '2016-05-01', '2016-05-05', '2016-05-16', '2016-10-03', '2016-12-25', '2016-12-26', '2017-01-01', '2017-04-14', '2017-04-17', '2017-05-01', '2017-05-25', '2017-06-05', '2017-10-03', '2017-10-31', '2017-12-25', '2017-12-26']\nholiday_de_austria = ['2015-01-01', '2015-01-06', '2015-04-06', '2015-05-01', '2015-05-14', '2015-05-25', '2015-06-04', '2015-08-15', '2015-10-26', '2015-11-01', '2015-12-08', '2015-12-25', '2015-12-26', '2016-01-01', '2016-01-06', '2016-03-28', '2016-05-01', '2016-05-05', '2016-05-16', '2016-05-26', '2016-08-15', '2016-10-26', '2016-11-01', '2016-12-08', '2016-12-25', '2016-12-26', '2017-01-01', '2017-01-06', '2017-04-17', '2017-05-01', '2017-05-25', '2017-06-05', '2017-06-15', '2017-08-15', '2017-10-26', '2017-11-01', '2017-12-08', '2017-12-25', '2017-12-26']\nholiday_de_switzerland = ['2015-01-01', '2015-04-03', '2015-05-14', '2015-08-01', '2015-12-25', '2016-01-01', '2016-03-25', '2016-05-05', '2016-08-01', '2016-12-25', '2017-01-01', '2017-04-14', '2017-05-25', '2017-08-01', '2017-12-25']\n\n#holiday_zh = ['2015-01-01', '2015-02-18', '2015-02-19', '2015-02-20', '2015-02-21', '2015-02-22', '2015-02-23', '2015-02-27', '2015-04-03', '2015-04-04', '2015-04-05', '2015-04-06', '2015-04-07', '2015-05-01', '2015-05-25', '2015-06-19', '2015-06-20', '2015-07-01', '2015-09-03', '2015-09-28', '2015-10-01', '2015-10-09', '2015-10-10', '2015-10-21', '2015-12-25', '2015-12-26', '2016-01-01', '2016-02-07', '2016-02-08', '2016-02-09', '2016-02-10', '2016-02-11', '2016-02-12', '2016-02-29', '2016-03-25', '2016-03-26', '2016-03-28', '2016-04-04', '2016-04-05', '2016-05-01', '2016-05-02', '2016-05-14', '2016-06-09', '2016-06-10', '2016-07-01', '2016-09-15', '2016-09-16', '2016-09-28', '2016-10-01', '2016-10-10', '2016-12-25', '2016-12-26', '2016-12-27', '2017-01-01', '2017-01-02', '2017-01-27', '2017-01-28', '2017-01-29', '2017-01-30', '2017-01-31', '2017-02-01', '2017-02-27', '2017-02-28', '2017-04-03', '2017-04-04', '2017-04-14', '2017-04-15', '2017-04-17', '2017-05-01', '2017-05-03', '2017-05-29', '2017-05-30', '2017-07-01', '2017-10-01', '2017-10-02', '2017-10-04', '2017-10-05', '2017-10-09', '2017-10-10', '2017-10-28', '2017-12-25', '2017-12-26']\n\nholiday_zh_hongkong = ['2015-01-01', '2015-02-19', '2015-02-20', '2015-04-03', '2015-04-04', '2015-04-05', '2015-04-06', '2015-04-07', '2015-05-01', '2015-05-25', '2015-06-20', '2015-07-01', '2015-09-03', '2015-09-28', '2015-10-01', '2015-10-21', '2015-12-25', '2015-12-26', '2016-01-01', '2016-02-08', '2016-02-09', '2016-02-10', '2016-03-25', '2016-03-26', '2016-03-28', '2016-04-04', '2016-05-01', '2016-05-02', '2016-05-14', '2016-06-09', '2016-07-01', '2016-09-16', '2016-10-01', '2016-10-10', '2016-12-25', '2016-12-26', '2016-12-27', '2017-01-01', '2017-01-02', '2017-01-28', '2017-01-30', '2017-01-31', '2017-04-04', '2017-04-14', '2017-04-15', '2017-04-17', '2017-05-01', '2017-05-03', '2017-05-30', '2017-07-01', '2017-10-01', '2017-10-02', '2017-10-05', '2017-10-28', '2017-12-25', '2017-12-26']\nholiday_zh_taiwan = ['2015-01-01', '2015-02-18', '2015-02-19', '2015-02-20', '2015-02-21', '2015-02-22', '2015-02-23', '2015-02-23', '2015-02-27', '2015-04-03', '2015-04-05', '2015-04-06', '2015-06-19', '2015-06-20', '2015-09-28', '2015-10-09', '2015-10-10', '2016-01-01', '2016-02-07', '2016-02-08', '2016-02-09', '2016-02-10', '2016-02-11', '2016-02-12', '2016-02-29', '2016-04-04', '2016-04-05', '2016-06-09', '2016-06-10', '2016-09-15', '2016-09-16', '2016-09-28', '2016-10-10', '2017-01-01', '2017-01-02', '2017-01-27', '2017-01-28', '2017-01-29', '2017-01-30', '2017-01-31', '2017-02-01', '2017-02-27', '2017-02-28', '2017-04-03', '2017-04-04', '2017-05-01', '2017-05-29', '2017-05-30', '2017-10-04', '2017-10-09', '2017-10-10']\n\nholidays_en_us = pd.DataFrame({\n  'holiday': 'US public holiday',\n  'ds': pd.to_datetime(holiday_en_us),\n  'lower_window': 0,\n  'upper_window': 0,\n})\n\nholidays_en_uk = pd.DataFrame({\n  'holiday': 'UK public holiday',\n  'ds': pd.to_datetime(holiday_en_uk),\n  'lower_window': 0,\n  'upper_window': 0,\n})\n\nholidays_en_canada = pd.DataFrame({\n  'holiday': 'Canada public holiday',\n  'ds': pd.to_datetime(holiday_en_canada),\n  'lower_window': 0,\n  'upper_window': 0,\n})\n\nholidays_en = pd.concat((holidays_en_us, holidays_en_uk, holidays_en_canada))\n\nholidays_ru_russia = pd.DataFrame({\n  'holiday': 'Russia public holiday',\n  'ds': pd.to_datetime(holiday_ru_russia),\n  'lower_window': 0,\n  'upper_window': 0,\n})\n\nholidays_ru = holidays_ru_russia\n\nholidays_es_mexico = pd.DataFrame({\n  'holiday': 'Mexico public holiday',\n  'ds': pd.to_datetime(holiday_es_mexico),\n  'lower_window': 0,\n  'upper_window': 0,\n})\n\nholidays_es_spain = pd.DataFrame({\n  'holiday': 'Spain public holiday',\n  'ds': pd.to_datetime(holiday_es_spain),\n  'lower_window': 0,\n  'upper_window': 0,\n})\n\nholidays_es_colombia = pd.DataFrame({\n  'holiday': 'Colombia public holiday',\n  'ds': pd.to_datetime(holiday_es_colombia),\n  'lower_window': 0,\n  'upper_window': 0,\n})\n\nholidays_es = pd.concat((holidays_es_mexico, holidays_es_spain, holidays_es_colombia))\n\nholidays_fr_france = pd.DataFrame({\n  'holiday': 'France public holiday',\n  'ds': pd.to_datetime(holiday_fr_france),\n  'lower_window': 0,\n  'upper_window': 0,\n})\n\nholidays_fr = holidays_fr_france\n\nholidays_jp_japan = pd.DataFrame({\n  'holiday': 'Japan public holiday',\n  'ds': pd.to_datetime(holiday_jp_japan),\n  'lower_window': 0,\n  'upper_window': 0,\n})\n\nholidays_jp = holidays_jp_japan\n\nholidays_de_germany = pd.DataFrame({\n  'holiday': 'Germany public holiday',\n  'ds': pd.to_datetime(holiday_de_germany),\n  'lower_window': 0,\n  'upper_window': 0,\n})\n\nholidays_de_austria = pd.DataFrame({\n  'holiday': 'Austria public holiday',\n  'ds': pd.to_datetime(holiday_de_austria),\n  'lower_window': 0,\n  'upper_window': 0,\n})\n\nholidays_de_switzerland = pd.DataFrame({\n  'holiday': 'Switzerland public holiday',\n  'ds': pd.to_datetime(holiday_de_switzerland),\n  'lower_window': 0,\n  'upper_window': 0,\n})\n\nholidays_de = pd.concat((holidays_de_germany, holidays_de_austria, holidays_de_switzerland))\n\nholidays_zh_hongkong = pd.DataFrame({\n  'holiday': 'HK public holiday',\n  'ds': pd.to_datetime(holiday_zh_hongkong),\n  'lower_window': 0,\n  'upper_window': 0,\n})\n\nholidays_zh_taiwan = pd.DataFrame({\n  'holiday': 'Taiwan public holiday',\n  'ds': pd.to_datetime(holiday_zh_taiwan),\n  'lower_window': 0,\n  'upper_window': 0,\n})\n\nholidays_zh = pd.concat((holidays_zh_hongkong, holidays_zh_taiwan))\n\nholidays_dict = {\"en\": holidays_en, \n                 \"ru\": holidays_ru, \n                 \"es\": holidays_es, \n                 \"fr\": holidays_fr, \n                 \"ja\": holidays_jp,\n                 \"de\": holidays_de,\n                 \"zh\": holidays_zh}","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ccb89f2b0cc92eedcd3b6d714c97ae4212bf5393","_cell_guid":"08c4b047-d6ae-4a08-98a6-ce55e85c7f39","trusted":true},"cell_type":"code","source":"def median_holiday_model(df_train, df_actual, p, lang, review=False, figSize=[12, 4]):\n    # Split the train and actual set\n    df_train[\"ds\"] = pd.to_datetime(df_train[\"ds\"])\n    df_actual[\"ds\"] = pd.to_datetime(df_actual[\"ds\"])\n    train_series = df_train[\"y\"]\n    train_series.index = df_train[\"ds\"]\n\n    if isinstance(lang, float) and math.isnan(lang):\n        df_train[\"holiday\"] = df_train[\"ds\"].dt.dayofweek >= 5\n        df_actual[\"holiday\"] = df_actual[\"ds\"].dt.dayofweek >= 5\n    else:\n        df_train[\"holiday\"] = (df_train[\"ds\"].dt.dayofweek >= 5) | df_train[\"ds\"].isin(holidays_dict[lang].ds)\n        df_actual[\"holiday\"] = (df_actual[\"ds\"].dt.dayofweek >= 5) | df_actual[\"ds\"].isin(holidays_dict[lang].ds)\n\n    # Combine the train and actual set\n    predict_holiday = median_holiday_helper(df_train, df_actual[df_actual[\"holiday\"]], p, True)\n    predict_non_holiday = median_holiday_helper(df_train, df_actual[~df_actual[\"holiday\"]], p, False)\n\n    forecast_series = predict_non_holiday.combine_first(predict_holiday)\n\n    actual_series = df_actual[\"y\"]\n    actual_series.index = df_actual[\"ds\"]\n\n    if review:\n        plot_prediction_and_actual_2(train_series, forecast_series, actual_series, figSize=figSize, title=\"Median model with holiday\")\n\n    return smape(forecast_series, actual_series)\n\n\ndef median_holiday_helper(df_train, df_actual, p, holiday):\n\n    def nanmedian_zero(a):\n        return np.nan_to_num(np.nanmedian(a))\n    \n    df_train[\"y\"] = df_train[\"y\"].astype(np.float64).values\n    df_actual[\"y\"] = df_actual[\"y\"].astype(np.float64).values\n\n    sample = df_train[-p:]\n\n    if holiday:\n        sample = sample[sample[\"holiday\"]]\n    else:\n        sample = sample[~sample[\"holiday\"]]\n\n    visits = nanmedian_zero(sample[\"y\"])\n\n    idx = np.arange(p) + np.arange(len(df_train) - p + 1)[:, None]\n    b = [row[row >= 0] for row in df_train[\"y\"].values[idx]]\n    pre_forecast = pd.Series(np.append(([float(\"nan\")] * (p - 1)), list(map(nanmedian_zero, b))))\n    pre_forecast.index = df_train[\"ds\"]\n\n    forecast_series = pd.Series(np.repeat(visits, len(df_actual)))\n    forecast_series.index = df_actual[\"ds\"]\n\n    forecast_series = pre_forecast.append(forecast_series)\n\n    return forecast_series","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"79170ffdb313705aca01d50f5ab4b150809aeecf","_cell_guid":"51348547-d50e-4bb9-9b80-d437280a324a","trusted":true},"cell_type":"code","source":"# This is to demo the median model - weekday, weekend and \nprint(train.iloc[[2]])\n\ndf_train = extract_series(X_train, 2, 5)\ndf_actual = extract_series(y_train, 2, 5)\nlang = X_train.iloc[2, 1]\nscore = median_holiday_model(df_train.copy(), df_actual.copy(), 15, lang, review=True)\nprint(\"The SMAPE score is: %.5f\" % score)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fffe6fb76a7606a0bdc4c7a6f38917dab2bb9cc4","_cell_guid":"8d3efd7d-ef3d-4e7e-9b93-e4da2cbc1c78"},"cell_type":"markdown","source":"## D. ARIMA model\n\nThe below use the ARIMA from a Python library statsmodels. Please refer to http://www.statsmodels.org/dev/generated/statsmodels.tsa.arima_model.ARIMA.html for details.\n\nThe model is slow and may throw exception if the model cannot find a solution. (Make the model difficult to build for all series).\n\nWe will further investigate it performance in the later section."},{"metadata":{"_uuid":"f03c747e4520763f8e4d7eaf3cfca801ba3b1afc","_cell_guid":"ca14c193-4782-45bc-b438-d9d515cef043","trusted":true},"cell_type":"code","source":"from statsmodels.tsa.arima_model import ARIMA   \nimport warnings\n\ndef arima_model(df_train, df_actual, p, d, q, figSize=[12, 4], review=False):\n    df_train = df_train.fillna(0)\n    train_series = df_train[\"y\"]\n    train_series.index = df_train[\"ds\"]\n\n    result = None\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\"ignore\")\n        try:\n            arima = ARIMA(train_series, [p, d, q])\n            result = arima.fit(disp=False)\n        except Exception as e:\n            print(\"\\tARIMA failed\", e)\n\n    #print(result.params)\n    start_idx = df_train[\"ds\"][d]\n    end_idx = df_actual[\"ds\"].max()\n    forecast_series = result.predict(start_idx, end_idx, typ=\"levels\")\n\n    actual_series = df_actual[\"y\"]\n    actual_series.index = pd.to_datetime(df_actual[\"ds\"])\n\n    if review:\n        plot_prediction_and_actual_2(train_series, forecast_series, actual_series, figSize=figSize, title=\"ARIMA model\")\n\n    return smape(forecast_series, actual_series)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = df_train.fillna(0)\ntrain_series = df_train[\"y\"]\ntrain_series.index = df_train[\"ds\"]\n\nresult = None\nwith warnings.catch_warnings():\n    warnings.filterwarnings(\"ignore\")\n    try:\n        arima = ARIMA(train_series, [4, 1, 4])\n        result = arima.fit(disp=False)\n    except Exception as e:\n        print(\"\\tARIMA failed\", e)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"AR params:\", result.arparams, \"MA params:\", result.maparams)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e16d0523074cfa0c3b87447f24ec9e238226b9b8","_cell_guid":"a42595cf-46d3-477e-9bb7-f0c44492dd21","trusted":true},"cell_type":"code","source":"# This is to demo the ARIMA model\nprint(train.iloc[[2]])\n\ndf_train = extract_series(X_train, 2, 5)\ndf_actual = extract_series(y_train, 2, 5)\nlang = X_train.iloc[2, 1]\nscore = arima_model(df_train.copy(), df_actual.copy(), 2, 1, 2, review=True)\nprint(\"The SMAPE score is : %.5f\" % score)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3905533849e82db4352abbef4d247d98df3ae258","_cell_guid":"eb67304c-ea85-49f6-9845-c5173ecd9766"},"cell_type":"markdown","source":"## E. Facebook prophet library\n\nFacebook prophet library is created by facebook and aims to create a human-friendly time series forecasting libary. For details, please refer to https://facebookincubator.github.io/prophet/\n\nThere are several favor, but I will focus on holiday, yearly and log model"},{"metadata":{"_uuid":"9f43ea61b480446da7cc51d73bf8afafacb54938","_cell_guid":"f16f77ec-5cd6-46aa-934d-31295451bf9f","trusted":true},"cell_type":"code","source":"def plot_prediction_and_actual(model, forecast, actual, xlim=None, ylim=None, figSize=None, title=None):\n    fig, ax = plt.subplots(1, 1, figsize=figSize)\n    ax.set_ylim(ylim)\n    ax.plot(pd.to_datetime(actual[\"ds\"]), actual[\"y\"], \"r.\")\n    model.plot(forecast, ax=ax);\n    ax.set_title(title)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start_date = df_actual.ds.min()\nend_date = df_actual.ds.max()\n\nactual_series = df_actual.y.copy()\nactual_series.index = df_actual.ds\n\ndf_train[\"y\"] = df_train[\"y\"].astype(np.float64).values\n\ndf_actual[\"y\"] = df_actual[\"y\"].astype(np.float64).values\n\nm = Prophet()\nm.fit(df_train)\nfuture = m.make_future_dataframe(periods=60)\nforecast = m.predict(future)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5a5590c9f5bfb769a5609ccd2c4149af8f617e08","_cell_guid":"36860209-bf20-46fd-9464-0839b80d2cb9","trusted":true},"cell_type":"code","source":"# simple linear model\ndef normal_model(df_train, df_actual, review=False):\n    start_date = df_actual.ds.min()\n    end_date = df_actual.ds.max()\n\n    actual_series = df_actual[\"y\"].copy()\n    actual_series.index = df_actual[\"ds\"]\n\n    df_train[\"y\"] = df_train[\"y\"].astype(np.float64).values\n    \n    df_actual[\"y\"] = df_actual[\"y\"].astype(np.float64).values\n\n    m = Prophet()\n    m.fit(df_train)\n    future = m.make_future_dataframe(periods=60)\n    forecast = m.predict(future)\n\n    if review:\n        ymin = min(df_actual[\"y\"].min(), df_train[\"y\"].min()) - 100\n        ymax = max(df_actual[\"y\"].max(), df_train[\"y\"].max()) + 100\n    \n        plot_prediction_and_actual(m, forecast, df_actual, ylim=[ymin, ymax], figSize=[12, 4], title=\"Normal model\")\n\n    mask = (forecast[\"ds\"] >= start_date) & (forecast[\"ds\"] <= end_date)\n    forecast_series = forecast[mask].yhat # filter  predictions\n    forecast_series.index = forecast[mask].ds\n    forecast_series[forecast_series < 0] = 0 # negative values correction\n\n    return smape(forecast_series, actual_series)\n\ndef holiday_model(df_train, df_actual, lang, review=False):\n    start_date = df_actual[\"ds\"].min()\n    end_date = df_actual[\"ds\"].max()\n\n    actual_series = df_actual[\"y\"].copy()\n    actual_series.index = df_actual[\"ds\"]\n\n    df_train[\"y\"] = df_train[\"y\"].astype(np.float64).values\n\n    df_actual[\"y\"] = df_actual[\"y\"].astype(np.float64).values\n\n    if isinstance(lang, float) and math.isnan(lang):\n        holidays = None\n    else:\n        holidays = holidays_dict[lang]\n\n    m = Prophet(holidays=holidays)\n    m.fit(df_train)\n    future = m.make_future_dataframe(periods=60)\n    forecast = m.predict(future)\n\n    if review:\n        ymin = min(df_actual[\"y\"].min(), df_train[\"y\"].min()) -100\n        ymax = max(df_actual[\"y\"].max(), df_train[\"y\"].max()) +100\n        plot_prediction_and_actual(m, forecast, df_actual, ylim=[ymin, ymax], figSize=[12, 4], title=\"Holiday model\")\n\n    mask = (forecast[\"ds\"] >= start_date) & (forecast[\"ds\"] <= end_date)\n    forecast_series = forecast[mask].yhat\n    forecast_series.index = forecast[mask].ds\n    forecast_series[forecast_series < 0] = 0\n\n    return smape(forecast_series, actual_series)\n\ndef yearly_model(df_train, df_actual, lang, review=False):\n    start_date = df_actual[\"ds\"].min()\n    end_date = df_actual[\"ds\"].max()\n\n    actual_series = df_actual[\"y\"].copy()\n    actual_series.index = df_actual[\"ds\"]\n\n    df_train[\"y\"] = df_train[\"y\"].astype(np.float64).values\n\n    df_actual[\"y\"] = df_actual[\"y\"].astype(np.float64).values\n\n    if isinstance(lang, float) and math.isnan(lang):\n        holidays = None\n    else:\n        holidays = holidays_dict[lang]\n\n    m = Prophet(holidays=holidays, yearly_seasonality=True)\n    m.fit(df_train)\n    future = m.make_future_dataframe(periods=60)\n    forecast = m.predict(future)\n        \n    if review:\n        ymin = min(df_actual[\"y\"].min(), df_train[\"y\"].min()) - 100\n        ymax = max(df_actual[\"y\"].max(), df_train[\"y\"].max()) + 100\n        plot_prediction_and_actual(m, forecast, df_actual, ylim=[ymin, ymax], figSize=[12, 4], title=\"Yealry model\")\n\n    mask = (forecast[\"ds\"] >= start_date) & (forecast[\"ds\"] <= end_date)\n    forecast_series = forecast[mask].yhat\n    forecast_series.index = forecast[mask].ds\n    forecast_series[forecast_series < 0] = 0\n\n    return smape(forecast_series, actual_series)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c9124dba2e2e0a5f31ba0beda2e8a95c73ecedc1","_cell_guid":"ac907ab4-6951-47b2-a881-9ad0bb641c90","trusted":true},"cell_type":"code","source":"# log model\ndef normal_model_log(df_train, df_actual, review=False):\n    start_date = df_actual[\"ds\"].min()\n    end_date = df_actual[\"ds\"].max()\n\n    actual_series = df_actual[\"y\"].copy()\n    actual_series.index = df_actual[\"ds\"]\n\n    df_train[\"y\"] = df_train[\"y\"].astype(np.float64).values\n    df_train[\"y\"] = np.log1p(df_train[\"y\"])\n\n    df_actual[\"y\"] = df_actual[\"y\"].astype(np.float64).values\n    df_actual[\"y\"] = np.log1p(df_actual[\"y\"])\n\n    m = Prophet()\n    m.fit(df_train)\n    future = m.make_future_dataframe(periods=60)\n    forecast = m.predict(future)\n    \n    if review:\n        ymin = min(df_actual[\"y\"].min(), df_train[\"y\"].min()) - 2\n        ymax = max(df_actual[\"y\"].max(), df_train[\"y\"].max()) + 2\n        plot_prediction_and_actual(m, forecast, df_actual, ylim=[ymin, ymax], figSize=[12, 4], title=\"Normal model in log\")\n\n    mask = (forecast[\"ds\"] >= start_date) & (forecast[\"ds\"] <= end_date)\n    forecast_series = np.expm1(forecast[mask].yhat)\n    forecast_series.index = forecast[mask].ds\n    forecast_series[forecast_series < 0] = 0\n\n    return smape(forecast_series, actual_series)\n\ndef holiday_model_log(df_train, df_actual, lang, review=False):\n    start_date = df_actual[\"ds\"].min()\n    end_date = df_actual[\"ds\"].max()\n\n    actual_series = df_actual[\"y\"].copy()\n    actual_series.index = df_actual[\"ds\"]\n\n    df_train[\"y\"] = df_train[\"y\"].astype(np.float64).values\n    df_train[\"y\"] = np.log1p(df_train[\"y\"])\n\n    df_actual[\"y\"] = df_actual[\"y\"].astype(np.float64).values\n    df_actual[\"y\"] = np.log1p(df_actual[\"y\"])\n\n    if isinstance(lang, float) and math.isnan(lang):\n        holidays = None\n    else:\n        holidays = holidays_dict[lang]\n\n    m = Prophet(holidays=holidays)\n    m.fit(df_train)\n    future = m.make_future_dataframe(periods=60)\n    forecast = m.predict(future)\n    \n    if review:\n        ymin = min(df_actual[\"y\"].min(), df_train[\"y\"].min()) - 2\n        ymax = max(df_actual[\"y\"].max(), df_train[\"y\"].max()) + 2\n        plot_prediction_and_actual(m, forecast, df_actual, ylim=[ymin, ymax], figSize=[12, 4], title=\"Holiday model in log\")\n\n    mask = (forecast[\"ds\"] >= start_date) & (forecast[\"ds\"] <= end_date)\n    forecast_series = np.expm1(forecast[mask].yhat)\n    forecast_series.index = forecast[mask].ds\n    forecast_series[forecast_series < 0] = 0\n\n    return smape(forecast_series, actual_series)\n\ndef yearly_model_log(df_train, df_actual, lang, review=False):\n    start_date = df_actual.ds.min()\n    end_date = df_actual.ds.max()\n    \n    actual_series = df_actual.y.copy()\n    actual_series.index = df_actual.ds\n\n    df_train[\"y\"] = df_train[\"y\"].astype(np.float64).values\n    df_train[\"y\"] = np.log1p(df_train[\"y\"])\n\n    df_actual[\"y\"] = df_actual[\"y\"].astype(\"float\").values\n    df_actual[\"y\"] = np.log1p(df_actual[\"y\"])\n\n    if isinstance(lang, float) and math.isnan(lang):\n        holidays = None\n    else:\n        holidays = holidays_dict[lang]\n        \n    m = Prophet(holidays=holidays, yearly_seasonality=True)\n    m.fit(df_train)\n    future = m.make_future_dataframe(periods=60)\n    forecast = m.predict(future)\n\n    if review:\n        ymin = min(df_actual[\"y\"].min(), df_train[\"y\"].min()) - 2\n        ymax = max(df_actual[\"y\"].max(), df_train[\"y\"].max()) + 2\n        plot_prediction_and_actual(m, forecast, df_actual, ylim=[ymin, ymax], figSize=[12, 4], title=\"Yearly model in log\")\n\n    mask = (forecast[\"ds\"] >= start_date) & (forecast[\"ds\"] <= end_date)\n    forecast_series = np.expm1(forecast[mask].yhat)\n    forecast_series.index = forecast[mask].ds\n    forecast_series[forecast_series < 0] = 0\n\n    return smape(forecast_series, actual_series)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2be1e7acedf5b938ac62af9bd0cc3445deaedecd","_cell_guid":"5ea8c040-f6a1-4aec-92c9-4c659908961a","trusted":true},"cell_type":"code","source":"# This is to demo the facebook prophet model\nprint(train.iloc[[2]])\n\ndf_train = extract_series(X_train, 2, 5)\ndf_actual = extract_series(y_train, 2, 5)\nlang = X_train.iloc[2, 1]\nscore = holiday_model_log(df_train.copy(), df_actual.copy(), lang, review=True)\nprint(\"The SMAPE score is : %.5f\" % score)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4f42b04b568d3faab544815b2960881f491bb839","_cell_guid":"1039edbc-fc66-4b3a-9d5c-8f58b88066e4"},"cell_type":"markdown","source":"## F. Sample series analysis (For script reconciliation)"},{"metadata":{"_uuid":"26fd25bd241293c5a70078dc8a49101ceeddf177","_cell_guid":"d24966ad-e24b-4f15-8289-6c31acf7303f"},"cell_type":"markdown","source":"### Case 1: SMAPE evaluation near zero and SMAPE score is too big"},{"metadata":{"_uuid":"24d413fecd75c366a1afb56362fbd4f61fd27743","_cell_guid":"93edcdf8-f679-44fd-a7ca-f30a4e06e6af","trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8950b58c273e8d1f304d3874c19c8d970eae24e1","_cell_guid":"266500b3-cee0-4c6d-add1-d856e3d65cc2","trusted":true},"cell_type":"code","source":"print(train.iloc[[2]])\n\ndf_train = extract_series(X_train, 2, 5)\ndf_actual = extract_series(y_train, 2, 5)\nlang = X_train.iloc[2, 1]\nscore = holiday_model(df_train.copy(), df_actual.copy(), lang, review=True)\nprint(\"The SMAPE score is : %.5f\" % score)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d5c1ef4155f38326acd3f3c91110c39d7d623c8a","_cell_guid":"9a9f2476-5884-469a-8edf-8587f258b61b"},"cell_type":"markdown","source":"### Case 2: Yearly model is the best model"},{"metadata":{"_uuid":"20d94726c954632bdcc15cfa16a5334085174112","scrolled":false,"_cell_guid":"bace57c0-6635-4d3a-8c08-37905ded472c","trusted":true},"cell_type":"code","source":"print(train.iloc[[4464]])\n\ndf_train = extract_series(X_train, 4464, 5)\ndf_actual = extract_series(y_train, 4464, 5)\nlang = X_train.iloc[4464, 1]\n\nscore = holiday_model(df_train.copy(), df_actual.copy(), lang, review=True)\nprint(\"The SMAPE score is : %.5f\" % score)\n\nscore = holiday_model_log(df_train.copy(), df_actual.copy(), lang, review=True)\nprint(\"The SMAPE score is : %.5f\" % score)\n\nscore = yearly_model(df_train.copy(), df_actual.copy(), lang, review=True)\nprint(\"The SMAPE score is : %.5f\" % score)\n\nscore = yearly_model_log(df_train.copy(), df_actual.copy(), lang, review=True)\nprint(\"The SMAPE score is : %.5f\" % score)\n\nscore = median_model(df_train.copy(), df_actual.copy(), 14, review=True)\nprint(\"The SMAPE score is : %.5f\" % score)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3c53ee45f04e8353b58773cc9daad8568d9e72e9","_cell_guid":"7bcd1ddf-179f-4f02-a5dc-e2508f2f0931"},"cell_type":"markdown","source":"### Case 3: Non-yearly model is better"},{"metadata":{"_uuid":"ed258f644bc4369b67134298857ed37eee7c3ff5","_cell_guid":"96cfa1a0-82dd-4540-959e-1bf8cee97399","trusted":true},"cell_type":"code","source":"train.iloc[[6245]]\n\ndf_train = extract_series(X_train, 6245, 5)\ndf_actual = extract_series(y_train, 6245, 5)\nlang = X_train.iloc[6245, 1]\nscore = holiday_model_log(df_train.copy(), df_actual.copy(), lang, review=True)\nprint(\"The SMAPE score is : %.5f\" % score)\n\nscore = yearly_model_log(df_train.copy(), df_actual.copy(), lang, review=True)\nprint(\"The SMAPE score is : %.5f\" % score)\n\nscore = median_model(df_train.copy(), df_actual.copy(), 14, review=True)\nprint(\"The SMAPE score is : %.5f\" % score)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eed2138dc51c80efc7d0ccc4484ad5c4d6016177","_cell_guid":"9014a21d-bf34-45df-ab60-b702dff43d1d"},"cell_type":"markdown","source":"### Case 4: SMAPE score is too high for all proposed models"},{"metadata":{"_uuid":"992e8486db836f77989723cf3204181a0703253e","_cell_guid":"6bf9e927-a141-408d-8ab7-927d74fe9d19","trusted":true},"cell_type":"code","source":"train.iloc[[80002]]\n\ndf_train = extract_series(X_train, 80002, 5)\ndf_actual = extract_series(y_train, 80002, 5)\nlang = X_train.iloc[80002, 1]\ntitle = X_train.iloc[80002, 4]\nprint(title)\n\nscore = holiday_model(df_train.copy(), df_actual.copy(), lang, review=True)\nprint(\"The SMAPE score is : %.5f\" % score)\n\nscore = holiday_model_log(df_train.copy(), df_actual.copy(), lang, review=True)\nprint(\"The SMAPE score is : %.5f\" % score)\n\nscore = yearly_model_log(df_train.copy(), df_actual.copy(), lang, review=True)\nprint(\"The SMAPE score is : %.5f\" % score)\n\n# Please use this case to check your implementation of SMAPE\nscore = median_model(df_train.copy(), df_actual.copy(), 14, review=True)\nprint(\"The SMAPE score is : %.5f\" % score)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"45f88848f6c62854cc7a249644f42ff5ab4eced9","_cell_guid":"bcedcdbc-dc97-4d4e-a46c-678aef805e77"},"cell_type":"markdown","source":"### Case 5: SMAPE score is too high for all proposed models"},{"metadata":{"_uuid":"b401c383500e117689177a81c7190e00fd5d4f70","_cell_guid":"740c9c66-db23-477e-a859-32b9f13895f5","trusted":true},"cell_type":"code","source":"train.iloc[[80009]]\n\ndf_train = extract_series(X_train, 80009, 5)\ndf_actual = extract_series(y_train, 80009, 5)\nlang = X_train.iloc[80009, 1]\ntitle = X_train.iloc[80009, 4]\nprint(title)\n\nscore = holiday_model(df_train.copy(), df_actual.copy(), review=True, lang=lang)\nprint(\"The SMAPE score is : %.5f\" % score)\n\nscore = holiday_model_log(df_train.copy(), df_actual.copy(), lang, review=True)\nprint(\"The SMAPE score is : %.5f\" % score)\n\nscore = yearly_model_log(df_train.copy(), df_actual.copy(), lang, review=True)\nprint(\"The SMAPE score is : %.5f\" % score)\n\nscore = median_model(df_train.copy(), df_actual.copy(), 14, review=True)\nprint(\"The SMAPE score is : %.5f\" % score)\n\nscore = arima_model(df_train.copy(), df_actual.copy(), 2, 1, 2, review=True)\nprint(\"The SMAPE score is : %.5f\" % score)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"22f16608fb3bb04d2677f1c5fe10d22e8de2d0f1","_cell_guid":"b79fae38-900b-4568-a5c1-c4e09c8e656e"},"cell_type":"markdown","source":"### Case 6: SMAPE score is too high for all proposed models"},{"metadata":{"_uuid":"d2a2835f3739b3cd3f0b2d642b53d4f98503d558","scrolled":false,"_cell_guid":"e926c977-a13f-44a7-b947-02cbd0ce45a7","trusted":true},"cell_type":"code","source":"train.iloc[[14211]]\n\ndf_train = extract_series(X_train, 14211, 5)\ndf_actual = extract_series(y_train, 14211, 5)\nlang = X_train.iloc[14211, 1]\ntitle = X_train.iloc[14211, 4]\nprint(title)\nscore = holiday_model(df_train.copy(), df_actual.copy(), review=True, lang=lang)\nprint(\"The SMAPE score is : %.5f\" % score)\n\nscore = holiday_model_log(df_train.copy(), df_actual.copy(), lang, review=True)\nprint(\"The SMAPE score is : %.5f\" % score)\n\nscore = yearly_model_log(df_train.copy(), df_actual.copy(), lang, review=True)\nprint(\"The SMAPE score is : %.5f\" % score)\n\n# if there is too many zero, just use normal is OK.\nscore = median_model(df_train.copy(), df_actual.copy(), 14, review=True)\nprint(\"The SMAPE score is : %.5f\" % score)\n\nscore = arima_model(df_train.copy(), df_actual.copy(), 7, 1, 2, review=True)\nprint(\"The SMAPE score is : %.5f\" % score)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bae92d3c1d4e641f160181fa697fc0db611d7a86","_cell_guid":"6ef8d842-6b5c-4fc7-8a49-a6b6b72b58c0"},"cell_type":"markdown","source":"### Case ?: Adhoc study"},{"metadata":{"_uuid":"cb1a78f470e4135f3e6e2093cfd4dee3a3043edb","scrolled":false,"_cell_guid":"e7244a09-d28b-47bd-94f8-05c5091bb8d3","trusted":true},"cell_type":"code","source":"series_num = 145033\nseries_num = 145057\n\nprint(train.iloc[[series_num]])\n\ndf_train = extract_series(X_train, series_num, 5)\ndf_actual = extract_series(y_train, series_num, 5)\n\nlang = X_train.iloc[series_num, 1]\ntitle = X_train.iloc[series_num, 4]\nprint(title)\n\ntry:\n    score = median_model(df_train.copy(), df_actual.copy(), 14, review=True)\n    print(\"The SMAPE score is : %.5f\" % score)\nexcept Exception as e:\n    print(\"Error in calculating median model\", e)\n\ntry:\n    score = holiday_model(df_train.copy(), df_actual.copy(), review=True,lang = lang)\n    print(\"The SMAPE score is : %.5f\" % score)\nexcept Exception as e:\n    print(\"Error in calculating holiday model\", e)\n\ntry:\n    score = holiday_model_log(df_train.copy(), df_actual.copy(), lang, review=True)\n    print(\"The SMAPE score is : %.5f\" % score)\nexcept Exception as e:\n    print(\"Error in calculating holiday model in log\", e)\n\ntry:\n    score = yearly_model_log(df_train.copy(), df_actual.copy(), lang, review=True)\n    print(\"The SMAPE score is : %.5f\" % score)\nexcept Exception as e:\n    print(\"Error in calculating yearly model in log\", e)\n\ntry:\n    score = arima_model(df_train.copy(), df_actual.copy(), 7, 1, 2, review=True)\n    print(\"The SMAPE score is : %.5f\" % score)\nexcept Exception as e:\n    print(\"Error in calculating arima model\", e)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6f5596a8ad3d79e261df1d80174b238f07c8dfc8","_cell_guid":"eddfe1ff-8074-4beb-940b-c7d048d515a6"},"cell_type":"markdown","source":"## 4. Selected model performance (validation score) over train dataset\n\nIn this session, we wil train the model and do prediction over 145000+ series in dataset. \nTo find out the validation score for comparison"},{"metadata":{"_uuid":"f1f20720611d8b754be84626695aa47840aaf5a1","_cell_guid":"0f8be8a7-6d92-4817-bc3e-85f6dfd21c8f","trusted":true},"cell_type":"code","source":"import glob\n\ndef read_from_folder(path):\n    filenames = glob.glob(path + \"/*.csv\")\n\n    dfs = []\n    for filename in filenames:\n        dfs.append(pd.read_csv(filename, index_col=0))\n    \n    frame = pd.concat(dfs)\n    return frame.sort_index()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b9171b0b831ddb461f8d50f9d3179180b3171a81","_cell_guid":"f7d00919-d6cf-403f-bbf9-c983c18b9743","trusted":true},"cell_type":"code","source":"# TODO: overall validation score in one number.\ndef validation_score(score_series):\n    return score_series.mean()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6df92f7616e7e7936e95beb680f283b743b8479f","_cell_guid":"0805700f-ba13-46b8-993e-aad1cc23ad65","trusted":true},"cell_type":"code","source":"valid_fn = r\"../input/wiktraffictimeseriesforecast/validation_score.csv\"\nvalid_score_data = pd.read_csv(valid_fn, index_col=0)\n\nprint(valid_score_data[0:10])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"valid_score_data","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a9dff8bf2917d7cef03964e1a1c47563db002257","_cell_guid":"947ddf57-d10a-44f8-b9ab-e7470707166d"},"cell_type":"markdown","source":"### A. Simple median model\n\nWe will train up the median model using popular choice 7 to 49, with step 7, and compare the overall score."},{"metadata":{"_uuid":"27a1a57f047143412b2000a239ee9f56e1f8c8e4","_cell_guid":"5be881c9-e59e-42eb-89a4-82c0777c82b0","trusted":true},"cell_type":"code","source":"# Check which model is the best\nprint(\"Validation score for median model (7 days) is: %.6f\" % validation_score(valid_score_data[\"median7\"]))\nprint(\"Validation score for median model (14 days) is: %.6f\" % validation_score(valid_score_data[\"median14\"]))\nprint(\"Validation score for median model (21 days) is: %.6f\" % validation_score(valid_score_data[\"median21\"]))\nprint(\"Validation score for median model (28 days) is: %.6f\" % validation_score(valid_score_data[\"median28\"]))\nprint(\"Validation score for median model (35 days) is: %.6f\" % validation_score(valid_score_data[\"median35\"]))\nprint(\"Validation score for median model (42 days) is: %.6f\" % validation_score(valid_score_data[\"median42\"]))\nprint(\"Validation score for median model (49 days) is: %.6f\" % validation_score(valid_score_data[\"median49\"]))\n\nfig, axs  = plt.subplots(4, 2, figsize=[12,12])\nvalid_score_data[\"median7\"].plot.hist(bins=40, ax=axs[0][0])\nvalid_score_data[\"median14\"].plot.hist(bins=40, ax=axs[0][1])\nvalid_score_data[\"median21\"].plot.hist(bins=40, ax=axs[1][0])\nvalid_score_data[\"median28\"].plot.hist(bins=40, ax=axs[1][1])\nvalid_score_data[\"median35\"].plot.hist(bins=40, ax=axs[2][0])\nvalid_score_data[\"median42\"].plot.hist(bins=40, ax=axs[2][1])\nvalid_score_data[\"median49\"].plot.hist(bins=40, ax=axs[3][0])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"81a4eb1a4c237aacf2c06b8d5c19646ee2a773cc","_cell_guid":"a03e2e3f-8ddb-4eae-af05-fc6b011c1594"},"cell_type":"markdown","source":"### B. Median model - weekday, weekend, holiday"},{"metadata":{"_uuid":"c1c934998daa244f426f747f9a5362440b06d113","_cell_guid":"8d9ea6cb-332f-43dc-b9df-e0b5081ed0ed","trusted":true},"cell_type":"code","source":"print(\"Validation score for median model w/holiday (7 days) is: %.6f\" % validation_score(valid_score_data[\"median7_h\"]))\nprint(\"Validation score for median model w/holiday (14 days) is: %.6f\" % validation_score(valid_score_data[\"median14_h\"]))\nprint(\"Validation score for median model w/holiday (21 days) is: %.6f\" % validation_score(valid_score_data[\"median21_h\"]))\nprint(\"Validation score for median model w/holiday (28 days) is: %.6f\" % validation_score(valid_score_data[\"median28_h\"]))\nprint(\"Validation score for median model w/holiday (35 days) is: %.6f\" % validation_score(valid_score_data[\"median35_h\"]))\nprint(\"Validation score for median model w/holiday (42 days) is: %.6f\" % validation_score(valid_score_data[\"median42_h\"]))\nprint(\"Validation score for median model w/holiday (49 days) is: %.6f\" % validation_score(valid_score_data[\"median49_h\"]))\n\nfig, axs  = plt.subplots(4, 2, figsize=[12, 12])\nvalid_score_data[\"median7_h\"].plot.hist(bins=40, ax=axs[0][0])\nvalid_score_data[\"median14_h\"].plot.hist(bins=40, ax=axs[0][1])\nvalid_score_data[\"median21_h\"].plot.hist(bins=40, ax=axs[1][0])\nvalid_score_data[\"median28_h\"].plot.hist(bins=40, ax=axs[1][1])\nvalid_score_data[\"median35_h\"].plot.hist(bins=40, ax=axs[2][0])\nvalid_score_data[\"median42_h\"].plot.hist(bins=40, ax=axs[2][1])\nvalid_score_data[\"median49_h\"].plot.hist(bins=40, ax=axs[3][0])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bf844730db0123db1c1cbb09ad6b4bcb85eb404e","_cell_guid":"746a9c9c-6bdc-4577-91cf-61dfca3afa93"},"cell_type":"markdown","source":"### C. ARIMA model\n\nCurrently, it is no promising, and median seems a better base line, so I give up this section."},{"metadata":{"_uuid":"cf15ab6eb24b181889e283d907edf67bfd6e3174","_cell_guid":"87e6ef50-1392-43f5-989a-9e35b9ee675d"},"cell_type":"markdown","source":"### D. Facebook model\nWe will train up the model using model with yearly and non-yearly model to see the difference"},{"metadata":{"_uuid":"5ac78581fb57079909ab4f1ad35599216e52ddf7","_cell_guid":"b6ff168f-c927-4f6e-889e-16ea52d1654b","trusted":true},"cell_type":"code","source":"print(\"Validation score for holiday model is: %.6f\" % validation_score(valid_score_data[\"holiday\"]))\nprint(\"Validation score for holiday model w/log is: %.6f\" % validation_score(valid_score_data[\"holiday_log\"]))\nprint(\"Validation score for yearly model w/log is: %.6f\" % validation_score(valid_score_data[\"yearly_log\"]))\n\nfig, axs  = plt.subplots(3, 1, figsize=[12, 12])\nvalid_score_data[\"holiday\"].plot.hist(bins=40, ax=axs[0])\naxs[0].set_title(\"Holiday model\")\nvalid_score_data[\"holiday_log\"].plot.hist(bins=40, ax=axs[1])\naxs[1].set_title(\"Holiday model w/log\")\nvalid_score_data[\"yearly_log\"].plot.hist(bins=40, ax=axs[2])\naxs[2].set_title(\"Yearly model w/log\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"33b68b34bcf8f5bb3116f26deeab8c36ef6a1327","_cell_guid":"06234020-ef09-43d6-a158-2001674f664e"},"cell_type":"markdown","source":"### E. mixed model\n\nIn this section, we will try to mix the model together to give a better prediction model"},{"metadata":{"_uuid":"65cc25e7b304aca87b199c480330a08da2d1f69c","_cell_guid":"cfa216a4-e638-4b6f-b9d9-1a39c8b1e658","trusted":true},"cell_type":"code","source":"def model_to_use(median, holiday_log, yearly_log):\n    result = median\n    if median * 1 > yearly_log:\n        result = yearly_log\n    elif median * 1 > holiday_log:\n        result = holiday_log\n\n    return result\n\ndef model_to_use_linear(median, holiday_log, yearly_log):\n    result = median\n    if median * 1 > yearly_log:\n        result = yearly_log\n    elif median * 1 > holiday_log:\n        result = holiday_log\n\n    return result\n\nmodel_score = valid_score_data.apply(lambda x: model_to_use(x[\"median14\"], x[\"holiday_log\"], x[\"yearly_log\"]), axis=1)\n\nprint(\"Validation score for a proposed model is: %.6f\" % validation_score(model_score))\nmodel_score.plot.hist(bins=40)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"df64f2422afc0b8dd04d8cd5fa9d6325b72ef009","_cell_guid":"aeb57480-2340-4288-b7fb-48c97c2c34f2","trusted":true},"cell_type":"code","source":"model_score_2 = valid_score_data.min(axis=1)\nprint(\"Best possible Validation score for a mixed model is: %.6f\" % validation_score(model_score_2))\n\nmodel_score_2.plot.hist(bins=40)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def model_to_use(median, holiday_log, yearly_log):\n    result = median\n    if median * 1 > yearly_log:\n        result = yearly_log\n    elif median * 1 > holiday_log:\n        result = holiday_log\n\n    return result\n\ndef model_to_use_linear(median, holiday_log, yearly_log):\n    result = median\n    if median * 1 > yearly_log:\n        result = yearly_log\n    elif median * 1 > holiday_log:\n        result = holiday_log\n\n    return result\n\nmodel_score = valid_score_data.apply(lambda x: model_to_use(x[\"median14\"], x[\"holiday_log\"], x[\"yearly_log\"]), axis=1)\n\nprint(\"Validation score for a proposed model is: %.6f\" % validation_score(model_score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Попытка улучшить скор лучше медианной модели\n#### Будем обучать классификатор, который должен для каждой статьи подбирать лучшую для нее модель временного ряда"},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\nimport re\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom sklearn import preprocessing\nfrom sklearn import utils\nfrom sklearn.utils import resample\nfrom mlxtend.feature_selection import SequentialFeatureSelector as SFS\nfrom sklearn.feature_selection import RFECV\nfrom sklearn import metrics\nfrom sklearn.metrics import make_scorer\nfrom sklearn.tree import DecisionTreeClassifier\nimport lightgbm\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set(style=\"white\")\n\ndef get_script_time(script_time):\n    hours = script_time // 3600\n    minutes = (script_time % 3600) // 60\n    seconds = script_time % 60\n    result = (f\"{hours}h \" if hours > 0 else \"\") + (f\"{minutes}m \" if minutes > 0 else \"\") + f\"{seconds}s\"\n    return result","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Считаем различные факторы для каждой статьи"},{"metadata":{"trusted":true},"cell_type":"code","source":"t_start = time.time()\n\ntry:\n    df = pd.read_csv(\"../input/featuredata/data.csv\")\n\nexcept FileNotFoundError:\n    df = valid_score_data[[\"median21_h\", \"holiday_log\", \"yearly_log\"]].round(4)\n\n    # Выбираем лучшую модель из трех для каждой статьи\n    df[\"best_model\"] = df.idxmin(axis=1)\n\n    # Считаем агрегации для каждой статьи за все дни\n    aggregations = [\"mean\", \"median\", \"min\", \"max\", \"std\"]\n    df = pd.concat([df, X_train.iloc[:, 5:].agg(aggregations, axis=1).round(4)], axis=1)\n    df[\"std_%\"] = np.around(df[\"std\"] / df[\"mean\"] * 100, 4)\n    df.loc[df[\"mean\"] == 0, \"std_%\"] = 0\n\n    print(f\"Total aggregations done in {get_script_time(int(time.time() - t_start))}\")\n    print(\"\\n\" + \"\".join([\"=\" for _ in range(80)]) + \"\\n\")\n\n    # Считаем агрегации за последние n дней\n    for p in range(7, 56, 7):\n        tmp = X_train.iloc[:, -p:].agg(aggregations, axis=1).round(4)\n        tmp.rename(columns={col: f\"{col}_{p}d\" for col in tmp.columns}, inplace=True)\n        df = pd.concat([df, tmp], axis=1)\n        df[f\"std_%_{p}d\"] = np.around(df[f\"std_{p}d\"] / df[f\"mean_{p}d\"] * 100, 4)\n        df.loc[df[f\"mean_{p}d\"] == 0, f\"std_%_{p}d\"] = 0\n\n        if p != 49:\n            for i in range(p + 7, 56, 7):\n                tmp = X_train.iloc[:, -i:-p].agg(aggregations, axis=1).round(4)\n                tmp.rename(columns={col: f\"{col}_{p}-{i}d\" for col in tmp.columns}, inplace=True)\n                df = pd.concat([df, tmp], axis=1)\n                df[f\"std_%_{p}-{i}d\"] = np.around(df[f\"std_{p}-{i}d\"] / df[f\"mean_{p}-{i}d\"] * 100, 4)\n                df.loc[df[f\"mean_{p}-{i}d\"] == 0, f\"std_%_{p}-{i}d\"] = 0\n\n        print(f\"{p} days aggregations done in {get_script_time(int(time.time() - t_start))}\")\n\n    df.to_csv(\"data.csv\", index=False)\n    print(\"\\n\" + \"\".join([\"=\" for _ in range(80)]) + \"\\n\")\n\n\n    # Отношения показателей за последние n дней к n + 7 дней и ко всем дням\n    for idx, feature in enumerate(df.columns.tolist()):\n        days = re.findall(\"\\d+[d]\", feature)\n        if days and \"-\" not in feature:\n            aggregation = feature[:feature.find(days[0])-1]\n            df[f\"{feature}_to_total_ratio\"] = np.around(df[feature] / df[aggregation], 4)\n            df.loc[df[aggregation] == 0, f\"{feature}_to_total_ratio\"] = 0\n\n            if days[0] != \"49d\":\n                for to_days in range(int(days[0][:-1]) + 7, 56, 7):\n                    df[f\"{feature}_to_{to_days}d_ratio\"] = np.around(df[feature] / df[f\"{aggregation}_{to_days}d\"], 4)\n                    df.loc[df[f\"{aggregation}_{to_days}d\"] == 0, f\"{feature}_to_{to_days}d_ratio\"] = 0\n\n        elif \"-\" in feature:\n            days = int(re.findall(\"\\d+[-]\", feature)[0][:-1])\n            to_days = int(re.findall(\"\\d+[d]\", feature)[0][:-1])\n            aggregation = feature[:feature.find(str(days))-1]\n            df[f\"{aggregation}_{days}d_to_{days}-{to_days}d_ratio\"] = np.around(df[f\"{aggregation}_{days}d\"] /\n                                                                                df[f\"{aggregation}_{days}-{to_days}d\"], 4)\n\n            df.loc[df[f\"{aggregation}_{days}-{to_days}d\"] == 0, f\"{aggregation}_{days}d_to_{days}-{to_days}d_ratio\"] = 0\n\n            for d in range(days + 7, 49, 7):\n                df[f\"{aggregation}_{days}-{to_days}d_to_{d}-{d + 7}d_ratio\"] = np.around(df[f\"{aggregation}_{days}-{to_days}d\"] /\n                                                                                         df[f\"{aggregation}_{d}-{d + 7}d\"], 4)\n    \n                df.loc[df[f\"{aggregation}_{d}-{d + 7}d\"] == 0, f\"{aggregation}_{days}-{to_days}d_to_{d}-{d + 7}d_ratio\"] = 0\n\n        if (idx + 1) % 20 == 0:\n            print(f\"{idx + 1} ratio features calculated in {get_script_time(int(time.time() - t_start))}\")\n\n    # Таргет\n    df[\"target\"] = preprocessing.LabelEncoder().fit_transform(df[\"best_model\"])\n    df.to_csv(\"data.csv\", index=False)\n\n\nprint(df.shape, \"\\n\")\nprint(f\"Done in {get_script_time(int(time.time() - t_start))}\", \"\\n\")\ndf.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Из сильно скоррелированных между собой факторов оставляем один"},{"metadata":{"trusted":true},"cell_type":"code","source":"try:\n    features_to_use = pd.read_csv(\"../input/sfs-features-to-use/features_to_use.csv\")[\"feature\"].tolist()\n\nexcept FileNotFoundError:\n    t_start = time.time()\n    corr_threshold = .98\n    corr_matrix = df.drop(dont_use_cols, axis=1).corr().abs()\n    corr_matrix = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n    drop_columns = [col for col in corr_matrix.columns.tolist() if any(corr_matrix[col] >= corr_threshold)]\n\n    df.drop(drop_columns, axis=1, inplace=True)\n\n    pd.DataFrame(data={\"feature\": drop_columns}).to_csv(\"dropped_features_by_big_correlation.csv\", index=False)\n\n    print(f\"{len(drop_columns)} columns dropped with correlation >= {corr_threshold}\", \"\\n\")\n    print(f\"Done in {get_script_time(int(time.time() - t_start))}\", \"\\n\")\n    print(df.shape, \"\\n\")\n    df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Берем в классификатор лучший набор фичей по метрике взвешенной F-меры, отобранных по методу последовательного включения фичей"},{"metadata":{"trusted":true},"cell_type":"code","source":"try:\n    features_to_use = pd.read_csv(\"../input/sfs-features-to-use/features_to_use.csv\")[\"feature\"].tolist()\n    print(len(features_to_use))\n\nexcept FileNotFoundError:\n    t_start = time.time()\n\n    sfs = SFS(estimator=lightgbm.LGBMClassifier(objective=\"multiclass\", n_jobs=-1),\n              k_features=(20, 600),\n              cv=5,\n              scoring=\"f1_weighted\",\n              n_jobs=-1,\n              verbose=2)\n\n    sfs = sfs.fit(df.drop(dont_use_cols, axis=1), df[\"target\"])\n\n    print(\"\\n\" + \"\".join([\"=\" for _ in range(100)]) + \"\\n\")\n    print(f\"Done in {get_script_time(int(time.time() - t_start))}\")\n\n    pd.DataFrame(data={\"feature\": list(sfs.subsets_[24][\"feature_names\"])}).to_csv(\"features_to_use.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Смотрим на распределение отобранных фичей в разрезе трех лучших моделей временных рядов"},{"metadata":{"trusted":true},"cell_type":"code","source":"features = features_to_use\n\nncols = 4\nnrows = np.ceil(len(features) / ncols).astype(np.int64)\n\nfig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=[8 * ncols, 6 * nrows])\n\nfor idx, feature in enumerate(features):\n    row, col = int(idx / ncols), int(idx % ncols)\n    axes[row, col].set_title(feature)\n\n    legend = []\n    for ind, label in enumerate(np.sort(df[\"best_model\"].unique())):\n        values = df[(df[\"best_model\"] == label) & (df[feature].notnull())][feature].values\n        quantile_1 = np.quantile(values, .025)\n        quantile_2 = np.quantile(values, .975)\n        values = values[(values > max(0, quantile_1)) & (values < quantile_2)]\n        sns.distplot(values, bins=1000, kde=True, hist_kws=dict(alpha=.5), ax=axes[row, col])\n        legend += [label]\n\n    axes[row, col].legend(legend)\n    axes[row, col].plot()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Подбираем лучшие параметры для классификатора"},{"metadata":{"trusted":true},"cell_type":"code","source":"try:\n    score_df = pd.read_csv(\"../input/gridsearch-results/GridSearch_results.csv\")\n    score_df = score_df.sort_values(by=\"score\", ascending=False).reset_index(drop=True)\n\nexcept FileNotFoundError:\n    start = time.time()\n    score_df = pd.DataFrame(data={})\n\n    best_score = 0\n    best_num_leaves = 31\n    best_min_data_in_leaf = 1\n    best_min_split_gain = 0\n    best_colsample_by_tree = 1.0\n    best_reg_lambda = 0.0\n    best_reg_alpha = 0.0\n    best_learning_rate = .1\n    best_n_estimators = 100\n\n    best_params = {}\n    best_params[\"num_leaves\"] = best_num_leaves\n    best_params[\"min_data_in_leaf\"] = best_min_data_in_leaf\n    best_params[\"min_split_gain\"] = best_min_split_gain\n    best_params[\"colsample_by_tree\"] = best_colsample_by_tree\n    best_params[\"reg_lambda\"] = best_reg_lambda\n    best_params[\"reg_alpha\"] = best_reg_alpha\n    best_params[\"learning_rate\"] = best_learning_rate\n    best_params[\"n_estimators\"] = best_n_estimators\n\n    def get_cv_score(estimator, data, shuffle_random_state):\n        X, y = utils.shuffle(data[features_to_use], data[\"target\"], random_state=shuffle_random_state)\n        offset = int(data.shape[0] * .8)\n        X_tr, y_tr = X[:offset], y[:offset]\n        X_te, y_te = X[offset:], y[offset:]\n\n        estimator.fit(X_tr, y_tr)\n        cv_score_train = metrics.f1_score(y_tr, estimator.predict_proba(X_tr).argmax(axis=1), average=\"weighted\")\n        cv_score_test = metrics.f1_score(y_te, estimator.predict_proba(X_te).argmax(axis=1), average=\"weighted\")\n\n        return cv_score_train, cv_score_test\n\n\n    print(\"Start tuning Num leaves & Min data in leaf...\", \"\\n\")\n\n    for num_leaves in [2, 4, 6, 8, 10, 12, 25, 50, 75, 100, 150, 200, 250, 300, 400, 500, 1000]:\n        for min_data_in_leaf in [2, 4, 6, 8, 10, 20, 30, 40, 50, 100, 250, 500, 1000, 1500, 2000]:\n            t_start = time.time()\n            lgb = lightgbm.LGBMClassifier(objective=\"multiclass\",\n                                          n_jobs=-1,\n                                          random_state=42,\n                                          num_leaves=num_leaves,\n                                          min_data_in_leaf=min_data_in_leaf)\n\n            cv_scores_train, cv_scores_test = np.array([]), np.array([])\n            for random_state in [42, 61, 87, 104, 121]:\n                cv_score_train, cv_score_test = get_cv_score(estimator=lgb, data=df, shuffle_random_state=random_state)\n                cv_scores_train = np.append(cv_scores_train, cv_score_train)\n                cv_scores_test = np.append(cv_scores_test, cv_score_test)\n\n            cv_score = np.mean(cv_scores_test) - np.std(cv_scores_test)\n            cv_score -= abs(np.mean(cv_scores_train) - np.mean(cv_scores_test))\n\n            if cv_score > best_score:\n                best_num_leaves = num_leaves\n                best_min_data_in_leaf = min_data_in_leaf\n                best_score = cv_score\n                best_params[\"num_leaves\"] = best_num_leaves\n                best_params[\"min_data_in_leaf\"] = best_min_data_in_leaf\n                best_params[\"best_score\"] = best_score\n\n            dct = {\"num_leaves\": num_leaves,\n                   \"min_data_in_leaf\": min_data_in_leaf,\n                   \"min_split_gain\": best_min_split_gain,\n                   \"colsample_by_tree\": best_colsample_by_tree,\n                   \"reg_lambda\": best_reg_lambda,\n                   \"reg_alpha\": best_reg_alpha,\n                   \"learning_rate\": best_learning_rate,\n                   \"n_estimators\": best_n_estimators,\n                   \"mean_score_train\": cv_scores_train.mean(),\n                   \"mean_score_test\": cv_scores_test.mean(),\n                   \"std_score_train\": cv_scores_train.std(),\n                   \"std_score_test\": cv_scores_test.std(),\n                   \"score\": cv_score}\n\n            score_df = pd.concat([score_df, pd.DataFrame(dct, index=[0])], axis=0, ignore_index=True)\n\n            print(f\"num_leaves = {num_leaves}; min_data_in_leaf = {min_data_in_leaf}\")\n            print(f\"Best score {np.around(best_score, 4)}. Spent {get_script_time(int(time.time() - t_start))}\", \"\\n\")\n\n\n    print(\"\\n\" + \"\".join([\"=\" for i in range(80)]) + \"\\n\")\n    print(\"Start tuning Min Split Gain...\", \"\\n\")\n\n    for min_split_gain in [.01, .02, .03, .05, .1, .2, .5, .7, .8, .9]:\n        t_start = time.time()\n        lgb = lightgbm.LGBMClassifier(objective=\"multiclass\",\n                                      n_jobs=-1,\n                                      random_state=42,\n                                      num_leaves=best_num_leaves,\n                                      min_data_in_leaf=best_min_data_in_leaf,\n                                      min_split_gain=min_split_gain)\n\n        cv_scores_train, cv_scores_test = np.array([]), np.array([])\n        for random_state in [42, 61, 87, 104, 121]:\n            cv_score_train, cv_score_test = get_cv_score(estimator=lgb, data=df, shuffle_random_state=random_state)\n            cv_scores_train = np.append(cv_scores_train, cv_score_train)\n            cv_scores_test = np.append(cv_scores_test, cv_score_test)\n\n        cv_score = np.mean(cv_scores_test) - np.std(cv_scores_test)\n        cv_score -= abs(np.mean(cv_scores_train) - np.mean(cv_scores_test))\n\n        if cv_score > best_score:\n            best_min_split_gain = min_split_gain\n            best_score = cv_score\n            best_params[\"min_split_gain\"] = min_split_gain\n            best_params[\"best_score\"] = best_score\n\n        dct = {\"num_leaves\": best_num_leaves,\n               \"min_data_in_leaf\": best_min_data_in_leaf,\n               \"min_split_gain\": min_split_gain,\n               \"colsample_by_tree\": best_colsample_by_tree,\n               \"reg_lambda\": best_reg_lambda,\n               \"reg_alpha\": best_reg_alpha,\n               \"learning_rate\": best_learning_rate,\n               \"n_estimators\": best_n_estimators,\n               \"mean_score_train\": cv_scores_train.mean(),\n               \"mean_score_test\": cv_scores_test.mean(),\n               \"std_score_train\": cv_scores_train.std(),\n               \"std_score_test\": cv_scores_test.std(),\n               \"score\": cv_score}\n\n        score_df = pd.concat([score_df, pd.DataFrame(dct, index=[0])], axis=0, ignore_index=True)\n\n        print(f\"min_split_gain = {min_split_gain}. Best score {np.around(best_score, 4)}.\")\n        print(f\"Spent {get_script_time(int(time.time() - t_start))}\", \"\\n\")\n\n\n    print(\"\\n\" + \"\".join([\"=\" for _ in range(80)]) + \"\\n\")\n    print(\"Start tuning Colsample by Tree...\", \"\\n\")\n\n    for colsample_by_tree in [.9, .8, .7, .6, .5, .4, .3, .2, .1]:\n        t_start = time.time()\n        lgb = lightgbm.LGBMClassifier(objective=\"multiclass\",\n                                      n_jobs=-1,\n                                      random_state=42,\n                                      num_leaves=best_num_leaves,\n                                      min_data_in_leaf=best_min_data_in_leaf,\n                                      min_split_gain=best_min_split_gain,\n                                      colsample_by_tree=colsample_by_tree)\n\n        cv_scores_train, cv_scores_test = np.array([]), np.array([])\n        for random_state in [42, 61, 87, 104, 121]:\n            cv_score_train, cv_score_test = get_cv_score(estimator=lgb, data=df, shuffle_random_state=random_state)\n            cv_scores_train = np.append(cv_scores_train, cv_score_train)\n            cv_scores_test = np.append(cv_scores_test, cv_score_test)\n\n        cv_score = np.mean(cv_scores_test) - np.std(cv_scores_test)\n        cv_score -= abs(np.mean(cv_scores_train) - np.mean(cv_scores_test))\n\n        if cv_score > best_score:\n            best_colsample_by_tree = colsample_by_tree\n            best_score = cv_score\n            best_params[\"colsample_by_tree\"] = colsample_by_tree\n            best_params[\"best_score\"] = best_score\n\n        dct = {\"num_leaves\": best_num_leaves,\n               \"min_data_in_leaf\": best_min_data_in_leaf,\n               \"min_split_gain\": best_min_split_gain,\n               \"colsample_by_tree\": colsample_by_tree,\n               \"reg_lambda\": best_reg_lambda,\n               \"reg_alpha\": best_reg_alpha,\n               \"learning_rate\": best_learning_rate,\n               \"n_estimators\": best_n_estimators,\n               \"mean_score_train\": cv_scores_train.mean(),\n               \"mean_score_test\": cv_scores_test.mean(),\n               \"std_score_train\": cv_scores_train.std(),\n               \"std_score_test\": cv_scores_test.std(),\n               \"score\": cv_score}\n\n        score_df = pd.concat([score_df, pd.DataFrame(dct, index=[0])], axis=0, ignore_index=True)\n\n        print(f\"colsample_by_tree = {colsample_by_tree}. Best score {np.around(best_score, 4)}.\")\n        print(f\"Spent {get_script_time(int(time.time() - t_start))}\", \"\\n\")\n\n\n    print(\"\\n\" + \"\".join([\"=\" for _ in range(80)]) + \"\\n\")\n    print(\"Start tuning Reg Lambda...\", \"\\n\")\n\n    for reg_lambda in [.01, .02, .03, .05, .1, .2, .3, .5, 1, 5, 10, 25, 50, 100]:\n        t_start = time.time()\n        lgb = lightgbm.LGBMClassifier(objective=\"multiclass\",\n                                      n_jobs=-1,\n                                      random_state=42,\n                                      num_leaves=best_num_leaves,\n                                      min_data_in_leaf=best_min_data_in_leaf,\n                                      min_split_gain=best_min_split_gain,\n                                      colsample_by_tree=best_colsample_by_tree,\n                                      reg_lambda=reg_lambda)\n\n        cv_scores_train, cv_scores_test = np.array([]), np.array([])\n        for random_state in [42, 61, 87, 104, 121]:\n            cv_score_train, cv_score_test = get_cv_score(estimator=lgb, data=df, shuffle_random_state=random_state)\n            cv_scores_train = np.append(cv_scores_train, cv_score_train)\n            cv_scores_test = np.append(cv_scores_test, cv_score_test)\n\n        cv_score = np.mean(cv_scores_test) - np.std(cv_scores_test)\n        cv_score -= abs(np.mean(cv_scores_train) - np.mean(cv_scores_test))\n\n        if cv_score > best_score:\n            best_reg_lambda = reg_lambda\n            best_score = cv_score\n            best_params[\"reg_lambda\"] = reg_lambda\n            best_params[\"best_score\"] = best_score\n\n        dct = {\"num_leaves\": best_num_leaves,\n               \"min_data_in_leaf\": best_min_data_in_leaf,\n               \"min_split_gain\": best_min_split_gain,\n               \"colsample_by_tree\": best_colsample_by_tree,\n               \"reg_lambda\": reg_lambda,\n               \"reg_alpha\": best_reg_alpha,\n               \"learning_rate\": best_learning_rate,\n               \"n_estimators\": best_n_estimators,\n               \"mean_score_train\": cv_scores_train.mean(),\n               \"mean_score_test\": cv_scores_test.mean(),\n               \"std_score_train\": cv_scores_train.std(),\n               \"std_score_test\": cv_scores_test.std(),\n               \"score\": cv_score}\n\n        score_df = pd.concat([score_df, pd.DataFrame(dct, index=[0])], axis=0, ignore_index=True)\n\n        print(f\"reg_lambda = {reg_lambda}. Best score {np.around(best_score, 4)}.\")\n        print(f\"Spent {get_script_time(int(time.time() - t_start))}\", \"\\n\")\n\n\n    print(\"\\n\" + \"\".join([\"=\" for _ in range(80)]) + \"\\n\")\n    print(\"Start tuning Reg Alpha...\", \"\\n\")\n\n    for reg_alpha in [.01, .02, .03, .05, .1, .5, 1, 5, 10, 25, 50, 100]:\n        t_start = time.time()\n        lgb = lightgbm.LGBMClassifier(objective=\"multiclass\",\n                                      n_jobs=-1,\n                                      random_state=42,\n                                      num_leaves=best_num_leaves,\n                                      min_data_in_leaf=best_min_data_in_leaf,\n                                      min_split_gain=best_min_split_gain,\n                                      colsample_by_tree=best_colsample_by_tree,\n                                      reg_lambda=best_reg_lambda,\n                                      reg_alpha=reg_alpha)\n\n        cv_scores_train, cv_scores_test = np.array([]), np.array([])\n        for random_state in [42, 61, 87, 104, 121]:\n            cv_score_train, cv_score_test = get_cv_score(estimator=lgb, data=df, shuffle_random_state=random_state)\n            cv_scores_train = np.append(cv_scores_train, cv_score_train)\n            cv_scores_test = np.append(cv_scores_test, cv_score_test)\n\n        cv_score = np.mean(cv_scores_test) - np.std(cv_scores_test)\n        cv_score -= abs(np.mean(cv_scores_train) - np.mean(cv_scores_test))\n\n        if cv_score > best_score:\n            best_reg_alpha = reg_alpha\n            best_score = cv_score\n            best_params[\"reg_alpha\"] = reg_alpha\n            best_params[\"best_score\"] = best_score\n\n        dct = {\"num_leaves\": best_num_leaves,\n               \"min_data_in_leaf\": best_min_data_in_leaf,\n               \"min_split_gain\": best_min_split_gain,\n               \"colsample_by_tree\": best_colsample_by_tree,\n               \"reg_lambda\": best_reg_lambda,\n               \"reg_alpha\": reg_alpha,\n               \"learning_rate\": best_learning_rate,\n               \"n_estimators\": best_n_estimators,\n               \"mean_score_train\": cv_scores_train.mean(),\n               \"mean_score_test\": cv_scores_test.mean(),\n               \"std_score_train\": cv_scores_train.std(),\n               \"std_score_test\": cv_scores_test.std(),\n               \"score\": cv_score}\n\n        score_df = pd.concat([score_df, pd.DataFrame(dct, index=[0])], axis=0, ignore_index=True)\n\n        print(f\"reg_alpha = {reg_alpha}. Best score {np.around(best_score, 4)}.\")\n        print(f\"Spent {get_script_time(int(time.time() - t_start))}\", \"\\n\")\n\n\n    print(\"\\n\" + \"\".join([\"=\" for _ in range(80)]) + \"\\n\")\n    print(\"Start tuning Learning Rate & N estimators...\", \"\\n\")\n\n    for learning_rate in [.2, .1, .05, .03, .02, .01, .005, .001]:\n        for n_estimators in [50, 100, 200, 300, 400, 500, 750, 1000, 1500, 2000]:\n            t_start = time.time()\n            lgb = lightgbm.LGBMClassifier(objective=\"multiclass\",\n                                          n_jobs=-1,\n                                          random_state=42,\n                                          num_leaves=best_num_leaves,\n                                          min_data_in_leaf=best_min_data_in_leaf,\n                                          min_split_gain=best_min_split_gain,\n                                          colsample_by_tree=best_colsample_by_tree,\n                                          reg_lambda=best_reg_lambda,\n                                          reg_alpha=best_reg_alpha,\n                                          learning_rate=learning_rate,\n                                          n_estimators=n_estimators)\n\n            cv_scores_train, cv_scores_test = np.array([]), np.array([])\n            for random_state in [42, 61, 87, 104, 121]:\n                cv_score_train, cv_score_test = get_cv_score(estimator=lgb, data=df, shuffle_random_state=random_state)\n                cv_scores_train = np.append(cv_scores_train, cv_score_train)\n                cv_scores_test = np.append(cv_scores_test, cv_score_test)\n\n            cv_score = np.mean(cv_scores_test) - np.std(cv_scores_test)\n            cv_score -= abs(np.mean(cv_scores_train) - np.mean(cv_scores_test))\n\n            if cv_score > best_score:\n                best_learning_rate = learning_rate\n                best_n_estimators = n_estimators\n                best_score = cv_score\n                best_params[\"learning_rate\"] = best_learning_rate\n                best_params[\"n_estimators\"] = n_estimators\n                best_params[\"best_score\"] = best_score\n\n            dct = {\"num_leaves\": best_num_leaves,\n                   \"min_data_in_leaf\": best_min_data_in_leaf,\n                   \"min_split_gain\": best_min_split_gain,\n                   \"colsample_by_tree\": best_colsample_by_tree,\n                   \"reg_lambda\": best_reg_lambda,\n                   \"reg_alpha\": best_reg_alpha,\n                   \"learning_rate\": learning_rate,\n                   \"n_estimators\": n_estimators,\n                   \"mean_score_train\": cv_scores_train.mean(),\n                   \"mean_score_test\": cv_scores_test.mean(),\n                   \"std_score_train\": cv_scores_train.std(),\n                   \"std_score_test\": cv_scores_test.std(),\n                   \"score\": cv_score}\n\n            score_df = pd.concat([score_df, pd.DataFrame(dct, index=[0])], axis=0, ignore_index=True)\n\n            print(f\"learning_rate = {learning_rate}; n_estimators = {n_estimators}\")\n            print(f\"Best score {np.around(best_score, 4)}. Spent {get_script_time(int(time.time() - t_start))}\", \"\\n\")\n\n    score_df.to_csv(\"GridSearch_results.csv\", index=False)\n    print(\"\\n\" + \"\".join([\"=\" for _ in range(80)]) + \"\\n\")\n    print(f\"GridSearch done in {get_script_time(int(time.time() - start))}\", \"\\n\")\n\nbest_params = score_df[score_df.columns.tolist()[:-5]].to_dict(\"records\")[0]\nprint(\"LGBMCLassifier best parameters:\")\nprint(best_params)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Для каждой статьи берем модель временного ряда с большей вероятностью по классификатору и считаем финальный SMAPE скор"},{"metadata":{"trusted":true},"cell_type":"code","source":"try:\n    df.drop([\"model_choice\", \"smape\"], axis=1, inplace=True)\nexcept KeyError:\n    pass\n\nlgb = lightgbm.LGBMClassifier(**best_params, random_state=42, objective=\"multiclass\", n_jobs=-1)\n\nlgb.fit(df[features_to_use], df[\"target\"])\n\nprobas = lgb.predict_proba(df[features_to_use])\n\nprint(metrics.classification_report(df[\"target\"], probas.argmax(axis=1),\n                                    target_names=[\"holiday_log\", \"median21_h\", \"yearly_log\"]))\n\ndct = df[[\"target\", \"best_model\"]].drop_duplicates().to_dict(\"records\")\n\ndf[\"model_choice\"] = pd.Series(probas.argmax(axis=1)).map({d[\"target\"]: d[\"best_model\"] for d in dct})\n\ndf[\"smape\"] = [df[df[\"model_choice\"].iloc[i]].iloc[i] for i in range(df.shape[0])]\n\n\nprint(\"\\n\" + \"\".join([\"=\" for _ in range(100)]) + \"\\n\")\nprint(f\"Best median model SMAPE score: %.5f\" % np.mean(df[\"median21_h\"]))\nprint(f\"Mixed model SMAPE score: %.5f\" % np.mean(df[\"smape\"]))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}