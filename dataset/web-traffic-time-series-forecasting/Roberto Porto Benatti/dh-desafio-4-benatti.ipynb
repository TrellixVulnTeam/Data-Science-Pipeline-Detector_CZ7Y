{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Web Traffic Time Series Forecasting\n\n**Forecast future traffic to Wikipedia pages**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Collecting DATA"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"base_url = '/kaggle/input/web-traffic-time-series-forecasting/'\n\nkey_1 = pd.read_csv(base_url+'key_1.csv')\ntrain_1 = pd.read_csv(base_url+'train_1.csv')\nsample_submission_1 = pd.read_csv(base_url+'sample_submission_1.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_1.shape, key_1.shape, sample_submission_1.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Understanding the DATA"},{"metadata":{},"cell_type":"markdown","source":"**train_1.csv**\n\n- Contains 145.063 rows representing different Wikipedia URL pages\n- Contains 551 columns, first column is the URL page and then each column represents a value in time from 2015-07-01 to 2016-12-31 (1.5 year, total of 550 days), where the value is the number of visits to the page in that day\n\nJul/2015 - 31 days  \nAug/2015 - 31 days  \nSep/2015 - 30 days  \nOct/2015 - 31 days  \nNov/2015 - 30 days  \nDec/2015 - 31 days  \n\nTotal: 184 days\n\n2016 - 366 days (leap year)\n\nTotal: 184 + 366 = 550 days"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_1.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**key_1.csv**\n\n- Contains 8.703.780 rows, each one representing the \"URL page\"_\"datetime\", where datetime varies from 2017-01-01 to 2017-03-01 (total of 60 days), which is the result of the total number of pages multiplied by 60 days (145063 x 60 = 8.703.780)\n- Contains 2 columns, first one is the \"URL page\"_\"datetime\", second one is the ID for that page"},{"metadata":{"trusted":true},"cell_type":"code","source":"key_1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(key_1.Page[0])\nprint\nprint(key_1.Page[59])\nprint\nprint(key_1.Page[60])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**sample_submission_1.csv**\n\n- Contains 8.703.780 rows, each one having the ID for the page and respective number of visits to the page at that datetime"},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission_1.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In summary:\n\nWe need to predict the number of visits for the period between 2017-01-01 to 2017-03-1 (60 days) from training data (train_1) containing the visits to the 145063 pages in previous period given between 2015-07-01 to 2016-12-31 (550 days)."},{"metadata":{},"cell_type":"markdown","source":"## Exploratory Data Analisys (EDA)"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_1.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a list of wikipedia main sites \nsites = [\"wikipedia.org\", \"commons.wikimedia.org\", \"www.mediawiki.org\"]\n\n# Function to create a new column having the site part of the article page\ndef filter_by_site(page):\n    for site in sites:\n        if site in page:\n            return site\n\n# Creating a new column having the site part of the article page\ntrain_1['Site'] = train_1.Page.apply(filter_by_site)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_1['Site'].value_counts(dropna=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns; sns.set()\n\nplt.figure(figsize=(12, 6))\nplt.title(\"Number of Wikipedia Articles by Sites\", fontsize=\"18\")\ntrain_1['Site'].value_counts().plot.bar(rot=0);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking which country codes exist in the article pages\ntrain_1.Page.str.split(pat=\".wikipedia.org\", expand=True).iloc[:,0].str[-3:].value_counts().index.to_list()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a list of country codes\ntrain_1.Page.str.split(pat=\".wikipedia.org\", expand=True).iloc[:,0].str[-2:].value_counts().index.to_list()[0:7]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking which agents + access exist in the article pages and creating a list with them\ntrain_1.Page.str.split(pat=\".wikipedia.org\", expand=True).iloc[:,1].str[1:].value_counts().index.to_list()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating the list of country codes and agents\ncountries = train_1.Page.str.split(pat=\".wikipedia.org\", expand=True).iloc[:,0].str[-2:].value_counts().index.to_list()[0:7]\nagents = train_1.Page.str.split(pat=\".wikipedia.org\", expand=True).iloc[:,1].str[1:].value_counts().index.to_list()\n\n# Function to create a new column having the country code part of the article page\ndef filter_by_country(page):\n    for country in countries:\n        if \"_\"+country+\".\" in page:\n            return country\n\n# Creating a new column having the country code part of the article page\ntrain_1['Country'] = train_1.Page.apply(filter_by_country)\n\n# Function to create a new column having the agent + access part of the article page\ndef filter_by_agent(page):\n    for agent in agents:\n        if agent in page:\n            return agent\n\n# Creating a new column having the agent part of the article page\ntrain_1['Agent'] = train_1.Page.apply(filter_by_agent)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Understanding what are the NaN values for the Country column\n# It seems that the URL page does not contain the country code for those cases\n\ntrain_1.Page[train_1['Country'].isna() == True]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12, 6))\nplt.title(\"Number of Wikipedia Articles by Country\", fontsize=\"18\")\ntrain_1['Country'].value_counts(dropna=False).plot.bar(rot=0);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_1['Agent'].value_counts(dropna=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12, 6))\nplt.title(\"Number of Wikipedia Articles by Agents/Access\", fontsize=\"18\")\ntrain_1['Agent'].value_counts().plot.bar(rot=0);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a sample dataset from the Train dataset for analysis\ntrain_1_sample = train_1.drop(['Site','Country','Agent'], axis=1).sample(6, random_state=42)\ntrain_1_sample","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Transposing the sample dataset to have Date Time at the index\ntrain_1_sampleT = train_1_sample.drop('Page', axis=1).T\ntrain_1_sampleT.columns = train_1_sample.Page.values\ntrain_1_sampleT.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_1_sampleT.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting the Series from the sample dataset \nplt.figure(figsize=(16,8))\n\nfor k, v in enumerate(train_1_sampleT.columns):\n    plt.subplot(2, 3, k + 1)\n    plt.title( str(v.split(\".org\")[0])+\".org\"+\"\\n\"+str(v.split(\".org\")[1]) )\n    train_1_sampleT[v].plot()\n\nplt.tight_layout();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting the Series from the sample dataset at the same graph\nplt.figure(figsize=(15,8))\n\nfor v in train_1_sampleT.columns:\n    plt.plot(train_1_sampleT[v])\n    plt.legend(loc='upper center');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting the histograms for the Series from the sample dataset\nplt.figure(figsize=(16,8))\n\nfor k, v in enumerate(train_1_sampleT.columns):\n    plt.subplot(2, 3, k + 1)\n    plt.title( str(v.split(\".org\")[0])+\".org\"+\"\\n\"+str(v.split(\".org\")[1]) )\n    sns.distplot(train_1_sampleT[v])\n\nplt.tight_layout();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking that the number of visits to the Wikipedia Articles have Gaussian Distribution (p-value=0)\nfrom scipy.stats import kstest, ks_2samp\n\npages = list(train_1_sampleT.columns)\n\nprint(\"Kolgomorov-Smirnov - Normality Test\")\nprint()\n\nfor p in pages:\n    print(p,':', kstest(train_1_sampleT[p], 'norm', alternative = 'less'))    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Exploring Groups of Time Series for Different Sites     "},{"metadata":{"trusted":true},"cell_type":"code","source":"# List of the main Wikipedia Article sites\nsites","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating sample datasets from the train dataset and filtering them by sites\ntrain_1_sample_site0 = train_1[train_1['Site'] == sites[0]].drop(['Site','Country','Agent'], axis=1).sample(6, random_state=42)\ntrain_1_sample_site1 = train_1[train_1['Site'] == sites[1]].drop(['Site','Country','Agent'], axis=1).sample(6, random_state=42)\ntrain_1_sample_site2 = train_1[train_1['Site'] == sites[2]].drop(['Site','Country','Agent'], axis=1).sample(6, random_state=42)\n\n# Transposing them to have the Date Time as index\ntrain_1_sampleT_site0 = train_1_sample_site0.drop('Page', axis=1).T\ntrain_1_sampleT_site0.columns = train_1_sample_site0.Page.values\ntrain_1_sampleT_site1 = train_1_sample_site1.drop('Page', axis=1).T\ntrain_1_sampleT_site1.columns = train_1_sample_site1.Page.values\ntrain_1_sampleT_site2 = train_1_sample_site2.drop('Page', axis=1).T\ntrain_1_sampleT_site2.columns = train_1_sample_site2.Page.values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Time Series of \"WIKIPEDIA.ORG\" sites only**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting the Series from the sample datasets\nplt.figure(figsize=(16,8))\n\nfor k, v in enumerate(train_1_sampleT_site0.columns):\n    plt.subplot(2, 3, k + 1)\n    plt.title( str(v.split(\".org\")[0])+\".org\"+\"\\n\"+str(v.split(\".org\")[1]) )\n    train_1_sampleT_site0[v].plot()\n\nplt.tight_layout();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting the Series from the sample datasets at the same graph\nplt.figure(figsize=(15,8))\n\nfor v in train_1_sampleT_site0.columns:\n    plt.plot(train_1_sampleT_site0[v])\n    plt.legend(loc='upper center');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Time Series of \"COMMONS.WIKIMEDIA.ORG\" sites only**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting the Series from the sample datasets\nplt.figure(figsize=(16,8))\n\nfor k, v in enumerate(train_1_sampleT_site1.columns):\n    plt.subplot(2, 3, k + 1)\n    plt.title( str(v.split(\".org\")[0])+\".org\"+\"\\n\"+str(v.split(\".org\")[1]) )\n    train_1_sampleT_site1[v].plot()\n\nplt.tight_layout();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting the Series from the sample datasets at the same graph\nplt.figure(figsize=(15,8))\n\nfor v in train_1_sampleT_site1.columns:\n    plt.plot(train_1_sampleT_site1[v])\n    plt.legend(loc='upper center');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Time Series of \"WWW.MEDIAWIKI.ORG\" sites only**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting the Series from the sample datasets\nplt.figure(figsize=(16,8))\n\nfor k, v in enumerate(train_1_sampleT_site2.columns):\n    plt.subplot(2, 3, k + 1)\n    plt.title( str(v.split(\".org\")[0])+\".org\"+\"\\n\"+str(v.split(\".org\")[1]) )\n    train_1_sampleT_site2[v].plot()\n\nplt.tight_layout();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting the Series from the sample datasets at the same graph\nplt.figure(figsize=(15,8))\n\nfor v in train_1_sampleT_site2.columns:\n    plt.plot(train_1_sampleT_site2[v])\n    plt.legend(loc='upper center');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_1_sampleT_site2.columns[4]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Notes:\n\nFor all the sites samples, some series presented missing data (NaNs).\n\nFor one of the WWW.MEDIAWIKI.ORG Series sample, noticed there was no data at all.  \nFor this series, the URL contains the IP address instead of DNS name and it starts with \"User:\""},{"metadata":{},"cell_type":"markdown","source":"### Exploring a Group of Time Series for a Specific Country - DE"},{"metadata":{"trusted":true},"cell_type":"code","source":"# List of the Wikipedia Article country codes\ncountries","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a sample dataset from the train dataset for countries having \"de\" code\ntrain_1_sample_de = train_1[train_1['Country'] == countries[2]].drop(['Site','Country','Agent'], axis=1).sample(6, random_state=42)\n\n# Transposing the sample dataset to have Date Time at the index\ntrain_1_sampleT_de = train_1_sample_de.drop('Page', axis=1).T\ntrain_1_sampleT_de.columns = train_1_sample_de.Page.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting the Series from the sample dataset\nplt.figure(figsize=(16,8))\n\nfor k, v in enumerate(train_1_sampleT_de.columns):\n    plt.subplot(2, 3, k + 1)\n    plt.title( str(v.split(\".org\")[0])+\".org\"+\"\\n\"+str(v.split(\".org\")[1]) )\n    train_1_sampleT_de[v].plot()\n\nplt.tight_layout();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting the Series from the sample datasets at the same graph\nplt.figure(figsize=(15,8))\n\nfor v in train_1_sampleT_de.columns:\n    plt.plot(train_1_sampleT_de[v])\n    plt.legend(loc='upper center');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modeling with Facebook Prophet"},{"metadata":{},"cell_type":"markdown","source":"Facebook Prophet function is used do define a Prophet forecasting model in Python.  \n\nI will now use Prophet to model a specific Time Series got from samples of the training dataset. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import Prophet library\nfrom fbprophet import Prophet","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Picked up one Time Series for the prophet modeling\ntrain_1_sampleT.columns[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a dataframe for the Time Series from the train_1 samples dataset\nds = pd.Series(train_1_sampleT.index)\ny = pd.Series(train_1_sampleT.iloc[:,1].values)\nframe = { 'ds': ds, 'y': y }\ndf = pd.DataFrame(frame)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.plot();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Instantiate and fit the Prophet model with no hyperparameters at all\nm = Prophet()\nm.fit(df);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make dataframe for the future predictions to the next 60 days\n# By default it will also include the dates from the history\n# In summary it will have 550 + 60 days (610)\nfuture = m.make_future_dataframe(periods=60)\nfuture.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predicting the values from the future dataframe\nforecast = m.predict(future)\nforecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"forecast.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The forecast object here is a new dataframe that includes a column yhat with the forecast, \n# as well as columns for components and uncertainty intervals\nforecast.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting the forecast by calling the Prophet.plot method and passing in the forecast dataframe\nfig1 = m.plot(forecast)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting the forecast components by calling the Prophet.plot_components method\n# By default it includes the trend and seasonality of the time series\nfig2 = m.plot_components(forecast)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting both the Actual values and Predict values at the same graph for comparison\nplt.figure(figsize=(15, 7))\nplt.plot(df.y)                  # Actual values in default blue color\nplt.plot(forecast.yhat, \"g\");   # Predicted values in green color","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Conclusion**: in this case it was possible to capture only the trend"},{"metadata":{},"cell_type":"markdown","source":"### Prophet - Saturating forecasts"},{"metadata":{},"cell_type":"markdown","source":"As per the above results, the time Series prediction shows a trend to the bottom, reaching negative values, which is not accepted in this case. There should be no negative visits to a Wikipedia Article...\n\nFor this reason, I tried to use the prophet logistic growth model handling a Saturating Minimum, setting the floor value to zero. However, in order to use a logistic growth trend with a saturating minimum, a maximum capacity must also be specified."},{"metadata":{"trusted":true},"cell_type":"code","source":"forecast['yhat'].tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Setting the floor value to 0 and the capacity to a lower value in the future\ndf['cap'] = 500\ndf['floor'] = 0.0\nfuture['cap'] = 500\nfuture['floor'] = 0.0\n\n# Instantiating prophet 'logistic' growth mode, then fitting and predicting future values\nm = Prophet(growth='logistic')\nforecast = m.fit(df).predict(future)\n\n# Plotting both the forecast predictions and components\nfig1 = m.plot(forecast)\nfig2 = m.plot_components(forecast)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Conclusion:** in this case the prediction trend reached the capacity value defined (500). I will need to explore other prophet parameters to get better results. "},{"metadata":{},"cell_type":"markdown","source":"### Prophet - Seasonality"},{"metadata":{},"cell_type":"markdown","source":"I will include the default seasonality parameters to the Prophet model now."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Instantiate prophet with default seasonality parameters, fitting and predicting the future\n# Plotting both the forecast and its components\n# I will keep the default growth='linear' by now instead of 'logistic'\nm = Prophet(daily_seasonality=True, weekly_seasonality=True, yearly_seasonality=True)\nforecast = m.fit(df).predict(future)\nfig1 = m.plot(forecast)\nfig2 = m.plot_components(forecast)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting both the Actual values and Predict values at the same graph for comparison\nplt.figure(figsize=(15, 7))\nplt.plot(df.y)                  # Actual values in default blue color\nplt.plot(forecast.yhat, \"g\");   # Predicted values in green color","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Conclusion:** In this case, the fit was much better, which was expected since the seasonality capture the most relevant frequencies. Seasonalities are estimated using a partial Fourier sum. However, we could not capture the high picks."},{"metadata":{},"cell_type":"markdown","source":"### Prophet - Changepoints"},{"metadata":{},"cell_type":"markdown","source":"Now I will explore the use of Prophet changepoints to automatically detect these abrupt changes in the time series trajectories and see if it will allow the trend to adapt appropriately. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the locations of the significant changepoints\nfrom fbprophet.plot import add_changepoints_to_plot\nfig = m.plot(forecast)\na = add_changepoints_to_plot(fig.gca(), m, forecast)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By default changepoints are only inferred for the first 80% of the time series in order to have plenty of runway for projecting the trend forward and to avoid overfitting fluctuations at the end of the time series.\n\nSince I still see some changepoints after 80%, I will increase it to check for other ones."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Increasing the 'changepoint_range' parameter from default 80% to 90%\nm = Prophet(daily_seasonality=True, weekly_seasonality=True, yearly_seasonality=True,\n            changepoint_range=0.9)\nforecast = m.fit(df).predict(future)\nfig = m.plot(forecast)\na = add_changepoints_to_plot(fig.gca(), m, forecast)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"deltas = m.params['delta'].mean(0)\nfig = plt.figure(facecolor='w', figsize=(10, 6))\nax = fig.add_subplot(111)\nax.bar(range(len(deltas)), deltas, facecolor='#0072B2', edgecolor='#0072B2')\nax.grid(True, which='major', c='gray', ls='-', lw=1, alpha=0.2)\nax.set_ylabel('Rate change')\nax.set_xlabel('Potential changepoint')\nfig.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Conclusion:** The trend is going down faster when increasing the changepoint_range, making the prediction values more negative, which doesn't make sense. So I will keep changepoint_range to default 80%."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Changing the changepoint_range back to 80% since I don't want to make the trend more negative\n# Also increasing the changepoint_prior_scale from default 0.05 to 0.7\n# By default, changepoint_prior_scale parameter is set to 0.05, andi ncreasing it will make the trend more flexible\nm = Prophet(daily_seasonality=True, weekly_seasonality=True, yearly_seasonality=True,\n            changepoint_range=0.8, changepoint_prior_scale=0.7)\nforecast = m.fit(df).predict(future)\nfig = m.plot(forecast)\na = add_changepoints_to_plot(fig.gca(), m, forecast)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"deltas = m.params['delta'].mean(0)\nfig = plt.figure(facecolor='w', figsize=(10, 6))\nax = fig.add_subplot(111)\nax.bar(range(len(deltas)), deltas, facecolor='#0072B2', edgecolor='#0072B2')\nax.grid(True, which='major', c='gray', ls='-', lw=1, alpha=0.2)\nax.set_ylabel('Rate change')\nax.set_xlabel('Potential changepoint')\nfig.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting both the Actual values and Predict values at the same graph for comparison\nplt.figure(figsize=(15, 7))\nplt.plot(df.y)                  # Actual values in default blue color\nplt.plot(forecast.yhat, \"g\");   # Predicted values in green color","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Conclusion:** Now we got a pretty good model at this point."},{"metadata":{},"cell_type":"markdown","source":"### Prophet - Holidays"},{"metadata":{},"cell_type":"markdown","source":"Now I will include a dataframe for holidays. Since the wikipedia article time series I am analyzing has the country code \"es\", I will use the Spain holiday. I will also add years from 2015 to 2017 to the dataframe."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_1_sampleT.columns[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"_es.\" in train_1_sampleT.columns[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from datetime import date\nimport holidays\n\n# Select country\nes_holidays = holidays.Spain(years = [2015,2016,2017])\nes_holidays = pd.DataFrame.from_dict(es_holidays, orient='index')\nes_holidays = pd.DataFrame({'holiday': 'Spain', 'ds': es_holidays.index})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"es_holidays.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Instantiate prophet with seasonality, changepoints and holidays parameters\nm = Prophet(daily_seasonality=True, weekly_seasonality=True, yearly_seasonality=True,\n            changepoint_range=0.8, changepoint_prior_scale=0.7,\n            holidays=es_holidays)\nm.add_country_holidays(country_name='ES')\n# Fitting and predicting the future\nforecast = m.fit(df).predict(future)\n# Plotting both the forecast and its components\nfig1 = m.plot(forecast)\nfig2 = m.plot_components(forecast)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Prophet - Uncertainty interval"},{"metadata":{},"cell_type":"markdown","source":"#### Uncertainty in the trend"},{"metadata":{},"cell_type":"markdown","source":"The width of the uncertainty intervals (by default 80%) can be set using the parameter interval_width.  \nI will increase it to 95%."},{"metadata":{},"cell_type":"markdown","source":"#### Uncertainty in seasonality"},{"metadata":{},"cell_type":"markdown","source":"This parameter determines if the model uses Maximum a posteriori (MAP) estimation or a full Bayesian inference with the specified number of Markov Chain Monte Carlo (MCMC) samples to train and predict.\nSo if you make MCMC zero then it will do MAP estimation, otherwise you need to specify the number of samples to use with MCMC.\n\nSource: <a href=\"https://towardsdatascience.com/implementing-facebook-prophet-efficiently-c241305405a3\">Implementing Facebook Prophet efficiently</a>\n\nSince we are using the SMAPE as the evaluation metric, I decided to keep mcmc_samples parameters to the default zero value."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Instantiate prophet with seasonality, changepoints and holidays parameters\nm = Prophet(daily_seasonality=True, weekly_seasonality=True, yearly_seasonality=True,\n            changepoint_range=0.8, changepoint_prior_scale=0.7,\n            holidays=es_holidays,\n            interval_width=0.95,\n            mcmc_samples=0)\nm.add_country_holidays(country_name='ES')\n# Fitting and predicting the future\nforecast = m.fit(df).predict(future)\n# Plotting both the forecast and its components\nfig1 = m.plot(forecast)\nfig2 = m.plot_components(forecast)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"deltas = m.params['delta'].mean(0)\nfig = plt.figure(facecolor='w', figsize=(10, 6))\nax = fig.add_subplot(111)\nax.bar(range(len(deltas)), deltas, facecolor='#0072B2', edgecolor='#0072B2')\nax.grid(True, which='major', c='gray', ls='-', lw=1, alpha=0.2)\nax.set_ylabel('Rate change')\nax.set_xlabel('Potential changepoint')\nfig.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15, 7))\nplt.plot(df.y)\nplt.plot(forecast.yhat, \"g\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## An interactive figure of the forecast created with Plotly"},{"metadata":{"trusted":true},"cell_type":"code","source":"from fbprophet.plot import plot_plotly\nimport plotly.offline as py\npy.init_notebook_mode()\n\nfig = plot_plotly(m, forecast)  # This returns a plotly Figure\npy.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Prophet - All parameters"},{"metadata":{},"cell_type":"markdown","source":"Let us look at a summary of some of the most important Prophet parameters for reference.\n\n**Trend parameters**\n\nParameter and Description\n\n- growth -> linear’ or ‘logistic’ to specify a linear or logistic trend\n- changepoints -> List of dates at which to include potential changepoints (automatic if not specified)\n- n_changepoints -> If changepoints is not supplied, you may provide the number of changepoints to be automatically included\n- changepoint_prior_scale -> Parameter for changing flexibility of automatic changepoint selection\n\n**Seasonality & Holiday Parameters**\n\nParameter and Description\n\n- yearly_seasonality -> Fit yearly seasonality\n- weekly_seasonality -> Fit weekly seasonality\n- daily_seasonality -> Fit daily seasonality\n- holidays -> Feed dataframe containing holiday name and date\n- seasonality_prior_scale -> Parameter for changing strength of seasonality model\n- holidays_prior_scale -> Parameter for changing strength of holiday model\n\nSource: https://www.analyticsvidhya.com/blog/2018/05/generate-accurate-forecasts-facebook-prophet-python-r/"},{"metadata":{"trusted":true},"cell_type":"code","source":"m.params","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Evaluating the Model"},{"metadata":{},"cell_type":"markdown","source":"SMAPE function\n\n$$ SMAPE = \\frac{100\\%}{n} \\sum_{t=1}^{n} \\frac{\\left|F_t - A_t\\right|}{(\\left|A_t\\right|+\\left|F_t\\right|)/2} $$"},{"metadata":{"trusted":true},"cell_type":"code","source":"def smape(y_true, y_pred):\n    denominator = (np.abs(y_true) + np.abs(y_pred))\n    diff = np.abs(y_true - y_pred) / denominator\n    diff[denominator == 0] = 0.0\n    return 200 * np.mean(diff)\n\n# Source: http://shortnotes.herokuapp.com/how-to-implement-smape-function-in-python-149","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Calculating the SMAPE for the time series prediction for the visits at a single URL page"},{"metadata":{"trusted":true},"cell_type":"code","source":"smape_single_page = smape(df.y, forecast.yhat)\nsmape_single_page","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Prophet - Cross Validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"from fbprophet.diagnostics import cross_validation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# horizon: forecast horizon\n# initial: size of the initial training period\n# period: spacing between cutoff dates\n#\n# Here we do cross-validation to assess prediction performance on a horizon of 60 days, \n# starting with 130 days of training data in the first cutoff and then making predictions every 60 days\n# On this 610 days time series, this corresponds to 8 total forecasts\n\ncv_results = cross_validation(m, initial='360 days', period='30 days', horizon='60 days')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"smape_baseline = smape(cv_results.y, cv_results.yhat)\nsmape_baseline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Prophet - Running for Multiple Time Series"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_1_all = train_1.drop(['Page','Site','Country','Agent'], axis=1).T\ntrain_1_all.columns = train_1.Page.values\ntrain_1_all.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_1_all.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Filling up NaN values with 0 visits to avoid breaking the model fit\ntrain_1_all.fillna(0, inplace=True)\n\n# Selecting a few series to run the Prophet model against\nnum_series = 10\ntrain_1_sample = train_1_all.sample(num_series, axis=1, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting the Series from the sample datasets at the same graph\nplt.figure(figsize=(15,8))\n\nfor v in train_1_sample.columns:\n    plt.plot(train_1_sample[v])\n    plt.legend(loc='upper center');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nsmape_partial = 0\n\nfor k, v in enumerate(train_1_sample.columns):\n    ds = pd.Series(train_1_sample.index)\n    y = pd.Series(train_1_sample.iloc[:,k].values)\n    frame = { 'ds': ds, 'y': y }\n    df = pd.DataFrame(frame)\n    m_partial = Prophet(daily_seasonality=True, weekly_seasonality=True, yearly_seasonality=True)\n    forecast = m_partial.fit(df).predict(future)\n    smape_partial += smape(df.y, forecast.yhat)\n\nsmape_average = smape_partial / len(train_1_sample.columns)\nsmape_average","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Multivariate Time Series models"},{"metadata":{},"cell_type":"markdown","source":"I could be using Multivariate Time Series (MTS) instead of the univariate models against all Time Series.  \nFollowing this approach, below are some ideas I could try in the future:\n\n- Vector Auto Regression (VAR)\n  - Johansen’s test for checking the stationarity of any multivariate time series data  \n    (statsmodels.tsa.vector_ar.vecm import coint_johansen)\n  - Fit the model using VAR model from statsmodel library  \n    (from statsmodels.tsa.vector_ar.var_model import VAR)  \n- Random Forest  \n- Recurrent Neural Networs (RNN)  \n\nSources:  \n\n<a href=\"https://link.medium.com/miaEiLC0c1\">A Multivariate Time Series Guide to Forecasting and Modeling (with Python codes)</a>)  \n<a href=\"https://towardsdatascience.com/multivariate-time-series-forecasting-using-random-forest-2372f3ecbad1\">Multivariate Time Series Forecasting Using Random Forest</a>)  \n<a href=\"https://link.medium.com/XFbTA4O0c1\">Interpreting recurrent neural networks on multivariate time series</a>"},{"metadata":{},"cell_type":"markdown","source":"## Multiple Time Series in parallel  \n\nAnother idea could be the use of Python multiprocessing package to forecast multiple Time Series in parallel.  \n\nSource:  \n\n<a href=\"https://medium.com/spikelab/forecasting-multiples-time-series-using-prophet-in-parallel-2515abd1a245\">Forecasting multiple time-series using Prophet in parallel</a>"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Submitting to Kaggle"},{"metadata":{"trusted":true},"cell_type":"code","source":"# train_1_sampleT.columns[1]+\"_\"+\"2017-01-01\"\n# train_1_sampleT.columns[1]+\"_\"+\"2017-01-01\" in list(key_1.Page.values)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}