{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input/web-traffic-time-series-forecasting'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\ntrainfile2 = os.path.join(r'/kaggle/input/web-traffic-time-series-forecasting', 'train_2.csv.zip')\nkeyfile2 = os.path.join(r'/kaggle/input/web-traffic-time-series-forecasting', 'key_2.csv.zip')\nsubmissionfile2 = os.path.join(r'/kaggle/input/web-traffic-time-series-forecasting', 'sample_submission_2.csv.zip')\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\ntrain_1 = pd.read_csv(trainfile2, compression='zip')\nkey_1 = pd.read_csv(keyfile2, compression='zip')\nsubmission_2 = pd.read_csv(submissionfile2, compression='zip')\n\n\nkey_1.shape, train_1.shape\n\n\nkey_1.head()\n\ntrain_1.head()\n\n# key_1.loc[0, 'Page']\n# key_1.loc[1, 'Page']\n# key_1.loc[8703779:, 'Page'].values\n\nkey_1['date'] = key_1.Page.apply(lambda x: x[-10:])\nkey_1['Page'] = key_1.loc[:, 'Page'].apply(lambda x: x[:-11])\n\n\ndef chunks(x, size):\n    for pos in range(0, len(x), size):\n        yield x[pos:pos + size]\n\n\nimport hashlib\nkey_1['Page'] = key_1.Page.apply(lambda x: hashlib.md5(x.encode('utf-8')).hexdigest())\n\n\ntrain_1.shape\n\ntrain_1.head()\n\n\nimport hashlib\ntrain_1['Page'] = train_1.Page.apply(lambda x: hashlib.md5(x.encode('utf-8')).hexdigest())\n\ntrain_1.head()\n\n\n# Replace NA values with 0\ntrain_1.fillna(0, inplace=True)\n\n\ntrain_1.head()\n\n\n# # Problem Statement:\n# \n# Stage 1:\n# \n#     Training Set: 1 July 2015 - 31 Dec 2016 : \n#     \n#     2015-07-01    2016-12-31\n# \n#     Predict: 1 Jan 2017 - 1 Mar 2017 : \n#     \n#     2017-01-01    2017-03-01\n# \n# Stage 2:\n# \n#     Test Set: 1 July 2015 - 1 Sept 2017 : \n#     \n#     2015-07-01    2017-09-01\n# \n#     Predict: 13 Sept 2017 and 13 Nov 2017: \n#     \n#     2017-09-13    2017-11-13\n\n# In[ ]:\n\ntrain_page_1 = train_1.Page\n\n\ntrain_page_1 = pd.DataFrame(train_page_1)\n\ntrain_1.drop('Page', axis=1, inplace=True)\n\ntrain_1.head()\n\n\n# from sklearn.preprocessing import MinMaxScaler\n# sc = MinMaxScaler()\n# #vals = np.array(train_1.iloc[:, :]).reshape(-1, 1)\n# train_n1 = sc.fit_transform(train_1)\n\n# train_n1.shape\n\n# train_n1[:, -61:].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using Dataframe train dataset\ny_train_1 = train_1.iloc[:, -64:]\nX_train_1 = train_1.iloc[:, :-64]\n\n# y_train_1 = train_n1[:, -61:]\n# X_train_1 = train_n1[:, :-61]\n\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.wrappers.scikit_learn import KerasRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\n\n\nfrom sklearn.model_selection import KFold\n# \n# define 10-fold cross validation test harness\n\n# Create the model using the NestedLSTM class - two layers are a good starting point\n# Feel free to play around with the number of nodes & other model parameters\nmodel = Sequential()\nmodel.add(Dense(256, input_dim=X_train_1.shape[1], init='normal', activation='relu'))\n#model.add(Dense(512, init='normal', activation='relu'))\nmodel.add(Dense(256, init='normal', activation='relu'))\nmodel.add(Dense(128, init='normal', activation='relu'))\nmodel.add(Dense(64, init='normal', activation='relu'))\nmodel.add(Dense(32, init='normal', activation='relu'))\nmodel.add(Dense(64, init='normal'))\nmodel.compile(loss='mean_squared_error', optimizer = 'adam', metrics = ['accuracy'])\n\n# It's training time!\nBATCH = 2000\n\nprint('Training time, it is...')\nmodel.fit(X_train_1, y_train_1,\n          batch_size=BATCH,\n          epochs=50,\n          validation_split=0.2,\n          shuffle=True\n         )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using Dataframe test dataset\nX_test_1 = train_1.iloc[:, 64:]\n\n# #y_train_1 = train_n1[:, -61:]\n# X_test_1 = train_n1[:, 61:]\n\ny_test_1 = model.predict(X_test_1)\n\nX_test_1.shape, y_test_1.shape\n\n\ndti = pd.DataFrame(pd.date_range(start='2017-09-11', periods=64, freq='D'), columns=['d'])\n\ntest1 = np.concatenate((X_test_1, y_test_1), axis=1)\n\ntest1.shape\n\ntest1.shape, train_1.shape\n\n# y_pred1 = sc.inverse_transform(test1)\n# y_pred1 = y_pred1.iloc[:, -61:]\n\n# Using Dataframe\ny_pred1 = pd.DataFrame(test1)\ny_pred1 = y_pred1.iloc[:, -64:]\n\n\ny_pred1.columns = dti.d.values\n\ny_pred1.head()\n\n#y_pred1 = y_pred1.iloc[:, :-1]\n\ny_pred1 = y_pred1.clip(lower=0)\n\ny_pred1.head()\n\ntrain_1.head()\n\n#pd.concat([train_1, y_pred1], axis=1)\nypred1_cols = y_pred1.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_train1 = pd.concat([train_1, y_pred1], axis=1)\n\nfinal_train1.columns = np.append(final_train1.columns[:-64].values , np.hstack(pd.DataFrame(final_train1.columns[-64:].values).apply(lambda x: x.dt.strftime('%Y-%m-%d')).values))\n\nfinal_train1.head()\n\ntrain_file_2 = pd.read_csv(trainfile2, compression='zip')\n\nfinal_train1['Page'] = train_file_2.Page\n\nfinal_train1.head()\n\n#final_train1.to_csv('final_train2.csv')\n\n#submission_2 = pd.read_csv('sample_submission_2.csv', index_col=0)\n#final_train1 = pd.read_csv('final_train2.csv', index_col=0)\n\nfinal_train1 = final_train1.melt(id_vars='Page', var_name='date', value_name='val')\n\n#key_1 = pd.read_csv('key_2_new.csv', index_col=0)\n\nkey_1.head()\n\nfinal_train1.shape\n\nfinal_train1.head()\n\n\nimport hashlib\n# final_train1.loc[80000000:88633493, 'Page'] = final_train1.loc[80000000:, 'Page'].apply(lambda x: hashlib.md5(x.encode('utf-8')).hexdigest())\n\ndef chunks(x, size):\n    for pos in range(0, len(x), size):\n        yield x[pos:pos + size]\n\nfinal_train1.tail()\n\nfinal_train1 = final_train1.iloc[-8993906:, :]\n\nfinal_train1.shape\n\nsize = 1000000\nimport hashlib\n#for pos in range(0, len(final_train1), size):\nfinal_train1.loc[:, 'Page'] = final_train1.loc[:, 'Page'].apply(lambda x: hashlib.md5(x.encode('utf-8')).hexdigest())\n\n\nfinal_train1.tail()\n\nkey_1.head()\n\nfinal_train1.head()\n\n\n# i = 1 \n# for chunk in chunks(final_train1, 200000):\n#     chunk.to_csv('final_train1_chunk_' + str(i) + '.csv')\n#     i = i + 1\n\nkey_1.shape\n\nfinal_train_res1 = final_train1\n\nfinal1 = pd.merge(key_1, final_train_res1, on=['Page', 'date'], how='left')\n\nfinal1.drop(['Page', 'date'], axis=1, inplace=True)\n\nfinal1.head()\n\nfinal1.isna().sum()\n\nfinal1.fillna(0, inplace=True)\n\nfinal1.shape, submission_2.shape\n\n\nfinal1.columns = ['Id', 'Visits']\nfinal1.to_csv('submission.csv', index=False)\n\n\nfinal1.tail()\n\n\nsubmission_2.shape, final1.shape\n\nsubmission_2.tail()\n\nfinal1.iloc[8993904:, :]","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}