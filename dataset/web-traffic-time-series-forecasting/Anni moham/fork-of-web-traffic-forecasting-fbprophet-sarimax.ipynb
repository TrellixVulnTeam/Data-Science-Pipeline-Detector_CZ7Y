{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from fbprophet import Prophet","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/web-traffic-time-series-forecasting/train_1.csv.zip\")\ntrain.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_pages = train['Page']\nfirst_page = all_pages[0]\nfirst_page","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_allT = train.set_index('Page').T.reset_index().rename(columns={'index':'Date'})\ntrain_allT.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_allT.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.DataFrame(train_allT, columns = ['Date',first_page]) \ndf = df.rename(columns={'Date':'ds', first_page:'y'})\ndf.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\ndf.plot()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Prophet()\nmodel.fit(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.tail(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"future = model.make_future_dataframe(periods=60)\nfuture.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"future.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"forecast = model.predict(future)\nforecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.plot(forecast)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result = forecast['yhat']\nresult","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result.tail(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_result = result[-60:]\nsub_result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_result.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig1 = model.plot(forecast)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig2 = model.plot_components(forecast)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"key = pd.read_csv(\"/kaggle/input/web-traffic-time-series-forecasting/key_1.csv.zip\")\nkey","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"listOfIds = key.index[key['Page'].str.contains(first_page)].values\nlistOfIds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"listOfIds.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv(\"/kaggle/input/web-traffic-time-series-forecasting/sample_submission_1.csv.zip\")\nsub","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub['Visits'].loc[listOfIds.min():listOfIds.max()] = sub_result.values.round()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def my_function():\n    for page in all_pages:\n        #print(page)\n        df = pd.DataFrame(train_allT, columns = ['Date',page]) \n        df = df.rename(columns={'Date':'ds', page:'y'})\n        m = Prophet()\n        m.fit(df)\n        future = m.make_future_dataframe(periods=60)\n        forecast = m.predict(future)\n        result = forecast['yhat']\n        sub_result = result[-60:]\n        listOfIds = key.index[key['Page'].str.contains(page)].values\n        sub['Visits'].loc[listOfIds.min():listOfIds.max()] = sub_result.values.round()\n        #print(sub)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_function()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Reading data...')\nkey_1 = pd.read_csv('/kaggle/input/web-traffic-time-series-forecasting/key_2.csv.zip')\ntrain_1 = pd.read_csv('/kaggle/input/web-traffic-time-series-forecasting/train_2.csv.zip')\nss_1 = pd.read_csv('/kaggle/input/web-traffic-time-series-forecasting/sample_submission_2.csv.zip')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Preprocessing...')\n# train_1.fillna(0, inplace=True)\n\nprint('Processing...')\nids = key_1.Id.values\npages = key_1.Page.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('key_1...')\nd_pages = {}\nfor id, page in zip(ids, pages):\n    d_pages[id] = page[:-11]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('train_1...')\npages = train_1.Page.values\n# visits = train_1['2016-12-31'].values # Version 1 score: 60.6\n# visits = np.round(np.mean(train_1.drop('Page', axis=1).values, axis=1)) # Version 2 score: 64.8\n# visits = np.round(np.mean(train_1.drop('Page', axis=1).values[:, -14:], axis=1)) # Version 3 score: 52.5\n# visits = np.round(np.mean(train_1.drop('Page', axis=1).values[:, -7:], axis=1)) # Version 4 score: 53.7\n# visits = np.round(np.mean(train_1.drop('Page', axis=1).values[:, -21:], axis=1)) # Version 5, 6 score: 51.3\n# visits = np.round(np.mean(train_1.drop('Page', axis=1).values[:, -28:], axis=1)) # Version 7 score: 51.1\n# visits = np.round(np.median(train_1.drop('Page', axis=1).values[:, -28:], axis=1)) # Version 8 score: 47.1 \n# visits = np.round(np.median(train_1.drop('Page', axis=1).values[:, -35:], axis=1)) # Version 9 score: 46.6\n# visits = np.round(np.median(train_1.drop('Page', axis=1).values[:, -42:], axis=1)) # Version 10 score: 46.3\n# visits = np.round(np.median(train_1.drop('Page', axis=1).values[:, -49:], axis=1)) # Version 11 score: 46.2\n# visits = np.nan_to_num(np.round(np.nanmedian(train_1.drop('Page', axis=1).values[:, -49:], axis=1))) # Version 12 score: 45.7\nvisits = np.nan_to_num(np.round(np.nanmedian(train_1.drop('Page', axis=1).values[:, -56:], axis=1))) # scorer 41.8 #find medianen de sidste 56 dage og skift nan ud med 0\n\nd_visits = {}\nfor page, visits_number in zip(pages, visits):\n    d_visits[page] = visits_number\n    # for hver page i pages og visit i visits gem antal visits på page\n\nprint('Modifying sample submission...') # læs submissionfilen ind\nss_ids = ss_1.Id.values\nss_visits = ss_1.Visits.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d_visits","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ss_ids","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i, ss_id in enumerate(ss_ids):\n    ss_visits[i] = d_visits[d_pages[ss_id]] #sæt første ss_id i d_pages-listen for at finde page-navn. sæt så page-navn i d_visits for at finde tal-værdien, \n    #som gemmes i stedet for ss_tal-værdien inkrementalt.\n\nprint('Saving submission...')\nsubm = pd.DataFrame({'Id': ss_ids, 'Visits': ss_visits})\nsubm.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Pre-processing and feature engineering train data...')\ntrain_flattened = pd.melt(train[list(train.columns[-49:])+['Page']], id_vars='Page', var_name='date', value_name='Visits')\ntrain_flattened['date'] = train_flattened['date'].astype('datetime64[ns]')\ntrain_flattened['weekend'] = ((train_flattened.date.dt.dayofweek) // 5 == 1).astype(float)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualisation\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.figure(figsize=(50, 8))\nmean_group = train_flattened[['Page','date','Visits']].groupby(['date'])['Visits'].mean()\nplt.plot(mean_group)\nplt.title('Time Series - Average')\nplt.show()\nplt.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"times_series_means =  pd.DataFrame(mean_group).reset_index(drop=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_date_index = times_series_means[['date','Visits']].set_index('date')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tsa.stattools import adfuller\n# Run Dicky-Fuller test\nresult = adfuller(df_date_index)\n\n# Print test statistic\nprint(result[0])\n\n# Print p-value\nprint(result[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n# Create figure\nfig, (ax1, ax2) = plt.subplots(2,1, figsize=(12,8))\n \n# Plot the ACF of savings on ax1\nplot_acf(df_date_index, zero=False, ax=ax1, lags=10)\n\n# Plot the PACF of savings on ax2\nplot_pacf(df_date_index, zero=False, ax=ax2, lags=10)\n\nplt.show()\nplt.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create empty list to store search results\norder_aic_bic=[]\n\n# Loop over p values from 0-2\nfor p in range(3):\n  # Loop over q values from 0-2\n    for q in range(3):\n        try:\n            # create and fit ARMA(p,q) model\n            model = SARIMAX(df_date_index, order=(p,0,q), seasonal_order=(1,2,0,7))\n            results = model.fit()\n           \n\n            # Append order and results tuple\n            order_aic_bic.append((p,q, results.aic, results.bic))\n            print(p,q,results.aic, results.bic)\n            \n        except:\n            print(p, q, None, None)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tsa.statespace.sarimax import SARIMAX\n# Create and fit model\nmodel = SARIMAX(df_date_index, order=(2,0,1), trend='c')\nresults = model.fit()\n\n# Create the 4 diagostics plots\nresults.plot_diagnostics()\nplt.show()\nplt.close()\n\n# Print summary\nprint(results.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import seasonal decompose\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\n# Perform additive decomposition\ndecomp = seasonal_decompose(df_date_index, \n                            freq=7)\n\n# Plot decomposition\ndecomp.plot()\nplt.show()\nplt.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pip install pmdarima","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pmdarima as pm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model1 = pm.auto_arima(df_date_index,\n                      seasonal=True, m=7,\n                      d=0, D=1, \n                 \t      max_p=2, max_q=2,\n                      trace=True,\n                      error_action='ignore',\n                      suppress_warnings=True)\n                       \n# Print model summary\nprint(model1.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import model class\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\n\n# Create model object\nmodel = SARIMAX(df_date_index, \n                order=(2,0,1), \n                seasonal_order=(1,1,1,7), \n                trend='c')\n# Fit model\nresults = model.fit()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot common diagnostics\nresults.plot_diagnostics()\nplt.show()\nplt.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create forecast object\nforecast_object = results.get_forecast(steps=90)\n\n# Extract prediction mean\nmean = forecast_object.predicted_mean\n\n# Extract the confidence intervals\nconf_int = forecast_object.conf_int()\n\n# Extract the forecast dates\ndates = mean.index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_date_index.index = pd.to_datetime(df_date_index.index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print last predicted mean\nprint(mean.iloc[-1])\n\n# Print last confidence interval\nprint(conf_int.iloc[-1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Validating Forecast\npred = results.get_prediction(start=pd.to_datetime('2016-12-01'), dynamic=False)\npred_ci = pred.conf_int()\nax = df_date_index['2016':].plot(label='observed')\npred.predicted_mean.plot(ax=ax, label='One-step ahead Forecast', alpha=.7, figsize=(14, 7))\nax.fill_between(pred_ci.index,\n                pred_ci.iloc[:, 0],\n                pred_ci.iloc[:, 1], color='k', alpha=.2)\nax.set_xlabel('Date')\nax.set_ylabel('Sales')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_forecasted = pred.predicted_mean\ny_truth = df_date_index['2016-10-01':]\nmse = ((y_forecasted - y_truth) ** 2).mean()\nprint('The Mean Squared Error of our forecasts is {}'.format(round(mse, 2)))\n#The MSE is a measure of the quality of an estimator — it is always non-negative, \n#and the smaller the MSE, the closer we are to finding the line of best fit.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_uc = results.get_forecast(steps=100)\npred_ci = pred_uc.conf_int()\nax = df_date_index.plot(label='observed', figsize=(14, 7))\npred_uc.predicted_mean.plot(ax=ax, label='Forecast')\nax.fill_between(pred_ci.index,\n                pred_ci.iloc[:, 0],\n                pred_ci.iloc[:, 1], color='k', alpha=.25)\nax.set_xlabel('Date')\nax.set_ylabel('Sales')\nplt.legend()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}