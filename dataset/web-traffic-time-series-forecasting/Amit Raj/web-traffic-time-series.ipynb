{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Imports**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom datetime import datetime\n\nfrom statsmodels.tsa.holtwinters import Holt\nfrom statsmodels.tsa.holtwinters import ExponentialSmoothing\n\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom sklearn.metrics import mean_squared_error\n\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.stattools import adfuller, kpss\nfrom statsmodels.tsa.stattools import acf, pacf\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Loading data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#(Data is quite large will take time to load)\ndf = pd.read_csv('../input/web-traffic-time-series-forecasting/train_1.csv.zip')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The dataset consists of approximately 145k time series. Each of these time series represent a number of daily views of a different Wikipedia article, starting from July, 1st, 2015 up until December 31st, 2016."},{"metadata":{},"cell_type":"markdown","source":"Checking the Dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From ~145k timeseries of pages filtering out the timeseries of the movie **\"300 *Rise of an Empire*\"**"},{"metadata":{"trusted":true},"cell_type":"code","source":"ts300 = df[df['Page'].str.match('^300') == True]\nts300","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Exporting working timeseries."},{"metadata":{"trusted":true},"cell_type":"code","source":"ts300.to_csv('op.csv',index=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Checking presense of null values and counting the timeseries with null values greater than 0."},{"metadata":{"trusted":true},"cell_type":"code","source":"sum(ts300.isna().sum()> 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dataset contains **No Null values**"},{"metadata":{},"cell_type":"markdown","source":"Checking the source of all selected timeseries."},{"metadata":{"trusted":true},"cell_type":"code","source":"ts300['Page'].apply(lambda x: x.split('.')[-1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Filtering and selecting one copy of all agents."},{"metadata":{"trusted":true},"cell_type":"code","source":"ts300 = ts300.loc[[48635, 69487, 139186, 116257]]\n#ts300['Page'].apply(lambda x: x.split('.')[-1])\nts300","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Lineplot of all the agents "},{"metadata":{"trusted":true},"cell_type":"code","source":"ts300.set_index('Page').T.plot(figsize=(20,10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All agents nearly follows same trend.<br> Selecting timeseries of \"all_access_all_agents\" as it represents traffic from all agents."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_300 = ts300.loc[[69487]].T\ndf_300.rename(columns={69487:'300_movie'}, inplace=True)\ndf_300 = df_300.rename_axis(\"date\")\ndf_300.drop('Page', inplace=True)\ndf_300.index = pd.to_datetime(df_300.index)\ndf_300['300_movie']=df_300['300_movie'].astype('float')\ndf_300","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Plotting Timeseries of 300 Movie (all_access_all_agents)"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_300.plot(figsize=(10,5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploring parts of timeseries"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_300['2015'].plot(figsize=(10,5),title=\"2015-Jul to 2015-Dec\")\ndf_300['2016-07':].plot(figsize=(10,5),title=\"2016-Jul to 2016-Dec\")\ndf_300['2016':].plot(figsize=(10,5),title=\"2016\")\ndf_300['2015':'2016-07'].plot(figsize=(10,5),title=\"2015-07 to 2016-07\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In timeseries of year 2015 and Between 2015-07 to 2016-07 we can see lot of variation.<br>\nSelecting timeseries of 2015-07 to 2016-07 as it includes timeseries of 2015 too."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cut = df_300['2015':'2016-07']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Analysing Quarterly variation in timeseries"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cut['300_movie'].resample('Q').mean().plot(figsize=(10,6), title=\"Quarterly Plot (using mean)\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Average traffic was continusly decreasing in last two quarters of 2015.<br>\nBut in first two quarters of 2016 countinous increase in traffic was observed."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cut['300_movie'].resample('Q').median().plot(figsize=(10,6), title=\"Quarterly Plot (using median)\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Median tell diffrent story than the mean, the last two quarter of 2015 was decreasing but the boost in traffic of first two quarters of 2016 was not very large, infact decrease can be observed in traffic of second quarter of 2016."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cut['300_movie'].resample('Q').plot(figsize=(20,10), title=\"Quarterly Plot\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, in the bigger picture we notice two spikes in Q2 and Q3 which might have influnced mean to show continous increase in first two quarters of 2016."},{"metadata":{},"cell_type":"markdown","source":"# Distribution of traffic in the timeseries"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,10))\nsns.distplot(a=df_cut['300_movie'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Average Traffic lies between 0-500"},{"metadata":{},"cell_type":"markdown","source":"# Distribution of monthly traffic"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cut['month']=df_cut.index.month_name()\nplt.figure(figsize=(10,6))\nsns.boxplot(x='month', y='300_movie', data=df_cut)\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Huge outliers can be observed in the June and July."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,6))\nsns.barplot(x='month', y='300_movie', data=df_cut)\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This plot show better picture of how distributed the traffic is.<br>\nLarge variation can be seen in June."},{"metadata":{},"cell_type":"markdown","source":"# Checking Seasonality in by decomposing timeseries"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Multiplicative Decomposition \nresult_mul = seasonal_decompose(df_cut['300_movie'], model='multiplicative', extrapolate_trend='freq')\n\n# Additive Decomposition\nresult_add = seasonal_decompose(df_cut['300_movie'], model='additive', extrapolate_trend='freq')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams.update({'figure.figsize': (15,10)})\nresult_mul.plot().suptitle('Multiplicative Decompose', fontsize=22)\nplt.subplots_adjust(top=0.90)\nresult_add.plot().suptitle('Additive Decompose', fontsize=22)\nplt.subplots_adjust(top=0.90)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Additive model performed better than the Multiplicative as less 'Resid' can be seen in Additive model."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extract the Components ----\n# Actual Values = Product of (Seasonal * Trend * Resid)\ndf_reconstructed = pd.concat([result_add.seasonal, result_add.trend, result_add.resid, result_add.observed], axis=1)\ndf_reconstructed.columns = ['seas', 'trend', 'resid', 'actual_values']\ndf_reconstructed.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Testing timeseries Stationarity "},{"metadata":{"trusted":true},"cell_type":"code","source":"# ADF Test\nresult = adfuller(df_cut['300_movie'].values, autolag='AIC')\n\nprint(f'ADF Statistic: {result[0]}')\nprint(f'p-value: {result[1]}')\nfor key, value in result[4].items():\n    print('Critial Values:')\n    print(f'   {key}, {value}')\n\n# KPSS Test\nresult = kpss(df_cut['300_movie'].values, regression='c')\nprint('\\nKPSS Statistic: %f' % result[0])\nprint('p-value: %f' % result[1])\nfor key, value in result[3].items():\n    print('Critial Values:')\n    print(f'   {key}, {value}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"p-value of ADF statstic is greater than 0.05, accepting null hypothesis that there is a unit root. "},{"metadata":{},"cell_type":"markdown","source":"# Detrending the Time Series"},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy import signal\ndetrended = signal.detrend(df_cut['300_movie'].values)\nplt.plot(detrended)\nplt.title('300 Movie Traffic detrended by subtracting the least squares fit', fontsize=12)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Deseasonalizing the Time Series"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Subtracting the Trend Component.\n\n# Time Series Decomposition\nresult_mul = seasonal_decompose(df_cut['300_movie'], model='multiplicative', extrapolate_trend='freq')\nresult_add = seasonal_decompose(df_cut['300_movie'], model='additive', extrapolate_trend='freq')\n\n# Deseasonalize\ndeseasonalized_add = df_cut['300_movie'] / result_add.seasonal\ndeseasonalized_mul = df_cut['300_movie'] / result_mul.seasonal\n\n# Plot\nfig, axs = plt.subplots(2)\naxs[0].plot(deseasonalized_mul)\naxs[0].set_title('300_movie traffic Deseasonalized (multiplicative model)', fontsize=16)\naxs[1].plot(deseasonalized_add)\naxs[1].set_title('300_movie traffic Deseasonalized (additive model) ', fontsize=16)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Testing for Seasonality"},{"metadata":{"trusted":true},"cell_type":"code","source":"from pandas.plotting import autocorrelation_plot\n\n# Draw Plot\nplt.rcParams.update({'figure.figsize':(9,5), 'figure.dpi':120})\nautocorrelation_plot(df_cut['300_movie'].tolist())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Draw Plot\nfig, axes = plt.subplots(1,2,figsize=(16,3), dpi= 100)\nplot_acf(df_cut['300_movie'].tolist(), lags=50, ax=axes[0])\nplot_pacf(df_cut['300_movie'].tolist(), lags=50, ax=axes[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Forcasting with Holt Winters"},{"metadata":{"trusted":true},"cell_type":"code","source":"int(df_300.shape[0]*0.90)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"split = int(df_cut.shape[0]*0.90) #gives 495\nTrain = df_cut[:split]\nTest = df_cut[split-1:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_hat_avg = Test.copy()\nfit1 = Holt(np.asarray(Train['300_movie'])).fit()\ny_hat_avg['Holt_Winter'] = fit1.predict(start=split, end=df_cut.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,8))\nplt.plot(Train.index, Train['300_movie'], label='Train')\nplt.plot(Test.index,Test['300_movie'], label='Test')\nplt.plot(y_hat_avg.index,y_hat_avg['Holt_Winter'], label='Holt_Winter')\nplt.legend(loc='best')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Holt Winter with Exponential Smoothing"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cut.index.freq = 'D' # Start of the month\ntrain, test = df_cut.iloc[:split, 0], df_cut.iloc[split:, 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = ExponentialSmoothing(train, trend='add', seasonal='add', seasonal_periods=12, damped=True)\nhw_model = model.fit(optimized=True, use_boxcox=False, remove_bias=False)\npred = hw_model.predict(start=test.index[0], end=test.index[-1])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(train.index, train, label='Train')\nplt.plot(test.index, test, label='Test')\nplt.plot(pred.index, pred, label='Holt-Winters')\nplt.legend(loc='best');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Forcasting with ARIMA"},{"metadata":{"trusted":true},"cell_type":"code","source":"split = int(df_cut.shape[0]*0.90)\ntrain, test = df_cut['300_movie'].iloc[0:split], df_cut['300_movie'].iloc[split:df_cut.shape[0]]\nmovie_300 = [x for x in train]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = list()\nfor t in range(len(test)):\n\tmodel = ARIMA(movie_300, order=(5,1,4))\n\tmodel_fit = model.fit(disp=0)\n\toutput = model_fit.forecast()\n\tyhat = output[0]\n\tpredictions.append(yhat)\n\tobs = test[t]\n\tmovie_300.append(obs)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"error = mean_squared_error(test, predictions)\nprint('Test MSE: %.3f' % error)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(test.values)\nplt.plot(predictions, color='red')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Holt winter on full data set (300 Movie)"},{"metadata":{"trusted":true},"cell_type":"code","source":"split = int(df_300.shape[0]*0.90) #gives 495\nTrain = df_300[:split]\nTest = df_300[split-1:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_hat_avg = Test.copy()\nfit1 = Holt(np.asarray(Train['300_movie'])).fit()\ny_hat_avg['Holt_Winter'] = fit1.predict(start=split, end=df_300.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,8))\nplt.plot(Train.index, Train['300_movie'], label='Train')\nplt.plot(Test.index,Test['300_movie'], label='Test')\nplt.plot(y_hat_avg.index,y_hat_avg['Holt_Winter'], label='Holt_Winter')\nplt.legend(loc='best')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Holt Winters with Exponential Smoothing"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_300.index.freq = 'D' # Daily Data\nsplit = int(df_300.shape[0]*0.90) \ntrain, test = df_300.iloc[:split, 0], df_300.iloc[split:, 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = ExponentialSmoothing(train, trend='add', seasonal='add', seasonal_periods=12, damped=True)\nhw_model = model.fit(optimized=True, use_boxcox=False, remove_bias=False)\npred = hw_model.predict(start=test.index[0], end=test.index[-1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(train.index, train, label='Train')\nplt.plot(test.index, test, label='Test')\nplt.plot(pred.index, pred, label='Holt-Winters')\nplt.legend(loc='best');","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}