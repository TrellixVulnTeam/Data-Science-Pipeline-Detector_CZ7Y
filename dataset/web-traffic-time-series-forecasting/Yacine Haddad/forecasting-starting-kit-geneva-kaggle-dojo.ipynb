{"nbformat_minor":1,"metadata":{"kernelspec":{"display_name":"Python 2","language":"python","name":"python2"},"language_info":{"pygments_lexer":"ipython3","name":"python","file_extension":".py","version":"3.6.4","nbconvert_exporter":"python","codemirror_mode":{"name":"ipython","version":3},"mimetype":"text/x-python"}},"nbformat":4,"cells":[{"source":"# Wikipedia Traffic Forecasting\n## A starting kit\n**V. Croft, Y. Haddad**\n\nThis is a very short kernel aiming to help on getting hands into time series analysis and forecasting. We will be using data from the [Wikipedia Traffic Forecasting challange](https://www.kaggle.com/c/web-traffic-time-series-forecasting) on Kaggle.\n\nThis example is based on these kernels:\n* https://www.kaggle.com/muonneutrino/wikipedia-traffic-data-exploration\n* https://www.kaggle.com/screech/ensemble-of-arima-and-lstm-model-for-wiki-pages\n\n\nYou can find this example on Github ([https://github.com/PythonDSGeneva/KaggleDojoTimeSeriesForcasting](https://github.com/PythonDSGeneva/KaggleDojoTimeSeriesForcasting)).","cell_type":"markdown","metadata":{}},{"source":"# Some basic libraries \nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport re\n%matplotlib inline","cell_type":"code","outputs":[],"metadata":{"collapsed":true},"execution_count":null},{"source":"## Loading data","cell_type":"markdown","metadata":{}},{"source":"df_train = pd.read_csv('./input/train_1.csv').fillna(0)\ndf_train.head()","cell_type":"code","outputs":[],"metadata":{},"execution_count":null},{"source":"df_train.info()","cell_type":"code","outputs":[],"metadata":{},"execution_count":null},{"source":"## Feature Engineering\n\n### Language information\n\nWe might reasonably assume different languages used in Wikipedia might affect the dataset. A simple regular expression can be used to search for the language code in the wikipedia URL. There are also a number of non-wikipedia URLs that will fail the regex search. These are wikimedia pages, so we give them the code 'na' since we can't determine their language. Many of these will be things like images that do not really have a language.","cell_type":"markdown","metadata":{}},{"source":"def get_language(page):\n    res = re.search('[a-z][a-z].wikipedia.org',page)\n    if res:\n        return res.group(0)[0:2]\n    return 'na'","cell_type":"code","outputs":[],"metadata":{"collapsed":true},"execution_count":null},{"source":"df_train['lang'] = df_train.Page.map(get_language)","cell_type":"code","outputs":[],"metadata":{"collapsed":true},"execution_count":null},{"source":"df_train[df_train.Page == 'NoÃ«l_fr.wikipedia.org_all-access_all-agents']","cell_type":"code","outputs":[],"metadata":{},"execution_count":null},{"source":"There are 7 languages plus the media pages. The languages used here are: English, Japanese, German, French, Chinese, Russian, and Spanish. This will make any analysis of the URLs difficult since there are four different writing systems to be dealt with (Latin, Cyrillic, Chinese, and Japanese). \n\nFirst, let's create dataframes for the different types of entries and calculate the sum of all views.","cell_type":"markdown","metadata":{}},{"source":"lang_sets = {}\nlang_sets['en'] = df_train[df_train.lang=='en'].iloc[:,0:-1]\nlang_sets['ja'] = df_train[df_train.lang=='ja'].iloc[:,0:-1]\nlang_sets['de'] = df_train[df_train.lang=='de'].iloc[:,0:-1]\nlang_sets['na'] = df_train[df_train.lang=='na'].iloc[:,0:-1]\nlang_sets['fr'] = df_train[df_train.lang=='fr'].iloc[:,0:-1]\nlang_sets['zh'] = df_train[df_train.lang=='zh'].iloc[:,0:-1]\nlang_sets['ru'] = df_train[df_train.lang=='ru'].iloc[:,0:-1]\nlang_sets['es'] = df_train[df_train.lang=='es'].iloc[:,0:-1]\n\nsums = {}\nfor key in lang_sets:\n    sums[key] = lang_sets[key].iloc[:,1:].sum(axis=0) / lang_sets[key].shape[0]","cell_type":"code","outputs":[],"metadata":{"collapsed":true},"execution_count":null},{"source":"and visualise","cell_type":"markdown","metadata":{}},{"source":"days = [r for r in range(sums['en'].shape[0])]\n\nfig = plt.figure(1,figsize=[18,5])\nplt.ylabel('Views per Page')\nplt.xlabel('Day')\nplt.title('Pages in Different Languages')\nlabels={'en':'English','ja':'Japanese','de':'German',\n        'na':'Media','fr':'French','zh':'Chinese',\n        'ru':'Russian','es':'Spanish'\n       }\n\nfor key in sums:\n    plt.plot(days,sums[key],label = labels[key] )\n    \nplt.legend()\nplt.show()","cell_type":"code","outputs":[],"metadata":{},"execution_count":null},{"source":"### Looking at Periodic Structure with FFTs\n\nAll solutions for time series forecasting are highly dependent on the length of the sequences in question. Some work very well for short sequences, up to 100-300 items but will forget information from older items on longer sequences. Competition time series is up to 700 days long, so it might be necessary to find some method to \"strengthen\" GRU memory.\n\nLooking at the Fast Fourier Transform (FFT) shows us the strongest frequencies in the periodic signal and can give a first estimate as to the length of some of the sequences.","cell_type":"markdown","metadata":{}},{"source":"from scipy.fftpack import fft\ndef plot_with_fft(key):\n    f, ax = plt.subplots(2, figsize=(18,8))\n    ax[0].set_ylabel('Views per Page')\n    ax[0].set_xlabel('Day')\n    ax[0].set_title(labels[key])\n    ax[0].plot(days,sums[key],label = labels[key] )\n\n    fft_complex = fft(sums[key])\n    fft_mag = [np.sqrt(np.real(x)*np.real(x)+np.imag(x)*np.imag(x)) for x in fft_complex]\n    fft_xvals = [day / float(days[-1]) for day in days]\n    npts = len(fft_xvals) // 2 + 1\n    fft_mag = fft_mag[:npts]\n    fft_xvals = fft_xvals[:npts]\n        \n    ax[1].set_ylabel('FFT Magnitude')\n    ax[1].set_xlabel(r\"Frequency [days]$^{-1}$\")\n    ax[1].set_title('Fourier Transform')\n    ax[1].plot(fft_xvals[1:],fft_mag[1:],label = labels[key] )\n    ax[1].axvline(x=1./7,color='red',alpha=0.3)\n    ax[1].axvline(x=2./7,color='red',alpha=0.3)\n    ax[1].axvline(x=3./7,color='red',alpha=0.3)\n\n    plt.show()\n\nfor key in sums:\n    plot_with_fft(key)","cell_type":"code","outputs":[],"metadata":{"scrolled":false},"execution_count":null},{"source":"# Individual Entry Data","cell_type":"markdown","metadata":{}},{"source":"def plot_entry(key,idx, ax):\n    data = lang_sets[key].iloc[idx,1:]\n    ax.plot(days,data, label=df_train.iloc[lang_sets[key].index[idx],0])\n    ax.set_xlabel('day')\n    ax.set_ylabel('views')","cell_type":"code","outputs":[],"metadata":{"collapsed":true},"execution_count":null},{"source":"f, ax = plt.subplots(figsize=(18,4))\nplot_entry(key='en', idx=4, ax=ax )\nplot_entry(key='en', idx=5, ax=ax )\nplot_entry(key='en', idx=6, ax=ax )\nplot_entry(key='en', idx=7, ax=ax )\nplot_entry(key='en', idx=8, ax=ax )\nax.legend()","cell_type":"code","outputs":[],"metadata":{},"execution_count":null},{"source":"Nothing special about those pages as I selected them randomly. The only purpose is that you can explore individual entries, one by one, and see if there is any correlation with a social event, holiday, or a movie release. \n\nAs a small exercise, try to find the Christmas page in French and explain the trends  ;) ","cell_type":"markdown","metadata":{}},{"source":"## Base prediction\nMany methods and tools can be used for time series forecasting. \nexamples: RNN, autoencoders, Kernel methods, autoregressive integrated moving average models.\n\nFor simplicity and to provide baseline model, we will only focus on two methods, the traditional autoregressive integrated moving average model and Recurrent Neural Network RNN. The later became is the trending method for prediction, will try to make a simple Keras model using LSTM cells.\n\n### Buidling an ARIMA forcasting model","cell_type":"markdown","metadata":{}},{"source":"npages = 5\ntop_pages = {}\nfor key in lang_sets:\n    print(key)\n    sum_set = pd.DataFrame(lang_sets[key][['Page']])\n    sum_set['total'] = lang_sets[key].sum(axis=1)\n    sum_set = sum_set.sort_values('total',ascending=False)\n    print(sum_set.head(10))\n    top_pages[key] = sum_set.index[0]\n    print('\\n\\n')","cell_type":"code","outputs":[],"metadata":{},"execution_count":null},{"source":"from statsmodels.tsa.arima_model import ARIMA ","cell_type":"code","outputs":[],"metadata":{"collapsed":true},"execution_count":null},{"source":"import warnings\n\ncols = df_train.columns[1:-1]\nfor key in top_pages:\n    data = np.array(df_train.loc[top_pages[key],cols],'f')\n    result = None\n    with warnings.catch_warnings():\n        warnings.filterwarnings('ignore')\n        try:\n            arima = ARIMA(data,[2,1,4])\n            result = arima.fit(disp=False)\n        except:\n            try:\n                arima = ARIMA(data,[2,1,2])\n                result = arima.fit(disp=False)\n            except:\n                print(df_train.loc[top_pages[key],'Page'])\n                print('\\tARIMA failed')\n    #print(result.params)\n    pred = result.predict(2,599,typ='levels')\n    x = [i for i in range(600)]\n    i=0\n    \n    f, ax = plt.subplots(figsize=(18,4))\n    ax.plot(x[2:len(data)],data[2:] ,label='Data')\n    ax.plot(x[2:],pred,label='ARIMA Model')\n    print str(df_train.loc[top_pages[key],'Page'])\n    ax.set_title(str(df_train.loc[top_pages[key],'Page']).decode('utf-8'))\n    ax.set_xlabel('Days')\n    ax.set_ylabel('Views')\n    ax.legend()\n    plt.show()","cell_type":"code","outputs":[],"metadata":{"scrolled":false},"execution_count":null},{"source":"### Buidling an LSTM model using Keras","cell_type":"markdown","metadata":{}},{"source":"in this part you need Keras installed. Instruction can be found on [keras.io](keras.io).\nIn this section will try to train an LSTM models for top pages of all the languages for demonstration. We will, therefore, drop the `Page` column from the dataframe.","cell_type":"markdown","metadata":{}},{"source":"df_train = df_train.drop('Page',axis = 1)\ndf_train.head()","cell_type":"code","outputs":[],"metadata":{},"execution_count":null},{"source":"#Packages for pre processing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\n\n # Importing the Keras libraries and packages for LSTM\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM","cell_type":"code","outputs":[],"metadata":{"collapsed":true},"execution_count":null},{"source":"def keras_regressor():\n    regressor = Sequential()\n    # Adding the input layerand the LSTM layer\n    regressor.add(LSTM(units = 8, activation = 'relu', input_shape = (None, 1)))\n    # Adding the output layer\n    regressor.add(Dense(units = 1))\n    # Compiling the RNN\n    regressor.compile(optimizer = 'rmsprop', loss = 'mean_squared_error')\n    return regressor\n\nfor key in sums:\n    f, ax = plt.subplots(figsize=(18,4))\n    \n    row = [0]*sums[key].shape[0]\n    for i in range(sums[key].shape[0]):\n        row[i] = sums[key][i]\n\n    #Using Data From Random Row for Training and Testing\n    X = row[0:549]\n    y = row[1:550]\n    \n    # Splitting the dataset into the Training set and Test set\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n    \n    # Feature Scaling\n    sc = MinMaxScaler()\n    X_train = np.reshape(X_train,(-1,1))\n    y_train = np.reshape(y_train,(-1,1))\n    X_train = sc.fit_transform(X_train)\n    y_train = sc.fit_transform(y_train)\n    #Reshaping Array\n    X_train = np.reshape(X_train, (384,1,1))\n\n    # Initialising the RNN\n    regressor = keras_regressor()\n\n    # Fitting the RNN to the Training set\n    regressor.fit(X_train, y_train, batch_size = 10, epochs = 100, verbose = 0)\n\n    # Getting the predicted Web View\n    inputs = X\n    inputs = np.reshape(inputs,(-1,1))\n    inputs = sc.transform(inputs)\n    inputs = np.reshape(inputs, (549,1,1))\n    y_pred = regressor.predict(inputs)\n    y_pred = sc.inverse_transform(y_pred)\n\n    print(key)\n    #Visualising Result\n    ax.plot(y, color = 'red', label = 'Real Web View')\n    ax.plot(y_pred, color = 'blue', label = 'Predicted Web View')\n    ax.set_title ('Web View Forecasting: %s' % key)\n    ax.set_xlabel('Number of Days from Start')\n    ax.set_ylabel('Web View')\n    ax.legend()\n    plt.show()","cell_type":"code","outputs":[],"metadata":{"scrolled":false},"execution_count":null},{"source":"## Make submission\n\nThe models above are very simple ones and need to be optimised. You could, therefore, consider a further features engineering and hyperparameters optimisation for the proposed models. You can also use other frameworks to build your forecasting models. Few examples are `pytorch`, `tensorflow`, `sklearn` ...etc. \n\nThe last step will be to ma e a submission, and for that, you can refer to the documentation on Kaggle platform to make a submission.","cell_type":"markdown","metadata":{"collapsed":true}}]}