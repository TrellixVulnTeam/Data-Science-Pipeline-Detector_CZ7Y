{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport re\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(context='notebook',\n        style='whitegrid',\n        palette='deep',\n        font='sans-serif',\n        font_scale=1,\n        color_codes=True,\n        rc=None)\n\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport tensorflow as tf\nfrom keras.preprocessing.sequence import TimeseriesGenerator","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tsa.seasonal import seasonal_decompose\nimport statsmodels.graphics.tsaplots as sgt\nimport statsmodels.tsa.stattools as sts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import LSTM\n\nimport datetime, os\nfrom keras.preprocessing.sequence import TimeseriesGenerator","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.__version__","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loading Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = pd.read_csv('/kaggle/input/web-traffic-data-set/train_1.csv')\ntrain = train_data\ntrain.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Replacing nan values with forward fill","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntrain = train.fillna(method='ffill', downcast='infer')\ntrain.tail(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Converting data to integer values to reduce memory consumption","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfor cols in train.columns[1:]:\n    train[cols] = pd.to_numeric(train[cols], downcast='integer')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Transposing the dataframe with time stamps in index, and page names in columns","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.DataFrame(train.iloc[:,1:].values.T,\n            columns=train.Page.values, index=train.columns[1:])\ndf.index = pd.to_datetime(df.index, errors='ignore',\n                                            dayfirst=False,\n                                            yearfirst=False,\n                                            utc=None,\n                                            format=\"%Y/%m/%d\",\n                                            exact=False,\n                                            unit=None,\n                                            infer_datetime_format=True,\n                                            origin='unix',\n                                            cache=True)\ndf.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### List of different types of webpages","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"list(df.columns)[:10]  # First 10 pages","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Creating Separate Dataframe for Wikipedia hits","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"wikipedia = (df.filter(like='wikipedia'))\nwikipedia","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Plotting 1st 10 page hits","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"wikipedia.iloc[:,0:10].plot(figsize=(20,10))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Languages of wikipedia pages","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_language(page):\n    res = re.search('[a-z][a-z].wikipedia.org',page)\n    if res:\n        return res[0][0:2]\n    return 'other'\n\n(wikipedia.columns.map(get_language)).unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len((wikipedia.columns.map(get_language)).unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"languages = list((wikipedia.columns.map(get_language)).unique())\nlanguages.remove('other')\nlanguages","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Creating Separate Dataframes for every language of wikipedia pages","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for lang in (languages):\n    locals()['lang_'+str(lang)] = wikipedia.loc[:, wikipedia.columns.str.contains('_'+str(lang)+'.wiki')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lang_en.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for lang in (languages):\n    locals()['hits_'+str(lang)] = np.array(locals()['lang_'+str(lang)].iloc[:,:].sum(axis=1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for lang in (languages):\n    print((locals()['hits_'+str(lang)]).shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"keys = languages\nvalues = ['Chinese', 'French', 'English', 'Russian', 'German', 'Japanese', 'Spanish']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d = dict(zip(keys,values))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"index = wikipedia.index\n\nhits = pd.DataFrame(index=index, columns=list(d.values()))\nhits = hits.fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for key, value in d.items():\n    hits[value] = locals()['hits_'+str(key)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hits","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hits.plot(figsize=(25,8), title ='Hits on Wikipedia pages per Language', fontsize=15)\nplt.legend(loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Auto Correlation\nIt is showing that data points even after lag of 25 is also relevant.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams[\"figure.dpi\"] = 100\nhits.iloc[:,0:1].plot(figsize=(20,4))\nsgt.plot_acf(np.array(hits.iloc[:,0:1]),\n            ax=None,\n            lags=None,\n            alpha=0.05,\n            use_vlines=True,\n            unbiased=False,\n            fft=False,\n            missing='none',\n            title='Autocorrelation',\n            zero=False,  # Not including the 1st term as its acf w.r.t. itself will always be 1.\n            vlines_kwargs=None)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Partial Auto-correlation\nShows that upto 11 lags, the data points are relvant.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams[\"figure.dpi\"] = 100\nhits.iloc[:,0:1].plot(figsize=(20,4))\nsgt.plot_pacf(np.array(hits.iloc[:,0:1]),\n            ax=None,\n            lags=None,\n            alpha=0.05,\n            method='ols',\n            use_vlines=True,\n            title='Partial Autocorrelation',\n            zero=False,    # Not including the 1st term as its pacf w.r.t. itself will always be 1.\n            vlines_kwargs=None)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train-Test Split","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"brk = 0.8\ndata_split = int(len(hits)*brk)\ndata_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X, y = hits.iloc[:data_split,:], hits.iloc[data_split:,:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nscaler.fit(X)\n\nscaled_X = scaler.transform(X)\nscaled_y = scaler.transform(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(scaled_X.max(), scaled_X.min())\nprint(scaled_y.max(), scaled_y.min())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_df = (pd.DataFrame(scaled_X))\ny_df = (pd.DataFrame(scaled_y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20,8), dpi=100)\nplt.suptitle('Train-Test Split', fontsize=20)\nX_df.plot(ax=axes[0], title='Train Data')\ny_df.plot(ax=axes[1], title='Test Data')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(scaled_y[3:13,:]).plot(figsize=(15,5), title='Periodicity')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(scaled_X.shape)\nprint(scaled_y.shape)\nprint('No. of features = '+str(scaled_X.shape[1]))\nprint('No. of train instances = '+str(scaled_X.shape[0]))\nprint('No. of test instances = '+str(scaled_y.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# LSTM \n## (Predicting on sum of web hits of wikipedia pages per language)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Time Series Generator\n#### From ACF and PACF plots, it is optimal to select a time sequence of 7-10 time stamps. The reason of choosing 7 is for weekly cycle.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"length = 7\nbatch = 1\n\nn_features = scaled_X.shape[1]\nn_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"generator = TimeseriesGenerator(data = scaled_X,\n                                targets = scaled_X,\n                                length = length,\n                                sampling_rate=1,\n                                stride=1,\n                                start_index=0,\n                                end_index=None,\n                                shuffle=False,\n                                reverse=False,\n                                batch_size=batch)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import LSTM\n\nimport datetime, os\n\nmodel = Sequential(layers=None, name=\"LSTM_Model\")\n\nmodel.add(LSTM( units = 400,               \n                activation='tanh',\n                input_shape=( length, n_features),                \n                recurrent_activation='sigmoid',\n                use_bias=True,\n                kernel_initializer='glorot_uniform',\n                recurrent_initializer='orthogonal',\n                bias_initializer='zeros',\n                unit_forget_bias=True,\n                kernel_regularizer=None,\n                recurrent_regularizer=None,\n                bias_regularizer=None,\n                activity_regularizer=None,\n                kernel_constraint=None,\n                recurrent_constraint=None,\n                bias_constraint=None,\n                dropout=0.0,\n                recurrent_dropout=0.0,\n                implementation=2,\n                return_sequences=True,\n                return_state=False,\n                go_backwards=False,\n                stateful=False,\n                time_major=False,\n                unroll=False\n            ) )\nmodel.add(LSTM(units = 500, return_sequences=True))\n\nmodel.add(LSTM(units = 500, return_sequences=False))\n\nmodel.add(Dense(700, activation=\"relu\", name=\"layer1\"))\n\nmodel.add(Dense(100, activation=\"relu\", name=\"layer2\"))\n\n\nmodel.add(Dense( units = n_features,               \n                activation='relu',\n                use_bias=True,                        \n                kernel_initializer='glorot_uniform',  \n                bias_initializer='zeros',             \n                kernel_regularizer=None,              \n                bias_regularizer=None,                \n                activity_regularizer=None,            \n                kernel_constraint=None,               \n                bias_constraint=None))                \n\n\n\nmodel.compile(optimizer='adam', loss='mse')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping\n\nearly_stop = EarlyStopping(monitor='val_loss',\n                        min_delta=0,\n                        patience=20,\n                        verbose=1,  \n                        mode='auto',\n                        baseline=None,  \n                                               \n                        restore_best_weights=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"validation_generator = TimeseriesGenerator(scaled_y,scaled_y, length=length, batch_size=batch)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nhistory = model.fit(generator,\n                    steps_per_epoch=None,\n                    epochs=500,\n                    verbose=1,\n                    callbacks=[early_stop],\n                    validation_data = validation_generator,\n                    validation_steps=None,\n                    validation_freq=1,\n                    class_weight=None,\n                    max_queue_size=10,\n                    workers=1,\n                    use_multiprocessing=False,\n                    shuffle=True,\n                    initial_epoch=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(history.history.keys())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"losses = pd.DataFrame(model.history.history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams[\"figure.dpi\"] = 100\nlosses.plot(figsize=(10,5))\nplt.title('Losses')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Predictions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntest_predictions = []\n\nfirst_eval_batch = scaled_X[-length:]\ncurrent_batch = first_eval_batch.reshape((1, length, n_features))\n\nfor i in range(len(scaled_y)):\n    \n    current_pred = model.predict(current_batch,verbose=0)[0]\n    \n    test_predictions.append(current_pred) \n    \n    current_batch = np.append(current_batch[:,1:,:],[[current_pred]],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(np.array(test_predictions).shape)\nprint(scaled_y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(np.array(test_predictions).max(), np.array(test_predictions).min())\nprint(scaled_y.max(), scaled_y.min())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"true_predictions = scaler.inverse_transform(test_predictions)\nprint(true_predictions.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(np.array(true_predictions).max(), np.array(true_predictions).min())\nprint(np.array(y).max(), np.array(y).min())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t_l = len(scaled_y)\nt_l","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(dpi=100)\nplt.plot(np.linspace(0,t_l,t_l), scaled_y[:,0:1] , label='True Values',c='g')\nplt.plot(np.linspace(0,t_l,t_l), np.array(test_predictions)[:,0:1], label='Predicted Values',c='r')\nplt.title(hits.columns[0], fontsize=20)\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(np.linspace(0,t_l,t_l), scaled_y[:,1:2] , label='True Values',c='g')\nplt.plot(np.linspace(0,t_l,t_l), np.array(test_predictions)[:,1:2], label='Predicted Values',c='r')\nplt.title(hits.columns[1], fontsize=20)\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(np.linspace(0,t_l,t_l), scaled_y[:,2:3] , label='True Values',c='g')\nplt.plot(np.linspace(0,t_l,t_l), np.array(test_predictions)[:,2:3], label='Predicted Values',c='r')\nplt.title(hits.columns[2], fontsize=20)\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(np.linspace(0,t_l,t_l), scaled_y[:,3:4] , label='True Values',c='g')\nplt.plot(np.linspace(0,t_l,t_l), np.array(test_predictions)[:,3:4], label='Predicted Values',c='r')\nplt.title(hits.columns[3], fontsize=20)\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(np.linspace(0,t_l,t_l), scaled_y[:,4:5] , label='True Values',c='g')\nplt.plot(np.linspace(0,t_l,t_l), np.array(test_predictions)[:,4:5], label='Predicted Values',c='r')\nplt.title(hits.columns[4], fontsize=20)\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(np.linspace(0,t_l,t_l), scaled_y[:,5:6] , label='True Values',c='g')\nplt.plot(np.linspace(0,t_l,t_l), np.array(test_predictions)[:,5:6], label='Predicted Values',c='r')\nplt.title(hits.columns[5], fontsize=20)\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(np.linspace(0,t_l,t_l), scaled_y[:,6:7] , label='True Values',c='g')\nplt.plot(np.linspace(0,t_l,t_l), np.array(test_predictions)[:,6:7], label='Predicted Values',c='r')\nplt.title(hits.columns[6], fontsize=20)\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Inference:\nIt can be infered that prediction on the spanish wikipedia pages has been the best. Now it can be improved by tweaking the LSTM architecture and the length of time sequence to be fed. The model has been tested with 440 training instances, which is too low, and tested on 110 instances. No. of features is 7.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}