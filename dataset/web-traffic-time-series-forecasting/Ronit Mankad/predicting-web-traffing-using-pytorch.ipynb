{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Approach Used: "},{"metadata":{},"cell_type":"markdown","source":"### Inputs"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport re\nimport os\n%matplotlib inline\n\n\nimport argparse\nimport pickle\n\nimport numpy as np; np.seterr(invalid='ignore')\nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(os.listdir('../input/'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Initial EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/web-traffic-time-series-forecasting/train_1.csv').fillna(0)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_language(page):\n    res = re.search('[a-z][a-z].wikipedia.org', page)\n    if res:\n        return res[0][0:2]\n    return 'na'\n\ntrain['lang'] = train.Page.map(get_language)\n\nfrom collections import Counter\n\nprint(Counter(train.lang))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lang_sets = {}\nlang_sets['en'] = train[train.lang=='en'].iloc[:,0:-1]\nlang_sets['ja'] = train[train.lang=='ja'].iloc[:,0:-1]\nlang_sets['de'] = train[train.lang=='de'].iloc[:,0:-1]\nlang_sets['na'] = train[train.lang=='na'].iloc[:,0:-1]\nlang_sets['fr'] = train[train.lang=='fr'].iloc[:,0:-1]\nlang_sets['zh'] = train[train.lang=='zh'].iloc[:,0:-1]\nlang_sets['ru'] = train[train.lang=='ru'].iloc[:,0:-1]\nlang_sets['es'] = train[train.lang=='es'].iloc[:,0:-1]\n\nsums = {}\nfor key in lang_sets:\n    sums[key] = lang_sets[key].iloc[:,1:].sum(axis=0) / lang_sets[key].shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"days = [r for r in range(sums['en'].shape[0])]\n\nfig = plt.figure(1, figsize=[10, 10])\nplt.ylabel('View Per Page')\nplt.xlabel('Day')\nplt.title('Pages in Different Languages')\nlabels={'en':'English','ja':'Japanese','de':'German',\n        'na':'Media','fr':'French','zh':'Chinese',\n        'ru':'Russian','es':'Spanish'\n       }\n\nfor key in sums:\n    plt.plot(days,sums[key],label = labels[key] )\n    \nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Creating Medians of Fibonacci"},{"metadata":{"trusted":true},"cell_type":"code","source":"parser = {\n    'offset': 803,\n    'val_len': 64,\n    'seed': 20170913,\n    'windows': [7, 14, 21, 35, 56, 91, 147, 238, 385, 623],\n    'forecast_start': '2017-09-11',\n    'forecast_end': '2017-11-13'\n}\nargs = argparse.Namespace(**parser)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def smape(y_pred, y_true):\n    y_pred = np.around(y_pred)\n    denominator = y_true + y_pred\n    diff = np.abs(y_true - y_pred) / denominator\n    diff[denominator == 0] = 0\n    return 200 * np.nanmean(diff)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Getting data...\")\nfull = pd.read_csv('../input/web-traffic-time-series-forecasting/train_2.csv')\nfull.iloc[:, 1:] = full.iloc[:, 1:].fillna(method='ffill', axis=1).fillna(\n        method='bfill', axis=1)\ndatetime_list = pd.date_range(args.forecast_start, args.forecast_end)\nfor datetime in datetime_list:\n    full[datetime.date().isoformat()] = 0\n\nprint(\"Constructing test set...\")\ntest = pd.melt(full[list(\n    full.columns[args.offset+1:args.offset+args.val_len+1])+['Page']],\n    id_vars='Page', var_name='Date', value_name=\"Visits\")\ntest['Date'] = test['Date'].astype('datetime64[ns]')\ntest['Weekend'] = test['Date'].dt.dayofweek >= 5\n\nprint(\"Constructing train set...\")\ntrain = full.iloc[:, :args.offset+1]\n\nprint(\"Getting medians...\")\nfor i in args.windows:\n    print(i, end=' ')\n    val = 'MW'+str(i)\n    tmp = pd.melt(train[list(train.columns[-i:])+['Page']],\n                  id_vars='Page', var_name='Date', value_name=val)\n    tmp['Date'] = tmp['Date'].astype('datetime64[ns]')\n    tmp['Weekend']= tmp['Date'].dt.dayofweek >= 5           \n    tmp1 = tmp.groupby(['Page', 'Weekend']).median().reset_index()\n    test = test.merge(tmp1, how='left')\nprint(\"\\n\")\n\nprint(\"Getting median of medians...\")\ntest['Predict'] = test[[\"MW7\", \"MW7\", \"MW14\", \"MW21\", \"MW35\", \"MW56\", \"MW91\",\n    \"MW147\", \"MW238\", \"MW385\", \"MW623\"]].median(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.makedirs('../working/{}'.format(args.seed))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(os.listdir('../working'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open(\"../working/{}/pred_fib.pkl\".format(args.seed), \"wb\") as f:\n    predict_df = test[[\"Page\", \"Date\", \"Predict\"]].pivot(\n        index='Page', columns='Date')['Predict'].loc[full[\"Page\"]]\n    pickle.dump(predict_df.values, f)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Now predicting using LSTM"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport argparse\nimport pickle\nimport time\n\nimport numpy as np; np.seterr(invalid='ignore')\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import MultiStepLR\nfrom torch.utils.data import TensorDataset, DataLoader","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"parser = {\n    'data_path': '../input/web-traffic-time-series-forecasting/',\n    'train_file': 'train_2.csv',\n    'key_file': 'key_2.csv',\n    'intermediate_path': '../working/',\n    'train_len': 385,\n    'train_skip': 91,\n    'val_len': 64,\n    'offset': 803,\n    'batch_size': 256,\n    'hidden_size': 256,\n    'log_every': 10,\n    'read_from_file': False,\n    'train': False,\n    'model_name': '',\n    'forecast': True,\n    'cuda': True,\n    'seed': 20170913,\n}\nargs = argparse.Namespace(**parser)\n\nargs.cuda = args.cuda and torch.cuda.is_available()\ntorch.manual_seed(args.seed)\n\nargs.intermediate_path = os.path.join(args.intermediate_path, str(args.seed))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class DenseLSTMForecast(nn.Module):\n    def __init__(self, hidden_size):\n        super(DenseLSTMForecast, self).__init__()\n        self.lstm1 = nn.LSTMCell(1, hidden_size)\n        self.lstm2 = nn.LSTMCell(hidden_size+1, hidden_size)\n        self.lstm3 = nn.LSTMCell(2*hidden_size+1, hidden_size)\n        self.linear = nn.Linear(3*hidden_size+1, 1)\n        self.hidden_size = hidden_size\n\n    def forward(self, x, future=0):\n        o = []\n        tt = torch.cuda if args.cuda else torch\n        h1_t = Variable(tt.FloatTensor(x.size(0), self.hidden_size).zero_())\n        c1_t = Variable(tt.FloatTensor(x.size(0), self.hidden_size).zero_())\n        h2_t = Variable(tt.FloatTensor(x.size(0), self.hidden_size).zero_())\n        c2_t = Variable(tt.FloatTensor(x.size(0), self.hidden_size).zero_())\n        h3_t = Variable(tt.FloatTensor(x.size(0), self.hidden_size).zero_())\n        c3_t = Variable(tt.FloatTensor(x.size(0), self.hidden_size).zero_())\n        \n        for x_t in x.chunk(x.size(1), dim=1):\n            x_t = x_t.squeeze(dim=1)\n            h1_t, c1_t = self.lstm1(x_t, (h1_t, c1_t))\n            h1d_t = torch.cat([x_t, h1_t], dim=1)\n            h2_t, c2_t = self.lstm2(h1d_t, (h2_t, c2_t))\n            h2d_t = torch.cat([x_t, h1_t, h2_t], dim=1)\n            h3_t, c3_t = self.lstm3(h2d_t, (h3_t, c3_t))\n            h3d_t = torch.cat([x_t, h1_t, h2_t, h3_t], dim=1)\n            o_t = self.linear(h3d_t)\n            o.append(o_t)\n\n            \n        for i in range(future):\n            h1_t, c1_t = self.lstm1(o_t, (h1_t, c1_t))\n            h1d_t = torch.cat([o_t, h1_t], dim=1)\n            h2_t, c2_t = self.lstm2(h1d_t, (h2_t, c2_t))\n            h2d_t = torch.cat([o_t, h1_t, h2_t], dim=1)\n            h3_t, c3_t = self.lstm3(h2d_t, (h3_t, c3_t))\n            h3d_t = torch.cat([o_t, h1_t, h2_t, h3_t], dim=1)\n            o_t = self.linear(h3d_t)\n            o.append(o_t)\n\n        return torch.stack(o, dim=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_data():\n    raw_data_file = os.path.join(args.intermediate_path, 'raw_data.pkl')\n    scaled_data_file = os.path.join(args.intermediate_path,\n                                    'scaled_data.pkl')\n    scaler_file = os.path.join(args.intermediate_path, 'scaler.pkl')\n    \n    if not args.read_from_file:\n        data_df = pd.read_csv(os.path.join(args.data_path, args.train_file),\n                              index_col='Page')\n        raw_data = data_df.values.copy()\n        data_df = data_df.fillna(method='ffill', axis=1).fillna(\n            method='bfill', axis=1)\n        data = np.nan_to_num(data_df.values.astype('float32'))\n        data = np.log1p(data)\n        scaler = StandardScaler()\n        scaler.fit(np.swapaxes(data, 0, 1))\n        scaled_data = scaler.transform(np.swapaxes(data, 0, 1))\n        scaled_data = np.swapaxes(scaled_data, 0, 1)\n        \n        with open(raw_data_file, 'wb') as f:\n            pickle.dump(raw_data, f)\n        with open(scaled_data_file, 'wb') as f:\n            pickle.dump(scaled_data, f)\n        with open(scaler_file, 'wb') as f:\n            pickle.dump(scaler, f)\n    else:\n        with open(raw_data_file, 'rb') as f:\n            raw_data = pickle.load(f)\n        with open(scaled_data_file, 'rb') as f:\n            scaled_data = pickle.load(f)\n        with open(scaler_file, 'rb') as f:\n            scaler = pickle.load(f)\n    return raw_data, scaled_data, scaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(raw_data, scaled_data, scaler, model, criterion, optimizer):\n    p = np.random.permutation(scaled_data.shape[0])\n    inverse_p = np.argsort(p)\n    \n    input_tensor = torch.from_numpy(\n        scaled_data[p, :(args.offset-1)]).unsqueeze(2)\n    target_tensor = torch.from_numpy(\n        scaled_data[p, 1:args.offset]).unsqueeze(2)\n    dataset = TensorDataset(input_tensor, target_tensor)\n    data_loader = DataLoader(dataset, args.batch_size)\n    \n    train_loss = 0\n#    val_output_list = []\n    init_time = time.time()\n    for i, (inputt, target) in enumerate(data_loader):\n        if args.cuda:\n            inputt = inputt.cuda()\n            target = target.cuda()\n        inputt = Variable(inputt)\n        target = Variable(target)\n        \n#        output = model(inputt, future=args.val_len)\n        output = model(inputt)\n        pos = np.random.randint(args.train_skip,\n                                inputt.size(1)-args.train_len+1)\n        loss = criterion(output[:, pos:pos+args.train_len],\n                         target[:, pos:pos+args.train_len])\n        optimizer.zero_grad()\n        loss.backward()\n        nn.utils.clip_grad_norm(model.parameters(), 3, 'inf')\n        optimizer.step()\n        train_loss += loss.item() * inputt.size(0)\n#        val_output_list.append(output[:, -args.val_len:]\n#                               .data.squeeze(2).cpu().numpy())\n        \n        if i % args.log_every == 0:\n            print(\"   % Time: {:4.0f}s | Batch: {:4} | \"\n                  \"Train loss: {:.4f}\".format(\n                      time.time()-init_time, i+1, loss.item()))\n        \n#    val_output_all = np.concatenate(val_output_list, axis=0)[inverse_p]\n#    prediction = np.swapaxes(scaler.inverse_transform(\n#           np.swapaxes(val_output_all, 0, 1)), 0, 1)\n#    prediction = np.clip(np.exp(prediction)-1, 0, None)\n#    var_target = raw_data[:, args.offset:args.offset+args.val_len]\n    \n    train_loss /= scaled_data.shape[0]\n#    val_loss = smape(prediction, var_target)\n    val_loss = 0\n    print(\"=\"*10)\n    print(\"   % Epoch: {} | Time: {:4.0f}s | \"\n          \"Train loss: {:.4f} | Val loss: {:.4f}\"\n          .format(epoch, time.time()-init_time, train_loss, val_loss))\n    print(\"=\"*10)\n    return val_loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def forecast(raw_data, scaled_data, scaler, model):\n    input_tensor = torch.from_numpy(scaled_data[:,\n            :args.offset]).unsqueeze(2)\n    target_tensor = torch.zeros(input_tensor.size(0))\n    dataset = torch.utils.data.TensorDataset(input_tensor, target_tensor)\n    data_loader = DataLoader(dataset, 128)\n    \n    output_list = []\n    for i, (inputt, _) in enumerate(data_loader):\n        if args.cuda:\n            inputt = inputt.cuda()\n        inputt = Variable(inputt)\n        output = model(inputt, args.val_len)\n        output_list.append(output.data.squeeze(2).cpu().numpy()\n                           [:, -args.val_len:])\n        \n    output_all = np.concatenate(output_list, axis=0)\n    prediction = np.swapaxes(scaler.inverse_transform(\n            np.swapaxes(output_all, 0, 1)), 0, 1)\n\n    prediction = np.clip(np.exp(prediction) - 1, 0, None)\n    return prediction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def save_model(model, epoch, loss):\n    model_file = os.path.join(args.intermediate_path,\n                              \"model_{}_epoch{}_loss{:.4f}.pth\"\n                              .format(args.seed, epoch, loss))\n    torch.save(model.state_dict(), os.path.join(model_file))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data, scaled_data, scaler = get_data()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = DenseLSTMForecast(args.hidden_size)\nif args.cuda:\n    model.cuda()\ncriterion = nn.L1Loss()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer = optim.SGD(model.parameters(), lr=0.001)\nscheduler = MultiStepLR(optimizer, milestones=[2, 4])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if args.train:\n    for epoch in range(1, 7):\n        scheduler.step()\n        print(\"=> EPOCH {} with lr {}\".format(epoch, scheduler.get_lr()))\n        val_loss = train(raw_data, scaled_data, scaler,\n                         model, criterion, optimizer)\n        save_model(model, epoch, val_loss)\nelse:\n    #model_file = os.path.join(args.intermediate_path, args.model_name)\n    model.load_state_dict(torch.load('../input/modelweights/model_e6.pth'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if args.forecast:\n    print('Predicting....')\n    prediction = forecast(raw_data, scaled_data, scaler, model)\n#     print(\"SMAPE: {}\".format(smape(prediction, raw_data[:,\n#        args.offset:args.offset+args.val_len])))\n    with open(os.path.join(args.intermediate_path,\n                           \"pred_rnn.pkl\"), \"wb\") as f:\n        pickle.dump(prediction, f)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Ensembling the Predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"parser = {\n    'offset': 803,\n    'test_len': 63,\n    'seed': 20170913,\n    'forecast_start': '2017-09-13',\n    'forecast_end': '2017-11-13'\n}\nargs = argparse.Namespace(**parser)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def ensemble(pred_fib, pred_rnn, y_true):\n    for i in range(0, 11):\n        y_pred = (pred_fib * i + pred_rnn * (10 - i)) / 10\n        print(\"{} fib + {} rnn = {}\".format(i, 10-i, smape(y_pred, y_true)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full = pd.read_csv('../input/web-traffic-time-series-forecasting/train_2.csv', index_col='Page')\ny_true = full.iloc[:, -args.test_len:].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open(\"../working/{}/pred_fib.pkl\".format(args.seed), \"rb\") as f:\n    pred_fib = pickle.load(f)\nwith open(\"../working/{}/pred_rnn.pkl\".format(args.seed), \"rb\") as f:\n    pred_rnn = pickle.load(f)\n    \nprediction = (pred_rnn + pred_fib) / 2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.DataFrame()\ntest[\"Page\"] = full.index\ndatetime_list = pd.date_range(args.forecast_start, args.forecast_end)\nfor datetime in datetime_list:\n    test[datetime.date().isoformat()] = 0\ntest.iloc[:, 1:] = np.around(prediction[:, 2:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.melt(test, id_vars='Page', var_name='Date', value_name=\"Visits\")\n\nkey_df = pd.read_csv('../input/web-traffic-time-series-forecasting/key_2.csv')\nkey_df['Date'] = key_df['Page'].apply(lambda a: a[-10:])\nkey_df['Page'] = key_df['Page'].apply(lambda a: a[:-11])\nkey_df = key_df.merge(test, how=\"left\")\n\nkey_df[['Id', 'Visits']].to_csv(\n    '../working/{}/submission.csv'.format(args.seed), index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"key_df.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}