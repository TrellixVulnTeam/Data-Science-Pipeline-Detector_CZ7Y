{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"!pip install u8darts[torch]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nimport datetime\nimport random\n\nfrom sklearn.base import TransformerMixin\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.impute import SimpleImputer\n\nimport joblib\nimport pickle\nimport os\n\nfrom darts import TimeSeries\nfrom darts.metrics import mape, smape\nfrom darts.models import RNNModel, NBEATSModel\nfrom darts.utils.data.sequential_dataset import SequentialDataset\nfrom darts.utils.data.horizon_based_dataset import HorizonBasedDataset\n\nfrom torch.nn import L1Loss\nimport torch\nimport torch.optim as optim","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Fixing random seeds"},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.manual_seed(1337)\nrandom.seed(1337)\nnp.random.seed(1337)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loading data"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/web-traffic-time-series-forecasting/train_2.csv.zip', compression='zip', index_col='Page').fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns = pd.DatetimeIndex(df.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Config"},{"metadata":{"trusted":true},"cell_type":"code","source":"LOOKBACK = 3\nCHUNK_OUTPUT_FINAL_LENGTH = (datetime.date(2017, 11, 13) - datetime.date(2017, 9, 10)).days\nCHUNK_OUTPUT_LENGTH = 32\nCHUNK_INPUT_LENGTH = CHUNK_OUTPUT_LENGTH*LOOKBACK","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"CACHEDIR = './'\nmemory = joblib.Memory(CACHEDIR, verbose=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cleaning cache if changing the params\n!rm ./training_sequence.pickle ./test_sequence.pickle ./validation_sequence.pickle","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training "},{"metadata":{"trusted":true},"cell_type":"code","source":"def walk_forward_split(df: pd.DataFrame,\n                       chunk_input_length: int,\n                       chunk_output_length: int,\n                       validset_ratio: float = 0.2,\n                       testset_ratio: float = 0.1):\n    \n    dataset_length = df.shape[0]\n    timespan = df.shape[1]\n\n    testset_start_index = timespan - int(timespan * testset_ratio)\n    validset_start_index = testset_start_index - int(timespan * validset_ratio)\n\n    training_set = df.iloc[:, :validset_start_index]\n    validation_set = df.iloc[:, validset_start_index-chunk_input_length:testset_start_index]\n    test_set = df.iloc[:, testset_start_index-chunk_input_length:]\n\n    return training_set, validation_set, test_set\n\n\ndef array_to_seq(arr, date_index):\n    ts_sequence = []\n    for i in tqdm(range(len(arr))):\n        ts_sequence.append(TimeSeries.from_times_and_values(date_index, arr[i, :]))\n\n    return ts_sequence","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_set, validation_set, test_set = walk_forward_split(df=df,\n                                                            chunk_input_length=CHUNK_INPUT_LENGTH,\n                                                            chunk_output_length=CHUNK_OUTPUT_LENGTH,\n                                                            validset_ratio=0.15,\n                                                            testset_ratio=0.0)                                                            ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class LocalMinMaxScaler(TransformerMixin):\n\n    def __init__(self, minimum=None):\n        self.minimum = minimum\n\n    def fit(self, X, y=None):\n        if isinstance(X, pd.DataFrame):\n              X = X.values\n\n        self.min_ = X.min(axis=1).reshape(-1, 1) if self.minimum is None else self.minimum\n        self.max_ = X.max(axis=1).reshape(-1, 1)\n\n        return self\n\n    def transform(self, X, y=None):\n        return np.divide(X - self.min_, self.max_ - self.min_, out=np.zeros_like(X), where=(self.max_ - self.min_) != 0.0)\n\n    def inverse_transform(self, X):\n        return X * (self.max_ - self.min_) + self.min_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class VariableSizeImputer(TransformerMixin):\n    def __init__(self, fill_value=0.0):\n        self.fill_value = fill_value\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X, y=None):\n        return X.fillna(self.fill_value)\n\n    def inverse_transform(self, X):\n        return X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imputer = VariableSizeImputer()\nscaler = LocalMinMaxScaler(minimum=0.1)\n#log_mapper = FunctionTransformer(func=np.log1p, inverse_func=np.expm1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipe = make_pipeline(imputer,\n                     #log_mapper,\n                     scaler)\npipe.fit(training_set)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"CACHE_ON = False\n\ndef transform_and_cache(dataset, columns, fpath):\n    if CACHE_ON:\n        if os.path.exists(fpath):\n            return joblib.load(fpath)\n        else:\n            ret = array_to_seq(pipe.transform(dataset), columns)\n            joblib.dump(ret, fpath)\n            return ret\n    else:\n        return array_to_seq(pipe.transform(dataset), columns)\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_sequence = transform_and_cache(\n    training_set, \n    training_set.columns, \n    os.path.join(CACHEDIR, 'training_sequence.pickle'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"validation_sequence = transform_and_cache(\n    validation_set, \n    validation_set.columns,\n    os.path.join(CACHEDIR, 'validation_sequence.pickle'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_sequence = transform_and_cache(test_set, \n                                    test_set.columns,\n                                    os.path.join(CACHEDIR,'test_sequence.pickle'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from darts.utils.data.horizon_based_dataset import HorizonBasedDataset\n\ntraining_dataset = HorizonBasedDataset(target_series = training_sequence,\n                                       output_chunk_length=CHUNK_OUTPUT_LENGTH,\n                                       lh=(1,2),\n                                       lookback=LOOKBACK)\n\nvalidation_dataset = HorizonBasedDataset(target_series = validation_sequence,\n                                         output_chunk_length=CHUNK_OUTPUT_LENGTH,\n                                         lh=(1,2),\n                                         lookback=LOOKBACK)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def divide_no_nan(a, b):\n    result = a / b\n    result[result != result] = .0\n    result[result == np.inf] = .0\n    return result\n\ndef smape_loss(forecast, target):\n    return 200 * torch.mean(divide_no_nan(torch.abs(forecast - target),\n                                          torch.abs(forecast.data) + torch.abs(target.data)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"N_EPOCHS = 25\n\n\nNUM_STACKS=1\nNUM_BLOCKS=4\nNUM_LAYERS=4\nLAYER_WIDTH=64\nMODEL_NAME = 'NBEATS'\nBATCH_SIZE = 1024\n\nmodel = NBEATSModel(input_chunk_length=CHUNK_OUTPUT_LENGTH*LOOKBACK,\n                    output_chunk_length=CHUNK_OUTPUT_LENGTH,\n                    nr_epochs_val_period=1,\n                    num_stacks=NUM_STACKS,\n                    num_blocks=NUM_BLOCKS,\n                    num_layers=NUM_LAYERS,\n                    layer_widths=LAYER_WIDTH,\n                    generic_architecture=True,\n                    model_name=MODEL_NAME,\n                    batch_size=BATCH_SIZE,\n                    #log_tensorboard=True,\n                    n_epochs=N_EPOCHS,\n                    loss_fn=smape_loss)\n\n# model fitting\nprint(\"STARTING TRAINING..\")\nmodel.fit_from_dataset(train_dataset=training_dataset, val_dataset=validation_dataset,verbose=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predictions and Post-Processing"},{"metadata":{},"cell_type":"markdown","source":"## Reading the mapping for the competition submission format"},{"metadata":{"trusted":true},"cell_type":"code","source":"KEYS = '../input/web-traffic-time-series-forecasting/key_2.csv.zip'\ndef read_keys():\n    keys = pd.read_csv(KEYS, compression='zip')\n    id_dict = {}\n\n    for page, page_id in zip(keys['Page'], keys['Id']):\n        id_dict.update({page:page_id})\n\n    return id_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"id_dict = read_keys()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Predicting"},{"metadata":{"trusted":true},"cell_type":"code","source":"#model = torch.load('/content/drive/MyDrive/NBEATS_2_stacks/checkpoint_24.pth.tar')\npred = model.predict(n=CHUNK_OUTPUT_FINAL_LENGTH, series=test_sequence)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Post-processing"},{"metadata":{"trusted":true},"cell_type":"code","source":"def map_page_and_time_to_id(page, date):\n    return id_dict[page+'_'+str(date)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = np.array([p.values().squeeze() for p in pred])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"post_proc_pred = pipe.inverse_transform(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = []\ntime_index = pred[0].time_index()\n\nfor page, ts in tqdm(zip(training_set.index, post_proc_pred)):\n    for timestamp, value in zip(time_index[2:], ts[2:]):\n        # value[0] since values() returns an array of arrays\n        submission.append([map_page_and_time_to_id(page, timestamp.date()), value])\n\nsubmission_df = pd.DataFrame.from_records(submission, columns=['Id', 'Visits'])\n\nassert len(submission_df) == 8993906\n\n# saving the output file\nsubmission_df.to_csv('submission.csv', index=False)\nprint('FILE SAVED')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kaggle competitions submit -c web-traffic-time-series-forecasting -f submission.csv -m \"NBEATS 1 stack, 4 blocks, 4 layers, 64 layer width, smape loss\"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}