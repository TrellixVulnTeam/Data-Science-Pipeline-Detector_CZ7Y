{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport joblib\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.preprocessing import PowerTransformer, MultiLabelBinarizer\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import f1_score, precision_recall_curve, auc\n\nfrom imblearn.over_sampling import SMOTE\n\nimport tensorflow as tf\nfrom tensorflow import keras\n\ndata_folder = '../input/DontGetKicked'\nmodel_folder = './model'\n\nif not os.path.exists(model_folder):\n    os.mkdir(model_folder)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv(data_folder + '/training.csv',index_col = 'RefId')\ndata['PurchDate'] = pd.to_datetime(data['PurchDate'])\n# set y_train\ny_train = data['IsBadBuy'].values\n# List of processed column for train\nX_train, f_train = [], []\n\nprint(f'Number of rows: {len(y_train)}, kicked auction: {(y_train==1).sum()}/{len(y_train)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 0. External enrichment of zipcode level data\n\nWe introduce zipcode level household income data and population density data to featurize the zipcode. This external data is useful when zipcodes in test dataset didn't exist in training data. Such features link the similarity between zipcode area."},{"metadata":{"trusted":false},"cell_type":"code","source":"zip_income = pd.read_csv(data_folder + '/MedianZIP-2010.csv')\nzip_income['MedianHouseholdIncome'] = zip_income['MedianHouseholdIncome'].str.replace(',','').astype(float)\nzip_pop = pd.read_csv(data_folder + '/pop_density_2010.csv')\n# join external data of zipcode income and population\ndata = data.merge(zip_income, left_on = 'VNZIP1',right_on = 'Zip/ZCTA',how = 'left').drop(columns = ['Zip/ZCTA'])\ndata = data.merge(zip_pop, left_on = 'VNZIP1',right_on = 'Zip/ZCTA',how = 'left').drop(columns = ['Zip/ZCTA'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Columnwise Feature Engineering\n\n## 1.1 Numerical Column"},{"metadata":{"trusted":false},"cell_type":"code","source":"# numerical features\nquant_fs = ['VehicleAge','VehOdo','MMRAcquisitionAuctionAveragePrice',\n            'MMRAcquisitionAuctionCleanPrice','MMRAcquisitionRetailAveragePrice',\n            'MMRAcquisitonRetailCleanPrice','MMRCurrentAuctionAveragePrice',\n            'MMRCurrentAuctionCleanPrice','MMRCurrentRetailCleanPrice','VehBCost','WarrantyCost',\n            'MedianHouseholdIncome','Pop2010','Area','PopDensity']\n\nfig,ax = plt.subplots(4,4, figsize = (16,10))\nfor i, col in enumerate(quant_fs):\n    ax[i//4, i%4].hist(data[col],bins = 20)\n    ax[i//4, i%4].set_title(col)\nfig.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Yeo-Johnson scaler and feature correlation"},{"metadata":{"trusted":false},"cell_type":"code","source":"# minimum imputation\nX = data[quant_fs]\nyj_scaler = PowerTransformer()\nX = yj_scaler.fit_transform(X)\n\nfig, ax = plt.subplots(figsize = (8,8))\nax.imshow(np.corrcoef(np.nan_to_num(X,0).T))\nax.set_yticks(range(len(quant_fs)))\nax.set_yticklabels(quant_fs)\nax.set_xticks(range(len(quant_fs)))\nax.set_xticklabels(quant_fs,rotation = 90)\nfig.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Iterative PCA for imputation"},{"metadata":{"trusted":false},"cell_type":"code","source":"# value for imputation\nimp = IterativeImputer(max_iter=20, random_state=0)\nX = imp.fit_transform(X)\n\nprint(f'Number of numerical features: {X.shape[1]}')\n\n# add features\nX_train.append(X)\nf_train += quant_fs","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.2 Categorized Columns\n\n### 1.2.1 Car Feature Extraction"},{"metadata":{"trusted":false},"cell_type":"code","source":"X_cat = []\n\ndef extract_car_features(row, feature_list = None):\n    # Sync Feature string\n    f_str = ' '.join([str(row['Make']), str(row['Trim']), str(row['Model']), str(row['SubModel']),str(row['Size'])]).upper()\n    # replace annoying string\n    f_str = f_str.replace('&','').replace('/','')\n    f_list = f_str.split(' ')\n    if feature_list is None:\n        return [c for c in list(set(f_list)) if c!='']\n    else:\n        return [c for c in list(set(f_list)) if c in feature_list]\n    \n\n# multiple encoding\ndata['car_features'] = data.apply(extract_car_features, axis = 1)\n\nm_bin = MultiLabelBinarizer()\nX_car = m_bin.fit_transform(data['car_features'])\ncar_features = m_bin.classes_\n\nprint(f'Number of features assotiate to car: {len(car_features)}')\n\n# append features\nX_cat.append(X_car)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.2.2 One-hot encode dense feature columns"},{"metadata":{"trusted":false},"cell_type":"code","source":"dense_cols = ['TopThreeAmericanName','Nationality','Auction',\n              'Transmission','WheelType','PRIMEUNIT','AUCGUART','IsOnlineSale']\n\n# One-hot encoding\ndf_dense = pd.get_dummies(data[dense_cols])\n\n# append features\nX_train.append(df_dense.values)\nf_train += list(df_dense.columns)\n\ndense_cols_encoded = df_dense.columns\n\nprint(f'Number of Encoded Dense features: {len(dense_cols_encoded)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.2.3 Embedding Sparse Column"},{"metadata":{"trusted":false},"cell_type":"code","source":"sparse_cols = ['Color','BYRNO','VNZIP1','VNST']\ndata['BYRNO'] = data['BYRNO'].astype(str)\ndata['VNZIP1'] = data['VNZIP1'].astype(str)\n\n# Onhot encoding for future embedding\ndf_other = pd.get_dummies(data[sparse_cols])\n# list of cols encoded\nsparse_cols_encoded = list(df_other.columns)\nX_cat.append(df_other.values)\n\nX_sparse = np.hstack(X_cat)\n\nprint(f'Total Features for embedding: {X_sparse.shape[1]}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Train encoder decoder"},{"metadata":{"trusted":false},"cell_type":"code","source":"# encoder\ndef autoencoder(dims, n_feature):\n    # Flatten inputs\n    input_layer = keras.layers.Input(shape=(n_feature,),name='Input')\n    x = input_layer\n    \n    # encoder sequence\n    for i, n in enumerate(dims[:-1]):\n        x = keras.layers.Dense(n, activation='relu', name=f'encoder_{i}')(x)\n\n    # Encoded layer\n    encoded = keras.layers.Dense(dims[-1],activity_regularizer=keras.regularizers.l1(1e-5),\n                                 name='encoded')(x) \n    x = encoded\n    \n    # decoder sequence\n    decoder_dims = dims[::-1]\n    for i, n in enumerate(decoder_dims[1:]):\n        x = keras.layers.Dense(n, activation='relu', name=f'decoder_{i}')(x)\n\n    # output\n    x = keras.layers.Dense(n_feature, name='decoded')(x)\n    decoded = x\n    return (keras.models.Model(inputs=input_layer, outputs=decoded, name='autoencoder'), \n            keras.models.Model(inputs=input_layer, outputs=encoded, name='encoder'))\n\n# encoder dimensions \ndims = [256,64,16]\nencoder_decoder, encoder = autoencoder(dims,X_sparse.shape[1])\n\n# training specifications\noptimizer = keras.optimizers.Adam(lr=2e-4)\nencoder_decoder.compile(optimizer= optimizer, loss='mse')\n\nkeras.utils.plot_model(encoder_decoder, rankdir='LR',show_shapes =True,to_file='encoder_decoder.png')","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"# Train auto encoder\nhistory = encoder_decoder.fit(X_sparse, X_sparse, batch_size=128, epochs=50, \n                              validation_split= 0.1, verbose = 1)\n# save model\nencoder_decoder.save(model_folder + '/autoencoder')\nencoder.save(model_folder + '/encoder')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"fig,ax = plt.subplots(figsize=(6,4))\nax.plot(history.history['loss'],label = 'Training')\nax.plot(history.history['val_loss'],label = 'Validation')\nax.set_xlabel('Epoch')\nax.set_ylabel('Decoded MSE')\nax.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Transform"},{"metadata":{"trusted":false},"cell_type":"code","source":"# embedding layer\nX_embd = encoder.predict(X_sparse, verbose=0)\n\n# append the feature\nX_train.append(X_embd)\nf_train += [f'Embbeded {i}' for i in range(X_embd.shape[1])]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.3 Prepare for final features"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Concat all features created\nX_train = np.hstack(X_train)\n# Check the integrity\nassert len(f_train) == X_train.shape[1]\n\n# save result\nnp.savez(data_folder +'/train_data.npz',X_train = X_train, y_train = y_train)\n\nprint(f'Final Features: {X_train.shape[1]}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Training"},{"metadata":{},"cell_type":"markdown","source":"## 2.1 SMOTE + Random Forest + cross validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"f1_cv,auc_cv = [], []\n\n# Stratified k-fold validation\nskf = StratifiedKFold(n_splits = 5)\nfig,ax = plt.subplots(figsize = (6,6))\n\ncolors = ['tab:blue','tab:orange', 'tab:green','tab:red','tab:purple']\nfor i, k in enumerate(range(3,16,3)):\n    # Cross validation\n    f1,pr_auc = [],[]\n    for t_idx, v_idx in skf.split(X_train,y_train):\n        # Resample with SMOTE\n        sm = SMOTE(random_state=42,n_jobs = 8)\n        X_res, y_res = sm.fit_resample(X_train[t_idx,:], y_train[t_idx])\n        \n        # train with random foreset\n        clf = RandomForestClassifier(n_estimators = 300,max_depth=k,n_jobs = 8)\n        clf.fit(X_res, y_res)\n        \n        # Evaluate by F1 and precision-recall curve AUC\n        y_pred = clf.predict_proba(X_train[v_idx,:])\n        prc, rcl, _ = precision_recall_curve(y_train[v_idx],y_pred[:,1])\n        ax.plot(rcl,prc,color = colors[i], alpha = 0.2)\n        f1.append((2*prc*rcl/(prc + rcl+1e-6)).max())\n        pr_auc.append(auc(rcl, prc))\n        \n    ax.plot(rcl,prc,color = colors[i], alpha = 0.3,label = f'Max Depth = {k}')\n    f1_cv.append(np.mean(f1))\n    auc_cv.append(np.mean(pr_auc))\n    \n    print(f'max_depth = {k}, CV PR-AUC: {np.mean(pr_auc):.4f}, Max F1 score: {np.mean(f1):.4f}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.2 Train with Optimal model\n\nChoose the model with max_depth = 12 and pick the threshold with largest F1 score."},{"metadata":{"trusted":false},"cell_type":"code","source":"# Resample with SMOTE\nsm = SMOTE(random_state=42,n_jobs = 8)\nX_res, y_res = sm.fit_resample(X_train, y_train)\n\n# train with random foreset\nclf = RandomForestClassifier(n_estimators =300, max_depth=12, n_jobs = 8)\nclf.fit(X_res, y_res)\n\ny_pred = clf.predict_proba(X_train)\n_, _, thres = precision_recall_curve(y_train,y_pred[:,1])\n\nopt_idx = (2*prc*rcl/(prc + rcl+1e-6)).argmax()\nthreshold = thres[opt_idx]\n\nprint(f'Optimal prediction threshold: {thres[opt_idx]:.4f}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Test data Prediction"},{"metadata":{"trusted":false},"cell_type":"code","source":"data = pd.read_csv(data_folder +'/test.csv')\ndata['PurchDate'] = pd.to_datetime(data['PurchDate'])\n\n# join external data of zipcode income and population\ndata = data.merge(zip_income, left_on = 'VNZIP1',right_on = 'Zip/ZCTA',how = 'left').drop(columns = ['Zip/ZCTA'])\ndata = data.merge(zip_pop, left_on = 'VNZIP1',right_on = 'Zip/ZCTA',how = 'left').drop(columns = ['Zip/ZCTA'])\n\nX_test, X_cat = [],[]\n\n# numerical data\nX = data[quant_fs]\nX = yj_scaler.transform(X)\nX = imp.transform(X)\n\nX_test.append(X)\n\n# car feature encoding\ndata['car_features'] = data.apply(lambda x: extract_car_features(x, feature_list = m_bin.classes_), axis = 1)\nX_cat.append(m_bin.transform(data['car_features']))\n\n# One-hot encoding dense features\ndf_dense = pd.get_dummies(data[dense_cols]).reindex(columns = dense_cols_encoded, fill_value = 0)\n\n# append features\nX_test.append(df_dense.values)\n\n# other spase featurs\nsparse_cols = ['Color','BYRNO','VNZIP1','VNST']\ndata['BYRNO'] = data['BYRNO'].astype(str)\ndata['VNZIP1'] = data['VNZIP1'].astype(str)\n\n# Onhot encoding for future embedding\ndf_other = pd.get_dummies(data[sparse_cols]).reindex(columns = sparse_cols_encoded, fill_value =0)\nX_cat.append(df_other.values)\n\nX_sparse = np.hstack(X_cat)\n\n# embedding layer\nX_embd = encoder.predict(X_sparse, verbose=0)\nX_test.append(X_embd)\n# Prepare final featurs\nX_test = np.hstack(X_test)\n# Prediction\ny_proba = clf.predict_proba(X_test)\ny_pred = y_proba[:,1] > 0.5\nprint(f'Number of positive prediction: {y_pred.sum()} ({100*y_pred.mean():.2f}%)')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"y_pred = y_proba[:,1]\nresult = pd.DataFrame(columns = ['RefId','IsBadBuy'])\nresult['RefId'] = data['RefId']\nresult['IsBadBuy']= y_pred\nresult.to_csv('prediction.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Generate Prediction Result\n\nKaggle score: 0.239, rank around 110"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}