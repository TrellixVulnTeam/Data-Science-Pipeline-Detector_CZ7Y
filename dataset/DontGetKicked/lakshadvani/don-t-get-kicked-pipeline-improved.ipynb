{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Auction Kick Prediction - Laksh Advani**"},{"metadata":{},"cell_type":"markdown","source":"In this notebook we will explore the dataset to predict if the car purchased at the Auction is a good or bad buy."},{"metadata":{},"cell_type":"markdown","source":"## Import Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n#Pipeline\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.compose import make_column_transformer\n\n#For Missing Value and Feature Engineering\nfrom sklearn.feature_selection import SelectKBest, chi2, f_classif, VarianceThreshold\nfrom sklearn.impute import SimpleImputer, KNNImputer, MissingIndicator\nfrom sklearn.preprocessing import KBinsDiscretizer, OneHotEncoder, MinMaxScaler\nfrom sklearn.decomposition import PCA\n\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n\nimport time\n\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import GridSearchCV\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import train_test_split\nfrom collections import Counter\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nfrom sklearn.metrics import f1_score\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Import Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/DontGetKicked/training.csv\")\ntest = pd.read_csv(\"../input/DontGetKicked/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.describe().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ncategorical_cols = list(train.select_dtypes(include = ['object', 'category']).columns)\nnumerical_cols = list(train.select_dtypes(include = ['int64']).columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numerical_cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_cols","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above cells we can see that we have a variety of numerical and cateorical features. Out train set has 72983 examples and the test set has 48707. In the following cells we will try to flesh out some more information about these features to detect covariance."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(train['MMRAcquisitionAuctionAveragePrice'])\nsns.distplot(train['MMRCurrentAuctionAveragePrice'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(train['VehOdo'])\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that most of the cars have around 70000 miles on them. "},{"metadata":{"trusted":true},"cell_type":"code","source":"\nsns.pairplot(train, vars=['IsBadBuy','VehicleAge','VehOdo','MMRCurrentAuctionAveragePrice', 'MMRAcquisitionAuctionAveragePrice', 'VehOdo','WarrantyCost'], palette=\"husl\")\nplt.title('pairplot of the auction features')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the pairplot above we can see the relationships in the dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,15))\ncor = train.corr()\nsns.heatmap(cor, annot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Pearson correlation heatmap above gives us an idea about covariance in the dataset, from what we can see the cost variables ar ehighly correlated, we will make a note of this and investigate whether we need to drop these later. "},{"metadata":{"trusted":true},"cell_type":"code","source":"\nc = Counter(train['IsBadBuy'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see from the data above the classes are imbalanced so we need to ensure that we select a different metric instead of 'accuracy'. We also need to set class weights as 'balanced' with our models."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe().T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Divide Dataset into X and Y"},{"metadata":{"trusted":true},"cell_type":"code","source":"#create X and y datasets for splitting \nX = train.drop(['IsBadBuy','RefId'], axis=1)\n\n#X = train.drop(['IsBadBuy'], axis=1)\ny = train['IsBadBuy']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.describe().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#import train_test_split library\n\n# create train test split\nX_train, X_test, y_train, y_test = train_test_split( X,  y, test_size=0.3, random_state=0)  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.describe().T\n\nnumerical_features = [c for c, dtype in zip(X.columns, X.dtypes)\n                     if dtype.kind in ['i','f'] and c !='PassengerId']\ncategorical_features = [c for c, dtype in zip(X.columns, X.dtypes)\n                     if dtype.kind not in ['i','f']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Setup Pipeline "},{"metadata":{"trusted":true},"cell_type":"code","source":"preprocessor = make_column_transformer(\n    \n    (make_pipeline(\n    KNNImputer(n_neighbors=2, weights=\"uniform\"),\n    MinMaxScaler()), numerical_features),\n    \n    (make_pipeline(\n    SimpleImputer(strategy = 'constant', fill_value = 'missing'),\n    OneHotEncoder(categories = 'auto', handle_unknown = 'ignore')), categorical_features),\n    \n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preprocessor_best = make_pipeline(preprocessor, \n                                  VarianceThreshold(), \n                                  SelectKBest(f_classif, k = 50)\n                                 )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nRF_Model = make_pipeline(preprocessor_best, LGBMClassifier(class_weight=\"balanced\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Grid Search"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of trees in random forest\n\n\n\nn_estimators = [int(x) for x in np.linspace(start = 100, stop = 1000, num = 10)]\n# Number of features to consider at every split\n#max_features = ['auto', 'sqrt']\n#Maximum number of levels in tree\nmax_depth = [10,15,20,25,50]\n\nparam_grid = {'lgbmclassifier__n_estimators': n_estimators,\n               'lgbmclassifier__max_depth': max_depth\n               #'randomforestclassifier__min_samples_split': min_samples_split,\n               #'randomforestclassifier__min_samples_leaf': min_samples_leaf,\n               #'randomforestclassifier__bootstrap': bootstrap\n             }\n\nRF_Model = make_pipeline(preprocessor_best, LGBMClassifier(class_weight=\"balanced\"))\nrf_RandomGrid1 = GridSearchCV(estimator = RF_Model, param_grid = param_grid, cv = 3, verbose=1, n_jobs = -1,scoring = 'f1')\nrf_RandomGrid1.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_RandomGrid1.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"yhat = rf_RandomGrid1.best_estimator_.predict(X_test)\n\nprint(yhat)\n\n\n\nf1_score(yhat, y_test, average='macro')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ncm = confusion_matrix(yhat, y_test)\nprint(cm)\nfig = plt.figure()\nax = fig.add_subplot(111)\ncax = ax.matshow(cm)\nplt.title('Confusion matrix of the classifier')\nfig.colorbar(cax)\n\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n# sorted(zip(clf.feature_importances_, X.columns), reverse=True)\nfeature_imp = pd.DataFrame(sorted(zip(rf_RandomGrid1.best_estimator_._final_estimator.feature_importances_,X.columns)), columns=['Value','Feature'])\n\nplt.figure(figsize=(20, 10))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False))\nplt.title('LightGBM Features (avg over folds)')\nplt.tight_layout()\nplt.show()\nplt.savefig('lgbm_importances-01.png')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the plot above we can see that features like 'odometer', 'age', 'size', 'nationality' and 'color' are some of the most iportant features which influence the LightGBM model. \n\nGiven that there is significant class imbalance I decided to use the F-1 score as a metric for measuring the success of the model. \n\nApart from the LightGBM model I tried the Logistic Regression classifier which gave us a F-1 score of ~0.6.\n\nEven tho the aution price features were correlated, removing them did negatively affected the score of the model.\n\nFinally to summarizie, the F-1 score for the LightGBM model is 0.65. To improve upon this model I would look into feature transformation and try out more advanced models like Recurrent Neural Networks. \n\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}