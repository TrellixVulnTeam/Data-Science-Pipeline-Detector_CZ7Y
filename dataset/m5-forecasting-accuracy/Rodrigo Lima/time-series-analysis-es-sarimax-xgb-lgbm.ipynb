{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Kaggle Competition: M5 Forecasting \n## Estimate the unit sales of Walmart retail goods\n\n- Analise de Series Temporais\n\n- Autor: Rodrigo de Lima Oliveira\n\n- Referências:\n\n    - [DSA] https://www.datascienceacademy.com.br/\n    - [MLCOURSER.AI] https://mlcourse.ai/\n    - [KAGGLE] https://www.kaggle.com/kashnitsky/topic-9-part-1-time-series-analysis-in-python"},{"metadata":{},"cell_type":"markdown","source":"# Part 1. Carregando as bibliotecas"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Importando bibliotecas que serao utilizadas neste projeto\nimport pandas as pd\nimport numpy as np\n\nfrom itertools import product\nfrom multiprocessing import Pool\nfrom scipy.stats import kurtosis, skew\nfrom scipy.optimize import minimize\nimport scipy.stats as scs\nimport statsmodels.formula.api as smf\nimport statsmodels.tsa.api as smt\nimport statsmodels.api as sm\n\n# Stats\nfrom scipy import stats\nfrom scipy.stats import skew, norm\nfrom scipy.stats import randint as sp_randint\nfrom scipy.stats import uniform as sp_uniform\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport pickle\nimport datetime\nfrom dateutil.relativedelta import relativedelta \nimport time\nimport gc\nimport os\nfrom tqdm import tqdm_notebook\n\n# Ignorar warnings\nimport warnings\nwarnings.filterwarnings(action=\"ignore\")\n\n# Seta algumas opções no Jupyter para exibição dos datasets\npd.set_option('display.max_columns', 200)\npd.set_option('display.max_rows', 200)\n\n# Variavel para controlar o treinamento no Kaggle\nTRAIN_OFFLINE = False\n\n# Variavel para indicar o path local\nLOCAL_DATA_FOLDER  = 'data/'\nKAGGLE_DATA_FOLDER = '/kaggle/input/m5-forecasting-accuracy/'","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Importando bibliotecas do sklearn\nfrom sklearn import preprocessing, metrics\nfrom sklearn.preprocessing import MinMaxScaler, OneHotEncoder, LabelEncoder, StandardScaler\n\nfrom sklearn.metrics import r2_score, median_absolute_error, mean_absolute_error\nfrom sklearn.metrics import median_absolute_error, mean_squared_error, mean_squared_log_error\n\nfrom sklearn.model_selection import KFold, train_test_split, GridSearchCV, cross_val_score, TimeSeriesSplit\nfrom sklearn.linear_model import LinearRegression\n\n# lib de modelos de machine learning\nimport xgboost as xgb\nimport lightgbm as lgb","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Part 2. Importando os Dados"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Funcao para reducao da memoria utilizada\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df\n\n\n# Funcao para realizar a leitura dos arquivos LOCAL ou do KAGGLE\ndef read_data():\n    \n    # Se for local\n    if TRAIN_OFFLINE:\n\n        calendar               = pd.read_csv(os.path.join(LOCAL_DATA_FOLDER, 'calendar.csv'))\n        sell_prices            = pd.read_csv(os.path.join(LOCAL_DATA_FOLDER, 'sell_prices.csv'))\n        sales_train_validation = pd.read_csv(os.path.join(LOCAL_DATA_FOLDER, 'sales_train_validation.csv'))\n        submission             = pd.read_csv(os.path.join(LOCAL_DATA_FOLDER, 'sample_submission.csv'))\n\n    # Se estiver no ambiente do Kaggle\n    else:\n        \n        calendar               = pd.read_csv(os.path.join(KAGGLE_DATA_FOLDER, 'calendar.csv'))\n        sell_prices            = pd.read_csv(os.path.join(KAGGLE_DATA_FOLDER, 'sell_prices.csv'))\n        sales_train_validation = pd.read_csv(os.path.join(KAGGLE_DATA_FOLDER, 'sales_train_validation.csv'))\n        submission             = pd.read_csv(os.path.join(KAGGLE_DATA_FOLDER, 'sample_submission.csv'))\n\n    calendar               = reduce_mem_usage(calendar)\n    sell_prices            = reduce_mem_usage(sell_prices)\n    sales_train_validation = reduce_mem_usage(sales_train_validation)\n    submission             = reduce_mem_usage(submission)\n        \n    return calendar, sell_prices, sales_train_validation, submission\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Leitura dos dados e aplicando redução de memória\ncalendar, sell_prices, sales_train_validation, submission = read_data()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Part 3. Feature Engineering"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Funcao para realizar o merge dos datasets retornando apenas um dataframe\ndef reshape_and_merge(calendar, sell_prices, sales_train_validation, submission, nrows=55000000, merge=False):\n    \n    # realizando o reshape dos dados de venda usando melt\n    sales_train_validation = pd.melt(sales_train_validation, id_vars = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], var_name = 'day', value_name = 'demand')\n    print('Melted sales train validation tem {} linhas e {} colunas'.format(sales_train_validation.shape[0], sales_train_validation.shape[1]))\n    #sales_trian_validation = reduce_mem_usage(sales_train_validation)\n    \n    # separando os registros de teste e validacao\n    test_rows = [row for row in submission['id'] if 'validation' in row]\n    val_rows = [row for row in submission['id'] if 'evaluation' in row]\n    \n    test = submission[submission['id'].isin(test_rows)]\n    val = submission[submission['id'].isin(val_rows)]\n    \n    # renomeando as colunas\n    test.columns = ['id', 'd_1914', 'd_1915', 'd_1916', 'd_1917', 'd_1918', 'd_1919', 'd_1920', 'd_1921', \n                    'd_1922', 'd_1923', 'd_1924', 'd_1925', 'd_1926', 'd_1927', 'd_1928', 'd_1929', 'd_1930', \n                    'd_1931', 'd_1932', 'd_1933', 'd_1934', 'd_1935', 'd_1936', 'd_1937', 'd_1938', 'd_1939', \n                    'd_1940', 'd_1941']\n    val.columns = ['id', 'd_1942', 'd_1943', 'd_1944', 'd_1945', 'd_1946', 'd_1947', 'd_1948', 'd_1949', \n                   'd_1950', 'd_1951', 'd_1952', 'd_1953', 'd_1954', 'd_1955', 'd_1956', 'd_1957', 'd_1958', \n                   'd_1959', 'd_1960', 'd_1961', 'd_1962', 'd_1963', 'd_1964', 'd_1965', 'd_1966', 'd_1967', \n                   'd_1968', 'd_1969']\n    \n    # obtendo somente dados do produto e removendo registros duplicados \n    product = sales_train_validation[['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']].drop_duplicates()\n    \n    # realizando merge com a tabela de produto\n    test = test.merge(product, how = 'left', on = 'id')\n    val = val.merge(product, how = 'left', on = 'id')\n    \n    # realizando o reshape dos dados de test e validacao\n    test = pd.melt(test, id_vars = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], var_name = 'day', value_name = 'demand')\n    val = pd.melt(val, id_vars = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], var_name = 'day', value_name = 'demand')\n    \n    # criando uma nova coluna para definir dados de treino, teste e validacao\n    sales_train_validation['part'] = 'train'\n    test['part'] = 'test'\n    val['part'] = 'val'\n    \n    # criando um so dataset com a juncao de todos os registros de treino, validacao e teste\n    data = pd.concat([sales_train_validation, test, val], axis = 0)\n    \n    # removendo datasets anteriores\n    del sales_train_validation, test, val\n    \n    # selecionando somente alguns registros para treinamento\n    data = data.loc[nrows:]\n    \n    # removendo os dados de validacao\n    data = data[data['part'] != 'val']\n    \n    # realizando o merge com calendario e preco\n    if merge:\n        data = pd.merge(data, calendar, how = 'left', left_on = ['day'], right_on = ['d'])\n        data.drop(['d', 'day', 'weekday'], inplace = True, axis = 1)\n        data = data.merge(sell_prices, on = ['store_id', 'item_id', 'wm_yr_wk'], how = 'left')\n        print('Dataset final para treino tem {} linhas e {} colunas'.format(data.shape[0], data.shape[1]))\n    else: \n        pass\n    \n    return data\n\n# Funcao para tratamento valores missing transformacao das features categoricas e numericas\ndef transform(data):\n    \n    # realizando tratamento em valores missing nas features categoricas\n    nan_features_cat = ['event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']\n    for feature in nan_features_cat:\n        data[feature].fillna('unknown', inplace = True)\n    \n    # realizando tratamento em valores missing na feature sell_price\n    data['sell_price'].fillna(0, inplace = True)\n        \n    # transformando features categorias em numericas para realizar as previsoes\n    encoder = preprocessing.LabelEncoder()\n    data['id_encode'] = encoder.fit_transform(data['id'])\n    \n    cat = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', \n           'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']\n    for feature in cat:\n        encoder = preprocessing.LabelEncoder()\n        data[feature] = encoder.fit_transform(data[feature])\n    \n    return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\n\n# Realizando o reshape e o merge dos datasets\ndata = reshape_and_merge(calendar, sell_prices, sales_train_validation, submission, nrows=45000000, merge=True)\n\n# Chamando as funcoes de transformacao dos dados\ndata = transform(data)\n\n# Visualizando o cabecalho do dataset final\ndata.head()\n\n# Limpando dados da memória\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Verificando a data de inicio e fim do dataset\nprint(min(data['date']), max(data['date']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Part 4. Time Series em Python"},{"metadata":{},"cell_type":"markdown","source":"### Vamos dar uma olhada em como trabalhar com séries temporais usando esta competição do Kaggle. Então vamos analisar:\n- Métodos e modelos que podemos usar para previsões neste dataset\n- Aplicar suavização exponencial dupla e tripla\n- Analisar estacionariedade\n- Criar modelo SARIMA\n- Fazer previsões usando o xgboost e lightgbm"},{"metadata":{"trusted":false},"cell_type":"code","source":"data[data['id'] == 'FOODS_3_634_WI_2_validation'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"data[(data['id'] == 'FOODS_3_634_WI_2_validation') & (data['demand'] > 0) & (data['part'] == 'train')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Selecionar apenas os dados de treino e validação para as analises\n# Selecionando somente 1 item para testes: FOODS_3_634_WI_2\ndf = data[(data['date'] <= '2016-04-24') & (data['id'] == 'FOODS_3_634_WI_2_validation') & (data['demand'] > 0) & (data['demand'] <= 15)]\n\n# Selecionando apenas algumas colunas para a analise e treinamento\ndf = df[['date','demand','dept_id','cat_id','store_id','state_id','event_name_1','event_type_1','snap_WI','sell_price']]\n\n# Transformando a data como index \ndf = df.set_index('date')\n\n# Visualizando o resultado do dataset\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(24, 7))\nplt.plot(df['demand'])\nplt.title('Soma das Vendas por dia')\nplt.grid(True)\nplt.xticks(rotation=90)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Visualizando informações de distribuicao da variavel \"demand\"\nsns.set_style(\"white\")\nsns.set_color_codes(palette='deep')\nf, ax = plt.subplots(figsize=(12, 8))\n\n# Fit a normal distribution\nmu, std = norm.fit(df['demand'])\n\n# Verificando a distribuicao de frequencia da variavel \"demand\"\nsns.distplot(df['demand'], color=\"b\", fit = stats.norm)\nax.xaxis.grid(False)\nax.set(ylabel=\"Frequency\")\nax.set(xlabel=\"Demand\")\nax.set(title=\"Demand distribution: mu = %.2f,  std = %.2f\" % (mu, std))\nsns.despine(trim=True, left=True)\n\n# Adicionando Skewness e Kurtosis\nax.text(x=1.1, y=1, transform=ax.transAxes, s=\"Skewness: %f\" % df['demand'].skew(),\\\n        fontweight='demibold', fontsize=10, verticalalignment='top', horizontalalignment='right',\\\n        backgroundcolor='white', color='xkcd:poo brown')\nax.text(x=1.1, y=0.95, transform=ax.transAxes, s=\"Kurtosis: %f\" % df['demand'].kurt(),\\\n        fontweight='demibold', fontsize=10, verticalalignment='top', horizontalalignment='right',\\\n        backgroundcolor='white', color='xkcd:dried blood')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.1. Métricas para a previsão\n- Antes de começar a realizar as previsões, vamos entender como medir o desempenho de nossas previsões e dar uma olhada na métrica que será usada."},{"metadata":{},"cell_type":"markdown","source":"- [Mean Absolute Error](http://scikit-learn.org/stable/modules/model_evaluation.html#mean-absolute-error): essa é uma métrica interpretável porque tem a mesma unidade de medida que a série inicial, $[0, +\\infty)$\n\n$MAE = \\frac{\\sum\\limits_{i=1}^{n} |y_i - \\hat{y}_i|}{n}$ \n\n```python\nsklearn.metrics.mean_absolute_error\n```\n---\n\n- Mean Absolute Percentage Error: é o mesmo que o MAE, mas é calculado como uma porcentagem, o que é muito conveniente quando você deseja explicar a qualidade do modelo, $[0, +\\infty)$\n\n$MAPE = \\frac{100}{n}\\sum\\limits_{i=1}^{n} \\frac{|y_i - \\hat{y}_i|}{y_i}$ "},{"metadata":{"trusted":false},"cell_type":"code","source":"def mean_absolute_percentage_error(y_true, y_pred): \n    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n\ndef rmse(y_true, y_pred):\n    return np.sqrt(mean_squared_error(y_true, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.2. Mover, suavizar, avaliar\n\nVamos começar com uma hipótese: \"amanhã será o mesmo de hoje\". No entanto, através de um modelo como o $ \\ hat {y} _ {t} = y_ {t-1} $, assumiremos que o valor futuro de nossa variável depende da média dos valores anteriores de $ k $. Portanto, usaremos a **média móvel**.\n\n\n$\\hat{y}_{t} = \\frac{1}{k} \\displaystyle\\sum^{k}_{n=1} y_{t-n}$"},{"metadata":{"trusted":false},"cell_type":"code","source":"def moving_average(series, n):\n    \"\"\"\n        Calculate average of last n observations\n    \"\"\"\n    return np.average(series[-n:])\n\n#  realizando as previsões dos últimos 28 dias\nmoving_average(df, 28)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Infelizmente, não podemos fazer previsões do futuro - para obter o próximo valor, precisamos que os valores anteriores sejam realmente observados. Mas a média móvel tem outro uso - suavizar a série temporal original para identificar tendências. O Pandas tem uma implementação disponível com [`DataFrame.rolling (window).mean()`] (http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.rolling.html). Quanto maior a janela, mais suave a tendência. No caso de dados com muito ruído, geralmente encontrados em dados de finanças ou mercado de ações, esse procedimento pode ajudar a detectar padrões comuns."},{"metadata":{"trusted":false},"cell_type":"code","source":"def plotMovingAverage(series, window, plot_intervals=False, scale=1.96, plot_anomalies=False):\n\n    \"\"\"\n        series - dataframe with timeseries\n        window - rolling window size \n        plot_intervals - show confidence intervals\n        plot_anomalies - show anomalies \n\n    \"\"\"\n    rolling_mean = series.rolling(window=window).mean()\n\n    plt.figure(figsize=(15,5))\n    plt.title(\"Moving average\\n window size = {}\".format(window))\n    plt.plot(rolling_mean, \"g\", label=\"Rolling mean trend\")\n\n    # Plot confidence intervals for smoothed values\n    if plot_intervals:\n        mae = mean_absolute_error(series[window:], rolling_mean[window:])\n        deviation = np.std(series[window:] - rolling_mean[window:])\n        lower_bond = rolling_mean - (mae + scale * deviation)\n        upper_bond = rolling_mean + (mae + scale * deviation)\n        plt.plot(upper_bond, \"r--\", label=\"Upper Bond / Lower Bond\")\n        plt.plot(lower_bond, \"r--\")\n        \n        # Having the intervals, find abnormal values\n        if plot_anomalies:\n            anomalies = pd.DataFrame(index=series.index, columns=series.columns)\n            anomalies[series<lower_bond] = series[series<lower_bond]\n            anomalies[series>upper_bond] = series[series>upper_bond]\n            plt.plot(anomalies, \"ro\", markersize=10)\n        \n    plt.plot(series[window:], label=\"Actual values\")\n    plt.legend(loc=\"upper left\")\n    plt.grid(True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#  Vamos suavizar usando uma janela de 7 dias\nplotMovingAverage(df['demand'], 7)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#  Vamos suavizar usando uma janela de 28 dias\nplotMovingAverage(df['demand'], 28)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Quando aplicamos a suavização nos dados, podemos ver claramente a dinâmica das vendas no período."},{"metadata":{},"cell_type":"markdown","source":"Também podemos traçar os intervalos de confiança."},{"metadata":{"trusted":false},"cell_type":"code","source":"plotMovingAverage(df['demand'], 28, plot_intervals=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Weighted average** é uma modificação simples da média móvel. Os pesos somam `1` com pesos maiores atribuídos a observações mais recentes.\n\n\n$\\hat{y}_{t} = \\displaystyle\\sum^{k}_{n=1} \\omega_n y_{t+1-n}$"},{"metadata":{"trusted":false},"cell_type":"code","source":"def weighted_average(series, weights):\n    \"\"\"\n        Calculate weighter average on series\n    \"\"\"\n    result = 0.0\n    weights.reverse()\n    for n in range(len(weights)):\n        result += series.iloc[-n-1] * weights[n]\n    return float(result)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"weighted_average(df['demand'], [0.6, 0.3, 0.1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.3 Exponential smoothing"},{"metadata":{},"cell_type":"markdown","source":"Agora, vamos ver o que acontece se, em vez de ponderar os últimos valores de $ k $ da série temporal, começarmos a ponderar todas as observações disponíveis enquanto diminuímos exponencialmente os pesos à medida que avançamos mais no tempo. Existe uma fórmula para isso **[exponential smoothing (https://en.wikipedia.org/wiki/Exponential_smoothing)** que irá nos ajudar com esta fórmula:\n\n$$\\hat{y}_{t} = \\alpha \\cdot y_t + (1-\\alpha) \\cdot \\hat y_{t-1} $$\n\n\nAqui, o valor do modelo é uma média ponderada entre o valor verdadeiro atual e os valores anteriores do modelo. O peso $\\alpha$ é chamado de fator de suavização. Ele define a rapidez com que \"esqueceremos\" a última observação verdadeira. Quanto menor $\\alpha$, maior a influência das observações anteriores e mais suave será a série.\n\nA exponencialidade está oculta na recursividade da função - multiplicamos por $(1-\\alpha)$ a cada passada, o que já contém uma multiplicação por $(1-\\alpha)$ dos valores anteriores do modelo."},{"metadata":{"trusted":false},"cell_type":"code","source":"def exponential_smoothing(series, alpha):\n    \"\"\"\n        series - dataset with timestamps\n        alpha - float [0.0, 1.0], smoothing parameter\n    \"\"\"\n    result = [series[0]] # first value is same as series\n    for n in range(1, len(series)):\n        result.append(alpha * series[n] + (1 - alpha) * result[n-1])\n    return result","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def plotExponentialSmoothing(series, alphas):\n    \"\"\"\n        Plots exponential smoothing with different alphas\n        \n        series - dataset with timestamps\n        alphas - list of floats, smoothing parameters\n        \n    \"\"\"\n    with plt.style.context('seaborn-white'):    \n        plt.figure(figsize=(15, 7))\n        for alpha in alphas:\n            plt.plot(exponential_smoothing(series, alpha), label=\"Alpha {}\".format(alpha))\n        plt.plot(series.values, \"c\", label = \"Actual\")\n        plt.legend(loc=\"best\")\n        plt.axis('tight')\n        plt.title(\"Exponential Smoothing\")\n        plt.grid(True);","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plotExponentialSmoothing(df['demand'], [0.3, 0.05])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.4 Double exponential smoothing"},{"metadata":{},"cell_type":"markdown","source":"Até agora, os métodos apresentados foram para uma única previsão de um ponto futuro (com uma boa suavização). Isso é legal, mas também não é suficiente. Vamos estender a suavização exponencial para que possamos prever dois pontos futuros (e claro, também incluiremos mais suavização).\n\nA decomposição em série nos ajudará obtendo dois componentes: intercept (nível) $\\ell$ e slope (tendência) $b$. Aprendemos a prever intercept (ou valor esperado da série) com nossos métodos anteriores; agora, aplicaremos a mesma suavização exponencial à tendência, assumindo que a direção futura das mudanças na série temporal depende das alterações ponderadas anteriores. Como resultado, obtemos o seguinte conjunto de funções:\n\n$$\\ell_x = \\alpha y_x + (1-\\alpha)(\\ell_{x-1} + b_{x-1})$$\n\n$$b_x = \\beta(\\ell_x - \\ell_{x-1}) + (1-\\beta)b_{x-1}$$\n\n$$\\hat{y}_{x+1} = \\ell_x + b_x$$\n\n\nA primeira descreve o intercept, que depende do valor atual da série. O segundo termo agora está dividido em valores anteriores do nível e da tendência. A segunda função descreve a tendência, que depende das mudanças de nível na etapa atual e do valor anterior da tendência. Nesse caso, o coeficiente $\\beta$ é um peso para a suavização exponencial. A previsão final é a soma dos valores do modelo do intercept e da tendência."},{"metadata":{"trusted":false},"cell_type":"code","source":"def double_exponential_smoothing(series, alpha, beta):\n    \"\"\"\n        series - dataset with timeseries\n        alpha - float [0.0, 1.0], smoothing parameter for level\n        beta - float [0.0, 1.0], smoothing parameter for trend\n    \"\"\"\n    # first value is same as series\n    result = [series[0]]\n    for n in range(1, len(series)+1):\n        if n == 1:\n            level, trend = series[0], series[1] - series[0]\n        if n >= len(series): # forecasting\n            value = result[-1]\n        else:\n            value = series[n]\n        last_level, level = level, alpha*value + (1-alpha)*(level+trend)\n        trend = beta*(level-last_level) + (1-beta)*trend\n        result.append(level+trend)\n    return result","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def plotDoubleExponentialSmoothing(series, alphas, betas):\n    \"\"\"\n        Plots double exponential smoothing with different alphas and betas\n        \n        series - dataset with timestamps\n        alphas - list of floats, smoothing parameters for level\n        betas - list of floats, smoothing parameters for trend\n    \"\"\"\n    \n    with plt.style.context('seaborn-white'):    \n        plt.figure(figsize=(20, 8))\n        for alpha in alphas:\n            for beta in betas:\n                plt.plot(double_exponential_smoothing(series, alpha, beta), label=\"Alpha {}, beta {}\".format(alpha, beta))\n        plt.plot(series.values, label = \"Actual\")\n        plt.legend(loc=\"best\")\n        plt.axis('tight')\n        plt.title(\"Double Exponential Smoothing\")\n        plt.grid(True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plotDoubleExponentialSmoothing(df['demand'], alphas=[0.9, 0.02], betas=[0.9, 0.02])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Agora temos que ajustar dois parâmetros: $\\alpha$ e $\\beta$. O primeiro é responsável pela suavização da série em torno da tendência, a segunda pela suavização da própria tendência. Quanto maiores os valores, maior o peso das observações mais recentes e menos suavizada será a série."},{"metadata":{},"cell_type":"markdown","source":"## 4.5 Triple exponential smoothing a.k.a. Holt-Winters\n\nVimos a suavização exponencial e a dupla suavização exponencial. Desta vez, vamos entrar em suavização exponencial triplo.\n\n\nA idéia é adicionar um terceiro componente - a sazonalidade. Isso significa que não devemos usar esse método se não for esperado que nossa série temporal tenha sazonalidade. Os componentes sazonais do modelo explicam variações repetidas em torno da interceptação e tendência e serão especificadas pela duração da temporada, ou seja, pelo período após o qual as variações se repetem. Para cada observação do período, há um componente separado; por exemplo, se o período for de 28 dias (uma sazonalidade mensal), teremos 28 componentes sazonais, um para cada dia do mês.\n\nCom isso, vamos escrever um novo sistema de equações:\n\n$$\\ell_x = \\alpha(y_x - s_{x-L}) + (1-\\alpha)(\\ell_{x-1} + b_{x-1})$$\n\n$$b_x = \\beta(\\ell_x - \\ell_{x-1}) + (1-\\beta)b_{x-1}$$\n\n$$s_x = \\gamma(y_x - \\ell_x) + (1-\\gamma)s_{x-L}$$\n\n$$\\hat{y}_{x+m} = \\ell_x + mb_x + s_{x-L+1+(m-1)modL}$$\n\n\nO intercept agora depende do valor atual da série menos qualquer componente sazonal correspondente. A tendência permanece inalterada e o componente sazonal depende do valor atual da série menos o intercept e do valor anterior do componente. Leve em consideração que o componente é suavizado em todas os períodos disponíveis; por exemplo, se tivermos um componente de segunda-feira, será calculado apenas a média de outras segundas-feiras. Você pode ler mais sobre como a média funciona e como é feita a aproximação inicial da tendência e dos componentes sazonais [aqui] (http://www.itl.nist.gov/div898/handbook/pmc/section4/pmc435.htm). Agora que temos o componente sazonal, podemos prever não apenas um ou dois passos à frente, mas também um futuro arbitrário $m$, o que é muito bom.\n\nAbaixo está o código para um modelo triplo de suavização exponencial, também conhecido pelos sobrenomes de seus criadores, Charles Holt e seu aluno Peter Winters. Além disso, o método Brutlag foi incluído no modelo para produzir intervalos de confiança:\n\n\n$$\\hat y_{max_x}=\\ell_{x−1}+b_{x−1}+s_{x−T}+m⋅d_{t−T}$$\n\n$$\\hat y_{min_x}=\\ell_{x−1}+b_{x−1}+s_{x−T}-m⋅d_{t−T}$$\n\n$$d_t=\\gamma∣y_t−\\hat y_t∣+(1−\\gamma)d_{t−T},$$\n\nonde $T$ é a duração do período, $d$ é o desvio previsto. Outros parâmetros foram obtidos da tripla suavização exponencial. Você pode ler mais sobre o método e sua aplicabilidade à detecção de anomalias em séries temporais [aqui] (http://fedcsis.org/proceedings/2012/pliks/118.pdf)."},{"metadata":{"trusted":false},"cell_type":"code","source":"class HoltWinters:\n    \n    \"\"\"\n    Holt-Winters model with the anomalies detection using Brutlag method\n    \n    # series - initial time series\n    # slen - length of a season\n    # alpha, beta, gamma - Holt-Winters model coefficients\n    # n_preds - predictions horizon\n    # scaling_factor - sets the width of the confidence interval by Brutlag (usually takes values from 2 to 3)\n    \n    \"\"\"\n    \n    \n    def __init__(self, series, slen, alpha, beta, gamma, n_preds, scaling_factor=1.96):\n        self.series = series\n        self.slen = slen\n        self.alpha = alpha\n        self.beta = beta\n        self.gamma = gamma\n        self.n_preds = n_preds\n        self.scaling_factor = scaling_factor\n        \n        \n    def initial_trend(self):\n        sum = 0.0\n        for i in range(self.slen):\n            sum += float(self.series[i+self.slen] - self.series[i]) / self.slen\n        return sum / self.slen  \n    \n    def initial_seasonal_components(self):\n        seasonals = {}\n        season_averages = []\n        n_seasons = int(len(self.series)/self.slen)\n        # let's calculate season averages\n        for j in range(n_seasons):\n            season_averages.append(sum(self.series[self.slen*j:self.slen*j+self.slen])/float(self.slen))\n        # let's calculate initial values\n        for i in range(self.slen):\n            sum_of_vals_over_avg = 0.0\n            for j in range(n_seasons):\n                sum_of_vals_over_avg += self.series[self.slen*j+i]-season_averages[j]\n            seasonals[i] = sum_of_vals_over_avg/n_seasons\n        return seasonals   \n\n          \n    def triple_exponential_smoothing(self):\n        self.result = []\n        self.Smooth = []\n        self.Season = []\n        self.Trend = []\n        self.PredictedDeviation = []\n        self.UpperBond = []\n        self.LowerBond = []\n        \n        seasonals = self.initial_seasonal_components()\n        \n        for i in range(len(self.series)+self.n_preds):\n            if i == 0: # components initialization\n                smooth = self.series[0]\n                trend = self.initial_trend()\n                self.result.append(self.series[0])\n                self.Smooth.append(smooth)\n                self.Trend.append(trend)\n                self.Season.append(seasonals[i%self.slen])\n                \n                self.PredictedDeviation.append(0)\n                \n                self.UpperBond.append(self.result[0] + \n                                      self.scaling_factor * \n                                      self.PredictedDeviation[0])\n                \n                self.LowerBond.append(self.result[0] - \n                                      self.scaling_factor * \n                                      self.PredictedDeviation[0])\n                continue\n                \n            if i >= len(self.series): # predicting\n                m = i - len(self.series) + 1\n                self.result.append((smooth + m*trend) + seasonals[i%self.slen])\n                \n                # when predicting we increase uncertainty on each step\n                self.PredictedDeviation.append(self.PredictedDeviation[-1]*1.01) \n                \n            else:\n                val = self.series[i]\n                last_smooth, smooth = smooth, self.alpha*(val-seasonals[i%self.slen]) + (1-self.alpha)*(smooth+trend)\n                trend = self.beta * (smooth-last_smooth) + (1-self.beta)*trend\n                seasonals[i%self.slen] = self.gamma*(val-smooth) + (1-self.gamma)*seasonals[i%self.slen]\n                self.result.append(smooth+trend+seasonals[i%self.slen])\n                \n                # Deviation is calculated according to Brutlag algorithm.\n                self.PredictedDeviation.append(self.gamma * np.abs(self.series[i] - self.result[i]) \n                                               + (1-self.gamma)*self.PredictedDeviation[-1])\n                     \n            self.UpperBond.append(self.result[-1] + \n                                  self.scaling_factor * \n                                  self.PredictedDeviation[-1])\n\n            self.LowerBond.append(self.result[-1] - \n                                  self.scaling_factor * \n                                  self.PredictedDeviation[-1])\n\n            self.Smooth.append(smooth)\n            self.Trend.append(trend)\n            self.Season.append(seasonals[i%self.slen])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.6 Time series cross validation\n\nAntes de começar a construir um modelo, vamos verificar como estimar os parâmetros do modelo automaticamente.\n\nNão há nada incomum aqui; como sempre, temos que escolher uma função de perda adequada para a tarefa que nos dirá quão próximo o modelo se aproxima dos dados. Em seguida, usando a validação cruzada, avaliaremos nossa função de perda escolhida para os parâmetros de modelo fornecidos, calcularemos o gradiente, ajustamos os parâmetros do modelo e assim por diante, eventualmente descendo para o mínimo global.\n\n\nVocê pode estar se perguntando como fazer a validação cruzada para séries temporais porque as séries temporais possuem essa estrutura temporal e não é possível misturar valores aleatoriamente sem preserva essa estrutura. Com a randomização, todas as dependências de tempo entre as observações serão perdidas. É por isso que teremos que usar uma abordagem mais complicada para otimizar os parâmetros do modelo. Não sei se existe um nome oficial para isso, mas em [CrossValidated] (https://stats.stackexchange.com/questions/14099/using-k-fold-cross-validation-for-time-series-model-selection), é possível encontrar algumas respostas para esse método que é \"validação cruzada em uma base contínua\".\n\nA idéia é bastante simples - treinamos nosso modelo em um pequeno segmento da série cronológica desde o início até $t$, fazemos previsões para as próximas etapas de $t+n$ e calculamos um erro. Em seguida, expandimos nossa amostra de treinamento para o valor $t+n$, fazemos previsões de $t+n$ até $t+2*n$ e continuamos movendo janela de teste da série cronológica até atingirmos a última observação disponível. Como resultado, temos muitas amostras $n$ entre a amostra de treinamento inicial e a última observação.\n\n<img src=\"https://habrastorage.org/files/f5c/7cd/b39/f5c7cdb39ccd4ba68378ca232d20d864.png\"/>"},{"metadata":{},"cell_type":"markdown","source":"Agora, sabendo como configurar a validação cruzada, podemos encontrar os parâmetros ideais para o modelo Holt-Winters. Lembre-se de que temos sazonalidade diária, daí o parâmetro slen = 1."},{"metadata":{"trusted":false},"cell_type":"code","source":"def timeseriesCVscore(params, series, loss_function=mean_squared_error, slen=28):\n    \"\"\"\n        Returns error on CV  \n        \n        params - vector of parameters for optimization\n        series - dataset with timeseries\n        slen - season length for Holt-Winters model\n    \"\"\"\n    # errors array\n    errors = []\n    \n    values = series.values\n    alpha, beta, gamma = params\n    \n    # set the number of folds for cross-validation\n    tscv = TimeSeriesSplit(n_splits=2) \n    \n    # iterating over folds, train model on each, forecast and calculate error\n    for train, test in tscv.split(values):\n\n        model = HoltWinters(series=values[train], \n                            slen=slen, \n                            alpha=alpha, \n                            beta=beta, \n                            gamma=gamma, \n                            n_preds=len(test))\n        \n        model.triple_exponential_smoothing()\n        \n        predictions = model.result[-len(test):]\n        actual = values[test]\n        \n        error = loss_function(predictions, actual)\n        errors.append(error)\n        \n    return np.mean(np.array(errors))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\n\nnew_data = df['demand']\n\n# initializing model parameters alpha, beta and gamma\nx = [0, 0, 0] \n\n# Minimizing the loss function \nopt = minimize(timeseriesCVscore, \n               x0=x, \n               args=(new_data, mean_squared_error), \n               method=\"TNC\", \n               bounds = ((0, 1), (0, 1), (0, 1))\n              )\n\n# Take optimal values...\nalpha_final, beta_final, gamma_final = opt.x\nprint(alpha_final, beta_final, gamma_final)\n\n# ...and train the model with them, forecasting for the next 28 days\nmodel = HoltWinters(new_data, \n                    slen = 28, \n                    alpha = alpha_final, \n                    beta = beta_final, \n                    gamma = gamma_final, \n                    n_preds = 28, \n                    scaling_factor = 3)\n\nmodel.triple_exponential_smoothing()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def plotHoltWinters(series, plot_intervals=False, plot_anomalies=False):\n    \"\"\"\n        series - dataset with timeseries\n        plot_intervals - show confidence intervals\n        plot_anomalies - show anomalies \n    \"\"\"\n    \n    plt.figure(figsize=(20, 10))\n    plt.plot(model.result, label = \"Model\")\n    plt.plot(series.values, label = \"Actual\")\n    \n    #error = mean_absolute_percentage_error(series.values, model.result[:len(series)])\n    #plt.title(\"Mean Absolute Percentage Error: {0:.2f}%\".format(error))\n    \n    error = rmse(series.values, model.result[:len(series)])\n    plt.title(\"RMSE: {0:.2f}\".format(error))\n    \n    if plot_anomalies:\n        anomalies = np.array([np.NaN]*len(series))\n        anomalies[series.values<model.LowerBond[:len(series)]] = \\\n            series.values[series.values<model.LowerBond[:len(series)]]\n        anomalies[series.values>model.UpperBond[:len(series)]] = \\\n            series.values[series.values>model.UpperBond[:len(series)]]\n        plt.plot(anomalies, \"o\", markersize=10, label = \"Anomalies\")\n    \n    if plot_intervals:\n        plt.plot(model.UpperBond, \"r--\", alpha=0.5, label = \"Up/Low confidence\")\n        plt.plot(model.LowerBond, \"r--\", alpha=0.5)\n        plt.fill_between(x=range(0,len(model.result)), y1=model.UpperBond, \n                         y2=model.LowerBond, alpha=0.2, color = \"grey\")    \n        \n    plt.vlines(len(series), ymin=min(model.LowerBond), ymax=max(model.UpperBond), linestyles='dashed')\n    plt.axvspan(len(series)-60, len(model.result), alpha=0.3, color='lightgrey')\n    plt.grid(True)\n    plt.axis('tight')\n    plt.legend(loc=\"best\", fontsize=13);","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plotHoltWinters(df['demand'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plotHoltWinters(df['demand'], plot_intervals=True, plot_anomalies=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A julgar pelos gráficos, nosso modelo conseguiu aproximar com sucesso as séries temporais iniciais, capturando a sazonalidade diária, a tendência geral de queda e até algumas anomalias. Se você observar os desvios do modelo, poderá ver claramente que o modelo reage bastante às mudanças na estrutura da série, mas depois retorna rapidamente o desvio aos valores normais, essencialmente \"esquecendo\" o passado. Esse recurso do modelo nos permite criar rapidamente sistemas de detecção de anomalias, mesmo para ruídos nos dados da série, sem gastar muito tempo na preparação dos dados e no treinamento do modelo."},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(25, 5))\nplt.plot(model.PredictedDeviation)\nplt.grid(True)\nplt.axis('tight')\nplt.title(\"Brutlag's predicted deviation\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.7 Abordagem Econometrica"},{"metadata":{},"cell_type":"markdown","source":"### Estacionariedade\n\n\nAntes de começar, devo mencionar uma propriedade importante da série temporal: [**stationarity**] (https://en.wikipedia.org/wiki/Stationary_process).\n\nSe um processo é estacionário, isso significa que ele não altera suas propriedades estatísticas ao longo do tempo, ou seja, sua média e variância. (A constância da variação é chamada de [homoscedasticidade] (https://en.wikipedia.org/wiki/Homoscedasticity)) A função de covariância não depende do tempo, deve depender apenas da distância entre as observações. Você pode ver issonas imagens do post de [Sean Abu] (http://www.seanabu.com/2016/03/22/time-series-seasonal-ARIMA-model-in-python/):\n\n- O gráfico vermelho abaixo não é estacionário porque a média aumenta com o tempo.\n\n<img src=\"https://habrastorage.org/files/20c/9d8/a63/20c9d8a633ec436f91dccd4aedcc6940.png\"/>\n\n- Observando a variação de valores ao longo do tempo\n\n<img src=\"https://habrastorage.org/files/b88/eec/a67/b88eeca676d642449cab135273fd5a95.png\"/>\n\n- Finalmente, a covariância do i termo e do (i + m) termo não deve ser uma função do tempo. No gráfico a seguir, você notará que o spread se aproxima à medida que o tempo aumenta. Portanto, a covariância não é constante como no gráfico da direita.\n\n\n<img src=\"https://habrastorage.org/files/2f6/1ee/cb2/2f61eecb20714352840748b826e38680.png\"/>\n\nEntão, por que a estacionariedade é tão importante? Porque é fácil fazer previsões em uma série estacionária, pois podemos assumir que as propriedades estatísticas futuras não serão diferentes daquelas atualmente observadas. A maioria dos modelos de séries temporais, de uma maneira ou de outra, tenta prever essas propriedades (média ou variação, por exemplo). As previsões futuras estariam erradas se a série original não estivesse estacionária. Infelizmente, a maioria das séries temporais que vemos fora dos livros didáticos não é estacionária, mas podemos (e devemos) mudar isso.\n\nEntão, vamos ver como detectar a não estacionariedade. Analisaremos o ruído branco e os randow walks para aprender a passar de um para outro."},{"metadata":{},"cell_type":"markdown","source":"## 4.8 Tratar a Não Estacionariedade e construir o modelo SARIMA"},{"metadata":{},"cell_type":"markdown","source":"Vamos construir um modelo ARIMA, percorrendo todos os estágios para tornar uma série estacionária."},{"metadata":{},"cell_type":"markdown","source":"Aqui está o código para renderizar os gráficos."},{"metadata":{"trusted":false},"cell_type":"code","source":"def tsplot(y, lags=None, figsize=(12, 7), style='bmh'):\n    \"\"\"\n        Plot time series, its ACF and PACF, calculate Dickey–Fuller test\n        \n        y - timeseries\n        lags - how many lags to include in ACF, PACF calculation\n    \"\"\"\n    if not isinstance(y, pd.Series):\n        y = pd.Series(y)\n        \n    with plt.style.context(style):    \n        fig = plt.figure(figsize=figsize)\n        layout = (2, 2)\n        ts_ax = plt.subplot2grid(layout, (0, 0), colspan=2)\n        acf_ax = plt.subplot2grid(layout, (1, 0))\n        pacf_ax = plt.subplot2grid(layout, (1, 1))\n        \n        y.plot(ax=ts_ax)\n        p_value = sm.tsa.stattools.adfuller(y)[1]\n        ts_ax.set_title('Time Series Analysis Plots\\n Dickey-Fuller: p={0:.5f}'.format(p_value))\n        smt.graphics.plot_acf(y, lags=lags, ax=acf_ax)\n        smt.graphics.plot_pacf(y, lags=lags, ax=pacf_ax)\n        plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"tsplot(df['demand'], lags=60)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Surpreendentemente, as séries iniciais são estacionárias; o teste de Dickey-Fuller rejeitou a hipótese nula de que uma raiz unitária está presente. Na verdade, podemos ver isso no próprio gráfico - não temos uma tendência visível; portanto, a média é constante e a variação é praticamente estável. A única coisa que resta é a sazonalidade, com a qual temos que lidar antes da modelagem. Para fazer isso, vamos usar a \"diferença sazonal\", que significa uma simples subtração da série de si mesma com um atraso igual ao período sazonal."},{"metadata":{"trusted":false},"cell_type":"code","source":"df_diff = df['demand'] - df['demand'].shift(28)\ntsplot(df_diff[28:], lags=60)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Agora é muito melhor com a sazonalidade visível desaparecida."},{"metadata":{},"cell_type":"markdown","source":"## 4.9 Família ARIMA\n\nExplicarei esse modelo construindo letra por letra. $SARIMA(p, d, q)(P, D, Q, s)$, Média móvel sazonal de regressão automática (Seasonal Autoregression Moving Average model):\n\n\n- $AR(p)$ - modelo de regressão automática, isto é, regressão da série temporal em si mesma. A suposição básica é que os valores da série atual dependem de seus valores anteriores com algum atraso (ou vários atrasos). O atraso máximo no modelo é referido como $p$. Para determinar o $p$ inicial, é necessário examinar o gráfico do PACF e encontrar o maior atraso significativo o qual a maioria dos outros atrasos se torna insignificante.\n\n\n- $ MA(q)$ - modelo de média móvel. Sem entrar em muitos detalhes, modela o erro da série cronológica, novamente com a suposição de que o erro atual depende do anterior com algum atraso, conhecido como $q$. O valor inicial pode ser encontrado no gráfico ACF com a mesma lógica de antes.\n\n\nVamos combinar nossas 4 primeiras letras:\n\n$AR(p) + MA(q) = ARMA(p, q)$\n\n\nO que temos aqui é o modelo de média móvel autorregressiva! Se a série é estacionária, pode ser aproximada com estas 4 letras. Vamos continuar.\n\n\n- $I(d)$ - ordem de integração. Este é simplesmente o número de diferenças não sazonais necessárias para tornar a série estacionária. No nosso caso, é apenas 1 porque usamos as primeiras diferenças.\n\nA adição desta letra às quatro nos dá o modelo $ARIMA$, que pode manipular dados não estacionários com a ajuda de diferenças não sazonais. Ótimo, mais uma letra pela frente!\n\n\n- $S(s)$ - é responsável pela sazonalidade e igual à duração do tamanho do período da série\n\nCom isso, temos três parametros: $(P, D, Q)$\n\n- $P$ - ordem de regressão automática para o componente sazonal do modelo, que pode ser derivado do PACF. Mas você precisa observar o número de atrasos significativos, que são os múltiplos da duração do período da serie. Por exemplo, se o período é igual a 28 dias e vemos que os atrasos de 28 e 56 são significativas no PACF, isso significa que o $P$ inicial deve ser 2.\n\n\n- $Q$ - lógica semelhante usando o gráfico ACF.\n\n\n- $D$ - ordem de integração sazonal. Isso pode ser igual a 1 ou 0, dependendo se as diferenças sazonais foram aplicadas ou não."},{"metadata":{},"cell_type":"markdown","source":"Agora que sabemos como definir os parâmetros iniciais, vamos dar uma olhada no gráfico final mais uma vez e definir os parâmetros:"},{"metadata":{"trusted":false},"cell_type":"code","source":"tsplot(df_diff[28+1:], lags=60)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- $p$ é provavelmente 4, já que é o atraso mais significativo no PACF\n- $d$ é igual a 1 porque tivemos as primeiras diferenças\n- $q$ deve estar em torno de 4, assim como no ACF\n- $P$ pode ser 2, uma vez que os atrasos de 28 e 56 são um pouco significativas no PACF\n- $D$ novamente é igual a 1 porque realizamos diferenciação sazonal\n- $Q$ é provavelmente 1. O 28º atraso no ACF é significativo, enquanto o 56º não."},{"metadata":{},"cell_type":"markdown","source":"Vamos testar vários modelos e ver qual é o melhor."},{"metadata":{"trusted":false},"cell_type":"code","source":"# setting initial values and some bounds for them\nps = range(2, 3)\nd=1 \nqs = range(2, 3)\nPs = range(0, 2)\nD=1 \nQs = range(0, 2)\ns = 28 # season length is still 28\n\n# creating list with all the possible combinations of parameters\nparameters = product(ps, qs, Ps, Qs)\nparameters_list = list(parameters)\nlen(parameters_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def optimizeSARIMA(parameters_list, d, D, s):\n    \"\"\"\n        Return dataframe with parameters and corresponding AIC\n        \n        parameters_list - list with (p, q, P, Q) tuples\n        d - integration order in ARIMA model\n        D - seasonal integration order \n        s - length of season\n    \"\"\"\n    \n    results = []\n    best_aic = float(\"inf\")\n\n    for param in tqdm_notebook(parameters_list):\n        # we need try-except because on some combinations model fails to converge\n        try:\n            model=sm.tsa.statespace.SARIMAX(df['demand'], \n                                            order=(param[0], d, param[1]), \n                                            seasonal_order=(param[2], D, param[3], s)).fit(disp=-1)\n        except:\n            continue\n        aic = model.aic\n        # saving best model, AIC and parameters\n        if aic < best_aic:\n            best_model = model\n            best_aic = aic\n            best_param = param\n        results.append([param, model.aic])\n\n    result_table = pd.DataFrame(results)\n    result_table.columns = ['parameters', 'aic']\n    # sorting in ascending order, the lower AIC is - the better\n    result_table = result_table.sort_values(by='aic', ascending=True).reset_index(drop=True)\n    \n    return result_table","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\nresult_table = optimizeSARIMA(parameters_list, d, D, s)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"result_table.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# set the parameters that give the lowest AIC\np, q, P, Q = result_table.parameters[0]\n\nbest_model=sm.tsa.statespace.SARIMAX(df['demand'], \n                                     order=(p, d, q),\n                                     seasonal_order=(P, D, Q, s)).fit(disp=-1)\nprint(best_model.summary())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Vamos inspecionar os resíduos do modelo."},{"metadata":{"trusted":false},"cell_type":"code","source":"tsplot(best_model.resid[28+1:], lags=60)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"É claro que os resíduos são estacionários e não há autocorrelações aparentes. Vamos fazer previsões usando nosso modelo."},{"metadata":{"trusted":false},"cell_type":"code","source":"def plotSARIMA(series, model, n_steps):\n    \"\"\"\n        Plots model vs predicted values\n        \n        series - dataset with timeseries\n        model - fitted SARIMA model\n        n_steps - number of steps to predict in the future\n        \n    \"\"\"\n    # adding model values\n    dfCopy = series.copy()\n    \n    dfCopy['arima_model'] = model.fittedvalues\n    # making a shift on s+d steps, because these values were unobserved by the model\n    # due to the differentiating\n    dfCopy['arima_model'][:s+d] = np.NaN    \n    \n    # forecasting on n_steps forward \n    forecast = model.predict(start = dfCopy.shape[0], end = dfCopy.shape[0]+n_steps)\n    forecast = dfCopy['arima_model'].append(forecast)\n\n    # calculate error, again having shifted on s+d steps from the beginning\n    error = rmse(dfCopy['demand'][s+d:], dfCopy['arima_model'][s+d:])\n\n    plt.figure(figsize=(15, 7))\n    plt.title(\"RMSE: {0:.2f}\".format(error))\n    #plt.plot(forecast, color='r', label=\"model\")\n    #plt.axvspan(dfCopy.index[-1], forecast.index[-1], alpha=0.5, color='lightgrey')\n    plt.plot(dfCopy['demand'], label=\"actual\")\n    plt.legend()\n    plt.grid(True);","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plotSARIMA(df, best_model, 28)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No final, obtivemos previsões muito boas. Nosso modelo errou 2.96 (RMSE), em média, o que é muito, muito bom. No entanto, os custos gerais da preparação dos dados, da preparação da série e da seleção de parâmetros podem não valer essa precisão."},{"metadata":{},"cell_type":"markdown","source":"# Part 5. Modelos lineares em séries temporais\n\n\nEsses modelos analisados até aqui demandam muito tempo para a preparação de dados (como no SARIMA) ou exigem treinamento frequente sobre novos dados (novamente, SARIMA) ou são difíceis de ajustar (bom exemplo - SARIMA). Portanto, muitas vezes é muito mais fácil selecionar algumas features das séries temporais existentes e criar um modelo de regressão linear simples ou, por exemplo, uma random forest.\n\nEssa abordagem não é apoiada pela teoria e quebra várias suposições (por exemplo, o teorema de Gauss-Markov, especialmente para erros não correlacionados), mas é muito útil na prática e é frequentemente usada em competições.\n"},{"metadata":{},"cell_type":"markdown","source":"## 5.1 Feature Extraction"},{"metadata":{},"cell_type":"markdown","source":"Quais as features podemos extrair deste dataset?\n* Lags of time series\n* Window statistics:\n    - Max/min value of series in a window\n    - Average/median value in a window\n    - Window variance\n    - etc.\n* Date and time features:\n    - Eventos especiais\n    - Dia da semana, mês, ano,...\n    - Feriados\n    - etc.\n* Target encoding \n* Forecasts de outros modelos (note que podemos perder o desempenho da previsão dessa maneira)"},{"metadata":{},"cell_type":"markdown","source":"Vamos analisar alguns dos métodos e ver o que podemos extrair dos dados desta série temporal da competição\n\n## 5.2 Atrasos (lags) da série temporal\n\nMudando a série $n$ para trás, obtemos uma feature em que o valor atual da série temporal está alinhado com seu valor no tempo $t-n$. Se fizermos uma mudança de 1 lag e treinarmos um modelo com essa nova feature, o modelo poderá prever um passo à frente e observar o estado atual da série. Aumentar o atraso (lag), digamos, até 28, permitirá que o modelo faça previsões 28 passos à frente; no entanto, usará os dados observados nas 28 etapas para trás. Se algo mudar fundamentalmente a série durante esse período não observado, o modelo não capturará essas alterações e retornará previsões com um grande erro. Portanto, durante a seleção do atraso inicial, é preciso encontrar um equilíbrio entre a qualidade ideal da previsão e a duração de previsão."},{"metadata":{"trusted":false},"cell_type":"code","source":"# Criar uma copia do dataset original\nnew_df = df.copy()\n\n# Visualizando o dataset\nnew_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Adicionando features considerando o atraso da demanda de 7 a 28 dias\nfor i in range(7, 29):\n    new_df[\"lag_{}\".format(i)] = new_df['demand'].shift(i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Visualizando o resultado do dataset\nnew_df.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ótimo, geramos um conjunto de dados aqui. Por que agora não treinamos um modelo?"},{"metadata":{"trusted":false},"cell_type":"code","source":"# para o cross-validation da serie temporal\ntscv = TimeSeriesSplit(n_splits=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def timeseries_train_test_split(X, y, test_size):\n    \"\"\"\n        Perform train-test split with respect to time series structure\n    \"\"\"\n    \n    # get the index after which test set starts\n    test_index = int(len(X)*(1-test_size))\n    \n    X_train = X.iloc[:test_index]\n    y_train = y.iloc[:test_index]\n    X_test  = X.iloc[test_index:]\n    y_test  = y.iloc[test_index:]\n    \n    return X_train, X_test, y_train, y_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"y = new_df.dropna()['demand']\nX = new_df.dropna().drop(['demand'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# split com 10% para dados de teste\nX_train, X_test, y_train, y_test = timeseries_train_test_split(X, y, test_size=0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# machine learning em 2 linhas\nlr = LinearRegression()\nlr.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def plotModelResults(model, X_train=X_train, X_test=X_test, plot_intervals=False, plot_anomalies=False):\n    \"\"\"\n        Plots modelled vs fact values, prediction intervals and anomalies\n    \n    \"\"\"\n    \n    prediction = model.predict(X_test)\n    \n    plt.figure(figsize=(15, 7))\n    plt.plot(y_test.values, label=\"actual\", linewidth=2.0)\n    plt.plot(prediction, \"g\", label=\"prediction\", linewidth=2.0)\n    \n    if plot_intervals:\n        cv = cross_val_score(model, X_train, y_train, \n                                    cv=tscv, \n                                    scoring=\"neg_mean_absolute_error\")\n        mae = cv.mean() * (-1)\n        deviation = cv.std()\n        \n        scale = 1.96\n        lower = prediction - (mae + scale * deviation)\n        upper = prediction + (mae + scale * deviation)\n        \n        plt.plot(lower, \"r--\", label=\"upper bond / lower bond\", alpha=0.5)\n        plt.plot(upper, \"r--\", alpha=0.5)\n        \n        if plot_anomalies:\n            anomalies = np.array([np.NaN]*len(y_test))\n            anomalies[y_test<lower] = y_test[y_test<lower]\n            anomalies[y_test>upper] = y_test[y_test>upper]\n            plt.plot(anomalies, \"o\", markersize=10, label = \"Anomalies\")\n    \n    error = rmse(y_test, prediction)\n    plt.title(\"RMSE: {0:.2f}\".format(error))\n    plt.legend(loc=\"best\")\n    plt.tight_layout()\n    plt.grid(True);\n    \ndef plotCoefficients(model):\n    \"\"\"\n        Plots sorted coefficient values of the model\n    \"\"\"\n    \n    coefs = pd.DataFrame(model.coef_, X_train.columns)\n    coefs.columns = [\"coef\"]\n    coefs[\"abs\"] = coefs.coef.apply(np.abs)\n    coefs = coefs.sort_values(by=\"abs\", ascending=False).drop([\"abs\"], axis=1)\n    \n    plt.figure(figsize=(15, 7))\n    coefs.coef.plot(kind='bar')\n    plt.grid(True, axis='y')\n    plt.hlines(y=0, xmin=0, xmax=len(coefs), linestyles='dashed');","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plotModelResults(lr, plot_intervals=True)\nplotCoefficients(lr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Os atrasos (lags) simples e a regressão linear nos deram previsões  distantes do SARIMA em termos de qualidade. Como existem muitas features desnecessárias, faremos a Feature Selection daqui a pouco. Por enquanto, vamos continuar a engenharia!"},{"metadata":{},"cell_type":"markdown","source":"Adicionaremos algumas features de data. Para fazer isso, precisamos transformar a data no formato `datetime`."},{"metadata":{"trusted":false},"cell_type":"code","source":"new_df.index = pd.to_datetime(new_df.index)\nnew_df[\"day\"] = new_df.index.day\nnew_df[\"weekday\"] = new_df.index.weekday\nnew_df['is_weekend'] = new_df.weekday.isin([5,6])*1\nnew_df.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Podemos visualizar o resultado:"},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(16, 5))\nplt.title(\"Encoded features\")\nnew_df['day'].plot()\nnew_df['weekday'].plot()\nnew_df['is_weekend'].plot()\nplt.grid(True);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Como agora temos escalas diferentes em nossas features, precisamos transformá-los na mesma escala para explorar a importância, posteriormente, a regularização."},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X = new_df.dropna().drop(['demand'], axis=1)\n\nX_train, X_test, y_train, y_test = timeseries_train_test_split(X, y, test_size=0.1)\n\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nlr = LinearRegression()\nlr.fit(X_train_scaled, y_train)\n\nplotModelResults(lr, X_train=X_train_scaled, X_test=X_test_scaled, plot_intervals=True)\nplotCoefficients(lr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"O erro permaneceu o mesmo."},{"metadata":{},"cell_type":"markdown","source":"## 5.3 Target encoding\n\nGostaria de adicionar outra variante para codificar features categóricas: encoding por valor médio. Se não for desejável criar um conjunto de dados usando muitas variáveis dummy que podem levar à perda de informações e se elas não puderem ser usadas como valores reais devido a conflitos como \"0 horas < 23 horas\", então é possível codificar uma variável com valores um pouco mais interpretáveis. A idéia natural é codificar com o valor médio da variável de destino. No nosso exemplo, todos os dias da semana podem ser codificados pelo número médio correspondente de vendas durante esse dia. É muito importante garantir que o valor médio seja calculado apenas no conjunto de treinamento (ou apenas na validação cruzada atual), para que o modelo conheça o futuro."},{"metadata":{"trusted":false},"cell_type":"code","source":"def code_mean(data, cat_feature, real_feature):\n    \"\"\"\n    Returns a dictionary where keys are unique categories of the cat_feature,\n    and values are means over real_feature\n    \"\"\"\n    return dict(data.groupby(cat_feature)[real_feature].mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"average_day = code_mean(new_df, 'day', \"demand\")\nplt.figure(figsize=(12, 5))\nplt.title(\"Médias diárias\")\npd.DataFrame.from_dict(average_day, orient='index')[0].plot()\nplt.grid(True);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finalmente, vamos juntar todas as transformações em uma única função."},{"metadata":{"trusted":false},"cell_type":"code","source":"def prepareData(series, lag_start, lag_end, test_size, target_encoding=False):\n    \"\"\"\n        series: pd.DataFrame\n            dataframe with timeseries\n\n        lag_start: int\n            initial step back in time to slice target variable \n            example - lag_start = 1 means that the model \n                      will see yesterday's values to predict today\n\n        lag_end: int\n            final step back in time to slice target variable\n            example - lag_end = 4 means that the model \n                      will see up to 4 days back in time to predict today\n\n        test_size: float\n            size of the test dataset after train/test split as percentage of dataset\n\n        target_encoding: boolean\n            if True - add target averages to the dataset\n        \n    \"\"\"\n    \n    # copy of the initial dataset\n    new_df = df.copy()\n\n    # lags of series\n    for i in range(7, 29):\n        new_df[\"lag_{}\".format(i)] = new_df['demand'].shift(i)\n\n    # datetime features\n    new_df.index = pd.to_datetime(new_df.index)\n    new_df[\"day\"] = new_df.index.day\n    new_df[\"weekday\"] = new_df.index.weekday\n    new_df['is_weekend'] = new_df.weekday.isin([5,6])*1\n\n    if target_encoding:\n        # calculate averages on train set only\n        test_index = int(len(new_df.dropna())*(1-test_size))\n        new_df['weekday_average'] = list(map(code_mean(new_df[:test_index], 'weekday', \"demand\").get, new_df['weekday']))\n        new_df[\"day_average\"] = list(map(code_mean(new_df[:test_index], 'day', \"demand\").get, new_df['day']))\n\n        # frop encoded variables \n        new_df.drop([\"day\", \"weekday\"], axis=1, inplace=True)\n    \n    # train-test split\n    y = new_df.dropna()['demand']\n    X = new_df.dropna().drop(['demand'], axis=1)\n\n    X_train, X_test, y_train, y_test = timeseries_train_test_split(X, y, test_size=test_size)\n\n    return X_train, X_test, y_train, y_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X_train, X_test, y_train, y_test =\\\nprepareData(df, lag_start=1, lag_end=29, test_size=0.1, target_encoding=True)\n\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nlr = LinearRegression()\nlr.fit(X_train_scaled, y_train)\n\nplotModelResults(lr, X_train=X_train_scaled, X_test=X_test_scaled, plot_intervals=True, plot_anomalies=True)\nplotCoefficients(lr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Aparentemente vemos Overfitting! Algumas features tiveram uma importancia negativa tão grande no conjunto de dados de treinamento que o modelo decidiu concentrar todas as suas forças nele. Como resultado, a qualidade da previsão caiu. Esse problema pode ser resolvido de várias maneiras; por exemplo, podemos calcular a codificação de destino não para todo o conjunto de treinamento, mas para algumas janelas. Dessa forma, as codificações da última janela observada provavelmente descreverão melhor o estado atual da série. Como alternativa, podemos simplesmente descartá-lo manualmente, pois temos certeza de que isso piora as coisas neste caso."},{"metadata":{"trusted":false},"cell_type":"code","source":"X_train, X_test, y_train, y_test =\\\nprepareData(df, lag_start=1, lag_end=29, test_size=0.1, target_encoding=False)\n\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5.4 Regularization e Feature Selection \n\nComo já sabemos, nem todos as features são igualmente importantes - alguns podem levar a overfitting, enquanto outras devem ser removidas. Além da inspeção manual, podemos aplicar a regularização. Dois dos modelos de regressão mais populares com regularização são as regressões de Ridge e Lasso. Ambos adicionam mais algumas restrições à nossa função de perda.\n\nNo caso da regressão de Ridge, essas restrições são a soma dos quadrados dos coeficientes multiplicados pelo coeficiente de regularização. Quanto maior o coeficiente de uma feature, maior será a nossa perda. Portanto, tentaremos otimizar o modelo, mantendo os coeficientes razoavelmente baixos.\n\nComo resultado dessa regularização de $L2$, teremos um viés mais alto e uma menor variação, de modo que o modelo generalize melhor (pelo menos é o que esperamos que aconteça).\n\nO segundo modelo de regressão, regressão de Lasso, adiciona à função de perda, não quadrados, mas valores absolutos dos coeficientes. Como resultado, durante o processo de otimização, os coeficientes das features sem importância podem se tornar zeros, o que permite a seleção automatizada de recursos. Esse tipo de regularização é chamado $L1$."},{"metadata":{},"cell_type":"markdown","source":"Primeiro, vamos ter certeza de que temos features a serem descartadas e que os dados possuem features altamente correlacionadas."},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(10, 8))\nsns.heatmap(X_train.corr());","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.linear_model import LassoCV, RidgeCV\n\nridge = RidgeCV(cv=tscv)\nridge.fit(X_train_scaled, y_train)\n\nplotModelResults(ridge, \n                 X_train=X_train_scaled, \n                 X_test=X_test_scaled, \n                 plot_intervals=True, \n                 plot_anomalies=True)\nplotCoefficients(ridge)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Podemos ver claramente que alguns coeficientes estão se aproximando cada vez mais de zero (embora eles nunca cheguem a ele) à medida que sua importância no modelo diminui."},{"metadata":{"trusted":false},"cell_type":"code","source":"lasso = LassoCV(cv=tscv)\nlasso.fit(X_train_scaled, y_train)\n\nplotModelResults(lasso, \n                 X_train=X_train_scaled, \n                 X_test=X_test_scaled, \n                 plot_intervals=True, \n                 plot_anomalies=True)\nplotCoefficients(lasso)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A regressão Lasso acabou sendo mais conservadora; removeu algumas lag features importantes o que piorou a qualidade da previsão."},{"metadata":{},"cell_type":"markdown","source":"# Part 6. XGBoost e LightGBM"},{"metadata":{"trusted":false},"cell_type":"code","source":"from xgboost import XGBRegressor \n\nxgb = XGBRegressor()\nxgb.fit(X_train_scaled, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plotModelResults(xgb, \n                 X_train=X_train_scaled, \n                 X_test=X_test_scaled, \n                 plot_intervals=True, \n                 plot_anomalies=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from lightgbm import LGBMRegressor \n\nlgb = LGBMRegressor()\nlgb.fit(X_train_scaled, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plotModelResults(lgb, \n                 X_train=X_train_scaled, \n                 X_test=X_test_scaled, \n                 plot_intervals=True, \n                 plot_anomalies=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":4}