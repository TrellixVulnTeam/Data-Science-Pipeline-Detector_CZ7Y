{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Competition is over but learning continues!\nEntered the competition quite late, so could not implement all ideas in time.\nBut happy that I could learn a lot and so sharing this kernel.\nPrimarily this kernel is about:\n* Data pipeline for LSTM model\n* How we can have two inputs, one feeding to LSTM and other to linear regression kind of model after concatenating with output of LSTM\n* Feature Engineering and a bit of EDA\n* Model Validation based on result analysis\n* And most important, null loss handling. I was repeatedly getting null loss as I trained the model. As it turned out, it was because I was initializing variables as empty instead of zeros.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Importing Libraries","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport os\nimport numpy as np\nfrom tensorflow import keras\nfrom keras import Sequential\nfrom keras.layers import LSTM, Dense, Input, BatchNormalization, Flatten\nimport tensorflow as tf\nbase_path = \"/kaggle/input/m5-forecasting-accuracy/\"\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Thought I would use TPU, but finally did not.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"try:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n    # set: this is always the case on Kaggle.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Method to generate prediction","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def save_pred():\n    submission_data = pd.read_csv(base_path + \"sample_submission.csv\")\n    test_data1 = train_validation_data[train_validation_data[\"id\"].isin(list(submission_data[\"id\"].values))].copy()\n    test_data2 = train_evaluation_data[train_evaluation_data[\"id\"].isin(list(submission_data[\"id\"].values))].copy()\n    test_generator1 = DataGenerator(test_data1, for_test=True, y=False, max_time_series=1913)\n    test_generator2 = DataGenerator(test_data2, for_test=True, y=False, max_time_series=1941)\n    pred1 = model.predict(test_generator1)\n    pred2 = model.predict(test_generator2)\n    \n    cols = list(submission_data.columns.values)\n    cols.remove(\"id\")\n\n    test_data1 = test_data1[[\"id\"]]\n    df = pd.DataFrame(pred1) \n    df.columns = cols\n    test_data1 = pd.concat([test_data1, df], axis=1)\n    \n    test_data2 = test_data2[[\"id\"]]\n    df = pd.DataFrame(pred2)\n    df.columns = cols\n    test_data2 = pd.concat([test_data2, df], axis=1)\n    \n    test_data_final = pd.concat([test_data1, test_data2])\n    test_data_final.describe()\n    \n    return test_data_final","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### LSTM model\nAdded tanh, when I was struggling with nan value. But realized it is beacause of data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x = tf.constant(np.array([[1,2,3],[4,5,6]]))\ny = tf.constant(np.array([[2,3,4],[5,6,7]]))\ntf.concat([x,y], axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model(n_features=6, loss=0.001):\n    with strategy.scope():\n       \n        input_a = Input(shape=(28, n_features), name=\"time_series_data\")\n        input_b = Input(shape=(28, n_features-1), name=\"flat_data\")\n        lstm_layer = LSTM(128, return_sequences=True)(input_a)\n        norm_layer = BatchNormalization()(lstm_layer)\n        flat_layer = Flatten()(norm_layer)\n        dense_layer_a = Dense(28, activation=\"relu\")(flat_layer)\n        #dense_layer_a = Flatten(dense_layer_a)\n        print(dense_layer_a.shape)\n        qty_layer = input_a[:,:,0]\n        print(qty_layer.shape)\n        arr = []\n        for i in range(28):\n            concat_layer = tf.concat([qty_layer[0,i:28], dense_layer_a[0,0:i]], axis=0)\n            #print(\"concat\",concat_layer.shape)\n            arr.append(concat_layer)\n        new_qty_data = tf.reshape(tf.concat(arr, axis=0),(-1,28,28))\n        print(new_qty_data.shape)\n        \n        \n        \n        concat_layer = tf.concat([new_qty_data, input_b], axis=2)\n        dense_layer_b = Dense(128, activation=\"relu\")(concat_layer)\n        output_layer  = Dense(1, activation=\"relu\")(dense_layer_b)\n            \n        model = keras.Model(inputs=[input_a, input_b], outputs=output_layer)\n        opt = keras.optimizers.Adam(learning_rate=loss)\n        rmsprop = keras.optimizers.RMSprop(lr=0.00001, rho=0.9, epsilon=1e-08)\n        model.compile(loss='mse', optimizer=opt, metrics=['mse'])\n        return model\n\nbuild_model()    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Generator\nIt was very satisfying to build this data generator. Though with tensorflow 2.0, I should have used dataset.\nSo my intution was that if I have to predict qty for an item, I would look at the below:\n1. Recent trend\n2. Trend on special days like weekdays\n3. Trend on month days\n4. Trend on snap days\n5. Trend on holidays \n6. Trend on day before holiday\nAlso helpful will be to see trends for \n1. Store as a whole\n2. Category as a whole\n3. Department as whole\n4. State wise trends\n5. Price fluctuations\n\nI will need to see how the qty varied with these events and then knowing events for the future, try to predict the qty. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class DataGenerator(keras.utils.Sequence):\n    'Generates data for Keras'\n    def __init__(self, data, max_time_series, n_features=6, for_test=False, y=True):\n        'Initialization'\n        self.data = data\n        self.batch_size = 1\n        self.list_IDs = list(data[\"id\"].values)\n        self.n_time_series = 28\n        self.n_pred = 28\n        self.max_time_series = max_time_series\n        self.tot_time_series = self.max_time_series - self.n_time_series -self.n_pred + 1\n        self.n_features = n_features #len(type_columns) + len(name_columns) + 6\n        self.for_test = for_test\n        self.shuffle = False if for_test else True\n        self.on_epoch_end()\n        self.y = y\n\n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        num_data = 1 if not self.y else(0.01 if self.for_test else 2)\n        return int(np.floor(len(self.list_IDs) * num_data))\n\n    def __getitem__(self, index):\n        'Generate one batch of data'\n        # Generate indexes of the batch\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n\n        # Find list of IDs\n        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n\n        # Generate data\n        X, y = self.__data_generation(list_IDs_temp)\n\n        return X, y\n\n    def on_epoch_end(self):\n        'Updates indexes after each epoch'\n        self.indexes = np.arange(len(self.list_IDs))\n        if self.shuffle == True:\n            np.random.shuffle(self.indexes)\n\n    def get_col_trend(self,col, all_qty_calendar_data, all_qty_row, calendar_row,calendar_y_row, X_i,X_flat_i, idx):\n        #all_qty_calendar_data[col] = all_qty_calendar_data[col].fillna(\"\")\n        #calendar_row[col] = calendar_row[col].fillna(\"\")\n        #calendar_y_row[col] = calendar_y_row[col].fillna(\"\")\n        all_qty_row.columns = list(all_qty_calendar_data[col].values)\n        df = all_qty_row.T.reset_index()\n        df.columns = [col,\"qty\"]\n        df_grp = df.groupby(col).mean()\n        dict_weekday = df_grp.to_dict()[\"qty\"]\n        summary_data = list(calendar_row[col].map(dict_weekday).values)\n        X_i[:,idx] = summary_data\n        summary_data = list(calendar_y_row[col].map(dict_weekday).values)\n        X_flat_i[:,idx-1] = summary_data\n                \n    def __data_generation(self, list_IDs_temp):\n        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n        X = np.zeros((self.batch_size, self.n_time_series, self.n_features))\n        X_flat = np.zeros((self.batch_size, self.n_pred, self.n_features-1))\n        y = np.zeros((self.batch_size, self.n_pred), dtype=float)\n\n        for i, ID in enumerate(list_IDs_temp):\n            row = self.data[self.data[\"id\"] == ID]\n            start_day = row[\"start_day\"].values[0]\n            if self.for_test:\n                if self.y:\n                    start = self.max_time_series - self.n_time_series - self.n_pred + 1\n                else:\n                    start = self.max_time_series - self.n_time_series + 1\n            else:\n                start = np.random.randint(low=start_day, high=self.tot_time_series+1, size=1)[0]\n\n            qty_cols = [\"d_\" + str(k) for k in range(start, start + self.n_time_series)]\n            #if (not self.y):\n            #    print(qty_cols)\n\n            y_cols = [\"d_\" + str(k) for k in range(start + self.n_time_series, start + self.n_time_series+self.n_pred)]\n            #if (not self.y):\n            #    print(y_cols)\n            \n            all_qty_cols = [\"d_\" + str(k) for k in range(start_day, self.max_time_series+1)]\n        \n            X_i = np.zeros((self.n_time_series, self.n_features))\n            X_flat_i = np.zeros((self.n_pred, self.n_features-1))\n            \n            calendar_row = calendar_data[calendar_data[\"d\"].isin(qty_cols)]\n            calendar_y_row = calendar_data[calendar_data[\"d\"].isin(y_cols)]\n            X_i[:,0] = row[qty_cols].values\n            num_cols = 1\n            if 1==2:\n                X_i[:,num_cols] = calendar_row[\"weekend\"].values\n                state_id = row[\"state_id\"].values[0]\n                snap_col = \"snap_\" + state_id\n                X_i[:,2] = calendar_row[snap_col].values\n        \n            \n            all_qty_row = row[all_qty_cols]\n            all_qty_calendar_data = calendar_data[calendar_data[\"d\"].isin(all_qty_cols)]\n            \n            self.get_col_trend(\"wday\", all_qty_calendar_data, all_qty_row, calendar_row,calendar_y_row, X_i, X_flat_i,1)\n            self.get_col_trend(\"month\", all_qty_calendar_data, all_qty_row, calendar_row,calendar_y_row, X_i, X_flat_i,2)\n            self.get_col_trend(\"event_name_1\", all_qty_calendar_data, all_qty_row, calendar_row,calendar_y_row, X_i,X_flat_i, 3)\n            state_id = row[\"state_id\"].values[0]\n            snap_col = \"snap_\" + state_id\n            self.get_col_trend(snap_col, all_qty_calendar_data, all_qty_row, calendar_row,calendar_y_row, X_i,X_flat_i, 4)\n            self.get_col_trend(\"prv_event_name_1\", all_qty_calendar_data, all_qty_row, calendar_row,calendar_y_row, X_i,X_flat_i, 5)\n            X[i,:] = X_i\n            X_flat[i,:] = X_flat_i\n            \n            if self.y:\n                y[i,:] = row[y_cols].values\n                if 1==2:\n                    if min_qty > 0:\n                        y[i,:] = y[i,:] - min_qty\n                    if max_qty != min_qty:\n                        y[i,:] = y[i,:] / (max_qty - min_qty)\n        y = y if self.y else None\n        \n        return {\"time_series_data\":X, \"flat_data\":X_flat }, y","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Method for error analysis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef get_sample_pred(n_features=3):\n    submission_data = pd.read_csv(base_path + \"sample_submission.csv\")\n    test_data1 = train_validation_data[train_validation_data[\"id\"].isin(list(submission_data[\"id\"].values))].copy().head(20)\n    test_data2 = train_evaluation_data[train_evaluation_data[\"id\"].isin(list(submission_data[\"id\"].values))].copy().head(20)\n    test_generator1 = DataGenerator(test_data1, n_features=n_features, for_test=True, y=False, max_time_series=1913)\n    test_generator2 = DataGenerator(test_data2, n_features=n_features, for_test=True, y=False, max_time_series=1941)\n    pred1 = model.predict(test_generator1)\n    pred2 = model.predict(test_generator2)\n    \n    cols = list(submission_data.columns.values)\n    cols.remove(\"id\")\n\n    test_data1 = test_data1[[\"id\"]]\n    df = pd.DataFrame(pred1) \n    df.columns = cols\n    test_data1 = pd.concat([test_data1, df], axis=1)\n\n    \n    test_data2 = test_data2[[\"id\"]]\n    df = pd.DataFrame(pred2)\n    df.columns = cols\n    test_data2 = pd.concat([test_data2, df], axis=1)\n    \n    test_data_final = pd.concat([test_data1, test_data2])\n    test_data_final.describe()\n    return test_data1, test_data2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Read Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntrain_validation_data = pd.read_csv(base_path + \"sales_train_validation.csv\")\ntrain_evaluation_data = pd.read_csv(base_path + \"sales_train_evaluation.csv\")\ncalendar_data = pd.read_csv(base_path + \"calendar.csv\")\nprices_data = pd.read_csv(base_path + \"sell_prices.csv\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_qty_cols = [col for col in train_validation_data.columns.values if \"d_\" in col]\ntrain_validation_data[\"start_day\"] = train_validation_data.apply(lambda row: np.array(row[all_qty_cols]>0).argmax() + 1, axis=1)\n\nall_qty_cols = [col for col in train_evaluation_data.columns.values if \"d_\" in col]\ntrain_evaluation_data[\"start_day\"] = train_evaluation_data.apply(lambda row: np.array(row[all_qty_cols]>0).argmax() + 1, axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Analysis\nLet us see days on which max sale happens. As we can see below, it is weekends and so this should be a feature","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"calendar_data[\"prv_event_name_1\"] = calendar_data[\"event_name_1\"].shift(-1).fillna(\"\")\ncalendar_data[\"event_name_1\"] = calendar_data[\"event_name_1\"].fillna(\"\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_qty_cols = [\"d_\" + str(k) for k in range(1, 1941+1)]\nimp_days = list(train_evaluation_data[all_qty_cols].sum().sort_values().tail(10).index.values)\ncalendar_data[calendar_data[\"d\"].isin(imp_days)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualizing the trend!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"all_qty_cols = [\"d_\" + str(k) for k in range(1, 1941+1)]\nfood_data = train_evaluation_data[train_evaluation_data[\"cat_id\"]==\"FOODS\"]\nimp_days = list(food_data[all_qty_cols].sum().sort_values().tail(5).index.values)\ncalendar_data[calendar_data[\"d\"].isin(imp_days)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_qty_cols = [\"d_\" + str(k) for k in range(1, 1941+1)]\nhobbies_data = train_evaluation_data[train_evaluation_data[\"cat_id\"]==\"HOBBIES\"]\nimp_days = list(food_data[all_qty_cols].sum().sort_values().tail(5).index.values)\ncalendar_data[calendar_data[\"d\"].isin(imp_days)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_qty_cols = [\"d_\" + str(k) for k in range(1, 1941+1)]\nhousehold_data = train_evaluation_data[train_evaluation_data[\"cat_id\"]==\"HOUSEHOLD\"]\nimp_days = list(food_data[all_qty_cols].sum().sort_values().tail(5).index.values)\ncalendar_data[calendar_data[\"d\"].isin(imp_days)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.plot(list(range(1, 1941+1)), train_evaluation_data[all_qty_cols].sum().values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_validation_data.head(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_evaluation_data.head(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if 1==2:\n    calendar_data[\"weekend\"] = calendar_data[\"wday\"].map(lambda x: 1 if x<=2 else 0)\n    calendar_data.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Instantiate train and validation data generators!","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#Features for 28 day time period before and for days to be predicted\n* Weekday qty diff trend\n* Month qty diff trend\n* event_name_1 qty diff trend\n* snap qty diff trend\n\n* Category Weekday qty diff trend\n* Category Month qty diff trend\n* Category event_name_1 qty diff trend\n* Category snap qty diff trend\n\n* Store Weekday qty diff trend\n* Store Month qty diff trend\n* Store event_name_1 qty diff trend\n* Store snap qty diff trend\n\n* State Weekday qty diff trend\n* State Month qty diff trend\n* State event_name_1 qty diff trend\n* State snap qty diff trend","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"training_generator = DataGenerator(train_validation_data, 1913,6)\nvalidation_generator = DataGenerator(train_evaluation_data, 1941,6, for_test=True)\nfor X,y in validation_generator:\n    print(X[\"time_series_data\"].shape, y.shape)\n    break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = build_model()\nmodel_history = model.fit_generator(generator=training_generator,\n                    validation_data=validation_generator, verbose=1, epochs=2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Get some predictions for result analysis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_data = pd.read_csv(base_path + \"sample_submission.csv\")\ntest_data1 = train_validation_data[train_validation_data[\"id\"].isin(list(submission_data[\"id\"].values))].copy().tail(1)\n   \ntest_generator1 = DataGenerator(test_data1, n_features=6, for_test=True, y=False, max_time_series=1913)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for X,y in test_generator1:\n    pred = model.predict([X[\"time_series_data\"],X[\"flat_data\"]])\n    break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model2 = keras.Model(inputs=model.inputs, outputs=model.layers[-3].output)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for X,y in test_generator1:\n    pred2 = model2.predict([X[\"time_series_data\"],X[\"flat_data\"]])\n    break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(pred2[0][:,0:1])\nprint(pred2[0][:,27:28])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data1.tail(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"item_id = \"FOODS_3_827_WI_3_validation\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Plot prediction and true values.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"row1= train_validation_data[train_validation_data[\"id\"]==item_id]\nrow1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"row2 = train_evaluation_data[train_evaluation_data[\"id\"]==item_id.replace(\"validation\",\"evaluation\")]\nrow2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,4))\ncol = [\"d_\" + str(i+1) for i in range(1913)]\nplt.scatter(list(range(1913))[-28:], list(row1[col].values[0])[-28:], marker=\"x\")\n\ncol = [\"d_\" + str(i+1) for i in range(1941)]\nplt.scatter(list(range(1942))[-28:], list(row2[col].values[0])[-28:])\n\n\ncol = [\"F\" + str(i+1) for i in range(28)]\nfor i in range(1):\n    row3 = pred[0,i,:]\n    plt.scatter(list(range(1942))[-28:], list(pred.flatten()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### See if there are any special events/holidays days etc that if considered would have helped the model to train better","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"col = [\"d_\" + str(i+1) for i in range(1914,1942)]\ncalendar_data[calendar_data[\"d\"].isin(col)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### See weekday trend","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"col = [\"d_\" + str(i+1) for i in range(1941)]\nplt.scatter(calendar_data[\"weekday\"].values[-20:], list(row2[col].values[0])[-20:], c=\"g\")\n\n\ncol = [\"F\" + str(i+1) for i in range(28)]\nplt.scatter(calendar_data[\"weekday\"].values[-20:], list(pred.flatten())[-20:], c=\"r\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#save_pred().to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Conclusion\nLots learnt, would have been much better if I would have entered the competition in time as would have been able to experiment with lot more ideas.\nOne idea that want to implement is to use differences as features instead of absolutes. \n\n### Please do share comments/ideas. This community is so wonderful. It is extremely motivating to be able to interact! ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}