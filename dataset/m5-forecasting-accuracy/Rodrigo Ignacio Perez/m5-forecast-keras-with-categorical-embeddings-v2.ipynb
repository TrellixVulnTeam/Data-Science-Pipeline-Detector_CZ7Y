{"cells":[{"metadata":{},"cell_type":"markdown","source":"# M5 Forecast: Keras with Categorical Embeddings V2\n\nThis notebook tries to model expected sales of product groups. Since many of the features are categorical, we use this example to show how embedding layers make life easy when dealing with categoric inputs for neural nets by skipping the step of making dummy variables by hand.\n\nData preprocessing and feature engineering is very similar (but not identical) to this [R kernel](https://www.kaggle.com/mayer79/m5-forecast-keras-embeddings-with-r) and uses ideas from the two excellent kernels [Very fst Model](https://www.kaggle.com/ragnar123/very-fst-model) and [M5 ForecasteR](https://www.kaggle.com/kailex/m5-forecaster-0-57330).\n\nTo gain an extra 3 GB of RAM, we do not use GPU acceleration for the training.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport gc\nimport os\nfrom tqdm.notebook import tqdm\nimport pywt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"path = \"../input/m5-forecasting-accuracy\"\n\ncalendar = pd.read_csv(os.path.join(path, \"calendar.csv\"))\nselling_prices = pd.read_csv(os.path.join(path, \"sell_prices.csv\"))\nsample_submission = pd.read_csv(os.path.join(path, \"sample_submission.csv\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales = pd.read_csv(os.path.join(path, \"sales_train_validation.csv\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Describe and prepare data\n\nWe will now go through all data sets and prepare them for modelling.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Calendar data\n\nFor each date (covering both training and test data), we have access to useful calendar information.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"calendar.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i, var in enumerate([\"year\", \"weekday\", \"month\", \"event_name_1\", \"event_name_2\", \n                         \"event_type_1\", \"event_type_2\", \"snap_CA\", \"snap_TX\", \"snap_WI\"]):\n    plt.figure()\n    g = sns.countplot(calendar[var])\n    g.set_xticklabels(g.get_xticklabels(), rotation=45)\n    g.set_title(var)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OrdinalEncoder\n\ndef prep_calendar(df):\n    df = df.drop([\"date\", \"weekday\"], axis=1)\n    df = df.assign(d = df.d.str[2:].astype(int))\n    df = df.fillna(\"missing\")\n    cols = list(set(df.columns) - {\"wm_yr_wk\", \"d\"})\n    df[cols] = OrdinalEncoder(dtype=\"int\").fit_transform(df[cols])\n    df = reduce_mem_usage(df)\n    return df\n\ncalendar = prep_calendar(calendar)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"calendar.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Notes for modeling\n\n**Features** deemed to be useful:\n\n- \"wday\", \"year\", \"month\" -> integer coding & embedding\n- \"event_name_1\", \"event_type_1\" -> integer coding & embedding\n- \"snap_XX\" -> numeric (they are dummies)\n\n**Reshape required**: No\n\n**Merge key(s)**: \"d\", \"wm_yr_wk\"","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Selling prices\n\nContains selling prices for each store_id, item_id_wm_yr_wk combination.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"selling_prices.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Derive some time related features:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def prep_selling_prices(df):\n    gr = df.groupby([\"store_id\", \"item_id\"])[\"sell_price\"]\n    df[\"sell_price_rel_diff\"] = gr.pct_change()\n    df[\"sell_price_roll_sd7\"] = gr.transform(lambda x: x.rolling(7).std())\n    df[\"sell_price_cumrel\"] = (gr.shift(0) - gr.cummin()) / (1 + gr.cummax() - gr.cummin())\n    df = reduce_mem_usage(df)\n    return df\n\nselling_prices = prep_selling_prices(selling_prices)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"selling_prices.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Notes for modeling\n\n**Features**:\n\n- sell_price and derived features -> numeric\n\n**Reshape**: No\n\n**Merge key(s)**: to sales data by store_id, item_id, wm_yr_wk (through calendar data)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Sales data\n\nContains the number of sold items (= our response) as well as some categorical features.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sales.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i, var in enumerate([\"state_id\", \"store_id\", \"cat_id\", \"dept_id\"]):\n    plt.figure()\n    g = sns.countplot(sales[var])\n    g.set_xticklabels(g.get_xticklabels(), rotation=45)\n    g.set_title(var)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales.item_id.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Reshaping\n\nWe now reshape the data from wide to long, using \"id\" as fixed and swapping \"d_x\" columns. Along this process, we also add structure for submission data and reduce data size.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def reshape_sales(df, drop_d = None):\n    if drop_d is not None:\n        df = df.drop([\"d_\" + str(i + 1) for i in range(drop_d)], axis=1)\n    df = df.assign(id=df.id.str.replace(\"_validation\", \"\"))\n    df = df.reindex(columns=df.columns.tolist() + [\"d_\" + str(1913 + i + 1) for i in range(2 * 28)])\n    df = df.melt(id_vars=[\"id\", \"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"],\n                 var_name='d', value_name='demand')\n    df = df.assign(d=df.d.str[2:].astype(\"int16\"))\n    return df\n\nsales = reshape_sales(sales, 1000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Distribution of the response","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(sales[\"demand\"][sales[\"demand\"] <= 10]);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Add time-lagged features\n\nAdd some of the derived features from kernel https://www.kaggle.com/ragnar123/very-fst-model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Add a denoise function  Discrete Wavelet Transform (DWT)\ndef maddest(d, axis=None):\n    return np.nanmean(np.absolute(d - np.mean(d, axis)), axis)\ndef denoise_signal(x, wavelet='db4', level=1):\n    x=x.fillna(0)\n    coeff = pywt.wavedec(x, wavelet, mode=\"per\")\n    sigma = (1/0.6745) * maddest(coeff[-level])\n\n    uthresh = sigma * np.sqrt(2*np.log(len(x)))\n    coeff[1:] = (pywt.threshold(i, value=uthresh, mode='hard') for i in coeff[1:])\n\n    return pywt.waverec(coeff, wavelet, mode='per')\n#denoise_signal(x_1)\n\ndef prep_sales(df):\n    #df['lag_t28'] = df.groupby(['id'])['demand'].transform(lambda x: x.shift(28))\n    #df['lag_t29'] = df.groupby(['id'])['demand'].transform(lambda x: x.shift(29))\n    #df['lag_t30'] = df.groupby(['id'])['demand'].transform(lambda x: x.shift(30))\n    \n    #df['denoise_lag_t28'] = pd.DataFrame(denoise_signal(df.groupby(['id'])['demand'].transform(lambda x: x.shift(28)))).set_index(df.groupby(['id'])['demand'].transform(lambda x: x.shift(28)).index) #Insted of using the rolled mean average we use the denoised signal\n    df['denoise_lag_t28'] =  df.groupby(['id'])['demand'].transform(lambda x: x.shift(28))\n    df['rolling_mean_t7'] = df.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(7).mean())\n    df['rolling_mean_t30'] = df.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(30).mean())\n    \n    df['rolling_mean_t60'] = df.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(60).mean())\n    df['rolling_mean_t90'] = df.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(90).mean())\n    df['rolling_mean_t180'] = df.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(180).mean())\n    df['rolling_std_t7'] = df.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(7).std())\n    df['rolling_std_t30'] = df.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(30).std())\n\n    #Remove rows with NAs except for submission rows. rolling_mean_t180 was selected as it produces most missings\n    df = df[(df.d >= 1914) | (pd.notna(df.rolling_mean_t180))]\n    df = reduce_mem_usage(df)\n\n    return df\n\nsales = prep_sales(sales)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nsales.head","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Notes for modeling\n\n**Features**\n- \"dept_id\", \"item_id\", \"store_id\": Integer coding & embedding\n- lagged features derived from response: Numeric\n\n**Reshape**:\n- Reshape days as \"d\" from wide to long -> \"demand\" will be response variable\n\n**Merges**:\n1. Join calendar features by \"d\"\n2. Join selling prices by \"store_id\", \"item_id\" and (\"wm_yr_wk\" from calendar)\n\nComment: Submission dates: \"d_1914\" - \"d_1969\"","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Combine data sources","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sales = sales.merge(calendar, how=\"left\", on=\"d\")\ngc.collect()\nsales.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales = sales.merge(selling_prices, how=\"left\", on=[\"wm_yr_wk\", \"store_id\", \"item_id\"])\nsales.drop([\"wm_yr_wk\"], axis=1, inplace=True)\ngc.collect()\nsales.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del selling_prices","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Prepare data for Keras interface","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Ordinal encoding of remaining categoricals","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_id_cols = [\"item_id\", \"dept_id\", \"store_id\", \"cat_id\", \"state_id\"]\ncat_cols = cat_id_cols + [\"wday\", \"month\", \"year\", \"event_name_1\", \n                          \"event_type_1\", \"event_name_2\", \"event_type_2\"]\n\n# In loop to minimize memory use\nfor i, v in tqdm(enumerate(cat_id_cols)):\n    sales[v] = OrdinalEncoder(dtype=\"int\").fit_transform(sales[[v]])\n\nsales = reduce_mem_usage(sales)\nsales.head()\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Impute numeric columns","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"num_cols = [\"sell_price\", \"sell_price_rel_diff\", \"sell_price_roll_sd7\", \"sell_price_cumrel\"\n            ,'denoise_lag_t28'\n            , \"rolling_mean_t7\"\n            , \"rolling_mean_t30\"\n            ,\"rolling_mean_t60\"\n            ,\"rolling_mean_t90\"\n            ,\"rolling_mean_t180\"\n            ,\"rolling_std_t7\"\n            ,\"rolling_std_t30\"\n           ]\nbool_cols = [\"snap_CA\", \"snap_TX\", \"snap_WI\"]\ndense_cols = num_cols + bool_cols\n\n# Need to do column by column due to memory constraints\nfor i, v in tqdm(enumerate(num_cols)):\n    sales[v] = sales[v].fillna(sales[v].median())\n    \nsales.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Separate submission data and reconstruct id columns","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ntest = sales[sales.d >= 1914]\ntest = test.assign(id=test.id + \"_\" + np.where(test.d <= 1941, \"validation\", \"evaluation\"),\n                   F=\"F\" + (test.d - 1913 - 28 * (test.d > 1941)).astype(\"str\"))\ntest.head()\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Make training data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Input dict for training with a dense array and separate inputs for each embedding input\ndef make_X(df):\n    X = {\"dense1\": df[dense_cols].to_numpy()}\n    for i, v in enumerate(cat_cols):\n        X[v] = df[[v]].to_numpy()\n    return X\n\n# Submission data\nX_test = make_X(test)\n\n# One month of validation data\n#Two moths maybe \nn_months=2\n\n\nflag = (sales.d < 1914) & (sales.d >= 1914 - (28*n_months))\nvalid = (make_X(sales[flag]),\n         sales[\"demand\"][flag])\n\n# Rest is used for training\nflag = sales.d < 1914 - (28*n_months)\nX_train = make_X(sales[flag])\ny_train = sales[\"demand\"][flag]\n                             \ndel sales, flag\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.unique(X_train[\"state_id\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## The model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Imports","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow.keras as keras\nfrom keras.layers import Dropout\n\nfrom tensorflow.keras.callbacks import LearningRateScheduler\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.layers import Dense, Input, Embedding, Dropout, concatenate, Flatten ,LSTM, TimeDistributed\nfrom tensorflow.keras.models import Model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Architecture with embeddings","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model(lr=0.002,lstm_network=False):\n    tf.random.set_seed(173)\n\n    tf.keras.backend.clear_session()\n    gc.collect()\n\n    # Dense input\n    dense_input = Input(shape=(len(dense_cols), ), name='dense1')\n\n    # Embedding input\n    wday_input = Input(shape=(1,), name='wday')\n    month_input = Input(shape=(1,), name='month')\n    year_input = Input(shape=(1,), name='year')\n    event_name_1_input = Input(shape=(1,), name='event_name_1')\n    event_type_1_input = Input(shape=(1,), name='event_type_1')\n    event_name_2_input = Input(shape=(1,), name='event_name_2')\n    event_type_2_input = Input(shape=(1,), name='event_type_2')\n    item_id_input = Input(shape=(1,), name='item_id')\n    dept_id_input = Input(shape=(1,), name='dept_id')\n    store_id_input = Input(shape=(1,), name='store_id')\n    cat_id_input = Input(shape=(1,), name='cat_id')\n    state_id_input = Input(shape=(1,), name='state_id')\n\n    wday_emb = Flatten()(Embedding(7, 1)(wday_input))\n    month_emb = Flatten()(Embedding(12, 1)(month_input))\n    year_emb = Flatten()(Embedding(6, 1)(year_input))\n    event_name_1_emb = Flatten()(Embedding(31, 1)(event_name_1_input))\n    event_type_1_emb = Flatten()(Embedding(5, 1)(event_type_1_input))\n    event_name_2_emb = Flatten()(Embedding(5, 1)(event_name_2_input))\n    event_type_2_emb = Flatten()(Embedding(5, 1)(event_type_2_input))\n\n    item_id_emb = Flatten()(Embedding(3049, 3)(item_id_input))\n    dept_id_emb = Flatten()(Embedding(7, 1)(dept_id_input))\n    store_id_emb = Flatten()(Embedding(10, 1)(store_id_input))\n    cat_id_emb = Flatten()(Embedding(3, 1)(cat_id_input))\n    state_id_emb = Flatten()(Embedding(3, 1)(state_id_input))\n\n    \n    if lstm_network:\n    \n        x = concatenate([tf.reshape(dense_input,(-1,1,15)) ,(Embedding(7, 1)(wday_input)),(Embedding(12, 1)(month_input)),\n                         (Embedding(6, 1)(year_input)),(Embedding(31, 1)(event_name_1_input)),\n                         (Embedding(5, 1)(event_type_1_input)),(Embedding(5, 1)(event_name_2_input)),\n                         (Embedding(5, 1)(event_type_2_input)),(Embedding(3049, 3)(item_id_input)),\n                         (Embedding(7, 1)(dept_id_input)),(Embedding(10, 1)(store_id_input)),\n                         (Embedding(3, 1)(cat_id_input)),(Embedding(3, 1)(state_id_input))])\n\n        x = LSTM( 150, return_sequences = True, dropout = 0.3, recurrent_dropout = 0.3)(x)\n        x = LSTM( 70, return_sequences = True, dropout = 0.3, recurrent_dropout = 0.3)(x)\n        x = LSTM( 10, return_sequences = False, dropout = 0.3, recurrent_dropout = 0.3)(x)\n        outputs = Dense(1, activation=\"linear\", name='output')(x)\n    else:\n        x = concatenate([dense_input, wday_emb, month_emb, year_emb, \n                         event_name_1_emb, event_type_1_emb, \n                         event_name_2_emb, event_type_2_emb, \n                         item_id_emb, dept_id_emb, store_id_emb,\n                         cat_id_emb, state_id_emb])\n\n        x = Dense(150, activation=\"tanh\")(x)\n        #x = Dropout(0.1)(x)\n        x = BatchNormalization()(x)\n        x = Dense(70, activation=\"tanh\")(x)\n        #x = Dropout(0.1)(x)\n        x = BatchNormalization()(x)\n        x = Dense(10, activation=\"tanh\")(x)\n        outputs = Dense(1, activation=\"linear\", name='output')(x)\n\n\n    inputs = {\"dense1\": dense_input, \"wday\": wday_input, \"month\": month_input, \"year\": year_input, \n              \"event_name_1\": event_name_1_input, \"event_type_1\": event_type_1_input,\n              \"event_name_2\": event_name_2_input, \"event_type_2\": event_type_2_input,\n              \"item_id\": item_id_input, \"dept_id\": dept_id_input, \"store_id\": store_id_input, \n              \"cat_id\": cat_id_input, \"state_id\": state_id_input}\n\n    # Connect input and output\n    model = Model(inputs, outputs)\n\n    model.compile(loss=keras.losses.mean_squared_error,\n                  metrics=[\"mse\"],\n                  optimizer=keras.optimizers.Adam(learning_rate=lr))\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lstm_network=False\nin_lrate=0.0002\nmodel = create_model(0.0002, lstm_network=lstm_network)\nmodel.summary()\nkeras.utils.plot_model(model, 'model.png', show_shapes=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Calculate derivatives and fit model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"if lstm_network:\n    model.load_weights('../input/model20/model.h5')\nimport math\nfrom tensorflow.keras.callbacks import ModelCheckpoint\ndef step_decay(epoch):\n\tinitial_lrate = 0.0002\n\tdrop = 0.5\n\tepochs_drop = 3.0\n\tlrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n\treturn lrate\nlrate = LearningRateScheduler(step_decay)\nmcp_save = ModelCheckpoint('modelBest.hdf5', save_best_only=True, monitor='val_loss', mode='min')\ncallbacks_list = [lrate,mcp_save]\n\n\nhistory = model.fit(X_train,\n                    y_train,\n                    batch_size=10000,\n                    epochs=100,\n                    shuffle=True,\n                    validation_data=valid,\n                    callbacks=callbacks_list)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Plot the evaluation metrics over epochs","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'valid'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history.history[\"val_loss\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.load_weights('modelBest.hdf5')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Submission","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = model.predict(X_test, batch_size=10000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test[\"demand\"] = pred.clip(0)\nsubmission = test.pivot(index=\"id\", columns=\"F\", values=\"demand\").reset_index()[sample_submission.columns]\nsubmission = sample_submission[[\"id\"]].merge(submission, how=\"left\", on=\"id\")\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission[sample_submission.id==\"FOODS_1_001_TX_2_validation\"].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}