{"cells":[{"metadata":{},"cell_type":"markdown","source":"#  Rasmus og Lasse Walmart 2/2","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We have been asked by Walmart to precisely forecast certain products 28 days in advance for 3 different stores across America. The 3 stores are located in Texas, California and Wisconsin. They all have the same departments, where they sell different items.\n\nFor this competition we will measure our models accuracy from the metric is Weighted Root Mean Squared Scaled Error (WRMSSE) ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"![](https://i.imgur.com/uqhsf3d.png)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Libraries","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom statsmodels.tsa.arima_model import ARIMA\nimport gc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Importing datasets","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"calendar = pd.read_csv('../input/m5-forecasting-accuracy/calendar.csv')\ntrain_sales = pd.read_csv('../input/m5-forecasting-accuracy/sales_train_validation.csv')\nsample_submission = pd.read_csv('../input/m5-forecasting-accuracy/sample_submission.csv')\nsell_prices = pd.read_csv('../input/m5-forecasting-accuracy/sell_prices.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To see datasets and infomation about these, please go to our other kernal","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Merging datasets","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"From our analisys we can see that the 3 tables calender, sell_prices and train_sales all need to be merged together into one dataframe which we want to use to train on model for predictions","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Kaggle offers 16GB of ram for free, but unfortunatly this is not enough for a dataset of this size, therefore we have \"found\" a method to reduce memory usage of the datasets.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"calendar = reduce_mem_usage(calendar)\nsell_prices = reduce_mem_usage(sell_prices)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def melt_train_data(sales_train_validation):\n    # Turns the table, so that we keep all our id columns as colums but turn day so that each day is its own row and demand is the value that was in d_something before. Goes from 30490 rows × 1919 columns to 60034810 rows × 9 columns\n    sales_train_validation = pd.melt(sales_train_validation, id_vars = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], var_name = 'day', value_name = 'demand')\n    sales_train_validation = reduce_mem_usage(sales_train_validation)\n     # seperate test dataframes\n    test_rows = [row for row in sample_submission['id'] if 'validation' in row]\n    test = sample_submission[sample_submission['id'].isin(test_rows)]\n    \n    \n    # change column names\n    test.columns = ['id', 'd_1914', 'd_1915', 'd_1916', 'd_1917', 'd_1918', 'd_1919', 'd_1920', 'd_1921', 'd_1922', 'd_1923', 'd_1924', 'd_1925', 'd_1926', 'd_1927', 'd_1928', 'd_1929', 'd_1930', 'd_1931', \n                      'd_1932', 'd_1933', 'd_1934', 'd_1935', 'd_1936', 'd_1937', 'd_1938', 'd_1939', 'd_1940', 'd_1941']\n   \n    \n    product = sales_train_validation[['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']].drop_duplicates()\n    \n   \n    test = test.merge(product, how = 'left', on = 'id')\n    \n    test = pd.melt(test, id_vars = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], var_name = 'day', value_name = 'demand')\n    \n    sales_train_validation['part'] = 'train'\n    test['part'] = 'test'\n    \n    data =pd.concat([sales_train_validation, test], axis = 0)\n    del(sales_train_validation, test)\n    return data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this method we pandas melt on the sales_train_validation set so it becomes tall dataset insted of a wide one. Basically, you \"melt\" data so that each row is a unique id-variable combination","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data = melt_train_data(train_sales)\ndel(train_sales)\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the next section we want to inspect the new demand(day) values ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"demand_pattern = data.groupby('day')['demand'].sum().to_frame().reset_index()\ndemand_pattern['day_number'] = demand_pattern['day'].str.split(\"_\",n = 1, expand = True)[1].astype('int32')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,12))\nplt.plot(demand_pattern['demand'])\nplt.xlabel('Days', fontsize=18)\nplt.ylabel('Demand', fontsize=16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that at some point around 1000 some data has gone missing and there is a drop in demand from right before the missing data.\nFor this reason we choose to only keep the data from after the drop. Our reason for this is that this data we have now is more consistent.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"demand_pattern = demand_pattern[demand_pattern['day_number']>1101]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,12))\nplt.plot(demand_pattern['demand'])\nplt.xlabel('Days', fontsize=18)\nplt.ylabel('Demand', fontsize=16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the graph above we can see that the last part is missing. This is the values we are going to predict later. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data[data['day'].isin(demand_pattern['day'])]\ndel(demand_pattern)\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def merge_with_calendar(calendar, sales_train_validation):\n        data = pd.merge(sales_train_validation, calendar, how = 'left', left_on = ['day'], right_on =['d'])\n        data.drop(['d', 'day'], inplace = True, axis = 1)\n        print('rows: {} and columns: {}'.format(data.shape[0], data.shape[1]))\n        return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = merge_with_calendar(calendar, data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def merge_with_sales(sell_prices, data):\n    data = data.merge(sell_prices, on = ['store_id', 'item_id', 'wm_yr_wk'], how = 'left')\n    data = data.drop(columns=['wm_yr_wk'])\n    print('rows: {} and columns: {}'.format(data.shape[0], data.shape[1]))\n    return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = merge_with_sales(sell_prices,data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Cleaning and Transforming our datasets","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"To avoid NAN values in our dataset we want to first specify, that a NAN value, is no_event and after we transform the non-numeric value to an numeric value with the label encoder.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def fill_nan_values(data):\n        data['event_name_1'] = data['event_name_1'].fillna('no_event')\n        data['event_type_1'] = data['event_type_1'].fillna('no_event')\n        data['event_name_2'] = data['event_name_2'].fillna('no_event')\n        data['event_type_2'] = data['event_type_2'].fillna('no_event')\n        return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"   def transform_data(data):\n        data = fill_nan_values(data)\n        \n        le = LabelEncoder()\n        \n        data['event_type_1'] = le.fit_transform(data.event_type_1)\n        data['event_name_1'] = le.fit_transform(data.event_name_1)\n        data['event_type_2'] = le.fit_transform(data.event_type_2)\n        data['event_name_2'] = le.fit_transform(data.event_name_2)\n        \n        #Next we want to clean our dataset for non-numeric values. We especially want to transform our different ids for departments, stores and items.\n        data['dept_id'] = le.fit_transform(data.dept_id)\n        data['cat_id'] = le.fit_transform(data.cat_id)\n        data['store_id'] = le.fit_transform(data.store_id)\n        data['item_id'] = le.fit_transform(data.item_id)\n        data['state_id'] = le.fit_transform(data.state_id)\n        \n        return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = transform_data(data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are removing the column weekday as the dataset already have a column called dayofweek.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data.drop(columns=['weekday'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature engineering","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Since we have to predict 28 days in the future. We are making new columns to predict rolling mean and std. 28 days in the past. We calculate mean and std for both weekly basis and monthly. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_demand_features(data):\n    data['lag'] = data.groupby(['id'])['demand'].transform(lambda d: d.shift(28))\n    data['demand_mean_7d'] = data.groupby(['id'])['demand'].transform(lambda d: d.shift(28).rolling(7).mean())\n    data['demand_std_7d'] = data.groupby(['id'])['demand'].transform(lambda d: d.shift(28).rolling(7).std())\n    data['demand_mean_30d'] = data.groupby(['id'])['demand'].transform(lambda d: d.shift(28).rolling(30).mean())\n    data['demand_std_30d'] = data.groupby(['id'])['demand'].transform(lambda d: d.shift(28).rolling(30).std())\n    return data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Secondly we are creating a few new price features. With trial and error we have found that std inpacts the model the most. Therefore we calculate std. for the last 7 days and the last month. Same as above. We have also calculated the price change from day to day.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_price_features(data):\n    data['lag_1d'] = data.groupby(['id'])['sell_price'].transform(lambda x: x.shift(1))\n    data['price_change_1d'] = (data['lag_1d'] - data['sell_price']) / (data['lag_1d'])\n    data['price_std_7d'] = data.groupby(['id'])['sell_price'].transform(lambda d: d.rolling(7).std())\n    data['price_std_30d'] = data.groupby(['id'])['sell_price'].transform(lambda d: d.rolling(30).std())\n    data.drop(['lag_1d'], inplace = True, axis = 1)\n    return data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The column 'date' was in a format that our model could not use. Therefore we use pandas 'to_datetime' to make in into a value that the model can accept. And while we are at it, we added a column for each type of datetime.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_time_features(data):\n    data['date'] = pd.to_datetime(data['date'])\n    data['year'] = data['date'].dt.year\n    data['month'] = data['date'].dt.month\n    data['week'] = data['date'].dt.week\n    data['day'] = data['date'].dt.day\n    data['dayofweek'] = data['date'].dt.dayofweek\n    return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = create_demand_features(data)\ndata = create_price_features(data)\ndata = create_time_features(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = reduce_mem_usage(data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have now cleaned all of our data and are now ready to create a model and predict.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Predictive model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgb\nfrom sklearn.metrics import mean_squared_error","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"We are importing lightgbm, which is a gradient boosting machine so we can use it to predict.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"features = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'year', 'month', 'week', 'day', 'dayofweek', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', \n            'snap_CA', 'snap_TX', 'snap_WI', 'sell_price', 'lag', 'demand_mean_7d', 'demand_std_7d', 'demand_mean_30d', 'demand_std_30d', 'price_change_1d', 'price_std_7d', 'price_std_30d']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n        \"objective\" : \"poisson\",\n        \"metric\" :\"rmse\",\n        \"learning_rate\" : 0.1,\n        'num_iterations' : 200,\n        'num_leaves': 128,\n}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have chosen to use lightgbm as our prediction model. The reason for this choice is space consumption. Lightgbm is a gradient boosting framework that uses tree based learning altorithm. The model uses low memory to run which we were looking for, since our kaggle kept restarting. \nLightgbm's documentations states that it is a framework to use on larger datasets. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_lgb(data):\n    # going to evaluate with the last 28 days\n    x_train = data[data['date'] <= '2016-03-27']\n    y_train = x_train['demand']\n    x_test = data[data['part']=='test']\n    y_test = x_test['demand']\n    prediction_set = data[data['part']=='test']\n    \n    train_set = lgb.Dataset(x_train[features], y_train)\n    test_set = lgb.Dataset(x_test[features], y_test)\n    \n    del x_train, y_train\n\n    model = lgb.train(params, train_set, valid_sets = [train_set, test_set], verbose_eval = 20)\n    test_pred = model.predict(x_test[features])\n    test_score = np.sqrt(mean_squared_error(test_pred, y_test))\n    print(f'the test root-mean-square error score is {test_score}')\n    y_pred = model.predict(prediction_set[features])\n    prediction_set['demand'] = y_pred\n    return prediction_set","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction_set = run_lgb(data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this final section of the report we create the final submission file. And make it out to be a csv file for submission.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(prediction_set, submission):\n    predictions = prediction_set[['id', 'date', 'demand']]\n    predictions = pd.pivot(predictions, index = 'id', columns = 'date', values = 'demand').reset_index()\n    predictions.columns = ['id'] + ['F' + str(i + 1) for i in range(28)]\n\n    evaluation_rows = [row for row in submission['id'] if 'evaluation' in row] \n    evaluation = submission[submission['id'].isin(evaluation_rows)]\n\n    validation = submission[['id']].merge(predictions, on = 'id')\n    final = pd.concat([validation, evaluation])\n    return final","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final = predict(prediction_set,sample_submission) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final.to_csv('Submission_2.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}