{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Context\n\nTraditionally we transfer categorical variables to one-hot encoding. The disadvantage is that this assumes independence among entities & creates a sparse matrix. This can lead to out-of-memory (OOM) while training. \n\nEntity embeddings instead use a vector to represent each entity. So each categorical variable is represented as a vector of floating points. Embeddings can does capture richer relationships & complexities. \n\n\n**Related paper**\n\n[Entity Embeddings of Categorical Variables](https://arxiv.org/abs/1604.06737) \n\nRachal Thomas' blog on Fast.ai [Categorical Embeddings](https://www.fast.ai/2018/04/29/categorical-embeddings/)\n\n# Process flow\n\n>> I am running this on CPU, but you can add a GPU for faster training (*around 3x speedup*)\n\n1. Feature engineering is done for each store (store_id). Features are similar to this [notebook by kkiler](https://www.kaggle.com/kneroma/m5-first-public-notebook-under-0-50) \n2. `TabularList` is used to create a DataBunch for store_id = 1\n3. Model is trained for a single store\n4. Separate models are trained for each store (*Not shown here. Models are stored in a public dataset*)\n5. Test data is processed\n6. [Trained models](https://www.kaggle.com/skylord/m5-forecasting-models) are used to then predict for each store in the test data\n\n","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from  datetime import datetime, timedelta\nimport time\nimport gc\nimport numpy as np, pandas as pd\nimport gc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from fastai import *      # import * is considered as a bad coding practice! will have to change this!\nfrom fastai.tabular import *","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"start_nb = time.time()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"CAL_DTYPES={\"event_name_1\": \"category\", \"event_name_2\": \"category\", \"event_type_1\": \"category\", \n         \"event_type_2\": \"category\", \"weekday\": \"category\", 'wm_yr_wk': 'int16', \"wday\": \"int16\",\n        \"month\": \"int16\", \"year\": \"int16\", \"snap_CA\": \"float32\", 'snap_TX': 'float32', 'snap_WI': 'float32' }\nPRICE_DTYPES = {\"store_id\": \"category\", \"item_id\": \"category\", \"wm_yr_wk\": \"int16\",\"sell_price\":\"float32\" }\n\npd.options.display.max_columns = 50\nh = 28 \nmax_lags = 70\ntr_last = 1913\nfday = datetime(2016,4, 25) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Define helper functions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_dt(is_train = True, nrows = None, first_day = 1200, store_id = None):\n    \n    start = time.time()\n    if store_id == None and is_train:\n        print(\"ERROR: No store_id provided.Please provide an id [0-9]\")\n        return None\n        \n    prices = pd.read_csv(\"../input/m5-forecasting-accuracy/sell_prices.csv\", dtype = PRICE_DTYPES)\n    \n    \n    for col, col_dtype in PRICE_DTYPES.items():\n        if col_dtype == \"category\":\n            prices[col] = prices[col].cat.codes.astype(\"int16\")\n            prices[col] -= prices[col].min()\n            \n    cal = pd.read_csv(\"../input/m5-forecasting-accuracy/calendar.csv\", dtype = CAL_DTYPES)\n    cal[\"date\"] = pd.to_datetime(cal[\"date\"])\n    for col, col_dtype in CAL_DTYPES.items():\n        if col_dtype == \"category\":\n            cal[col] = cal[col].cat.codes.astype(\"int16\")\n            cal[col] -= cal[col].min()\n    \n    # Filter out the values for store_id\n    if is_train:\n        prices = prices[prices['store_id'] == store_id]\n        \n    print(f\"Shape of Store - {store_id} dataframe : \", prices.shape)\n    \n    \n    start_day = max(1 if is_train  else tr_last-max_lags, first_day)\n    numcols = [f\"d_{day}\" for day in range(start_day,tr_last+1)]\n    catcols = ['id', 'item_id', 'dept_id','store_id', 'cat_id', 'state_id']\n    dtype = {numcol:\"float32\" for numcol in numcols} \n    dtype.update({col: \"category\" for col in catcols if col != \"id\"})\n    dt = pd.read_csv(\"../input/m5-forecasting-accuracy/sales_train_validation.csv\", \n                     nrows = nrows, usecols = catcols + numcols, dtype = dtype)\n    \n    for col in catcols:\n        if col != \"id\":\n            dt[col] = dt[col].cat.codes.astype(\"int16\")\n            dt[col] -= dt[col].min()\n    \n    if not is_train:\n        for day in range(tr_last+1, tr_last+ 28 +1):\n            dt[f\"d_{day}\"] = np.nan\n    \n    dt = pd.melt(dt,\n                  id_vars = catcols,\n                  value_vars = [col for col in dt.columns if col.startswith(\"d_\")],\n                  var_name = \"d\",\n                  value_name = \"sales\")\n    \n    dt = dt.merge(cal, on= \"d\", copy = False)\n    dt = dt.merge(prices, on = [\"store_id\", \"item_id\", \"wm_yr_wk\"], copy = False)\n    end = time.time()\n    \n    print(\"Processing time: \", (end-start))\n    return dt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_fea(dt):\n    start = time.time()\n    lags = [7, 28]\n    lag_cols = [f\"lag_{lag}\" for lag in lags ]\n    for lag, lag_col in zip(lags, lag_cols):\n        dt[lag_col] = dt[[\"id\",\"sales\"]].groupby(\"id\")[\"sales\"].shift(lag)\n\n    wins = [7, 28]\n    for win in wins :\n        for lag,lag_col in zip(lags, lag_cols):\n            dt[f\"rmean_{lag}_{win}\"] = dt[[\"id\", lag_col]].groupby(\"id\")[lag_col].transform(lambda x : x.rolling(win).mean())\n   \n    \n    date_features = {\n        \n        \"wday\": \"weekday\",\n        \"week\": \"weekofyear\",\n        \"month\": \"month\",\n        \"quarter\": \"quarter\",\n        \"year\": \"year\",\n        \"mday\": \"day\",\n#         \"ime\": \"is_month_end\",\n#         \"ims\": \"is_month_start\",\n    }\n    \n#     dt.drop([\"d\", \"wm_yr_wk\", \"weekday\"], axis=1, inplace = True)\n    \n    for date_feat_name, date_feat_func in date_features.items():\n        if date_feat_name in dt.columns:\n            dt[date_feat_name] = dt[date_feat_name].astype(\"int16\")\n        else:\n            dt[date_feat_name] = getattr(dt[\"date\"].dt, date_feat_func).astype(\"int16\")\n    \n    # Drop NA values\n    dt.dropna(inplace = True)     \n    \n    # Sort the dataframe on 'saledate' so we can easily create a validation set that data is in the 'future' of what's in the training set\n    dt = dt.sort_values(by='date', ascending=False)\n    dt = dt.reset_index(drop=True)\n    end = time.time()\n    print(\"Processing time: \", (end- start))\n    \n    return dt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import shutil\n\ndef checkDiskSpace():\n    total, used, free = shutil.disk_usage(\"/\")\n    \n    free = (free // (2**30))\n    \n    if free < 10:\n        return -1\n    else:\n        return 0 \n         \n\n# print(\"Total: %d GiB\" % (total // (2**30)))\n# print(\"Used: %d GiB\" % (used // (2**30)))\n# print(\"Free: %d GiB\" % (free // (2**30)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Pre-processing of train data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_feats = ['item_id', 'dept_id','store_id', 'cat_id', 'state_id'] + [\"event_name_1\", \"event_name_2\", \"event_type_1\", \"event_type_2\"]\nuseless_cols = [\"id\", \"date\", \"sales\",\"d\", \"wm_yr_wk\", \"weekday\"]\n#train_cols = df.columns[~df.columns.isin(useless_cols)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"FIRST_DAY = 1 # If you want to load all the data set it to '1' -->  Great  memory overflow  risk !","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"idx = 1\ntrain_df = create_dt(is_train=True, first_day= FIRST_DAY, store_id = idx)\ntrain_df = create_fea(train_df)\nprint(train_df.shape)\nprint(train_df['date'].min(), train_df['date'].max())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Sort by date (used for train/validation splits)\ntrain_df.sort_values(by='date', inplace=True)\n\n# convert sales value to log scale\ntrain_df['sales'] = np.log(train_df['sales'] + 1)  # Taking logarithm values for sales\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Calculate where we should cut the validation set. We pick the most recent 'n' records in training set \n# where n is the number of entries in test set. \n\ncut = train_df['date'][(train_df['date'] == train_df['date'][62500])].index.max()\nprint(cut)\nvalid_idx = range(cut)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define categorical, continous & dependent variables\n\ncat_vars = ['wm_yr_wk', 'weekday', 'wday', 'month', 'year', 'event_name_1',\n           'event_type_1', 'event_name_2', 'event_type_2', 'snap_CA', 'snap_TX',\n           'snap_WI', 'week', 'quarter', 'mday']\n\ncont_vars = ['lag_7', 'lag_28', 'rmean_7_7', 'rmean_28_7',\n           'rmean_7_28', 'rmean_28_28', 'sell_price']\n\ndep_var = 'sales'\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We want to limit the price range for our prediction to be within the history sale price range, so we need to calculate the y_range\n# Note that we multiplied the maximum of 'SalePrice' by 1.2 so when we apply sigmoid, the upper limit will also be covered. \nmax_y = np.max(train_df['sales'])*1.2\ny_range = torch.tensor([0, max_y], device=defaults.device)\nprint(y_range)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model training","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Defining pre-processing we want for our fast.ai DataBunch\nprocs=[FillMissing, Categorify, Normalize]\n\n# Use fast.ai datablock api to put our training data into the DataBunch, getting ready for training\ndata = (TabularList.from_df(train_df, cat_names=cat_vars, cont_names=cont_vars, procs=procs)\n                       .split_by_idx(valid_idx)\n                       .label_from_df(dep_var)\n                       .databunch())\n\n\n# Create our tabular learner. The dense layer is 1000 and 500 two layer NN. We used dropout, hai \nlearn = tabular_learner(data, layers=[512,256, 128], ps=[0.05,0.01, 0.5], emb_drop=0.04, \n                            y_range=y_range, metrics=rmse)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.lr_find()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the learning rates\nlearn.recorder.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# learn.fit_one_cycle(3, 1e-2, wd=0.2)\n    \n# print(f\"Saving model...export_{idx}\")\n# learn.export(file = Path(f\"/kaggle/working/export_{idx}.pkl\")) # Save the model\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn = load_learner('/kaggle/input/m5-forecasting-models/', file=f'export_{idx}.pkl')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nte = create_dt(False)\nte = create_fea(te)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(te.shape)\nte.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Lets predict with a sample test row","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.predict(te.loc[1])[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Raw prediction: \",learn.predict(te.loc[1]))\nprint(\"Taking exponentials: \",(np.exp(learn.predict(te.loc[1])[1]) -1) )\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"end_nb = time.time()\n\nprint(\"Notebook processing time: \", (end_nb- start_nb))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}