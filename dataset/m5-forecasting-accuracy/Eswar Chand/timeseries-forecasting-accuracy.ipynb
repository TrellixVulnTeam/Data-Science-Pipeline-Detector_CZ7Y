{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Importing all Necessary Packages"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import os\nimport gc\nimport time\nimport math\nimport datetime\nfrom math import log, floor\nfrom sklearn.neighbors import KDTree\n\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom sklearn.utils import shuffle\nfrom tqdm.notebook import tqdm as tqdm\n\nimport seaborn as sns\nfrom matplotlib import colors\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import Normalize\n\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\n\nimport pywt\nfrom statsmodels.robust import mad\n\nimport scipy\nimport statsmodels\nfrom scipy import signal\nimport statsmodels.api as sm\nfrom fbprophet import Prophet\nfrom scipy.signal import butter, deconvolve\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_memory_usage():\n    return np.round(psutil.Process(os.getpid()).memory_info()[0]/2.**30, 2) \n        \ndef sizeof_fmt(num, suffix='B'):\n    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n        if abs(num) < 1024.0:\n            return \"%3.1f%s%s\" % (num, unit, suffix)\n        num /= 1024.0\n    return \"%.1f%s%s\" % (num, 'Yi', suffix)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Memory Reducer\n# :df pandas dataframe to reduce size             # type: pd.DataFrame()\n# :verbose                                        # type: bool\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                       df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def merge_by_concat(df1, df2, merge_on):\n    merged_gf = df1[merge_on]\n    merged_gf = merged_gf.merge(df2, on=merge_on, how='left')\n    new_columns = [col for col in list(merged_gf) if col not in merge_on]\n    df1 = pd.concat([df1, merged_gf[new_columns]], axis=1)\n    return df1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TARGET = 'sales'         # Our main target\nEND_TRAIN = 1913         # Last day in train set\nMAIN_INDEX = ['id','d']  # We can identify item by these columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train_val = pd.read_csv('../input/m5-forecasting-accuracy/sales_train_validation.csv')\nselling_prices= pd.read_csv('../input/m5-forecasting-accuracy/sell_prices.csv')\ncalendar= pd.read_csv('../input/m5-forecasting-accuracy/calendar.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train_val.sample(6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"selling_prices.sample(6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"calendar.sample(6)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"ids = sorted(list(set(sales_train_val['id'])))\nd_cols = [c for c in sales_train_val.columns if 'd_' in c]\nx_1 = sales_train_val.loc[sales_train_val['id'] == ids[1]].set_index('id')[d_cols].values[0]\nx_2 = sales_train_val.loc[sales_train_val['id'] == ids[2]].set_index('id')[d_cols].values[0]\nx_3 = sales_train_val.loc[sales_train_val['id'] == ids[3]].set_index('id')[d_cols].values[0]\nx_4 = sales_train_val.loc[sales_train_val['id'] == ids[4]].set_index('id')[d_cols].values[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(x_1)\nprint(x_2)\nprint(x_3)\nprint(x_4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#ids[1],ids[2],ids[3] are random samples , you can choose any number\nfig = make_subplots(rows=2, cols=2)\n\nfig.add_trace(go.Scatter(x=np.arange(len(x_1)), y=x_1, showlegend=False,\n                    mode='lines', name=\"First sample\",\n                         marker=dict(color=\"mediumseagreen\")),\n             row=1, col=1)\n\nfig.add_trace(go.Scatter(x=np.arange(len(x_2)), y=x_2, showlegend=False,\n                    mode='lines', name=\"Second sample\",\n                         marker=dict(color=\"violet\")),\n             row=1, col=2)\n\nfig.add_trace(go.Scatter(x=np.arange(len(x_3)), y=x_3, showlegend=False,\n                    mode='lines', name=\"Third sample\",\n                         marker=dict(color=\"dodgerblue\")),\n             row=2, col=1)\n\nfig.add_trace(go.Scatter(x=np.arange(len(x_4)), y=x_4, showlegend=False,\n                    mode='lines', name=\"Third sample\",\n                         marker=dict(color=\"pink\")),\n             row=2, col=2)\nfig.update_layout(height=1200, width=800, title_text=\"Sample sales\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These are sales data from randomly selected stores in the dataset. As expected, the sales data is very erratic, owing to the fact that so many factors affect the sales on a given day. On certain days, the sales quantity is zero, which indicates that a certain product may not be available on that day."},{"metadata":{"trusted":true},"cell_type":"code","source":"#ids[1],ids[2],ids[3] are random samples , you can choose any number\nx_1 = sales_train_val.loc[sales_train_val['id'] == ids[1]].set_index('id')[d_cols].values[0][:90]\nx_2 = sales_train_val.loc[sales_train_val['id'] == ids[2]].set_index('id')[d_cols].values[0][0:90]\nx_3 = sales_train_val.loc[sales_train_val['id'] == ids[3]].set_index('id')[d_cols].values[0][0:90]\nx_4 = sales_train_val.loc[sales_train_val['id'] == ids[4]].set_index('id')[d_cols].values[0][0:90]\nfig = make_subplots(rows=2, cols=2)\n\nfig.add_trace(go.Scatter(x=np.arange(len(x_1)), y=x_1, showlegend=False,\n                    mode='lines+markers', name=\"First sample\",\n                         marker=dict(color=\"mediumseagreen\")),\n             row=1, col=1)\n\nfig.add_trace(go.Scatter(x=np.arange(len(x_2)), y=x_2, showlegend=False,\n                    mode='lines+markers', name=\"Second sample\",\n                         marker=dict(color=\"violet\")),\n             row=1, col=2)\n\nfig.add_trace(go.Scatter(x=np.arange(len(x_3)), y=x_3, showlegend=False,\n                    mode='lines+markers', name=\"Third sample\",\n                         marker=dict(color=\"dodgerblue\")),\n             row=2, col=1)\n\nfig.add_trace(go.Scatter(x=np.arange(len(x_4)), y=x_4, showlegend=False,\n                    mode='lines+markers', name=\"Fourth sample\",\n                         marker=dict(color=\"pink\")),\n             row=2, col=2)\n\nfig.update_layout(height=1200, width=800, title_text=\"Sample sales snippets\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the above plots, I simply zoom in to sample snippets in the sales data. As stated earlier, we can clearly see that the sales data is very erratic and volatile. Sometimes, the sales are zero for a few days in a row, and at other times, it remains at its peak value for a few days. Therefore, we need some sort of \"denoising\" techniques to find the underlying trends in the sales data and make forecasts."},{"metadata":{},"cell_type":"markdown","source":"So lets perform denoising methods to extract the underlying pattern"},{"metadata":{},"cell_type":"markdown","source":"# Wavelet Denoising"},{"metadata":{},"cell_type":"markdown","source":"Wavelet denoising (usually used with electric signals) is a way to remove the unnecessary noise from a time series. This method calculates coefficients called the \"wavelet coefficients\". These coefficients decide which pieces of information to keep (signal) and which ones to discard (noise).\n\nWe make use of the MAD (mean absolute deviation) value to understand the randomness in the sales and accordingly decide the minimum threshold for the wavelet coefficients in the time series. We filter out the low coefficients from the wavelets and reconstruct the sales data from the remaining coefficients and that's it; we have successfully removed noise from the sales data."},{"metadata":{"trusted":true},"cell_type":"code","source":"def maddest(d, axis=None):\n    return np.mean(np.absolute(d - np.mean(d, axis)), axis)\n\ndef denoise_signal(x, wavelet='db4', level=1):\n    coeff = pywt.wavedec(x, wavelet, mode=\"per\")\n    sigma = (1/0.6745) * maddest(coeff[-level])\n\n    uthresh = sigma * np.sqrt(2*np.log(len(x)))\n    coeff[1:] = (pywt.threshold(i, value=uthresh, mode='hard') for i in coeff[1:])\n\n    return pywt.waverec(coeff, wavelet, mode='per')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_w1 = denoise_signal(x_1)\ny_w2 = denoise_signal(x_2)\ny_w3 = denoise_signal(x_3)\ny_w4= denoise_signal(x_4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = make_subplots(rows=4, cols=1)\n\nfig.add_trace(go.Scatter(x=np.arange(len(x_1)), y=x_1, showlegend=False,\n                    mode='lines+markers', name=\"First sample\",\n                         marker=dict(color=\"mediumseagreen\")),\n             row=1, col=1)\n\nfig.add_trace(go.Scatter(x=np.arange(len(x_1)), y=y_w1, showlegend=False,\n                    mode='lines+markers', name=\"First sample\",\n                         marker=dict(color=\"black\")),\n             row=1, col=1)\n\nfig.add_trace(go.Scatter(x=np.arange(len(x_2)), y=x_2, showlegend=False,\n                    mode='lines+markers', name=\"Second sample\",\n                         marker=dict(color=\"violet\")),\n             row=2, col=1)\n\nfig.add_trace(go.Scatter(x=np.arange(len(x_2)), y=y_w2, showlegend=False,\n                    mode='lines+markers', name=\"Second sample\",\n                         marker=dict(color=\"black\")),\n             row=2, col=1)\n\nfig.add_trace(go.Scatter(x=np.arange(len(x_3)), y=x_3, showlegend=False,\n                    mode='lines+markers', name=\"Third sample\",\n                         marker=dict(color=\"dodgerblue\")),\n             row=3, col=1)\n\nfig.add_trace(go.Scatter(x=np.arange(len(x_3)), y=y_w3, showlegend=False,\n                    mode='lines+markers', name=\"Third sample\",\n                         marker=dict(color=\"black\")),\n             row=3, col=1)\n\nfig.add_trace(go.Scatter(x=np.arange(len(x_4)), y=x_4, showlegend=False,\n                    mode='lines+markers', name=\"Fourth sample\",\n                         marker=dict(color=\"pink\")),\n             row=4, col=1)\n\nfig.add_trace(go.Scatter(x=np.arange(len(x_4)), y=y_w4, showlegend=False,\n                    mode='lines+markers', name=\"Fourth sample\",\n                         marker=dict(color=\"black\")),\n             row=4, col=1)\n\nfig.update_layout(height=1200, width=800, title_text=\"Sample sales snippets\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the above graphs, the black lineplots represent the denoised sales and the light lineplots represent the original sales. We can see that Wavelet denoising is able to successfully find the \"general trend\" in the sales data without getting distracted by the noise. Finding these high- trends or patterns in the sales may be useful in generating features to train a model."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(nrows=4, ncols=2, figsize=(30, 20))\n\nax[0, 0].plot(x_1, color='seagreen', marker='o') \nax[0, 0].set_title('Original Sales', fontsize=24)\nax[0, 1].plot(y_w1, color='red', marker='.') \nax[0, 1].set_title('After Wavelet Denoising', fontsize=24)\n\nax[1, 0].plot(x_2, color='seagreen', marker='o') \nax[1, 0].set_title('Original Sales', fontsize=24)\nax[1, 1].plot(y_w2, color='red', marker='.') \nax[1, 1].set_title('After Wavelet Denoising', fontsize=24)\n\nax[2, 0].plot(x_3, color='seagreen', marker='o') \nax[2, 0].set_title('Original Sales', fontsize=24)\nax[2, 1].plot(y_w3, color='red', marker='.') \nax[2, 1].set_title('After Wavelet Denoising', fontsize=24)\n\nax[3, 0].plot(x_4, color='seagreen', marker='o') \nax[3, 0].set_title('Original Sales', fontsize=24)\nax[3, 1].plot(y_w4, color='red', marker='.') \nax[3, 1].set_title('After Wavelet Denoising', fontsize=24)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Here the green graphs represents original sales and red graphs represents denoised sales"},{"metadata":{},"cell_type":"markdown","source":"# Average Smoothing"},{"metadata":{},"cell_type":"markdown","source":"Average smooting is a relatively simple way to denoise time series data. In this method, we take a \"window\" with a fixed size (like 10). We first place the window at the beginning of the time series (first ten elements) and calculate the mean of that section. We now move the window across the time series in the forward direction by a particular \"stride\", calculate the mean of the new window and repeat the process, until we reach the end of the time series. All the mean values we calculated are then concatenated into a new time series, which forms the denoised sales data."},{"metadata":{"trusted":true},"cell_type":"code","source":"def average_smoothing(signal, kernel_size=3, stride=1):\n    sample = []\n    start = 0\n    end = kernel_size\n    while end <= len(signal):\n        start = start + stride\n        end = end + stride\n        sample.extend(np.ones(end - start)*np.mean(signal[start:end]))\n    return np.array(sample)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_a1 = average_smoothing(x_1)\ny_a2 = average_smoothing(x_2)\ny_a3 = average_smoothing(x_3)\ny_a4= average_smoothing(x_4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = make_subplots(rows=4, cols=1)\n\nfig.add_trace(go.Scatter(x=np.arange(len(x_1)), y=x_1, showlegend=False,\n                    mode='lines+markers', name=\"First sample\",\n                         marker=dict(color=\"mediumseagreen\")),\n             row=1, col=1)\n\nfig.add_trace(go.Scatter(x=np.arange(len(x_1)), y=y_a1, showlegend=False,\n                    mode='lines+markers', name=\"First sample\",\n                         marker=dict(color=\"black\")),\n             row=1, col=1)\n\nfig.add_trace(go.Scatter(x=np.arange(len(x_2)), y=x_2, showlegend=False,\n                    mode='lines+markers', name=\"Second sample\",\n                         marker=dict(color=\"violet\")),\n             row=2, col=1)\n\nfig.add_trace(go.Scatter(x=np.arange(len(x_2)), y=y_a2, showlegend=False,\n                    mode='lines+markers', name=\"Second sample\",\n                         marker=dict(color=\"black\")),\n             row=2, col=1)\n\nfig.add_trace(go.Scatter(x=np.arange(len(x_3)), y=x_3, showlegend=False,\n                    mode='lines+markers', name=\"Third sample\",\n                         marker=dict(color=\"dodgerblue\")),\n             row=3, col=1)\n\nfig.add_trace(go.Scatter(x=np.arange(len(x_3)), y=y_a3, showlegend=False,\n                    mode='lines+markers', name=\"Third sample\",\n                         marker=dict(color=\"black\")),\n             row=3, col=1)\n\nfig.add_trace(go.Scatter(x=np.arange(len(x_4)), y=x_4, showlegend=False,\n                    mode='lines+markers', name=\"Fourth sample\",\n                         marker=dict(color=\"pink\")),\n             row=4, col=1)\n\nfig.add_trace(go.Scatter(x=np.arange(len(x_4)), y=y_a4, showlegend=False,\n                    mode='lines+markers', name=\"Fourth sample\",\n                         marker=dict(color=\"black\")),\n             row=4, col=1)\n\nfig.update_layout(height=1200, width=800, title_text=\"Sample sales snippets\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the above graphs, the black lineplots represent the denoised sales and the light lineplots represent the original sales. We can see that average smoothing is not as effective as Wavelet denoising at finding macroscopic trends and pattersns in the data. A lot of the noise in the original sales persists even after denoising. Therefore, wavelet denoising is clearly more effective at finding trends in the sales data. Nonetheless, average smoothing or \"rolling mean\" can also be used to calculate useful features for modeling"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(nrows=4, ncols=2, figsize=(30, 20))\n\nax[0, 0].plot(x_1, color='seagreen', marker='o') \nax[0, 0].set_title('Original Sales', fontsize=24)\nax[0, 1].plot(y_a1, color='red', marker='.') \nax[0, 1].set_title('After Wavelet Denoising', fontsize=24)\n\nax[1, 0].plot(x_2, color='seagreen', marker='o') \nax[1, 0].set_title('Original Sales', fontsize=24)\nax[1, 1].plot(y_a2, color='red', marker='.') \nax[1, 1].set_title('After Wavelet Denoising', fontsize=24)\n\nax[2, 0].plot(x_3, color='seagreen', marker='o') \nax[2, 0].set_title('Original Sales', fontsize=24)\nax[2, 1].plot(y_a3, color='red', marker='.') \nax[2, 1].set_title('After Wavelet Denoising', fontsize=24)\n\nax[3, 0].plot(x_4, color='seagreen', marker='o') \nax[3, 0].set_title('Original Sales', fontsize=24)\nax[3, 1].plot(y_a4, color='red', marker='.') \nax[3, 1].set_title('After Wavelet Denoising', fontsize=24)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here the green graphs represent original sales and red graphs represents Smoothened sales.\nAs compared to the weavelet denoising, smoothening is not so useful to draw insights from the pattern"},{"metadata":{},"cell_type":"markdown","source":"# Stores and Sales"},{"metadata":{},"cell_type":"markdown","source":"Rolling Average Price vs. Time for each store"},{"metadata":{"trusted":true},"cell_type":"code","source":"past_sales = sales_train_val.set_index('id')[d_cols] \\\n    .T \\\n    .merge(calendar.set_index('d')['date'],\n           left_index=True,\n           right_index=True,\n            validate='1:1') \\\n    .set_index('date')\n\nstore_list = selling_prices['store_id'].unique()\nmeans = []\nfig = go.Figure()\nfor s in store_list:\n    store_items = [c for c in past_sales.columns if s in c]\n    data = past_sales[store_items].sum(axis=1).rolling(90).mean()\n    means.append(np.mean(past_sales[store_items].sum(axis=1)))\n    fig.add_trace(go.Scatter(x=np.arange(len(data)), y=data, name=s))\n    \nfig.update_layout(yaxis_title=\"Sales\", xaxis_title=\"Time\", title=\"Rolling Average Sales vs. Time (per store)\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the above graph, I have plotted rolling sales across all stores in the dataset. Almost every sales curve has \"linear oscillation\" trend at the macroscopic level. Basically, the sales oscillate like a sine wave about a certain mean value, but this mean value has an upward linear trend. This implies that the sales are oscillating at a higher and higher level every few months.\n\nThis trend is reminiscent of the business cycle, where economies have short-term oscillatory fluctuations but grow linearly in the long run. Maybe, such small-scale trends at the level of stores add up to decide trends we see at the macroeconomic level. Below is an illustration of the macroeconomic business cycle:"},{"metadata":{},"cell_type":"markdown","source":"Mean Sales Vs. Store name"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.DataFrame(np.transpose([means, store_list]))\ndf.columns = [\"Mean sales\", \"Store name\"]\npx.bar(df, y=\"Mean sales\", x=\"Store name\", color=\"Store name\", title=\"Mean sales vs. Store name\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Rolling Average Sales vs. Time (California)"},{"metadata":{"trusted":true},"cell_type":"code","source":"greens = [\"mediumaquamarine\", \"mediumseagreen\", \"seagreen\", \"green\"]\nstore_list = selling_prices['store_id'].unique()\nfig = go.Figure()\nmeans = []\nstores = []\nfor i, s in enumerate(store_list):\n    if \"ca\" in s or \"CA\" in s:\n        store_items = [c for c in past_sales.columns if s in c]\n        data = past_sales[store_items].sum(axis=1).rolling(90).mean()\n        means.append(np.mean(past_sales[store_items].sum(axis=1)))\n        stores.append(s)\n        fig.add_trace(go.Scatter(x=np.arange(len(data)), y=data, name=s, marker=dict(color=greens[i])))\n    \nfig.update_layout(yaxis_title=\"Sales\", xaxis_title=\"Time\", title=\"Rolling Average Sales vs. Time (California)\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the above graph, we can see the large disparity in sales among California stores. The sales curves almost never intersect each other. This may indicate that there are certain \"hubs\" of development in California which do not change over time. And other areas always remain behind these \"hubs\". The average sales in descending order are CA_3, CA_1, CA_2, CA_4. The store CA_3 has maximum sales while the store CA_4 has minimum sales."},{"metadata":{},"cell_type":"markdown","source":"Mean Sales Vs. Store name( California)"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.DataFrame(np.transpose([means, stores]))\ndf.columns = [\"Mean sales\", \"Store name\"]\npx.bar(df, y=\"Mean sales\", x=\"Store name\", color=\"Store name\", title=\"Mean sales vs. Store name\", color_continuous_scale=greens)\n\n\nfig = go.Figure(data=[\n    go.Bar(name='', x=stores, y=means, marker={'color' : greens})])\n\nfig.update_layout(title=\"Mean sales vs. Store name (California)\", yaxis=dict(title=\"Mean sales\"), xaxis=dict(title=\"Store name\"))\nfig.update_layout(barmode='group')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the above plots, we can see the same relationship. The store CA_3 has maximum sales while the store CA_4 has minimum sales."},{"metadata":{},"cell_type":"markdown","source":"# Modeling"},{"metadata":{},"cell_type":"markdown","source":"Train/Val split\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset = sales_train_val[d_cols[-100:-30]]\nval_dataset = sales_train_val[d_cols[-30:]]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = make_subplots(rows=3, cols=1)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70), mode='lines', y=train_dataset.loc[0].values, marker=dict(color=\"dodgerblue\"), showlegend=False,\n               name=\"Original signal\"),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=val_dataset.loc[0].values, mode='lines', marker=dict(color=\"darkorange\"), showlegend=False,\n               name=\"Denoised signal\"),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70), mode='lines', y=train_dataset.loc[1].values, marker=dict(color=\"dodgerblue\"), showlegend=False),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=val_dataset.loc[1].values, mode='lines', marker=dict(color=\"darkorange\"), showlegend=False),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70), mode='lines', y=train_dataset.loc[2].values, marker=dict(color=\"dodgerblue\"), showlegend=False),\n    row=3, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=val_dataset.loc[2].values, mode='lines', marker=dict(color=\"darkorange\"), showlegend=False),\n    row=3, col=1\n)\n\nfig.update_layout(height=1200, width=800, title_text=\"Train (blue) vs. Validation (orange) sales\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"ARIMA"},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = []\nfor row in tqdm(train_dataset[train_dataset.columns[-30:]].values[:3]):\n    fit = sm.tsa.statespace.SARIMAX(row, seasonal_order=(0, 1, 1, 7)).fit()\n    predictions.append(fit.forecast(30))\npredictions = np.array(predictions).reshape((-1, 30))\nerror_arima = np.linalg.norm(predictions[:3] - val_dataset.values[:3])/len(predictions[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_1 = predictions[0]\npred_2 = predictions[1]\npred_3 = predictions[2]\n\nfig = make_subplots(rows=3, cols=1)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70), mode='lines', y=train_dataset.loc[0].values, marker=dict(color=\"dodgerblue\"),\n               name=\"Train\"),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=val_dataset.loc[0].values, mode='lines', marker=dict(color=\"darkorange\"),\n               name=\"Val\"),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=pred_1, mode='lines', marker=dict(color=\"seagreen\"),\n               name=\"Pred\"),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70), mode='lines', y=train_dataset.loc[1].values, marker=dict(color=\"dodgerblue\"), showlegend=False),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=val_dataset.loc[1].values, mode='lines', marker=dict(color=\"darkorange\"), showlegend=False),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=pred_2, mode='lines', marker=dict(color=\"seagreen\"), showlegend=False,\n               name=\"Denoised signal\"),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70), mode='lines', y=train_dataset.loc[2].values, marker=dict(color=\"dodgerblue\"), showlegend=False),\n    row=3, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=val_dataset.loc[2].values, mode='lines', marker=dict(color=\"darkorange\"), showlegend=False),\n    row=3, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=pred_3, mode='lines', marker=dict(color=\"seagreen\"), showlegend=False,\n               name=\"Denoised signal\"),\n    row=3, col=1\n)\n\nfig.update_layout(height=1200, width=800, title_text=\"ARIMA\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this way we can predict the upcoming sales"},{"metadata":{},"cell_type":"markdown","source":"Plase upvote if you find it useful"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}