{"cells":[{"metadata":{},"cell_type":"markdown","source":"please comments about making Seq2Seq models !"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#import\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pylab as plt\nimport seaborn as sns\nfrom itertools import cycle\nimport datetime\nfrom sklearn import preprocessing, metrics\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM,Dropout\nfrom keras.layers import RepeatVector,TimeDistributed\nfrom numpy import array\nfrom keras.models import Sequential, load_model\nimport re\nfrom tqdm import tqdm\nimport os\npd.set_option('max_columns', 50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def reduce_mem_usage(df, verbose=False):\n    start_mem = df.memory_usage().sum() / 1024 ** 2\n    int_columns = df.select_dtypes(include=[\"int\"]).columns\n    float_columns = df.select_dtypes(include=[\"float\"]).columns\n    for col in int_columns:\n        df[col] = pd.to_numeric(df[col], downcast=\"integer\")\n    for col in float_columns:\n        df[col] = pd.to_numeric(df[col], downcast=\"float\")\n    end_mem = df.memory_usage().sum() / 1024 ** 2\n    if verbose:\n        print(\"Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)\".format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# データ\nINPUT_DIR = '../input/m5-forecasting-accuracy'\ntrain_sales = reduce_mem_usage(pd.read_csv(f'{INPUT_DIR}/sales_train_validation.csv'))\ncalendar = reduce_mem_usage(pd.read_csv(f'{INPUT_DIR}/calendar.csv'))\nsellp = reduce_mem_usage(pd.read_csv(f'{INPUT_DIR}/sell_prices.csv'))\nsell_prices = reduce_mem_usage(sellp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"time_series_columns = [f'd_{i}' for i in range(1, 1970)]\ntransfer_cal = pd.DataFrame(calendar[['event_name_1','event_type_1','event_name_2','event_type_2','snap_CA','snap_TX','snap_WI']].values.T, index=['event_name_1','event_type_1','event_name_2','event_type_2','snap_CA','snap_TX','snap_WI'], columns= time_series_columns)\n#欠損値を0で補完\ntransfer_cal = transfer_cal.fillna(0)\n#なんか入ってたら10を入れる\nevent_name_1_se = transfer_cal.loc['event_name_1'].apply(lambda x: x if re.search(\"^\\d+$\", str(x)) else np.nan).fillna(10)\nevent_name_2_se = transfer_cal.loc['event_name_2'].apply(lambda x: x if re.search(\"^\\d+$\", str(x)) else np.nan).fillna(10)\n#日にち変換\ncalendar['date'] = pd.to_datetime(calendar['date'])\n\n#欠損値を補完\nnan_features = ['event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']\nfor feature in nan_features:\n    calendar[feature].fillna('unknown', inplace = True)\n#エンコーディング\ncat = ['event_name_1','event_type_1','event_name_2','event_type_2','snap_CA','snap_TX','snap_WI']\nfor feature in cat:\n    encoder = preprocessing.LabelEncoder()\n    calendar[feature] = encoder.fit_transform(calendar[feature])\n    \ntransfer_cal = pd.DataFrame(calendar[['event_name_1','event_type_1','event_name_2','event_type_2','snap_CA','snap_TX','snap_WI']].values.T,index=['event_name_1','event_type_1','event_name_2','event_type_2','snap_CA','snap_TX','snap_WI'])\n\n#メインのデータとcalendarを結合した。\nprice_fea = calendar[['wm_yr_wk','date']].merge(sell_prices, on = ['wm_yr_wk'], how = 'left')\n#各商品を一行にして提出ファイルと同じようにする\nprice_fea['id'] = price_fea['item_id']+'_'+price_fea['store_id']+'_validation'\n#pivotでLSTMで使用できるように整える\ndf = price_fea.pivot(index='id',columns='date',values='sell_price')\n\n#型合わせ\ndf = reduce_mem_usage(df)\nprice_fea = reduce_mem_usage(price_fea)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_salesの商品数とdfを結合\n#正規化も考慮\n#後で正規化があってるか確認\nprice_df = train_sales.merge(df,on=['id'],how= 'left').iloc[:,1919:]\nprice_df.index = train_sales.id\nprice_df = price_df.fillna(0)\nprice_df[\"max\"] = price_df.max(axis=1)\nfor com in [x for x in price_df.columns if (x!=\"max\")]:\n    price_df[com] = (price_df[com])/price_df[\"max\"]\n    \nprice_df = price_df.drop([\"max\"],axis=1)\nprice_df = reduce_mem_usage(price_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#正規化も考慮\ntime_series_data = train_sales[[f'd_{i}' for i in range(1, 1913 + 1)]]\ntime_series_data[\"max\"] = time_series_data.max(axis=1)\npur_max = time_series_data.max(axis=1)\nfor com in tqdm([x for x in time_series_data.columns if (x!=\"max\")]):\n    time_series_data[com] = (time_series_data[com])/time_series_data[\"max\"]\n    \ntime_series_data = time_series_data.drop([\"max\"],axis=1)\ntime_series_data = reduce_mem_usage(time_series_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#正規化も考慮\ntransfer_cal[\"max\"] = transfer_cal.max(axis=1)\n\nfor com in tqdm([x for x in transfer_cal.columns if (x!=\"max\")]):\n    transfer_cal[com] = (transfer_cal[com])/transfer_cal[\"max\"]\n    \ntransfer_cal = transfer_cal.drop([\"max\"],axis=1)\ntransfer_cal = reduce_mem_usage(transfer_cal)\ntransfer_cal.columns  = price_df.columns\ntransfer_cal.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from datetime import timedelta\ndef daterange(_start, _end):\n    for n in range((_end - _start).days):\n        yield _start + timedelta(n)\n\n#わかりにくいからcolumn名を日付に変える\nstart = pd.Timestamp(2011,1,29)\nend   = pd.Timestamp(2016,4,25)\n\na = []\nfor i in daterange(start, end):\n    a.append(i)\n\ntime_series_data.columns = a\ntime_series_data.head()\ndel a,start,end","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# d_1914~d_1941を予測するためにd_1913以降の最新のデータ100個を抜き出す\nX_test_1 = []\n\nTS_test_1 = time_series_data[time_series_data.columns[-100:]]\nran = [x for x in daterange(pd.Timestamp(2016,1,16),pd.Timestamp(2016,4,25))]\nTF_test_1 = transfer_cal[ran]\nPD_test_1 = price_df[ran]\n\nfor i in tqdm(range(time_series_data.shape[0])):\n    X_test_1.append([list(t) for t in zip(TF_test_1.loc['event_name_1'],\n                                   TF_test_1.loc['event_type_1'],\n                                   TF_test_1.loc['event_name_2'],\n                                   TF_test_1.loc['event_type_2'],\n                                   TF_test_1.loc['snap_CA'],\n                                   TF_test_1.loc['snap_TX'],\n                                   TF_test_1.loc['snap_WI'],\n                                   PD_test_1.iloc[i],\n                                   TS_test_1.iloc[i][0:100],)])\n                                         \nX_test_1 = np.asarray(X_test_1, dtype=np.float32)\n\nprint(X_test_1.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# seq2seqの訓練データを作成する\n# X_train_1 : 2015/12/19 ~ 2016/3/27 の100データ\n# y_train_1 : 2016/3/28 ~ 28データ\nimport datetime\n\nX_train_1 = []\ny_train_1 = []\n\nTS_1 = time_series_data[time_series_data.columns[-128:]]\nran = [x for x in daterange(pd.Timestamp(2015,12,19),pd.Timestamp(2016,3,28))]\nTF_1 = transfer_cal[ran]\nPD_1 = price_df[ran]\n\n\nfor i in tqdm(range(time_series_data.shape[0])):\n    X_train_1.append([list(t) for t in zip(TF_1.loc['event_name_1'],\n                                   TF_1.loc['event_type_1'],\n                                   TF_1.loc['event_name_2'],\n                                   TF_1.loc['event_type_2'],\n                                   TF_1.loc['snap_CA'],\n                                   TF_1.loc['snap_TX'],\n                                   TF_1.loc['snap_WI'],\n                                   PD_1.iloc[i],\n                                   TS_1.iloc[i][0:100],)])\n\n    y_train_1.append([list(t) for t in zip(TS_1.iloc[i][100:128])])\n                                         \nX_train_1 = np.asarray(X_train_1, dtype=np.float32)\ny_train_1 = np.asarray(y_train_1, dtype=np.float32)\n\nprint(X_train_1.shape)\nprint(y_train_1.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# X_train_2 : 2015/11/22 ~ 2016/2/29 の100データ\n# y_train_2 : 2016/3/28 ~ 28データ\n\nX_train_2 = []\ny_train_2 = []\n\nran = [x for x in daterange(pd.Timestamp(2015,11,22),pd.Timestamp(2016,3,1))]\nTS_2 = time_series_data[ran]\nTF_2 = transfer_cal[ran]\nPD_2 = price_df[ran]\na = time_series_data[[x for x in daterange(pd.Timestamp(2016,3,1),pd.Timestamp(2016,3,29))]]\n\n\nfor i in tqdm(range(time_series_data.shape[0])):\n    X_train_2.append([list(t) for t in zip(TF_2.loc['event_name_1'],\n                                   TF_2.loc['event_type_1'],\n                                   TF_2.loc['event_name_2'],\n                                   TF_2.loc['event_type_2'],\n                                   TF_2.loc['snap_CA'],\n                                   TF_2.loc['snap_TX'],\n                                   TF_2.loc['snap_WI'],\n                                   PD_2.iloc[i],\n                                   TS_2.iloc[i],)])\n\n    y_train_2.append([list(t) for t in zip(a.iloc[i])])\n                                         \nX_train_2 = np.asarray(X_train_2, dtype=np.float32)\ny_train_2 = np.asarray(y_train_2, dtype=np.float32)\n\nprint(X_train_2.shape)\nprint(y_train_2.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import *\nfrom keras.models import *\nfrom keras.utils import *\nfrom keras.initializers import *\nimport tensorflow as tf\nfrom keras.optimizers import *\nimport keras\n\ndef model(tr,te,va_tr):\n    num_encoder_tokens = tr.shape[1]\n    num_decoder_tokens = 28\n    #お試しで中間層は28\n    latent_dim = 32\n\n    # <encoder>\n    encoder_inputs = Input(shape=(None,8),name='encoder_input_1')\n    e_i =BatchNormalization(axis=-1)(encoder_inputs)\n    #RNNレイヤーを複数積み重ねたい時は、各時刻で層間のデータのやり取りがあるので、(少なくとも最後の層以外は)必ずTrueにしなければならないようです。\n    encoder_out_1, state_h_1, state_c_1 = LSTM(latent_dim, return_state=True,return_sequences=True,name='encoder_LSTM_1')(e_i)\n    encoder_out_2, state_h_2, state_c_2 = LSTM(latent_dim, return_state=True,return_sequences=True,name='encoder_LSTM_2')(encoder_out_1)\n    encoder_states = [state_h_2, state_c_2]\n    # のちに使うのでencode_modelを作成しておく\n    encoder_model = Model(encoder_inputs,encoder_states)\n    #encoder_model.summary()\n\n    # <decoder>\n    decoder_inputs = Input(shape=(None,1),name='decoder_input_1')  \n    d_i=BatchNormalization(axis=-1)(decoder_inputs)\n    decoder_out_1, de_state_h_1, de_state_c_1 = LSTM(latent_dim, return_state=True,return_sequences=True,name='decoder_LSTM_1')(d_i,initial_state=encoder_states)\n    decoder_out_2, de_state_h_2, de_state_c_2 = LSTM(latent_dim, return_state=True,return_sequences=True,name='decoder_LSTM_2')(decoder_out_1)\n    decoder_outputs = Dense(1,activation='sigmoid',name='output')(decoder_out_2)\n    \n    model = Model(inputs=[encoder_inputs, decoder_inputs], outputs=decoder_outputs)\n    #model.summary()\n    model.compile(optimizer=Adam(lr=0.0001,beta_1=0.9), loss='mse')\n    \n    # model訓練\n    # seq2seqでは、decoder訓練に「一つ前の時刻データ」を入力し、「現時刻のデータ」出力するように学習する\n    model.fit([tr[:,:,:8],np.pad(te[:,1:,:],[(0,0),(1,0),(0,0)])],te,epochs=2, batch_size=5000,verbose=1)\n    \n    decoder_state_input_h = Input(shape=(latent_dim,))\n    decoder_state_input_c = Input(shape=(latent_dim,))\n    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n    \n    decoder_outputs, state_h, state_c = LSTM(latent_dim, return_state=True,return_sequences=True,name='decoder_LSTM_2')(decoder_inputs, initial_state=decoder_states_inputs)\n    decoder_states = [state_h, state_c]\n    decoder_outputs = Dense(1,activation='sigmoid',name='output')(decoder_outputs)\n\n    decoder_model = Model([decoder_inputs] + decoder_states_inputs,[decoder_outputs] + decoder_states)\n    \n    # <予測>\n    states_value = encoder_model.predict(va_tr[:,:,:8])\n    target_seq = np.zeros((30490,1,1))\n    target_seq[[0, 0, 1]]= 1.\n\n    for i in range(0,28):\n        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n\n        # Update states\n        states_value = [h, c]\n        if i==0:\n            a = output_tokens\n        else:\n            a = np.concatenate([a, output_tokens], 1)\n\n        target_seq = output_tokens\n    \n    \n    return a","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# まず第一フェーズの予測をする\npred = model(X_train_2,y_train_2,X_train_1)\nfrom sklearn.metrics import mean_absolute_error\nprint(\"mas : {}\".format(mean_absolute_error(pred.reshape([30490*28,1]),y_train_1.reshape([30490*28,1]))))\n\nvalidation_prediction = model(X_train_1,y_train_1,X_test_1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 第一フェーズの予測を組み合わせて、第二フェーズで使用すべきデータを準備する\n\n# eva_test : ~ d_1913　の最新データ100個\n\nX_test_2 = []\ny_test_2 = []\neva_test = []\n\nfor i in tqdm(range(time_series_data.shape[0])):\n    X_test_2.append([list(t) for t in zip(transfer_cal.loc['event_name_1'],\n                                   transfer_cal.loc['event_type_1'][-128:-28],\n                                   transfer_cal.loc['event_name_2'][-128:-28],\n                                   transfer_cal.loc['event_type_2'][-128:-28],\n                                   transfer_cal.loc['snap_CA'][-128:-28],\n                                   transfer_cal.loc['snap_TX'][-128:-28],\n                                   transfer_cal.loc['snap_WI'][-128:-28],\n                                   price_df.iloc[i][-128:-28],)])\n    \n    y_test_2.append([list(t) for t in zip(time_series_data.iloc[i][-72:])])\n    \n    eva_test.append([list(t) for t in zip(transfer_cal.loc['event_name_1'],\n                                   transfer_cal.loc['event_type_1'][-28:],\n                                   transfer_cal.loc['event_name_2'][-28:],\n                                   transfer_cal.loc['event_type_2'][-28:],\n                                   transfer_cal.loc['snap_CA'][-28:],\n                                   transfer_cal.loc['snap_TX'][-28:],\n                                   transfer_cal.loc['snap_WI'][-28:],\n                                   price_df.iloc[i][-28:],)])\n                                         \nX_test_2 = np.asarray(X_test_2, dtype=np.float32)               \ny_test_2 = np.asarray(y_test_2, dtype=np.float32)\neva_test = np.asarray(eva_test, dtype=np.float32) \n\ny_test_2 = np.concatenate([y_test_2,validation_prediction],1)\n\nprint(X_test_2.shape)\nprint(y_test_2.shape)\nprint(eva_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#第2フェーズの予測をする\nevaluation_prediction = model(X_test_2,y_test_2,eva_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 第一フェースのデータをまとめる\nvalidation = pd.DataFrame(validation_prediction.astype(np.float32).reshape(X_train_1.shape[0],28))\nvalidation[\"max\"] = pur_max\n\nfor com in [x for x in validation.columns if x!=\"max\"]:\n    validation[com] = validation[com]*validation[\"max\"]\n    validation[com] = validation[com].apply(lambda x:np.rint(x))\n\nvalidation = validation.drop(\"max\",axis=1) \nvalidation.columns = [\"F{}\".format(x) for x in range(1,29)]\nvalidation.insert(0,\"id\",price_df.index)\nvalidation.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 第二フェースのデータをまとめる\nevaluation = pd.DataFrame(evaluation_prediction.astype(np.float32).reshape(X_train_1.shape[0],28))\nevaluation[\"max\"] = pur_max\n\nfor com in [x for x in evaluation.columns if x!=\"max\"]:\n    evaluation[com] = evaluation[com]*evaluation[\"max\"]\n    evaluation[com] = evaluation[com].apply(lambda x:np.rint(x))\n\nevaluation = evaluation.drop(\"max\",axis=1) \nevaluation.columns = [\"F{}\".format(x) for x in range(1,29)]\nevaluation.insert(0,\"id\",[x.replace(\"validation\",'evaluation') for x in price_df.index])\nevaluation.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#提出\nsubmission = validation.append(evaluation)\nsubmission.to_csv('submission.csv', index=False)\nsubmission.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}