{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Distributed Training with Spark Demo\n\nPySpark is often used with large datasets that don't fit in memory on a single machine. Distributed training refers to traning a model over several workers across a cluster. Spark's MLLib handles distributing the machine learning training process. This process generates one model for a huge dataset.\n\nIn this demo, we'll be doing something different. We're going to train one model per group, and then scale this training process with Spark. We'll be training multiple models across multiple workers and keeping track of the performance. Pandas and Sklearn code will be wrapped in a Pandas User Defined Function (UDF) and applied by PySpark.\n\nNote that the purpose of the demo is not to create the best model, but to illustrate scaling training multiple models in a distributed fashion. As such, we won't be going in so deep into the data\n\n* [Installing Prerequisites](#installing-prerequisites)\n* [Exploring the Dataset](#exploring-dataset)\n    - [Competition Setup](#competition-setup)\n    - [Data Files](#data-files)\n    - [Initial Exploration](#initial-exploration)\n* [Compressing Timeseries](#compressing-timeseries)\n* [Binary Blob](#binary-blob)\n* [Spark Orchestration](#spark-orchestration)\n* [Performance Evaluation](#section-three)"},{"metadata":{},"cell_type":"markdown","source":"<a id = \"installing-prerequisites\"></a>\n## Installing Prerequisites (PySpark and Java 8)\n\nEven though the (documentation)[https://spark.apache.org/docs/3.0.0/#downloading] says PySpark 3+ works with Java 11, I was running into some errors with Pandas_UDFs and PyArrow types so I just decided to install Java 8 instead. In general, PySpark with Java 8 will be more stable."},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# Install java 8\n! apt remove -y openjdk-11-jre-headless\n! apt install -y openjdk-8-jdk openjdk-8-jre\n\n# Check version\n! java -version","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Install pyspark\n!pip install pyspark==3.0.1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"exploring-dataset\"></a>\n## Exploring the Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Imports\nimport pandas as pd\nimport numpy as np\nimport pickle\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom datetime import date\nfrom typing import List, Any, Dict\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"competition-setup\"></a>\n### Competition Setup\n\nThis notebook wil use data from the [M5 Forecasting](https://www.kaggle.com/c/m5-forecasting-accuracy) competition, which asks participants to predict sales of Walmart products over a 28-day period, given the historical sales data. Plot below is taken from this [notebook](https://www.kaggle.com/headsortails/back-to-predict-the-future-interactive-m5-eda). \n\nThe plot shows the competition setup, orange is the training period. Yellow and blue show the validation and evaluation periods, respectively. \n\nFor this demo, we'll just be concerned with training and validation (will be referred to as test set)"},{"metadata":{},"cell_type":"markdown","source":"![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F1014468%2F5855ba35843d22a319e3682e5bb2e9de%2FScreenshot%202020-05-29%20at%2020.23.16.png?generation=1590866269400767&alt=media)"},{"metadata":{},"cell_type":"markdown","source":"<a id=#data_files></a>\n### Data Files\n\nFor this competition, `sales_train_validation.csv` was initially given. One more before the competition deadline, \n`sales_train_evalutation.csv` was released with labels for the final 28 days.\n\nFor this demo we'll just concern ourselves with 3 files:\n\n\n- `calendar.csv` - Contains information about the dates on which the products are sold.\n- `sales_train_validation.csv` - Contains the historical daily unit sales data per product and store [d_1 - d_1913]\n- `sell_prices.csv` - Contains information about the price of the products sold per store and date."},{"metadata":{},"cell_type":"markdown","source":"<a id = \"initial-exloration\"></a>\n### Initial Exploration\n\n**Calendar**\n\nFirst, we'll take a look at the calendar data. It is the smallest of the three files. It contains the dates, year_wk, and events that happened. This also contains events, along with a binary variable if SNAP purchases were allowed on that [date](https://www.kaggle.com/c/m5-forecasting-accuracy/discussion/133614)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loading in calendar data\ncalendar = pd.read_csv('../input/m5-forecasting-accuracy/calendar.csv')\ncalendar['date'] = pd.to_datetime(calendar['date'])\ncalendar.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"START_DATE = calendar['date'].min()\nEND_DATE = calendar['date'].max()\nprint(\"Calendar length = \" + str(calendar.shape[0]) + \" days\")\nprint(\"Ending date: \" + str(END_DATE))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Note that the length of the calendar is 1969 days. Days 1 - 1913 are the training set. Days 1914 - 1941 are the validation set, and days 1942 - 1969 are the evaluation set."},{"metadata":{},"cell_type":"markdown","source":"**Prices**\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loading in the prices\nprices = pd.read_csv('../input/m5-forecasting-accuracy/sell_prices.csv')\nprices.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks like the price is defined per week, and we need to join this to the calendar data to get the appropriate time series for each product. I am keeping wday to to include another feature."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Join prices to calendar\nprices = calendar[['wm_yr_wk', 'date', 'wday']].merge(prices, on = 'wm_yr_wk', how='inner')\nprices.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This is a check to make sure we know the sell_prices beforehand\n# to determine if we can use it as a feature\nprices['date'].max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting one timeseries\ntemp = prices.loc[(prices['item_id'] == 'HOBBIES_1_012') & (prices['store_id'] == \"CA_1\")]\nsns.lineplot(temp['date'], temp['sell_price'])\nplt.title('Sell price for HOBBIES_1_012 over time')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"compressing-timeseries\"></a>\n## Compressing timeseries data into a list\n\nThe first concept in this demo is we can compress the size of our data by putting it in a list format, and then just keeping track of the start date. There is an assumption here that the data is continuous."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nprices = prices.groupby(['store_id', 'item_id']).agg({'date': min,'sell_price': lambda x: list(x), \n                                                      'wday': lambda x: list(x)}).reset_index()\\\n           .rename(columns = {'date':'sell_price_start_date'})\nprices.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales = pd.read_csv('../input/m5-forecasting-accuracy/sales_train_validation.csv')\nsales.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Note the format of the raw data. Each day in a column. The timeseries for an item goes from left to right.\n\nSimilar to the transformation we did for the price data, we can also convert this to a list to save memory."},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = sales.columns[6:]\nsales['sales'] = sales[cols].values.tolist()\nsales = sales[['item_id', 'store_id', 'sales']]\nsales['sales_start_date'] = START_DATE","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting one timeseries for sales\ntemp = sales.loc[(sales['item_id'] == 'HOBBIES_1_012') & (sales['store_id'] == \"CA_1\")]\ndr = pd.date_range(START_DATE,periods=len(temp.iloc[0][\"sales\"]), freq=\"d\")\ntemp = pd.DataFrame({'date': dr, 'sales': temp.iloc[0]['sales']})\nsns.lineplot(temp['date'], temp['sales'])\nplt.title('Sales for HOBBIES_1_012 over time')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = sales.merge(prices, on = ['item_id', 'store_id'], how = 'inner')\ndata.head()\nprint(data.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Sampling Due to Memory Constraints\n\nSampling rows of the products due to memory limitations of Kaggle"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data.sample(frac = 0.1).reset_index()\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"binary-blob\"></a>\n## Using Binary Blobs to Pass Data\n\nHere we have a code snippet to combine the timeseries into one dataframe. This dataframe will be pickled into a binary blob that will be passed to workers in the following cell."},{"metadata":{"trusted":true},"cell_type":"code","source":"row = data.iloc[0]\ndr1 = pd.date_range(row[\"sales_start_date\"],periods=len(row[\"sales\"]), freq=\"d\")\ndf = pd.DataFrame({\"sales\":row[\"sales\"]},index = dr1)\ndr2 = pd.date_range(row[\"sell_price_start_date\"],periods=len(row[\"sell_price\"]), freq=\"d\")\ndf[\"price\"] = pd.Series(row[\"sell_price\"],index = dr2)\ndf['wday'] = pd.Series(row[\"wday\"], index = dr2)\ndf.dropna(inplace = True)\ndf.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfor index ,row in data.iterrows():\n    dr1 = pd.date_range(row[\"sales_start_date\"],periods=len(row[\"sales\"]), freq=\"d\")\n    df = pd.DataFrame({\"quantity\":row[\"sales\"]},index = dr1)\n    dr2 = pd.date_range(row[\"sell_price_start_date\"],periods=len(row[\"sell_price\"]), freq=\"d\")\n    df[\"price\"] = pd.Series(row[\"sell_price\"],index = dr2)\n    df['wday'] = pd.Series(row[\"wday\"], index = dr2)\n    df=df.dropna()\n    data.loc[index, \"start_date\"] = df.index[0].date()\n    data.loc[index, \"timeseries\"] = pickle.dumps(df)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_test_split(df: pd.DataFrame, train_end = pd.to_datetime(\"2015-06-01\"), target='quantity', test_period = 28):\n    if (df.index[-1]-train_end).days<test_period:\n        return None\n    n_train = df.shape[0] - 28\n    y = df[target]\n    x = df.drop(target, axis = 1)\n    X_train = x.iloc[:n_train]\n    X_test = x.iloc[n_train:]\n    y_train = y.iloc[:n_train]\n    y_test = y.iloc[n_train:]\n    return X_train, X_test, y_train, y_test\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def generate_train_set(df:pd.DataFrame):\n    for i, row in df.iterrows():\n        result = train_test_split(pickle.loads(row[\"timeseries\"]))\n        if result is None:\n            continue\n        X_train, X_test, y_train, y_test = result\n        df.loc[i, \"X_train\"] = pickle.dumps(X_train)\n        df.loc[i, \"X_test\"] = pickle.dumps(X_test)\n        df.loc[i, \"y_train\"] = pickle.dumps(y_train)\n        df.loc[i, \"y_test\"] = pickle.dumps(y_test)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = generate_train_set(data)\ndata = data[['item_id', 'store_id', 'X_train', 'X_test', 'y_train', 'y_test']]\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"spark-orchestration\"></a>\n## Spark as Orchestration DataFrame\n\nHere we create a Spark DataFrame from the Pandas DataFrame with the pickled rows. This Spark DataFrame will serve as the orchestration piece to run Sklearn models on each row."},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark.sql.types import StructType, StructField, StringType, FloatType, BinaryType\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\nin_schema = StructType([\n    StructField(\"item_id\", StringType(), False),\n    StructField(\"store_id\", StringType(), False),\n    StructField(\"X_train\", BinaryType(), False),\n    StructField(\"X_test\", BinaryType(), False),\n    StructField(\"y_train\", BinaryType(), False),\n    StructField(\"y_test\", BinaryType(), False)\n])\ndata_spark = spark.createDataFrame(data, in_schema)\ndata_spark.show(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"evaluation\"></a>\n# Setting up models for evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import r2_score\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, LSTM\n\ndef eval_lr(x_train, y_train, x_test):\n    model = LinearRegression(fit_intercept=True)\n    model.fit(x_train,y_train)\n    return model, model.predict(x_test)\n\ndef eval_svr(x_train, y_train, x_test):\n    model = SVR(C=1.0, epsilon=0.2)\n    model.fit(x_train,y_train)\n    return model, model.predict(x_test)\n\ndef eval_nn(x_train,y_train,x_test):\n    model = Sequential()\n    model.add(Dense(32, input_dim=x_train.shape[1], activation='relu'))\n    model.add(Dense(16, activation='relu'))\n    model.add(Dense(1, activation='linear'))\n    model.compile(\n        loss=\"mae\",\n        optimizer=\"adam\",\n        metrics=[\"mean_absolute_error\"],\n    )\n    model.fit(x_train,y_train, epochs=4, batch_size=16)\n    return 1, model.predict(x_test) # keras models can't be pickled so it's useless for this demo","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark.sql.functions import pandas_udf\nfrom pyspark.sql.functions import PandasUDFType\n\nout_schema = StructType([\n    StructField(\"item_id\", StringType(), False),\n    StructField(\"store_id\", StringType(), False),\n    StructField(\"score\", FloatType(), False),\n    StructField(\"model_name\", StringType(), False),\n    StructField(\"model\", BinaryType(), False)\n])\n\n@pandas_udf(out_schema, PandasUDFType.GROUPED_MAP)\n# Input/output are both a pandas.DataFrame\ndef evaluate_model(df):\n    result = pd.DataFrame()\n    row = df.iloc[0].to_dict()\n    x_train, y_train = pickle.loads(row[\"X_train\"]), pickle.loads(row[\"y_train\"])\n    x_test, y_test = pickle.loads(row[\"X_test\"]), pickle.loads(row[\"y_test\"])\n    for eval_func in [eval_lr, eval_svr, eval_nn]:\n        model, pred = eval_func(x_train, y_train, x_test)\n        score = r2_score(y_test, pred)\n        result = result.append({\"item_id\": row[\"item_id\"], \"store_id\": row[\"store_id\"], \n                                \"score\": score, \"model_name\": eval_func.__name__,\n                               \"model\": pickle.dumps(model)}, ignore_index=True)\n\n    return result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nevaluation = data_spark.groupBy([\"item_id\", \"store_id\"]).apply(evaluate_model).toPandas()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Comparing Model Performance"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reading in data to join back categorical\nsales = pd.read_csv('../input/m5-forecasting-accuracy/sales_train_validation.csv')\nevaluation = evaluation.merge(sales[['cat_id', 'state_id', 'item_id', 'store_id']], on = ['item_id', 'store_id'], how = 'left')\nevaluation.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Average score for each model\ntemp = evaluation.loc[evaluation['score'] > 0]\nsns.barplot(temp['state_id'],temp['score'], hue=temp['model_name'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Getting best model for each product"},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluation = evaluation.sort_values('score', ascending = False)\nevaluation = evaluation.groupby(['item_id', 'store_id']).first()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This plot shows how many times each model performed the best for a given product."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x=\"cat_id\", hue=\"model_name\", data=evaluation)\nplt.title('Best model')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x=\"state_id\", hue=\"model_name\", data=evaluation)\nplt.title('Best model')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}