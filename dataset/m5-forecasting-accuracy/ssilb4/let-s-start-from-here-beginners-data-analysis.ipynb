{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Data Visualization Notebook\n\n1st public version: Created on May 6th, 2020.  \n2nd public version: Created on May 7th, 2020.  \n3rd public version: Created on May 10th, 2020.  \n4th public version: Created on May 15th, 2020.  (Add Department Analysis)  \n5th public version: Created on May 16th, 2020.  (Add Next week event and value analysis)  \n6th public version: Created on May 16th, 2020.  (Add [Dynamic Factor Analysis](#Dynamic-Factor-Analysis-Trial))  \n7th public version: Created on May 23th, 2020.  (Fix some codes in [Standard deviation analysis in each store](#Standard-deviation-analysis-in-each-store), Thank you for your comment, Kevin.)  \n8th public version: Created on May 26th, 2020. (Fix Bugs and Add Calendar Plot)\n9th public version: Created on May 26th, 2020. (Add more precise PCA Plot, See [PCA Section](#PCA-Trial))\n\n안녕하세요, 여러분.  \n저는 이 competition ** M5 -accuracy- ** 도전자 중 한명입니다.  \n저는 테스트 데이터셋을 예측하기 위해 몇몇의 유용한 insight를 알고 싶습니다.\n\n여기서 저는 특히 저 같은 초보자들을 위한 데이터 시각화를 위한 노트북을 만들었습니다.  \n저는 여러분들의 조회와 댓글 모두 감사합니다.\n\nhttps://www.kaggle.com/robikscube/m5-forecasting-starter-data-exploration   \n이 노트북은 정말 좋습니다.  \n만약 여러분들이 시간이 있다면, 저는 한 번 보는 것을 강력 추천합니다.  \n그리고 저는 Reference 섹션에서 다른 좋은 커널들을 소개할 것입니다.  \n\n\n<!--\n\nHello, everyone.  \nI'm one of the challenger of this competition **M5 -accuracy- **.  \nI'd like to find some useful insights for predictiong the test dataset.  \n\nHere I created a notebook for data visualization especially for beginners like me.  \nI appreciate all of your views and comments.\n\nhttps://www.kaggle.com/robikscube/m5-forecasting-starter-data-exploration  \nHis notebook is super great.  \nIf you have time, I strongly recommend you to take a look.\nAnd I'll introduce other good kernels in Reference Section\n-->\n\nHere is table of contents in this notebook:\n- [Import Libraries and Data Input](#Import-Libraries-and-Data-Input)\n- [Data Cleaning](#Data-Cleaning)\n- [Data Visualization](#Data-Visualization)\n   -  [Total Item Sold Transition](#Total-Item-Sold-Transition)\n   -  [Item Sold in each day type](#Item-Sold-in-each-day-type)\n   -  [Item sold in each State and Store](#Item-sold-in-each-State-and-Store)\n      - [Standard deviation analysis in each store](#Standard-deviation-analysis-in-each-store)\n   -  [Item Sold relation Analysis](#Item-Sold-relation-Analysis)\n   -  [Dynamic Factor Analysis Trial](#Dynamic-Factor-Analysis-Trial)\n   -  [Store Analysis](#Store-Analysis)\n   -  [Snap Purchase Analysis](#Snap-Purchase-Analysis)\n   -  [Event Pattern Analysis](#Event-Pattern-Analysis)\n   -  [One Item Features Analysis](#One-Item-Features-Analysis)\n   -  [Sell Price Analysis](#Sell-Price-Analysis)\n   -  [Sell price and value relationship](#Sell-price-and-value-relationship)\n   -  [Department Analysis](#Department-Analysis)\n   -  [Next Week Events and This Week Sales Relationship Analysis](#Next-Week-Events-and-This-Week-Sales-Relationship-Analysis)\n   -  [Relationship of Lag Variables](#Relationship-of-Lag-Variables)\n   -  [Calendar Visualization](#Calendar-Visualization)\n   -  [PCA Trial](#PCA-Trial)\n  \n- [Summary](#Summary)\n- [Future Work](#Future-Work)\n- [References](#References)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Import Libraries and Data Input","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nimport seaborn as sns\nimport gc\nimport lightgbm as lgb\nimport time\n# import datetime\n# import xgboost as xgb\n# import time\n# import itertools\n# from sklearn.linear_model import LinearRegression\n# from sklearn.model_selection import train_test_split\n# from sklearn.preprocessing import OneHotEncoder\n# from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import r2_score\n\n%matplotlib inline\nsns.set()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"INPUT_DIR = '/kaggle/input/m5-forecasting-accuracy'\n\ncalendar_df = pd.read_csv(f\"{INPUT_DIR}/calendar.csv\")\nsell_prices_df = pd.read_csv(f\"{INPUT_DIR}/sell_prices.csv\")\nsales_train_validation_df = pd.read_csv(f\"{INPUT_DIR}/sales_train_validation.csv\")\nsample_submission_df = pd.read_csv(f\"{INPUT_DIR}/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> 메모리 사용 감소를 위해\n>\n> calendar 관련 숫자 데이터들을 숫자 타입으로 변경 \n>\n> calendar 관련 문자 데이터들을 category 타입으로 변경 ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calendar data type cast -> Memory Usage Reduction\ncalendar_df[[\"month\", \"snap_CA\", \"snap_TX\", \"snap_WI\", \"wday\"]] = calendar_df[[\"month\", \"snap_CA\", \"snap_TX\", \"snap_WI\", \"wday\"]].astype(\"int8\")\ncalendar_df[[\"wm_yr_wk\", \"year\"]] = calendar_df[[\"wm_yr_wk\", \"year\"]].astype(\"int16\") \ncalendar_df[\"date\"] = calendar_df[\"date\"].astype(\"datetime64\")\n\nnan_features = ['event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']\nfor feature in nan_features:\n    calendar_df[feature].fillna('unknown', inplace = True)\n\ncalendar_df[[\"weekday\", \"event_name_1\", \"event_type_1\", \"event_name_2\", \"event_type_2\"]] = calendar_df[[\"weekday\", \"event_name_1\", \"event_type_1\", \"event_name_2\", \"event_type_2\"]] .astype(\"category\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> 메모리 사용 감소를 위해\n>\n> 숫자 타입으로 변경","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Sales Training dataset cast -> Memory Usage Reduction\nsales_train_validation_df.loc[:, \"d_1\":] = sales_train_validation_df.loc[:, \"d_1\":].astype(\"int16\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> sell_price 에 ID 컬럼 생성","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sell_prices_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make ID column to sell_price dataframe\nsell_prices_df.loc[:, \"id\"] = sell_prices_df.loc[:, \"item_id\"] + \"_\" + sell_prices_df.loc[:, \"store_id\"] + \"_validation\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sell_prices_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sell_prices_df = pd.concat([sell_prices_df, sell_prices_df[\"item_id\"].str.split(\"_\", expand=True)], axis=1)\nsell_prices_df = sell_prices_df.rename(columns={0:\"cat_id\", 1:\"dept_id\"})\nsell_prices_df[[\"store_id\", \"item_id\", \"cat_id\", \"dept_id\"]] = sell_prices_df[[\"store_id\",\"item_id\", \"cat_id\", \"dept_id\"]].astype(\"category\")\nsell_prices_df = sell_prices_df.drop(columns=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sell_prices_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Cleaning\n먼저, 모든 3개의 데이터 프레임을 합쳐봅시다.  \n중요한 것은 모델을 더 쉽게 예측하기 위해 데이터 포맷을 넓은 것에서 긴 것으로 바꾸는 것입니다.  \n(이 노트북은 예측 모델 자체를 다루지는 않습니다.)\n\n<!--\nFirst, let's combine all three dataframe.  \nThe important thing is changing data format from wide to long to make prediction model easier  \n(Though this notebook doesn't dive into predicition model itself.)\n-->\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_dataframe():\n    # Wide format dataset \n    df_wide_train = sales_train_validation_df.drop(columns=[\"item_id\", \"dept_id\", \"cat_id\", \"state_id\",\"store_id\", \"id\"]).T\n    df_wide_train.index = calendar_df[\"date\"][:1913]\n    df_wide_train.columns = sales_train_validation_df[\"id\"]\n    \n    # Making test label dataset\n    df_wide_test = pd.DataFrame(np.zeros(shape=(56, len(df_wide_train.columns))), index=calendar_df.date[1913:], columns=df_wide_train.columns)\n    df_wide = pd.concat([df_wide_train, df_wide_test])\n\n    # Convert wide format to long format\n    df_long = df_wide.stack().reset_index(1)\n    df_long.columns = [\"id\", \"value\"]\n\n    del df_wide_train, df_wide_test, df_wide\n    gc.collect()\n    \n    df = pd.merge(pd.merge(df_long.reset_index(), calendar_df, on=\"date\"), sell_prices_df, on=[\"id\", \"wm_yr_wk\"])\n    df = df.drop(columns=[\"d\"])\n#     df[[\"cat_id\", \"store_id\", \"item_id\", \"id\", \"dept_id\"]] = df[[\"cat_id\"\", store_id\", \"item_id\", \"id\", \"dept_id\"]].astype(\"category\")\n    df[\"sell_price\"] = df[\"sell_price\"].astype(\"float16\")   \n    df[\"value\"] = df[\"value\"].astype(\"int32\")\n    df[\"state_id\"] = df[\"store_id\"].str[:2].astype(\"category\")\n\n\n    del df_long\n    gc.collect()\n\n    return df\n\ndf = make_dataframe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def add_date_feature(df):\n    df[\"year\"] = df[\"date\"].dt.year.astype(\"int16\")\n    df[\"month\"] = df[\"date\"].dt.month.astype(\"int8\")\n    df[\"week\"] = df[\"date\"].dt.week.astype(\"int8\")\n    df[\"day\"] = df[\"date\"].dt.day.astype(\"int8\")\n    df[\"quarter\"]  = df[\"date\"].dt.quarter.astype(\"int8\")\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = add_date_feature(df)\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Visualization\n## Total Item Sold Transition","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_series = df.groupby([\"cat_id\", \"date\"])[\"value\"].sum()\ntemp_series","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12, 4))\nplt.plot(temp_series[temp_series.index.get_level_values(\"cat_id\") == \"FOODS\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"cat_id\") == \"FOODS\"].values, label=\"FOODS\")\nplt.plot(temp_series[temp_series.index.get_level_values(\"cat_id\") == \"HOUSEHOLD\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"cat_id\") == \"HOUSEHOLD\"].values, label=\"HOUSEHOLD\")\nplt.plot(temp_series[temp_series.index.get_level_values(\"cat_id\") == \"HOBBIES\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"cat_id\") == \"HOBBIES\"].values, label=\"HOBBIES\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"# of sold items\")\nplt.title(\"Total Item Sold Transition of each Category\")\nplt.legend()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Point of the graph**\n\n1. FOODS는 이 3개의 카테고리 중에서 가장 많이 팔린 품목 카테고리이다.  \n   HOUSEHOLD는 2번째이고, HOBBIES는 가장 적게 팔린 품목이다.\n   \n2. FOODS 카테고리는 분명히 몇몇의 주기적인 특성을 갖고 있다.  \n   일년 동안, 이것은 겨울보다 여름에 더 많이 팔린 것으로 보인다. 그러나, 이것을 검증해봐야한다.\n\n3. HOUSEHOLD 카테고리 품목의 판매량은 2011년부터 점점 증가했다.  \n   그러나, 2011년에 상점에 몇몇 품목이 없었기 때문일 지도 모른다.  \n   그래서 우리는 상점에서의 총 품목을 고려해봐야 한다.  \n   주기적인 특성은 FOODS와 비교해서 이 카테고리에서는 그렇게 분명하지는 않다.\n\n4. HOBBIES 카테고리에선, 주기적 특성이 HOUSEHOLD 카테고리처럼 분명하지 않다.  \n\n5. 몇몇 부분에서 (연말 쯤), 모든 카테고리들은 어떤 판매도 없다. 그래서 나는 훈련 모델에서 이런 날들을 가져왔는 지 고려해봐야 한다고 생각한다.\n\n자, 가장 최근인 2015년을 보자!\n\n<!--\n1. FOODS is the most sold item category of these three categories.  \n   HOUSEHOLD is the 2nd one, and HOBBIES are the least sold one.  \n\n\n2. FOODS category appearently has some periodical feature.   \n   During one year, it seems more items are sold in summer than in winter, however, we have to verify this.  \n   As for more short time interval, it seems the trend has monthly or weekly features. (Let's take a look below)\n\n3. HOUSEHOLD category items sold is gradually increasing from 2011.  \n   However, it may be because some items are not in the store in 2011.  \n   So we have to take the total item in the store into account.\n   Periodical Features are not so clear in this category compared to FOODS.\n  \n4. In HOBBIES category, periodical features are less appearent like HOUSEHOLD category.\n\n5. In some point (around the end of year), all categories don't have any sold.  So I think we have to consider whether we take these days into account when training models.\n\nSo let's take a look at the latest year, 2015!\n-->","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_series = temp_series.loc[temp_series.index.get_level_values(\"date\") >= \"2015-01-01\"]\nplt.figure(figsize=(12, 4))\nplt.plot(temp_series[temp_series.index.get_level_values(\"cat_id\") == \"FOODS\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"cat_id\") == \"FOODS\"].values, label=\"FOODS\")\nplt.plot(temp_series[temp_series.index.get_level_values(\"cat_id\") == \"HOUSEHOLD\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"cat_id\") == \"HOUSEHOLD\"].values, label=\"HOUSEHOLD\")\nplt.plot(temp_series[temp_series.index.get_level_values(\"cat_id\") == \"HOBBIES\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"cat_id\") == \"HOBBIES\"].values, label=\"HOBBIES\")\nplt.xlabel(\"Year-Month\")\nplt.ylabel(\"# of sold items\")\nplt.title(\"Total Item Sold Transition of each Category from 2015\")\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. 모든 카테고리에서, 주기적 추세는 주별로 보여진다.  \n   이전 그래프에선, 우리는 쉽게 HOUSEHOLD와 HOBBIES가 주간 특성을 인식할 수 없었으나, 이 그래프에선느 할 수 있다.\n   \n2. 모든 품목이 판매된 양이 0인 날은 크리스마스 처럼 보인다. 신년은 아니라는 것은 밑에서 확인하자.\n\n<!--\n1. In all categories, the periodical trends is seemed weekly.  \n   In previous graph, we can't easily recognize that HOUSEHOLD and HOBBIES have weekly features, but in this graph we can.\n   \n2. The day when all item sold is 0 is seemed to be Christmas Day, not new year's day, confirm it below.\n-->","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot only December, 2015\ntemp_series = temp_series.loc[(temp_series.index.get_level_values(\"date\") >= \"2015-12-01\") & (temp_series.index.get_level_values(\"date\") <= \"2015-12-31\")]\nplt.figure(figsize=(12, 4))\nplt.plot(temp_series[temp_series.index.get_level_values(\"cat_id\") == \"FOODS\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"cat_id\") == \"FOODS\"].values, label=\"FOODS\")\nplt.plot(temp_series[temp_series.index.get_level_values(\"cat_id\") == \"HOUSEHOLD\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"cat_id\") == \"HOUSEHOLD\"].values, label=\"HOUSEHOLD\")\nplt.plot(temp_series[temp_series.index.get_level_values(\"cat_id\") == \"HOBBIES\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"cat_id\") == \"HOBBIES\"].values, label=\"HOBBIES\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"# of sold items\")\nplt.title(\"Total sold item per day in December, 2015\")\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"크리스마스에 판매된 품목이 0처럼 보인다. *.loc* 함수로 확인해보자.\n\n<!--\nOn Christmas Day, the items sold are seemed to be 0, let's check it with *.loc* method\n-->","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_series.loc[(temp_series.index.get_level_values(\"date\") >= \"2015-12-24\") & (temp_series.index.get_level_values(\"date\") <= \"2015-12-26\")]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"몇몇의 품목은 심지어 크리스마스에도 팔렸다. 그러나 나는 noisy된 값이 있다고 완전히 생각한다.\n\n지금까지, 우리는 판매된 품목이 주간 특징을 가지고 있다는 것을 알 수 있었다. 이것을 생각해보자:\n**다음 질문: 그 주의 어느 날에 가장 많이 판매될까?**\n\n<!--\nSome items are sold even on Christmas Day, but I think these are completely noisy values.   \n\nUntil now, we can find the items sold have something weekly fetures. So let's think this:   \n**Next Question: Which day of the week is the items sold most?**\n-->","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Item Sold in each day type","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_series = df.groupby([\"cat_id\", \"wday\"])[\"value\"].sum()\ntemp_series","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(6, 4))\nleft = np.arange(1,8) \nwidth = 0.3\nweeklabel = [\"Saturday\", \"Sunday\", \"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\"]    # Please Confirm df\n\n\nplt.bar(left, temp_series[temp_series.index.get_level_values(\"cat_id\") == \"FOODS\"].values, width=width, label=\"FOODS\")\nplt.bar(left + width, temp_series[temp_series.index.get_level_values(\"cat_id\") == \"HOUSEHOLD\"].values, width=width, label=\"HOUSEHOLD\")\nplt.bar(left + width + width, temp_series[temp_series.index.get_level_values(\"cat_id\") == \"HOBBIES\"].values, width=width, label=\"HOBBIES\")\nplt.legend(bbox_to_anchor=(1.01, 1.01))\nplt.xticks(left, weeklabel, rotation=60)\nplt.xlabel(\"day of week\")\nplt.ylabel(\"# of sold items\")\nplt.title(\"Total sold item in each daytype\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. 우리가 추측할 수 있는 것처럼, 토요일이나 일요일이 가장 많이 팔린 날이다.  \n   화요일과 수요일은 가장 적게 팔린 날이다.  \n   -> 나중에, 우리는 이런 관계 요소를 히트맵으로 시각화할 것이다. 다음을 보자!\n\n2. HOBBIES가 FOODS나 HOUSEHOLD와 비교해서 날짜에 그렇게 의존적이지는 않다.\n\n<!--\n1. As we can probraly guess, Saturday or Sunday is the day which the items are most sold.  \n   Tuesday or Wednesday is the least sold days.  \n   -> Later, we visualize these correlation factors with heatmap. Looking forward to it!\n\n2. HOBBIES are not so day dependent compared to FOODS or HOUSEHOLD.  \n-->","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Item sold in each State and Store","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_series = df.groupby([\"state_id\", \"date\"])[\"value\"].sum()\ntemp_series","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12, 4))\nplt.plot(temp_series[temp_series.index.get_level_values(\"state_id\") == \"CA\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"state_id\") == \"CA\"].values, label=\"CA\")\nplt.plot(temp_series[temp_series.index.get_level_values(\"state_id\") == \"TX\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"state_id\") == \"TX\"].values, label=\"TX\")\nplt.plot(temp_series[temp_series.index.get_level_values(\"state_id\") == \"WI\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"state_id\") == \"WI\"].values, label=\"WI\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"# of sold items\")\nplt.title(\"Total Item Sold Transition of each State\")\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. CA는 3개의 주 중에서 가장 많이 팔린 주이다.\n   TX와 WI는 2011년과 2012년을 제외하곤 엄청 다르지는 않다.\n\n2. 모든 3개의 주는 카테고리 기반의 품목 판매 그래프에서 이미 본 것처럼 일부 주기적인 특징이 있다.\n\n먼저, CA에서의 상점을 보자.\n\n<!--\n1. CA is the most sold state of these three states.  \n   TX and WI are not so different except for the year 2011 and 2012.\n  \n2. All three states have some periodical features as we've already seen in category-based item sold graph. \n\nFirst, let's focus on stores in CA.\n-->","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_series = df.groupby([\"store_id\", \"date\"])[\"value\"].sum()\n\nplt.figure(figsize=(12, 4))\nplt.plot(temp_series[temp_series.index.get_level_values(\"store_id\") == \"CA_1\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"store_id\") == \"CA_1\"].values, label=\"CA_1\")\nplt.plot(temp_series[temp_series.index.get_level_values(\"store_id\") == \"CA_2\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"store_id\") == \"CA_2\"].values, label=\"CA_2\")\nplt.plot(temp_series[temp_series.index.get_level_values(\"store_id\") == \"CA_3\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"store_id\") == \"CA_3\"].values, label=\"CA_3\")\nplt.plot(temp_series[temp_series.index.get_level_values(\"store_id\") == \"CA_4\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"store_id\") == \"CA_4\"].values, label=\"CA_4\")\n\nplt.xlabel(\"Year\")\nplt.ylabel(\"# of sold items\")\nplt.title(\"Total Item Sold Transition of each Store in CA\")\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. CA 에서의 3개의 상점은 상품의 판매 기록이 비슷하다.\n   CA_3은 다른 상점과 비교해서 약간 더 많은 양을 판매했다.\n\n2. 각각의 상점의 표준 편차는 다른 것 같다. 나중에 확인해보자.\n\n3. 2015년 봄/여름쯤부터, CA_2의 판매 기록이 빠르게 증가했다. 우리는 이유를 조사해야한다.\n\n<!--\n1. Three stores in CA have similar amount of item sold record.  \n   CA_3 has more item sold a little bit compared to others.  \n\n2. The standard deviation of each store seems different, confirm it later.\n\n3. From around 2015 Spring or Summer, CA_2 increased its sold record rapidly. We have to investigate the reasons.\n-->","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_series = df.groupby([\"store_id\", \"date\"])[\"item_id\"].count()\n\nplt.figure(figsize=(12, 4))\nplt.plot(temp_series[temp_series.index.get_level_values(\"store_id\") == \"CA_1\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"store_id\") == \"CA_1\"].values, label=\"CA_1\")\nplt.plot(temp_series[temp_series.index.get_level_values(\"store_id\") == \"CA_2\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"store_id\") == \"CA_2\"].values, label=\"CA_2\")\nplt.plot(temp_series[temp_series.index.get_level_values(\"store_id\") == \"CA_3\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"store_id\") == \"CA_3\"].values, label=\"CA_3\")\nplt.plot(temp_series[temp_series.index.get_level_values(\"store_id\") == \"CA_4\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"store_id\") == \"CA_4\"].values, label=\"CA_4\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"# of item entries\")\nplt.title(\"Total item entries in each CA stores\")\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3. 2015년 봄/여름쯤부터, CA_2의 판매 기록이 빠르게 증가했다. 우리는 이유를 조사해야한다.\n\n-> 이것은 CA_2에 등록된 상품이 빠르게 증가했기 때문이다.  \n2015년 여름 이후에, CA의 모든 상점들이 상품의 양을 비슷하게 등록했다.\n\n<!--\n3. From around 2015 Spring or Summer, CA_2 increased its sold record rapidly. We have to investigate the reasons.\n\n-> It is because item registered in CA_2 increased rapidly.  \nAfter summer in 2015, all stores in CA have similar registered item count\n-->","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"(20200528 추가 시작)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Standard deviation analysis in each store\n이제 각 상점의 표준 편차를 확인해보자.  \n이전 섹션에서, 우리는 CA_3이 CA의 다른 상점보다 약간 더 많은 판매를 가진다는 것을 알아냈다.  \n이 상점들의 편차는 어떨까?\n\n\n<!--\n### Standard deviation analysis in each store\nNow let's check each store's standard deviation.  \nIn the previous section, we found that CA_3 has a little bit more sales than other stores in CA.  \nHow about deviation of these stores?\n-->","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_series = df.groupby([\"state_id\", \"store_id\", \"year\", \"month\"])[\"value\"].std()\ntemp_series","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(1, 3, figsize=(12, 6), sharey=True)\n\n# We can use for loop, of course! And that'll be better, sorry, this is for my easy trial. \nsns.lineplot(x=temp_series[(temp_series.index.get_level_values(\"state_id\") == \"CA\")].index.get_level_values(\"month\"), \n             y=temp_series[(temp_series.index.get_level_values(\"state_id\") == \"CA\")].values, \n             hue=temp_series[(temp_series.index.get_level_values(\"state_id\") == \"CA\")].index.get_level_values(\"store_id\"), \n             legend=False,\n             ax=axs[0])\nsns.lineplot(x=temp_series[temp_series.index.get_level_values(\"state_id\") == \"TX\"].index.get_level_values(\"month\"),\n             y=temp_series[temp_series.index.get_level_values(\"state_id\") == \"TX\"].values, \n             hue=temp_series[temp_series.index.get_level_values(\"state_id\") == \"TX\"].index.get_level_values(\"store_id\"), \n             legend=False,\n             ax=axs[1])\nsns.lineplot(x=temp_series[temp_series.index.get_level_values(\"state_id\") == \"WI\"].index.get_level_values(\"month\"),\n             y=temp_series[temp_series.index.get_level_values(\"state_id\") == \"WI\"].values, \n             hue=temp_series[temp_series.index.get_level_values(\"state_id\") == \"WI\"].index.get_level_values(\"store_id\"),\n             ax=axs[2])\n\n\n\nplt.legend(bbox_to_anchor=(1.01, 1.01))\naxs[0].set_title(\"CA\")\naxs[0].set_xticks(range(1, 13))\naxs[0].set_ylabel(\"Standard deviation of sold items in one month\")\naxs[1].set_title(\"TX\")\naxs[1].set_xticks(range(1, 13))\naxs[2].set_title(\"WI\")\naxs[2].set_xticks(range(1, 13))\n\nfig.suptitle(\"Standard deviation of sold items in one month in each store\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 원문 커널에는 없어짐\ntemp_series = df.groupby([\"store_id\", \"date\"])[\"value\"].std()\ntemp_series","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 원문 커널에는 없어짐\nplt.figure(figsize=(12, 4))\nplt.plot(temp_series[temp_series.index.get_level_values(\"store_id\") == \"CA_1\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"store_id\") == \"CA_1\"].values, label=\"CA_1\")\nplt.plot(temp_series[temp_series.index.get_level_values(\"store_id\") == \"CA_2\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"store_id\") == \"CA_2\"].values, label=\"CA_2\")\nplt.plot(temp_series[temp_series.index.get_level_values(\"store_id\") == \"CA_3\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"store_id\") == \"CA_3\"].values, label=\"CA_3\")\nplt.plot(temp_series[temp_series.index.get_level_values(\"store_id\") == \"CA_4\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"store_id\") == \"CA_4\"].values, label=\"CA_4\")\n\nplt.xlabel(\"Year\")\nplt.ylabel(\"Standard deviation of sold items\")\nplt.title(\"Standard deviation of sold items in CA stores\")\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. seaborn 에서 lineplot 함수로 사용하면, 우리는 신뢰구간을 보여주는 에러 범위에 대한 하나의 선을 그릴 수 있다.  \n   여러분은 seaborn lineplot을 찾고, 이 문장을 볼 수 있다. (https://seaborn.pydata.org/generated/seaborn.lineplot.html)\n   > 기본적으로, 이 plot은 각 x 값에 대한 많은 y값을 종합하고, 중심의 경향의 평가와 그 평가에 대한 신뢰구간을 보여준다.\n   \n이 경우에 이것은 4년 동안의 각 월별 판매 상품의 신뢰구간에 대한 표준 편차를 그린다. (2011년부터 2015년까지)\n\n\n2. CA_3가 CA에서 가장 많이 팔린 상점이 된 이후로, 표준 편차 또한 다른 곳보다 높아졌다.\n   seaborn을 사용하여 우리는 이런 좋은 plot을 만들 수 있다.\n\n\n다른 주를 확인해보자. 다음은 WI 이다.!\n\n<!--\n1. With lineplot method in seaborn, we can plot single line with error bands showing a confidence interval.  \n   You can serach seaborn lineplot and find this sentence. (https://seaborn.pydata.org/generated/seaborn.lineplot.html)\n>  By default, the plot aggregates over multiple y values at each value of x and shows an estimate of the central tendency and a confidence interval for that estimate.  \n\n   In this case, this command plots each month item sold standard deviation with confidence interval in 4 years. (From 2011 to 2015)\n   \n\n2. Since CA_3 is the most sold store in CA, standard deviation of this store is also higher than others.  \n   By using seaborn, we can create these good plots.\n   \n \nLet's check other state, WI next!\n-->","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"(20200528) 추가 끝 (1까지)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_series = df.groupby([\"store_id\", \"date\"])[\"value\"].sum()\n\nplt.figure(figsize=(12, 4))\nplt.plot(temp_series[temp_series.index.get_level_values(\"store_id\") == \"WI_1\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"store_id\") == \"WI_1\"].values, label=\"WI_1\")\nplt.plot(temp_series[temp_series.index.get_level_values(\"store_id\") == \"WI_2\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"store_id\") == \"WI_2\"].values, label=\"WI_2\")\nplt.plot(temp_series[temp_series.index.get_level_values(\"store_id\") == \"WI_3\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"store_id\") == \"WI_3\"].values, label=\"WI_3\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"# of sold items\")\nplt.title(\"Total item sold in each WI stores\")\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. WI에서의 상점은 상품 판매하는 상품의 수가 비슷하다.  \n   2013 전에는, WI_3는 WI에서 가장 많이 판매된 상점이다. 그러나 WI_2가 점점 그 비율이 커졌다. (특히 2012년 여름 쯤)\n   \n2. 몇몇의 지점에서, WI_1에서 판매하는 상품의 수가 빠르게 증가했다. (약 2012년 11월)\n\n-> 전체 품절 수는 그 날의 항목의 수에 의존한다. 그래서 지금 우리는 CA 상점에서 우리가 했던 것처럼 각 상점에서의 전체 상품 항목을 확인하다.\n   \n<!--\n1. Stores in WI have similar item sold count.  \n   Before 2013, WI_3 is the most sold store in WI, but WI_2 gradually increases its proportion. (Especially around summer in 2012)\n   \n2. In some point, WI_1 rapidly increase its sold item count. (Around on November 2012)\n\n-> Total Sold out count depends on the number of entries at that day.  So Now we check the total item entries in each store as we did in CA stores.\n-->","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_series = df.groupby([\"store_id\", \"date\"])[\"item_id\"].count()\n\nplt.figure(figsize=(12, 4))\nplt.plot(temp_series[temp_series.index.get_level_values(\"store_id\") == \"WI_1\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"store_id\") == \"WI_1\"].values, label=\"WI_1\")\nplt.plot(temp_series[temp_series.index.get_level_values(\"store_id\") == \"WI_2\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"store_id\") == \"WI_2\"].values, label=\"WI_2\")\nplt.plot(temp_series[temp_series.index.get_level_values(\"store_id\") == \"WI_3\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"store_id\") == \"WI_3\"].values, label=\"WI_3\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"# of item entries\")\nplt.title(\"Total item entries in each WI stores\")\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"우리가 이미 위에서 본 것처럼, 등록된 상품의 수의 추세는 각 상점에서와는 다르다.  \nWI_2는 2012년 여름쯤에 상품 등록이 증가했고, WI_3는 그 해 11월 쯤에 증가했다.\n\n2013년 부터, 모든 상점은 비슷한 추세를 가지고 있다. 다음은, TX 주에서의 상점이다!\n\n<!--\nAs we've already seen above, the registered item count trends are different in each store.  \nWI_2 increased its item register around summer in 2012, then WI_3 increased around November of that year.  \n\nFrom 2013, all stores have similar trend.  Next, stores in TX states!\n-->","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_series = df.groupby([\"store_id\", \"date\"])[\"value\"].sum()\n\nplt.figure(figsize=(12, 4))\nplt.plot(temp_series[temp_series.index.get_level_values(\"store_id\") == \"TX_1\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"store_id\") == \"TX_1\"].values, label=\"TX_1\")\nplt.plot(temp_series[temp_series.index.get_level_values(\"store_id\") == \"TX_2\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"store_id\") == \"TX_2\"].values, label=\"TX_2\")\nplt.plot(temp_series[temp_series.index.get_level_values(\"store_id\") == \"TX_3\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"store_id\") == \"TX_3\"].values, label=\"TX_3\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"Total sold item per day\")\nplt.title(\"Total item sold in each TX stores\")\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. 이런, 2015년에 몇몇의 극심한 지점이 있는 것 같다.  \n   예를 들어, 2월 쯤, TX2는 판매된 상품이 거의 0개이다. (나는 이 상점이 예외적으로 닫았을 것이라 가정한다.)  \n   반대로, 그 해 한 여름날에, TX_3는 그것의 총 판매량이 매우 증가했다.  \n\n2. TX_2는 2014년 전에 특히, 가장 많은 상품을 판매했다.\n\n<!--\n1. Oops, in 2015, it seems some extreme points exist.  \n   For exmaple, around Febrary, TX_2 has almost 0 item sold. (I assume this store is closed exceptionally.)\n   In contrast, in one summer day of that year, TX_3 increased its total item sold exprosively.\n   \n2. TX_2 has most item sold especially before 2014.\n-->","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_series = df.groupby([\"store_id\", \"date\"])[\"item_id\"].count()\n\nplt.figure(figsize=(12, 4))\nplt.plot(temp_series[temp_series.index.get_level_values(\"store_id\") == \"TX_1\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"store_id\") == \"TX_1\"].values, label=\"TX_1\")\nplt.plot(temp_series[temp_series.index.get_level_values(\"store_id\") == \"TX_2\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"store_id\") == \"TX_2\"].values, label=\"TX_2\")\nplt.plot(temp_series[temp_series.index.get_level_values(\"store_id\") == \"TX_3\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"store_id\") == \"TX_3\"].values, label=\"TX_3\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"Total item entries\")\nplt.title(\"Total item entries in each TX stores\")\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"다른 주와 비교해서, TX 상점은 등록된 항목과 비슷한 경향을 가지고 있다.\n\n<!--\nCompared to other states, TX stores have similar tendency regarding registered entries.\n-->","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_series = df.groupby([\"store_id\", \"date\"])[\"value\"].sum()\ntemp_series","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find the day when items are sold less than 1000 of each store\n# Let's take a look at TX_2 for example\ntemp_series.loc[(temp_series.values < 1000) & (temp_series.index.get_level_values(\"date\") <= \"2016-04-22\")].loc[\"TX_2\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. 이런, 2015년에, 이것은 극변한 지점이 있는 것 같다.  \n   예를 들어, 2월 쯤에, TX_2 는 거의 0개의 상품을 판매했다. (나는 이 상점이 예외적으로 닫았을 것이라 가정한다.)\n   \n   -> 2015-03-24 에, TX_2는 거의 상품을 판매하지 않았다.\n\n<!--\n1. Oops, in 2015, it seems some extreme points exist.  \n   For exmaple, around Febrary, TX_2 has almost 0 item sold. (I assume this store is closed exceptionally.)\n   \n   -> On 2015-03-24, TX_2 has very little item sold. \n-->","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find the day when items are sold most of each store\ntemp_series.groupby([\"store_id\"]).idxmax()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_series = temp_series.reset_index()\ntemp_series","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(temp_series[(temp_series[\"store_id\"] == \"CA_1\") & ((temp_series[\"date\"] >= \"2013-07-15\") & (temp_series[\"date\"] <= \"2013-10-15\"))][\"date\"],\n         temp_series[(temp_series[\"store_id\"] == \"CA_1\") & ((temp_series[\"date\"] >= \"2013-07-15\") & (temp_series[\"date\"] <= \"2013-10-15\"))][\"value\"])\nplt.xticks(rotation=60)\nplt.ylabel(\"# of sold items\")\nplt.xlabel(\"date\")\nplt.title(\"Item sold transition around its most sold day in CA_1 store\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"(20200528 갱신)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Item Sold relation Analysis\n진행 중...  \n(저는 Dynamic Factor Analysis 를 적용하고 이 튜토리얼의 코드를 실행하려고 노력했습니다. 그러나 유익한 결과를 얻을 수 없을 것 같습니다:\nhttps://www.statsmodels.org/dev/examples/notebooks/generated/statespace_dfm_coincident.html\n\n더 상세하게 이 방법을 이해할 수 있는 누구든지, 댓글을 남겨주시면 감사하겠습니다.\n\n<!--\nUnder Construction...  \n(I tried to apply Dynamic Factor Analysis and execute the codes of this tutorial, but it seems I couldn't get informative outcome:\nhttps://www.statsmodels.org/dev/examples/notebooks/generated/statespace_dfm_coincident.html\n\nTo Whoever can understand this method more specifically, I appreciate your comments.\n-->","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Dynamic Factor Analysis Trial\nDynamic Factor 분석은 PCA (Principle Component Analysis) 같은 차원 축소 기법 중 하나입니다.  \n우리가 몇몇의 시계열 데이터를 가지고 있다고 가정하면, 우리는 시계열 데이터의 숨겨진 요소를 발견하기 위해 이 기법을 적용할 수 있습니다.  \n(여러분이 여기에서 더 알고 싶으면: https://www.statsmodels.org/dev/examples/notebooks/generated/statespace_dfm_coincident.html)  \n그리고 제가 밑에서 시도하는 것은 이 statsmodel's 웹 사이트의 튜토리얼과 유사합니다.\n\n우리는 이것을 각 상점의 판매 상품을 정하거나 각 상점 사이의 몇몇의 숨겨진 요소를 발견하는 데 적용할 것입니다.\n\n<!--\nDynamic Factor analysis is one of the dimention reduction technoloies like PCA (Principle Component Analysis)  \nSuppose we have some time-series data, we can apply this technique to find the hidden factor of these time-series data.  \n(You can learn more here: https://www.statsmodels.org/dev/examples/notebooks/generated/statespace_dfm_coincident.html)  \nAnd what I tried below is similar to the tutorial of this statsmodel's website.\n\nWe apply this to the target item sold of each store and find some hidden factors among stores.\n-->","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import statsmodels.api as sm\nimport scipy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Analysis target item is the most sold one.\n# In this Dynamic Factor Analysis Session, we'll try to find the hidden factor of this item sales transition among states.\nitem_id = \"FOODS_3_090\"\nstate_list = [\"CA\", \"TX\", \"WI\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# First, we extract the target item sold sum in each state\ntemp_series = df[(df.date >= \"2015-01-01\") & (df.date <= \"2016-01-01\") & (df.item_id == item_id)].groupby([\"date\", \"state_id\"])[\"value\"].sum()\n\n# Then convert it to dataframe type.\ntemp_df = pd.concat([pd.DataFrame(temp_series[temp_series.index.get_level_values(\"state_id\") == \"CA\"].values),\n                     pd.DataFrame(temp_series[temp_series.index.get_level_values(\"state_id\") == \"TX\"].values), \n                     pd.DataFrame(temp_series[temp_series.index.get_level_values(\"state_id\") == \"WI\"].values)], axis=1) \ntemp_df.columns = state_list\ntemp_df.index = temp_series[temp_series.index.get_level_values(\"state_id\") == \"CA\"].index.get_level_values(\"date\")\ntemp_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Next Step, we create diff column, which shows the difference of today and tommorow's sold count.  \n# For flattening, we apply log function and standardization\n\ndiff_cols = [\"diff_\" + state for state in state_list]      # diff columns (row data)\nstd_cols = [\"std_diff_\" + state for state in state_list]   # diff columns (after standardization)\n\nfor state in state_list:\n    col = \"diff_\" + state\n    temp_df[col] = np.log(temp_df[state] + 0.1).diff() * 100\n    temp_df[col] = temp_df[col].fillna(method=\"bfill\")\n    \n    # Standardization\n    std_col = \"std_\" + col\n    temp_df[std_col] = (temp_df[col] - temp_df[col].mean()) / temp_df[col].std()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Conveert it to Z-value\ntemp_df[std_cols] = temp_df[std_cols].apply(scipy.stats.zscore, axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create the model\n# This time, for simplicity, unobserved factor (k_factors) is 1, and factor_order is 1 (i.e. it follows an AR(1) process) , \n# error_order is 1 (i.e. error has order 1 autocorelated), \n\nendog = temp_df.loc[:, std_cols]\nmod = sm.tsa.DynamicFactor(endog, k_factors=1, factor_order=1, error_order=1)\ninitial_res = mod.fit(method='powell', disp=False)\nres = mod.fit(initial_res.params, disp=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(res.summary(separate_params=False))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"이것은 모델을 적용한 결과의 요약입니다.\n이것은 모델 정보와 몇몇의 통계적인 값을 설명합니다.\n\n<!--\nThis is the summary result of model fitting. \nIt describes model information and some statistical values.  \n-->","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from pandas_datareader.data import DataReader\n\nfig, ax = plt.subplots(figsize=(13,3))\n\n# Plot the factor\ndates = endog.index._mpl_repr()\nax.plot(dates, res.factors.filtered[0], label='Factor')\nax.legend()\n\n# Retrieve and also plot the NBER recession indicators\nrec = DataReader('USREC', 'fred', start=temp_df.index.min(), end=temp_df.index.max())\nylim = ax.get_ylim()\nplt.title(\"Fluctuations extracted by Factor 1\")\nplt.ylabel(\"Fluctuations\")\nplt.xlabel(\"Year-Month\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"간단히 말해서, 목표 판매 상품의 데이터셋에서, 우리는 주 사이에 관측되지 않은 하나의 요소를 추출합니다.\n그리고 위 plot은 이 요소가 어떻게 각 상점에 있는 판매 상품에 영향을 미치는 지 보여줍니다.\n\n아래 코드는 이 요소가 전체 판매 변화를 어떻게 설명하는 지에 대한 $R^2$ 값을 계산합니다.\n\n<!--\nIn short, from the dataset of the target item sold, we extract one unobserved factor among states.  \nAnd the above plot shows how this factor effects to the item sold in each state.  \n\nThe code below calculates $R^2$ values of how this factor explains the total sold transition.\n-->","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"res.coefficients_of_determination","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"res.plot_coefficients_of_determination();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"우리는 이 요소가 각 상점 컬럼을 잘 설명할 수 있다는 것을 알아봤습니다. 특히 TX 주에서.\n따라서, 우리는 이 세 개의 주들이 하나의 관측되지 않은 요소에 의해 설명된다고 결론지을 수 있습니다.\n(주 별로 그렇게 다르지 않다.)\n\n<!--\nWe can find this factor explains each store column well, especially for TX state,  \nThus, we can conclude these three states are explained by one unobserved factor.  \n(Not so different from state to state)\n-->","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"(20200528 끝) ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Store Analysis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_series = df.groupby([\"store_id\", \"cat_id\"])[\"value\"].sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"store_id_list_by_state = [[\"CA_1\", \"CA_2\", \"CA_3\", \"CA_4\"], [\"TX_1\", \"TX_2\", \"TX_3\"], [\"WI_1\", \"WI_2\", \"WI_3\"]] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(3, 4, figsize=(16, 12), sharey=True) \n\nfor row in range(len(store_id_list_by_state)):\n    for col in range(len(store_id_list_by_state[row])):\n        axs[row, col].bar(x=temp_series[temp_series.index.get_level_values(\"store_id\") == store_id_list_by_state[row][col]].index.get_level_values(\"cat_id\"),\n                          height=temp_series[temp_series.index.get_level_values(\"store_id\") == store_id_list_by_state[row][col]].values,\n                         color=[\"orange\", \"green\", \"blue\"], label=[\"FOODS\", \"HOBBIES\", \"HOUSEHOLD\"])\n        axs[row, col].set_title(store_id_list_by_state[row][col])\n        axs[row, col].set_ylabel(\"# of items\")\n\nfig.suptitle(\"Each category item sold in each store\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(3, 4, figsize=(16, 12), sharey=True) \n\nfor row in range(len(store_id_list_by_state)):\n    for col in range(len(store_id_list_by_state[row])):\n        axs[row, col].bar(x=temp_series[temp_series.index.get_level_values(\"store_id\") == store_id_list_by_state[row][col]].index.get_level_values(\"cat_id\"),\n                          height=temp_series[temp_series.index.get_level_values(\"store_id\") == store_id_list_by_state[row][col]].values / temp_series[temp_series.index.get_level_values(\"store_id\") == store_id_list_by_state[row][col]].sum(),\n                         color=[\"orange\", \"green\", \"blue\"], label=[\"FOODS\", \"HOBBIES\", \"HOUSEHOLD\"])\n        axs[row, col].set_title(store_id_list_by_state[row][col])\n        axs[row, col].set_ylabel(\"% of each category\")\n\nfig.suptitle(\"Each category item sold percentage in each store\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_id = \"FOODS\"\n\ntemp_series = df.groupby([\"store_id\", \"cat_id\", \"wday\"])[\"value\"].sum()\ntemp_series = temp_series[temp_series.index.get_level_values(\"cat_id\") == cat_id]\ntemp_series","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weekday = [\"Sat\", \"Sun\", \"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Combine all these three figures.\ncat_list = [\"FOODS\", \"HOBBIES\", \"HOUSEHOLD\"]\ncolor_list = [\"orange\", \"green\", \"blue\"]\ntemp_series = df.groupby([\"store_id\", \"cat_id\", \"wday\"])[\"value\"].sum()\nwidth = 0.25\n\nfig, axs = plt.subplots(3, 4, figsize=(20, 12), sharey=True) \n\nfor row in range(len(store_id_list_by_state)):\n    for col in range(len(store_id_list_by_state[row])):\n        for i, cat in enumerate(cat_list):\n            height_numerator = temp_series[(temp_series.index.get_level_values(\"cat_id\") == cat) & (temp_series.index.get_level_values(\"store_id\") == store_id_list_by_state[row][col])].values\n            height_denominater = height_numerator.sum()\n\n            axs[row, col].bar(x=temp_series[(temp_series.index.get_level_values(\"cat_id\") == cat) & (temp_series.index.get_level_values(\"store_id\") == store_id_list_by_state[row][col])].index.get_level_values(\"wday\") + width * (i-1),\n                              height=height_numerator / height_denominater,\n                             tick_label=weekday, color=color_list[i], width=width, label=cat)\n            axs[row, col].set_title(store_id_list_by_state[row][col])\n            axs[row, col].legend()\n            \nfig.suptitle(\"HOBBIES item sold in each store in each day\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Snap Purchase Analysis\nsnap 구매가 허용된 날은 어떻게 분포되었는 지 확인하자.\n\n<!--\nLet's see how snap purchase allowed day is distributed.\n-->","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(1, 3, sharey=True)\nfig.suptitle(\"Snap Purchase Enable Day Count of each store\")\n\nsns.countplot(x=\"snap_CA\", data =calendar_df, ax=axs[0])\nsns.countplot(x=\"snap_TX\", data =calendar_df, ax=axs[1])\nsns.countplot(x=\"snap_WI\", data =calendar_df, ax=axs[2])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. 좋다, snap 구매가 허용된 날의 총 수는 3개의 상점에서 유사한 것으로 보인다.\n\n2. snap 구매가 허용된 날의 총 수는 가능하지 않은 날의 약 절반정도이다.\n\n다음으로 snap 구매가 일년 동안 얼마나 분포되었는 지 확인해보자.\n\n<!--\n1. OK, the total count of snap purchase enable day looks similar in these three stores.\n\n2. The total count of snap purchase enable day is about one half of that of non-enable day.\n\nNext Let's see whether snap purchase is how-distributed in one year.\n-->","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_df = calendar_df.groupby([\"year\"])[[\"snap_CA\", \"snap_TX\", \"snap_WI\"]].sum()\ntemp_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. 좋다. 각 주별로 총 snap 구매가 허용된 날의 총 수는 모든 해에서 동일하다.\n\n2. 2011년부터 2015년까지, snap 구매가 허용된 날은 약 120일이다.  \n   (2016년에 이르러서야, 1년의 절반이었다.)\n\n<!--\n1. OK. Total snap purchase allowed day of each state is the same in all years.\n\n2. From 2011 to 2015, there are about 120 days when snap purchase is allowed.  \n   (As for 2016, we only have the first half of whole year.)\n-->","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# This cell is just visuallizing the above dataframe.\nplt.bar(temp_df.index, temp_df.snap_CA)\nplt.ylabel(\"# of snap purchase allowed day\")\nplt.xlabel(\"Year\")\nplt.title(\"Snap Purchase allowed day yearly transition\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"좋다, 한 해에 총 수가 모든 해와 모든 주에서 거의 같다.  \n월간 분포는 어떨까?\n\n<!--\nOK, total count in one year is almost the same in all years and all states.\nHow about monthly distribution?\n-->","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_df = calendar_df[calendar_df[\"year\"] == 2015].groupby([\"month\"])[[\"snap_CA\", \"snap_TX\", \"snap_WI\"]].sum()\ntemp_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"그 해 동안, 한 달동안, 10일의 snap 구매가 허용된 날들이 있었다.  \n이 경향은 2012년부터 2015년까지 같다.  \n(2011년에 1월에는 snap 날이 없다.)\n\n<!--\nThrough the year, we have 10 snap purchase allowed days in one month.  \nThis tendency is the same from 2012 to 2015.  \n(In 2011, no snap days in January)\n-->","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Just visualizing the above dataframe\nplt.bar(temp_df.index, temp_df.snap_CA)\nplt.ylabel(\"# of snap purchase allowed day\")\nplt.xlabel(\"Month\")\nplt.title(\"Snap Purchase allowed day monthly trend\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"좋다, 한달에 총 수는 전체 해로 보면 같다.  \n주간 분포는 어떨까?\n\n<!--\nOK, total count in one month is the same through the whole year.\nHow about weekly distribution?\n-->","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_df = calendar_df[calendar_df[\"year\"] == 2015].groupby([\"weekday\"])[[\"snap_CA\", \"snap_TX\", \"snap_WI\"]].sum()\ntemp_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"주간 경향에 얘기한 것처럼, 우리는 편향된 분포를 찾을 수 없다.  \n연간 총합과 월간 총합처럼 거의 동일하게 분포되어 있다.\n\n\n<!--\nRegarding weekly trend, we can find no biased distribution.  \nThis is also almost uniformly distributed like year total and month total.\n-->","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.bar(temp_df.index, temp_df.snap_CA)\nplt.xticks(rotation=60)\nplt.ylabel(\"# of snap purchase allowed day\")\nplt.xlabel(\"Day type\")\nplt.title(\"Snap Purchase allowed day weekly trend\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"위에 것들에서, 우리는 \"snap이 가능한 날은 3일에 한번 처럼 동일하게 분포되었구나.\" 라고 생각할 지도 모른다. 왜냐하면 모든 barplot이 어떤 달이나 어떤 날이나 그렇게 다르지 않다는 것을 보여주기 때문이다.  \n그러나, 이것은 **완전히 다르다** 라는 것을 밑에서 보여줄 것이다.\n(예, snap 구매 가능한 날은 편향되게 분포되어 있다.)\n\n<!--\nFrom above things, we may think \"oh, snap_enable day is distributed uniformly, like 1 day in 3 consecutive days.\" because all of these barplot show it's not so different in any month, any day.  \nHowever, it is **completely different** as I'll show you below.  \n(i.e. snap purchase enable day is distributed biasedly.)\n-->","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make temp dataframe with necessary information\ntemp_df = df.groupby([\"date\", \"state_id\"])[[\"value\"]].sum()\ntemp_df = temp_df.reset_index()\ntemp_df = temp_df.merge(calendar_df[[\"date\", \"snap_CA\", \"snap_TX\", \"snap_WI\"]], on=\"date\")\ntemp_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"예를 들어 가장 많이 판매된 상품을 찾고, snap 구매가 허용된 flag와 values를 사이의 관계를 보자.\n\n<!--\nFind the most item sold day for example and take a look at the relationship between snap purchase allowed flag and values.\n-->","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"np.argmax(temp_df.groupby([\"date\", \"state_id\"])[\"value\"].sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_df = temp_df[(temp_df.date >= \"2016-02-15\") & (temp_df.date <= \"2016-03-25\") & (temp_df.state_id == \"CA\")]\ntemp_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax1 = plt.subplots()\nplt.xticks(rotation=60)\nax1.plot(\"date\", \"value\", data=temp_df[temp_df.state_id == \"CA\"])\nax2 = ax1.twinx()  \nax2.scatter(\"date\", \"snap_CA\", data=temp_df[temp_df.state_id == \"CA\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"위 그림에서, 각 plot은 CA에서 snap 구매가 허용된 날인 지 아닌 지를 의미한다.  \n여러분들이 보는 것처럼, snap 구매는 3일 후에 하루처럼 규칙적으로 분포되어 있지 않다.  \n(ex. 2016-03-31 > 2016-03-04 > 2016-03-07 > ...)  \n이것은 위 그럼처럼 사실 편향되게 분포되어 있다.  \n(예, Snap 구매가 가능한 날은 2016-03-01 부터 2016-03-19까지 계속된다.)  \n그리고 이런 날에, 판매는 또한 증가했다.\n\n<!--\nIn above figure, each plot means whether the day allows snap purchase or not in CA.  \nAs you can see, snap purchase enable day is not regularly distributed like one day in three consective days.  \n(ex. 2016-03-01 > 2016-03-04 > 2016-03-07 > ...)  \nIt is actually biasedly distributed like the figure above.  \n(i.e. Snap purchase Enable Day continues from 2016-03-01 to 2016-03-10)  \nAnd on these days, sales are also increased.  \n-->","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Event Pattern Analysis\nevent_name_1 컬럼에서 행사 패턴을 확인하자.  \n(event_name_2 컬럼의 경우, event_name_1 컬럼과 비교하면 null이 아닌 값들이 훨씬 적다.)\n\n<!--\nLet's check event pattern in event_name_1 column.  \n(As for event_name_2 column, there are much less non-null values compeared to event_name_1 column.\n-->","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8, 6))\nsns.countplot(x=\"event_type_1\", data=calendar_df[calendar_df[\"event_name_1\"] != \"unknown\"])\nplt.xticks(rotation=90)\nplt.title(\"Event Type Count in event name 1 column\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"좋다, 행사 타입 분포는 위 그래프와 같다.  \n(대부분의 값들이 사실 \"unknown\" 이나 시각화를 위해 불명값은 제외했다.)\n\n<!--\nOK, event tyoe distributes like the graph above.   \n(Most of the values are actually \"unknown\", but for visualization, I omitted unknown value)\n-->","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's check the distribution of snap purchase day and event day\n# Accirding to the graph, Snap CA is allowed especially when sport event occurs.\n\nplt.figure(figsize=(8, 6))\nsns.countplot(x=\"event_type_1\", data=calendar_df[calendar_df[\"event_name_1\"] != \"unknown\"], hue=\"snap_CA\")\nplt.xticks(rotation=90)\nplt.legend(bbox_to_anchor=(1.01, 1.01))\nplt.title(\"Snap Purchse allowed day Count in each event category\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"이벤트 날의 판매를 확인하자!\n\n<!--\nLet's check the sales of event day!\n-->","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_series = df.groupby([\"cat_id\", \"event_type_1\"])[\"value\"].mean()\ntemp_series","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.bar(x=temp_series[temp_series.index.get_level_values(\"cat_id\") == \"HOBBIES\"].index.get_level_values(\"event_type_1\"), \n        height=temp_series[temp_series.index.get_level_values(\"cat_id\") == \"HOBBIES\"].values)\nplt.title(\"HOBBIES Item Sold mean in each event type\")\nplt.ylabel(\"Item sold mean\")\nplt.xlabel(\"Event Type\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"나는 몇몇의 문화적이거나 스포츠 행사가 일어났을 때, HOBBIES 상품이 더 잘 팔리는 경향이 있지 않을까 생각했다.  \n그러나, 이 plot은 이런 가설이 분명하다고 의미하지 않는다.\n\n<!--\nI thought when some cultual or sporting event occurs, HOBBIES item are more likely to be sold.  \nHowever, this plot doesn't mean this hypothesis clearly.\n-->","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## One Item Features Analysis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# find out most sold item for example\ndf[df[\"value\"] == df[\"value\"].max()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"이 데이터셋에서 대부분의 매진 상품은 FOODS_3_090_CA_3_validation 이다.\n\n<!--\nThe most sold out item in this dataseet is FOODS_3_090_CA_3_validation\n-->","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"target_id = \"FOODS_3_090_CA_3_validation\"\ntemp_df = df[df[\"id\"] == target_id]\ntemp_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weekday = [\"Sat\", \"Sun\", \"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\"]\n\n# Create one hot weekday column from wday column to calculate correlation later. \nfor idx, val in enumerate(weekday):\n    temp_df.loc[:, val] = (temp_df[\"wday\"] == idx + 1).astype(\"int8\")\n\ntemp_df\n# sns.heatmap(temp_df[[\"value\", \"snap_CA\", ]].corr(), annot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create Event Flag (Any events occur: 1, otherwise: 0)\n# Create Each Event Type Flag\ntemp_df.loc[:, \"is_event_day\"] = (temp_df[\"event_name_1\"] != \"unknown\").astype(\"int8\")\ntemp_df.loc[:, \"is_sport_event\"] = (temp_df[\"event_type_1\"] == \"Sporting\").astype(\"int8\")\ntemp_df.loc[:, \"is_cultural_event\"] = (temp_df[\"event_type_1\"] == \"Cultural\").astype(\"int8\")\ntemp_df.loc[:, \"is_national_event\"] = (temp_df[\"event_type_1\"] == \"National\").astype(\"int8\")\ntemp_df.loc[:, \"is_religious_event\"] = (temp_df[\"event_type_1\"] == \"Religious\").astype(\"int8\")\n\ntemp_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot Heatmap with these columns made in previous cells\nplt.figure(figsize=(14, 10))\nsns.heatmap(temp_df[[\"value\", \"sell_price\", \"snap_CA\", \"is_event_day\", \"is_sport_event\", \"is_cultural_event\", \"is_national_event\", \"is_religious_event\"] + weekday].corr(), annot=True)\nplt.title(\"Heatmap with values, snap_CA,  event_flag and weekday columns\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"우리는 다음의 것들을 알 수 있다.\n1. value와 다른 열들의 상관관계에 관해:\n   - snap 구매와 다른 행사 flag는 거의 상관관계가 없다.\n   - 토요일은 value에 가장 많은 긍정적 영향을 갖고 있고, 화요일은 가장 부정적인 영향을 갖고 있다.\n     (우리는 이전에 토요일이 한 주에 가장 많은 상품을 판매한 날이라고 보았다. [here](#Item-Sold-in-each-day-type).)  \n     \n     \n2. snap_CA와 평일 커럼의 상관관계에 관해:\n   - 우리가 이전에 본 것처럼, snap_CA는 각 날의 타입에 동일하게 분포되어 있다.\n     따라서, snap_CA와 평일(월,화,수 ..) 컬럼 사이에 상관관계는 거의 0이다.  \n\n\n3. 나머지:\n   - 행사와 일요일의 상관관계를 보면, 거의 0.089이다.\n     저는 대부분의 행사 일요일에 했다고 생각했다. 그러나 이것은 제가 예상한 것처럼 많지 않았다.\n   - 판매 가격에 관해서는 아래를 보자.\n\n<!--\nWe can find the following things.\n1. Regarding value and other columns correlation:\n   - snap purchase and other events flag has little correltion.\n   - Saturday has the most positive effect on values, and Tuesday has the most negative effect.  \n     (We've previously seen Saturday is the most item sold day in one week [here](#Item-Sold-in-each-day-type).)\n     \n2. Regarding snap_CA and weekdays columns correlation:\n   - As we've previously seen, snap_CA is uniformly distributed in each day type.  \n     Thus, the correlation between snap_CA and weekdays columns (ex. Monday, Tuesday, ...) are almost 0.\n\n3. Others:\n   - Looking at event and sunday correlation,it is just 0.089.  \n     I thought most part of events oocur on Sunday, but it wasn't so much as I had exoected.\n   - Regarding sell price, we look below.\n-->","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Sell Price Analysis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.groupby(\"cat_id\")[\"sell_price\"].mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.groupby(\"cat_id\")[\"sell_price\"].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*.decribe* 함수를 사용했을 때, mean = NaN인 이유를 이해하지 못 했다. 그러나 *.mean()* 함수는 카테고리의 판매 가격 평균을 정확히 계산한다.  \n여러 방식으로 그려보자.\n\n<!--\nI don't understand why mean = NaN when using *.describe* method, however, *.mean()* method accurately calculate category's sell price mean.  \nLet's plot it with some ways!\n-->","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(data=df, x=\"cat_id\", y='sell_price')\nplt.title(\"Boxplot of sell prices in each category\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"일부 HOUSEHOLD 카테고리의 가격은 100 \\$ 보다 훨씬 비싸다.  \n반면에, FOODS은 거의 약5에서 10 \\$ 이고, 큰 편차가 없다.\n\n<!--\nThe price of some Household category is super expensive like over 100 \\$.  \nOn the other hand, foods are mostly around 5 to 10 \\$ and don't have a large deviation.\n-->","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12, 6))\nsns.boxplot(data=df, x=\"cat_id\", y='sell_price', hue=\"store_id\")\nplt.title(\"Boxplot of sell prices in each store\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"이미 HOUSEHOLD 중 일부가 예외적으로 판매 가격이 비싸다는 것을 봤다.\n이제 이 상품들을 몇몇의 상점을 제외하면 더 이상 어디서도 판매하지 않는 다는 것을 알았다.\n\n<!--\nWe've already seen HOUSEHOLD has some exceptionally expensive sell price.  \nNow we found these items aren't sold anywhere, but only in some stores.\n-->","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# One Item Sell Price Transition\nsns.lineplot(data=df[df[\"item_id\"] == \"FOODS_3_090\"], x='date', y='sell_price', hue=\"store_id\")\nplt.legend(bbox_to_anchor=(1.01, 1.01))\nplt.title(\"Sell price change of 'FOODS_3_090' in each store\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"상품이 같다고 하면, 판매 가격은 각 상점과 각 계절에 따라 약간 다르다. -> 프로모션 시즌?\n\n<!--\nThough the item is same, sell price is slightly different in each store and each season. -> Some promotion season?\n-->","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"is_event_day\"] = (df[\"event_name_1\"] != \"unknown\").astype(\"int8\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(df[df[\"item_id\"] == \"FOODS_3_090\"][[\"value\", \"sell_price\", \"is_event_day\"]].corr(), annot=True)\nplt.title(\"Heatmap of value, sell_price and event flag\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"행사 날 flag와 판매 가격은 그렇게 강한 관계가 없다.\n그러나, 우리가 몇몇 행사를 위해 상품을 샀을 때, 우리는 아마도 행사 1주에서 1일 전에 산다. 당일이 아니라.\n그래서 우리는 이것을 고려해야한다. (나는 이것을 나중에 분석할 것이다.)\n\n<!--\nEvent Day flag and Sell Price don't have so strong relationship.  \nHowever, when we buy items for some events, we perhaps buy items 1 week ~ 1 day before the event, not on the same day.  \nSo we have to take this into consideration.  (I'll tackle with this analysis later.)\n-->","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_df = df.groupby([\"date\", \"cat_id\"])[\"sell_price\"].mean()\ntemp_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,4))\nsns.lineplot(x=temp_df[temp_df.index.get_level_values(\"cat_id\") == \"FOODS\"].index.get_level_values(\"date\"), y=temp_df[temp_df.index.get_level_values(\"cat_id\") == \"FOODS\"].values, label=\"FOODS\")\nsns.lineplot(x=temp_df[temp_df.index.get_level_values(\"cat_id\") == \"HOUSEHOLD\"].index.get_level_values(\"date\"), y=temp_df[temp_df.index.get_level_values(\"cat_id\") == \"HOUSEHOLD\"].values, label=\"HOUSEHOLD\")\nsns.lineplot(x=temp_df[temp_df.index.get_level_values(\"cat_id\") == \"HOBBIES\"].index.get_level_values(\"date\"), y=temp_df[temp_df.index.get_level_values(\"cat_id\") == \"HOBBIES\"].values, label=\"HOBBIES\")\nplt.legend()\nplt.xlabel(\"Year\")\nplt.ylabel(\"Mean price\")\nplt.title(\"Mean price transition of each category\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"각 카테고리의 상품 가격의 평균은 이 데이터셋의 주기에서 몇몇의 변화가 있는 것 같다.\n이것은 단순히 이후 주기에 비싼 상품이 증가했기 때문일까?\n\n<!--\nThe mean of item price in each category seems to have some change in these dataset periods.  \nIs this simply because the expensive item increased in later periods?\n-->","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_df = df.groupby([\"date\", \"cat_id\"])[\"item_id\"].count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,4))\nsns.lineplot(x=temp_df[temp_df.index.get_level_values(\"cat_id\") == \"FOODS\"].index.get_level_values(\"date\"), y=temp_df[temp_df.index.get_level_values(\"cat_id\") == \"FOODS\"].values, label=\"FOODS\")\nsns.lineplot(x=temp_df[temp_df.index.get_level_values(\"cat_id\") == \"HOUSEHOLD\"].index.get_level_values(\"date\"), y=temp_df[temp_df.index.get_level_values(\"cat_id\") == \"HOUSEHOLD\"].values, label=\"HOUSEHOLD\")\nsns.lineplot(x=temp_df[temp_df.index.get_level_values(\"cat_id\") == \"HOBBIES\"].index.get_level_values(\"date\"), y=temp_df[temp_df.index.get_level_values(\"cat_id\") == \"HOBBIES\"].values, label=\"HOBBIES\")\nplt.legend()\nplt.xlabel(\"Year\")\nplt.ylabel(\"Registered Item Counts\")\nplt.title(\"Registered Item Counts Transition in each category\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"이런 값의 양이 다른다는 것을 통해서 모든 카테고리는 비슷한 추세를 갖고 있다.\n2015부터, 등록된 상품의 수는 상대적으로 상수처럼 보인다.\n\n<!--\nAll categories have similar trends though the volume of these values are different.  \nFrom 2015, the number of registered items is seemed to be relatively constant.\n-->","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Sell price and value relationship","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.jointplot(df[\"value\"], df[\"sell_price\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"이 결과 또한 흥미롭다.\n물론, 상품의 가격이 비쌀 때, 상품이 덜 판매된다. (왼쪽위 영역)\n그리고 가격이 낮아질 때, 더 많은 상품이 팔린다. (오른쪽아래 영역)\n그러나, 관계는 선형적이지 않은 것 같으나 역 비례처럼 보인다.\n\n<!--\nThis result is also interesting.  \nOf course, when the price of item is expensive, less items are sold. (left top field)  \nAnd when the price gets lower, more items are sold.  (Right bottom field)  \nHowever, the relationship is not likely linear but likely to be inverse propotion.\n-->","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Discount Season Presumption\n모든 상품이 Walmart의 평소보다 더 낮은 가격인 기간이 있을까?\n\n<!--\nIs there a period when all items have lower price than usual in Walmart?\n-->","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"sell_price_diff\"] = df.groupby(\"id\")[\"sell_price\"].transform(lambda x: x - x.mean()).astype(\"float32\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.lineplot(df[df[\"item_id\"] == \"FOODS_3_090\"][\"date\"],df[df[\"item_id\"] == \"FOODS_3_090\"][\"sell_price_diff\"], hue=df[\"store_id\"]) \nplt.legend(bbox_to_anchor=(1.01, 1.01))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"모든 상점이 2013년 하반기에 평균 가격보다 더 낮은 가격이었다.\n\n<!--\nAll store has lower price than mean value in the latter half of 2013.  \n-->","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_df = df[df[\"item_id\"] == \"FOODS_3_090\"].groupby(\"date\")[\"value\"].sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax1 = plt.subplots(figsize=(12, 4))\nax1.plot(temp_df)\nax1.set_ylabel(\"# of Sold items\")\nax2 = ax1.twinx()  \nax2.plot(df[(df[\"item_id\"] == \"FOODS_3_090\") & (df[\"store_id\"] == \"CA_3\")][\"date\"],\n         df[df[\"item_id\"] == \"FOODS_3_090\"].groupby(\"date\")[\"sell_price_diff\"].mean(), color=\"red\")\nax2.set_ylabel(\"price_diff [\\$]\") \nplt.title(\"FOODS_3_090 sold number and price difference from mean\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"FOODS_3_090의 경우, 가격이 평균보다 낮아졌을 때, 판매 수는 더 높았다.  \n특히, 2013의 하반기의 상품의 판매 수는 평소보다 약 2.5배 더 높았다. 그리고 이것은 이 상품이 평소보다 더 낮은 가격일 때의 기간이다.  \n이것은 이 상품이 같은 지점에 연속적인 날들에 판매가 없던 것 같다.  \n이것은 모델을 만들 때 노이즈가 될 수 있다.  \n\n다른 상품을 보자.\n\n<!--\nAs for FOODS_3_090, when the price gets lower than its mean, the number of sold gets higher. \nEspecially, the latter half of 2013, # of sold items gets higher about 2.5 times than usual, and this is the period when this item had lower price than usual. \nIt seems that this item has no sell for some consectuive days in some points.  \nIt can be noise when making models.\n\n\nLet's see some other items.\n-->","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find the most sold items in  HOBBIES section\nnp.argmax(df[df[\"cat_id\"] == \"HOBBIES\"].groupby(\"item_id\")[\"value\"].sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_df = df[df[\"item_id\"] == \"HOBBIES_1_371\"].groupby(\"date\")[\"value\"].sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax1 = plt.subplots(figsize=(12, 4))\nax1.plot(temp_df)\nax1.set_ylabel(\"# of Sold items\")\nax2 = ax1.twinx()  \nax2.plot(df[(df[\"item_id\"] == \"HOBBIES_1_371\") & (df[\"store_id\"] == \"CA_3\")][\"date\"],\n         df[df[\"item_id\"] == \"HOBBIES_1_371\"].groupby(\"date\")[\"sell_price_diff\"].mean(), color=\"red\")\nax2.set_ylabel(\"price_diff [\\$]\") \nplt.title(\"HOBBIES_1_371 sold number and price difference from mean\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"이전의 FOOD 상품 결과에 비교하면, 이 상품은 판매 가격의 더 낮은 변동성을 갖고 있다.  \n그리고 판매 상품의 수 또한 더 낮은 변동성이다.\n(FOOD 상품의 경우, 최고 시즌에 평소보다 약 2.5배 더 많았다.)\n\n다시, 2012년 하반기에 일부 판매되지 않은 기간이 있다는 것을 알았다.\n모든 상품이 판매되지 않은 이런 기간이 있을까?\n\n<!--\nCompared to the previous FOOD item result, this item has lower volatility of sell price.   \nAnd the number of sold items has also lower volatility.  \n(In the FOOD item, we have around 2.5 times as much as usual sell on the top season)  \n\nAgain, we see some non sold period in the latter half of 2012.  \nDoes all item have these kinds of non-sold period?\n-->","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"(20200528 시작)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_series = df.groupby([\"date\", \"item_id\"])[\"value\"].sum()\ntemp_series","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find Top 12 items that the mean of sold counts in all stores is high\nhigh_sold_item_top12 = temp_series.groupby(\"item_id\").mean().sort_values(ascending=False)[:12].index\nhigh_sold_item_top12","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(3, 4, figsize=(20, 12), sharey=True, sharex=True) \n\nfor row in range(3):\n    for col in range(4):\n        target_item = high_sold_item_top12[row*4+col]\n        \n        axs[row, col].plot(temp_series[temp_series.index.get_level_values(\"item_id\") == target_item].values)\n        axs[row, col].set_title(target_item)\n        axs[row, col].set_ylabel(\"# of sold items in all stores\")\n#         axs[idx, col].set_xticks(temp_series[temp_series.index.get_level_values(\"item_id\") == target_item].index.get_level_values(\"date\"))\n        axs[row, col].legend()\n            \nfig.suptitle(\"Top 12 item sold in all stores in each day\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"저는 모든 상점에서 하루 동안 가장 많이 팔린 12개의 상품을 가져왔습니다.\n그 결과는 \"FOODS_3_090\"이 다른 것보다 훨씬 더 높은 판매를 가지고 있다는 것을 보여줍니다.\n그리고 여기에 있는 모든 상품은 FOODS 카테고리이고 부서 번호는 3입니다!\n\n그래서, 다음 질문은, 각 부서에 어떤 feature가 있을까요?\n예를 들어, FOODS_3 부서에서, 싸고 자주 사용되는 상품을 다루고 있을까요?\n\n<!--\nI got top 12 sold items in all stores per one day.  \nThe result shows \"FOODS_3_090\" has very higher sold than others.  \nAnd all items here are FOODS category and department no is 3!\n\nSo, next question, are there any features in each dept?  \nFor example, in FOODS_3 department, do they treat cheap and frequently-used item?\n-->","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Quarter Analysis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Take a closer look,\nsns.violinplot(x = df.loc[df[\"year\"] == 2015, \"quarter\"], y = df.loc[df[\"year\"] == 2015, \"value\"])\nplt.ylim(0, 10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"우리는 3분기에 (7월부터 9월) 판매된 상품이 다른 분기 보다 적다는 것을 있다는 것을 알 수 있습니다. \n\nviolinplot 함수는 이 분포에 대한 값을 그릴 수 있습니다.  \n여러분은 seaborn violinplot 을 찾을 수 있습니다. (https://seaborn.pydata.org/generated/seaborn.violinplot.html)  \n> violin plot은 box와 whisker plot과 비슷한 역할을 합니다. 이것은 정량적인 데이터의 분포를 하나 (또는 그 이상)의 categorical 변수의 몇 단계로 보여줍니다. 그래서 그 분포들이 비교될 수 있게 해줍니다. 실제 datapoint에 상응하는 plot 요소의 전부가 있는 box plot과 달리, violin plot은 분포를 하나의 근본적인 분포에 대한 kernel density estimation(커널밀도추정)를 중요하게 여깁니다.\n\n따라서, boxplot과 violinplot 사이의 차이는, 공식 문서에 써있는 것처럼, violinplot은 kernel density estimation(커널밀도추정) 을 그립니다.\n\n<!--\nWe can see in quarter 3 (from July to September), the item sold seems less than other quarters.\n\nviolinplot method can plot values with its distribution.  \nYou can search seaborn violinplot (https://seaborn.pydata.org/generated/seaborn.violinplot.html)\n> A violin plot plays a similar role as a box and whisker plot. It shows the distribution of quantitative data across several levels of one (or more) categorical variables such that those distributions can be compared. Unlike a box plot, in which all of the plot components correspond to actual datapoints, the violin plot features a kernel density estimation of the underlying distribution.\n\nThus, the difference between boxplot and violinplot is, as the official document says, violinplot plots with its kernel density estimation.\n-->","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Department Analysis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_df = df.groupby([\"date\", \"cat_id\", \"dept_id\"])[\"value\", \"sell_price\"].mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(2, 3, figsize=(14, 10)) \n\n\nfor col in range(3):\n    target_cat = cat_list[col]\n    sns.scatterplot(\"value\", \"sell_price\", hue=temp_df[temp_df.index.get_level_values(\"cat_id\") == target_cat].index.get_level_values(\"dept_id\") ,\n                data=temp_df[temp_df.index.get_level_values(\"cat_id\") == target_cat], ax=axs[0, col])\n#     axs[0, col].plot(temp_series[temp_series.index.get_level_values(\"item_id\") == target_item].values)\n    axs[0, col].set_title(f\"{target_cat} \")\n    axs[0, col].set_ylabel(\"sell_price\")\n    axs[0, col].set_xlabel(\"value\")\n#         axs[idx, col].set_xticks(temp_series[temp_series.index.get_level_values(\"item_id\") == target_item].index.get_level_values(\"date\"))\n    axs[0, col].legend()\n    \n    temp_series = df[df[\"cat_id\"] == target_cat].groupby([\"date\", \"dept_id\"])[\"item_id\"].count()\n    sns.lineplot(x=temp_series.index.get_level_values(\"date\"), y=temp_series.values, hue=temp_series.index.get_level_values(\"dept_id\"), ax=axs[1, col])\n    axs[1, col].set_title(f\"{target_cat}\")\n    axs[1, col].set_ylabel(\"count\")\n    axs[1, col].legend()\n            \nfig.suptitle(\"Daily Average value - price plot and item count transition in each dept.\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"이런 분석을 통해, 우리는 부서에 대한 다음의 것들을 알 수 있습니다.\n\n**Common**\n- 각 카테고리의 부서는 각 부서의 평균 판매 가격에 의해 분류되는 것 같다.\n\n**FOODS**\n- dept_3 은 다른 것과 비교해서 더 낮은 가격과 높은 용량의 plot 을 가지고 있다.\n- 추가로, dept_3은 다른 부서보다 더 많은 상품을 가지고 있다.\n\n**HOBIES**\n- dept_1은 dept2 보다 더 많은 상품을 갖고 있다.\n\n**HOUSEHOLD**\n- 다른 카테고리와 달리, 이 카테고리는 부서 별로 분명한 차이를 가지고 있지 않은 것 같다.\n\n<!--\nFrom these analysis, we can find the following things regarding department.\n\n**Common**\n- Departments in each category are seemed to be classified by average sell price of its department.  \n\n**FOODS**\n- dept_3 has lower price and high volume plot compared to others.  \n- In addition, dept_3 has more items than other departments.\n\n**HOBBIES**\n- dept_1 has more items than dept_2\n\n**HOUSEHOLD**\n- Unlike other categories, this category seemed that it doesn't have clear difference in each department.\n-->","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Next Week Events and This Week Sales Relationship Analysis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"num_of_next_week_event\"] = df.groupby(\"id\")[\"is_event_day\"].transform(lambda x: x.shift(-7).rolling(7).sum().fillna(0)).astype(\"int8\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df.id == \"HOBBIES_1_008_CA_1_validation\"][25:50]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"거래가 올바른 지 아닌 지 확인해보자.\n\n\"num_of_next_week_event\" 컬럼을 보면, 위 dataframe에 있는 0이 아닌 첫 번째 지점이 \"2011-03-02\" (수요일) 이다.  \n우리가 다음 주 행사를 알고 싶기 때문에, 이것은 \"2011-03-09\" (다음 주 수요일)에 몇몇의 행사가 있다는 것을 의미하고 이것은 사실이다.  \n\"2011-03-10\" 과 \"2011-03-15\" 사이에, 우리는 어떤 더 많은 행사가 있지 않다. 그래서 num_of_next_week_event 컬럼은 \"2011-03-03\" 과 \"2011-03-08\" 은 1이 돼야 한다.\n(이것은 우리가 오직 \"2011-03-09\" 만 세기 때문이다.)\n그리고 \"2011-03-16\" 행사에 상응하는 next_week_event_sum 컬럼은 \"2011-03-09\" 에 1을 갖는다.  \n다음 날에, 우리는 또한 행사를 가지기 때문에, 이에 상응하는, next_week_event_sum 컬럼은 \"2011-03-10\"에 2를 갖는다.  \n\n이 설명이 여러분은 이해가 되나요?\n\n이런 식으로, 우리는 next_week 행사 feature를 추가할 수 있습니다. 왜냐하면, 우리는 종종 근 미래의 큰 행사가 있을 때 쇼핑하기 때문입니다.  \n이제 이 컬럼이 정말로 긍정적인 관계가 있는 지 알아봅시다.\n\n<!--\nLet's check whether the trainsaction is right.  \n\nLooking at \"num_of_next_week_event\" column, the first point of non-zero in the above dataframe is on \"2011-03-02\" (Wednesday) .  \nSince we want to know the next week event sum, this means on \"2011-03-09\" (next Wednesday) we have some events and this is true.  \nBetween \"2011-03-10\" and \"2011-03-15\", we dont' have any more events, so the num_of_next_week_event column between \"2011-03-03\" and \"2011-03-08\" should be 1 \n(This is because we count only \"2011-03-09\" day)  \nAnd corresponding to the event on \"2011-03-16\", next_week_event_sum column gets 1 on \"2011-03-09'.  \nNext day, we also have events, so corresponding to that, next_week_event_sum column gets 2 from \"2011-03-10\".  \n\nDoes this explanation make sense to you?\n\nLike this way, we can add feature of next_week event, since we ofen go shopping when we have big events in the near furture.  \nNow let's see whether this column has really positive relation to the value.\n-->","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(1, 3, figsize=(8, 6), sharey=True)\nsns.scatterplot(\"num_of_next_week_event\", \"value\", data=df[df[\"cat_id\"] == \"HOBBIES\"], color=\"green\",ax=axs[0])\nsns.scatterplot(\"num_of_next_week_event\", \"value\", data=df[df[\"cat_id\"] == \"HOUSEHOLD\"], color=\"orange\", ax=axs[1])\nsns.scatterplot(\"num_of_next_week_event\", \"value\", data=df[df[\"cat_id\"] == \"FOODS\"], ax=axs[2])\naxs[0].set_title(\"HOBBIES\")\naxs[1].set_title(\"HOUSEHOLD\")\naxs[2].set_title(\"FOODS\")\n\nfig.suptitle(\"Relationship between next week events count and sold item counts\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"사실, 우리는 다음 주 행사와 이번 주 판매 수 사이에 분명한 관계를 알 수 없었습니다.  \n저는 만약 다음 주에 많은 행사가 있다면 판매가 증가하기를 기대했습니다.  \n우리는 이 부분에 대해 더 많은 분석이 필요합니다.  \n\n이제까지, 우리는 종종 subplot을 사용하고 다른 plot인 것 처럼 각 카테고리를 뺐습니다.  \nseaborn의 많은 함수들은 거대한 특징을 가지고 있고, 이것은 하나의 plot에 있는 모든 카테고리를 다른 색으로 뺄 수 있습니다.  \n이것은 매우 유용합니다. 그러니 이번에는, 이것은 종종 메모리 에러를 일으키고, 커널을 죽게 만듭니다. 그래서 이번에는 사용하지 않습니다.\n\n<!--\nActually, we couldn't see the clear relationship between next week event and this week's sold count.  \nI had expected that the sales would increase if there are many events in the next week. \nWe need more analysis regarding this point.\n\nSo far, we often use subplots and drop each category as a different plot.  \nMany methods of Seaborn have hue attribute and this can drop all categories in one plot with different color.  \nIt is very useful, but this time, it often causes Memory Error and makes kernel die, so this time I didn't use it.\n-->","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"(20200528 끝)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Relationship of Lag Variables","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"lag_1\"] = df.groupby(\"id\")[\"value\"].transform(lambda x: x.shift(1)).astype(\"float16\")\ndf[\"lag_7\"] = df.groupby(\"id\")[\"value\"].transform(lambda x: x.shift(7)).astype(\"float16\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plt.figure(figsize=(8, 8))\n# sns.pairplot(df[[\"cat_id\", \"value\", \"lag_1\"]], hue=\"cat_id\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(df[[\"cat_id\", \"value\", \"lag_1\", \"lag_7\"]], hue=\"cat_id\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"이런 경우에는 아마도 하루나 한 주의 지연 변수가 중요하다.\n나의 다음 버전에서, 판매 날과 과거 며칠사이의 관계의 강점을 알아볼 것이다.\n\n<!--\nMaybe 1 day or 1 week lag variables are important in this case.  \nI'll find the strength of correlation between the sold of the day and past few days in my next version.\n-->","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"(20200528 시작)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Calendar Visualization","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install calmap","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import calmap\n\ntemp_df = df.groupby([\"state_id\", \"date\"])[\"value\"].sum()\ntemp_df = temp_df.reset_index()\ntemp_df = temp_df.set_index(\"date\")\ntemp_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(3, 1, figsize=(10, 10))\ncalmap.yearplot(temp_df.loc[temp_df[\"state_id\"] == \"CA\", \"value\"], year=2015, ax=axs[0])\naxs[0].set_title(\"CA\")\ncalmap.yearplot(temp_df.loc[temp_df[\"state_id\"] == \"TX\", \"value\"], year=2015, ax=axs[1])\naxs[1].set_title(\"TX\")\ncalmap.yearplot(temp_df.loc[temp_df[\"state_id\"] == \"WI\", \"value\"], year=2015, ax=axs[2])\naxs[2].set_title(\"WI\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dark Red days 는 그 날에 판매가 큰 것을 의미합니다.\n반대로, white days는 그 날에 판매가 적은 것을 의미합니다.\n(여러분은 크리스마스 때 plot이 하얀 cell을 가지고 있다는 것을 알 수 있습니다.)\n\n<!--\nDark Red days mean the sales on that day is large.  \nOn the contrary, white days mean the sales on that day is low.  \n(You can see on Christmas day the plot has white cells.)\n-->","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"(20200528 끝)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## PCA Trial","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"(20200628) 시작 (개선됨)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"지금부터, PCA를 사용해서 상점 사이에 유사함을 알아봅시다!\n\n<!--\nFrom now on, let's find out the similarities among stores by using PCA!\n-->","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import LabelEncoder","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# First, we try store clustering by using weekly total sales \ntemp_df = df.groupby([\"store_id\", \"wm_yr_wk\"])[\"value\"].sum()\ntemp_df = temp_df.reset_index()\ntemp_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert it to wide format\ntemp_df_wide = temp_df.pivot(index=\"store_id\", columns=\"wm_yr_wk\", values=\"value\")\ntemp_df_wide","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# By using PCA, we could decrease 10 rows * 282 dimentions to 10 rows * 2 dimentions\npca = PCA(n_components=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca.fit(temp_df_wide)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" pca.explained_variance_ratio_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"이 값은 요소들이 총 데이터셋을 얼마나 많은 퍼센티지로 설명하는 지 보여줍니다.\n이 경우에, 주요 요소 1은 데이터의 85%를 설명하고 요소 2가 8%를 설명한다는 것입니다.\n그래서 데이터의 총 93%가 이 요소들로 설명될 수 있습니다.\n\n<!--\nThis value shows how much percentage do these components explain the total dataset.  \nIn this case, principal component 1 explains 85 % of the data and component 2 does 8 %.   \nSo the total 93 % of the data could be explained with these components\n-->","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"result = pca.transform(temp_df_wide)\nresult","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result_df = pd.DataFrame(result)\nresult_df.index = temp_df_wide.index\nresult_df.columns = [\"PC1\", \"PC2\"]\nresult_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = result_df.plot(kind='scatter', x='PC2', y='PC1', figsize=(12, 6))\n\nfor idx, store_id in enumerate(result_df.index):\n    ax.annotate(  \n        store_id,\n       (result_df.iloc[idx].PC2, result_df.iloc[idx].PC1)\n    )\n\nax.set_title(\"PCA result of all shops\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"이제 우리는 상품 카테고리에 같은 것들을 사용할 수 있습니다.\n그리고 상품을 clustering 해봅시다.\n\n<!--\nNow we can do the same thing in item category.  \nAnd let's try item clustering.\n-->","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_df = df.groupby([\"item_id\", \"wm_yr_wk\"])[\"value\"].sum()\ntemp_df = temp_df.reset_index()\ntemp_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_df = temp_df.fillna(method=\"bfill\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert it to wide format\ntemp_df_wide = temp_df.pivot(index=\"item_id\", columns=\"wm_yr_wk\", values=\"value\")\ntemp_df_wide","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# By using PCA, we could decrease 10 rows * 282 dimentions to 10 rows * 2 dimentions\npca = PCA(n_components=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca.fit(temp_df_wide)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca.explained_variance_ratio_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result = pca.transform(temp_df_wide)\nresult","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result_df = pd.DataFrame(result)\nresult_df.index = temp_df_wide.index\nresult_df.columns = [\"PC1\", \"PC2\"]\nresult_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result_df.index.str[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8, 6))\nsns.scatterplot(x=\"PC2\", y=\"PC1\", data=result_df, hue=result_df.index.str[:5])\nplt.title(\"PCA result for item_id column\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"위 plot에서, 우리는 FOODS 카테고리도 다른 카테고리 보다 더 많은 분산을 갖고 있다는 것을 알 수 있었습니다.\nFOODS 카테고리는 다른 것과 비교해서 PC1 에서 몇몇의 매우 높은 점들을 갖고 있습니다.\n\n<!--\nFrom the above plot, we could also find FOODS category has more variance than other categories.  \nFOODS category has some extra high point in PC1 compared to others.\n-->","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"(20200528 끝)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Summary\n   이 노트북에서, 몇몇의 쉬운 데이터 시각화를 통해 아래와 같은 데이터셋에 관한 몇몇 요소를 알 수 있었다.\n   1. 각 카테고리에서 모든 상품의 판매 변화\n      - 몇몇의 주기적인 영향 (주간과 월간)\n      - 크리스마스에 거의 판매가 없다.\n   2. 어떤 카테고리가 가장 많이 팔렸나?\n      - FOODS가 이 3개의 카테고리에서 가장 많이 상품이 팔렸다.\n      - HOUSEHOLD 카테고리가 두번째이고, HOBBIES가 가장 적게 팔린 것이다.\n   3. 어떤 날이 가장 많이 팔렸나?\n      - 토요일과 일요일이 가장 많은 상품이 판매된 날이다.\n      - 반대로, 화요일 같은 평일이 가장 적게 팔린 날이다.\n   4. 각 주마다 모든 상점에서 이동\n      - CA에서 CA_3 상점이 가장 많이 팔린 상점이다.\n      - 다른 주에서 그렇게 많은 차이가 나타나지 않았다.\n      - 이런 판매 이동은 종종 등록된 상품 항목과 연관이 있다.\n   5. Snap 구매가 가능한 날의 시각화\n      - 전체 년에 Snap 구매가 가능한 날의 총 수는 2011년 부터 거의 같다.\n      - 한달 동안 Snap 구매가 가능한 날의 총 수는 매달 10일이다.\n      - 각 일별로 Snap 구매가 가능한 날의 총 수는 동등하게 분포되어 있다.\n      - 그러나 Snap 구매가 가능한 flag를 따르면, 편향된 패턴이 있다.\n        (예, 규칙적으로 3일 후에 하루같지는 않으나, 한주의 모든 날이거나 다음 주에 없거나 처럼은 있다.)\n        \n   (20200528 6번 추가)\n   6. Dynamic 요소 분석 시험\n      - 우리는 하나의 상품 (FOODS_3_090) 에 있는 dynamic 요소 분석을 사용해서 관측되지 않은 요소를 알아냈습니다.\n      - 이 요소는 각 상정에서 판매 변화를 잘 설명해줍니다.\n        \n   그리고 마지막으로 히트맵을 사용해서 몇 개의 요소를 시각화한다.\n\n<!--\n   In this notebook, through some easy data visualization, we found some points regarding this dataset like below.\n   1. The transition of all items sold in each category \n      - Some periodical effect. (Weekly and monthly)\n      - On christmas day, there are almost no sales\n   2. Which category is the most sold one?  \n      - FOODS is the most sold item category of these three categories.\n      - HOUSEHOLD category is the 2nd one, and the HOBBIES are the least sold one.\n   3. Which day type is the most sold day? \n      - Saturday and Sunday is the most item sold day types.\n      - In contrast, on weekdays like Tuesday, there are less item sold.\n   4. The transition in all stores by each state\n      - In CA, CA_3 store is the most sold store.  \n      - In other states, not so much difference appeared.  \n      - These sales trasition often corresponds to the registered item entries.\n   5. Snap purchase allowed day visuaizaiton\n      - The total count of Snap purchase allowed day in whole year is almost the same from 2011.\n      - The total count of Snap purchase allowed day in one month is 10 in every month.\n      - The total count of Snap purchase allowed day in each day type is almost uniformly distributed.\n      - However, there are some biased patterns regarding snap purchase allowed flag.\n        (i.e. it is not like one day in three consective days regularly, but all days in one week and none in next week)\n\n   6. Dynamic Factor Analysis Trial\n      - We found the unobserveed factor by using dynamic factor analysis in one item (FOODS_3_090) \n      - This factor explains the sales transition in each state well.\n    \n   And finally we visualize some points by using heatmap.\n-->","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Future Work\nIf I have time, I'd like to tackle with the following things.\n\n1. Apply Dynamic Analysis and find out the relationship among state and stores.\n2. Make One item or one store prediction model for beginners like me to learn how to use lightgbm as a regressor.\n3. Check out the pre-processing effect. Is that effective considering the noise samples like Christmas or other irregularly days.\n4. More detailed analysis and find out some useful information for making prediction.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# References\nFollowing notebooks are the great notebooks in this competition. \nFor whom hasn't check these notebooks, I strongly recommend you to take a look at these notebooks.\n(I'm sorry if I missed some other great kernels, I'll take a lookt at other notebooks if I have enough time.)\n\nData Visualization:\n\n- **M5 Forecasting - Starter Data Exploration**  \n  https://www.kaggle.com/robikscube/m5-forecasting-starter-data-exploration  \n\n-  **Back to (predict) the future - Interactive M5 EDA**  \n   https://www.kaggle.com/headsortails/back-to-predict-the-future-interactive-m5-eda\n\nMaking Prediction:\n\n- **M5 - Three shades of Dark: Darker magic**  \n  https://www.kaggle.com/kyakovlev/m5-three-shades-of-dark-darker-magic\n\n-  **Very fst Model**  \n   https://www.kaggle.com/ragnar123/very-fst-model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Acknowledgment\n\nI apologize that my english are somewhat wrong and my codes are not so beautiful one like others' codes.  \nHowever, I tried hard to make simle codes as much as I can especially for beginners like me to learn how to use matplotlib and seaborn to do data visualization.  \nAny comments or upvotes can be my very strong motivation towards much harder work! Thank you!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}