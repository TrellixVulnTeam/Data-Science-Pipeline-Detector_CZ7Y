{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import math\nimport random\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\nfrom tqdm.notebook import tqdm \nfrom IPython.display import clear_output\n\nfrom xgboost import XGBRegressor\n\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Loading datasets\n\n# Contains date features, like week of year and holidays\ncalendar = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/calendar.csv', index_col='d')\n\n# Contains item sales by day, the target features \nsales = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sales_train_validation.csv')\n\n# Contais item sell prices by departments\nsell = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sell_prices.csv')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Objective\n\nThe detailed competition objective and rules are described in [M5 Forecasting - Accuracy](https://www.kaggle.com/c/m5-forecasting-accuracy) competition page.\n\nThe summary is that: given a retail product and stores historical sales by day dataset, the goal is forecast the sales by day over 30k products for next 27 days.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Notes\n- `d_` columns are the goal columns in past to use in model.\n- The predictions are about columns `d_1914 - d_1941` ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Parameters","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Days in past to retrive data for model training \nLAST_N_DAYS = 365 * 2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Sample from population\nThe dataset are huge*. There are over 30k of items and each of them have historical sales data over `LAST_N_DAYS` days. \n\nLets use a sample of data sets for predictions. \n\n\\* *In this study, its about 22 milion rows of data (30490 items x 730 days)* ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# CALCULATE THE SAMPLE SIZE\n# Original code from Marek Madejski in his github repository: https://github.com/veekaybee/data/blob/master/samplesize.py\n# Extracted from his article for sample size: http://veekaybee.github.io/2015/08/04/how-big-of-a-sample-size-do-you-need/ \ndef get_sample_size(population_size, confidence_level, confidence_interval):\n    # SUPPORTED CONFIDENCE LEVELS: 50%, 68%, 90%, 95%, and 99%\n    confidence_level_constant = [50,.67], [68,.99], [90,1.64], [95,1.96], [99,2.57]\n    \n    Z = 0.0\n    p = 0.5\n    e = confidence_interval/100.0\n    N = population_size\n    n_0 = 0.0\n    n = 0.0\n\n    # LOOP THROUGH SUPPORTED CONFIDENCE LEVELS AND FIND THE NUM STD\n    # DEVIATIONS FOR THAT CONFIDENCE LEVEL\n    for i in confidence_level_constant:\n        if i[0] == confidence_level:\n            Z = i[1]\n\n    if Z == 0.0:\n        return -1\n\n    # CALC SAMPLE SIZE\n    n_0 = ((Z**2) * p * (1-p)) / (e**2)\n\n    # ADJUST SAMPLE SIZE FOR FINITE POPULATION\n    n = n_0 / (1 + ((n_0 - 1) / float(N)) )\n\n    return int(math.ceil(n)) # THE SAMPLE SIZE","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Calculating the sample size needed to make predictions. At this point in competiton, to improve my leaderboard, I make aggressive parameters.\n\nIn real life (and avoid overfitting too) the good confidence level is about 95 and margin error acceptable is about 2.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculating the sample size\npopulation = len(sales)\nconfidence_level = 99.0\nmargin_error_acceptable = 1\n\nsample_size = get_sample_size(population, confidence_level, margin_error_acceptable)\n\nprint(f\"Sample size needed: {sample_size}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Check items by store\nTo define the sample data, we will divide it proportionally according to the quantity of items per store.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sales.store_id.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Well, we have the same quantitiy of items in each store. So, we must divide the sample equally for each store.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"stores_quantity = len(sales.store_id.unique())\n\nstore_sample_size = round(sample_size / stores_quantity)\n\nprint(f\"Each store sample size will have {store_sample_size} items\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Split the store samples","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_item_samples_by_store(store_id, sample_size):\n        \n    temp = sales.query(f\"store_id == '{store_id}'\").copy()\n    temp['index'] = temp.index\n    \n    # List that will containing the sample indexes\n    items_sample_indexes = []\n    \n    for sample in range(sample_size):\n        row = random.randint(0, len(temp)-1)\n\n        item_index = temp.iloc[row]['index']\n\n        items_sample_indexes.append(item_index)\n        \n        # Drops the sample drawn \n        temp.drop(item_index, axis=0, inplace=True)\n            \n    return items_sample_indexes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Drawing samples for each store\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"stores = sales.store_id.unique()\n\nsamples_by_store = {}\n\n# Progress bar params\ntotal = len(stores)\ndesc = 'Drawing samples by store'\n    \nfor store in tqdm(stores, total=total, desc=desc):\n    items_index_list = get_item_samples_by_store(store, store_sample_size)\n    samples_by_store.update( {store : items_index_list} )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preprocessing data\n\nCleaning dataframes to preprocessing","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preparing feature name cols\n\ninitial_day = 1914 - LAST_N_DAYS\n\ntrain_indexes = [\n    f\"d_{x}\"\n    for x in range(initial_day, 1914)\n]\n\ntarget_indexes = [\n    f\"d_{x}\"\n    for x in range(1914, 1970)\n]\n\nsales_train_features = list(sales.columns[1:6])\nsales_train_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dataframes to get features and targets data\n\n# Contains historical sales for each day and item\ndf_targets = sales[train_indexes].copy()\ndf_targets","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Contains store and item names\ndf_features = sales[sales_train_features].copy()\ndf_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calendar (empty) dataframe for prediction\nlast = len(target_indexes) - 1\ncalendar_predict = calendar.loc[target_indexes[0]:target_indexes[last]]\ncalendar_predict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Filtering calendar dataset for data from LAST_N_DAYS\nlast = len(train_indexes) - 1\ncalendar = calendar.loc[train_indexes[0]:train_indexes[last]]\ncalendar","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Historical sell data\nsell = sell.query(f\"wm_yr_wk >= {min(calendar.wm_yr_wk)}\")\nsell","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Relevant features in calendar\ncalendar_cols = [\n    'wm_yr_wk',\n    'wday',\n    'month',\n    'year',\n    'event_type_1',\n    'event_type_2'\n]\n\ncalendar = calendar[calendar_cols].copy()\ncalendar_predict = calendar_predict[calendar_cols].copy()\n\ncalendar","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Encoding string features ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nfeatures_encoder = LabelEncoder()\ncalendar_encoder = LabelEncoder()\n\n# Encoding sales features\ndf_features_encoded = df_features.apply(features_encoder.fit_transform)\n\n# Encoding calendar features\ncalendar_cols_encode = calendar.select_dtypes('object').columns\n\ncalendar[calendar_cols_encode] = calendar[calendar_cols_encode].fillna('-')\ncalendar[calendar_cols_encode] = calendar[calendar_cols_encode].apply(calendar_encoder.fit_transform)\n\ncalendar_predict[calendar_cols_encode] = calendar_predict[calendar_cols_encode].fillna('-')\ncalendar_predict[calendar_cols_encode] = calendar_predict[calendar_cols_encode].apply(calendar_encoder.fit_transform) \n\ncalendar","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Sells features\nsells_encoder = LabelEncoder()\n\nsells_encode_cols = ['store_id', 'item_id']\nsells_features = ['wm_yr_wk', 'sell_price']\n\nsell_encoded = sell[sells_encode_cols].apply(sells_encoder.fit_transform) \nsell_encoded[sells_features] = sell[sells_features]\n\nsell_encoded","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Transposing data from `sell` and `target` dataframes\nData transposing for preparing datasets to made sales predictions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Multidimension dataframes (for each store)\ntrain_data = []\n\n# Just print processing stores rows\ncurrent_store = 0\ntotal_stores = len(samples_by_store)\n\n    \nfor stores_samples in samples_by_store.values():\n    current_store += 1\n    for index in tqdm(stores_samples, total=len(stores_samples), desc=f'Creating mutidimensional dataframe {current_store}/{total_stores}'):\n        # Transposing targets for each item\n        # New datafame in multidimension dataframe\n        tmp_train_data = pd.DataFrame()\n        \n        # Calendar features\n        for col in calendar_cols:\n            tmp_train_data[col] = np.array(calendar[col])\n\n        # Unique id to match sell price in sell dataframe\n        item_id = df_features.loc[index, 'item_id']\n        store_id = df_features.loc[index, 'store_id']\n\n        # Sales features\n        for feature in sales_train_features:\n            tmp_train_data[feature] = df_features_encoded.loc[index, feature]\n\n        # Input sell price    \n        min_week = min(tmp_train_data.wm_yr_wk)\n\n        sell_cols = ['wm_yr_wk', 'item_id', 'store_id', 'sell_price']\n        sell_item = sell_encoded.query(f\"item_id == '{item_id}' & store_id == '{store_id}' & wm_yr_wk >= {min_week}\")[sell_cols]\n\n        tmp_train_data['sell_price'] = tmp_train_data.merge(sell_item, on='wm_yr_wk', how='left')['sell_price']\n\n        # No sell price indicates no sell made\n        tmp_train_data['sell_price'].fillna(0, inplace=True)\n\n        # Target transpose from row to column\n        tmp_train_data['target'] = np.array(df_targets.iloc[index].values)\n        \n        train_data.append(tmp_train_data)\n        \n    clear_output(wait=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Defining `X` and `y` data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X = pd.DataFrame()\ny = np.array([])\n\nfor dataset in train_data:\n    X = X.append(dataset.loc[:, dataset.columns != 'target'])\n    y = np.append(y, dataset['target'].to_numpy())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# year col is not necessary for predicting\n# causes a worst prediction\nX.drop('year', axis=1, inplace=True) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Splitting train and test data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model evaluation\n\nThe best model prediction was got using `XGBoost`. The evaluation was made by tries with different hyper params, and comparing to `RandomForest` model too.\n\nI tried to run a RandomGridSearch to find the best params, but this runs hours by all night along and in the end, causes a memory exception, so that's very difficult to use in this case. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Making cross validation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Params for evaluation\nparams_xgb = {\n    \"objective\": \"reg:squarederror\",\n    'max_depth': 50, \n    'min_child_weight': 1,    \n    'learning_rate': 0.05,\n    'alpha': 10,\n    'gamma': 10\n}\n\nfit_params = {\n    'early_stopping_rounds': 30,\n    'eval_metric': 'mae',\n    'eval_set': [[X_test, y_test]]\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_xg_boost_mae(hyper_params, fit_params, X, y):\n    model_xgb = XGBRegressor(\n        n_estimators=1000,\n        objective=hyper_params['objective'],\n        max_depth=hyper_params['max_depth'],\n        min_child_weight=hyper_params['min_child_weight'],\n        alpha=hyper_params['alpha'],\n        gamma=hyper_params['gamma'],\n        n_jobs=4\n    )\n    return cross_val_score(\n        model_xgb, \n        X, y, \n        cv=5, \n        scoring='neg_mean_absolute_error', \n        fit_params=fit_params\n    ).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mae = get_xg_boost_mae(params_xgb, fit_params, X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mae * -1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train\n\nXGBoost model with best hyper params","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def set_xgb_model():\n    return XGBRegressor(\n        n_estimators=1000,\n        objective=params_xgb['objective'],\n        max_depth=params_xgb['max_depth'],\n        min_child_weight=params_xgb['min_child_weight'],\n        alpha=params_xgb['alpha'],\n        gamma=params_xgb['gamma'],\n        early_stopping_rounds=fit_params['early_stopping_rounds'],\n        learning_rate=params_xgb['learning_rate'], \n        n_jobs=4  \n    )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preparing dataset for training model\ntrain_data = X.copy()\ntrain_data['target'] = y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training a model for each store\nmodels = []\n\nfor store in tqdm(X.store_id.unique(), total=len(X.store_id.unique()), desc='Fitting a model for each store...'):\n    models.append(set_xgb_model())\n    \n    # X_store: contains data only for the store model\n    X_store = train_data.loc[train_data.store_id == store].copy() \n    \n    # Important: drop target for X training\n    X_store.drop('target', axis=1, inplace=True)\n     \n    y_store = train_data.loc[train_data.store_id == store]['target']\n    \n    models[store].fit(X_store, y_store)   ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Predict\n\nFirst of all, lets prepare the predict dataframe","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating dataframe to with features to predict sales \ndf_predict = pd.DataFrame()\n\nsell_cols = ['wm_yr_wk', 'item_id', 'store_id', 'sell_price']\n\nfor index in tqdm(sales.index, total=len(sales), desc=f'Creating predict dataframe'):\n    # Transposing targets for each item\n    # New datafame in multidimension dataframe\n    tmp_df = pd.DataFrame()\n\n    # Calendar features\n    for col in calendar_cols:\n        tmp_df[col] = np.array(calendar_predict[col])\n\n    # Sales features\n    for feature in sales_train_features:\n        tmp_df[feature] = df_features_encoded.loc[index, feature]\n\n    # Input sell price\n    min_week = min(tmp_df.wm_yr_wk)\n    \n    sell_item = sell_encoded.query(f\"item_id == '{item_id}' & store_id == '{store_id}' & wm_yr_wk >= {min_week}\")[sell_cols]\n\n    tmp_df['sell_price'] = tmp_df.merge(sell_item, on='wm_yr_wk', how='left')['sell_price']\n\n    # No sell price indicates no sell made\n    tmp_df['sell_price'].fillna(0, inplace=True)\n \n    df_predict = df_predict.append(tmp_df)  \n\nclear_output(wait=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Analyzing MAE for different hyper params\n# predict with year column causes a worst prediction\n# So, let's drop it\ndf_predict.drop('year', axis=1, inplace=True) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating target column\ndf_predict['predictions'] = None","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Making predictions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making predictions for items by each store model\nfor store in tqdm(X.store_id.unique(), total=len(X.store_id.unique()), desc='Making predictions for each store...'):\n    \n    # Getting a dataset contaning a single store\n    to_predict = df_predict.loc[df_predict.store_id == store].copy()\n    \n    # Drop null predictions column for this dataset to be predicted \n    to_predict.drop('predictions', axis=1, inplace=True)\n\n    # Making predictions using the specific store model    \n    predictions = np.round(models[store].predict(to_predict))\n\n    # Assigning the predictions in result dataset    \n    df_predict.loc[df_predict.store_id == store, 'predictions'] = np.absolute(predictions)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Reversing encoded features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# item_id decoded\ndf_predict['item_id'] = sells_encoder.inverse_transform(df_predict['item_id'])\ndf_predict.head() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This inverse_transform for store_id is not working, so its made manually\nreverse_encode_dict = {\n    key : value\n    for key, value in zip(df_features_encoded.store_id.unique(), df_features.store_id.unique())\n} \n\ndf_predict.replace({\"store_id\": reverse_encode_dict}, inplace=True) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preparing item name for submission dataset\ndf_predict['item_validation'] = df_predict.apply(lambda x: x.item_id + '_' + x.store_id , axis=1)\ndf_predict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preparing submission dataset\ndf_predict_submission = df_predict[['item_validation', 'predictions']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for item in tqdm(df_predict.item_validation.unique(), total=len(df_predict.item_validation.unique()), desc='Preparing prediction dataset...'):\n    \n    item_predictions = df_predict.query(f'item_validation == \"{item}\"')['predictions']\n\n    submission.loc[submission.id == item+'_validation', 'F1':'F28'] = item_predictions[:28].ravel()\n    submission.loc[submission.id == item+'_evaluation', 'F1':'F28'] = item_predictions[28:].ravel()\n    \n    # Removing already proccessed predictions to optimize execution \n    df_predict = df_predict.iloc[28:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission.csv')  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}