{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"from  datetime import datetime, timedelta\nimport gc\nimport numpy as np, pandas as pd\nimport lightgbm as lgb","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> This notebook aims to push the public LB under 0.50. Certainly, the competition is not yet at its peak and there clearly remains room for improvement.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Credits\n\n* [First R notebook](https://www.kaggle.com/kailex/m5-forecaster-v2)\n* [Python translation](https://www.kaggle.com/kneroma/m5-forecast-v2-python)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"CAL_DTYPES={\"event_name_1\": \"category\", \"event_name_2\": \"category\", \"event_type_1\": \"category\", \n         \"event_type_2\": \"category\", \"weekday\": \"category\", 'wm_yr_wk': 'int16', \"wday\": \"int16\",\n        \"month\": \"int16\", \"year\": \"int16\", \"snap_CA\": \"float32\", 'snap_TX': 'float32', 'snap_WI': 'float32' }\nPRICE_DTYPES = {\"store_id\": \"category\", \"item_id\": \"category\", \"wm_yr_wk\": \"int16\",\"sell_price\":\"float32\" }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.options.display.max_columns = 50\nD='../input/m5-forecasting-accuracy/'","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"h = 28 \nmax_lags = 70\ntr_last = 1941\nfday = datetime(2016,5, 23)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"state = 'TX' \nstate_enc_dict={'CA': 0, 'TX' : 1, 'WI':2}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_dt(is_train = True, nrows = None, first_day = 1200, state=state):\n    prices = pd.read_csv(D+\"sell_prices.csv\", dtype = PRICE_DTYPES)\n    for col, col_dtype in PRICE_DTYPES.items():\n        if col_dtype == \"category\":\n            prices[col] = prices[col].cat.codes.astype(\"int16\")\n            prices[col] -= prices[col].min()\n            \n    cal = pd.read_csv(D+\"calendar.csv\", dtype = CAL_DTYPES)\n    cal[\"date\"] = pd.to_datetime(cal[\"date\"])\n    for col, col_dtype in CAL_DTYPES.items():\n        if col_dtype == \"category\":\n            cal[col] = cal[col].cat.codes.astype(\"int16\")\n            cal[col] -= cal[col].min()\n    \n    start_day = max(1 if is_train  else tr_last-max_lags, first_day)\n    numcols = [f\"d_{day}\" for day in range(start_day,tr_last+1)]\n    catcols = ['id', 'item_id', 'dept_id','store_id', 'cat_id', 'state_id']\n    dtype = {numcol:\"float32\" for numcol in numcols} \n    dtype.update({col: \"category\" for col in catcols if col != \"id\"})\n    dt = pd.read_csv(D+\"sales_train_evaluation.csv\", \n                     nrows = nrows, usecols = catcols + numcols, dtype = dtype)\n    for col in catcols:\n        if col != \"id\":\n            dt[col] = dt[col].cat.codes.astype(\"int16\")\n            dt[col] -= dt[col].min()\n    \n    if not is_train:\n        for day in range(tr_last+1, tr_last+ 28 +1):\n            dt[f\"d_{day}\"] = np.nan\n    \n    dt = pd.melt(dt,\n                  id_vars = catcols,\n                  value_vars = [col for col in dt.columns if col.startswith(\"d_\")],\n                  var_name = \"d\",\n                  value_name = \"sales\")\n    \n    dt = dt.merge(cal, on= \"d\", copy = False)\n    dt = dt.merge(prices, on = [\"store_id\", \"item_id\", \"wm_yr_wk\"], copy = False)\n    dt = dt.query(f\"state_id == {state_enc_dict[state]}\")\n    return dt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_fea(dt):\n    lags = [1, 2, 3, 5, 7]\n    lag_cols = [f\"lag_{lag}\" for lag in lags ]\n    for lag, lag_col in zip(lags, lag_cols):\n        dt[lag_col] = dt[[\"id\",\"sales\"]].groupby(\"id\")[\"sales\"].shift(lag)\n\n    wins = [7, 14]\n    for win in wins :\n        for lag,lag_col in zip(lags, lag_cols):\n            dt[f\"rmean_{lag}_{win}\"] = dt[[\"id\", lag_col]].groupby(\"id\")[lag_col].transform(lambda x : x.rolling(win).mean())\n\n    \n    \n    date_features = {        \n        \"wday\": \"weekday\",\n        \"week\": \"weekofyear\",\n        \"month\": \"month\",\n        \"quarter\": \"quarter\",\n        \"year\": \"year\",\n        \"mday\": \"day\",\n#         \"ime\": \"is_month_end\",\n#         \"ims\": \"is_month_start\",\n    }\n    \n#     dt.drop([\"d\", \"wm_yr_wk\", \"weekday\"], axis=1, inplace = True)\n    \n    for date_feat_name, date_feat_func in date_features.items():\n        if date_feat_name in dt.columns:\n            dt[date_feat_name] = dt[date_feat_name].astype(\"int16\")\n        else:\n            dt[date_feat_name] = getattr(dt[\"date\"].dt, date_feat_func).astype(\"int16\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"FIRST_DAY = 1180 # If you want to load all the data set it to '1' -->  Great  memory overflow  risk !","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\ndf = create_dt(is_train=True, first_day= FIRST_DAY)\ndf.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\ncreate_fea(df)\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.sample(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df.dropna(inplace = True)\n# df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_feats = ['item_id', 'dept_id','store_id', 'cat_id'] + [\"event_name_1\", \"event_type_1\",\n                                                           \"event_name_2\", \"event_type_2\",]\nsnap_useless = {\"CA\": [\"snap_TX\", \"snap_WI\"], \"TX\": [\"snap_CA\", \"snap_WI\"], \"WI\": [\"snap_TX\", \"snap_CA\"] }\nuseless_cols = [\"id\", \"date\", \"sales\",\"d\", \"wm_yr_wk\", \"weekday\", \"state_id\"] + snap_useless[state]\ntrain_cols = df.columns[~df.columns.isin(useless_cols)]\nX_train = df[train_cols]\ny_train = df[\"sales\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"valid_days = [\"d_\" + str(_) for _ in range(1914, 1942)]\nvalid_inds = np.array([_ in valid_days for _ in df['d'].values])\nvalid_data = lgb.Dataset(X_train.iloc[valid_inds], label = y_train.iloc[valid_inds],categorical_feature=cat_feats,\n                        free_raw_data=False)   # This is just a subsample of the training set, not a real validation set !\n# train_data = lgb.Dataset(X_train.iloc[~valid_inds], label = y_train.iloc[~valid_inds], categorical_feature=cat_feats, free_raw_data=False)\ntrain_data = lgb.Dataset(X_train, label = y_train, categorical_feature=cat_feats, free_raw_data=False)\nvalid_inds.sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n        \"objective\" : \"poisson\",\n        \"metric\" :\"rmse\",\n        \"force_row_wise\" : True,\n        \"learning_rate\" : 0.075,\n#         \"sub_feature\" : 0.8,\n        \"sub_row\" : 0.75,\n        \"bagging_freq\" : 1,\n        \"lambda_l2\" : 0.05,\n        \"lambda_l1\" : 0.05,\n#         \"nthread\" : 4\n        'early_stopping_rounds': 50, \n        #'device' : 'gpu',\n        'verbosity': 1,\n        'num_iterations' : 10000,\n        #'num_leaves': 128,\n        #\"min_data_in_leaf\": 50,             \n}\n\n# SEED = 42\n# params = {\n#         'boosting_type': 'gbdt',         # Standart boosting type\n#         'objective': 'regression',       # Standart loss for RMSE\n#         'metric': ['rmse'],              # as we will use rmse as metric \"proxy\"\n#         'subsample': 0.8,                \n#         'subsample_freq': 1,\n#         'learning_rate': 0.3,           # 0.5 is \"fast enough\" for us\n#         'num_leaves': 2**7-1,            # We will need model only for fast check\n#         'min_data_in_leaf': 2**6-1,      # So we want it to train faster even with drop in generalization \n#         'feature_fraction': 0.8,\n# #         'n_estimators': 2000,            # We don't want to limit training (you can change 5000 to any big enough number)\n#         'early_stopping_rounds': 30,     # We will stop training almost immediately (if it stops improving) \n#         'seed': SEED,\n#         \"lambda_l2\" : 0.1,\n#         'num_iterations' : 3000,\n#         'verbose': -1,\n#  }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nm_lgb = lgb.train(params, train_data, valid_sets = [train_data, valid_data], verbose_eval=100) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m_lgb.save_model(f\"model_{state}.lgb\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\n# alphas = [1.035, 1.03, 1.025, 1.02]\nalphas = [1.0]\nweights = [1/len(alphas)]*len(alphas)\nsub = 0.\n\nfor icount, (alpha, weight) in enumerate(zip(alphas, weights)):\n\n    te = create_dt(False)\n    cols = [f\"F{i}\" for i in range(1,29)]\n\n    for tdelta in range(0, 28):\n        day = fday + timedelta(days=tdelta)\n        print(icount, day.strftime(\"%Y-%m-%d\"))\n        tst = te[(te.date >= day - timedelta(days=max_lags)) & (te.date <= day)].copy()\n        create_fea(tst)\n        tst = tst.loc[tst.date == day , train_cols]\n        te.loc[te.date == day, \"sales\"] = alpha*m_lgb.predict(tst) # magic multiplier by kyakovlev\n\n\n\n    te_sub = te.loc[te.date >= fday, [\"id\", \"sales\"]].copy()\n#     te_sub.loc[te.date >= fday+ timedelta(days=h), \"id\"] = te_sub.loc[te.date >= fday+timedelta(days=h), \n#                                                                           \"id\"].str.replace(\"validation$\", \"evaluation\")\n    te_sub[\"F\"] = [f\"F{rank}\" for rank in te_sub.groupby(\"id\")[\"id\"].cumcount()+1]\n    te_sub = te_sub.set_index([\"id\", \"F\" ]).unstack()[\"sales\"][cols].reset_index()\n    te_sub.fillna(0., inplace = True)\n    te_sub.sort_values(\"id\", inplace = True)\n    te_sub.reset_index(drop=True, inplace = True)\n    te_sub.to_csv(f\"submission_{icount}.csv\",index=False)\n    if icount == 0 :\n        sub = te_sub\n        sub[cols] *= weight\n    else:\n        sub[cols] += te_sub[cols]*weight\n    print(icount, alpha, weight)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub2 = sub.copy()\nsub[\"id\"] = sub[\"id\"].str.replace(\"evaluation$\", \"validation\")\nsub = pd.concat([sub, sub2], axis=0, sort=False)\nsub.to_csv(f\"submission_{state}.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.id.nunique(), sub[\"id\"].str.contains(\"evaluation$\").sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.shape","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}