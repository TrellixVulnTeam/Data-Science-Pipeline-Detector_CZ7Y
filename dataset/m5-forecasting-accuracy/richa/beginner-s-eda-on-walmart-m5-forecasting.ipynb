{"cells":[{"metadata":{},"cell_type":"markdown","source":"This notebook is a beginners attempt on exploring data for walmart shop. And some of the code is being inspired from notebooks of the fellow participants.  "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pylab as plt\nimport seaborn as sns\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"stv = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sales_train_validation.csv')\ns_sub = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sample_submission.csv')\ns_price = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sell_prices.csv')\ncal = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/calendar.csv')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stv","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Categories {}\".format(stv['cat_id'].unique()))\nprint(\"States {}\".format(stv['state_id'].unique()))\nprint(\"Stores {}\".format(stv['store_id'].unique()))\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are three categories \"Food\", \"Household\", \"Foods\". Three States Texas, California, Wisconsin. CA has 4 stores, TX has 3 stores and WI has 3 stores"},{"metadata":{},"cell_type":"markdown","source":"We have item ids, categories and historic sales from day1 to day 1913. There are 30490 items in the shop.And there are categories of hobbies , foods and household. There are 3 states CA, WI and TX and 11 stores distributed in these states."},{"metadata":{"trusted":true},"cell_type":"code","source":"cal.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the calendar data , we hadates , weekdays month year . Also, days as d1, d2, d3 given as in sales_trained data."},{"metadata":{"trusted":true},"cell_type":"code","source":"stv.mean(axis=1).sort_values()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"21104,18055,8412 ids with maximum sales overall. In terms of total sale and average sale both.So, we will try to analyze them first.\nFOODS_3_586_TX_3_validation,\nFOODS_3_586_TX_2_validation,\nFOODS_3_090_CA_3_validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# to get only days columns\nd_cols = [col for col in stv.columns if 'd_' in col]\n# and keeping ids aside we will only look into sales\nstv.loc[stv['id'] == 'FOODS_3_090_CA_3_validation'].set_index('id')[d_cols].T.plot(figsize=(15, 5),title='FOODS_3_090_CA_3 sales by \"d\" number')\nplt.legend('')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"examples = ['FOODS_3_090_CA_3','FOODS_3_586_TX_3','FOODS_3_586_TX_2']\nex1= stv.loc[stv['id'] == 'FOODS_3_090_CA_3_validation'][d_cols].T\nex1 = ex1.rename(columns={8412:'FOODS_3_090_CA_3'})\nex2= stv.loc[stv['id'] == 'FOODS_3_586_TX_3_validation'][d_cols].T\nex2 = ex2.rename(columns={21104:'FOODS_3_586_TX_3'})\nex3= stv.loc[stv['id'] == 'FOODS_3_586_TX_2_validation'][d_cols].T\nex3 = ex3.rename(columns={18055:'FOODS_3_586_TX_2'})\nexamples_df = [ex1,ex2,ex3]\nex1 = ex1.reset_index().rename(columns={'index':'d'})\nex1 = ex1.merge(cal,how='left',validate='1:1')\nfor i in [0,1,2]:\n    examples_df[i] = examples_df[i].reset_index().rename(columns={'index':'d'})\n    examples_df[i] = examples_df[i].merge(cal,how='left',validate='1:1')   \n    fig,(ax1,ax2,ax3) = plt.subplots(1,3,figsize=(16,3))\n    examples_df[i].groupby('wday').mean()[examples[i]].plot(kind='line',title='average sale: day of week',ax=ax1)\n    examples_df[i].groupby('month').mean()[examples[i]].plot(kind='line',title='average monthly sale',ax=ax2)\n    examples_df[i].groupby('year').mean()[examples[i]].plot(kind='line',title='average yearly sale',ax=ax3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Analysing random 20 samples"},{"metadata":{"trusted":true},"cell_type":"code","source":"samples = stv.sample(20, random_state=200).set_index('id')[d_cols].T\\\n            .merge(cal.set_index('d')['date'],left_index=True,right_index=True,validate='1:1').set_index('date')\nfig,axs = plt.subplots(10, 2, figsize=(16, 24))\naxs = axs.flatten()\nfor i in range(len(samples.columns)):\n    samples[samples.columns[i]].plot(title=samples.columns[i],ax=axs[i])\nplt.tight_layout()\nplt.show()\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"There are many days when the sale is zero."},{"metadata":{},"cell_type":"markdown","source":"Sale based on each category"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(data=stv,x='cat_id')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Sale of Foods is the most and sale of hobbies is the least. This make sense because we all need Food for survival but hobbies are leisure activity which people do in free time."},{"metadata":{"trusted":true},"cell_type":"code","source":"cats = ['FOODS','HOBBIES','HOUSEHOLD']\nid_cols = ['id','item_id','dept_id','store_id','state_id']\ndaily_sale = stv.groupby('cat_id').sum().T.reset_index().rename(columns={'index':'d'}).merge(cal,how='left',validate='1:1')\ndaily_sale = daily_sale.set_index('date')\nfig, axs =  plt.subplots(3, figsize=(16, 24))\naxs = axs.flatten()\nfor i in range(len(cats)):\n    daily_sale[cats[i]].plot(title= cats[i], ax = axs[i])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The sale of every items of every category drops to zero 25th december as the shops are closed on that day."},{"metadata":{"trusted":true},"cell_type":"code","source":"from matplotlib.pyplot import figure\ndef plot_Graph3Series(series, title,labels):\n    figure(num=None, figsize=(8, 6), dpi=80, facecolor='w', edgecolor='k')\n    plt.plot(series[0],'-b', label=labels[0])\n    plt.plot(series[1],'-r', label=labels[1])\n    plt.plot(series[2],'-g', label=labels[2])\n    plt.title(title)\n    plt.legend(framealpha=1, frameon=True)\n    plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#monthly_sale = daily_sale['FOODS'].groupby('month')\ndaily_sale = stv.groupby('cat_id').sum().T.reset_index().rename(columns={'index':'d'}).rename(columns={'index':'d'})#\nfig, axs =  plt.subplots(3, figsize=(6, 14))\naxs = axs.flatten()\nfor i in range(len(cats)):   \n    sale = daily_sale[['d',cats[i]]].merge(cal,how='left',validate='1:1').set_index('date')\n    sale.groupby('month')[cats[i]].mean().plot(title = cats[i],ax=axs[i])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking into average monthly sale for each category. There is a drop of sale for Food and household in the month of May and december. The reason for this drop could be that most of the people go on vacation during this month. The sale of hobbies declines in August and Sept and reaches the lowest in Sept. Reason could be that is the start of new sessions in schools and colleges and people don't have time for hobbies. (This is just a guess)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#daily_sale['date'] = pd.to_datetime(daily_sale['date'])\n#everydaydf.groupby([everydaydf[date_column].dt.to_period(\"M\")]).sum()\ndaily_sale = daily_sale.merge(cal,how='left',validate='1:1').set_index('date')\ndaily_sale['date'] = pd.to_datetime(daily_sale.index)\n#daily_sale = daily_sale.groupby([daily_sale['date'].dt.to_period(\"M\")])['FOODS'].sum()\n#.index = monthlyWithoutOutliersdf.index.to_timestamp()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"series = []\nfor i in range(len(cats)):\n    x = daily_sale.groupby([daily_sale['date'].dt.to_period(\"M\")])[cats[i]].sum()\n    x.index = x.index.to_timestamp()\n    series.append(x)\nplot_Graph3Series(series,'Monthly Sale in categories',cats)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Sale of all the categories over the 5 years as a time series curve. While the sale of hobbies has not changed much in these 5 years. There is an upward trend in the sale of foods and household. there are more fluctuations in food sale, lets try to get the answer as we explore more."},{"metadata":{"trusted":true},"cell_type":"code","source":"past_sales = stv.set_index('id')[d_cols] \\\n    .T \\\n    .merge(cal.set_index('d')['date'],\n           left_index=True,\n           right_index=True,\n            validate='1:1') \\\n    .set_index('date')\nfor i in stv['cat_id'].unique():\n    items_col = [c for c in past_sales.columns if i in c]\n    past_sales[items_col] \\\n        .sum(axis=1) \\\n        .plot(figsize=(15, 5),\n              alpha=0.8,\n              title='Total Sales by Item Type')\nplt.legend(stv['cat_id'].unique())\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Daily sale of three categories over the five years. An upward trend can be seen in daily food sale. \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"state_list = stv['state_id'].unique()\nfor s in state_list:\n    store_items = [c for c in past_sales.columns if s in c]\n    past_sales[store_items] \\\n        .sum(axis=1) \\\n        .rolling(30).mean() \\\n        .plot(figsize=(15, 5),\n              alpha=0.8,\n              title='Rolling 90 Day Average Total Sales (10 stores)')\nplt.legend(state_list)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Sale in CA is the most. And interestingly the curve of sale in WI and TX has similar nature of peaks and troughs. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# ----------------------------------------------------------------------------\n# Author:  Nicolas P. Rougier\n# License: BSD\n# ----------------------------------------------------------------------------\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Polygon\nfrom datetime import datetime\nfrom dateutil.relativedelta import relativedelta\n\ndef calmap(ax, year, data):\n    ax.tick_params('x', length=0, labelsize=\"medium\", which='major')\n    ax.tick_params('y', length=0, labelsize=\"x-small\", which='major')\n\n    # Month borders\n    xticks, labels = [], []\n    start = datetime(year,1,1).weekday()\n    for month in range(1,13):\n        first = datetime(year, month, 1)\n        last = first + relativedelta(months=1, days=-1)\n\n        y0 = first.weekday()\n        y1 = last.weekday()\n        x0 = (int(first.strftime(\"%j\"))+start-1)//7\n        x1 = (int(last.strftime(\"%j\"))+start-1)//7\n\n        P = [ (x0,   y0), (x0,    7),  (x1,   7),\n              (x1,   y1+1), (x1+1,  y1+1), (x1+1, 0),\n              (x0+1,  0), (x0+1,  y0) ]\n        xticks.append(x0 +(x1-x0+1)/2)\n        labels.append(first.strftime(\"%b\"))\n        poly = Polygon(P, edgecolor=\"black\", facecolor=\"None\",\n                       linewidth=1, zorder=20, clip_on=False)\n        ax.add_artist(poly)\n    \n    ax.set_xticks(xticks)\n    ax.set_xticklabels(labels)\n    ax.set_yticks(0.5 + np.arange(7))\n    ax.set_yticklabels([\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"])\n    ax.set_title(\"{}\".format(year), weight=\"semibold\")\n    \n    # Clearing first and last day from the data\n    valid = datetime(year, 1, 1).weekday()\n    data[:valid,0] = np.nan\n    valid = datetime(year, 12, 31).weekday()\n    # data[:,x1+1:] = np.nan\n    data[valid+1:,x1] = np.nan\n\n    # Showing data\n    ax.imshow(data, extent=[0,53,0,7], zorder=10, vmin=-1, vmax=1,\n              cmap=\"RdYlBu_r\", origin=\"lower\", alpha=.75)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nsscale = StandardScaler()\npast_sales.index = pd.to_datetime(past_sales.index)\nfor i in stv['cat_id'].unique():\n    fig, axes = plt.subplots(3, 1, figsize=(20, 8))\n    items_col = [c for c in past_sales.columns if i in c]\n    sales2013 = past_sales.loc[past_sales.index.isin(pd.date_range('31-Dec-2012',\n                                                                   periods=371))][items_col].mean(axis=1)\n    vals = np.hstack(sscale.fit_transform(sales2013.values.reshape(-1, 1)))\n    calmap(axes[0], 2013, vals.reshape(53,7).T)\n    sales2014 = past_sales.loc[past_sales.index.isin(pd.date_range('30-Dec-2013',\n                                                                   periods=371))][items_col].mean(axis=1)\n    vals = np.hstack(sscale.fit_transform(sales2014.values.reshape(-1, 1)))\n    calmap(axes[1], 2014, vals.reshape(53,7).T)\n    sales2015 = past_sales.loc[past_sales.index.isin(pd.date_range('29-Dec-2014',\n                                                                   periods=371))][items_col].mean(axis=1)\n    vals = np.hstack(sscale.fit_transform(sales2015.values.reshape(-1, 1)))\n    calmap(axes[2], 2015, vals.reshape(53,7).T)\n    plt.suptitle(i, fontsize=30, x=0.4, y=1.01)\n    plt.tight_layout()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the calendar heatmap, for all the categories sale is mostly concentrated in weekends or during holidays."},{"metadata":{},"cell_type":"markdown","source":"Sales per state with seasonality"},{"metadata":{"trusted":true},"cell_type":"code","source":"#fig, axes = plt.subplots(3, 1, figsize=(20, 8))\n#items_col = [c for c in past_sales.columns if i in c]\nitems_col = []\nfig, axes = plt.subplots(3, 1, figsize=(12, 16))\nindex=0\nfor i in stv['state_id'].unique():\n    items_col = [c for c in past_sales.columns if i in c]\n    state_sales = past_sales[items_col].sum(axis=1)\n    state_sales.plot(ax=axes[index],title=i)\n    index = index+1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cal","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"state_cols=[]\ncat_cols = []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"past_sales['date'] = pd.to_datetime(past_sales.index)\n#past_sales.index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"past_sales = stv.set_index('id')[d_cols].T\nfig,axs = plt.subplots(1,3,figsize=(16,3))\naxs = axs.flatten()\ni=0\nfor s in stv['state_id'].unique():\n    series = pd.DataFrame()\n    state_cols= [c for c in past_sales.columns if s in c]\n    for cat in stv['cat_id'].unique():\n        cat_cols= [c for c in state_cols if cat in c]\n        x = pd.DataFrame(past_sales[cat_cols].sum(axis=1))\n        x['d'] = past_sales[cat_cols].sum(axis=1).index \n        x= x.merge(cal.set_index('d'),how='left',left_index=True,right_index=True,validate='1:1')\n        x = x.groupby('wday').mean()[0].rename(cat)\n        series = pd.concat([series,x],axis=1) \n    series.plot(ax=axs[i],title='Average Sale on day of week for '+ s)\n    i=i+1\n       ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Average sale on a single day of the week in three states. This also shows that there are high sale on weekends and by the time we reach midweek .. sale drops and again there is increase in sale as move towards Friday or weekend"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,axs = plt.subplots(1,3,figsize=(16,3))\naxs = axs.flatten()\ni=0\nfor s in stv['state_id'].unique():\n    series = pd.DataFrame()\n    state_cols= [c for c in past_sales.columns if s in c]\n    for cat in stv['cat_id'].unique():\n        cat_cols= [c for c in state_cols if cat in c]\n        x = pd.DataFrame(past_sales[cat_cols].sum(axis=1))\n        x['d'] = past_sales[cat_cols].sum(axis=1).index \n        x= x.merge(cal.set_index('d'),how='left',left_index=True,right_index=True,validate='1:1')\n        x = x.groupby('month').mean()[0].rename(cat)\n        series = pd.concat([series,x],axis=1) \n    series.plot(ax=axs[i],title='Average monthly Sale for '+ s)\n    i=i+1\n       ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Average monthly sale in three states. Again, there is hardly any variation in hobbies sale throughtout the year. We can observe a in the sale of food and households in the month of May."},{"metadata":{},"cell_type":"markdown","source":"**Now, lets look into calendar data and try to find some pattern throught the year**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#snap_count = cal.groupby(['weekday','snap_TX'])['snap_TX'].count()\n#snap_count['weekday'] = snap_count.index\n#snap_count = snap_count.rename({0:'off',1:'on'})\nfig,axs = plt.subplots(1,3,figsize=(16,3))\nsns.countplot(data=cal,x='weekday',hue='snap_CA', ax=axs[0])\nsns.countplot(data=cal,x='weekday',hue='snap_TX', ax=axs[1])\nsns.countplot(data=cal,x='weekday',hue='snap_WI', ax=axs[2])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Sale with snap benefits in all the three states looks same for all the days of the week."},{"metadata":{"trusted":true},"cell_type":"code","source":"#import datetime\n#date1 = datetime.datetime.strptime('2012-31-12',\"%Y-%d-%m\") \ndate_range = pd.date_range('31-Dec-2012',periods=371)\ncal['date'] = pd.to_datetime(cal['date']) \nsales2013 = cal.loc[(cal['date'] >= min(date_range)) & (cal['date'] <= max(date_range))]['snap_TX']\nvals = np.hstack(sscale.fit_transform(sales2013.values.reshape(-1, 1)))\nfig, axes = plt.subplots(figsize=(20, 8))\ncalmap(axes, 2013, vals.reshape(53,7).T)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Snap benefits mostly occurs on first two weeks of the month. The graph above only shows for the year 2013.. Similarly we can look into snap benefits patterns for all the three years"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfig,axs = plt.subplots(1,3,figsize=(16,3))\naxs = axs.flatten()\ni=0\n#for s in stv['state_id'].unique():\n#    series = pd.DataFrame()\n#    state_cols= [c for c in past_sales.columns if s in c]\nfor cat in stv['cat_id'].unique():\n    cat_cols= [c for c in past_sales.columns if cat in c]\n    x = pd.DataFrame(past_sales[cat_cols].sum(axis=1))\n    x['d'] = past_sales[cat_cols].sum(axis=1).index \n    x= x.merge(cal.set_index('d'),how='left',left_index=True,right_index=True,validate='1:1')\n    sns.boxplot(y=x[0], x=x['event_type_1'], ax=axs[i]).set_title(cat)\n    i=i+1\n       ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is increase in sale of food during sporting and religious events.However, there is slight increase in sale of hobbies during cultural events. And these increase or these pattern are self explanatory."},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfig,axs = plt.subplots(3,3,figsize=(16,16))\n#axs = axs.flatten()\ni=0\nj=0\nfor s in stv['state_id'].unique():\n    series = pd.DataFrame()\n    state_cols= [c for c in past_sales.columns if s in c]\n    j=0\n    for cat in stv['cat_id'].unique():\n        cat_cols= [c for c in state_cols if cat in c]\n        x = pd.DataFrame(past_sales[cat_cols].sum(axis=1))\n        x['d'] = past_sales[cat_cols].sum(axis=1).index \n        x= x.merge(cal.set_index('d'),how='left',left_index=True,right_index=True,validate='1:1')\n        col = 'snap_'+s\n        sns.boxplot(y=x[0], x=x[col], ax=axs[i][j]).set_title(cat + \" \"+ s)\n        j=j+1\n    i=i+1\n       ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since the snap benefits are mainly for poor people. We can clearly see rise of sale of food items during snap benefits for each countries. And there is no difference between sale of hobbies during snap benefits."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(y=x[0], x=x['event_type_1'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(1,3,figsize=(16,6))\nsns.countplot(data=cal, x=cal['snap_CA'], ax=axes[0]) \nsns.countplot(data=cal, x=cal['snap_WI'], ax=axes[1]) \nsns.countplot(data=cal, x=cal['snap_TX'], ax=axes[2]) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(data=cal['event_name_1'], x=cal['event_type_1']) #.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Most of the events are religious and national."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}