{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Credits to","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"[1] https://www.kaggle.com/mayer79/m5-forecast-keras-with-categorical-embeddings-v2 \n\n[2] https://www.kaggle.com/ragnar123/very-fst-model\n\n[3] https://www.kaggle.com/mayer79/m5-forecast-poisson-loss\n\n[4] https://www.kaggle.com/sayanotsu/walmart-weekly-sales-aggregation ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\nimport pandas as pd\nimport numpy as np\nimport gc\nimport os\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.model_selection import train_test_split\nimport time\nimport pickle\nimport lightgbm as lgb\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn import metrics\nfrom datetime import datetime, timedelta","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LAGS = [7, 28]\nWINDOWS = [7, 28]\nFIRST = 1914 + 28\nLENGTH = 28\nFOLDS = 3\ndrop_d = 900 # best to ditch the first two years aka 730, 900 for crossval\n\nnp.random.seed(0)\n\nfeatures = ['month', 'week', 'dept_id', 'snap_CA', 'wday', 'item_id', 'cat_id',\n       'snap_TX', 'sell_price', 'lag_t28', 'rolling_mean_lag28_w7',\n       'event_type_2', 'event_name_1', 'event_type_1',\n       'lag_t7', 'snap_WI', 'sell_price_rel_diff', 'event_name_2', 'year', 'rolling_mean_lag7_w7',\n       'rolling_mean_lag7_w28', 'rolling_mean_lag28_w28', 'state_id','store_id', 'clusters']\n\ncat_feats = ['item_id', 'dept_id','store_id', 'cat_id', 'state_id'] + [\"event_name_1\", \"event_name_2\", \"event_type_1\", \"event_type_2\"]\n\ndrop = ['id', 'd', 'demand', \"date\", \"sales\", \"weekday\"]\n\nencoder_file = open('encoders', 'wb')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nGot from: [1]\n'''\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float32)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nGot from: [1]\n'''\npath = \"../input/m5-forecasting-accuracy\"\n\ncalendar = pd.read_csv(os.path.join(path, \"calendar.csv\"))\nselling_prices = pd.read_csv(os.path.join(path, \"sell_prices.csv\"))\nsample_submission = reduce_mem_usage(pd.read_csv(os.path.join(path, \"sample_submission.csv\")))\nsales = pd.read_csv(os.path.join(path, \"sales_train_evaluation.csv\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nGot from: [1]\n- The columns \"Date\" and \"Weekday\" are dropped as they contain redundant information.\n- Normally, the column \"d\" is like \"d_1,d_2,...\". Make it \"1,2,..\" and the type integer\n- If there is no event (I think), there is NA. We will replace them with \"NoEvent\" string. Originally, it was replaced with\n  \"missing\", but I don't think it makes sense as I don't think there is missing information, I think they just left \n  the days without any event as NA.\n- We enumerate most of the columns:\n    - We do not enumerate \"d\" and \"wm_yr_weak\" because we will use these columns for joins.\n    - Why do we enumerate month and day? I think it is because they start from 1, not 0.\n    - Originally, the binary columns \"snap_X\" were also enumerated. I don't think it is necessary. The only neccessary step\n      was to convert their type from int64 to int as it uses less space; but reduce_mem_usage will take care of that.\n- I would suggest saving the OrdinalEncoder in case we need to reverse the transformations\n'''\ndef prep_calendar(df,encoder_file):\n    \n    # from[4], add week as a column to dataframe \n    df['date'] = pd.to_datetime(df['date'])\n    wm_yr_wk_values = np.sort(df['wm_yr_wk'].unique())\n    wm_yr_wk_values_new = np.arange(len(wm_yr_wk_values))\n    wm_yr_wk_values_replace = dict(zip(wm_yr_wk_values, wm_yr_wk_values_new))\n    df['week'] = df['wm_yr_wk'].replace(wm_yr_wk_values_replace).astype('int16')\n    del wm_yr_wk_values_replace, wm_yr_wk_values_new, wm_yr_wk_values\n\n    df = df.drop([\"weekday\"], axis=1)  # date was dropped here\n    df = df.assign(d = df.d.str[2:].astype(int))\n    df = df.fillna(\"NoEvent\")\n    cols = list(set(df.columns) - {\"wm_yr_wk\", \"d\",\"snap_CA\",\"snap_TX\",\"snap_WI\"}) \n    oe = OrdinalEncoder(dtype=\"int\")\n    df[cols] = oe.fit_transform(df[cols])\n    pickle.dump(oe,encoder_file)\n    df = reduce_mem_usage(df)\n    return df\n\n'''\nGot from: [1]\nOriginally, there were features added in this part. I excluded them until we decide whether to use those or not.\n'''\ndef prep_selling_prices(df):\n    df = reduce_mem_usage(df)\n    return df\n\n'''\nGot from: [1]\n- We drop the first \"drop_d\" days. Originally, this is set to 1000. When it is set to this value,\n  the shape we get 29,544,810 rows. When we don't set it, we get 60,034,810 rows. I think for now \n  we can keep this functionality, as it may be useful if we would like to discard some of the days.\n- In some id's, we have \"_validation\". Those are deleted.\n- reindex: Conform DataFrame to new index with optional filling logic (obtained from pandas doc). \n  We add days 1914+2*28 to prepare data from submission\n- We have to melt the sales dataframe since days are contained as columns.\n- assign: Returns a new object with all original columns in addition to new ones. Existing columns \n  that are re-assigned will be overwritten (obtained from pandas doc). Again, we make the values \n  \"d_1, d-2,...\" to \"1,2,...\"\n'''\n#We have to melt sales for sure because the days are columns, which is not desirable.\ndef reshape_sales(df, drop_d = None):\n    if drop_d is not None:\n        df = df.drop([\"d_\" + str(i + 1) for i in range(drop_d)], axis=1)\n    df = df.assign(id=df.id.str.replace(\"_validation\", \"\"))\n    df = df.assign(id=df.id.str.replace(\"_evaluation\", \"\"))\n    df = df.reindex(columns=df.columns.tolist() + [\"d_\" + str(1913 + i + 1) for i in range(2 * 28)])\n    df = df.melt(id_vars=[\"id\", \"item_id\", \"dept_id\", \"cat_id\", \"store_id\", 'state_id','clusters'],\n                 var_name='d', value_name='demand')\n    df = df.assign(d=df.d.str[2:].astype(\"int16\"), demand=df.demand.astype(\"float32\"))\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import KMeans\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn import mixture\n\nmeans = np.zeros((30490,3))\nweights= np.zeros((30490,3))\ncovs= np.zeros((30490,3))\n\nsales2 = sales.drop([\"item_id\",\"dept_id\",\"cat_id\",\"store_id\",\"state_id\",\"id\"],axis=1)\n\nfor i in range(30490):\n    gmm = mixture.GaussianMixture(n_components=3,covariance_type=\"full\")\n    gmm.fit(sales2.iloc[i].values.reshape(-1, 1))\n    ctr = 0\n    for gauss in np.argsort(gmm.weights_)[::-1]:\n        means[i][ctr] = gmm.means_[gauss]\n        weights[i][ctr] = gmm.weights_[gauss]\n        covs[i][ctr] = gmm.covariances_[gauss]\n        ctr += 1\n        \nsales2[\"mean_0\"]= means[:,0]\nsales2[\"mean_1\"]= means[:,1]\nsales2[\"mean_2\"]= means[:,2]\nsales2[\"cov_0\"] = covs[:,0]\nsales2[\"cov_1\"] = covs[:,1]\nsales2[\"cov_2\"] = covs[:,2]\nsales2[\"weights_0\"] =weights[:,0]\nsales2[\"weights_1\"] =weights[:,1]\nsales2[\"weights_2\"] =weights[:,2]\n\nsales2 = sales2[[\"mean_0\",\"mean_1\" ,\"mean_2\" ,\"cov_0\" ,\"cov_1\" ,\"cov_2\",\"weights_0\",\"weights_1\",\"weights_2\"]]\n\nscaler = MinMaxScaler()\nsales2 = scaler.fit_transform(sales2)\ngc.collect()\n\ninertias = []\nfor i in range(2,20):\n    kmeans = KMeans(n_clusters=i, random_state=0).fit(sales2)\n    print(kmeans.inertia_)\n    inertias.append(kmeans.inertia_)\n\nkmeans = KMeans(n_clusters=6, random_state=0).fit_predict(sales2)\n\nsales[\"clusters\"] = kmeans\n\n\ndel sales2,kmeans, inertias,scaler,means,weights,covs\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"calendar = prep_calendar(calendar,encoder_file)\nselling_prices = prep_selling_prices(selling_prices)\nsales = reshape_sales(sales, drop_d)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nGot from: [1]\n- Merge all the dataframes and delete the unnecessary ones\n- time.sleep() added to make sure garbage collector finishes its job before the next merge\n'''\nsales = sales.merge(calendar, how=\"left\", on=\"d\")\ndel calendar\ngc.collect()\ntime.sleep(5)\nsales = sales.merge(selling_prices, how=\"left\", on=[\"wm_yr_wk\", \"store_id\", \"item_id\"])\ndel selling_prices\nsales.drop([\"wm_yr_wk\"], axis=1, inplace=True)\ngc.collect()\ntime.sleep(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nGot from: [1]\n- We will also save the encoders in the pickle file.\n- The loop is slightly changed\n'''\n\ncat_id_cols = [\"item_id\", \"dept_id\", \"store_id\", \"cat_id\", \"state_id\"]\n\n# In loop to minimize memory use\nfor col in cat_id_cols:\n    oe = OrdinalEncoder(dtype=\"int\")\n    sales[col] = oe.fit_transform(sales[[col]])\n    pickle.dump(oe,encoder_file)    \nsales = reduce_mem_usage(sales)\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nGot from: [1]\n- These features were originally added in prep_selling_prices function to the \n  selling_prices dataframe, which does not exist anymore. But we can use the same\n  code to add these to sales dataframe as well as the columns are the same.\n- New feature=\"sell_price_rel_diff\"\n  pct_change(): Computes the percentage change from the immediately previous row by default. (Obtained from pandas doc.)\n  The two lines below adds the percentage of change of each item that is sold in the stores. Of course, for the\n  first datapoint, there is no previous, so this code produces an NA. Since there are 3049x10=30490 different (item,store)\n  pairs, this new column has 30490 NAs.\n- New feature=\"sell_price_roll_sd7\"\n  Rolling standard deviation: Moving Standard Deviation is a statistical measurement of market volatility (Google). We check the\n  past 7 days.\n- New feature=\"sell_price_cumrel\"\n  I think this is cumulative related frequency. I am not sure and I did not understand it clearly. I think\n  it is some kind of normalization, because we subtract the minimum and divide by max-min+1.\n- It runs without problems, but the RAM gets almost filled up so it gives you a heart attack.\n- No need to call reduce_mem_usage() after as I tried and it did not save any additional space.\n'''\n\ngr = sales.groupby([\"store_id\", \"item_id\"])[\"sell_price\"]\nsales[\"sell_price_rel_diff\"] = gr.pct_change()\n#sales[\"sell_price_cumrel\"] = (gr.shift(0) - gr.cummin()) / (1 + gr.cummax() - gr.cummin())\n\ndel gr\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def demand_features(df):\n    \"\"\" Derive features from sales data and remove rows with missing values \"\"\"\n    \n    for lag in LAGS:\n        df[f'lag_t{lag}'] = df.groupby('id')['demand'].transform(lambda x: x.shift(lag)).astype(\"float32\")\n        for w in WINDOWS:\n            df[f'rolling_mean_lag{lag}_w{w}'] = df.groupby('id')[f'lag_t{lag}'].transform(lambda x: x.rolling(w).mean()).astype(\"float32\")\n            \n            \n        date_features = {\n        \n        \"wday\": \"weekday\",\n        \"week\": \"weekofyear\",\n        \"month\": \"month\",\n        \"quarter\": \"quarter\",\n        \"year\": \"year\",\n        \"mday\": \"day\",\n    }\n    \n    df[\"date\"] = pd.to_datetime(df[\"date\"])\n    for date_feat_name, date_feat_func in date_features.items():\n        if date_feat_name in df.columns:\n            df[date_feat_name] = df[date_feat_name].astype(\"int16\")\n        else:\n            df[date_feat_name] = getattr(df[\"date\"].dt, date_feat_func).astype(\"int16\")\n        \n    return df\n\ndef demand_features_eval(df):\n    \"\"\" Same as demand_features but for the step-by-step evaluation \"\"\"\n    out = df.groupby('id', sort=False).last()\n    for lag in LAGS:\n        out[f'lag_t{lag}'] = df.groupby('id', sort=False)['demand'].nth(-lag-1).astype(\"float32\")\n        for w in WINDOWS:\n            out[f'rolling_mean_lag{lag}_w{w}'] = df.groupby('id', sort=False)['demand'].nth(list(range(-lag-w, -lag))).groupby('id', sort=False).mean().astype(\"float32\")\n    \n    return out.reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales = demand_features(sales)\nsales = sales[sales.d > (drop_d + max(LAGS) + max(WINDOWS))]\nsales = reduce_mem_usage(sales)\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = list(set(sales.columns) - {'id', 'd', 'demand', \"date\", \"sales\", \"weekday\"}) # dropped date, sales and weekday as they don't contribute much\ntest = sales[sales.d >= FIRST - max(LAGS) - max(WINDOWS)] # add windows needed to provide test features\nsales = sales[sales.d < FIRST]\nlabels = sales['demand']#.values.astype(np.float32)\nsales = sales[x]\nencoder_file.close()\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# taken from [3]\ndef fit_model(train, valid):\n    \"\"\" Fit LightGBM model \"\"\"\n     \n    params = {\n                'boosting_type': 'gbdt',\n                'objective': 'tweedie',\n                'tweedie_variance_power': 1.1,\n                'metric': 'rmse',\n                'subsample': 0.5,\n                'subsample_freq': 1,\n                'learning_rate': 0.031,\n                'num_leaves': 2**11-1,\n                'min_data_in_leaf': 2**12-1,\n                'feature_fraction': 0.5,\n                'max_bin': 100,\n                'n_estimators': 800, #800\n                'boost_from_average': False,\n                'verbose': -1,\n                'seed': 0\n                } \n\n    fit = lgb.train(params, \n                    train, \n                    num_boost_round = 2000, # should be 2000\n                    valid_sets = [valid], \n                    early_stopping_rounds = 200,\n                    verbose_eval = 100)\n    \n    lgb.plot_importance(fit, importance_type=\"gain\", precision=0, height=0.5, figsize=(6, 10));\n    \n    return fit","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def temp_pred_to_csv(fit, test, x, fold, cols=sample_submission.columns, file=\"submission.csv\"):\n    \"\"\" Calculate predictions and append to submission csv \"\"\"\n    \n    # Recursive prediction\n    for i, day in enumerate(np.arange(FIRST, FIRST + LENGTH)):\n        test_day = demand_features_eval(test[(test.d <= day) & (test.d >= day - max(LAGS) - max(WINDOWS))])\n        test.loc[test.d == day, \"demand\"] = fit.predict(test_day[x]) * 1.03 \n    \n    test = test[test.d>=1914]\n    # Prepare for reshaping\n    test = test.assign(id=test.id + \"_\" + np.where(test.d < FIRST - 28 + LENGTH, \"validation\", \"evaluation\"),\n                       F=\"F\" + (test.d - FIRST + 28 + 1 - LENGTH * (test.d >= FIRST - 28 + LENGTH)).astype(\"str\"))\n    \n    #print(test.head())\n    \n    test = test[['id', 'demand', 'F']]#drop([\"item_id\",\"dept_id\",\"cat_id\",\"store_id\",\"state_id\",\"d\"],axis=1,inplace=True)\n    #test = test.reset_index(drop=True)\n    test.drop_duplicates(subset=None, keep='first', inplace=True)\n    test = test.reset_index(drop=True)\n    # Reshape\n    submission = test.pivot(index=\"id\", columns=\"F\", values=\"demand\").reset_index()[cols].fillna(1)\n    #sample_submission.iloc[30490:,:] = submission\n\n    # Export\n    submission.to_csv(file+f'{fold}', index=False)\n    \n    return True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kf = GroupKFold(FOLDS)\ngroup = sales['week'].astype(str) + '_' + sales['year'].astype(str)\nprint('get folds and groups')    \n\n\n#fake_valid_inds = np.random.choice(sales.index.values, 1_000_000, replace = False)\n#train_inds = np.setdiff1d(sales.index.values, fake_valid_inds)\ncat_feats = [sales.columns.get_loc(c) for c in cat_feats if c in sales]\nsales_x = sales.values.astype(np.float32)\n#sales_x_val = sales.loc[fake_valid_inds].values.astype(np.float32)\ndel sales\nsales_y = labels.values.astype(np.float32)\n#sales_y_val = labels.loc[fake_valid_inds].values.astype(np.float32)\ndel labels\ngc.collect()\n\nprint('preprared data...')\n\n\nfor fold, (trn_idx, val_idx) in enumerate(kf.split(sales_x, sales_y, group)):\n        \n    print(f'Training fold {fold + 1}')\n    train_set = lgb.Dataset(sales_x[trn_idx], sales_y[trn_idx], categorical_feature=cat_feats, \n                         free_raw_data=False)\n    val_set = lgb.Dataset(sales_x[val_idx], sales_y[val_idx], categorical_feature=cat_feats, \n                         free_raw_data=False)         \n    fit = fit_model(train_set, val_set)\n    del train_set, val_set\n    gc.collect()\n    print(f'Finished training {fold + 1}')\n\n\n    # predict test\n    #fit.booster_.save_model(f'mode{fold}.txt')\n    temp_pred_to_csv(fit, test, x, fold)\n    \npreds_0=pd.read_csv('submission'+'.csv0')\npreds_1=pd.read_csv('submission'+'.csv1')\npreds_2=pd.read_csv('submission'+'.csv2')\n\npreds = pd.DataFrame()\npreds['id'] = preds_0['id']\nfor item in preds_0:\n    if item!='id':\n        preds[item]=(preds_0[item]*(1/FOLDS))+(preds_1[item]*(1/FOLDS))+(preds_2[item]*(1/FOLDS))\npreds.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submiss=pd.read_csv('submission'+'.csv')\ncols = ['F'+str(i)+'_other' for i in range(1, 29)]\nindexes = [i for i in range(60980) if i % 2 == 1]\nsub = submiss.iloc[indexes]\n\nsubmiss.drop(indexes, inplace=True)\nsub = sub.assign(F1=0)\nsub = sub.assign(F2=0)\nsub = sub.assign(F3=0)\nsub = sub.assign(F4=0)\nsub = sub.assign(F5=0)\nsub = sub.assign(F6=0)\nsub = sub.assign(F7=0)\nsub = sub.assign(F8=0)\nsub = sub.assign(F9=0)\nsub = sub.assign(F10=0)\nsub = sub.assign(F11=0)\nsub = sub.assign(F12=0)\nsub = sub.assign(F13=0)\nsub = sub.assign(F14=0)\nsub = sub.assign(F15=0)\nsub = sub.assign(F16=0)\nsub = sub.assign(F17=0)\nsub = sub.assign(F18=0)\nsub = sub.assign(F19=0)\nsub = sub.assign(F20=0)\nsub = sub.assign(F21=0)\nsub = sub.assign(F22=0)\nsub = sub.assign(F23=0)\nsub = sub.assign(F24=0)\nsub = sub.assign(F25=0)\nsub = sub.assign(F26=0)\nsub = sub.assign(F27=0)\nsub = sub.assign(F28=0)\n    \nsub = pd.concat([sub, submiss])\nsub = sample_submission.join(sub, lsuffix='_other')\nsub = sub.drop(cols + ['id'],axis=1)\nsub['id'] = sub['id_other']\nsub = sub.drop('id_other', axis=1)\nsub = sub[sample_submission.columns]\nsub.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}