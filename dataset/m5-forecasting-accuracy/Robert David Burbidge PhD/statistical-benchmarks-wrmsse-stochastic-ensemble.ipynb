{"cells":[{"metadata":{},"cell_type":"markdown","source":"Implementation of the statistical benchamark forecasting techniques from the [M5 Participants Guide](https://mofc.unic.ac.cy/m5-competition/).\n\nThis kernel is based on [for_Japanese_beginner(with WRMSSE in LGBM))](https://www.kaggle.com/girmdshinsei/for-japanese-beginner-with-wrmsse-in-lgbm). Thanks [@Tsuru](https://www.kaggle.com/girmdshinsei).  \n\nFinal submission is a stochastic ensemble of the statistical benchmarks. Contains some parallelization."},{"metadata":{},"cell_type":"markdown","source":"## module import"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\nimport pandas as pd\nimport numpy as np\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_rows', 500)\nimport matplotlib.pyplot as plt\nimport seaborn as sns; sns.set()\nimport lightgbm as lgb\nfrom sklearn import preprocessing, metrics\nfrom sklearn.preprocessing import LabelEncoder\nimport gc\nimport os\nfrom tqdm import tqdm_notebook as tqdm\nfrom scipy.sparse import csr_matrix\nfrom joblib import Parallel, delayed\nimport random\nfrom statsmodels.tsa.api import SimpleExpSmoothing\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## functionの定義"},{"metadata":{},"cell_type":"markdown","source":"reduce_mem_usageは、データのメモリを減らすためにデータ型を変更する関数です。  https://qiita.com/hiroyuki_kageyama/items/02865616811022f79754　を参照ください。"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns: #columns毎に処理\n        col_type = df[col].dtypes\n        if col_type in numerics: #numericsのデータ型の範囲内のときに処理を実行. データの最大最小値を元にデータ型を効率的なものに変更\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"read_dataはデータの読み込みと, reduce_mem_usageの適用を行う関数"},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_data():\n    print('Reading files...')\n    calendar = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/calendar.csv')\n    calendar = reduce_mem_usage(calendar)\n    print('Calendar has {} rows and {} columns'.format(calendar.shape[0], calendar.shape[1]))\n    \n    sell_prices = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sell_prices.csv')\n    sell_prices = reduce_mem_usage(sell_prices)\n    print('Sell prices has {} rows and {} columns'.format(sell_prices.shape[0], sell_prices.shape[1]))\n    \n    sales_train_val = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sales_train_validation.csv')\n    print('Sales train validation has {} rows and {} columns'.format(sales_train_val.shape[0], sales_train_val.shape[1]))\n    \n    submission = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sample_submission.csv')\n    \n    return calendar, sell_prices, sales_train_val, submission","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"PandasのdataFrameをきれいに表示する関数"},{"metadata":{"trusted":true},"cell_type":"code","source":"import IPython\n\ndef display(*dfs, head=True):\n    for df in dfs:\n        IPython.display.display(df.head() if head else df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"calendar, sell_prices, sales_train_val, submission = read_data()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 予測期間とitem数の定義\nNUM_ITEMS = sales_train_val.shape[0]  # 30490\nDAYS_PRED = submission.shape[1] - 1  # 28","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## data加工 "},{"metadata":{},"cell_type":"markdown","source":"### 1.最初にカテゴリ変数の処理"},{"metadata":{},"cell_type":"markdown","source":"As [@kaushal2896](https://www.kaggle.com/kaushal2896) suggested in [this comment](https://www.kaggle.com/harupy/m5-baseline#770558), encode the categorical columns before merging to prevent the notebook from crashing even with the full dataset. [@harupy](https://www.kaggle.com/harupy) also use this encoding suggested in [m5-baseline](https://www.kaggle.com/harupy/m5-baseline).  \nメモリの効率利用のため, カテゴリ変数をあらかじめLabel encoding."},{"metadata":{"trusted":true},"cell_type":"code","source":"def explanatory_variables(df):\n    \n    df['ex1'] = df['snap_CA'] + df['snap_TX'] + df['snap_WI']\n    df['ex2'] = 1 * (pd.notnull(df['event_name_1']) | pd.notnull(df['event_name_2']))\n\n    return df\n\n\ncalendar = explanatory_variables(calendar).pipe(reduce_mem_usage)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"calendar.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.sales_train_validationのmelt処理  \n（時系列の特徴量が作りやすいように, id毎に横に並んだ時系列データを、（id , 時系列）で縦に変換）"},{"metadata":{},"cell_type":"markdown","source":"pandasのmeltを使いdemand(売上数量)を縦に並べる.  \n* pandasのmeltは https://qiita.com/ishida330/items/922caa7acb73c1540e28　を参照ください。\n* dataの行数が莫大になるので, Kaggle Notebookのmemory制限を考慮し、nrowsで直近365*2日分（2年分）のデータに限定（TODO:環境に応じて期間を変更）"},{"metadata":{"trusted":true},"cell_type":"code","source":"nrows = 365 * 2 * NUM_ITEMS","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#加工前  \ndisplay(sales_train_val.head(5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train_val = pd.melt(sales_train_val,\n                                     id_vars = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], \n                                     var_name = 'day', value_name = 'demand')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#加工後  \ndisplay(sales_train_val.head(5))\nprint('Melted sales train validation has {} rows and {} columns'.format(sales_train_val.shape[0],\n                                                                            sales_train_val.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train_val = sales_train_val.iloc[-nrows:,:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.1と同様に予測部分(validation/evaluation部分)のmelt処理し, 学習データと結合する. 出力はdataという変数."},{"metadata":{},"cell_type":"markdown","source":"予測部分のsubmission fileを同じくmelt処理し、sales_train_valとつなげる。  \n処理の注意点:  \n* submission fileの列名を\"d_xx\"形式に変更する. submission fileで縦に結合されたvalidationとevaluationを一度分割し、それぞれことなる28日間の列名\"d_xx\"をそれぞれ付与。\n* submission fileには, idの詳細（item, department, state等）が無いためidをキーに, sales validationから取得したproductを結合\n* test2は、6/1まで不要なため削除"},{"metadata":{"trusted":true},"cell_type":"code","source":"# seperate test dataframes\n\n# submission fileのidのvalidation部分と, ealuation部分の名前を取得\ntest1_rows = [row for row in submission['id'] if 'validation' in row]\ntest2_rows = [row for row in submission['id'] if 'evaluation' in row]\n\n# submission fileのvalidation部分をtest1, ealuation部分をtest2として取得\ntest1 = submission[submission['id'].isin(test1_rows)]\ntest2 = submission[submission['id'].isin(test2_rows)]\n\n# test1, test2の列名の\"F_X\"の箇所をd_XXX\"の形式に変更\ntest1.columns = [\"id\"] + [f\"d_{d}\" for d in range(1914, 1914 + DAYS_PRED)]\ntest2.columns = [\"id\"] + [f\"d_{d}\" for d in range(1942, 1942 + DAYS_PRED)]\n\n# test2のidの'_evaluation'を置換\n#test1['id'] = test1['id'].str.replace('_validation','')\ntest2['id'] = test2['id'].str.replace('_evaluation','_validation')\n\n# sales_train_valからidの詳細部分(itemやdepartmentなどのid)を重複なく一意に取得。\nproduct = sales_train_val[['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']].drop_duplicates()\n\n# idをキーにして, idの詳細部分をtest1, test2に結合する.\ntest1 = test1.merge(product, how = 'left', on = 'id')\ntest2 = test2.merge(product, how = 'left', on = 'id')\n\n# test1, test2をともにmelt処理する.（売上数量:demandは0）\ntest1 = pd.melt(test1, id_vars = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], \n                var_name = 'day', value_name = 'demand')\n\ntest2 = pd.melt(test2, id_vars = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], \n                var_name = 'day', value_name = 'demand')\n\n# validation部分と, evaluation部分がわかるようにpartという列を作り、 test1,test2のラベルを付ける。\nsales_train_val['part'] = 'train'\ntest1['part'] = 'test1'\ntest2['part'] = 'test2'\n\n# sales_train_valとtest1, test2の縦結合.\ndata = pd.concat([sales_train_val, test1, test2], axis = 0)\n\n# memoryの開放\ndel test1, test2, sales_train_val\n\n# delete test2 for now(6/1以前は, validation部分のみ提出のため.)\ndata = data[data['part'] != 'test2']\n\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### 4.dataにcalendar/sell_pricesを結合"},{"metadata":{"trusted":true},"cell_type":"code","source":"#calendarの結合\n# drop some calendar features(不要な変数の削除:weekdayやwdayなどはdatetime変数から後ほど作成できる。)\ncalendar.drop(['weekday', 'wday', 'month', 'year'], \n              inplace = True, axis = 1)\n\n# notebook crash with the entire dataset (maybee use tensorflow, dask, pyspark xD)(dayとdをキーにdataに結合)\ndata = pd.merge(data, calendar, how = 'left', left_on = ['day'], right_on = ['d'])\ndata.drop(['d', 'day'], inplace = True, axis = 1)\n\n# memoryの開放\ndel calendar\ngc.collect()\n\n#sell priceの結合\n# get the sell price data (this feature should be very important)\ndata = data.merge(sell_prices, on = ['store_id', 'item_id', 'wm_yr_wk'], how = 'left')\nprint('Our final dataset to train has {} rows and {} columns'.format(data.shape[0], data.shape[1]))\n\n# memoryの開放\ndel sell_prices\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### 5 dataから特徴量生成\n* groupby & transofrmの変換方法はこちらを参照:https://qiita.com/greenteabiscuit/items/132e0f9b1479926e07e0\n* shift/rollingなどの役割はこちらを参照:https://note.nkmk.me/python-pandas-rolling/ (ここでmeltがうまく効きます。)\n　ラグ変数や過去の平均値などの特徴量が生成できる。\n* 変数は, すべてlagを28以上にして, F1~F28の予測を1つのモデルで表現するのが目的。\n* TODO：特徴量の生成方法は色々変更可能. ShiftやRollingの値の変更などなど"},{"metadata":{"trusted":true},"cell_type":"code","source":"display(data.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## train/testの分割とmodelの推定"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data[['date', 'demand', 'id', 'ex1', 'ex2', 'sell_price']]\n\n# going to evaluate with the last 28 days\nx_train = data[data['date'] <= '2016-03-27']\ny_train = x_train['demand']\nx_val = data[(data['date'] > '2016-03-27') & (data['date'] <= '2016-04-24')]\ny_val = x_val['demand']\ntest = data[(data['date'] > '2016-04-24')]\n\n#dataの削除（メモリの削除）\n#del data\n#gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## WRMSSE calculation"},{"metadata":{},"cell_type":"markdown","source":"LightGBMのMetricとして, WRMSSEの効率的な計算を行う。あくまで, 28day-lagで1つのモデルの予測するときにLGBMで効率的なWRMSSEの計算を行う場合である。\n* weight_matという0 or 1の疎行列で、効率的にaggregation levelを行列積で計算出来るようにしている"},{"metadata":{"trusted":true},"cell_type":"code","source":"weight_mat = np.c_[np.identity(NUM_ITEMS).astype(np.int8), #item :level 12\n                   np.ones([NUM_ITEMS,1]).astype(np.int8), # level 1\n                   pd.get_dummies(product.state_id.astype(str),drop_first=False).astype('int8').values,\n                   pd.get_dummies(product.store_id.astype(str),drop_first=False).astype('int8').values,\n                   pd.get_dummies(product.cat_id.astype(str),drop_first=False).astype('int8').values,\n                   pd.get_dummies(product.dept_id.astype(str),drop_first=False).astype('int8').values,\n                   pd.get_dummies(product.state_id.astype(str) + product.cat_id.astype(str),drop_first=False).astype('int8').values,\n                   pd.get_dummies(product.state_id.astype(str) + product.dept_id.astype(str),drop_first=False).astype('int8').values,\n                   pd.get_dummies(product.store_id.astype(str) + product.cat_id.astype(str),drop_first=False).astype('int8').values,\n                   pd.get_dummies(product.store_id.astype(str) + product.dept_id.astype(str),drop_first=False).astype('int8').values,\n                   pd.get_dummies(product.item_id.astype(str),drop_first=False).astype('int8').values,\n                   pd.get_dummies(product.state_id.astype(str) + product.item_id.astype(str),drop_first=False).astype('int8').values\n                   ].T\n\nweight_mat_csr = csr_matrix(weight_mat)\ndel weight_mat; gc.collect()\n\ndef weight_calc(data,product):\n\n    # calculate the denominator of RMSSE, and calculate the weight base on sales amount\n    \n    sales_train_val = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sales_train_validation.csv')\n    \n    d_name = ['d_' + str(i+1) for i in range(1913)]\n    \n    sales_train_val = weight_mat_csr * sales_train_val[d_name].values\n    \n    # calculate the start position(first non-zero demand observed date) for each item / 商品の最初の売上日\n    # 1-1914のdayの数列のうち, 売上が存在しない日を一旦0にし、0を9999に置換。そのうえでminimum numberを計算\n    df_tmp = ((sales_train_val>0) * np.tile(np.arange(1,1914),(weight_mat_csr.shape[0],1)))\n    \n    start_no = np.min(np.where(df_tmp==0,9999,df_tmp),axis=1)-1\n    \n    \n    # denominator of RMSSE / RMSSEの分母\n    weight1 = np.sum((np.diff(sales_train_val,axis=1)**2),axis=1)/(1913-start_no)\n    \n    # calculate the sales amount for each item/level\n    df_tmp = data[(data['date'] > '2016-03-27') & (data['date'] <= '2016-04-24')]\n    df_tmp['amount'] = df_tmp['demand'] * df_tmp['sell_price']\n    df_tmp =df_tmp.groupby(['id'])['amount'].apply(np.sum).values\n    \n    weight2 = weight_mat_csr * df_tmp \n\n    weight2 = weight2/np.sum(weight2)\n    \n    del sales_train_val\n    gc.collect()\n    \n    return weight1, weight2\n\n\nweight1, weight2 = weight_calc(data,product)\n\ndef wrmsse(preds, data):\n    \n    preds = preds.astype('int32')\n    \n    # actual obserbed values / 正解ラベル\n    y_true = data.get_label()\n    y_true = y_true.astype('int32')\n    \n    # number of columns\n    num_col = len(y_true)//NUM_ITEMS\n    \n    # reshape data to original array((NUM_ITEMS*num_col,1)->(NUM_ITEMS, num_col) ) / 推論の結果が 1 次元の配列になっているので直す\n    reshaped_preds = np.array(preds).reshape(num_col, NUM_ITEMS).T\n    reshaped_true = np.array(y_true).reshape(num_col, NUM_ITEMS).T\n    \n    x_name = ['pred_' + str(i) for i in range(num_col)]\n    x_name2 = [\"act_\" + str(i) for i in range(num_col)]\n          \n    train = np.array(weight_mat_csr*np.c_[reshaped_preds, reshaped_true])\n    \n    score = np.sum(\n                np.sqrt(\n                    np.mean(\n                        np.square(\n                            train[:,:num_col] - train[:,num_col:])\n                        ,axis=1) / weight1) * weight2)\n    \n    return 'wrmsse', score, False\n\ndef wrmsse_simple(preds, data):\n    \n    # actual obserbed values / 正解ラベル\n    y_true = data.get_label()\n    \n    # number of columns\n    num_col = len(y_true)//NUM_ITEMS\n    \n    # reshape data to original array((NUM_ITEMS*num_col,1)->(NUM_ITEMS, num_col) ) / 推論の結果が 1 次元の配列になっているので直す\n    reshaped_preds = np.array(preds).reshape(num_col, NUM_ITEMS).T\n    reshaped_true = np.array(y_true).reshape(num_col, NUM_ITEMS).T\n    \n    train = np.c_[reshaped_preds, reshaped_true]\n    \n    weight2_2 = weight2[:NUM_ITEMS]\n    weight2_2 = weight2_2/np.sum(weight2_2)\n    \n    score = np.sum(\n                np.sqrt(\n                    np.mean(\n                        np.square(\n                            train[:,:num_col] - train[:,num_col:])\n                        ,axis=1) /  weight1[:NUM_ITEMS])*weight2_2)\n    \n    return 'wrmsse', score, False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# useful dataset for speeding up the following calculations\nuids = pd.concat([pd.Series(np.arange(30490)), pd.Series(x_train['id'].unique())], axis=1)\nuids.columns = ['idx', 'id']\ndemand = x_train.merge(uids, on='id').sort_values(by=['idx', 'date'], axis=0)['demand']\nnum_vals = int(len(demand)/30490)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# helper function\ndef intervals(ts):\n    y = np.zeros(len(ts))\n    k = 0\n    counter = 0\n    for tmp in range(len(ts)):\n        if(ts[tmp]==0):\n            counter = counter + 1\n        else:\n            k = k + 1\n            y[k] = counter\n            counter = 1\n    y = np.array(y)\n    y[np.isnan(y)] = 1\n    y = y[y > 0]\n    return y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Simple Exponential Smoothing\ndef SES(ts, alpha=0.1, h=56):\n    if ts is None or len(ts)==0:\n        return np.zeros(h)\n    \n    y = np.zeros(len(ts)+1)\n    y[0] = ts[0]\n        \n    for t in range(len(ts)):\n        y[t+1] = alpha*ts[t]+(1-alpha)*y[t]\n  \n    return np.concatenate([y[0:len(ts)], np.repeat(y[-1:],h)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 1. Naive (LB: 1.46378)\ny_pred_Naive = np.tile(x_train[x_train['date'] == '2016-03-27']['demand'], 28)\ny_test_Naive = np.tile(x_train[x_train['date'] == '2016-03-27']['demand'], 28)\nwrmsse_Naive = wrmsse(y_pred_Naive, lgb.Dataset(x_val, y_val))[1]\nprint(f'WRMSSE for Naive method: {wrmsse_Naive}')\ndel y_pred_Naive","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 2. Seasonal Naive (LB: 0.86967)\ny_pred_sNaive = np.tile(x_train[(x_train['date'] > '2016-03-20') & (x_train['date'] <= '2016-03-27')]['demand'], 4)\ny_test_sNaive = np.tile(x_train[(x_train['date'] > '2016-03-20') & (x_train['date'] <= '2016-03-27')]['demand'], 4)\nwrmsse_sNaive = wrmsse(y_pred_sNaive, lgb.Dataset(x_val, y_val))[1]\nprint(f'WRMSSE for Seasonal Naive method: {wrmsse_sNaive}')\ndel y_pred_sNaive","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 3. Simple Exponential Smoothing (LB: 1.07202)\ndef mySES(ts):\n    if ts is None or len(ts)==0:\n        ts = np.asarray([0, 0])\n    \n    start_period = np.argmin(ts!=0)\n    ts = ts[start_period:]\n    MSE = {}\n    for alpha in np.arange(0.1,0.3,0.1):\n        MSE[str(alpha)] = np.mean(np.square(SES(ts, alpha)[0:len(ts)] - ts))\n    opt_alpha = float(list(MSE.keys())[np.argmin(MSE.values())])\n    return SES(ts, opt_alpha)[-56:]\n    \n\ny_pred_SES = Parallel(n_jobs=-1)(delayed(mySES)(\n        demand[i*num_vals:(i+1)*num_vals].values\n    ) for i in tqdm(range(30490)))\ny_pred_SES = np.concatenate(y_pred_SES)\ny_pred_SES = y_pred_SES.reshape([56, 30490], order='F')\ny_test_SES = y_pred_SES[28:56,:].reshape([28*30490,1])\ny_pred_SES = y_pred_SES[0:28,:].reshape([28*30490,1])\ny_test_SES = np.concatenate(y_test_SES)\ny_pred_SES = np.concatenate(y_pred_SES)\nwrmsse_SES = wrmsse(y_pred_SES, lgb.Dataset(x_val, y_val))[1]\nprint(f\"WRMSSE for SES method: {wrmsse_SES}\")\ndel y_pred_SES","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 4. Moving Averages (LB: 1.09815)\ndef MA(ts, h=56):\n    mse = np.ones(14)*np.inf\n    for k in np.arange(2,16):\n        y = np.repeat(np.nan, len(ts))\n        for i in np.arange(k+1,len(ts)+1):\n            y[i-1] = np.mean(ts[(i-k-1):i])\n        mse[k-2] = np.mean(np.square(y[~np.isnan(y)]-ts[~np.isnan(y)]))\n        k = np.argmin(mse)+2\n    forecast = np.repeat(np.mean(ts[-k:]), h)\n    return forecast\n\n\ny_pred_MA = Parallel(n_jobs=-1)(delayed(MA)(\n        demand[i*num_vals:(i+1)*num_vals].values\n    ) for i in tqdm(range(30490)))\ny_pred_MA = np.concatenate(y_pred_MA)\ny_pred_MA = y_pred_MA.reshape([56, 30490], order='F')\ny_test_MA = y_pred_MA[28:56,:].reshape([28*30490,1])\ny_pred_MA = y_pred_MA[0:28,:].reshape([28*30490,1])\ny_test_MA = np.concatenate(y_test_MA)\ny_pred_MA = np.concatenate(y_pred_MA)\nwrmsse_MA = wrmsse(y_pred_MA, lgb.Dataset(x_val, y_val))[1]\nprint(f\"WRMSSE for MA method: {wrmsse_MA}\")\ndel y_pred_MA","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 5. Croston's (LB: 1.05648)\ndef Croston(ts, h=56, alpha=0.1, debias=1.0):\n    yd = np.mean(SES(ts[ts!=0])[-56:])\n    yi = np.mean(SES(intervals(ts))[-56:])\n    return np.repeat(yd/yi, h)*debias\n\n\ny_pred_Croston = Parallel(n_jobs=-1)(delayed(Croston)(\n        demand[i*num_vals:(i+1)*num_vals].values\n    ) for i in tqdm(range(30490)))\ny_pred_Croston = np.concatenate(y_pred_Croston)\ny_pred_Croston = y_pred_Croston.reshape([56, 30490], order='F')\ny_test_Croston = y_pred_Croston[28:56,:].reshape([28*30490,1])\ny_pred_Croston = y_pred_Croston[0:28,:].reshape([28*30490,1])\ny_test_Croston = np.concatenate(y_test_Croston)\ny_pred_Croston = np.concatenate(y_pred_Croston)\nwrmsse_Croston = wrmsse(y_pred_Croston, lgb.Dataset(x_val, y_val))[1]\nprint(f\"WRMSSE for Croston's method: {wrmsse_Croston}\")\ndel y_pred_Croston","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 6. Optimized Croston's (LB: 1.05804)\ndef optCroston(ts, h=56, debias=1.0):\n    yd = np.mean(mySES(ts[ts!=0])[-56:])\n    yi = np.mean(mySES(intervals(ts))[-56:])\n    return np.repeat(yd/yi, h)*debias\n\n\ny_pred_optCroston = Parallel(n_jobs=-1)(delayed(optCroston)(\n        demand[i*num_vals:(i+1)*num_vals].values\n    ) for i in tqdm(range(30490)))\ny_pred_optCroston = np.concatenate(y_pred_optCroston)\ny_pred_optCroston = y_pred_optCroston.reshape([56, 30490], order='F')\ny_test_optCroston = y_pred_optCroston[28:56,:].reshape([28*30490,1])\ny_pred_optCroston = y_pred_optCroston[0:28,:].reshape([28*30490,1])\ny_test_optCroston = np.concatenate(y_test_optCroston)\ny_pred_optCroston = np.concatenate(y_pred_optCroston)\nwrmsse_optCroston = wrmsse(y_pred_optCroston, lgb.Dataset(x_val, y_val))[1]\nprint(f\"WRMSSE for Optimized Croston's method: {wrmsse_optCroston}\")\ndel y_pred_optCroston","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 7. Syntetos-Boylan Approximation (LB: 1.09166)\ny_pred_SBA = Parallel(n_jobs=-1)(delayed(lambda x: Croston(x, debias=0.95))(\n        demand[i*num_vals:(i+1)*num_vals].values\n    ) for i in tqdm(range(30490)))\ny_pred_SBA = np.concatenate(y_pred_SBA)\ny_pred_SBA = y_pred_SBA.reshape([56, 30490], order='F')\ny_test_SBA = y_pred_SBA[28:56,:].reshape([28*30490,1])\ny_pred_SBA = y_pred_SBA[0:28,:].reshape([28*30490,1])\ny_test_SBA = np.concatenate(y_test_SBA)\ny_pred_SBA = np.concatenate(y_pred_SBA)\nwrmsse_SBA = wrmsse(y_pred_SBA, lgb.Dataset(x_val, y_val))[1]\nprint(f\"WRMSSE for SBA method: {wrmsse_SBA}\")\ndel y_pred_SBA","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 8. Teunter-Syntetos-Babai method (LB: 1.06812)\n# This should be optimized over the smoothing parameters by grid search\ndef TSB(ts,extra_periods=56,alpha=0.4,beta=0.4):\n    d = np.array(ts) # Transform the input into a numpy array\n    cols = len(d) # Historical period length\n    d = np.append(d,[np.nan]*extra_periods) # Append np.nan into the demand array to cover future periods\n    \n    #level (a), probability(p) and forecast (f)\n    a,p,f = np.full((3,cols+extra_periods),np.nan)\n    # Initialization\n    first_occurence = np.argmax(d[:cols]>0)\n    a[0] = d[first_occurence]\n    p[0] = 1/(1 + first_occurence)\n    f[0] = p[0]*a[0]\n                 \n    # Create all the t+1 forecasts\n    for t in range(0,cols): \n        if d[t] > 0:\n            a[t+1] = alpha*d[t] + (1-alpha)*a[t] \n            p[t+1] = beta*(1) + (1-beta)*p[t]  \n        else:\n            a[t+1] = a[t]\n            p[t+1] = (1-beta)*p[t]       \n        f[t+1] = p[t+1]*a[t+1]\n        \n    # Future Forecast\n    a[cols+1:cols+extra_periods] = a[cols]\n    p[cols+1:cols+extra_periods] = p[cols]\n    f[cols+1:cols+extra_periods] = f[cols]\n                      \n    #df = pd.DataFrame.from_dict({\"Demand\":d,\"Forecast\":f,\"Period\":p,\"Level\":a,\"Error\":d-f})\n    return f[-extra_periods:]\n\n\ny_pred_TSB = Parallel(n_jobs=-1)(delayed(TSB)(\n        demand[i*num_vals:(i+1)*num_vals].values\n    ) for i in tqdm(range(30490)))\ny_pred_TSB = np.concatenate(y_pred_TSB)\ny_pred_TSB = y_pred_TSB.reshape([56, 30490], order='F')\ny_test_TSB = y_pred_TSB[28:56,:].reshape([28*30490,1])\ny_pred_TSB = y_pred_TSB[0:28,:].reshape([28*30490,1])\ny_test_TSB = np.concatenate(y_test_TSB)\ny_pred_TSB = np.concatenate(y_pred_TSB)\nwrmsse_TSB = wrmsse(y_pred_TSB, lgb.Dataset(x_val, y_val))[1]\nprint(f\"WRMSSE for TSB method: {wrmsse_TSB}\")\ndel y_pred_TSB","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 9. Aggregate-Disaggregate Intermittent Demand Approach (LB: 1.07268)\ndef ADIDA(ts, h=56):\n    a1 = np.ceil(np.mean(intervals(ts)))\n    idx = [0 if (np.isnan(a1)) or (a1==0) else -int((len(ts) // int(a1)) * a1)][0]\n    if np.isnan(a1):\n        a1 = 1\n    agg_ser = pd.Series(ts[idx:]).rolling(int(a1)).sum()\n    agg_ser = agg_ser[np.arange(0, len(agg_ser), int(a1))] # non-overlapping\n    agg_ser[np.isnan(agg_ser)] = np.mean(agg_ser)\n    forecast = mySES(agg_ser.values)[-56:]/a1\n    return forecast\n\n\ny_pred_ADIDA = Parallel(n_jobs=-1)(delayed(ADIDA)(\n        demand[i*num_vals:(i+1)*num_vals].values\n    ) for i in tqdm(range(30490)))\ny_pred_ADIDA = np.concatenate(y_pred_ADIDA)\ny_pred_ADIDA = y_pred_ADIDA.reshape([56, 30490], order='F')\ny_test_ADIDA = y_pred_ADIDA[28:56,:].reshape([28*30490,1])\ny_pred_ADIDA = y_pred_ADIDA[0:28,:].reshape([28*30490,1])\ny_test_ADIDA = np.concatenate(y_test_ADIDA)\ny_pred_ADIDA = np.concatenate(y_pred_ADIDA)\nwrmsse_ADIDA = wrmsse(y_pred_ADIDA, lgb.Dataset(x_val, y_val))[1]\nprint(f\"WRMSSE for ADIDA method: {wrmsse_ADIDA}\")\ndel y_pred_ADIDA\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# stochastic ensemble\ndel demand, x_train, y_train, x_val, y_val \ngc.collect()\n\nprobs = 1.0/np.array([wrmsse_Naive, wrmsse_sNaive, wrmsse_SES, wrmsse_MA, \n                    wrmsse_Croston, wrmsse_optCroston, wrmsse_SBA, wrmsse_TSB, wrmsse_ADIDA]) / \\\n    sum(1.0/np.array([wrmsse_Naive, wrmsse_sNaive, wrmsse_SES, wrmsse_MA, \n                    wrmsse_Croston, wrmsse_optCroston, wrmsse_SBA, wrmsse_TSB, wrmsse_ADIDA]))\nprobs = np.cumsum(probs)\n\nrs = [random.random() for i in range(30490*28)]\ndef f(rs): \n    return np.argmax(rs<probs)\nrcols = np.vectorize(f)(rs)\n\ntest['demand'] = np.select([rcols==0, rcols==1, rcols==2, rcols==3, rcols==4, rcols==5, rcols==6, rcols==7, rcols==8], \n         [y_test_Naive, y_test_sNaive, y_test_SES, y_test_MA, y_test_Croston, y_test_optCroston, y_test_SBA, y_test_TSB, y_test_ADIDA])\ntest.fillna(0, inplace=True)\n\npredictions = test[['id', 'date', 'demand']]\npredictions = predictions.pivot(index = 'id', columns = 'date', values = 'demand').reset_index()\npredictions.columns = ['id'] + ['F' + str(i + 1) for i in range(28)]\n\nevaluation_rows = [row for row in submission['id'] if 'evaluation' in row]\nevaluation = submission[submission['id'].isin(evaluation_rows)]\n\nvalidation = submission[['id']].merge(predictions, on = 'id')\nfinal = pd.concat([validation, evaluation])\nfinal.to_csv('submission.csv', index = False)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To be continued ..."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}