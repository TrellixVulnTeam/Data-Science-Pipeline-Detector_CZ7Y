{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# This notebook offers class M5ACC_SubmissionGatekeeper\n# which you can use to validate consistency of your\n# M5 accuracy forecasts\n#\n# It prvides the following checks: \n#\n# 1. No NaN-s and negative values in the forecast;\n#\n# 2. Forecasted DAILY volumes for every item \n# and their aggregations \n# should be within historical [min, 2*max] ranges \n# for daily volumes of these items \n# and their aggregations across all 12 levels;\n# \n# 3. Forecasted MONTHLY (defined as \"calculated as a total \n# for 28 days in a row\") volumes for every item \n# and their aggregations\n# should be within historical [min, 2*max] ranges \n# for monthly volumes for these items \n# and their aggregations across all 12 levels;\n#\n# 4. No sales should be forecasted for discontinued items \n# (defined as items with no past sales \n# during the benchmarking period);\n# \n# 5. Percent of non-zero values (defined as above 0.5) \n# in a 28-day period should be \n# within historical [min-10%, max+10%] range.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from __future__ import annotations\nimport numpy as np\nimport pandas as pd \nimport copy\nimport os\nimport logging\nfrom collections import namedtuple\nfrom typing import Optional, Callable, Any","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"%%time\n# Here we load M5 datasets\n#\nINPUT_DATA_DIR=\"/kaggle/input/m5-forecasting-accuracy\"\nOUTPUT_DATA_DIR=\".\"\n\nm5_sales_history_dtypes={\"d_\"+str(i):'int32' for i in range(1, 1914)}\nm5_sales_history_dtypes.update({'id':'category'\n                             ,'item_id':'category'\n                             ,'dept_id':'category'\n                             ,'cat_id':'category'\n                             ,'store_id':'category'\n                             ,'state_id':'category'\n                            })\n\nM5_SALES_TRAIN_VALIDATION_DF=pd.read_csv(\n    os.path.join(\n        INPUT_DATA_DIR,\"sales_train_validation.csv\")\n    ,dtype=m5_sales_history_dtypes)\n\nM5_SAMPLE_ACC_SUBMISSION_DF=pd.read_csv(os.path.join(\n    INPUT_DATA_DIR,\"sample_submission.csv\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# There are 2 large cells below with classes defined inside the cells. \n# Skip these cells for now and\n# go to the bottom of the notebbok to see an example of\n# how the submission validator works","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class LoggableObject:\n    \"\"\" Base class for types that are able to log messages.\n    A wrapper for standard Python logging functionality,\n    extended with ability to automatically\n    append a logged message with information\n    about an object that generated the message.\"\"\"\n\n    def __init__(self\n            , logger_name: str \n            , reveal_loggers_identity: bool = True\n            , new_handler = None\n            , new_level:Optional[int] = None\n            , new_formatter = None\n            ):\n        self.logger = logging.getLogger(logger_name)\n        self.reveal_identity = reveal_loggers_identity\n        self.update_logger(new_level, new_handler, new_formatter)\n\n    def reveal_self_names(self) -> str:\n        \"\"\" Find the name(s) of variable(s) that hold self, if possible.\n\n        The function uses a naive approach,\n        it does not always find all the names\"\"\"\n\n        all_names = set()\n\n        for fi in inspect.stack():\n            local_vars = fi.frame.f_locals\n            names = {name for name in local_vars if local_vars[name] is self}\n            all_names |= names\n\n        all_names = list(all_names)\n        if \"self\" in all_names:\n            all_names.remove(\"self\")\n\n        if len(all_names) == 0:\n            return \"\"\n        elif len(all_names) == 1:\n            return all_names[0]\n        else:\n            return str(all_names).replace(\"'\", \"\")\n\n    def __str__(self) -> str:\n        description = f\"Logged messages are labeled '{self.logger.name}' and send via {str(self.logger.handlers)}. \"\n        return description\n\n    def update_logger(self\n            , new_level:int = logging.DEBUG\n            , new_handler = logging.StreamHandler()\n            , new_formatter = logging.Formatter(\n                '%(asctime)s %(name)s %(levelname)s: %(message)s',\n                datefmt=\"%I:%M:%S\")\n            ) -> LoggableObject:\n        if new_level is not None:\n            self.logger.setLevel(new_level)\n\n        if new_handler is not None:\n            if new_level is not None:\n                new_handler.setLevel(new_level)\n            if new_formatter is not None:\n                new_handler.setFormatter(new_formatter)\n\n            for h in self.logger.handlers:\n                if str(h) == str(new_handler):\n                    self.logger.removeHandler(h)\n                    self.logger.addHandler(new_handler)\n                    break\n            else:\n                self.logger.addHandler(new_handler)\n\n        return self\n\n    def append_str_with_identity_info(self, a_str:str) -> str:\n        if self.reveal_identity:\n            self_id_str = self.reveal_self_names()\n            self_id_str += ':' + type(self).__qualname__\n            self_id_str = self_id_str.lstrip(':')\n            full_len = len(a_str)\n            a_str = a_str.rstrip('\\n')\n            num_eols = full_len - len(a_str)\n            a_str += ' /* logged by ' + self_id_str + ' */'\n            a_str += num_eols * '\\n'\n        return a_str\n\n    def debug(self, msg:Optional[str] = None, *args, **kwargs):\n        self.log(level=logging.DEBUG, msg=msg, *args, **kwargs)\n\n    def info(self, msg:Optional[str] = None, *args, **kwargs):\n        self.log(level=logging.INFO, msg=msg, *args, **kwargs)\n\n    def warning(self, msg:Optional[str] = None, *args, **kwargs):\n        self.log(level=logging.WARNING, msg=msg, *args, **kwargs)\n\n    def error(self, msg:Optional[str] = None, *args, **kwargs):\n        self.log(level=logging.ERROR, msg=msg, *args, **kwargs)\n\n    def critical(self, msg:Optional[str] = None, *args, **kwargs):\n        self.log(level=logging.CRITICAL, msg=msg, *args, **kwargs)\n\n    def fatal(self, msg:Optional[str] = None, *args, **kwargs):\n        self.log(level=logging.FATAL, msg=msg, *args, **kwargs)\n\n    def log(self, level:int, msg:Optional[str] = None, *args, **kwargs):\n        if msg is None:\n            msg = str(self)\n        msg = self.append_str_with_identity_info(msg)\n        self.logger.log(level, msg, *args, **kwargs)  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class M5ACC_SubmissionGatekeeper(LoggableObject):\n    \"\"\" A class that validates consistency of M5 accuracy forcasts\"\"\"\n\n    def __init__(\n            self\n            ,traning_data_df\n            ,sample_submission_df\n            ,n_years_to_benchmark_against=1\n            ,drop_Xmas_days=True\n            ,n_df_lines_to_print_in_report=5\n            ,logging_level = logging.DEBUG):\n        \n        # Let's perform basic consistency checks to make sure\n        # baseline datasets traning_data_df and sample_submission_df\n        # were not accidentally altered\n        # after they were loaded from input .csv files\n\n        self.level_labels_set = {\"store_id\", \"item_id\", \"cat_id\"\n            , \"state_id\", \"dept_id\"}\n\n        assert isinstance(traning_data_df, type(pd.DataFrame()))\n        assert isinstance(sample_submission_df, type(pd.DataFrame()))\n        assert sample_submission_df.shape == (60980, 29)\n        assert len(traning_data_df) == 30490\n        assert self.level_labels_set.issubset(traning_data_df.columns)\n\n        assert set(traning_data_df.state_id.unique()\n                   ) == {'CA', 'TX', 'WI'}\n\n        assert set(traning_data_df.cat_id.unique()\n                   ) == {'HOBBIES', 'HOUSEHOLD', 'FOODS'}\n\n        assert {'d_' + str(n) for n in range(1, 1914)\n                }.issubset(traning_data_df.columns)\n\n        assert {'F' + str(n) for n in range(1, 29)\n                }.issubset(sample_submission_df.columns)\n\n        assert {'id'}.issubset(sample_submission_df.columns)\n        assert traning_data_df.d_1.sum() == 32631\n        assert traning_data_df.d_42.sum() == 25572\n        assert traning_data_df.d_1913.sum() == 49795\n        assert set(traning_data_df.id.unique()\n                   ).issubset(sample_submission_df.id.unique())\n\n        \n        # now, let's make sure __init__ received correct arguments\n        assert n_years_to_benchmark_against in {1, 2, 3, 4, 5}\n        assert drop_Xmas_days in {True, False}\n        assert n_df_lines_to_print_in_report in range(1, 101)\n        assert logging_level in {\n            logging.DEBUG, logging.INFO, logging.WARNING}\n        \n        # finally, actual initialisation\n        super().__init__(logger_name=\"M5\",reveal_loggers_identity=False)\n        super().update_logger(new_level = logging_level)\n\n        self.n_years = n_years_to_benchmark_against\n        self.n_df_lines_to_print_in_report = (\n            n_df_lines_to_print_in_report )\n\n        self.sample_submission_df = sample_submission_df\n        self.training_df = copy.deepcopy(traning_data_df)\n\n        Xmas_days_list = [\n            \"d_331\", \"d_697\", \"d_1062\", \"d_1427\", \"d_1792\"]\n\n        if drop_Xmas_days:\n            self.training_df.drop(columns=Xmas_days_list\n                                  , inplace=True)\n\n        self.training_df[\"id\"] = (\n                self.training_df[\"item_id\"].astype(\"str\")\n                + \"_\" + self.training_df[\"store_id\"].astype(\"str\"))\n\n        self.training_df.set_index(\"id\", inplace=True)\n\n        all_training_day_numbers = [int(s[2:])\n                                    for s in self.training_df.columns\n                                    if s.startswith(\"d_\")]\n\n        n_days_to_benchmark_against = 365 * n_years_to_benchmark_against\n        if n_days_to_benchmark_against > len(all_training_day_numbers):\n            n_days_to_benchmark_against = len(all_training_day_numbers)\n\n        dates_used_for_benchmark = sorted(\n            all_training_day_numbers)[-n_days_to_benchmark_against:]\n\n        self.training_df = (\n            self.training_df[[\"d_\" + str(n)\n                              for n in dates_used_for_benchmark]\n                             + list(self.level_labels_set)])\n\n        self.training_AtL = self._twelve_aggregations(self.training_df)\n\n        self.level_labels_df = (\n            copy.deepcopy(self.training_df[self.level_labels_set]))\n\n        self.training_tL = (\n            self.training_df.drop(columns=self.level_labels_set))\n\n        del self.training_df\n\n        self.daily_minimums = self.training_AtL.min(axis=\"columns\")\n        self.daily_maximums = self.training_AtL.max(axis=\"columns\")\n\n        self.training_monthly_totals_AtL = self.training_AtL.rolling(\n            window=28, min_periods=28, axis=\"columns\").sum()\n\n        self.training_monthly_totals_AtL.dropna(\n            axis=\"columns\", inplace=True)\n\n        self.monthly_minimums = (\n            self.training_monthly_totals_AtL.min(axis=\"columns\"))\n\n        self.monthly_maximums = (\n            self.training_monthly_totals_AtL.max(axis=\"columns\"))\n\n        self.no_sales = (\n            list(self.training_tL[\n                self.training_tL.sum(axis=\"columns\") == 0]\n                 .index.unique()))\n\n        training_nonzeros_tL = self.training_tL.astype(\"bool\").astype(\"int\")\n\n        training_nonzeros_rolling_monthly_tL = (\n            training_nonzeros_tL.rolling(\n                window=28, min_periods=28, axis=\"columns\"\n            ).sum())\n\n        training_nonzeros_rolling_monthly_tL.dropna(\n            axis=\"columns\", inplace=True)\n\n        historical_monthly_nonzeros = (\n            training_nonzeros_rolling_monthly_tL.sum())\n\n        self.time_window_size = 28 * len(self.training_tL)\n\n        self.min_nonzeros = (\n            100.0 * historical_monthly_nonzeros.min() \n            / self.time_window_size)\n\n        self.max_nonzeros = (\n            100.0 * historical_monthly_nonzeros.max() \n            / self.time_window_size)\n\n    def is_submission_structure_OK(\n            self\n            , submission_to_check_df):\n        \"\"\"Perfom basic formal check for a submission DataFrame\"\"\"\n\n        passed_basic_checks = True\n\n        if not isinstance(submission_to_check_df, type(pd.DataFrame())):\n            self.error(\"Expected Pandas DataFrame.\\n\")\n            return False\n\n        if submission_to_check_df.shape != self.sample_submission_df.shape:\n            self.error(f\"Submission dataset has a wrong shape\"\n                + f\" {submission_to_check_df.shape}, \"\n                + f\"Expected shape is {self.sample_submission_df.shape}.\\n\")\n            passed_basic_checks = False\n\n        if (set(submission_to_check_df.columns)\n                != set(self.sample_submission_df.columns)):\n\n            self.error(\"Submission dataset has wrong column names.\\n\")\n            passed_basic_checks = False\n\n        else:\n            self.info(f\"Submission dataset has correct column names\"\n                + \" (check passed).\\n\")\n\n        if (set(submission_to_check_df.id.unique())\n                != set(self.sample_submission_df.id.unique())):\n\n            self.error(\"Submission dataset has wrong values\"\n                + \" in 'id' columns.\\n\")\n\n            passed_basic_checks = False\n\n        else:\n            self.info(f\"Submission dataset has correct values\"\n                + \" in 'id' column (check passed).\\n\")\n\n        num_of_na_s = submission_to_check_df.isna().sum().sum()\n\n        if num_of_na_s > 0:\n            self.error(f\"Submission dataset contains\"\n                + f\" {num_of_na_s} N/A-s.\\n\")\n            passed_basic_checks = False\n        else:\n            self.info(f\"Submission dataset does not contain\"\n                + \" N/A-s (check passed).\\n\")\n\n        num_of_negatives = (\n            (submission_to_check_df.drop(columns=[\"id\"]) < 0).sum().sum())\n\n        if num_of_negatives > 0:\n            self.error(f\"Submission dataset contains\"\n                + f\" {num_of_negatives} negative values.\\n\")\n            passed_basic_checks = False\n        else:\n            self.info(f\"Submission dataset does not contain\"\n                + \" negative values (check passed).\\n\")\n\n        return passed_basic_checks\n\n    def create_val_eval(\n            self\n            , submission_to_check_df):\n        \"\"\"Split a submission dataset into Validation and Evaluation\"\"\"\n\n        if not self.is_submission_structure_OK(submission_to_check_df):\n            return None\n\n        all_ids = list(submission_to_check_df.id.unique())\n        validation_ids = [i for i in all_ids if i.endswith(\"_validation\")]\n        evaluation_ids = [i for i in all_ids if i.endswith(\"_evaluation\")]\n        if not (len(validation_ids) == len(evaluation_ids) == 30490):\n            self.error(\"Submission dataframe contains incorrect id-s\")\n            return None\n\n        self.info(\"...splitting a candidate submission dataset\"\n            + \" into Validation and Evaluation sets...\\n\")\n\n        validation_tL = (\n            copy.deepcopy(\n                submission_to_check_df[\n                    submission_to_check_df.id.isin(validation_ids)]))\n\n        validation_suffix_len = len(\"_validation\")\n        validation_tL[\"id\"] = validation_tL[\"id\"].str[:-validation_suffix_len]\n        validation_tL.set_index(\"id\", inplace=True)\n        validation_tL.columns = [int(c[1:]) for c in validation_tL.columns]\n        validation_tL.columns = [\n            \"d_\" + str(1913 + c) for c in validation_tL.columns]\n\n        evaluation_tL = (\n            copy.deepcopy(\n                submission_to_check_df[\n                    submission_to_check_df.id.isin(evaluation_ids)]))\n\n        evaluation_suffix_len = len(\"_evaluation\")\n        evaluation_tL[\"id\"] = evaluation_tL[\"id\"].str[:-evaluation_suffix_len]\n        evaluation_tL.set_index(\"id\", inplace=True)\n        evaluation_tL.columns = [int(c[1:]) for c in evaluation_tL.columns]\n        evaluation_tL.columns = [\n            \"d_\" + str(1941 + c) for c in evaluation_tL.columns]\n\n        if not (len(self.training_tL)\n                == len(validation_tL)\n                == len(evaluation_tL)):\n            self.error(\"Submission file contains wrong # of id-s\")\n            return None\n\n        if not (set(self.training_tL.index)\n                == set(validation_tL.index)\n                == set(evaluation_tL.index)):\n            self.error(\"Submission file contains incorrect id-s\")\n            return None\n\n        ValEval = namedtuple(\"M5ValEval\", [\"validation\", \"evaluation\"])\n\n        return ValEval(validation=validation_tL, evaluation=evaluation_tL)\n\n    def _one_aggregation(\n            self\n            , data_to_aggregate_tL\n            , ids_to_group_by_list):\n        \"\"\"Create one (out of 12) level of aggregation\"\"\"\n\n        assert len(ids_to_group_by_list) in {1, 2}\n        assert set(ids_to_group_by_list).issubset(self.level_labels_set)\n\n        ids_to_drop_set = self.level_labels_set - set(ids_to_group_by_list)\n\n        aggregated_tL = (data_to_aggregate_tL.drop(columns=ids_to_drop_set)\n            .groupby(ids_to_group_by_list).sum())\n\n        if len(ids_to_group_by_list) == 2:\n            aggregated_tL[\"line_id\"] = (\n                aggregated_tL.index.get_level_values(0).astype(\"str\")\n                + \"_\"\n                + aggregated_tL.index.get_level_values(1).astype(\"str\"))\n\n            aggregated_tL = aggregated_tL.set_index(\"line_id\")\n\n        else:\n            aggregated_tL.index = aggregated_tL.index.astype(\"str\") + \"_X\"\n\n        return aggregated_tL\n\n    def _twelve_aggregations(\n            self\n            , input_tL):\n        \"\"\"Create all 12 levels of aggregation\"\"\"\n\n        assert self.level_labels_set.issubset(input_tL.columns)\n\n        d_labels_set = {c for c in input_tL.columns\n                        if c.startswith(\"d_\")}\n\n        assert (len(d_labels_set) + len(self.level_labels_set)\n                == len(input_tL.columns))\n\n        assert len(input_tL) == 30490\n\n        all_aggregations_list = []\n        aggregated_totals = copy.deepcopy(input_tL)\n\n        aggregated_totals.drop(\n            columns=self.level_labels_set, inplace=True)\n\n        aggregated_totals = pd.DataFrame(aggregated_totals.sum())\n        aggregated_totals.columns = [\"Total_X\"]\n        aggregated_totals = aggregated_totals.transpose()\n        all_aggregations_list += [aggregated_totals]\n\n        aggregation_fields_list = [['item_id'], ['dept_id'], ['cat_id']\n            , ['store_id'], [\"state_id\"]\n            , ['state_id', 'cat_id']\n            , ['state_id', 'dept_id']\n            , ['store_id', 'cat_id']\n            , ['store_id', 'dept_id']\n            , ['state_id', 'item_id']\n            , ['item_id', 'store_id']]\n\n        for af in aggregation_fields_list:\n            all_aggregations_list += [self._one_aggregation(input_tL, af)]\n\n        output_AtL = pd.concat(all_aggregations_list)\n\n        input_cksum = 12 * input_tL[d_labels_set].sum().sum()\n        output_cksum = output_AtL.sum().sum()\n\n        assert (abs(input_cksum - output_cksum)\n                < (input_cksum + output_cksum) * 0.00000000001)\n\n        return output_AtL\n\n    def check_28day_df(\n            self\n            , to_check_tL\n            , name):\n        \"\"\"Perfom advanced consistency checks for a 28-day DataFrame\"\"\"\n\n        assert to_check_tL.shape == (30490, 28)\n        assert isinstance(name, str)\n\n        passed_sanity_checks = True\n\n        self.info(\"<_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_>\\n\")\n        self.info(f\"...performing consistency checks\"\n            + f\" for the {name} dataset...\\n\")\n\n        # let's look at % of non-zero values per 28-day period\n        # and benchmark it against historical data.\n        # since techically predictions can have non-integer values,\n        # we will count any x<0.5 as zero\n\n        current_nonzeros = (100.0 * (to_check_tL >= 0.5).sum().sum()\n            / self.time_window_size)\n\n        if not ((self.min_nonzeros - 10)\n                < current_nonzeros\n                < (self.max_nonzeros + 10)):\n\n            self.warning(f\"{name} dataset (no-aggregation version) \"\n                + f\"has {current_nonzeros:.0f}% of non-zero values,\"\n                + f\" while historically there were between\"\n                + f\" {self.min_nonzeros:.0f}% and {self.max_nonzeros:.0f}%\"\n                + f\" non-zeros in any 28-day period in the final\"\n                + f\" {self.n_years} year(s) of the training data\\n\")\n\n            passed_sanity_checks = False\n\n        else:\n            self.info(f\"Density of non-zero values in {name} dataset\"\n                + f\" looks OK (check passed).\\n\")\n\n        # some items-per-store had 0 sales for the final years\n        # of the training period; if we forecast non-zero future sales\n        # for these items-stores, it will look strange\n\n        if not (to_check_tL[\n            to_check_tL.index.isin(self.no_sales)].sum().sum()== 0):\n\n            should_be_no_sales_df = (\n                to_check_tL[to_check_tL.index.isin(self.no_sales)\n                ].sum(axis=\"columns\"))\n\n            bad_items = (\n                list(should_be_no_sales_df[should_be_no_sales_df != 0\n                                           ].index.unique()))\n\n            self.warning(f\"{len(self.no_sales)} items had zero sales\"\n                + f\" during the final {self.n_years} year(s) of the\"\n                + f\" training period. We assume they were discontinued\"\n                + f\" from distribution in respected stores.\"\n                + f\" However, {len(bad_items)} of these items have\"\n                + f\" non-zero sales in original\"\n                + f\" (no aggregation) {name} dataset.\\n\")\n\n            self.debug(\"Offending items are:\\n\" + str(bad_items) + \"\\n\")\n\n            passed_sanity_checks = False\n\n        else:\n\n            self.info(f\"{name} dataset does not predict sales for\"\n                + \" discontinued items (check passed).\\n\")\n\n        # now let's switch our attention from an original sales forecast\n        # to an aggegated version\n\n        to_check_AtL = (\n            self._twelve_aggregations(self.level_labels_df.join(to_check_tL)))\n\n        # let's check daily max/min item-per-store sales\n        # against historical data\n\n        dataset_maximums = to_check_AtL.max(axis=\"columns\")\n\n        above_doubled_maximums = (\n                dataset_maximums > 2 * self.daily_minimums)\n\n        num_above_doubled_maximums = above_doubled_maximums.sum()\n\n        if num_above_doubled_maximums > 0:\n            self.warning(f\"Aggregated {name} dataset contains\"\n                + f\" {num_above_doubled_maximums} daily values\"\n                + f\" which are above doubled daily maximums (calculated for\"\n                + f\" the final {self.n_years} year(s) of training data) \\n\")\n\n            offences = dataset_maximums[above_doubled_maximums]\n            offences.sort_values(ascending=False, inplace=True)\n            offences = pd.DataFrame(offences)\n            offences.columns = [\"Max values\"]\n            self.debug(f\"Some offending examples from\"\n                + f\" aggregated {name} dataset:\\n\"\n                + str(offences.head(self.n_df_lines_to_print_in_report))\n                + \"\\n\")\n\n            passed_sanity_checks = False\n\n        else:\n            self.info(f\"Daily maximums in {name} dataset\"\n                         + \" are OK (check passed).\\n\")\n\n        # this particular check only makes sense\n        # if we have removed Christmas day from the training dataset\n        # or if we have performed Christmas imputation,\n        # otherwise the check has no impact\n        dataset_minimums = to_check_AtL.min(axis=\"columns\")\n        below_minimums = (dataset_minimums < self.daily_minimums)\n        num_below_minimums = below_minimums.sum()\n\n        if num_below_minimums > 0:\n            self.warning(f\"Aggregated {name} dataset contains\"\n                + f\" {num_below_minimums} items \"\n                + f\"with values that are below daily minimums \"\n                + f\"(calculated for the final {self.n_years}\"\n                + f\" year(s) of training data) \\n\")\n\n            offences = dataset_minimums[below_minimums]\n            offences.sort_values(inplace=True)\n            offences = pd.DataFrame(offences)\n            offences.columns = [\"Min values\"]\n            self.debug(f\"Some offending examples from\"\n                + f\" aggregated {name} dataset:\\n\"\n                + str(offences.head(self.n_df_lines_to_print_in_report))\n                + \"\\n\")\n\n            passed_sanity_checks = False\n        else:\n            self.info(f\"Daily minimums in {name} dataset are\"\n                         + f\" OK (check passed).\\n\")\n\n        # let's check monthly max/min item-per-store sales\n        # against historical data (calculated over rolling 28-day periods)\n\n        monthly_totals = to_check_AtL.sum(axis=\"columns\")\n        above_doubled_maximums = (\n                self.monthly_maximums * 2 < monthly_totals)\n\n        times_above_doubled_maximums = above_doubled_maximums.sum()\n\n        if times_above_doubled_maximums > 0:\n            self.warning(f\"Aggregated {name} dataset contains \"\n                + f\"{times_above_doubled_maximums} monthly total values \"\n                + f\"which are above doubled monthly maximums \"\n                + f\"(calculated for the final {self.n_years}\"\n                + f\" year(s) of training data). \\n\")\n\n            offences = monthly_totals[above_doubled_maximums]\n            offences.sort_values(ascending=False, inplace=True)\n            offences = pd.DataFrame(offences)\n            offences.columns = [\"Monthly totals\"]\n            self.debug(f\"Some offending examples from aggregated\"\n                + f\" {name} dataset:\\n\"\n                + str(offences.head(self.n_df_lines_to_print_in_report))\n                + \"\\n\")\n\n            passed_sanity_checks = False\n        else:\n            self.info(f\"All monthly totals in {name} dataset\"\n                + \" are below doubled historical\"\n                + \" maximums (check passed).\\n\")\n\n        below_minimums = (monthly_totals < self.monthly_minimums)\n        times_below_minimums = below_minimums.sum()\n\n        if times_below_minimums > 0:\n            self.warning(f\"Aggregated {name} dataset\"\n                + f\" contains {times_below_minimums} monthly\"\n                + f\"total values which are below monthly minimums\"\n                + f\" (calculated for the final {self.n_years}\"\n                + f\" year(s) of training data. \\n\")\n\n            offences = monthly_totals[below_minimums]\n            offences.sort_values(ascending=False, inplace=True)\n            offences = pd.DataFrame(offences)\n            offences.columns = [\"Monthly totals\"]\n            self.debug(f\"Some offending examples from aggregated\"\n                + f\" {name} dataset:\\n\"\n                + str(offences.head(self.n_df_lines_to_print_in_report))\n                + \"\\n\")\n\n            passed_sanity_checks = False\n        else:\n            self.info(f\"All monthly totals in {name} dataset are \"\n                         + f\"above historical minimums (check passed).\\n\")\n\n        return passed_sanity_checks\n\n    def is_submission_healthy(\n            self\n            , submission_to_check_df\n            , n_df_lines_to_print_in_report=\"default\"):\n        \"\"\"Run all checks for a candidate M5-Acc submission dataframe.\"\"\"\n\n        if n_df_lines_to_print_in_report != \"default\":\n            assert n_df_lines_to_print_in_report in range(1, 101)\n\n            self.n_df_lines_to_print_in_report = (\n                n_df_lines_to_print_in_report)\n\n        self.info(\"Starting sanity checking process\"\n            + \" for a candidate submission...\\n\")\n\n        two_datasets = self.create_val_eval(submission_to_check_df)\n\n        if two_datasets is not None:\n\n            val_is_OK = self.check_28day_df(\n                two_datasets.validation, \"Validation\")\n\n            eval_is_OK = self.check_28day_df(\n                two_datasets.evaluation, \"Evaluation\")\n\n            submission_is_OK = (val_is_OK and eval_is_OK)\n\n        else:\n\n            submission_is_OK = False\n\n        self.info(\"<_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_>\\n\")\n\n        if submission_is_OK:\n            self.info(\"...sanity check for a candidate submission\"\n                + \" has finished with a good news. The submission\"\n                + \" meets basic criteria and is healthy.\"\n                + \" It's OK to proceed and upload \"\n                + \" the submission to Kaggle.\")\n        else:\n            self.warning(\"...sanity check for a candidate submission\"\n                + \" has finished with a BAD news. The\"\n                + \" submission does not meet basic criteria,\"\n                + \" it is NOT HEALTHY.\")\n\n        return submission_is_OK","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Here is an example of how M5ACC_SubmissionGatekeeper class should be used\n#\n# Its constructor expects two main (required) parameters: \n# (1) a pd.DataFrame with original M5 sales_train_validation.csv dataset,\n# and (2) another pd.DataFrame with original sample_submission.csv dataset\n# \n# there are also three more (optional) parameters:\n# (3) n_years_to_benchmark_against indicated the depth of historic data that \n# we wnat to use to generate benchmarks. More is not nessesearly better here \n# because we are mostly collecting daily/monthly min-s/max-s.\n# We recommend setting it to 1 or 2 years.\n# (4) drop_Xmas_days indicates that we want to \n# exclude Christmas from the analysis.\n# While techically Christmas days are present in the training dataset, \n# it appears that the stores were actually closed on these days, \n# as they contain near-zero values.\n# (5) df_lines_to_ptint_in_report contains a number of lines we wnat to \n# print while outputing offending datasets in DEBUG mode\n# (6) logging_level instructs the Gatekeeper to produce more or less \n# detailed report\n#\n# Once M5ACC_Submission_Gatekeeper object is created, \n# its method .is_submission_healthy(pd.DataFrame) \n# should be used to validate consistancy of your M5 submission\n#\n# It only works for \"M5 Forecasting - Accuracy\" competition\n# Submissions to \"M5 Forecasting - Uncertainty\" competition can NOT be validated\n# by M5ACC_Submission_Gatekeeper\n#\n# Depending on the logging_level (logging.DEBUG, logging.INFO, \n# logging.WARNING, etc.), .is_submission_healthy() can produce \n# more or less detailed output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# Here we create an instance of our sanity / consistency validator\n# The process may take a few dozens of seconds\ngatekeeper = M5ACC_SubmissionGatekeeper(\n    M5_SALES_TRAIN_VALIDATION_DF\n    ,M5_SAMPLE_ACC_SUBMISSION_DF\n    ,n_years_to_benchmark_against = 1\n    ,drop_Xmas_days = True\n    ,logging_level = logging.DEBUG )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Here we create a submission dataset populated with random numbers\n# We will use it to demostrate how the validator works\nrandom_submission_df = copy.deepcopy(M5_SAMPLE_ACC_SUBMISSION_DF)\nf_labels = ['F'+str(n) for n in range(1,29)]\nrandom_submission_df[f_labels] = np.random.rand(60980,28)*3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# Now, let's use the validator to \"diagnose\" the submission\nsubmission_is_good = gatekeeper.is_submission_healthy(random_submission_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if submission_is_good:\n    random_submission_df.to_csv(\n        os.path.join(OUTPUT_DATA_DIR,\"new_submission_file.csv\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now, if it looks interesting to you, scroll up and \n# spend some time reading the code for class M5ACC_SubmissionGatekeeper\n# Disregard class LoggableObject as it does not add much value \n# from the perspective of the competition - it is just a small helper tool ","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}