{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **Problem Statement**\nIn this competition, the fifth iteration, you will use hierarchical sales data from Walmart, the worldâ€™s largest company by revenue, to forecast daily sales for the next 28 days. The data, covers stores in three US States (California, Texas, and Wisconsin) and includes item level, department, product categories, and store details. In addition, it has explanatory variables such as price, promotions, day of the week, and special events. Together, this robust dataset can be used to improve forecasting accuracy.\n\n**How much camping gear will one store sell each month in a year?** ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Evaluation Metrics\nThis competition uses a Weighted Root Mean Squared Scaled Error (RMSSE).","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Data\n* **calendar.csv** - Contains information about the dates on which the products are sold.\n* **sales_train_validation.csv** - Contains the historical daily unit sales data per product and store [d_1 - d_1913]\n* **sample_submission.csv** - The correct format for submissions. Reference the Evaluation tab for more info.\n* **sell_prices.csv** - Contains information about the price of the products sold per store and date.\n* **sales_train_evaluation.csv** - Includes sales [d_1 - d_1941] (labels used for the Public leaderboard)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Content:\n1. Loading Libraries\n2. Importing files\n3. Summary Statistics\n4. Analysis of Selling Prices\n5. Time Series Analysis \n6. Impact of Events\n7. Impact of SNAP days","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#  Loading Libraries","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n!pip install calplot\nimport calplot\nimport cufflinks as cf\ncf.go_offline()\ncf.set_config_file(offline=False, world_readable=True)\n!pip install chart_studio \nimport chart_studio.plotly as py\nimport plotly.graph_objs as go","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Importing Files","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"calendar = pd.read_csv('//kaggle/input/m5-forecasting-accuracy/calendar.csv')\nsales_train_validation = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sales_train_validation.csv')\nsample_submission = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sample_submission.csv')\nsell_prices = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sell_prices.csv')\nsales_train_evaluation = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sales_train_evaluation.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Summary Statistics","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"First, I loaded the sales_train_validation dataset which is our main datasets and contains the historical daily unit sales data per product and store for 1,913 days from 29-Jan-2011.","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"sales_train_validation.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"[column_train,row_train] = sales_train_validation.shape\ncolumn_train,row_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The sales_train_validation dataset has 30,490 rows and 1,919 columns.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(sales_train_validation.id.str.contains('validation')))\nprint(len(sales_train_validation.id.unique()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"There are 30,490 unique items in the datasets.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(sales_train_validation.id.unique()))\nprint(len(sales_train_validation.item_id.unique()))\nprint(sales_train_validation.dept_id.unique())\nprint(sales_train_validation.cat_id.unique())\nprint(sales_train_validation.store_id.unique())\nprint(sales_train_validation.state_id.unique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 3,049 unique item_id, 7 unique dept_id, 3 unique cat_id, 10 unique store_id and 3 unique state_id.\nThis data set belongs to three states of US, CA (California), TX (Texas) & WI (Wisconsin).","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Lets change the date columns from the current format of \"d_\" to date time format \"dd-mm-yyyy\" for time series analysis.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = sales_train_validation\ndf1 = sales_train_validation.set_index('id').drop(['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], axis=1).transpose()\ndf1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n = len(calendar) - len(df1)\ndf2 = calendar[['date', 'd']].set_index('d').iloc[:-n]\ndf3 = pd.concat([df2,df1], axis=1).set_index('date')\ndf3","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Visualization of time series of 20 random item to see pattern.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(16,30))\nfor i,j in zip(df3.sample(n=20, axis=1), range(20)):\n    ax=plt.subplot(10,2,j + 1) \n    df3[[i]].plot(ax=ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above we can see that some of the items has been selling for the compltete period of the datasets, but some where introduced latter and some were discontinued.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Lets visualize the dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df4 = pd.concat([df[['id','item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']].set_index('id'),df3.transpose()], axis=1)\ndf4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"unique_units = df4.state_id.value_counts()\nprint('The count of total number of unique items is:\\n', unique_units)\nunique_units.plot(title='Distribution of Total items by State', kind='pie', autopct='%1.1f%%', figsize=(10,6))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"CA has the highest contibution to sales of unique items with 40% contibution, while the remaining has 30% contibution each. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Total_Sales = df4.groupby('state_id').sum().sum(axis=1)\nTotal_Sales.plot(title='Distribution of Total sales by State', kind='pie', autopct='%1.1f%%', figsize=(10,6))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"43.6%, 28.8% and 27.6% of the total sales has been from CA, TX and WI respectively.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df4.groupby(['cat_id']).sum().transpose().sum().plot(title='Sales Distribution by category', kind='pie', autopct='%1.1f%%',\n        shadow=True, figsize=(10,6))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"68.6%, 22.0% and 9.3% of the total sales has been from the categories FOODS, HOUSEHOLD and HOBBIES respectively.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_dist = df4.groupby(['cat_id','state_id']).sum().sum(axis=1).unstack('cat_id')\n(cat_dist.transpose() / cat_dist.transpose().sum()).transpose().plot(kind='bar')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Each state has similar pattern of sales by category with FOODS contibuting to the major share. WI has the highest contribution from FOODS category in comparison to other states. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df4.groupby(['dept_id']).sum().transpose().sum().plot(title='Sales Distribution by sub category', kind='pie', autopct='%1.1f%%',\n        shadow=True, figsize=(10,6))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* FOODS_3 has the highest contibution to sales with 49.3% share followed by FOODS_2 and FOODS_1\n* HOUSEHOLD_1 has the highest contibution to sales in the HOUSEHOLD category with 17.5% share followed by HOUSEHOLD_2\n* HOBBIES_1 has the highest contibution to sales in the HOBBIES category with 8.5% share followed by HOBBIES_2","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dept_dist = df4.groupby(['dept_id','state_id']).sum().sum(axis=1).unstack('dept_id')\n(dept_dist.transpose() / dept_dist.transpose().sum()).transpose().iplot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Each state has similar pattern of sales by sub category with FOODS_3 contibuting to the major share. TX has the highest contribution from FOODS_3 category in comparison to other states. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df4.groupby(['store_id']).sum().transpose().sum().plot(title='Sales Distribution by store', kind='pie', autopct='%1.1f%%',\n        shadow=True, figsize=(10,6))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Store CA_3 has the highest share of sales with 17.0% share.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"store_state_dist = df4.groupby(['store_id','state_id']).sum().sum(axis=1).unstack('store_id')\n(store_state_dist.transpose() / store_state_dist.transpose().sum()).transpose().iplot(\n    kind='bar', title='Sales distribution of stores in each state')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* In CA CA_3 has the highest share of sales with 39% share follwed by CA_1, CA_2 and CA_4\n* In TX TX_2 has the highest share of sales with 38.17% share follwed by TX_3 and TX_1\n* In WI WI_2 has the highest share of sales with 36.11% share follwed by WI_3 and WI_1","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dept_store_dist = df4.groupby(['dept_id','store_id']).sum().sum(axis=1).unstack('dept_id')\n(dept_store_dist.transpose() / dept_store_dist.transpose().sum()).transpose().iplot(\n    kind='bar', title='Sales Distribution of Sub Categories by Store_ID')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* FOOD_3 has the highest contibution of sales in all the stores with WI_3 has the highest contribution where FOODS_3 contribute to 54.76% of the total sales.\n* All the stores has the similar pattern of sales of sub categories expect CA_2, where contribution by FOODS_1 is more than FOODS_2","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Analysis of Selling Prices","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sell_prices['wm_yr_wk'] = sell_prices['wm_yr_wk'].astype(str)\n\nsell_prices['month'] = sell_prices['wm_yr_wk'].str[0:1]\nsell_prices['year'] = sell_prices['wm_yr_wk'].str[1:3]\nsell_prices['year'] = '20' + sell_prices['year'].astype(str)\nsell_prices['week'] = sell_prices['wm_yr_wk'].str[3:5]\nsell_prices['state_id'] = sell_prices['store_id'].str.split('_', 1).str[0]\nsell_prices['cat_id'] = sell_prices['item_id'].str.split('_', 1).str[0]\nsell_prices['dept_id'] = sell_prices['item_id'].str.split('_').str[0] + '_' + sell_prices['item_id'].str.split('_').str[1]\n\nsell_prices.drop('wm_yr_wk', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sell_prices.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sell_prices['sell_price'].plot(kind='hist', figsize=(10,5), bins=20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Almost all of the items has the selling price in the range of \\\\$0-\\\\$20\n* Most of the items has the selling price in the range of \\\\$0-\\\\$10","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sell_prices.groupby(['year','state_id']).mean().unstack('state_id').boxplot(\n    figsize=(10,3), vert=False)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The average selling price of all the itmes is highest in WI followed by CA and TX\n* WI has the closest range of selling price whereas TX has the largest range of selling price\n* The range of avg. selling price of all the items is between \\\\$4.15-\\\\$4.50 ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sell_prices.groupby(['year','store_id']).mean().unstack('store_id').boxplot(figsize=(10,7), vert=False)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* There is variation in selling prices in each of the store\n* Store WI_1 has the highest average selling price, also it has the closest range of average selling price\n* Store TX_2 has the lowest average selling price and it has the largest range of average selling price\n* Stores WI_1, WI_2, WI_3, TX_3, CA_2 has sales of costly items\n* Stores TX_1, TX_2, CA_1, CA_3, CA_4 has sales of both costly and cheap items","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sell_prices.groupby(['year','cat_id']).mean().unstack('cat_id').boxplot(figsize=(12,3), vert=False)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* HOUSEHOLD is the most expensive category with highest avg. selling price and close range\n* FOODS is the cheapest category with lowest avg. selling price and close range","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sell_prices.groupby(['year','dept_id']).mean().unstack('dept_id').boxplot(figsize=(12,4), vert=False)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* In the HOBBIES category the HOBBIES_1 sub category is expensive while HOBBIES_2 sub category is the cheapest\n* In the HOUSEHOLD category the HOUSEHOLD_2 sub category is expensive while HOUSEHOLD_1 sub category is cheap\n* In the FOODS category the FOODS_2 sub category is expensive followed by FOODS_3 and FOODS_1","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Variation of Selling Prices across different Time Frames ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sell_prices.groupby(['week','year']).mean().unstack('week').boxplot(figsize=(10,12), vert=False)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The avg selling price is higher at the begining and latter part of the year\n* In middle of the year i.e., between the week 15 and week 28, there is dip in the avg selling price\n* The range of selling price is closest in the weeks 5 to 8 and 25 to 30","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sell_prices.groupby(['week','year']).mean().unstack('year').boxplot(figsize=(14,5), vert=False)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is a consolidation in the selling price, as with increase in year the avg selling price is also increasing. Also, the range of selling is decreasing with increase in year. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Events","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"calendar.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"event_1 = pd.merge(calendar[['date','weekday','month','year','d']], \n                   calendar[['d','event_name_1','event_type_1']].dropna(), on='d')\nprint(event_1)\nprint(\"There are 162 events in the calender dateset\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"event_1.groupby(['event_type_1','year'])['event_name_1'].size().unstack(\n    'event_type_1').iplot(kind='barh', title='Event Type 1')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are on average 6 cultural, 10 national, 10 Religious and 3 sporting events in a year.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"event_1.groupby(['month','year'])['event_name_1'].size().unstack('year').iplot(\n    kind='bar', title='Events 1 by Month')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Month 2 i.e., February has the highest number of events and months 8 & 9 i.e., August & September has the lowest number of events.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"event_1.groupby(['weekday','year'])['event_name_1'].size().unstack('year').iplot(\n    kind='bar', title='Event 1 by day of the Week')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Most of the events are organized on Sunday and Monday\n* Friday and Saturday has the least number of the events","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"event_2 = pd.merge(calendar[['date','weekday','month','year','d']], \n                   calendar[['d','event_name_2','event_type_2']].dropna(), on='d')\nprint(event_2)\nprint(\"There are 5 Event 2 in the calendar dataset\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"event_2.groupby(['event_type_2','year'])['event_name_2'].size().unstack(\n    'event_type_2').iplot(kind='barh', title='Event 2')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* 2011, 2013, 2014 & 2016 had one cultural event 2\n* 2014 had one religious event 2","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"event_2.groupby(['month','year'])['event_name_2'].size().unstack('year').iplot(\n    kind='barh', title='Event 2 by month of the Year')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"event_2.groupby(['weekday','year'])['event_name_2'].size().unstack('year').iplot(\n    kind='barh', title='Event 2 by day of the week')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All the events were on Sunday every year.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.merge(calendar[['date','weekday','month','year','d']], \n                   calendar[['d','event_name_1','event_type_1','event_name_2','event_type_2']].dropna(), on='d')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the calendar dataset there are 5 days at which both event 1 and event 2 fell on the same day.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# SNAP Days","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"snap = pd.merge(calendar[['date','weekday','month','year','d']], \n                   calendar[['d','snap_CA','snap_TX','snap_WI']].loc[~(calendar[['snap_CA','snap_TX','snap_WI']]==0).all(axis=1)], \n                on='d')\nsnap","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"snap_CA = snap.groupby(['snap_CA','year']).size()[1].to_frame().reset_index().rename(columns={0:'CA'})\nsnap_TX = snap.groupby(['snap_TX','year']).size()[1].to_frame().reset_index().rename(columns={0:'TX'})\nsnap_WI = snap.groupby(['snap_WI','year']).size()[1].to_frame().reset_index().rename(columns={0:'WI'})\n\npd.merge(pd.merge(snap_CA,snap_TX,on='year'),snap_WI,on='year').set_index(\n    'year').iplot(kind='barh', title='SNAP Days')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"snap_CA_1 = snap.groupby(['snap_CA','month']).size()[1].to_frame().reset_index().rename(columns={0:'CA'})\nsnap_TX_1 = snap.groupby(['snap_TX','month']).size()[1].to_frame().reset_index().rename(columns={0:'TX'})\nsnap_WI_1 = snap.groupby(['snap_WI','month']).size()[1].to_frame().reset_index().rename(columns={0:'WI'})\n\npd.merge(pd.merge(snap_CA_1,snap_TX_1,on='month'),snap_WI_1,on='month').set_index(\n    'month').iplot(kind='barh', title='SNAP Days by month of the year')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"snap.groupby(['snap_CA','month','year']).size()[1].unstack('year').iplot(\n    kind='bar', title='CA SNAP Days by month')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"snap.groupby(['snap_TX','month','year']).size()[1].unstack('year').iplot(\n    kind='bar', title='TX SNAP Days by month')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"snap.groupby(['snap_WI','month','year']).size()[1].unstack('year').iplot(\n    kind='bar', title='WI SNAP Days by month')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 10 SNAP Days every month for all the states. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"snap.groupby(['snap_CA','weekday','year']).size()[1].unstack('year').iplot(\n    kind='barh', title='CA SNAP Days by day of the Week')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"snap.groupby(['snap_TX','weekday','year']).size()[1].unstack('year').iplot(\n    kind='barh', title='TX SNAP Days by day of the Week')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"snap.groupby(['snap_WI','weekday','year']).size()[1].unstack('year').iplot(\n    kind='barh', title='WI SNAP Days by day of the Week')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.merge(calendar[['date','weekday','month','year','d']], \n                   calendar[['d','snap_CA','snap_TX','snap_WI']].loc[\n                       (calendar[['snap_CA','snap_TX','snap_WI']]==1).all(axis=1)], \n                on='d')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Calendar View of SNAP Days in each of the states","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## CA","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"days = list(pd.to_datetime(calendar.date))\nevents = pd.Series(list(calendar.snap_CA), index=days)\n\ncalplot.calplot(events, cmap='RdBu', colorbar=False)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## TX","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"days = list(pd.to_datetime(calendar.date))\nevents = pd.Series(list(calendar.snap_TX), index=days)\n\ncalplot.calplot(events, cmap='RdBu', colorbar=False)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## WI","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"days = list(pd.to_datetime(calendar.date))\nevents = pd.Series(list(calendar.snap_WI), index=days)\n\ncalplot.calplot(events, cmap='RdBu', colorbar=False)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* In all the states SNAP days fall on the first half of the month\n* In all the states SNAP days are organized on same day of the month","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Time Series Analysis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cummulative_sales = df3.transpose().sum().to_frame().rename(columns={0:'cummulative_sales'})\ncummulative_sales.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cummulative_sales.iplot(title='Time Series Plots - Cummulative Sales')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The Time Series plot of the cummulative sales shows that there is an increasing trend in the cummulative sales with seasonality.\n* There is one day every year at which the sale is almost equal to 0, the day is 25th December.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"I am assumung that the trend of the Time Series is linear and thus is using additive decomposition to decompose the Times Series to the components: Level, Trend, Seasonality and Noise for futher analysis.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"result = seasonal_decompose(cummulative_sales[\n    'cummulative_sales'].values, period=7, model='additive')\nplt.rcParams.update({'figure.figsize': (12,8)})\nresult.plot().suptitle('Additive Decomposition', fontsize=22)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The time series has an increasing Trend\n* There is a strong weekly seasonilty","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df3.index = pd.to_datetime(df3.index)\ndf5 = pd.concat([df[['id','item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']].set_index('id'),\n                 df3.groupby(pd.Grouper(freq='1M')).sum().transpose()], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df5.groupby('state_id').sum().transpose().iplot(title='Time Series Plots - Statewise')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The trend for CA & WI is icreasing while that of TX is decreasing\n* CA has the highest fluctuation in sales among the three states","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"result = seasonal_decompose(df.groupby(\n    'state_id').sum().transpose().CA.values, period=7, model='additive')\nplt.rcParams.update({'figure.figsize': (12,8)})\nresult.plot().suptitle('Time Series of CA', fontsize=22)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result = seasonal_decompose(df4.groupby(\n    'state_id').sum().transpose().TX.values, period=7, model='additive')\nplt.rcParams.update({'figure.figsize': (12,8)})\nresult.plot().suptitle('Time Series of TX', fontsize=22)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result = seasonal_decompose(df4.groupby(\n    'state_id').sum().transpose().WI.values, period=7, model='additive')\nplt.rcParams.update({'figure.figsize': (12,8)})\nresult.plot().suptitle('Time Series of WI', fontsize=22)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Trend of WI is increasing at much faster rate than CA & TX\n* There is strong monthly seasonality in CA & TX","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df5.groupby(['state_id','cat_id']).sum().transpose().CA.iplot(title='CA sales by Category')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* There is monthly seasonality in FOODS category\n* The trend of FOODS and HOUSEHOLD is increasing while that of HOBBIES is flat","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df5.groupby(['state_id','cat_id']).sum().transpose().TX.iplot(title='TX sales by Category')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Trend of HOUSEHOLD category is increasing\n* Trend of FOODS category is decreasing\n* Trend of HOBBIES category is flat","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df5.groupby(['state_id','cat_id']).sum().transpose().WI.iplot(title='WI sales by Category')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Trend of all the three categories are increasing.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Thus, except for WI in other states the trend of the categories are either decreasing or flat.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df5.groupby(['state_id','store_id']).sum().transpose().CA.iplot(title='CA sales by Stores')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Trend of CA_1, CA_2 & CA_4 is increasing while the trend of CA_3 is slightly decreasing\n* Trend of CA_2 is increasing rapidly in the recent year in comparison to other stores","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df5.groupby(['state_id','store_id']).sum().transpose().TX.iplot(title='TX sales by Store')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Trend of TX_3 is increasing while that of TX_1 & TX_2 is decreasing\n* Trend of TX_2 is decreasing at a much faster rate than that of TX_1","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df5.groupby(['state_id','store_id']).sum().transpose().WI.iplot(title='WI sales by Store')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Trend of all the three stores are increasing\n* Trend of WI_2 is increasing at much faster rate than that of WI_1 & WI_3","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df5.groupby(['store_id']).sum().transpose().iplot(title='Sales by Stores')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Impact of Events","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df7 = pd.merge(calendar[['date','weekday','month','year']], \n                   pd.concat([df2,df1], axis=1), on='date').set_index('date')\ndf7.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df7.drop(['month'], axis=1).groupby(['weekday','year']).sum().sum(axis=1).unstack(\n    'weekday').iplot(kind='bar', title='Sales by day of the Week')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Saturday & Sunday has the highest sales in any week\n* Tuesday, Wednesday & Thursday has the lowest sales in any week","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df7.drop(['weekday'], axis=1).groupby(['month','year']).sum().sum(axis=1).unstack(\n    'month').iplot(kind='bar', title='Sales by Month')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* June, July, August & September has the highest sales in a year\n* January, February & December has the lowest sales in a year","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Sales by day of the year","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"days = list(pd.to_datetime(cummulative_sales.index))\nevents = pd.Series(list(cummulative_sales.cummulative_sales), index=days)\n\ncalplot.calplot(events, cmap='CMRmap')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* There is a high weekly correlation in sales\n* In a month most of the sales happens in latter part of the month\n* There is one day every year when sales is 0, the day is 25th Dec","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cummulative_sales.iplot(kind='hist')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* From the histogram we can see that there is only 5 days in the dataset which is oulier","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Event 1 & Event 2","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cummulative_sales_1 = pd.merge(calendar, cummulative_sales.reset_index(), on='date')\ncummulative_sales_1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cummulative_sales_1.groupby(['weekday']).mean()['cummulative_sales'].plot(kind='barh', figsize=(12,6))\nplt.axvline(x=cummulative_sales_1.cummulative_sales.mean(), color='k', linestyle='--')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Most of the sales happens on Saturday & Sunday in a week","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cummulative_sales_2 = cummulative_sales_1.groupby(['weekday']).mean()['cummulative_sales'].reset_index()\ncummulative_sales_2.loc[cummulative_sales_2.weekday=='Saturday']['cummulative_sales'].values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Average sales on Saturday is 41,546.894","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cummulative_sales_2.loc[\n    (cummulative_sales_2.weekday=='Saturday') | (cummulative_sales_2.weekday=='Sunday')].mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Average sales on Saturday & Sunday is 41,338.458.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* Since most of the sales happens on weekends, so to see the effect of the events we needs to map the sale of all the events that happens on Monday to weekends","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"event_days_sales = cummulative_sales_1[\n    ((cummulative_sales_1.event_name_1.notnull()) | (cummulative_sales_1.event_name_2.notnull()))]\ncummulative_sales_1[\"weekend_precede_event\"] = np.nan\n\ndef update_weekend_precede_event(week_e,wday,e1,e2):\n    e2 = '_' + e2 if type(e2) == str else ''\n    drift = e1 + e2\n    if wday == 1:\n        cummulative_sales_1.loc[\n            (cummulative_sales_1['wm_yr_wk']==week_e)&(cummulative_sales_1[\n                'wday']==1),\"weekend_precede_event\"] = drift\n    else:\n        cummulative_sales_1.loc[\n            (cummulative_sales_1[\n                'wm_yr_wk']==week_e)&((cummulative_sales_1['wday']==1)|(cummulative_sales_1[\n                'wday']==2)),\"weekend_precede_event\"] = drift\n        \n_ = event_days_sales.apply(lambda row : update_weekend_precede_event(row[\n    'wm_yr_wk'],row['wday'],row['event_name_1'], row['event_name_2']),axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cummulative_sales_1.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Plotting the sales of events","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cummulative_sales_1.groupby(['weekend_precede_event','weekday'])[\n    'cummulative_sales'].mean().unstack('weekday').mean(axis=1).sort_values(ascending = False).plot(kind='bar', figsize=(16,6))\nplt.axhline(y=cummulative_sales_2.loc[\n    (cummulative_sales_2.weekday=='Saturday') | (\n        cummulative_sales_2.weekday=='Sunday')].mean().values, color='black', linestyle='--')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that on 15 of the events, the sales is greater than the average sales. Thus, is a spike in sales on 15 events.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Effect of SNAP days","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"snap_1 = pd.merge(snap, cummulative_sales, on='date')\nsnap_1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"snap_CA_1 = pd.merge(snap[['date','snap_CA']], df4.groupby([\n    'state_id']).sum().T['CA'].reset_index().rename(\n    columns={'index':'date'}), on='date').groupby(['snap_CA']).mean().reset_index()\nsnap_CA_1.columns = ['snap', 'CA_sales']\nsnap_TX_1 = pd.merge(snap[['date','snap_TX']], df4.groupby([\n    'state_id']).sum().T['TX'].reset_index().rename(\n    columns={'index':'date'}), on='date').groupby(['snap_TX']).mean().reset_index()\nsnap_TX_1.columns = ['snap', 'TX_sales']\nsnap_WI_1 = pd.merge(snap[['date','snap_WI']], df4.groupby([\n    'state_id']).sum().T['WI'].reset_index().rename(\n    columns={'index':'date'}), on='date').groupby(['snap_WI']).mean().reset_index()\nsnap_WI_1.columns = ['snap', 'WI_sales']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.merge(pd.merge(snap_CA_1,snap_TX_1, on='snap'),snap_WI_1, on='snap').set_index('snap').T.plot(\n    kind='bar', figsize=(10,8), title='Snap Days effect')\nplt.axhline(y=df4.groupby(['state_id']).sum().mean(axis=1).to_frame().T.CA.values, color='red', linestyle='--')\nplt.text(0,df4.groupby(['state_id']).sum().mean(axis=1).to_frame().T.CA.values,'Average sales in CA', size=14)\nplt.axhline(y=df4.groupby(['state_id']).sum().mean(axis=1).to_frame().T.TX.values, color='k', linestyle='--')\nplt.text(0.5,df4.groupby(['state_id']).sum().mean(axis=1).to_frame().T.TX.values,'Average sales in TX', size=14)\nplt.axhline(y=df4.groupby(['state_id']).sum().mean(axis=1).to_frame().T.WI.values, color='blue', linestyle='--')\nplt.text(1.5,df4.groupby(['state_id']).sum().mean(axis=1).to_frame().T.WI.values,'Average sales in WI', size=14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* There is a spike in sales on SNAP days at all the sales.\n* WI has highest increase in SNAP days compared to CA & TX","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Highly Influenced by : https://www.kaggle.com/anirbansen3027/m5-forecasting-exhaustive-eda-beginner","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}