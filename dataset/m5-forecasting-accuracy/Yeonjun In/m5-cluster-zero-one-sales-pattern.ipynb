{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Cluster zero / one sales pattern\n\nHi all :)\n\nI am novice at Data Science and analysis. If you have any suggestion or advice, Please comment it. It will relly help me!!\n\nI have tried to make some clusters with zero / one sales pattern based on the concept of **Jaccard similarity**, inspired by [this amazing kernel](https://www.kaggle.com/jpmiller/grouping-items-by-stockout-pattern)\n\nThe output of clusters for all ids might (i think) lead to feature engineering or new cross validation strategy or another new insights!\n\n\nThe logic is\n\n1. Level 1 clustering using kernel density estimation based on missing values\n    - A wide range of missing values count for all ids => To compare zero / one sales pattern, grouping ids which have similar missing distribution into one cluster.\n\n2. Substitute original sales value\n    - Substitute nan for 0, 0 for -1, values > 0 for 1.\n    \n3. Level 2 clustering\n    - Hierarchy Clustering level 1 clusters into more groups.\n    \n    \nThen, enjoy kaggle ~!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom tqdm import tqdm\nfrom scipy.cluster.hierarchy import linkage, fcluster\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I used `grid_part_1.pkl` file from [this great kernel](https://www.kaggle.com/kyakovlev/m5-simple-fe). ","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"grid_df = pd.read_pickle('/kaggle/input/m5-simple-fe/grid_part_1.pkl')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Transform to original data set form","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_df = grid_df[['id','d','sales']].pivot(index='id',columns='d').reset_index()\nids = pd.DataFrame(grid_df['id'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_df = grid_df['sales'].iloc[:,:1913]\n\n# values > 0 for 1, missing for nan, 0 for -1\ngrid_df = pd.DataFrame(np.where(grid_df.isnull(),np.nan,\n                                np.where(grid_df > 0, 1, -1)))\n\ngrid_df.columns = [f'd_{i}' for i in range(1,1914)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d1_peak = grid_df.notnull().sum(axis=1)\ncluster = d1_peak.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plot kde plot of the number of missing values over all ids\n\nI set threshold below based on Heuristic. You can change it.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,6))\nsns.kdeplot(d1_peak)\nplt.plot([925,925],[0,0.0030]); plt.plot([1300,1300],[0,0.0030]); plt.plot([1700,1700],[0,0.0030])\nplt.text(x=700,y=0.002,s='Cluster 1'); plt.text(x=1020,y=0.002,s='Cluster 2')\nplt.text(x=1420,y=0.002,s='Cluster 3'); plt.text(x=2000,y=0.002,s='Cluster 4')\nplt.title('# of Nan distribution')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"c1_mask = (d1_peak <= 925)\nc2_mask = (d1_peak > 925) & (d1_peak <= 1300)\nc3_mask = (d1_peak > 1300) & (d1_peak <= 1700)\nc4_mask = (d1_peak > 1700)\n\ncluster[c1_mask] = 1\ncluster[c2_mask] = 2\ncluster[c3_mask] = 3\ncluster[c4_mask] = 4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_df['cluster'] = cluster\n\n# missing values for 0\ngrid_df = grid_df.fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_df['cluster'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see on above graph, cluster 4 consists an half of all ids.\n\nSo, I tried to level 2 group only for cluster 4.\n\nBefore level 2 cluster, i will show you very simple example for how Jaccard similarity is calculated.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Jaccard similarity calculation\n\nIf there are two binary feature A,B\n\n`A = [1,0,0,0,1,0]`\n\n`B = [1,1,0,0,0,0]`\n\nLet's calculate Jaccard similarity","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"A = np.array([1,0,0,0,1,0])\nB = np.array([1,1,0,0,0,0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.sum(A == B) / len(A)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 4 same values over 6 on each index.\n\nSo, Jaccard similarity between A and B becomes 0.67\n\nSo simple and intuitive definition. Then Lets move onto level 2 cluster","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"I tried two similarity, Jaccard and the score similar to Jaccard.\n\nLets take a look at these examples.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 1. Jaccard similarity clustering","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cluster_df = grid_df[grid_df['cluster'] == 1]\ncluster_array = cluster_df.values\ncluster_array = np.where(cluster_array == 0, np.nan, cluster_array)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"length = cluster_array.shape[0] \nfor i in tqdm(range(0, int(length/10))):\n    for j in range(i, length):\n        np.sum(cluster_array[i,:-1] == cluster_array[j,:-1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It takes 90 sec for 500 instances to calculate. \n\ncluster 1,2,3 have about 5000 instances respectively, cluster 4 has 15000 instances.\n\nIt will takes about an hour only to calculate distance matrix. \n\nSo, i found out another way.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 2. the index similar to Jaccard \n\n$\nIndex = \\frac{sum(A==B)~ -~ sum(A!=B)}{len(A)}\n$\n\nI used this index by matrix inner product.\n\nAs i noted above, cluster 4 has 15000 instances, so i made level 2 cluster only for cluster 4","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def Clustering(cluster_lv1_name, cluster_lv2_num):\n    \n    cluster_df = grid_df[grid_df['cluster'] == cluster_lv1_name]\n    \n    if cluster_lv2_num == 1:\n        print('Pass : Cluster', cluster_lv1_name)\n        \n    else:\n        print('Making dist_matrix : Cluster', cluster_lv1_name)\n        cluster_array = cluster_df.values\n        dist_matrix = np.dot(cluster_array, cluster_array.T)\n\n        ## this part, linkage, takes about 30 minutes.\n        ## If you have another idea for reducing running time,\n        ## Please advise me !\n        Z = linkage(dist_matrix, method='ward')\n        cluster_num = fcluster(Z, t=cluster_lv2_num, criterion='maxclust')\n        cluster_df['cluster'] = cluster_df['cluster'].astype(str) + '_' + cluster_num.astype(str)\n\n    return cluster_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plan_clustering = {\n    #cluster_lv1_name : how many cluster_lv2 to make\n    1:1,\n    2:1,\n    3:1,\n    4:4\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%time\ndf_list = list()\nfor lv1, lv2 in plan_clustering.items():\n    df_name = f'cluster_{lv1}_df'\n\n    cluster_df = Clustering(cluster_lv1_name = lv1, cluster_lv2_num = lv2)\n    globals()[df_name] = cluster_df\n    \n    df_list += [cluster_df['cluster']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cls_total = pd.concat(df_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cluster_df = pd.concat([ids, cls_total], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cluster_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cluster_df['cluster'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cluster_df.to_pickle('zero_one_cluster.pkl')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}