{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction\n\nIn this competition, the fifth iteration, we will use hierarchical sales data from Walmart, the worldâ€™s largest company by revenue, to forecast daily sales for the next 28 days.\n    The data, covers stores in three US States :\n    \n    -California,\n    -Texas,\n    -Wisconsin\n    \n   and includes item level:\n   \n    -department,\n    -product categories,\n    -store details. \n    \n   In addition, it has explanatory variables such as:\n   \n    -price,\n    -promotions,\n    -day of the week,\n    -special events. \n    \n   Together, this robust dataset can be used to improve forecasting accuracy.\n   \n   ## Datasets\n   \n   \n\n**calendar.csv** - Contains information about the dates on which the products are sold.\n\n**sales_train_validation.csv** - Contains the historical daily unit sales data per product and store [d_1 - d_1913]\n\n**sell_prices.csv** - Contains information about the price of the products sold per store and date.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Imports","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loading and defining our data","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"sell_prices = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sell_prices.csv')\nsell_prices.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sell_prices['id'] = sell_prices['item_id'] + '_' + sell_prices['store_id']\nsell_prices['sell_price'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sales_train_evaluation.csv')\nsales.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"calendar = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/calendar.csv')\ncalendar.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"calendar.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_training = sales.iloc[:,6:]\nsales_training.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Now let's take a  look at our target","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sample_submission.csv')\nsample_submission","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As expected, we will have to prepare a 56 days forecast. First 28 will serve for a validation and the following 28 will remain unknown and will evaluate our final scorein the overall ranking","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Exploration and visualization\nFor that, we will first start with a simple exploration.\nSo let's plot some of our time series to see what are we gonna be dealing with.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rows = [0, 42, 1024, 10024]\nfig, axes = plt.subplots(nrows=len(rows), ncols=1, figsize=(10,15))\nfor ax, row in zip(axes, rows):\n    sales_training.iloc[row].plot(ax=ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, the data seems to be changing on a rapid daily basis. \nInterestingly enough, as promissed in the competition description, there seems to be an occasional **intermittency** - that is - the data sometimes stops changing and just stays at zero.\n\nWe will look into this later, for now, let's take a one more look, this time at the average monthly sales.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rows = [0, 42, 1024, 10024]\nfig, axes = plt.subplots(nrows=len(rows), ncols=1, figsize=(10,15))\nfor ax, row in zip(axes, rows):\n    sales_training.iloc[row].rolling(30).mean().plot(ax=ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing and data engineering","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"So far, no strong trends differ those series. But to make sure that we can efficiently train on all time series simultaneously, we can 'integrate' them by making them all stationery, and then normalize them.\n\nWe will do that by **discrete-differentiating** each and every one of them untill thy **Augmented Dickey-Fueller test** will give us an 95% chance that each series has no trend left!\n\nTo keep those series still representing out data, we will only put all those levels of differentiation and scales aside for the sake of forecasting and then integrate and scale them back up for a final result.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tsa.stattools import adfuller","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"D = []\n\ntry:\n    stationarity_differences = pd.read_csv('/kaggle/input/stationary-differences/stationarity_differences.csv').iloc[:,0]\nexcept FileNotFoundError:\n    for index, row in sales_training.iterrows():\n        d = 0\n        p_val = adfuller(row, autolag='AIC')[1]\n        while p_val > 0.05:\n            d += 1\n            row = row.diff()[1:]\n            p_val = adfuller(row, autolag='AIC')[1]\n        D.append(d)\n    pd.Series(D).to_csv('./stationarity_differences.csv', index=False)\n    stationarity_differences = pd.read_csv('/kaggle/input/stationary-differences/stationarity_differences.csv').iloc[:,0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stationarity_differences.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_training.iloc[12791].plot()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Well.. we definitely should address that intermittency at some point.. But other that that, it looks pretty good.\nLet's differenciate our data one single time (we will ignore this single anomaly, it's non-statonarity will not do us nearly any charm), and then scale them for their final form","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"stationary_train_sales = np.diff(sales_training.values, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler(with_mean=False)\nscaler.fit(stationary_train_sales.T)\nX_train = scaler.transform(stationary_train_sales.T).T\nscales = scaler.scale_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"calendar","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_normalized = calendar[['wm_yr_wk','d']].iloc[:1941]\nsales_normalized = pd.DataFrame(X_train, columns=sales_normalized['d'][1:])\nsales_normalized.insert(0, 'id', sales['item_id'] + '_' + sales['store_id'])\nsales_normalized","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rows = [0, 42, 1024, 10024]\nfig, axes = plt.subplots(nrows=len(rows), ncols=1, figsize=(10,15))\nfor ax, row in zip(axes, rows):\n    sales_normalized.iloc[row, 1:].plot(ax=ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**And there we have it!**\nEvery row is now stationery and scaled to a common point of reference! \n\nBut before we go any further, let's check what happens if we try to bring it back.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_normalized","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rows = [42, 10024]\nfig, axes = plt.subplots(nrows=len(rows), ncols=1, figsize=(10,15))\nfor ax, row in zip(axes, rows):\n    integrated_series = np.cumsum(sales_normalized.iloc[row, 1:]*scales[row])\n    c = sales_training.iloc[row, 0]\n    integrated_series = pd.Series(integrated_series + c).shift(1)\n    integrated_series[:100].plot(ax=ax, style='r--', legend=True, label='re-integrated')\n    sales_training.iloc[row][:100].plot(ax=ax, legend=True, label='original')\n    total_numerical_error = np.abs(np.array(pd.Series(integrated_series)[1:].to_numpy() - sales_training.iloc[row,1:-1].to_numpy())).sum()\n    ax.set_title('Total numerical error: {:.2f}'.format(total_numerical_error))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Great! It can be brought back to original function without nearly any numerical loss.\n\n\nNow we just have to addres the intermittency and we are good to go! Let's start with analysing item-store relations in our sell_price data, maybe when the products were out of the store, their price was 0 or missing.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(sell_prices['id'].value_counts(), kde=False, axlabel='number of weeks the product was priced on')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Interesting, it seems like item-store relations differ in amount of days they were priced on. That might be responsible for our intermittency.\n\n**Unfortunately** after some exploration and research into sell_price data, we had conclude that it was not very helpfull for figuring out intermittency. Apparently, sometimes the price for a product was just missing in the database, sometimes it was there even though not a single unit was sold in weeks. We had to figure out a different way to recognize this itermittency.\n\nLet's try making subtitute assumption: \n\n**if the amount of items sold stayed as 0 for at least 2 weeks, we treat this part as intermittent**. That way, we will be able to very quickly evaluate which days not to consider for training. Even if that means ignoring some products that were being on the shelves, but rarely bought.. well we can't really do everythng here given the time we have, and that would be more in 'rare event analysis' category so the best model for that would probably be a standard exponential distribution anyway.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_two_week_sum = sales_training.rolling(14, axis=1).sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in range(13):\n    sales_two_week_sum.iloc[:, col] = sales_two_week_sum.iloc[:, 13]\n    \nis_off_the_shelf = sales_two_week_sum == 0\n#to the days when the products were off for 14 last days we add those 14 days\nis_off_the_shelf = is_off_the_shelf | is_off_the_shelf.shift(-13, axis=1)\nis_on_the_shelf = is_off_the_shelf == False\n# True/False to 1/0\n# is_on_the_shelf = is_on_the_shelf.astype('int')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"is_on_the_shelf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Great! Let's see how our data works when we add this on-the-shelf information","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rows = [0, 42, 1024, 10024]\nfig, axes = plt.subplots(nrows=len(rows), ncols=1, figsize=(10,15))\nfor ax, row in zip(axes, rows):\n    shelf = pd.DataFrame(is_on_the_shelf.iloc[row])\n    shelf.columns = ['is_on_the_shelf']\n    shelf['sold'] = sales_training.iloc[row]\n    shelf = shelf.reset_index()\n    shelf.drop('index', inplace=True, axis=1)\n    shelf[shelf['is_on_the_shelf'] == True]['sold'].plot(legend=True, label='on shelf', ax=ax)\n    shelf[shelf['is_on_the_shelf'] == False]['sold'].plot(style='o', legend=True, label='not on shelf', ax=ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature engineering\n\nNow that we have or data cleaned up, we can finally go to the next stage of our project - feature engineering!\n\nSo, the plan is - since data looks pretty tidy already we will simply encode all the categorical features, match them to the data and prepare a sort of a pipeline to get the series to generate 'train_x' and 'train_y'.\n\nLet's pick features one by one, examine them and choose the suitable encoding.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sales['dept_id'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales['cat_id'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales['state_id'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder = OneHotEncoder()\ndept_encoded = encoder.fit_transform(sales['dept_id'].values.reshape(-1,1))\ncat_encoded = encoder.fit_transform(sales['cat_id'].values.reshape(-1,1))\nstate_encoded = encoder.fit_transform(sales['state_id'].values.reshape(-1,1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*sale_prices* table was quite in the wrong format, so we had to do some merging and pivoting to match the format of the rest of our data. All that needed a lot of RAM and time, so it was done on a separate kernel, that can be found [here](https://www.kaggle.com/patrykradon/train-price-parser/notebook). \n\nAnother problem was that there was about 20% missing data there. Thats not ideal, especially that as we have mentioned - usually products were actally sold, despite their missing price.\nBut since it is that way and we have already taken care of products being off the shelf, let's just make another assumption:\n\n**If someone did not care to note the price down, it probably did not change**, so all that we had to do was to impute the null values with those from before they went missing. If the beggining was missing we would fill it with one from after it first appeared. \n\nSince this other notebook was for parsing anyway, we did Pandas ffil() followed by bfill() before saving it. Of course we had to differentiate it once, because we have to be consistent about differentiation when it comes to time series.\n\nNow all we have to do is fetch ready parsed data from there.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_prices = pd.read_csv('/kaggle/input/train-price-parser/train_prices.csv')\n#sort IDs to match our order of IDs\ntrain_prices['id'] = train_prices['id'].astype(\"category\")\ncorrect_order = sales['item_id'] + '_' + sales['store_id']\ntrain_prices['id'].cat.set_categories(correct_order, inplace=True)\ntrain_prices = train_prices.sort_values([\"id\"]).reset_index().drop(columns=['index'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('train_prices - observed {:.2f}% missing values'.format(train_prices.isnull().sum(axis=1).mean()/1970 * 100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.impute import SimpleImputer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imputer = SimpleImputer(strategy='constant',fill_value='no_event')\nimputed_calendar_primary = imputer.fit_transform(calendar['event_name_1'].to_numpy().reshape(-1,1))\nimputed_calendar_secondary = imputer.fit_transform(calendar['event_name_2'].to_numpy().reshape(-1,1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imputed_calendar = np.hstack((imputed_calendar_primary,imputed_calendar_secondary))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# a quick note - this has for some reason already beed 'differenciated', by which a mean the holidays lasting for few days\n# are denoted as beggining and end of the holliday\nencoder = OneHotEncoder()\ncalendar_encoded = encoder.fit_transform(imputed_calendar)\n\n# the line meaning that no event happens dubled so we throw one out\ncalendar_encoded = calendar_encoded[:,:-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# never forget to equally differentiate every time series!\nis_on_the_shelf_diff = is_on_the_shelf.diff(axis=1).iloc[:,1:]\nis_on_the_shelf_diff = is_on_the_shelf_diff.astype('int')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train/test generator","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"In total, with the information we have gathered, we can plan out what our algorithm will try to do:\n\n* t - any point in time where our observations take place\n* p - amount of days that we will try to predict\n* k - amount of days before the time window we want to forecast used by our algorithm \n\n## Intermittency\n\nNow, we do not have a reliable data about products being on the shelf or not, so we will have to use our own estimate made by us in a previous section. Normally, it would be up for client to establish if the productss will be on the shelves or not during the forecasted time, but since it is a competiton we will make another asumption:\n\n**if at the end of known period the produt was off the shelf, we predict that it will be off the shelf for the next p days**\n\n## Algorithm\n\n(on purple, we have coloured the <span style=\"color:purple\">facts</span>, so the things we know about the values we try to predict.) \n\n**algorithm** <- sales_normalized(t : t+k), sales_normalized(t-365+k : t-365+k+p), <span style=\"color:purple\">is_on_the_shelf(t+k : t+k+p), calendar_encoded(t+k : t+k+p), dept_encoded(t+k : t+k+p), cat_encoded(t+k : t+k+p), state_encoded(t+k : t+k+p)</span> \n\n\n**algorithm** -> sales_training(t+k : t+k+p) * <span style=\"color:purple\">is_on_the_shelf(t+k : t+k+p)</span>\n\n\nKeeping all that in mind, let's build a generator that will produce the input data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class M5_SeriesGenerator:\n    def __init__(self):\n        self.day_zero = 1941\n        self.max_rows = 30490\n        self.rows_remaining = np.arange(self.max_rows)\n        \n    def reset(self):\n        self.rows_remaining = np.arange(self.max_rows)\n        \n    def next_batch(self, in_points=30, out_points=3, batch_size=10):\n        X_batch = []\n        X_past_batch = []\n        scale_batch = []\n        c_batch = []\n        facts_batch = []\n        y_batch = []\n        \n        for _ in range(batch_size):\n            if self.rows_remaining.shape[0] == 0:\n                return False, (None, None)\n        \n            row = np.random.randint(self.rows_remaining.shape[0])\n            self.rows_remaining = np.delete(self.rows_remaining, row)\n            X_train_start = self.day_zero-366-in_points\n            X_prev_year_start = self.day_zero-2*365\n\n            while is_on_the_shelf.iloc[row, X_train_start+in_points] == False:\n                if self.rows_remaining.shape[0] == 0:\n                    return False, (None, None)\n                row = np.random.randint(self.rows_remaining.shape[0])\n                self.rows_remaining = np.delete(self.rows_remaining, row)\n\n            Xsales_train = sales_normalized.iloc[row, X_train_start+1:X_train_start+in_points+1].values.astype(np.float32)\n            Xsales_prev_year = sales_normalized.iloc[row, X_prev_year_start:X_prev_year_start+out_points].values.astype(np.float32)\n\n            Y_train_start = X_train_start+in_points\n            Yprices_train = train_prices.iloc[row, Y_train_start+2:Y_train_start+out_points+2].values.astype(np.float32)\n            Yevents_train = calendar_encoded[Y_train_start+1:Y_train_start+out_points+1, :].toarray().astype(int)\n            Ydept_train = np.tile(dept_encoded[row].toarray().astype(int),(out_points,1))\n            Ycat_train = np.tile(cat_encoded[row].toarray().astype(int),(out_points,1))\n            Ystate_train = np.tile(state_encoded[row].toarray().astype(int),(out_points,1))\n            Ysales_train = sales_training.iloc[row, Y_train_start+1:Y_train_start+out_points+1].values.astype(int).flatten()\n            \n            Yfacts = np.hstack((Yprices_train.reshape(-1, 1), Yevents_train, Ydept_train, Ycat_train, Ystate_train))\n            integral_constant = sales_training.iloc[row, X_train_start+in_points]\n            scale = scales[row]\n            \n            X_batch.append(Xsales_train.reshape(-1, 1))\n            X_past_batch.append(Xsales_prev_year.reshape(-1, 1))\n            scale_batch.append(scale)\n            c_batch.append(integral_constant)\n            facts_batch.append(Yfacts)\n            y_batch.append(Ysales_train)\n        return True, ((np.asarray(X_batch), np.concatenate((np.asarray(X_past_batch), np.asarray(facts_batch)), axis=2), np.asarray(scale_batch), np.asarray(c_batch)), np.asarray(y_batch))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def eval_series_data_gen(in_points = 120, out_points=28, end_of_data=1913, max_row=30490):    \n    row = 0\n    while row < max_row:\n        X_batch = []\n        X_past_batch = []\n        scale_batch = []\n        c_batch = []\n        facts_batch = []\n        y_batch = []\n        X_train_start = end_of_data-1-in_points\n        X_prev_year_start = end_of_data-365\n        \n        if is_on_the_shelf.iloc[row, X_train_start+in_points] == False:\n            row += 1\n            yield False, None\n        else:\n            Xsales_train = sales_normalized.iloc[row, X_train_start+1:X_train_start+in_points+1].values.astype(np.float32)\n            Xsales_prev_year = sales_normalized.iloc[row, X_prev_year_start:X_prev_year_start+out_points].values.astype(np.float32)\n\n            Y_train_start = X_train_start+in_points\n            Yprices_train = train_prices.iloc[row, Y_train_start+2:Y_train_start+out_points+2].values.astype(np.float32)\n            Yevents_train = calendar_encoded[Y_train_start+1:Y_train_start+out_points+1, :].toarray().astype(int)\n            Ydept_train = np.tile(dept_encoded[row].toarray().astype(int),(out_points,1))\n            Ycat_train = np.tile(cat_encoded[row].toarray().astype(int),(out_points,1))\n            Ystate_train = np.tile(state_encoded[row].toarray().astype(int),(out_points,1))\n            Ysales_train = sales_training.iloc[row, Y_train_start+1:Y_train_start+out_points+1].values.astype(int).flatten()\n\n            Yfacts = np.hstack((Yprices_train.reshape(-1, 1), Yevents_train, Ydept_train, Ycat_train, Ystate_train))\n            integral_constant = sales_training.iloc[row, X_train_start+in_points]\n            scale = scales[row]\n\n            X_batch.append(Xsales_train.reshape(-1, 1))\n            X_past_batch.append(Xsales_prev_year.reshape(-1, 1))\n            scale_batch.append(scale)\n            c_batch.append(integral_constant)\n            facts_batch.append(Yfacts)\n            y_batch.append(Ysales_train)\n            row += 1\n            yield True, (np.asarray(X_batch), np.concatenate((np.asarray(X_past_batch), np.asarray(facts_batch)), axis=2), np.asarray(scale_batch), np.asarray(c_batch))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model selection\n\nNow that we have our inputs and desired outputs, it is time to build a model that will learn to connect these two.\n\nFor this project we will use  **RNN layers with GRU memory units** for the time series inputs, along with auxhilary inputs for our facts. All that will go through one **final dense layer** hopefully giving us complex enough model to succesfully forecast the 56 following days.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import BatchNormalization","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class M5_Net(keras.Model):\n    def __init__(self, input_timesteps, output_timesteps, batch_size=1):\n        super(M5_Net, self).__init__()\n        self.input_timesteps = input_timesteps\n        self.output_timesteps = output_timesteps\n        self.batch_size = batch_size\n\n        self.gru1 = tf.keras.layers.GRU(32, return_sequences=True)\n        self.gru1a = tf.keras.layers.GRU(64, return_sequences=True)\n        self.gru2 = tf.keras.layers.GRU(64, return_sequences=True)\n        self.gru2a = tf.keras.layers.GRU(32, return_sequences=True)\n        self.gru_out = tf.keras.layers.GRU(1, return_sequences=True)\n        self.dense1 = keras.layers.Dense(self.output_timesteps, activation=\"selu\", kernel_initializer=\"lecun_normal\")\n        \n    def call(self, input_data):\n        series_data, historical_data, scale, integral_constant = input_data\n        \n        x = BatchNormalization()(self.gru1(series_data))\n        x = BatchNormalization()(self.gru1a(x))\n        x = tf.reshape(x, [self.batch_size, -1])\n        x = BatchNormalization()(self.dense1(x))\n        x = tf.reshape(x, [self.batch_size, -1, 1])\n        x = tf.concat([x,\n                       historical_data,\n                       np.expand_dims(np.tile(integral_constant, (self.output_timesteps,1)).T, axis=2),\n                       np.expand_dims(np.tile(scale, (self.output_timesteps,1)).T, axis=2)\n                      ], axis=2)\n        x = BatchNormalization()(self.gru2(x))\n        x = BatchNormalization()(self.gru2a(x))\n        x = BatchNormalization()(self.gru_out(x))\n        x = tf.reshape(x, [self.batch_size, -1])\n        \n        @tf.function\n        def inverse_normalize(x):\n            sales_pred = tf.transpose(tf.math.multiply(tf.transpose(x), y=scale))\n            sales_pred = tf.math.cumsum(sales_pred, axis=1)\n            sales_pred += np.tile(integral_constant, (self.output_timesteps,1)).T\n            return sales_pred\n        \n        sales_pred = inverse_normalize(x)\n        return sales_pred","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training and tuning the model\n\nIt's time to make a training loop, put on some metrics, initialize our parameters and run our model through a couple of epochs.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from math import sqrt\n\nIN_POINTS = 120\nOUT_POINTS = 28\nBATCH_SIZE = 16\nmodel = M5_Net(input_timesteps=IN_POINTS, output_timesteps=OUT_POINTS, batch_size=BATCH_SIZE)\n\nloss_object = tf.keras.losses.MeanSquaredError()\noptimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n\ndef loss(model, x, y, training):\n    y_ = model(x, training=training)\n\n    return loss_object(y_true=y, y_pred=y_)\n\ndef grad(model, inputs, targets):\n    with tf.GradientTape() as tape:\n        loss_value = loss(model, inputs, targets, training=True)\n    return loss_value, tape.gradient(loss_value, model.trainable_variables)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model.load_weights('/kaggle/input/checkpoints2/croc_model2.ckpt')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nM5_series_gen = M5_SeriesGenerator()\nbatch_sequence = [4, 4, 4, 4]\ntraining = []\nVAL_SIZE = 1000\nvalidation = []\nfor epoch in range(len(batch_sequence)):\n    BATCH_SIZE = batch_sequence[epoch]\n    model.batch_size = BATCH_SIZE\n    epoch_loss = []\n    more_data_available, (X_train, y_train) = M5_series_gen.next_batch(in_points=IN_POINTS, out_points=OUT_POINTS, batch_size=BATCH_SIZE)\n    while True:\n        more_data_available, (X_train, y_train) = M5_series_gen.next_batch(in_points=IN_POINTS, out_points=OUT_POINTS, batch_size=BATCH_SIZE)\n        if more_data_available == False:\n            break;\n            \n        loss_value, grads = grad(model, X_train, y_train)\n        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n        clipped_loss = loss_object(y_true=y_train, y_pred=tf.clip_by_value(model(X_train, training=True), clip_value_min=0, clip_value_max=np.inf))\n        epoch_loss.append(sqrt(clipped_loss))\n    training.append(np.array(epoch_loss).mean())\n    epoch_val = []\n    for on, X_val in eval_series_data_gen(in_points=IN_POINTS, out_points=OUT_POINTS, end_of_data=1913, max_row=VAL_SIZE):\n        model.batch_size = 1\n        if on:\n            val = tf.clip_by_value(model(X_val, training=True), clip_value_min=0, clip_value_max=np.inf).numpy().squeeze()\n            epoch_val.append(val)\n        else:\n            epoch_val.append(np.zeros(OUT_POINTS))\n        model.batch_size = BATCH_SIZE\n    validation.append(np.array(epoch_val).mean())\n    print(training[-1], validation[-1])\n    print(f'Epoch {epoch} training loss: {training[-1]}, Epoch {epoch} validation loss: {validation[-1]}')\n    model.save_weights('./croc_model{}.ckpt'.format(epoch))\n    M5_series_gen.reset()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.Series(training).plot(legend=True, label='training')\npd.Series(validation).plot(legend=True, label='validation')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualizing the outcome\n\nFinally, we can visualize our outcome by drawing it along with ground truth","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"N = 16\nmodel.batch_size = N\n_, (X_train, y_train) = M5_series_gen.next_batch(in_points=IN_POINTS, out_points=OUT_POINTS, batch_size=N)\nrows = np.arange(N)\nfig, axes = plt.subplots(nrows=len(rows), ncols=1, figsize=(10,N*3))\ny_ = tf.clip_by_value(model(X_train, training=True), clip_value_min=0, clip_value_max=np.inf)\nfor ax, row in zip(axes, rows):\n    pd.Series(y_train[row]).plot(legend=True, label='ground truth',ax=ax)\n    pd.Series(y_[row]).plot(legend=True, label='forecast',ax=ax)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So to sum up, it seems like it sometimes recognizes some specific behafior it tries to mimic, and sometimes it just classifies the outcome as too random and just outcomes an average.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Final results\n\nWe can now try to predict our validation and evaluation data, throw it at the website to rank us and visualize the outcome. First, let's see how we did - the result will be seend as the score for this notebook.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"IN_POINTS = 120\nOUT_POINTS = 28\nmodel.batch_size = 1\n\nvalidation = []\nevaluation = []\niteration = 0\nfor  on, X in eval_series_data_gen(in_points=IN_POINTS, out_points=OUT_POINTS, end_of_data=1913):\n    if on:\n        val = tf.clip_by_value(model(X, training=True), clip_value_min=0, clip_value_max=np.inf).numpy().squeeze()\n        validation.append(val)\n    else:\n        validation.append(np.zeros(OUT_POINTS))\n        \nfor  on, X in eval_series_data_gen(in_points=IN_POINTS, out_points=OUT_POINTS, end_of_data=1941):\n    if on:\n        ev = tf.clip_by_value(model(X, training=True), clip_value_min=0, clip_value_max=np.inf).numpy().squeeze()\n        evaluation.append(ev)\n    else:\n        evaluation.append(np.zeros(OUT_POINTS))\n\nsample_submission.iloc[:30490, 1:] = validation\nsample_submission.iloc[30490:, 1:] = evaluation\n        \nsample_submission.to_csv('./final_prediction.csv', index=False)\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Summary\n\nThe project turned out to be very edecational in very different ways. We know that training it specificaly for validation days was a little bit of a cheat and we wish we had enough time to make it an general tool for such tasks but for what we had, it turned out very satisfying. ","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}