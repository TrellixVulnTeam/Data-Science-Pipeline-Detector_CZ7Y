{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Time Series Comparison Models (SARIMAX Vs LSTM Vs fbprophet)"},{"metadata":{},"cell_type":"markdown","source":"## Main Objective"},{"metadata":{},"cell_type":"markdown","source":"The aims of this report is to compare different time series algorithms (SAIMAX, fbprophet and LSTM ), As part of this study; These models will be used alongside with time series for forecasting purpose  to assess the pros and cons for each one of them\nNote: This study has been by run by using Modest workstation (specs):  \n**Intel(R) Core ™ - 5i 6400 CPU @ 2.7GHz  \nRAM (8GB)  \nand GPU for LSTM model.**\n"},{"metadata":{},"cell_type":"markdown","source":"## Benefits:"},{"metadata":{},"cell_type":"markdown","source":"In general, all Data scientists can benefit from this report however beginner Data scientist who just started their learning journey can benefit more as this study provides enough adequate details on creating time series models from scratch and by using real dataset. In addition to that, normal users who do not have machine learning workstation equipped by powerful tools like GPU can also benefit from this report."},{"metadata":{},"cell_type":"markdown","source":"## Dataset"},{"metadata":{},"cell_type":"markdown","source":"Source:\nYou can download the dataset from Kaggle website (https://www.kaggle.com/c/m5-forecasting-accuracy/data), the source of this data is https://mofc.unic.ac.cy/m5-competition/\nDataset has been lunched as part of The M5 Competition\nAbout Dataset:\nIt used hierarchical sales data, generously made available by Walmart, starting at the item level and aggregating to that of departments, product categories and stores in three geographical areas of the US: California, Texas, and Wisconsin.\n\nBesides the time series data, it also included explanatory variables such as price, promotions, day of the week, and special events (e.g. Super Bowl, Valentine’s Day, and Orthodox Easter) that affect sales which are used to improve forecasting accuracy.\n"},{"metadata":{},"cell_type":"markdown","source":"## Data Attribute:"},{"metadata":{},"cell_type":"markdown","source":"Dataset contains  4 CSV sheets as shown beknow:\n* calendar.csv - Contains information about the dates on which the products are sold.\n* sales_train_validation.csv - Contains the historical daily unit sales data per product and store [d_1 - d_1913]\n* sample_submission.csv - The correct format for submissions. Reference the Evaluation tab for more info.\n* sell_prices.csv - Contains information about the price of the products sold per store and date.\n* sales_train_evaluation.csv - Includes sales [d_1 - d_1941] (labels used for the Public leaderboard)\n\nEach training dataset has around 30490 rows and 1919 columns, each row represents product, and each column represents day (time series is daily frequency)  \n\nListing all columns is not right option because their number is so high, however and for sake of this report we will shed light on the following columns:\nd_number of days columns which represent days number starting from d_1,d_2,d_3 …d_1913, there are 1913 days which equal to  5 years (End of 2011 to mid of 2016) and two months, type of column is ‘integer\n  \nIn addition to train dataset, Calendar dataset will be used to convert days number to datetime type and to create holidays dataset for fbprophet model \n"},{"metadata":{},"cell_type":"markdown","source":"## Import Libraries "},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nimport statsmodels.api as sm\nimport seaborn as sns\n#import pmdarima as pm\nfrom dateutil.relativedelta import relativedelta\nimport warnings\nwarnings.simplefilter(action='ignore')\nfrom sklearn.metrics import accuracy_score\nimport time\nimport tensorflow as tf\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, SimpleRNN, LSTM, Activation, Dropout\nimport math\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nimport fbprophet  # if you have an issue (on your workstation) with fbprophet on python3.8 i would suggest you to use python3.7 instead\nfrom fbprophet import Prophet\nfrom numpy import median\nimport math\nimport statistics","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data cleaning / feature engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"# reading and inserting the csv sheet into DataFrame\ndf = pd.read_csv('../input/m5-forecasting-accuracy/sales_train_validation.csv')\ndf.head(3)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# No null cells!!\ndf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# reading and inserting the calendar csv sheet into Dataframe, this data will be used as lookup to get the right date for each day in df dataframe\ncalendar = pd.read_csv('../input/m5-forecasting-accuracy/calendar.csv')\ncalendar.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Creating new Dataframe and set the datetime as index**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(columns=['item_id', 'dept_id','cat_id','store_id','state_id'],axis=1, inplace=True) # Drop the unnecessary columns\ndata = df.T  # seting the days as index for new dataframe(Data)\ndata.columns = df['id'] # ading product id column to Data \ndata.index.name = None   # remove the name of index\ndata.drop(index='id',inplace=True) # remove the first row\n# change the format and the freq. of index to datetime format\ndata.index = calendar['date'][0:1913]\ndata.index = pd.to_datetime(data.index)\ndata.index.freq= 'd'\ndata.index","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Since the number of products is so high we will focus on product with highest number of purchasments, Below code will get is this product** "},{"metadata":{"trusted":true},"cell_type":"code","source":"max_value =data.max().to_frame()\nmax_prod = max(data.max())\nmax_value.loc[max_value[0] == max_prod]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# WE noctice that numbe of purchasments is zero for arround first 10 months which looks justified if product was not linched or avaliable yet in the stores at that time thats why this interval will be ecluded from time series \nplt.figure(figsize=(12,8))\nplt.plot(data.index,data['FOODS_3_090_CA_3_validation'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observation:  \nThe first few montths have no data (mabye the product was not luanched at that time or not avaliable in the stores) so exclude these months is good approach**\n"},{"metadata":{},"cell_type":"markdown","source":"***\n\n**Checking the type of time series (stationary or non-statinary)**\n\n***"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using the decompostion method...\nss_decomposition = seasonal_decompose(x=data['FOODS_3_090_CA_3_validation'])\nestimated_trend = ss_decomposition.trend\nestimated_seasonal = ss_decomposition.seasonal\nestimated_residual = ss_decomposition.resid\n\n#fig, ax = plt.subplots(3,1, figsize=(12,8))\nfig, ax = plt.subplots(3,figsize=(12,8) )\n#plt.figure(figsize=(12,5))\n#plt.subplot(3,1,1)\nax[0].plot(data.index[250:],estimated_trend[250:], label='Trend')\nax[0].legend(loc='upper left')\n\n#plt.subplot(3,1,2)\nax[1].plot(data.index[250:300],estimated_seasonal[250:300], label='Seasonal')\nax[1].legend(loc='upper left')\n\n#plt.subplot(3,1,3)\nax[2].plot(data.index,estimated_residual, label='Residual')\nax[2].legend(loc='upper left')\n\nadf, pvalue, usedlag, nobs, critical_values, icbest = adfuller(data['FOODS_3_090_CA_3_validation'])\n\nprint(f'adf is {adf}')\nprint(f'pvalue is {pvalue}')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observations:**\n* p-value is **0.0007317275745300305** which is much less than .05 so we can reject the \"null hypothesis\" which says time series is not stationary\n* Data has weekly sesonality\n* Data does not have constant varaiance.\n\nlet us plot the acf and pacf as well"},{"metadata":{"trusted":true},"cell_type":"code","source":"#plt.subplot(3,1,4)\nplot_acf(data['FOODS_3_090_CA_3_validation'],lags=100, zero=False)\n\n\n#plt.subplot(3,1,5)\nplot_pacf(data['FOODS_3_090_CA_3_validation'], zero=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observations:**\n* All significants lags in Partial Autocorrelation chart are positive and largest one is lag1 so we can tuning p parameter in range(0,3) and q in range(0,2).\n* All significants lags in  Autocorrelation chart are positive and largest one is lag1 so we can tuning P parameter in range(0,3) and Q in range(0,2) in seasonal_orders"},{"metadata":{"trusted":true},"cell_type":"code","source":"# diffrencing method\n\ndata['shift'] = data['FOODS_3_090_CA_3_validation'].shift(7)\ndata['week_lag'] = data['FOODS_3_090_CA_3_validation'] - data['shift']\n\nss_decomposition = seasonal_decompose(x=data['week_lag'].dropna())\nlag_trend = ss_decomposition.trend\nlag_seasonal = ss_decomposition.seasonal\nlag_residual = ss_decomposition.resid\n\n#fig, ax = plt.subplots(3,1, figsize=(12,8))\nfig, ax = plt.subplots(3,figsize=(12,8) )\n#plt.figure(figsize=(12,5))\n#plt.subplot(3,1,1)\nax[0].plot(data.index[7:],lag_trend, label='Trend')\nax[0].legend(loc='upper left')\n\n#plt.subplot(3,1,2)\nax[1].plot(data.index[800:830],lag_seasonal[800:830], label='Seasonal')\nax[1].legend(loc='upper left')\n\n#plt.subplot(3,1,3)\nax[2].plot(data.index[:30],lag_residual[:30], label='Residual')\nax[2].legend(loc='upper left')\n\nadf_dif, pvalue_dif,usedlag, nobs, critical_values, icbest = adfuller(data['week_lag'].dropna())\n\nprint(f'adf is {adf_dif}')\nprint(f'pvalue is {pvalue_dif}')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observation:**  \ndifferencing has reduced the varaince as shown Trend plot after differencing with Trend plot befor differencing, so we expect value of d will be in range of(0,2)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#plt.subplot(3,1,4)\nplot_acf(data['week_lag'][7:],zero=False)\n\n\n#plt.subplot(3,1,5)\nplot_pacf(data['week_lag'][7:],zero=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## SARIMA MODEL"},{"metadata":{},"cell_type":"markdown","source":"**Preparing the training and testing data for all models**"},{"metadata":{"trusted":true},"cell_type":"code","source":"start_interval = 250 # removing the intial interval where no sold item data is shown\ntest_sample= 100\ntrain_SARIMA = data['FOODS_3_090_CA_3_validation'][start_interval:-test_sample].astype('float')\ntest_SARIMA = train_SARIMA[-test_sample:].astype('float')\ntrain_LSTM = data['FOODS_3_090_CA_3_validation'][start_interval:-2*test_sample].astype('float')\ntest_LSTM = data['FOODS_3_090_CA_3_validation'][-2*test_sample:-test_sample].astype('float')\ntrain_prophet = data[['FOODS_3_090_CA_3_validation']][start_interval:-test_sample].astype('float')\ntrue = data['FOODS_3_090_CA_3_validation'][-test_sample:].astype('float')\nprint('full data length', data[['FOODS_3_090_CA_3_validation']][start_interval:].shape)\nprint('train_SARIMA', train_SARIMA.shape)\nprint('test_SARIMA', test_SARIMA.shape)\nprint('train_LSTM', train_LSTM.shape)\nprint('test_LSTM', test_LSTM.shape)\nprint(\"train prophet\", train_prophet.shape)\nprint(\"true \", true.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Tuning  hyparameter by using self developed method aims to reduce the number of iteration in order to enhance the performance of your workstation incase ready grid search tools do not run smoothly**  \nTou can expand the below cell code to review the code"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Custamized grid search\n\n\np_min,p_max = 0,3\nd_min,d_max= 0,2\nq_min,q_max = 0,2\n\nP_min,P_max = 0,3\nD_min, D_max = 0,2\nQ_min,Q_max = 0,2\n\ntrend= 'n' # ['n','c','t','ct']\nm=42\n\np_m = int(round(median([p_min,p_max]),0))\nd_m = int(round(median([d_min,d_max]),0))\nq_m = int(round(median([q_min,q_max]),0))\nP_m = int(round(median([P_min,P_max]),0))\nD_m = int(round(median([D_min,D_max]),0))          \nQ_m = int(round(median([Q_min,Q_max]),0))          \n          \n\nstart_time_init = time.time()\nduppl_order = []\n\nresult = []\n\nif (p_m,d_m,q_m) != (p_max,d_max,q_max):\n    order = (p_m,q_m,d_m)\n    seasonal_order = (P_m,D_m,Q_m,m)\n    \n    \n    if order + seasonal_order in duppl_order:\n        pass\n    else:\n        model2 = sm.tsa.statespace.SARIMAX(train_SARIMA[-400:], order=order, seasonal_order=seasonal_order, trend='n').fit()\n        yhat = round(model2.predict(start = '2016-01-16' , end= '2016-04-24', dynamic=True),0)\n        rmse_m = math.sqrt(sum(pow((yhat - true),2))/len(true))\n        result.append(({'order': (p_m,d_m,q_m),'seasonal_order': seasonal_order,'rmse':rmse_m}))\n        duppl_order.append(order+seasonal_order)\n        print(order,seasonal_order,'rmse_(p_m,q_m,d_m):', rmse_m)\n\n    \n    order = (p_max,d_max,q_max)\n    if order + seasonal_order in duppl_order:\n        pass\n    else:\n        model2 = sm.tsa.statespace.SARIMAX(train_SARIMA[-400:], order=order, seasonal_order=seasonal_order, trend='n').fit()\n        yhat = round(model2.predict(start = '2016-01-16' , end= '2016-04-24', dynamic=True),0)\n        rmse_max = math.sqrt(sum(pow((yhat - true),2))/len(true))\n        result.append(({'order': (p_max,d_max,q_max),'seasonal_order': seasonal_order,'rmse':rmse_max}))\n        duppl_order.append(order+seasonal_order)\n        print(order,seasonal_order,'rmse_(p_max,d_max,q_max):',rmse_max)\n\n    order = (p_min,d_min,q_min)\n    if order + seasonal_order in duppl_order:\n        pass\n    else:\n        model2 = sm.tsa.statespace.SARIMAX(train_SARIMA[-400:], order=order, seasonal_order=seasonal_order, trend='n').fit()\n        yhat = round(model2.predict(start = '2016-01-16' , end= '2016-04-24', dynamic=True),0)\n        rmse_min = math.sqrt(sum(pow((yhat - true),2))/len(true))\n        result.append(({'order': (p_min,q_min,d_min),'seasonal_order': seasonal_order,'rmse':rmse_min}))\n        duppl_order.append(order+seasonal_order)\n        print(order,seasonal_order,'rmse_(p_min,d_min,q_min):',rmse_min)\n    \n   \n    if rmse_max < rmse_min:\n        p_loop = range(p_m,p_max)\n        d_loop = range(d_m,d_max)\n        q_loop = range(q_m,q_max)\n    else:\n        p_loop = range(p_min,p_m)\n        d_loop = range(d_min,d_m)\n        q_loop = range(q_min,q_m)\n        \n        \n    for p_ in p_loop:\n        for d_ in d_loop:\n            for q_ in q_loop:\n                start_time = time.time()\n                order=(p_,d_,q_)\n                seasonal_order = (P_m,D_m,Q_m,m)\n                if order + seasonal_order in duppl_order:\n                    pass\n                else:\n                    model2 = sm.tsa.statespace.SARIMAX(train_SARIMA[-400:], order=order, seasonal_order=seasonal_order, trend='n').fit()\n                    yhat = round(model2.predict(start = '2016-01-16' , end= '2016-04-24', dynamic=True),0)\n                    rmse = math.sqrt(sum(pow((yhat - true),2))/len(true))\n                    result.append(({'order':order,'seasonal_order':seasonal_order,'rmse':rmse}))\n                    duppl_order.append(order+seasonal_order)\n                    duration = time.time() - start_time\n                    print(order,seasonal_order,'rmse:' ,rmse , 'duration:',duration)\n                    \n    output = pd.DataFrame(result)\n    fit_model = output[output['rmse'] == (output['rmse'].min())]\n    order_fit = fit_model.order.values[0]\n    \n    \n    if (P_m,D_m,Q_m) != (P_max,D_max,Q_max):\n        order = order_fit\n        seasonal_order = (P_m,D_m,Q_m,m)\n        \n        if order + seasonal_order in duppl_order:\n            pass\n        else:\n            model2 = sm.tsa.statespace.SARIMAX(train_SARIMA[-400:], order=order, seasonal_order=seasonal_order, trend='n').fit()\n            yhat = round(model2.predict(start = '2016-01-16' , end= '2016-04-24', dynamic=True),0)\n            Rmse_m = math.sqrt(sum(pow((yhat - true),2))/len(true))\n            result.append(({'order': order_fit,'seasonal_order': (P_m,D_m,Q_m,m),'rmse':Rmse_m}))\n            duppl_order.append(order+seasonal_order)\n            print(order,seasonal_order,'Rmse_(P_m,D_m,Q_m,m):', Rmse_m)\n        \n        seasonal_order = (P_max,D_Max,Q_Max,m)\n        if order + seasonal_order in duppl_order:\n            pass\n        else:\n            model2 = sm.tsa.statespace.SARIMAX(train_SARIMA[-400:], order=order, seasonal_order=seasonal_order, trend='n').fit()\n            yhat = round(model2.predict(start = '2016-01-16' , end= '2016-04-24', dynamic=True),0)\n            Rmse_max = math.sqrt(sum(pow((yhat - true),2))/len(true))\n            result.append(({'order': order_fit,'seasonal_order': (P_max,D_max,Q_max,m),'rmse':Rmse_max}))\n            duppl_order.append(order+seasonal_order)\n            print(order,seasonal_order,'Rmse_(P_max,D_max,Q_max,m):',Rmse_max)\n        \n        seasonal_order = (P_min,D_min,Q_min,m)\n        if order + seasonal_order in duppl_order:\n            pass\n        else:\n            model2 = sm.tsa.statespace.SARIMAX(train_SARIMA[-400:], order=order, seasonal_order=seasonal_order, trend='n').fit()\n            yhat = round(model2.predict(start = '2016-01-16' , end= '2016-04-24', dynamic=True),0)\n            Rmse_min = math.sqrt(sum(pow((yhat - true),2))/len(true))\n            result.append(({'order': order_fit,'seasonal_order': (P_min,D_min,Q_min,m),'rmse':Rmse_min}))\n            duppl_order.append(order+seasonal_order)\n            print(order,seasonal_order,'Rmse_(P_min,D_min,Q_min,m):',Rmse_min)\n\n        \n        if rmse_max < rmse_min:\n            P_loop = range(P_m,P_max)\n            D_loop = range(D_m,D_max)\n            Q_loop = range(Q_m,Q_max)\n        else:\n            P_loop = range(P_min,P_m)\n            D_loop = range(D_min,D_m)\n            Q_loop = range(Q_min,Q_m)\n        \n        \n        for P_ in P_loop:\n            for D_ in D_loop:\n                for Q_ in Q_loop:\n                    start_time = time.time()\n                    order = order_fit\n                    seasonal_order = (P_,D_,Q_,m)\n                    if order + seasonal_order in duppl_order:\n                        pass\n                    else:\n                        model2 = sm.tsa.statespace.SARIMAX(train_SARIMA[-400:], order=order, seasonal_order= seasonal_order, trend='n').fit()\n                        yhat = round(model2.predict(start = '2016-01-16' , end= '2016-04-24', dynamic=True),0)\n                        rmse = math.sqrt(sum(pow((yhat - true),2))/len(true))\n                        result.append(({'order':order_fit,'seasonal_order':(P_,D_,Q_,m),'rmse':rmse}))\n                        duppl_order.append(order+seasonal_order)\n                        duration = time.time() - start_time\n                        print(order,(P_,D_,Q_,m),'rmse:' ,rmse , 'duration:',duration)\n        output2 = pd.DataFrame(result)\n        fit_model2 = output2[output2['rmse'] == (output2['rmse'].min())]\n        print('best model: ',fit_model2.order.values,fit_model2.seasonal_order.values, 'rmse:', fit_model2.rmse.values)\n    \n    \n    else:\n        order = order_fit\n        seasonal_order_best = (P_m,D_m,Q_m,m)\n        \n        if order + seasonal_order in duppl_order:\n            pass\n        else:\n            model2 = sm.tsa.statespace.SARIMAX(train_SARIMA[-400:], order=order, seasonal_order=seasonal_order, trend='n').fit()\n            yhat = round(model2.predict(start = '2016-01-16' , end= '2016-04-24', dynamic=True),0)\n            rmse = math.sqrt(sum(pow((yhat - true),2))/len(true))\n            result.append(({'order': order,'seasonal_order': seasonal_order,'rmse':rmse}))\n            duppl_order.append(order+seasonal_order)\n            print(order,seasonal_order,'rmse:',rmse)\n\nelse:\n    order = (p_m,q_m,d_m)\n    if (P_m,D_m,Q_m) != (P_max,D_max,Q_max):\n        \n        \n        seasonal_order = (P_m,D_m,Q_m,m)\n        if order + seasonal_order in duppl_order:\n            pass\n        else:\n            model2 = sm.tsa.statespace.SARIMAX(train_SARIMA[-400:], order=order, seasonal_order= seasonal_order, trend='n').fit()\n            yhat = round(model2.predict(start = '2016-01-16' , end= '2016-04-24', dynamic=True),0)\n            Rmse_m = math.sqrt(sum(pow((yhat - true),2))/len(true))\n            result.append(({'order': order_fit,'seasonal_order': (P_m,D_m,Q_m,m),'rmse':Rmse_m}))\n            duppl_order.append(order+seasonal_order)\n            print(order,seasonal_order,'Rmse_(P_m,D_m,Q_m,m):', Rmse_m)\n        \n        seasonal_order =  (P_max,D_max,Q_max,m)\n        if order + seasonal_order in duppl_order:\n            pass\n        else:\n            model2 = sm.tsa.statespace.SARIMAX(train_SARIMA[-400:], order=order, seasonal_order=seasonal_order, trend='n').fit()\n            yhat = round(model2.predict(start = '2016-01-16' , end= '2016-04-24', dynamic=True),0)\n            Rmse_max = math.sqrt(sum(pow((yhat - true),2))/len(true))\n            result.append(({'order': order_fit,'seasonal_order': (P_max,D_max,Q_max,m),'rmse':Rmse_max}))\n            duppl_order.append(order+seasonal_order)\n            print(order,seasonal_order,'Rmse_(P_max,D_max,Q_max,m):',Rmse_max)\n            \n        seasonal_order = (P_min,D_min,Q_min,m)\n        if order + seasonal_order in duppl_order:\n            pass\n        else:\n            model2 = sm.tsa.statespace.SARIMAX(train_SARIMA[-400:], order=order, seasonal_order= seasonal_order, trend='n').fit()\n            yhat = round(model2.predict(start = '2016-01-16' , end= '2016-04-24', dynamic=True),0)\n            Rmse_min = math.sqrt(sum(pow((yhat - true),2))/len(true))\n            result.append(({'order': order_fit,'seasonal_order': (P_min,D_min,Q_min,m),'rmse':Rmse_min}))\n            duppl_order.append(order+seasonal_order)\n            print(order,seasonal_order,'Rmse_(P_min,D_min,Q_min,m):',Rmse_min)\n        \n        \n            \n        if rmse_max < rmse_min:\n            P_loop = range(P_m,P_max)\n            D_loop = range(D_m,D_max)\n            Q_loop = range(Q_m,Q_max)\n            \n        else:\n            P_loop = range(P_min,P_m)\n            D_loop = range(D_min,D_m)\n            Q_loop = range(Q_min,Q_m)\n            \n        \n        for P_ in P_loop:\n            for D_ in D_loop:\n                for Q_ in Q_loop:\n                    start_time = time.time()\n                    \n                    seasonal_order = (P_,D_,Q_,m)\n                    if order + seasonal_order in duppl_order:\n                        pass\n                    else:\n                        model2 = sm.tsa.statespace.SARIMAX(train_SARIMA[-400:], order=order, seasonal_order= seasonal_order, trend='n').fit()\n                        yhat = round(model2.predict(start = '2016-01-16' , end= '2016-04-24', dynamic=True),0)\n                        rmse = math.sqrt(sum(pow((yhat - true),2))/len(true))\n                        result.append(({'order':order_fit,'seasonal_order':(P_,D_,Q_,m),'rmse':rmse}))\n                        duppl_order.append(order+seasonal_order)\n                        duration = time.time() - start_time\n                        print(order,(P_,D_,Q_,m),'rmse:' ,rmse , 'duration:',duration)\n        \n        \n        output2 = pd.DataFrame(result)\n        fit_model2 = output2[output2['rmse'] == (output2['rmse'].min())]\n        print('best model: ',fit_model2.order.values,fit_model2.seasonal_order.values, 'rmse:', fit_model2.rmse.values)\n        \n    else:\n        seasonal_order = (P_m,D_m,Q_m,m)\n        \n        if order + seasonal_order in duppl_order:\n            pass\n        else:\n            model2 = sm.tsa.statespace.SARIMAX(train_SARIMA[-400:], order=order, seasonal_order=seasonal_order, trend='n').fit()\n            yhat = round(model2.predict(start = '2016-01-16' , end= '2016-04-24', dynamic=True),0)\n            rmse = math.sqrt(sum(pow((yhat - true),2))/len(true))\n            result.append(({'order': order,'seasonal_order': seasonal_order,'rmse':rmse}))\n            duppl_order.append(order+seasonal_order)\n            print(order,seasonal_order,'rmse:',rmse)\n \nduration = time.time() - start_time_init\n\nprint('Full total time: ',duration)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Building and fitting SARIMX model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"start_time = time.time()\nseasonal_order = (2, 1, 1, 42)\norder_m = (2,1,1)\nmodel_sarima = sm.tsa.statespace.SARIMAX(train_SARIMA, order=order_m, seasonal_order=seasonal_order, trend='n').fit()\nduration = (time.time() - start_time) / 60\nprint(f'training is done within:{duration} minutes')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# out of sample prediction for (100 days):\nyhat_sarima = model_sarima.predict(start = '2016-01-16' , end= '2016-04-24', dynamic=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Calculating the Rmse for SARIMX model for 100 days,The rmse trend going up as forcasting days increases**"},{"metadata":{"trusted":true},"cell_type":"code","source":"sarimax_Rmse = []\nfor i in np.arange(10,100,10):\n    sarimax_Rmse.append(math.sqrt(sum(pow((yhat_sarima[:i] - true[:i]),2))/len(true[:i])))\nsarimax_Rmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"future_fcst = model_sarima.get_forecast(100)\n# That will have a method to pull in confidence interval \nconfidence_int = future_fcst.conf_int(alpha = 0.01)\nconfidence_int['yhat'] = abs(confidence_int['lower FOODS_3_090_CA_3_validation'] + confidence_int['upper FOODS_3_090_CA_3_validation']/2)\nconfidence_int","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nplt.plot(true, label='Ground Truth')\nplt.plot(yhat_sarima, label='Predicted')\nplt.title('SARIMAX Model --- Ground Truth Vs Predicted(Rmse= 32.42313988')\nplt.ylabel('Sales')\n\n\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Plot the diagnostics**"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_sarima.plot_diagnostics(lags=12,figsize = (20,10),);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observation:**\n* Histogrham and estimated density: We  see green and orange lines are close and having the same shape which is good indicator that our model has fit the data properly\n* Correlogram: We See that correlation is not significant of course except at lag0 which normal as any data is 100 correlated with itself.\n* Normal Q-Q: We see that red line overlap most of blue dots which is good indiactor.\n"},{"metadata":{},"cell_type":"markdown","source":"## LSMT Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create x_train,y_train for LSTM model\n\ninput_days = 12\nx_train, y_train = [],[]\nfor i in range(0,train_LSTM.shape[0] - input_days,3):\n    x_train.append(train_LSTM[i:i+input_days])\n    y_train.append(train_LSTM[i+input_days])\n\n    \nx_train = np.array(x_train)\nx_train = x_train.reshape(x_train.shape[0],x_train.shape[1],1)\ny_train = np.asarray(y_train)\nprint('x_train shape', x_train.shape)\nprint('y_train shape', y_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# building simple LSMT model:\n\nstart_time = time.time()\ncell_units = 1000\nepochs = 1500\n\nmodel_LSTM = Sequential() \n\nmodel_LSTM.add(LSTM(cell_units,input_shape=(x_train.shape[1],1))) #return_sequences= True))\n    \n\nmodel_LSTM.add(Dense(1))\n    \n\nmodel_LSTM.compile(loss='mean_squared_error', optimizer='adam')\nmodel_LSTM.fit(x_train, y_train, epochs=epochs, batch_size=64, verbose=1)\n\n\nduration = (time.time() - start_time) / 60\nprint(f'training is done within:{duration} minutes')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_LSTM.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***\n\n**Forcasting (LSTM model)**\n\n***"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating x_test data (its shape should be match with training data and with input_shape)\nx_test = []\nfor i in range(0,test_LSTM.shape[0] - input_days + 1):\n    x_test.append(test_LSTM[i:i + input_days])\n               \nx_test = np.asarray(x_test)    \nx_test = x_test.reshape(x_test.shape[0],x_test.shape[1],1)\nprint('x_test shape', x_test.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# prediction function which goes through each row where each row contains 12 data cells and each cell value will be replaced by predicted value...\npred = []\n\nfor i in range(x_test.shape[0]):\n    for _ in range(x_test.shape[1]):\n        prediction = model_LSTM.predict(x_test[i:i+1,:,:])\n        pred.append(prediction)\n        x_test[i,:-1,:] = x_test[i,1:,:]\n        x_test[i,-1,:] = prediction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reshape the output of precidiction array from (89,12,1) shape to (100,) shape (input_days = 12)\nyhat_lstm = np.arange(0,100,1,'float')\ny_flatten = x_test.flatten()\nyhat_lstm[:input_days] = y_flatten[:input_days] # \nk = 2\nfor i in range(input_days,test_LSTM.shape[0]):\n    yhat_lstm[i] = y_flatten[(k*input_days) - 1]  # {formula:2*input_days - 1}\n    k +=1\n    \nlstm_result = pd.DataFrame(data=yhat_lstm, index=true.index, columns=['yhat'])\nlstm_result['true'] = true","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Rmse for LSTM**"},{"metadata":{"trusted":true},"cell_type":"code","source":"lstm_Rmse = []\n\nfor i in np.arange(10,100,10):\n    lstm_rmse = math.sqrt(sum(pow((lstm_result['yhat'][:i] - true[:i]),2))/len(true[:i]))\n    lstm_Rmse.append(lstm_rmse)\nlstm_Rmse\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nplt.plot(true, label='Ground Truth')\nplt.plot(lstm_result.yhat, label='Predicted')\nplt.title('LSTM Model --- Ground Truth Vs Predicted(Rmse= {})'.format(lstm_rmse))\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## fbprophet Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# prepare dataframe for fprophet model\ntrain_prophet['ds'] = train_prophet.index.values\ntrain_prophet.rename(columns={'FOODS_3_090_CA_3_validation':'y'},inplace=True)\ntrain_prophet = train_prophet[['ds','y']]\ntrain_prophet.columns.name = None\ntrain_prophet","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#fprophet model without holidays\nstart_time = time.time()\nm = Prophet(weekly_seasonality=True)\nm.fit(train_prophet)\n\nfuture = m.make_future_dataframe(periods=100)\nforecast = m.predict(future)\npredicted_prophet = forecast[['yhat']][-test_sample:]\npredicted_prophet.set_index(true.index,inplace=True)\npredicted_prophet.index\n\nduration = time.time() - start_time\nprint(f'training is done within:{duration} minutes')\n\n\nplt.figure(figsize=(12,8))\nplt.plot(true, label='Ground Truth')\nplt.plot(predicted_prophet, label='Predicted')\nplt.title('fbprophet(Without Holidays) --- Ground Truth Vs Predicted(Rmse= 76')\nplt.ylabel('Sales')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#fprophet model with holidays\n\nholidays = calendar[calendar['event_type_1'].isnull() == False]\nholidays = holidays[['date','event_name_1', 'event_type_1']]\nholidays.rename(columns={'date':'ds','event_name_1':'holiday'},inplace=True)\n\nstart_time = time.time()\n\nm_holi = Prophet(weekly_seasonality=True,holidays=holidays)\nm_holi.fit(train_prophet)\n\nfuture = m_holi.make_future_dataframe(periods=100)\nforecast = m_holi.predict(future)\npredicted_prophet = forecast[['yhat']][-test_sample:]\npredicted_prophet.set_index(true.index,inplace=True)\npredicted_prophet.index\n\nprint(f'training is done within:{duration} minutes')\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**fbprophet Rmse**"},{"metadata":{"trusted":true},"cell_type":"code","source":"prophet_Rmse = []\nfor i in np.arange(10,100,10):\n    prophet_rmse = math.sqrt(sum(pow((predicted_prophet.yhat[:i] - true[:i]),2))/len(true[:i]))\n    prophet_Rmse.append(prophet_rmse)\nprophet_Rmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nplt.plot(true, label='Ground Truth')\nplt.plot(predicted_prophet, label='Predicted')\nplt.title('fbprophet(With Holidays) --- Ground Truth Vs Predicted(Rmse= {}'.format(prophet_rmse))\nplt.ylabel('Sales')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Recomended Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nplt.plot(sarimax_Rmse, label='SARIMAX Rmse')\nplt.plot(lstm_Rmse, label='lstm_Rmse')\nplt.plot(prophet_Rmse, label='fbprophet Rmse')\nplt.ylabel('RMSE')\nplt.xlabel('No.Days')\nplt.title('fbprophet Vs SARIMX Vs Lstm')\nplt.xticks(np.arange(10), ['10', '20', '30','40', '50', '60','70', '80', '90','100'])\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observation**  \n\n* Accuracy of fbprophet for first 10 days is the best, but after then the SARIMAX defeats fbprophet.\n* Accuracy of LSTM comes in the middle between SARIMAX and fbprophet and it does not drop."},{"metadata":{},"cell_type":"markdown","source":"| Model | Training Time (Minutes) }\n| --- | --- |\n| SARIMAX | 5.8 |\n| fbprophet | 3.0 |\n| LSTM | 35.0* , 1.7** |\n*(By using CPU only)\n**(By using GPU)\n\n* SARIMAX model residual is the lowest one and the shape of the predicted values is best match the shape of actual values,So SARIMAX model will be the recommended model consedering its more open for enhancement via tune hyperparameter, the only limitation comes from hardware requirments since powerful workstation is high requiered to perform grid searching . \n\n* Fbprophet will be good option if short term forecasting is required (let us say period of two weeks ), its training time is the lowest and does not need powerful workstation, so its good choice especially if number of time series which need to be forecasted is high,Its performance and ability to fitech the seasonality give its good credit.\n\n* LSTM model could be also selected if dataset is big enough as it can get the pattern properly but ؤertainly would require powerful workstation equipped with  GPU and high RAM as well .\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}