{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Kaggle's M5 forecasting Competition\n\nAuthor: `Armando Miguel Trejo Marrufo`"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"%matplotlib inline\nimport pandas as pd\nimport numpy as np \nimport matplotlib.pyplot as plt \nimport warnings\nimport missingno as msno \nimport seaborn as sns \nimport plotly.graph_objects as go\nfrom sklearn.utils import shuffle\nwarnings.filterwarnings(\"ignore\")\n\n# Display all columns\npd.set_option('display.max_columns', None)\n\n# List files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input/m5-forecasting-accuracy'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***Notes***\n\n**In order to properly reproduce this results, run this notebook locally, for this Kaggle version I had only taken a shuffle version of 1000000 from the sell_prices dataset and the first 1000 rows of sales_train_validation\n\nAlso the function that output visualizations have a `break` at the end, you're free to remove it to properly visualize the output for all the values of the required variable.\n** \n\n\n1. ```dt_complementary``` merges the original calendar and price data to know the ```date``` when the ```product_id``` of the ```store_id``` was saled at. Additionaly to know if they were particular events like ```SNAP``` purchases or major events, check the documentation.\n\n2. ```dt_sales_s```. In case you don't have enough RAM take initial n rows of the original dataframe and shuffle the data, because it is ordered.\n\n3. ```dt_sales_melt```. Melt the dataframe so that each sale by product can be seen as row.\n\n4. ```dt_work```. Merges ```dt_sales_melt``` with ```dt_complementary``` to know for each day of sale the price and relevant events associated to that day."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load data\nfiles = ['/kaggle/input/m5-forecasting-accuracy/calendar.csv', \n         '/kaggle/input/m5-forecasting-accuracy/sales_train_validation.csv',\n         '/kaggle/input/m5-forecasting-accuracy/sell_prices.csv']\n\ndata = [pd.read_csv(f) for f in files]\ndt_calendar, dt_sales, dt_prices = data\n\n# Merge calendar and prices\ndt_prices_s = shuffle(dt_prices, n_samples = 1000000)\ndt_complementary = dt_prices_s.merge(dt_calendar, how='left', on='wm_yr_wk')\n\n# Shuffle data (it is originally ordered) and take n rows (if you don't have enough RAM)\ndt_sales_s = shuffle(dt_sales, n_samples = 1000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Melt sales data\nindicators = [f'd_{i}' for i in range(1,1914)]\n\ndt_sales_melt = pd.melt(dt_sales_s, \n                        id_vars = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'],\n                        value_vars = indicators, var_name = 'day_key', value_name = 'sales_day')\n\ndt_sales_melt['day'] = dt_sales_melt['day_key'].apply(lambda x: x[2:]).astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data to work with\ncolumns = ['store_id','item_id','sell_price','date','year','d','event_name_1','event_type_1','event_name_2','event_type_2','snap_CA','snap_TX','snap_WI']\ndt_work = dt_sales_melt.merge(dt_complementary[columns], how = 'left', left_on=['item_id','store_id','day_key'], right_on=['item_id','store_id','d'])\nprint(dt_work.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploratory Data Analysis\n\nThe objective of this section is to get a general picture of possible relationships between the objective variable `sales` and the dependent variables like `sell_price` and `event_type`. Later on, we could confirm or reject this relationships through a Corfirmatory Data Analysis. "},{"metadata":{},"cell_type":"markdown","source":"#### 1. Missing values \n\n* The initial heatmap `General glimpse` show us that there is missing data in relevant variables like `date` and `sell_price`. The variable `event_name` is natural to have this behavior because we're tagging only relevant days like the **superbowl**. \n\n* The `NA's as bar` allow us to see that only 20% of the data is missing. Therefore, we could assume to drop this missing data. However, we see that it is relate to the variable `d`, so we could plot the distribution of `sale_day` where `d` is missing to see if there were product with positive sell, if not we could savely discard this observations. The distribution plot of this variable show us that there is no sell these days, so we would discard null values based on the `d` column.\n\n* To understand is the missing data is more promient by department, we sort the data by `department_id` and then plot the heatmap. If the missing data was homogeneously distributed we would see repeated patterns of missing data, but actually there are some particular periods by department when the `sell_price` and data is missing, it could be that this product stock out or that they decided to not sold it for a certain time. \n\n* Finally, the dendogram let us understand the correlate variable completition. For example, the `sales_day` could be completed with the information of the `snap_TX` variable. But, we would discard this results as we would further consider robust methods to this imputation like ***MICE*** or a bayesian one."},{"metadata":{"trusted":true},"cell_type":"code","source":"#General glimpse\nmsno.matrix(dt_work)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# NA's as bar plot\nmsno.bar(dt_work)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# NA's by department\nmsno.matrix(dt_work.sort_values(by=['dept_id']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dendogram of NA's values\nmsno.dendrogram(dt_work)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distribution of d variable\nsns.distplot(dt_work[dt_work.d.isna()].sales_day, color='b')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(dt_work.shape)\ndt_work = dt_work.dropna(how='any', subset=['d'])\nprint(dt_work.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 2. Global Granularity\n\nIn this section, we visualize the time series of the ```sum``` of the ```sales_day``` and the ```mean``` of ```sell_price``` by different levels of granularity. For example, sales and price by state. Above, the ```particular granularity``` show us a deeper level of granularity by displaying the data for the different levels of granularity in each store, for example, the sales and price by category of each store. "},{"metadata":{"trusted":true},"cell_type":"code","source":"def timeseries_global(data:'pd.DataFrame',level:str):\n    \n    # Group data by level \n    dt_level = data.groupby([level, 'day']).agg({'sales_day':'sum','sell_price':'mean'}) \\\n                                                         .reset_index() \\\n                                                         .sort_values(by=[level,'day'])\n    \n    # Visualize by components of level \n    levels = dt_level[level].unique()\n\n    for l in levels: \n        df = dt_level[(dt_level[level] == l)]\n        f, (ax1, ax2) = plt.subplots(1, 2, sharey=False, figsize=(16,8))\n        plt.style.use('ggplot')\n        ax1.plot(df['sales_day'], color = 'blue')\n        ax1.set_xticklabels(labels = df.day.values, rotation=70)\n        ax1.grid(False)\n        ax1.set_title(f\"Time series of sales for {l}\")\n        ax2.plot(df['sell_price'], color = 'red')\n        ax2.set_xticklabels(labels = df.day.values, rotation=70)\n        ax2.grid(False)\n        ax2.set_title(f\"Time series of price for {l}\")\n        break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# State level\ntimeseries_global(dt_work,'state_id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Store level\ntimeseries_global(dt_work,'store_id')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By visualizing the data by `state_id` and `store_id` we see a general upward trend of average `sell_price` and sales. However, there are some particular behaviors like `CA_2` that suddenly pops up its sales from the 8 observation on. The genera conclusion as that we level down the level of granularity we see the particular behaviors of set of data, that is, if we consider data only by state level we could strongly affirm that the sales and the prices are augmenting with the time, but as we further level down the level of granularity we conclude that there are particular behaviors. \n\nTherefore, let's plot the data considering departments and categories by each store, this we will call it the **particular granularity** level. \n\n#### 3. Particular granularity"},{"metadata":{"trusted":true},"cell_type":"code","source":"def timeseries_particular(data:'pd.DataFrame',level_1:str, level_2:str):\n    \n    # Group data by level \n    dt_level = data.groupby([level_1,level_2, 'day']).agg({'sales_day':'sum','sell_price':'mean'}) \\\n                                                               .reset_index() \\\n                                                               .sort_values(by=[level_1,level_2,'day'])\n    \n    # Visualize by components of level \n    l1, l2 = dt_level[level_1].unique(), dt_level[level_2].unique() \n    iterables = [(a, b) for a in l1 for b in l2]\n\n    for i in iterables: \n        a, b = i\n        df = dt_level[(dt_level[level_1] == a) & (dt_level[level_2] == b)]\n        f, (ax1, ax2) = plt.subplots(1, 2, sharey=False, figsize=(16,8))\n        plt.style.use('ggplot')\n        ax1.plot(df['sales_day'], color = 'blue')\n        ax1.set_xticklabels(labels = df.day.values, rotation=70)\n        ax1.grid(False)\n        ax1.set_title(f\"Time series of sales for {a} and {b}\")\n        ax2.plot(df['sell_price'], color = 'red')\n        ax2.set_xticklabels(labels = df.day.values, rotation=70)\n        ax2.grid(False)\n        ax2.set_title(f\"Time series of price for {a} and {b}\")\n        break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Level: Categories of each store \ntimeseries_particular(dt_work,'store_id', 'cat_id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Level: departments by store\ntimeseries_particular(dt_work, 'store_id', 'dept_id')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we get to the level that we could discern some relationship between the sales and the price variable, that is, if the price decrease the sale is going to increase like in the `FOODS_1` department of `CA_1`. On the other hand, we see the opposite relationship in the `FOODS_2` department of `CA_2`, that when the price goes up the sale goes up.\n\nOne more thing to notice is that there are periods where the price remains constant for a long time and then suddenly goes up or down. To further analysis this behavior, let's use the dash library that allow us to use a date slider."},{"metadata":{"trusted":true},"cell_type":"code","source":"def timeseries_range_slider(data:'pd.DataFrame',level_1:str, level_2:str):\n     \n    # Group data by level \n    dt_level = data.groupby([level_1,level_2, 'date']).agg({'sales_day':'sum','sell_price':'mean'}) \\\n                                                      .reset_index() \\\n                                                      .sort_values(by=[level_1,level_2,'date'])\n    \n    # Visualize by components of level \n    l1, l2 = dt_level[level_1].unique(), dt_level[level_2].unique() \n    iterables = [(a, b) for a in l1 for b in l2]\n\n    for i in iterables: \n        a, b = i\n        df = dt_level[(dt_level[level_1] == a) & (dt_level[level_2] == b)]\n        \n        mxm = max(df['sales_day'])\n        if mxm < 50:\n            weight = 5\n        elif mxm < 100:\n            weight = 10\n        elif mxm < 200: \n            weight = 25  \n        else:\n            weight = 35\n        \n        fig = go.Figure()\n        fig.add_trace(go.Scatter(x=df.date, \n                                 y=df['sales_day'], \n                                 name='Sales',\n                                 line_color='deepskyblue'))\n\n        fig.add_trace(go.Scatter(x=df.date, \n                                 y=df['sell_price']*weight, \n                                 name='Price average',\n                                 line_color='dimgray'))\n\n        fig.update_layout(title_text=f'Time Series of sales with Rangeslider for {a} and {b}',\n                          xaxis_rangeslider_visible=True)\n        fig.show()\n        break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Timeseries with a range slider \ntimeseries_range_slider(dt_work, 'store_id', 'dept_id')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* One of the key advantages of the time slider is that it allow us to better visualize unusual behavior in the data. For example for the department `HOBBIES_2` of the state `CA_3` it has an usual sale value the 11th of December, maybe some product was on special sale before Christmas. \n\n* Another advantage of this visualization is that let us confirm seasonality patterns in the sale and to understand that this is not related to the price, becuase there are case where the price is constant or has a constant trend but the sales fluctuates like the `HOUSEHOLD` department of `WI_3` from Jun 2012 on.\n\n* By noticing this particular behavior first we would need to relate these particular sales day to an event provided in the dataset. If we see that there is no relationship at all, we could use statistical tools for outlier treatment or structural breaks."},{"metadata":{},"cell_type":"markdown","source":"***4. Event_name_1 sale day analysis***\n\nWe're now considering annotating special events to `date`, that is, we would like to visualize if the sell have some particular behaviors when there are events like the `Super Bowl` or `Christmas Day`.\n\nLet's take as example the year `2012` of the department `HOUSEHOLD_2` for the store `WI_3`, we notice that:\n\n* Sales is increasing prior to the Superbowl and Hallowen or in Thanks Giving.\n* In general, when the days are tagged as special event tha sale tends to be greater than the average. \n* There are some events when the sale surprisingly decreases like the `Mother's day`. Considering it is a department related to things for the house, we could gain further insight of the products in this category but the `id` is anonymous. For this reason, we're going to assume that they were products like `tv's` or `videogames` because Mothers do not like to receive a 70 inches tv as gift, that is more for parents. "},{"metadata":{"trusted":true},"cell_type":"code","source":"def timeseries_range_slider_event(data:'pd.DataFrame',level_1:str, level_2:str, level_3:str, level_4:str):\n     \n    # Group data by level \n    dt_level = data.groupby([level_1,level_2, level_3]).agg({'sales_day':'sum','sell_price':'mean'}) \\\n                                                      .reset_index() \\\n                                                      .sort_values(by=[level_1,level_2,level_3])\n    \n    dt_ant = data.groupby([level_1, level_2, level_3, level_4]).agg({'sales_day':'sum','sell_price':'mean'}) \\\n                                                                               .reset_index() \\\n                                                                               .sort_values(by=[level_1,level_2,level_3])\n    \n    \n    # Visualize by components of level \n    l1, l2 = dt_level[level_1].unique(), dt_level[level_2].unique() \n    iterables = [(a, b) for a in l1 for b in l2]\n\n    for i in iterables: \n        a, b = i\n        df = dt_level[(dt_level[level_1] == a) & (dt_level[level_2] == b)]\n        df_ant = dt_ant[(dt_ant[level_1] == a) & (dt_ant[level_2] == b)]\n        \n        # Annotations\n        events = df_ant[['date', 'event_name_1']]\n        events = events.set_index('date')\n        ants = [dict(x = date, y = 10, \n                       xref = 'x', yref = 'y', \n                       textangle = 45,\n                       font=dict(color = 'black', size = 8),\n                       text = f'{value[0]}')\n                       for date, value in zip(events.index, \n                                              events.values)]\n        \n        # Weights for price\n        mxm = max(df['sales_day'])\n        if mxm < 50:\n            weight = 5\n        elif mxm < 100:\n            weight = 10\n        elif mxm < 200: \n            weight = 25  \n        else:\n            weight = 35\n        \n        fig = go.Figure()\n        fig.add_trace(go.Scatter(x=df.date, \n                                 y=df['sales_day'], \n                                 name='Sales',\n                                 line_color='deepskyblue'))\n\n        fig.add_trace(go.Scatter(x=df.date, \n                                 y=df['sell_price']*weight, \n                                 name='Price average',\n                                 line_color='dimgray'))\n\n        fig.update_layout(title_text=f'Time Series of sales with Rangeslider for {a} and {b}',\n                          xaxis_rangeslider_visible=True,\n                          annotations = ants,\n                          height=800,\n                          width=1100)\n        \n        \n        fig.show()\n        break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Mark special events as annotation in the time series\ntimeseries_range_slider_event(dt_work,'store_id','dept_id','date','event_name_1')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***5. SNAP sale day visual analysis***\n\nLet's consider the department `FOODS_1` of the store `CA_1` and take the year 2012 as an example. \n\n* The general pattern of behavior is that it appears to be repetitive periods throught the year when snap days are allowed for a week. \n\n* Considering March, April, August and December 2012 it seems that on average the sales where higher than the previous days. However, we could no further affirm that `SNAP` days tend to increase sales because there is a fluctuation that is inherent to this specific department throughout the year. In other words, there are periods of increasing sales (From January to March, July to September and December) as so are periods of static/constant sales (April to Juin) and decreasing sales (October to November). \n\nOne could get deeper and deeper in this analysis, but my objective is to get a general glimpse that allows me to start hypothezing about the relationships between the variables. This, we're going to developing in the Confirmatory Data Analysis. "},{"metadata":{"trusted":true},"cell_type":"code","source":"def timeseries_range_slider_snap(data:'pd.DataFrame',level_1:str, level_2:str, level_3:str):\n     \n    # Group data by level \n    dt_level = data.groupby([level_1,level_2, level_3]).agg({'sales_day':'sum','sell_price':'mean'}) \\\n                                                       .reset_index() \\\n                                                       .sort_values(by=[level_1,level_2,level_3])\n\n    # Visualize by components of level \n    l1, l2 = dt_level[level_1].unique(), dt_level[level_2].unique() \n    iterables = [(a, b) for a in l1 for b in l2]\n    \n    states = ['WI','CA','TX']\n\n    for i in iterables: \n        a, b = i\n        df = dt_level[(dt_level[level_1] == a) & (dt_level[level_2] == b)]\n\n        # Annotations\n        for s in states:\n            if a.startswith(s):\n                col = 'snap_' + s\n                df_ant = data[(data[col] == 1) & (data[level_1] == a)].groupby([level_1, level_2, level_3, col]) \\\n                                                                              .agg({'sales_day':'sum','sell_price':'mean'}) \\\n                                                                              .reset_index() \\\n                                                                              .sort_values(by=[level_1, level_2, level_3])    \n    \n        events = df_ant[['date', col]]\n        events = events.set_index('date')\n\n        ants = [dict(x = date, \n                     y = 10, \n                     xref = 'x', \n                     yref = 'y', \n                     textangle = 45,\n                     font=dict(color = 'black', size = 6),\n                               text = col)\n                     for date, value in zip(events.index, \n                                            events.values)]\n        \n        \n        # Weights for price\n        mxm = max(df['sales_day'])\n        if mxm < 50:\n            weight = 5\n        elif mxm < 100:\n            weight = 10\n        elif mxm < 200: \n            weight = 25  \n        else:\n            weight = 35\n        \n        fig = go.Figure()\n        fig.add_trace(go.Scatter(x=df.date, \n                                 y=df['sales_day'], \n                                 name='Sales',\n                                 line_color='deepskyblue'))\n\n        fig.add_trace(go.Scatter(x=df.date, \n                                 y=df['sell_price']*weight, \n                                 name='Price average',\n                                 line_color='dimgray'))\n\n        fig.update_layout(title_text=f'Time Series of sales with Rangeslider for {a} and {b}',\n                          xaxis_rangeslider_visible=True,\n                          annotations = ants,\n                          height=600,\n                          width=800)\n        \n        \n        fig.show()\n        break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Timeseries annotated with SNAP days\ntimeseries_range_slider_snap(dt_work, 'store_id', 'dept_id', 'date')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conclusions\n\n\nThe general picture is that there seems to be at the same a **`strong` and `weak` relationship between the objective variable `sales_day` and the dependent variables through periods of time**. For example, we saw that the `sell_price` remained constant but sale decreased or increased. It is highly important to take into account this events because we will further seek to establish a statistical relationship (like Pearson's coefficient). Thus, before rejecting that there is not at all association between variables (a global association throughout the time) we could say that this association tends to fluctuate through time but that in general it is a weak one. This is going to be our key study in the `Confirmatory Data Analysis`. "}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}