{"cells":[{"metadata":{},"cell_type":"markdown","source":"# This kernel is:\n## - Based on [Very fst Model](https://www.kaggle.com/ragnar123/very-fst-model). Thanks [@ragnar123](https://www.kaggle.com/ragnar123).\n## - Automatically uploaded by [push-kaggle-kernel](https://github.com/harupy/push-kaggle-kernel).\n## - Formatted by [Black](https://github.com/psf/black).","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Objective\n\n* Make a baseline model that predict the validation (28 days).\n* This competition has 2 stages, so the main objective is to make a model that can predict the demand for the next 28 days.","execution_count":null},{"metadata":{"lines_to_next_cell":2,"title":"[code]","trusted":true},"cell_type":"code","source":"import os\nimport gc\nimport warnings\nimport time\nimport pandas as pd\nfrom pandas.plotting import register_matplotlib_converters\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import LabelEncoder\n\nwarnings.filterwarnings(\"ignore\")\npd.set_option(\"display.max_columns\", 500)\npd.set_option(\"display.max_rows\", 500)\nregister_matplotlib_converters()\nsns.set()","execution_count":null,"outputs":[]},{"metadata":{"title":"[code]","trusted":true},"cell_type":"code","source":"import IPython\n\n\ndef display(*dfs, head=True):\n    for df in dfs:\n        IPython.display.display(df.head() if head else df)","execution_count":null,"outputs":[]},{"metadata":{"title":"[code]","trusted":true},"cell_type":"code","source":"def on_kaggle():\n    return \"KAGGLE_KERNEL_RUN_TYPE\" in os.environ","execution_count":null,"outputs":[]},{"metadata":{"title":"[code]","trusted":true},"cell_type":"code","source":"if on_kaggle():\n    os.system(\"pip install --quiet mlflow_extend\")","execution_count":null,"outputs":[]},{"metadata":{"title":"[code]","trusted":true},"cell_type":"code","source":"def reduce_mem_usage(df, verbose=False):\n    start_mem = df.memory_usage().sum() / 1024 ** 2\n    int_columns = df.select_dtypes(include=[\"int\"]).columns\n    float_columns = df.select_dtypes(include=[\"float\"]).columns\n\n    for col in int_columns:\n        df[col] = pd.to_numeric(df[col], downcast=\"integer\")\n\n    for col in float_columns:\n        df[col] = pd.to_numeric(df[col], downcast=\"float\")\n\n    end_mem = df.memory_usage().sum() / 1024 ** 2\n    if verbose:\n        print(\n            \"Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)\".format(\n                end_mem, 100 * (start_mem - end_mem) / start_mem\n            )\n        )\n    return df","execution_count":null,"outputs":[]},{"metadata":{"title":"[code]","trusted":true},"cell_type":"code","source":"start = time.clock()\ndef read_data():\n    INPUT_DIR = \"/kaggle/input\" if on_kaggle() else \"input\"\n    INPUT_DIR = f\"{INPUT_DIR}/m5-forecasting-accuracy\"\n\n    print(\"Reading files...\")\n\n    calendar = pd.read_csv(f\"{INPUT_DIR}/calendar.csv\").pipe(reduce_mem_usage)\n    prices = pd.read_csv(f\"{INPUT_DIR}/sell_prices.csv\").pipe(reduce_mem_usage)\n\n    sales = pd.read_csv(f\"{INPUT_DIR}/sales_train_validation.csv\",).pipe(\n        reduce_mem_usage\n    )\n    submission = pd.read_csv(f\"{INPUT_DIR}/sample_submission.csv\").pipe(\n        reduce_mem_usage\n    )\n\n    print(\"sales shape:\", sales.shape)\n    print(\"prices shape:\", prices.shape)\n    print(\"calendar shape:\", calendar.shape)\n    print(\"submission shape:\", submission.shape)\n\n    # calendar shape: (1969, 14)\n    # sell_prices shape: (6841121, 4)\n    # sales_train_val shape: (30490, 1919)\n    # submission shape: (60980, 29)\n\n    return sales, prices, calendar, submission","execution_count":null,"outputs":[]},{"metadata":{"title":"[code]","trusted":true},"cell_type":"code","source":"sales, prices, calendar, submission = read_data()\n\nNUM_ITEMS = sales.shape[0]  # 30490\nDAYS_PRED = submission.shape[1] - 1  # 28","execution_count":null,"outputs":[]},{"metadata":{"lines_to_next_cell":2},"cell_type":"markdown","source":"As [@kaushal2896](https://www.kaggle.com/kaushal2896) suggested in [this comment](https://www.kaggle.com/harupy/m5-baseline#770558), encode the categorical columns before merging to prevent the notebook from crashing even with the full dataset.","execution_count":null},{"metadata":{"title":"[code]","trusted":true},"cell_type":"code","source":"def encode_categorical(df, cols):\n    for col in cols:\n        # Leave NaN as it is.\n        le = LabelEncoder()\n        not_null = df[col][df[col].notnull()]\n        df[col] = pd.Series(le.fit_transform(not_null), index=not_null.index)\n\n    return df\n\n\ncalendar = encode_categorical(\n    calendar, [\"event_name_1\", \"event_type_1\", \"event_name_2\", \"event_type_2\"]\n).pipe(reduce_mem_usage)\n\nsales = encode_categorical(\n    sales, [\"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"],\n).pipe(reduce_mem_usage)\n\nprices = encode_categorical(prices, [\"item_id\", \"store_id\"]).pipe(reduce_mem_usage)","execution_count":null,"outputs":[]},{"metadata":{"title":"[code]","trusted":true},"cell_type":"code","source":"def extract_num(ser):\n    return ser.str.extract(r\"(\\d+)\").astype(np.int16)\n\n\ndef reshape_sales(sales, submission, d_thresh=0, verbose=True):\n    # melt sales data, get it ready for training\n    id_columns = [\"id\", \"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"]\n\n    # get product table.\n    product = sales[id_columns]\n\n    sales = sales.melt(id_vars=id_columns, var_name=\"d\", value_name=\"demand\",)\n    sales = reduce_mem_usage(sales)\n\n    # separate test dataframes.\n    vals = submission[submission[\"id\"].str.endswith(\"validation\")]\n    evals = submission[submission[\"id\"].str.endswith(\"evaluation\")]\n\n    # change column names.\n    vals.columns = [\"id\"] + [f\"d_{d}\" for d in range(1914, 1914 + DAYS_PRED)]\n    evals.columns = [\"id\"] + [f\"d_{d}\" for d in range(1942, 1942 + DAYS_PRED)]\n\n    # merge with product table\n    evals[\"id\"] = evals[\"id\"].str.replace(\"_evaluation\", \"_validation\")\n    vals = vals.merge(product, how=\"left\", on=\"id\")\n    evals = evals.merge(product, how=\"left\", on=\"id\")\n    evals[\"id\"] = evals[\"id\"].str.replace(\"_validation\", \"_evaluation\")\n\n    if verbose:\n        print(\"validation\")\n        display(vals)\n\n        print(\"evaluation\")\n        display(evals)\n\n    vals = vals.melt(id_vars=id_columns, var_name=\"d\", value_name=\"demand\")\n    evals = evals.melt(id_vars=id_columns, var_name=\"d\", value_name=\"demand\")\n\n    sales[\"part\"] = \"train\"\n    vals[\"part\"] = \"validation\"\n    evals[\"part\"] = \"evaluation\"\n\n    data = pd.concat([sales, vals, evals], axis=0)\n\n    del sales, vals, evals\n\n    data[\"d\"] = extract_num(data[\"d\"])\n    data = data[data[\"d\"] >= d_thresh]\n\n    # delete evaluation for now.\n    data = data[data[\"part\"] != \"evaluation\"]\n\n    gc.collect()\n\n    if verbose:\n        print(\"data\")\n        display(data)\n\n    return data\n\n\ndef merge_calendar(data, calendar):\n    calendar = calendar.drop([\"weekday\", \"wday\", \"month\", \"year\"], axis=1)\n    return data.merge(calendar, how=\"left\", on=\"d\")\n\n\ndef merge_prices(data, prices):\n    return data.merge(prices, how=\"left\", on=[\"store_id\", \"item_id\", \"wm_yr_wk\"])","execution_count":null,"outputs":[]},{"metadata":{"title":"[code]","trusted":true},"cell_type":"code","source":"data = reshape_sales(sales, submission, d_thresh=1941 - int(365 * 2))\ndel sales\ngc.collect()\n\ncalendar[\"d\"] = extract_num(calendar[\"d\"])\ndata = merge_calendar(data, calendar)\ndel calendar\ngc.collect()\n\ndata = merge_prices(data, prices)\ndel prices\ngc.collect()\n\ndata = reduce_mem_usage(data)","execution_count":null,"outputs":[]},{"metadata":{"title":"[code]","trusted":true},"cell_type":"code","source":"def add_demand_features(df):\n    for diff in [0]:\n        shift = DAYS_PRED + diff\n        df[f\"shift_t{shift}\"] = df.groupby([\"id\"])[\"demand\"].transform(\n            lambda x: x.shift(shift)\n        )\n\n    for window in [60, 90]:\n        df[f\"rolling_std_t{window}\"] = df.groupby([\"id\"])[\"demand\"].transform(\n            lambda x: x.shift(DAYS_PRED).rolling(window).std()\n        )\n\n    for window in [7, 30, 60, 90, 180]:\n        df[f\"rolling_mean_t{window}\"] = df.groupby([\"id\"])[\"demand\"].transform(\n            lambda x: x.shift(DAYS_PRED).rolling(window).mean()\n        )\n\n#     for window in [7, 30, 60]:\n#         df[f\"rolling_min_t{window}\"] = df.groupby([\"id\"])[\"demand\"].transform(\n#             lambda x: x.shift(DAYS_PRED).rolling(window).min()\n#         )\n\n#     for window in [7, 30, 60]:\n#         df[f\"rolling_max_t{window}\"] = df.groupby([\"id\"])[\"demand\"].transform(\n#             lambda x: x.shift(DAYS_PRED).rolling(window).max()\n#         )\n\n#     df[\"rolling_skew_t30\"] = df.groupby([\"id\"])[\"demand\"].transform(\n#         lambda x: x.shift(DAYS_PRED).rolling(30).skew()\n#     )\n    df[\"rolling_kurt_t30\"] = df.groupby([\"id\"])[\"demand\"].transform(\n        lambda x: x.shift(DAYS_PRED).rolling(30).kurt()\n    )\n    return df\n\n\ndef add_price_features(df):\n#     df[\"shift_price_t1\"] = df.groupby([\"id\"])[\"sell_price\"].transform(\n#         lambda x: x.shift(1)\n#     )\n#     df[\"price_change_t1\"] = (df[\"shift_price_t1\"] - df[\"sell_price\"]) / (\n#         df[\"shift_price_t1\"]\n#     )\n#     df[\"rolling_price_max_t365\"] = df.groupby([\"id\"])[\"sell_price\"].transform(\n#         lambda x: x.shift(1).rolling(365).max()\n#     )\n#     df[\"price_change_t365\"] = (df[\"rolling_price_max_t365\"] - df[\"sell_price\"]) / (\n#         df[\"rolling_price_max_t365\"]\n#     )\n\n    df[\"rolling_price_std_t7\"] = df.groupby([\"id\"])[\"sell_price\"].transform(\n        lambda x: x.rolling(7).std()\n    )\n#     df[\"rolling_price_std_t30\"] = df.groupby([\"id\"])[\"sell_price\"].transform(\n#         lambda x: x.rolling(30).std()\n#     )\n    return df\n\n\ndef add_time_features(df, dt_col):\n    df[dt_col] = pd.to_datetime(df[dt_col])\n    attrs = [\n#         \"year\",\n#         \"quarter\",\n        \"month\",\n        \"week\",\n#        \"day\",\n        \"dayofweek\",\n    ]\n\n    for attr in attrs:\n        dtype = np.int16 if attr == \"year\" else np.int8\n        df[attr] = getattr(df[dt_col].dt, attr).astype(dtype)\n\n    df[\"is_weekend\"] = df[\"dayofweek\"].isin([5, 6]).astype(np.int8)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class CustomTimeSeriesSplitter:\n    def __init__(self, n_splits=5, train_days=80, test_days=20, day_col=\"d\"):\n        self.n_splits = n_splits\n        self.train_days = train_days\n        self.test_days = test_days\n        self.day_col = day_col\n\n    def split(self, X, y=None, groups=None):\n        SEC_IN_DAY = 3600 * 24\n        sec = (X[self.day_col] - X[self.day_col].iloc[0]) * SEC_IN_DAY\n        duration = sec.max()\n\n        train_sec = self.train_days * SEC_IN_DAY\n        test_sec = self.test_days * SEC_IN_DAY\n        total_sec = test_sec + train_sec\n\n        if self.n_splits == 1:\n            train_start = duration - total_sec\n            train_end = train_start + train_sec\n\n            train_mask = (sec >= train_start) & (sec < train_end)\n            test_mask = sec >= train_end\n\n            yield sec[train_mask].index.values, sec[test_mask].index.values\n\n        else:\n            # step = (duration - total_sec) / (self.n_splits - 1)\n            step = DAYS_PRED * SEC_IN_DAY\n\n            for idx in range(self.n_splits):\n                # train_start = idx * step\n                shift = (self.n_splits - (idx + 1)) * step\n                train_start = duration - total_sec - shift\n                train_end = train_start + train_sec\n                test_end = train_end + test_sec\n\n                train_mask = (sec > train_start) & (sec <= train_end)\n\n                if idx == self.n_splits - 1:\n                    test_mask = sec > train_end\n                else:\n                    test_mask = (sec > train_end) & (sec <= test_end)\n\n                yield sec[train_mask].index.values, sec[test_mask].index.values\n\n    def get_n_splits(self):\n        return self.n_splits","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_cv_days(cv, X, dt_col, day_col):\n    for ii, (tr, tt) in enumerate(cv.split(X)):\n        print(f\"----- Fold: ({ii + 1} / {cv.n_splits}) -----\")\n        tr_start = X.iloc[tr][dt_col].min()\n        tr_end = X.iloc[tr][dt_col].max()\n        tr_days = X.iloc[tr][day_col].max() - X.iloc[tr][day_col].min() + 1\n\n        tt_start = X.iloc[tt][dt_col].min()\n        tt_end = X.iloc[tt][dt_col].max()\n        tt_days = X.iloc[tt][day_col].max() - X.iloc[tt][day_col].min() + 1\n\n        df = pd.DataFrame(\n            {\n                \"start\": [tr_start, tt_start],\n                \"end\": [tr_end, tt_end],\n                \"days\": [tr_days, tt_days],\n            },\n            index=[\"train\", \"test\"],\n        )\n\n        display(df)\n\n\ndef plot_cv_indices(cv, X, dt_col, lw=10):\n    n_splits = cv.get_n_splits()\n    _, ax = plt.subplots(figsize=(20, n_splits))\n\n    # Generate the training/testing visualizations for each CV split\n    for ii, (tr, tt) in enumerate(cv.split(X)):\n        # Fill in indices with the training/test groups\n        indices = np.array([np.nan] * len(X))\n        indices[tt] = 1\n        indices[tr] = 0\n\n        # Visualize the results\n        ax.scatter(\n            X[dt_col],\n            [ii + 0.5] * len(indices),\n            c=indices,\n            marker=\"_\",\n            lw=lw,\n            cmap=plt.cm.coolwarm,\n            vmin=-0.2,\n            vmax=1.2,\n        )\n\n    # Formatting\n    MIDDLE = 15\n    LARGE = 20\n    ax.set_xlabel(\"Datetime\", fontsize=LARGE)\n    ax.set_xlim([X[dt_col].min(), X[dt_col].max()])\n    ax.set_ylabel(\"CV iteration\", fontsize=LARGE)\n    ax.set_yticks(np.arange(n_splits) + 0.5)\n    ax.set_yticklabels(list(range(n_splits)))\n    ax.invert_yaxis()\n    ax.tick_params(axis=\"both\", which=\"major\", labelsize=MIDDLE)\n    ax.set_title(\"{}\".format(type(cv).__name__), fontsize=LARGE)\n    return ax","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"day_col = \"d\"\ncv_params = {\n    \"n_splits\": 3,\n    \"train_days\": int(365 * 1.5),\n    \"test_days\": DAYS_PRED,\n    \"day_col\": day_col,\n}\ncv = CustomTimeSeriesSplitter(**cv_params)","execution_count":null,"outputs":[]},{"metadata":{"title":"[code]","trusted":true},"cell_type":"code","source":"data = add_demand_features(data).pipe(reduce_mem_usage)\ndata = add_price_features(data).pipe(reduce_mem_usage)\ndt_col = \"date\"\ndata = add_time_features(data, dt_col).pipe(reduce_mem_usage)\ndata = data.sort_values(\"date\")\n\nprint(\"start date:\", data[dt_col].min())\nprint(\"end date:\", data[dt_col].max())\nprint(\"data shape:\", data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample = data.iloc[::1000][[day_col, dt_col]].reset_index(drop=True)\nshow_cv_days(cv, sample, dt_col, day_col)\nplot_cv_indices(cv, sample, dt_col)\n\ndel sample\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfeatures = [\n    # demand features\n    \"shift_t28\",\n    # std\n    \"rolling_std_t60\",\n    \"rolling_std_t90\",\n    # mean\n    \"rolling_mean_t7\",\n    \"rolling_mean_t30\",\n    \"rolling_mean_t60\",\n    \"rolling_mean_t90\",\n    \"rolling_mean_t180\",\n    # others\n    \"rolling_kurt_t30\",\n    # price features\n    \"sell_price\",\n    \"rolling_price_std_t7\",\n    # time features\n    \"month\",\n    \"week\",\n    \"dayofweek\",\n    \"is_weekend\",\n]\n\ndata_zero=data.fillna(0) # convert NaN to zero\nis_train = data_zero[\"d\"] < 1914\n\n# Attach \"d\" to X_train for cross validation.\nX_train = data_zero[is_train][['id']+[day_col] + features +[\"demand\"]].reset_index(drop=True)\ny_train = data_zero[is_train][['id']+[\"demand\"]].reset_index(drop=True)\nX_test = data_zero[~is_train][['id']+features].reset_index(drop=True)\n\n\nfor idx_fold, (idx_trn, idx_val) in enumerate(cv.split(X_train, y_train)):\n    continue\n    print(f\"\\n----- Fold: ({idx_fold + 1} / {cv.get_n_splits()}) -----\\n\")\n    models={}\n    X_trn, X_val = X_train.iloc[idx_trn], X_train.iloc[idx_val]\n    y_trn, y_val = y_train.iloc[idx_trn], y_train.iloc[idx_val]\n    for item_id_sub,data_sub in X_trn.groupby('id'):  #train models seperately according to item_id\n        X_sub_train=data_sub[features].reset_index(drop=True)\n        y_sub_train=data_sub[['demand']].reset_index(drop=True)\n        models[item_id_sub]=LinearRegression().fit(X_sub_train,y_sub_train)\n        del X_sub_train, y_sub_train\n    print(\"MODEL TRAINED SUCCESS\")\n    preds = np.zeros(y_val.shape[0])\n    ix=0\n    size=y_val.shape[0]\n    for (idx, row) in X_val.iterrows(): \n        \n        print(size-ix)\n        ft=np.array(row[features].values.tolist()).reshape(1,-1)\n        preds[ix] = models[row['id']].predict(ft)\n        ix=ix+1\n    print(\"PREDICT SUCCESS\")\n    rmse_val=rmse(y_val['demand'],preds)\n    \n    print(\"valid's rmse: %f\\n\" %(rmse_val))\n    del idx_trn, idx_val, X_trn, X_val, y_trn, y_val\n    gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for idx_fold, (idx_trn, idx_val) in enumerate(cv.split(X_train, y_train)):\n    if idx_fold==0:\n        continue\n    print(f\"\\n----- Fold: ({idx_fold + 1} / {cv.get_n_splits()}) -----\\n\")\n    models={}\n    X_trn, X_val = X_train.iloc[idx_trn], X_train.iloc[idx_val]\n    y_trn, y_val = y_train.iloc[idx_trn], y_train.iloc[idx_val]\n    for item_id_sub,data_sub in X_trn.groupby('id'):  #train models seperately according to item_id\n        X_sub_train=data_sub[features].reset_index(drop=True)\n        y_sub_train=data_sub[['demand']].reset_index(drop=True)\n        models[item_id_sub]=LinearRegression().fit(X_sub_train,y_sub_train)\n        del X_sub_train, y_sub_train\n    print(\"MODEL TRAINED SUCCESS\")\n    preds = np.zeros(y_val.shape[0])\n    ix=0\n    size=y_val.shape[0]\n    for (idx, row) in X_val.iterrows(): \n        \n        print(size-ix)\n        ft=np.array(row[features].values.tolist()).reshape(1,-1)\n        preds[ix] = models[row['id']].predict(ft)\n        ix=ix+1\n    print(\"PREDICT SUCCESS\")\n    rmse_val=rmse(y_val['demand'],preds)\n    \n    print(\"valid's rmse: %f\\n\" %(rmse_val))\n    del idx_trn, idx_val, X_trn, X_val, y_trn, y_val\n    gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_val","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for idx_fold, (idx_trn, idx_val) in enumerate(cv.split(X_train, y_train)):\n    print(f\"\\n----- Fold: ({idx_fold + 1} / {cv.get_n_splits()}) -----\\n\")\n    models={}\n    X_trn, X_val = X_train.iloc[idx_trn], X_train.iloc[idx_val]\n    y_trn, y_val = y_train.iloc[idx_trn], y_train.iloc[idx_val]\n#     for item_id_sub,data_sub in X_trn.groupby('id'):  #train models seperately according to item_id\n#         X_sub_train=data_sub[features].reset_index(drop=True)\n#         y_sub_train=data_sub[['demand']].reset_index(drop=True)\n#         models[item_id_sub]=LinearRegression().fit(X_sub_train,y_sub_train)\n#         del X_sub_train, y_sub_train\n    \n    preds = np.zeros(y_val.shape[0])\n    size=y_val.shape[0]\n    ix=0\n    for (idx, row) in X_val.group.iterrows(): \n        \n        print(idx)\n        print(row['id'])\n        ft=np.array(row[features].values.tolist()).reshape(1,-1)\n        preds[ix] = models[row['id']].predict(ft)\n        ix=ix+1\n        print(ft)\n        #print(row[features])\n        #print(preds[0])\n    \n    #rmse_val=rmse(y_val['demand'],preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = np.zeros(y_val.shape[0])\nix=0\nfor (idx, row) in X_val.iterrows(): \n\n    print(idx)\n    print(row['id'])\n    ft=np.array(row[features].values.tolist()).reshape(1,-1)\n    preds[ix] = models[row['id']].predict(ft)\n    ix=ix+1\n    print(ft)\n    #print(row[features])\n    #print(preds[0])\n\nrmse_val=rmse(y_val['demand'],preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"\\n----- Fold: ({0 + 1} / {cv.get_n_splits()}) -----\\n\")\nprint(\"valid's rmse: 2.976036\\n\" %(rmse_val))\nprint(f\"\\n----- Fold: ({1 + 1} / {cv.get_n_splits()}) -----\\n\")\nprint(\"valid's rmse: 2.875921\\n\" %(rmse_val))\nprint(f\"\\n----- Fold: ({2 + 1} / {cv.get_n_splits()}) -----\\n\")\nprint(\"valid's rmse: 2.932364\\n\" %(rmse_val))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = [\n    # demand features\n    \"shift_t28\",\n    # std\n    \"rolling_std_t60\",\n    \"rolling_std_t90\",\n    # mean\n    \"rolling_mean_t7\",\n    \"rolling_mean_t30\",\n    \"rolling_mean_t60\",\n    \"rolling_mean_t90\",\n    \"rolling_mean_t180\",\n    # others\n    \"rolling_kurt_t30\",\n    # price features\n    \"sell_price\",\n    \"rolling_price_std_t7\",\n    # time features\n    \"month\",\n    \"week\",\n    \"dayofweek\",\n    \"is_weekend\",\n]\n\ndata_zero=data.fillna(0) # convert NaN to zero\nis_train = data_zero[\"d\"] < 1914\n\n# Attach \"d\" to X_train for cross validation.\nX_train = data_zero[is_train][['id']+[day_col] + features].reset_index(drop=True)\ny_train = data_zero[is_train][['id']+[\"demand\"]].reset_index(drop=True)\nX_test = data_zero[~is_train][['id']+features].reset_index(drop=True)\n\n\nfor idx_fold, (idx_trn, idx_val) in enumerate(cv.split(X_train, y_train)):\n    print(f\"\\n----- Fold: ({idx_fold + 1} / {cv.get_n_splits()}) -----\\n\")\n    models={}\n    X_trn, X_val = X_train.iloc[idx_trn], X_train.iloc[idx_val]\n    y_trn, y_val = y_train.iloc[idx_trn], y_train.iloc[idx_val]\n    print(X_trn)\n    print(y_trn)\n    print(X_val)\n    print(y_val)\n    break\n#     for item_id_sub,data_sub in X_trn.groupby('id'):  #train models seperately according to item_id\n#         X_sub_train=data_sub[features].reset_index(drop=True)\n#         y_sub_train=data_sub[['demand']].reset_index(drop=True)\n#         models[item_id_sub]=LinearRegression().fit(X_sub_train,y_sub_train)\n#         del X_sub_train, y_sub_train\n    \n#     train_set = lgb.Dataset(\n#         X_trn.drop(drop_when_train, axis=1),\n#         label=y_trn,\n#         categorical_feature=[\"item_id\"],\n#     )\n#     val_set = lgb.Dataset(\n#         X_val.drop(drop_when_train, axis=1),\n#         label=y_val,\n#         categorical_feature=[\"item_id\"],\n#     )\n\n#     model = lgb.train(\n#         bst_params,\n#         train_set,\n#         valid_sets=[train_set, val_set],\n#         valid_names=[\"train\", \"valid\"],\n#         **fit_params,\n#     )\n#     models.append(model)\n\n#     del idx_trn, idx_val, X_trn, X_val, y_trn, y_val\n#     gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## filter feature\nfrom sklearn.linear_model import LinearRegression\nfeatures = [\n    # demand features\n    \"shift_t28\",\n    # std\n    \"rolling_std_t60\",\n    \"rolling_std_t90\",\n    # mean\n    \"rolling_mean_t7\",\n    \"rolling_mean_t30\",\n    \"rolling_mean_t60\",\n    \"rolling_mean_t90\",\n    \"rolling_mean_t180\",\n    # others\n    \"rolling_kurt_t30\",\n    # price features\n    \"sell_price\",\n    \"rolling_price_std_t7\",\n    # time features\n    \"month\",\n    \"week\",\n    \"dayofweek\",\n    \"is_weekend\",\n]\n\nis_train = data_zero[\"d\"] < 1914 \n# keep these two columns to use later.\nid_date = data[~is_train][[\"id\", \"date\"]].reset_index(drop=True)\n\ndata_zero=data.fillna(0) # convert NaN to zero\n\nmodels={}\n\nfor item_id_sub,data_sub in data_zero[is_train].groupby('id'):  #train models seperately according to item_id\n    X_sub_train=data_sub[features].reset_index(drop=True)\n    y_sub_train=data_sub[['demand']].reset_index(drop=True)\n    models[item_id_sub]=LinearRegression().fit(X_sub_train,y_sub_train)\n    del X_sub_train, y_sub_train","execution_count":null,"outputs":[]},{"metadata":{"title":"[code]","trusted":true},"cell_type":"code","source":"X_test=X_train.fillna(0)\n\npreds = np.zeros(X_test.shape[0])\n\npreds = models.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"","execution_count":null},{"metadata":{"title":"[code]","trusted":true},"cell_type":"code","source":"def rmse(y_true, y_pred):\n    return np.sqrt(mean_squared_error(y_true, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# https://github.com/harupy/mlflow-extend","execution_count":null},{"metadata":{"title":"[code]","trusted":true},"cell_type":"code","source":"def make_submission(test, submission):\n    preds = test[[\"id\", \"date\", \"demand\"]]\n    preds = preds.pivot(index=\"id\", columns=\"date\", values=\"demand\").reset_index()\n    preds.columns = [\"id\"] + [\"F\" + str(d + 1) for d in range(DAYS_PRED)]\n\n    vals = submission[[\"id\"]].merge(preds, how=\"inner\", on=\"id\")\n    evals = submission[submission[\"id\"].str.endswith(\"evaluation\")]\n    final = pd.concat([vals, evals])\n\n    assert final.drop(\"id\", axis=1).isnull().sum().sum() == 0\n    assert final[\"id\"].equals(submission[\"id\"])\n\n    final.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"title":"[code]","trusted":true},"cell_type":"code","source":"make_submission(id_date.assign(demand=preds), submission)\nelapsed = (time.clock() - start)\nprint(\"Time used:\",elapsed)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}