{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport seaborn as sns\nsns.set()\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\nfrom fbprophet import Prophet\nfrom fbprophet.plot import plot_plotly\nimport plotly.offline as py\npy.init_notebook_mode()\n\nimport time\nfrom tqdm import tqdm_notebook as tqdm\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/m5-forecasting-accuracy/sales_train_validation.csv\")\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"calendar = pd.read_csv(\"/kaggle/input/m5-forecasting-accuracy/calendar.csv\")\ncalendar.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sell_prices = pd.read_csv(\"/kaggle/input/m5-forecasting-accuracy/sell_prices.csv\")\nsell_prices.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"series_cols = train.columns[train.columns.str.contains(\"d_\")].values\nlevel_cols = train.columns[train.columns.str.contains(\"d_\")==False].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_palette(\"colorblind\")\nfig, ax = plt.subplots(5,1,figsize=(20,28))\ntrain[series_cols].sum().plot(ax=ax[0])\nax[0].set_title(\"Top-Level-1: Summed product sales of all stores and states\")\nax[0].set_ylabel(\"Unit sales of all products\");\ntrain.groupby(\"state_id\")[series_cols].sum().transpose().plot(ax=ax[1])\nax[1].set_title(\"Level-2: Summed product sales of all stores per state\");\nax[1].set_ylabel(\"Unit sales of all products\");\ntrain.groupby(\"store_id\")[series_cols].sum().transpose().plot(ax=ax[2])\nax[2].set_title(\"Level-3: Summed product sales per store\")\nax[2].set_ylabel(\"Unit sales of all products\");\ntrain.groupby(\"cat_id\")[series_cols].sum().transpose().plot(ax=ax[3])\nax[3].set_title(\"Level-4: Summed product sales per category\")\nax[3].set_ylabel(\"Unit sales of all products\");\ntrain.groupby(\"dept_id\")[series_cols].sum().transpose().plot(ax=ax[4])\nax[4].set_title(\"Level-4: Summed product sales per product department\")\nax[4].set_ylabel(\"Unit sales of all products\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv(\"/kaggle/input/m5-forecasting-accuracy/sample_submission.csv\")\nsubmission.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.choice(submission.id.values, replace=False, size=15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def find_quartil(l):\n    \n    if \"0.005\" in l:\n        return 0.005\n    elif \"0.025\" in l:\n        return 0.025\n    elif \"0.165\" in l:\n        return 0.165\n    elif \"0.25\" in l:\n        return 0.25\n    elif \"0.5\" in l:\n        return 0.5\n    elif \"0.75\" in l:\n        return 0.75\n    elif \"0.835\" in l:\n        return 0.835\n    elif \"0.975\" in l:\n        return 0.975\n    elif \"0.995\" in l:\n        return 0.995\n    else:\n        return 0\n\ndef find_state(l):\n    if \"CA\" in l:\n        return \"CA\"\n    elif \"TX\" in l:\n        return \"TX\"\n    elif \"WI\" in l:\n        return \"WI\"\n    else:\n        return \"Unknown\"\n\ndef find_category(l):\n    if \"FOODS\" in l:\n        return \"foods\"\n    elif \"HOBBIES\" in l:\n        return \"hobbies\"\n    elif \"HOUSEHOLD\" in l:\n        return \"household\"\n    else:\n        return \"Unknown\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_eda = pd.DataFrame(submission.id, columns=[\"id\"])\nsubmission_eda.loc[:, \"lb_type\"] = np.where(submission.id.str.contains(\"validation\"), \"validation\", \"evaluation\")\nsubmission_eda.loc[:, \"u\"] = submission.id.apply(lambda l: find_quartil(l))\nsubmission_eda.loc[:, \"state\"] = submission.id.apply(lambda l: find_state(l))\nsubmission_eda.loc[:, \"category\"] = submission.id.apply(lambda l: find_category(l))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_palette(\"husl\")\n\nfig, ax = plt.subplots(3,3,figsize=(20,20))\nsns.countplot(submission_eda.u, ax=ax[0,0]);\nsns.countplot(submission_eda.lb_type, ax=ax[0,1]);\nsns.countplot(submission_eda.state, ax=ax[1,0]);\nsns.countplot(submission_eda.loc[submission_eda.lb_type==\"validation\"].state, ax=ax[1,1]);\nsns.countplot(submission_eda.loc[submission_eda.lb_type==\"evaluation\"].state, ax=ax[1,2]);\nsns.countplot(submission_eda.category, ax=ax[2,0]);\nsns.countplot(submission_eda.loc[submission_eda.lb_type==\"validation\"].category, ax=ax[2,1]);\nsns.countplot(submission_eda.loc[submission_eda.lb_type==\"evaluation\"].category, ax=ax[2,2]);\nfor n in range(1,3):\n    ax[n,2].set_title(\"in evaluation\")\n    ax[n,1].set_title(\"in validation\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def spl_denominator(train_series):\n    N = len(train_series)\n    sumup = 0\n    for n in range(1, N):\n        sumup += np.abs(train_series[n]-train_series[n-1])\n    return sumup/(N-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def spl_numerator(dev_series, Q, u):\n    sumup = 0\n    for m in range(len(dev_series)):\n        if Q[m] <= dev_series[m]:\n            sumup += (dev_series[m] - Q[m])*u\n        else:\n            sumup += (Q[m] - dev_series[m])*(1-u)\n    return sumup","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def spl(train_series, dev_series, Q, u):\n    h = len(dev_series)\n    spl_denomina = spl_denominator(train_series)\n    spl_numera = spl_numerator(dev_series, Q, u)\n    \n    return spl_numera/(h*spl_denomina)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"idx = 1000","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[level_cols].iloc[idx]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.loc[train.item_id==\"HOUSEHOLD_1_445\"].store_id.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,5))\nplt.plot(train[series_cols].iloc[idx].values, 'o')\nplt.title(\"Item 445 daily sales in shop CA_1\");\nplt.xlabel(\"observed days\")\nplt.ylabel(\"Unit sales\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"timeseries = train[series_cols].iloc[idx].values\nh = 28\n\ntrain_timeseries = timeseries[0:len(timeseries)-h]\ndev_timeseries = timeseries[(len(timeseries)-h)::]\n\nprint(len(train_timeseries), len(dev_timeseries))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"naive_val = train_timeseries[-1]\nnaive_Q = np.ones(dev_timeseries.shape) * naive_val\nnaive_Q","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"spl(train_timeseries, dev_timeseries, naive_Q, 0.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"naive_val","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"residuals = train_timeseries - naive_val","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1,2,figsize=(20,5))\nsns.distplot(residuals, ax=ax[0], kde=False)\nax[0].set_xlabel(\"residuals\")\nax[0].set_ylabel(\"frequency\");\nax[0].set_title(\"Distribution of residuals\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.mean(residuals)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"std_dev = np.std(residuals)\nstd_h = np.ones(dev_timeseries.shape)\n\nfor h in range(1, 29):\n    std_h[h-1] = std_dev * np.sqrt(h)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"std_h","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_lower = np.ones(len(std_h))\ny_upper = np.ones(len(std_h))\nfor h in range(len(std_h)):\n    low_val = naive_Q[h] - 2.58 * std_h[h]\n    if low_val < 0:\n        y_lower[h] = 0\n    else:\n        y_lower[h] = low_val\n    y_upper[h] = naive_Q[h] + 2.58 * std_h[h]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,5))\nplt.plot(y_lower, c=\"r\", label=\"0.005 boundary\")\nplt.plot(y_upper, c=\"g\", label=\"0.995 boundary\")\nplt.plot(naive_Q, 'o', c=\"b\", label=\"predicted value\")\nplt.title(\"Computing 99% PI for one timeseries of level 12\");\nplt.xlabel(\"time horizont h=28 days\")\nplt.ylabel(\"Unit sales\");\nplt.legend();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"timeseries = train[series_cols].sum().values\nlen(timeseries)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_timeseries = timeseries[0:-28]\neval_timeseries = timeseries[-28::]\nprint(len(train_timeseries), len(eval_timeseries))\ndays = np.arange(1, len(series_cols)+1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,5))\nplt.plot(days[0:-28], train_timeseries, label=\"train\")\nplt.plot(days[-28::], eval_timeseries, label=\"validation\")\nplt.title(\"Top-Level-1: Summed product sales of all stores and states\");\nplt.legend()\nplt.xlabel(\"Day\")\nplt.ylabel(\"Unit sales\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dates = calendar.iloc[0:len(timeseries)].date.values\ndf = pd.DataFrame(dates, columns=[\"ds\"])\ndf.loc[:, \"y\"] = timeseries\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = df.iloc[0:-28]\ntrain_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eval_df = df.iloc[-28::]\neval_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"uncertainty_interval_width = 0.25","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m = Prophet(interval_width=uncertainty_interval_width)\nm.fit(train_df)\nfuture = m.make_future_dataframe(periods=28)\nforecast = m.predict(future)\nforecast.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col_int = ['ds', 'yhat', 'yhat_lower', 'yhat_upper']\nforecast[col_int].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(forecast.iloc[-28::].yhat.values, 'o', label=\"predicted yhat\")\nplt.plot(eval_df.y.values, 'o-', label=\"target\")\nplt.legend();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plot_plotly(m, forecast)  \npy.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"uncertainty_interval_width = 0.25","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f_cols = [col for col in submission.columns if \"F\" in col]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_val = submission[submission.id.str.contains(\"validation\")].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plugin_total_predictions():\n    \n    for uncertainty_interval_width in [0.005, 0.025, 0.165, 0.25]:\n        upper = 1-uncertainty_interval_width\n        lower = uncertainty_interval_width\n    \n        m = Prophet(interval_width=uncertainty_interval_width)\n        m.fit(df)\n        future = m.make_future_dataframe(periods=28)\n        forecast = m.predict(future)\n    \n        submission_val.loc[\n            (submission_val.id.str.contains(\"Total\")) & (submission_val.id.str.contains(str(lower))),f_cols\n        ] = np.round(forecast.yhat_lower.values[-28::])\n    \n        submission_val.loc[\n            (submission_val.id.str.contains(\"Total\")) & (submission_val.id.str.contains(str(upper))),f_cols\n        ] = np.round(forecast.yhat_upper.values[-28::])\n    \n    submission_val.loc[\n        (submission_val.id.str.contains(\"Total\")) & (submission_val.id.str.contains(str(0.5))),f_cols\n    ] = forecast.yhat.values[-28::]\n    \n    return submission_val","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_val = plugin_total_predictions()\nsubmission_val.loc[submission_val.id.str.contains(\"Total\")]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ndevice","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class MyLSTM(nn.Module):\n    \n    def __init__(self, input_dim, hidden_dim, batch_size, num_layers=1, output_dim=1):\n        super().__init__()\n        self.input_dim = input_dim\n        self.hidden_dim = hidden_dim\n        self.batch_size = batch_size\n        self.num_layers = num_layers\n        \n        self.lstm = nn.LSTM(input_size=self.input_dim,\n                            hidden_size=self.hidden_dim,\n                            num_layers=self.num_layers,\n                            dropout = 0.25)\n        self.linear = nn.Linear(self.hidden_dim, output_dim)\n        \n    def init_hidden(self):\n        self.h_zero = torch.zeros(self.num_layers, self.batch_size, self.hidden_dim).to(device)\n        self.c_zero = torch.zeros(self.num_layers, self.batch_size, self.hidden_dim).to(device)\n    \n    def forward(self, x):\n        lstm_output, (h_n, c_n) = self.lstm(x.view(len(x), self.batch_size, -1),\n                                           (self.h_zero, self.c_zero))\n        last_time_step = lstm_output.view(self.batch_size, len(x), self.hidden_dim)[-1]\n        pred = self.linear(last_time_step)\n        return pred\n    \n\ndef train_model(model, data_dict, lr=1e-4, num_epochs=500):\n    \n    loss_fun = torch.nn.MSELoss(reduction=\"mean\")\n    optimiser = torch.optim.Adam(model.parameters(), lr=lr)\n    \n    train_losses = np.zeros(num_epochs)\n    phases = [\"train\", \"eval\"]\n    losses_dict = {\"train\": [], \"eval\": []}\n    predictions_dict = {\"train\": [], \"eval\": [] }\n    \n    for n in range(num_epochs):\n        \n        for phase in phases:\n            \n            x = data_dict[phase][\"input\"].to(device, dtype=torch.float)\n            y = data_dict[phase][\"target\"].to(device, dtype=torch.float)\n            \n            if phase == \"train\":\n                model.train()\n            else:\n                model.eval()\n        \n            optimiser.zero_grad()\n            \n            model.init_hidden()\n            y_pred = model(x)\n            \n            if n == (num_epochs-1):\n                predictions_dict[phase] = y_pred.float().cpu().detach().numpy()\n            \n            loss = loss_fun(y_pred.float(), y)\n            losses_dict[phase].append(loss.item())\n            \n            if n % 50 == 0:\n                print(\"{} loss: {}\".format(phase, loss.item()))\n            \n            if phase == 'train':\n                loss.backward()\n                optimiser.step()\n        \n    return losses_dict, predictions_dict\n\ndef create_sequences(timeseries, seq_len):\n    inputs = []\n    targets = []\n    \n    max_steps = len(timeseries) - (seq_len+1)\n    \n    for t in range(max_steps):\n        x = timeseries[t:(t+seq_len)]\n        y = timeseries[t+seq_len]\n        inputs.append(x)\n        targets.append(y)\n    \n    return np.array(inputs), np.array(targets)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"diff_series = np.diff(timeseries)\ntrain_size = np.int(0.7 * len(diff_series))\ntrain_diff_series = diff_series[0:train_size]\neval_diff_series = diff_series[train_size::]\nscaler = MinMaxScaler(feature_range=(-1,1))\nscaled_train = scaler.fit_transform(train_diff_series.reshape(-1, 1))\nscaled_eval = scaler.transform(eval_diff_series.reshape(-1,1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1,2,figsize=(20,5))\nax[0].plot(scaled_train, '-o', c=\"b\")\nax[1].plot(scaled_eval, '-o', c=\"g\")\nax[0].set_title(\"Single preprocessed top timeseries in train\")\nax[1].set_title(\"Single preprocessed top timeseries in eval\");\nax[0].set_xlabel(\"Days in dataset\")\nax[1].set_xlabel(\"Days in dataset\")\nax[0].set_ylabel(\"$\\Delta y$ scaled\")\nax[1].set_ylabel(\"$\\Delta y$ scaled\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seq_len = 400\ninput_dim = 1\nhidden_dim = 128\nnum_epochs = 600\nlr=0.0005\n\n\nx_train, y_train = create_sequences(scaled_train, seq_len)\nx_eval, y_eval = create_sequences(scaled_eval, seq_len)\n\nx_train = torch.from_numpy(x_train).float()\ny_train = torch.from_numpy(y_train).float()\nx_eval = torch.from_numpy(x_eval).float()\ny_eval = torch.from_numpy(y_eval).float()\n\ndata_dict = {\"train\": {\"input\": x_train, \"target\": y_train},\n             \"eval\": {\"input\": x_eval, \"target\": y_eval}}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = MyLSTM(input_dim=input_dim,\n               hidden_dim=hidden_dim,\n               batch_size=seq_len)\nmodel = model.to(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"run_training = True\nif run_training:\n    losses_dict, predictions_dict = train_model(model, data_dict, num_epochs=num_epochs, lr=lr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if run_training:\n    \n    fig, ax = plt.subplots(3,1,figsize=(20,20))\n    ax[0].plot(losses_dict[\"train\"], '.-', label=\"train\", c=\"red\")\n    ax[0].set_xlabel(\"Epochs\")\n    ax[0].set_ylabel(\"MSE\")\n    ax[0].plot(losses_dict[\"eval\"], '.-', label=\"eval\", c=\"blue\");\n    ax[0].legend();\n\n    ax[1].plot(predictions_dict[\"train\"], '-o', c=\"red\")\n    ax[1].plot(y_train, '-o', c=\"green\")\n    ax[1].set_title(\"Fitted and true values of y in train\");\n    ax[1].set_ylabel(\"Unit sales y\");\n    ax[1].set_xlabel(\"Number of days in train\");\n\n    ax[2].plot(predictions_dict[\"eval\"], '-o', c=\"red\")\n    ax[2].plot(y_eval, '-o', c=\"green\")\n    ax[2].set_title(\"Predicted and true values of y in eval\");\n    ax[2].set_xlabel(\"Number of days in eval\");\n    ax[2].set_ylabel(\"Unit sales y\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.graphics.tsaplots import plot_acf\n\nif run_training:\n    \n    train_residuals = y_train-predictions_dict[\"train\"]\n    eval_residuals = y_eval-predictions_dict[\"eval\"]\n    \n    fig, ax = plt.subplots(2,2,figsize=(20,10))\n    sns.distplot(train_residuals, ax=ax[0,0], color=\"red\")\n    sns.distplot(eval_residuals, ax=ax[0,1], color=\"green\")\n    ax[0,0].set_title(\"Train residuals\")\n    ax[0,1].set_title(\"Eval residuals\")\n    ax[0,0].set_xlabel(\"$y_{true} - y_{pred}$\")\n    ax[0,1].set_xlabel(\"$y_{true} - y_{pred}$\")\n    ax[0,0].set_ylabel(\"density\")\n    ax[0,1].set_ylabel(\"density\")\n    \n    plot_acf(train_residuals, ax=ax[1,0])\n    plot_acf(eval_residuals, ax=ax[1,1])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sampled_residuals = np.random.choice(train_residuals[:, 0], size=len(y_train), replace=True)\nsampled_residuals = sampled_residuals.reshape(-1,1)\nnew_response = predictions_dict[\"train\"] + sampled_residuals","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(2,2,figsize=(20,10))\nax[0,0].plot(predictions_dict[\"train\"][0:200], 'o-', color=\"purple\")\nax[0,0].set_title(\"Original fitted values $y_{pred}$ in \")\nax[0,0].set_xlabel(\"200 example days\")\nax[0,0].set_ylim(-0.4, 0.4)\nax[0,0].set_ylabel(\"$y_{fitted}$\")\n\nax[0,1].plot(new_response[0:200,0], 'o-', color=\"orange\")\nax[0,1].set_title(\"Response values $y^{*}$ using sampled residuals\");\nax[0,1].set_xlabel(\"200 example days\")\nax[0,1].set_ylabel(\"$y^{*}$\");\nax[0,1].set_ylim(-0.4, 0.4)\n\nax[1,0].plot(sampled_residuals[0:200], 'o-', color=\"cornflowerblue\")\nax[1,0].set_title(\"Sampled residuals\")\nax[1,0].set_xlabel(\"200 example days\")\nax[1,0].set_ylabel(\"$\\epsilon$\")\n\nax[1,1].plot(y_train[0:200], 'o-', color=\"firebrick\")\nax[1,1].set_title(\"True values $y_{train}$\")\nax[1,1].set_xlabel(\"200 example days\")\nax[1,1].set_ylabel(\"$y_{train}$\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"responses = []\nfor n in range(100):\n    # sample residuals using the historical residuals found in train\n    sampled_residuals = np.random.choice(train_residuals[:, 0], size=len(y_eval), replace=True)\n    sampled_residuals = sampled_residuals.reshape(-1,1)\n    # create a synthetic future timeseries of eval by adding sampled residuals\n    new_response = predictions_dict[\"eval\"] + sampled_residuals\n    # reverse the scaling\n    new_response = scaler.inverse_transform(new_response)\n    # concat the first value of the evaluation series and the response series\n    new_response = np.hstack((timeseries[train_size], new_response[:,0]))\n    # reverse the differnciation (trend removal) using cumsum\n    new_response = np.cumsum(new_response)\n    # save the future timeseries\n    responses.append(new_response)\n    \nresponses = np.array(responses)\nresponses.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_eval.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"median_series = np.median(responses, axis=0)\neval_series = scaler.inverse_transform(y_eval)\neval_series = np.cumsum(np.hstack((timeseries[train_size-1], eval_series[:,0])))\nlow_q = 0.25\nup_q = 0.75","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}