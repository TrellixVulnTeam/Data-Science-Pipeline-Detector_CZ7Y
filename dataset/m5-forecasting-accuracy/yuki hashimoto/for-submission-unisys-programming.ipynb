{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# UNISYS ハイスキルエンジニア プログラミング課題\n日本ユニシスの選考で使用するNotebook．<br>\n# 今回の方針\n- titanicで挫折して以来のKaggleであるので，いろんなNotebookを参考にしてできるだけ予測精度を上げる．\n- テーブルデータや需要の予測は今までやったことがないので初心者でも簡単に予測できるfbprophetで実装する\n\n# 準備\n## 今回使用するPythonパッケージのインポート","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nfrom tqdm.notebook import tqdm\n\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\n\nfrom fbprophet import Prophet\n\nfrom joblib import Parallel, delayed","metadata":{"execution":{"iopub.status.busy":"2021-11-15T13:29:41.907098Z","iopub.execute_input":"2021-11-15T13:29:41.907416Z","iopub.status.idle":"2021-11-15T13:29:41.913813Z","shell.execute_reply.started":"2021-11-15T13:29:41.907381Z","shell.execute_reply":"2021-11-15T13:29:41.913184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 使用するデータのロード\n### memoryの節約\n計算のコストやmemoryの使用量を削減するために型のキャストを行う．<br>\nmemoryの削減は[このNotebook](https://www.kaggle.com/omershect/learning-pytorch-lstm-deep-learning-with-m5-data)を参考にした．<br>\nこの関数は時間がかかるため，tqdmを用いてプログレスバーの表示機能を追加した．","metadata":{}},{"cell_type":"code","source":"def reduce_mem_usage(new_data: pd.DataFrame, verbose: bool=True) -> pd.DataFrame:\n    \"\"\"\n    メモリの使用量を削減する\n\n    Parameters\n    ----------\n    new_data : pd.DataFrame\n        memoryを削減したいテーブルデータ\n    verbose : bool\n        memory削減量の標準出力の有無\n\n    Returns\n    -------\n    new_data : new_data.DataFrame\n        memory削減後テーブルデータ\n    \"\"\"\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']    # 想定されるデータの型\n    start_mem = new_data.memory_usage().sum() / 1024**2    # 初期のメモリの使用量\n    \n    # 列ごとの型指定を行うためのループ\n    for col in tqdm(new_data.columns):\n        col_type = new_data[col].dtypes\n        # 型が数値型の場合はキャストする\n        if col_type in numerics: \n            c_min = new_data[col].min()\n            c_max = new_data[col].max()\n            # 型がint型の場合\n            if str(col_type)[:3] == 'int':\n                # 最小値と最大値の値に応じて適宜キャスト\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    new_data[col] = new_data[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    new_data[col] = new_data[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    new_data[col] = new_data[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    new_data[col] = new_data[col].astype(np.int64)  \n            # 型がfloat型の場合\n            else:\n                # 最小値と最大値の値に応じて適宜キャスト\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    new_data[col] = new_data[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    new_data[col] = new_data[col].astype(np.float32)\n                else:\n                    new_data[col] = new_data[col].astype(np.float64)    \n    end_mem = new_data.memory_usage().sum() / 1024**2    # 削減後のメモリの使用量\n    if verbose:    # 標準出力\n        print(f'Mem. usage decreased to {end_mem:5.2f} Mb ({(100 * (start_mem - end_mem) / start_mem):.1f}% reduction)')\n    return new_data","metadata":{"execution":{"iopub.status.busy":"2021-11-15T13:29:41.916184Z","iopub.execute_input":"2021-11-15T13:29:41.916804Z","iopub.status.idle":"2021-11-15T13:29:41.933236Z","shell.execute_reply.started":"2021-11-15T13:29:41.91675Z","shell.execute_reply":"2021-11-15T13:29:41.932565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### データの事前知識\n- **calendar.csv**<br>\nContains information about the dates on which the products are sold.<br>\n(製品が販売された日付の情報を含む.)<br>\n- **sales_train_validation.csv**<br>\nContains the historical daily unit sales data per product and store [d_1 - d_1913]<br>\n(製品別・店舗別の過去の日次販売台数データ[d_1 - d_1913]を収録.)<br>\n- **sample_submission.csv**<br>\nThe correct format for submissions. Reference the Evaluation tab for more info.<br>\n(提出するための正しいフォーマットです．詳細は「評価」タブを参照してください．)<br>\n- **sell_prices.csv**<br>\nContains information about the price of the products sold per store and date.<br>\n(店舗ごと・日付ごとの販売商品の価格情報を収録．)<br>\n- **sales_train_evaluation.csv**<br>\nIncludes sales [d_1 - d_1941] (labels used for the Public leaderboard)<br>\n(セールスを含む [d_1 - d_1941] （パブリック・リーダーボードに使用されるラベル）)<br>\n\n先ほど作った関数を利用してデータを読み込む","metadata":{}},{"cell_type":"code","source":"# load data with reducing memory usage\ncalender = reduce_mem_usage(pd.read_csv('../input/m5-forecasting-accuracy/calendar.csv'))\nvalidate = reduce_mem_usage(pd.read_csv('../input/m5-forecasting-accuracy/sales_train_validation.csv'))\nsample_submission = reduce_mem_usage(pd.read_csv('../input/m5-forecasting-accuracy/sample_submission.csv'))\nprices = reduce_mem_usage(pd.read_csv('../input/m5-forecasting-accuracy/sell_prices.csv'))\nevaluate = reduce_mem_usage(pd.read_csv('../input/m5-forecasting-accuracy/sales_train_evaluation.csv'))","metadata":{"execution":{"iopub.status.busy":"2021-11-15T13:29:41.934277Z","iopub.execute_input":"2021-11-15T13:29:41.934731Z","iopub.status.idle":"2021-11-15T13:34:53.822484Z","shell.execute_reply.started":"2021-11-15T13:29:41.934684Z","shell.execute_reply":"2021-11-15T13:34:53.821473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA\n## データの確認\nどのようなテーブルデータが入っているかや欠損値がないかなどを確認しないことには可視化もできないのでまずはデータを表示してみる．\n### calender.csvについて","metadata":{}},{"cell_type":"code","source":"# show head of dataframe\ncalender.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-15T13:34:53.82533Z","iopub.execute_input":"2021-11-15T13:34:53.825672Z","iopub.status.idle":"2021-11-15T13:34:53.851749Z","shell.execute_reply.started":"2021-11-15T13:34:53.825627Z","shell.execute_reply":"2021-11-15T13:34:53.850842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### headerの内訳\n- date           : ハイフンで区切ってある日付 (xxxx-yy-zz)<br>\n- wm_yr_wl       : 土曜日に+1され，1年毎に100の位が+1され10の位以下はリセットされる<br>\n- weekday        : 曜日 (Saturday, Sunday, ...)<br>\n- wday           : 曜日を数字で表したもの (Sat=1, Sun=2, ...)<br>\n- month, year    : それぞれ月，年 (1, 2, ... | 2011, 2012, ...)<br>\n- d              : 通し番号 (d1, d2, ...)<br>\n- event_name_x   : イベントの名前，1日に2つある場合はevent_name_2に入力してある (SuperBowl, ValentinesDay)<br>\n- event_type_x   : イベントのタイプ (Sporting, Cultural, National)<br>\n- snap_XX        : 金券のようなもの．CA(California)，TX(Texas)，WI(Wisconsin)の3種類","metadata":{}},{"cell_type":"code","source":"# null_check\ncalender.isnull().sum().sort_values(ascending = False)","metadata":{"execution":{"iopub.status.busy":"2021-11-15T13:34:53.852961Z","iopub.execute_input":"2021-11-15T13:34:53.853204Z","iopub.status.idle":"2021-11-15T13:34:53.867152Z","shell.execute_reply.started":"2021-11-15T13:34:53.853174Z","shell.execute_reply":"2021-11-15T13:34:53.866131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`event_name`と`event_type`には`NaN`が含まれているので扱い注意\n### sales_train_validation.csvについて","metadata":{}},{"cell_type":"code","source":"# show head of dataframe\nvalidate.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-15T13:34:53.868874Z","iopub.execute_input":"2021-11-15T13:34:53.869184Z","iopub.status.idle":"2021-11-15T13:34:53.908352Z","shell.execute_reply.started":"2021-11-15T13:34:53.869139Z","shell.execute_reply":"2021-11-15T13:34:53.907449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### headerの内訳\n- id             : カテゴリー・店舗などの情報を持ったid (xxxx-yy-zz)<br>\n- item_id        : アイテムのid<br>\n- dept_id        : 部門のid<br>\n- cat_id         : カテゴリーのid<br>\n- store_id       : 店舗id<br>\n- state_id       : 州のid<br>\n- d_x            : x日目に売れた数","metadata":{}},{"cell_type":"code","source":"# null check\nvalidate.isnull().sum().sort_values(ascending = False)","metadata":{"execution":{"iopub.status.busy":"2021-11-15T13:34:53.909625Z","iopub.execute_input":"2021-11-15T13:34:53.90988Z","iopub.status.idle":"2021-11-15T13:34:54.252284Z","shell.execute_reply.started":"2021-11-15T13:34:53.909851Z","shell.execute_reply":"2021-11-15T13:34:54.251537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### sample_submission.csvについて","metadata":{}},{"cell_type":"code","source":"# show head of dataframe\nsample_submission.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-15T13:34:54.253577Z","iopub.execute_input":"2021-11-15T13:34:54.253842Z","iopub.status.idle":"2021-11-15T13:34:54.273544Z","shell.execute_reply.started":"2021-11-15T13:34:54.253812Z","shell.execute_reply":"2021-11-15T13:34:54.272814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### headerの内訳\n- id             : カテゴリー・店舗などの情報を持ったid (xxxx-yy-zz)<br>\n- F1~28          : その日の売り上げ<br>\n\n### sell_prices.csvについて","metadata":{}},{"cell_type":"code","source":"# show head of dataframe\nprices.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-15T13:34:54.27476Z","iopub.execute_input":"2021-11-15T13:34:54.275099Z","iopub.status.idle":"2021-11-15T13:34:54.284887Z","shell.execute_reply.started":"2021-11-15T13:34:54.27507Z","shell.execute_reply":"2021-11-15T13:34:54.283792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### headerの内訳\n- store_id       : 店舗id<br>\n- item_id        : アイテムのid<br>\n- wm_yr_wk       : 土曜日に+1され，1年毎に100の位が+1され10の位以下はリセットされる\n- sell_price     : 商品の値段","metadata":{}},{"cell_type":"code","source":"# null check\nprices.isnull().sum().sort_values(ascending = False)","metadata":{"execution":{"iopub.status.busy":"2021-11-15T13:34:54.287433Z","iopub.execute_input":"2021-11-15T13:34:54.287665Z","iopub.status.idle":"2021-11-15T13:34:55.810466Z","shell.execute_reply.started":"2021-11-15T13:34:54.287631Z","shell.execute_reply":"2021-11-15T13:34:55.809705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### sales_train_evaluation.csvについて","metadata":{}},{"cell_type":"code","source":"# show head of dataframe\nevaluate.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-15T13:34:55.811554Z","iopub.execute_input":"2021-11-15T13:34:55.811796Z","iopub.status.idle":"2021-11-15T13:34:56.156798Z","shell.execute_reply.started":"2021-11-15T13:34:55.811767Z","shell.execute_reply":"2021-11-15T13:34:56.155962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# null check\nevaluate.isnull().sum().sort_values(ascending = False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## データの可視化\nどのような形式でデータが格納されているかは確認できた．<br>\n普段Notebookは使わないが，matplotlibよりplotlyの方がグラフの拡大や値の参照などができ便利そう．<br>\n勉強がてら，今回はplotlyでグラフの表示をしてみる\n### **sales_train_validation.csv**について\n#### 適当なデータでの需要","metadata":{}},{"cell_type":"code","source":"# 日付データは今後の可視化で必要なので配列として抜き出す\ndate_list = calender.date.to_list()\n\n# validateのdfのcolumnsからd_のものを抜き出す\nids = sorted(list(set(validate['id'])))\nd_cols = [c for c in validate.columns if 'd_' in c]\n\n# 適当にデータを抜き出す\nx_1 = validate.loc[10, d_cols].to_list()\nx_2 = validate.loc[100, d_cols].to_list()\nx_3 = validate.loc[1000, d_cols].to_list()\n\n# plotlyを利用した可視化\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=date_list[:len(x_1)], y=x_1,\n                    mode='lines', name=validate.iloc[10,0],opacity=0.5))\n\nfig.add_trace(go.Scatter(x=date_list[:len(x_2)], y=x_2,\n                    mode='lines', name=validate.iloc[100,0],opacity=0.5))\n\nfig.add_trace(go.Scatter(x=date_list[:len(x_3)], y=x_3,\n                    mode='lines', name=validate.iloc[1000,0],opacity=0.5))\n\n# データスタンプの存在するグラフを扱いやすくする\nfig.update_layout(\n    xaxis=dict(\n        rangeselector=dict(\n            buttons=list([\n                dict(count=1, label=\"month\", step=\"month\", stepmode=\"backward\"),\n                dict(count=6, label=\"6month\", step=\"month\", stepmode=\"backward\"),\n                dict(count=1, label=\"year\", step=\"year\", stepmode=\"backward\"),\n                dict(step=\"all\")])),\n        rangeslider=dict(visible=True),\n        type=\"date\"))\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-15T13:34:56.157889Z","iopub.execute_input":"2021-11-15T13:34:56.158115Z","iopub.status.idle":"2021-11-15T13:34:56.519463Z","shell.execute_reply.started":"2021-11-15T13:34:56.158087Z","shell.execute_reply":"2021-11-15T13:34:56.518531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"初めてのplotlyだが，かなり便利な予感がする．<br>\nとりあえず，適当に取り出したサンプルから分かることは以下のこと\n- HOBBIE_1_011のように需要に波がある商品があるということ\n- HOBBIE_1_105のようにある程度通年需要がある商品があること\n- いずれにしてもかなりノイジーなデータであること\n\n次は店舗ごとの需要の合計を見てみる<br>\nこれを見ればもう少し傾向が見えてくるだろう<br>\nこの後で数回グラフを表示するので，関数を作っておく","metadata":{}},{"cell_type":"code","source":"def line_graph(df: pd.DataFrame, opacity=1, use_rolling: bool=True, window: int=7, colors=px.colors.qualitative.Plotly) -> None:\n    \"\"\"\n    折れ線グラフを表示する\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        グラフを表示させたいテーブルデータ\n    opacity : int\n        グラフの濃さ\n    use_rolling : bool\n        移動平均のグラフを表示するかどうか\n    window : int\n        移動平均を利用する際の窓幅\n    colors :\n        グラフの色\n    \"\"\"\n    org_opacity = opacity\n    fig = go.Figure()\n    data_length = len(df.columns)\n    for i, (idx, row) in enumerate(df.iterrows()):\n        # 移動平均\n        if use_rolling:\n            fig.add_trace(go.Scatter(x=date_list[:data_length], y=row.rolling(window).mean(), line=dict(color=colors[i]),\n                            mode='lines', name=f'{idx}_rolling',opacity=opacity))\n            opacity = 0.2\n        fig.add_trace(go.Scatter(x=date_list[:data_length], y=row, line=dict(color=colors[i]),\n                    mode='lines', name=idx,opacity=opacity))\n        opacity=org_opacity\n    # データスタンプの存在するグラフを扱いやすくする\n    fig.update_layout(\n        xaxis=dict(\n            rangeselector=dict(\n                buttons=list([\n                    dict(count=1, label=\"month\", step=\"month\", stepmode=\"backward\"),\n                    dict(count=6, label=\"6month\", step=\"month\", stepmode=\"backward\"),\n                    dict(count=1, label=\"year\", step=\"year\", stepmode=\"backward\"),\n                    dict(step=\"all\")])),\n            rangeslider=dict(visible=True),\n            type=\"date\"))\n    fig.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-15T13:34:56.521247Z","iopub.execute_input":"2021-11-15T13:34:56.521863Z","iopub.status.idle":"2021-11-15T13:34:56.537506Z","shell.execute_reply.started":"2021-11-15T13:34:56.521815Z","shell.execute_reply":"2021-11-15T13:34:56.536733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 店舗ごとの需要","metadata":{}},{"cell_type":"code","source":"# 店舗毎の合計を求める\nstore_sales = validate.groupby('store_id').sum()\n# グラフの表示\nline_graph(store_sales, use_rolling=True)","metadata":{"execution":{"iopub.status.busy":"2021-11-15T13:34:56.539051Z","iopub.execute_input":"2021-11-15T13:34:56.539585Z","iopub.status.idle":"2021-11-15T13:34:57.656939Z","shell.execute_reply.started":"2021-11-15T13:34:56.539548Z","shell.execute_reply":"2021-11-15T13:34:57.656334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"店舗ごとに需要の合計と1週間の移動平均をプロットしてみたところ以下のような特徴が見えてきた<br>\n- 12/25クリスマスはwalmartがクリスマス休日であることから毎年売れ行きが0近くになっている(なぜか売れている商品もあるが．．．)\n- 1週間ごとの周期があり，週末の需要が高い\n- 1か月ごとの周期があり，月初めの需要が高い\n- 1年ごとの周期があり，夏の需要が高い\n- 全体的に売り上げが伸びている．\n- 売り上げが増加する時期もあれば減少する時期もある(需要のトレンド)\n\n全ての店舗がこれに当てはまるわけではないが，直感とあった傾向であるので予測モデルに組み込みたい<br>\n同様にカテゴリごとのグラフもプロットしてみる\n#### カテゴリごとの需要","metadata":{}},{"cell_type":"code","source":"# カテゴリ毎の合計を求める\ndept_sales = validate.groupby('dept_id').sum()\n# グラフの表示\nline_graph(dept_sales, use_rolling=True)","metadata":{"execution":{"iopub.status.busy":"2021-11-15T13:34:57.6582Z","iopub.execute_input":"2021-11-15T13:34:57.658599Z","iopub.status.idle":"2021-11-15T13:34:58.626066Z","shell.execute_reply.started":"2021-11-15T13:34:57.658567Z","shell.execute_reply":"2021-11-15T13:34:58.625247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"周期に関しては店舗ごとのグラフと同様の傾向．<br>\nカテゴリ別で特有なのは\n- HOUSEHOLD_1が年々売り上げを伸ばしていること\n- FOOD_3の売り上げが顕著に高い\n- HOBBIES_2はほぼ売れない\n\n周期性は見えてきたので今度は棒グラフで売り上げなどの傾向を見てみる<br>\nこちらはボックスプロットで表示したいので，こちらも関数を作っておく","metadata":{}},{"cell_type":"code","source":"def box_plot(df: pd.DataFrame, colors=px.colors.qualitative.Plotly) -> None:\n    \"\"\"\n    折れ線グラフを表示する\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        グラフを表示させたいテーブルデータ\n    colors :\n        グラフの色\n    \"\"\"\n    fig = go.Figure()\n    for i, (idx, row) in enumerate(df.iterrows()):\n        fig.add_trace(go.Box(\n            y=row,\n            name=idx,\n            marker_color=colors[i],\n            boxmean=True # represent mean\n        ))\n    fig.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-15T13:34:58.627276Z","iopub.execute_input":"2021-11-15T13:34:58.627505Z","iopub.status.idle":"2021-11-15T13:34:58.633657Z","shell.execute_reply.started":"2021-11-15T13:34:58.627476Z","shell.execute_reply":"2021-11-15T13:34:58.63288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 店舗ごとの需要","metadata":{}},{"cell_type":"code","source":"box_plot(store_sales)","metadata":{"execution":{"iopub.status.busy":"2021-11-15T13:34:58.634958Z","iopub.execute_input":"2021-11-15T13:34:58.635272Z","iopub.status.idle":"2021-11-15T13:34:58.682895Z","shell.execute_reply.started":"2021-11-15T13:34:58.635242Z","shell.execute_reply":"2021-11-15T13:34:58.682108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"店舗による需要の差は大きい．<br>\n例えばCA_3は平均して6000個の需要が毎日あるのに対し，CA_4は2000個の需要である\n#### カテゴリごとの需要","metadata":{}},{"cell_type":"code","source":"box_plot(dept_sales)","metadata":{"execution":{"iopub.status.busy":"2021-11-15T13:34:58.684322Z","iopub.execute_input":"2021-11-15T13:34:58.68497Z","iopub.status.idle":"2021-11-15T13:34:58.707055Z","shell.execute_reply.started":"2021-11-15T13:34:58.684931Z","shell.execute_reply":"2021-11-15T13:34:58.706331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"カテゴリによる需要の差は大きい．<br>\n例えばFOODS_3は平均して17000個の需要が毎日あるのに対し，HOBBIES_2は0個の需要である．<br>\nまた，データのばらつきもFOOD_3は大きいがそのほかは比較的小さい<br>\n\n以上のように店舗やカテゴリ特有の需要があることが分かるため，機械学習で扱う際は店舗・カテゴリごとに標準化したい．\n# 機械学習\nある程度EDAが終わり，3つの周期性があることや土日の売り上げが高いことなどが分かった．<br>\nこのような情報をもとに予測を行いたい．<br>\nまずはテーブルデータを機械学習で扱いやすい形に変形していく．<br>\n## データの前処理\n機械学習では入力の値を標準化することが多い．<br>\n正規化するものや[-1,1]の範囲に李スケールするものなどがある．<br>\n今回はボックスプロットからわかった店舗・カテゴリごとのばらつきを考慮して，店舗・カテゴリごとの需要割合で標準化する([参考](https://www.kaggle.com/raghvenbhati/prophet-forecasts))．<br>\nまずは`evaluate`のデータから店舗とカテゴリでグルーピングした集合テーブルを作る（分母となる）","metadata":{}},{"cell_type":"code","source":"sum_group = evaluate.groupby(['dept_id','store_id'], as_index=False).sum()\n_denominator_ave = sum_group[['dept_id','store_id']]\n\ndenominator_ave = pd.DataFrame(sum_group.iloc[:,1550:].mean(axis=1),columns=['denominator_ave'])\ndenominator_ave = pd.concat([_denominator_ave, denominator_ave], axis=1)\ndenominator_ave.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-15T13:34:58.708292Z","iopub.execute_input":"2021-11-15T13:34:58.708728Z","iopub.status.idle":"2021-11-15T13:34:59.561975Z","shell.execute_reply.started":"2021-11-15T13:34:58.708696Z","shell.execute_reply":"2021-11-15T13:34:59.561404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"次は分子を計算する","metadata":{}},{"cell_type":"code","source":"index = evaluate[['id','dept_id','store_id']]\nnumerator_ave = pd.DataFrame(evaluate.iloc[:,1550:].mean(axis=1),columns=['numerator_ave'])\nnumerator_ave = pd.concat([index, numerator_ave], axis=1)\nnumerator_ave.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-15T13:34:59.562995Z","iopub.execute_input":"2021-11-15T13:34:59.563349Z","iopub.status.idle":"2021-11-15T13:34:59.602422Z","shell.execute_reply.started":"2021-11-15T13:34:59.563315Z","shell.execute_reply":"2021-11-15T13:34:59.601891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"割合を追加する","metadata":{}},{"cell_type":"code","source":"fraction = pd.merge(numerator_ave, denominator_ave, on =['dept_id','store_id'])\nfraction['fraction_ave'] = fraction['numerator_ave']/fraction['denominator_ave']\nfraction","metadata":{"execution":{"iopub.status.busy":"2021-11-15T13:34:59.603728Z","iopub.execute_input":"2021-11-15T13:34:59.604053Z","iopub.status.idle":"2021-11-15T13:34:59.641731Z","shell.execute_reply.started":"2021-11-15T13:34:59.604012Z","shell.execute_reply":"2021-11-15T13:34:59.640866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"fbprophetでは不定期に表れる休日を考慮した予測ができるので，イベントが起こった日付を取り出す","metadata":{}},{"cell_type":"code","source":"holiday1 = calender.iloc[858:1969,].loc[calender['event_name_1'].notnull()][['event_name_1','date']].rename(columns={'event_name_1':'holiday','date':'ds'})\nholiday2 = calender.iloc[858:1969,].loc[calender['event_name_1'].notnull()][['event_type_1','date']].rename(columns={'event_type_1':'holiday','date':'ds'})\nholiday3 = calender.iloc[858:1969,].loc[calender['event_name_2'].notnull()][['event_name_2','date']].rename(columns={'event_name_2':'holiday','date':'ds'})\nholiday4 = calender.iloc[858:1969,].loc[calender['event_name_2'].notnull()][['event_type_2','date']].rename(columns={'event_type_2':'holiday','date':'ds'})\nholidays = pd.concat((holiday1, holiday2,holiday3,holiday4))\nholidays.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-15T13:34:59.643084Z","iopub.execute_input":"2021-11-15T13:34:59.643391Z","iopub.status.idle":"2021-11-15T13:34:59.670695Z","shell.execute_reply.started":"2021-11-15T13:34:59.64335Z","shell.execute_reply":"2021-11-15T13:34:59.66987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 学習と予測\nデータの前処理が終わったので，実際に学習と予測に移る．<br>\n今回はfbprophetを利用して予測を行う．<br>\n今回データ分析から見えた情報を含む以下の条件を追加でモデルに組み込む\n- EDAで発見できたデータの周期\n- 休日やイベントがある日\n- 年月日の情報（年々売り上げが伸びていたり，トレンドがあるため）","metadata":{}},{"cell_type":"code","source":"def prophet(i):\n    \"\"\"\n    需要の予測をする関数\n\n    Parameters\n    ----------\n    i : int\n        予測したいpandasの行\n\n    Returns\n    -------\n    pred: pd.DataFrame\n        予測テーブルデータ\n    \"\"\"\n    # 予測モデルのインスタンス化\n    # yealy_seasonality大きいほど周期性を強く考慮する\n    # 休日のデータはここで引数として入力する\n    m = Prophet(yearly_seasonality=24, holidays=holidays)\n    \n    # 周期性の指定(EDAで分かった3つの周期を考慮する)\n    m.add_seasonality(name='yearly', period=365, fourier_order=10)\n    m.add_seasonality(name='monthly', period=365/12, fourier_order=10)\n    m.add_seasonality(name='weekly', period=7, fourier_order=5)\n    \n    # 予測したいモデルの入力\n    # ds:datestamp y:正解ラベル\n    tsdf = pd.DataFrame({\n      'ds': pd.to_datetime(calender.iloc[858:1941,]['date'].reset_index(drop=True)),\n      'y': sum_group.iloc[i,860:1943].reset_index(drop=True),\n    })\n    # 説明変数の追加（年月日の情報を追加する）\n    tsdf['wday']=calender.iloc[858:1941,]['wday'].reset_index(drop=True)\n    tsdf['month']=calender.iloc[858:1941,]['month'].reset_index(drop=True)\n    tsdf['year']=calender.iloc[858:1941,]['year'].reset_index(drop=True)\n    #m.add_regressor('sell_price')\n    m.add_regressor('wday')\n    m.add_regressor('month')\n    m.add_regressor('year')\n    # 学習\n    m.fit(tsdf)\n    # 予測する範囲を指定\n    future = m.make_future_dataframe(periods=28)\n    future['wday']=calender.iloc[858:1969,]['wday'].reset_index(drop=True)\n    future['month']=calender.iloc[858:1969,]['month'].reset_index(drop=True)\n    future['year']=calender.iloc[858:1969,]['year'].reset_index(drop=True)\n    # 予測\n    forecast = m.predict(future)\n    pred = pd.DataFrame(forecast.iloc[1083:1112,]['yhat'])\n    pred['dept_id']=denominator_ave.iloc[i,]['dept_id']\n    pred['store_id']=denominator_ave.iloc[i,]['store_id']\n    return pred","metadata":{"execution":{"iopub.status.busy":"2021-11-15T13:34:59.671812Z","iopub.execute_input":"2021-11-15T13:34:59.672019Z","iopub.status.idle":"2021-11-15T13:34:59.685025Z","shell.execute_reply.started":"2021-11-15T13:34:59.671994Z","shell.execute_reply":"2021-11-15T13:34:59.684388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fbprophetはGPUに対応していないのでCPUの並列計算で処理速度を担保する\npred = Parallel(n_jobs=-1)(delayed(prophet)(i) for i in range(denominator_ave.shape[0]))\n\n# 各商品ごとに推測された需要を組み合わせる\npreds = pd.concat(pred[0:70])\npreds['period']=preds.index\npreds['period']=preds['period']-1082\n\n# 提出のフォーマットに適した形に変形する\npivot_preds=preds.pivot_table(index=['dept_id','store_id'], columns='period', values='yhat')\nprev_submission = pd.merge(fraction, pivot_preds, on =['dept_id','store_id'])\nfor i in range(28):\n    # 標準化した値をもとに戻す\n    prev_submission.iloc[:,(6+i)] = prev_submission.iloc[:,(6+i)]*prev_submission['fraction_ave']\n\nsubmission=prev_submission[['id', 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28]]\nsubmission.columns = sample_submission.columns\nsubmission_index=sample_submission[['id']]\nsubmission = pd.merge(submission_index, submission, on = 'id', how = 'left')\nsubmission = submission.fillna(0)    # 欠損値を0に置き換える\nsubmission.to_csv('submission.csv',index=False)    # 提出用のcsvファイルを作成する","metadata":{"execution":{"iopub.status.busy":"2021-11-15T13:34:59.68637Z","iopub.execute_input":"2021-11-15T13:34:59.686693Z","iopub.status.idle":"2021-11-15T13:36:51.722166Z","shell.execute_reply.started":"2021-11-15T13:34:59.686649Z","shell.execute_reply":"2021-11-15T13:36:51.721313Z"},"trusted":true},"execution_count":null,"outputs":[]}]}