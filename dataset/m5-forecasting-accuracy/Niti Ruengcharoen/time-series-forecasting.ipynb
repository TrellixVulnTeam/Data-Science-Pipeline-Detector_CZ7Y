{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport gc\nimport time\nimport math\nimport datetime\nfrom math import log, floor\nfrom sklearn.neighbors import KDTree\n\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom sklearn.utils import shuffle\nfrom tqdm.notebook import tqdm as tqdm\n\nimport seaborn as sns\nfrom matplotlib import colors\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import Normalize\n\nfrom plotly.subplots import make_subplots\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\n\n# import pywt\nfrom statsmodels.robust import mad\n\nimport scipy\nimport statsmodels\nfrom scipy import signal\nimport statsmodels.api as sm\nfrom fbprophet import Prophet\nfrom scipy.signal import butter, deconvolve\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load data"},{"metadata":{"trusted":true},"cell_type":"code","source":"calendar = pd.read_csv('../input/m5-forecasting-accuracy/calendar.csv')\n\n# sales_train_eval = pd.read_csv('../input/m5-forecasting-accuracy/sales_train_evaluation.csv')\n\nsales_train_val = pd.read_csv('../input/m5-forecasting-accuracy/sales_train_validation.csv')\n\nsell_prices = pd.read_csv('../input/m5-forecasting-accuracy/sell_prices.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# #Add zero sales for the remaining days 1942-1969\n# for d in range(1942,1970):\n#     col = 'd_' + str(d)\n#     sales_train_val[col] = 0\n#     sales_train_val[col] = sales_train_val[col].astype(np.int16)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Before reduce size\nsales_train_val_size = np.round(sales_train_val.memory_usage().sum()/(1024*1024),1)\n# sales_train_eval_size = np.round(sales_train_eval.memory_usage().sum()/(1024*1024),1)\ncalendar_size = np.round(calendar.memory_usage().sum()/(1024*1024),1)\nsell_prices_size = np.round(sell_prices.memory_usage().sum()/(1024*1024),1)\n\nprint(f'sales_train_val_size   {sales_train_val_size} byte')\nprint(f'calendar_size          {calendar_size} byte')\nprint(f'sell_prices_size       {sell_prices_size} byte')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n#Reduce size in order to save memory\ndef downcast(df):\n    cols = df.dtypes.index.tolist()\n    types = df.dtypes.values.tolist()\n    for i,t in enumerate(types):\n        if 'int' in str(t):\n            if df[cols[i]].min() > np.iinfo(np.int8).min and df[cols[i]].max() < np.iinfo(np.int8).max:\n                df[cols[i]] = df[cols[i]].astype(np.int8)\n            elif df[cols[i]].min() > np.iinfo(np.int16).min and df[cols[i]].max() < np.iinfo(np.int16).max:\n                df[cols[i]] = df[cols[i]].astype(np.int16)\n            elif df[cols[i]].min() > np.iinfo(np.int32).min and df[cols[i]].max() < np.iinfo(np.int32).max:\n                df[cols[i]] = df[cols[i]].astype(np.int32)\n            else:\n                df[cols[i]] = df[cols[i]].astype(np.int64)\n        elif 'float' in str(t):\n            if df[cols[i]].min() > np.finfo(np.float16).min and df[cols[i]].max() < np.finfo(np.float16).max:\n                df[cols[i]] = df[cols[i]].astype(np.float16)\n            elif df[cols[i]].min() > np.finfo(np.float32).min and df[cols[i]].max() < np.finfo(np.float32).max:\n                df[cols[i]] = df[cols[i]].astype(np.float32)\n            else:\n                df[cols[i]] = df[cols[i]].astype(np.float64)\n        elif t == np.object:\n            if cols[i] == 'date':\n                df[cols[i]] = pd.to_datetime(df[cols[i]], format='%Y-%m-%d')\n            \n    return df \n\nsales_train_val = downcast(sales_train_val)\n# sales_train_eval = downcast(sales_train_eval)\ncalendar = downcast(calendar)\nsell_prices = downcast(sell_prices)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#After reduce size\nsales_train_val_size = np.round(sales_train_val.memory_usage().sum()/(1024*1024),1)\n# sales_train_eval_size = np.round(sales_train_eval.memory_usage().sum()/(1024*1024),1)\ncalendar_size = np.round(calendar.memory_usage().sum()/(1024*1024),1)\nsell_prices_size = np.round(sell_prices.memory_usage().sum()/(1024*1024),1)\n\nprint(f'sales_train_val_size   {sales_train_val_size} byte')\nprint(f'calendar_size          {calendar_size} byte')\nprint(f'sell_prices_size       {sell_prices_size} byte')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Information"},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://i0.wp.com/mofc.unic.ac.cy/wp-content/uploads/2020/01/diagram.png?fit=1276%2C705&ssl=1\" width=\"1200\">\n"},{"metadata":{},"cell_type":"markdown","source":"# Data Preparation"},{"metadata":{},"cell_type":"markdown","source":"## Amount Sale by State"},{"metadata":{"trusted":true},"cell_type":"code","source":"group = sales_train_val.groupby(['state_id','store_id','cat_id','dept_id'],as_index=False)['item_id'].count().dropna()\n\ngroup['walmart'] = 'Walmart Distribution'\n\nfig = px.treemap(group, path=['walmart', 'state_id', 'store_id', 'cat_id', 'dept_id'], values='item_id',\n                  color='item_id',\n                  color_continuous_scale='RdBu',\n                  title='Walmart: Distribution of items')\n\nfig.update_layout()\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d_col = [c for c in sales_train_val.columns if 'd_' in c]\n\ndf = pd.merge(calendar.set_index('d'),sales_train_val.set_index('id')[d_col].T, left_index=True, right_index=True, validate='1:1')\n\ndf = df.reset_index().set_index('date').rename(columns={'index':'d'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"state_list = sales_train_val.state_id.unique() #['CA', 'TX', 'WI']\nmeans = []\nfig = go.Figure()\nfor s in state_list:\n    state_items = [c for c in df.columns if s in c] #ex: HOBBIES_1_001_WI_3_validation\n    data = df[state_items].sum(axis=1).rolling(30).mean()\n#     means.append(np.mean(df[state_items].sum(axis=1))) #store to mean for each store CA, TX ....\n    fig.add_trace(go.Scatter(x=data.index, y=data, name=s)) #plot each store CA, TX ...\n    \nfig.update_layout(yaxis_title=\"Sales\", xaxis_title=\"Time\", title=\"Rolling 30 days Average Sales vs. Time (per state)\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure()\n\nfor s in state_list:\n        state_items = [c for c in df.columns if s in c]\n        data = df[state_items].sum(axis=1).rolling(30).mean()\n        fig.add_trace(go.Box(x=[s]*len(data), y=data, name=s))\n    \nfig.update_layout(yaxis_title=\"Sales\", xaxis_title=\"Time\", title=\"Rolling 30 days Average Sales vs. State\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* CA has a significantly higher sales to others states because CA has 4 stores while others have only 3 stores"},{"metadata":{"trusted":true},"cell_type":"code","source":"store_list = sales_train_val.store_id.unique() #['CA_1', 'CA_2', 'CA_3', 'CA_4', 'TX_1', 'TX_2', 'TX_3', 'WI_1', 'WI_2', 'WI_3']\nmeans = []\nfig = go.Figure()\nfor s in store_list:\n    store_items = [c for c in df.columns if s in c] #ex: HOBBIES_1_001_WI_3_validation\n    data = df[store_items].sum(axis=1).rolling(30).mean()\n    means.append(np.mean(df[store_items].sum(axis=1))) #store to mean for each store CA_1, CA_2 ....\n    fig.add_trace(go.Scatter(x=data.index, y=data, name=s)) #plot each store CA_1, CA_2 ...\n    \nfig.update_layout(yaxis_title=\"Sales\", xaxis_title=\"Time\", title=\"Rolling Average 30 days Sales vs. Time (per store)\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Sales of WI_2 has drastically increased in 2012 and become the 2nd of higest sales in 2016"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure()\n\nfor s in store_list:\n        store_items = [c for c in df.columns if s in c]\n        data = df[store_items].sum(axis=1).rolling(30).mean()\n        fig.add_trace(go.Box(x=[s]*len(data), y=data, name=s))\n    \nfig.update_layout(yaxis_title=\"Sales\", xaxis_title=\"Time\", title=\"Rolling 30 days Average Sales vs. Store\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_list = sales_train_val.cat_id.unique() #['HOBBIES', 'HOUSEHOLD', 'FOODS']\nmeans = []\nfig = go.Figure()\nfor s in cat_list:\n    cat_items = [c for c in df.columns if s in c] #ex: HOBBIES_1_001_WI_3_validation\n    data = df[cat_items].sum(axis=1).rolling(30).mean()\n    means.append(np.mean(df[cat_items].sum(axis=1))) #store to mean for each store HOBBIES, HOUSEHOLD ....\n    fig.add_trace(go.Scatter(x=data.index, y=data, name=s)) #plot each store HOBBIES, HOUSEHOLD ...\n    \nfig.update_layout(yaxis_title=\"Sales\", xaxis_title=\"Time\", title=\"Rolling Average 30 Sales vs. Time (per category)\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure()\nmeans = []\nfor s in cat_list:\n        store_items = [c for c in df.columns if s in c]\n        data = df[store_items].sum(axis=1).rolling(30).mean()\n        fig.add_trace(go.Box(x=[s]*len(data), y=data, name=s))\n    \nfig.update_layout(yaxis_title=\"Sales\", xaxis_title=\"Time\", title=\"Rolling 30 days Average Sales vs. category\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dept_list = sales_train_val.dept_id.unique() #['HOBBIES_1', 'HOBBIES_2', 'HOUSEHOLD_1', 'HOUSEHOLD_2', 'FOODS_1', 'FOODS_2', 'FOODS_3']\nfig = go.Figure()\nmeans = []\nfor s in dept_list:\n    dept_items = [c for c in df.columns if s in c] #ex: HOBBIES_1_001_WI_3_validation\n    data = df[dept_items].sum(axis=1).rolling(30).mean()\n    means.append(np.mean(df[dept_items].sum(axis=1))) #store to mean for each store HOBBIES, HOUSEHOLD ....\n    fig.add_trace(go.Scatter(x=data.index, y=data, name=s)) #plot each store HOBBIES, HOUSEHOLD ...\n    \nfig.update_layout(yaxis_title=\"Sales\", xaxis_title=\"Time\", title=\"Rolling Average 30 Sales vs. Time (per department)\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure()\nmeans = []\nfor s in dept_list:\n        dept_items = [c for c in df.columns if s in c]\n        data = df[dept_items].sum(axis=1).rolling(30).mean()\n        fig.add_trace(go.Box(x=[s]*len(data), y=data, name=s))\n    \nfig.update_layout(yaxis_title=\"Sales\", xaxis_title=\"Time\", title=\"Rolling 30 days Average Sales vs. Department\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d_col = [c for c in sales_train_val.columns if 'd_' in c]\n\nstore = sales_train_val.groupby('store_id')[d_col].sum()\nts = store.reset_index().T.reset_index()\nts.columns = ['d', 'CA_1', 'CA_2', 'CA_3', 'CA_4', 'TX_1', 'TX_2', 'TX_3', 'WI_1', 'WI_2', 'WI_3']\nts = ts.drop([0]).set_index('d')\n\nts = ts.merge(calendar.set_index('d'), left_index=True, right_index=True, validate='1:1')\nts['CA'] = ts.CA_1 + ts.CA_2 + ts.CA_3 + ts.CA_4\nts['TX'] = ts.TX_1 + ts.TX_2 + ts.TX_3\nts['WI'] = ts.WI_1 + ts.WI_2 + ts.WI_3\nts['sale'] = ts.CA + ts.TX + ts.WI\nts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#mean sale by state\nfig = make_subplots(rows=1, cols=1)\n\nfig = go.Figure()\n\nstates = sales_train_val.state_id.unique().tolist()\nts['CA'] = ts['CA'].astype('int64')\nts['TX'] = ts['TX'].astype('int64')\nts['WI'] = ts['WI'].astype('int64')\n\nfor i in states:\n    fig.add_trace(go.Scatter(x=ts.groupby('month')['CA','TX', 'WI'].mean().index , y=ts.groupby('month')['CA','TX', 'WI'].mean()[i],\n                        mode='lines+markers',\n                        name=i))\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#mean sale by state\ntw = ts.groupby('weekday')['CA','TX', 'WI'].mean().reindex(['Saturday', 'Sunday', 'Monday','Tuesday','Wednesday','Thursday', 'Friday'])\n\nfig = make_subplots(rows=1, cols=1)\n\nfig = go.Figure()\n\nstates = sales_train_val.state_id.unique().tolist()\n\nfor i in states:\n    fig.add_trace(go.Scatter(x=tw.index, y=tw[i],\n                        mode='lines+markers',\n                        name=i))\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = make_subplots(rows=1, cols=1)\n\nfig = go.Figure()\n\nfig.add_trace(go.Scatter(x=ts.date, y=ts.sale,\n                        mode='lines',\n                        name='Sales of total state of Walart'))\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(20, 35))\n\nax1 = fig.add_subplot(511)\nsns.boxplot(data=ts, x='year', y=ts.sale, ax=ax1)\n\nax2 = fig.add_subplot(512)\nsns.boxplot(data=ts, x='month', y=ts.sale, ax=ax2)\n\nax3 = fig.add_subplot(513)\nsns.boxplot(data=ts, x='weekday', y=ts.sale, ax=ax3)\n\nax4 = fig.add_subplot(514)\nsns.boxplot(data=ts, x='event_type_1', y=ts.sale, ax=ax4)\n\nax5 = fig.add_subplot(515)\nsns.boxplot(data=ts, x='event_name_1', y=ts.sale, ax=ax5)\nax5.tick_params(axis='x', labelrotation=90)\n# ax5.set_xticklabels(xlabels, rotation=90 )\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"events_1_data = ts['event_type_1'].value_counts()\nfig = plt.figure(figsize=(11, 5))\nax = fig.add_subplot()\nax.pie(x=events_1_data.values,\n       labels=events_1_data.index,\n       shadow=True,\n       radius=1,\n       autopct='%1.1f%%')\nax.set_title('Distribution of Type 1 Events')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for year in ts.year.unique():\n    t = ts[ts.year == year]\n    fig = plt.figure(figsize=(20, 6))\n    ax = fig.add_subplot()\n    t_event_1 = t.loc[t.event_type_1.notnull()]\n    ax.plot(t.date, t.sale)\n    ax.scatter(t_event_1.date,\n               t_event_1.sale,\n               color='red',\n               label='Type 1 Event')\n    ax.set_xticks(t.date.values[::30])\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Sales')\n    ax.grid()\n    ax.set_title(f'Sales of total state of Walart for {year}')\n    ax.legend()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Analysis"},{"metadata":{},"cell_type":"markdown","source":"`sales_train_validation` Dataset is our train data set: [D1 - D1913].\n\n`sales_train_evalutaion` Dataset is data used to evaluate our models, it contains [D1914 - D1941]."},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train_eval = pd.read_csv('../input/m5-forecasting-accuracy/sales_train_evaluation.csv')\n\ncalendar = pd.read_csv('../input/m5-forecasting-accuracy/calendar.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"forecast_period = 28 #D1914-D1941\n\nd_fcst_columns = sales_train_eval.columns[-forecast_period:].tolist() #d_1914-d_1941","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_ground_truth(idx, df, d_fcst_columns):\n    return df.loc[idx, d_fcst_columns].values  \n\ndef plot_results(fcst, y_eval, rmse, algo, item):\n    fig = plt.figure(figsize=(11, 5))\n    ax = fig.add_subplot()\n    ax.plot(fcst, color='red', label='Forecast')\n    ax.plot(y_eval, color='blue', label='Ground Truth')\n    ax.set_title(f' {algo} for {item}, RMSE: {rmse}')\n    ax.grid()\n    ax.legend()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"choice_data = sales_train_eval.copy()\n\nchoice_data['d_val'] = sales_train_eval[d_col].mean(axis=1)\n\nchoice_data.drop(columns=d_col,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"idx1 = choice_data.loc[choice_data['d_val'] >= 50].sample(n=1, random_state=1).index\nidx2 = choice_data.loc[(choice_data['d_val'] <= 5) & (choice_data['d_val'] > 1)].sample(n=1, random_state=1).index\nidx = idx1.tolist() + idx2.tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ts_test = sales_train_eval.iloc[idx]\ntest_items = ts_test.id.unique().tolist()\ntest_items = [x[:-11] for x in test_items]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ts_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_items","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dataframe for RMSE\nrmse_summary = pd.DataFrame({\"items\":test_items}, index=idx)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ARIMA (AutoRegressive Integrated Moving Average)\n\n* ARIMA: Non-Seasonal.\n* SARIMA: Seasonal ARIMA.\n* SARIMAX: Seasonal ARIMA with eXogenous variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"from matplotlib.gridspec import GridSpec\nfrom fbprophet import Prophet\nfrom fbprophet.plot import plot_plotly, plot_components_plotly, add_changepoints_to_plot\nfrom sklearn.metrics import mean_squared_error\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nimport statsmodels.api as sm\n# import calendar","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_ts_example(data, d_cols, calendar_data, item_id, store_id, idx=None):\n    try:\n        if idx is None:\n            ts = data.loc[(data['item_id'] == item_id) & (data['store_id'] == store_id)]\n            ts = ts[d_cols].T.reset_index()\n            ts.columns = ['d', 'sales']\n        else:\n            ts = data.loc[idx][d_cols].reset_index()\n            ts.columns = ['d', 'sales']\n        # Make sure that sales column's type is int\n        ts[\"sales\"] = ts[\"sales\"].astype(\"int\")\n        return merge_with_calendar(ts, calendar_data)\n    except Exception as e:\n        print(f'Can not extract time series: {e}')\n         \ndef merge_with_calendar(data, calendar_data):\n    # data should have a date column \"d\"\n    assert 'd' in data.columns, 'DataFrame should have a column \"d\" !'\n    # Merge With Calendar\n    cal = calendar_data[['d', 'date', 'wm_yr_wk', 'weekday', 'wday', 'month', 'year',\n                    'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']]\n    d = pd.merge(cal, data, on=\"d\")\n    # Fill Missing Event Values with None\n    for col in ['event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']:\n        d[col].fillna('None', inplace=True)\n    return d","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def choose_sarimax_order_and_forecast(ps,ds,qs, y_train, y_eval):\n    best_model, best_rmse, best_order, best_fcst = None, None, None, None\n    for p in ps:\n        for d in ds:\n            for q in qs:\n                order = (p,d,q)\n                model = sm.tsa.SARIMAX(y_train, \n                               order=order, \n                               trend='c',\n                               enforce_invertibility=False,\n                               enforce_stationarity=False).fit(disp=False, warn_convergence=False)\n                fcst = model.predict(start=len(y_train), end=len(y_train) - 1 + len(y_eval))\n                try:\n                    fcst = [round(x) for x in fcst]\n                    rmse = round(np.sqrt(mean_squared_error(fcst, y_eval)), 3)\n                    if (best_rmse is None) or (rmse < best_rmse):\n                        best_model, best_rmse, best_order, best_forecast= model, rmse, order, fcst\n                except Exception as e:\n                    print(f'For order={order}, model results are invalid: {e}')\n    print(f\"Best Order: {best_order}\")\n    return best_rmse, best_forecast","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# example: FOODS_3_586_CA_3_evaluation (idx[0]) for training data\ndf0 = get_ts_example(ts_test, d_col, calendar, item_id=None, store_id=None, idx=idx[0])\n\n# df0['date'] = df0['date'].apply(lambda x : pd.to_datetime(x))\n\ndf0 = df0[['date','sales']]\n\ny_train = df0[\"sales\"].values\n\ndf0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result = seasonal_decompose(y_train, model='additive', period=365)\n\nfig = result.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(2,1,figsize=(20,7))\n\nsm.tsa.graphics.plot_acf(y_train, lags=30, ax=ax[0])\n\nax[0].set_title('Autocorreation Function: lags=30')\n\nsm.tsa.graphics.plot_pacf(y_train, lags=30, ax=ax[1])\n\nax[1].set_title('Partial Autocorreation Function: lags=30')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting Autocorrelation with pandas\nfig, ax = plt.subplots(1,1,figsize=(20,7))\npd.plotting.autocorrelation_plot(y_train, ax=ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sarimax_model = sm.tsa.SARIMAX(y_train, \n                               order=(7,1,7), \n                               trend='c',\n                               enforce_invertibility=False,\n                               enforce_stationarity=False).fit(disp=False, warn_convergence=False)\nsarimax_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# actual data\ny_eval = get_ground_truth(idx[0], sales_train_eval, d_fcst_columns)\ny_eval","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# forecast data\nfcst = sarimax_model.predict(start=len(y_train), end=len(y_train) - 1 + len(y_eval))\nfcst","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rmse = round(np.sqrt(mean_squared_error(fcst, y_eval)), 3)\nrmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_results(fcst, y_eval, rmse, \"SARIMAX\", test_items[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# SARIMAX Parameters Grid\nps = range(1,8)\nds = range(0,2)\nqs = range(0,8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rmse_sarimax = []\nfor i,ix in enumerate(idx):\n    \n    print(f\"Processing {test_items[i]}...\")\n    \n    # Get Time Series (Train)\n    df0 = get_ts_example(ts_test, d_col, calendar, item_id=None, store_id=None, idx=ix)\n    df0['date'] = df0['date'].apply(lambda x : pd.to_datetime(x))\n    df0 = df0[['date','sales']]\n    y_train = df0[\"sales\"].values\n    \n    # Get y_eval (actual data)\n    y_eval = get_ground_truth(ix, sales_train_eval, d_fcst_columns)\n    \n    # Train SARIMAX model\n    rmse, fcst = choose_sarimax_order_and_forecast(ps,ds,qs, y_train, y_eval)\n    print(f'rmse: {rmse}')\n    \n    # Plot\n    plot_results(fcst, y_eval, rmse, \"SARIMAX\", test_items[i])\n    rmse_sarimax.append(rmse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d_col = [c for c in sales_train_eval.columns if 'd_' in c]\neval_data = pd.merge(calendar, sales_train_eval.groupby(['state_id'])[d_col].sum().T.reset_index().rename(columns = {'index':'d'}) , on=\"d\")[-28:]\neval_data['sale'] = eval_data.CA+eval_data.TX+eval_data.WI\neval_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"states_rmse = []\nstates = ['CA', 'TX', 'WI', 'sale']\n\nfor i in states:\n    \n    print(f\"Processing {i}...\")\n    \n    y_train = ts[i].values.astype(int)\n    \n    y_eval = eval_data[i].values\n    \n    rmse, fcst = choose_sarimax_order_and_forecast(ps,ds,qs, y_train, y_eval)\n    print(f'rmse: {rmse}')\n\n    # Plot\n    plot_results(fcst, y_eval, rmse, \"SARIMAX\", i)\n    states_rmse.append(rmse)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}