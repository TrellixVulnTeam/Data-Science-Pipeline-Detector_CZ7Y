{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"I have updated this notebook to modify the wrmsse function  at 29th Mar.  \nNew wrmsse function for LGBM metric calculate wrmsse only for last 28 days to consider non-zero demand period.  \nPlease refer comment section. I have commented the detail of my fixing.\n(note:I have also remove some variable to reduce the run-time and changed 'objective' in lgbm to 'poisson'.)\n\nThis kernel is:  \n- Based on [Very fst Model](https://www.kaggle.com/ragnar123/very-fst-model). Thanks [@ragnar123](https://www.kaggle.com/ragnar123).  \n- Based on [m5-baseline](https://www.kaggle.com/harupy/m5-baseline). Thank [@harupy](https://www.kaggle.com/harupy).  \nto explain the detail of these great notebook by Japanese especially for beginner.  \n\nAdditionaly, I have added an relatively efficient evaluation of WRSSE for LGBM metric to these kernel.","metadata":{}},{"cell_type":"markdown","source":"## module import","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\nimport pandas as pd\nimport numpy as np\nimport dask.dataframe as dd\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_rows', 500)\nimport matplotlib.pyplot as plt\nimport seaborn as sns; sns.set()\nimport lightgbm as lgb\n#import dask_xgboost as xgb\n#import dask.dataframe as dd\nfrom sklearn import preprocessing, metrics\nfrom sklearn.preprocessing import LabelEncoder\nimport gc\nimport os\nfrom tqdm import tqdm\nfrom scipy.sparse import csr_matrix\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-01T00:46:22.712444Z","iopub.execute_input":"2021-12-01T00:46:22.713128Z","iopub.status.idle":"2021-12-01T00:46:25.133982Z","shell.execute_reply.started":"2021-12-01T00:46:22.712785Z","shell.execute_reply":"2021-12-01T00:46:25.132003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## functionの定義","metadata":{}},{"cell_type":"markdown","source":"reduce_mem_usageは、データのメモリを減らすためにデータ型を変更する関数です。  \n('reduce_mem_usage' is a functin which reduce memory usage by changing data type.)\nhttps://qiita.com/hiroyuki_kageyama/items/02865616811022f79754　を参照ください。","metadata":{}},{"cell_type":"code","source":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns: #columns毎に処理\n        col_type = df[col].dtypes\n        if col_type in numerics: #numericsのデータ型の範囲内のときに処理を実行. データの最大最小値を元にデータ型を効率的なものに変更\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df\n","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2021-12-01T00:46:25.136108Z","iopub.execute_input":"2021-12-01T00:46:25.136484Z","iopub.status.idle":"2021-12-01T00:46:25.216929Z","shell.execute_reply.started":"2021-12-01T00:46:25.136426Z","shell.execute_reply":"2021-12-01T00:46:25.215384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"PandasのdataFrameをきれいに表示する関数\n(This function is to diplay a head of Pandas DataFrame.)","metadata":{}},{"cell_type":"code","source":"import IPython\n\ndef display(*dfs, head=True):\n    for df in dfs:\n        IPython.display.display(df.head() if head else df)","metadata":{"execution":{"iopub.status.busy":"2021-12-01T00:46:28.690805Z","iopub.execute_input":"2021-12-01T00:46:28.691537Z","iopub.status.idle":"2021-12-01T00:46:28.69661Z","shell.execute_reply.started":"2021-12-01T00:46:28.691275Z","shell.execute_reply":"2021-12-01T00:46:28.695484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 特徴量作成","metadata":{}},{"cell_type":"code","source":"%%time\n\ndata = pd.read_pickle(\"../input/make-merged-dataset/m5_all_data_merged.pickle\")\n#data = data[data['id']==\"HOBBIES_1_001_CA_1_validation\"].head(600)\ndata.head(3)","metadata":{"execution":{"iopub.status.busy":"2021-12-01T00:46:30.731929Z","iopub.execute_input":"2021-12-01T00:46:30.732484Z","iopub.status.idle":"2021-12-01T00:46:42.410099Z","shell.execute_reply.started":"2021-12-01T00:46:30.732419Z","shell.execute_reply":"2021-12-01T00:46:42.409286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# サンプルの特徴量作成関数\n# 既存の列であるsub_id1,2,7,8,9,10,12は作成不要。\ndef sample_fe(data):    \n\n    ##################################################\n    # sub_id3,4,5,6 曜日、週、月、年\n    ##################################################\n    # 日付に関する特徴量を作成する元となるdate列を指定\n    dt_col = \"date\"\n    # 上記作成元の列をdatetime型に変換\n    data[dt_col] = pd.to_datetime(data[dt_col])\n    # 作成したい日付関連の特徴量リストを指定\n    attrs = [\n        \"year\",\n        \"month\",\n        \"week\",\n        \"dayofweek\",\n    ]\n    # 上記で指定したリストの特徴量を作成\n    for attr in tqdm(attrs):\n        dtype = np.int16 if attr == \"year\" else np.int8\n        data[attr] = getattr(data[dt_col].dt, attr).astype(dtype)\n    print(\"Phase1 : sub_id=3,4,5,6 日付関連特徴量 finished\")\n\n        \n    ##################################################\n    # sub_id=11 価格増減率(今週の商品価格/先週の商品価格)\n    ##################################################\n    # 先週の価格列を作成　※価格は週ごとに出ている\n    tmp_data = data[['id','wm_yr_wk','sell_price']]\n    tmp_data = tmp_data.drop_duplicates()\n    tmp_data.columns = ['id','wm_yr_wk','lastweek_sell_price']\n    tmp_data['wm_yr_wk'] = tmp_data['wm_yr_wk']+1\n    data = pd.merge(data, tmp_data, on=['id','wm_yr_wk'], how='left', copy=False)\n    del tmp_data\n    gc.collect()\n\n\n    # 今週の価格/先週の価格で、価格の増減率を計算\n    data[\"price_volatility_w1\"] = data[\"sell_price\"]/data[\"lastweek_sell_price\"]\n    print(\"Phase2 : sub_id=11 価格増減率 finished\")\n    \n\n    ##################################################    \n    # sub_id=13 直近の売上平均\n    ##################################################\n    # 売上平均を取りたい日数リストを作成\n    windows = [7]\n    # 上記作成したリストに基づいて、売上の移動平均を取る\n    for window in tqdm(windows):\n        data['rolling_demand_mean_'+str(window)] = data.groupby(['id'])['demand'].transform(lambda x: x.rolling(window=window).mean()).astype(np.float16)\n    print(\"Phase3 : sub_id=13 直近の売上平均 finished\")\n\n        \n    ##################################################\n    # sub_id=14 前年同時期の売上平均\n    ##################################################\n    # sub_id=13で作成した移動平均を365日shiftし、値を取得\n    for window in tqdm(windows):\n        data['last_year_rolling_demand_mean'] = data.groupby(['id'],as_index=False)['rolling_demand_mean_'+str(window)].shift(365).astype(np.float16)\n    print(\"Phase4 : sub_id=14 前年同時期の売上平均 finished\")\n        \n    ##################################################        \n    # sub_id=15 売上のシフト\n    ##################################################\n    # 作成したいラグのリストを作成\n    # lags = [1,2,3,6,12,24,36]\n    lags = [1,2]\n    # 上記で作成したリストを用いて、ラグを作成\n    for lag in tqdm(lags):\n        data['demand_lag_'+str(lag)] = data.groupby(['id'],as_index=False)['demand'].shift(lag).astype(np.float16)\n    print(\"Phase5 : sub_id=15 売上のシフト finished\")\n        \n    # 特徴量を追加したデータフレームを返す\n    return data\n","metadata":{"execution":{"iopub.status.busy":"2021-12-01T00:47:07.45087Z","iopub.execute_input":"2021-12-01T00:47:07.451308Z","iopub.status.idle":"2021-12-01T00:47:07.465815Z","shell.execute_reply.started":"2021-12-01T00:47:07.451248Z","shell.execute_reply":"2021-12-01T00:47:07.464765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ndata = sample_fe(data)\ndata = reduce_mem_usage(data)","metadata":{"execution":{"iopub.status.busy":"2021-12-01T00:47:11.725483Z","iopub.execute_input":"2021-12-01T00:47:11.725849Z","iopub.status.idle":"2021-12-01T00:48:45.766356Z","shell.execute_reply.started":"2021-12-01T00:47:11.725801Z","shell.execute_reply":"2021-12-01T00:48:45.765385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"data.head(400)","metadata":{"execution":{"iopub.status.busy":"2021-11-30T11:10:56.133041Z","iopub.execute_input":"2021-11-30T11:10:56.133463Z","iopub.status.idle":"2021-11-30T11:10:56.518025Z","shell.execute_reply.started":"2021-11-30T11:10:56.133406Z","shell.execute_reply":"2021-11-30T11:10:56.516798Z"}}},{"cell_type":"code","source":"# 不要列の削除\ndata.drop(['lastweek_sell_price'], inplace = True, axis = 1)\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-12-01T00:48:53.727861Z","iopub.execute_input":"2021-12-01T00:48:53.728229Z","iopub.status.idle":"2021-12-01T00:48:55.632586Z","shell.execute_reply.started":"2021-12-01T00:48:53.728189Z","shell.execute_reply":"2021-12-01T00:48:55.631668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(data.head())","metadata":{"execution":{"iopub.status.busy":"2021-12-01T00:49:03.177893Z","iopub.execute_input":"2021-12-01T00:49:03.1783Z","iopub.status.idle":"2021-12-01T00:49:03.207631Z","shell.execute_reply.started":"2021-12-01T00:49:03.178239Z","shell.execute_reply":"2021-12-01T00:49:03.206707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 出力","metadata":{}},{"cell_type":"code","source":"data.to_pickle(\"m5_add_features_data_sample.pickle\")","metadata":{"execution":{"iopub.status.busy":"2021-12-01T01:03:03.808112Z","iopub.execute_input":"2021-12-01T01:03:03.808501Z","iopub.status.idle":"2021-12-01T01:03:09.281109Z","shell.execute_reply.started":"2021-12-01T01:03:03.808457Z","shell.execute_reply":"2021-12-01T01:03:09.280119Z"},"trusted":true},"execution_count":null,"outputs":[]}]}