{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"I have updated this notebook to modify the wrmsse function  at 29th Mar.  \nNew wrmsse function for LGBM metric calculate wrmsse only for last 28 days to consider non-zero demand period.  \nPlease refer comment section. I have commented the detail of my fixing.\n(note:I have also remove some variable to reduce the run-time and changed 'objective' in lgbm to 'poisson'.)\n\nThis kernel is:  \n- Based on [Very fst Model](https://www.kaggle.com/ragnar123/very-fst-model). Thanks [@ragnar123](https://www.kaggle.com/ragnar123).  \n- Based on [m5-baseline](https://www.kaggle.com/harupy/m5-baseline). Thank [@harupy](https://www.kaggle.com/harupy).  \nto explain the detail of these great notebook by Japanese especially for beginner.  \n\nAdditionaly, I have added an relatively efficient evaluation of WRSSE for LGBM metric to these kernel.","metadata":{}},{"cell_type":"markdown","source":"## module import","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\nimport pandas as pd\nimport numpy as np\nimport dask.dataframe as dd\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_rows', 500)\nimport matplotlib.pyplot as plt\nimport seaborn as sns; sns.set()\nimport lightgbm as lgb\n#import dask_xgboost as xgb\n#import dask.dataframe as dd\nfrom sklearn import preprocessing, metrics\nfrom sklearn.preprocessing import LabelEncoder\nimport gc\nimport os\nfrom tqdm import tqdm\nfrom scipy.sparse import csr_matrix\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-06T23:32:14.923228Z","iopub.execute_input":"2021-12-06T23:32:14.923649Z","iopub.status.idle":"2021-12-06T23:32:17.330744Z","shell.execute_reply.started":"2021-12-06T23:32:14.923597Z","shell.execute_reply":"2021-12-06T23:32:17.329783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## functionの定義","metadata":{}},{"cell_type":"markdown","source":"reduce_mem_usageは、データのメモリを減らすためにデータ型を変更する関数です。  \n('reduce_mem_usage' is a functin which reduce memory usage by changing data type.)\nhttps://qiita.com/hiroyuki_kageyama/items/02865616811022f79754　を参照ください。","metadata":{}},{"cell_type":"code","source":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns: #columns毎に処理\n        col_type = df[col].dtypes\n        if col_type in numerics: #numericsのデータ型の範囲内のときに処理を実行. データの最大最小値を元にデータ型を効率的なものに変更\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df\n","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2021-12-06T23:32:17.332356Z","iopub.execute_input":"2021-12-06T23:32:17.332715Z","iopub.status.idle":"2021-12-06T23:32:17.409915Z","shell.execute_reply.started":"2021-12-06T23:32:17.332668Z","shell.execute_reply":"2021-12-06T23:32:17.408624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"PandasのdataFrameをきれいに表示する関数\n(This function is to diplay a head of Pandas DataFrame.)","metadata":{}},{"cell_type":"code","source":"import IPython\n\ndef display(*dfs, head=True):\n    for df in dfs:\n        IPython.display.display(df.head() if head else df)","metadata":{"execution":{"iopub.status.busy":"2021-12-06T23:32:17.412297Z","iopub.execute_input":"2021-12-06T23:32:17.412824Z","iopub.status.idle":"2021-12-06T23:32:17.42436Z","shell.execute_reply.started":"2021-12-06T23:32:17.412756Z","shell.execute_reply":"2021-12-06T23:32:17.423363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## train/testの分割とmodelの推定","metadata":{}},{"cell_type":"markdown","source":"02_make_featuresノートブックで作成した特徴量列追加済みのpickleファイルを読み込み","metadata":{}},{"cell_type":"code","source":"data = pd.read_pickle('../input/02-make-features/m5_add_features_data_sample.pickle')\ndata.head(3)","metadata":{"execution":{"iopub.status.busy":"2021-12-06T23:32:19.270595Z","iopub.execute_input":"2021-12-06T23:32:19.271031Z","iopub.status.idle":"2021-12-06T23:32:31.96933Z","shell.execute_reply.started":"2021-12-06T23:32:19.270968Z","shell.execute_reply":"2021-12-06T23:32:31.968136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2016/3/27より前を学習用、2016/3/27~2016/4/24（28day）を検証用として分割  \n（LightGBMのEarly stoppingの対象）","metadata":{}},{"cell_type":"code","source":"# 学習用データセットを作成\nx_train = data[data['date'] <= '2016-03-27']\ny_train = x_train['demand']\n\n# 検証用データセットを作成\nx_val = data[(data['date'] > '2016-03-27') & (data['date'] <= '2016-04-24')]\ny_val = x_val['demand']\n\n# submission用のデータセットを作成\ntest = data[(data['date'] > '2016-04-24')]\n\n#dataの削除（メモリの削除）\n#del data\n#gc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-12-06T23:48:03.148884Z","iopub.execute_input":"2021-12-06T23:48:03.149307Z","iopub.status.idle":"2021-12-06T23:48:06.578929Z","shell.execute_reply.started":"2021-12-06T23:48:03.149258Z","shell.execute_reply":"2021-12-06T23:48:06.577787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"modelのLGBMでの推定　　\n* early stoppingのmetricに全体のRMSEを使っているため, コンペの指標のWRMSSEとは異なる.","metadata":{}},{"cell_type":"code","source":"# モデルに投入する特徴量のリストを作成\nfeatures = [\n    \"cat_id\",\n    \"dept_id\",\n    \"item_id\",\n    \"state_id\",\n    \"store_id\",\n    \"event_name_1\",\n    \"event_name_2\",\n    \"snap_CA\",\n    \"snap_TX\",\n    \"snap_WI\",\n    \n    # 売上の特徴量\n    \"rolling_demand_mean_7\",\n    \"last_year_rolling_demand_mean\",\n    \"demand_lag_1\",\n    \"demand_lag_2\",\n\n    # 価格の特徴量\n    \"sell_price\",\n    \"price_volatility_w1\",\n\n    # 時間の特徴量\n    \"year\",\n    \"month\",\n    \"dayofweek\",\n    \"wm_yr_wk\",\n]\n\n# LightGBMのハイパーパラメーターを設定\nparams = {\n    'boosting_type': 'gbdt',\n    'metric': 'rmse',\n    'objective': 'regression',\n    'n_jobs': -1,\n    'seed': 236,\n    'learning_rate': 0.1,\n    'bagging_fraction': 0.75,\n    'bagging_freq': 10, \n    'colsample_bytree': 0.75}\n\n# 学習用・検証用データセットをLightGBM用のデータセットに変換\ntrain_set = lgb.Dataset(x_train[features], y_train)\nval_set = lgb.Dataset(x_val[features], y_val)\n\n# 不要な変数の削除\ndel x_train, y_train\ngc.collect()\n\n# modelの学習を実行（今回は学習過程で50回連続でrmseが改善しなかった場合、早期に学習を切り上げる＝early stoppingする設定。）\nmodel = lgb.train(params, train_set, num_boost_round = 2500, early_stopping_rounds = 50, valid_sets = [train_set, val_set], verbose_eval = 100)\n\n# 学習したモデルを使用して、検証用データセットについてdemandを予測\nval_pred = model.predict(x_val[features])\n# 実測値と予測値の平均二乗誤差＝rmseを計算し、精度を確認\nval_score = np.sqrt(metrics.mean_squared_error(val_pred, y_val))\nprint(f'Our val rmse score is {val_score}')\n\n# 同様に学習したモデルを使用して、submission用データセットについてdemandを予測\ny_pred = model.predict(test[features])\n# 予測値をdemand列に格納\ntest['demand'] = y_pred\n","metadata":{"execution":{"iopub.status.busy":"2021-12-06T23:48:15.373691Z","iopub.execute_input":"2021-12-06T23:48:15.374081Z","iopub.status.idle":"2021-12-06T23:51:37.200351Z","shell.execute_reply.started":"2021-12-06T23:48:15.374034Z","shell.execute_reply":"2021-12-06T23:51:37.199452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## submission fileの出力","metadata":{}},{"cell_type":"code","source":"# サンプルのsubmissionファイルを読み込み\nsubmission = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2021-12-06T23:58:33.568591Z","iopub.execute_input":"2021-12-06T23:58:33.569078Z","iopub.status.idle":"2021-12-06T23:58:33.792004Z","shell.execute_reply.started":"2021-12-06T23:58:33.569024Z","shell.execute_reply":"2021-12-06T23:58:33.790726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# demandにモデルの予測値が入ったsubmission用データセットから必要な列のみを抽出\npredictions = test[['id', 'date', 'demand']]\n# ピボット関数を適用させて横持ちに変換し、日ごとのdemandが入った列名をF1~F28に変更\npredictions = pd.pivot(predictions, index = 'id', columns = 'date', values = 'demand').reset_index()\npredictions.columns = ['id'] + ['F' + str(i + 1) for i in range(28)]\n\n# idの値にevaluationが含まれているレコードのみ抽出\nevaluation_rows = [row for row in submission['id'] if 'evaluation' in row] \nevaluation = submission[submission['id'].isin(evaluation_rows)]\n\n# submission.csvと上記で作成したpredictionとを、id列をキーに結合\nvalidation = submission[['id']].merge(predictions, on = 'id')\n\n# validation期間とevaluation期間のデータセットを縦に結合\nfinal = pd.concat([validation, evaluation])\n# finalをsubmission.csvとして出力\nfinal.to_csv('submission.csv', index = False)\n","metadata":{"execution":{"iopub.status.busy":"2021-12-06T23:58:36.887062Z","iopub.execute_input":"2021-12-06T23:58:36.887715Z","iopub.status.idle":"2021-12-06T23:58:40.655693Z","shell.execute_reply.started":"2021-12-06T23:58:36.887659Z","shell.execute_reply":"2021-12-06T23:58:40.654518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# submission用のデータセットを確認\nfinal.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-12-06T23:58:50.411077Z","iopub.execute_input":"2021-12-06T23:58:50.411468Z","iopub.status.idle":"2021-12-06T23:58:50.447528Z","shell.execute_reply.started":"2021-12-06T23:58:50.411419Z","shell.execute_reply":"2021-12-06T23:58:50.446694Z"},"trusted":true},"execution_count":null,"outputs":[]}]}