{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"I have updated this notebook to modify the wrmsse function  at 29th Mar.  \nNew wrmsse function for LGBM metric calculate wrmsse only for last 28 days to consider non-zero demand period.  \nPlease refer comment section. I have commented the detail of my fixing.\n(note:I have also remove some variable to reduce the run-time and changed 'objective' in lgbm to 'poisson'.)\n\nThis kernel is:  \n- Based on [Very fst Model](https://www.kaggle.com/ragnar123/very-fst-model). Thanks [@ragnar123](https://www.kaggle.com/ragnar123).  \n- Based on [m5-baseline](https://www.kaggle.com/harupy/m5-baseline). Thank [@harupy](https://www.kaggle.com/harupy).  \nto explain the detail of these great notebook by Japanese especially for beginner.  \n\nAdditionaly, I have added an relatively efficient evaluation of WRSSE for LGBM metric to these kernel.","metadata":{}},{"cell_type":"markdown","source":"## module import","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\nimport pandas as pd\nimport numpy as np\nimport dask.dataframe as dd\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_rows', 500)\nimport matplotlib.pyplot as plt\nimport seaborn as sns; sns.set()\nimport lightgbm as lgb\n#import dask_xgboost as xgb\n#import dask.dataframe as dd\nfrom sklearn import preprocessing, metrics\nfrom sklearn.preprocessing import LabelEncoder\nimport gc\nimport os\nfrom tqdm import tqdm\nfrom scipy.sparse import csr_matrix\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-02T04:19:19.73044Z","iopub.execute_input":"2021-12-02T04:19:19.731212Z","iopub.status.idle":"2021-12-02T04:19:22.305181Z","shell.execute_reply.started":"2021-12-02T04:19:19.731108Z","shell.execute_reply":"2021-12-02T04:19:22.303933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## functionの定義","metadata":{}},{"cell_type":"markdown","source":"reduce_mem_usageは、データのメモリを減らすためにデータ型を変更する関数です。  \n('reduce_mem_usage' is a functin which reduce memory usage by changing data type.)\nhttps://qiita.com/hiroyuki_kageyama/items/02865616811022f79754　を参照ください。","metadata":{}},{"cell_type":"code","source":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns: #columns毎に処理\n        col_type = df[col].dtypes\n        if col_type in numerics: #numericsのデータ型の範囲内のときに処理を実行. データの最大最小値を元にデータ型を効率的なものに変更\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df\n","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2021-12-02T04:19:22.307034Z","iopub.execute_input":"2021-12-02T04:19:22.307327Z","iopub.status.idle":"2021-12-02T04:19:22.384503Z","shell.execute_reply.started":"2021-12-02T04:19:22.307292Z","shell.execute_reply":"2021-12-02T04:19:22.383432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"read_dataはデータの読み込みと, reduce_mem_usageの適用を行う関数  \n('read data' is a function to read the files and apply the 'reduce_mem_usage'.)","metadata":{}},{"cell_type":"code","source":"def read_data():\n    print('Reading files...')\n    calendar = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/calendar.csv')\n    calendar = reduce_mem_usage(calendar)\n    print('Calendar has {} rows and {} columns'.format(calendar.shape[0], calendar.shape[1]))\n    \n    sell_prices = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sell_prices.csv')\n    sell_prices = reduce_mem_usage(sell_prices)\n    print('Sell prices has {} rows and {} columns'.format(sell_prices.shape[0], sell_prices.shape[1]))\n    \n    sales_train_val = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sales_train_validation.csv')\n    print('Sales train validation has {} rows and {} columns'.format(sales_train_val.shape[0], sales_train_val.shape[1]))\n    \n    submission = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sample_submission.csv')\n    \n    return calendar, sell_prices, sales_train_val, submission","metadata":{"execution":{"iopub.status.busy":"2021-12-02T04:19:22.386376Z","iopub.execute_input":"2021-12-02T04:19:22.386794Z","iopub.status.idle":"2021-12-02T04:19:22.401344Z","shell.execute_reply.started":"2021-12-02T04:19:22.386746Z","shell.execute_reply":"2021-12-02T04:19:22.400404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"PandasのdataFrameをきれいに表示する関数\n(This function is to diplay a head of Pandas DataFrame.)","metadata":{}},{"cell_type":"code","source":"import IPython\n\ndef display(*dfs, head=True):\n    for df in dfs:\n        IPython.display.display(df.head() if head else df)","metadata":{"execution":{"iopub.status.busy":"2021-12-02T04:19:22.40262Z","iopub.execute_input":"2021-12-02T04:19:22.403069Z","iopub.status.idle":"2021-12-02T04:19:22.417326Z","shell.execute_reply.started":"2021-12-02T04:19:22.403016Z","shell.execute_reply":"2021-12-02T04:19:22.416369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"calendar, sell_prices, sales_train_val, submission = read_data()","metadata":{"execution":{"iopub.status.busy":"2021-12-02T04:19:22.423179Z","iopub.execute_input":"2021-12-02T04:19:22.423569Z","iopub.status.idle":"2021-12-02T04:19:37.149632Z","shell.execute_reply.started":"2021-12-02T04:19:22.423517Z","shell.execute_reply":"2021-12-02T04:19:37.148676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 予測期間とitem数の定義 / number of items, and number of prediction period\nNUM_ITEMS = sales_train_val.shape[0]  # 30490\nDAYS_PRED = submission.shape[1] - 1  # 28","metadata":{"execution":{"iopub.status.busy":"2021-12-02T04:19:37.15298Z","iopub.execute_input":"2021-12-02T04:19:37.15332Z","iopub.status.idle":"2021-12-02T04:19:37.159076Z","shell.execute_reply.started":"2021-12-02T04:19:37.15327Z","shell.execute_reply":"2021-12-02T04:19:37.157749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## data加工 (data transform)","metadata":{}},{"cell_type":"markdown","source":"### 1.最初にカテゴリ変数の処理(categorical variable)","metadata":{}},{"cell_type":"markdown","source":"As [@kaushal2896](https://www.kaggle.com/kaushal2896) suggested in [this comment](https://www.kaggle.com/harupy/m5-baseline#770558), encode the categorical columns before merging to prevent the notebook from crashing even with the full dataset. [@harupy](https://www.kaggle.com/harupy) also use this encoding suggested in [m5-baseline](https://www.kaggle.com/harupy/m5-baseline).  \nメモリの効率利用のため, カテゴリ変数をあらかじめLabel encoding.","metadata":{}},{"cell_type":"code","source":"def encode_categorical(df, cols):\n    \n    for col in cols:\n        # Leave NaN as it is.\n        le = LabelEncoder()\n        #not_null = df[col][df[col].notnull()]\n        df[col] = df[col].fillna('nan')\n        df[col] = pd.Series(le.fit_transform(df[col]), index=df.index)\n\n    return df\n\n\ncalendar = encode_categorical(\n    calendar, [\"event_name_1\", \"event_type_1\", \"event_name_2\", \"event_type_2\"]\n).pipe(reduce_mem_usage)\n\nsales_train_val = encode_categorical(\n    sales_train_val, [\"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"],\n).pipe(reduce_mem_usage)\n\nsell_prices = encode_categorical(sell_prices, [\"item_id\", \"store_id\"]).pipe(\n    reduce_mem_usage\n)\n","metadata":{"execution":{"iopub.status.busy":"2021-12-02T04:19:37.161088Z","iopub.execute_input":"2021-12-02T04:19:37.161615Z","iopub.status.idle":"2021-12-02T04:21:44.456294Z","shell.execute_reply.started":"2021-12-02T04:19:37.161549Z","shell.execute_reply":"2021-12-02T04:21:44.455214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sales_train_valからidの詳細部分(itemやdepartmentなどのid)を重複なく一意に取得しておく。(extract a detail of id columns)\nproduct = sales_train_val[['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']].drop_duplicates()","metadata":{"execution":{"iopub.status.busy":"2021-12-02T04:21:44.457675Z","iopub.execute_input":"2021-12-02T04:21:44.457998Z","iopub.status.idle":"2021-12-02T04:21:44.623863Z","shell.execute_reply.started":"2021-12-02T04:21:44.457956Z","shell.execute_reply":"2021-12-02T04:21:44.622962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.sales_train_validationのmelt処理(apply melt to sales_train_validation)  \n（時系列の特徴量が作りやすいように, id毎に横に並んだ時系列データを、（id , 時系列）で縦に変換）  \n(apply melt to sales_train_validation(time series) to make it easier to treat.)","metadata":{}},{"cell_type":"markdown","source":"pandasのmeltを使いdemand(売上数量)を縦に並べる.  \n* pandasのmeltは https://qiita.com/ishida330/items/922caa7acb73c1540e28　を参照ください。\n* dataの行数が莫大になるので, Kaggle Notebookのmemory制限を考慮し、nrowsで直近365*2日分（2年分）のデータに限定（TODO:環境に応じて期間を変更）","metadata":{}},{"cell_type":"code","source":"nrows = 365 * 2 * NUM_ITEMS","metadata":{"execution":{"iopub.status.busy":"2021-12-02T04:21:44.625714Z","iopub.execute_input":"2021-12-02T04:21:44.626293Z","iopub.status.idle":"2021-12-02T04:21:44.63144Z","shell.execute_reply.started":"2021-12-02T04:21:44.62617Z","shell.execute_reply":"2021-12-02T04:21:44.630325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#加工前  \ndisplay(sales_train_val.head(5))","metadata":{"execution":{"iopub.status.busy":"2021-12-02T04:21:44.633697Z","iopub.execute_input":"2021-12-02T04:21:44.634371Z","iopub.status.idle":"2021-12-02T04:21:44.96358Z","shell.execute_reply.started":"2021-12-02T04:21:44.634154Z","shell.execute_reply":"2021-12-02T04:21:44.962482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"to remove data before first non-zero demand date, replace these demand as np.nan.","metadata":{}},{"cell_type":"code","source":"d_name = ['d_' + str(i+1) for i in range(1913)]\n# d_XX列の値を取得\nsales_train_val_values = sales_train_val[d_name].values\nprint(sales_train_val_values)","metadata":{"execution":{"iopub.status.busy":"2021-12-02T04:21:44.965364Z","iopub.execute_input":"2021-12-02T04:21:44.966112Z","iopub.status.idle":"2021-12-02T04:21:45.041118Z","shell.execute_reply.started":"2021-12-02T04:21:44.966049Z","shell.execute_reply":"2021-12-02T04:21:45.039929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# calculate the start position(first non-zero demand observed date) for each item / 商品の最初の売上日\n# 1-1914のdayの数列のうち, 売上が存在しない日を一旦0にし、0を9999に置換。そのうえでminimum numberを計算\n# tmp=は各商品について1-1913のベクトルを作成\ntmp = np.tile(np.arange(1,1914),(sales_train_val_values.shape[0],1))\nprint(tmp)","metadata":{"execution":{"iopub.status.busy":"2021-12-02T04:21:45.0428Z","iopub.execute_input":"2021-12-02T04:21:45.043439Z","iopub.status.idle":"2021-12-02T04:21:45.158567Z","shell.execute_reply.started":"2021-12-02T04:21:45.043246Z","shell.execute_reply":"2021-12-02T04:21:45.15754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 売上が0ではない日が1日以上存在する商品に絞り、売上がない日は０、売上が１つ以上ある場合はd_XXの値が入った行列を作成\ndf_tmp = ((sales_train_val_values>0) * tmp)\nprint(df_tmp)","metadata":{"execution":{"iopub.status.busy":"2021-12-02T04:21:45.159973Z","iopub.execute_input":"2021-12-02T04:21:45.160332Z","iopub.status.idle":"2021-12-02T04:21:45.917891Z","shell.execute_reply.started":"2021-12-02T04:21:45.160258Z","shell.execute_reply":"2021-12-02T04:21:45.916833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 計算手順の補足\n# 売上が0の日を9999で埋め（後ほどminを取った際に、売上が発生した最初の日を取得するため）、それ以外の場合はd_XXの値が入った行列を作成\nnp.where(df_tmp==0,9999,df_tmp)","metadata":{"execution":{"iopub.status.busy":"2021-12-02T04:21:45.919607Z","iopub.execute_input":"2021-12-02T04:21:45.920333Z","iopub.status.idle":"2021-12-02T04:21:46.289058Z","shell.execute_reply.started":"2021-12-02T04:21:45.920275Z","shell.execute_reply":"2021-12-02T04:21:46.287757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 売上が0の日は9999、それ以外の場合はd_XXの値が入っている中から各最小日を取得し、１引く。商品の売上が初めてあった日-1の値を取得している。\nstart_no = np.min(np.where(df_tmp==0,9999,df_tmp),axis=1)-1\nprint(start_no)","metadata":{"execution":{"iopub.status.busy":"2021-12-02T04:21:46.291016Z","iopub.execute_input":"2021-12-02T04:21:46.29166Z","iopub.status.idle":"2021-12-02T04:21:46.717837Z","shell.execute_reply.started":"2021-12-02T04:21:46.291444Z","shell.execute_reply":"2021-12-02T04:21:46.716803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 計算手順の補足\n# 商品が初めて売れた日の逆数を取得。全て1未満の値になる\n1/(start_no+1)","metadata":{"execution":{"iopub.status.busy":"2021-12-02T04:21:46.719511Z","iopub.execute_input":"2021-12-02T04:21:46.719912Z","iopub.status.idle":"2021-12-02T04:21:46.726074Z","shell.execute_reply.started":"2021-12-02T04:21:46.719851Z","shell.execute_reply":"2021-12-02T04:21:46.725267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 計算手順の補足\n# 商品が初めて売れた日の逆数で、対角行列を作成する\nnp.diag(1/(start_no+1))","metadata":{"execution":{"iopub.status.busy":"2021-12-02T04:21:46.727458Z","iopub.execute_input":"2021-12-02T04:21:46.72794Z","iopub.status.idle":"2021-12-02T04:21:46.810066Z","shell.execute_reply.started":"2021-12-02T04:21:46.727769Z","shell.execute_reply":"2021-12-02T04:21:46.808977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 計算手順の補足\n# 商品が初めて売れた日の逆数で対角行列を作成し、1-1914までの日付が商品分並んだ行列tmpと行列の積をとる。\n# 売上が初めてあった日以降が1以上の値となっている行列が得られる。\nnp.dot(np.diag(1/(start_no+1)) , tmp)","metadata":{"execution":{"iopub.status.busy":"2021-12-02T04:21:46.811282Z","iopub.execute_input":"2021-12-02T04:21:46.81161Z","iopub.status.idle":"2021-12-02T04:22:40.284478Z","shell.execute_reply.started":"2021-12-02T04:21:46.811566Z","shell.execute_reply":"2021-12-02T04:22:40.283371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 売上が初めてあった日以前はTrue, 以降はFalseが入ったflagを作成\nflag = np.dot(np.diag(1/(start_no+1)) , tmp)<1\nprint(flag)","metadata":{"execution":{"iopub.status.busy":"2021-12-02T04:22:40.286242Z","iopub.execute_input":"2021-12-02T04:22:40.286942Z","iopub.status.idle":"2021-12-02T04:23:33.456337Z","shell.execute_reply.started":"2021-12-02T04:22:40.28688Z","shell.execute_reply":"2021-12-02T04:23:33.455233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 上記で作成したflagを用いて、売上が初めてあった日以前はNaN、それ以降は売上の値が入った行列を作成\nsales_train_val_values = np.where(flag,np.nan,sales_train_val_values)\nprint(sales_train_val_values)","metadata":{"execution":{"iopub.status.busy":"2021-12-02T04:23:33.458006Z","iopub.execute_input":"2021-12-02T04:23:33.458461Z","iopub.status.idle":"2021-12-02T04:23:33.807831Z","shell.execute_reply.started":"2021-12-02T04:23:33.458398Z","shell.execute_reply":"2021-12-02T04:23:33.80676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 作成した行列を、元のデータフレームに入れる\nsales_train_val[d_name] = sales_train_val_values\n\n# 不要な変数は削除しメモリ開放してあげる\ndel tmp,sales_train_val_values\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-12-02T04:23:33.809524Z","iopub.execute_input":"2021-12-02T04:23:33.810206Z","iopub.status.idle":"2021-12-02T04:24:39.052657Z","shell.execute_reply.started":"2021-12-02T04:23:33.810149Z","shell.execute_reply":"2021-12-02T04:24:39.051883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sales_train_val.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-02T04:24:39.054221Z","iopub.execute_input":"2021-12-02T04:24:39.054948Z","iopub.status.idle":"2021-12-02T04:24:39.631879Z","shell.execute_reply.started":"2021-12-02T04:24:39.05489Z","shell.execute_reply":"2021-12-02T04:24:39.630853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"calculate number of period after first non-zero demand date","metadata":{}},{"cell_type":"code","source":"# 売上開始日が最も遅かったものについて、売上開始後に何日分のデータが存在しているか確認\n1913-np.max(start_no)","metadata":{"execution":{"iopub.status.busy":"2021-12-02T04:24:39.63306Z","iopub.execute_input":"2021-12-02T04:24:39.633359Z","iopub.status.idle":"2021-12-02T04:24:39.639673Z","shell.execute_reply.started":"2021-12-02T04:24:39.633318Z","shell.execute_reply":"2021-12-02T04:24:39.638713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# meltを利用し、残したい列をid_varsに指定。他の列はdemand列として縦持ちにする。\nsales_train_val = pd.melt(sales_train_val,\n                                     id_vars = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], \n                                     var_name = 'day', value_name = 'demand')\nsales_train_val.head(3)","metadata":{"execution":{"iopub.status.busy":"2021-12-02T04:24:39.641192Z","iopub.execute_input":"2021-12-02T04:24:39.641523Z","iopub.status.idle":"2021-12-02T04:24:45.20072Z","shell.execute_reply.started":"2021-12-02T04:24:39.641473Z","shell.execute_reply":"2021-12-02T04:24:45.199657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#加工後  \ndisplay(sales_train_val.head(5))\nprint('Melted sales train validation has {} rows and {} columns'.format(sales_train_val.shape[0],\n                                                                            sales_train_val.shape[1]))","metadata":{"execution":{"iopub.status.busy":"2021-12-02T04:24:45.20211Z","iopub.execute_input":"2021-12-02T04:24:45.20261Z","iopub.status.idle":"2021-12-02T04:24:45.219262Z","shell.execute_reply.started":"2021-12-02T04:24:45.202397Z","shell.execute_reply":"2021-12-02T04:24:45.218062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 各商品の直近2年間分のデータに限定\nsales_train_val = sales_train_val.iloc[-nrows:,:]\n\n# さらに、demand列がnull\"ではない\"レコードに限定\nsales_train_val = sales_train_val[~sales_train_val.demand.isnull()]","metadata":{"execution":{"iopub.status.busy":"2021-12-02T04:24:45.220739Z","iopub.execute_input":"2021-12-02T04:24:45.221084Z","iopub.status.idle":"2021-12-02T04:24:46.513044Z","shell.execute_reply.started":"2021-12-02T04:24:45.221033Z","shell.execute_reply":"2021-12-02T04:24:46.511878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sales_train_val.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-02T04:24:46.514703Z","iopub.execute_input":"2021-12-02T04:24:46.515167Z","iopub.status.idle":"2021-12-02T04:24:46.529772Z","shell.execute_reply.started":"2021-12-02T04:24:46.515099Z","shell.execute_reply":"2021-12-02T04:24:46.528638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.1と同様に予測部分(validation/evaluation部分)のmelt処理し, 学習データと結合する. 出力はdataという変数.","metadata":{}},{"cell_type":"markdown","source":"予測部分のsubmission fileを同じくmelt処理し、sales_train_valとつなげる。  \n処理の注意点:  \n* submission fileの列名を\"d_xx\"形式に変更する. submission fileで縦に結合されたvalidationとevaluationを一度分割し、それぞれことなる28日間の列名\"d_xx\"をそれぞれ付与。\n* submission fileには, idの詳細（item, department, state等）が無いためidをキーに, sales validationから取得したproductを結合\n* test2は、6/1まで不要なため削除","metadata":{}},{"cell_type":"code","source":"# seperate test dataframes\n\n# submission fileのidのvalidation部分と, ealuation部分の名前を取得\ntest1_rows = [row for row in submission['id'] if 'validation' in row]\ntest2_rows = [row for row in submission['id'] if 'evaluation' in row]\n\n# submission fileのvalidation部分をtest1, ealuation部分をtest2として取得\ntest1 = submission[submission['id'].isin(test1_rows)]\ntest2 = submission[submission['id'].isin(test2_rows)]\n\n# test1, test2の列名の\"F_X\"の箇所をd_XXX\"の形式に変更\ntest1.columns = [\"id\"] + [f\"d_{d}\" for d in range(1914, 1914 + DAYS_PRED)]\ntest2.columns = [\"id\"] + [f\"d_{d}\" for d in range(1942, 1942 + DAYS_PRED)]\n\n# test2のidの'_evaluation'を置換\n#test1['id'] = test1['id'].str.replace('_validation','')\ntest2['id'] = test2['id'].str.replace('_evaluation','_validation')\n\n\n# idをキーにして, idの詳細部分をtest1, test2に結合する.\ntest1 = test1.merge(product, how = 'left', on = 'id')\ntest2 = test2.merge(product, how = 'left', on = 'id')\n\n# test1, test2をともにmelt処理する.（売上数量:demandは0）\ntest1 = pd.melt(test1, id_vars = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], \n                var_name = 'day', value_name = 'demand')\n\ntest2 = pd.melt(test2, id_vars = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], \n                var_name = 'day', value_name = 'demand')\n\n# validation部分と, evaluation部分がわかるようにpartという列を作り、 test1,test2のラベルを付ける。\nsales_train_val['part'] = 'train'\ntest1['part'] = 'test1'\ntest2['part'] = 'test2'\n\n# sales_train_valとtest1, test2の縦結合.\ndata = pd.concat([sales_train_val, test1, test2], axis = 0)\n\n# memoryの開放\ndel sales_train_val, test1, test2\n\n# delete test2 for now(6/1以前は, validation部分のみ提出のため.)\ndata = data[data['part'] != 'test2']\n\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-12-02T04:24:46.531401Z","iopub.execute_input":"2021-12-02T04:24:46.531697Z","iopub.status.idle":"2021-12-02T04:24:59.908589Z","shell.execute_reply.started":"2021-12-02T04:24:46.531657Z","shell.execute_reply":"2021-12-02T04:24:59.907509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.dataにcalendar/sell_pricesを結合","metadata":{"trusted":true}},{"cell_type":"code","source":"#calendarの結合\n# drop some calendar features(不要な変数の削除:weekdayやwdayなどはdatetime変数から後ほど作成できる。)\ncalendar.drop(['weekday', 'wday', 'month', 'year'], \n              inplace = True, axis = 1)\n\n# notebook crash with the entire dataset (maybee use tensorflow, dask, pyspark xD)(dayとdをキーにdataに結合)\ndata = pd.merge(data, calendar, how = 'left', left_on = ['day'], right_on = ['d'])\ndata.drop(['d', 'day'], inplace = True, axis = 1)\n\n# memoryの開放\ndel  calendar\ngc.collect()\n\n#sell priceの結合\n# get the sell price data (this feature should be very important)\ndata = data.merge(sell_prices, on = ['store_id', 'item_id', 'wm_yr_wk'], how = 'left')\nprint('Our final dataset to train has {} rows and {} columns'.format(data.shape[0], data.shape[1]))\n\n# memoryの開放\ndel  sell_prices\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-12-02T04:24:59.909971Z","iopub.execute_input":"2021-12-02T04:24:59.910325Z","iopub.status.idle":"2021-12-02T04:25:28.948465Z","shell.execute_reply.started":"2021-12-02T04:24:59.910224Z","shell.execute_reply":"2021-12-02T04:25:28.947716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head(3)","metadata":{"execution":{"iopub.status.busy":"2021-12-02T04:25:28.949519Z","iopub.execute_input":"2021-12-02T04:25:28.950052Z","iopub.status.idle":"2021-12-02T04:25:28.971447Z","shell.execute_reply.started":"2021-12-02T04:25:28.950007Z","shell.execute_reply":"2021-12-02T04:25:28.970586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.tail(3)","metadata":{"execution":{"iopub.status.busy":"2021-12-02T04:25:28.972679Z","iopub.execute_input":"2021-12-02T04:25:28.97304Z","iopub.status.idle":"2021-12-02T04:25:28.993313Z","shell.execute_reply.started":"2021-12-02T04:25:28.972996Z","shell.execute_reply":"2021-12-02T04:25:28.992134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 出力","metadata":{}},{"cell_type":"code","source":"data.to_pickle(\"m5_all_data_merged.pickle\")","metadata":{"execution":{"iopub.status.busy":"2021-12-02T04:25:37.813439Z","iopub.execute_input":"2021-12-02T04:25:37.814086Z","iopub.status.idle":"2021-12-02T04:25:46.478252Z","shell.execute_reply.started":"2021-12-02T04:25:37.814039Z","shell.execute_reply":"2021-12-02T04:25:46.477368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}