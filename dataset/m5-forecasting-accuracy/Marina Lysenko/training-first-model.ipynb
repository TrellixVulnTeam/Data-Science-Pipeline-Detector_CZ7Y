{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport time\nimport numpy as np \nimport pandas as pd\nfrom itertools import product\nimport joblib\nimport warnings\n\n\nwarnings.simplefilter('ignore')\npd.options.display.max_columns = 50\n\n\ndef preprocess_sales(sales):\n    sales['weekofyear'] = sales['date'].dt.weekofyear.astype(np.uint8)\n    sales['quater'] = sales['date'].dt.quarter.astype(np.uint8)\n    sales['day'] = sales['date'].dt.day.astype(np.uint8)\n    sales['sell_price'] = sales['sell_price'].fillna(0)\n    sales['event_type_1'] = sales['event_type_1'].cat.add_categories(\"unknown\").fillna(\"unknown\")\n    sales['event_type_2'] = sales['event_type_2'].cat.add_categories(\"unknown\").fillna(\"unknown\")\n    sales['event_name_1'] = sales['event_name_1'].cat.add_categories(\"unknown\").fillna(\"unknown\")\n    sales['event_name_2'] = sales['event_name_2'].cat.add_categories(\"unknown\").fillna(\"unknown\")\n    return sales\n\n\ndef read_data(is_train=True, first_day=1, last_day=1913, observation_window=57, \n             calendar_file=None, prices_file=None, sales_file=None):\n    \"\"\"\n    is_train: признак, мы считываем данные для обучения модели или submission\n    first_day: с какого дня читаем данные (например, первые 300 дней можем пропустить).\n    Акутально только, если is_train = True\n    last_day: последний день в выборке \n    observation_window: окно наблюдения, за которое нам нужно прочитать данные (актуально только\n    если is_train = False. Например, для имплементации нам нужны данные только за 50 дней назад)\n    calendar_file, prices_file, sales_file: путь к файлам с данными\n    \"\"\"\n    start_day = first_day if is_train else last_day - observation_window\n    finish_day = min(1913, last_day)\n\n    daycols = ['d_{}'.format(i) for i in range(start_day, finish_day + 1)]\n    daycols = {c: np.int16 for c in daycols}\n\n    catcols = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']\n    catcols = {c: 'category' for c in catcols}\n    sales_dtypes = {**catcols, **daycols}\n\n    calendar_dtypes = dict(\n          wm_yr_wk=np.int16, \n          weekday='category', \n          wday=np.uint8, \n          month=np.uint8,\n          year=np.int16,\n          event_name_1='category', \n          event_type_1='category', \n          event_name_2='category', \n          event_type_2='category', \n          snap_CA=np.uint8, \n          snap_TX=np.uint8, \n          snap_WI=np.uint8\n    )\n\n    prices_dtypes = dict(\n         store_id='category', \n         item_id='category', \n         wm_yr_wk=np.int16, \n         sell_price=np.float32\n    )\n\n    calendar = pd.read_csv(calendar_file, dtype=calendar_dtypes)\n    calendar['date'] = pd.to_datetime(calendar['date'])\n    calendar['d'] = calendar['d'].str[2:10].astype(np.int16)\n\n    sell_prices = pd.read_csv(prices_file, dtype=prices_dtypes)\n\n    sales = pd.read_csv(sales_file, dtype=sales_dtypes)\n\n    # add empy day sales for test data we should predict\n    if not is_train:\n        for day in range(last_day + 1, last_day + 29):\n            daycols[f\"d_{day}\"] = np.int16\n            sales[f\"d_{day}\"] = np.nan\n\n    sales = pd.melt(sales, id_vars=catcols, value_vars=daycols, var_name=\"d\", value_name=\"sales\")\n    sales['d'] = sales['d'].str[2:10].astype(np.int16)\n    print(\"Max day\", sales['d'].max())\n\n    print(\"Sales shape before merge calendar\", sales.shape)\n    sales = sales.merge(calendar, on='d', how='left', copy=False)\n    print(\"Sales shape after merge calendar\", sales.shape)\n    sales = sales.merge(sell_prices, on=[\"store_id\", \"item_id\", \"wm_yr_wk\"], how='left', copy=False)\n    sales['store_id'] = sales['store_id'].astype('category')\n    sales['item_id'] = sales['item_id'].astype('category')\n    print(\"Sales shape after merge prices\", sales.shape)\n\n    # some date based features and preprocessing\n    sales = preprocess_sales(sales)\n    \n    return sales\n\n\ndef read_train_data(use_cache=True, first_day=1, last_day=1913):\n    joblib_filename='/kaggle/input/combine-train-data/sales_train_validation.joblib'\n    if os.path.isfile(joblib_filename) and use_cache:\n        sales = joblib.load(joblib_filename)\n        sales = preprocess_sales(sales)        \n        return sales\n    return read_data(is_train=True, first_day=first_day, last_day=last_day, \n                     calendar_file='/kaggle/input/m5-forecasting-accuracy/calendar.csv', \n                     prices_file='/kaggle/input/m5-forecasting-accuracy/sell_prices.csv', \n                     sales_file='/kaggle/input/m5-forecasting-accuracy/sales_train_validation.csv'\n                    )\n\n\ndef read_test_data(last_day=1913, obesrvation_window=57):\n    return read_data(is_train=False, last_day=last_day, observation_window=obesrvation_window,\n                     calendar_file='/kaggle/input/m5-forecasting-accuracy/calendar.csv', \n                     prices_file='/kaggle/input/m5-forecasting-accuracy/sell_prices.csv', \n                     sales_file='/kaggle/input/m5-forecasting-accuracy/sales_train_validation.csv'\n                    )\n\n\ndef feature_engineering(df, shifts_size=None, windows_size=None):\n\n    for shift_size in shifts_size:\n        \n        sales_shift_col = f'sales_shift_{shift_size}D'\n        sell_price_shift_col = f'sell_price_shift_{shift_size}D'\n    \n        shifted_df = df[['id', 'sales', 'sell_price']].groupby('id')[['sales', 'sell_price']].shift(shift_size)\n        df[sales_shift_col] = shifted_df['sales'].fillna(0).astype(np.float32)\n        df[sell_price_shift_col] = shifted_df['sell_price'].fillna(0).astype(np.float32)\n        \n        for window_size in windows_size:        \n\n            rolled_features = df[['id', sales_shift_col, sell_price_shift_col]].rolling(window_size).agg({\n                sales_shift_col: ['mean', 'std'], \n                sell_price_shift_col: ['min', 'mean']                \n            })\n\n            rolled_features.columns = ['_'.join(c) + f'_{window_size}D' for c in rolled_features]\n            rolled_features.fillna(0, inplace=True)\n            rolled_features = rolled_features.apply(lambda x: x.astype(np.float32))\n\n            df = pd.concat([df, rolled_features], axis=1)\n            del rolled_features\n        \n        del shifted_df\n    \n    return df\n\n\ndef get_weights_validation(data):\n#     https://raw.githubusercontent.com/Mcompetitions/M5-methods/master/validation/weights_validation.csv\n    last_28_days = data[data['d'] >= 1886]\n    last_28_days['revenue'] = last_28_days['sales'] * last_28_days['sell_price']\n\n    item_weights = []\n    last_28_days['total'] = 'Total'\n    last_28_days['x'] = 'X'\n    levels = {\n        'Level1': ['total', 'x'], \n        'Level2': ['state_id', 'x'], \n        'Level3': ['store_id', 'x'], \n        'Level4': ['cat_id', 'x'], \n        'Level5': ['dept_id', 'x'], \n        'Level6': ['state_id', 'cat_id'], \n        'Level7': ['state_id', 'dept_id'], \n        'Level8': ['store_id', 'cat_id'], \n        'Level9': ['store_id', 'dept_id'], \n        'Level10': ['item_id', 'x'], \n        'Level11': ['state_id', 'item_id'], \n        'Level12': ['item_id', 'store_id']\n    }\n    for level_id, aggrs in levels.items():\n        aggr_1, aggr_2 = aggrs\n\n        sales_aggr = last_28_days.groupby([aggr_1, aggr_2]).agg(dict(revenue='sum')) / last_28_days['revenue'].sum()\n        sales_aggr.reset_index(inplace=True)\n        sales_aggr.columns = ['Agg_Level_1', 'Agg_Level_2', 'Weight']\n        sales_aggr['Level_id'] = level_id\n        sales_aggr = sales_aggr[['Level_id', 'Agg_Level_1', 'Agg_Level_2', 'Weight']]\n        item_weights.append(sales_aggr)\n\n    item_weights = pd.concat(item_weights)\n    item_weights.reset_index(drop=True, inplace=True)\n    return item_weights\n\n\ndef get_weights(data, min_d, max_d=1913):\n    sample = data[data['d'].between(min_d, max_d)]\n    sample['revenue'] = sample['sales'] * sample['sell_price']\n    weights = sample.groupby('id').agg(dict(revenue='sum')) / sample['revenue'].sum()\n    weights.sort_index(inplace=True)\n    weights = np.array(weights['revenue'])\n    return weights\n\n\ndef get_denominator(data, min_d, max_d=1913):\n    data_short = data[['id', 'sales', 'd']]\n    data_short = data_short[data_short['d'].between(min_d, max_d)]\n    data_short['sales_prev'] = data_short[[\"id\",\"sales\"]].groupby(\"id\")[\"sales\"].shift(1)\n    data_short = data_short[data_short['sales'] > 0] \n    data_short['squared_diff'] = (data_short['sales'] - data_short['sales_prev']) ** 2\n    denominator = data_short.groupby('id').agg(dict(squared_diff='mean'))\n    denominator.sort_index(inplace=True)\n    denominator = np.array(denominator['squared_diff'])\n    return denominator    \n\n\ndef get_numerator(y_true, y_pred, cnt_items=30490, cnt_days=28):\n    assert len(y_true) == len(y_pred)\n    \n    cnt_rows = int(cnt_items * cnt_days)\n    y_true = y_true[-cnt_rows:]\n    y_pred = y_pred[-cnt_rows:]\n\n    diff = (y_true - y_pred) ** 2\n    diff = diff.reshape(cnt_items, cnt_days)\n    return diff.mean(axis=1)\n\n\nclass WRMSSE:\n    def __init__(self, weights, denominator, cnt_items=30490, cnt_days=28):\n        self.weights = weights\n        self.denominator = denominator\n        self.cnt_items = cnt_items\n        self.cnt_days = cnt_days\n                \n    def __call__(self, y_true, y_pred):\n        numerator = get_numerator(y_true, y_pred, cnt_items=self.cnt_items, cnt_days=self.cnt_days)\n        rmsse = ((numerator / self.denominator) ** 0.5)\n        return np.dot(self.weights.reshape(1, -1), rmsse.reshape(-1, 1))[0][0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Time Series Deep Learning","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"%%time\n\n\ndf = read_train_data(use_cache=True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.loc[df['store_id'] == 'CA_1']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\nimport pandas as pd\nimport numpy as np \nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom tensorflow import keras \nfrom tensorflow.keras import backend as K\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# make sure you run on tensorlfow 2.2.0. This notebook won't work on 2.1.0 ;(\ntf.__version__","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# seed random generators\n# don't suggest to do it in production \nrandom.seed(42)\nnp.random.seed(42)\ntf.random.set_seed(42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Helper functions \n\n#### First, let's prepare a code which generates sliding windows. \nWe are going to use numpy index_tricks for that. Function  takes original data and ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# создаем 3 мерные x,y\ndef create_sliding_windows(arr, window_size):\n    \"\"\" Takes original data matrix with shape (N, D) and creates sliding windows of size window_size. \n    Result matrix has shape (M, W, D), where\n        - M is number of observations equal to (N - window_size + 1)\n        - W is window_size\n        - D - dimensinality of the original feature space\n        \n    Parameters\n    ----------\n    arr : np.ndarray \n        Original 2D matrix\n    window_size : int\n        Size of the sliding window\n    \n    Returns\n    -------\n    result : np.ndarray \n        Resulting 3D matrix where each observation is a sliding window. \n    \"\"\"\n    arr = arr.astype(np.float)\n    (stride,) = arr.strides\n    return np.lib.index_tricks.as_strided(\n        arr,\n        (arr.shape[0] - window_size + 1, window_size),\n        strides=[stride, stride],\n        writeable=False,\n    )\n\n\n# other helper functions for plotting losses and predictions later\ndef plot_loss(history): \n    plt.figure(figsize=(10, 5))\n    loss = history['loss']\n    line = plt.plot(range(len(loss)), loss)\n    line[0].set_label('Training loss')\n    if 'val_loss' in history: \n        val_loss = history['val_loss']\n        line = plt.plot(range(len(val_loss)), val_loss)\n        line[0].set_label('Validation loss')\n    plt.legend()\n    \n    #как харашо обучаем\ndef plot_predictions(x, y_true, y_pred):\n    plt.figure(figsize=(10, 5))\n    x_indexes = np.arange(x.shape[0])\n    y_indexes = np.arange(x.shape[0], x.shape[0] + y_true.shape[0])\n    line1 = plt.plot(x_indexes, x)\n    line2 = plt.plot(y_indexes, y_true)\n    line3 = plt.plot(y_indexes, y_pred)\n    \n    line1[0].set_label('Historical data')\n    line2[0].set_label('Ground truth')\n    line3[0].set_label('Forecast')\n    plt.legend()\n    \n    \nclass MeanScaler(object):\n    def __init__(self, copy=True):\n        self.copy = copy\n        self.scale_ = None\n        \n    def fit(self, X): \n        self.scale_ = np.mean(np.abs(X), axis=1, keepdims=True) + 1\n        return self\n    \n    def _reshaped_scale(self, X_shape): \n        new_shape = (self.scale_.shape[0],) + (1,) * (len(X_shape) - 1)\n        return np.reshape(self.scale_, new_shape)\n    \n    def transform(self, X): \n        if self.copy: \n            X = X.copy()\n        \n        scale_ = self._reshaped_scale(X.shape)\n        X /= scale_ \n        return X\n    \n    def inverse_transform(self, X): \n        if self.copy: \n            X = X.copy()\n        \n        scale_ = self._reshaped_scale(X.shape)\n        X *= scale_\n        return X \n    \n    def fit_transform(self, X): \n        return self.fit(X).transform(X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's plot some data and highlight several sliding windows it it. Есть и тренд и сезонность. окно в 18 точек, слайдит слева направо по окну и ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#https://www.kaggle.com/jcolless/m5-rnn-wavenet-n-beats-approach#N-BEATS","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df['sales'].values\nwindow_size = 28\ny_windows = create_sliding_windows(y, window_size)\n\n_, ax = plt.subplots(1, figsize=(10, 5))\nax.plot(np.arange(y.shape[0]), y)\ncolors = plt.rcParams['axes.prop_cycle'].by_key()['color']\nfor i, window_idx in enumerate([10, 15, 50, 120]): \n    p = patches.Rectangle((window_idx, np.min(y)), window_size, 600, linewidth=2, edgecolor=colors[i], linestyle='--', facecolor='none')\n    ax.add_patch(p)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we need to have some way to create train/validation partitions.\nIdeally we would use something line sklearn.model_selection.TimeSeriesSplit, which allows to create several backtest and do something very similar to traditional cross validation. But we are going to use single backtest here to keep things simple.\n\nNote we don't do shuffling on this function cause it will break \"time awareness\" in dataset and we will end up learning on future to predict the past.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# \ndef train_validation_split(X, y, validation_pct=0.1): \n    size = X.shape[0]\n    validation_rows = int(size * validation_pct) \n    \n    index = np.arange(size)\n    validation = index[-validation_rows:]\n    training = index[:-validation_rows] \n    \n    return X[training], y[training], X[validation], y[validation]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Prepare simple baselines.\n\nWe will use 2 very simple baselines: \n- **latest naive baseline**: outputs last known value (e.g. If we have data up until Sunday and want to predict for next week - each day gets Sunday's value as prediction)\n- **seasonal naive baseline**: outputs last known seasonal value (e.g. If we have data up until Sunday and want to predict for next week - future Monday get's last known Monday value as prediction, Tuesday - last known Tuesday, etc.)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# latest \ndef simple_baseline(x, y):\n    \"\"\" Simple naive baseline predictions. \n    For each row in y predicts latest known value. \n    \"\"\"\n    latest = x[:, -1, :]\n    y_pred = np.tile(latest, (1, y.shape[1]))\n    return y_pred\n\n\ndef seasonal_baseline(x, y, period): \n    \"\"\" Seasonal naive baseline predictions. \n    \"\"\"\n    indexes = np.arange(y.shape[1]) - period\n    return np.squeeze(x[:, indexes])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Prepare evaluation metrics.\nMASE: mean absolute scaled error\nsMAPE: scaled mean absolute percentage error\nOWA: overall weighted average (M4 competition metric)\nRMSE","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def mase(y_true, y_pred, y_naive_pred):\n    baseline_mae = mean_absolute_error(y_true, y_naive_pred)\n    mae = mean_absolute_error(y_true, y_pred)\n    return mae / baseline_mae\n\n\ndef smape(y_true, y_pred):\n    error = np.abs(y_pred - y_true) / ((np.abs(y_true) + np.abs(y_pred)) / 2)\n    error[~np.isfinite(error)] = 0.0\n    return error.sum() * 100.0 / len(y_true)\n\n\ndef owa(smape, smape_naive, mase, mase_naive): \n    return (smape / smape_naive + mase / mase_naive) / 2.\n\n\ndef rmse(y_true, y_pred): \n    return np.sqrt(mean_squared_error(y_true, y_pred))\n\n\ndef evaluate(y_true, y_pred, y_pred_naive): \n    RMSE = rmse(y_true, y_pred)\n    sMAPE = smape(y_true, y_pred)\n    sMAPE_naive = smape(y_true, y_pred_naive)\n    MASE = mase(y_true, y_pred, y_pred_naive)\n    MASE_naive = mase(y_true, y_pred_naive, y_pred_naive)\n    OWA = owa(sMAPE, sMAPE_naive, MASE, MASE_naive)\n    \n    print('\\tRMSE   {}'.format(RMSE))\n    print('\\tsMAPE  {}'.format(sMAPE))\n    print('\\tMASE   {}'.format(MASE))\n    print('\\tOWA    {}'.format(OWA))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Sanity check: evaluating baselines","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data = df['sales'].values\nfdw = 28*4\nfw = 28\ndata_windows = create_sliding_windows(data, window_size=fdw + fw)\nX = data_windows[:, :fdw, np.newaxis]\ny = data_windows[:, fdw:]\n\nX_train, y_train, X_val, y_val = train_validation_split(X, y, validation_pct=0.2)\n\ny_pred_simple = simple_baseline(X_val, y_val)\ny_pred_seasonal = seasonal_baseline(X_val, y_val, period=28)# один спайк это год, сезонность нужно знать наперед\n\nprint('Simple baseline')\nevaluate(y_val, y_pred_simple, y_pred_simple)\n\nprint('Seasonal baseline')\nevaluate(y_val, y_pred_seasonal, y_pred_seasonal)\n\nplot_predictions(X_val[0], y_val[0], y_pred_simple[0])\nplot_predictions(X_val[0], y_val[0], y_pred_seasonal[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Time to build models!\nLet's start with very simple seq2seq-like network, which has RNN encoder, RNN decoder and linear output layer.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_rnn_cells(rnn_units):\n    cells = []\n    for units in rnn_units:\n        cells.append(keras.layers.LSTMCell(units))\n    return cells\n\n#Модель!\nclass LSTMEstimator(keras.Model): \n    def __init__(self, fw, rnn_units, output_activation='linear'): \n        super(LSTMEstimator, self).__init__()\n        \n        self.encoder_layer = keras.layers.RNN(create_rnn_cells(rnn_units))\n        self.repeat_layer = keras.layers.RepeatVector(fw)#повторить на кол-во предикшинов с прошлого шага\n        self.decoder_layer = keras.layers.RNN(create_rnn_cells(rnn_units), return_sequences=True)\n        self.output_layer = keras.layers.Dense(1, activation=output_activation)\n        \n    def call(self, inputs): #модефайним как лояра друг за другом екзекьютятся\n        outputs = self.encoder_layer(inputs)\n        outputs = self.repeat_layer(outputs)\n        outputs = self.decoder_layer(outputs)\n        outputs = self.output_layer(outputs)\n        return outputs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = LSTMEstimator(fw=28, rnn_units=[32])\nmodel.compile(loss='mae', optimizer=keras.optimizers.Adam(0.01))#дефолтное значение\nhistory = model.fit(\n    X_train, \n    y_train,\n    validation_data=(X_val, y_val), \n    epochs=200, \n    batch_size=8,\n    shuffle=True, \n    verbose=False,\n)\nplot_loss(history.history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = model.predict(X_val)\ni = 10\nplot_predictions(X_val[i], y_val[i], y_pred[i])\nevaluate(y_val, np.squeeze(y_pred), y_pred_seasonal)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_scaler = MeanScaler()\ntrain_scaler.fit(X_train)\n\nval_scaler = MeanScaler()\nval_scaler.fit(X_val)\n\nmodel = LSTMEstimator(fw=12, rnn_units=[32])\nmodel.compile(loss='mae', optimizer=keras.optimizers.Adam(0.01))\nhistory = model.fit(\n    train_scaler.transform(X_train), \n    train_scaler.transform(y_train),\n    validation_data=(val_scaler.transform(X_val), val_scaler.transform(y_val)), \n    epochs=200, \n    batch_size=8,\n    shuffle=True, \n    verbose=False,\n)\nplot_loss(history.history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = model.predict(val_scaler.transform(X_val))\ny_pred = val_scaler.inverse_transform(y_pred)\ny_pred_seasonal = seasonal_baseline(X_val, y_val, period=12)\ni = 10\nplot_predictions(X_val[i], y_val[i], y_pred[i])\nevaluate(y_val, np.squeeze(y_pred), y_pred_seasonal)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### DeepAR  по воркшопу https://www.youtube.com/watch?v=OP4wa3cFSG8&list=PLmFBTSyQ2rzUy1K929-6RRfFcfDZPUq6U&index=2","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class GaussianLayer(keras.layers.Layer):\n    def __init__(self, **kwargs):\n        super(GaussianLayer, self).__init__(**kwargs)\n        self.W_mu = None\n        self.b_mu = None\n        self.W_sigma = None\n        self.b_sigma = None\n\n    def build(self, input_shape):\n        super(GaussianLayer, self).build(input_shape)\n        \n        dim = input_shape[-1]\n        \n        self.W_mu = self.add_weight(\n            name='W_mu', \n            shape=(dim, 1), \n            initializer='glorot_normal', \n            trainable=True,\n        )\n        self.b_mu = self.add_weight(\n            name='b_mu', \n            shape=(1,), \n            initializer='zeros', \n            trainable=True,\n        )\n        \n        self.W_sigma = self.add_weight(\n            name='W_sigma',\n            shape=(dim, 1),\n            initializer='glorot_normal',\n            trainable=True,\n        )\n        self.b_sigma = self.add_weight(\n            name='b_sigma',\n            shape=(1,),\n            initializer='zeros', \n            trainable=True,\n        )        \n        \n    def call(self, inputs):\n        mu = K.dot(inputs, self.W_mu)\n        mu = K.bias_add(mu, self.b_mu, data_format='channels_last')\n        \n        sigma = K.dot(inputs, self.W_sigma)\n        sigma = K.bias_add(sigma, self.b_sigma, data_format='channels_last')\n        sigma = K.softplus(sigma) + K.epsilon()\n        \n        return tf.squeeze(mu, axis=-1), tf.squeeze(sigma, axis=-1) \n    \n\ndef gaussian_loss(y_true, mu, sigma):\n    loss = (\n         tf.math.log(sigma) \n        + 0.5 * tf.math.log(2 * np.pi) \n        + 0.5 * tf.square(tf.math.divide(y_true - mu, sigma))\n    )\n    return tf.reduce_mean(loss)\n\n    \ndef gaussian_sample(mu, sigma): \n    mu = tf.expand_dims(mu, axis=-1)\n    sigma = tf.expand_dims(sigma, axis=-1)\n    \n    samples = tf.random.normal((300,), mean=mu, stddev=sigma)\n    return tf.reduce_mean(samples, axis=-1)\n\n    \nclass DeepAR(keras.Model): \n    def __init__(self, fw, rnn_units): \n        super(DeepAR, self).__init__()\n        \n        self.encoder_layer = keras.layers.RNN(create_rnn_cells(rnn_units))\n        self.repeat_layer = keras.layers.RepeatVector(fw)\n        self.decoder_layer = keras.layers.RNN(create_rnn_cells(rnn_units), return_sequences=True)\n        self.gaussian_layer = GaussianLayer()\n\n    def call(self, inputs): \n        outputs = self.encoder_layer(inputs)\n        outputs = self.repeat_layer(outputs)\n        outputs = self.decoder_layer(outputs)\n        return self.gaussian_layer(outputs)\n\n    def train_step(self, inputs):\n        x, y = inputs\n        \n        with tf.GradientTape() as tape: \n            mu, sigma = self(x)\n            loss_val = self.loss(y, mu, sigma)\n\n        grads = tape.gradient(loss_val, self.trainable_weights)\n        grads = [tf.clip_by_value(grad, -1., 1.) for grad in grads]\n        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n        \n        return {'loss': loss_val}\n\n    def test_step(self, inputs):\n        x, y = inputs\n        mu, sigma = self(x, training=False)\n        loss_val = self.loss(y, mu, sigma)\n        return {'loss': loss_val}\n    \n    def predict_step(self, inputs): \n        x, = inputs\n        mu, sigma = self(x, training=False)\n        return gaussian_sample(mu, sigma)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rnn_units = [32]\n\ndeep_ar = DeepAR(fw, rnn_units)\n\ntrain_scaler = MeanScaler()\ntrain_scaler.fit(X_train)\n\nval_scaler = MeanScaler()\nval_scaler.fit(X_val)\n\ndeep_ar.compile(loss=gaussian_loss, optimizer=keras.optimizers.Adam(0.01))\n\nhistory = deep_ar.fit(\n    train_scaler.transform(X_train), \n    train_scaler.transform(y_train),\n    validation_data=(val_scaler.transform(X_val), val_scaler.transform(y_val)), \n    epochs=200, \n    batch_size=8,\n    shuffle=True, \n    verbose=False,\n)\nplot_loss(history.history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = deep_ar.predict(val_scaler.transform(X_val))\ny_pred = val_scaler.inverse_transform(y_pred)\ni = 10\nplot_predictions(X_val[i], y_val[i], y_pred[i])\nevaluate(y_val, np.squeeze(y_pred), y_pred_seasonal)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Modeling Poisson distribution","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class PoissonLayer(keras.layers.Layer):\n    def __init__(self, **kwargs):\n        super(PoissonLayer, self).__init__(**kwargs)\n        self.W_rate = None\n        self.b_rate = None\n\n    def build(self, input_shape):\n        super(PoissonLayer, self).build(input_shape)\n        \n        dim = input_shape[0][-1]\n        \n        self.W_rate = self.add_weight(\n            name='W_rate', \n            shape=(dim, 1), \n            initializer='glorot_uniform', \n            trainable=True,\n        )\n        self.b_rate = self.add_weight(\n            name='b_rate', \n            shape=(1,), \n            initializer='zeros', \n            trainable=True,\n        )\n\n    def call(self, inputs):\n        x, scale = inputs\n        \n        rate = K.dot(x, self.W_rate)\n        rate = K.bias_add(rate, self.b_rate, data_format='channels_last')\n        rate = K.softplus(rate) + K.epsilon()\n        rate = rate * scale\n        \n        return tf.squeeze(rate, axis=-1)\n\n\nclass MeanScalerLayer(keras.layers.Layer): \n    \n    def call(self, inputs): \n        scale = tf.reduce_mean(tf.abs(inputs), axis=1, keepdims=True) + 1.0\n        outputs = inputs / scale\n        return outputs, scale\n\n\ndef poisson_loss(y_true, rate): \n    loss = -1.0 * (\n        y_true * tf.math.log(rate) \n        - tf.math.lgamma(y_true + 1.0) \n        - rate\n    )\n    return tf.reduce_mean(loss)\n\n\ndef poisson_sample(rate): \n    sample = tf.random.poisson((300,), lam=rate)\n    sample = tf.reduce_mean(sample, axis=0)\n    return tf.expand_dims(sample, axis=-1)\n\n\nclass DeepARPos(keras.Model): \n    def __init__(self, fw, rnn_units): \n        super(DeepARPos, self).__init__()\n        \n        self.mean_scaler_layer = MeanScalerLayer()\n        self.encoder_layer = keras.layers.RNN(create_rnn_cells(rnn_units))\n        self.repeat_layer = keras.layers.RepeatVector(fw)\n        self.decoder_layer = keras.layers.RNN(create_rnn_cells(rnn_units), return_sequences=True)\n        self.poisson_layer = PoissonLayer()\n\n    def call(self, inputs): \n        outputs, scale = self.mean_scaler_layer(inputs)\n        outputs = self.encoder_layer(outputs)\n        outputs = self.repeat_layer(outputs)\n        outputs = self.decoder_layer(outputs)\n        return self.poisson_layer([outputs, scale])\n\n    def train_step(self, inputs):\n        x, y = inputs\n        \n        with tf.GradientTape() as tape: \n            rate = self(x)\n            loss_val = self.loss(y, rate)\n    \n        grads = tape.gradient(loss_val, self.trainable_weights)\n        grads = [tf.clip_by_value(grad, -1., 1.) for grad in grads]\n        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n        \n        return {'loss': loss_val}\n\n    def test_step(self, inputs):\n        x, y = inputs\n        rate = self(x, training=False)\n        loss_val = self.loss(y, rate)\n        return {'loss': loss_val}\n    \n    def predict_step(self, inputs): \n        x, = inputs\n        rate = self(x, training=False)\n        return poisson_sample(rate)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rnn_units = [32]\n\ndeep_ar = DeepARPos(fw, rnn_units)\ndeep_ar.compile(loss=poisson_loss, optimizer=keras.optimizers.Adam(0.01))\n\nhistory = deep_ar.fit(\n    X_train,\n    y_train,\n    validation_data=(X_val, y_val), \n    epochs=200, \n    batch_size=8,\n    shuffle=True, \n    verbose=False,\n)\nplot_loss(history.history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = deep_ar.predict(X_val)\n\ni = 10\nplot_predictions(X_val[i], y_val[i], y_pred[i])\nevaluate(y_val, np.squeeze(y_pred), y_pred_seasonal)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}