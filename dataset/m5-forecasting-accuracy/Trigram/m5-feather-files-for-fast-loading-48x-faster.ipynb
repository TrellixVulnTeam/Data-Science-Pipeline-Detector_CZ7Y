{"cells":[{"metadata":{},"cell_type":"markdown","source":"# M5: Feather Files for Fast Data Loading\n\n---\n\nAs you all know, reading in the CSV files takes time for this competition so you can use the `feather` file format to load your files faster.\n\n---\n\n+ Alternatively, you can read it in from the dataset here: https://www.kaggle.com/nxrprime/m5-feather-files","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's test it:","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import plotly.express as px\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n# Copy from ragnar's kernel to reduce memory usage\nfrom pandas.api.types import is_datetime64_any_dtype as is_datetime\n\ndef reduce_mem_usage(df, use_float16=False):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        if is_datetime(df[col]):\n            # skip datetime type\n            continue\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if use_float16 and c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"%%time\nstv = pd.read_csv('../input/m5-forecasting-accuracy/sales_train_validation.csv')\nsales = pd.read_csv('../input/m5-forecasting-accuracy/sell_prices.csv')\ncal = pd.read_csv('../input/m5-forecasting-accuracy/calendar.csv')\nss = pd.read_csv('../input/m5-forecasting-accuracy/sample_submission.csv')\n# From michael mayer's kernel\nfrom sklearn.preprocessing import OrdinalEncoder\ndef prep_calendar(df):\n    df = df.drop([\"date\", \"weekday\"], axis=1)\n    df = df.assign(d = df.d.str[2:].astype(int))\n    df = df.fillna(\"missing\")\n    cols = list(set(df.columns) - {\"wm_yr_wk\", \"d\"})\n    df[cols] = OrdinalEncoder(dtype=\"int\").fit_transform(df[cols])\n    df = reduce_mem_usage(df)\n    return df\n\ndef prep_selling_prices(df):\n    gr = df.groupby([\"store_id\", \"item_id\"])[\"sell_price\"]\n    df[\"sell_price_rel_diff\"] = gr.pct_change()\n    df[\"sell_price_roll_sd7\"] = gr.transform(lambda x: x.rolling(7).std())\n    df[\"sell_price_cumrel\"] = (gr.shift(0) - gr.cummin()) / (1 + gr.cummax() - gr.cummin())\n    df = reduce_mem_usage(df)\n    return df\n\ndef reshape_sales(df, drop_d = None):\n    if drop_d is not None:\n        df = df.drop([\"d_\" + str(i + 1) for i in range(drop_d)], axis=1)\n    df = df.assign(id=df.id.str.replace(\"_validation\", \"\"))\n    df = df.reindex(columns=df.columns.tolist() + [\"d_\" + str(1913 + i + 1) for i in range(2 * 28)])\n    df = df.melt(id_vars=[\"id\", \"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"],\n                 var_name='d', value_name='demand')\n    df = df.assign(d=df.d.str[2:].astype(\"int16\"))\n    return df\n\ndef prep_sales(df):\n    df['min'] = df['demand'].apply('min')\n    df['max'] = df['demand'].apply('max')\n    df['std'] = df['demand'].apply('std')\n    df['mean'] = df['demand'].apply('mean')\n\n    # Remove rows with NAs except for submission rows. rolling_mean_t180 was selected as it produces most missings\n    df = reduce_mem_usage(df)\n\n    return df","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"stv = reshape_sales(stv)\nimport gc\ngc.collect()\nstv = prep_sales(stv)\ngc.collect()\nstv.to_feather('sales_train_validation.feather')\ndel stv\nsales = prep_selling_prices(sales)\ngc.collect()\nsales.to_feather('sell_prices.feather')\ndel sales\ncal = prep_calendar(cal) ### does not help cal\ngc.collect()\ncal.to_feather('calendar.feather')\ndel cal\nss.to_feather('sample_submission.feather')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have a total time of 10.6s secs spent reading the data. Not bad, but it **can** be improved.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nstv = pd.read_feather('sales_train_validation.feather')\nsales = pd.read_feather('sell_prices.feather')\ncal = pd.read_feather('calendar.feather')\nss = pd.read_feather('sample_submission.feather')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n\n### Nice! We got a 48x improvement over earlier reading the data. This can really be helpful with speed later!\n\n---","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}