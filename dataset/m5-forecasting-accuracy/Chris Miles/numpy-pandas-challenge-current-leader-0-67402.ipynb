{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nDAYS_BACK = 28","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Top scores and links to notebook posted here: \n*  0.67402[this notebook](https://www.kaggle.com/chrisrichardmiles/numpy-pandas-challenge-current-leader-71444/edit)\n* 0.71444 [this notebook](https://www.kaggle.com/chrisrichardmiles/numpy-pandas-challenge-current-leader-71444/edit)"},{"metadata":{},"cell_type":"markdown","source":"# Create a model utilizing only numpy and pandas \n## Goals: \n* Create basic and explainable models\n* Create models that can improve the application of more advanced techniques\n\n## Allowed: \n* Using insights gained from other analysis(visual, statistical, ML), but the final model must be constructed \"by hand\" with only numpy and pandas. \n\n\n\n## Not allowed: \n* using any other imports when constructing final csv for submission\n\n## Beginners: Please fork this as a starter. The functions I created to group the data and create submissions could be useful. \n\n## Intermediate/Advanced: Use knowledge extracted from your advanced algorithms and apply it to create a simple, explainable model. \n "},{"metadata":{},"cell_type":"markdown","source":"# Helper functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"def add_snap_col(df_in):\n    \"\"\"adds a 'snap_day' column to a dataframe that contains a state_id column and the columns 'snap_CA', 'snap_TX', \n    and snap_WI\"\"\"\n    df = pd.get_dummies(df_in, columns=['state_id'])\n    df['snap_day'] = (df.snap_CA * df.state_id_CA) + (df.snap_WI * df.state_id_WI) + (df.snap_TX * df.state_id_TX)\n    del df['state_id_WI'], df['state_id_CA'], df['state_id_TX']\n    return df\n\n\n\ndef melt_merge_snap(df):\n    df = df.melt(['id', 'state_id'], var_name='d', value_name='demand')\n    df = df.merge(cal)\n    df = add_snap_col(df)\n    return df\n\n\n\ndef get_sub():\n    \"\"\"returns a tidy dataframe version of the sample submission, merged with the calendar data, \n    without the 'demand' column. It can be used to join to a group by series to make predictions  \"\"\"\n    # make a copy of the sample submission\n    sub = ss.copy()\n    # change the column names to match the last 28 days\n    sub.columns = ['id'] + ['d_' + str(1914+x) for x in range(28)]\n    # select only the rows with an id with the validation tag\n    sub = sub.loc[sub.id.str.contains('validation')]\n    # melt this dataframe and merge it with the calendar so we can join it with group_by series we create\n    sub = sub.melt('id', var_name='d', value_name='demand')\n    sub = sub.merge(cal)\n    \n    \n    # add state_id column so that we can add the snap_day column\n    sub['state_id'] = sub.id.str.split('_', expand=True)[3]\n    \n    # add the snap_day column\n    sub = add_snap_col(sub)\n    \n    return sub.drop('demand', axis='columns')\n\n\n\ndef join_sub_groupby(sub, group):\n    \"\"\" \n    Joins the sub dataframe created by get_sub to a groupby series\n    \"\"\"\n    return sub.join(group, on=group.index.names)\n\n\n\ndef make_sub(df_in, ss, filename='submission.csv'): \n    \"\"\"\n    Takes a dataframe in the form given by join_sub_groupby, or any dataframe with the proper index and and 'd' colums.\n    returns a csv submission file in the correct format\n    \"\"\"\n    # pivot df to get it into the proper format for submission\n    df = df_in.pivot(index='id', columns='d', values='demand')\n    # need to reset index to take care of columns. comment next line out to see what i mean \n    df.reset_index(inplace=True)\n    \n    submission = ss[['id']].copy()    \n    submission = submission.merge(df)    \n    # we must copy the dataframe to match the format of the submission file which is twice as long as what we have\n    submission = pd.concat([submission, submission], axis=0)\n    # reset the id colum to have the same values as the sample submission\n    submission['id'] = ss.id.values\n    # rename the columns to match the sample submission format \n    submission.columns = ['id'] + ['F' + str(i) for i in range(1,29)]\n    \n    submission.to_csv(filename, index=False)  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Simple models: Just using the last known 28 days. \nmodel 1: avg of last 28, grouped by id, weekday           **LB score: .75238**\n\nmodel 2: avg of last 28, grouped by id, weekday, and snap **LB score: .72969**\n\n#### same as simple models, but different time periods\n\nmodel 3: avg of last 90, grouped by id, weekday, and snap **LB score: .71444**\n\nmodel 3_dark_magic: avg of last 90, grouped by id, weekday, and snap**LB score: .67402**\n\nmodel 4: avg of last 365, grouped by id, weekday, and snap **LB score: 0.88767**"},{"metadata":{},"cell_type":"markdown","source":"# Load data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"PATH = '/kaggle/input/m5-forecasting-accuracy/'\ncal = pd.read_csv(f'{PATH}calendar.csv')\nsell_prices = pd.read_csv(f'{PATH}sell_prices.csv')\nss = pd.read_csv(f'{PATH}sample_submission.csv')\nstv = pd.read_csv(f'{PATH}sales_train_validation.csv')\nstv_id = stv[['id','state_id']]\nstv = stv.iloc[:, :-DAYS_BACK]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Lets try different time ranges to use. I will always include the most recent data"},{"metadata":{"trusted":true},"cell_type":"code","source":"last_90 = pd.concat([stv_id, stv.iloc[:,-90:]], axis=1) # we include 0, and 5 to get the id and state id columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Melt the d_ columns, merge with calendar, and add a snap_day column. Snap column indicates if the item is snap eligible that day."},{"metadata":{"trusted":true},"cell_type":"code","source":"# last_28 = melt_merge_snap(last_28)\n# last_28.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"last_90 = melt_merge_snap(last_90)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"last_90.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model 1: group by id, and wday and average over demand"},{"metadata":{"trusted":true},"cell_type":"code","source":"# get the demand for each product, grouped by weekday\n# by_weekday = last_28.groupby(['id','wday'])['demand'].mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model 2: groupby id, wday, snap_day"},{"metadata":{"trusted":true},"cell_type":"code","source":"# by_weekday_snap = last_28.groupby(['id', 'wday', 'snap_day'])['demand'].mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"by_weekday_snap_90 = last_90.groupby(['id', 'wday', 'snap_day'])['demand'].mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# by_weekday_snap_365 = last_365.groupby(['id', 'wday', 'snap_day'])['demand'].mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prepare a copy of the submission file to merge with the groupby series"},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = get_sub()\nsub.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Join the sub dataframe to a group_by series and create our final dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"# df_final_model_1 = join_sub_groupby(sub, by_weekday)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df_final_model_2 = join_sub_groupby(sub, by_weekday_snap)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_final_model_3 = join_sub_groupby(sub, by_weekday_snap_90)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df_final_model_3_dark_magic = df_final_model_3.copy()\n# df_final_model_3_dark_magic['demand'] = df_final_model_3_dark_magic['demand'] * 1.04","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df_final_model_4 = join_sub_groupby(sub, by_weekday_snap_365)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Make a submission file of the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# make_sub(df_final_model_1, ss, filename='model1sub.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# make_sub(df_final_model_2, ss, filename='model2sub.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"make_sub(df_final_model_3, ss, filename='model3sub.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# make_sub(df_final_model_3_dark_magic, ss, filename='model3dmsub.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# make_sub(df_final_model_4, ss, filename='model4sub.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Now apply more insight to creating a simple model"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns; sns.set()\nimport gc\n\nfrom sklearn import preprocessing\nimport lightgbm as lgb\n\nfrom typing import Union\nfrom tqdm.notebook import tqdm_notebook as tqdm\n\nDATA_DIR = '/kaggle/input/m5-forecasting-accuracy/'\n\nclass WRMSSEEvaluator_dashboard(object):\n\n    def __init__(self, train_df: pd.DataFrame, valid_df: pd.DataFrame, \n                 calendar: pd.DataFrame, prices: pd.DataFrame):\n        train_y = train_df.loc[:, train_df.columns.str.startswith('d_')]\n        train_target_columns = train_y.columns.tolist()\n        weight_columns = train_y.iloc[:, -28:].columns.tolist()\n\n        train_df['all_id'] = 'all'  # for lv1 aggregation\n\n        id_columns = train_df.loc[:, ~train_df.columns.str.startswith('d_')]\\\n                     .columns.tolist()\n        valid_target_columns = valid_df.loc[:, valid_df.columns.str.startswith('d_')]\\\n                               .columns.tolist()\n\n        if not all([c in valid_df.columns for c in id_columns]):\n            valid_df = pd.concat([train_df[id_columns], valid_df], \n                                 axis=1, sort=False)\n\n        self.train_df = train_df\n        self.valid_df = valid_df\n        self.calendar = calendar\n        self.prices = prices\n\n        self.weight_columns = weight_columns\n        self.id_columns = id_columns\n        self.valid_target_columns = valid_target_columns\n\n        weight_df = self.get_weight_df()\n\n        self.group_ids = (\n            'all_id',\n            'state_id',\n            'store_id',\n            'cat_id',\n            'dept_id',\n            ['state_id', 'cat_id'],\n            ['state_id', 'dept_id'],\n            ['store_id', 'cat_id'],\n            ['store_id', 'dept_id'],\n            'item_id',\n            ['item_id', 'state_id'],\n            ['item_id', 'store_id']\n        )\n\n        for i, group_id in enumerate(tqdm(self.group_ids)):\n            train_y = train_df.groupby(group_id)[train_target_columns].sum()\n            scale = []\n            for _, row in train_y.iterrows():\n                series = row.values[np.argmax(row.values != 0):]\n                scale.append(((series[1:] - series[:-1]) ** 2).mean())\n            setattr(self, f'lv{i + 1}_scale', np.array(scale))\n            setattr(self, f'lv{i + 1}_train_df', train_y)\n            setattr(self, f'lv{i + 1}_valid_df', valid_df.groupby(group_id)\\\n                    [valid_target_columns].sum())\n\n            lv_weight = weight_df.groupby(group_id)[weight_columns].sum().sum(axis=1)\n            setattr(self, f'lv{i + 1}_weight', lv_weight / lv_weight.sum())\n\n    def get_weight_df(self) -> pd.DataFrame:\n        day_to_week = self.calendar.set_index('d')['wm_yr_wk'].to_dict()\n        weight_df = self.train_df[['item_id', 'store_id'] + self.weight_columns]\\\n                    .set_index(['item_id', 'store_id'])\n        weight_df = weight_df.stack().reset_index()\\\n                   .rename(columns={'level_2': 'd', 0: 'value'})\n        weight_df['wm_yr_wk'] = weight_df['d'].map(day_to_week)\n\n        weight_df = weight_df.merge(self.prices, how='left',\n                                    on=['item_id', 'store_id', 'wm_yr_wk'])\n        weight_df['value'] = weight_df['value'] * weight_df['sell_price']\n        weight_df = weight_df.set_index(['item_id', 'store_id', 'd'])\\\n                    .unstack(level=2)['value']\\\n                    .loc[zip(self.train_df.item_id, self.train_df.store_id), :]\\\n                    .reset_index(drop=True)\n        weight_df = pd.concat([self.train_df[self.id_columns],\n                               weight_df], axis=1, sort=False)\n        return weight_df\n\n    def rmsse(self, valid_preds: pd.DataFrame, lv: int) -> pd.Series:\n        valid_y = getattr(self, f'lv{lv}_valid_df')\n        score = ((valid_y - valid_preds) ** 2).mean(axis=1)\n        scale = getattr(self, f'lv{lv}_scale')\n        return (score / scale).map(np.sqrt) \n\n    def score(self, valid_preds: Union[pd.DataFrame, \n                                       np.ndarray]) -> float:\n        assert self.valid_df[self.valid_target_columns].shape \\\n               == valid_preds.shape\n\n        if isinstance(valid_preds, np.ndarray):\n            valid_preds = pd.DataFrame(valid_preds, \n                                       columns=self.valid_target_columns)\n\n        valid_preds = pd.concat([self.valid_df[self.id_columns], \n                                 valid_preds], axis=1, sort=False)\n\n        all_scores = []\n        for i, group_id in enumerate(self.group_ids):\n\n            valid_preds_grp = valid_preds.groupby(group_id)[self.valid_target_columns].sum()\n            setattr(self, f'lv{i + 1}_valid_preds', valid_preds_grp)\n            \n            lv_scores = self.rmsse(valid_preds_grp, i + 1)\n            setattr(self, f'lv{i + 1}_scores', lv_scores)\n            \n            weight = getattr(self, f'lv{i + 1}_weight')\n            lv_scores = pd.concat([weight, lv_scores], axis=1, \n                                  sort=False).prod(axis=1)\n            \n            all_scores.append(lv_scores.sum())\n            \n        self.all_scores = all_scores\n\n        return np.mean(all_scores)\n    \n\n    \ndef create_viz_df(df,lv):\n    \n    df = df.T.reset_index()\n    if lv in [6,7,8,9,11,12]:\n        df.columns = [i[0] + '_' + i[1] if i != ('index','') \\\n                      else i[0] for i in df.columns]\n    df = df.merge(calendar.loc[:, ['d','date']], how='left', \n                  left_on='index', right_on='d')\n    df['date'] = pd.to_datetime(df.date)\n    df = df.set_index('date')\n    df = df.drop(['index', 'd'], axis=1)\n    \n    return df\n\ndef create_dashboard(evaluator, by_level_only=False, model_name=None):\n    \n    wrmsses = [np.mean(evaluator.all_scores)] + evaluator.all_scores\n    labels = ['Overall'] + [f'Level {i}' for i in range(1, 13)]\n\n    ## WRMSSE by Level\n    plt.figure(figsize=(12,5))\n    ax = sns.barplot(x=labels, y=wrmsses)\n    ax.set(xlabel='', ylabel='WRMSSE')\n    \n    #######################ALTERATION##########################\n    title = 'WRMSSE by Level'\n    if model_name: \n        title = f'WRMSSE by Level for {model_name}'\n    plt.title(title, fontsize=20, fontweight='bold')\n    #######################ALTERATION-COMPLETE##########################\n\n  \n    for index, val in enumerate(wrmsses):\n        ax.text(index*1, val+.01, round(val,4), color='black', \n                ha=\"center\")\n        \n    #######################ALTERATION##########################\n    if by_level_only:       # stops function early for quick plotting of \n        plt.show()          # for quick plotting of levels\n        return\n    #######################ALTERATION-COMPLETE##########################\n\n    # configuration array for the charts\n    n_rows = [1, 1, 4, 1, 3, 3, 3, 3, 3, 3, 3, 3]\n    n_cols = [1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n    width = [7, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14]\n    height = [4, 3, 12, 3, 9, 9, 9, 9, 9, 9, 9, 9]\n    \n    for i in range(1,13):\n        \n        scores = getattr(evaluator, f'lv{i}_scores')\n        weights = getattr(evaluator, f'lv{i}_weight')\n        \n        if i > 1 and i < 9:\n            if i < 7:\n                fig, axs = plt.subplots(1, 2, figsize=(12, 3))\n            else:\n                fig, axs = plt.subplots(2, 1, figsize=(12, 8))\n                \n            ## RMSSE plot\n            scores.plot.bar(width=.8, ax=axs[0], color='g')\n            axs[0].set_title(f\"RMSSE\", size=14)\n            axs[0].set(xlabel='', ylabel='RMSSE')\n            if i >= 4:\n                axs[0].tick_params(labelsize=8)\n            for index, val in enumerate(scores):\n                axs[0].text(index*1, val+.01, round(val,4), color='black', \n                            ha=\"center\", fontsize=10 if i == 2 else 8)\n            \n            ## Weight plot\n            weights.plot.bar(width=.8, ax=axs[1])\n            axs[1].set_title(f\"Weight\", size=14)\n            axs[1].set(xlabel='', ylabel='Weight')\n            if i >= 4:\n                axs[1].tick_params(labelsize=8)\n            for index, val in enumerate(weights):\n                axs[1].text(index*1, val+.01, round(val,2), color='black', \n                            ha=\"center\", fontsize=10 if i == 2 else 8)\n                    \n            fig.suptitle(f'Level {i}: {evaluator.group_ids[i-1]}', size=24 ,\n                         y=1.1, fontweight='bold')\n            plt.tight_layout()\n            plt.show()\n\n        trn = create_viz_df(getattr(evaluator, f'lv{i}_train_df')\\\n                            .iloc[:, -28*3:], i)\n        val = create_viz_df(getattr(evaluator, f'lv{i}_valid_df'), i)\n        pred = create_viz_df(getattr(evaluator, f'lv{i}_valid_preds'), i)\n\n        n_cate = trn.shape[1] if i < 7 else 9\n\n        fig, axs = plt.subplots(n_rows[i-1], n_cols[i-1], \n                                figsize=(width[i-1],height[i-1]))\n        if i > 1:\n            axs = axs.flatten()\n\n        ## Time series plot\n        for k in range(0, n_cate):\n\n            ax = axs[k] if i > 1 else axs\n\n            trn.iloc[:, k].plot(ax=ax, label='train')\n            val.iloc[:, k].plot(ax=ax, label='valid')\n            pred.iloc[:, k].plot(ax=ax, label='pred')\n            ax.set_title(f\"{trn.columns[k]}  RMSSE:{scores[k]:.4f}\", size=14)\n            ax.set(xlabel='', ylabel='sales')\n            ax.tick_params(labelsize=8)\n            ax.legend(loc='upper left', prop={'size': 10})\n\n        if i == 1 or i >= 9:\n            fig.suptitle(f'Level {i}: {evaluator.group_ids[i-1]}', size=24 , \n                         y=1.1, fontweight='bold')\n        plt.tight_layout()\n        plt.show()\n        \ntrain_df = pd.read_csv('../input/m5-forecasting-accuracy/sales_train_validation.csv')\ncalendar = pd.read_csv('../input/m5-forecasting-accuracy/calendar.csv')\nsell_prices = pd.read_csv('../input/m5-forecasting-accuracy/sell_prices.csv')\ntrain_df = train_df.loc[:, :'d_' + str(1913)]\n\ntrain_fold_df = train_df.iloc[:, :-28]\nvalid_fold_df = train_fold_df.iloc[:, -28:].copy()\n# Instantiate an evaluator for scoring validation periodstarting day 1886\ne = WRMSSEEvaluator_dashboard(train_fold_df, valid_fold_df, calendar, sell_prices)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m = pd.read_csv('model3sub.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = m.iloc[:30490, 1:].values * 1.01","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_ = e.score(preds)\ncreate_dashboard(e, by_level_only=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}