{"cells":[{"metadata":{"_uuid":"9c85c16e-4435-4725-a2ba-85e0c50a8dfc","_cell_guid":"2502457f-e57c-4789-b891-c358dae7b83b","trusted":true},"cell_type":"markdown","source":"## import","execution_count":null},{"metadata":{"_uuid":"a4b37630-0bae-426c-a29c-478bd65b99f9","_cell_guid":"5981560e-1173-46f2-a873-5123d72ee336","trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\nimport pandas as pd\nimport numpy as np\nimport dask.dataframe as dd\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_rows', 500)\nimport matplotlib.pyplot as plt\nimport seaborn as sns; sns.set()\nimport lightgbm as lgb\nimport optuna.integration.lightgbm as lgb_optuna\n#import dask_xgboost as xgb\n#import dask.dataframe as dd6\nfrom sklearn import preprocessing, metrics\nfrom sklearn.preprocessing import LabelEncoder\nimport gc\nimport os\nfrom tqdm import tqdm_notebook as tqdm\nfrom scipy.sparse import csr_matrix\nimport pickle\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"72a2bdc4-9107-4a5d-b195-1d900a75092a","_cell_guid":"6369f9e7-0056-4cf6-9c38-45f8671a890c","trusted":true},"cell_type":"code","source":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns: #columns毎に処理\n        col_type = df[col].dtypes\n        if col_type in numerics: #numericsのデータ型の範囲内のときに処理を実行. データの最大最小値を元にデータ型を効率的なものに変更\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cf956280-e30a-4817-807d-a92b6d545304","_cell_guid":"82d615fb-4477-4d77-b437-d250775e7f00","trusted":true},"cell_type":"markdown","source":"## Use Files","execution_count":null},{"metadata":{"_uuid":"22b8ccdf-fd13-4ee2-a590-fdb956280bc1","_cell_guid":"8c8d5e45-3cad-4f46-ab57-af93bb7bc82f","trusted":true},"cell_type":"code","source":"PATHS = {}\nfor store_id in ['CA_1','CA_2','CA_3','CA_4','TX_1','TX_2','TX_3','WI_1','WI_2','WI_3']:\n    PATHS[store_id] = '/kaggle/input/m5-all-data/df_' + store_id + '.pkl'\n    \nPATHS2 = {}\nPATHS2['CA_1'] = '/kaggle/input/binary-challenge-evaluation-ca-1-2/binary_pred_CA_1.pkl'\nPATHS2['CA_2'] = '/kaggle/input/binary-challenge-evaluation-ca-1-2/binary_pred_CA_2.pkl'\nPATHS2['CA_3'] = '/kaggle/input/binary-challenge-evaluation-ca-3-4/binary_pred_CA_3.pkl'\nPATHS2['CA_4'] = '/kaggle/input/binary-challenge-evaluation-ca-3-4/binary_pred_CA_4.pkl'\nPATHS2['TX_1'] = '/kaggle/input/binary-challenge-evaluation-tx-1-2-3/binary_pred_TX_1.pkl'\nPATHS2['TX_2'] = '/kaggle/input/binary-challenge-evaluation-tx-1-2-3/binary_pred_TX_2.pkl'\nPATHS2['TX_3'] = '/kaggle/input/binary-challenge-evaluation-tx-1-2-3/binary_pred_TX_3.pkl'\nPATHS2['WI_1'] = '/kaggle/input/binary-challenge-evaluation-wi-1-2-3/binary_pred_WI_1.pkl'\nPATHS2['WI_2'] = '/kaggle/input/binary-challenge-evaluation-wi-1-2-3/binary_pred_WI_2.pkl'\nPATHS2['WI_3'] = '/kaggle/input/binary-challenge-evaluation-wi-1-2-3/binary_pred_WI_3.pkl'\n\nPATHS3 = {}\nPATHS3['CA_1'] = '/kaggle/input/m5-lags-features-1to35-ca/lags_df_1to35_CA_1.pkl'\nPATHS3['CA_2'] = '/kaggle/input/m5-lags-features-1to35-ca/lags_df_1to35_CA_2.pkl'\nPATHS3['CA_3'] = '/kaggle/input/m5-lags-features-1to35-ca/lags_df_1to35_CA_3.pkl'\nPATHS3['CA_4'] = '/kaggle/input/m5-lags-features-1to35-ca/lags_df_1to35_CA_4.pkl'\nPATHS3['TX_1'] = '/kaggle/input/fork-of-m5-lags-features-1to35-tx-and-wi/lags_df_1to35_TX_1.pkl'\nPATHS3['TX_2'] = '/kaggle/input/fork-of-m5-lags-features-1to35-tx-and-wi/lags_df_1to35_TX_2.pkl'\nPATHS3['TX_3'] = '/kaggle/input/fork-of-m5-lags-features-1to35-tx-and-wi/lags_df_1to35_TX_3.pkl'\nPATHS3['WI_1'] = '/kaggle/input/fork-of-m5-lags-features-1to35-tx-and-wi/lags_df_1to35_WI_1.pkl'\nPATHS3['WI_2'] = '/kaggle/input/fork-of-m5-lags-features-1to35-tx-and-wi/lags_df_1to35_WI_2.pkl'\nPATHS3['WI_3'] = '/kaggle/input/fork-of-m5-lags-features-1to35-tx-and-wi/lags_df_1to35_WI_3.pkl'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"te2 = pd.read_pickle('../input/m5-target-encoding2/te_60.pkl')\nevent_lag = pd.read_pickle('../input/kernel1f1484cf46/event_lag_df.pkl').drop(columns=['sales', 'event_name_1','event_lag_0'])\n\ndef load_data(store_id):\n    df1 = pd.read_pickle(PATHS[store_id])\n    df2 = pd.read_pickle(PATHS2[store_id])\n    df3 = pd.read_pickle(PATHS3[store_id]).drop(columns=['store_id', 'sales']).iloc[:,:29]\n    df1 = df1.merge(df2,on=['id', 'd'],how='left')\n    df1 = df1.merge(df3,on=['id', 'd'],how='left')\n    df1 = df1.merge(te2,on=['id', 'd'],how='left')\n    df1 = df1.merge(event_lag,on=['id', 'd'],how='left')\n    del df2,df3\n    df1.id = df1.id.astype('category')\n    gc.collect()\n\n    return df1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"740adcda-49e9-43e7-bd60-5a6559016bd9","_cell_guid":"1f039978-013c-4fd6-ae34-c2f9548912b5","trusted":true},"cell_type":"markdown","source":"## Define Features","execution_count":null},{"metadata":{"_uuid":"3598c9f8-ba06-478f-be2c-30188ad84d6f","_cell_guid":"85b60c2c-f7bb-495e-8511-f7ae6830f79e","trusted":true},"cell_type":"code","source":"TARGET = 'sales'\n\nbasic_features = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', \n        'release', 'sell_price', 'price_max', 'price_min', 'price_std',\n       'price_mean', 'price_norm', 'price_nunique', 'item_nunique',\n       'price_momentum', 'price_momentum_m', 'price_momentum_y',\n       'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2',\n       'snap_CA', 'snap_TX', 'snap_WI', 'tm_d', 'tm_w', 'tm_m', 'tm_y',\n       'tm_wm', 'tm_dw', 'tm_w_end']\n\nencoding_features = ['te_id_28', 'te_item_id_28', 'te_dept_id_28', 'te_cat_id_28',\n       'te_store_id_28', 'te_state_id_28', 'te_id_tm_dw_28',\n       'te_item_id_tm_dw_28', 'te_dept_id_tm_dw_28', 'te_cat_id_tm_dw_28',\n       'te_store_id_tm_dw_28', 'te_state_id_tm_dw_28'] + [\n       'te_id_60', 'te_item_id_60',\n       'te_dept_id_60', 'te_cat_id_60', 'te_store_id_60', 'te_state_id_60',\n       'te_id_tm_dw_60', 'te_item_id_tm_dw_60', 'te_dept_id_tm_dw_60',\n       'te_cat_id_tm_dw_60', 'te_store_id_tm_dw_60', 'te_state_id_tm_dw_60']\n\nlag_features = [\n        'sales_lag_28','sales_lag_29', 'sales_lag_30', 'sales_lag_31', 'sales_lag_32',\n        'sales_lag_33', 'sales_lag_34', 'sales_lag_35', 'sales_lag_36','sales_lag_37',\n        'sales_lag_38', 'sales_lag_39', 'sales_lag_40','sales_lag_41', 'sales_lag_42', \n        'rolling_mean_7', 'rolling_std_7','rolling_mean_14', 'rolling_std_14',\n        'rolling_mean_30','rolling_std_30', 'rolling_mean_60', 'rolling_std_60',\n        'rolling_mean_180', 'rolling_std_180',]\n\nday_by_day = ['sales_lag_1', 'sales_lag_2', 'sales_lag_3', 'sales_lag_4',\n       'sales_lag_5', 'sales_lag_6', 'sales_lag_7', 'sales_lag_8',\n       'sales_lag_9', 'sales_lag_10', 'sales_lag_11', 'sales_lag_12',\n       'sales_lag_13', 'sales_lag_14', 'sales_lag_15', 'sales_lag_16',\n       'sales_lag_17', 'sales_lag_18', 'sales_lag_19', 'sales_lag_20',\n       'sales_lag_21', 'sales_lag_22', 'sales_lag_23', 'sales_lag_24',\n       'sales_lag_25', 'sales_lag_26', 'sales_lag_27']\n\nrecursive_features =['rolling_mean_tmp_1_7','rolling_mean_tmp_1_14',\n    'rolling_mean_tmp_1_30','rolling_mean_tmp_1_60',\n    'rolling_mean_tmp_7_7','rolling_mean_tmp_7_14',\n    'rolling_mean_tmp_7_30','rolling_mean_tmp_7_60',\n    'rolling_mean_tmp_14_7','rolling_mean_tmp_14_14',\n    'rolling_mean_tmp_14_30','rolling_mean_tmp_14_60']\n\n\t\n\nadditional_features = ['binary_pred'] + ['event_lag_1','event_lag_2','event_lag_3','event_lag_4','event_lag_5','event_lag_6',\n     'event_lag_-7','event_lag_-6','event_lag_-5','event_lag_-4','event_lag_-3','event_lag_-2','event_lag_-1'] + day_by_day\n\nremove_features = ['store_id', 'state_id', \n                   'te_store_id_28', 'te_state_id_28', 'te_store_id_tm_dw_28', 'te_state_id_tm_dw_28',\n                   'te_store_id_60', 'te_state_id_60', 'te_store_id_tm_dw_60', 'te_state_id_tm_dw_60',\n                   'snap_CA', 'snap_TX', 'snap_WI']\n\nuse_enc_feat = True\nuse_lag_feat = True\nuse_rec_feat = False\nuse_add_feat = True\n\nfeature = basic_features\nif use_enc_feat:\n    feature += encoding_features\nif use_lag_feat:\n    feature += lag_features\nif use_rec_feat:\n    feature += recursive_features\nif use_add_feat:\n    feature += additional_features\n    \nfeature = [i for i in feature if i not in remove_features]\n    \nlen(feature)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6f5dda14-04ea-4801-95d2-766d112d23a2","_cell_guid":"9863ff87-ef18-45d2-b984-e8bb98126694","trusted":true},"cell_type":"markdown","source":"## Model","execution_count":null},{"metadata":{"_uuid":"1fb39bea-cc32-437d-bb64-656b5200d259","_cell_guid":"5ed506ac-f878-4054-a37d-fa7266eaecf1","trusted":true},"cell_type":"code","source":"# define lgbm simple model using custom loss and eval metric for early stopping\ndef run_lgb(train, val,test, features, custom_loss, custom_eval, optuna_params={}, use_custom=True):\n\n    train_set = lgb.Dataset(train[features], train[TARGET].values, free_raw_data=False, params={'data_type':'train'})\n    del train\n    gc.collect()\n    \n    val_set = lgb.Dataset(val[features], val[TARGET].values, free_raw_data=False, params={'data_type':'validation'})\n    del val\n    gc.collect()\n    \n    test_set = lgb.Dataset(test[features], test[TARGET].values, free_raw_data=False, params={'data_type':'test'})\n    del test\n    gc.collect()\n\n    if use_custom:\n        params = {\n            'boosting_type': 'gbdt',\n            'first_metric_only': True,\n            'objective': 'custom',\n            'metric': 'custom',\n#             'n_jobs': -1,\n#             'seed': 42,\n#             'lambda_l1': 0.011947955379673579,\n#             'lambda_l2': 0.0002267728823544454,\n#             'num_leaves': 31,\n#             'feature_fraction': 0.45199999999999996,\n#              'bagging_fraction': 1.0,\n#              'bagging_freq': 0,\n#              'min_child_samples': 50,\n#             'learning_rate': 0.1,\n#             'n_estimators': 20,\n#             'bagging_fraction': 0.75,\n#             'bagging_freq': 10, \n#             'colsample_bytree': 0.75\n        }\n        params.update(optuna_params)\n        \n        model = lgb.train(params, train_set, num_boost_round = 1500, early_stopping_rounds = 100, \n                          valid_sets = [train_set, val_set, test_set], valid_names=['Train','Val','Test'], \n                          verbose_eval = 10, fobj = custom_loss, feval = custom_eval)\n    else:\n        params = {\n        'boosting_type': 'gbdt',\n        'objective': 'tweedie',\n        'first_metric_only': True,\n        'tweedie_variance_power': 1.1,\n        'metric': 'custom',\n        'n_jobs': -1,\n        'seed': 42,\n        'learning_rate': 0.1,\n        'bagging_fraction': 0.75,\n        'bagging_freq': 10, \n        'colsample_bytree': 0.75}\n        params.update(optuna_params)\n\n        model = lgb.train(params, train_set, num_boost_round = 1500, early_stopping_rounds = 100, \n                          valid_sets = [train_set, val_set, test_set], valid_names=['Train','Val','Test'],\n                          verbose_eval = 10,feval = custom_eval)\n    return model\n\n# define lgbm simple model using custom loss and eval metric for early stopping\ndef run_lgb_no_early_stopping(train, features, custom_loss, custom_eval, optuna_params={}, use_custom=True, num_boost_round = 200):\n\n    train_set = lgb.Dataset(train[features], train[TARGET].values, free_raw_data=False, params={'data_type':'train'})\n    del train\n    gc.collect()\n\n    if use_custom:\n        params = {\n            'boosting_type': 'gbdt',\n            'first_metric_only': True,\n            'objective': 'custom',\n            'metric': 'rmse',\n#             'n_jobs': -1,\n#             'seed': 42,\n#             'learning_rate': 0.1,\n# #             'n_estimators': 20,\n#             'bagging_fraction': 0.75,\n#             'bagging_freq': 10, \n#             'colsample_bytree': 0.75\n        }\n        params.update(optuna_params)\n        \n        model = lgb.train(params, train_set, valid_sets = [train_set], num_boost_round = num_boost_round, verbose_eval = 10, fobj = custom_loss)\n    else:\n        params = {\n        'boosting_type': 'gbdt',\n        'objective': 'tweedie',\n        'first_metric_only': True,\n        'tweedie_variance_power': 1.1,\n        'metric': 'custom',\n        'n_jobs': -1,\n        'seed': 42,\n        'learning_rate': 0.1,\n        'bagging_fraction': 0.75,\n        'bagging_freq': 10, \n        'colsample_bytree': 0.75}\n        params.update(optuna_params)\n\n        model = lgb.train(params, train_set, num_boost_round = num_boost_round, early_stopping_rounds = 100, \n                          valid_sets = [train_set, val_set, test_set], valid_names=['Train','Val','Test'],\n                          verbose_eval = 10,feval = custom_eval)\n    return model\n\n# define lgbm simple model using custom loss and eval metric for early stopping\ndef run_lgb_optuna(train, val,test, features, custom_loss, custom_eval, use_custom=True):\n\n    train_set = lgb.Dataset(train[features], train[TARGET].values, free_raw_data=False, params={'data_type':'train'})\n    del train\n    gc.collect()\n    \n    val_set = lgb.Dataset(val[features], val[TARGET].values, free_raw_data=False, params={'data_type':'validation'})\n    del val\n    gc.collect()\n    \n    test_set = lgb.Dataset(test[features], test[TARGET].values, free_raw_data=False, params={'data_type':'test'})\n    del test\n    gc.collect()\n    \n    best_params, history = {}, []\n\n    if use_custom:\n        params = {\n            'boosting_type': 'gbdt',\n            'first_metric_only': True,\n            'objective': 'custom',\n            'metric': 'wrmsse',\n            }\n        model = lgb_optuna.train(params, train_set, num_boost_round = 1500, early_stopping_rounds = 100, \n                          valid_sets = [train_set, val_set, test_set], valid_names=['Train','Val','Test'], \n                          verbose_eval = 10, fobj = custom_loss, feval = custom_eval,best_params=best_params,tuning_history=history)\n    else:\n        params = {\n        'boosting_type': 'gbdt',\n        'objective': 'tweedie',\n        'first_metric_only': True,\n        'tweedie_variance_power': 1.1,\n        'metric': 'custom'\n        }\n        \n        model = lgb_optuna.train(params, train_set, num_boost_round = 1500, early_stopping_rounds = 100, \n                          valid_sets = [train_set, val_set, test_set], valid_names=['Train','Val','Test'],\n                          verbose_eval = 10,feval = custom_eval,best_params=best_params,tuning_history=history)\n    return model, best_params, history","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"21935284-3bc8-46d5-a352-e539c5f8518b","_cell_guid":"1681ca21-9798-4e5e-9383-2b967d7260d7","trusted":true},"cell_type":"markdown","source":"## Loss Function","execution_count":null},{"metadata":{"_uuid":"82982217-f835-4ff2-80dc-d6fdf4d55704","_cell_guid":"07f0a665-f4b8-4cb4-a8a3-11b210283fe6","trusted":true},"cell_type":"code","source":"# define cost and eval functions\ndef custom_asymmetric_train(y_pred, y_true):\n    y_true = y_true.get_label()\n    a = 1.15\n    b = 1\n    residual = (y_true - y_pred).astype(\"float\")\n    grad = np.where(residual < 0, -2 * residual * b, -2 * residual * a)\n    hess = np.where(residual < 0, 2 * b, 2 * a)\n    return grad, hess\n\ndef tweedie(y_pred, y_true):\n    p = 1.1\n    y_true = y_true.get_label()\n    grad = -y_true*y_pred**(-p) + y_pred**(1-p)\n    hess = p*y_true*y_pred**(-p-1)-(1-p)*y_pred**(-p)\n    return grad, hess\n\ndef tweedie2(y_pred, y_true):\n    p = 1.09\n    y_true = y_true.get_label()\n    grad = -y_true*np.exp(y_pred*(1-p))+np.exp(y_pred*(2-p))\n    hess = -(1-p)*y_true*np.exp(y_pred*(1-p))+(2-p)*np.exp(y_pred*(2-p))\n    return grad, hess\n\ndef tweedie3(y_pred, y_true):\n    p = y_true.get_data()['p'].values\n    print(p)\n    y_true = y_true.get_label()\n    print(y_true)\n    print(y_pred)\n    grad = np.where(p<1,-y_true + np.exp(y_pred),-y_true*np.exp(y_pred*(1-p))+np.exp(y_pred*(2-p)))\n    grad = np.where(p>1.5,-y_true*np.exp(y_pred*(1-1.5))+np.exp(y_pred*(2-1.5)),grad)\n    hess = np.where(p<1,np.exp(y_pred),-(1-p)*y_true*np.exp(y_pred*(1-p))+(2-p)*np.exp(y_pred*(2-p)))\n    hess = np.where(p>1.5,-y_true*np.exp(y_pred*(1-1.5))+np.exp(y_pred*(2-1.5)),hess)\n    print(grad,hess)\n    return grad, hess\n\ndef tweedie_sum(y_pred, y_true):\n    p1 = 1.1\n    p2 = 1.5\n    y_true = y_true.get_label()\n    grad = -y_true*np.exp(y_pred*(1-p1))+np.exp(y_pred*(2-p1))\n    grad += -y_true*np.exp(y_pred*(1-p2))+np.exp(y_pred*(2-p2))\n    hess = -(1-p1)*y_true*np.exp(y_pred*(1-p1))+(2-p1)*np.exp(y_pred*(2-p1))\n    hess += -(1-p2)*y_true*np.exp(y_pred*(1-p2))+(2-p2)*np.exp(y_pred*(2-p2))\n#     print(grad,hess)\n    return grad, hess\n\n# define cost and eval functions\ndef custom_asymmetric_train_2(y_pred, y_true):\n    y_true = y_true.get_label()\n    residual = (y_true - y_pred).astype(\"float\")\n    grad = np.where(residual < 0, -2 * residual, -2 * residual * 1.15)\n    grad = np.where(y_true == 0, (1-0.5)*np.exp(y_pred), grad)\n    hess = np.where(residual < 0, 2, 2 * 1.15)\n    hess = np.where(y_true == 0, (1-0.5)*np.exp(y_pred), hess)\n\n    return grad, hess\n\ndef zero_inflated_poisson_loss(y_pred, y_true):\n    p = 0.3\n    y_true = y_true.get_label()\n    y_pred = np.exp(y_pred)\n    grad = np.where(y_true == 0, (1-p)*y_pred,-y_true + y_pred)\n    hess = np.where(y_true == 0, (1-p)*y_pred, y_pred)\n    return grad, hess\n\ndef poisson(y_pred, y_true):\n    y_true = y_true.get_label()\n    y_pred = np.exp(y_pred)\n    grad = -y_true + y_pred\n    hess = y_pred\n    return grad, hess\n\ndef custom_a(y_pred, y_true):\n    y_true = y_true.get_label()\n    d = y_pred - y_true \n    grad = np.tanh(d)/y_true\n    hess = (1.0 - grad*grad)/y_true\n    return grad, hess\n\ndef my_objective(y_pred, y_true):\n    y_true = y_true.get_label()\n    d = y_pred - y_true \n    grad = 2*d\n    hess = 2*d/d\n    return grad, hess\n\ndef loglikelood(preds, train_data):\n    labels = train_data.get_label()\n    preds = 1. / (1. + np.exp(-preds))\n    grad = preds - labels\n    hess = preds * (1. - preds)\n    return grad, hess\n\n\ndef asymmetric_plus_tweedie(y_pred, y_true):\n    p = 1.06\n    a = 1.2\n    b = 1\n    y_true = y_true.get_label()\n    residual = (y_true - y_pred).astype(\"float\")\n    y_pred = np.exp(y_pred)\n    grad = -y_true*y_pred**(1-p)+y_pred**(2-p)\n    grad += np.where(residual < 0, -2 * (y_true-y_pred)*y_pred * b, -2 * (y_true-y_pred)*y_pred * a)\n    hess = -(1-p)*y_true*y_pred**(1-p)+(2-p)*y_pred**(2-p)\n    hess += np.where(residual < 0, -2 * (y_true-2*y_pred)*y_pred * b, -2 * (y_true-2*y_pred)*y_pred * a)\n    return grad, hess\n\n\ndef asymmetric_tweedie(y_pred, y_true):\n    p = 1.0\n    a = 1.2\n    b = 1\n    y_true = y_true.get_label()\n    residual = (y_true - y_pred).astype(\"float\")\n    y_pred = np.exp(y_pred)\n    grad = np.where(residual < 0,(-y_true*y_pred**(1-p)+y_pred**(2-p)) * b,(-y_true*y_pred**(1-p)+y_pred**(2-p)) * a)\n    hess = np.where(residual < 0,(-(1-p)*y_true*y_pred**(1-p)+(2-p)*y_pred**(2-p))*b,(-(1-p)*y_true*y_pred**(1-p)+(2-p)*y_pred**(2-p))*a)\n    return grad, hess","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6f923b89-c377-405f-858f-5ff40bd494b2","_cell_guid":"3d4acf5d-af6e-43f9-9bb1-4178b819fccf","trusted":true},"cell_type":"code","source":"def link_exp(pred):\n    return np.exp(pred)\ndef link_normal(pred):\n    return pred\nlink_func_dict = {'tweedie2':link_exp,'custom_asymmetric_train':link_normal,'asymmetric_plus_tweedie':link_exp,'asymmetric_tweedie':link_exp}\nloss_func_dict = {'tweedie2':tweedie2,'custom_asymmetric_train':custom_asymmetric_train,'asymmetric_plus_tweedie':asymmetric_plus_tweedie,'asymmetric_tweedie':asymmetric_tweedie}\nLOSS_NAME = 'tweedie2'\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2cb971c9-6467-4df0-a9f9-ae6e3bfa97d6","_cell_guid":"9a6aa270-c081-4643-b713-7c430db54f7a","trusted":true},"cell_type":"markdown","source":"## Evaluation Function","execution_count":null},{"metadata":{"_uuid":"4783e392-4619-4648-b22d-f8a028d9f22e","_cell_guid":"5b206bad-18d3-471d-a119-c5a85cfbb3cd","trusted":true},"cell_type":"code","source":"def get_weight_mat(product):\n    NUM_ITEMS = len(product['id'].unique()) \n    weight_mat = np.c_[np.ones([NUM_ITEMS,1]).astype(np.int8), # level 1\n                       pd.get_dummies(product.state_id.astype(str),drop_first=False).astype('int8').values,\n                       pd.get_dummies(product.store_id.astype(str),drop_first=False).astype('int8').values,\n                       pd.get_dummies(product.cat_id.astype(str),drop_first=False).astype('int8').values,\n                       pd.get_dummies(product.dept_id.astype(str),drop_first=False).astype('int8').values,\n                       pd.get_dummies(product.state_id.astype(str) + product.cat_id.astype(str),drop_first=False).astype('int8').values,\n                       pd.get_dummies(product.state_id.astype(str) + product.dept_id.astype(str),drop_first=False).astype('int8').values,\n                       pd.get_dummies(product.store_id.astype(str) + product.cat_id.astype(str),drop_first=False).astype('int8').values,\n                       pd.get_dummies(product.store_id.astype(str) + product.dept_id.astype(str),drop_first=False).astype('int8').values,\n                       pd.get_dummies(product.item_id.astype(str),drop_first=False).astype('int8').values,\n                       pd.get_dummies(product.state_id.astype(str) + product.item_id.astype(str),drop_first=False).astype('int8').values,\n                       np.identity(NUM_ITEMS).astype(np.int8) #item :level 12\n                       ].T\n\n    weight_mat_csr = csr_matrix(weight_mat)\n    del weight_mat; gc.collect()\n    return weight_mat_csr\n\ndef weight1_calc(product,weight_mat_csr):\n    # calculate the denominator of RMSSE, and calculate the weight base on sales amount\n    sales_train_val = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sales_train_evaluation.csv')\n    sales_train_val = sales_train_val.set_index('id')\n    sales_train_val = sales_train_val.loc[product.id]\n    d_name = ['d_' + str(i+1) for i in range(1941)]\n    sales_train_val = weight_mat_csr * sales_train_val[d_name].values\n    # calculate the start position(first non-zero demand observed date) for each item / 商品の最初の売上日\n    # 1-1914のdayの数列のうち, 売上が存在しない日を一旦0にし、0を9999に置換。そのうえでminimum numberを計算\n    df_tmp = ((sales_train_val>0) * np.tile(np.arange(1,1942),(weight_mat_csr.shape[0],1)))\n    start_no = np.min(np.where(df_tmp==0,9999,df_tmp),axis=1)-1\n    flag = np.dot(np.diag(1/(start_no+1)) , np.tile(np.arange(1,1942),(weight_mat_csr.shape[0],1)))<1\n    sales_train_val = np.where(flag,np.nan,sales_train_val)\n    # denominator of RMSSE / RMSSEの分母\n    weight1 = np.nansum(np.diff(sales_train_val,axis=1)**2,axis=1)/(1941-start_no)\n\n    del sales_train_val\n    gc.collect()\n    \n    return weight1\n\ndef weight2_calc(data_v,product,weight_mat_csr):\n    # calculate the sales amount for each item/level\n    df_tmp = data_v[['id','sales','sell_price']]\n    df_tmp['amount'] = df_tmp['sales'] * df_tmp['sell_price']\n    df_tmp =df_tmp.groupby(['id'])['amount'].apply(np.sum)\n    df_tmp = df_tmp[product.id].values\n    weight2 = weight_mat_csr * df_tmp \n    weight2 = weight2/np.sum(weight2)\n    \n    return  weight2\n\ndef weight2_calc_for_train(data_v,product,weight_mat_csr):\n    # calculate the sales amount for each item/level\n    df_tmp = data_v[['id','sales','sell_price']]\n    df_tmp['amount'] = df_tmp['sales'] * df_tmp['sell_price']\n    df_tmp.amount = df_tmp.amount.astype(int)\n    df_tmp =df_tmp.groupby(['id'])['amount'].apply(np.sum)\n    df_tmp = df_tmp[product.id].values\n    weight2 = weight_mat_csr * df_tmp \n    weight2 = weight2/np.sum(weight2)\n    \n    return  weight2\n\ndef wrmsse(preds, data):\n    \n    # this function is calculate for last 28 days to consider the non-zero demand period\n    \n    y_true = data.get_label()\n    if use_custom_loss:\n        preds = link_func_dict[LOSS_NAME](preds)\n    \n    if data.params['data_type']=='train':\n        return 'wrmsse', 1371, False\n        weight2 = weight2_tr\n        MASK = np.isin(np.arange(train_width_date*NUM_ITEMS),lost_days)\n\n        pred_tmp = np.arange(train_width_date*NUM_ITEMS)\n        pred_tmp[MASK] = 0\n        pred_tmp[~MASK] = preds\n        preds = pred_tmp\n        \n        y_true_tmp = np.arange(train_width_date*NUM_ITEMS)\n        y_true_tmp[MASK] = 0\n        y_true_tmp[~MASK] = y_true\n        y_true = y_true_tmp\n        \n    elif data.params['data_type']=='validation':\n        weight2 = weight2_val\n    elif data.params['data_type']=='test':\n        weight2 = weight2_te\n\n    reshaped_preds = preds.reshape([-1, NUM_ITEMS]).T\n    reshaped_true = y_true.reshape([-1, NUM_ITEMS]).T    \n          \n    train = weight_mat_csr*(reshaped_preds - reshaped_true)\n    score = np.sum(\n                np.sqrt(\n                    np.mean(\n                        np.square(train)\n                        ,axis=1) / weight1) * weight2)\n#     print(np.mean(np.sqrt(np.mean(np.square(reshaped_preds-reshaped_true),axis=1))))\n    return 'wrmsse', score, False\n\ndef wrmsse_sub(preds, y_true):\n              \n    reshaped_preds = preds.reshape([-1, NUM_ITEMS]).T\n    reshaped_true = y_true.reshape([-1, NUM_ITEMS]).T    \n    if use_custom_loss:\n        preds = link_func_dict[LOSS_NAME](preds)\n        \n    train = weight_mat_csr*(reshaped_preds - reshaped_true)\n\n    score = np.sum(\n                np.sqrt(\n                    np.mean(\n                        np.square(train)\n                        ,axis=1) / weight1) * weight2)\n\n    return 'wrmsse', score, False","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c24c3fe9-90cd-473b-a84b-660c4d70b376","_cell_guid":"eec56f51-0c11-4e75-b055-9a6ec05f7c77","trusted":true},"cell_type":"code","source":"def rmse(preds, data):\n    y_true = data.get_label()\n    if use_custom_loss:\n        preds = link_func_dict[LOSS_NAME](preds)\n                  \n    train = preds - y_true\n    \n    score = np.mean(np.sqrt(np.mean(np.square(train))))\n    \n    return 'rmse', score, False","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a1a0b005-cb55-4077-ade0-4a0987a19a6f","_cell_guid":"8995a384-eb12-4bf3-baa3-9edb7176cfb9","trusted":true},"cell_type":"code","source":"def metrics(preds, data):\n    \"\"\"複数の評価指標を計算するための関数\"\"\"\n    return [\n        wrmsse(preds, data),\n        rmse(preds, data)\n    ]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3f5316ba-2ea7-45f3-b57e-26e9d95d1997","_cell_guid":"8a40be0b-8ade-43f4-8baa-0aed705ab0f7","trusted":true},"cell_type":"markdown","source":"## Preparation","execution_count":null},{"metadata":{"_uuid":"42117479-9ac8-4775-878c-2e51a52821ff","_cell_guid":"8de59f05-d823-4f0b-88dc-158fedf9c008","trusted":true},"cell_type":"markdown","source":"### Training Parameters","execution_count":null},{"metadata":{"_uuid":"9a8eed6f-55bc-4535-b801-ecdb30cbd328","_cell_guid":"f9764bf9-8aee-45de-8ed3-836405a6ca00","trusted":true},"cell_type":"code","source":"use_custom_loss = True\n\ntest_start_date = 1941 - 28\ntest_end_date = 1941\n\ntrain_width_date = 365 * 5\nval_width_date = 28\nshift_width_date = 28\nmin_train_date = 0\n\nslide_list = []\nfor i in range(test_start_date-1,1,-shift_width_date):\n    end_date = i\n    split_date = end_date - val_width_date\n    start_date = split_date - train_width_date\n    if start_date < min_train_date:\n        break\n    slide_list.append([start_date,split_date,end_date])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fcd356a7-80af-48e4-afc0-35f4fd47f97c","_cell_guid":"dab738c3-af32-4572-9944-18b611858a5d","trusted":true},"cell_type":"code","source":"# 5 times validation for fast notebook\nslide_list = slide_list[:5]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"265df82c-c9b7-45e3-b1f6-1d13ceff3ab1","_cell_guid":"749e2334-347b-4b62-8b82-36862c8e6109","trusted":true},"cell_type":"code","source":"slide_list","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b2a82350-aefb-4869-acaf-c31b87ae12ac","_cell_guid":"0e66cd79-6e69-470e-9673-5a27ee482c5d","trusted":true},"cell_type":"code","source":"%%time\n\nproduct = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sales_train_evaluation.csv')\nproduct = product[['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']].drop_duplicates()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f9d9f31-9393-4096-bcf2-aae435003546","_cell_guid":"7b46f5a1-a754-4e40-aa7f-c7dad297e9e1","trusted":true},"cell_type":"markdown","source":"## Make Parameter Grid","execution_count":null},{"metadata":{"_uuid":"df96d529-ea51-462f-9832-c5e740221bd1","_cell_guid":"b0d3b20d-b87d-4e55-8214-d0d60ecf07c9","trusted":true},"cell_type":"markdown","source":"## Training","execution_count":null},{"metadata":{"_uuid":"9b03d25f-6341-42ee-b59d-7bc3f51752f0","_cell_guid":"331d439b-9121-4830-abc2-105ff30bde54","trusted":true},"cell_type":"code","source":"STORE_IDS = list(product.store_id.unique())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9693b9fd-2eaf-4ffd-b16e-dbdd6cd7fc13","_cell_guid":"51c948b4-42a4-4811-a9fa-ff7fbf2f5ac7","trusted":true},"cell_type":"code","source":"# store_id = 'CA_1'\n# start_date, split_date, end_date = slide_list[0] \n# print('start_date, split_date, end_date:',start_date, split_date, end_date)\n# print('store_id:',store_id)\n# print('load dataset')\n# df = load_data(store_id)\n# day_mask = (df.d>=start_date)&(df.d<split_date)\n# train = df[day_mask]\n# train_ids = train.id.unique()\n# NUM_ITEMS = len(train_ids)\n\n# day_mask = (df.d>=split_date)&(df.d<=end_date)\n# val = df[day_mask]\n# val = val[val.id.isin(train_ids)]\n# day_mask = (df.d>=test_start_date)&(df.d<=test_end_date)\n# test = df[day_mask]\n# del df\n# gc.collect()\n\n# test_tmp = test[test.id.isin(train_ids)]\n# product_tmp = product[product.id.isin(train_ids)]\n\n# print('calc weight')\n# weight_mat_csr = get_weight_mat(product_tmp)\n# weight1 = weight1_calc(product_tmp,weight_mat_csr)\n# weight2_te = weight2_calc(test_tmp,product_tmp,weight_mat_csr)\n# weight2_val = weight2_calc(val,product_tmp,weight_mat_csr)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n\"lambda_l1\":0.0000013771438547444856,\n\"lambda_l2\":1.8704380943506742,\n\"num_leaves\":27,\n\"feature_fraction\":0.8,\n\"bagging_fraction\":0.8154703815794264,\n\"bagging_freq\":1,\n\"min_child_samples\":20\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for PREDICT_DAY in range(1,29): \n# #        tmp_lag = day_by_day_lag_features[PREDICT_DAY-1:]\n#     remove_lag = []\n#     for i in range(1, PREDICT_DAY):\n#         remove_lag.append('sales_lag_{}'.format(i))\n#     new_features = list(set(feature) - set(remove_lag))\n#     print('train model; day', PREDICT_DAY)\n#     print(remove_lag)\n#     model = run_lgb(train,val,test_tmp, new_features + ['snap_' + store_id.split('_')[0]], loss_func_dict[LOSS_NAME], wrmsse, optuna_params=params)\n#    # model = run_lgb_no_early_stopping(train, new_features, loss_func_dict[LOSS_NAME], wrmsse, num_boost_round = num_boost_rounds[store_id])\n#     model_name = 'lgb_model_'+store_id+'_'+str(PREDICT_DAY)+'.bin'\n#     pickle.dump(model, open(model_name, 'wb'))\n#     del model\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"82f3ecd0-2777-4275-ba82-a4ca486fecd3","_cell_guid":"ef328e02-ffab-4a5a-850e-ab840fa8154a","trusted":true},"cell_type":"code","source":"# num_boost_rounds = {\n#     'CA_1': 250,\n#     'CA_2': 50,\n#     'CA_3': 50,\n#     'CA_4': 100,\n#     'TX_1': 250,\n#     'TX_2': 50,    \n#     'TX_3': 50,\n#     'WI_1': 75,\n#     'WI_2': 75,\n#     'WI_3': 100,\n# }","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e3848954-ef7f-4953-bd88-0fac6f81f220","_cell_guid":"74f33800-7fe1-4a03-9183-1a63cedb6ccc","trusted":true},"cell_type":"code","source":"store_id = 'CA_1'\nsub_start_date = 1942\nsub_end_date = 1969\nprint('store_id:',store_id)\nprint('load dataset')\ndf = load_data(store_id)\nday_mask = (df.d<sub_start_date)\ntrain = df[day_mask]\ntrain_ids = train.id.unique()\nNUM_ITEMS = len(train_ids)\ngc.collect()\nday_mask = (te2.d>=sub_start_date)&(te2.d<=sub_end_date)\nall_pred = te2[day_mask][['id','d']]\nall_pred['sales'] = 0\n\nfor PREDICT_DAY in range(1,29): \n#        tmp_lag = day_by_day_lag_features[PREDICT_DAY-1:]\n    remove_lag = []\n    for i in range(1, PREDICT_DAY):\n        remove_lag.append('sales_lag_{}'.format(i))\n    new_features = list(set(feature) - set(remove_lag))\n    print('train model; day', PREDICT_DAY)\n#     model = run_lgb(train,val,test_tmp, new_features, loss_func_dict[LOSS_NAME], wrmsse)\n    model = run_lgb_no_early_stopping(train, new_features + ['snap_' + store_id.split('_')[0]], loss_func_dict[LOSS_NAME], wrmsse, optuna_params=params, num_boost_round = 300)\n    day_mask = df.d == sub_start_date + PREDICT_DAY - 1\n    pred1 = model.predict(df[day_mask][new_features + ['snap_' + store_id.split('_')[0]]], num_iteration=100)\n    pred2 = model.predict(df[day_mask][new_features + ['snap_' + store_id.split('_')[0]]], num_iteration=200)\n    pred3 = model.predict(df[day_mask][new_features + ['snap_' + store_id.split('_')[0]]], num_iteration=300)\n    pred = (pred1 + pred2 + pred3)/3.0\n    pred = link_func_dict[LOSS_NAME](pred)\n    day_mask2 = all_pred.d == sub_start_date + PREDICT_DAY - 1\n    all_pred.loc[(all_pred.id.isin(train_ids))&(day_mask2),'sales'] += pred\n\n    model_name = 'lgb_model_'+store_id+'_'+str(PREDICT_DAY)+'.bin'\n    pickle.dump(model, open(model_name, 'wb'))\n    del model","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8078d7ce-3400-4ff7-b05f-7faea1dbfd76","_cell_guid":"1db209c6-3495-40a1-99d8-269cbc46b1d3","trusted":true},"cell_type":"markdown","source":"## Submission","execution_count":null},{"metadata":{"_uuid":"fb8f1b4a-8c81-4d4d-8dcb-90cc0e998a50","_cell_guid":"ad0a6a8f-9188-4c77-9059-82c3ebf7f8c1","trusted":true},"cell_type":"code","source":"all_pred","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eb023700-e98a-48ae-91a4-8cda446227b0","_cell_guid":"2ebd6877-8f97-4ea2-bf06-a930539df02a","trusted":true},"cell_type":"code","source":"sub = all_pred[['d','sales','id']].pivot(index='id', columns='d', values='sales').reset_index()\nsub.columns = ['id'] + ['F'+str(i) for i in range(1,29)]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"139fe940-81a8-4456-b085-13ef062958fa","_cell_guid":"f63e3394-d64c-4fda-b49e-2bcc8245ef1b","trusted":true},"cell_type":"code","source":"submission = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sample_submission.csv')[['id']]\nsubmission = submission.merge(sub, on=['id'], how='left').fillna(0)\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"86957d19-2258-4c52-be0c-604c53d04c48","_cell_guid":"4c25abf4-11ae-45fa-8267-819374078a26","trusted":true},"cell_type":"code","source":"submission[submission.F1!=0]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"39c235f1-65aa-446a-a21c-d899169edb42","_cell_guid":"f8dcc92a-540e-49f3-8f27-21bbb567ad88","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}