{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Note: This notebook has to have \"Internet\" enabled to install some packages","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Een aantal waarden om te onthouden:\n\naantal dagen aan data: 1914\n<br>aantal unieke producten: 30490\n<br>aantal unieke winkels: 10        (4 CA, 3 TX, 3 WIS)\n<br>aantal categoriÃ«n production: 3\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import all the things\n\nimport os\nimport sys\nimport gc\nimport warnings\n\nimport pandas as pd\nfrom pandas.plotting import register_matplotlib_converters\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import LabelEncoder\n\nwarnings.filterwarnings(\"ignore\")\npd.set_option(\"display.max_columns\", 500)\npd.set_option(\"display.max_rows\", 500)\nregister_matplotlib_converters()\nsns.set()\n\nimport IPython\n\n#Functie voor the correct display of dataframes in IPython\ndef display(*dfs, head=True):\n    for df in dfs:\n        IPython.display.display(df.head() if head else df)","execution_count":null,"outputs":[]},{"metadata":{"title":"[code]","trusted":true},"cell_type":"code","source":"def on_kaggle():\n    return \"KAGGLE_KERNEL_RUN_TYPE\" in os.environ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if on_kaggle():\n    os.system(\"pip install --quiet mlflow_extend\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#This function reduces memory (RAM) usage by downcasting variables to smaller datatypes wherever doing so would not result in loss of accuracy.\n#Sourced from https://www.kaggle.com/c/m5-forecasting-accuracy/discussion/136993\n\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024 ** 2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024 ** 2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define a function to load all input data into dataframes\n\ndef read_data():\n    INPUT_DIR = \"/kaggle/input\" if on_kaggle() else \"input\"\n    INPUT_DIR = f\"{INPUT_DIR}/m5-forecasting-accuracy\"\n\n    print(\"Reading files...\")\n\n    calendar = pd.read_csv(f\"{INPUT_DIR}/calendar.csv\").pipe(reduce_mem_usage)    # Calendar data\n    \n    prices = pd.read_csv(f\"{INPUT_DIR}/sell_prices.csv\").pipe(reduce_mem_usage)   # Sales prices over time for each product. Prices are given as weekly averages.\n\n    \n    sales = pd.read_csv(f\"{INPUT_DIR}/sales_train_validation.csv\",).pipe(         # Sales data per product. Sales data is for each day is placed in a separate column.\n        reduce_mem_usage)\n    submission = pd.read_csv(f\"{INPUT_DIR}/sample_submission.csv\").pipe(          # Submission sample format\n        reduce_mem_usage)\n\n    print(\"sales shape:\", sales.shape)\n    print(\"prices shape:\", prices.shape)\n    print(\"calendar shape:\", calendar.shape)\n    print(\"submission shape:\", submission.shape)\n\n    # calendar shape: (1969, 14)\n    # sell_prices shape: (6841121, 4)\n    # sales_train_val shape: (30490, 1919)\n    # submission shape: (60980, 29)\n\n    return sales, prices, calendar, submission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The actual loading of the data into memory\n\nsales, prices, calendar, submission = read_data()\n\nNUM_ITEMS = sales.shape[0]  # 30490\nDAYS_PRED = submission.shape[1] - 1  # 28","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Further reduction of memory usage by encoding the values of columns that contain string values as integers.\n\ndef encode_categorical(df, cols):\n    for col in cols:\n        # Leave NaN as it is.\n        le = LabelEncoder()\n        not_null = df[col][df[col].notnull()]\n        df[col] = pd.Series(le.fit_transform(not_null), index=not_null.index)\n\n    return df\n\n# Encode calendar events\ncalendar = encode_categorical(\n    calendar, [\"event_name_1\", \"event_type_1\", \"event_name_2\", \"event_type_2\"]\n).pipe(reduce_mem_usage)\n\n# Encode sales data columns\nsales = encode_categorical(\n    sales, [\"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"],\n).pipe(reduce_mem_usage)\n\n# Encode price data columns\nprices = encode_categorical(prices, [\"item_id\", \"store_id\"]).pipe(reduce_mem_usage)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Functions to reshape the dataframes into a format that more easily allows merging them together. In the end, each row of the resulting dataframe will\n# contain all information pertaining to a single product on a single day.\n\ndef extract_num(ser):\n    return ser.str.extract(r\"(\\d+)\").astype(np.int16)\n\n    \ndef reshape_sales(sales, submission, d_thresh=0, verbose=True):\n    # melt sales data, get it ready for training\n    id_columns = [\"id\", \"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"]\n\n    # get product table.\n    product = sales[id_columns]\n\n    sales = sales.melt(id_vars=id_columns, var_name=\"d\", value_name=\"demand\",)\n\n    sales = reduce_mem_usage(sales)\n    \n    # separate test dataframes into validation and evaluation dataframes\n    vals = submission[submission[\"id\"].str.endswith(\"validation\")]\n    evals = submission[submission[\"id\"].str.endswith(\"evaluation\")]\n\n    # change column names.\n    vals.columns = [\"id\"] + [f\"d_{d}\" for d in range(1914, 1914 + DAYS_PRED)]\n    evals.columns = [\"id\"] + [f\"d_{d}\" for d in range(1942, 1942 + DAYS_PRED)]\n\n    # merge with product table\n    evals[\"id\"] = evals[\"id\"].str.replace(\"_evaluation\", \"_validation\")\n    vals = vals.merge(product, how=\"left\", on=\"id\")\n    evals = evals.merge(product, how=\"left\", on=\"id\")\n    evals[\"id\"] = evals[\"id\"].str.replace(\"_validation\", \"_evaluation\")\n\n    if verbose:\n        print(\"validation\")\n        display(vals)\n\n        print(\"evaluation\")\n        display(evals)\n\n    vals = vals.melt(id_vars=id_columns, var_name=\"d\", value_name=\"demand\")\n    evals = evals.melt(id_vars=id_columns, var_name=\"d\", value_name=\"demand\") \n\n    sales[\"part\"] = \"train\"\n    vals[\"part\"] = \"validation\"\n    evals[\"part\"] = \"evaluation\"\n\n    data = pd.concat([sales, vals, evals], axis=0)\n    del sales, vals, evals\n\n    data[\"d\"] = extract_num(data[\"d\"])\n    data = data[data[\"d\"] >= d_thresh]\n\n    # delete evaluation for now.\n    data = data[data[\"part\"] != \"evaluation\"]\n\n    gc.collect()\n\n    if verbose:\n        print(\"data\")\n        display(data)\n\n    return data\n\n    \ndef merge_calendar(data, calendar):                                             \n    calendar = calendar.drop([\"weekday\", \"wday\", \"month\", \"year\"], axis=1)\n    return data.merge(calendar, how=\"left\", on=\"d\")\n\n\ndef merge_prices(data, prices):\n    return data.merge(prices, how=\"left\", on=[\"store_id\", \"item_id\", \"wm_yr_wk\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d_thresh= 1941 - int(365 * 1) #We take the most recent year of data\n\ntime_steps = 7 #The number of timesteps contained in training data that we feed into the RNN later on in the notebook\n\n\ndata = reshape_sales(sales, submission, d_thresh=d_thresh - time_steps) # For the RNN predictions we need data starting at <time_steps> days earlier.\ndel sales\ngc.collect()\n\ncalendar[\"d\"] = extract_num(calendar[\"d\"])\ndata = merge_calendar(data, calendar)\ndel calendar\ngc.collect()\n\ndata = merge_prices(data, prices)\ndel prices\ngc.collect()\n\ndata = reduce_mem_usage(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Result up until now of merging all those dataframes. This is still 'raw' data, i.e., no data here is new, only re-organized.\n\ndata.head(-5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def add_demand_features(df):\n        \n    #lag-features\n    #As our results section shows, removing the '7' improves the end score of the algorithm\n    for shift in [7,28]:\n        df[f\"shift_t{shift}\"] = df.groupby([\"id\"])[\"demand\"].transform(\n            lambda x: x.shift(shift)\n        )\n\n    for window in [7, 30, 60, 90, 180]:\n        df[f\"rolling_std_t{window}\"] = df.groupby([\"id\"])[\"demand\"].transform(\n            lambda x: x.shift(DAYS_PRED).rolling(window).std()\n        )\n\n    for window in [7, 30, 60, 90, 180]:\n        df[f\"rolling_mean_t{window}\"] = df.groupby([\"id\"])[\"demand\"].transform(\n            lambda x: x.shift(DAYS_PRED).rolling(window).mean()\n        )\n\n    for window in [7, 30, 60]:\n        df[f\"rolling_min_t{window}\"] = df.groupby([\"id\"])[\"demand\"].transform(\n            lambda x: x.shift(DAYS_PRED).rolling(window).min()\n        )\n\n    for window in [7, 30, 60]:\n        df[f\"rolling_max_t{window}\"] = df.groupby([\"id\"])[\"demand\"].transform(\n            lambda x: x.shift(DAYS_PRED).rolling(window).max()\n        )\n\n    df[\"rolling_skew_t30\"] = df.groupby([\"id\"])[\"demand\"].transform(\n        lambda x: x.shift(DAYS_PRED).rolling(30).skew()\n    )\n    df[\"rolling_kurt_t30\"] = df.groupby([\"id\"])[\"demand\"].transform(\n        lambda x: x.shift(DAYS_PRED).rolling(30).kurt()\n    )\n    return df\n\n\ndef add_price_features(df):\n    df[\"shift_price_t1\"] = df.groupby([\"id\"])[\"sell_price\"].transform(\n        lambda x: x.shift(1)\n    )\n    df[\"price_change_t1\"] = (df[\"shift_price_t1\"] - df[\"sell_price\"]) / (\n        df[\"shift_price_t1\"]\n    )\n    df[\"rolling_price_max_t365\"] = df.groupby([\"id\"])[\"sell_price\"].transform(\n        lambda x: x.shift(1).rolling(365).max()\n    )\n    df[\"price_change_t365\"] = (df[\"rolling_price_max_t365\"] - df[\"sell_price\"]) / (\n        df[\"rolling_price_max_t365\"]\n    )\n\n    df[\"rolling_price_std_t7\"] = df.groupby([\"id\"])[\"sell_price\"].transform(\n        lambda x: x.rolling(7).std()\n    )\n    df[\"rolling_price_std_t30\"] = df.groupby([\"id\"])[\"sell_price\"].transform(\n        lambda x: x.rolling(30).std()\n    )\n    return df.drop([\"rolling_price_max_t365\", \"shift_price_t1\"], axis=1)\n\n\ndef add_time_features(df, dt_col):\n    df[dt_col] = pd.to_datetime(df[dt_col])\n    attrs = [\n        \"year\",\n        \"quarter\",\n        \"month\",\n        \"week\",\n        \"day\",\n        \"dayofweek\",\n    ]\n\n    for attr in attrs:\n        dtype = np.int16 if attr == \"year\" else np.int8\n        df[attr] = getattr(df[dt_col].dt, attr).astype(dtype)\n\n    df[\"is_weekend\"] = df[\"dayofweek\"].isin([5, 6]).astype(np.int8)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = add_demand_features(data).pipe(reduce_mem_usage)\ndata = add_price_features(data).pipe(reduce_mem_usage)\ndt_col = \"date\"\ndata = add_time_features(data, dt_col).pipe(reduce_mem_usage)\ndata = data.sort_values(\"date\")\n\nprint(\"start date:\", data[dt_col].min())\nprint(\"end date:\", data[dt_col].max())\nprint(\"data shape:\", data.shape)\n\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def RNNdatasetByStoreTimesteppedScaledShifted(df, store_id, feature_keys, start, end, timestep):\n    X = []\n    Y = []\n    \n    for day in range(start, end):\n        XStep = []\n        \n        for step in range(timestep):\n            xdata = df.loc[df['d'] == day - timestep + step]                 # Data per day\n            xdata = xdata.loc[xdata['store_id'] == store_id]      # Data voor specific store\n            xtotal = xdata['demand'].sum()                        # Total sales for that day for that store\n            xdata = xdata.iloc[0][feature_keys]                   \n            xdata['demand'] = xtotal\n\n            XStep.append(xdata.values)\n            \n        ydata = df.loc[df['d'] == day]\n        ydata = ydata.loc[ydata['store_id'] == store_id]\n        ytotal = ydata['demand'].sum()            \n        \n        X.append(XStep)\n        Y.append(ytotal)\n        \n    X = np.asarray(X, dtype=np.float32)\n    Y = np.asarray(Y, dtype=np.float32)\n    Y = Y[..., np.newaxis]\n    \n    #Scale values to a [0,1] range\n    for i in range(np.shape(X)[-1]):\n        X[..., i] = X[..., i] / np.max(X[..., i])\n    Y[..., 0] = Y[..., 0] / np.max(Y[..., 0])\n\n    return (X, Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow.keras as keras\nimport tensorflow as tf\nfrom tensorflow.keras import layers\n\nNR_OF_STORES = 10\nd_end = 1914+1\n\nbatch_size = 128\nepochs = 40\n\nRNN_features = [\"demand\", \"dayofweek\", \"is_weekend\", \"month\", \"quarter\"]\n\ndef root_mean_squared_error(y_true, y_pred):\n        return keras.backend.sqrt(keras.backend.mean(keras.backend.square(y_pred - y_true)))\n\n    \nstore_models = []\n\n# Create and train models for every store in the dataset\nfor store_id in range(NR_OF_STORES):\n    \n    print(f\"Start training store {store_id}\")\n\n    #Timestepped\n    trainX, trainY = RNNdatasetByStoreTimesteppedScaledShifted(data, store_id, RNN_features, d_thresh, d_end, timestep=time_steps)\n    \n    print(\"trainX shape:\", np.shape(trainX))\n    print(\"trainY shape:\", np.shape(trainY))\n    \n    #The model with LSTM's\n    model = keras.Sequential()\n    model.add(layers.LSTM(512, input_shape=(time_steps, len(RNN_features)), return_sequences=True))\n    model.add(layers.Activation(keras.activations.relu))\n    model.add(layers.Dropout(0.2))\n    model.add(layers.LSTM(512))\n    model.add(layers.Activation(keras.activations.relu))\n    model.add(layers.Dropout(0.2))\n    model.add(layers.Dense(512))\n    model.add(layers.Activation(keras.activations.relu))\n    model.add(layers.Dropout(0.2))\n    model.add(layers.Dense(512))\n    model.add(layers.Activation(keras.activations.relu))\n    model.add(layers.Dropout(0.2))\n    model.add(layers.Dense(1))\n\n    model.compile(loss=root_mean_squared_error, optimizer='adam')\n\n    print(model.summary())\n\n    # Timestepped dataset\n    model.fit(trainX, trainY, batch_size=batch_size, epochs=epochs, verbose=2, shuffle=False)\n    store_models.append(model)\n    \n    del trainX\n    del trainY\n    gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def addRNNPredictions(df, models, d_thresh, d_end):\n    \n    preds = []\n    \n    for i, model in enumerate(models):\n        print(\"Starting predictions for store \" + str(i))\n        trainX, _ = RNNdatasetByStoreTimesteppedScaledShifted(data, i, RNN_features, d_thresh, d_end, timestep=time_steps)\n        store_preds = model.predict(trainX)\n        preds.append(store_preds)\n        \n    print(\"predictions done\\n\")\n\n    df['RNN_pred'] = 0\n        \n        \n    for day in range(d_thresh, d_end):\n        \n        if(day%10 == 0):\n            print(\"Inserting day \" + str(day))\n    \n        dagdata = df.loc[df['d'] == day]\n\n        for store_id, model in enumerate(models):\n            dagdata.loc[dagdata['store_id'] == store_id, 'RNN_pred'] = preds[store_id][day-d_thresh]\n            \n        df.loc[df['d'] == day] = dagdata\n\n        \n    del preds\n    del trainX\n    del store_preds\n    del dagdata\n    \n    gc.collect()\n    \n    return df\n    \n    \ndata = addRNNPredictions(data, store_models, d_thresh, d_end)\ndata = data[data[\"d\"] >= d_thresh]\n\ndata = reduce_mem_usage(data)\n\ndel store_models\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class CustomTimeSeriesSplitter:\n    def __init__(self, n_splits=5, train_days=80, test_days=20, day_col=\"d\"):\n        self.n_splits = n_splits\n        self.train_days = train_days\n        self.test_days = test_days\n        self.day_col = day_col\n\n    def split(self, X, y=None, groups=None):\n        SEC_IN_DAY = 3600 * 24\n        sec = (X[self.day_col] - X[self.day_col].iloc[0]) * SEC_IN_DAY\n        duration = sec.max()\n\n        train_sec = self.train_days * SEC_IN_DAY\n        test_sec = self.test_days * SEC_IN_DAY\n        total_sec = test_sec + train_sec\n\n        if self.n_splits == 1:\n            train_start = duration - total_sec\n            train_end = train_start + train_sec\n\n            train_mask = (sec >= train_start) & (sec < train_end)\n            test_mask = sec >= train_end\n\n            yield sec[train_mask].index.values, sec[test_mask].index.values\n\n        else:\n            # step = (duration - total_sec) / (self.n_splits - 1)\n            step = DAYS_PRED * SEC_IN_DAY\n\n            for idx in range(self.n_splits):\n                # train_start = idx * step\n                shift = (self.n_splits - (idx + 1)) * step\n                train_start = duration - total_sec - shift\n                train_end = train_start + train_sec\n                test_end = train_end + test_sec\n\n                train_mask = (sec > train_start) & (sec <= train_end)\n\n                if idx == self.n_splits - 1:\n                    test_mask = sec > train_end\n                else:\n                    test_mask = (sec > train_end) & (sec <= test_end)\n\n                yield sec[train_mask].index.values, sec[test_mask].index.values\n\n    def get_n_splits(self):\n        return self.n_splits","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"day_col = \"d\"\ncv_params = {\n    \"n_splits\": 3,\n    \"train_days\": int(365 * 1.5),\n    \"test_days\": DAYS_PRED,\n    \"day_col\": day_col,\n}\ncv = CustomTimeSeriesSplitter(**cv_params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_cv_days(cv, X, dt_col, day_col):\n    for ii, (tr, tt) in enumerate(cv.split(X)):\n        print(f\"----- Fold: ({ii + 1} / {cv.n_splits}) -----\")\n        tr_start = X.iloc[tr][dt_col].min()\n        tr_end = X.iloc[tr][dt_col].max()\n        tr_days = X.iloc[tr][day_col].max() - X.iloc[tr][day_col].min() + 1\n\n        tt_start = X.iloc[tt][dt_col].min()\n        tt_end = X.iloc[tt][dt_col].max()\n        tt_days = X.iloc[tt][day_col].max() - X.iloc[tt][day_col].min() + 1\n\n        df = pd.DataFrame(\n            {\n                \"start\": [tr_start, tt_start],\n                \"end\": [tr_end, tt_end],\n                \"days\": [tr_days, tt_days],\n            },\n            index=[\"train\", \"test\"],\n        )\n\n        display(df)\n\n\ndef plot_cv_indices(cv, X, dt_col, lw=10):\n    n_splits = cv.get_n_splits()\n    _, ax = plt.subplots(figsize=(20, n_splits))\n\n    # Generate the training/testing visualizations for each CV split\n    for ii, (tr, tt) in enumerate(cv.split(X)):\n        # Fill in indices with the training/test groups\n        indices = np.array([np.nan] * len(X))\n        indices[tt] = 1\n        indices[tr] = 0\n\n        # Visualize the results\n        ax.scatter(\n            X[dt_col],\n            [ii + 0.5] * len(indices),\n            c=indices,\n            marker=\"_\",\n            lw=lw,\n            cmap=plt.cm.coolwarm,\n            vmin=-0.2,\n            vmax=1.2,\n        )\n\n    # Formatting\n    MIDDLE = 15\n    LARGE = 20\n    ax.set_xlabel(\"Datetime\", fontsize=LARGE)\n    ax.set_xlim([X[dt_col].min(), X[dt_col].max()])\n    ax.set_ylabel(\"CV iteration\", fontsize=LARGE)\n    ax.set_yticks(np.arange(n_splits) + 0.5)\n    ax.set_yticklabels(list(range(n_splits)))\n    ax.invert_yaxis()\n    ax.tick_params(axis=\"both\", which=\"major\", labelsize=MIDDLE)\n    ax.set_title(\"{}\".format(type(cv).__name__), fontsize=LARGE)\n    return ax","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample = data.iloc[::1000][[day_col, dt_col]].reset_index(drop=True)\nshow_cv_days(cv, sample, dt_col, day_col)\nplot_cv_indices(cv, sample, dt_col)\n\ndel sample\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = [\n    \"item_id\",\n    \"dept_id\",\n    \"cat_id\",\n    \"store_id\",\n    \"state_id\",\n    \"event_name_1\",\n    \"event_type_1\",\n    \"event_name_2\",\n    \"event_type_2\",\n    \"snap_CA\",\n    \"snap_TX\",\n    \"snap_WI\",\n    \"sell_price\",\n    # demand features\n    \"shift_t7\",\n    \"shift_t28\",\n#     \"shift_t29\",\n#     \"shift_t30\",\n    # std\n    \"rolling_std_t7\",\n    \"rolling_std_t30\",\n    \"rolling_std_t60\",\n    \"rolling_std_t90\",\n    \"rolling_std_t180\",\n    # mean\n    \"rolling_mean_t7\",\n    \"rolling_mean_t30\",\n    \"rolling_mean_t60\",\n    \"rolling_mean_t90\",\n    \"rolling_mean_t180\",\n    # min\n    \"rolling_min_t7\",\n    \"rolling_min_t30\",\n    \"rolling_min_t60\",\n    # max\n    \"rolling_max_t7\",\n    \"rolling_max_t30\",\n    \"rolling_max_t60\",\n    # others\n    \"rolling_skew_t30\",\n    \"rolling_kurt_t30\",\n    # price features\n    \"price_change_t1\",\n    \"price_change_t365\",\n    \"rolling_price_std_t7\",\n    \"rolling_price_std_t30\",\n    # time features\n    \"year\",\n    \"quarter\",\n    \"month\",\n    \"week\",\n    \"day\",\n    \"dayofweek\",\n    \"is_weekend\",\n    # RNN features\n    \"RNN_pred\",\n]\n\n# prepare training and test data.\n# 2011-01-29 ~ 2016-04-24 : d_1    ~ d_1913\n# 2016-04-25 ~ 2016-05-22 : d_1914 ~ d_1941 (public)\n# 2016-05-23 ~ 2016-06-19 : d_1942 ~ d_1969 (private)\n\nis_train = data[\"d\"] < 1914\n\n# Attach \"d\" to X_train for cross validation.\nX_train = data[is_train][[day_col] + features].reset_index(drop=True)\ny_train = data[is_train][\"demand\"].reset_index(drop=True)\nX_test = data[~is_train][features].reset_index(drop=True)\n\n# keep these two columns to use later.\nid_date = data[~is_train][[\"id\", \"date\"]].reset_index(drop=True)\n\ndel data\ngc.collect()\n\nprint(\"X_train shape:\", X_train.shape)\nprint(\"X_test shape:\", X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_lgb(bst_params, fit_params, X, y, cv, drop_when_train=None):\n    models = []\n\n    if drop_when_train is None:\n        drop_when_train = []\n\n    for idx_fold, (idx_trn, idx_val) in enumerate(cv.split(X, y)):\n        print(f\"\\n----- Fold: ({idx_fold + 1} / {cv.get_n_splits()}) -----\\n\")\n\n        X_trn, X_val = X.iloc[idx_trn], X.iloc[idx_val]\n        y_trn, y_val = y.iloc[idx_trn], y.iloc[idx_val]\n        train_set = lgb.Dataset(\n            X_trn.drop(drop_when_train, axis=1),\n            label=y_trn,\n            categorical_feature=[\"item_id\"],\n        )\n        val_set = lgb.Dataset(\n            X_val.drop(drop_when_train, axis=1),\n            label=y_val,\n            categorical_feature=[\"item_id\"],\n        )\n\n        model = lgb.train(\n            bst_params,\n            train_set,\n            valid_sets=[train_set, val_set],\n            valid_names=[\"train\", \"valid\"],\n            **fit_params,\n        )\n        models.append(model)\n\n        del idx_trn, idx_val, X_trn, X_val, y_trn, y_val\n        gc.collect()\n\n    return models","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bst_params = {\n    \"boosting_type\": \"gbdt\",\n    \"metric\": \"rmse\",\n    \"objective\": \"regression\",\n    \"n_jobs\": -1,\n    \"seed\": 42,\n    \"learning_rate\": 0.1,\n    \"bagging_fraction\": 0.75,\n    \"bagging_freq\": 10,\n    \"colsample_bytree\": 0.75,\n}\n\nfit_params = {\n    \"num_boost_round\": 100_000,\n    \"early_stopping_rounds\": 50,\n    \"verbose_eval\": 100,\n}\n\nmodels = train_lgb(\n    bst_params, fit_params, X_train, y_train, cv, drop_when_train=[day_col]\n)\n\ndel X_train, y_train\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def rmse(y_true, y_pred):\n    return np.sqrt(mean_squared_error(y_true, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imp_type = \"gain\"\nimportances = np.zeros(X_test.shape[1])\npreds = np.zeros(X_test.shape[0])\n\nfor model in models:\n    preds += model.predict(X_test)\n    importances += model.feature_importance(imp_type)\n\npreds = preds / cv.get_n_splits()\nimportances = importances / cv.get_n_splits()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from mlflow_extend import mlflow, plotting as mplt\n\nwith mlflow.start_run():\n    mlflow.log_params_flatten({\"bst\": bst_params, \"fit\": fit_params, \"cv\": cv_params})\n\n\nfeatures = models[0].feature_name()\n_ = mplt.feature_importance(features, importances, imp_type, limit=30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_submission(test, submission):\n    preds = test[[\"id\", \"date\", \"demand\"]]\n    preds = preds.pivot(index=\"id\", columns=\"date\", values=\"demand\").reset_index()\n    preds.columns = [\"id\"] + [\"F\" + str(d + 1) for d in range(DAYS_PRED)]\n\n    vals = submission[[\"id\"]].merge(preds, how=\"inner\", on=\"id\")\n    evals = submission[submission[\"id\"].str.endswith(\"evaluation\")]\n    final = pd.concat([vals, evals])\n\n    assert final.drop(\"id\", axis=1).isnull().sum().sum() == 0\n    assert final[\"id\"].equals(submission[\"id\"])\n\n    final.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"make_submission(id_date.assign(demand=preds), submission)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}