{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn import preprocessing\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom math import sqrt\nimport numba as nb\nimport scipy.sparse as sp\nfrom scipy.optimize import fmin_l_bfgs_b, fmin_ncg\nfrom numpy.lib.stride_tricks import as_strided\nfrom sklearn.utils.extmath import safe_sparse_dot\nfrom sklearn.utils import check_consistent_length, check_array\nfrom sklearn.utils import check_random_state\nfrom sklearn.base import BaseEstimator\n\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom functools import reduce","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def trcg(Ax, r, x, n_iterations=1000, tr_delta=0, rtol=1e-5, atol=1e-8, args=(), verbose=False):\n    if n_iterations > 0:\n        n_iterations = min(n_iterations, len(x))\n\n    p, iteration = r.copy(), 0\n    tr_delta_sq = tr_delta ** 2\n\n    rtr, rtr_old = np.dot(r, r), 1.0\n    cg_tol = sqrt(rtr) * rtol + atol\n    region_breached = False\n    while (iteration < n_iterations) and (sqrt(rtr) > cg_tol):\n        Ap = Ax(p, *args)\n        iteration += 1\n        if verbose:\n            print(\"\"\"iter %2d |Ap| %5.3e |p| %5.3e \"\"\"\n                  \"\"\"|r| %5.3e |x| %5.3e beta %5.3e\"\"\" %\n                  (iteration, np.linalg.norm(Ap), np.linalg.norm(p),\n                   np.linalg.norm(r), np.linalg.norm(x), rtr / rtr_old))\n        # end if\n\n        # ddot(&n, p, &inc, Ap, &inc);\n        alpha = rtr / np.dot(p, Ap)\n        # daxpy(&n, &alpha, p, &inc, x, &inc);\n        x += alpha * p\n        # daxpy(&n, &( -alpha ), Ap, &inc, r, &inc);\n        r -= alpha * Ap\n\n        # check trust region (diverges from tron.cpp in liblinear and leml-imf)\n        if tr_delta_sq > 0:\n            xTx = np.dot(x, x)\n            if xTx > tr_delta_sq:\n                xTp = np.dot(x, p)\n                if xTp > 0:\n                    # backtrack into the trust region\n                    p_nrm = np.linalg.norm(p)\n\n                    q = xTp / p_nrm\n                    eta = (q - sqrt(max(q * q + tr_delta_sq - xTx, 0))) / p_nrm\n\n                    # reproject onto the boundary of the region\n                    r += eta * Ap\n                    x -= eta * p\n                else:\n                    # this never happens maybe due to CG iteration properties\n                    pass\n                # end if\n\n                region_breached = True\n                break\n            # end if\n        # end if\n\n        # ddot(&n, r, &inc, r, &inc);\n        rtr, rtr_old = np.dot(r, r), rtr\n        # dscal(&n, &(rtr / rtr_old), p, &inc);\n        p *= rtr / rtr_old\n        # daxpy(&n, &one, r, &1, p, &1);\n        p += r\n    # end while\n\n    return iteration, region_breached\n\n\ndef tron(func, x, n_iterations=1000, rtol=1e-3, atol=1e-5, args=(),\n         verbose=False):\n    \n    eta0, eta1, eta2 = 1e-4, 0.25, 0.75\n    sigma1, sigma2, sigma3 = 0.25, 0.5, 4.0\n\n    f_valp_, f_grad_, f_hess_ = func\n\n    iteration, cg_iter = 0, 0\n\n    fval = f_valp_(x, *args)\n    grad = f_grad_(x, *args)\n    grad_norm = np.linalg.norm(grad)\n\n    # make a copy of `-grad` and zeros like `x`\n    # r, z = -grad, np.zeros_like(x)\n    delta, grad_norm_tol = grad_norm, grad_norm * rtol + atol\n    while iteration < n_iterations and grad_norm > grad_norm_tol:\n        r, z = -grad, np.zeros_like(x)\n        # tolerances and n_iterations as in leml-imf\n        cg_iter, region_breached = trcg(\n            f_hess_, r, z, tr_delta=delta, args=args,\n            n_iterations=20, rtol=1e-1, atol=0.0)\n\n        z_norm = np.linalg.norm(z)\n        if iteration == 0:\n            delta = min(delta, z_norm)\n\n        # trcg finds z and r s.t. r + A z = -g and \\|r\\|\\to \\min\n        # f(x) - f(x+z) ~ -0.5 * (2 g'z + z'Az) = -0.5 * (g'z + z'(-r))\n        linear = np.dot(z, grad)\n        approxred = -0.5 * (linear - np.dot(z, r))\n\n        # The value and the actual reduction: compute the forward pass.\n        fnew = f_valp_(x + z, *args)\n        actualred = fval - fnew\n\n        if linear + actualred < 0:\n            alpha = max(sigma1, 0.5 * linear / (linear + actualred))\n\n        else:\n            alpha = sigma3\n\n        # end if\n\n        if actualred < eta0 * approxred:\n            delta = min(max(alpha, sigma1) * z_norm, sigma2 * delta)\n\n        elif actualred < eta1 * approxred:\n            delta = max(sigma1 * delta, min(alpha * z_norm, sigma2 * delta))\n\n        elif actualred < eta2 * approxred:\n            delta = max(sigma1 * delta, min(alpha * z_norm, sigma3 * delta))\n\n        else:\n            # patch 2018-08-30: new addition from tron.cpp at\n            #  https://github.com/cjlin1/liblinear/blob/master/tron.cpp\n            if region_breached:\n                delta = sigma3 * delta\n\n            else:\n                delta = max(delta, min(alpha * z_norm, sigma3 * delta))\n\n        # end if\n\n        if verbose:\n            print(\"\"\"iter %2d act %5.3e pre %5.3e delta %5.3e \"\"\"\n                  \"\"\"f %5.3e |z| %5.3e |g| %5.3e CG %3d\"\"\" %\n                  (iteration, actualred, approxred,\n                   delta, fval, z_norm, grad_norm, cg_iter))\n        # end if\n\n        if actualred > eta0 * approxred:\n            x += z\n            fval, grad = fnew, f_grad_(x, *args)\n            grad_norm = np.linalg.norm(grad)\n            iteration += 1\n\n            # r, z = -grad, np.zeros_like(x)\n        # end if\n\n        if fval < -1e32:\n            if verbose:\n                print(\"WARNING: f < -1.0e+32\")\n            break\n        # end if\n\n        if abs(actualred) <= 0 and approxred <= 0:\n            if verbose:\n                print(\"WARNING: actred and prered <= 0\")\n            break\n        # end if\n\n        if abs(actualred) <= 1e-12 * abs(fval) and \\\n           abs(approxred) <= 1e-12 * abs(fval):\n            if verbose:\n                print(\"WARNING: actred and prered too small\")\n            break\n        # end if\n\n        if delta <= rtol * (z_norm + atol):\n            if verbose:\n                print(\"WARNING: degenerate trust region\")\n            break\n        # end if\n    # end while\n\n    return cg_iter","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"@nb.njit(\"float64[:,::1](float64[:,::1], float64[:,::1])\",fastmath=True, cache=False, error_model=\"numpy\")\ndef ar_resid(Z, phi):\n    \"\"\"Compute the AR(p) residuals of the multivariate data in Z.\"\"\"\n    n_components, n_order = phi.shape\n\n    resid = Z[n_order:].copy()\n    for k in range(n_order):\n        # r_t -= y_{t-(p-k)} * \\beta_{p - k} (\\phi is reversed \\beta)\n        resid -= Z[k:k - n_order] * phi[:, k]\n\n    return resid\n\n#@nb.njit(\"float64[:,::1](float64[:,::1], float64[:,::1], float64[:,::1])\",fastmath=True, cache=False, error_model=\"numpy\")\ndef ar_hess_vect(V, Z, phi):\n    \"\"\"Compute the Hessian-vector product of the AR(p) square loss for `V`.\"\"\"\n    n_components, n_order = phi.shape\n\n    # compute the AR(p) residuals over V\n    resid = ar_resid(V, phi)\n\n    # get the derivative w.r.t. the series\n    hess_v = np.zeros_like(V)\n    hess_v[n_order:] = resid\n    for k in range(n_order):\n        hess_v[k:k - n_order] -= resid * phi[:, k]\n\n    return hess_v\n\n#@nb.njit(\"float64[:,::1](float64[:,::1], float64[:,::1])\",         fastmath=True, cache=False, error_model=\"numpy\")\ndef ar_grad(Z, phi):\n    \"\"\"Compute the gradient of the AR(p) l2 loss w.r.t. the time-series `Z`.\"\"\"\n    return ar_hess_vect(Z, Z, phi)\n\n\ndef precompute_graph_reg(adj):\n    \"\"\"Precompute the neighbor average discrepancy operator.\"\"\"\n\n    # make a copy of the adjacency matrix and the outbound degree\n    resid, deg = adj.astype(float), adj.getnnz(axis=1)\n\n    # scale the rows : D^{-1} A\n    resid.data /= deg[adj.nonzero()[0]]\n\n    # subtract the matrix from the diagonalized mask\n    return sp.diags((deg > 0).astype(float)) - resid\n\n\ndef graph_resid(F, adj):\n    \"\"\"Get the residual of the outgoing neighbor average of `F`.\"\"\"\n    return safe_sparse_dot(adj, F.T).T\n\n\ndef graph_grad(F, adj):\n    \"\"\"Compute the gradient of the outgoing neighbors average w.r.t. `F`.\"\"\"\n    return safe_sparse_dot(adj.T, graph_resid(F, adj).T).T\n\n\ndef graph_hess_vect(V, F, adj):\n    \"\"\"Get the Hessian-vector product of the outgoing neighbors average.\"\"\"\n    return graph_grad(V, adj)\n\ndef l2_loss_valj(Y, Z, F):\n    if sp.issparse(Y):\n        R = csr_gemm(1, Z, F, -1, Y.copy())\n        return sp.linalg.norm(R, ord=\"fro\") ** 2\n\n    return np.linalg.norm(Y - np.dot(Z, F), ord=\"fro\") ** 2\n\ndef f_step_tron_valj(f, Y, Z, C_F, eta_F, adj):\n    \"\"\"Compute current value the f-step loss.\"\"\"\n    (n_samples, n_targets), n_components = Y.shape, Z.shape[1]\n\n    F = f.reshape(n_components, n_targets)\n    objective = l2_loss_valj(Y, Z, F)\n\n    if sp.issparse(Y):\n        coef = C_F * Y.nnz / (n_components * n_targets)\n    else:\n        coef = C_F * n_samples / n_components\n\n    if C_F > 0:\n        if eta_F < 1:\n            reg_f_l2 = np.linalg.norm(F, ord=\"fro\") ** 2\n        else:\n            reg_f_l2, eta_F = 0., 1.\n        # end if\n\n        if sp.issparse(adj) and (eta_F > 0):\n            reg_f_graph = np.linalg.norm(graph_resid(F, adj), ord=\"fro\") ** 2\n        else:\n            reg_f_graph, eta_F = 0., 0.\n        # end if\n\n        reg_f = reg_f_l2 * (1 - eta_F) + reg_f_graph * eta_F\n        objective += reg_f * coef\n    # end if\n\n    return 0.5 * objective\n\ndef f_step_tron_grad(f, Y, Z, C_F, eta_F, adj):\n    \"\"\"Compute the gradient of the f-step objective.\"\"\"\n    (n_samples, n_targets), n_components = Y.shape, Z.shape[1]\n\n    F = f.reshape(n_components, n_targets)\n    if sp.issparse(Y):\n        coef = C_F * Y.nnz / (n_components * n_targets)\n\n        grad = safe_sparse_dot(Z.T, csr_gemm(1, Z, F, -1, Y.copy()))\n        grad += (1 - eta_F) * coef * F\n\n    else:\n        coef = C_F * n_samples / n_components\n\n        ZTY, ZTZ = np.dot(Z.T, Y), np.dot(Z.T, Z)\n        if C_F > 0 and eta_F < 1:\n            ZTZ.flat[::n_components + 1] += (1 - eta_F) * coef\n\n        grad = np.dot(ZTZ, F) - ZTY\n    # end if\n\n    if C_F > 0 and sp.issparse(adj) and eta_F > 0:\n        grad += graph_grad(F, adj) * (eta_F * coef)\n\n    return grad.reshape(-1)\n\ndef f_step_tron_hess(v, Y, Z, C_F, eta_F, adj):\n    \"\"\"Get the Hessian-vector product for the f-step objective.\"\"\"\n    (n_samples, n_targets), n_components = Y.shape, Z.shape[1]\n\n    V = v.reshape(n_components, n_targets)\n    if sp.issparse(Y):\n        coef = C_F * Y.nnz / (n_components * n_targets)\n\n        hess_v = safe_sparse_dot(Z.T, csr_gemm(1, Z, V, 0, Y.copy()))\n        hess_v += (1 - eta_F) * coef * V\n\n    else:\n        coef = C_F * n_samples / n_components\n\n        ZTZ = np.dot(Z.T, Z)\n        if C_F > 0 and eta_F < 1:\n            ZTZ.flat[::n_components + 1] += (1 - eta_F) * coef\n\n        hess_v = np.dot(ZTZ, V)\n    # end if\n\n    if C_F > 0 and sp.issparse(adj) and eta_F > 0:\n        hess_v += graph_grad(V, adj) * (eta_F * coef)\n\n    return hess_v.reshape(-1)\n\n\ndef f_step_tron(F, Y, Z, C_F, eta_F, adj, rtol=5e-2, atol=1e-4, verbose=False,\n                **kwargs):\n    \"\"\"TRON solver for the f-step minimization problem.\"\"\"\n    f_call = f_step_tron_valj, f_step_tron_grad, f_step_tron_hess\n\n    tron(f_call, F.ravel(), n_iterations=5, rtol=rtol, atol=atol,\n         args=(Y, Z, C_F, eta_F, adj), verbose=verbose)\n\n    return F\n\n\ndef f_step_ncg_hess_(F, v, Y, Z, C_F, eta_F, adj):\n    \"\"\"A wrapper of the hess-vector product for ncg calls.\"\"\"\n    return f_step_tron_hess(v, Y, Z, C_F, eta_F, adj)\n\n\ndef f_step_ncg(F, Y, Z, C_F, eta_F, adj, **kwargs):\n    \"\"\"Solve the F-step using scipy's Newton-CG.\"\"\"\n    FF = fmin_ncg(f=f_step_tron_valj, x0=F.ravel(), disp=False,\n                  fprime=f_step_tron_grad, fhess_p=f_step_ncg_hess_,\n                  args=(Y, Z, C_F, eta_F, adj))\n\n    return FF.reshape(F.shape)\n\n\ndef f_step_lbfgs(F, Y, Z, C_F, eta_F, adj, **kwargs):\n    \"\"\"Solve the F-step using scipy's L-BFGS method.\"\"\"\n    FF, f, d = fmin_l_bfgs_b(func=f_step_tron_valj, x0=F.ravel(), iprint=0,\n                             fprime=f_step_tron_grad,\n                             args=(Y, Z, C_F, eta_F, adj))\n\n    return FF.reshape(F.shape)\n\n\n# In[33]:\ndef f_step_prox_func(F, Y, Z, C_F, eta_F, adj):\n    \"\"\"An interface to the f-step objective for unraveled matrices.\"\"\"\n    return f_step_tron_valj(F.ravel(), Y, Z, C_F, eta_F, adj)\n\n\n# In[34]:\ndef f_step_prox_grad(F, Y, Z, C_F, eta_F, adj):\n    \"\"\"An interface to the f-step objective gradient for unraveled matrices.\"\"\"\n    return f_step_tron_grad(F.ravel(), Y, Z, C_F, eta_F, adj).reshape(F.shape)\n\n\n# In[35]:\ndef f_step_prox(F, Y, Z, C_F, eta_F, adj, lip=1e-2, n_iter=25, alpha=1.0,\n                **kwargs):\n    gamma_u, gamma_d = 2, 1.1\n\n    # get the gradient\n    grad = f_step_prox_grad(F, Y, Z, C_F, eta_F, adj)\n    grad_F = np.dot(grad.flat, F.flat)\n\n    f0, lip0 = f_step_prox_func(F, Y, Z, C_F, eta_F, adj), lip\n    for _ in range(n_iter):\n        # F_new = (1 -  alpha) * F + alpha * np.maximum(F - lr * grad, 0.)\n        # prox-sgd operation\n        F_new = np.maximum(F - grad / lip, 0.)\n\n        # FGM Lipschitz search\n        delta = f_step_prox_func(F_new, Y, Z, C_F, eta_F, adj) - f0\n        linear = np.dot(grad.flat, F_new.flat) - grad_F\n        quad = np.linalg.norm(F_new - F, ord=\"fro\") ** 2\n        if delta <= linear + lip * quad / 2:\n            break\n\n        lip *= gamma_u\n    # end for\n\n    # lip = max(lip0, lip / gamma_d)\n    lip = lip / gamma_d\n\n    return F_new, lip\n\ndef f_step(F, Y, Z, C_F, eta_F, adj, kind=\"fgm\", **kwargs):\n    \"\"\"A common subroutine solving the f-step minimization problem.\"\"\"\n    lip = np.inf\n    if kind == \"fgm\":\n        F, lip = f_step_prox(F, Y, Z, C_F, eta_F, adj, **kwargs)\n    elif kind == \"tron\":\n        F = f_step_tron(F, Y, Z, C_F, eta_F, adj, **kwargs)\n    elif kind == \"ncg\":\n        F = f_step_ncg(F, Y, Z, C_F, eta_F, adj, **kwargs)\n    elif kind == \"lbfgs\":\n        F = f_step_lbfgs(F, Y, Z, C_F, eta_F, adj, **kwargs)\n    else:\n        raise ValueError(f\"\"\"Unrecognized method `{kind}`\"\"\")\n\n    return F, lip\n\n\n# In[41]:\ndef phi_step(phi, Z, C_Z, C_phi, eta_Z, nugget=1e-8):\n    # return a set of independent AR(p) ridge estimates.\n    (n_components, n_order), n_samples = phi.shape, Z.shape[0]\n    if n_order < 1 or n_components < 1:\n        return np.empty((n_components, n_order))\n\n    if not (C_Z > 0 and eta_Z > 0):\n        return np.zeros_like(phi)\n\n    # embed into the last dimensions\n    shape = Z.shape[1:] + (Z.shape[0] - n_order, n_order + 1)\n    strides = Z.strides[1:] + Z.strides[:1] + Z.strides[:1]\n    Z_view = as_strided(Z, shape=shape, strides=strides)\n\n    # split into y (d x T-p) and Z (d x T-p x p) (all are views!)\n    y, Z_lagged = Z_view[..., -1], Z_view[..., :-1]\n\n    # compute the SVD: thin, but V is d x p x p\n    U, s, Vh = np.linalg.svd(Z_lagged, full_matrices=False)\n    if C_phi > 0:\n        # the {V^{H}}^{H} (\\Sigma^2 + C I)^{-1} \\Sigma part is reduced\n        #  to columnwise operations\n        gain = (C_Z * eta_Z * n_order) * s\n        gain /= gain * s + C_phi * (n_samples - n_order)\n    else:\n        # do the same cutoff as in np.linalg.pinv(...)\n        large = s > nugget * np.max(s, axis=-1, keepdims=True)\n        gain = np.divide(1, s, where=large, out=s)\n        gain[~large] = 0\n    # end if\n\n    # get the U' y part and the final estimate\n    # $\\phi_j$ corresponds to $p-j$-th lag $j = 0,\\,\\ldots,\\,p-1$\n    return np.einsum(\"ijk,ij,isj,is->ik\", Vh, gain, U, y)\n\ndef z_step_tron_valh(z, Y, F, phi, C_Z, eta_Z):\n    \"\"\"Get the value of the z-step objective.\"\"\"\n    n_samples, n_targets = Y.shape\n    n_components, n_order = phi.shape\n\n    Z = z.reshape(n_samples, n_components)\n    objective = l2_loss_valj(Y, Z, F)\n\n    if sp.issparse(Y):\n        coef = C_Z * Y.nnz / (n_samples * n_components)\n    else:\n        coef = C_Z * n_targets / n_components\n\n    if C_Z > 0:\n        if eta_Z < 1:\n            reg_z_l2 = np.linalg.norm(Z, ord=\"fro\") ** 2\n        else:\n            reg_z_l2, eta_Z = 0., 1.\n        # end if\n\n        if eta_Z > 0 and n_samples > n_order:\n            reg_z_ar_j = np.linalg.norm(ar_resid(Z, phi), ord=2, axis=0) ** 2\n            reg_z_ar = np.sum(reg_z_ar_j) * n_samples / (n_samples - n_order)\n        else:\n            reg_z_ar, eta_Z = 0., 0.\n        # end if\n\n        # reg_z was implicitly scaled by T d or nnz(Y)\n        reg_z = reg_z_l2 * (1 - eta_Z) + reg_z_ar * eta_Z\n        objective += reg_z * coef\n    # end if\n\n    return 0.5 * objective\n\ndef z_step_tron_grad(z, Y, F, phi, C_Z, eta_Z):\n    \"\"\"Compute the gradient of the z-step objective.\"\"\"\n    n_samples, n_targets = Y.shape\n    n_components, n_order = phi.shape\n\n    Z = z.reshape(n_samples, n_components)\n    if sp.issparse(Y):\n        coef = C_Z * Y.nnz / (n_samples * n_components)\n\n        grad = safe_sparse_dot(csr_gemm(1, Z, F, -1, Y.copy()), F.T)\n        grad += (1 - eta_Z) * coef * Z\n\n    else:\n        coef = C_Z * n_targets / n_components\n\n        YFT, FFT = np.dot(Y, F.T), np.dot(F, F.T)\n        if C_Z > 0 and eta_Z < 1:\n            FFT.flat[::n_components + 1] += (1 - eta_Z) * coef\n\n        grad = np.dot(Z, FFT) - YFT\n    # end if\n\n    if C_Z > 0 and eta_Z > 0:\n        ratio = n_samples / (n_samples - n_order)\n        grad += ar_grad(Z, phi) * (ratio * eta_Z * coef)\n\n    return grad.reshape(-1)\n\n\n# In[49]:\ndef z_step_tron_hess(v, Y, F, phi, C_Z, eta_Z):\n    \"\"\"Compute the Hessian-vector product of the z-step objective for v.\"\"\"\n    n_samples, n_targets = Y.shape\n    n_components, n_order = phi.shape\n\n    V = v.reshape(n_samples, n_components)\n    if sp.issparse(Y):\n        coef = C_Z * Y.nnz / (n_samples * n_components)\n\n        hess_v = safe_sparse_dot(csr_gemm(1, V, F, 0, Y.copy()), F.T)\n        hess_v += (1 - eta_Z) * coef * V\n\n    else:\n        coef = C_Z * n_targets / n_components\n\n        FFT = np.dot(F, F.T)\n        if C_Z > 0 and eta_Z < 1:\n            FFT.flat[::n_components + 1] += (1 - eta_Z) * coef\n\n        hess_v = np.dot(V, FFT)\n    # end if\n\n    if C_Z > 0 and eta_Z > 0:\n        # should call ar_hess_vect(V, Z, adj) but no Z is available\n        ratio = n_samples / (n_samples - n_order)\n        hess_v += ar_grad(V, phi) * ratio * eta_Z * coef\n\n    return hess_v.reshape(-1)\n\n\n# In[50]:\ndef z_step_tron(Z, Y, F, phi, C_Z, eta_Z, rtol=5e-2, atol=1e-4, verbose=False):\n    \"\"\"TRON solver for the f-step minimization problem.\"\"\"\n    f_call = z_step_tron_valh, z_step_tron_grad, z_step_tron_hess\n\n    tron(f_call, Z.ravel(), n_iterations=5, rtol=rtol, atol=atol,\n         args=(Y, F, phi, C_Z, eta_Z), verbose=verbose)\n\n    return Z\n\n\ndef z_step_ncg_hess_(Z, v, Y, F, phi, C_Z, eta_Z):\n    \"\"\"A wrapper of the hess-vector product for ncg calls.\"\"\"\n    return z_step_tron_hess(v, Y, F, phi, C_Z, eta_Z)\n\n\ndef z_step_ncg(Z, Y, F, phi, C_Z, eta_Z, **kwargs):\n    \"\"\"Solve the Z-step using scipy's Newton-CG.\"\"\"\n    ZZ = fmin_ncg(f=z_step_tron_valh, x0=Z.ravel(), disp=False,\n                  fprime=z_step_tron_grad, fhess_p=z_step_ncg_hess_,\n                  args=(Y, F, phi, C_Z, eta_Z))\n    return ZZ.reshape(Z.shape)\n\n\ndef z_step_lbfgs(Z, Y, F, phi, C_Z, eta_Z, **kwargs):\n    \"\"\"Solve the Z-step using scipy's L-BFGS method.\"\"\"\n    ZZ, f, d = fmin_l_bfgs_b(func=z_step_tron_valh, x0=Z.ravel(), iprint=0,\n                             fprime=z_step_tron_grad,\n                             args=(Y, F, phi, C_Z, eta_Z))\n\n    return ZZ.reshape(Z.shape)\n\n\ndef z_step(Z, Y, F, phi, C_Z, eta_Z, kind=\"tron\", **kwargs):\n    \"\"\"A common subroutine solving the Z-step minimization problem.\"\"\"\n    if kind == \"tron\":\n        Z = z_step_tron(Z, Y, F, phi, C_Z, eta_Z, **kwargs)\n    elif kind == \"ncg\":\n        Z = z_step_ncg(Z, Y, F, phi, C_Z, eta_Z, **kwargs)\n    elif kind == \"lbfgs\":\n        Z = z_step_lbfgs(Z, Y, F, phi, C_Z, eta_Z, **kwargs)\n    else:\n        raise ValueError(f\"\"\"Unrecognized method `{kind}`\"\"\")\n\n    return Z","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n@nb.njit(\"(float64, float64[:,::1], float64[:,::1], \"\"float64, int32[::1], int32[::1], float64[::1])\",fastmath=True, error_model=\"numpy\", parallel=False, cache=False)\ndef _csr_gemm(alpha, X, D, beta, Sp, Sj, Sx):\n    # computes\\mathcal{P}_\\Omega(X D) -- n1 x n2 sparse matrix\n    if abs(beta) > 0:\n        for i in nb.prange(len(X)):\n            # compute e_i' XD e_{Sj[j]}\n            for j in range(Sp[i], Sp[i+1]):\n                dot = np.dot(X[i], D[:, Sj[j]])\n                Sx[j] = beta * Sx[j] + alpha * dot\n        # end for\n    else:\n        for i in nb.prange(len(X)):\n            # compute e_i' XD e_{Sj[j]}\n            for j in range(Sp[i], Sp[i+1]):\n                Sx[j] = alpha * np.dot(X[i], D[:, Sj[j]])\n        # end for\n    # end if\n\n\ndef csr_gemm(alpha, X, D, beta, Y):\n    _csr_gemm(alpha, X, D, beta, Y.indptr, Y.indices, Y.data)\n    return Y\n\n\ndef csr_column_means(X):\n    f_sums = np.bincount(X.indices, weights=X.data, minlength=X.shape[1])\n    n_nnz = np.maximum(np.bincount(X.indices, minlength=X.shape[1]), 1.)\n\n    return (f_sums / n_nnz)[np.newaxis]\n\ndef b_step_tron_valj(b, Y, X, C_B):\n    \"\"\"Compute current value the b-step loss.\"\"\"\n    (n_samples, n_targets), n_features = Y.shape, X.shape[1]\n\n    B = b.reshape(n_features, n_targets)\n    objective = l2_loss_valj(Y, X, B)\n\n    if sp.issparse(Y):\n        coef = C_B * Y.nnz / (n_features * n_targets)\n    else:\n        coef = C_B * n_samples / n_features\n\n    if C_B > 0:\n        reg_b = np.linalg.norm(B, ord=\"fro\") ** 2\n\n        objective += reg_b * coef\n    # end if\n\n    return 0.5 * objective\n\n\ndef b_step_tron_grad(b, Y, X, C_B):\n    \"\"\"Compute the gradient of the b-step objective.\"\"\"\n    (n_samples, n_targets), n_features = Y.shape, X.shape[1]\n\n    B = b.reshape(n_features, n_targets)\n    if sp.issparse(Y):\n        coef = C_B * Y.nnz / (n_features * n_targets)\n\n        grad = safe_sparse_dot(X.T, csr_gemm(1, X, B, -1, Y.copy()))\n        grad += coef * B\n\n    else:\n        coef = C_B * n_samples / n_features\n\n        XTY, XTX = np.dot(X.T, Y), np.dot(X.T, X)\n        if C_B > 0:\n            XTX.flat[::n_features + 1] += coef\n        grad = np.dot(XTX, B) - XTY\n    return grad.reshape(-1)\n\ndef b_step_tron_hess(v, Y, X, C_B):\n    \"\"\"Get the Hessian-vector product for the b-step objective.\"\"\"\n    (n_samples, n_targets), n_features = Y.shape, X.shape[1]\n    V = v.reshape(n_features, n_targets)\n    if sp.issparse(Y):\n        coef = C_B * Y.nnz / (n_features * n_targets)\n        hess_v = safe_sparse_dot(X.T, csr_gemm(1, X, V, 0, Y.copy()))\n        hess_v += coef * V\n\n    else:\n        coef = C_B * n_samples / n_features\n        XTX = np.dot(X.T, X)\n        if C_B > 0:\n            XTX.flat[::n_features + 1] += coef\n        hess_v = np.dot(XTX, V)\n    return hess_v.reshape(-1)\n\ndef b_step_tron(B, Y, X, C_B, rtol=5e-2, atol=1e-4, verbose=False, **kwargs):\n    \"\"\"TRON solver for the b-step minimization problem.\"\"\"\n    f_call = b_step_tron_valj, b_step_tron_grad, b_step_tron_hess\n\n    tron(f_call, B.ravel(), n_iterations=5, rtol=rtol, atol=atol,\n         args=(Y, X, C_B), verbose=verbose)\n\n    return B\n\ndef soft_prox(x, c):\n    return np.maximum(x - c, 0.) + np.minimum(x + c, 0.)\n\ndef b_step(B, Y, X, C_B, kind=\"tron\", **kwargs):\n    \"\"\"A common subroutine solving the b-step minimization problem.\"\"\"\n    lip = np.inf\n    if kind == \"tron\":\n        B = b_step_tron(B, Y, X, C_B, **kwargs)\n    # elif kind == \"fgm\":\n    #     B, lip = b_step_prox(B, Y, X, C_B, **kwargs)\n    else:\n        raise ValueError(f\"\"\"Unrecognozed optimization `{kind}`\"\"\")\n\n    return B, lip\n\ndef trmf_init(data, n_components, n_order, random_state=None):\n    random_state = check_random_state(random_state)\n    n_samples, n_targets = data.shape\n    if sp.issparse(data):\n        U, s, Vh = sp.linalg.svds(data, k=n_components)\n\n        order = np.argsort(s)[::-1]\n        U, s, Vh = U[:, order], s[order], Vh[order]\n    else:\n        U, s, Vh = np.linalg.svd(data, full_matrices=False)\n\n    factors = U[:, :n_components].copy()\n    loadings = Vh[:n_components].copy()\n    loadings *= s[:n_components, np.newaxis]\n\n    n_svd_factors = factors.shape[1]\n    if n_svd_factors < n_components:\n        random_factors = random_state.normal(\n            scale=0.01, size=(n_samples, n_components - n_svd_factors))\n        factors = np.concatenate([factors, random_factors], axis=1)\n\n    n_svd_loadings = loadings.shape[0]\n    if n_svd_loadings < n_components:\n        random_loadings = random_state.normal(\n            scale=0.01, size=(n_components - n_svd_loadings, n_targets))\n        loadings = np.concatenate([loadings, random_loadings], axis=0)\n\n    phi = np.zeros((n_components, n_order))\n    ar_coef = phi_step(phi, factors, 1.0, 0., 1.0)\n    return factors, loadings, ar_coef\n\ndef trmf(data, n_components, n_order, C_Z, C_F, C_phi, eta_Z,\n         eta_F=0., adj=None, fit_intercept=False, regressors=None, C_B=0.0,\n         tol=1e-6, n_max_iterations=2500, n_max_mf_iter=5,\n         f_step_kind=\"fgm\", z_step_kind=\"tron\", random_state=None):\n    if not all(C >= 0 for C in (C_Z, C_F, C_phi, C_B)):\n        raise ValueError(\"\"\"Negative ridge regularizer coefficient.\"\"\")\n\n    if not all(0 <= eta <= 1 for eta in (eta_Z, eta_F)):\n        raise ValueError(\"\"\"Share `eta` is not within `[0, 1]`.\"\"\")\n\n    if not (n_components > 0):\n        raise ValueError(\"\"\"Empty latent factors are not supported.\"\"\")\n\n    if C_F > 0 and eta_F > 0:\n        if not sp.issparse(adj):\n            raise TypeError(\"\"\"The adjacency matrix must be sparse.\"\"\")\n\n        # precompute the outbound average dsicrepancy operator\n        adj = precompute_graph_reg(adj)\n    # end if\n\n    # prepare the regressors\n    n_samples, n_targets = data.shape\n    if isinstance(regressors, str):\n        if regressors != \"auto\" or True:\n            raise ValueError(f\"\"\"Invalid regressor setting `{regressors}`\"\"\")\n\n        if sp.issparse(data):\n            raise ValueError(\"\"\"`data` cannot be sparse in \"\"\"\n                             \"\"\"autoregression mode\"\"\")\n\n        # assumes order-1 explicit autoregression\n        regressors, data = data[:-1], data[1:]\n        n_samples, n_targets = data.shape\n\n    elif regressors is None:\n        regressors = np.empty((n_samples, 0))\n\n    # end if\n\n    check_consistent_length(regressors, data)\n\n    # by default the intercept is zero\n    intercept = np.zeros((1, n_targets))\n\n    # initialize the regression coefficients\n    n_regressors = regressors.shape[1]\n    beta = np.zeros((n_regressors, n_targets))\n\n    # prepare smart guesses\n    factors, loadings, ar_coef = trmf_init(data, n_components, n_order,\n                                           random_state=random_state)\n\n    # prepare for estimating the coefs of the exogenous ridge regression\n    if fit_intercept and n_regressors > 0:\n        regressors_mean = regressors.mean(axis=0, keepdims=True)\n        regressors_cntrd = regressors - regressors_mean\n    else:\n        regressors_cntrd = regressors\n    # end if\n\n    # initialize the outer loop\n    ZF, lip_f, lip_b = np.dot(factors, loadings), 500.0, 500.0\n    ZF_old_norm, delta = np.linalg.norm(ZF, ord=\"fro\"), +np.inf\n\n    if sp.issparse(data):\n        # run the trmf algo\n        for iteration in range(n_max_iterations):\n            if (delta <= ZF_old_norm * tol) and (iteration > 0):\n                break\n\n            # Fit the exogenous ridge-regression with an optional intercept\n            if fit_intercept or n_regressors > 0:\n                resid = csr_gemm(-1, factors, loadings, 1, data.copy())\n\n                if fit_intercept:\n                    intercept = csr_column_means(resid)\n\n                if n_regressors > 0:\n                    if fit_intercept:\n                        resid.data -= intercept[0, resid.indices]\n                    # end if\n\n                    # solve for beta\n                    beta, lip_b = b_step(beta, resid, regressors_cntrd, C_B,\n                                         kind=\"tron\")\n\n                    # mean(R) - mean(X) beta = mu\n                    if fit_intercept:\n                        intercept -= np.dot(regressors_mean, beta)\n                    # end if\n                # end if\n\n                # prepare the residuals for the trmf loop\n                resid = data.copy()\n                if n_regressors > 0:\n                    resid = csr_gemm(-1, regressors, beta, 1, resid)\n\n                if fit_intercept:\n                    resid.data -= intercept[0, resid.indices]\n            else:\n                resid = data\n            # end if\n\n            # update (F, Z), then phi\n            for inner_iter in range(n_max_mf_iter):\n                loadings, lip_f = f_step(loadings, resid, factors, C_F, eta_F,\n                                         adj, kind=f_step_kind, lip=lip_f)\n\n                factors = z_step(factors, resid, loadings, ar_coef,\n                                 C_Z, eta_Z, kind=z_step_kind)\n            # end for\n\n            if n_order > 0:\n                ar_coef = phi_step(ar_coef, factors, C_Z, C_phi, eta_Z)\n            # end if\n\n            # recompute the reconstruction and convergence criteria\n            ZF, ZF_old = np.dot(factors, loadings), ZF\n            delta = np.linalg.norm(ZF - ZF_old, ord=\"fro\")\n            ZF_old_norm = np.linalg.norm(ZF_old, ord=\"fro\")\n        # end for\n    else:\n        # run the trmf algo\n        for iteration in range(n_max_iterations):\n            #print(\"loop1\")\n            if (delta <= ZF_old_norm * tol) and (iteration > 0):\n                break\n\n            # Fit the exogenous ridge-regression with an optional intercept\n            if fit_intercept or n_regressors > 0:\n                resid = data - ZF\n                if fit_intercept:\n                    intercept = resid.mean(axis=0, keepdims=True)\n                # end if\n\n                if n_regressors > 0:\n                    if fit_intercept:\n                        resid -= intercept\n                    # end if\n\n                    # solve for beta\n                    beta, lip_b = b_step(beta, resid, regressors_cntrd, C_B,\n                                         kind=\"tron\")\n\n                    # mean(R) - mean(X) beta = mu\n                    if fit_intercept:\n                        intercept -= np.dot(regressors_mean, beta)\n                    # end if\n                # end if\n\n                resid = data.copy()\n                if n_regressors > 0:\n                    resid -= np.dot(regressors, beta)\n\n                if fit_intercept:\n                    resid -= intercept\n            else:\n                resid = data\n            # end if\n\n            # update (F, Z), then phi\n            for inner_iter in range(n_max_mf_iter):\n                #print(\"loop2\")\n                loadings, lip_f = f_step(loadings, resid, factors, C_F, eta_F,\n                                         adj, kind=f_step_kind, lip=lip_f)\n\n                factors = z_step(factors, resid, loadings, ar_coef,\n                                 C_Z, eta_Z, kind=z_step_kind)\n            # end for\n\n            if n_order > 0:\n                ar_coef = phi_step(ar_coef, factors, C_Z, C_phi, eta_Z)\n            # end if\n\n            # recompute the reconstruction and convergence criteria\n            ZF, ZF_old = np.dot(factors, loadings), ZF\n            delta = np.linalg.norm(ZF - ZF_old, ord=\"fro\")\n            ZF_old_norm = np.linalg.norm(ZF_old, ord=\"fro\")\n        # end for\n    # end if\n\n    return factors, loadings, ar_coef, intercept, beta\n\n\n# Modified `In[50]`\ndef trmf_forecast_factors(n_ahead, ar_coef, prehist):\n    n_components, n_order = ar_coef.shape\n    if n_ahead < 1:\n        raise ValueError(\"\"\"`n_ahead` must be a positive integer.\"\"\")\n\n    if len(prehist) < n_order:\n        raise TypeError(\"\"\"Factor history is too short.\"\"\")\n\n    forecast = np.concatenate([\n        prehist[-n_order:] if n_order > 0 else prehist[:0],\n        np.zeros((n_ahead, n_components))\n    ], axis=0)\n\n    # compute the dynamic forecast\n    for h in range(n_order, n_order + n_ahead):\n        # ar_coef are stored in little endian lag order: from lag p to lag 1\n        #  from the least recent to the most recent!\n        forecast[h] = np.einsum(\"il,li->i\", ar_coef, forecast[h - n_order:h])\n\n    return forecast[-n_ahead:]\n\n\n# Extra functionality\ndef trmf_forecast_targets(n_ahead, loadings, ar_coef, intercept, beta,\n                          factors, regressors=None, mode=\"exog\"):\n    n_regressors, n_targets = beta.shape\n    if regressors is None:\n        if n_regressors > 0:\n            raise TypeError(\"\"\"Regressors must be provided.\"\"\")\n        regressors = np.empty((n_ahead, 0))\n\n    #regressors = check_array(regressors, dtype=\"numeric\",\n     #                        accept_sparse=False, ensure_min_features=0)\n\n    if regressors.shape[1] != n_regressors:\n        raise TypeError(\"\"\"Invalid number of regressor features.\"\"\")\n\n    if mode == \"exog\":\n        if regressors.shape[0] < n_ahead:\n            raise TypeError(\"\"\"Not enough future observations.\"\"\")\n\n    elif mode == \"auto\":\n        if n_regressors != n_targets:\n            raise TypeError(\"\"\"Invalid `beta` for mode `auto`.\"\"\")\n\n        if regressors.shape[0] < 1:\n            raise TypeError(\"\"\"Insufficient history of targets.\"\"\")\n    # end if\n\n    # step 1: predict the latent factors\n    forecast = trmf_forecast_factors(n_ahead, ar_coef, factors)\n    factors_forecast = np.dot(forecast, loadings)\n\n    # step 2: predict the targets\n    if mode == \"exog\":\n        # assume the regressors are exogenous\n        targets = np.dot(regressors, beta) + factors_forecast + intercept\n\n    elif mode == \"auto\":\n        # Assume the regressors are order 1 autoregressors (can be\n        #  order-q but needs embedding).\n        targets = np.concatenate([\n            regressors[-1:],\n            np.zeros((n_ahead, n_regressors), dtype=regressors.dtype)\n        ], axis=0)\n\n        # compute the dynamic forecast\n        for h in range(n_ahead):\n            targets[h + 1] = intercept + np.dot(targets[h], beta) \\\n                             + factors_forecast[h]\n        # end for\n    # end if\n\n    return targets[-n_ahead:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TRMFRegressor(BaseEstimator):\n    def __init__(self,\n                 n_components,\n                 n_order,\n                 C_Z=1e-1,\n                 C_F=1e-1,\n                 C_phi=1e-2,\n                 eta_Z=0.5,\n                 eta_F=0.,\n                 adj=None,\n                 C_B=0.0,\n                 fit_regression=False,\n                 fit_intercept=True,\n                 nonnegative_factors=True,\n                 tol=1e-5,\n                 n_max_iterations=1000,\n                 n_max_mf_iter=5,\n                 z_step_kind=\"tron\",\n                 f_step_kind=\"tron\",\n                 random_state=None):\n        super(TRMFRegressor, self).__init__()\n\n        self.n_components = n_components\n        self.n_order = n_order\n        self.C_Z = C_Z\n        self.C_F = C_F\n        self.C_phi = C_phi\n        self.eta_Z = eta_Z\n        self.eta_F = eta_F\n        self.adj = adj\n        self.C_B = C_B\n        self.fit_regression = fit_regression\n        self.fit_intercept = fit_intercept\n        self.tol = tol\n        self.n_max_iterations = n_max_iterations\n        self.n_max_mf_iter = n_max_mf_iter\n        self.nonnegative_factors = nonnegative_factors\n        self.random_state = random_state\n        self.z_step_kind = z_step_kind\n        self.f_step_kind = f_step_kind\n\n    def fit(self, X, y=None, sample_weight=None):\n        if not self.fit_regression:\n            if y is not None:\n                raise TypeError(\"\"\"Exogenous regressors provided in `X`, \"\"\"\n                                \"\"\"yet `fit_regression` is false.\"\"\")\n            X, y = None, X\n\n        else:\n            if y is None:\n                raise TypeError(\"\"\"Endogenous data are is not provided \"\"\"\n                                \"\"\"in `y`, yet `fit_regression` is True.\"\"\")\n        # end if\n\n        if X is None:\n            X = np.empty((y.shape[0], 0))\n        # end if\n\n        check_consistent_length(X, y)\n\n        f_step_kind = \"fgm\" if self.nonnegative_factors else self.f_step_kind\n        estimates = trmf(y, self.n_components, self.n_order, self.C_Z,\n                         self.C_F, self.C_phi, self.eta_Z, self.eta_F,\n                         adj=self.adj, fit_intercept=self.fit_intercept,\n                         regressors=X, C_B=self.C_B, tol=self.tol,\n                         n_max_iterations=self.n_max_iterations,\n                         n_max_mf_iter=self.n_max_mf_iter,\n                         f_step_kind=f_step_kind,\n                         z_step_kind=self.z_step_kind,\n                         random_state=self.random_state)\n\n        # Record the estimates in this instance's properties\n        factors, loadings, ar_coef, intercept, beta = estimates\n\n        self.factors_, self.loadings_ = factors, loadings\n        self.ar_coef_ = ar_coef\n\n        self.coef_, self.intercept_ = beta, intercept\n\n        # self.fitted_ = np.dot(X, beta) + np.dot(factors, loadings) \\\n        #                + intercept\n\n        return self\n    \n    def fit_predict(self, n_ahead):\n        fitted_targets = np.dot(self.factors_,self.loadings_)\n        forecast_targets = trmf_forecast_targets(n_ahead, self.loadings_, self.ar_coef_, self.intercept_, self.coef_,self.factors_)\n        predicted = np.concatenate([fitted_targets,forecast_targets],axis=0)\n        return predicted\n        \n    def forecast_factors(self, n_ahead):\n        return trmf_forecast_factors(n_ahead, self.ar_coef_,\n                                     prehist=self.factors_)\n\n    def predict(self, X=None, n_ahead=10):\n        if self.fit_regression:\n            X = check_array(X, dtype=\"numeric\", accept_sparse=False)\n        else:\n            X = np.empty((n_ahead, 0))\n        # end if\n\n        return trmf_forecast_targets(\n            n_ahead, self.loadings_, self.ar_coef_, self.intercept_,\n            self.coef_, self.factors_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nos.listdir('/kaggle/input/m5-forecasting-accuracy/')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_dir=\"/kaggle/input/m5-forecasting-accuracy/\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"main_data=pd.read_csv(input_dir+'sales_train_evaluation.csv')\nmain_columns=main_data.columns[:6]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"value_column=main_data.columns[6:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"value_column","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def ReturnFirstSellDate(df,main_columns,value_column):\n    main_data=df.copy()\n    First_Sold_Date=pd.DataFrame(np.transpose(np.nonzero(main_data[value_column].values))).drop_duplicates(0).reset_index(drop=True)\n    First_Sold_Date.columns=['Time_Series_Index','First_Date_sold']\n    Feature_vector=pd.concat((main_data[main_columns],First_Sold_Date),axis=1)\n    return Feature_vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"FeatureVector=ReturnFirstSellDate(main_data,main_columns,value_column)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dict_label_encoders={}\ndict_one_hot_encoders={}\nfor col in main_columns[2:]:\n    dict_label_encoders[col]= preprocessing.LabelEncoder()\n    dict_one_hot_encoders[col]= preprocessing.OneHotEncoder()\n    FeatureVector['lb_enc_'+col]=dict_label_encoders[col].fit_transform(main_data[col].values)\n    dict_one_hot_encoders['onehot_enc_'+col]=preprocessing.OneHotEncoder(sparse=False)\n    Base=pd.DataFrame(dict_one_hot_encoders['onehot_enc_'+col].fit_transform(main_data[col].values.reshape(main_data.shape[0],1)))\n    Base.columns=['one_hot_col'+str(j) for j in Base.columns]\n    FeatureVector[Base.columns]=Base\n    ","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"FeatureVector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_data=main_data[value_column[-365*2-28:-28]].values\nX_data.shape\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_data=main_data[value_column[-28:]].values\nY_data.shape","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#Use TRMF W matrix as Catagorical embedding ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import NMF\n","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# X_data=main_data[value_column[-365*2-28:-28]].values\n# for vec in range(100,200,50):\n#     model = NMF(n_components=vec, init='random', random_state=0,l1_ratio=0.5)\n#     W = model.fit_transform(X_data)\n#     #H = model.components_\n#     print(vec,model.reconstruction_err_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model = NMF(n_components=150, init='random', random_state=0,l1_ratio=0.5)\n# W = model.fit_transform(X_data)\n# #H = model.components_\n# print(150,model.reconstruction_err_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# %%time\n# X_data=main_data[value_column].values\n# X_data.shape\n# # 60 12\n# # Score 1.2208639911218397\n# # 90 12\n# # Score 1.2221116627832116\n# # 120 12\n# # Score 1.2243798351230097\n# # CPU times: user 1h 9min 4s, sys: 11min 7s, total: 1h 20min 11s\n# # Wall time: 33min 7s\n\n# for k in [60,120,150]:\n#     for j in [12]:\n#         for p in [False]:\n#             n_components = k\n#             n_order = j\n\n#             params = {\n#                 \"n_components\" : n_components,\n#                 \"n_order\" : n_order,\n#                 \"C_Z\" : 5e1,\n#                 \"C_F\" : 5e-4,\n#                 \"C_phi\" : 1e-4,\n#                 \"eta_Z\" : 0.25,\n#                 \"C_B\" : 0.,\n#                 \"fit_regression\" : False,\n#                 \"fit_intercept\" : False,\n#                 \"nonnegative_factors\" : p,\n#                 \"n_max_iterations\" : 1000,\n#                 \"f_step_kind\" : \"tron\",\n#                 \"tol\" : 1e-5,'random_state':32\n#             }\n#             regressor= TRMFRegressor(**params)\n#             regressor.fit(X_data.T)\n\n#             import gc\n#             from scipy.sparse import csr_matrix\n\n#             calendar = pd.read_csv(input_dir+'calendar.csv')\n#             sell_prices = pd.read_csv(input_dir+'sell_prices.csv')\n#             sales = pd.read_csv(input_dir+'sales_train_validation.csv')\n\n#             def transform(df):\n#                 newdf = df.melt(id_vars=[\"id\"], var_name=\"d\", value_name=\"sale\")\n#                 newdf.sort_values(by=['id', \"d\"], inplace=True)\n#                 newdf.reset_index(inplace=True)\n#                 return newdf\n\n#             from sklearn.metrics import mean_squared_error\n\n#             def rmse(df, gt):\n#                 df = transform(df)\n#                 gt = transform(gt)\n#                 return mean_squared_error(df[\"sale\"], gt[\"sale\"])\n#             sample_submission = pd.read_csv(input_dir+'sample_submission.csv')\n#             sample_submission = sample_submission[sample_submission.id.str.endswith('validation')]\n\n#             NUM_ITEMS = sales.shape[0]    # 30490\n#             DAYS_PRED = sample_submission.shape[1] - 1    # 28\n\n#             # To make it simpler, I will run only the last 10 days\n#             DAYS_PRED = 10\n#             idcols = [\"id\", \"item_id\", \"state_id\", \"store_id\", \"cat_id\", \"dept_id\"]\n#             product = sales[idcols]\n\n#             # create weight matrix\n#             pd.get_dummies(product.state_id, drop_first=False)\n#             weight_mat = np.c_[\n#                np.ones([NUM_ITEMS,1]).astype(np.int8), # level 1\n#                pd.get_dummies(product.state_id, drop_first=False).values,\n#                pd.get_dummies(product.store_id, drop_first=False).values,\n#                pd.get_dummies(product.cat_id, drop_first=False).values,\n#                pd.get_dummies(product.dept_id, drop_first=False).values,\n#                pd.get_dummies(product.state_id + product.cat_id, drop_first=False).values,\n#                pd.get_dummies(product.state_id + product.dept_id, drop_first=False).values,\n#                pd.get_dummies(product.store_id + product.cat_id, drop_first=False).values,\n#                pd.get_dummies(product.store_id + product.dept_id, drop_first=False).values,\n#                pd.get_dummies(product.item_id, drop_first=False).values,\n#                pd.get_dummies(product.state_id + product.item_id, drop_first=False).values,\n#                np.identity(NUM_ITEMS).astype(np.int8) #item :level 12\n#             ].T\n\n#             weight_mat = weight_mat.astype(\"int8\")\n#             weight_mat, weight_mat.shape\n#             weight_mat_csr = csr_matrix(weight_mat)\n#             del weight_mat; gc.collect()\n#             def cal_weight1(product):\n#                 sales_train_val = sales\n#                 d_name = ['d_' + str(i+1) for i in range(1913)]\n\n#                 sales_train_val = weight_mat_csr * sales_train_val[d_name].values\n\n\n#                 df_tmp = ((sales_train_val>0) * np.tile(np.arange(1,1914),(weight_mat_csr.shape[0],1)))\n\n#                 start_no = np.min(np.where(df_tmp==0,9999,df_tmp),axis=1)-1\n\n#                 flag = np.dot(np.diag(1/(start_no+1)) , np.tile(np.arange(1,1914),(weight_mat_csr.shape[0],1)))<1\n\n#                 sales_train_val = np.where(flag, np.nan, sales_train_val)\n\n#                 # denominator of RMSSE / RMSSE\n#                 weight1 = np.nansum(np.diff(sales_train_val,axis=1)**2,axis=1)/(1913-start_no)\n\n#                 return weight1\n\n#             weight1 = cal_weight1(product)\n#             # Get the last 28 days for weight2\n#             cols = [\"d_{}\".format(i) for i in range(1886, 1886+28)]\n\n#             data = sales[[\"id\", 'store_id', 'item_id'] + cols]\n\n#             data = data.melt(id_vars=[\"id\", 'store_id', 'item_id'], var_name=\"d\", value_name=\"sale\")\n#             data = pd.merge(data, calendar, how = 'left', left_on = ['d'], right_on = ['d'])\n#             data = data[[\"id\", 'store_id', 'item_id', \"sale\", \"wm_yr_wk\"]]\n#             data = data.merge(sell_prices, on = ['store_id', 'item_id', 'wm_yr_wk'], how = 'left')\n#             def cal_weight2(data):\n#                 # calculate the sales amount for each item/level\n#                 df_tmp = data\n#                 df_tmp['amount'] = df_tmp['sale'] * df_tmp['sell_price']\n#                 df_tmp =df_tmp.groupby(['id'])['amount'].apply(np.sum)\n#                 df_tmp = df_tmp.values\n\n#                 weight2 = weight_mat_csr * df_tmp \n\n#                 weight2 = weight2/np.sum(weight2)\n#                 return weight2\n\n#             weight2 = cal_weight2(data)\n#             weight2.shape\n#             def wrmsse(preds, y_true):\n#                 # number of columns\n#                 num_col = DAYS_PRED\n\n#                 reshaped_preds = preds.reshape(num_col, NUM_ITEMS).T\n#                 reshaped_true = y_true.reshape(num_col, NUM_ITEMS).T\n\n\n#                 train = weight_mat_csr*np.c_[reshaped_preds, reshaped_true]\n\n#                 score = np.sum(\n#                             np.sqrt(\n#                                 np.mean(\n#                                     np.square(\n#                                         train[:,:num_col] - train[:,num_col:])\n#                                     ,axis=1) / weight1) * weight2)\n\n#                 return score\n#             # WRMSSE score\n#             X_data.shape\n#             all_pred=regressor.predict(n_ahead=28)\n#             forecast_value=all_pred[-28:].T\n#             (forecast_value-Y_data)\n#             FeatureVector['emb_item']=[j for j in regressor.loadings_.T]\n#             regressor.factors_.shape\n#             forecast_value_df=pd.DataFrame(forecast_value)\n#             Y_data_df=pd.DataFrame(Y_data)\n#             forecast_value_df['id']=main_data['id']\n#             Y_data_df['id']=main_data['id']\n#             DAYS_PRED=28\n#             dft = transform(forecast_value_df)\n#             gtt = transform(Y_data_df)\n#             print(n_components,n_order)\n#             print('Score', wrmsse(dft[\"sale\"].to_numpy(), gtt[\"sale\"].to_numpy()))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nX_data=main_data[value_column].values\nX_data.shape\n# 60 12\n# Score 1.2208639911218397\n# 90 12\n# Score 1.2221116627832116\n# 120 12\n# Score 1.2243798351230097\n# CPU times: user 1h 9min 4s, sys: 11min 7s, total: 1h 20min 11s\n# Wall time: 33min 7s\n\nfor k in [60]:\n    for j in [12]:\n        for p in [False]:\n            n_components = k\n            n_order = j\n\n            params = {\n                \"n_components\" : n_components,\n                \"n_order\" : n_order,\n                \"C_Z\" : 5e1,\n                \"C_F\" : 5e-4,\n                \"C_phi\" : 1e-4,\n                \"eta_Z\" : 0.25,\n                \"C_B\" : 0.,\n                \"fit_regression\" : False,\n                \"fit_intercept\" : False,\n                \"nonnegative_factors\" : p,\n                \"n_max_iterations\" : 1000,\n                \"f_step_kind\" : \"tron\",\n                \"tol\" : 1e-5,'random_state':32\n            }\n            regressor= TRMFRegressor(**params)\n            regressor.fit(X_data.T)\n\n            import gc\n            from scipy.sparse import csr_matrix\n\n            calendar = pd.read_csv(input_dir+'calendar.csv')\n            sell_prices = pd.read_csv(input_dir+'sell_prices.csv')\n            sales = pd.read_csv(input_dir+'sales_train_validation.csv')\n\n            def transform(df):\n                newdf = df.melt(id_vars=[\"id\"], var_name=\"d\", value_name=\"sale\")\n                newdf.sort_values(by=['id', \"d\"], inplace=True)\n                newdf.reset_index(inplace=True)\n                return newdf\n\n            from sklearn.metrics import mean_squared_error\n\n            def rmse(df, gt):\n                df = transform(df)\n                gt = transform(gt)\n                return mean_squared_error(df[\"sale\"], gt[\"sale\"])\n            sample_submission = pd.read_csv(input_dir+'sample_submission.csv')\n            sample_submission = sample_submission[sample_submission.id.str.endswith('validation')]\n\n            NUM_ITEMS = sales.shape[0]    # 30490\n            DAYS_PRED = sample_submission.shape[1] - 1    # 28\n\n            # To make it simpler, I will run only the last 10 days\n            DAYS_PRED = 10\n            idcols = [\"id\", \"item_id\", \"state_id\", \"store_id\", \"cat_id\", \"dept_id\"]\n            product = sales[idcols]\n\n            # create weight matrix\n            pd.get_dummies(product.state_id, drop_first=False)\n            weight_mat = np.c_[\n               np.ones([NUM_ITEMS,1]).astype(np.int8), # level 1\n               pd.get_dummies(product.state_id, drop_first=False).values,\n               pd.get_dummies(product.store_id, drop_first=False).values,\n               pd.get_dummies(product.cat_id, drop_first=False).values,\n               pd.get_dummies(product.dept_id, drop_first=False).values,\n               pd.get_dummies(product.state_id + product.cat_id, drop_first=False).values,\n               pd.get_dummies(product.state_id + product.dept_id, drop_first=False).values,\n               pd.get_dummies(product.store_id + product.cat_id, drop_first=False).values,\n               pd.get_dummies(product.store_id + product.dept_id, drop_first=False).values,\n               pd.get_dummies(product.item_id, drop_first=False).values,\n               pd.get_dummies(product.state_id + product.item_id, drop_first=False).values,\n               np.identity(NUM_ITEMS).astype(np.int8) #item :level 12\n            ].T\n\n            weight_mat = weight_mat.astype(\"int8\")\n            weight_mat, weight_mat.shape\n            weight_mat_csr = csr_matrix(weight_mat)\n            del weight_mat; gc.collect()\n            def cal_weight1(product):\n                sales_train_val = sales\n                d_name = ['d_' + str(i+1) for i in range(1913)]\n\n                sales_train_val = weight_mat_csr * sales_train_val[d_name].values\n\n\n                df_tmp = ((sales_train_val>0) * np.tile(np.arange(1,1914),(weight_mat_csr.shape[0],1)))\n\n                start_no = np.min(np.where(df_tmp==0,9999,df_tmp),axis=1)-1\n\n                flag = np.dot(np.diag(1/(start_no+1)) , np.tile(np.arange(1,1914),(weight_mat_csr.shape[0],1)))<1\n\n                sales_train_val = np.where(flag, np.nan, sales_train_val)\n\n                # denominator of RMSSE / RMSSE\n                weight1 = np.nansum(np.diff(sales_train_val,axis=1)**2,axis=1)/(1913-start_no)\n\n                return weight1\n\n            weight1 = cal_weight1(product)\n            # Get the last 28 days for weight2\n            cols = [\"d_{}\".format(i) for i in range(1886, 1886+28)]\n\n            data = sales[[\"id\", 'store_id', 'item_id'] + cols]\n\n            data = data.melt(id_vars=[\"id\", 'store_id', 'item_id'], var_name=\"d\", value_name=\"sale\")\n            data = pd.merge(data, calendar, how = 'left', left_on = ['d'], right_on = ['d'])\n            data = data[[\"id\", 'store_id', 'item_id', \"sale\", \"wm_yr_wk\"]]\n            data = data.merge(sell_prices, on = ['store_id', 'item_id', 'wm_yr_wk'], how = 'left')\n            def cal_weight2(data):\n                # calculate the sales amount for each item/level\n                df_tmp = data\n                df_tmp['amount'] = df_tmp['sale'] * df_tmp['sell_price']\n                df_tmp =df_tmp.groupby(['id'])['amount'].apply(np.sum)\n                df_tmp = df_tmp.values\n\n                weight2 = weight_mat_csr * df_tmp \n\n                weight2 = weight2/np.sum(weight2)\n                return weight2\n\n            weight2 = cal_weight2(data)\n            weight2.shape\n            def wrmsse(preds, y_true):\n                # number of columns\n                num_col = DAYS_PRED\n\n                reshaped_preds = preds.reshape(num_col, NUM_ITEMS).T\n                reshaped_true = y_true.reshape(num_col, NUM_ITEMS).T\n\n\n                train = weight_mat_csr*np.c_[reshaped_preds, reshaped_true]\n\n                score = np.sum(\n                            np.sqrt(\n                                np.mean(\n                                    np.square(\n                                        train[:,:num_col] - train[:,num_col:])\n                                    ,axis=1) / weight1) * weight2)\n\n                return score\n            # WRMSSE score\n            X_data.shape\n            all_pred=regressor.predict(n_ahead=28)\n            forecast_value=all_pred[-28:].T\n            (forecast_value-Y_data)\n            FeatureVector['emb_item']=[j for j in regressor.loadings_.T]\n            regressor.factors_.shape\n            forecast_value_df=pd.DataFrame(forecast_value)\n            Y_data_df=pd.DataFrame(Y_data)\n            forecast_value_df['id']=main_data['id']\n            Y_data_df['id']=main_data['id']\n            DAYS_PRED=28\n            dft = transform(forecast_value_df)\n            gtt = transform(Y_data_df)\n            print(n_components,n_order)\n            print('Score', wrmsse(dft[\"sale\"].to_numpy(), gtt[\"sale\"].to_numpy()))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nX_data.shape\nall_pred=regressor.fit_predict(n_ahead=28)\nforecast_value=all_pred[-28:].T\n(forecast_value-Y_data)\nFeatureVector['emb_item']=[j for j in regressor.loadings_.T]\nregressor.factors_.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"regressor.loadings_.T.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"regressor.factors_.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_pred.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nregressor.forecast_factors(n_ahead=28).shape\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"FeatureVector.to_pickle('FeatureVector_all_60_eval.pkl')\n#regressor.factors_-eval.to_pickle('factors_.pkl')\nnp.save('factors_60_eval', regressor.factors_)\n#regressor.forecast_factors(n_ahead=28).to_pickle('forecast_factors_all.pkl')\nnp.save('forecast_factors_all_60_eval', regressor.forecast_factors(n_ahead=28))\n\n\nnp.save('regressor_loadings__60_eval',regressor.loadings_.T)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"regressor.loadings_.T.to_pickle('regressorloadings_60_eval.pkl')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#FeatureVector.to_pickle('FeatureVector_60.pkl')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#regressor.loadings_.T.to_pickle('regressorloadings.pkl')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}