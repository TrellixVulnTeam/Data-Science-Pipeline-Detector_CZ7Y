{"cells":[{"metadata":{},"cell_type":"markdown","source":"# This kernel is:\n## - Based on [Very fst Model](https://www.kaggle.com/ragnar123/very-fst-model). Thanks [@ragnar123](https://www.kaggle.com/ragnar123).\n## - Automatically uploaded by [push-kaggle-kernel](https://github.com/harupy/push-kaggle-kernel).\n## - Formatted by [Black](https://github.com/psf/black)."},{"metadata":{},"cell_type":"markdown","source":"# Objective\n\n* Make a baseline model that predict the validation (28 days).\n* This competition has 2 stages, so the main objective is to make a model that can predict the demand for the next 28 days."},{"metadata":{"lines_to_next_cell":2,"title":"[code]","trusted":false},"cell_type":"code","source":"import gc\nimport os\nimport warnings\n\nimport pandas as pd\nfrom pandas.plotting import register_matplotlib_converters\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import LabelEncoder\n\nwarnings.filterwarnings(\"ignore\")\npd.set_option(\"display.max_columns\", 500)\npd.set_option(\"display.max_rows\", 500)\nregister_matplotlib_converters()\nsns.set()","execution_count":null,"outputs":[]},{"metadata":{"title":"[code]","trusted":false},"cell_type":"code","source":"import IPython\n\n\ndef display(*dfs, head=True):\n    for df in dfs:\n        IPython.display.display(df.head() if head else df)","execution_count":null,"outputs":[]},{"metadata":{"title":"[code]","trusted":false},"cell_type":"code","source":"def on_kaggle():\n    return \"KAGGLE_KERNEL_RUN_TYPE\" in os.environ","execution_count":null,"outputs":[]},{"metadata":{"title":"[code]","trusted":false},"cell_type":"code","source":"if on_kaggle():\n    os.system(\"pip install --quiet mlflow_extend\")","execution_count":null,"outputs":[]},{"metadata":{"title":"[code]","trusted":false},"cell_type":"code","source":"def reduce_mem_usage(df, verbose=True):\n    numerics = [\"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\n    start_mem = df.memory_usage().sum() / 1024 ** 2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if (\n                    c_min > np.finfo(np.float16).min\n                    and c_max < np.finfo(np.float16).max\n                ):\n                    df[col] = df[col].astype(np.float16)\n                elif (\n                    c_min > np.finfo(np.float32).min\n                    and c_max < np.finfo(np.float32).max\n                ):\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() / 1024 ** 2\n    if verbose:\n        print(\n            \"Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)\".format(\n                end_mem, 100 * (start_mem - end_mem) / start_mem\n            )\n        )\n    return df","execution_count":null,"outputs":[]},{"metadata":{"title":"[code]","trusted":false},"cell_type":"code","source":"def read_data():\n    INPUT_DIR = \"/kaggle/input\" if on_kaggle() else \"input\"\n    INPUT_DIR = f\"{INPUT_DIR}/m5-forecasting-accuracy\"\n\n    print(\"Reading files...\")\n\n    calendar = pd.read_csv(f\"{INPUT_DIR}/calendar.csv\").pipe(reduce_mem_usage)\n    sell_prices = pd.read_csv(f\"{INPUT_DIR}/sell_prices.csv\").pipe(reduce_mem_usage)\n\n    sales_train_val = pd.read_csv(f\"{INPUT_DIR}/sales_train_validation.csv\",).pipe(\n        reduce_mem_usage\n    )\n    submission = pd.read_csv(f\"{INPUT_DIR}/sample_submission.csv\").pipe(\n        reduce_mem_usage\n    )\n\n    print(\"calendar shape:\", calendar.shape)\n    print(\"sell_prices shape:\", sell_prices.shape)\n    print(\"sales_train_val shape:\", sales_train_val.shape)\n    print(\"submission shape:\", submission.shape)\n\n    # calendar shape: (1969, 14)\n    # sell_prices shape: (6841121, 4)\n    # sales_train_val shape: (30490, 1919)\n    # submission shape: (60980, 29)\n\n    return calendar, sell_prices, sales_train_val, submission","execution_count":null,"outputs":[]},{"metadata":{"title":"[code]","trusted":false},"cell_type":"code","source":"calendar, sell_prices, sales_train_val, submission = read_data()\n\nNUM_ITEMS = sales_train_val.shape[0]  # 30490\nDAYS_PRED = submission.shape[1] - 1  # 28","execution_count":null,"outputs":[]},{"metadata":{"lines_to_next_cell":2},"cell_type":"markdown","source":"As [@kaushal2896](https://www.kaggle.com/kaushal2896) suggested in [this comment](https://www.kaggle.com/harupy/m5-baseline#770558), encode the categorical columns before merging to prevent the notebook from crashing even with the full dataset."},{"metadata":{"title":"[code]","trusted":false},"cell_type":"code","source":"def encode_categorical(df, cols):\n    for col in cols:\n        # Leave NaN as it is.\n        le = LabelEncoder()\n        not_null = df[col][df[col].notnull()]\n        df[col] = pd.Series(le.fit_transform(not_null), index=not_null.index)\n\n    return df\n\n\ncalendar = encode_categorical(\n    calendar, [\"event_name_1\", \"event_type_1\", \"event_name_2\", \"event_type_2\"]\n).pipe(reduce_mem_usage)\n\nsales_train_val = encode_categorical(\n    sales_train_val, [\"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"],\n).pipe(reduce_mem_usage)\n\nsell_prices = encode_categorical(sell_prices, [\"item_id\", \"store_id\"]).pipe(\n    reduce_mem_usage\n)","execution_count":null,"outputs":[]},{"metadata":{"title":"[code]","trusted":false},"cell_type":"code","source":"def melt(\n    sales_train_val, submission, nrows=55_000_000, verbose=True,\n):\n    # melt sales data, get it ready for training\n    id_columns = [\"id\", \"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"]\n\n    # get product table.\n    product = sales_train_val[id_columns]\n\n    sales_train_val = sales_train_val.melt(\n        id_vars=id_columns, var_name=\"d\", value_name=\"demand\",\n    )\n\n    sales_train_val = reduce_mem_usage(sales_train_val, verbose=False)\n\n    if verbose:\n        print(\"melted\")\n        display(sales_train_val)\n\n    # separate test dataframes.\n    vals = submission[submission[\"id\"].str.endswith(\"validation\")]\n    evals = submission[submission[\"id\"].str.endswith(\"evaluation\")]\n\n    # change column names.\n    vals.columns = [\"id\"] + [f\"d_{d}\" for d in range(1914, 1914 + DAYS_PRED)]\n    evals.columns = [\"id\"] + [f\"d_{d}\" for d in range(1942, 1942 + DAYS_PRED)]\n\n    # merge with product table\n    evals[\"id\"] = evals[\"id\"].str.replace(\"_evaluation\", \"_validation\")\n    vals = vals.merge(product, how=\"left\", on=\"id\")\n    evals = evals.merge(product, how=\"left\", on=\"id\")\n    evals[\"id\"] = evals[\"id\"].str.replace(\"_validation\", \"_evaluation\")\n\n    if verbose:\n        print(\"validation\")\n        display(vals)\n\n        print(\"evaluation\")\n        display(evals)\n\n    vals = vals.melt(id_vars=id_columns, var_name=\"d\", value_name=\"demand\")\n    evals = evals.melt(id_vars=id_columns, var_name=\"d\", value_name=\"demand\")\n\n    sales_train_val[\"part\"] = \"train\"\n    vals[\"part\"] = \"validation\"\n    evals[\"part\"] = \"evaluation\"\n\n    data = pd.concat([sales_train_val, vals, evals], axis=0)\n\n    del sales_train_val, vals, evals\n\n    data = data.loc[nrows:]\n\n    # delete evaluation for now.\n    data = data[data[\"part\"] != \"evaluation\"]\n\n    gc.collect()\n\n    if verbose:\n        print(\"data\")\n        display(data)\n\n    return data\n\n\ndef extract_d(df):\n    return df[\"d\"].str.extract(r\"d_(\\d+)\").astype(np.int16)\n\n\ndef merge_calendar(data, calendar):\n    calendar = calendar.drop([\"weekday\", \"wday\", \"month\", \"year\"], axis=1)\n    return data.merge(calendar, how=\"left\", on=\"d\").assign(d=extract_d)\n\n\ndef merge_sell_prices(data, sell_prices):\n    return data.merge(sell_prices, how=\"left\", on=[\"store_id\", \"item_id\", \"wm_yr_wk\"])","execution_count":null,"outputs":[]},{"metadata":{"title":"[code]","trusted":false},"cell_type":"code","source":"data = melt(sales_train_val, submission, nrows=27_500_000)\ndel sales_train_val\ngc.collect()\n\ndata = merge_calendar(data, calendar)\ndel calendar\ngc.collect()\n\ndata = merge_sell_prices(data, sell_prices)\ndel sell_prices\ngc.collect()\n\ndata = reduce_mem_usage(data)","execution_count":null,"outputs":[]},{"metadata":{"title":"[code]","trusted":false},"cell_type":"code","source":"def add_demand_features(df):\n    for diff in [0, 1, 2]:\n        shift = DAYS_PRED + diff\n        df[f\"shift_t{shift}\"] = df.groupby([\"id\"])[\"demand\"].transform(\n            lambda x: x.shift(shift)\n        )\n\n    for size in [7, 30, 60, 90, 180]:\n        df[f\"rolling_std_t{size}\"] = df.groupby([\"id\"])[\"demand\"].transform(\n            lambda x: x.shift(DAYS_PRED).rolling(size).std()\n        )\n\n    for size in [7, 30, 60, 90, 180]:\n        df[f\"rolling_mean_t{size}\"] = df.groupby([\"id\"])[\"demand\"].transform(\n            lambda x: x.shift(DAYS_PRED).rolling(size).mean()\n        )\n\n    df[\"rolling_skew_t30\"] = df.groupby([\"id\"])[\"demand\"].transform(\n        lambda x: x.shift(DAYS_PRED).rolling(30).skew()\n    )\n    df[\"rolling_kurt_t30\"] = df.groupby([\"id\"])[\"demand\"].transform(\n        lambda x: x.shift(DAYS_PRED).rolling(30).kurt()\n    )\n    return df\n\n\ndef add_price_features(df):\n    df[\"shift_price_t1\"] = df.groupby([\"id\"])[\"sell_price\"].transform(\n        lambda x: x.shift(1)\n    )\n    df[\"price_change_t1\"] = (df[\"shift_price_t1\"] - df[\"sell_price\"]) / (\n        df[\"shift_price_t1\"]\n    )\n    df[\"rolling_price_max_t365\"] = df.groupby([\"id\"])[\"sell_price\"].transform(\n        lambda x: x.shift(1).rolling(365).max()\n    )\n    df[\"price_change_t365\"] = (df[\"rolling_price_max_t365\"] - df[\"sell_price\"]) / (\n        df[\"rolling_price_max_t365\"]\n    )\n\n    df[\"rolling_price_std_t7\"] = df.groupby([\"id\"])[\"sell_price\"].transform(\n        lambda x: x.rolling(7).std()\n    )\n    df[\"rolling_price_std_t30\"] = df.groupby([\"id\"])[\"sell_price\"].transform(\n        lambda x: x.rolling(30).std()\n    )\n    return df.drop([\"rolling_price_max_t365\", \"shift_price_t1\"], axis=1)\n\n\ndef add_time_features(df, dt_col):\n    df[dt_col] = pd.to_datetime(df[dt_col])\n    attrs = [\n        \"year\",\n        \"quarter\",\n        \"month\",\n        \"week\",\n        \"day\",\n        \"dayofweek\",\n        \"is_year_end\",\n        \"is_year_start\",\n        \"is_quarter_end\",\n        \"is_quarter_start\",\n        \"is_month_end\",\n        \"is_month_start\",\n    ]\n\n    for attr in attrs:\n        dtype = np.int16 if attr == \"year\" else np.int8\n        df[attr] = getattr(df[dt_col].dt, attr).astype(dtype)\n\n    df[\"is_weekend\"] = df[\"dayofweek\"].isin([5, 6]).astype(np.int8)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"title":"[code]","trusted":false},"cell_type":"code","source":"data = add_demand_features(data).pipe(reduce_mem_usage)\ndata = add_price_features(data).pipe(reduce_mem_usage)\ndt_col = \"date\"\ndata = add_time_features(data, dt_col).pipe(reduce_mem_usage)\ndata = data.sort_values(\"date\")\n\nprint(\"start date:\", data[dt_col].min())\nprint(\"end date:\", data[dt_col].max())\nprint(\"data shape:\", data.shape)","execution_count":null,"outputs":[]},{"metadata":{"title":"[code]","trusted":false},"cell_type":"code","source":"class CustomTimeSeriesSplitter:\n    def __init__(self, n_splits=5, train_days=80, test_days=20, day_col=\"d\"):\n        self.n_splits = n_splits\n        self.train_days = train_days\n        self.test_days = test_days\n        self.day_col = day_col\n\n    def split(self, X, y=None, groups=None):\n        SEC_IN_DAY = 3600 * 24\n        sec = (X[self.day_col] - X[self.day_col].iloc[0]) * SEC_IN_DAY\n        duration = sec.max()\n\n        train_sec = self.train_days * SEC_IN_DAY\n        test_sec = self.test_days * SEC_IN_DAY\n        total_sec = test_sec + train_sec\n\n        if self.n_splits == 1:\n            train_start = duration - total_sec\n            train_end = train_start + train_sec\n\n            train_mask = (sec >= train_start) & (sec < train_end)\n            test_mask = sec >= train_end\n\n            yield sec[train_mask].index.values, sec[test_mask].index.values\n\n        else:\n            # step = (duration - total_sec) / (self.n_splits - 1)\n            step = DAYS_PRED * SEC_IN_DAY\n\n            for idx in range(self.n_splits):\n                # train_start = idx * step\n                shift = (self.n_splits - (idx + 1)) * step\n                train_start = duration - total_sec - shift\n                train_end = train_start + train_sec\n                test_end = train_end + test_sec\n\n                train_mask = (sec > train_start) & (sec <= train_end)\n\n                if idx == self.n_splits - 1:\n                    test_mask = sec > train_end\n                else:\n                    test_mask = (sec > train_end) & (sec <= test_end)\n\n                yield sec[train_mask].index.values, sec[test_mask].index.values\n\n    def get_n_splits(self):\n        return self.n_splits","execution_count":null,"outputs":[]},{"metadata":{"title":"[code]","trusted":false},"cell_type":"code","source":"day_col = \"d\"\ncv_params = {\n    \"n_splits\": 2,\n    \"train_days\": int(365 * 2.5),\n    \"test_days\": DAYS_PRED,\n    \"day_col\": day_col,\n}\ncv = CustomTimeSeriesSplitter(**cv_params)","execution_count":null,"outputs":[]},{"metadata":{"title":"[code]","trusted":false},"cell_type":"code","source":"def show_cv_days(cv, X, dt_col, day_col):\n    for ii, (tr, tt) in enumerate(cv.split(X)):\n        print(f\"----- Fold: ({ii + 1} / {cv.n_splits}) -----\")\n        tr_start = X.iloc[tr][dt_col].min()\n        tr_end = X.iloc[tr][dt_col].max()\n        tr_days = X.iloc[tr][day_col].max() - X.iloc[tr][day_col].min() + 1\n\n        tt_start = X.iloc[tt][dt_col].min()\n        tt_end = X.iloc[tt][dt_col].max()\n        tt_days = X.iloc[tt][day_col].max() - X.iloc[tt][day_col].min() + 1\n\n        df = pd.DataFrame(\n            {\n                \"start\": [tr_start, tt_start],\n                \"end\": [tr_end, tt_end],\n                \"days\": [tr_days, tt_days],\n            },\n            index=[\"train\", \"test\"],\n        )\n\n        display(df)\n\n\ndef plot_cv_indices(cv, X, dt_col, lw=10):\n    n_splits = cv.get_n_splits()\n    _, ax = plt.subplots(figsize=(20, n_splits))\n\n    # Generate the training/testing visualizations for each CV split\n    for ii, (tr, tt) in enumerate(cv.split(X)):\n        # Fill in indices with the training/test groups\n        indices = np.array([np.nan] * len(X))\n        indices[tt] = 1\n        indices[tr] = 0\n\n        # Visualize the results\n        ax.scatter(\n            X[dt_col],\n            [ii + 0.5] * len(indices),\n            c=indices,\n            marker=\"_\",\n            lw=lw,\n            cmap=plt.cm.coolwarm,\n            vmin=-0.2,\n            vmax=1.2,\n        )\n\n    # Formatting\n    MIDDLE = 15\n    LARGE = 20\n    ax.set_xlabel(\"Datetime\", fontsize=LARGE)\n    ax.set_xlim([X[dt_col].min(), X[dt_col].max()])\n    ax.set_ylabel(\"CV iteration\", fontsize=LARGE)\n    ax.set_yticks(np.arange(n_splits) + 0.5)\n    ax.set_yticklabels(list(range(n_splits)))\n    ax.invert_yaxis()\n    ax.tick_params(axis=\"both\", which=\"major\", labelsize=MIDDLE)\n    ax.set_title(\"{}\".format(type(cv).__name__), fontsize=LARGE)\n    return ax","execution_count":null,"outputs":[]},{"metadata":{"title":"[code]","trusted":false},"cell_type":"code","source":"sample = data.iloc[::1000][[day_col, dt_col]].reset_index(drop=True)\nshow_cv_days(cv, sample, dt_col, day_col)\nplot_cv_indices(cv, sample, dt_col)\n\ndel sample\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"title":"[code]","trusted":false},"cell_type":"code","source":"features = [\n    \"item_id\",\n    \"dept_id\",\n    \"cat_id\",\n    \"store_id\",\n    \"state_id\",\n    \"event_name_1\",\n    \"event_type_1\",\n    \"snap_CA\",\n    \"snap_TX\",\n    \"snap_WI\",\n    \"sell_price\",\n    # demand features.\n    \"shift_t28\",\n    \"shift_t29\",\n    \"shift_t30\",\n    \"rolling_std_t7\",\n    \"rolling_std_t30\",\n    \"rolling_std_t60\",\n    \"rolling_std_t90\",\n    \"rolling_std_t180\",\n    \"rolling_mean_t7\",\n    \"rolling_mean_t30\",\n    \"rolling_mean_t60\",\n    \"rolling_mean_t90\",\n    \"rolling_mean_t180\",\n    \"rolling_skew_t30\",\n    \"rolling_kurt_t30\",\n    # price features\n    \"price_change_t1\",\n    \"price_change_t365\",\n    \"rolling_price_std_t7\",\n    \"rolling_price_std_t30\",\n    # time features.\n    \"year\",\n    \"month\",\n    \"week\",\n    \"day\",\n    \"dayofweek\",\n    \"is_weekend\",\n]\n\n# prepare training and test data.\n# 2011-01-29 ~ 2016-04-24 : d_1    ~ d_1913\n# 2016-04-25 ~ 2016-05-22 : d_1914 ~ d_1941 (public)\n# 2016-05-23 ~ 2016-06-19 : d_1942 ~ d_1969 (private)\n\nmask = data[\"date\"] <= \"2016-04-24\"\n\n# Attach \"d\" to X_train for cross validation.\nX_train = data[mask][[day_col] + features].reset_index(drop=True)\ny_train = data[mask][\"demand\"].reset_index(drop=True)\nX_test = data[~mask][features].reset_index(drop=True)\n\n# keep these two columns to use later.\nid_date = data[~mask][[\"id\", \"date\"]].reset_index(drop=True)\n\ndel data\ngc.collect()\n\nprint(\"X_train shape:\", X_train.shape)\nprint(\"X_test shape:\", X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"title":"[code]","trusted":false},"cell_type":"code","source":"def train_lgb(bst_params, fit_params, X, y, cv, drop_when_train=None):\n    models = []\n\n    if drop_when_train is None:\n        drop_when_train = []\n\n    for idx_fold, (idx_trn, idx_val) in enumerate(cv.split(X, y)):\n        print(f\"\\n---------- Fold: ({idx_fold + 1} / {cv.get_n_splits()}) ----------\\n\")\n\n        X_trn, X_val = X.iloc[idx_trn], X.iloc[idx_val]\n        y_trn, y_val = y.iloc[idx_trn], y.iloc[idx_val]\n        train_set = lgb.Dataset(X_trn.drop(drop_when_train, axis=1), label=y_trn)\n        val_set = lgb.Dataset(X_val.drop(drop_when_train, axis=1), label=y_val)\n\n        model = lgb.train(\n            bst_params,\n            train_set,\n            valid_sets=[train_set, val_set],\n            valid_names=[\"train\", \"valid\"],\n            **fit_params,\n        )\n        models.append(model)\n\n        del idx_trn, idx_val, X_trn, X_val, y_trn, y_val\n        gc.collect()\n\n    return models","execution_count":null,"outputs":[]},{"metadata":{"title":"[code]","trusted":false},"cell_type":"code","source":"bst_params = {\n    \"boosting_type\": \"gbdt\",\n    \"metric\": \"rmse\",\n    \"objective\": \"regression\",\n    \"n_jobs\": -1,\n    \"seed\": 42,\n    \"learning_rate\": 0.1,\n    \"bagging_fraction\": 0.75,\n    \"bagging_freq\": 10,\n    \"colsample_bytree\": 0.75,\n}\n\nfit_params = {\n    \"num_boost_round\": 100_000,\n    \"early_stopping_rounds\": 50,\n    \"verbose_eval\": 100,\n}\n\nmodels = train_lgb(\n    bst_params, fit_params, X_train, y_train, cv, drop_when_train=[day_col]\n)\n\ndel X_train, y_train\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"title":"[code]","trusted":false},"cell_type":"code","source":"def rmse(y_true, y_pred):\n    return np.sqrt(mean_squared_error(y_true, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"title":"[code]","trusted":false},"cell_type":"code","source":"imp_type = \"gain\"\nimportances = np.zeros(X_test.shape[1])\npreds = np.zeros(X_test.shape[0])\n\nfor model in models:\n    preds += model.predict(X_test)\n    importances += model.feature_importance(imp_type)\n\npreds = preds / cv.get_n_splits()\nimportances = importances / cv.get_n_splits()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# https://github.com/harupy/mlflow-extend"},{"metadata":{"title":"[code]","trusted":false},"cell_type":"code","source":"from mlflow_extend import mlflow, plotting as mplt\n\nwith mlflow.start_run():\n    mlflow.log_params_flatten({\"bst\": bst_params, \"fit\": fit_params, \"cv\": cv_params})\n\n\nfeatures = models[0].feature_name()\n_ = mplt.feature_importance(features, importances, imp_type, limit=30)","execution_count":null,"outputs":[]},{"metadata":{"title":"[code]","trusted":false},"cell_type":"code","source":"def make_submission(test, submission):\n    preds = test[[\"id\", \"date\", \"demand\"]]\n    preds = preds.pivot(index=\"id\", columns=\"date\", values=\"demand\").reset_index()\n    preds.columns = [\"id\"] + [\"F\" + str(d + 1) for d in range(DAYS_PRED)]\n\n    evals = submission[submission[\"id\"].str.endswith(\"evaluation\")]\n    vals = submission[[\"id\"]].merge(preds, how=\"inner\", on=\"id\")\n    final = pd.concat([vals, evals])\n\n    assert final.drop(\"id\", axis=1).isnull().sum().sum() == 0\n    assert final[\"id\"].equals(submission[\"id\"])\n\n    final.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"title":"[code]","trusted":false},"cell_type":"code","source":"make_submission(id_date.assign(demand=preds), submission)","execution_count":null,"outputs":[]}],"metadata":{"jupytext":{"cell_metadata_filter":"title,-all","main_language":"python","notebook_metadata_filter":"-all"}},"nbformat":4,"nbformat_minor":4}