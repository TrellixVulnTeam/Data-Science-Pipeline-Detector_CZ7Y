{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","pycharm":{"is_executing":false},"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport tensorflow as tf\nimport tensorflow.keras as keras\n\nimport os\nimport gc\nimport json","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"is_executing":false,"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"USING_KAGGLE = True\nIS_TRAINING = True\n\nif USING_KAGGLE:\n    INPUT_DIR = '/kaggle/input'\nelse:\n    INPUT_DIR = './input'\n\nPLOT_DIR = './plots/'\n\nif IS_TRAINING:\n    HISTORY_DIR = './history/'\n    MODEL_DIR = './models/'\n\n    for dirname in (HISTORY_DIR, PLOT_DIR, MODEL_DIR):\n        if not os.path.exists(dirname):\n            os.mkdir(dirname)\nelse:\n    HISTORY_DIR = f'{INPUT_DIR}/m5-forecasting-models/history/'\n    MODEL_DIR = f'{INPUT_DIR}/m5-forecasting-models/models/'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data loading","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"PREDICTION_SIZE = 28\n\ndef compute_cum_sum(price_info, row, day_columns, week_columns):\n    weekly_prices = {\n        week: price_info.loc[[week, row['item_id'], row['store_id']],\n            'sell_price'\n        ].values[0]\n        for week in np.unique(week_columns)\n    }\n    \n    daily_sales = row[day_columns].to_numpy()\n    daily_prices = [weekly_prices[week] for week in week_columns]\n\n    return np.inner(daily_sales, daily_prices)\n\n\ndef compute_cum_sales(sales_info, price_info, date_info):\n    day_columns = sales_info.columns[-PREDICTION_SIZE:]\n    week_nos = [date_info.loc[date_info['d'] == day, 'wm_yr_wk'].values[0] for day in day_columns]\n\n    price_info = price_info.loc[price_info['wm_yr_wk'].isin(week_nos)]\n    price_info.set_index(['wm_yr_wk', 'item_id', 'store_id'], inplace=True)\n\n    sales_info['cum_sales'] = sales_info.apply(lambda x: compute_cum_sum(price_info, x, day_columns, week_nos), axis=1)\n\n    print(sales_info.head())","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"is_executing":false},"trusted":true},"cell_type":"code","source":"import time\n\nstart = time.time()\ndf_dates = reduce_mem_usage(pd.read_csv(f'{INPUT_DIR}/m5-forecasting-accuracy/calendar.csv'))\nend = time.time()\nprint(f'Load df_dates in {end - start:.02f}s')\n\nstart = time.time()\ndf_prices = reduce_mem_usage(pd.read_csv(f'{INPUT_DIR}/m5-forecasting-accuracy/sell_prices.csv'))\nend = time.time()\nprint(f'Load df_prices in {end - start:.02f}s')\n\n\nCOMPLETE_SALES_PATH = 'sales_complete.csv'\nUSE_CACHED_SALES = False\n\nif USE_CACHED_SALES and os.path.exists(COMPLETE_SALES_PATH):\n    print('Using df_sales previously computed in this session...')\n    df_sales = pd.read_csv(COMPLETE_SALES_PATH)\nelif USE_CACHED_SALES and os.path.exists(f'{INPUT_DIR}/m5-forecasting-models/{COMPLETE_SALES_PATH}'):\n    print('Using df_sales from dataset...')\n    df_sales = pd.read_csv(f'{INPUT_DIR}/m5-forecasting-models/{COMPLETE_SALES_PATH}')\nelse:\n    start = time.time()\n    df_sales = reduce_mem_usage(pd.read_csv(f'{INPUT_DIR}/m5-forecasting-accuracy/sales_train_evaluation.csv'))\n    middle = time.time()\n    print(f'Load df_sales in {middle - start:.02f}s')\n\n    print('Generating cumulative sales...', end=' ')\n    compute_cum_sales(df_sales, df_prices, df_dates)\n    end = time.time()\n    print(f'done in {end - start:.2f}s')\n\n    df_sales.to_csv(COMPLETE_SALES_PATH)\n\nassert 'cum_sales' in df_sales.columns","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"is_executing":false},"trusted":true},"cell_type":"code","source":"from sklearn import preprocessing\n\ndf_dates['event_type_1'] = df_dates['event_type_1'].fillna('').astype(str)\ndf_dates['event_type_2'] = df_dates['event_type_2'].fillna('').astype(str)\n\nevent_type_encoder = preprocessing.LabelEncoder()\nevent_type_encoder.fit(np.concatenate([df_dates['event_type_1'], df_dates['event_type_2']]))\n\nevent_types_1 = event_type_encoder.transform(df_dates['event_type_1'])\nevent_types_2 = event_type_encoder.transform(df_dates['event_type_2'])\n\ndate_features = np.swapaxes(np.array([\n    event_types_1,\n    event_types_2,\n    df_dates['wday'],\n    df_dates['month'],\n    df_dates['snap_CA'],\n    df_dates['snap_TX'],\n    df_dates['snap_WI'],\n]), 0, 1)\n\nprint(date_features.shape)\n\nNUM_DATE_FEATURES = date_features.shape[1]","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"is_executing":false},"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\ndata_scaler = MinMaxScaler(feature_range=(-1, 1))\ndata_np = data_scaler.fit_transform(np.swapaxes(df_sales.loc[:, 'd_1':].to_numpy(), 0, 1))\n\nprint(data_np.shape)\n\nNUM_PRODUCTS = data_np.shape[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del df_prices\ndel df_dates\n\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class BaseGenerator(keras.utils.Sequence):\n    def __init__(self, data, window_size, prediction_size, num_products):\n        self.data = data\n        self.window_size = window_size\n        self.prediction_size = prediction_size\n        self.num_products = num_products\n\n    def __len__(self):\n        return self.data.shape[0] - self.window_size - self.prediction_size + 1\n\n    def __getitem__(self, index):\n        X_sales = self.data[index:index + self.window_size].reshape((1, self.window_size, self.num_products))\n\n        y = self.data[index + self.window_size:index + self.window_size + self.prediction_size].reshape((1, self.prediction_size, self.num_products))\n\n        return X_sales, y\n\n\nclass DateGenerator(BaseGenerator):\n    def __init__(self, data, date_features, window_size, prediction_size, num_products):\n        super().__init__(data, window_size, prediction_size, num_products)\n        self.date_features = date_features\n\n    def __getitem__(self, index):\n        X_sales, y = super().__getitem__(index)\n        X_dates = np.swapaxes(self.date_features[index + self.window_size:index + self.window_size + self.prediction_size], 0, 1).reshape((1, self.prediction_size, -1))\n\n        return (X_sales, X_dates), y\n\nclass Dataset:\n    def __init__(self, data, window_size, prediction_size, num_products, date_features=None, train_test_ratio=0.75):\n        x_test = np.expand_dims(data[-window_size - prediction_size:-prediction_size, :], 0)\n        y_test = data[-prediction_size:, :]\n\n        x_evaluation = np.expand_dims(data[-window_size:, :], 0)\n        \n        num_training = int((data.shape[0] - window_size - prediction_size) * train_test_ratio)\n        \n        data_train = data[:num_training, :]\n        data_validation = data[num_training:-window_size - prediction_size, :]\n\n        if date_features is None:\n            self.train_generator = BaseGenerator(data_train, window_size, prediction_size, num_products)\n            self.validation_generator = BaseGenerator(data_validation, window_size, prediction_size, num_products)\n\n            self.x_test = x_test\n            self.y_test = y_test\n            self.x_evaluation = x_evaluation\n        else:\n            date_features_train = date_features[:num_training, :]\n            date_features_validation = date_features[num_training:num_training + data_validation.shape[0], :]\n\n            x_dates_test = np.expand_dims(date_features[-2 * prediction_size:-prediction_size, :], 0)\n            x_dates_evaluation = np.expand_dims(date_features[-prediction_size:, :], 0)\n\n            self.train_generator = DateGenerator(data_train, date_features_train, window_size, prediction_size, num_products)\n            self.validation_generator = DateGenerator(data_validation, date_features_validation, window_size, prediction_size, num_products)\n\n            self.x_test = (x_test, x_dates_test)\n            self.y_test = y_test\n            self.x_evaluation = (x_evaluation, x_dates_evaluation)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loss","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import scipy.sparse\nfrom tensorflow.keras import losses\nimport time\n\n\nclass WRMSELoss(losses.Loss):\n    def __init__(self, sales_info, name='WRMSE', use_cached=True):\n        super().__init__(name=name)\n        self.sales_info = sales_info\n        self.level_keys = [\n            (['item_id', 'store_id'], 30490),\n            (['item_id', 'state_id'], 9147),\n            (['item_id'], 3049),\n            (['store_id', 'dept_id'], 70),\n            (['store_id', 'cat_id'], 30),\n            (['state_id', 'dept_id'], 21),\n            (['state_id', 'cat_id'], 9),\n            (['dept_id'], 7),\n            (['cat_id'], 3),\n            (['store_id'], 10),\n            (['state_id'], 3),\n            (None, 1),\n        ]\n        self.use_cached = use_cached\n\n        self.contribution_matrix = self.time_computation(self.compute_contribution_matrix, 'Computing contribution matrix')\n        self.weights = self.time_computation(self.compute_weights, 'Computing weights')\n\n        self.contribution_matrix_tf = self.time_computation(lambda: self.csr_matrix_to_tf(self.contribution_matrix), 'Converting contribution matrix')\n        self.weights_tf = self.time_computation(lambda: tf.convert_to_tensor(self.weights, dtype=tf.float32), 'Converting weights')\n\n    @staticmethod\n    def time_computation(computation, label):\n        print(f'{label}...', end=' ')\n        start = time.time()\n        result = computation()\n        end = time.time()\n        print(f'done in {end - start:.2f}s')\n\n        return result\n\n    def compute_contribution_matrix(self):\n        file_path = 'contribution_matrix.npz'\n\n        if self.use_cached and os.path.exists(file_path):\n            csr = scipy.sparse.load_npz(file_path)\n        elif self.use_cached and os.path.exists(f'{INPUT_DIR}/m5-forecasting-models/{file_path}'):\n            csr = scipy.sparse.load_npz(f'{INPUT_DIR}/m5-forecasting-models/{file_path}')\n        else:\n            num_lowest_level = self.sales_info.shape[0]\n\n            contribution_matrix = [\n                np.identity(num_lowest_level),\n            ]\n\n            for keys, amount in self.level_keys[1:-1]:\n                groups = self.sales_info.groupby(keys)\n                assert len(groups) == amount, f'Found {len(groups)} groups instead of {amount}'\n\n                new_layer = np.zeros((amount, num_lowest_level))\n\n                for index, (group_keys, group_df) in enumerate(groups):\n                    ids = group_df.index.tolist()\n                    new_layer[index, ids] = 1\n\n                contribution_matrix.append(new_layer)\n\n            contribution_matrix.append(np.ones((1, num_lowest_level)))\n\n            concatenated = np.concatenate(contribution_matrix, axis=0)\n            csr = scipy.sparse.csr_matrix(concatenated)\n            \n            del concatenated\n            gc.collect()\n            \n            scipy.sparse.save_npz(file_path, csr)\n\n        return scipy.sparse.csr_matrix(csr)\n\n    @staticmethod\n    def csr_matrix_to_tf(csr_matrix):\n        coo = csr_matrix.tocoo()\n        indices = np.mat([coo.row, coo.col]).transpose()\n        return tf.cast(tf.SparseTensor(indices, coo.data, coo.shape), tf.float32)\n\n    def compute_weights(self):\n        prices = self.sales_info['cum_sales'].to_numpy()\n        leveled_prices = self.contribution_matrix * prices\n\n        total_sum = sum(prices[:self.level_keys[0][1]])\n\n        weights = []\n\n        start = 0\n        for key, amount in self.level_keys:\n            end = start + amount\n\n            current_prices = leveled_prices[start:end]\n            weights.append(current_prices / (len(self.level_keys)) / total_sum)\n\n            start = end\n\n        return np.concatenate(weights, axis=0)\n\n    def call(self, y_true, y_pred):\n        y_true_full = tf.sparse.sparse_dense_matmul(self.contribution_matrix_tf, tf.transpose(tf.squeeze(y_true)))\n        y_pred_full = tf.sparse.sparse_dense_matmul(self.contribution_matrix_tf, tf.transpose(tf.squeeze(y_pred)))\n\n        rmse = tf.sqrt(tf.reduce_sum(tf.math.squared_difference(y_true_full, y_pred_full), axis=1)) / PREDICTION_SIZE\n\n        loss = tf.tensordot(rmse, self.weights_tf, 1)\n\n        return loss","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Models","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras import models, layers\n\n\ndef create_simple_model(window_size, predictions_size, num_products, state_size=512, regularizer=None):\n    model_input = layers.Input(shape=(window_size, num_products))\n    encoder_decoder = layers.LSTM(state_size, kernel_regularizer=regularizer, recurrent_regularizer=regularizer, bias_regularizer=regularizer)(model_input)\n    encoder_decoder = layers.RepeatVector(predictions_size)(encoder_decoder)\n    encoder_decoder = layers.LSTM(state_size, return_sequences=True, kernel_regularizer=regularizer, recurrent_regularizer=regularizer, bias_regularizer=regularizer)(encoder_decoder)\n    encoder_decoder = layers.TimeDistributed(layers.Dense(num_products, kernel_regularizer=regularizer, bias_regularizer=regularizer))(encoder_decoder)\n    \n    model = models.Model(inputs=model_input, outputs=encoder_decoder)\n\n    return model\n\n\ndef create_model_with_dates(window_size, predictions_size, num_products, num_date_features, state_size=512, regularizer=None):\n    input_encoder_decoder = layers.Input(shape=(window_size, num_products))\n    encoder_decoder = layers.LSTM(state_size, kernel_regularizer=regularizer, recurrent_regularizer=regularizer, bias_regularizer=regularizer)(input_encoder_decoder)\n    encoder_decoder = layers.RepeatVector(predictions_size)(encoder_decoder)\n    encoder_decoder = layers.LSTM(state_size, return_sequences=True, kernel_regularizer=regularizer, recurrent_regularizer=regularizer, bias_regularizer=regularizer)(encoder_decoder)\n    encoder_decoder = models.Model(inputs=input_encoder_decoder, outputs=encoder_decoder)\n    \n    input_date_branch = layers.Input(shape=(predictions_size, num_date_features))\n\n    output = layers.concatenate([encoder_decoder.output, input_date_branch])\n    output = layers.TimeDistributed(layers.Dense(num_products, kernel_regularizer=regularizer, bias_regularizer=regularizer))(output)\n\n    model = models.Model(inputs=[encoder_decoder.input, input_date_branch], outputs=output)\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras import regularizers\n\nWINDOW_SIZE = 100\nWINDOW_SIZE_LARGE = 200\nPREDICTION_SIZE = 28\n\nMSE_LABEL = 'mean_squared_error'\nMSE_LOSS = MSE_LABEL\nWRMSE_LABEL = 'wrmse'\nWRMSE_LOSS = WRMSELoss(df_sales, WRMSE_LABEL)\n\nREGULARIZER = regularizers.l2()\n\nbase_dataset = Dataset(data_np, WINDOW_SIZE, PREDICTION_SIZE, NUM_PRODUCTS)\ndates_dataset = Dataset(data_np, WINDOW_SIZE, PREDICTION_SIZE, NUM_PRODUCTS, date_features=date_features)\nlarge_dataset = Dataset(data_np, WINDOW_SIZE_LARGE, PREDICTION_SIZE, NUM_PRODUCTS)\n\nALL_MODELS = {\n    'base_model': {\n        'model': create_simple_model(WINDOW_SIZE, PREDICTION_SIZE, NUM_PRODUCTS, state_size=100),\n        'dataset': base_dataset,\n        'loss_name': MSE_LABEL,\n        'loss': MSE_LOSS,\n        'name': 'Basic model (MSE loss)',\n    },\n    'large_model_mse': {\n        'model': create_simple_model(WINDOW_SIZE, PREDICTION_SIZE, NUM_PRODUCTS),\n        'dataset': base_dataset,\n        'loss_name': MSE_LABEL,\n        'loss': MSE_LOSS,\n        'name': 'Large model (MSE loss)',\n    },\n    'large_model': {\n        'model': create_simple_model(WINDOW_SIZE, PREDICTION_SIZE, NUM_PRODUCTS),\n        'dataset': base_dataset,\n        'loss_name': WRMSE_LABEL,\n        'loss': WRMSE_LOSS,\n        'name': 'Large model (custom WRMSE loss)',\n    },\n    'large_model_regularizer': {\n        'model': create_simple_model(WINDOW_SIZE, PREDICTION_SIZE, NUM_PRODUCTS, regularizer=REGULARIZER),\n        'dataset': base_dataset,\n        'loss_name': WRMSE_LABEL,\n        'loss': WRMSE_LOSS,\n        'name': 'Large model (custom WRMSE loss + regularizer)',\n    },\n    'large_model_large_window': {\n        'model': create_simple_model(WINDOW_SIZE_LARGE, PREDICTION_SIZE, NUM_PRODUCTS),\n        'dataset': large_dataset,\n        'loss_name': WRMSE_LABEL,\n        'loss': WRMSE_LOSS,\n        'name': 'Large model (custom WRMSE loss + large window)',\n    },\n    'large_model_large_window_regularizer': {\n        'model': create_simple_model(WINDOW_SIZE_LARGE, PREDICTION_SIZE, NUM_PRODUCTS, regularizer=REGULARIZER),\n        'dataset': large_dataset,\n        'loss_name': WRMSE_LABEL,\n        'loss': WRMSE_LOSS,\n        'name': 'Large model (custom WRMSE loss + large window + regularizer)',\n    },\n    'large_model_dates': {\n        'model': create_model_with_dates(WINDOW_SIZE, PREDICTION_SIZE, NUM_PRODUCTS, NUM_DATE_FEATURES),\n        'dataset': dates_dataset,\n        'loss_name': WRMSE_LABEL,\n        'loss': WRMSE_LOSS,\n        'name': 'Large model (custom WRMSE loss + date features)',\n    },\n    'large_model_dates_regularizer': {\n        'model': create_model_with_dates(WINDOW_SIZE, PREDICTION_SIZE, NUM_PRODUCTS, NUM_DATE_FEATURES, regularizer=REGULARIZER),\n        'dataset': dates_dataset,\n        'loss_name': WRMSE_LABEL,\n        'loss': WRMSE_LOSS,\n        'name': 'Large model (custom WRMSE loss + date features + regularizer)',\n    },\n    'large_model_large_window_dates_regularizer': {\n        'model': create_model_with_dates(WINDOW_SIZE, PREDICTION_SIZE, NUM_PRODUCTS, NUM_DATE_FEATURES, regularizer=REGULARIZER),\n        'dataset': dates_dataset,\n        'loss_name': WRMSE_LABEL,\n        'loss': WRMSE_LOSS,\n        'name': 'Large model (custom WRMSE loss + large window + date features + regularizer)',\n    },\n}","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"is_executing":false,"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"from tensorflow.keras import optimizers, callbacks\n\n\ndef train_model(slug, model):\n    print(f'Training model: \"{model[\"name\"]}\"')\n\n    model['model'].summary()\n    model['model'].compile(loss=model['loss'], optimizer=optimizers.Adam(0.001), metrics=[MSE_LOSS, WRMSE_LOSS])\n    \n    val_metric_name = f'val_{model[\"loss_name\"]}'\n    model_location = f'{MODEL_DIR}{slug}.h5'\n\n    early_stopping = callbacks.EarlyStopping(monitor=val_metric_name, patience=15, mode='min', verbose=1)\n    model_checkpoint = callbacks.ModelCheckpoint(model_location, monitor=val_metric_name, save_best_only=True, save_weights_only=True, mode='min', verbose=1)\n    reduce_lr = callbacks.ReduceLROnPlateau(monitor=val_metric_name, patience=4, mode='min', verbose=1)\n\n    history = model['model'].fit_generator(\n        model['dataset'].train_generator,\n        validation_data=model['dataset'].validation_generator,\n        epochs=50,\n        callbacks=[early_stopping, model_checkpoint, reduce_lr]\n    )\n\n    hist_df = pd.DataFrame(history.history)\n    hist_df.to_csv(f'{HISTORY_DIR}{slug}.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"is_executing":false},"trusted":true},"cell_type":"code","source":"tf.keras.backend.set_floatx('float32')\n\nif IS_TRAINING:\n    for slug, model in ALL_MODELS.items():\n        train_model(slug, model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Evaluation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef plot_learning_curve(slug, model):\n    df_history = pd.read_csv(f'{HISTORY_DIR}{slug}.csv')\n\n    epochs = list(range(1, len(df_history.index) + 1))\n\n    plt.plot(epochs, df_history['loss'], label='Training loss')\n    plt.plot(epochs, df_history['val_loss'], label='Validation loss')\n    plt.plot(epochs, df_history['wrmse'], label='Training WRMSE')\n    plt.plot(epochs, df_history['val_wrmse'], label='Validation WRMSE')\n    plt.legend()\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.title(model[\"name\"])\n\n    plt.show()\n    plt.savefig(f'{PLOT_DIR}{slug}.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for slug, model in ALL_MODELS.items():\n    plot_learning_curve(slug, model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for slug, model in ALL_MODELS.items():\n    model['model'].load_weights(f'{MODEL_DIR}{slug}.h5')\n\n    actual = tf.convert_to_tensor(data_scaler.inverse_transform(model['dataset'].y_test), dtype=tf.float32)\n    prediction = tf.convert_to_tensor(data_scaler.inverse_transform(model['model'].predict(model['dataset'].x_test).squeeze()), dtype=tf.float32)\n\n    score = WRMSE_LOSS.call(actual, prediction).numpy()\n\n    print(model['name'], 'WRMSE:', score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission","execution_count":null},{"metadata":{"pycharm":{"is_executing":false},"trusted":true},"cell_type":"code","source":"BEST_MODEL = 'large_model_large_window_regularizer'\n\nmodel = ALL_MODELS[BEST_MODEL]\nmodel['model'].load_weights(f'{MODEL_DIR}{BEST_MODEL}.h5')\n\nmodel['model'].summary()\n\ny_validation = data_scaler.inverse_transform(model['model'].predict(model['dataset'].x_test).squeeze()).T\ny_evaluation = data_scaler.inverse_transform(model['model'].predict(model['dataset'].x_evaluation).squeeze()).T\n\ny_full = np.concatenate([y_validation, y_evaluation], axis=0)\n\npred_df = pd.DataFrame(y_full, columns=[f'F{i}' for i in range(1, PREDICTION_SIZE + 1)])\n\nevaluation_ids = df_sales['id'].values\nvalidation_ids = [i.replace('evaluation', 'validation') for i in evaluation_ids]\nall_ids = np.concatenate([validation_ids, evaluation_ids])\n\npred_df.insert(0, 'id', all_ids)\n\nprint(pred_df.head())\n\npred_df.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}