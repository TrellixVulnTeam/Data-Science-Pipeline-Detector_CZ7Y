{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nfrom pandas.tseries.holiday import USFederalHolidayCalendar\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.externals import joblib\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\nfrom bayes_opt import BayesianOptimization\nfrom datetime import datetime\nfrom datetime import timedelta\nimport calendar\nimport gc\nimport os\nimport time","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_stv = pd.read_csv('../input/m5-forecasting-accuracy/sales_train_validation.csv')\nprint(f'Item length : {len(df_stv)}')\n\ndf_sp = pd.read_csv('../input/m5-forecasting-accuracy/sell_prices.csv')\nprint(f'Item length : {len(df_sp)}')\n\ndf_calendar = pd.read_csv('../input/m5-forecasting-accuracy/calendar.csv')\ndf_calendar['date'] = pd.to_datetime(df_calendar['date'], format='%Y-%m-%d')\nprint(f'calendar length : {len(df_calendar)}')\n\n# externel dataset\ndf_holiday = pd.read_csv('../input/federal-holidays-usa-19662020/usholidays.csv', index_col=0)\ndf_holiday.columns = ['date', 'holiday']\ndf_holiday['date'] = pd.to_datetime(df_holiday['date'], format='%Y-%m-%d')\n\n# oil prices dataset\ndf_oil_US = pd.read_csv('../input/m5-external-data/Weekly_US_Gasoline_Diesel_Prices.csv', header=6)\ndf_oil_CA = pd.read_csv('../input/m5-external-data/Weekly_California_Gasoline_Prices.csv', header=6)\ndf_oil_WI = pd.read_csv('../input/m5-external-data/Weekly_Minnesota_Gasoline_Prices.csv', header=6)\ndf_oil_TX = pd.read_csv('../input/m5-external-data/Weekly_Texas_Gasoline_Prices.csv', header=6)\ndf_oil_US.columns = ['date', 'US_diesel', 'US_gasoline']\ndf_oil_CA.columns = ['date', 'gasoline']\ndf_oil_WI.columns = ['date', 'gasoline']\ndf_oil_TX.columns = ['date', 'gasoline']\ndf_oil_US['date'] = pd.to_datetime(df_oil_US['date'], format='%m/%d/%Y')\ndf_oil_CA['date'] = pd.to_datetime(df_oil_CA['date'], format='%m/%d/%Y')\ndf_oil_WI['date'] = pd.to_datetime(df_oil_WI['date'], format='%m/%d/%Y')\ndf_oil_TX['date'] = pd.to_datetime(df_oil_TX['date'], format='%m/%d/%Y')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission = pd.read_csv('../input/m5-forecasting-accuracy/sample_submission.csv')\nsample_submission.set_index('id', inplace=True)\nsample_submission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_calendar","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_holiday","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_preproc_data(stv, calendar, sp, start_day):\n    start = time.time()\n    gc.collect()\n    \n    global df_oil_US, df_oil_CA, df_oil_WI, df_oil_TX\n    \n    for i in range(1, 57):\n        stv['d_'+str(i+1913)] = np.zeros(len(stv))\n    \n    # 1913일 판매 데이터를 row-wise로 변환\n    data_ = pd.melt(stv,\n                   id_vars=stv.columns[:6],\n                   value_vars=stv.columns[5+start_day:],\n                   var_name='d',\n                   value_name='solds')\n    \n    \n    def shift_with_copy(df, back_shift=-7):\n        a = df.fillna('').copy()\n        for n in range(-1, back_shift-1, -1):\n            b = df.shift(n).fillna('')\n            for i in range(len(a)):\n                a[i] += b[i]\n        return a\n    \n    # event정보 처리\n    calendar['event_name_1'] = shift_with_copy(calendar['event_name_1'], -7)\n    calendar['event_type_1'] = shift_with_copy(calendar['event_type_1'], -7)\n    calendar['event_name_2'] = shift_with_copy(calendar['event_name_2'], -7)\n    calendar['event_type_2'] = shift_with_copy(calendar['event_type_2'], -7)\n    \n    # holiday 추가 / 처리\n    calendar = calendar.merge(df_holiday, on='date', how='left')\n    calendar['holiday'] = shift_with_copy(calendar['holiday'], -7)\n    \n    # melting을 위해 SNAP정보를 뒤로\n    c = list(calendar.columns)\n    calendar = calendar[c[:11]+[c[-1]]+c[11:14]]\n    \n    \n    # SNAP 정보 row-wise 변환\n    calendar = pd.melt(calendar,\n                   id_vars=calendar.columns[:12],\n                   value_vars=calendar.columns[12:],\n                   var_name='state_id',\n                   value_name='SNAP'\n                  )\n\n    # state를 KEY값으로 활용할 수 있도록 처리\n    calendar['state_id'] = calendar['state_id'].apply(lambda x: x[-2:])\n    # SNAP값 binarize\n    calendar['SNAP'] = calendar['SNAP'].apply(lambda x: x==1)\n    \n    # calendar 매핑\n    data_ = data_.merge(calendar, on=['d', 'state_id'], how='inner', copy=False)\n    \n    # df_sp 매핑\n    data_ = data_.merge(sp, on=['store_id', 'item_id', 'wm_yr_wk'], how='left', copy=False)\n    \n    # 비어있는 sell_price -> 재고 없음\n    # 재고없음은 0으로 처리\n    data_['sell_price'] = data_['sell_price'].fillna(0)\n    \n    \n    # 유가 4주 shift\n    df_oil_US['US_diesel'] = df_oil_US['US_diesel'].shift(4)\n    df_oil_US['US_gasoline'] = df_oil_US['US_gasoline'].shift(4)\n    df_oil_CA['gasoline'] = df_oil_CA['gasoline'].shift(4)\n    df_oil_WI['gasoline'] = df_oil_WI['gasoline'].shift(4)\n    df_oil_TX['gasoline'] = df_oil_TX['gasoline'].shift(4)\n\n    # calendar와 병합을 위해 날짜 동기화\n    start_date = calendar.loc[0, 'date']\n    end_date = calendar.loc[len(calendar)-1, 'date'] + timedelta(days=1)\n    df_oil_US = df_oil_US[(df_oil_US['date'] >= start_date) & (df_oil_US['date'] <= end_date)]\n    df_oil_CA = df_oil_CA[(df_oil_CA['date'] >= start_date) & (df_oil_CA['date'] <= end_date)]\n    df_oil_WI = df_oil_WI[(df_oil_WI['date'] >= start_date) & (df_oil_WI['date'] <= end_date)]\n    df_oil_TX = df_oil_TX[(df_oil_TX['date'] >= start_date) & (df_oil_TX['date'] <= end_date)]\n    df_oil_US['date'] = df_oil_US['date'] - timedelta(days=1)\n    df_oil_CA['date'] = df_oil_CA['date'] - timedelta(days=1)\n    df_oil_WI['date'] = df_oil_WI['date'] - timedelta(days=1)\n    df_oil_TX['date'] = df_oil_TX['date'] - timedelta(days=1)\n\n    df_oil_CA['state_id'] = 'CA'\n    df_oil_WI['state_id'] = 'WI'\n    df_oil_TX['state_id'] = 'TX'\n    df_oil_state = pd.concat([df_oil_CA, df_oil_TX, df_oil_WI], axis=0)\n\n    # 유가정보의 date를 calendar의 wm_yr_wk로 변환\n    df_oil_US['wm_yr_wk'] = df_oil_US['date'].apply(lambda x:calendar[calendar['date']==x].iloc[0, 1])\n    df_oil_state['wm_yr_wk'] = df_oil_state['date'].apply(lambda x:calendar[calendar['date']==x].iloc[0, 1])\n    \n    # US, state별 전체 유가정보를 data와 병합\n    data_ = data_.merge(df_oil_US.drop(columns='date'), on='wm_yr_wk', how='left')\n    data_ = data_.merge(df_oil_state.drop(columns='date'), on=['wm_yr_wk', 'state_id'], how='left')\n    \n    # 유가 정보를 28days fore-shift\n                        \n    # 필요없는 컬럼을 지워보자\n    data_ = data_.drop(columns=['weekday', 'year'])\n\n    # 컬럼 타입변환\n    data_['d'] = data_['d'].apply(lambda x: x[2:]).astype(np.uint16)\n    dict_convert = {'item_id':'category',\n                    'dept_id':'category',\n                    'cat_id':'category',\n                    'store_id':'category',\n                    'state_id':'category',\n                    'solds':np.uint16,\n                    'wday':'category',\n                    'month':'category',\n                    'date':'datetime64',\n                    'SNAP':'category',\n                    'sell_price':np.float32,\n                    'event_name_1':'category',\n                    'event_type_1':'category',\n                    'event_name_2':'category',\n                    'event_type_2':'category',\n                    'holiday':'category'\n                   }\n\n    display(data_)\n    for c, t in dict_convert.items():\n        data_[c] = data_[c].astype(t)\n    \n    print('전처리 종료')\n    print(f'Preprocessing --> running time : {str(timedelta(seconds=time.time() - start))}')\n    \n    return reduce_mem_usage(data_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def m5_fe(data_):\n    start = time.time()\n    # feature engineering\n    # sell_price\n    data_.sort_values(by=['id', 'd'], inplace=True)\n    \n    # solds feature\n    for s in [28]:\n        for r in [7, 14, 28]:\n            data_[f'mean_solds_{s}s_{r}r'] = data_.groupby(['id'])['solds'].shift(s).rolling(r, min_periods=1).mean().reset_index(0, drop=True)\n    \n    # sell_price feature\n    for r in [7, 14, 28]:\n        data_[f'mean_sp_{r}r'] = data_.groupby(['id'])['sell_price'].rolling(r, min_periods=1).mean().reset_index(0, drop=True)\n            \n    # oil prices feature\n    for r in [7, 14, 28]:\n        data_[f'mean_US_diesel_{r}r'] = data_.groupby(['id'])['US_diesel'].rolling(r, min_periods=1).mean().reset_index(0, drop=True)\n        data_[f'mean_US_gasoline_{r}r'] = data_.groupby(['id'])['US_gasoline'].rolling(r, min_periods=1).mean().reset_index(0, drop=True)\n        data_[f'mean_gasoline_{r}r'] = data_.groupby(['id'])['gasoline'].rolling(r, min_periods=1).mean().reset_index(0, drop=True)\n    \n    # release-day feature\n    def get_rd(sp_per_id):\n        ret = np.zeros(len(sp_per_id))\n        cur_id = sp_per_id.iloc[0, 0]\n        day_cnt = 0\n        for i, (_id, sp) in enumerate(zip(sp_per_id['id'], sp_per_id['sell_price'])):\n            if _id != cur_id:\n                cur_id = _id\n                day_cnt = 0\n            if sp != 0:\n                ret[i] = day_cnt\n                day_cnt += 1\n            else:\n                day_cnt = 0\n        return ret\n    \n    data_['rd'] = get_rd(data_[['id', 'sell_price']])\n    print('rd features')\n\n    # price-steady-period feature\n    def get_psp(sp_per_id):\n        ret = np.zeros(len(sp_per_id))\n        cur_id = sp_per_id.iloc[0, 0]\n        cur_sp = sp_per_id.iloc[0, 1]\n        day_cnt = 0\n        for i , (_id, sp) in enumerate(zip(sp_per_id['id'], sp_per_id['sell_price'])):\n            if _id != cur_id:\n                cur_id = _id\n                day_cnt = 0\n            if sp != cur_sp or sp == 0:\n                cur_sp = sp\n                day_cnt = 0\n            else:\n                day_cnt += 1\n                ret[i] = day_cnt\n        return ret\n \n    data_['psp'] = get_psp(data_[['id', 'sell_price']])\n    print('price steady period')\n    \n    display(data_)\n    display(data_.dtypes)\n    \n    print(f'Feature engineering --> running time : {str(timedelta(seconds=time.time() - start))}')\n    \n    return reduce_mem_usage(data_.dropna())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"START_DAY = 1000\ndata = get_preproc_data(df_stv, df_calendar, df_sp, START_DAY)\ndata = m5_fe(data)\ncat_features = [f for f in data.columns if data[f].dtype.name == 'category']\n\n#del df_sp, df_stv, df_calendar\ngc.collect()\ndata.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Bayesian hyperparameter optimization\ndef bayesian_opt(data_, start_val=1800, init_iter=5, n_iter=10, num_iterations=1000):\n    L_RATE = 0.04\n    train_set = data_[data_['d']<start_val]\n    valid_set = data_[(data_['d']>=start_val) & (data_['d']<=1913)]\n\n    # make lgb.Dataset\n    train_data = lgb.Dataset(train_set[train_set.columns.drop(['id', 'solds', 'date'])],\n                                 label=train_set['solds'],\n                                 categorical_feature=cat_features,\n                                 free_raw_data=False)\n    valid_data = lgb.Dataset(valid_set[valid_set.columns.drop(['id', 'solds', 'date'])],\n                             label=valid_set['solds'],\n                             categorical_feature=cat_features,\n                             free_raw_data=False)\n    def hyp_lgbm(num_leaves, feature_fraction, bagging_fraction, lambda_l1, lambda_l2):\n        # default params\n        params = {'application':'regression',\n                 'num_iterations':num_iterations,\n                 'learning_rate':0.01,\n                 'early_stopping_round':10,\n                 'metric':'rmse'}\n        # modifying\n        params['num_leaves'] = int(round(num_leaves))\n        params['feature_fraction'] = max(min(feature_fraction, 1), 0)\n        params['bagging_fraction'] = max(min(bagging_fraction, 1), 0)\n        params['lambda_l1'] = lambda_l1\n        params['lambda_l2'] = lambda_l2\n        \n        # fit\n        model = lgb.train(params,\n                     train_set=train_data,\n                     valid_sets=[valid_data],\n                     verbose_eval=None)\n        return -1 * model.best_score['valid_0']['rmse']\n    \n    # hyperparams' range\n    pds = {'num_leaves':(30, 200),\n          'feature_fraction':(0.5, 1),\n          'bagging_fraction':(0.2, 0.8),\n          'lambda_l1': (0.0, 0.95),\n          'lambda_l2': (0.0, 0.95)\n          }\n    \n    # surrogate model\n    optimizer = BayesianOptimization(hyp_lgbm, pds, random_state=42)\n    \n    # optimize\n    optimizer.maximize(init_points=init_iter, n_iter=n_iter)\n    \n    return optimizer\n    \nopt = bayesian_opt(data, 1800)\nopt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"opt.max","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"####################################\n# Split train-valid dataset\n# d_0 ~ d_1913 : train+validation\n# d_1914 ~ d_1969 : test\n####################################\nL_RATE = 0.01\nNUM_ITER = 5000\nstart_val = 1913\n\nparams = {\n        'boosting_type': 'gbdt',\n        'metric': 'rmse',\n        'objective': 'regression',\n        'n_jobs': -1,\n        'seed': 236,\n        'learning_rate': L_RATE,\n        'bagging_fraction': 0.2,\n        'feature_fraction': 1.0,\n        'max_depth':-1,\n        'lambda_l1':0.95,\n        'lambda_l2':0.0,\n        'num_leaves':256,\n        'verbose':1}\n\ntrain_set = data[data['d']<=start_val]\nvalid_set = data[(data['d']>1800)&(data['d']<=1913)]\nX_test = data[(data['d']>1913)&(data['d']<=1941)].drop(columns=['solds', 'date'])\ndel data\ngc.collect()\n\n# make lgb.Dataset\ntrain_data = lgb.Dataset(train_set[train_set.columns.drop(['id', 'solds', 'date'])],\n                             label=train_set['solds'],\n                             categorical_feature=cat_features)\n\nvalid_data = lgb.Dataset(valid_set[valid_set.columns.drop(['id', 'solds', 'date'])],\n                         label=valid_set['solds'],\n                         categorical_feature=cat_features)\n\ndel train_set, valid_set\ngc.collect()\n\n# fit\nmodel = lgb.train(params,\n                 train_set=train_data,\n                 valid_sets=[valid_data],\n                 num_boost_round=NUM_ITER,\n                 early_stopping_rounds=250,\n                 verbose_eval=100)\n\nmodel","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10, 10))\nplt.xticks(color='y')\nplt.yticks(color='y')\nf_importance = pd.DataFrame(data={'fname':model.feature_name(), 'fval':model.feature_importance()})\nf_importance.sort_values(by='fval', ascending=False, inplace=True)\nsns.barplot(x='fval', y='fname', data=f_importance)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test['pred_solds'] = model.predict(X_test.drop(columns=['id']))\nsubmission = X_test[['id', 'd', 'pred_solds']].pivot(index='id', columns='d').iloc[:, :28]\nsubmission.columns = ['F'+str(i) for i in range(1, 29)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_submission = sample_submission + submission\nmy_submission = my_submission.loc[sample_submission.index, :].fillna(value=0.)\nmy_submission = np.maximum(0, my_submission)\nmy_submission.to_csv('submission.csv')\nmy_submission","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}