{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This is my commonly-used standard data science libraries\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n\n# Basic package\nimport re\nimport itertools\nimport datetime\nfrom dateutil import parser\nimport os\nimport operator\n\n# DS basic\nfrom scipy import stats\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O\n\n\n# Viz\nimport matplotlib as mpl\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\npd.set_option('max_columns', 50)\n#pd.options.display.max_columns = None\nplt.style.use('bmh')\ncolor_pal = plt.rcParams['axes.prop_cycle'].by_key()['color']\ncolor_cycle = itertools.cycle(plt.rcParams['axes.prop_cycle'].by_key()['color'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"# Acknowledgements\n\n1. [M5 Competition : EDA + Models ðŸ“ˆ ](https://www.kaggle.com/tarunpaparaju/m5-competition-eda-models) ~ by Rob Mulla\n2. [EDA and Baseline Model ](https://www.kaggle.com/rdizzl3/eda-and-baseline-model)\n3. [Mean encodings and PCA options](https://www.kaggle.com/kyakovlev/m5-custom-features)\n4. [Lags and rolling lags](https://www.kaggle.com/kyakovlev/m5-lags-features)\n5. [Base Grid and base features (calendar/price/etc)](https://www.kaggle.com/kyakovlev/m5-simple-fe)\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Understand data and the task\n* There are two parallel competitions: Accuracy and Uncertainty\n     + The accuracy competition will use the metric: Weighted Root Mean Squared Scaled Error (RMSSE)\n     + The uncertainty competition will use the metric: Weighted Scaled Pinball Loss (WSPL)\n\n\n* The sales data from Wal-Mart which covers stores in three US States (California, Texas, and Wisconsin) includes item level, department, product categories, and store details.\n\n* Structure of sales file: The sales file is formatted in the unit of each items/physical objects(30490 items totally) which is uniquely identified by ids consisting of the item type, state, and store without given exact item names.  For columns, except the basic and redundant information(Again,the item type, state, and store) given, each columns represent sales for each days  \n\n* In the calendar and sell_price files, it provides explanatory variables such as price, promotions, day of the week, and special events.\n\n* I transpose the row index and columns since for sequential data, it will easily analyse time-relavant characteristics of sales as series, especially after using real date format as index so that we can not only use existing time features but also extract more time feature by utilising pandas library like day of week, holiday etc. \n\n* For submission, the next-28-day sales would be forecast (1914 to 1941). Also the forcast for 1942 to 1969 will be used for evaluation later.\n\n* As expected, the sales data is very erratic, owing to the fact that so many factors affect the sales on a given day. On certain days, the sales quantity is zero, which indicates that a certain product may not be available on that day (as noted by Rob in his kernel).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"past_sales = sale_df.set_index('id')[d_cols].T \\\n    .merge(cal_df.set_index('d')['date'], left_index=True, right_index=True) \\\n    .set_index('date')\npast_sales.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis\n* Distribution of sales values(Good Discussion in [EDA and Baseline Model ](https://www.kaggle.com/rdizzl3/eda-and-baseline-model)), including\n    + the distribution of zeros for each of the series has a mean around 0.8 which means there is a lot of intermittent data!\n    + the distribution of max number of sales for each of the series is also explored and  'a lot of the items have a max number of sales **between 2 and 12**. There are also some items with a very high number of sales for a particular item. It might be fruitful to investigate these items and whether it was a holiday or not.'\n* Sales by items\n    + a lot of the items have intermittent demand. These are series that have many zeros with bursts of demand inbetween. This will be one of the biggest challenges in this competition.\n    + In the analysis above it looks like a lot of the time series data start with leading zeros. I believe we can characterize these leading zeros as items that were not selling or available to sell for those periods of time. This might not be a good assumption for every series. We can investigate the distribution of leading zeros, this could help us bring down the large data size (although may not be a good choice for algorightms such as ARIMA).\n(as noted in https://www.kaggle.com/rdizzl3/eda-and-baseline-model)\n* Total Sales by Categories\n* Sales broken down by time variables\n\n* Distribution of price\n\n* Sales Price\n\n* Summary:\n    + there is a trend in the sale data; therefore, when modelling using tree models like light gbm, may need multiply a constant(e.g.1.1) to reach the trend.\n    + seasonality in week.\n    + many 0.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"## Distribution of sales values\npd.Series((stv[d_cols].values != 0).argmax(axis=1)).hist(figsize=(25, 5), bins=100)\n\n\n\n## Sales by items\n# Import widgets\nfrom ipywidgets import widgets, interactive, interact\n# import ipywidgets as widgets\nfrom IPython.display import display\n\n\ndays = range(1, 1913 + 1)\nd_cols = [f'd_{i}' for i in days]\n\nids = np.random.choice(stv['id'].unique().tolist(), 1000)\n\nseries_ids = widgets.Dropdown(\n    options=ids,\n    value=ids[0],\n    description='series_ids:'\n)\n\ndef plot_data(series_ids):\n    df = stv.loc[stv['id'] == series_ids][d_cols]\n    df = pd.Series(df.values.flatten())\n\n    df.plot(figsize=(20, 10), lw=2, marker='*')\n    df.rolling(7).mean().plot(figsize=(20, 10), lw=2, marker='o', color='orange')\n    plt.axhline(df.mean(), lw=3, color='red')\n    plt.grid()\n    \nw = interactive(\n    plot_data,\n    series_ids=series_ids\n)\ndisplay(w)\n\n\n## Total Sales by Categories\n# easily using 'stv' but no much info related to real dates. So use 'past_sales'\n# stv.groupby('cat_id').sum().T.plot()\n\nfor i in stv['cat_id'].unique():\n    stv.groupby('cat_id').sum()\n    items_col = [c for c in past_sales.columns if i in c]\n    past_sales[items_col] \\\n        .sum(axis=1) \\\n        .plot(figsize=(15, 5),\n              alpha=0.8,\n              title='Total Sales by Item Type')\nplt.legend(stv['cat_id'].unique())\nplt.show()\n\n\n## Sales broken down by time variables\n# - Now that we have our example item lets see how it sells by:\n#     - Day of the week\n#     - Month\n#     - Year\n\n# Merge calendar on our items' data\nexample = stv.loc[stv['id'] == 'FOODS_3_090_CA_3_validation'][d_cols].T\nexample = example.rename(columns={8412:'FOODS_3_090_CA_3'}) # Name it correctly\nexample = example.reset_index().rename(columns={'index': 'd'}) # make the index \"d\"\nexample = example.merge(cal, how='left', validate='1:1')\n# Select more top selling examples\nexample2 = stv.loc[stv['id'] == 'HOBBIES_1_234_CA_3_validation'][d_cols].T\nexample2 = example2.rename(columns={6324:'HOBBIES_1_234_CA_3'}) # Name it correctly\nexample2 = example2.reset_index().rename(columns={'index': 'd'}) # make the index \"d\"\nexample2 = example2.merge(cal, how='left', validate='1:1')\n\nexample3 = stv.loc[stv['id'] == 'HOUSEHOLD_1_118_CA_3_validation'][d_cols].T\nexample3 = example3.rename(columns={6776:'HOUSEHOLD_1_118_CA_3'}) # Name it correctly\nexample3 = example3.reset_index().rename(columns={'index': 'd'}) # make the index \"d\"\nexample3 = example3.merge(cal, how='left', validate='1:1')\n\n\nexamples = ['FOODS_3_090_CA_3','HOBBIES_1_234_CA_3','HOUSEHOLD_1_118_CA_3']\nexample_df = [example, example2, example3]\nfor i in [0, 1, 2]:\n    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 3))\n    example_df[i].groupby('wday').mean()[examples[i]] \\\n        .plot(kind='line',\n              title='average sale: day of week',\n              lw=5,\n              color=color_pal[0],\n              ax=ax1)\n    example_df[i].groupby('month').mean()[examples[i]] \\\n        .plot(kind='line',\n              title='average sale: month',\n              lw=5,\n              color=color_pal[4],\n\n              ax=ax2)\n    example_df[i].groupby('year').mean()[examples[i]] \\\n        .plot(kind='line',\n              lw=5,\n              title='average sale: year',\n              color=color_pal[2],\n\n              ax=ax3)\n    fig.suptitle(f'Trends for item: {examples[i]}',\n                 size=20,\n                 y=1.1)\n    plt.tight_layout()\n    plt.show()\n    \n    \n\n## Sales Price\nfig, ax = plt.subplots(figsize=(15, 5))\nstores = []\nfor store, d in sellp.query('item_id == \"FOODS_3_090\"').groupby('store_id'):\n    d.plot(x='wm_yr_wk',\n          y='sell_price',\n          style='.',\n          color=next(color_cycle),\n          figsize=(15, 5),\n          title='FOODS_3_090 sale price over time',\n         ax=ax,\n          legend=store)\n    stores.append(store)\n    plt.legend()\nplt.legend(stores)\nplt.show()\n\n\n\n## Distribution of price\n\nsellp['Category'] = sellp['item_id'].str.split('_', expand=True)[0]\nfig, axs = plt.subplots(1, 3, figsize=(15, 4))\ni = 0\nfor cat, d in sellp.groupby('Category'):\n    ax = d['sell_price'].apply(np.log1p) \\\n        .plot(kind='hist',\n                         bins=20,\n                         title=f'Distribution of {cat} prices',\n                         ax=axs[i],\n                                         color=next(color_cycle))\n    ax.set_xlabel('Log(price)')\n    i += 1\nplt.tight_layout()\n\n\n\n\n\n## TODO\n# - Simple prediction based on historical average sale by day of week\n# - Facebook prophet model\n# - lgbm/xgb model based on day features\nthirty_day_avg_map = stv.set_index('id')[d_cols[-30:]].mean(axis=1).to_dict()\nfcols = [f for f in ss.columns if 'F' in f]\nfor f in fcols:\n    ss[f] = ss['id'].map(thirty_day_avg_map).fillna(0)\n    \nss.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}