{"cells":[{"metadata":{"id":"C86KB56vfKYM"},"cell_type":"markdown","source":"# M5 forecasting -Accuracy\n\nThe M5 forecasting(Accuracy) challenge, is a challenge put up by the  University of Nicosia which comprises of sales data from 10 Walmart stores in 3 states with 3049 items in the stores. The objective as given by the host is as follows:\nThe objective of the M5 forecasting competition is to advance the theory and practice of forecasting by identifying the method(s) that provide the most accurate point forecasts for each of the 42,840 time series of the competition.\nThe value 42840 is formed as a result of the hierarchical nature of the problem with each step requiring a solution. \nThe different hiearchies are:\n\n1\tUnit sales of all products, aggregated for all stores/states:\t1\n\n2\tUnit sales of all products, aggregated for each State:\t3\n\n3\tUnit sales of all products, aggregated for each store :\t10\n\n4\tUnit sales of all products, aggregated for each category:\t3\n\n5\tUnit sales of all products, aggregated for each department:\t7\n\n6\tUnit sales of all products, aggregated for each State and category:\t9\n\n7\tUnit sales of all products, aggregated for each State and department:\t21\n\n8\tUnit sales of all products, aggregated for each store and category:\t30\n\n9\tUnit sales of all products, aggregated for each store and department:\t70\n\n10\tUnit sales of product x, aggregated for all stores/states:\t3,049\n\n11\tUnit sales of product x, aggregated for each State:\t9,147\n\n12\tUnit sales of product x, aggregated for each store:\t30,490\n\nTotal\t42,840 \n\nFor further details on the competion visit: https://www.kaggle.com/c/m5-forecasting-accuracy/data\n\nIn this kernel, we will take a look at this huge dataset,doing EDA,optimizing for lower end workstations and apply computationally inexpensive models(Prophet,SARIMAX) using Colab and Kaggle Notebooks.\n\nThis kernel is for those who are starting out with time series and using Prophet, a very straightforward library for time series and very limited resources. You may find it helpful to get your data in proper format as well and the code written in very straightforward terms! Hope you enjoy this kernel!\n\n\n\n","execution_count":null},{"metadata":{"id":"s_xLCTtUSMFQ"},"cell_type":"markdown","source":"## Importing the data and installing the required libraries:\n","execution_count":null},{"metadata":{"id":"hQweOHTqSWt1"},"cell_type":"markdown","source":"Note that we use an older version of numpy as the library pmdarima(used for finding order for SARIMA) is not compatible with the current version(as of 3/6/2020) with colab.","execution_count":null},{"metadata":{"id":"x-dDEyWVf3Tx","trusted":true},"cell_type":"code","source":"#importing libraries for the analysis:\nimport pandas as pd\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom matplotlib import dates\n# Load specific forecasting tools\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\n!pip install --upgrade numpy==1.18.3 \nimport numpy as np\n!pip install pmdarima\nfrom pmdarima import auto_arima                              # for determining ARIMA orders","execution_count":null,"outputs":[]},{"metadata":{"id":"RgkLlP7Mebzr","trusted":true},"cell_type":"code","source":"# Converting the csv files to pandas dataframe:\ncalender=pd.read_csv('/kaggle/input/m5-forecasting-accuracy/calendar.csv',parse_dates=True)\nsales=pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sales_train_evaluation.csv',parse_dates=True)\nprices=pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sell_prices.csv')\ncalender['date']=pd.to_datetime(calender['date'])\nsample_submission=pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"id":"vCvkm3ypgBcf"},"cell_type":"markdown","source":"# A brief look at the various dataframes given:\nNow that we have assigned the csv files to dataframes, let us take a look at the data given to us.","execution_count":null},{"metadata":{"id":"6lRdcXYkTxPL"},"cell_type":"markdown","source":"### Calender:\nThe calender dataframe gives us details on the dates, events,holidays SNAP(explained in the EDA section) etc for the duration for around the 5 years the sales data is provided.","execution_count":null},{"metadata":{"id":"DEip38Z1exaX","outputId":"eb26286a-2549-4fef-d16b-0a18017aa7eb","trusted":true},"cell_type":"code","source":"# get the dimensions of the first dataframe 'calender' and show the first five rows:\nprint(calender.shape)\ncalender.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"Aaw-BzubKtLl","outputId":"5d06cf0a-45eb-498b-8cec-1e1548174cc2","trusted":true},"cell_type":"code","source":"# Information on the different columns of the dataframe:\ncalender.info()","execution_count":null,"outputs":[]},{"metadata":{"id":"Vlm_KlKfUWEN"},"cell_type":"markdown","source":"### Sales:\nThe sales data gives us the number of items sold everyday for each item in a store for all the 3049 items in 10 stores.\n","execution_count":null},{"metadata":{"id":"7NcOk3gUflb-","outputId":"bd08b016-9775-483e-f9cc-4b809dfd9e4b","trusted":true},"cell_type":"code","source":"# get the dimensions of the first dataframe 'sales' and show the first five rows:\nprint(sales.shape)\nsales.head(5)","execution_count":null,"outputs":[]},{"metadata":{"id":"MXcpQHYWMNq-","outputId":"ec0b0075-b6e4-484e-fbba-e3e3ff9bdef0","trusted":true},"cell_type":"code","source":"# Information on the different columns of the dataframe:\nsales.info()","execution_count":null,"outputs":[]},{"metadata":{"id":"GgmQq3rYUpA6"},"cell_type":"markdown","source":"### Prices:\nThe prices dataframe gives us the prices of the all the items in the stores on a weekly basis.","execution_count":null},{"metadata":{"id":"Xv6O2ziStYGH","outputId":"583f08d3-3e33-43a2-fd92-0a10068b7aa5","trusted":true},"cell_type":"code","source":"# get the dimensions of the first dataframe 'prices' and show the first five rows:\nprint(prices.shape)\nprices.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"pMmK1aBoRIIT","outputId":"84f269a0-d9c6-4f80-f2d5-1e4f1b376026","trusted":true},"cell_type":"code","source":"# Information on the different columns of the dataframe:\nprices.info()","execution_count":null,"outputs":[]},{"metadata":{"id":"BVZaOQ3fFDNH"},"cell_type":"markdown","source":"# Optimizing the dataframes:\nWe first want to start by making a final dataset which contains all the necessary values in one table so we can do our analysis with ease.\n\nA major roadblock while doing this analysis is the size of the final dataset I intended to create. With more than 40 million rows and quite a few columns, I realized that my code platform(colab) could not support the ram usage(colab provides 12 gb ram,a little less than my personal computer) of such a large unoptimized dataset. Hence, this section tries to reduce the size of the dataset so as to be able to run the notebook.\n\nFirst, we start of by changing all the most of the columns with string type data to categorical dtype as almost all the columns are categories in this dataset.","execution_count":null},{"metadata":{"id":"4AD9f14qh-ee","trusted":true},"cell_type":"code","source":"#Converting dtypes to 'categorical' and filling nans with -1( we use -1 to further reduce the dataset size as compared to nans) for all the 3 dataframes:\ncalender=calender.fillna(-1)\ncalender[['event_type_1','event_type_2','event_name_1','event_name_2']]=calender[['event_type_1','event_type_2','event_name_1','event_name_2']].astype(('category'))\n\nsales[['id','item_id','cat_id','store_id','state_id']]=sales[['id','item_id','cat_id','store_id','state_id']].astype('category')\n\n# For prices column we combine the item_id and store_id to form the id of the data which can later be joined with sales dataframe:\nprices['id']=prices['item_id']+'_'+prices['store_id']+'_evaluation'\n\nprices[['id','store_id','item_id']]=prices[['id','store_id','item_id']].astype('category')\n# We also drop store_id and item_id as they no longer play any role in the dataset and all the information is stored in 'id'.\nprices.drop(['store_id','item_id'],axis=1,inplace=True)\n# We also drop dept_id from sales as we will note be using the column:\nsales.drop('dept_id',axis=1,inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"GzdKRxxEi8TE"},"cell_type":"markdown","source":"Next, to further reduce the size of the dataframes we downscale the integer and float types of the various columns in the dataframe. For eg, if the data has a range less that int(8) but the data type is attributed to int(32), the code below reduces to the datatype to int(8) saving a lot  of storage.","execution_count":null},{"metadata":{"id":"khHdtROKFHjL","trusted":true},"cell_type":"code","source":"# This very convinient piece of code is commonly found on kaggle competitions which performs the above tasks for all the rows:\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df\n    ","execution_count":null,"outputs":[]},{"metadata":{"id":"aXZC0XeZFbut","outputId":"93461229-3806-4f69-af47-9a10fe66cfb3","trusted":true},"cell_type":"code","source":"# Applying the above function to the prices dataframe and calander dataframe:\n# We apply the function to sales dataframe after applying melt to sales:\nprices=reduce_mem_usage(prices)\ncalender=reduce_mem_usage(calender)","execution_count":null,"outputs":[]},{"metadata":{"id":"UNmATcNmjtoE"},"cell_type":"markdown","source":"Next, we make the dataframe sales into a more usable format where each day is a column rather than a row. This essentially makes the table vertical.","execution_count":null},{"metadata":{"id":"jXlIM8YjufZf","outputId":"e6288f81-e465-416f-c7fe-cb9d612c01b6","trusted":true},"cell_type":"code","source":"# We use pd.melt to do the task above, which essentially bring the table to the format given below:\nsales=pd.melt(sales,id_vars=['id','item_id','cat_id','store_id','state_id'])\nsales.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"iT_Qe1LpkKmk"},"cell_type":"markdown","source":"We will now use our memory reduction function on this dataframe as pandas/numpy processes data much slower on multiple columns as compared to multiple rows.","execution_count":null},{"metadata":{"id":"isHtNvg1MvrD","outputId":"858b1ce1-e26a-4aa5-ff28-7108572104cc","trusted":true},"cell_type":"code","source":"sales=reduce_mem_usage(sales)","execution_count":null,"outputs":[]},{"metadata":{"id":"K45Dj8WSkbfd"},"cell_type":"markdown","source":"We can see that there is a huge reduction in all the dataframes sizes which will prevent colab from crashing due to excessive RAM usage.\nWith this, we come to an end to data optimization.","execution_count":null},{"metadata":{"id":"f1UOPkzZkqRT"},"cell_type":"markdown","source":"## Forming the final dataframe:\nNow that we have optimized our individual dataframes we will combine them three to make a convinient dataframe with all the information needed for our analysis in one place.","execution_count":null},{"metadata":{"id":"LpN5eGKxyrSF","trusted":true},"cell_type":"code","source":"# Here we merge all three dataframes:\ndf=pd.merge(pd.merge(calender[['date','d','wm_yr_wk',\n       'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2',\n       'snap_CA', 'snap_TX', 'snap_WI']],sales,left_on='d',right_on='variable',how='inner'),prices,left_on=['id','wm_yr_wk'],right_on=['id','wm_yr_wk'],how='inner')\n\n# We get rid of the columns on which the dataframe was joined on as we already have the date column instead:\ndf.drop(['d','variable','wm_yr_wk'],axis=1,inplace=True)\n# Rearranging the columns to our convinience:\ncols=['date','id', 'item_id', 'cat_id', 'store_id', 'state_id','sell_price','event_name_1', 'event_type_1', 'event_name_2','event_type_2', 'snap_CA', 'snap_TX', 'snap_WI','value']\ndf=df[cols]\n","execution_count":null,"outputs":[]},{"metadata":{"id":"7xDggdaCm4JM"},"cell_type":"markdown","source":"Given below is a brief look at our final dataframe:","execution_count":null},{"metadata":{"id":"OVT-u-s30V08","outputId":"94366454-ac7f-46a8-a5a7-03b563845df5","trusted":true},"cell_type":"code","source":"df.tail()","execution_count":null,"outputs":[]},{"metadata":{"id":"GdOpKiGtOyLM","outputId":"ae177bd7-f86e-4bc8-8608-08e8e93a9295","trusted":true},"cell_type":"code","source":"# Given below is information on the various columns of the dataframe:\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"id":"Fn59FDhqnOP9"},"cell_type":"markdown","source":"With this we have come to an end of this section. Now we can proceed with our Exploratory Data Analysis for our final dataset.","execution_count":null},{"metadata":{"id":"qMKuI2ud8BOu"},"cell_type":"markdown","source":"# Overview of the project:\nBefore we start with the EDA, we need to figure out what we need to look for. Especially considering the size of the dataframe and the reduced computational power I personally possess, the main aim of the EDA is going to select a suitable model for our final analysis. Especially considering the complex heriarchy and the many variables we have to choose how we perform our EDA very conservatively. Performing EDA will only work in subsets of the main dataset(trying otherwise inevitibly leads to Colab crashing). So we will decide now what we need to do in our EDA section to get to our goal of choosing an appropriate model.\n\nBefore we even start our EDA we can start by eliminating models that will not be suitable for our use. \n\nSome basic concepts:\nTop down model: In this kind of models, we look at all the 3049 items, and forecast them for all the stores together rather than forecast for individual stores. Since sales in the stores are correlated, after we forecast for the items, we can distribute the items according to the store's market share. \nThe primary advantage of this kind of a method is that we have to run a relatively smaller number of time series analysis models at the sacrifice at some approximation.\nBottom up model: This is right the opposite of the model above. We make a forecast for every item in every store individually. Which means that we will need to run 30490 models which is quite an incredible number. The advantage being we wont have to approximately divide the market share and predict. Another disadvantage to note is that sometimes, trends which can be noted in the overall scheme goes missing for individual store, but if that is prevelant here we can find out.\n\nGlobal model: These models considering all the items together as sales of each are not independant to one another.\n\n### Elimination of models: \nWe will start by eliminating the most complex models and going from there.\n1. RNN's(Recurrent Neural Networks) with LSTM via the global  model. Here we feed the entire final dataset to the RNN. The advantage of this is that the RNN can potentially figure the dependance of the various variables on each other. It is an extremely complex model which we cannot simply run due to the fact that putting the entire dataset into any model to process is simply out of scope in terms of computational power, RAM, complexity of hyper parameter tuning etc.\n2. Another such global bottom up method is VAR,VARIMA,seasonal VARIMA etc, again,due to its sheer complexity, various tuning difficulties, size etc we will not even consider these methods.\n3. RNN's in general: Even the simplest two layer RNN done for 3049 items is simply out of our scope and even if it was computationally viable, it would much better be to use another less complex method made specifically for our use case.\n4. Combination/ensemble: We will only be running a single model on the entire dataset for reducing the complexity and computational needs.\n5. Models such as Simple exponential smoothing, MA model etc. These models are not considered as almost in most cases SARIMA or Prophet performs considerably better than these especially with the datasets large number of exogenous variables, multiple seasonality etc.\n\n## Models under consideration:\n\nThere are still a fair number of models not considered, but with the current domain knowledge the final options from which we will choose from are:\n\n\n1. SARIMAX  :A traditional model which is still quite widely used and very poweful as well. It can include exogenous variables as well as seasonality. If we do indeed have the computational capabilities and there is a potential gain in accuracy we will implement SARIMAX bottom up.\n2. Facebook's : This is a library put out by facebook which could potentially be extremely useful in this use case. It is easy to implement yet very powerful capable of taking external variables and is known to perform very well with various seasonal changes as it is modelled based on FB's forecasting algorithms.\n\nThis is the final list of the models we have in consideration. We will start of with a general EDA for the table trying to compare check the extent of relation between the various states, stores, values etc,effects of holidays and exogenous variables etc and how we reduce the dimensions etc. We mostly do this to check whether top down model will give us satisfactory results,reduce dimensionality and gain insights into the data. Once we have completed the EDA, we will based on our results etc, choose subsets of the total data and try the  models on it giving us a better understanding on which model we should choose without high computational costs. \n\n\nWith this we have come to the end of this section and now will proceed with the EDA.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","execution_count":null},{"metadata":{"id":"lx6FhRavsxqr"},"cell_type":"markdown","source":"# Exploratory Data analysis:\n\nFor our EDA, initially we will take a look at how each of the explanotory variables influence the target variable.\n\n### Total sales over time : \nFirst we will check how the total sales varies over time of all items to get an idea of the trends, seasonality etc.We will be doing this in two steps, we will plot out the data for the entire duration of the dataset. And then, take a look at subsets of data to check for further weekly/monthly seasonality etc.","execution_count":null},{"metadata":{"id":"5Af81-Q2b99A","outputId":"06b2afd4-8830-4778-8fcf-9111e2991ae0","trusted":true},"cell_type":"code","source":"df.tail()","execution_count":null,"outputs":[]},{"metadata":{"id":"b5wbRupnNGkN","outputId":"59ba2ea0-369e-459c-f611-0809d9285b05","trusted":true},"cell_type":"code","source":"df[['date','value']].groupby('date').agg({'value':'sum'}).plot(figsize=(20,8),grid=True);","execution_count":null,"outputs":[]},{"metadata":{"id":"995-uOTwN2jb"},"cell_type":"markdown","source":"Given above is the plot for the total duration for all the sales of all items. There are a few notable features. One is the general upward trend seen over the years. There also seems to be a visible seasonality that is consistent through the years. Another point that can be noted is the huge dip around the christmas season as well as lesser dips in other points which indicates the importance of inputting holiday details.\n\nNext we will take a look at one years worth of data","execution_count":null},{"metadata":{"id":"sKF9lIMMN3HD","outputId":"73aa4f54-e8fb-48ee-a144-5d868825674e","trusted":true},"cell_type":"code","source":"df.loc[(df.date>'2015-01-01')&(df.date<'2016-01-01')][['date','value']].groupby('date').agg({'value':'sum'}).plot(figsize=(20,8),grid=True);","execution_count":null,"outputs":[]},{"metadata":{"id":"usv9ciHXR72i"},"cell_type":"markdown","source":"Similar to the graph above, this too shows seasonality, we can see the slight seasonality over the year as well as monthly/weekly seasonality. We can see a small spike at the start of almost everymonth as well as weekly fluctuations. It could be  that people purchase more during the weekend than on weekdays.Let us have a closer look at a random normal month and check how the weekly seasonality varies.\n","execution_count":null},{"metadata":{"id":"WBqi-qRXQZYA","outputId":"c33ee658-7767-41b3-edd7-9356f346b519","trusted":true},"cell_type":"code","source":"ax=df.loc[(df.date>'2014-08-01')&(df.date<'2014-09-01')][['date','value']].groupby('date').agg({'value':'sum'}).plot(figsize=(20,8))\nax.xaxis.set_minor_locator(dates.DayLocator())\nax.xaxis.set_minor_formatter(dates.DateFormatter(\"%a-%B-%d\"))\nax.tick_params(which='minor', rotation=45)\nax.grid(b=True, which='minor')","execution_count":null,"outputs":[]},{"metadata":{"id":"yieYMM6-Z8bk"},"cell_type":"markdown","source":"Looking through a few individual months worth of data, we notice that the shopping activity is least during the weeekends and increases over the week as shown above. Also, there seems to be a slight downward trend across the month maybe because people tend to spend more when their salary comes which often is at the start of a month.","execution_count":null},{"metadata":{"id":"oQV1XqUrMIYw"},"cell_type":"markdown","source":"### Sum of sales by store:\n\n I would like to see how the sales vary by store for all the products. We can get an  idea of how items pass through each store and their respective market shares and trends in general.  We will be using a rolling 90 day window to understand the trends and seasonality a little better. However, note that, the weekly seasonality gets lost due to the fact that we are using a 90 day rolling window.","execution_count":null},{"metadata":{"id":"cnVsDgJI1xzq","outputId":"fa3635d2-46ed-4853-81ce-ee90f658ccdb","trusted":true},"cell_type":"code","source":"storewise=df[['date','store_id','value']].groupby(['date','store_id']).agg({'value':'sum'})\nstorewise.reset_index(inplace=True)\nstorewise.pivot(index=\"date\", columns=\"store_id\", values=\"value\").rolling(window=90).mean().plot(figsize=(20,8),grid=True,title='Sum of sales by store');","execution_count":null,"outputs":[]},{"metadata":{"id":"jbFim-mc95K7"},"cell_type":"markdown","source":"Straight away we can notice a few important points. For one, we see an inital upward trend followed by stability over the past few years and a very gradual rise overall. Most prominent is the seasonality in the year which is very pronounced and consistent through the years. There a few anomalies like the WI_2 store shows a great surge towards the end and a similar surge is notice in CA_2. We can see that there are distinct differences between the stores.","execution_count":null},{"metadata":{"id":"Mr17mQIn_6ab"},"cell_type":"markdown","source":"### Sum of sales by category :\nSimilar to the previous section we will try to asses the effect of the category on the sum of sales.","execution_count":null},{"metadata":{"id":"PzS912uiQBnc","outputId":"45df5e41-40bf-4ca6-f6cb-babb63030752","trusted":true},"cell_type":"code","source":"category_wise=df[['date','cat_id','value']].groupby(['date','cat_id']).agg({'value':'sum'})\ncategory_wise.reset_index(inplace=True)\ncategory_wise.pivot(index=\"date\", columns=\"cat_id\", values=\"value\").rolling(window=90).mean().plot(figsize=(20,8),grid=True,title='Sum of sales by category');","execution_count":null,"outputs":[]},{"metadata":{"id":"5D94pyUjBuEp"},"cell_type":"markdown","source":"Here we can see a few interesting trends. The food category's sales are fairly seasonal and has a steady increase over the past few years and has much higher sales as compared to  household items and hobbies which is to be expected. The other two also show some extend of growth and seasonality but nowehere as pronounced as food.","execution_count":null},{"metadata":{"id":"o3U4S0dycI-t"},"cell_type":"markdown","source":"### Sum of sales by state:","execution_count":null},{"metadata":{"id":"_dZKZdo8cmPO"},"cell_type":"markdown","source":"Now we will take a look at how sales varies by state.","execution_count":null},{"metadata":{"id":"y2Dw7cmyS2wp","outputId":"6a204c72-a3e3-4c4a-b408-0b6efd009724","trusted":true},"cell_type":"code","source":"statewise=df[['date','state_id','value']].groupby(['date','state_id']).agg({'value':'sum'})\nstatewise.reset_index(inplace=True)\nstatewise.pivot(index=\"date\", columns=\"state_id\", values=\"value\").rolling(window=90).mean().plot(figsize=(20,8),grid=True,title='Sum of sales by state');","execution_count":null,"outputs":[]},{"metadata":{"id":"LSIZXfgccrRt"},"cell_type":"markdown","source":"Again, as expected, we see similar trends and seasonal compenents as the other graphs above. The main point to note is the difference between the sales in terms of sales and how WI has such a high growth rate in comparison to TX.","execution_count":null},{"metadata":{"id":"WLkeMuUfc73L"},"cell_type":"markdown","source":"### Sales by price:\nThis is potentially one of the most important exogenous variables given to us. The price often influences greatly how much people purchase an item.  For this particular section we will not be using all of the data to plot, instead we will look at  a random item to check how price changes affects the sales of an item and then check the general trends by category.","execution_count":null},{"metadata":{"id":"rjCxv9x3DvZV","outputId":"ef84c1de-2a87-44ee-eea7-ae10be9ecf64","trusted":true},"cell_type":"code","source":"item1=df[['item_id','sell_price','value']].loc[df['item_id']=='HOBBIES_1_008']\nsns.barplot(x='sell_price',y='value',data=item1).set_title('Item1')\nsns.set(rc={'figure.figsize':(10,5)})\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"id":"9P8KENGgk-LT"},"cell_type":"markdown","source":"Straightaway we can see how price influences the sale of a particular item and it's importance in the analysis. Let us take a  look at how price affects the sales of items for categories as whole.","execution_count":null},{"metadata":{"id":"quJy1_L9fN28","outputId":"30328c5e-e12d-40cb-ab8a-dba15e97f619","trusted":true},"cell_type":"code","source":"item1= df[['cat_id','sell_price','value']].loc[df['cat_id']=='FOODS']\nsns.scatterplot(x='sell_price',y='value',data=item1).set_title('Effect of price on sales for food')\nsns.set(rc={'figure.figsize':(10,5)})\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"yrzcLup4lxr0"},"cell_type":"markdown","source":"We observe that more expensive items are much less often bought and the graph is highly right skewed. A similar trend is observed in all three categories. \n\nWe can see that prices play a big role in the sales as expected.\nNext we will see how events affect the sales.\n\n### Effect of events on sales:","execution_count":null},{"metadata":{"id":"wJQm7lDWmsiY","outputId":"0b9a9824-66d8-4b4c-b273-69dd697f7e31","trusted":true},"cell_type":"code","source":"ax=sns.barplot(x='event_name_1',y='value',data=df[['event_name_1','value']].groupby('event_name_1').agg({'value':'mean'}).sort_values(['value']).reset_index())\nax.tick_params(which='both',rotation=90)\nsns.set(rc={'figure.figsize':(20,8)})","execution_count":null,"outputs":[]},{"metadata":{"id":"Wz7YekkEril_"},"cell_type":"markdown","source":"We can notice a few things right away, the major American holidays like christmas, thankgiving,New year etc have a reduced sales. This is most likely due to the fact that these days are spent with family with purchases for the day done much earlier, perhaps a look into a week earlier than these dates would have a relative spike .There is almost no sales on christmas as it is the only holiday for all the  stores. Some other days like the SuperBowl, another important American event is marked with high sales. We can see how events play a role in the sales data. \nAnd in our final part of our EDA, we will take a look at how SNAP affects sales.\n\n### Effect of SNAP(Supplemental Nutrition Assistance Program) on sales:\nSNAP provides a monthly supplement for purchasing nutritious food. The columns for the three states tell us whether the store allows purchase using this program on any given date. Our goal is to see how much SNAP affects sales.","execution_count":null},{"metadata":{"id":"-iqEUuQbo6dY","outputId":"88f86c33-bc00-4e02-cf1d-dfca2c3de0d6","trusted":true},"cell_type":"code","source":"fig, ax =plt.subplots(1,3)\nsns.barplot(x='snap_CA',y='value',data=df[['snap_CA','value']].groupby('snap_CA').agg({'value':'mean'}).sort_values(['value']).reset_index(),ax=ax[0])\nsns.set(rc={'figure.figsize':(10,6)})\nsns.barplot(x='snap_TX',y='value',data=df[['snap_TX','value']].groupby('snap_TX').agg({'value':'mean'}).sort_values(['value']).reset_index(),ax=ax[1])\nsns.barplot(x='snap_WI',y='value',data=df[['snap_WI','value']].groupby('snap_WI').agg({'value':'mean'}).sort_values(['value']).reset_index(),ax=ax[2])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"03O77VX-2afj"},"cell_type":"markdown","source":"In all three there is a notable difference in terms of sales showing that there are more sales when SNAP is allowed which is intuitve but the scale is also quite notable and hence will be important for our analysis to include.\n","execution_count":null},{"metadata":{"id":"nAN9336RV5Pf"},"cell_type":"markdown","source":"## Choosing a model: \nNow we will decide which model to choose. For doing so we will run the model on a single item for a single store and we will check \n1. The accuracy\n2. The run time\nIf the run time for one of the models far exceeds the other, regardless of the accuracy we will be using the faster model.\nFirst we will start off with forming a dataset with one item.","execution_count":null},{"metadata":{"id":"W_v1kX4ffW1h","trusted":true},"cell_type":"code","source":"df['snap']=np.where(df['state_id']=='CA',df['snap_CA'] ,np.where( df['state_id']=='TX',df['snap_TX'],np.where(df['state_id']=='WI',df['snap_WI'],0 )))","execution_count":null,"outputs":[]},{"metadata":{"id":"8uHZk1lRybZ3","trusted":true},"cell_type":"code","source":"item1=df[['date', 'id', 'cat_id', 'sell_price','event_name_1',  'event_name_2','snap_CA', 'snap_TX', 'snap_WI','snap', 'value']].loc[df['id']=='HOBBIES_1_001_CA_1_evaluation']","execution_count":null,"outputs":[]},{"metadata":{"id":"UbzROUowWSrY"},"cell_type":"markdown","source":"Given above is the dataframe for one item, now we will run the SARIMAX model.","execution_count":null},{"metadata":{"id":"uqO7f0-nriDx"},"cell_type":"markdown","source":"### SARIMAX:\nThere are a few drawbacks of arima, for one, SARIMAX does not support multiple seasonality. Only one seasonality is taken into consideration. There is a way to overcome this by adding Fourier terms in the exogenous variable,however, for ease we will avoid it and use a much smaller timeframe. The past 365 days. The advantage of this, the size of the dataset is much reduced and also has the advantage of not needing to take into consideration, yearly seasonality. It also has drawbacks but for simplicity and lack of computational power we will stick to using this highly simplified model.","execution_count":null},{"metadata":{"id":"YHBkl9sZYo4m"},"cell_type":"markdown","source":"To find the ARIMA orders and the seasonality order we will be using auto_arima.","execution_count":null},{"metadata":{"id":"AeQo76bjZB62","trusted":true},"cell_type":"code","source":"train=item1[:-28]\ntest=item1.iloc[-28:]","execution_count":null,"outputs":[]},{"metadata":{"id":"wprMJNyhZUu8","outputId":"1274323f-63ab-42e0-9d39-309984401273","trusted":true},"cell_type":"code","source":"len(test)","execution_count":null,"outputs":[]},{"metadata":{"id":"J1Pb0ysuUFcT","outputId":"a3591ddb-8dfc-4dd1-f668-1f01f7c06a4e","trusted":true},"cell_type":"code","source":"item1.set_index('date')\nauto_arima(train['value'],seasonal=True,m=7,start_Q=0,start_P=0).summary()","execution_count":null,"outputs":[]},{"metadata":{"id":"2Y_MnmzqXY1L"},"cell_type":"markdown","source":"We have got the orders to be (0,1,1)X(0,0,0,7), now we will input these orders into the model and form our predictions.","execution_count":null},{"metadata":{"id":"wm3c5NundiKD","outputId":"f1183ffd-a34c-45f7-94d8-85a4102a7a27","trusted":true},"cell_type":"code","source":"model = SARIMAX(train['value'],exog=train[['sell_price','snap']].astype('float'),order=(0,1,1))\nresults = model.fit()\nresults.summary()","execution_count":null,"outputs":[]},{"metadata":{"id":"_FNKISC1f26L","outputId":"be705797-6f11-475d-de0f-4838e1b3ce80","trusted":true},"cell_type":"code","source":"exog=test[['snap', 'sell_price']].astype(float)\npredictions=results.predict(start=len(train),end=len(train)+len(test)-1,exog=exog)","execution_count":null,"outputs":[]},{"metadata":{"id":"JNrNUoe8ZjjN","trusted":true},"cell_type":"code","source":"predictions=predictions.to_numpy()","execution_count":null,"outputs":[]},{"metadata":{"id":"L1_UDMcRgpUJ","trusted":true},"cell_type":"code","source":"test=test.copy()\n\ntest['predictions']=predictions","execution_count":null,"outputs":[]},{"metadata":{"id":"2lMlnu4UZq59","trusted":true},"cell_type":"code","source":"test.reset_index(drop=True,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"DwtV71HY4x7f"},"cell_type":"markdown","source":"We have run the model with the exogenous variables and now we will try plotting the predicted values against the actual ones and after that we will find the RMSE values.","execution_count":null},{"metadata":{"id":"PW8MTMcVgiX8","outputId":"8e1dc31e-3e46-4dbc-fa16-0475a24c99da","trusted":true},"cell_type":"code","source":"test[['value','predictions']].plot()","execution_count":null,"outputs":[]},{"metadata":{"id":"TCTeyKSB7KPN","outputId":"b14a568c-60ff-40dd-8c8c-fc0757e9ab38","trusted":true},"cell_type":"code","source":"from statsmodels.tools.eval_measures import mse,rmse\nRMSE= rmse(test['value'],test['predictions'])\nprint(\"RMSE:\",RMSE)","execution_count":null,"outputs":[]},{"metadata":{"id":"3C9qCjPOkaVO"},"cell_type":"markdown","source":"Straight away we can see that the predictions are very innacurate, this could be due to the fact that there are so many 0 values and that there is a lot of noise and barely any signal. One thing to note is that even using SARIMA, the model did not detect any weekly seasonality. The very sparse data points can lead to the predictions going so off. Some modifications can be made, such as introduce Fourier Terms, denoising and maybe even using the top down method so as to get overall trends etc will help.\nNext we will take a look at prophets predictions.\n\n### Prophet:\n\nA major advantage prophet holds is that it can take into consideration multiple seasonality, a part of the function is that it produces its own Fourier terms for seasonality.","execution_count":null},{"metadata":{"id":"dglZpXMrhxnx","trusted":true},"cell_type":"code","source":"#Importing required libraries:\nimport pandas as pd\nfrom fbprophet import Prophet\nfrom tqdm.notebook import tqdm","execution_count":null,"outputs":[]},{"metadata":{"id":"iQZFkOuAEppY","trusted":true},"cell_type":"code","source":"# Forming the holidays dataframe:\nholidays_df=df[['date','event_name_1']].loc[df['event_name_1']!=-1]\nholidays_df.drop_duplicates(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"a-EpDzAQFXso","outputId":"e7f38b0a-7831-4cd8-a2ee-a22551de5b98","trusted":true},"cell_type":"code","source":"holidays_df.columns=['ds','holiday']\nholidays_df","execution_count":null,"outputs":[]},{"metadata":{"id":"jhu3t9JskoPc","trusted":true},"cell_type":"code","source":"# Making the columns in the required format:\ncols=['ds','y','sell_price','snap']\nid=sample_submission['id']","execution_count":null,"outputs":[]},{"metadata":{"id":"ohRc3DrukxAb","outputId":"3fb54451-405a-4e2b-f6e6-3eb5af122f55","trusted":true},"cell_type":"code","source":"# Training and fitting the data:\nitem=train[['date','value','sell_price','snap']]\nitem.columns=cols  \nm = Prophet(weekly_seasonality=True,holidays=holidays_df)\nm.add_regressor('sell_price')\nm.add_regressor('snap')\nm.fit(item[-365:])","execution_count":null,"outputs":[]},{"metadata":{"id":"NPjV3HGrTScr","trusted":true},"cell_type":"code","source":"#  Forming the dataframe for our forecast:\nfuture = m.make_future_dataframe(periods=28)\nfuture['sell_price']=item1['sell_price'][-393:].to_numpy()\nfuture['snap']=item1['snap'][-393:].to_numpy()","execution_count":null,"outputs":[]},{"metadata":{"id":"TFAprSBDTYEf","trusted":true},"cell_type":"code","source":"# Forecasting the data:\nforecast = m.predict(future)[-28:]","execution_count":null,"outputs":[]},{"metadata":{"id":"bFreTYG5k_yk","trusted":true},"cell_type":"code","source":"# Putting the forecasted data with the actual values to test:\npreds=forecast['yhat']\npreds=preds.to_numpy()\ntest['yhat']=preds","execution_count":null,"outputs":[]},{"metadata":{"id":"MjEMzLdW83wm","outputId":"f75bf4c4-8e09-42a1-f1f1-9626aace8ff8","trusted":true},"cell_type":"code","source":"# Plotting the data:\nm.plot(m.predict(future));","execution_count":null,"outputs":[]},{"metadata":{"id":"mYfO2LDVlJVz","outputId":"8e3876a5-4f41-4e2e-c796-cd2ea4e15adf","trusted":true},"cell_type":"code","source":"test[['yhat','value']].plot();","execution_count":null,"outputs":[]},{"metadata":{"id":"rC8y3Nzi55LF"},"cell_type":"markdown","source":"There is a marked difference in this case. The model seems to performs quite a lot better. This does not mean that our model is anywhere close to being optimum. There are many points for improvement and more sophisticated, better tuned models are likely to perfrom a lot better. Let us take a look at the rmse values.","execution_count":null},{"metadata":{"id":"sMyEfgbm6tXQ","outputId":"b38554af-516b-4fa8-864d-0087d746ba3f","trusted":true},"cell_type":"code","source":"RMSE = rmse(test['yhat'],test['value'])\nprint(\"RMSE:\",RMSE) ","execution_count":null,"outputs":[]},{"metadata":{"id":"TUc4yePk9m6A"},"cell_type":"markdown","source":"The RMSE values are a lot lower than that than using ARIMA, that too without using exogenous variables. Hence we will proceed with Facebook's Prophet as our final model. We will include exogenous models for our final model. Even the run times are considerably lower for Prophet.","execution_count":null},{"metadata":{"id":"kN0Ydu3LrM4I"},"cell_type":"markdown","source":"## Modelling using Facebook's Prophet:\nFor our final model,we will be using Facebook's Prophet. Now how we will implement is as so:\nRunning on only one vm/machine will take a long time. Hence we are dividing the work into three, and for convinience we will be dividing by state. Colab will be predicting for the items in stores in Texas, I will be using my own machine for California and Wisconsin will be run on Kaggle's notebook. The code will be the same for all with a slight modification. After that all the predictions will be combined together on this notebook made in Google's Colaboratory.","execution_count":null},{"metadata":{"id":"avUwHZE5rOwk","trusted":true},"cell_type":"code","source":"# Forming the necessary dataframes for our final model:\ncols=['ds','y','sell_price','snap_TX']\nid=sample_submission['id']\n# Forming the submission dataframe:\nsubmission=pd.DataFrame(index=('F1', 'F2', 'F3', 'F4', 'F5', 'F6', 'F7', 'F8', 'F9', 'F10',\n       'F11', 'F12', 'F13', 'F14', 'F15', 'F16', 'F17', 'F18', 'F19', 'F20',\n       'F21', 'F22', 'F23', 'F24', 'F25', 'F26', 'F27', 'F28'))\nfuture_data=pd.merge(prices[['id','sell_price','wm_yr_wk']],calender[['date','wm_yr_wk']],left_on='wm_yr_wk',right_on='wm_yr_wk')","execution_count":null,"outputs":[]},{"metadata":{"id":"yonkqXEdhozY","outputId":"d25464ec-c80f-4c81-d56c-3f13120ee5ab","trusted":true},"cell_type":"code","source":"# To find where 'ID' for California ends and Texas begins:\nprint(id[id=='HOBBIES_1_001_TX_1_evaluation'].index)\nprint(id[id=='HOBBIES_1_001_WI_1_evaluation'].index)","execution_count":null,"outputs":[]},{"metadata":{"id":"SF0gdVVhl1s_"},"cell_type":"markdown","source":"Now we know how to divide the predictions between the various. We will be doing our predictions below and will run it on a for loop. Note that we save the data every 100 loops so as to not lose the data in case the runtime gets disconnected.","execution_count":null},{"metadata":{"id":"_oYU2qv0RCGf","outputId":"86fb6710-7e13-46a1-e742-5be5cf2e2a03","trusted":true},"cell_type":"code","source":"def predict(i) :\n   item=df[['date','value','sell_price','snap_TX']].loc[df['id']==i]\n   future_id=future_data[['sell_price','date']].loc[future_data['id']==i].sort_values('date')\n   item.columns=cols  \n   m = Prophet(yearly_seasonality=False,daily_seasonality=False,holidays=holidays_df)\n   m.add_regressor('sell_price')\n   m.add_regressor('snap_TX')\n   m.fit(item[-365:])\n   future = m.make_future_dataframe(periods=28)[-28:]\n   future['sell_price']=future_id['sell_price'][-28:].to_numpy()\n   future['snap_TX']=calender['snap_TX'][-28:].to_numpy()\n   forecast = m.predict(future)[['yhat']]\n   submission[i]=forecast.to_numpy()\n   if n%100==0:\n     submission.to_csv('submission_TX.csv')\nfor i,n in zip(tqdm(id[47590:47592]),range(0,2)):\n  predict(i)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Given below is simply a sample of how the output will look. All the subsets are combined and made into the desired format and submitted. To get more values change the id range to whatever you feel in the code above. Also the model above is not perfect either. Feel free to tweak the parameters in the library for better results.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.tail()","execution_count":null,"outputs":[]},{"metadata":{"id":"A969zXwiATVD"},"cell_type":"markdown","source":"### Combination of all the predictions to  a single dataframe:\nHaving done the predictions in sections there are quite a few to combine. Feel free to skip this section as it provides no further info. All the code below was done on colab and will not  run in this notebook, but its put to give an idea of how to format the final dataset into the output format expected in the competition and how to join the various predictions we have made from the various sources.","execution_count":null},{"metadata":{"id":"AzLpKxXe__q_","trusted":true,"collapsed":true},"cell_type":"code","source":"# Transposing them from being horizontal to vertical:\nTX1=TX1.transpose()\nTX2=TX2.transpose()\nWI1=WI1.transpose()\nWI2=WI2.transpose()\nWI3=WI3.transpose()\nCA1=CA1.transpose()\nCA2=CA2.transpose()\nCA3=CA3.transpose()\nCA4=CA4.transpose()\nCA5=CA5.transpose()\nCA6=CA6.transpose()\nCA7=CA7.transpose()\nCA8=CA8.transpose()","execution_count":null,"outputs":[]},{"metadata":{"id":"NGDa1i-SC-Wi","trusted":true},"cell_type":"code","source":"#Combining all of them into one dataframe:\nsubmission=CA1.append([CA2,CA3,CA4,CA5,CA6,CA7,CA8,TX1,TX2,WI1,WI2,WI3])","execution_count":null,"outputs":[]},{"metadata":{"id":"vyHqREepGnYz","trusted":true},"cell_type":"code","source":"# Formatting to specifications for kaggle submission:\nsubmission.reset_index(inplace=True)\nsubmission.columns=[sample_submission.columns]\nsubmission.to_csv('submission.csv')\nsubmission=pd.read_csv('/content/submission.csv')\nfinal=sample_submission[:30490].append(submission)\nfinal.drop('Unnamed: 0',inplace=True,axis=1)\nfinal.reset_index(drop=True,inplace=True)\nfinal.to_csv('final_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"id":"XLfvu-zGgQdu"},"cell_type":"markdown","source":"Note that the first 30490 rows are filled with 0s as they are the validation data set predictions which have not been done. Only the evaluation dataset predictions are taken into consideration for the final score.","execution_count":null},{"metadata":{"id":"tW3QNV0Wf4tl"},"cell_type":"markdown","source":"With that,we have come to an end to the project! Hope you enjoyed it!","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}