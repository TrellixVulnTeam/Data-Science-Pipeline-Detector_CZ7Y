{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nimport time\nimport gc\n\nimport matplotlib.pyplot as plt\n\n# from sklearn.ensemble import RandomForestRegressor\n\n!pip install catboost\n!pip install ipywidgets\n!jupyter nbextension enable --py widgetsnbextension\n# !pip install -U xgboost\n\n# import xgboost as xgb\n\n# import tensorflow as tf\n# print(\"TensorFlow version:\", tf.__version__)\n# AUTOTUNE = tf.data.experimental.AUTOTUNE\n\nfrom catboost import CatBoostRegressor\n#from catboost import CatBoostClassifier\n\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 50)\npd.set_option('display.width', 1000)\n\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b87ae544-1dc2-4ac9-b494-8ce241e1fc2e","_uuid":"776f7dcc-3080-4b0b-ae1d-0f8d61d3cba3","id":"eup5jF_CkeZi","outputId":"5162d787-22ed-4161-abf8-25f35fc95f0b","trusted":true},"cell_type":"code","source":"\n # %% Read and set up data\n#X_train = pd.read_pickle('/content/drive/My Drive/Colab Notebooks/A1/df_data_features.pkl')\nX_train = pd.read_pickle('../input/featureengineering/df_data_features.pkl')\nday_ini = X_train['d'].min()\nday_train_last = 1913\n\nprint('start day',day_ini)\n\nday_train_first = day_train_last - 1*365\n\n\n#X_train = X_train.loc[(X_train['d'] >= day_train_first) | ((X_train['month'].isin([4,5])) & ~(X_train['year'].isin([2016, 2015, 2014, 2013]))) , :]\nX_train = X_train.loc[(X_train['d'] >= day_train_first), :]\nX_train = X_train.loc[((X_train['Lag28_rmean28_id'].notna()) & (X_train['type'] =='train')) | (X_train['type'] =='val'), :]\n\n# X_train = data.loc[mask, :]\nprint('First training day: ', day_train_first)\nprint('Last training day: ', day_train_last)\nprint(X_train.groupby(['year', 'month'])['d'].count())\n\n\n\nX_train.columns = [c.replace('[','') for c in X_train.columns]\nX_train.columns = [c.replace(']','') for c in X_train.columns]\nX_train.columns = [c.replace(' ','_') for c in X_train.columns]\nX_train.columns = [c.replace(',','') for c in X_train.columns]\nX_train.columns = [c.replace(\"'\",'') for c in X_train.columns]\nprint(X_train.info())\nX_train.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"XJ0ys2hDzwUw","outputId":"ed076ff0-f292-476a-b379-e5de17bcacd6"},"cell_type":"code","source":"def repeat_data(df):\n    rep_month = 0\n    rep_event = 0\n    rep_event_test = 0\n    \n    months = [5]\n    events_test = ['Pesach End', 'OrthodoxEaster', 'Cinco De Mayo', \"Mother's day\"]\n    \n    # make new row with event2 instead of event1\n    events2 = df[(df['event_name_2'].notna())]\n    ts = events2.loc[:,'event_name_2']\n    events2.loc[:,'event_name_1'] = ts\n    ts = events2.loc[:,'event_type_1']\n    events2.loc[:,'event_type_2'] = ts\n    df = df.append(events2, ignore_index=True)\n    df = df.drop(['event_name_2', 'event_type_2'], axis='columns')\n    \n    \n    df_month = df.loc[df['month'].isin(months), :]\n    df_events_test = df.loc[df['event_name_1'].isin(events_test), :]\n    df_events = df.loc[df['event_name_1'].notna(), :]\n    \n    for i in range(rep_month):\n          df = df.append(df_month,ignore_index=True)\n    \n\n    for i in range(rep_event):\n          df = df.append(df_events,ignore_index=True)\n    \n\n    for i in range(rep_event_test):\n          df = df.append(df_events_test,ignore_index=True)\n    \n    df[['event_name_1', 'event_type_1']] = df[['event_name_1', 'event_type_1']].astype('category')\n    return df\n\n#print(X_train.shape)\n#X_train = repeat_data(X_train)\n#print(X_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Additional features\ndef addTrendFeatures(df):\n\n  df.loc[df['Lag1_rmean7_id'] > 0, 'Lag1_rmean1_id_div_Lag1_rmean7_id'] = df['Lag1_rmean1_id']/df['Lag1_rmean7_id']\n  df.loc[(df['Lag1_rmean7_id'] == 0) & (df['Lag1_rmean1_id'] > 0), 'Lag1_rmean1_id_div_Lag1_rmean7_id'] = df['Lag1_rmean1_id']\n  df.loc[(df['Lag1_rmean7_id'] == 0) & (df['Lag1_rmean1_id'] == 0), 'Lag1_rmean1_id_div_Lag1_rmean7_id'] = -1\n\n  df.loc[df['Lag1_rmean28_id'] > 0, 'Lag1_rmean7_id_div_Lag1_rmean28_id'] = df['Lag1_rmean7_id']/df['Lag1_rmean28_id']\n  df.loc[(df['Lag1_rmean28_id'] == 0) & (df['Lag1_rmean7_id'] > 0), 'Lag1_rmean7_id_div_Lag1_rmean28_id'] = df['Lag1_rmean7_id']\n  df.loc[(df['Lag1_rmean28_id'] == 0) & (df['Lag1_rmean7_id'] == 0), 'Lag1_rmean7_id_div_Lag1_rmean28_id'] = -1\n  df['Lag1_rmean1_id_div_Lag1_rmean7_id'] = df['Lag1_rmean1_id_div_Lag1_rmean7_id'].astype('float32')\n  df['Lag1_rmean7_id_div_Lag1_rmean28_id'] = df['Lag1_rmean7_id_div_Lag1_rmean28_id'].astype('float32')\n\n  print(df.head())\n  return df\n\ndef addDateTypeAvg(df, cols):\n    # The total dataset average over the grouping given columns in cols\n    print(f'Calculating item average over {cols} grouping')\n        \n    df[f'{cols}_Avg'] = df.groupby(cols)['saleCount'].transform(lambda x : np.round(x.mean(),2))\n\n    return df\n#print(X_train.info())\n\n#X_train = addDateTypeAvg(X_train, ['id','event_name_1'])\n#X_train = addDateTypeAvg(X_train, ['item_id','event_name_1'])\n#X_train[\"['id', 'event_name_1']_Avg\"] = X_train[\"['id', 'event_name_1']_Avg\"].astype('float32')\n#X_train[\"['item_id', 'event_name_1']_Avg\"] = X_train[\"['item_id', 'event_name_1']_Avg\"].astype('float32')\n#X_train = addTrendFeatures(X_train)\n#print(X_train.info())\n\n#X_train.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"onpZkV01lvFS","outputId":"3eb44be5-a135-4908-8598-2866fe83cdd5","trusted":true},"cell_type":"code","source":"\nbest_cols = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id',\n             'wday', 'month', 'year', 'event_name_1', 'event_type_1',\n             'snap_CA', 'snap_TX', 'snap_WI', 'sell_price',\n             'available',\n             'Lag1_rmean1_id', 'Lag7_rmean1_id', 'Lag28_rmean1_id',\n             'Lag1_rmean7_id', 'Lag7_rmean7_id', 'Lag28_rmean7_id',\n             'Lag1_rmean28_id', 'Lag28_rmean28_id',\n             \"id_wday_Avg\", \"id_month_Avg\",\n             \"item_id_wday_Avg\", \"item_id_month_Avg\"]\n\ncols_drop = ['event_name_2', 'event_type_2','available', 'cat_id','state_id', 'dept_id', 'year']\n\nX_train = X_train.drop(cols_drop, axis='columns')\nX_train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_ignore = ['id', 'd', 'saleCount', 'type',\n              'event_name_2', 'event_type_2', 'available']\n\ncol_feature = [i \n               for i in X_train.columns\n               if i not in cols_ignore]\n\n# VAlidation data mask\nmask_val = (X_train['d'].between(1913 - 365, 1913-365 +28)) | (X_train['d'].between(1913-28, 1913))\nmask_train = X_train['type']=='train'\n\nprint('Feature columns: \\n',col_feature)\n\n# print(X_train[col_feature].info())\ncat_cols = X_train[col_feature].select_dtypes(include=['category']).columns\nfor c in X_train.select_dtypes(include=['category']).columns:\n    if c not in ['id','type']:\n        X_train.loc[:,c] = X_train.loc[:,c].cat.codes\n\nX_train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train[(X_train['id'] == 'FOODS_3_318_CA_1_validation') &  mask_train]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# dropping NAN values on price\ndef repNAN(df, cols, val):\n    print('Setting NAN values to {}'.format(val))\n    for c in cols:\n        print(f'Col. {c} contains {df[c].isna().sum()} NAN')\n        df.loc[df[c].isna(), c] = val\n        \n    return df\n\ncols_rep = ['Lag1_rmean1_id', 'Lag7_rmean1_id', 'Lag28_rmean1_id',\n            'Lag1_rmean7_id', 'Lag7_rmean7_id', 'Lag28_rmean7_id',\n            'Lag1_rmean28_id', 'Lag28_rmean28_id']\n\n#X_train = repNAN(X_train, cols_rep, -1)\nX_train = repNAN(X_train, ['sell_price'], -1)\nX_train = repNAN(X_train, ['saleCount'], 0)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Basic neural network\ndef run_nn():\n    n_feat = X_train[col_feature].shape[1]\n    n_items = X_train['id'].nunique()\n    print(n_feat, n_items)\n    \n    model = tf.keras.Sequential([\n        tf.keras.layers.InputLayer(input_shape=(n_feat,)),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dense(n_feat*3, activation='sigmoid'),\n        tf.keras.layers.BatchNormalization(),\n        #tf.keras.layers.Dropout(rate=0.5),\n        tf.keras.layers.Dense(n_feat*2, activation='sigmoid'),\n        tf.keras.layers.BatchNormalization(),\n        #tf.keras.layers.Dropout(rate=0.8),\n        tf.keras.layers.Dense(n_feat*1, activation='sigmoid'),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dense(1, activation=None)])\n\n    model.summary()\n\n\n    model.compile(optimizer=\"Adam\", loss=\"mse\")\n    \n    callback = tf.keras.callbacks.EarlyStopping(monitor='mse', mode='auto',min_delta=0.01, patience=10)\n    \n    t0 = time.time()\n#     history = model.fit(X_train.loc[~mask_val & mask_train, col_feature],\n#                         X_train.loc[~mask_val & mask_train, 'saleCount'],\n#                         batch_size=n_items,\n#                         epochs=30,\n#                         callbacks=[callback],\n#                         validation_data=(X_train.loc[mask_val & mask_train, col_feature],\n#                                          X_train.loc[mask_val & mask_train, 'saleCount']),\n#                         validation_freq=1\n#                        )\n    history = model.fit(X_train.loc[mask_train, col_feature],\n                        X_train.loc[mask_train, 'saleCount'],\n                        batch_size=n_items,\n                        epochs=30)\n    plt.figure()\n    plt.plot(history.history['loss'],label='Training')\n    #plt.plot(history.history['val_loss'],label='Validation')\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend()\n    plt.show()\n\n    return model\n\n#model = run_nn()\n\n# predictions = model.predict(X_train.loc[(X_train['id'] == 'FOODS_3_318_CA_1_validation') &  mask_train, col_feature][-50:])\n\n# for i, j in zip(predictions, X_train.loc[(X_train['id'] == 'FOODS_3_318_CA_1_validation') &  mask_train, 'saleCount'][-50:]):\n#     print(i,j)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# XGBoost model\ndef run_xgboost(max_depth=6,n_estimators=1000,min_child_weight=1, learning_rate=0.01, gamma=0, subsample=1,colsample_bytree = 1):\n    \n    model = xgb.XGBRegressor(\n        learning_rate=learning_rate,\n        n_estimators=n_estimators,\n        max_depth = max_depth,\n        min_child_weight = min_child_weight,\n        gamma =  gamma,\n        subsample=subsample,\n        colsample_bytree = colsample_bytree,\n        random_state =1,\n        verbosity=2,\n     #   tree_method='gpu_hist'\n    ) \n    \n    model.fit(X_train.loc[mask_train, col_feature],\n              X_train.loc[mask_train, 'saleCount'],\n#               eval_set=[(X_train.loc[mask_val & mask_train, col_feature], X_train.loc[mask_val & mask_train, 'saleCount'])],\n#               eval_metric='rmse',\n#               early_stopping_rounds=10,\n              verbose=True)\n        \n#     model.fit(X_train.loc[~mask_val & mask_train, col_feature],\n#               X_train.loc[~mask_val & mask_train, 'saleCount'],\n# #               eval_set=[(X_train.loc[mask_val & mask_train, col_feature], X_train.loc[mask_val & mask_train, 'saleCount'])],\n# #               eval_metric='rmse',\n# #               early_stopping_rounds=10,\n#               verbose=True)\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Sklearn randomforest\ndef run_rf(n_range=1):\n  score_train = []\n  score_val = []\n  train_time = []\n\n# USe the categorical codes\n  for c in X_train.select_dtypes(include=['category']).columns:\n      if c not in ['id','type']:\n          X_train.loc[:,c] = X_train.loc[:,c].cat.codes\n\n  print(f'training with trees {n_range}')\n  for n in n_range:\n    model = RandomForestRegressor(n_estimators = n,\n                              max_depth = None,\n                              random_state=1,\n                              n_jobs = -1,# Use all available cores\n                              bootstrap = True,\n                              max_features='sqrt',\n                              min_samples_leaf = 10,\n                              verbose=10)\n    \n    t0 = time.time()\n    print('\\n',f'Training RF with n={n} initiated at {time.ctime(t0)}')\n    model.fit(X_train.loc[~mask_val & mask_train, col_feature], X_train.loc[~mask_val & mask_train, 'saleCount'])\n    dt = (time.time() - t0)/60\n    train_time.append(dt)\n    print('Tree depths: ', [estimator.get_depth() for estimator in model.estimators_])\n    score_train.append(model.score(X_train.loc[~mask_val & mask_train, col_feature], X_train.loc[~mask_val & mask_train, 'saleCount']))\n    score_val.append(model.score(X_train.loc[mask_val & mask_train, col_feature], X_train.loc[mask_val & mask_train, 'saleCount']))\n    \n    #rmse(model)\n    for i, (t, v) in enumerate(zip(score_train, score_val)):\n      print(f'{n_range[i]} trees (training, validation): ({np.round(t,2)}, {np.round(v,2)}), {np.round(train_time[i],3)}')\n\n    if n != n_range[-1]:\n      del model\n      gc.collect()\n    \n    return model\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_cb_val():\n  from catboost import Pool\n  model = CatBoostRegressor(iterations=2000,\n                            learning_rate=0.05,\n                            depth=16,\n                            random_strength = 0,\n                            l2_leaf_reg = 3,\n                            min_data_in_leaf = 1,\n                            boosting_type='Plain',\n                            border_count=256,\n                            task_type='GPU',\n                          #logging_level='Verbose',\n                            random_state=1,\n                            one_hot_max_size = 1)\n\n# USe the categorical codes\n  cat_cols = X_train[col_feature].select_dtypes(include=['category']).columns\n  print('Converting categorical datatype to object')\n  print(X_train[col_feature].columns)\n  # USe the categorical codes\n  # cat_idx = []\n  # for c in cat_cols:\n  #   print(c)\n  #   X_train.loc[:,c] = X_train.loc[:,c].astype('str')\n  #   cat_idx = np.append(cat_idx, X_train.columns.get_loc(c))\n  # cat_idx = [0,1,2,3,4,5,6]\n  cat_idx = []\n\n  # for c in X_train.select_dtypes(include=['category']).columns:\n  #   if c not in ['id','type']:\n  #     X_train.loc[:,c] = X_train.loc[:,c].cat.codes\n  # print('Data pool')\n  # train_data = Pool(\n  #     data=X_train.loc[~mask_val & mask_train, col_feature],\n  #     label=X_train.loc[~mask_val & mask_train, 'saleCount'],\n  #     cat_features = cat_idx)\n  \n  # val_data = Pool(\n  #     data=X_train.loc[mask_val & mask_train, col_feature],\n  #     label=X_train.loc[mask_val & mask_train, 'saleCount'],\n  #     cat_features = cat_idx)\n  # print('Model fit')\n  # model.fit(train_data,\n  #           eval_set = val_data,\n  #           early_stopping_rounds = 10)\n\n  model.fit(X_train.loc[~mask_val & mask_train, col_feature],\n            X_train.loc[~mask_val & mask_train, 'saleCount'],\n            eval_set = (X_train.loc[mask_val & mask_train, col_feature], \n                        X_train.loc[mask_val & mask_train, 'saleCount']),\n            early_stopping_rounds = 10,\n            cat_features = cat_idx, plot=True)\n\n  # rmse(model)\n  # feat_imp(model)\n\n  return model\n\ndef run_cb_train():\n    model = CatBoostRegressor(iterations=1000,\n                          learning_rate=0.05,\n                          depth=16,\n                          random_strength = 0,\n                          l2_leaf_reg = 5,\n                          min_data_in_leaf = 1,\n                          boosting_type='Plain',\n                          border_count=512,\n                          task_type='GPU',\n                          logging_level='Verbose',\n                          random_state=1,\n                          one_hot_max_size = 1\n                              )\n\n# USe the categorical codes\n#     print('Converting categorical datatype to object')\n#     #XT = X_train.loc[mask_train, col_feature]\n#     #yt = X_train.loc[mask_train, 'saleCount']\n#     cat_cols = X_train.loc[mask_train, col_feature].select_dtypes(include=['category']).columns\n#     cat_idx = []\n    \n#     for c in cat_cols:\n#         print(f'Converting column {c}')\n#         X_train[c] = X_train[c].cat.codes\n#         if (X_train[c] == -1).sum() > 0:\n#             next_cat = max(X_train[c].unique())\n#             print(f'Setting cat=-1 to {next_cat}')\n#             X_train.loc[X_train[c]==-1, c] = next_cat\n            \n#         cat_idx.append(X_train.loc[mask_train, col_feature].columns.get_loc(c))\n        \n#     print(cat_idx)\n    \n#     print(X_train.loc[mask_train, col_feature].isna().sum())\n    \n#     dat = Pool(\n#         data=X_train.loc[mask_train, col_feature],\n#         label=X_train.loc[mask_train, 'saleCount'],\n#         cat_features = cat_idx)\n  \n    print('Fitting model')\n    model.fit(X_train.loc[mask_train, col_feature],\n              X_train.loc[mask_train, 'saleCount'])\n  \n    #rmse(model)\n\n    return model\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def feat_imp(model):\n  feature_importance = model.get_feature_importance(prettified=True)\n  fig = plt.figure()\n  plt.bar(feature_importance.iloc[:,0], feature_importance.iloc[:,1])\n  plt.ylabel('Feature importance')\n  plt.xticks(rotation=90)\n  plt.tight_layout()\n  plt.savefig('feature.pdf')\n  plt.show()\n\n\ndef rmse(model):\n    pred_train = model.predict(X_train.loc[~mask_val & mask_train, col_feature])\n    pred_val = model.predict(X_train.loc[mask_val & mask_train, col_feature])\n\n    diff_train = X_train.loc[~mask_val & mask_train, 'saleCount'] - pred_train\n    diff_val = X_train.loc[mask_val & mask_train, 'saleCount'] - pred_val\n\n    rmse_train = (1/(len(pred_train)) * np.matmul(diff_train,diff_train))**(1/2)\n    rmse_val = (1/(len(pred_val))*np.matmul(diff_val, diff_val))**(1/2)\n    print('RMSE (train, val): ', rmse_train, rmse_val)","execution_count":null,"outputs":[]},{"metadata":{"id":"6bXvmpVy_Ly9","outputId":"ad2a5ee1-b6b4-47e0-9ba6-4594c3116d56","trusted":true},"cell_type":"code","source":"# RMSE (train, val):  0.0004321664752965253 0.0014114955018167845\n# 1055:\tlearn: 1.6697889\ttest: 1.8760822\tbest: 1.8760080 (1030)\ttotal: 7m 46s\tremaining: 6m 56s\n# With ID\n#757:\tlearn: 1.8419379\ttest: 1.8783848\tbest: 1.8782903 (747)\ttotal: 10m 38s\tremaining: 17m 26\n# RMSE (train, val):  0.0003278893567667356 0.001412440601992821\n\n#X_train.loc[mask_train, col_feature].info()\n\n#model = run_cb_val() \n\nmodel = run_cb_train()\n\n# print('Training model)')\n# model = run_xgboost(max_depth=10,n_estimators=150,min_child_weight=50, learning_rate=0.1, gamma=0, subsample=1,colsample_bytree = 0.4)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feat_imp(model)","execution_count":null,"outputs":[]},{"metadata":{"id":"K6wQONSwauCb","trusted":true},"cell_type":"code","source":"# Generate test set features\ndef addLaggedRollingAvg(df, grp_cols, val_col, windows, lag):\n    # The index is shifted by one, in order to use the previous win days\n    # and not the actual day where the feature is assigned\n    # It just requires 1 value to make a rolling avg, venthough the windows is perhaps 30\n    for w in windows:\n      for l in lag:\n        #print(f'Calculating item rolling average over {w} days at lag {l}')\n        col_name = f'Lag{l}_rmean{w}_{grp_cols}'\n        df[col_name] = df.groupby(grp_cols)[val_col]             \\\n            .transform(lambda x : np.round(x.shift(l).rolling(w, min_periods=1).mean(),2))\n\n    return df\n\nX_test = X_train.loc[X_train['d'] >= 1913-(28+1), :]\n\nd_min_val = X_train.loc[X_train['type'] == 'val', 'd'].min()\nd_max_val = X_train.loc[X_train['type'] == 'val', 'd'].max()\n\nX_test.loc[X_test['id'] == 'FOODS_3_318_CA_3_validation', :].head(100)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"_PlmxTxXgqlK","trusted":true},"cell_type":"code","source":"for d in range(d_min_val,d_max_val+1):\n    print(f'Day {d}')\n    X_test = addLaggedRollingAvg(X_test, grp_cols='id', val_col = 'saleCount', windows=[1],  lag = [1,7])\n    X_test = addLaggedRollingAvg(X_test, grp_cols='id', val_col = 'saleCount', windows=[7],  lag = [1, 7])\n    X_test = addLaggedRollingAvg(X_test, grp_cols='id', val_col = 'saleCount', windows=[28], lag = [1])\n    #X_test = addTrendFeatures(X_test)\n    y_pred = model.predict(X_test.loc[X_test['d'] == d, col_feature])\n    \n    print('Negative sale predictions: ', sum(y_pred < 0))\n    y_pred[y_pred < 0] = 0\n    \n    X_test.loc[X_test['d'] == d, 'saleCount'] = y_pred\n        \nX_test.loc[(X_test['id'] == 'FOODS_3_318_CA_3_validation') & (X_test['type'] == 'val'), :].head(30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"0ArPYmMSzwVl"},"cell_type":"code","source":"X_test[(X_test['d'] == d) & (X_test['saleCount'] < 0)]['d'].count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"_wz3OAw9zwVs"},"cell_type":"code","source":"# negative salecount predictions\nneg_sales = X_test.loc[(X_test['saleCount'] <0) & (X_test['type'] == 'val'), :]\nprint('Negative sale predictions: ', len(neg_sales))\nneg_sales.head()\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0476dd05-d803-405b-971c-31ec516d71af","_uuid":"b0a24830-9179-4346-945a-13120e1b5271","id":"GAeU6egokebA","trusted":true},"cell_type":"code","source":"print('Generating forecast')\n# print(X_test.loc[(X_test['item_id'] == 1437 )& (X_test['store_id'] == 0), col_feature])\n# y_pred = model.predict(X_test[col_feature])\n\nX_forecast = X_test.loc[X_test['type']=='val',['id', 'd', 'saleCount']].copy()\n\n#set negative sales to 0\nX_forecast.loc[(X_forecast['saleCount'] < 0), 'saleCount'] = 0\n\n# Round forecast to 3 decimals\nX_forecast.loc[:,'saleCount'] = np.round(X_forecast.loc[:,'saleCount'],3)\n\ndays = X_forecast['d'].unique()\nn_days = len(days)\nday_min = min(days)\n\nprint(days)\nassert n_days == 28, 'Wrong number of forecastet days'\n\nday_1 = days.min()\n\nX_forecast['d'] = 'F'+ X_forecast['d'].add(- day_min + 1).astype('str')\nprint('Done')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"661b0b3f-0ba5-446c-9416-bec2029cd7ec","_uuid":"4f839f8d-b9ab-488a-972f-15b21ee422f5","id":"1CO9oI1vkebF","trusted":true},"cell_type":"code","source":"\ndf_sub = pd.read_csv('../input/m5-forecasting-accuracy/sample_submission.csv')\n#df_sub = pd.read_csv('/content/drive/My Drive/Colab Notebooks/A1/sample_submission.csv')\n\nfcols = [f for f in df_sub.columns if 'F' in f]\n    \nfor f in fcols:\n    X_forecast_f = X_forecast[X_forecast['d'] == f]\n    df_sub[f] = df_sub.merge(X_forecast_f,\n              how = 'right',\n              left_on = 'id',\n              right_on = 'id')['saleCount']\n        \ndf_sub.fillna(0)\n#df_sub.to_csv('/content/drive/My Drive/Colab Notebooks/A1/submission_cb_3y_events.csv', index=False, na_rep=0)\ndf_sub.to_csv('submission.csv', index=False, na_rep=0)\nprint('Done')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}