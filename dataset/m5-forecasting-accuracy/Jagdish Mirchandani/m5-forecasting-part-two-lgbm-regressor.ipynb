{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"# Introduction\nWelcome to the \"M5 Forecasting - Accuracy\" competition! In this competition, contestants are challenged to forecast future sales at Walmart based on heirarchical sales in the states of California, Texas, and Wisconsin.\n\n# Task in hand\nIn this competition, we need to forecast the sales for [d_1942 - d_1969]. These rows form the test set.\n\nThe rows  [d_1914 - d_1941] form the validation set.\n\nRemaining rows form the training set.\n\n    This notebook covers Modelling only, to check EDA, check https://www.kaggle.com/jagdmir/m5-forecasting-part-one-eda.","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nplt.style.use(\"dark_background\")\nimport gc\nfrom sklearn.model_selection import train_test_split\nfrom lightgbm import LGBMRegressor\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# to display all the columns in the dataset\npd.pandas.set_option('display.max_columns', None)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# load data\ntrain = pd.read_csv(\"/kaggle/input/m5-forecasting-accuracy/sales_train_evaluation.csv\")\ncalendar = pd.read_csv(\"/kaggle/input/m5-forecasting-accuracy/calendar.csv\")\nsell_prices = pd.read_csv(\"/kaggle/input/m5-forecasting-accuracy/sell_prices.csv\")\nsample = pd.read_csv(\"/kaggle/input/m5-forecasting-accuracy/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape,calendar.shape,sell_prices.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"calendar.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Let's take a sneak peek of the data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"calendar.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sell_prices.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Check Null Values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isnull().sum().sort_values(ascending = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lots of zeros above shows particular item was either not sold on that particular day or was not in stock","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"calendar.isnull().sum().sort_values(ascending = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Add Zero sales for dates d_1942 to d_1969","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(1942,1970):\n    col = \"d_\"+ str(i)\n    train[col] = 0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Memory Usage Reduction\n\nWe need to melt the dataset in order to proceed further. but before we do that, we need to reduce the memory usage. if we dont reduce memory usage, we may get memory usage errors.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Downcast in order to save memory\ndef downcast(df):\n    cols = df.dtypes.index.tolist()\n    types = df.dtypes.values.tolist()\n    for i,t in enumerate(types):\n        if 'int' in str(t):\n            if df[cols[i]].min() > np.iinfo(np.int8).min and df[cols[i]].max() < np.iinfo(np.int8).max:\n                df[cols[i]] = df[cols[i]].astype(np.int8)\n            elif df[cols[i]].min() > np.iinfo(np.int16).min and df[cols[i]].max() < np.iinfo(np.int16).max:\n                df[cols[i]] = df[cols[i]].astype(np.int16)\n            elif df[cols[i]].min() > np.iinfo(np.int32).min and df[cols[i]].max() < np.iinfo(np.int32).max:\n                df[cols[i]] = df[cols[i]].astype(np.int32)\n            else:\n                df[cols[i]] = df[cols[i]].astype(np.int64)\n        elif 'float' in str(t):\n            if df[cols[i]].min() > np.finfo(np.float16).min and df[cols[i]].max() < np.finfo(np.float16).max:\n                df[cols[i]] = df[cols[i]].astype(np.float16)\n            elif df[cols[i]].min() > np.finfo(np.float32).min and df[cols[i]].max() < np.finfo(np.float32).max:\n                df[cols[i]] = df[cols[i]].astype(np.float32)\n            else:\n                df[cols[i]] = df[cols[i]].astype(np.float64)\n        elif t == np.object:\n            if cols[i] == 'date':\n                df[cols[i]] = pd.to_datetime(df[cols[i]], format='%Y-%m-%d')\n            else:\n                df[cols[i]] = df[cols[i]].astype('category')\n    return df  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = downcast(train)\nsell_prices = downcast(sell_prices)\ncalendar = downcast(calendar)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# MELT the Dataset (wide form to long form) ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_df = pd.melt(train, \n                  id_vars = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], \n                  var_name = 'd', \n                  value_name = \"sales\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#del train\n#gc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the dataset has been transformed, a new column **\"d\"** is added, this column will have all the different dates (d_1 to d_1969), there is another column added \"**sales**\", this column will have the sales info for that particular day.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Merge the datasets\n\nFirst we will merge grid_df which actuall has sales information for individual items with calendar dataframe, so that we can repalce d_1 etc values with actual dates","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"master = pd.merge(grid_df,calendar, on = \"d\")\nmaster.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del calendar,grid_df\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"master = pd.merge(master, sell_prices, on=['store_id','item_id','wm_yr_wk'], how='left') \nmaster.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del sell_prices\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Building - LGBM","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Let's convert Categorical Variables into numeric variables**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert numeric variables into categorical variables\nconv_id = dict(zip(master.id.cat.codes, master.id))\nconv_item_id = dict(zip(master.item_id.cat.codes, master.item_id))\nconv_dept_id = dict(zip(master.dept_id.cat.codes, master.dept_id))\nconv_cat_id = dict(zip(master.cat_id.cat.codes, master.cat_id))\nconv_store_id = dict(zip(master.store_id.cat.codes, master.store_id))\nconv_d_state_id = dict(zip(master.state_id.cat.codes, master.state_id))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"master.d = master['d'].apply(lambda x: x.split('_')[1]).astype(np.int16)\ncols = master.dtypes.index.tolist()\ntypes = master.dtypes.values.tolist()\nfor i,type in enumerate(types):\n    if type.name == 'category':\n        master[cols[i]] = master[cols[i]].cat.codes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"master.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"master.drop('date',1,inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Create Train,Validity and Test Dataframes","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"valid = master[(master['d']>=1914) & (master['d']<1942)][['id','d','sales']]\ntest = master[master['d']>=1942][['id','d','sales']]\neval_preds = test['sales']\nvalid_preds = valid['sales']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Training & Validation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cats = master.cat_id.astype('category').cat.codes.unique().tolist()\nfor cat in cats:\n    df = master[master['cat_id']==cat]\n    \n    # split the data into train,validate and test\n    X_train, y_train = df[df['d']<1914].drop('sales',axis=1), df[df['d']<1914]['sales']\n    X_valid, y_valid = df[(df['d']>=1914) & (df['d']<1942)].drop('sales',axis=1), df[(df['d']>=1914) & (df['d']<1942)]['sales']\n    X_test = df[df['d']>=1942].drop('sales',axis=1)\n    \n    #model\n    model = LGBMRegressor(\n        n_estimators=1000,\n        learning_rate=0.3,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        max_depth=8,\n        num_leaves=50,\n        min_child_weight=300\n    )\n    print('*****Prediction for Category: {}*****'.format(conv_cat_id[cat]))\n    model.fit(X_train, y_train, eval_set=[(X_train,y_train),(X_valid,y_valid)],\n             eval_metric='rmse', verbose=20, early_stopping_rounds=20)\n    valid_preds[X_valid.index] = model.predict(X_valid)\n    eval_preds[X_test.index] = model.predict(X_test)\n    del model, X_train, y_train, X_valid, y_valid\n    gc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Final Submission","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"valid['sales'] = valid_preds\nvalidation = valid[['id','d','sales']]\nvalidation = pd.pivot(validation, index='id', columns='d', values='sales').reset_index()\nvalidation.columns=['id'] + ['F' + str(i + 1) for i in range(28)]\nvalidation.id = validation.id.map(conv_id).str.replace('evaluation','validation')\n\n#Get the evaluation results\ntest['sales'] = eval_preds\nevaluation = test[['id','d','sales']]\nevaluation = pd.pivot(evaluation, index='id', columns='d', values='sales').reset_index()\nevaluation.columns=['id'] + ['F' + str(i + 1) for i in range(28)]\n#Remap the category id to their respective categories\nevaluation.id = evaluation.id.map(conv_id)\n\n#Prepare the submission\nsubmit = pd.concat([validation,evaluation]).reset_index(drop=True)\nsubmit.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}