{"cells":[{"metadata":{},"cell_type":"markdown","source":"> # M5 - accurate 'out_of_stock' feature\n\nIn the [previous notebook](https://www.kaggle.com/sibmike/m5-out-of-stock-feature-640x-faster), we have calculated supply gaps in under 8 minutes. However, we have used an arbitrary 100 years threshold to distinguish them. In this notebook, we are using confidence intervals from this amazing paper: [Longest Run of Heads](https://www.jstor.org/stable/2686886?origin=JSTOR-pdf&seq=1) by Mark F. Schilling\n\n\\\\( ER_{n} = log_{\\frac{1}{p}}(nq) + \\frac{\\gamma}{log(\\frac{1}{p})} - \\frac{1}{2} \\\\)\n\n\\\\( StdR_{n} = \\sqrt{\\frac{\\pi^2}{6*ln^2(\\frac{1}{p})}+\\frac{1}{12}} \\\\) \n\n\nPrediction intervals:\n\\\\( ER_{n} \\pm 2StdR_{n} \\\\)\n\n**_PPS: Special thanks to @nadare for outstanding 8x booster that cut time from 1 hr to under 8 mins!_**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def gap_interval(p, N):\n    \n    '''\n    Returns the longest expected Heads run \n    for N trials with probabiliity of Heads equal to p\n    \n    p - avg probability of zero sales on a given day\n    N - trials\n    '''\n    q = 1-p\n    gamma = 0.5777 #Euler's constant\n    \n    R = np.log(N*q)/np.log(1/p)+gamma/np.log(1/p)-0.5\n    sigma = np.sqrt(np.pi**2/(6*(np.log(1/p)**2))+1/12)\n    \n    d = R+3*sigma #97.5% confidence interval\n    if d < 1:\n        return 1\n    else:\n        return int(np.rint(d))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# General imports\nimport numpy as np\nimport pandas as pd\nimport os, sys, gc, time, warnings, pickle, psutil, random\nfrom tqdm.auto import tqdm\n\nimport seaborn as sns # data visualization library  \nimport matplotlib.pyplot as plt\n\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Memory Reducer\n# :df pandas dataframe to reduce size             # type: pd.DataFrame()\n# :verbose                                        # type: bool\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8, copy=False)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                       df[col] = df[col].astype(np.int16, copy=False)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32, copy=False)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64, copy=False)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16, copy=False)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32, copy=False)\n                else:\n                    df[col] = df[col].astype(np.float64, copy=False)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load data\ngrid_df = pd.read_pickle('/kaggle/input/m5-simple-fe/grid_part_1.pkl')\n\n#Fix dat type\ngrid_df.sales = grid_df.sales.fillna(0).astype(np.int32, copy=False)\n\n#Add aggregation level column\ngrid_df['level']=11\ngrid_df.level = grid_df.level.astype(np.int16, copy=False)\ngrid_df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Roll Up Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"group_ids = [['state_id'], ['store_id'], \n             ['cat_id'], ['dept_id'], ['state_id', 'cat_id'],  \n             ['state_id', 'dept_id'], ['store_id', 'cat_id'], ['store_id', 'dept_id'], \n             ['item_id'], ['state_id','item_id']]\n\ndef roll_up(df, groups):\n    '''\n    This function calculates aggregates for all levels except level 0.\n    '''\n    t_list=[]\n    for i, g in enumerate(tqdm(groups)):\n        t = grid_df[g+['d','sales']].groupby(g+['d']).sum().fillna(0).astype(np.int32)\n        t.reset_index(inplace=True)\n        \n        if len(g)>1: \n            t['id']=t[g[0]].astype(str)+'_'+t[g[1]].astype(str)\n        else:\n            t['id']=t[g[0]]\n        t.id = t.id.astype('category', copy=False)\n        \n        t['level'] = i+1\n        t_list += [t]\n        \n    return t_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create aggregate for level o:\nlevel_0 = grid_df[['d','sales']].groupby(['d'])[['sales']].sum()\nlevel_0.reset_index(inplace=True)\nlevel_0['id'] = 'all'\nlevel_0['level'] = 0\n\n# Calculate remaining levels and concat them:\nroll_list = [grid_df, level_0]\nroll_list += roll_up(grid_df, group_ids)\n\n#full_df = pd.concat(roll_list, sort=False)\ngrid_df = reduce_mem_usage(pd.concat(roll_list, sort=False))\ngrid_df.reset_index(drop=True, inplace=True)\ndel roll_list, level_0\n\n#free some memory with categoical vars:\ncols=['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id','level']\nfor col in cols:\n    grid_df[col]=grid_df[col].astype('category', copy=False)\n\n#free memory\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Find Gaps"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Note: in 'gap' column: 1 is a day without sales:\ngrid_df['gaps'] = (~(grid_df['sales'] > 0)).astype(np.int16)\ngrid_df.loc[grid_df.d>(1941-28),'gaps'] = 0\ntotal_days = 1942\n\nprods = list(grid_df.id.unique())\ne_list = [] #list to hold expected values of gaps\nd_dict = {} #list to hold avg probability of no sales\np_dict = {} #list to hold probs\n\n# magic x8 speed booster thanks to @nadare\nfor prod_id, df in tqdm(grid_df.groupby(\"id\")):   \n    # extract gap_series for a prod_id\n    sales_gaps = df.loc[:,'gaps']\n\n    # find and mark gaps\n    accum_add_prod = np.frompyfunc(lambda x, y: int((x+y)*y), 2, 1)\n    sales_gaps[:] = accum_add_prod.accumulate(df[\"gaps\"], dtype=np.object).astype(int)\n    sales_gaps[sales_gaps < sales_gaps.shift(-1)] = np.NaN\n    sales_gaps = sales_gaps.fillna(method=\"bfill\").fillna(method='ffill')\n    \n    # calculate initial probability    \n    zero_days = sum(sales_gaps>0)\n    \n    # our dataset does not have series with 0 days, but its subsets may not have 0 days\n    if zero_days == 0:\n        e_list += [sales_gaps]\n        d_dict[prod_id] = 1\n        p_dict[prod_id] = 0\n        \n    else:\n        p = zero_days/total_days+0.00001\n\n        #Find the longest expected run with 95% confidence:\n        d = gap_interval(p, total_days)\n\n        # cut out supply_gap days and run recursively\n        p1 = 0\n        d1 = 0\n        while p1 < p:\n\n            if p1!=0: \n                p = p1\n                d = d1\n\n            # Based on 95% confidence interval, change gap_interval() to your taste\n            gap_days = sum(sales_gaps>=d)\n\n            p1 = (zero_days-gap_days+0.0001)/(total_days-gap_days)      \n            d1 = gap_interval(p1, total_days-gap_days)\n\n        # add results to list it turns out masked replacemnt is a very expensive operation in pandas, so better do it in one go\n        e_list += [sales_gaps/d]\n        d_dict[prod_id] = d\n        p_dict[prod_id] = p","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# build gap series in one go, sort index\ns = pd.concat(e_list)\ns.sort_index(inplace=True)\n\n# create features\ngrid_df['gap_2std'] = s\ngrid_df['gap_interval_2std'] = grid_df.id.map(d_dict)\ngrid_df['prob_zero']=grid_df.id.map(p_dict)\n\n# optimize memory, which we will need for dumping pickle\ngrid_df=reduce_mem_usage(grid_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_df.gap_2std2 = grid_df.gap_2std * (grid_df['sales'] == 0).astype(np.int16)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# free memory\ndel s, e_list\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# dump pickle\ngrid_df.to_pickle('grid_part_1_agg.pkl')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Gaps EDA\n### Level 11 Gaps\nUnit sales of product x, aggregated for each State"},{"metadata":{"trusted":true},"cell_type":"code","source":"# e over 100 years does not make much sense\nm = grid_df['gap_2std']>=1\ngrid_df.loc[m,'gap_2std']=1\n\n# take a subsample to vizualise:\nnp.random.seed(19)\ndepts = list(grid_df.dept_id.dropna().unique())\n\nprod_list = []\nfor d in depts:\n    prod_by_dept=grid_df['item_id'][grid_df.dept_id == d].unique()\n    prod_list += list(np.random.choice(prod_by_dept,5))\n    \nm = grid_df.item_id.isin(prod_list)\nviz_df = grid_df[m]\nviz_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Level 10: Supply gaps by item_id aggregated over all stores:\nItuition our algorithm will provide more confident predictions for supply failure gaps for the top aggregation level."},{"metadata":{"trusted":true},"cell_type":"code","source":"m = viz_df.level == 9\nv_df = viz_df.loc[m].pivot(index='d', columns='id', values='gap_2std')\nv_df = v_df.reindex(sorted(v_df.columns), axis=1)\nf, ax = plt.subplots(figsize=(15, 20))\ntemp = sns.heatmap(v_df, cmap='Reds')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Level 11: Supply gaps by item_id aggregated by state:\nItuition our algorithm will provide a bit less confident predictions for supply failure gaps for the top aggregation level."},{"metadata":{"trusted":true},"cell_type":"code","source":"m = viz_df.level == 10\nv_df = viz_df.loc[m].pivot(index='d', columns='id', values='gap_2std')\nv_df = v_df.reindex(sorted(v_df.columns), axis=1)\nf, ax = plt.subplots(figsize=(15, 20))\ntemp = sns.heatmap(v_df, cmap='Reds')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Level 12: Supply gaps by item_id specific for a store:\nThese gaps will include gaps on previous level plus store specific supply failures.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"m = viz_df.level == 11\n\nids = list(viz_df.id.value_counts()[viz_df.id.value_counts()>0].index)\nn = viz_df.id.isin(ids)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"v_df = viz_df.loc[m&n, ['d','id','gap_2std']].copy()\nv_df.gap_2std = v_df.gap_2std.astype(np.float32)\nv_df = v_df.pivot(index='d', columns='id', values='gap_2std')\nv_df = v_df.reindex(sorted(v_df.columns), axis=1)\nf, ax = plt.subplots(figsize=(15, 20))\ntemp = sns.heatmap(v_df, cmap='Reds')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Finally lets calculate the proportion of non random gaps in original dataset.\n# as mentioned by @Amphi2 we should have dropped last 28 days, so lets substract them:\n\n(sum(grid_df['gap_2std'] >= 1) - 42840*28)/grid_df.shape[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" [Discussion here.](https://www.kaggle.com/c/m5-forecasting-accuracy/discussion/138085#790628)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}