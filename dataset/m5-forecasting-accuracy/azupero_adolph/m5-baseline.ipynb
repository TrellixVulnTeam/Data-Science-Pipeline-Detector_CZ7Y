{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Baseline"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"## Library"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\nfrom sklearn import preprocessing, metrics\nfrom sklearn.preprocessing import LabelEncoder\nimport gc\nimport os\nfrom tqdm import tqdm\nfrom scipy.sparse import csr_matrix\nimport warnings\nwarnings.filterwarnings('ignore')\n\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_rows', 500)\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Helper func"},{"metadata":{"trusted":true},"cell_type":"code","source":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns: #columns毎に処理\n        col_type = df[col].dtypes\n        if col_type in numerics: #numericsのデータ型の範囲内のときに処理を実行. データの最大最小値を元にデータ型を効率的なものに変更\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_data():\n    print('Reading files...')\n    calendar = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/calendar.csv')\n    calendar = reduce_mem_usage(calendar)\n    print('Calendar has {} rows and {} columns'.format(calendar.shape[0], calendar.shape[1]))\n    \n    sell_prices = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sell_prices.csv')\n    sell_prices = reduce_mem_usage(sell_prices)\n    print('Sell prices has {} rows and {} columns'.format(sell_prices.shape[0], sell_prices.shape[1]))\n    \n    sales_train_val = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sales_train_validation.csv')\n    print('Sales train validation has {} rows and {} columns'.format(sales_train_val.shape[0], sales_train_val.shape[1]))\n    \n    submission = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sample_submission.csv')\n    \n    return calendar, sell_prices, sales_train_val, submission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import IPython\n\ndef display(*dfs, head=True):\n    for df in dfs:\n        IPython.display.display(df.head() if head else df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"calendar, sell_prices, sales_train_val, submission = read_data()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 予測期間とitem数の定義 / number of items, and number of prediction period\nNUM_ITEMS = sales_train_val.shape[0]  # 30490\nDAYS_PRED = submission.shape[1] - 1  # 28","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## feature engineering"},{"metadata":{},"cell_type":"markdown","source":"### categorical feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"def encode_categorical(df, cols):\n    for col in cols:\n        # leave NaN\n        le = LabelEncoder()\n        df[col] = df[col].fillna('nan')\n        df[col] = pd.Series(le.fit_transform(df[col]), index=df.index)\n        \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"calendar = encode_categorical(calendar, [\"event_name_1\", \"event_type_1\", \"event_name_2\", \"event_type_2\"])\ncalendar = reduce_mem_usage(calendar)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train_val = encode_categorical(sales_train_val, ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']).pipe(reduce_mem_usage)\nsell_prices = encode_categorical(sell_prices, ['item_id', 'store_id']).pipe(reduce_mem_usage)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sales_train_valからidの詳細部分(itemやdepartmentなどのid)を重複なく一意に取得しておく。(extract a detail of id columns)\nproduct = sales_train_val[['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']].drop_duplicates()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### sales_train_validationのmelt処理\n- 時系列の特徴量が作りやすいように, id毎に横に並んだ時系列データを、（id , 時系列）で縦に変換）"},{"metadata":{"trusted":true},"cell_type":"code","source":"nrows = 365 * 2 * NUM_ITEMS","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display(sales_train_val.head(5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# d_name = ['d_' + str(i+1) for i in range(1913)]\nd_name = [column for column in sales_train_val.columns if 'd_' in column]\nsales_train_val_values = sales_train_val[d_name].values\n# calculate the start position(first non-zero demand observed date) for each item / 商品の最初の売上日\n# 1-1914のdayの数列のうち, 売上が存在しない日を一旦0にし、0を9999に置換。そのうえでminimum numberを計算\ntmp = np.tile(np.arange(1, 1914), (sales_train_val_values.shape[0], 1))\ndf_tmp = (sales_train_val_values > 0) * tmp\n\nstart_no = np.min(np.where(df_tmp==0, 9999, df_tmp), axis=1) - 1\nflag = np.dot(np.diag(1/(start_no+1)), tmp) < 1\n\nsales_train_val_values = np.where(flag, np.nan, sales_train_val_values)\nsales_train_val[d_name] = sales_train_val_values\n\ndel tmp, sales_train_val_values\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"1913-np.max(start_no)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train_val = pd.melt(sales_train_val, id_vars=['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], \n                          var_name='day', \n                          value_name='demand')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#加工後  \ndisplay(sales_train_val.head(5))\nprint('Melted sales train validation has {} rows and {} columns'.format(sales_train_val.shape[0], sales_train_val.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train_val = sales_train_val.iloc[-nrows:,:]\nsales_train_val = sales_train_val[~sales_train_val.demand.isnull()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display(sales_train_val.head(5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 予測部分(validation/evaluation部分)のmelt処理"},{"metadata":{"trusted":true},"cell_type":"code","source":"# submissionのidのvalidation部分とevaluation部分の名前を取得\ntest1_rows = [row for row in submission['id'] if 'validation' in row]\ntest2_rows = [row for row in submission['id'] if 'evaluation' in row]\n\ntest1 = submission[submission['id'].isin(test1_rows)]\ntest2 = submission[submission['id'].isin(test2_rows)]\n\n# F_Xをd_XXXに\ntest1.columns = ['id'] + [f'd_{d}' for d in range(1914, 1914 + DAYS_PRED)]\ntest2.columns = ['id'] + [f'd_{d}' for d in range(1942, 1942 + DAYS_PRED)]\n\n# test2の_evaluationを置換\ntest2['id'] = test2['id'].str.replace('_evaluation', '_validation')\n\n# idをキーにして, idの詳細部分をtest1, test2に結合する.\ntest1 = test1.merge(product, how='left', on='id')\ntest2 = test2.merge(product, how='left', on='id')\n\n# test1, test2をともにmelt処理する.（売上数量:demandは0）\ntest1 = pd.melt(test1, id_vars = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], var_name = 'day', value_name = 'demand')\ntest2 = pd.melt(test2, id_vars = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], var_name = 'day', value_name = 'demand')\n\n# validation部分と, evaluation部分がわかるようにpartという列を作り、 test1,test2のラベルを付ける。\nsales_train_val['part'] = 'train'\ntest1['part'] = 'test1'\ntest2['part'] = 'test2'\n\n# sales_train_valとtest1, test2の縦結合.\ndata = pd.concat([sales_train_val, test1, test2], axis = 0)\n\n# memoryの開放\ndel sales_train_val, test1, test2\n\n# delete test2 for now(6/1以前は, validation部分のみ提出のため.)\ndata = data[data['part'] != 'test2']\n\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### dataにcalendar/sell_pricesを結合"},{"metadata":{"trusted":true},"cell_type":"code","source":"# calendarの結合\ncalendar = calendar.drop(columns=['weekday', 'wday', 'month', 'year'], axis=1)\n\ndata = pd.merge(data, calendar, how='left', left_on=['day'], right_on=['d'])\ndata = data.drop(columns=['d', 'day'], axis=1)\n\ndel calendar\ngc.collect()\n\n# sell priceの結合\ndata = data.merge(sell_prices, on=['store_id', 'item_id', 'wm_yr_wk'], how='left')\nprint('Our final dataset to train has {} rows and {} columns'.format(data.shape[0], data.shape[1]))\n\ndel sell_prices\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### feature engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"def simple_fe(data):\n    \n    # demand features(過去の数量から変数生成)\n    \n    for diff in [0, 1, 2]:\n        shift = DAYS_PRED + diff\n        data[f\"shift_t{shift}\"] = data.groupby([\"id\"])[\"demand\"].transform(\n            lambda x: x.shift(shift)\n        )\n    '''\n    for size in [7, 30, 60, 90, 180]:\n        data[f\"rolling_std_t{size}\"] = data.groupby([\"id\"])[\"demand\"].transform(\n            lambda x: x.shift(DAYS_PRED).rolling(size).std()\n        )\n    '''\n    for size in [7, 30, 60, 90, 180]:\n        data[f\"rolling_mean_t{size}\"] = data.groupby([\"id\"])[\"demand\"].transform(\n            lambda x: x.shift(DAYS_PRED).rolling(size).mean()\n        )\n    '''\n    data[\"rolling_skew_t30\"] = data.groupby([\"id\"])[\"demand\"].transform(\n        lambda x: x.shift(DAYS_PRED).rolling(30).skew()\n    )\n    data[\"rolling_kurt_t30\"] = data.groupby([\"id\"])[\"demand\"].transform(\n        lambda x: x.shift(DAYS_PRED).rolling(30).kurt()\n    )\n    '''\n    # price features\n    # priceの動きと特徴量化（価格の変化率、過去1年間の最大価格との比など）\n    \n    data[\"shift_price_t1\"] = data.groupby([\"id\"])[\"sell_price\"].transform(\n        lambda x: x.shift(1)\n    )\n    data[\"price_change_t1\"] = (data[\"shift_price_t1\"] - data[\"sell_price\"]) / (\n        data[\"shift_price_t1\"]\n    )\n    data[\"rolling_price_max_t365\"] = data.groupby([\"id\"])[\"sell_price\"].transform(\n        lambda x: x.shift(1).rolling(365).max()\n    )\n    data[\"price_change_t365\"] = (data[\"rolling_price_max_t365\"] - data[\"sell_price\"]) / (\n        data[\"rolling_price_max_t365\"]\n    )\n\n    data[\"rolling_price_std_t7\"] = data.groupby([\"id\"])[\"sell_price\"].transform(\n        lambda x: x.rolling(7).std()\n    )\n    data[\"rolling_price_std_t30\"] = data.groupby([\"id\"])[\"sell_price\"].transform(\n        lambda x: x.rolling(30).std()\n    )\n    \n    # time features\n    # 日付に関するデータ\n    dt_col = \"date\"\n    data[dt_col] = pd.to_datetime(data[dt_col])\n    \n    attrs = [\n        \"year\",\n        \"quarter\",\n        \"month\",\n        \"week\",\n        \"day\",\n        \"dayofweek\",\n        \"is_year_end\",\n        \"is_year_start\",\n        \"is_quarter_end\",\n        \"is_quarter_start\",\n        \"is_month_end\",\n        \"is_month_start\",\n    ]\n\n    for attr in attrs:\n        dtype = np.int16 if attr == \"year\" else np.int8\n        data[attr] = getattr(data[dt_col].dt, attr).astype(dtype)\n\n    data[\"is_weekend\"] = data[\"dayofweek\"].isin([5, 6]).astype(np.int8)\n    \n    return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = simple_fe(data)\ndata = reduce_mem_usage(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display(data.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## train/test split"},{"metadata":{"trusted":true},"cell_type":"code","source":"# going to evaluate with the last 28 days\nx_train = data[data['date'] <= '2016-03-27']\ny_train = x_train['demand']\nx_val = data[(data['date'] > '2016-03-27') & (data['date'] <= '2016-04-24')]\ny_val = x_val['demand']\ntest = data[(data['date'] > '2016-04-24')]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## training"},{"metadata":{"trusted":true},"cell_type":"code","source":"# define random hyperparammeters for LGBM\nfeatures = [\n    \"item_id\",\n    \"dept_id\",\n    \"cat_id\",\n    \"store_id\",\n    \"state_id\",\n    \"event_name_1\",\n    \"event_type_1\",\n    \"event_name_2\",\n    \"event_type_2\",\n    \"snap_CA\",\n    \"snap_TX\",\n    \"snap_WI\",\n    \"sell_price\",\n    # demand features.\n    \"shift_t28\",\n    \"shift_t29\",\n    \"shift_t30\",\n    \"rolling_mean_t7\",\n    \"rolling_mean_t30\",\n    \"rolling_mean_t60\",\n    \"rolling_mean_t90\",\n    \"rolling_mean_t180\",\n    # price features\n    \"price_change_t1\",\n    \"price_change_t365\",\n    \"rolling_price_std_t7\",\n    \"rolling_price_std_t30\",\n    # time features.\n    \"year\",\n    \"month\",\n    \"week\",\n    \"day\",\n    \"dayofweek\",\n    \"is_year_end\",\n    \"is_year_start\",\n    \"is_quarter_end\",\n    \"is_quarter_start\",\n    \"is_month_end\",\n    \"is_month_start\",\n    \"is_weekend\",\n]\n\nparams = {\n    'boosting_type': 'gbdt',\n    'metric': 'rmse',\n    'objective': 'regression',\n    'n_jobs': -1,\n    'seed': 236,\n    'learning_rate': 0.1,\n    'bagging_fraction': 0.75,\n    'bagging_freq': 10, \n    'colsample_bytree': 0.75}\n\ntrain_set = lgb.Dataset(x_train[features], y_train)\nval_set = lgb.Dataset(x_val[features], y_val)\n\ndel x_train, y_train\n\n\n# model estimation\nmodel = lgb.train(params, train_set, num_boost_round = 2500, early_stopping_rounds = 50, valid_sets = [train_set, val_set], verbose_eval = 100)\nval_pred = model.predict(x_val[features])\nval_score = np.sqrt(metrics.mean_squared_error(val_pred, y_val))\nprint(f'Our val rmse score is {val_score}')\ny_pred = model.predict(test[features])\ntest['demand'] = y_pred","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = test[['id', 'date', 'demand']]\npredictions = pd.pivot(predictions, index = 'id', columns = 'date', values = 'demand').reset_index()\npredictions.columns = ['id'] + ['F' + str(i + 1) for i in range(28)]\n\nevaluation_rows = [row for row in submission['id'] if 'evaluation' in row] \nevaluation = submission[submission['id'].isin(evaluation_rows)]\n\nvalidation = submission[['id']].merge(predictions, on = 'id')\nfinal = pd.concat([validation, evaluation])\nfinal.to_csv('submission.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## WRMSSEの最適化\nLightGBMのMetricとして, WRMSSEの効率的な計算を行う。あくまで, 28day-lagで1つのモデルの予測するときにLGBMで効率的なWRMSSEの計算を行う場合である。\n\n- weight_matという0 or 1の疎行列で、効率的にaggregation levelを行列積で計算出来るようにしている\n- LightGBMのMetricを効率的に計算するためにGroupby fucntionを使うことを避けているが、そのため、non-rezo demandのデータを除くと効率的な計算ができない。そのためすべてのitemでnon-zero demand dataとなっている最後の28日分のみで検証するコードとなっている.\n- Sparce matrixは順序がProductのItem通りになっていないといけないので注意。"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}