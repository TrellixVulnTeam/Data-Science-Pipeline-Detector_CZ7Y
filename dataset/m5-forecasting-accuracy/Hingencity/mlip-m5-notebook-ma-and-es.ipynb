{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import mean_squared_error\nimport random\nfrom statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt\nfrom sklearn import metrics \nimport math\nfrom typing import Union\nfrom tqdm.auto import tqdm as tqdm\n\nfrom sklearn.model_selection import GridSearchCV\nfrom scipy.stats import uniform as sp_rand","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"######### Auxiliary functions ###############\n#Printing filenames\ndef print_dir():\n    import os\n    for dirname, _, filenames in os.walk('/kaggle/input'):\n        for filename in filenames:\n            print(os.path.join(dirname, filename))\n\n#Plotting\ndef plot_predictions(predictions, index, method, number_of_values = 84, number_of_values_to_predict = 112):\n    sns.set()\n    _ = plt.figure(figsize=(12,8))\n    _ = plt.plot(np.arange(number_of_values,number_of_values_to_predict), predictions[index], label='predicted')\n    _ = plt.plot(np.arange(number_of_values, number_of_values_to_predict), val_dataset.iloc[index].values, label='Val')\n    _ = plt.plot(np.arange(number_of_values), train_dataset.iloc[index].values, label='Train')\n    _ = plt.xlabel('days')\n    _ = plt.ylabel('product count')\n    _ = plt.legend()\n    _ = plt.title(f'Predictions of the {method} for the product: {df_stv.iloc[index][\"id\"]}')\n    plt.show()\n\n    \n######### Models #############################    \n#Moving_average\ndef moving_average(series, k):\n    '''\n    #Calculate average of last k observations\n    '''\n    return np.average(series[:,-k:], axis = 1)    \n\ndef moving_averages (series, h, k):\n    '''\n    #Calculate moving averages of all rows in a dataframe\n    '''\n    series = series[series.columns[:]].values\n    predictions = []\n    for i in range (h):\n        prediction = moving_average(series, k)\n        predictions.append(prediction)\n        series = np.append(series, [[x] for x in prediction], axis=1)   \n    return np.array(predictions).T\n\n\n#Exponential smoothing\ndef exponential_smoothing(trainingdata, trend = 'add', damped = True, seasonal = 'add', seasonal_periods = 28,\\\n                          smoothing_level = 0, smoothing_slope = 0, smoothing_seasonal = 1, optimized = True, forecasting_period = 28,\\\n                         use_boxcox = False, remove_bias = True, use_basinhopping = False):\n    \n    '''\n    #########parameters explained#################\n    \n    #trainingdata = df of training data\n    \n    #trend = \"add\" or \"mul\"\n    #damped = True or False\n    #seasonal = \"add\" or \"mul\"\n    #seasonal_periods = seasonal periods # of seasonal periods in a cycle (e.g. 7 days for weekly)\n    \n    #smoothing_level = float (0:1)\n    #smoothing_seasonal = float (0:1) 1 is include full strenght seasonality,\n    #seasonality strength level\n    #optimized = Estimate model parameters by maximizing the log-likelihood\n    \n    #forecasting_period = int\n    '''\n    \n    resultdict = {}\n    index = 0\n    for row in trainingdata.itertuples(index=False):  \n        #more parameters can be added to this function from statsmodels.tsa.holtwinters.ExponentialSmoothing\n        ses = ExponentialSmoothing(np.array(row), trend = trend, damped = damped,\\\n                                   seasonal = seasonal,seasonal_periods = seasonal_periods )\n        fit = ses.fit(smoothing_level=smoothing_level, smoothing_slope = smoothing_slope, smoothing_seasonal=smoothing_seasonal,\\\n                      optimized = optimized,use_boxcox = use_boxcox, remove_bias = remove_bias, use_basinhopping = use_basinhopping)\n        fcast = fit.forecast(forecasting_period)\n    \n    \n        resultdict.update({index: fcast})\n        index += 1\n    \n    resultdf = pd.DataFrame(resultdict).T\n    \n    return resultdf\n\n\n######### prediction scoring #########################\n#Compute rmse\ndef rmse(predictions, y):\n    return np.sqrt(mean_squared_error(y, predictions))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#WRMSSE evaluator\nclass WRMSSEEvaluator(object):\n    \n    group_ids = ( 'all_id', 'state_id', 'store_id', 'cat_id', 'dept_id', 'item_id',\n        ['state_id', 'cat_id'],  ['state_id', 'dept_id'], ['store_id', 'cat_id'],\n        ['store_id', 'dept_id'], ['item_id', 'state_id'], ['item_id', 'store_id'])\n\n    def __init__(self, \n                 train_df: pd.DataFrame, \n                 valid_df: pd.DataFrame, \n                 calendar: pd.DataFrame, \n                 prices: pd.DataFrame):\n        '''\n        intialize and calculate weights\n        '''\n        self.calendar = calendar\n        self.prices = prices\n        self.train_df = train_df\n        self.valid_df = valid_df\n        self.train_target_columns = [i for i in self.train_df.columns if i.startswith('d_')]\n        self.weight_columns = self.train_df.iloc[:, -28:].columns.tolist()\n\n        self.train_df['all_id'] = \"all\"\n\n        self.id_columns = [i for i in self.train_df.columns if not i.startswith('d_')]\n        self.valid_target_columns = [i for i in self.valid_df.columns if i.startswith('d_')]\n\n        if not all([c in self.valid_df.columns for c in self.id_columns]):\n            self.valid_df = pd.concat([self.train_df[self.id_columns], self.valid_df],\n                                      axis=1, \n                                      sort=False)\n        self.train_series = self.trans_30490_to_42840(self.train_df, \n                                                      self.train_target_columns, \n                                                      self.group_ids)\n        self.valid_series = self.trans_30490_to_42840(self.valid_df, \n                                                      self.valid_target_columns, \n                                                      self.group_ids)\n        self.weights = self.get_weight_df()\n        self.scale = self.get_scale()\n        self.train_series = None\n        self.train_df = None\n        self.prices = None\n        self.calendar = None\n\n    def get_scale(self):\n        '''\n        scaling factor for each series ignoring starting zeros\n        '''\n        scales = []\n        for i in tqdm(range(len(self.train_series))):\n            series = self.train_series.iloc[i].values\n            series = series[np.argmax(series!=0):]\n            scale = ((series[1:] - series[:-1]) ** 2).mean()\n            scales.append(scale)\n        return np.array(scales)\n    \n    def get_name(self, i):\n        '''\n        convert a str or list of strings to unique string \n        used for naming each of 42840 series\n        '''\n        if type(i) == str or type(i) == int:\n            return str(i)\n        else:\n            return \"--\".join(i)\n    \n    def get_weight_df(self) -> pd.DataFrame:\n        \"\"\"\n        returns weights for each of 42840 series in a dataFrame\n        \"\"\"\n        day_to_week = self.calendar.set_index(\"d\")[\"wm_yr_wk\"].to_dict()\n        weight_df = self.train_df[[\"item_id\", \"store_id\"] + self.weight_columns].set_index(\n            [\"item_id\", \"store_id\"]\n        )\n        weight_df = (\n            weight_df.stack().reset_index().rename(columns={\"level_2\": \"d\", 0: \"value\"})\n        )\n        weight_df[\"wm_yr_wk\"] = weight_df[\"d\"].map(day_to_week)\n        weight_df = weight_df.merge(\n            self.prices, how=\"left\", on=[\"item_id\", \"store_id\", \"wm_yr_wk\"]\n        )\n        weight_df[\"value\"] = weight_df[\"value\"] * weight_df[\"sell_price\"]\n        weight_df = weight_df.set_index([\"item_id\", \"store_id\", \"d\"]).unstack(level=2)[\n            \"value\"\n        ]\n        weight_df = weight_df.loc[\n            zip(self.train_df.item_id, self.train_df.store_id), :\n        ].reset_index(drop=True)\n        weight_df = pd.concat(\n            [self.train_df[self.id_columns], weight_df], axis=1, sort=False\n        )\n        weights_map = {}\n        for i, group_id in enumerate(tqdm(self.group_ids, leave=False)):\n            lv_weight = weight_df.groupby(group_id)[self.weight_columns].sum().sum(axis=1)\n            lv_weight = lv_weight / lv_weight.sum()\n            for i in range(len(lv_weight)):\n                weights_map[self.get_name(lv_weight.index[i])] = np.array(\n                    [lv_weight.iloc[i]]\n                )\n        weights = pd.DataFrame(weights_map).T / len(self.group_ids)\n\n        return weights\n\n    def trans_30490_to_42840(self, df, cols, group_ids, dis=False):\n        '''\n        transform 30490 sries to all 42840 series\n        '''\n        series_map = {}\n        for i, group_id in enumerate(tqdm(self.group_ids, leave=False, disable=dis)):\n            tr = df.groupby(group_id)[cols].sum()\n            for i in range(len(tr)):\n                series_map[self.get_name(tr.index[i])] = tr.iloc[i].values\n        return pd.DataFrame(series_map).T\n    \n    def get_rmsse(self, valid_preds) -> pd.Series:\n        '''\n        returns rmsse scores for all 42840 series\n        '''\n        score = ((self.valid_series - valid_preds) ** 2).mean(axis=1)\n        rmsse = (score / self.scale).map(np.sqrt)\n        return rmsse\n\n    def score(self, valid_preds: Union[pd.DataFrame, np.ndarray]) -> float:\n        assert self.valid_df[self.valid_target_columns].shape == valid_preds.shape\n\n        if isinstance(valid_preds, np.ndarray):\n            valid_preds = pd.DataFrame(valid_preds, columns=self.valid_target_columns)\n\n        valid_preds = pd.concat([self.valid_df[self.id_columns], valid_preds],\n                                axis=1, \n                                sort=False)\n        valid_preds = self.trans_30490_to_42840(valid_preds, \n                                                self.valid_target_columns, \n                                                self.group_ids, \n                                                True)\n        self.rmsse = self.get_rmsse(valid_preds)\n        self.contributors = pd.concat([self.weights, self.rmsse], \n                                      axis=1, \n                                      sort=False).prod(axis=1)\n        return np.sum(self.contributors)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Main part:","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Preprocessing","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(print_dir())\n\nINPUT_DIR = '/kaggle/input/m5-forecasting-accuracy'\ndf_cal = pd.read_csv(f'{INPUT_DIR}/calendar.csv') #calendar data\ndf_sp = pd.read_csv(f'{INPUT_DIR}/sell_prices.csv') #selling prices\ndf_ss = pd.read_csv(f'{INPUT_DIR}/sample_submission.csv') # sample submission\ndf_stv = pd.read_csv(f'{INPUT_DIR}/sales_train_validation.csv')# sales train validation\n\nINPUT_DIR_methods = '../input/m5methods'\nweightsdata = pd.read_csv(f'{INPUT_DIR_methods}/validation/weights_validation.csv')\nweightsdata = weightsdata.loc[weightsdata['Level_id'] == 'Level12']\n\nids = sorted(list(set(df_stv['id'])))\nd_cols = [c for c in df_stv.columns if 'd_' in c]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Train - Validation split","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#train/val split\ntrain_dataset = df_stv[d_cols[-112:-28]] #use last 84 (28*3) days for training\nval_dataset = df_stv[d_cols[-28:]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### First model: moving average","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions_ma =  moving_averages(train_dataset, h=28, k=35)\nprint(\"ma shape \",predictions_ma.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Second model: Exponential smoothing","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions_es = exponential_smoothing(train_dataset, smoothing_level = 0, trend = 'add', damped = False, seasonal = 'add', seasonal_periods = 28,\\\n                          smoothing_slope = 0, smoothing_seasonal = 0.5, optimized = False, forecasting_period = 28, use_boxcox = False,\\\n                                       remove_bias = True, use_basinhopping = False)\npredictions_es = predictions_es.to_numpy()\nprint(\"ES shape \", predictions_es.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Grid Search\nSource: https://machinelearningmastery.com/how-to-grid-search-triple-exponential-smoothing-for-time-series-forecasting-in-python/ ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# grid search holt winter's exponential smoothing\nfrom math import sqrt\nfrom multiprocessing import cpu_count\nfrom joblib import Parallel\nfrom joblib import delayed\nfrom warnings import catch_warnings\nfrom warnings import filterwarnings\nfrom statsmodels.tsa.holtwinters import ExponentialSmoothing\nfrom sklearn.metrics import mean_squared_error\nfrom numpy import array","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# one-step Holt Winter’s Exponential Smoothing forecast\ndef exp_smoothing_forecast(history, config):\n\ta_vals,t,t_vals,d,s,s_vals,p,b,r,bas = config\n\t# define model\n\thistory = array(history)\n\tmodel = ExponentialSmoothing(history, trend=t, damped=d, seasonal=s, seasonal_periods=p)\n\t# fit model\n\tmodel_fit = model.fit(smoothing_level = a_vals,smoothing_slope = t_vals,smoothing_seasonal = s_vals, optimized=True,\\\n                          use_boxcox=b, remove_bias=r, use_basinhopping = bas)\n\t# make one step forecast\n\tyhat = model_fit.predict(len(history), len(history))\n\treturn yhat[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# root mean squared error or rmse\ndef measure_rmse(actual, predicted):\n\treturn sqrt(mean_squared_error(actual, predicted))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split a univariate dataset into train/test sets\ndef train_test_split(data, n_test):\n \treturn data[:-n_test], data[-n_test:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# walk-forward validation for univariate data\ndef walk_forward_validation(data, n_test, cfg):\n\tpredictions = list()\n\t# split dataset\n\ttrain, test = train_test_split(data, n_test)\n\t# seed history with training dataset\n\thistory = [x for x in train]\n\t# step over each time-step in the test set\n\tfor i in range(len(test)):\n\t\t# fit model and make forecast for history\n\t\tyhat = exp_smoothing_forecast(history, cfg)\n\t\t# store forecast in list of predictions\n\t\tpredictions.append(yhat)\n\t\t# add actual observation to history for the next loop\n\t\thistory.append(test[i])\n\t# estimate prediction error\n\terror = measure_rmse(test, predictions)\n\treturn error","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# score a model, return None on failure\ndef score_model(data, n_test, cfg, debug=False):\n\tresult = None\n\t# convert config to a key\n\tkey = str(cfg)\n\t# show all warnings and fail on exception if debugging\n\tif debug:\n\t\tresult = walk_forward_validation(data, n_test, cfg)\n\telse:\n\t\t# one failure during model validation suggests an unstable config\n\t\ttry:\n\t\t\t# never show warnings when grid searching, too noisy\n\t\t\twith catch_warnings():\n\t\t\t\tfilterwarnings(\"ignore\")\n\t\t\t\tresult = walk_forward_validation(data, n_test, cfg)\n\t\texcept:\n\t\t\terror = None\n\t# check for an interesting result\n\tif result is not None:\n\t\tprint(' > Model[%s] %.3f' % (key, result))\n\treturn (key, result)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# grid search configs\ndef grid_search(data, cfg_list, n_test, parallel=True):\n\tscores = None\n\tif parallel:\n\t\t# execute configs in parallel\n\t\texecutor = Parallel(n_jobs=cpu_count(), backend='multiprocessing')\n\t\ttasks = (delayed(score_model)(data, n_test, cfg) for cfg in cfg_list)\n\t\tscores = executor(tasks)\n\telse:\n\t\tscores = [score_model(data, n_test, cfg) for cfg in cfg_list]\n\t# remove empty results\n\tscores = [r for r in scores if r[1] != None]\n\t# sort configs by error, asc\n\tscores.sort(key=lambda tup: tup[1])\n\treturn scores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def exp_smoothing_configs(seasonal=[None]):\n    models = list()\n    # define config lists\n    a_vals = np.linspace(0, 1, 5) #alpha coef\n    t_params = ['add', None] #trend\n    t_vals = np.linspace(0, 1, 5) #trend coef\n    d_params = [False] #dampened trend\n    s_params = ['add', None] #seasonality\n    s_vals = np.linspace(0, 1, 5) #seasonality coef\n    p_params = np.linspace(0, 112, 5) #number of seasonal periods\n    b_params = [False] #use boxcox\n    r_params = [True, False] #remove bias\n    use_basinhopping = [True, False] #use bassinhopping\n\n    # create config instances\n    for alpha in a_vals:\n        for t in t_params:\n            for tv in t_vals:\n                for d in d_params:\n                    for s in s_params:\n                        for sv in s_vals:\n                            for p in p_params:\n                                for b in b_params:\n                                    for r in r_params:\n                                        for bas in use_basinhopping:\n                                            cfg = [alpha,t,tv,d,s,sv,p,b,r,bas]\n                                            models.append(cfg)\n    return models","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if __name__ == '__main__':\n    # define dataset\n    data = np.mean(df_stv[d_cols[-364:]], axis=0)\n    print(data)\n    # data split\n    n_test = 28\n    # model configs\n    cfg_list = exp_smoothing_configs()\n    # grid search\n    scores = grid_search(data, cfg_list, n_test)\n    print('done')\n    # list top 3 configs\n    for cfg, error in scores[:3]:\n        print(cfg, error)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for cfg, error in scores[:10]:\n    print(cfg, error)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"file = open('configs.txt', 'w')\ncounter = 0\nfor cfg, error in scores[:50]:\n    counter +=1\n    string = f'{counter} {cfg}: {error}\\n'\n    file.write(string)\n\nfile.close() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### See results","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Compute the wrmsse scores\ntrain_fold_df = df_stv.iloc[:, :-28]\nvalid_fold_df = df_stv.iloc[:, -28:].copy()\nwrmsse = WRMSSEEvaluator(train_fold_df, valid_fold_df, df_cal, df_sp) #instantiate wrmsse class\nprint(f'Movering Average total WRMSSE score { wrmsse.score( predictions_ma)}\\n',\n      f'Exponential Smoothing total WRMSSE score: {wrmsse.score( predictions_es)}') ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot a random product\nrand_int = random.randint(0, val_dataset.shape[0])\n\n#plot moving average predictions\nplot_predictions(predictions_ma, rand_int, \"moving average\", 84, 112)\nprint(\"RMSE: \", rmse( val_dataset.iloc[rand_int,:], predictions_ma[rand_int,:]))\n\n\n#plot exponential smoothing predictions\nplot_predictions(predictions_es, rand_int, \"exponential smoothing\", 84, 112)\nprint(\"RMSE: \", rmse( val_dataset.iloc[rand_int,:], predictions_es[rand_int,:]))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Optimizing Models","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Optimizing MA\n#Which k returns the best score for moving average\nerrors = []\narng =  np.arange(5,75,5)\nfor i in arng:\n    preds = moving_averages(train_dataset, h=28, k=i)\n    errors.append(rmse(preds, val_dataset))\nsns.set()\n_ = plt.plot(arng, errors)\n_ = plt.xlabel('k value')\n_ = plt.ylabel('rmse')\n_ = plt.title('Plot of the the parameter k and the rmse for the moving average model')\n_ = plt.plot()\nk = (np.argmin(errors)+1)*5\nprint(\"best k value: \", k)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission\n\nRun and submit the best model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions =  moving_averages(df_stv[d_cols[-35:]], h=28, k=k\n#predictions = exponential_smoothing(df_stv[d_cols[-364:]], smoothing_level = 0, trend = 'add', damped = False, seasonal = 'add', seasonal_periods = 28,\\\n                           smoothing_slope = 0, smoothing_seasonal = 0.5, optimized = False, forecasting_period = 28, use_boxcox = False,\\\n                                        remove_bias = True, use_basinhopping = False) #using the last 364 days to make a prediction\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"first_cols = df_stv.loc[:,'id']\nresults = pd.concat([first_cols, predictions], axis=1)\nresults.columns= ['id']+[f'F{x}' for x in range(1,29)]\ndf_ss = pd.concat([results, df_ss.loc[30490:]],axis=0)\ndf_ss.to_csv('submission.csv', index=False) #make csv","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_ss.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_ss","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}