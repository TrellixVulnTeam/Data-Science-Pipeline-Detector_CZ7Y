{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Credits\n\nThis notebook uses [some parts of this nice kernel](https://www.kaggle.com/rohanrao/m5-how-to-get-your-public-lb-score-rank). Please upvote it too.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The M5 competition loss is a little ugly and so far the best available impelementations, even the most fine-tuned ones are around <font color=\"red\">10 s</font>. This makes thing harder if one wants to quick-test some ideas or build some grid-search pipelines as you'll be spending all your **CPU** time computing the loss function.  \n\nMy implementation is based on a deep inspection of the loss combined with an intelligent choice of pandas' components in order to put the running time for the **WRMSSE** loss around <font color=\"blue\">500 ms</font> !!  \n> Don't get us wrong, we won't be using any **TPU** or **GPU** accelerators here :) .  \n\n**Too much talk, let's dive in it !**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<h3 style=\"color: red;\">If you find this work helpful, please don't forget upvoting in order to get me motivated in sharing my hard work<h3/>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from pathlib import Path\nimport pandas as pd, numpy as np\nnp.set_printoptions(suppress=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"! ls ../input","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DATA_ROOT = Path(\"../input/m5-forecasting-accuracy\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Let's configure everything here","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"class M5Config:\n    def __init__(self,cat_cols=None,sales_path=None, add_fake_categories=True, start=1,\n                     end = 1913, days=None, evaluation=False,\n                read_calendar=True, read_sales=True, read_prices=True, read_sample_submission=False):\n        self.cat_cols = [\"id\", \"cat_id\", \"state_id\", \"dept_id\", \"store_id\", \"item_id\"] if cat_cols is None else cat_cols\n        self.col_groups = [\n                ('Total', 'X'),\n                ('cat_id', 'X'),\n                ('state_id', 'X'),\n                ('dept_id', 'X'),\n                ('store_id', 'X'),\n                ('item_id', 'X'),\n                ('state_id', 'cat_id'),\n                ('state_id', 'dept_id'),\n                ('store_id', 'cat_id'),\n                ('store_id', 'dept_id'),\n                ('state_id','item_id'),\n                ('item_id', 'store_id')]\n\n        self.evaluation = False\n        self.suffix = \"evaluation\" if self.evaluation else \"validation\"\n\n        self.sales_path = DATA_ROOT/f'sales_train_{self.suffix}.csv' if sales_path is None else sales_path\n        self.calendar_path = DATA_ROOT/\"calendar.csv\"\n        self.prices_path = DATA_ROOT/\"sell_prices.csv\"\n        self.sample_submission_path = DATA_ROOT/\"sample_submission.csv\"\n\n        self.add_fake_categories = add_fake_categories\n\n        self.start = start\n        self.end = end\n\n        \n        if days is None:\n            self.set_days()\n        else:\n            self.days = days\n\n        assert end > 28\n        self.set_weight_days()\n        \n        self.read_calendar = read_calendar\n        self.read_sales = read_sales\n        self.read_prices = read_prices\n        self.read_sample_submission = False\n        \n    def set_days(self):\n        self.days = [f\"d_{i}\" for i in range(self.start,self.end+1)]\n    \n    def set_weight_days(self):\n        self.weight_days = [f\"d_{i}\" for i in range(self.end-27, self.end+1)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# And what if we make a generic class to laod the data when needed ?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class M5Data:\n    def __init__(self, config=None):\n        self.config = config if config is not None else M5Config()\n        \n        self.cal = self.read(self.config.calendar_path) if self.config.read_calendar else None\n        self.sales = self.read(self.config.sales_path, usecols = self.config.cat_cols\n                               + self.config.days) if self.config.read_sales else None\n        self.prices = self.read(self.config.prices_path) if self.config.read_prices else None\n        self.sample_submission = self.read(self.config.sample_submission_path)\\\n                                            if self.config.read_sample_submission else None\n        \n        if self.config.add_fake_categories:\n            self.add_fake_categories()\n            \n        \n    def read(self, path, usecols=None):\n        return pd.read_csv(path.as_posix(), usecols=usecols)\n    \n    \n    def add_fake_categories(self):\n        self.sales[\"Total\"] = \"Total\"\n        self.sales[\"X\"] = \"X\"\n        \n        \n        \n    \n    def _to_42840(self, group):\n        assert group in self.config.col_groups\n#         print(group, self.sales.columns)\n        group = list(group)\n        df = self.sales[group+self.config.days].groupby(group)[self.config.days].sum()\n        df.reset_index(inplace=True)\n        df.rename(columns={group[0]:\"level1_val\", group[1]:\"level2_val\"}, inplace=True)\n        df[\"level1_name\"] = group[0]\n        df[\"level2_name\"] = group[1]\n        df[\"group_id\"] = self.config.col_groups.index(tuple(group))\n        df = df[[\"group_id\", \"level1_name\", \"level2_name\", \"level1_val\", \"level2_val\"]+self.config.days]\n        \n        return df\n    \n    def to_42840(self):\n        \n        df = pd.concat([self._to_42840(group) for group in self.config.col_groups], axis=0,sort=False)\n        df.sort_values([ \"level1_val\", \"level2_val\"], inplace=True)\n        df.reset_index(drop=True, inplace=True)\n        \n        return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### You can give a try to the M5Data API here :","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\ndata = M5Data()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.sales.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\ndata._to_42840((\"item_id\", \"store_id\")).head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Powerful API for Weights & Scales computation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class WeightAndScaleComputer:\n    def __init__(self, config=None, data=None):\n        self.config = config if config is not None else M5Config()\n        self.data = data if data is not None else M5Data(config=self.config)\n        \n        self.df_42840 = self.data.to_42840()\n        \n    def get_prices(self):\n        prices = self.data.prices.copy()\n        cal = self.data.cal\n\n        prices[\"id\"] = [\"{}_{}_{}\".format(item_id, store_id, self.config.suffix) \n                            for item_id, store_id in zip(prices.item_id, prices.store_id)] \n        \n        day_count = cal[\"d\"].str.replace(\"d_\", \"\").astype(int)\n\n        prices = prices[[\"wm_yr_wk\", \"id\", \"sell_price\"]].merge(\n                cal.loc[(day_count>= self.config.end-27) & (day_count<= self.config.end), [\"wm_yr_wk\", \"d\"]],\n                                                on = [\"wm_yr_wk\"])\n        prices = prices.set_index([\"id\", \"d\"]).sell_price.fillna(0.).unstack().fillna(0.)\n        return prices\n        \n    \n    def get_weights(self):\n        \n        # Backup old data\n        sales_backup = self.data.sales\n        df_42840_backup  = self.df_42840\n\n        data = self.data\n        sales = data.sales\n        \n        sales.sort_values(\"id\",inplace=True)\n        sales.reset_index(inplace=True, drop=True)\n        \n        prices = self.get_prices()\n        prices.sort_index(inplace=True)\n        prices.reset_index(inplace=True, drop=True)\n        prices = prices[self.config.weight_days]\n        \n        for i,col in enumerate(self.config.weight_days):\n            sales[col] = sales[col]*prices[col].values\n            \n        data.sales = sales\n        df_42840 = data.to_42840()\n        \n        df_42840[\"turnover\"] = df_42840[self.config.weight_days].sum(axis=1)\n        df_42840[\"level_turnover\"] = df_42840.groupby([\"level1_name\",\"level2_name\"]).turnover.transform(\"sum\")\n        df = df_42840[[\"group_id\", \"level1_name\", \"level2_name\", \"level1_val\", \"level2_val\"]].copy()\n        df[\"weights\"] = df_42840[\"turnover\"]/df_42840[\"level_turnover\"].values\n        \n        df.sort_values([\"level1_val\", \"level2_val\"], inplace=True)\n        df.reset_index(drop=True,inplace=True)\n        \n        # Restore old data\n        self.data.sales = sales_backup\n        self.df_42840 = df_42840_backup\n        \n        return df\n    \n    \n    def get_scales(self, kind=\"mae\"): # kind in ['mae', 'mse']\n        assert kind in ['mae', 'mse']\n        \n        df = self.df_42840[self.config.days].values\n        \n        diff = (np.abs if kind == \"mae\" else np.square )(df[:, 1:] - df[:, :-1]) \n        \n        is_start = df[:, :-1].cumsum(1) >= 1\n        \n        diff *= is_start\n        \n        starts = is_start.argmax(1)\n        size = df.shape[1] - starts - 1\n        \n        scales = diff.sum(1)/size\n        \n        df = self.df_42840[[\"level1_val\", \"level2_val\"]].copy()\n        df[\"scales\"] = scales\n        \n        df.sort_values([\"level1_val\", \"level2_val\"], inplace=True)\n        df.reset_index(drop=True,inplace=True)\n        \n        return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Weighted Root Mean Squared Scaled Error ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class WeightedRootMeanSquaredScaledError:\n        \"\"\"A fast routine for the  Weighted Root Mean Squared Scaled Error (WRMSSE).\n\nThis might be slow (one to two minutes) at initialisation in order to initiate all the routines required \nto accelerate on-the-fly WRMSSE computation.\n\"\"\"\n        \n        \n        def __init__(self, scales, weights, data=None):\n            self.set_weights_and_scales(weights = weights, scales =  scales)\n            self._scales = self.weights_and_scales[\"scales\"].values\n            self._weights = self.weights_and_scales[\"weights\"].values\n            \n            \n            if data is None:\n                config = M5Config(sales_path=DATA_ROOT/\"sales_train_evaluation.csv\", start = 1914, end=1914+27,\n                                        read_calendar=False,read_prices=False,read_sample_submission=False)\n                data = M5Data(config)\n                data.sales[\"id\"] = data.sales[\"id\"].str.replace(\"evaluation\", \"validation\")\n                \n            self.data = data\n            self.df_42840 = self.data.to_42840()[self.data.config.days]\n                \n            self.cat_data = M5Data(M5Config(read_calendar=False,read_prices=False,read_sample_submission=False))\n            self.cat_data  = self.cat_data.sales[self.cat_data.config.cat_cols]\n            \n            self.submission_config = M5Config(cat_cols = [\"id\"], days = [f\"F{i}\" for  i in range(1,29)],\n                                read_calendar=False,read_prices=False,read_sample_submission=False)\n            \n        def set_weights_and_scales(self, weights, scales):\n            weights = weights.merge(scales,on=[\"level1_val\", \"level2_val\"])\n            weights = weights.sort_values([\"level1_val\", \"level2_val\"]).reset_index(drop=True)\n            \n            self.weights_and_scales = weights\n            \n        def score(self, y_pred ):\n            \"\"\"Compute the WRMSSE.\n            \n            Parameters:\n            -----------\n            y_true: pd.DataFrame, Path,str-path, M5Data \n                pd.DataFram or path to a pd.DataFrame that consists of daily 30490-42840x28 evaluation data.\n                This dataframe must includes the 'id' column. \n                \n            y_pred: pd.DataFrame, Path,str-path, M5Data \n                pd.DataFram or path to a pd.DataFrame that consists of daily 30490-42840x28 prediction data.\n                This dataframe must includes the 'id' column.\n            \"\"\"\n            y_true = self.df_42840.values\n            y_pred = self.get_sub_data(y_pred)\n            assert y_true.shape == y_pred.shape\n            \n            rmsse = np.sqrt(np.square(y_true - y_pred).mean(1)/self._scales)\n            wrmsse = np.sum(self._weights*rmsse)/12.\n            \n            return wrmsse\n        \n        def get_sub_data(self, data):\n            config = self.submission_config\n            days = list(config.days)\n            \n            if isinstance(data, (str, Path)):\n                config.sales_path = Path(data)\n                data = M5Data(config)\n            \n            elif isinstance(data, pd.DataFrame):\n                sales = data\n                config.read_sales = False\n                config.add_fake_categories = False\n                data = M5Data(config)\n                data.sales = sales\n                config.read_sales = True\n                config.add_fake_categories = True\n                data.add_fake_categories()\n                \n            else:\n                assert isinstance(data, M5Data), \"The object type `{}` is not valid.\".format(type(data))\n                \n                    \n            data.sales = data.sales[[\"id\",\"X\",\"Total\"]+ days].merge(self.cat_data, on=\"id\")\n            \n            sales = data.to_42840()  if len(data.sales) < 42840 else data.sales\n            \n            return sales[days].values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Some tests for our loss API","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nwsc = WeightAndScaleComputer()\nwrmsse_scales = wsc.get_scales(kind=\"mse\")\nweights = wsc.get_weights()\nwrmsse_scales.shape, weights.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wrmsse_scales[(weights.level1_name==\"state_id\")&(weights.level2_name==\"cat_id\")]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's test it !\nNow we can give a real-world test to our API by using [this neat dataset](https://www.kaggle.com/kneroma/accuracy-best-public-lbs). Please upvote the dataset to make it more visible for all.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"WRMSSE =  WeightedRootMeanSquaredScaledError(scales = wrmsse_scales, weights=weights )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"! ls ../input/accuracy-best-public-lbs","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### My older submission (public kernel)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"kkiller_048874 = pd.read_csv(\"../input/accuracy-best-public-lbs/Kkiller_FirstPublicNotebookUnder050_048874.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nWRMSSE.score(kkiller_048874)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###  Ragnar's one","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ragnar_064127 = pd.read_csv(\"../input/accuracy-best-public-lbs/Ragnar_VeryFirstModel_064127.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nWRMSSE.score(ragnar_064127)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Konstantin's one","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"konstantin_064127 = pd.read_csv(\"../input/accuracy-best-public-lbs/Konstantin_ThreeShadesofDark_047506.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nWRMSSE.score(konstantin_064127)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Get your leaderboard position\n> And Fast.ai :)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Public LB Score\nWe can use the actual validation data labels to score our models and get the exact public LB score. For predicting on the final test data it is highly recommended to rerun your models with including the new validation data available.\n\nThe code below can be used to get your public LB score. The [dataset of M5 public LB](https://www.kaggle.com/rohanrao/m5-accuracy-final-public-lb) can be used to get your public LB rank.\n\nSo you can now work without needing to make submissions.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# public LB rank\ndef get_lb_rank(score):\n    \"\"\"\n    Get rank on public LB as of 2020-05-31 23:59:59\n    \"\"\"\n    df_lb = pd.read_csv(\"../input/m5-accuracy-final-public-lb/m5-forecasting-accuracy-publicleaderboard-rank.csv\")\n\n    return (df_lb.Score <= score).sum() + 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The scores and rank should match the public LB scores as of 31st May, 2020 midnight.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nprint(\"Kkiller's LB:\", get_lb_rank(WRMSSE.score(kkiller_048874)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nprint(\"Ragnar's LB:\", get_lb_rank(WRMSSE.score(ragnar_064127)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nprint(\"Konstantin's LB:\", get_lb_rank(WRMSSE.score(konstantin_064127)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Notes\n* Even if I put a lot of efforts making the evaluation function faster, there could still some room for improvements so your contribs' would be highly appreciated\n* I'm hard-working currently to port this implementation on Pytorch/Tensorflow in order to be able to use it directly as loss function. Stay tuned !","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align:center;size:1.2rem;weight:200\">Thank you for reading my Kernel through the end!</p>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<h1 style=\"color:pink;text-align:center\">Good Lunck Folks</h1>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}