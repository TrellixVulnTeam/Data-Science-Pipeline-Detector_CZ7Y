{"cells":[{"metadata":{},"cell_type":"markdown","source":"This notebook should serve as a very simple example and starting point of how to shape the data to be used in an LSTM model. Note that for now, we ignore all features except the timeseries data and choose a somewhat arbitrary number of timesteps. It should however give a basic starting point and an indicator of a baseline score for using an LSTM model."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport gc\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\n\nimport os\n\nfrom tqdm import trange, tqdm_notebook\n\nfrom keras import backend as K\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Flatten\nfrom keras.layers import Dropout\nfrom keras.layers import Reshape\nfrom keras.layers import LSTM\nfrom tensorflow.compat.v1.keras.layers import CuDNNLSTM \nfrom keras.layers import Conv1D\nfrom keras.utils import to_categorical\nfrom keras.layers import MaxPooling1D\nfrom keras.layers import  GlobalAveragePooling1D\nfrom keras.utils import to_categorical\n\nimport tensorflow as tf\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First lets load and compress the data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"input_path = \"../input/m5-forecasting-accuracy\"\n\ndef get_salesval_coltypes():\n    keys = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'] + \\\n        [f\"d_{i}\" for i in range(1, 1914)]\n    values = ['object', 'category', 'category', 'category', 'category', 'category'] +\\\n        [\"uint16\" for i in range(1, 1914)]\n    return dict(zip(keys, values))\n\nsubmission = pd.read_csv(os.path.join(input_path, 'sample_submission.csv'))\nsales_train_val = pd.read_csv(os.path.join(input_path, 'sales_train_validation.csv'), \n                              dtype=get_salesval_coltypes())\n\n#calendar = pd.read_csv(os.path.join(input_path, 'calendar.csv'))\n#sell_prices = pd.read_csv(os.path.join(input_path, 'sell_prices.csv'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now lets reshape the data into the 3D inputs required by the LSTM. As a starting point we'll use input sequences of 100 timesteps to predict 28 steps ahead."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Prepare scalars to normalize data\ninput_scaler = MinMaxScaler()\noutput_scaler = StandardScaler()\n\n# Our timeseries data is in cols d_1 to d_1913\ndata = sales_train_val.iloc[:, 6:]\n#data = (data-data.min())/(data.max()-data.min())\n\n# For LSTM, X needs to be a stack of shape (samples, timesteps, features)\n# So aiming at a shape of  = (~order of 30490 * timesteps, 28, 1)\n\n# For later - test train split, for now just get shapes right\nbase = []\npredictions = []\n\ntimesteps = 100\nprediction_steps = 28\n\n# Well just iterate through slicing timesteps until we get somewhat near the end. With a\n# proper train test split, we could be more precise\nfor i in range(1, 12):\n    samples = data.iloc[:, i*timesteps:i*timesteps+timesteps]\n    preds = data.iloc[:, i*timesteps+timesteps:i*timesteps+timesteps+prediction_steps]\n    base.extend(samples.to_numpy())\n    predictions.extend(preds.to_numpy())\n    #print(f\"Samples {samples.shape}, preds {preds.shape}\")\n\n# Scale and reshape our input\nX_train = np.array(base)\ninput_scaler.fit(X_train)\nX_train = input_scaler.transform(X_train)\nX_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n\n# Scale our prediction labels\nY_train_orig = np.array(predictions)\noutput_scaler.fit(Y_train_orig)\nY_train = output_scaler.transform(Y_train_orig)\nprint(X_train.shape)\nprint(Y_train.shape)\n\n# Note this could be horrible on memory. Later, need to look at generating this in batches\ndel predictions\ndel base\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we've prepared the data, lets create the required LSTM model based on the input shapes"},{"metadata":{"trusted":true},"cell_type":"code","source":"def root_mean_squared_error(y_true, y_pred):\n        return K.sqrt(K.mean(K.square(y_pred - y_true)))\n\nsteps = X_train.shape[1]\nn_features = X_train.shape[2]\nn_steps_out = Y_train.shape[1]\n\nmodel = tf.keras.Sequential()\nmodel.add(CuDNNLSTM(100, return_sequences=True, input_shape=(steps, n_features)))\nmodel.add(CuDNNLSTM(50))\n#model.add(LSTM(100, activation='relu'))\nmodel.add(tf.keras.layers.Dense(n_steps_out))\nmodel.compile(optimizer='adam', loss=root_mean_squared_error) # this loss needs changing to competition loss.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And finally, train the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\n# 0.6345 200 56\n# 0.5633 200 56 4m 14s\n\nmodel.fit(X_train, Y_train, epochs=2, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we have a trained model, we need to take the last set of timesteps from the input data and get our final predictions."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\n# Take a slice of n{timesteps} from the input data\nx_pred = data.iloc[:,-timesteps:].to_numpy()\n\n# Reshape to fit the format for input scalar\nx_pred = x_pred.reshape((len(sales_train_val), x_pred.shape[1]))\n# Normalize the input\nx_pred = input_scaler.transform(x_pred)\n# Reshape to fit the format for LSTM model\nx_pred = x_pred.reshape((len(sales_train_val), x_pred.shape[1], 1))\n\n# Get our predictions\nraw_pred = model.predict(x_pred)\n\n# Inverse to transform to get the predictions at the right scale\nall_pred = output_scaler.inverse_transform(raw_pred)\n# Round the predictions back to integers\nall_pred = np.round(np.abs(all_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To finish, we just need to stack our predictions into the format required for the submission file.\n\nAs we only predicted one set of 28 days, lets just stack them twice into the results file. This wouldn't be satisfactory for a final attempt on the private leaderboard, but for now while developing a model, it will do."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Stack our predictions into a dataframe\nvalidation = pd.concat([pd.DataFrame(all_pred[:,0:prediction_steps]), pd.DataFrame(all_pred[:,-prediction_steps:])])\nvalidation = validation.astype(int)\n\n# Reset index to match the submission dataframe\nvalidation.reset_index(inplace=True, drop=True)\n\n# Add the id column from the submission dataframe to our results\nvalidation['id'] = submission.id\nvalidation = validation.reindex(\n        columns=['id'] + [c for c in validation.columns if c != 'id'], copy=False)\n\n# Add the correct colummn names for the submission file format\nvalidation.columns = ['id'] + [f\"F{i}\" for i in range(1, 29)]\n\nvalidation.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}