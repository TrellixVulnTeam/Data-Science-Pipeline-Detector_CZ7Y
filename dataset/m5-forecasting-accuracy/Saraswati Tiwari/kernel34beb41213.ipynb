{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\nimport pandas as pd\nimport numpy as np\nimport dask.dataframe as dd\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_rows', 500)\nimport matplotlib.pyplot as plt\nimport seaborn as sns; sns.set()\nimport lightgbm as lgb\n#import dask_xgboost as xgb\n#import dask.dataframe as dd\nfrom sklearn import preprocessing, metrics\nfrom sklearn.preprocessing import LabelEncoder\nimport gc\nimport os\nfrom tqdm import tqdm\nfrom scipy.sparse import csr_matrix\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sell_prices = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sell_prices.csv')\nsell_prices.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sell_prices['id'] = sell_prices['item_id'] + '_' + sell_prices['store_id']\nsell_prices['sell_price'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sales_train_evaluation.csv')\nsales.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales.info()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"calendar = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/calendar.csv')\ncalendar.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"calendar.info()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_training = sales.iloc[:,6:]\nsales_training.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sample_submission.csv')\nsample_submission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rows = [0, 42, 1024, 10024]\nfig, axes = plt.subplots(nrows=len(rows), ncols=1, figsize=(10,15))\nfor ax, row in zip(axes, rows):\n    sales_training.iloc[row].plot(ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rows = [0, 42, 1024, 10024]\nfig, axes = plt.subplots(nrows=len(rows), ncols=1, figsize=(10,15))\nfor ax, row in zip(axes, rows):\n    sales_training.iloc[row].rolling(30).mean().plot(ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tsa.stattools import adfuller\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_training.iloc[12791].plot()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stationary_train_sales = np.diff(sales_training.values, axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler(with_mean=False)\nscaler.fit(stationary_train_sales.T)\nX_train = scaler.transform(stationary_train_sales.T).T\nscales = scaler.scale_\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"calendar\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_normalized = calendar[['wm_yr_wk','d']].iloc[:1941]\nsales_normalized = pd.DataFrame(X_train, columns=sales_normalized['d'][1:])\nsales_normalized.insert(0, 'id', sales['item_id'] + '_' + sales['store_id'])\nsales_normalized","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rows = [0, 42, 1024, 10024]\nfig, axes = plt.subplots(nrows=len(rows), ncols=1, figsize=(10,15))\nfor ax, row in zip(axes, rows):\n    sales_normalized.iloc[row, 1:].plot(ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_normalized\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rows = [42, 10024]\nfig, axes = plt.subplots(nrows=len(rows), ncols=1, figsize=(10,15))\nfor ax, row in zip(axes, rows):\n    integrated_series = np.cumsum(sales_normalized.iloc[row, 1:]*scales[row])\n    c = sales_training.iloc[row, 0]\n    integrated_series = pd.Series(integrated_series + c).shift(1)\n    integrated_series[:100].plot(ax=ax, style='r--', legend=True, label='re-integrated')\n    sales_training.iloc[row][:100].plot(ax=ax, legend=True, label='original')\n    total_numerical_error = np.abs(np.array(pd.Series(integrated_series)[1:].to_numpy() - sales_training.iloc[row,1:-1].to_numpy())).sum()\n    ax.set_title('Total numerical error: {:.2f}'.format(total_numerical_error))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(sell_prices['id'].value_counts(), kde=False, axlabel='number of weeks the product was priced on')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_two_week_sum = sales_training.rolling(14, axis=1).sum()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in range(13):\n    sales_two_week_sum.iloc[:, col] = sales_two_week_sum.iloc[:, 13]\n    \nis_off_the_shelf = sales_two_week_sum == 0\n#to the days when the products were off for 14 last days we add those 14 days\nis_off_the_shelf = is_off_the_shelf | is_off_the_shelf.shift(-13, axis=1)\nis_on_the_shelf = is_off_the_shelf == False\n# True/False to 1/0\n# is_on_the_shelf = is_on_the_shelf.astype('int')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"is_on_the_shelf\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rows = [0, 42, 1024, 10024]\nfig, axes = plt.subplots(nrows=len(rows), ncols=1, figsize=(10,15))\nfor ax, row in zip(axes, rows):\n    shelf = pd.DataFrame(is_on_the_shelf.iloc[row])\n    shelf.columns = ['is_on_the_shelf']\n    shelf['sold'] = sales_training.iloc[row]\n    shelf = shelf.reset_index()\n    shelf.drop('index', inplace=True, axis=1)\n    shelf[shelf['is_on_the_shelf'] == True]['sold'].plot(legend=True, label='on shelf', ax=ax)\n    shelf[shelf['is_on_the_shelf'] == False]['sold'].plot(style='o', legend=True, label='not on shelf', ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales['dept_id'].value_counts()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales['cat_id'].value_counts()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales['state_id'].value_counts()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder = OneHotEncoder()\ndept_encoded = encoder.fit_transform(sales['dept_id'].values.reshape(-1,1))\ncat_encoded = encoder.fit_transform(sales['cat_id'].values.reshape(-1,1))\nstate_encoded = encoder.fit_transform(sales['state_id'].values.reshape(-1,1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.impute import SimpleImputer\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imputer = SimpleImputer(strategy='constant',fill_value='no_event')\nimputed_calendar_primary = imputer.fit_transform(calendar['event_name_1'].to_numpy().reshape(-1,1))\nimputed_calendar_secondary = imputer.fit_transform(calendar['event_name_2'].to_numpy().reshape(-1,1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imputed_calendar = np.hstack((imputed_calendar_primary,imputed_calendar_secondary))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# a quick note - this has for some reason already beed 'differenciated', by which a mean the holidays lasting for few days\n# are denoted as beggining and end of the holliday\nencoder = OneHotEncoder()\ncalendar_encoded = encoder.fit_transform(imputed_calendar)\n\n# the line meaning that no event happens dubled so we throw one out\ncalendar_encoded = calendar_encoded[:,:-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# never forget to equally differentiate every time series!\nis_on_the_shelf_diff = is_on_the_shelf.diff(axis=1).iloc[:,1:]\nis_on_the_shelf_diff = is_on_the_shelf_diff.astype('int')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_training\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class M5_SeriesGenerator:\n    def __init__(self):\n        self.day_zero = 1941\n        self.max_rows = 30490\n        self.rows_remaining = np.arange(self.max_rows)\n        \n    def reset(self):\n        self.rows_remaining = np.arange(self.max_rows)\n        \n    def next_batch(self, in_points=30, out_points=3, batch_size=10):\n        X_batch = []\n        X_past_batch = []\n        scale_batch = []\n        c_batch = []\n        facts_batch = []\n        y_batch = []\n        \n        for _ in range(batch_size):\n            if self.rows_remaining.shape[0] == 0:\n                return False, (None, None)\n        \n            row = np.random.randint(self.rows_remaining.shape[0])\n            self.rows_remaining = np.delete(self.rows_remaining, row)\n            X_train_start = self.day_zero-366-in_points\n            X_prev_year_start = self.day_zero-2*365\n\n            while is_on_the_shelf.iloc[row, X_train_start+in_points] == False:\n                if self.rows_remaining.shape[0] == 0:\n                    return False, (None, None)\n                row = np.random.randint(self.rows_remaining.shape[0])\n                self.rows_remaining = np.delete(self.rows_remaining, row)\n\n            Xsales_train = sales_normalized.iloc[row, X_train_start+1:X_train_start+in_points+1].values.astype(np.float32)\n            Xsales_prev_year = sales_normalized.iloc[row, X_prev_year_start:X_prev_year_start+out_points].values.astype(np.float32)\n\n            Y_train_start = X_train_start+in_points\n            Yprices_train = train_prices.iloc[row, Y_train_start+2:Y_train_start+out_points+2].values.astype(np.float32)\n            Yevents_train = calendar_encoded[Y_train_start+1:Y_train_start+out_points+1, :].toarray().astype(int)\n            Ydept_train = np.tile(dept_encoded[row].toarray().astype(int),(out_points,1))\n            Ycat_train = np.tile(cat_encoded[row].toarray().astype(int),(out_points,1))\n            Ystate_train = np.tile(state_encoded[row].toarray().astype(int),(out_points,1))\n            Ysales_train = sales_training.iloc[row, Y_train_start+1:Y_train_start+out_points+1].values.astype(int).flatten()\n            \n            Yfacts = np.hstack((Yprices_train.reshape(-1, 1), Yevents_train, Ydept_train, Ycat_train, Ystate_train))\n            integral_constant = sales_training.iloc[row, X_train_start+in_points]\n            scale = scales[row]\n            \n            X_batch.append(Xsales_train.reshape(-1, 1))\n            X_past_batch.append(Xsales_prev_year.reshape(-1, 1))\n            scale_batch.append(scale)\n            c_batch.append(integral_constant)\n            facts_batch.append(Yfacts)\n            y_batch.append(Ysales_train)\n        return True, ((np.asarray(X_batch), np.concatenate((np.asarray(X_past_batch), np.asarray(facts_batch)), axis=2), np.asarray(scale_batch), np.asarray(c_batch)), np.asarray(y_batch))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def eval_series_data_gen(in_points = 120, out_points=28, end_of_data=1913, max_row=30490):    \n    row = 0\n    while row < max_row:\n        X_batch = []\n        X_past_batch = []\n        scale_batch = []\n        c_batch = []\n        facts_batch = []\n        y_batch = []\n        X_start = end_of_data-1-in_points\n        X_prev_year_start = end_of_data-365\n\n        Y_start = X_start+in_points\n        Ysales = sales_training.iloc[row, Y_start+1:Y_start+out_points+1].values.astype(int).flatten()\n        y_batch.append(Ysales)\n        \n        if is_on_the_shelf.iloc[row, X_start+in_points] == False:\n            row += 30490//max_row\n            yield False, (None, np.asarray(y_batch))\n        else:\n            Xsales = sales_normalized.iloc[row, X_start+1:X_start+in_points+1].values.astype(np.float32)\n            Xsales_prev_year = sales_normalized.iloc[row, X_prev_year_start:X_prev_year_start+out_points].values.astype(np.float32)\n\n\n            Yprices = train_prices.iloc[row, Y_start+2:Y_start+out_points+2].values.astype(np.float32)\n            Yevents = calendar_encoded[Y_start+1:Y_start+out_points+1, :].toarray().astype(int)\n            Ydept = np.tile(dept_encoded[row].toarray().astype(int),(out_points,1))\n            Ycat = np.tile(cat_encoded[row].toarray().astype(int),(out_points,1))\n            Ystate = np.tile(state_encoded[row].toarray().astype(int),(out_points,1))\n            \n\n            Yfacts = np.hstack((Yprices.reshape(-1, 1), Yevents, Ydept, Ycat, Ystate))\n            integral_constant = sales_training.iloc[row, X_start+in_points]\n            scale = scales[row]\n\n            X_batch.append(Xsales.reshape(-1, 1))\n            X_past_batch.append(Xsales_prev_year.reshape(-1, 1))\n            scale_batch.append(scale)\n            c_batch.append(integral_constant)\n            facts_batch.append(Yfacts)\n            \n            row += 30490//max_row\n            yield True, ((np.asarray(X_batch), np.concatenate((np.asarray(X_past_batch), np.asarray(facts_batch)), axis=2), np.asarray(scale_batch), np.asarray(c_batch)), np.asarray(y_batch))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import BatchNormalization","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class M5_Net(keras.Model):\n    def __init__(self, input_timesteps, output_timesteps, batch_size=1):\n        super(M5_Net, self).__init__()\n        self.input_timesteps = input_timesteps\n        self.output_timesteps = output_timesteps\n        self.batch_size = batch_size\n\n        self.gru1 = tf.keras.layers.GRU(32, return_sequences=True)\n        self.gru1a = tf.keras.layers.GRU(64, return_sequences=True)\n        self.gru2 = tf.keras.layers.GRU(64, return_sequences=True)\n        self.gru2a = tf.keras.layers.GRU(32, return_sequences=True)\n        self.gru_out = tf.keras.layers.GRU(1, return_sequences=True)\n        self.dense1 = keras.layers.Dense(self.output_timesteps, activation=\"selu\", kernel_initializer=\"lecun_normal\")\n        \n    def call(self, input_data):\n        series_data, historical_data, scale, integral_constant = input_data\n        \n        x = BatchNormalization()(self.gru1(series_data))\n        x = BatchNormalization()(self.gru1a(x))\n        x = tf.reshape(x, [self.batch_size, -1])\n        x = BatchNormalization()(self.dense1(x))\n        x = tf.reshape(x, [self.batch_size, -1, 1])\n        x = tf.concat([x,\n                       historical_data,\n                       np.expand_dims(np.tile(integral_constant, (self.output_timesteps,1)).T, axis=2),\n                       np.expand_dims(np.tile(scale, (self.output_timesteps,1)).T, axis=2)\n                      ], axis=2)\n        x = BatchNormalization()(self.gru2(x))\n        x = BatchNormalization()(self.gru2a(x))\n        x = BatchNormalization()(self.gru_out(x))\n        x = tf.reshape(x, [self.batch_size, -1])\n        \n        @tf.function\n        def inverse_normalize(x):\n            sales_pred = tf.transpose(tf.math.multiply(tf.transpose(x), y=scale))\n            sales_pred = tf.math.cumsum(sales_pred, axis=1)\n            sales_pred += np.tile(integral_constant, (self.output_timesteps,1)).T\n            return sales_pred\n        \n        sales_pred = inverse_normalize(x)\n        return sales_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from math import sqrt\n\nIN_POINTS = 120\nOUT_POINTS = 28\nBATCH_SIZE = 16\nmodel = M5_Net(input_timesteps=IN_POINTS, output_timesteps=OUT_POINTS, batch_size=BATCH_SIZE)\n\nloss_object = tf.keras.losses.MeanSquaredError()\n\ndef loss(model, x, y, training):\n    y_ = model(x, training=training)\n\n    return loss_object(y_true=y, y_pred=y_)\n\ndef grad(model, inputs, targets):\n    with tf.GradientTape() as tape:\n        loss_value = loss(model, inputs, targets, training=True)\n    return loss_value, tape.gradient(loss_value, model.trainable_variables)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\nM5_series_gen = M5_SeriesGenerator()\nbatch_sequence = []\ntraining = []\n\nVAL_SIZE = 1000\nvalidation = []\nfor epoch in range(len(batch_sequence)):\n    BATCH_SIZE = batch_sequence[epoch]\n    model.batch_size = BATCH_SIZE\n    epoch_loss = []\n    more_data_available, (X_train, y_train) = M5_series_gen.next_batch(in_points=IN_POINTS, out_points=OUT_POINTS, batch_size=BATCH_SIZE)\n    while True:\n        more_data_available, (X_train, y_train) = M5_series_gen.next_batch(in_points=IN_POINTS, out_points=OUT_POINTS, batch_size=BATCH_SIZE)\n        if more_data_available == False:\n            break;\n            \n        loss_value, grads = grad(model, X_train, y_train)\n        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n        clipped_loss = loss_object(y_true=y_train, y_pred=tf.clip_by_value(model(X_train, training=True), clip_value_min=0, clip_value_max=np.inf))\n        epoch_loss.append(sqrt(clipped_loss))\n        print(epoch_loss[-1])\n    training.append(np.array(epoch_loss).mean())\n    epoch_val_loss = []\n    model.batch_size = 1\n    for on, (X_val, y_true) in eval_series_data_gen(in_points=IN_POINTS, out_points=OUT_POINTS, end_of_data=1913, max_row=VAL_SIZE):\n        if on:\n            y_val = tf.clip_by_value(model(X_val, training=True), clip_value_min=0, clip_value_max=np.inf)\n        else:\n            y_val = np.zeros((1,OUT_POINTS))\n        val_loss = loss_object(y_true=y_true, y_pred=y_val)\n        epoch_val_loss.append(val_loss)\n    model.batch_size = BATCH_SIZE\n    validation.append(np.array(epoch_val_loss).mean())\n    print(f'Epoch {epoch} training loss: {training[-1]}, Epoch {epoch} validation loss: {validation[-1]}')\n    model.save_weights('./croc_model{}.ckpt'.format(epoch))\n    M5_series_gen.reset()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pd.Series(training).plot(legend=True, label='training')\n# pd.Series(validation).plot(legend=True, label='validation')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}