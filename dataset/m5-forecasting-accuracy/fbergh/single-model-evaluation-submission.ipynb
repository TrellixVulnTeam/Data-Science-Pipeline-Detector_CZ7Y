{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Single Model Evaluation Submission\n\nLink to the notebook with submission on validation data: [https://www.kaggle.com/fbergh/best-model-parameter-tuning?scriptVersionId=35311989](https://www.kaggle.com/fbergh/best-model-parameter-tuning?scriptVersionId=35311989)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Imports","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport gc #garbage collection\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # plotting\nfrom datetime import datetime, timedelta, date # handling dates\nfrom tqdm.notebook import tqdm # progress bars\n\n# LightGBM\nimport lightgbm as lgb","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Global variables","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Set this to true if you want only one iteration to run, for testing purposes.\nTEST_RUN = False\n\nMODEL_VERSION = 'final'\n\n# Do not truncate view when max_cols is exceeded\n# ref: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.set_option.html\npd.set_option('display.max_columns', 50) \n\n# Path to Data Folder\nKAGGLE_DATA_FOLDER = '/kaggle/input/m5-forecasting-accuracy'\n# Path to submission on validation data\nSUBMISSION_PATH = \"/kaggle/input/lgbmindividualbestsubmission/submission.csv\"\n\n# Path to Model over All Categories\nMODEL_PATH = \"/kaggle/input/lgbmindividualbestsubmission/model_v13_param_tuning.lgb\"\n\nBACKWARD_LAG = 60\nEND_DAY = 1913 + 28\n# Use this if you do not want to load in all data\n# Full data starts at 2011-01-29\n#BEGIN_DATE = '2015-02-11' \n#BEGIN_DATE = '2014-8-01' # Best submission so far\nBEGIN_DATE = '2013-08-01'\n#BEGIN_DATE = '2013-01-01'\n#BEGIN_DATE = '2012-01-01'\nBEGIN_DAY = str((datetime.strptime(BEGIN_DATE, '%Y-%m-%d') - datetime.strptime('2011-01-29', '%Y-%m-%d')).days)\nEVAL_SPLIT = '2016-05-22' # In this phase of the competition, this is the end date\nprint(datetime.strptime(EVAL_SPLIT, '%Y-%m-%d'))\nTASK_TYPE='CPU'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loading data and preprocessing","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Data types ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# N.B. LightGBM specifically requires the 'category' dtype\n# E.g. see https://stackoverflow.com/questions/56070396/why-does-categorical-feature-of-lightgbm-not-work\nCALENDAR_DTYPES = {\n    'date':             'str',\n    'wm_yr_wk':         'int16', \n    'weekday':          'category',\n    'wday':             'int16', \n    'month':            'int16', \n    'year':             'int16', \n    'd':                'object',\n    'event_name_1':     'category',\n    'event_type_1':     'category',\n    'event_name_2':     'category',\n    'event_type_2':     'category',\n    'snap_CA':          'int16', \n    'snap_TX':          'int16', \n    'snap_WI':          'int16'\n}\nPARSE_DATES = ['date']\nSALES_PRICES_DTYPES = {\n    'store_id':    'category', \n    'item_id':     'category', \n    'wm_yr_wk':    'int16',  \n    'sell_price':  'float32',\n    'sales': 'float32'\n}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Loading with preprocessing\n\n#### Convert sales dataframe from wide to long format\n\nWhereas in the \"wide\" dataframe one row contains columns with the corresponding sales/demand per day (1913 days at the moment), the new \"long\" dataframe has a new entry for each day. \n\nThe resulting dataframe (assuming you use all days) will therefore have 1913-1 less \"day\" columns (1919-1912+1 = 8 columns), and 30490x1913=58.327.370 rows.\n\nThis is achieved with [pandas.melt](https://pandas.pydata.org/docs/reference/api/pandas.melt.html) by specifying identifier variables and measured variables ('day' in this case), as well as the name of the output value.\n\nConvert sales_train_validation such that it becomes a function of day with output of sales/demand.\nUnpivots everything not set as id_var, so by default value_vars are all day entries.\n\n#### Merging dataframes\n\n[pandas merge doc](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.merge.html)\n\n- \"left\": left outer join that uses keys from left dataframe\n- left dataframe is \"sales_train_validation\", in which we have just defined the \"day\" column header\n- join \"day\" (e.g. d_1) on \"d\" from the calendar dataframe (also of form d_1)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_data(train=True):\n    \"\"\"\n    Load data\n    \"\"\"\n    \n    ##### SALES_TRAIN_VALIDATION\n    \n    print(\"Loading train and validation data\")   \n    # Dtype magic from https://www.kaggle.com/kneroma/m5-first-public-notebook-under-0-50#Changes\n    # Required to make LightGBM deal with categorical values\n    numcols = [f\"d_{day}\" for day in range(int(BEGIN_DAY),END_DAY+1)]\n    catcols = ['id', 'item_id', 'dept_id','store_id', 'cat_id', 'state_id']\n    dtype = {numcol:\"float32\" for numcol in numcols} \n    dtype.update({col: \"category\" for col in catcols if col != \"id\"})\n    \n    sales_train_validation = pd.read_csv(os.path.join(KAGGLE_DATA_FOLDER, 'sales_train_evaluation.csv'),\n                                                     usecols=catcols+numcols, dtype=dtype)\n    for col in catcols:\n        if col != \"id\":\n            sales_train_validation[col] = sales_train_validation[col].cat.codes.astype(\"int16\")\n            sales_train_validation[col] -= sales_train_validation[col].min()\n    \n    if not train:\n        # Add columns for future 28 days, 1914-1941\n        for day in range(END_DAY+1, END_DAY+28+1):\n            sales_train_validation[f\"d_{day}\"] = 0  # TODO this was np.nan before\n\n        # Then only keep data from the last BACKWARD_LAG days        \n        # If we remove the 'd_' prefix, we can compare day numbers\n        value_vars = [column for column in sales_train_validation.columns \n                              if (column.startswith('d_') and int(column.replace('d_', ''))>= END_DAY - BACKWARD_LAG)]\n    else:\n        # Immediately throw away all days before BEGIN_DAY\n        # Doing this so early is important because pd.melt increases memory significantly\n        value_vars = [col for col in sales_train_validation.columns \n                      if (col.startswith('d_') and (int(col.replace('d_', '')) >= int(BEGIN_DAY)))]\n    \n    print(\"Shape:\", sales_train_validation.shape )\n    print(\"Memory usage (Mb) before melting:\", sales_train_validation.memory_usage().sum() / 1024**2)\n    \n    sales_train_validation = pd.melt(\n        sales_train_validation, \n        id_vars = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'],\n        value_vars=value_vars,\n        var_name = 'day',\n        value_name = 'sales')\n    print(\"Completed melting, new shape:\", sales_train_validation.shape )\n    print(\"Colums after melting:\", sales_train_validation.columns)\n    print(\"Memory usage (Mb) after melting:\", sales_train_validation.memory_usage().sum() / 1024**2)\n    \n    columns = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']\n    \n    ####### CALENDAR\n    \n    print(\"Loading calendar\")\n    # Parse dates parses the dates as datetime objects! Pandas provides some nice functions on datetime objects.\n    calendar = pd.read_csv(os.path.join(KAGGLE_DATA_FOLDER, 'calendar.csv'), dtype=CALENDAR_DTYPES, parse_dates=['date'])\n    print(\"Calendar columns: \", calendar.columns)\n    print(\"Memory usage (Mb) calendar: \", calendar.memory_usage().sum() / 1024**2)\n    calendar.rename(columns={'d':'day'}, inplace=True)\n\n    for col, col_dtype in CALENDAR_DTYPES.items():\n        if col_dtype == \"category\":\n            calendar[col] = calendar[col].cat.codes.astype(\"int16\")\n            calendar[col] -= calendar[col].min()\n            \n    # Merge sales_train_validation and calendar\n    sales_train_validation = sales_train_validation.merge(calendar, on=\"day\", copy=False)\n    del calendar; gc.collect()\n    print(\"Merged calendar (in place)\")\n    print(\"Memory usage (Mb) after merging calendar:\", sales_train_validation.memory_usage().sum() / 1024**2)\n    print(\"Colums after merge:\", sales_train_validation.columns)\n    \n    ####### SELL PRICES\n    \n    sell_prices = pd.read_csv(os.path.join(KAGGLE_DATA_FOLDER, 'sell_prices.csv'), dtype=SALES_PRICES_DTYPES)\n    \n    # From https://www.kaggle.com/kneroma/m5-first-public-notebook-under-0-50#Changes\n    # TODO investigate normalization step\n    for col, col_dtype in SALES_PRICES_DTYPES.items():\n        if col_dtype == \"category\":\n            sell_prices[col] = sell_prices[col].cat.codes.astype(\"int16\")\n            sell_prices[col] -= sell_prices[col].min()\n\n    print(\"Memory usage (Mb) sell prices:\", sell_prices.memory_usage().sum() / 1024**2)\n    \n    columns = ['item_id', 'store_id', 'sell_price']\n    for feature in columns:\n        if feature == 'sell_price':\n            sell_prices[feature].fillna(0, inplace=True)\n    \n    # Merge in sell prices\n    sales_train_validation = sales_train_validation.merge(sell_prices, on=[\"store_id\",\"item_id\",\"wm_yr_wk\"], copy=False)\n    del sell_prices; gc.collect()\n    print(\"Merged sales prices (in place)\")\n    print(\"Memory usage (Mb) after merging sales:\", sales_train_validation.memory_usage().sum() / 1024**2)\n    print(\"Colums after merge:\", sales_train_validation.columns)\n     \n    #submission = pd.read_csv(os.path.join(KAGGLE_DATA_FOLDER, 'sample_submission.csv'))  \n    #return reduce_mem_usage(calendar), reduce_mem_usage(sell_prices), reduce_mem_usage(sales_train_validation)\n    return sales_train_validation","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature engineering\n\nNote that the make_features converts 'day' from an object to an integer, so be aware of this side-effect. I should probably move this to the load_data function.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"See [this tutorial of autoregression](https://machinelearningmastery.com/autoregression-models-time-series-forecasting-python/)\n\nInspiration: from [here](https://www.kaggle.com/vgarshin/m5-catboost), and from [here](https://www.kaggle.com/kneroma/m5-first-public-notebook-under-0-50#Changes).","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Q&A\n\nQ by Blazej: Why are you calculating rolling means of lags instead of rolling means of the actual values?\n\nA by Vlad-Marius Griguta: \n\n> Good question. The reason for using lagged values of the target variable is to reduce the effect of self-propagating errors through multiple predictions of the same model.\nThe objective is to predict 28 days in advance in each series. Therefore, to predict the 1st day in the series you can use the whole series of sales (up to lag1). However, to predict the 8th day you only have actual data for up to lag8 and to predict the whole series you have actuals up to lag28. What people have done at the beginning of the competition was to only use features computed from up to lag28 and apply regression (e.g. lightGBM). This is the safest option, as it does not require the use of 'predictions on predictions'. At the same time, it restrains the capacity of the model to learn features closer to the predicted values. I.e., it underperforms at predicting the 1st day, which could use much more of the latest values in the series than lag28. What this notebook is doing is to find a balance between 'predicting on predictions' and using the latest available information. Using features based on a lag that has some seasonal significance (lag7) seems to give positive results, while the fact that only two features (lag7 and rmean7_7) self-propagate errors keep the over-fitting problem under control.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"- N.B. month and year are already in the data. In this case, the only thing we do is reduce memory to int16. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# This function is copied from m5_catboost \n# minor change: I renamed 'd' to 'day'\n# minor change: I pass dates as strings, not datetime, so I convert them\n# The date_features contain pandas functions defined on datetimeIndex,\n# e.g. https://www.geeksforgeeks.org/python-pandas-datetimeindex-weekofyear/\ndef make_lag_features(strain):    \n    \"\"\"\n    N.B. If you adjust this function, make sure to also adjust make_features_for_day() below\n    \"\"\"\n    \n    # 1. Lagged sales\n    print('in dataframe:', strain.shape)\n    print(\"headers:\", strain.columns)\n    lags = [7, 28]\n    lag_cols = ['lag_{}'.format(lag) for lag in lags ]\n    for lag, lag_col in zip(lags, lag_cols):\n        # ATTENTIE: hier stond eerst id\n        strain[lag_col] = strain[['item_id', 'sales']].groupby('item_id')['sales'].shift(lag)\n    print('lag sales done')\n    \n    # 2. Rolling means for id (so aggregated sales on item, independent of store and state etc.)\n    windows= [7, 28]\n    #for window in windows:\n    #    for lag, lag_col in zip(lags, lag_cols):\n    #        window_col = f'id_rmean_{lag_col}_{window}'\n    #        strain[window_col] = strain[['id', lag_col]].groupby('id')[lag_col].transform(\n    #            lambda x: x.rolling(window).mean()\n    #        )\n    #    print(f'Rolling mean sales done for window {window} per item')\n       \n    # 3. Rolling means for item_id (items per state and per category type, so quite specific!)\n    for window in windows:\n        for lag, lag_col in zip(lags, lag_cols):\n            window_col = f'item_id_rmean_{lag_col}_{window}'\n            strain[window_col] = strain[['item_id', lag_col]].groupby('item_id')[lag_col].transform(\n                lambda x: x.rolling(window).mean()\n            )\n        print(f'Rolling mean sales done for window {window} per item_id')\n    \n    # 4. Rolling means for store_id, last week and last month\n    #for window in windows:\n    #    for lag, lag_col in zip(lags, lag_cols):\n    #        window_col = f'store_id_rmean_{lag_col}_{window}'\n    #        strain[window_col] = strain[['store_id', lag_col]].groupby('store_id')[lag_col].transform(\n    #            lambda x: x.rolling(window).mean()\n    #        )\n          \n    #window = 28\n    #lag_col = 'lag_28'\n    #window_col = f'store_id_rmean_{lag_col}_{window}'\n    #[window_col] = strain[['store_id', lag_col]].groupby('store_id')[lag_col].transform(\n    #    lambda x: x.rolling(window).mean())\n    #print(f'Rolling mean sales done for lag {lag_col} and window {window} per store_id')\n    \ndef make_date_features(dt):\n    # 3. New date features (values are corresponding pandas functions)\n    # Again, month and year are already in the original data\n    date_features = {\n        'week': 'weekofyear',\n        'quarter': 'quarter',\n        'mday': 'day',\n        \"wday\": \"weekday\",\n        \"month\": \"month\",\n        \"year\": \"year\"\n    }\n    \n    # Additional potential date features\n    # \"ime\": \"is_month_end\",\n    # \"ims\": \"is_month_start\",\n    \n    for date_feat_name, date_feat_func in date_features.items():\n        if date_feat_name in dt.columns:\n            dt[date_feat_name] = dt[date_feat_name].astype(\"int16\")\n        else:\n            dt[date_feat_name] = getattr(dt['date'].dt, date_feat_func).astype('int16') \n        \n    print('date features done')\n    dt['day'] = dt['day'].apply(lambda x: int(x.replace('d_', '')))  \n    print('out dataframe:', dt.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time \nsales_train_evaluation = load_data()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nsales_train_evaluation[\"sale\"] = ((sales_train_evaluation['sell_price'] * 100 % 10) < 6).astype('int8')\nmake_lag_features(sales_train_evaluation)\nmake_date_features(sales_train_evaluation)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train_evaluation.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Drop NaNs","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"before = len(sales_train_evaluation)\nsales_train_evaluation.dropna(inplace = True)\nafter = len(sales_train_evaluation)\nprint(f\"Reduced {(before-after)/before}%\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Splitting into train, validation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\n# SIMPLE SPLIT\nprint(\"Splitting data into train, validation, evaluation set\")\nnp.random.seed(777)\nSIZE = 2_000_000\n\nvalidation_idx = np.random.choice(sales_train_evaluation.index.values, SIZE, replace = False)\ntrain_idx = np.setdiff1d(sales_train_evaluation.index.values, validation_idx) # set difference\n\ntrain = sales_train_evaluation.loc[train_idx]\nvalidation = sales_train_evaluation.loc[validation_idx]\ndel sales_train_evaluation; gc.collect()\n\nprint(\"Train:\", train.shape)\nprint(\"Validation:\", validation.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.describe())\nprint(validation.describe())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_features = [\n    'item_id', 'dept_id', 'store_id', 'cat_id', 'state_id',\n    'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2'\n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"drop = [\"id\", \"date\", \"sales\", \"day\", \"wm_yr_wk\", \"weekday\"]\ntrain_columns = train.columns[~train.columns.isin(drop)]\nprint(train_columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## LGBM Model Definition","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_pool = lgb.Dataset(\n    data=train[train_columns],\n    label=train[\"sales\"], \n    categorical_feature=categorical_features,\n    free_raw_data=False)\ndel train; gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_pool = lgb.Dataset(\n    data=validation[train_columns],\n    label=validation[\"sales\"],\n    categorical_feature=categorical_features,\n    free_raw_data=False\n)\ndel validation; gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if TEST_RUN:\n    ITERATIONS=1\nelse:\n    ITERATIONS = 1200\n\nparams = {\n           \"objective\" : \"poisson\",\n           \"metric\" : \"rmse\",\n           \"force_row_wise\" : True,\n           \"learning_rate\" : 0.075,\n           \"sub_row\" : 0.75,\n           \"bagging_freq\" : 1,\n           \"lambda_l2\" : 0.1,\n           'verbosity': 1,\n           'num_iterations': ITERATIONS,\n           'num_leaves': 128,\n           \"min_data_in_leaf\": 100,\n           \"early_stopping\": 10\n         }","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nmodel = lgb.train(\n    params,\n    train_pool,\n    valid_sets = val_pool,\n    verbose_eval=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save_model(\"model_{}.lgb\".format(MODEL_VERSION))\ndel train_pool, val_pool; gc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Importance","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (18.0, 4)\n%matplotlib inline \n\nfig, ax = plt.subplots(figsize=(12,8))\nlgb.plot_importance(model, max_num_features=50, height=0.8, ax=ax)\nax.grid(False)\nplt.title(\"LightGBM - Feature Importance\", fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prediction","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Load data and add future days","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#%%time\ndf = load_data(train=False)\ndf[\"sale\"] = ((df['sell_price'] * 100 % 10) < 6).astype('int8')\nmake_date_features(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Prediction loop\n\nApply the \"recursive features\" approach here:\n\n- Predict the next day based on last BACKWARD_LAG days\n- Perform feature engineering on those days (same as during training)\n- Repeat, but now include the day for which we just predicted demand","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# TODO als ik deze functie gebruik gaat iets grandioos fout\n# Returnt een numpy ndarray!\n\n# Code to just compute features only for the single prediction day\n# Adapted with several changes from https://www.kaggle.com/poedator/m5-under-0-50-optimized#Prediction-stage \ndef lag_features_for_day(dt, day):\n    print(type(dt))\n    lags = [7, 28]\n    lag_cols = [f\"lag_{lag}\" for lag in lags]\n    # 1. Lag sales\n    for lag, lag_col in zip(lags, lag_cols):\n        dt.loc[dt['date'] == str(day), lag_col] = \\\n            dt.loc[dt['date'] == str(day-timedelta(days=lag)), 'sales'].values\n    \n    windows = [7, 28]\n    for window in windows:\n        for lag, lag_col in zip(lags, lag_cols):\n            df_window = dt[(dt['date'] <= str(day-timedelta(days=lag))) & (dt['date'] > str(day-timedelta(days=lag+window)))]\n            \n            # 2. Rolling means for id (so aggregated sales on item, independent of store and state etc.)\n            #df_window_grouped = df_window.groupby(\"id\").agg({'sales':'mean'}).reindex(dt.loc[dt['date']==str(day),'id'])\n            #dt.loc[dt['date'] == str(day),f'id_rmean_{lag_col}_{window}'] = df_window_grouped.sales.values   \n            \n            # 3. Rolling means for item_id (tems per state and per category type, so quite specific!)\n            df_window_grouped = df_window.groupby(\"item_id\").agg({'sales':'mean'}).reindex(dt.loc[dt['date']==str(day),'item_id'])\n            dt.loc[dt['date'] == str(day),f'item_id_rmean_{lag_col}_{window}'] = df_window_grouped.sales.values   \n            \n    # 4. Rolling mean for store_id lag 28 last 28 days\n    lag = 28\n    lag_col = 'lag_28'\n    window = 28\n    df_window = dt[(dt['date'] <= str(day-timedelta(days=lag))) & (dt['date'] > str(day-timedelta(days=lag+window)))]\n    df_window_grouped = df_window.groupby(\"store_id\").agg({'sales':'mean'}).reindex(dt.loc[dt['date']==str(day),'store_id'])\n    dt.loc[dt['date'] == str(day),f'store_id_rmean_{lag_col}_{window}'] = df_window_grouped.sales.values\n    print(f\"Features done for day {str(day)}\") \n        \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nEND_DATE = EVAL_SPLIT\nPREDICT_DAYS = 28\n\n# Predict from 2016-05-22 on\nfor f_day in tqdm(range(1,PREDICT_DAYS+1)):\n    pred_date = (datetime.strptime(END_DATE, '%Y-%m-%d') + timedelta(days=f_day)).date()\n    print(f\"Forecasting day {END_DAY+f_day}, date: {str(pred_date)}\")\n    pred_begin_date = pred_date - timedelta(days=BACKWARD_LAG+1)\n    # Select last BACKWARD_LAG days to use for predicting\n    prediction_data = df[(df['date'] >= str(pred_begin_date)) & (df['date'] <= str(pred_date))].copy()\n    \n    # Repeat feature engineering\n    lag_features_for_day(prediction_data, pred_date)\n    \n    # Only use the columns you trained on before\n    prediction_data = prediction_data.loc[prediction_data['date'] == str(pred_date), train_columns]\n    prediction = model.predict(prediction_data)   \n    print(\"Prediction\", prediction.size, prediction)\n    df.loc[df['date'] == str(pred_date), 'sales'] = prediction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del prediction_data\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission\n\nNow let's turn the prediction into a submission file. \nWe'll wrangle the long dataframe with the predictions into the correct format.\n\ncf. https://medium.com/@durgaswaroop/reshaping-pandas-dataframes-melt-and-unmelt-9f57518c7738 \n\nWe copy the old validation data from a submission, such that we maintain our current public leaderboard score (instead of getting a score of 0). Then, the newly trained model (also on the public leaderboard data) makes predictions for the private leaderboard data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_ = pd.read_csv(os.path.join(KAGGLE_DATA_FOLDER, 'sales_train_evaluation.csv'))\nsubmission = pd.read_csv(os.path.join(SUBMISSION_PATH))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We are only interested in the predicted days\n# We need the id for the row index, the day to calculate F_{x}, and the sales for the prediction values\nsubmission_eval = df.loc[df['date'] > END_DATE, ['id', 'day', 'sales']].copy()\n\n# Do not make negative predictions\nsubmission_eval.loc[submission_eval['sales'] < 0, 'sales'] = 0\n\n# Sort on id \nsubmission_eval.sort_values('id', inplace=True)\n\nsubmission_eval['day'] = submission_eval['day'].apply(lambda x: 'F{}'.format(x - END_DAY))\nprint(submission_eval.columns)\nprint(submission_eval.head(), submission_eval.tail(), sep=\"\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we have a single 'day' column. Instead, we want to have a separate column for each day.\nThe reverse of the melt operation is `pivot` .\nAn extra 'sales' descriptor is introduced that we remove again.\n'id' will serve as the index, but we want to reintroduce it as a column for submission with `reset_index()`\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# This is required to force the correct ordering after reshaping\nf_cols = ['F{}'.format(x) for x in range(1, 28 + 1)]\n\nsubmission_eval = submission_eval.pivot(index='id', columns='day')['sales'][f_cols].reset_index(level='id')\nprint(submission_eval.head(), submission_eval.shape, submission_eval.columns, sep=\"\\n\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.iloc[30490:, 1:] = submission_eval.iloc[:,1:].to_numpy()\nprint(submission.head(), submission.tail(), sep=\"\\n\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)\nprint('Submission shape', submission.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}