{"cells":[{"metadata":{},"cell_type":"markdown","source":"# M5-Accuracy Challenge: Ensemble Learning\n\n## Team: MLiPioneers\n\n### Starter Code Credits: [M5 - Three shades of Dark: Darker magic](https://www.kaggle.com/kyakovlev/m5-three-shades-of-dark-darker-magic)\n\n### Additional Input Data:\n1. [Simple FE](https://www.kaggle.com/kyakovlev/m5-simple-fe): Simple Feature Extraction. Involves creating a Grid dataframe which stores product release date (p1), product prices (p2) after multiple aggregations, momentum and normalization, calendar dates (p3)\n\n2. [Lags Features](https://www.kaggle.com/kyakovlev/m5-lags-features): Lags Feature Extraction usings Pandas function `shift()` and computing rolling lags. Apply on Simple FE output dataframes.\n\n3. [Custom Features](https://www.kaggle.com/kyakovlev/m5-custom-features): Covers FE creation approaches, Sequential FE validation, Dimension reduction, FE validation by Permutation importance, Mean encodings, Parallelization for FE. Use Mean Encodings as feature set. \n\n4. [AUX models](https://www.kaggle.com/kyakovlev/m5-aux-models): Includes pretrained models and preprocessed test sets.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# import libraries\nimport os, sys, gc, time, warnings, pickle, psutil, random\n\n# custom imports\nfrom multiprocessing import Pool        # Multiprocess Runs\n\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import model frameworks\n# import lightgbm as lgb\nimport xgboost as xgb","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Light Gradient Boosting\n\n### Parameter Explanation: \n\n#### 'boosting_type': 'gbdt'\nwe have 'goss' option for faster training but it normally leads to underfit. Also there is good 'dart' mode but it takes forever to train and model performance depends a lot on random factor \nhttps://www.kaggle.com/c/home-credit-default-risk/discussion/60921\n\n#### 'objective': 'tweedie'\nTweedie Gradient Boosting for Extremely Unbalanced Zero-inflated Data https://arxiv.org/pdf/1811.10192.pdf and many more articles about tweediie. Strange (for me) but Tweedie is close in results to my own ugly loss. My advice here - make OWN LOSS function: https://www.kaggle.com/c/m5-forecasting-accuracy/discussion/140564 and https://www.kaggle.com/c/m5-forecasting-accuracy/discussion/143070. I think many of you already using it (after poisson kernel appeared) (kagglers are very good with \"params\" testing and tuning). Try to figure out why Tweedie works. Probably it will show you new features options or data transformation (Target transformation?).\n\n#### 'tweedie_variance_power': 1.1\ndefault = 1.5. set this closer to 2 to shift towards a Gamma distribution. set this closer to 1 to shift towards a Poisson distribution. my CV shows 1.1 is optimal but you can make your own choice.\n\n#### 'metric': 'rmse'\nDoesn't mean anything to us as competition metric is different and we don't use early stoppings here. So rmse serves just for general model performance overview. Also we use \"fake\" validation set (as it makes part of the training set) so even general rmse score doesn't mean anything)) https://www.kaggle.com/c/m5-forecasting-accuracy/discussion/133834\n\n#### 'subsample': 0.5\nServes to fight with overfit this will randomly select part of data without resampling. Chosen by CV (my CV can be wrong!). Next kernel will be about CV\n\n#### 'subsample_freq': 1\nfrequency for bagging. default value - seems ok\n\n#### 'learning_rate': 0.03\nChosen by CV. Smaller - longer training but there is an option to stop in \"local minimum\". Bigger - faster training but there is a chance to not find \"global minimum\" minimum\n\n#### 'num_leaves': 2**11-1\n\n#### 'min_data_in_leaf': 2**12-1\n\nForce model to use more features. We need it to reduce \"recursive\" error impact. Also it leads to overfit that's why we use small. \n\n#### 'max_bin': 100\n\n#### l1, l2 regularizations\nhttps://towardsdatascience.com/l1-and-l2-regularization-methods-ce25e7fc831c: Good tiny explanation. l2 can work with bigger num_leaves but my CV doesn't show boost.\n                    \n#### 'n_estimators': 1400\nCV shows that there should be different values for each state/store. Current value was chosen  for general purpose. As we don't use any early stopings careful to not overfit Public LB.\n\n#### 'feature_fraction': 0.5\nLightGBM will randomly select part of features on each iteration (tree). We have maaaany features and many of them are \"duplicates\" and many just \"noise\"\ngood values here - 0.5-0.7 (by CV).\n\n#### 'boost_from_average': False\nThere is some \"problem\" to code boost_from_average for custom loss. 'True' makes training faster BUT carefully use it https://github.com/microsoft/LightGBM/issues/1514 not our case but good to know cons.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model parameters\n# lgb_params = {\n#                     'boosting_type': 'gbdt',\n#                     'objective': 'tweedie',\n#                     'tweedie_variance_power': 1.1,\n#                     'metric': 'rmse',\n#                     'subsample': 0.5,\n#                     'subsample_freq': 1,\n#                     'learning_rate': 0.03,\n#                     'num_leaves': 2**11-1,\n#                     'min_data_in_leaf': 2**12-1,\n#                     'feature_fraction': 0.5,\n#                     'max_bin': 100,\n#                     'n_estimators': 1400,\n#                     'boost_from_average': False,\n#                     'verbose': -1,\n#                 }\n\nxgb_params = {\n#     'objective':'reg:tweedie',\n#     'tweedie_variance_power': 1.1,\n    'objective': 'reg:squarederror',\n    'eval_metric':'rmse',\n    'subsample': 0.5,\n    'max_depth':7, \n    'max_bin': 100,\n    'eta':0.03}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Helper Functions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# set random seed (to make all processes deterministic)\ndef seed_everything(seed=0):\n    random.seed(seed)\n    np.random.seed(seed)\n    \n# Multiprocess Runs\ndef df_parallelize_run(func, t_split):\n    num_cores = np.min([N_CORES,len(t_split)])\n    pool = Pool(num_cores)\n    df = pd.concat(pool.map(func, t_split), axis=1)\n    pool.close()\n    pool.join()\n    return df\n\n# Helper to load data by store ID\ndef get_data_by_store(store):\n    \n    # Read and contact basic feature\n    df = pd.concat([pd.read_pickle(BASE),\n                    pd.read_pickle(PRICE).iloc[:,2:],\n                    pd.read_pickle(CALENDAR).iloc[:,2:]],\n                    axis=1)\n    \n    # Leave only relevant store\n    df = df[df['store_id']==store]\n\n    # As our Features Grids are aligned, we can use index to keep only necessary rows.\n    df2 = pd.read_pickle(MEAN_ENC)[mean_features]\n    df2 = df2[df2.index.isin(df.index)]\n    \n    df3 = pd.read_pickle(LAGS).iloc[:,3:]\n    df3 = df3[df3.index.isin(df.index)]\n    \n    df = pd.concat([df, df2], axis=1)\n    del df2 # to not reach memory limit \n    \n    df = pd.concat([df, df3], axis=1)\n    del df3 # to not reach memory limit \n    \n    # Create features list\n    features = [col for col in list(df) if col not in remove_features]\n    df = df[['id','d',TARGET]+features]\n    \n    # Skipping first n rows\n    df = df[df['d']>=START_TRAIN].reset_index(drop=True)\n    \n    return df, features\n\n# Helper to make dynamic rolling lags\ndef make_lag(LAG_DAY):\n    lag_df = base_test[['id','d',TARGET]]\n    col_name = 'sales_lag_'+str(LAG_DAY)\n    lag_df[col_name] = lag_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(LAG_DAY)).astype(np.float16)\n    return lag_df[[col_name]]\n\ndef make_lag_roll(LAG_DAY):\n    shift_day = LAG_DAY[0]\n    roll_wind = LAG_DAY[1]\n    lag_df = base_test[['id','d',TARGET]]\n    col_name = 'rolling_mean_tmp_'+str(shift_day)+'_'+str(roll_wind)\n    lag_df[col_name] = lag_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(shift_day).rolling(roll_wind).mean())\n    return lag_df[[col_name]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Global Parameters","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Global\nVER = 1                          # Our model version\nSEED = 42                        # We want all things\nseed_everything(SEED)            # to be as deterministic \n# lgb_params['seed'] = SEED        # as possible\nxgb_params['seed'] = SEED        # as possible\nN_CORES = psutil.cpu_count()     # Available CPU cores\n\n# LIMITS and const\nTARGET      = 'sales'            # Our target\nSTART_TRAIN = 0                  # We can skip some rows (Nans/faster training)\nEND_TRAIN   = 1913               # End day of our train set\nP_HORIZON   = 28                 # Prediction horizon\nUSE_AUX     = True              # Use or not pretrained models\n\n# FEATURES to remove. These features lead to overfit\n# or values not present in test set\nremove_features = ['id','state_id','store_id',\n                   'date','wm_yr_wk','d',TARGET]\nmean_features   = ['enc_cat_id_mean','enc_cat_id_std',\n                   'enc_dept_id_mean','enc_dept_id_std',\n                   'enc_item_id_mean','enc_item_id_std'] \n\n#PATHS for Features\nORIGINAL = '../input/m5-forecasting-accuracy/'\nBASE     = '../input/m5-simple-fe/grid_part_1.pkl'\nPRICE    = '../input/m5-simple-fe/grid_part_2.pkl'\nCALENDAR = '../input/m5-simple-fe/grid_part_3.pkl'\nLAGS     = '../input/m5-lags-features/lags_df_28.pkl'\nMEAN_ENC = '../input/m5-custom-features/mean_encoding_df.pkl'\n\n# AUX(pretrained) Models paths\n# AUX_MODELS = '../input/m5-aux-models/'\nAUX_MODELS = '../input/m5-xgb-aux-models/results/'\n\n#STORES ids\nSTORES_IDS = pd.read_csv(ORIGINAL+'sales_train_validation.csv')['store_id']\nSTORES_IDS = list(STORES_IDS.unique())\n\n#CATEGORY columns\ncat_cols = ['item_id', 'dept_id', 'cat_id', 'event_name_1', 'event_type_1', 'event_name_2',\n       'event_type_2', 'snap_CA', 'snap_TX', 'snap_WI']\n\n#SPLITS for lags creation\nSHIFT_DAY  = 28\nN_LAGS     = 15\nLAGS_SPLIT = [col for col in range(SHIFT_DAY,SHIFT_DAY+N_LAGS)]\nROLS_SPLIT = []\nfor i in [1,7,14]:\n    for j in [7,14,30,60]:\n        ROLS_SPLIT.append([i,j])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Aux Models\nIf you don't want to wait hours and hours to have result you can train each store in separate kernel and then just join result.\n\nIf we want to use pretrained models we can skip training (in our case do dummy training to show that we are good with memory and you can safely use this (all kernel) code)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# if USE_AUX:\n#     lgb_params['n_estimators'] = 2\n    \n# Here is some 'logs' that can compare\n#Train CA_1\n#[100]\tvalid_0's rmse: 2.02289\n#[200]\tvalid_0's rmse: 2.0017\n#[300]\tvalid_0's rmse: 1.99239\n#[400]\tvalid_0's rmse: 1.98471\n#[500]\tvalid_0's rmse: 1.97923\n#[600]\tvalid_0's rmse: 1.97284\n#[700]\tvalid_0's rmse: 1.96763\n#[800]\tvalid_0's rmse: 1.9624\n#[900]\tvalid_0's rmse: 1.95673\n#[1000]\tvalid_0's rmse: 1.95201\n#[1100]\tvalid_0's rmse: 1.9476\n#[1200]\tvalid_0's rmse: 1.9434\n#[1300]\tvalid_0's rmse: 1.9392\n#[1400]\tvalid_0's rmse: 1.93446\n\n#Train CA_2\n#[100]\tvalid_0's rmse: 1.88949\n#[200]\tvalid_0's rmse: 1.84767\n#[300]\tvalid_0's rmse: 1.83653\n#[400]\tvalid_0's rmse: 1.82909\n#[500]\tvalid_0's rmse: 1.82265\n#[600]\tvalid_0's rmse: 1.81725\n#[700]\tvalid_0's rmse: 1.81252\n#[800]\tvalid_0's rmse: 1.80736\n#[900]\tvalid_0's rmse: 1.80242\n#[1000]\tvalid_0's rmse: 1.79821\n#[1100]\tvalid_0's rmse: 1.794\n#[1200]\tvalid_0's rmse: 1.78973\n#[1300]\tvalid_0's rmse: 1.78552\n#[1400]\tvalid_0's rmse: 1.78158","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Train Models\n# for store_id in ['WI_3']:\n#     print('Train', store_id)\n    \n#     # Get grid for current store\n#     grid_df, features_columns = get_data_by_store(store_id)\n    \n#     print('load train and valid sets')\n#     # Masks for \n#     # Train (All data less than 1913)\n#     # \"Validation\" (Last 28 days - not real validation set)\n#     # Test (All data greater than 1913 day, with some gap for recursive features)\n#     train_mask = grid_df['d']<=END_TRAIN\n#     valid_mask = train_mask&(grid_df['d']>(END_TRAIN-P_HORIZON))\n#     preds_mask = grid_df['d']>(END_TRAIN-100)\n    \n#     # Apply masks and save lgb dataset as bin\n#     # to reduce memory spikes during dtype convertations\n# #     train_data = lgb.Dataset(grid_df[train_mask][features_columns], \n# #                        label=grid_df[train_mask][TARGET])\n#     temp_df = grid_df[train_mask][features_columns].copy()\n#     for c in cat_cols:\n#         temp_df[c] = temp_df[c].cat.codes\n#     train_data = xgb.DMatrix(temp_df, \n#                        label=grid_df[train_mask][TARGET])\n#     del temp_df\n#     train_data.save_binary('train_data.bin')\n#     # load train data\n# #     train_data = lgb.Dataset('train_data.bin')\n#     train_data = xgb.DMatrix('train_data.bin')\n    \n#     # prepare validation data\n# #     valid_data = lgb.Dataset(grid_df[valid_mask][features_columns], \n# #                        label=grid_df[valid_mask][TARGET])\n#     temp_df = grid_df[valid_mask][features_columns].copy()\n#     for c in cat_cols:\n#         temp_df[c] = temp_df[c].cat.codes\n#     valid_data = xgb.DMatrix(temp_df, \n#                        label=grid_df[valid_mask][TARGET])\n#     del temp_df\n#     valid_data.save_binary('valid_data.bin')\n#     # load train data\n#     valid_data = xgb.DMatrix('valid_data.bin')\n\n#     # Saving part of the dataset for later predictions\n#     # Removing features that we need to calculate recursively \n#     grid_df = grid_df[preds_mask].reset_index(drop=True)\n#     keep_cols = [col for col in list(grid_df) if '_tmp_' not in col]\n#     grid_df = grid_df[keep_cols]\n#     grid_df.to_pickle('test_'+store_id+'.pkl')\n#     del grid_df\n    \n#     # Launch seeder again to make lgb training 100% deterministic\n#     seed_everything(SEED)\n    \n#     print('train estimator')\n#     # train estimator\n# #     estimator = lgb.train(lgb_params,\n# #                           train_data,\n# #                           valid_sets = [valid_data],\n# #                           verbose_eval = 100,\n# #                           )\n#     estimator = xgb.train(xgb_params, \n#                           train_data, \n#                           evals=[(valid_data,'eval')],\n#                           num_boost_round=100,\n#                           early_stopping_rounds=10)\n    \n#     # Save model - it's not real '.bin' but a pickle file\n# #     model_name = 'lgb_model_'+store_id+'_v'+str(VER)+'.bin'\n#     model_name = 'xgb_model_'+store_id+'_v'+str(VER)+'.bin'\n#     pickle.dump(estimator, open(model_name, 'wb'))\n\n#     # Remove temporary files and objects \n#     # to free some hdd space and ram memory\n#     !rm train_data.bin\n# #     del train_data, valid_data, estimator\n#     del train_data, estimator\n#     gc.collect()\n    \n#     # \"Keep\" models features for predictions\n#     MODEL_FEATURES = features_columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Predictions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_df, features_columns = get_data_by_store('CA_1')\ndel grid_df\nMODEL_FEATURES = features_columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Recombine Test set after training\ndef get_base_test():\n    base_test = pd.DataFrame()\n\n    for store_id in STORES_IDS:\n        temp_df = pd.read_pickle(AUX_MODELS + 'test_'+store_id+'.pkl')\n        temp_df['store_id'] = store_id\n        base_test = pd.concat([base_test, temp_df]).reset_index(drop=True)\n    \n    return base_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create Dummy DataFrame to store predictions\nall_preds = pd.DataFrame()\n\n# Join back the Test dataset with a small part of the training data to make recursive features\nbase_test = get_base_test()\n\n# Timer to measure predictions time \nmain_time = time.time()\n\n# Loop over each prediction day. As rolling lags are the most timeconsuming, we will calculate it for whole day\nfor PREDICT_DAY in range(1,29):    \n    print('Predict | Day:', PREDICT_DAY)\n    start_time = time.time()\n\n    # Make temporary grid to calculate rolling lags\n    grid_df = base_test.copy()\n    grid_df = pd.concat([grid_df, df_parallelize_run(make_lag_roll, ROLS_SPLIT)], axis=1)\n        \n    for store_id in STORES_IDS:\n        # Read all our models and make predictions for each day/store pairs\n#         model_path = 'lgb_model_'+store_id+'_v'+str(VER)+'.bin' \n        model_path = 'xgb_model_'+store_id+'_v'+str(VER)+'.bin' \n        if USE_AUX:\n            model_path = AUX_MODELS + model_path\n        # load estimator\n        estimator = pickle.load(open(model_path, 'rb'))\n        \n        # apply masks per days and stores\n        day_mask = base_test['d']==(END_TRAIN+PREDICT_DAY)\n        store_mask = base_test['store_id']==store_id\n        mask = (day_mask)&(store_mask)\n#         base_test[TARGET][mask] = estimator.predict(grid_df[mask][MODEL_FEATURES])\n        temp_df = grid_df[mask][MODEL_FEATURES].copy()\n        for c in cat_cols:\n            temp_df[c] = temp_df[c].cat.codes\n        test_data = xgb.DMatrix(temp_df)\n        del temp_df\n        test_data.save_binary('test_data.bin')\n        test_data = xgb.DMatrix('test_data.bin')\n        base_test[TARGET][mask] = estimator.predict(test_data)\n    \n    # Make good column naming and add to all_preds DataFrame\n    temp_df = base_test[day_mask][['id',TARGET]]\n    temp_df.columns = ['id','F'+str(PREDICT_DAY)]\n    if 'id' in list(all_preds):\n        all_preds = all_preds.merge(temp_df, on=['id'], how='left')\n    else:\n        all_preds = temp_df.copy()\n        \n    print('#'*10, ' %0.2f min round |' % ((time.time() - start_time) / 60),\n                  ' %0.2f min total |' % ((time.time() - main_time) / 60),\n                  ' %0.2f day sales |' % (temp_df['F'+str(PREDICT_DAY)].sum()))\n    del temp_df\n\n# clean up dataframe\nall_preds = all_preds.reset_index(drop=True)\nall_preds","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Create Submission File (Export predictions)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reading competition sample submission and merging our predictions.\n# As we have predictions only for \"_validation\" data, we need to do fillna() for \"_evaluation\" items\nsubmission = pd.read_csv(ORIGINAL+'sample_submission.csv')[['id']]\nsubmission = submission.merge(all_preds, on=['id'], how='left').fillna(0)\nsubmission.to_csv('submission_v'+str(VER)+'.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}