{"cells":[{"metadata":{},"cell_type":"markdown","source":"このNotebookはYakovlevの [M5 - Three shades of Dark: Darker magic](https://www.kaggle.com/kyakovlev/m5-three-shades-of-dark-darker-magic) を研究するために作ったものをShareしています。\n\nThank Yakovlev for sharing very much.\n\n次のNotebookにも依存していることに注意しましょう。<br>\n[【日本語】M5 - Simple FE](https://www.kaggle.com/ejunichi/m5-simple-fe)<br>\n[M5 - Custom features](https://www.kaggle.com/kyakovlev/m5-custom-features)<br>\n[M5 - Lags features](https://www.kaggle.com/kyakovlev/m5-lags-features)　","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# General imports\nimport numpy as np\nimport pandas as pd\nimport os, sys, gc, time, warnings, pickle, psutil, random\n\n# custom imports\nfrom multiprocessing import Pool        # Multiprocess Runs\n\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### ヘルパー\n#################################################################################\n## シード生成\n# ：すべてのプロセスを確定的にするためのシード     # type: int\ndef seed_everything(seed=0):\n    random.seed(seed)\n    np.random.seed(seed)\n\n    \n## マルチプロセス実行\ndef df_parallelize_run(func, t_split):\n    num_cores = np.min([N_CORES,len(t_split)])\n    pool = Pool(num_cores)\n    df = pd.concat(pool.map(func, t_split), axis=1)\n    pool.close()\n    pool.join()\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### store IDによってデータをロードするヘルパー\n#################################################################################\n# データ読込\ndef get_data_by_store(store):\n    \n    # 基本特徴量を読んで連絡する\n    # BASE,PRICE,CALENDARは「【日本語】M5 - Simple FE」を参照\n    df = pd.concat([pd.read_pickle(BASE),\n                    pd.read_pickle(PRICE).iloc[:,2:],\n                    pd.read_pickle(CALENDAR).iloc[:,2:]],\n                    axis=1)\n    \n    # 関連する店舗のみを残す\n    df = df[df['store_id']==store]\n\n    # メモリの制限があるため、LAGとエンコード特徴量を個別に読み取り、\n    # 不要なアイテムを削除する必要があります。 \n    # 特徴量グリッドが整列されるため、必要な行のみを保持するために index を使用できます\n    # concat は merge よりも少ないメモリを使用するため、整列は適切です。\n    df2 = pd.read_pickle(MEAN_ENC)[mean_features] # 「M5 - Custom features」を参照\n    df2 = df2[df2.index.isin(df.index)]\n    \n    df3 = pd.read_pickle(LAGS).iloc[:,3:] # 「M5 - Lags features」を参照\n    df3 = df3[df3.index.isin(df.index)]\n    \n    df = pd.concat([df, df2], axis=1)\n    del df2 # メモリ制限に達しないように削除\n    \n    df = pd.concat([df, df3], axis=1)\n    del df3 # メモリ制限に達しないように削除\n    \n    # 特徴量リストを作成する\n    features = [col for col in list(df) if col not in remove_features]\n    df = df[['id','d',TARGET]+features]\n    \n    # 最初のn行をスキップ\n    df = df[df['d']>=START_TRAIN].reset_index(drop=True)\n    \n    return df, features\n\n# トレーニング後にテストを再結合\ndef get_base_test():\n    base_test = pd.DataFrame()\n\n    for store_id in STORES_IDS:\n        temp_df = pd.read_pickle('test_'+store_id+'.pkl')\n        temp_df['store_id'] = store_id\n        base_test = pd.concat([base_test, temp_df]).reset_index(drop=True)\n    \n    return base_test\n\n\n########################### 動的移動LAGを作成するヘルパー\n#################################################################################\ndef make_lag(LAG_DAY):\n    lag_df = base_test[['id','d',TARGET]]\n    col_name = 'sales_lag_'+str(LAG_DAY)\n    lag_df[col_name] = lag_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(LAG_DAY)).astype(np.float16)\n    return lag_df[[col_name]]\n\n\ndef make_lag_roll(LAG_DAY):\n    shift_day = LAG_DAY[0]\n    roll_wind = LAG_DAY[1]\n    lag_df = base_test[['id','d',TARGET]]\n    col_name = 'rolling_mean_tmp_'+str(shift_day)+'_'+str(roll_wind)\n    lag_df[col_name] = lag_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(shift_day).rolling(roll_wind).mean())\n    return lag_df[[col_name]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### モデルパラメータ\n#################################################################################\nimport lightgbm as lgb\nlgb_params = {\n                    'boosting_type': 'gbdt',\n                    'objective': 'tweedie',\n                    'tweedie_variance_power': 1.1,\n                    'metric': 'rmse',\n                    'subsample': 0.5,\n                    'subsample_freq': 1,\n                    'learning_rate': 0.03,\n                    'num_leaves': 2**11-1,\n                    'min_data_in_leaf': 2**12-1,\n                    'feature_fraction': 0.5,\n                    'max_bin': 100,\n                    'n_estimators': 1400,\n                    'boost_from_average': False,\n                    'verbose': -1,\n                } ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" モデルのパラメータを詳しく見てみましょう\n\n*  'boosting_type': 'gbdt'\n \n より高速なトレーニングのための'goss'オプションがあります \n しかし、通常はアンダーフィットにつながります。 \n また、良い'dart'モードもあります \n しかし、トレーニングは永遠にかかります \n モデルのパフォーマンスは、たくさんのランダムな要素に依存します。\n https://www.kaggle.com/c/home-credit-default-risk/discussion/60921\n\n*  'objective': 'tweedie'\n\n Tweedie Gradient Boosting\n Tweedie 分布はゼロに大量の点を持つ絶対連続な確率分布で\n 各イベント X が ガンマ分布に従い、イベントの起こる回数 N が\n ポアソン分布に従う確率過程 （複合ポアソン過程）として表される。\n https://arxiv.org/pdf/1811.10192.pdf\n\n （私にとっては）奇妙ですが、Tweedieは結果に近いです \n 私自身の醜い損失に。 \n 私のアドバイスは独自の損失関数を作成することです。\n https://www.kaggle.com/c/m5-forecasting-accuracy/discussion/140564\n https://www.kaggle.com/c/m5-forecasting-accuracy/discussion/143070\n poissonカーネルが登場した後、独自の損失関数\n （Kagglerはパラメータのテストとチューニングに非常に優れています）をすでに使用していると思います。 \n Tweedieが機能する理由を理解してみてください。 \n おそらく、新特徴量のオプションまたはデータ変換（ターゲット変換？）が表示されます。\n 'tweedie_variance_power': 1.1\n default = 1.5\n ガンマ分布にシフトするには、これを2に近く設定します\n ポアソン分布にシフトするには、これを1に近づけます\n 私のCVは1.1が最適であることを示しています\n しかし、あなたはあなた自身の選択をすることができます\n\n*  'metric': 'rmse'\n\n コンペの指標が違うため、何も意味しません。\n ここでは早期停止を使用しません。\n したがって、rmseは一般的なモデルのパフォーマンスの概要のみを提供します。\n また、「偽の」検証セットを使用します。\n （トレーニングセットの一部となり、一般的なrmseスコアでも意味がないため）\n https://www.kaggle.com/c/m5-forecasting-accuracy/discussion/133834\n\n*  'subsample': 0.5\n\n オーバーフィットと戦うために役立ちます\n これはリサンプリングせずにデータの一部をランダムに選択します\n CVによって選択（私のCVは間違っている可能性があります！\n 次のカーネルはCVについてです\n\n* 'subsample_freq': 1\n\n バギングの頻度 \n デフォルト値-良さそう\n\n* 'learning_rate': 0.03\n\n CVが選択\n 小さい-より長いトレーニング\n しかし、「極小値」で停止するオプションがあります\n 大きい-より速いトレーニング\n しかし、「最小値」が見つかりません\n\n* 'num_leaves': 2**11-1\n* 'min_data_in_leaf': 2**12-1\n\n より多くの特徴量を使用するようにモデルを強制する\n 「再帰的」エラーの影響を減らすために必要です\n また、オーバーフィットにつながります\n だから小さめに \n 'max_bin': 100\n\n* l1, l2 正則化\n\n https://towardsdatascience.com/l1-and-l2-regularization-methods-ce25e7fc831c\n 小さな良い説明\n l2はより大きなnum_leavesで動作します \n しかし、私のCVはブーストを表示しません\n                    \n* 'n_estimators': 1400\n\n CVは、州/店舗ごとに異なる値が必要であることを示しています。\n 現在の値は、一般的な目的で選択されています。\n パブリックLBに適合しないように注意して早期停止を使用しないためです。\n\n* 'feature_fraction': 0.5\n\n LightGBMは、各反復（決定木）で特徴量の一部をランダムに選択します。 \n とてもたくさんの特徴量があります。\n その多くは「重複」であり、多くは単なる「ノイズ」です\n CVによる良い値 - 0.5-0.7\n\n* 'boost_from_average': False\n\n boost_from_averageにカスタム損失「True」はトレーニングをより速くしますが、\n いくつかの「問題」があります。\n 私たちのケースではありませんが、短所に注意して使用してください\n https://github.com/microsoft/LightGBM/issues/1514","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### 変数\n#################################################################################\nVER = 1                          # バージョン\nSEED = 42                        # \nseed_everything(SEED)            # 私たちはすべてのものが出来るだけ\nlgb_params['seed'] = SEED        # 確定的であることを望みます。\nN_CORES = psutil.cpu_count()     # 使用可能なCPUコア\n\n\n#限界と定数\nTARGET      = 'sales'            # ターゲット\nSTART_TRAIN = 0                  # 一部の行をスキップできます(Nans/faster training)\nEND_TRAIN   = 1913               # trainの終了日\nP_HORIZON   = 28                 # 予測期間\nUSE_AUX     = True               # 事前学習済みモデルを使用するかどうか\n\n#削除する特徴量\n## これらの機能はオーバーフィットにつながります \n## または test に存在しない値\nremove_features = ['id','state_id','store_id',\n                   'date','wm_yr_wk','d',TARGET]\nmean_features   = ['enc_cat_id_mean','enc_cat_id_std',\n                   'enc_dept_id_mean','enc_dept_id_std',\n                   'enc_item_id_mean','enc_item_id_std'] \n\n#特徴量のパス\nORIGINAL = '../input/m5-forecasting-accuracy/'\nBASE     = '../input/m5-simple-fe/grid_part_1.pkl'\nPRICE    = '../input/m5-simple-fe/grid_part_2.pkl'\nCALENDAR = '../input/m5-simple-fe/grid_part_3.pkl'\nLAGS     = '../input/m5-lags-features/lags_df_28.pkl'\nMEAN_ENC = '../input/m5-custom-features/mean_encoding_df.pkl'\n\n\n# AUX（事前学習済み）モデルのパス\nAUX_MODELS = '../input/m5-aux-models/'\n\n\n#STORES id\nSTORES_IDS = pd.read_csv(ORIGINAL+'sales_train_validation.csv')['store_id']\nSTORES_IDS = list(STORES_IDS.unique())\n\n\n#LAG作成用の分割\nSHIFT_DAY  = 28\nN_LAGS     = 15\nLAGS_SPLIT = [col for col in range(SHIFT_DAY,SHIFT_DAY+N_LAGS)]\nROLS_SPLIT = []\nfor i in [1,7,14]:\n    for j in [7,14,30,60]:\n        ROLS_SPLIT.append([i,j])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* 結果を得るために何時間も待ちたくない場合は、各ストアを個別のカーネルでトレーニングしてから、結果に参加します。\n* 事前トレーニング済みモデルを使用する場合は、トレーニングをスキップできます（この場合、ダミートレーニングを実行して、メモリに問題がなく、この（すべてのカーネル）コードを安全に使用できることを示します）。","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### AUXモデル\n\nif USE_AUX:\n    lgb_params['n_estimators'] = 2\n    \n# ここに比較できるいくつかの「ログ」があります\n#Train CA_1\n#[100]\tvalid_0's rmse: 2.02289\n#[200]\tvalid_0's rmse: 2.0017\n#[300]\tvalid_0's rmse: 1.99239\n#[400]\tvalid_0's rmse: 1.98471\n#[500]\tvalid_0's rmse: 1.97923\n#[600]\tvalid_0's rmse: 1.97284\n#[700]\tvalid_0's rmse: 1.96763\n#[800]\tvalid_0's rmse: 1.9624\n#[900]\tvalid_0's rmse: 1.95673\n#[1000]\tvalid_0's rmse: 1.95201\n#[1100]\tvalid_0's rmse: 1.9476\n#[1200]\tvalid_0's rmse: 1.9434\n#[1300]\tvalid_0's rmse: 1.9392\n#[1400]\tvalid_0's rmse: 1.93446\n\n#Train CA_2\n#[100]\tvalid_0's rmse: 1.88949\n#[200]\tvalid_0's rmse: 1.84767\n#[300]\tvalid_0's rmse: 1.83653\n#[400]\tvalid_0's rmse: 1.82909\n#[500]\tvalid_0's rmse: 1.82265\n#[600]\tvalid_0's rmse: 1.81725\n#[700]\tvalid_0's rmse: 1.81252\n#[800]\tvalid_0's rmse: 1.80736\n#[900]\tvalid_0's rmse: 1.80242\n#[1000]\tvalid_0's rmse: 1.79821\n#[1100]\tvalid_0's rmse: 1.794\n#[1200]\tvalid_0's rmse: 1.78973\n#[1300]\tvalid_0's rmse: 1.78552\n#[1400]\tvalid_0's rmse: 1.78158","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### モデルのトレーニング\n#################################################################################\nfor store_id in STORES_IDS:\n    print('Train', store_id)\n    \n    # 現在の店舗のグリッドを取得\n    grid_df, features_columns = get_data_by_store(store_id)\n    \n    # マスク\n    # Train （1913未満のすべてのデータ）\n    # \"Validation\" （過去28日間-実際の検証セットではない）\n    # Test （1913日を超えるすべてのデータ、再帰的特徴量のギャップあり）\n    train_mask = grid_df['d']<=END_TRAIN\n    valid_mask = train_mask&(grid_df['d']>(END_TRAIN-P_HORIZON))\n    preds_mask = grid_df['d']>(END_TRAIN-100)\n    \n    # マスクを適用してlgbデータセットをbinとして保存し、\n    # dtype変換中のメモリスパイクを削減する\n    # https://github.com/Microsoft/LightGBM/issues/1032\n    # 変換を回避するには、常に np.float32 を使用するか、\n    # トレーニングを開始する前に bin に保存する必要があります\n    # https://www.kaggle.com/c/talkingdata-adtracking-fraud-detection/discussion/53773\n    train_data = lgb.Dataset(grid_df[train_mask][features_columns], \n                       label=grid_df[train_mask][TARGET])\n    train_data.save_binary('train_data.bin')\n    train_data = lgb.Dataset('train_data.bin')\n    \n    valid_data = lgb.Dataset(grid_df[valid_mask][features_columns], \n                       label=grid_df[valid_mask][TARGET])\n    \n    # 後の予測のためにデータセットの一部を保存する\n    # 再帰的に計算する必要がある特徴量を削除する\n    grid_df = grid_df[preds_mask].reset_index(drop=True)\n    keep_cols = [col for col in list(grid_df) if '_tmp_' not in col]\n    grid_df = grid_df[keep_cols]\n    grid_df.to_pickle('test_'+store_id+'.pkl')\n    del grid_df\n    \n    # シーダーを再度起動して、\n    # 各「コード行」np.randomが「進化」するlgbトレーニングを100％確定的にするため、\n    # 「リセット」する必要がある場合があります。\n    seed_everything(SEED)\n    estimator = lgb.train(lgb_params,\n                          train_data,\n                          valid_sets = [valid_data],\n                          verbose_eval = 100,\n                          )\n    \n    # モデルを保存-実際の「.bin」ではなく\n    # estimator = lgb.Booster(model_file='model.txt')\n    # pickleファイルとすると最良の反復（または保存反復）でのみ予測できます\n    # pickle.dumpは柔軟性を提供します。\n    # estimator.predict(TEST, num_iteration=100)\n    # num_iteration - 予測したい反復回数\n    # NULLまたは<= 0は最適な反復を使用することを意味します\n    model_name = 'lgb_model_'+store_id+'_v'+str(VER)+'.bin'\n    pickle.dump(estimator, open(model_name, 'wb'))\n\n    # 一時ファイルとオブジェクトを削除して、\n    # HDDスペースとRAMメモリを解放します\n    !rm train_data.bin\n    del train_data, valid_data, estimator\n    gc.collect()\n    \n    # 予測のためのモデル特徴量の保持\n    MODEL_FEATURES = features_columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### 予測\n#################################################################################\n\n# 予測を保存するためのダミーデータフレームを作成する\nall_preds = pd.DataFrame()\n\n# テストデータセットをトレーニングデータの一部に結合して、\n# 再帰的な特徴量を作成します\nbase_test = get_base_test()\n\n# 予測時間を測定するタイマー\nmain_time = time.time()\n\n# 各予測日にループする移動LAGは最も時間がかかるため、\n# 1日全体で計算します。\nfor PREDICT_DAY in range(1,29):    \n    print('Predict | Day:', PREDICT_DAY)\n    start_time = time.time()\n\n    # 移動LAGを計算するための一時的なグリッドを作成する\n    grid_df = base_test.copy()\n    grid_df = pd.concat([grid_df, df_parallelize_run(make_lag_roll, ROLS_SPLIT)], axis=1)\n        \n    for store_id in STORES_IDS:\n        \n        # すべてのモデルを読んで、日/店舗のペアごとに予測を行う\n        model_path = 'lgb_model_'+store_id+'_v'+str(VER)+'.bin' \n        if USE_AUX:\n            model_path = AUX_MODELS + model_path\n        \n        estimator = pickle.load(open(model_path, 'rb'))\n        \n        day_mask = base_test['d']==(END_TRAIN+PREDICT_DAY)\n        store_mask = base_test['store_id']==store_id\n        \n        mask = (day_mask)&(store_mask)\n        base_test[TARGET][mask] = estimator.predict(grid_df[mask][MODEL_FEATURES])\n    \n    # 適切な列の名前を付け、all_preds に追加します\n    temp_df = base_test[day_mask][['id',TARGET]]\n    temp_df.columns = ['id','F'+str(PREDICT_DAY)]\n    if 'id' in list(all_preds):\n        all_preds = all_preds.merge(temp_df, on=['id'], how='left')\n    else:\n        all_preds = temp_df.copy()\n        \n    print('#'*10, ' %0.2f min round |' % ((time.time() - start_time) / 60),\n                  ' %0.2f min total |' % ((time.time() - main_time) / 60),\n                  ' %0.2f day sales |' % (temp_df['F'+str(PREDICT_DAY)].sum()))\n    del temp_df\n    \nall_preds = all_preds.reset_index(drop=True)\nall_preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### 書き出す\n#################################################################################\n# コンぺのサンプル提出を読んで、予測を結合する\n# 「_validation」データのみの予測があるため\n# 「_evaluation」アイテムに対してfillna（）を実行する必要がある\nsubmission = pd.read_csv(ORIGINAL+'sample_submission.csv')[['id']]\nsubmission = submission.merge(all_preds, on=['id'], how='left').fillna(0)\nsubmission.to_csv('submission_v'+str(VER)+'.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##以下は翻訳しません。ご自身でご判断ください。\n\n# Summary\n\n# Of course here is no magic at all.\n# No \"Novel\" features and no brilliant ideas.\n# We just carefully joined all\n# our previous fe work and created a model.\n\n# Also!\n# In my opinion this strategy is a \"dead end\".\n# Overfits a lot LB and with 1 final submission \n# you have no option to risk.\n\n\n# Improvement should come from:\n# Loss function\n# Data representation\n# Stable CV\n# Good features reduction strategy\n# Predictions stabilization with NN\n# Trend prediction\n# Real zero sales detection/classification\n\n\n# Good kernels references \n## (the order is random and the list is not complete):\n# https://www.kaggle.com/ragnar123/simple-lgbm-groupkfold-cv\n# https://www.kaggle.com/jpmiller/grouping-items-by-stockout-pattern\n# https://www.kaggle.com/headsortails/back-to-predict-the-future-interactive-m5-eda\n# https://www.kaggle.com/sibmike/m5-out-of-stock-feature\n# https://www.kaggle.com/mayer79/m5-forecast-attack-of-the-data-table\n# https://www.kaggle.com/yassinealouini/seq2seq\n# https://www.kaggle.com/kailex/m5-forecaster-v2\n# https://www.kaggle.com/aerdem4/m5-lofo-importance-on-gpu-via-rapids-xgboost\n\n\n# Features were created in these kernels:\n## \n# Mean encodings and PCA options\n# https://www.kaggle.com/kyakovlev/m5-custom-features\n##\n# Lags and rolling lags\n# https://www.kaggle.com/kyakovlev/m5-lags-features\n##\n# Base Grid and base features (calendar/price/etc)\n# https://www.kaggle.com/kyakovlev/m5-simple-fe\n\n\n# Personal request\n# Please don't upvote any ensemble and copypaste kernels\n## The worst case is ensemble without any analyse.\n## The best choice - just ignore it.\n## I would like to see more kernels with interesting and original approaches.\n## Don't feed copypasters with upvotes.\n\n## It doesn't mean that you should not fork and improve others kernels\n## but I would like to see params and code tuning based on some CV and analyse\n## and not only on LB probing.\n## Small changes could be shared in comments and authors can improve their kernel.\n\n## Feel free to criticize this kernel as my knowlege is very limited\n## and I can be wrong in code and descriptions. \n## Thank you.","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}