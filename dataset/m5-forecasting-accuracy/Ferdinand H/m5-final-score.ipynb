{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This notebook shows my solution to the M5 Forecasting - Accuracy challenge.  \n\nThe basic ideas behind my solution was the following: \n\n* I evaluated, what a good score would be and I find out, that a discrepandacy of around 10% per item results in a score of around ~0.5. Since the task was to predict each item per day, I assumed that a different of 10% is accetable. Moreover, there is a random factor. Many items are not sold on many days and therefore, I assumed that it is partly random, if it will be sold or not. For items, which will be sold 10 times per day, a 10% discrapandacy means that it will sold between 9 and 11 times. I want to say, it is in the nature of the data, that a perfect prediction is impossible and I assumed that 10% is accetable and seeing the final best scores, shows that I might be right. \n* I make the assumption that predicting the number of sold items per store and department is easier and more stable than for each item. Furthermore, checking out the weights for the final evaluation shows that having the predictions correct for this higher level data, has the highest influence. Hence, having the prediction right there, then a 50% of the weighted scores are correct. \n* Focusing on the higher level data has also the advantages that it requires less memory. \n* For predicting the higher level data, I used a lightgbm classifier. I didn't much optimize the hyperparamters since the results were accetable from the beginning. Using lightgbm has the disadvantages that it is not a linear prediction and therefor, I could not forecasting a trend. However, I visually inspect the (high level) data and there might be a very slight trend but not so much. Furthermore, it was only a prediction of 28 days, hence I assumed that a trend would not have hugh impact and therefore, I ignore it. \n* For the prediction on an item level, I took the number of sold items per store & department and distribute it over all items. I calculate how many items (relative the total number per store & department) were sold in the last 28 days and multiply this relative number with the prediction sold items. \n\nFinally, I think there are a few flaws in the task and evaluation method. From a customer perspective, I assume it is important to know, how many items should be on storage. For items which are sold very often per day, a prediction per day might be the correct one. However, for items, which only sold once per week or even less, it does not make much seens trying to predict it on a daily base. ","metadata":{}},{"cell_type":"code","source":"from  datetime import datetime, timedelta\nimport numpy as np, pandas as pd\nfrom typing import Union\nimport numpy as np\nimport pandas as pd\nimport functools\nimport lightgbm as lgb\nimport matplotlib.pyplot as plt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation class\nAdjuste from https://www.kaggle.com/c/m5-forecasting-accuracy/discussion/133834\n\nThanks to [sakami](https://www.kaggle.com/sakami)","metadata":{}},{"cell_type":"code","source":"class WRMSSEEvaluator(object):\n    # Adjuste from https://www.kaggle.com/c/m5-forecasting-accuracy/discussion/133834\n    # Thanks to sakami\n    \n    group_ids = ( 'all_id', 'state_id', 'store_id', 'cat_id', 'dept_id', 'item_id',\n        ['state_id', 'cat_id'],  ['state_id', 'dept_id'], ['store_id', 'cat_id'],\n        ['store_id', 'dept_id'], ['item_id', 'state_id'], ['item_id', 'store_id'])\n\n    def __init__(self, \n                 train_df: pd.DataFrame, \n                 valid_df: pd.DataFrame, \n                 calendar: pd.DataFrame, \n                 prices: pd.DataFrame):\n        '''\n        intialize and calculate weights\n        '''\n        self.disp = False\n        self.calendar = calendar\n        self.prices = prices\n        self.train_df = train_df\n        self.valid_df = valid_df\n        self.train_target_columns = [i for i in self.train_df.columns if i.startswith('d_')]\n\n        self.train_df['all_id'] = \"all\"\n\n        self.id_columns = [i for i in self.train_df.columns if not i.startswith('d_')]\n        self.valid_target_columns = [i for i in self.valid_df.columns if i.startswith('d_')]\n\n        if not all([c in self.valid_df.columns for c in self.id_columns]):\n            self.valid_df = pd.concat([self.train_df[self.id_columns], self.valid_df],\n                                      axis=1, \n                                      sort=False)\n        self.train_series = self.trans_30490_to_42840(self.train_df, \n                                                      self.train_target_columns, \n                                                      self.group_ids)\n        self.valid_series = self.trans_30490_to_42840(self.valid_df, \n                                                      self.valid_target_columns, \n                                                      self.group_ids)\n        self.scale = self.get_scale()\n        \n    def get_train_set_preyear(): \n        # Validation on 1577, 1605 (starts at 25.05.2015)\n        # Training on 1 to 1576\n        return self.train_df    \n    \n    def get_train_set_premonth(): \n        # Validation on 1914, 1941 (starts on 25.04.2016)\n        return self.train_df\n    \n    def get_train_set_final(): \n        # No validation, final submission (starts on 23.05.2016)\n        # Submission from 1941 to 1969\n        return self.train_df\n\n    def get_scale(self):\n        '''\n        scaling factor for each series ignoring starting zeros\n        '''\n        scales = []\n        for i in range(len(self.train_series)):\n            series = self.train_series.iloc[i].values\n            series = series[np.argmax(series!=0):]\n            scale = ((series[1:] - series[:-1]) ** 2).mean()\n            scales.append(scale)\n        return np.array(scales)\n    \n    def get_name(self, i):\n        '''\n        convert a str or list of strings to unique string \n        used for naming each of 42840 series\n        '''\n        if type(i) == str or type(i) == int:\n            return str(i)\n        else:\n            return \"--\".join(i)\n    \n    @functools.lru_cache(maxsize=3)\n    def get_weight_df(self, weight_columns_start) -> pd.DataFrame:\n        \"\"\"\n        returns weights for each of 42840 series in a dataFrame\n        \"\"\"\n        weight_columns = ['d_%d' % (x) for x in range(weight_columns_start, weight_columns_start+28)]\n        day_to_week = self.calendar.set_index(\"d\")[\"wm_yr_wk\"].to_dict()\n        weight_df = self.train_df[[\"item_id\", \"store_id\"] + weight_columns].set_index(\n            [\"item_id\", \"store_id\"]\n        )\n        weight_df = (\n            weight_df.stack().reset_index().rename(columns={\"level_2\": \"d\", 0: \"value\"})\n        )\n        weight_df[\"wm_yr_wk\"] = weight_df[\"d\"].map(day_to_week)\n        weight_df = weight_df.merge(\n            self.prices, how=\"left\", on=[\"item_id\", \"store_id\", \"wm_yr_wk\"]\n        )\n        weight_df[\"value\"] = weight_df[\"value\"] * weight_df[\"sell_price\"]\n        weight_df = weight_df.set_index([\"item_id\", \"store_id\", \"d\"]).unstack(level=2)[\n            \"value\"\n        ]\n        weight_df = weight_df.loc[\n            zip(self.train_df.item_id, self.train_df.store_id), :\n        ].reset_index(drop=True)\n        weight_df = pd.concat(\n            [self.train_df[self.id_columns], weight_df], axis=1, sort=False\n        )\n        weights_map = {}\n        for i, group_id in enumerate(self.group_ids):\n            lv_weight = weight_df.groupby(group_id)[weight_columns].sum().sum(axis=1)\n            lv_weight = lv_weight / lv_weight.sum()\n            for i in range(len(lv_weight)):\n                weights_map[self.get_name(lv_weight.index[i])] = np.array(\n                    [lv_weight.iloc[i]]\n                )\n        weights = pd.DataFrame(weights_map).T / len(self.group_ids)\n\n        return weights\n\n    def trans_30490_to_42840(self, df, cols, group_ids, dis=False):\n        '''\n        transform 30490 series to all 42840 series\n        '''\n        series_map = {}\n        for i, group_id in enumerate(self.group_ids):\n            tr = df.groupby(group_id)[cols].sum()\n            for i in range(len(tr)):\n                series_map[self.get_name(tr.index[i])] = tr.iloc[i].values\n        return pd.DataFrame(series_map).T\n    \n    def get_rmsse(self, valid_preds) -> pd.Series:\n        '''\n        returns rmsse scores for all 42840 series\n        '''\n        score = ((self.valid_series - valid_preds) ** 2).mean(axis=1)\n        rmsse = (score / self.scale).map(np.sqrt)\n        return rmsse\n\n    def score(self, valid_preds: Union[pd.DataFrame, np.ndarray], weight_columns) -> float:\n        assert self.valid_df[self.valid_target_columns].shape == valid_preds.shape, f\"{self.valid_df[self.valid_target_columns].shape} vs {valid_preds.shape}\"\n\n        if isinstance(valid_preds, np.ndarray):\n            valid_preds = pd.DataFrame(valid_preds, columns=self.valid_target_columns)\n\n        valid_preds = pd.concat([self.valid_df[self.id_columns].set_index(self.valid_df.id), valid_preds], #.set_index(self.valid_df.id)\n                                axis=1, \n                                sort=False)\n        valid_preds = self.trans_30490_to_42840(valid_preds, \n                                                self.valid_target_columns, \n                                                self.group_ids, \n                                                False)\n        self.rmsse = self.get_rmsse(valid_preds)\n        self.contributors = pd.concat([self.get_weight_df(weight_columns), self.rmsse], \n                                      axis=1, \n                                      sort=False).prod(axis=1)\n        return np.sum(self.contributors)","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Calculating baseline score","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv('../input/m5-forecasting-accuracy/sales_train_evaluation.csv')\ncalendar = pd.read_csv('../input/m5-forecasting-accuracy/calendar.csv')\nprices = pd.read_csv('../input/m5-forecasting-accuracy/sell_prices.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start_time = 1914\nval_columns = ['d_%d' % (x) for x in range(start_time, start_time+28)]\nvalid_fold_df = train_df.loc[:, val_columns].copy()\ne = WRMSSEEvaluator(train_df, valid_fold_df, calendar, prices)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Test ground truth. Error should be 0 ","metadata":{}},{"cell_type":"code","source":"valid_gt_df = train_df.set_index('id').loc[:, val_columns].copy()\ne.score(valid_gt_df, 1886)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" Since the task was to predict each item per day, I assumed that a different of 10% is accetable. Moreover, there is a random factor. Many items are not sold on many days and therefore, I assumed that it is partly random, if it will be sold or not. For items, which will be sold 10 times per day, a 10% discrapandacy means that it will sold between 9 and 11 times. It is in the nature of the data, that a perfect prediction is impossible and I assumed that 10% is accetable. Therefore, I calculate the score by increasing and decreasing the number of sold items by 10%.","metadata":{}},{"cell_type":"code","source":"e.score(valid_gt_df*1.1, 1886), e.score(valid_gt_df*0.9, 1886)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here, we can see that a discrepandacy of around 10% per item results in a score of around ~0.54. I assume that is a good baseline score, which I should targeting. Targeting a lower score would result in the risk of overfitting or just having luck on the public leaderboard. However, on the private leadboard it might look complete different. Therefore, I was targeting an error of arround 0.5. \n\nAfter the final evaluation, the best score on the private leaderboard was 0.52043, which shows that my assumption was correct. ","metadata":{}},{"cell_type":"markdown","source":"# Read Data\n\nAdjuste from kkiller","metadata":{}},{"cell_type":"code","source":"CAL_DTYPES={\"event_name_1\": \"category\", \"event_name_2\": \"category\", \"event_type_1\": \"category\", \n         \"event_type_2\": \"category\", \"weekday\": \"category\", 'wm_yr_wk': 'int16', \"wday\": \"int16\",\n        \"month\": \"int16\", \"year\": \"int16\", \"snap_CA\": \"float32\", 'snap_TX': 'float32', 'snap_WI': 'float32' }\nPRICE_DTYPES = {\"store_id\": \"category\", \"item_id\": \"category\", \"wm_yr_wk\": \"int16\",\"sell_price\":\"float32\" }\n\ndef create_dt(is_train = True, nrows = None, first_day = 0, tr_last=1941, max_lags=57, skip_prices=False):\n    prices = pd.read_csv(\"../input/m5-forecasting-accuracy/sell_prices.csv\", dtype = PRICE_DTYPES)\n    for col, col_dtype in PRICE_DTYPES.items():\n        if col_dtype == \"category\":\n            prices[col] = prices[col].cat.codes.astype(\"int16\")\n            prices[col] -= prices[col].min()\n\n    cal = pd.read_csv(\"../input/m5-forecasting-accuracy/calendar.csv\", dtype = CAL_DTYPES)\n    cal[\"date\"] = pd.to_datetime(cal[\"date\"])\n    for col, col_dtype in CAL_DTYPES.items():\n        if col_dtype == \"category\":\n            cal[col] = cal[col].cat.codes.astype(\"int16\")\n            cal[col] -= cal[col].min()\n\n    start_day = max(1 if is_train  else tr_last-max_lags, first_day)\n    numcols = [f\"d_{day}\" for day in range(start_day,tr_last+1)]\n    catcols = ['id', 'item_id', 'dept_id','store_id', 'cat_id', 'state_id']\n    dtype = {numcol:\"float32\" for numcol in numcols} \n    dtype.update({col: \"category\" for col in catcols if col != \"id\"})\n    dt = pd.read_csv(\"../input/m5-forecasting-accuracy/sales_train_evaluation.csv\", \n                     nrows = nrows, usecols = catcols + numcols, dtype = dtype)\n\n    for d in range(1942, 1970): \n        dt['d_%d'%d] = np.nan\n    \n    for col in catcols:\n        if col != \"id\":\n            dt[col] = dt[col].cat.codes.astype(\"int16\")\n            dt[col] -= dt[col].min()\n\n    if not is_train:\n        for day in range(tr_last+1, tr_last+ 28 +1):\n            dt[f\"d_{day}\"] = np.nan\n\n    dt = pd.melt(dt,\n                  id_vars = catcols,\n                  value_vars = [col for col in dt.columns if col.startswith(\"d_\")],\n                  var_name = \"d\",\n                  value_name = \"sales\")\n\n    dt = dt.merge(cal, on= \"d\", copy = False)\n    if not skip_prices: \n        dt = dt.merge(prices, on = [\"store_id\", \"item_id\", \"wm_yr_wk\"], copy = False)\n\n    return dt","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data seperation \nI seperate the data into highlevel data (i.e. the total number per Store and department) and low level data (i.e. per item) \n\nI make the assumption that predicting the number of sold items per store and department is easier and more stable than for each item. Furthermore, checking out the weights for the final evaluation shows that having the predictions correct for this higher level data, has the highest influence. Hence, having the prediction right there, then a 50% of the weighted scores are correct. Last but not least, cocusing on the higher level data has also the advantages that it requires less memory. ","metadata":{}},{"cell_type":"code","source":"def get_highlevel_data():\n    cal = pd.read_csv(\"../input/m5-forecasting-accuracy/calendar.csv\", dtype = CAL_DTYPES)\n    cal[\"date\"] = pd.to_datetime(cal[\"date\"])\n    for col, col_dtype in CAL_DTYPES.items():\n        if col_dtype == \"category\":\n            cal[col] = cal[col].cat.codes.astype(\"int16\")\n            cal[col] -= cal[col].min()\n            \n\n    numcols = [f\"d_{day}\" for day in range(1,1969)]\n    catcols = ['id', 'item_id', 'dept_id','store_id', 'cat_id', 'state_id']\n    dtype = {numcol:\"float32\" for numcol in numcols} \n    dtype.update({col: \"category\" for col in catcols if col != \"id\"})\n    dt = pd.read_csv(\"../input/m5-forecasting-accuracy/sales_train_evaluation.csv\", dtype = dtype)\n    for col in catcols:\n        if col != \"id\":\n            dt[col] = dt[col].cat.codes.astype(\"int16\")\n            dt[col] -= dt[col].min()\n    \n    for d in range(1942, 1970): \n        dt['d_%d'%d] = np.nan\n        \n    dt1 = dt.groupby(['dept_id', 'store_id']).sum()\n    dt1.drop(columns=['cat_id','state_id', 'item_id'], inplace=True)\n    dt1 = dt1.join(dt[['dept_id', 'store_id', 'cat_id', 'state_id']].groupby(['dept_id', 'store_id']).mean())\n    dt = pd.melt(dt1.reset_index(),\n              id_vars = ['dept_id', 'store_id', 'cat_id', 'state_id'],\n              value_vars = [col for col in dt.columns if col.startswith(\"d_\")],\n              var_name = \"d\",\n              value_name = \"sales\")\n    dt = dt.merge(cal, on= \"d\", copy = False) # , how='outer')\n    return dt\n\ndef get_highestlevel_data():\n    raise NotImplementedError\n    cal = pd.read_csv(\"../input/m5-forecasting-accuracy/calendar.csv\", dtype = CAL_DTYPES)\n    cal[\"date\"] = pd.to_datetime(cal[\"date\"])\n    for col, col_dtype in CAL_DTYPES.items():\n        if col_dtype == \"category\":\n            cal[col] = cal[col].cat.codes.astype(\"int16\")\n            cal[col] -= cal[col].min()\n    numcols = [f\"d_{day}\" for day in range(1,1969)]\n    catcols = ['id', 'item_id', 'dept_id','store_id', 'cat_id', 'state_id']\n    dtype = {numcol:\"float32\" for numcol in numcols} \n    dtype.update({col: \"category\" for col in catcols if col != \"id\"})\n    dt = pd.read_csv(\"../input/m5-forecasting-accuracy/sales_train_evaluation.csv\", dtype = dtype)\n    for col in catcols:\n        if col != \"id\":\n            dt[col] = dt[col].cat.codes.astype(\"int16\")\n            dt[col] -= dt[col].min()\n    dt = dt.groupby(['cat_id', 'state_id']).sum()\n    dt = pd.melt(dt.reset_index(),\n              id_vars = ['cat_id', 'state_id'],\n              value_vars = [col for col in dt.columns if col.startswith(\"d_\")],\n              var_name = \"d\",\n              value_name = \"sales\")\n    dt = dt.merge(cal, on= \"d\", copy = False) # , how='outer')\n    return dt\n\ndef get_toplevel_data():\n    raise NotImplementedError\n    cal = pd.read_csv(\"../input/m5-forecasting-accuracy/calendar.csv\", dtype = CAL_DTYPES)\n    cal[\"date\"] = pd.to_datetime(cal[\"date\"])\n    for col, col_dtype in CAL_DTYPES.items():\n        if col_dtype == \"category\":\n            cal[col] = cal[col].cat.codes.astype(\"int16\")\n            cal[col] -= cal[col].min()\n    numcols = [f\"d_{day}\" for day in range(1,1969)]\n    catcols = ['id', 'item_id', 'dept_id','store_id', 'cat_id', 'state_id']\n    dtype = {numcol:\"float32\" for numcol in numcols} \n    dtype.update({col: \"category\" for col in catcols if col != \"id\"})\n    dt = pd.read_csv(\"../input/m5-forecasting-accuracy/sales_train_evaluation.csv\", dtype = dtype)\n    for col in catcols:\n        if col != \"id\":\n            dt[col] = dt[col].cat.codes.astype(\"int16\")\n            dt[col] -= dt[col].min()\n    dt = dt.sum().to_frame()\n    dt.drop(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], inplace=True)\n    dt = dt.reset_index()\n    dt.columns = ['d', 'sales']\n    dt = dt.merge(cal, on= \"d\" , copy = False) # , how='outer')\n    return dt\n","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Features\nI use mainly two different features\n\n1. For each item, the classifiere get the information how often it was sold in the last N days. \n2. The dataset provides also information about events. However, an event might have not only influence on the day of the event but maybe also the previous or the following days. Therefore, I also extend the event to N previous and following days. ","metadata":{}},{"cell_type":"code","source":"\ndef get_lag_features(dt, lag_length=29): \n    groupfeatures = ['dept_id', 'state_id', 'store_id', 'cat_id']\n    groupfeatures = [x for x in groupfeatures if x in dt.columns]\n    for lag in range(1, lag_length+1):\n        try: \n            dt['lag%d' % (lag)] = dt.groupby(groupfeatures)['sales'].shift(lag)\n        except ValueError: \n            dt['lag%d' % (lag)] = dt['sales'].shift(lag)\n    return dt\n\ndef get_lag_events(dt, lag_length=3): \n    event_features = ['event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']\n    for event_feature in event_features: \n        for lag in range(-lag_length, lag_length): \n            if lag == 0: \n                continue\n            dt['%s_%d' % (event_feature, lag)] = dt.groupby(['dept_id', 'store_id'])[event_feature].shift(lag)\n            dt['%s_%d' % (event_feature, lag)].fillna(0, inplace=True)\n    return dt     ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training \nTraining a lgb classifier on the high-level data","metadata":{}},{"cell_type":"code","source":"def train_lgb(df, start_valid=1886, end_valid=1914, \n              objective=None, learning_rate=0.075, boosting_type=None, num_iterations=1200): \n    if objective is None: \n        objective = \"poisson\"\n    if boosting_type is None: \n        boosting_type = 'gbdt'\n    df = df.dropna()\n    cat_feats_all = ['dept_id','store_id', 'cat_id', 'store_id'] + [\"event_name_1\", \"event_name_2\", \"event_type_1\", \"event_type_2\"]\n    cat_feats = [x for x in cat_feats_all if x in df.columns]\n    useless_cols = [\"id\", \"date\", \"sales\",\"d\", \"wm_yr_wk\", \"weekday\"]\n    train_cols = df.columns[~df.columns.isin(useless_cols)]\n    X_train = df[train_cols]\n    y_train = df[\"sales\"]\n    np.random.seed(777)\n    fake_valid_inds = df[df.d.isin(['d_%d'%x for x in range(start_valid, end_valid)])].index # np.random.choice(X_train.index.values, 1318, replace = False)\n    nonused_inds = df[df.d.isin(['d_%d'%x for x in range(end_valid, 2000)])].index # np.random.choice(X_train.index.values, 1318, replace = False)\n\n    train_inds = np.setdiff1d(X_train.index.values, fake_valid_inds)\n    train_inds = np.setdiff1d(train_inds, nonused_inds)\n    train_data = lgb.Dataset(X_train.loc[train_inds] , label = y_train.loc[train_inds], \n                             categorical_feature=cat_feats, free_raw_data=False)\n    fake_valid_data = lgb.Dataset(X_train.loc[fake_valid_inds], label = y_train.loc[fake_valid_inds],\n                                  categorical_feature=cat_feats,\n                     free_raw_data=False)\n    params = {\n            \"objective\" : objective,\n            \"metric\" :\"rmse\",\n            \"force_row_wise\" : True,\n            \"learning_rate\" : learning_rate,\n            \"boosting_type\" : boosting_type, \n            \"sub_row\" : 0.75,\n            \"bagging_freq\" : 1,\n            \"lambda_l2\" : 0.1,\n            \"metric\": [\"rmse\"],\n        'verbosity': 1,\n        'num_iterations' : num_iterations,\n        'num_leaves': 128,\n        \"min_data_in_leaf\": 100,\n    }\n    m_lgb = lgb.train(params, train_data, valid_sets = [fake_valid_data], verbose_eval=20) \n    return m_lgb, train_cols, ","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prediction of high level data","metadata":{}},{"cell_type":"code","source":"def predict(m_lgb, train_cols, df, valid_start_date=1886, pred_length=28, lag_length=29, feature_fcts=None): \n    if feature_fcts is None: \n        feature_fcts = []\n    df_valid = df[df.d.isin(['d_%d'%x for x in range(valid_start_date-lag_length, valid_start_date+pred_length)])]\n    df_valid.loc[df_valid.d.isin(['d_%d'%x for x in range(valid_start_date, valid_start_date+pred_length)]), 'sales'] = np.NAN\n    for idx in range(pred_length):\n        df_valid = df_valid.drop(df_valid[df_valid.d == 'd_%d' % (valid_start_date-lag_length-1+idx)].index)     \n        for feature_fct in feature_fcts: \n            df_valid = feature_fct(df_valid)\n        df_valid.loc[df_valid[train_cols].dropna().index, 'sales'] = m_lgb.predict(df_valid[train_cols].dropna())\n    return df_valid","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prediction of low level data","metadata":{}},{"cell_type":"markdown","source":"Using the relative number of soled items in the previous time. \n\nIn total, this means, that the LGB classifier predicts how many items per store and category will be sold. Furthermore, I also know, how often a specific item was sold relative to the total number of the sold item. With both information, I predict how a specific item will be sold. ","metadata":{}},{"cell_type":"code","source":"def create_rel_prediction_dt(df_valid, start_time=1886, pred_time=28): \n    endtime = min(start_time+27, 1941)\n    dt = create_dt(True, None, start_time, endtime, skip_prices=True)\n    dt.loc[:, 'sales'] = np.NAN\n    dt = dt.join(\n        df_valid.groupby(['dept_id', 'store_id', 'd']).sum()['sales'],\n        on=['dept_id', 'store_id', 'd'], rsuffix='_r'\n    )\n    relpred = create_dt(True, None, start_time-1-pred_time, start_time-1, skip_prices=True)\n    relpred = relpred.join(\n        relpred.groupby(['dept_id', 'store_id', 'd']).sum()['sales'],\n        on=['dept_id', 'store_id', 'd'], rsuffix='_sum'\n    )\n    relpred['sales_rel'] = relpred['sales'] / relpred['sales_sum']\n    dt = dt.join(relpred.groupby('id').mean()['sales_rel'], on='id')\n    dt['preds'] = dt['sales_r'] * dt['sales_rel']\n    dt = dt[['id', 'd', 'preds']].copy()\n    dt = dt.pivot(columns='d', index='id', values='preds')\n    return dt ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_rel_week_prediction_dt(df_valid, start_time=1886, pred_time=28): \n    endtime = min(start_time+27, 1941)\n    dt = create_dt(True, None, start_time, endtime, skip_prices=True)\n    dt.loc[:, 'sales'] = np.NAN\n    dt = dt.join(\n        df_valid.groupby(['dept_id', 'store_id', 'd']).sum()['sales'],\n        on=['dept_id', 'store_id', 'd'], rsuffix='_r'\n    )\n    relpred = create_dt(True, None, start_time-1-pred_time, start_time-1, skip_prices=True)\n    relpred = relpred.join(\n        relpred.groupby(['dept_id', 'store_id', 'd']).sum()['sales'],\n        on=['dept_id', 'store_id', 'd'], rsuffix='_sum'\n    )\n    relpred['sales_rel'] = relpred['sales'] / relpred['sales_sum']\n    dt = dt.join(relpred.groupby(['id', 'wday']).mean()['sales_rel'], on=['id', 'wday'])\n    dt['preds'] = dt['sales_r'] * dt['sales_rel']\n    dt = dt[['id', 'd', 'preds']].copy()\n    dt = dt.pivot(columns='d', index='id', values='preds')\n    return dt ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calc_kpi_high_level(df_valid, start_time=1886): \n    dt = create_dt(True, None, start_time, start_time+27, skip_prices=True)\n    dt.loc[:, 'sales'] = np.NAN\n    groupbyids = ['dept_id', 'store_id', 'state_id', 'store_id']\n    groupbyids = [x for x in groupbyids if x in df_valid.columns]\n    groupbyids.append('d')\n    dt = dt.join(\n        df_valid.groupby(groupbyids).sum()['sales'],\n        on=groupbyids, rsuffix='_r'\n    )\n    dt = dt.join(\n        dt.groupby(groupbyids).count()['sales_r'],\n        on=groupbyids, rsuffix='_r'\n    )\n    dt['preds'] = dt.sales_r / dt.sales_r_r\n    dt = dt[['id', 'd', 'preds']].copy()\n    dt = dt.pivot(columns='d', index='id', values='preds')\n    train_df = pd.read_csv('../input/m5-forecasting-accuracy/sales_train_evaluation.csv')\n    calendar = pd.read_csv('../input/m5-forecasting-accuracy/calendar.csv')\n    prices = pd.read_csv('../input/m5-forecasting-accuracy/sell_prices.csv')\n    val_columns = ['d_%d' % (x) for x in range(start_time, start_time+28)]\n    valid_fold_df = train_df.loc[:, val_columns].copy()\n    e = WRMSSEEvaluator(train_df, valid_fold_df, calendar, prices)\n    return [e.score(dt[[f'd_{x}' for x in range(start_time, start_time+28)]], start_time-28), \n            e.score(dt[[f'd_{x}' for x in range(start_time, start_time+28)]], 1886), \n            e.score(dt[[f'd_{x}' for x in range(start_time, start_time+28)]], 1914)]\n\n","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calc_kpi_complete(dt, start_time=1886): \n    train_df = pd.read_csv('../input/m5-forecasting-accuracy/sales_train_evaluation.csv')\n    calendar = pd.read_csv('../input/m5-forecasting-accuracy/calendar.csv')\n    prices = pd.read_csv('../input/m5-forecasting-accuracy/sell_prices.csv')\n    val_columns = ['d_%d' % (x) for x in range(start_time, start_time+28)]\n    valid_fold_df = train_df.loc[:, val_columns].copy()\n    valid_fold_df = valid_fold_df.reset_index()\n    e = WRMSSEEvaluator(train_df, valid_fold_df, calendar, prices)\n    return [e.score(dt, start_time-28), e.score(dt, 1886), e.score(dt, 1914)]","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dt = get_highlevel_data()\ndt = get_lag_features(dt)\ndt = get_lag_events(dt)\nm_lgb, traincols = train_lgb(dt, start_valid=1914, end_valid=1941)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"high_level_predictions_validation = predict(m_lgb, traincols, dt, feature_fcts=[get_lag_features], valid_start_date=1914)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"calc_kpi_high_level(high_level_predictions_validation, start_time=1914)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"low_level_predictions_validation = create_rel_prediction_dt(high_level_predictions_validation, start_time=1914, pred_time=28)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"calc_kpi_complete(low_level_predictions_validation[[f'd_{x}' for x in range(start_time, start_time+28)]], start_time=1914)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The score on the public leaderboard is 0.58110 with this submission, which fits very well to my calculation. ","metadata":{}},{"cell_type":"markdown","source":"# Tree-based classification \nI use the lgb classifier, which is a tree-based classifier. Similar to KNNs, tree-based classifiers only look for the best training samples and the value of these training samples is the prediction. Therefore, tree-based (or KNN) classifiers are not very well for predicting trends. Thus, the question is, do we have a trend here? ","metadata":{}},{"cell_type":"code","source":"df_vis = get_highlevel_data()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_vis.groupby('date').sales.sum().plot()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It seems that we have a long-term trend here. ","metadata":{}},{"cell_type":"code","source":"df_vis[df_vis.date.dt.year >= 2015].groupby('date').sales.sum().plot()\nplt.show() \ndf_vis[df_vis.date.dt.year >= 2016].groupby('date').sales.sum().plot()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"However, we need to keep in mind that we only need to predict the next 30 days ahead. If we look only at the data from 2015 and 2016 (and even if we only look at 2016), I don't see a strong trend. Yes, there might be a trend. However, I think, this trend is not very strong in the next 30 days so that the usage of a tree-based classifier is acceptable. ","metadata":{}},{"cell_type":"markdown","source":"# Create submission","metadata":{}},{"cell_type":"code","source":"high_level_predictions_evaluation = predict(m_lgb, traincols, dt, feature_fcts=[get_lag_features], valid_start_date=1942)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"low_level_predictions_evaluation = create_rel_prediction_dt(high_level_predictions_evaluation, start_time=1942, pred_time=28)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"low_level_predictions_validation = low_level_predictions_validation[[\"d_%d\" % (x) for x in range(1914, 1942)]]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"low_level_predictions_validation = low_level_predictions_validation.rename(columns={\"d_%d\"%(x+1914): \"F%d\"%(x+1) for x in range(28)})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"low_level_predictions_validation = low_level_predictions_validation.reset_index()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"low_level_predictions_validation[\"id\"] = low_level_predictions_validation[\"id\"].str.replace( \"evaluation$\", \"validation\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"low_level_predictions_evaluation = low_level_predictions_evaluation[[\"d_%d\" % (x) for x in range(1942, 1970)]]\nlow_level_predictions_evaluation = low_level_predictions_evaluation.rename(columns={\"d_%d\"%(x+1942): \"F%d\"%(x+1) for x in range(28)})\nlow_level_predictions_evaluation = low_level_predictions_evaluation.reset_index()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.concat([low_level_predictions_validation, low_level_predictions_evaluation])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Due to late submission the scores are: \n\n0.62260 (privat) and 0.58110 (public). \n\nSo, this is correct du to my previous submission","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}