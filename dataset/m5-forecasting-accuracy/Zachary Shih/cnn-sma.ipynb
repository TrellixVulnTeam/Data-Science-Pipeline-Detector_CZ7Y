{"cells":[{"metadata":{},"cell_type":"markdown","source":"# CNN+SMA\nThis notebook tests a method of using a trained CNN to predict the daily trend of sales, and the latest mean to adjust the result. This method came from the EDA and the result of the CNN in another notebook of mine (https://www.kaggle.com/zachary3141/m5-beginner-eda-cnn). From the previous work, I realized the predicted result itself is not good enough. The mean of the sale of different products in different stores can vary greatly. Therefore, I think it's reasonable to combine the mean of each product in each store with the predicted trend to improve the score.\n\nPlease vote up if you find this notebook interesting or useful.\n\n# SMA\nIn the previous notebook, I've tried tree different types of mean. **simple moving average**(SMA), **weighted moving average**(WMA), and **exponential moving average**(EMA). Since SMA seemed most stable, I used SMA in the following work."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom numpy import array\nfrom numpy import hstack\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_sales = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sales_train_validation.csv')\nsell_prices = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sell_prices.csv')\ncalendar = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/calendar.csv')\nsubmission_file = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def SMA(days, n):\n    total = 0\n    for i in range(n):\n        total = total + days[i]\n    return total/n\n\ndef count_SMA(orig, n):\n    ret = np.zeros(len(orig) - n)\n    for i in range(len(ret)):\n        ret[i] = SMA(np.array(orig[i:i+n]), n)\n    return ret","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cats = train_sales.cat_id.unique()\nstores = train_sales.store_id.unique()\nmean_array = []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for cat in cats:\n    for store in stores:\n        mean_array.append(train_sales.loc[train_sales['store_id'] == store].groupby(['cat_id']).mean().loc[cat])\n        mean_array.append(count_SMA(train_sales.loc[train_sales['store_id'] == store].groupby(['cat_id']).mean().loc[cat], 28))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from itertools import cycle\ncolor_cycle = cycle(plt.rcParams['axes.prop_cycle'].by_key()['color'])\nplt.figure(figsize=(14, 7))\nfor i in range(4):\n    temp = np.zeros(30)\n    plt.subplot(4,1,i+1)\n    plt.plot(range(len(mean_array[i*2])), mean_array[i*2], color=next(color_cycle))\n    plt.plot(range(30, len(mean_array[i*2+1])+30), mean_array[i*2+1], color=next(color_cycle))\n    plt.title(cats[0] + \"_\" + stores[i])\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"i = 0\nfor cat in cats:\n    for store in stores:\n        submission_file.loc[submission_file['id'].str.contains(cat) & submission_file['id'].str.contains(store),1:] = mean_array[i*2+1][-1]\n        i += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_file.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## result\nBy submitting the latest SMA with respect to the store_id and category_id. The score is 1.60147\n\n# CNN\nthe CNN used here is the same as the one used in the previous work."},{"metadata":{"trusted":true},"cell_type":"code","source":"def melt_sales(df):\n    df = df.drop([\"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"], axis=1).melt(\n        id_vars=['id'], var_name='d', value_name='demand')\n    return df\n\nsales = melt_sales(train_sales)\n\ndef map_f2d(d_col, id_col):\n    eval_flag = id_col.str.endswith(\"evaluation\")\n    return \"d_\" + (d_col.str[1:].astype(\"int\") + 1913 + 28 * eval_flag).astype(\"str\")\n\nsubmission = submission_file.melt(id_vars=\"id\", var_name=\"d\", value_name=\"demand\").assign( demand=np.nan, d = lambda df: map_f2d(df.d, df.id))\n\nsales_trend = train_sales.drop(columns = ['id','item_id','dept_id','cat_id','store_id','state_id']).mean().reset_index()\n\nsales_trend.rename(columns={'index':'d', 0: 'sales'}, inplace=True)\nsales_trend = sales_trend.merge(calendar[[\"wday\",\"month\",\"year\",\"d\"]], on=\"d\",how='left')\nsales_trend = sales_trend.drop(columns = [\"d\"])\n\ndef split_sequences(sequences, n_steps):\n    X, y = list(), list()\n    for i in range(len(sequences)):\n        # find the end of this pattern\n        end_ix = i + n_steps\n        # check if we are beyond the dataset\n        if end_ix > len(sequences):\n            break\n        # gather input and output parts of the pattern\n        seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1, -1]\n        X.append(seq_x)\n        y.append(seq_y)\n    return array(X), array(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"in_seq1 = array(sales_trend['wday'])\nin_seq2 = array(sales_trend['month'])\nin_seq3 = array(sales_trend['year'])\nout_seq = array(sales_trend['sales'])\nin_seq1 = in_seq1.reshape((len(in_seq1), 1))\nin_seq2 = in_seq2.reshape((len(in_seq2), 1))\nin_seq3 = in_seq3.reshape((len(in_seq3), 1))\nout_seq = out_seq.reshape((len(out_seq), 1))\ndataset = hstack((in_seq1, in_seq2, in_seq3, out_seq))\nn_steps = 7\nX, y = split_sequences(dataset, n_steps)\n\ntrain_x = X[:-30]\ntrain_y = y[:-30]\ntest_x = X[-30:]\ntest_y = y[-30:]\n\nn_features = train_x.shape[2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers import Flatten\nfrom keras.layers import TimeDistributed\nfrom keras.layers.convolutional import Conv1D\nfrom keras.layers.convolutional import MaxPooling1D","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps, n_features)))\nmodel.add(MaxPooling1D(pool_size=2))\nmodel.add(Flatten())\nmodel.add(Dense(50, activation='relu'))\nmodel.add(Dense(1))\nmodel.compile(optimizer='adam', loss='mse')\nmodel.fit(train_x, train_y, epochs=400, verbose=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subs = submission.groupby(['d']).mean().reset_index()\nresult = subs \n\nsubs = subs.merge(calendar[[\"wday\",\"month\",\"year\",\"d\"]], on=\"d\",how='left')\nsubs = subs.drop(columns = [\"d\", \"demand\"])\nsubs = pd.concat([sales_trend, subs], ignore_index=True, sort=False)\n\nin_seq1 = array(subs['wday'])\nin_seq2 = array(subs['month'])\nin_seq3 = array(subs['year'])\nout_seq = array(np.zeros(1969))\nin_seq1 = in_seq1.reshape((len(in_seq1), 1))\nin_seq2 = in_seq2.reshape((len(in_seq2), 1))\nin_seq3 = in_seq3.reshape((len(in_seq3), 1))\nout_seq = out_seq.reshape((len(out_seq), 1))\ndataset = hstack((in_seq1, in_seq2, in_seq3, out_seq))\nn_steps = 7\nX, y = split_sequences(dataset, n_steps)\n\nsubs = X[-56:]\n\ni = 0\nfor sub in subs:\n    sub = sub.reshape((1, n_steps, n_features))\n    result['demand'][i] = model.predict(sub, verbose=0)\n    i = i + 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, combine the SMA and CNN predicted value."},{"metadata":{"trusted":true},"cell_type":"code","source":"mean = result[\"demand\"].mean()\nresult[\"demand\"] = result[\"demand\"] - mean\n\nfor i in range(1,29):\n    submission_file.loc[submission_file.id.str.contains(\"validation\"), \"F\" + str(i)] += result[\"demand\"][i-1]\n    submission_file.loc[submission_file.id.str.contains(\"evaluation\"), \"F\" + str(i)] += result[\"demand\"][i + 28-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_file.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## result\nAs you can see form the following graph, eventhough the predicted result has the correct trend, the loctaion is far too low. And that's because the trained CNN is for general usage. It's only good for predicting the trends, like when to raise and when to drop. So, it would need a corresponding mean to move the prediction to the right height. \n\nAs a result, the submition score improves from **1.60147 to 1.38450**."},{"metadata":{"trusted":true},"cell_type":"code","source":"last_30_400 = np.zeros(30)\ni = 0\nfor test in test_x:\n    test = test.reshape((1, n_steps, n_features))\n    last_30_400[i] = model.predict(test, verbose=0)\n    i = i + 1\nmean = last_30_400.mean()\nlast_30_400 = last_30_400 - mean\nlast_30_400_cb = last_30_400 + mean_array[1][-30]\n\nplt.figure(figsize=(16, 8))\nplt.plot(range(30), mean_array[0][-30:], label=\"original\")\n#plt.plot(range(30), mean_array[0][-(30+ 7*4*12):-(7*4*12)], label=\"last year\")\nplt.plot(range(30), last_30_400_cb, label=\"predicted + SMA\")\nplt.plot(range(30), last_30_400, label=\"predicted\")\nplt.legend(loc=(1.0, 0.5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}