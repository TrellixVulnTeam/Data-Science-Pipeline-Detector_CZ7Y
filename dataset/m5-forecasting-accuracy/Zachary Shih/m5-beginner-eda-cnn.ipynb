{"cells":[{"metadata":{},"cell_type":"markdown","source":"# M5 beginner EDA + CNN\nThis notebook include some analyzing of the given data sets. And try to see things from different aspects and scales.<br/>\nIn the end of this notebook, I use CNN to predict the trend of sales base on time.<br/>\n<br/>\nThis is a forecasting competition, where we have to predict sales of 28 days in the future. So, first we will look at what data have been given. And do some simple EDA, try to find of what affects the sales.<br/>\n<br/>\nBy the way, I'm not a english native speaker, so if there are sentences that you don't understand please tell me, thanks.\n\nPlease vote up if find this notebook helpful or interesting. Cheers."},{"metadata":{},"cell_type":"markdown","source":"# Data"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom numpy import array\nfrom numpy import hstack\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_sales = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sales_train_validation.csv')\nsell_prices = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sell_prices.csv')\ncalendar = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/calendar.csv')\nsubmission_file = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## train_sales\n* id: validation id\n* item_id: item id\n* cat_id: category (ex. hobbies, household, foods)\n* store_id: from which store\n* state_id: in which state(ex. CA, TX, WI)"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_sales.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## sell_price\n* sell_price: price for the item in the week"},{"metadata":{"trusted":true},"cell_type":"code","source":"sell_prices.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## calender\n* event_name: special day(ex. SuperBowl)\n* snap: The Supplemental Nutrition Assistance Program"},{"metadata":{"trusted":true},"cell_type":"code","source":"calendar.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_file.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA\n## At the scale state "},{"metadata":{},"cell_type":"markdown","source":"* observe sales at the scale of ***state***\n    * California generally has better salls than the other two states.\n    * Apart from **foods** Texas is better than Wisconsin\n    * The total sales of the category is: Foods > household > hobbies"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_sales['total_sales'] = train_sales.sum(axis=1)\nsns.catplot(x=\"cat_id\", y=\"total_sales\",\n                hue=\"state_id\",\n                data=train_sales, kind=\"bar\",\n                height=8, aspect=1);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Below is the population of California, Texas, and Wisconsin. As you can see the order of population is the same as sales. (CA > TX > WI)<br/>\nEventhough the orders are the same, the difference gaps of sales between each state are not the same. For example, the population of Wisconsin is only 12.5% of California and 17% of Texas. But the sales of Wisconsin is only a bit less than the other states. <br/>\nSo, maybe obersveing at the scale of state is not good enough.<br/>\nNext, we will look at the problem at a smaller scale<br/>"},{"metadata":{},"cell_type":"markdown","source":"![](https://i.imgur.com/bUSEoMb.png)"},{"metadata":{},"cell_type":"markdown","source":"## At the scale of stores\nEventhough, California has the best sales, only CA_3 store has an out standing sales. The rest of California stores are just the same as other states, or even has the least sales(CA_4). <br/>\nIt is quite interesting to see that, even in lease populated states, Walmart still manage to reach certain sales. Perhaps the location and the number of stores in the area are the real factors.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x=\"store_id\", y=\"total_sales\",\n                hue=\"cat_id\",\n                data=train_sales, kind=\"bar\",\n                height=8, aspect=1);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Perspective of Time"},{"metadata":{"trusted":true},"cell_type":"code","source":"hobbies_state = train_sales.loc[(train_sales['cat_id'] == 'HOBBIES')].groupby(['state_id']).mean().T\nhobbies_state = hobbies_state.rename({'CA': 'HOBBIES_CA', 'TX': 'HOBBIES_TX', 'WI': 'HOBBIES_WI'}, axis=1)\nhousehold_state = train_sales.loc[(train_sales['cat_id'] == 'HOUSEHOLD')].groupby(['state_id']).mean().T\nhousehold_state = household_state.rename({'CA': 'HOUSEHOLD_CA', 'TX': 'HOUSEHOLD_TX', 'WI': 'HOUSEHOLD_WI'}, axis=1)\nfoods_state = train_sales.loc[(train_sales['cat_id'] == 'FOODS')].groupby(['state_id']).mean().T\nfoods_state = foods_state.rename({'CA': 'FOODS_CA', 'TX': 'FOODS_TX', 'WI': 'FOODS_WI'}, axis=1)\nnine_example = pd.concat([hobbies_state, household_state, foods_state], axis=1)\nnine_example = nine_example.drop('total_sales')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from itertools import cycle\ncolor_cycle = cycle(plt.rcParams['axes.prop_cycle'].by_key()['color'])\n\nfig, axs = plt.subplots(3,3, figsize=(10,10))\naxs = axs.flatten()\nax_idx = 0\nfor item in nine_example.columns:\n    nine_example[item].plot(title=item, color=next(color_cycle), ax=axs[ax_idx])\n    ax_idx += 1\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The time length of the upper graph is 5 years long, from 2011 to 2016. It is interesting to that there are yearly patterns  in the sales. For example, you can see around every 360 days there is a day when the sale is 0.<br/>\nAs you can see in the following parts, these results are costed by annual events. Like the 0 sales days I just mentioned are cost by Christmas. In the given data sets there are a lot of annual events, some have effects on the sales , and some do not."},{"metadata":{"trusted":true},"cell_type":"code","source":"nine_example.loc[nine_example['HOBBIES_CA'] == 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"calendar.loc[calendar['d'].isin(['d_331', 'd_697', 'd_1062', 'd_1427', 'd_1792'])]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Other special events \nSince special events like Christmas affects the sales in every state, perhaps there are other events that also make the sales go lower or higher nationally. In file \"calendar\", there are 30 differert events. Including Superbowl, Valentines day, Presidents day, etc.<br/>\nBelow, shows the stores mean sales in **HOBBIES** of each states, and points out the spacial events. It is pretty obvious that tehre are some events always appear in the same place compare to the sales trend. For instance, there are always two points beside the Christmas points(those equal to 0). Latter there is a clearer graph to show this."},{"metadata":{"trusted":true},"cell_type":"code","source":"event_date = calendar.loc[calendar['event_name_1'].isin(calendar.event_name_1.unique()[1:])].d\nHOBBIES_event = train_sales.loc[(train_sales['cat_id'] == 'HOBBIES')].groupby(['state_id']).mean().T.reset_index()\nHOBBIES_event = HOBBIES_event.loc[HOBBIES_event['index'].isin(event_date)]\nplt.figure(figsize=(15, 10))\nplt.subplot(3,1,1)\nnine_example['HOBBIES_CA'].plot(title='HOBBIES_CA', color=next(color_cycle))\nplt.scatter(HOBBIES_event.reset_index().level_0, HOBBIES_event['CA'],color=next(color_cycle), zorder=10)\nplt.subplot(3,1,2)\nnine_example['HOBBIES_TX'].plot(title='HOBBIES_TX', color=next(color_cycle))\nplt.scatter(HOBBIES_event.reset_index().level_0, HOBBIES_event['TX'],color=next(color_cycle), zorder=10)\nplt.subplot(3,1,3)\nnine_example['HOBBIES_WI'].plot(title='HOBBIES_WI', color=next(color_cycle))\nplt.scatter(HOBBIES_event.reset_index().level_0, HOBBIES_event['WI'],color=next(color_cycle), zorder=10)\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here is the clearer graph, ss you can see \"Thanksgiving\", \"Christmas\", and \"NewYear\" always appear in the bottom of the trend in every year and every state. While there are events with patterns, there are also many hug events without patterns. In the graph, point \"SuperBowl\" appears on different sides of the trend. Sometimes it's at the top, but sometimes it's at the bottom. So, in prediction events like SuperBowl should not be taken into account."},{"metadata":{},"cell_type":"markdown","source":"\n![](https://i.imgur.com/OlTQtrf.png)"},{"metadata":{},"cell_type":"markdown","source":"## Inspect the data at different time scale"},{"metadata":{"trusted":true},"cell_type":"code","source":"cal = calendar[['d', 'wday', 'month', 'year']]\ncal = cal.rename(columns={'d': 'index'})\nhobbies_state = train_sales.loc[(train_sales['cat_id'] == 'HOBBIES')].groupby(['state_id']).sum().T\nhobbies_state = hobbies_state.reset_index()\nhobbies_state = pd.merge(hobbies_state,cal, on='index')\nhousehold_state = train_sales.loc[(train_sales['cat_id'] == 'HOUSEHOLD')].groupby(['state_id']).sum().T\nhousehold_state = household_state.reset_index()\nhousehold_state = pd.merge(household_state,cal, on='index')\nfoods_state = train_sales.loc[(train_sales['cat_id'] == 'FOODS')].groupby(['state_id']).sum().T\nfoods_state = foods_state.reset_index()\nfoods_state = pd.merge(foods_state,cal, on='index')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(18, 18))\nplt.subplot(3,3,1)\nplt.title('hobbies')\nplt.plot(range(1, 7 + 1 ,1), hobbies_state.groupby(['wday']).mean().CA, label='CA')\nplt.plot(range(1, 7 + 1 ,1), hobbies_state.groupby(['wday']).mean().TX, label='TX')\nplt.plot(range(1, 7 + 1 ,1), hobbies_state.groupby(['wday']).mean().WI, label='WI')\nplt.legend(loc='upper right')\nplt.subplot(3,3,2)\nplt.title('household')\nplt.plot(range(1, 7 + 1 ,1), household_state.groupby(['wday']).mean().CA, label='CA')\nplt.plot(range(1, 7 + 1 ,1), household_state.groupby(['wday']).mean().TX, label='TX')\nplt.plot(range(1, 7 + 1 ,1), household_state.groupby(['wday']).mean().WI, label='WI')\nplt.legend(loc='upper right')\nplt.subplot(3,3,3)\nplt.title('foods')\nplt.plot(range(1, 7 + 1 ,1), foods_state.groupby(['wday']).mean().CA, label='CA')\nplt.plot(range(1, 7 + 1 ,1), foods_state.groupby(['wday']).mean().TX, label='TX')\nplt.plot(range(1, 7 + 1 ,1), foods_state.groupby(['wday']).mean().WI, label='WI')\nplt.legend(loc='upper right')\nplt.subplot(3,3,4)\nplt.title('hobbies')\nplt.plot(range(1, 12 + 1 ,1), hobbies_state.groupby(['month']).mean().CA, label='CA')\nplt.plot(range(1, 12 + 1 ,1), hobbies_state.groupby(['month']).mean().TX, label='TX')\nplt.plot(range(1, 12 + 1 ,1), hobbies_state.groupby(['month']).mean().WI, label='WI')\nplt.legend(loc='upper right')\nplt.subplot(3,3,5)\nplt.title('household')\nplt.plot(range(1, 12 + 1 ,1), household_state.groupby(['month']).mean().CA, label='CA')\nplt.plot(range(1, 12 + 1 ,1), household_state.groupby(['month']).mean().TX, label='TX')\nplt.plot(range(1, 12 + 1 ,1), household_state.groupby(['month']).mean().WI, label='WI')\nplt.legend(loc='upper right')\nplt.subplot(3,3,6)\nplt.title('foods')\nplt.plot(range(1, 12 + 1 ,1), foods_state.groupby(['month']).mean().CA, label='CA')\nplt.plot(range(1, 12 + 1 ,1), foods_state.groupby(['month']).mean().TX, label='TX')\nplt.plot(range(1, 12 + 1 ,1), foods_state.groupby(['month']).mean().WI, label='WI')\nplt.legend(loc='upper right')\nplt.subplot(3,3,7)\nplt.title('hobbies')\nplt.plot(range(2011, 2016 + 1 ,1), hobbies_state.groupby(['year']).mean().CA, label='CA')\nplt.plot(range(2011, 2016 + 1 ,1), hobbies_state.groupby(['year']).mean().TX, label='TX')\nplt.plot(range(2011, 2016 + 1 ,1), hobbies_state.groupby(['year']).mean().WI, label='WI')\nplt.legend(loc='upper right')\nplt.subplot(3,3,8)\nplt.title('household')\nplt.plot(range(2011, 2016 + 1 ,1), household_state.groupby(['year']).mean().CA, label='CA')\nplt.plot(range(2011, 2016 + 1 ,1), household_state.groupby(['year']).mean().TX, label='TX')\nplt.plot(range(2011, 2016 + 1 ,1), household_state.groupby(['year']).mean().WI, label='WI')\nplt.legend(loc='upper right')\nplt.subplot(3,3,9)\nplt.title('foods')\nplt.plot(range(2011, 2016 + 1 ,1), foods_state.groupby(['year']).mean().CA, label='CA')\nplt.plot(range(2011, 2016 + 1 ,1), foods_state.groupby(['year']).mean().TX, label='TX')\nplt.plot(range(2011, 2016 + 1 ,1), foods_state.groupby(['year']).mean().WI, label='WI')\nplt.legend(loc='upper right')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we look at the sales from differnt perspectives. Week, month, and year.\n* week\n    * In every state and every product type, all the trends of sales are the same. Peak on Saterday , decrease till Thursday, and rise on Friday. Therefore,  form a deep valley.\n    * Maybe this is the reason why there is a dense oscillation between growth and recession of the sales through the years.\n* month\n    * There is an obvious hill in the curves of \"household\" and \"food\" between May and September.\n    * Coinsidently, summer vacation starts from June to August. So, maybe when people start to enjoy their vacations, the needs for food and household increase. And that's why the sales of household and food reach their peaks between June and August. Then strat decreasing in September, when summer vacation ends.\n    * Perhaps this kind of up and down is why when the sales increase everey years, there is an \"S\" shape trend. \n    * Though there is an interesting thing to point out. While \"household\" and \"food\" increase in summer vacation. \"hobbies\" decreases in the exectly same period. And that is pretty weird, shouldn't people go outside and play?? Maybe some Americans can help me understand this.\n* year\n    * As the econemy grow in America, yearly sales in every state basically grow every year, except for year 2014. There is quite a bit of a set back in 2014. Perhaps something hug happened in that year.\n    * So here are a few things went on taht year: \"Ebola Epidemic Becomes Global Health Crisis\", \"Rise of ISIS\", \"California facing extreme drought\", \"World cup\", \"Ferguson protests\", \"Bill Cosby rape\""},{"metadata":{},"cell_type":"markdown","source":"## Perspective of price"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12, 12))\nplt.subplot(2,1,1)\nhobbies_1_prices = sell_prices.loc[sell_prices['item_id'].str.contains('HOBBIES_1')]\nhobbies_1_prices_CA = hobbies_1_prices.loc[hobbies_1_prices['store_id'].str.contains('CA')]\nhobbies_1_prices_TX = hobbies_1_prices.loc[hobbies_1_prices['store_id'].str.contains('TX')]\nhobbies_1_prices_WI = hobbies_1_prices.loc[hobbies_1_prices['store_id'].str.contains('WI')]\ngrouped_CA = hobbies_1_prices_CA.groupby(['wm_yr_wk'])['sell_price'].mean()\nplt.plot(grouped_CA.index, grouped_CA.values, label=\"CA\")\ngrouped_TX = hobbies_1_prices_TX.groupby(['wm_yr_wk'])['sell_price'].mean()\nplt.plot(grouped_TX.index, grouped_TX.values, label=\"TX\")\ngrouped_WI = hobbies_1_prices_WI.groupby(['wm_yr_wk'])['sell_price'].mean()\nplt.plot(grouped_WI.index, grouped_WI.values, label=\"WI\")\nplt.legend(loc=(1.0, 0.5))\nplt.title('HOBBIES_1 mean sell prices by state');\nplt.subplot(2,1,2)\ncal = calendar[['wm_yr_wk', 'd']]\ncal = cal.rename(columns={\"d\": \"index\"})\nhobbies_1 = train_sales.loc[train_sales['item_id'].str.contains('HOBBIES_1')]\nhobbies_1_CA = hobbies_1.loc[hobbies_1['store_id'].str.contains('CA')].drop(columns = ['id','item_id','dept_id','cat_id','store_id','state_id']).sum().reset_index().drop(1913)\nhobbies_1_TX = hobbies_1.loc[hobbies_1['store_id'].str.contains('TX')].drop(columns = ['id','item_id','dept_id','cat_id','store_id','state_id']).sum().reset_index().drop(1913)\nhobbies_1_WI = hobbies_1.loc[hobbies_1['store_id'].str.contains('WI')].drop(columns = ['id','item_id','dept_id','cat_id','store_id','state_id']).sum().reset_index().drop(1913)\nhobbies_1_CA = pd.merge(hobbies_1_CA, cal, on='index')\nhobbies_1_TX = pd.merge(hobbies_1_TX, cal, on='index')\nhobbies_1_WI = pd.merge(hobbies_1_WI, cal, on='index')\ngrouped_CA = hobbies_1_CA.drop(columns = \"index\").groupby(['wm_yr_wk']).sum()\nplt.plot(grouped_CA.index, grouped_CA.values, label=\"CA\")\ngrouped_TX = hobbies_1_TX.drop(columns = \"index\").groupby(['wm_yr_wk']).sum()\nplt.plot(grouped_TX.index, grouped_TX.values, label=\"TX\")\ngrouped_WI = hobbies_1_WI.drop(columns = \"index\").groupby(['wm_yr_wk']).sum()\nplt.plot(grouped_WI.index, grouped_WI.values, label=\"WI\")\nplt.legend(loc=(1.0, 0.5))\nplt.title('HOBBIES_1 sum sales by state');\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"* Here are a few funny things in these two graphs. The upper one shows the prices of hobbies_1 through the time series. The lower one shows the sales through the time series. Here are the funny stuffs:\n    * Whenever there is a raise in price, there is a drop in sales. \n    * After a drop in sales, it slowly climbs back. Then walmart raise it's price again, sales drops again. As this goes, walmart manages to get more money without losing customers in the long term.\n    * Walmart raise it's price nationally."},{"metadata":{},"cell_type":"markdown","source":"## Find the sales with different mean\nHere I use **simple moving average**, **weighted moving average**, and **exponential moving average** to find the tendency of sales in the close period.<br/>\nAs you can see in the graph each mean method revealed different tendency. This might come handy in the feature."},{"metadata":{"trusted":true},"cell_type":"code","source":"def SMA(days, n):\n    total = 0\n    for i in range(n):\n        total = total + days[i]\n    return total/n\n\ndef count_SMA(orig, n):\n    ret = np.zeros(len(orig) - n)\n    for i in range(len(ret)):\n        ret[i] = SMA(np.array(orig[i:i+n]), n)\n    return ret\n\ndef WMA(days, n):\n    total = 0\n    dev = 0\n    for i in range(n):\n        total = total + (n-i)*days[i]\n        dev = dev + (n-i)\n    return total/dev\n\ndef count_WMA(orig, n):\n    ret = np.zeros(len(orig) - n)\n    for i in range(len(ret)):\n        ret[i] = WMA(np.array(orig[i:i+n]), n)\n    return ret\n\ndef EMA(days, n):\n    total = 0\n    a = 2/(n+1)\n    for i in range(n):\n        total = total + a*(days[i] - total)\n    return total\n\ndef count_EMA(orig, n):\n    ret = np.zeros(len(orig) - n)\n    for i in range(len(ret)):\n        ret[i] = EMA(np.array(orig[i:i+n]), n)\n    return ret","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hobbies_1_CA = hobbies_1_CA.rename(columns={0: \"sales\"})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"CA_SMA_28 = count_SMA(hobbies_1_CA['sales'], 28)\nCA_WMA_28 = count_WMA(hobbies_1_CA['sales'], 28)\nCA_EMA_28 = count_EMA(hobbies_1_CA['sales'], 28)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12, 6))\nplt.subplot(3,1,1)\nplt.plot(range(len(hobbies_1_CA['sales'])), hobbies_1_CA['sales'], label=\"original\")\nplt.plot(range(len(CA_SMA_28)), CA_SMA_28, label=\"SMA\")\nplt.legend(loc=(1.0, 0.5))\nplt.subplot(3,1,2)\nplt.plot(range(len(hobbies_1_CA['sales'])), hobbies_1_CA['sales'], label=\"original\")\nplt.plot(range(len(CA_WMA_28)), CA_WMA_28, label=\"WMA\")\nplt.legend(loc=(1.0, 0.5))\nplt.subplot(3,1,3)\nplt.plot(range(len(hobbies_1_CA['sales'])), hobbies_1_CA['sales'], label=\"original\")\nplt.plot(range(len(CA_EMA_28)), CA_EMA_28, label=\"EMA\")\nplt.legend(loc=(1.0, 0.5))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modeling\nFrome the EDA we saw that there is obvious relationship between time(week day, month, year) and te sales. So, the training data attributes contains time. At the moment, these are the only attributre. Perhaps in the features there will be more attributes."},{"metadata":{"trusted":true},"cell_type":"code","source":"def melt_sales(df):\n    df = df.drop([\"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\", \"total_sales\"], axis=1).melt(\n        id_vars=['id'], var_name='d', value_name='demand')\n    return df\n\nsales = melt_sales(train_sales)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def map_f2d(d_col, id_col):\n    eval_flag = id_col.str.endswith(\"evaluation\")\n    return \"d_\" + (d_col.str[1:].astype(\"int\") + 1913 + 28 * eval_flag).astype(\"str\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = submission_file.melt(id_vars=\"id\", var_name=\"d\", value_name=\"demand\").assign( demand=np.nan, d = lambda df: map_f2d(df.d, df.id))\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_trend = train_sales.drop(columns = ['id','item_id','dept_id','cat_id','store_id','state_id', 'total_sales']).mean().reset_index()\nsales_trend.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_trend.rename(columns={'index':'d', 0: 'sales'}, inplace=True)\nsales_trend = sales_trend.merge(calendar[[\"wday\",\"month\",\"year\",\"d\"]], on=\"d\",how='left')\nsales_trend = sales_trend.drop(columns = [\"d\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def split_sequences(sequences, n_steps):\n    X, y = list(), list()\n    for i in range(len(sequences)):\n        # find the end of this pattern\n        end_ix = i + n_steps\n        # check if we are beyond the dataset\n        if end_ix > len(sequences):\n            break\n        # gather input and output parts of the pattern\n        seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1, -1]\n        X.append(seq_x)\n        y.append(seq_y)\n    return array(X), array(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"in_seq1 = array(sales_trend['wday'])\nin_seq2 = array(sales_trend['month'])\nin_seq3 = array(sales_trend['year'])\nout_seq = array(sales_trend['sales'])\nin_seq1 = in_seq1.reshape((len(in_seq1), 1))\nin_seq2 = in_seq2.reshape((len(in_seq2), 1))\nin_seq3 = in_seq3.reshape((len(in_seq3), 1))\nout_seq = out_seq.reshape((len(out_seq), 1))\ndataset = hstack((in_seq1, in_seq2, in_seq3, out_seq))\nn_steps = 7\nX, y = split_sequences(dataset, n_steps)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_x = X[:-30]\ntrain_y = y[:-30]\ntest_x = X[-30:]\ntest_y = y[-30:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_features = train_x.shape[2]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## CNN"},{"metadata":{},"cell_type":"markdown","source":"Reference: https://machinelearningmastery.com/how-to-develop-convolutional-neural-network-models-for-time-series-forecasting/"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers import Flatten\nfrom keras.layers import TimeDistributed\nfrom keras.layers.convolutional import Conv1D\nfrom keras.layers.convolutional import MaxPooling1D","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps, n_features)))\nmodel.add(MaxPooling1D(pool_size=2))\nmodel.add(Flatten())\nmodel.add(Dense(50, activation='relu'))\nmodel.add(Dense(1))\nmodel.compile(optimizer='adam', loss='mse')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(train_x, train_y, epochs=400, verbose=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"last_30_400 = np.zeros(30)\ni = 0\nfor test in test_x:\n    test = test.reshape((1, n_steps, n_features))\n    last_30_400[i] = model.predict(test, verbose=0)\n    i = i + 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Results\nBelow are the results of the trained CNN with different epochs. As you can see, CNN starts to the learn the trend in 50 epochs. After 300 epochs, the predicted trend and real trend has exectly the same ups and downs.<br/>\nThe result of this method is pretty bad. The score is 2.33440. I will keep work on this and see if I can do anything to improve it.\n![](https://i.imgur.com/qevsAxX.png)"},{"metadata":{"trusted":true},"cell_type":"code","source":"subs = submission.groupby(['d']).mean().reset_index()\nresult = subs ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subs = subs.merge(calendar[[\"wday\",\"month\",\"year\",\"d\"]], on=\"d\",how='left')\nsubs = subs.drop(columns = [\"d\", \"demand\"])\nsubs = pd.concat([sales_trend, subs], ignore_index=True, sort=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"in_seq1 = array(subs['wday'])\nin_seq2 = array(subs['month'])\nin_seq3 = array(subs['year'])\nout_seq = array(np.zeros(1969))\nin_seq1 = in_seq1.reshape((len(in_seq1), 1))\nin_seq2 = in_seq2.reshape((len(in_seq2), 1))\nin_seq3 = in_seq3.reshape((len(in_seq3), 1))\nout_seq = out_seq.reshape((len(out_seq), 1))\ndataset = hstack((in_seq1, in_seq2, in_seq3, out_seq))\nn_steps = 7\nX, y = split_sequences(dataset, n_steps)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subs = X[-56:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"i = 0\nfor sub in subs:\n    sub = sub.reshape((1, n_steps, n_features))\n    result['demand'][i] = model.predict(sub, verbose=0)\n    i = i + 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(1,29):\n    submission_file.loc[submission_file.id.str.contains(\"validation\"), \"F\" + str(i)] = result[\"demand\"][i-1]\n    submission_file.loc[submission_file.id.str.contains(\"evaluation\"), \"F\" + str(i)] = result[\"demand\"][i + 28-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_file.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}