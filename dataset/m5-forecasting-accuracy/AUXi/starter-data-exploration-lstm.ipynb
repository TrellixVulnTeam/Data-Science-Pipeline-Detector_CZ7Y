{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"こちらのNotebook[https://www.kaggle.com/robikscube/m5-forecasting-starter-data-exploration]\nに日本語訳や解説を付け加えたNotebook[https://www.kaggle.com/takahiro1127/starter-data-exploration-lstm]\nを再編集しています．","metadata":{}},{"cell_type":"markdown","source":"# 1. M5 Forecasting Challenge(コンペの概要)\n<img src=\"https://images.ctfassets.net/osv85d77hkdf/7LsZ5bZzvGaG6iwYkoKEUc/84afe0bf84371542fe56e6d5f0b3377b/hero_telescope_01_2x.png\" width=\"500\" height=\"300\" />\n\nThe goal of this notebook is to give competitors a quick overview of the 2020 M5 competition. After reading it you should have a good idea of the objective you are trying to solve, the data provided and the metrics you will be scored on.\n\nSome tl;dr items to note:\n- There are two parallel competitions: **Accuracy** and **Uncertainty**\n    - The accuracy competition will use the metric: **Weighted Root Mean Squared Scaled Error** (RMSSE)\n    - The uncertainty competition will use the metric: **Weighted Scaled Pinball Loss** (WSPL)\n- We are tasked with forecasting hierarchical sales data from Wal-Mart.\n- The data, covers stores in three US States (California, Texas, and Wisconsin) and includes item level, department, product categories, and store details.\n- In addition, it has explanatory variables such as price, promotions, day of the week, and special events.","metadata":{}},{"cell_type":"markdown","source":"**コンペの目標**  \nwal-martの売り上げを予測  \n**データの概要**  \n2011年1月29日から2016年4月24日までの約５年分の売上データ  \n(州名,商品の種類,部署,品目,店舗の詳細,価格や広告，曜日やイベントなど)  \n**評価指標**  \nWeighted Root Mean Squared Scaled Error (RMSSE)：目標値と実測値の差の二乗平均平方根 \n\n$$\n\\mbox{RMSSE} = \\displaystyle \\sqrt{\\frac{1}{h}\\displaystyle \\frac{\\sum_{t=n+1}^{n+h}\\left(Y_{t}-\\hat{Y_{t}}\\right)^2}{\\frac{1}{n-1}\\sum_{t=2}^{n}\\left(Y_{t} - Y_{t-1}\\right)^2}}\n$$","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pylab as plt\nimport seaborn as sns\nfrom itertools import cycle\npd.set_option('max_columns', 50)\nplt.style.use('bmh')\ncolor_cycle = cycle(plt.rcParams['axes.prop_cycle'].by_key()['color'])","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-22T03:31:34.085862Z","iopub.execute_input":"2022-02-22T03:31:34.086207Z","iopub.status.idle":"2022-02-22T03:31:34.09391Z","shell.execute_reply.started":"2022-02-22T03:31:34.086155Z","shell.execute_reply":"2022-02-22T03:31:34.09304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. データの取得\n- `calendar.csv` - Contains information about the dates on which the products are sold.\n- `sales_train_validation.csv` - Contains the historical daily unit sales data per product and store [d_1 - d_1913]\n- `sample_submission.csv` - The correct format for submissions. Reference the Evaluation tab for more info.\n- `sell_prices.csv` - Contains information about the price of the products sold per store and date.\n\nNot available yet:\n- `sales_train_evaluation.csv` - Available one month before competition deadline. Will include sales [d_1 - d_1941]","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0"}},{"cell_type":"markdown","source":"各データファイル\n- `calendar.csv` - その日のイベント、イベント種別、曜日など日付に紐づいたデータが記載\n- `sales_train_validation.csv` - 商品毎と店毎に分けたの売り上げ数(state_idはstore_idから一意に定まる、これはitem_id→dept_id, cat_idも同じ)\n- `sample_submission.csv` - 予測結果として提出するcsvの例\n- `sell_prices.csv` - その商品の店と日付ごとの価格","metadata":{}},{"cell_type":"code","source":"!ls -GFlash --color ../input/m5-forecasting-accuracy/","metadata":{"execution":{"iopub.status.busy":"2022-02-22T03:31:34.096438Z","iopub.execute_input":"2022-02-22T03:31:34.09744Z","iopub.status.idle":"2022-02-22T03:31:34.83892Z","shell.execute_reply.started":"2022-02-22T03:31:34.096711Z","shell.execute_reply":"2022-02-22T03:31:34.838122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read in the data\nINPUT_DIR = '../input/m5-forecasting-accuracy'\ncal = pd.read_csv(f'{INPUT_DIR}/calendar.csv')\nstv = pd.read_csv(f'{INPUT_DIR}/sales_train_validation.csv')\nss = pd.read_csv(f'{INPUT_DIR}/sample_submission.csv')\nsellp = pd.read_csv(f'{INPUT_DIR}/sell_prices.csv')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-22T03:31:34.842353Z","iopub.execute_input":"2022-02-22T03:31:34.842629Z","iopub.status.idle":"2022-02-22T03:31:43.287876Z","shell.execute_reply.started":"2022-02-22T03:31:34.842576Z","shell.execute_reply":"2022-02-22T03:31:43.287079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. 提出サンプルより予測するデータを確認\nWe are trying for forecast sales for 28 forecast days. The sample submission has the following format:\n- The columns represent 28 forecast days. We will fill these forecast days with our predictions.\n- The rows each represent a specific item. This id tells us the item type, state, and store. We don't know what these items are exactly.","metadata":{}},{"cell_type":"markdown","source":"何を予測しようとしているのかについて確認します．\nこのコンペを通じて、予測しようとしているのは28日分の売上（4週間（2016年4月25日から5月22日）の売上（販売数量））です．  \n提出のサンプルから次のようなフォーマットで予想すればいい事がわかります．\n\n- それぞれの列はこれから先の28日を1日ごとに示しています．今回はこれを埋めていく事になります．\n- それぞれの行は特定の商品を表しています．idによって商品の種別や州、店名までは特定することができます．  \nしかし、どの品目であるのか一意に定めることはできません．\n(注釈　つまりこれは東京都の新宿店の清涼飲料水の売上である事までは特定できても、コカコーラの売上であるというところまでは特定できないという事です．)","metadata":{}},{"cell_type":"code","source":"# 提出サンプル\nss.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-22T03:31:43.292908Z","iopub.execute_input":"2022-02-22T03:31:43.293147Z","iopub.status.idle":"2022-02-22T03:31:43.315355Z","shell.execute_reply.started":"2022-02-22T03:31:43.293097Z","shell.execute_reply":"2022-02-22T03:31:43.314454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We are given historic sales data in the `sales_train_validation` dataset.\n- rows exist in this dataset for days d_1 to d_1913. We are given the department, category, state, and store id of the item.\n- d_1914 - d_1941 represents the `validation` rows which we will predict in stage 1\n- d_1942 - d_1969 represents the `evaluation` rows which we will predict for the final competition standings.\n\n\n`sales_train_validation.csv` から、売上の時系列データを得ることができます．\n\n- それぞれの列は1日目から1913日目までの売上数を保持しています．  \nまた、その商品の部門名とカテゴリー名もデータに含まれています．  \nそれぞれの商品の売上を店毎に行を分けているため、店名とその店の存在する州名も保持しています．\n- stage1では1914日目から1941日目までの予測を行います．\n- このコンペの最終評価は1942日目から1969日目までの予測結果に基づいて行います．","metadata":{}},{"cell_type":"code","source":"# 商品毎と店毎に分けたの売り上げ数\nstv.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-22T03:31:43.318126Z","iopub.execute_input":"2022-02-22T03:31:43.318593Z","iopub.status.idle":"2022-02-22T03:31:43.354484Z","shell.execute_reply.started":"2022-02-22T03:31:43.318543Z","shell.execute_reply":"2022-02-22T03:31:43.353703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# その商品の店・日付ごとの価格\nsellp.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-22T03:37:34.816063Z","iopub.execute_input":"2022-02-22T03:37:34.816366Z","iopub.status.idle":"2022-02-22T03:37:34.826765Z","shell.execute_reply.started":"2022-02-22T03:37:34.816314Z","shell.execute_reply":"2022-02-22T03:37:34.826007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. データの可視化\n- Lets take a random item that sell a lot and see how it's sales look across the training data.\n- `FOODS_3_090_CA_3_validation` sells a lot\n- Note there are days where it appears the item is unavailable and sales flatline","metadata":{}},{"cell_type":"markdown","source":"商品を選んで、そのデータを可視化してみる．\n\n- まずはよく売れている商品からランダムに選択してその商品が訓練用データセットにおいてどのように推移しているかを見ていく事にする．\n- ` FOODS_3_090_CA_3_validation` がよく売れているようなので、これにする．\n- 商品が未発売であるためグラフ上で平行に推移している可能性があるので、注意してください．","metadata":{}},{"cell_type":"code","source":"d_cols = [c for c in stv.columns if 'd_' in c] # sales data columns\n# d_がカラム名に含まれているカラムのみ抽出して配列として保持\n\n# Below we are chaining the following steps in pandas:\n# 下のメソッドチェーンでは以下に示すデータ処理をpandasで行なっています．\n# 1. Select the item.\n# 商品の絞り込みを行います．\n# 2. Set the id as the index, Keep only sales data columns\n# idについてはindexとして保持するようにします．また、d_とつくカラムのみにデータを絞っています．\n# 3. Transform so it's a column\n# このままでは行なので、列に変えます．(一行を転置して一列にしている．)\n# 4. Plot the data\n# プロットしている．\nstv.loc[stv['id'] == 'FOODS_3_090_CA_3_validation'] \\\n    .set_index('id')[d_cols] \\\n    .T \\\n    .plot(figsize=(15, 5),\n          title='FOODS_3_090_CA_3 sales by \"d\" number',\n          color=next(color_cycle))\nplt.legend('')\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-22T03:31:43.355758Z","iopub.execute_input":"2022-02-22T03:31:43.35616Z","iopub.status.idle":"2022-02-22T03:31:43.676475Z","shell.execute_reply.started":"2022-02-22T03:31:43.356116Z","shell.execute_reply":"2022-02-22T03:31:43.675568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ぱっと見ですが、下の様なことが予想できます．\n- d_1からd_251は上で注意点として述べたように、この商品が未発売であるため、売上が0として計上されている可能性が高いです．\n- 何度かスパイクがあるので、そこに絞ってカレンダーを注視してみるのが良さそうです．\n- 周期的に売上が変動しているように見えるので曜日毎、日付ごとなどデータの集計方式を変える事に意味がありそう．","metadata":{}},{"cell_type":"markdown","source":"## 4.1 訓練データをカレンダーとマージ\n- We are given a calendar with additional information about past and future dates.\n- The calendar data can be merged with our days data\n- From this we can find weekly and annual trends","metadata":{}},{"cell_type":"markdown","source":"訓練用データをカレンダーcsvとマージしてみる．\n\n- calendar.csvは過去の日付から未来の日付まで用意してあります．\n- calendar.csvはstvのdaysデータとjoinする事ができる．\n- joinする事によって、週毎、年毎のトレンドをみる事ができる．","metadata":{}},{"cell_type":"code","source":"# Calendar data looks like this (only showing columns we care about for now)\n# カレンダーcsvはこの様な見た目をしています．\ncal[['d','date','event_name_1','event_name_2',\n     'event_type_1','event_type_2', 'snap_CA']].head()","metadata":{"execution":{"iopub.status.busy":"2022-02-22T03:31:43.677851Z","iopub.execute_input":"2022-02-22T03:31:43.6783Z","iopub.status.idle":"2022-02-22T03:31:43.697932Z","shell.execute_reply.started":"2022-02-22T03:31:43.678248Z","shell.execute_reply":"2022-02-22T03:31:43.697199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Merge calendar on our items' data\n# ここから、カレンダーcsvをstv(商品データ)にjoinしていく．\nexample = stv.loc[stv['id'] == 'FOODS_3_090_CA_3_validation'][d_cols].T\n#↑先ほどのメソッドチェーンと同様の処理を行なっている．\nexample = example.rename(columns={8412:'FOODS_3_090_CA_3'}) # Name it correctly\n#indexがカラム名になっていたので、それを商品IDに変更\nexample = example.reset_index().rename(columns={'index': 'd'}) # make the index \"d\"\n#indexを新たに付与\nexample = example.merge(cal, how='left', validate='1:1')\n#↑ここで、calendarとjoinする．\nexample.set_index('date')['FOODS_3_090_CA_3'].plot(figsize=(15, 5),\n                                                   color=next(color_cycle),\n                                                   title='FOODS_3_090_CA_3 sales by actual sale dates')\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-22T03:31:43.699406Z","iopub.execute_input":"2022-02-22T03:31:43.699897Z","iopub.status.idle":"2022-02-22T03:31:44.022513Z","shell.execute_reply.started":"2022-02-22T03:31:43.699848Z","shell.execute_reply":"2022-02-22T03:31:44.021764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"上の様に、日付表示する事ができる様になった．","metadata":{}},{"cell_type":"markdown","source":"# 5. 売上高を時間変数で分解\n- Now that we have our example item lets see how it sells by:\n    - Day of the week\n    - Month\n    - Year","metadata":{}},{"cell_type":"markdown","source":"売り上げを各日付に紐づいた変数に基づいて集計し直していきます．  \nこれから以下の日付指標に基づいて,`FOODS_3_090_CA_3`の分析を行います．\n- 曜日\n- 月\n- 年","metadata":{}},{"cell_type":"code","source":"fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 4))\nexample.groupby('wday').mean()['FOODS_3_090_CA_3'] \\\n    .plot(kind='line',\n          title='average sale: day of week',\n          lw=5,\n          color=next(\n              color_cycle),\n          ax=ax1)\n#↑曜日で一括りにして平均をとる\nexample.groupby('month').mean()['FOODS_3_090_CA_3'] \\\n    .plot(kind='line',\n          title='average sale: month',\n          lw=5,\n          color=next(\n               color_cycle),\n\n          ax=ax2)\n#↑月で一括りにして平均をとる\nexample.groupby('year').mean()['FOODS_3_090_CA_3'] \\\n    .plot(kind='line',\n          lw=5,\n          title='average sale: year',\n          color=next(\n              color_cycle),\n\n          ax=ax3)\n#↑年で一括りにして平均をとる\nfig.suptitle('Trends for item: FOODS_3_090_CA_3',\n             size=20,\n             y=1.1)\nplt.tight_layout()\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-22T03:31:44.023763Z","iopub.execute_input":"2022-02-22T03:31:44.024221Z","iopub.status.idle":"2022-02-22T03:31:44.945959Z","shell.execute_reply.started":"2022-02-22T03:31:44.024171Z","shell.execute_reply":"2022-02-22T03:31:44.944936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- 曜日による差がある事がわかる．\nただし曜日で生じる差が商品の特性によるものなのか、スーパーマーケットの訪問者数によるものなのかまだ判断がつかない．  \n(例えば商品がステーキ肉であった場合、週末はステーキを食べる傾向にあるのか、それとも週末に多くの家族が買い出しにくる傾向にあるのかがわからない．)\n- 月により差があることがわかる．\nスーパーの訪問者数が一定であると考えると、二倍程度の差が出ているので、旬や収穫時期が明確に存在するものである可能性がある．  \nもしくはハロウィン→カボチャのように季節のイベントと密接に関わりのあるものであることが想像できる．  \nもしくはスーパーの訪問者数が一定でない可能性もある．  \n例えばビーチの近くのお店だった場合、夏に観光客が多いため訪問者数の増加に従って商品の売り上げも伸びている．  \nつまり、商品のもつトレンドではなく、州や店舗のもつトレンドである可能性もある．\n- 年により差があることがわかる．\n単にこの店舗が2011年にオープンで物珍しさから2,3年の訪問者数が多かった可能性がある．  \nまた、店舗の商品の値段の推移などから経営方針に影響を受けている可能性も加味できる．","metadata":{}},{"cell_type":"markdown","source":"# 6. 商品ごとの売上の特徴を調査\n- Lets put it all together to plot 20 different items and their sales\n- Some observations from these plots:\n    - It is common to see an item unavailable for a period of time.\n    - Some items only sell 1 or less in a day, making it very hard to predict.\n    - Other items show spikes in their demand (super bowl sunday?) possibly the \"events\" provided to us could help with these.","metadata":{}},{"cell_type":"markdown","source":"それでは他の商品についても見ていきましょう．\n- 20個ほど商品を選んで、それらの売上を上と同様に集計します．\n※↓以下データから得られた情報に基づいての記述です．\n- グラフのデータについていくつか中止すべき点があります．\n - ある期間、商品が売られていない事がある．\n - 1日に売れる数が多くても1個の様な商品は予測することが難しい．\n - (多くても1日に一個しか売れない商品以外では)スパイク(その部分だけ売上が異常に多い事)が観測できる．  \n (おそらくsuper bowl(アメリカのビッグイベント)のある日曜日?)スパイクの起きている日については、calendar.csvのevent dataを見れば良い知見が得られそう．","metadata":{}},{"cell_type":"code","source":"twenty_examples = stv.sample(20, random_state=529) \\\n        .set_index('id')[d_cols] \\\n    .T \\\n    .merge(cal.set_index('d')['date'],\n           left_index=True,\n           right_index=True,\n            validate='1:1') \\\n    .set_index('date')\n# 20個抽出しています．","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-22T03:31:44.947395Z","iopub.execute_input":"2022-02-22T03:31:44.947911Z","iopub.status.idle":"2022-02-22T03:31:44.975845Z","shell.execute_reply.started":"2022-02-22T03:31:44.947853Z","shell.execute_reply":"2022-02-22T03:31:44.975141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axs = plt.subplots(10, 2, figsize=(15, 20))\naxs = axs.flatten()\nax_idx = 0\nfor item in twenty_examples.columns:\n    twenty_examples[item].plot(title=item,\n                              color=next(color_cycle),\n                              ax=axs[ax_idx])\n    ax_idx += 1\nplt.tight_layout()\nplt.show()\n# それぞれの商品毎にデータを表示しています．","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-22T03:31:44.977075Z","iopub.execute_input":"2022-02-22T03:31:44.977519Z","iopub.status.idle":"2022-02-22T03:31:48.648977Z","shell.execute_reply.started":"2022-02-22T03:31:44.977471Z","shell.execute_reply":"2022-02-22T03:31:48.648243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 7. 売上データを商品のタイプごとに分割\n- We have several item types:\n    - Hobbies\n    - Household\n    - Foods\n- Lets plot the total demand over time for each type","metadata":{}},{"cell_type":"markdown","source":"売上の時系列データを商品のタイプごとに分けて見ていきましょう．\n- 与えられたデータには以下の様な商品タイプがありました．\n  - 娯楽品\n  - 生活必需品\n  - 食料品\n- それぞれの商品タイプについてデータを集計し、グラフに表示していきたいと思います．","metadata":{}},{"cell_type":"code","source":"stv['cat_id'].unique()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-22T03:31:48.650239Z","iopub.execute_input":"2022-02-22T03:31:48.650631Z","iopub.status.idle":"2022-02-22T03:31:48.660024Z","shell.execute_reply.started":"2022-02-22T03:31:48.650588Z","shell.execute_reply":"2022-02-22T03:31:48.658943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stv.groupby('cat_id').count()['id'] \\\n    .sort_values() \\\n    .plot(kind='barh', figsize=(15, 5), title='Count of Items by Category')\nplt.show()\n# カテゴリー毎の品目数を表示","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-22T03:31:48.661984Z","iopub.execute_input":"2022-02-22T03:31:48.662479Z","iopub.status.idle":"2022-02-22T03:31:49.242316Z","shell.execute_reply.started":"2022-02-22T03:31:48.662301Z","shell.execute_reply":"2022-02-22T03:31:49.241387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"past_sales = stv.set_index('id')[d_cols] \\\n    .T \\\n    .merge(cal.set_index('d')['date'],\n           left_index=True,\n           right_index=True,\n            validate='1:1') \\\n    .set_index('date')\n# 商品のデータを上の処理と同じように転置して、日付データとjoinしています．\nfor i in stv['cat_id'].unique():\n    items_col = [c for c in past_sales.columns if i in c]\n    past_sales[items_col] \\\n        .sum(axis=1) \\\n        .plot(figsize=(15, 5),\n              alpha=0.8,\n              title='Total Sales by Item Type')\nplt.legend(stv['cat_id'].unique())\nplt.show()\n#カテゴリー毎のその日に売れた合計数を表しています．","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-22T03:31:49.243902Z","iopub.execute_input":"2022-02-22T03:31:49.244567Z","iopub.status.idle":"2022-02-22T03:31:50.754841Z","shell.execute_reply.started":"2022-02-22T03:31:49.24451Z","shell.execute_reply":"2022-02-22T03:31:50.753995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ここからいくつか読み取れる事があります．\n- 1/1(クリスマスとか大晦日の可能性有)には毎年全店休業になっている事です．\n- 全体的に右肩上がりではある．(これが出店数の増加によるものなのか、集客力アップによるものなのかなどはわからない．)\n- 何度か階段状に上がっている様に見える部分がある．(2013年や2015年など)","metadata":{}},{"cell_type":"markdown","source":"# 8. 店舗毎の売上を確認\nWe are provided data for 10 unique stores. What are the total sales by stores?\n- Note that some stores are more steady than others.\n- CA_2 seems to have a big change occur in 2015","metadata":{}},{"cell_type":"markdown","source":"店舗毎の売上\n与えられたデータは10店舗のものです．それぞれの店舗毎に売上をみていきます．\n※↓データから得られた知見\n- 安定した売上を出している店舗もあればそうでない変化を見せる店舗もある．\n- CA_2には2015年に大きな変化があった事が想像できる．(店舗の改装、増築など)","metadata":{}},{"cell_type":"code","source":"# 売上のヒストグラム\nsellp['sell_price'].hist(bins=50)\nplt.xlim(0,25)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T03:46:38.085918Z","iopub.execute_input":"2022-02-22T03:46:38.086233Z","iopub.status.idle":"2022-02-22T03:46:38.531835Z","shell.execute_reply.started":"2022-02-22T03:46:38.086182Z","shell.execute_reply":"2022-02-22T03:46:38.531011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"store_list = sellp['store_id'].unique()\nfor s in store_list:\n    store_items = [c for c in past_sales.columns if s in c]\n    past_sales[store_items] \\\n        .sum(axis=1) \\\n        .rolling(90).mean() \\\n        .plot(figsize=(15, 5),\n              alpha=0.8,\n              title='Rolling 90 Day Average Total Sales (10 stores)')\nplt.legend(store_list)\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-22T03:31:50.756249Z","iopub.execute_input":"2022-02-22T03:31:50.756811Z","iopub.status.idle":"2022-02-22T03:31:51.705054Z","shell.execute_reply.started":"2022-02-22T03:31:50.756743Z","shell.execute_reply":"2022-02-22T03:31:51.704256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looking at the same data a different way, we can plot a rolling 7 day total demand count by store. Note clearly that some stores have abrupt changes in their demand, it could be that the store expanded or a new competitor was built near by. Either way this is imporant to note when creating predictive models about demand pattern. ","metadata":{}},{"cell_type":"markdown","source":"上では日毎にデータを表示していましたが、次は週ごとに出して見ます．  \nいくつかの店舗は急激な売上の変化がある事がわかります．これはおそらく店舗の増築や競合他社の店舗が近くにできたことに起因するのでしょう．  \nどちらにしても、売上予測のモデルを作る際にこの急激な変化を加味することが重要になってきます．","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(5, 2, figsize=(15, 10), sharex=True)\naxes = axes.flatten()\nax_idx = 0\nfor s in store_list:\n    store_items = [c for c in past_sales.columns if s in c]\n    #その店舗にある商品idの配列をとっています．\n    past_sales[store_items] \\\n        .sum(axis=1) \\\n        .rolling(7).mean() \\\n        .plot(alpha=1,\n              ax=axes[ax_idx],\n              title=s,\n              lw=3,\n              color=next(color_cycle))\n    #past salesは商品の売上データを時系列グラフで表現しやすいように転置、カレンダーデータとjoinしたものです．\n    #その日の商品の売上の合計をだし、\n    #rolling 7をすることでその日から一週間前までの売上の平均ととっています．\n    ax_idx += 1\n# plt.legend(store_list)\nplt.suptitle('Weekly Sale Trends by Store ID')\nplt.tight_layout()\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-22T03:31:51.706345Z","iopub.execute_input":"2022-02-22T03:31:51.706795Z","iopub.status.idle":"2022-02-22T03:31:53.860464Z","shell.execute_reply.started":"2022-02-22T03:31:51.706724Z","shell.execute_reply":"2022-02-22T03:31:53.859719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 9. 売上とカレンダーのヒートマップ","metadata":{}},{"cell_type":"markdown","source":"売り上げのヒートマップ表示","metadata":{}},{"cell_type":"code","source":"# ----------------------------------------------------------------------------\n# Author:  Nicolas P. Rougier\n# License: BSD\n# ----------------------------------------------------------------------------\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Polygon\nfrom datetime import datetime\nfrom dateutil.relativedelta import relativedelta\n\n\ndef calmap(ax, year, data):\n    ax.tick_params('x', length=0, labelsize=\"medium\", which='major')\n    ax.tick_params('y', length=0, labelsize=\"x-small\", which='major')\n\n    # Month borders\n    xticks, labels = [], []\n    start = datetime(year,1,1).weekday()\n    for month in range(1,13):\n        first = datetime(year, month, 1)\n        last = first + relativedelta(months=1, days=-1)\n\n        y0 = first.weekday()\n        y1 = last.weekday()\n        x0 = (int(first.strftime(\"%j\"))+start-1)//7\n        x1 = (int(last.strftime(\"%j\"))+start-1)//7\n\n        P = [ (x0,   y0), (x0,    7),  (x1,   7),\n              (x1,   y1+1), (x1+1,  y1+1), (x1+1, 0),\n              (x0+1,  0), (x0+1,  y0) ]\n        xticks.append(x0 +(x1-x0+1)/2)\n        labels.append(first.strftime(\"%b\"))\n        poly = Polygon(P, edgecolor=\"black\", facecolor=\"None\",\n                       linewidth=1, zorder=20, clip_on=False)\n        ax.add_artist(poly)\n    \n    ax.set_xticks(xticks)\n    ax.set_xticklabels(labels)\n    ax.set_yticks(0.5 + np.arange(7))\n    ax.set_yticklabels([\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"])\n    ax.set_title(\"{}\".format(year), weight=\"semibold\")\n    \n    # Clearing first and last day from the data\n    valid = datetime(year, 1, 1).weekday()\n    data[:valid,0] = np.nan\n    valid = datetime(year, 12, 31).weekday()\n    # data[:,x1+1:] = np.nan\n    data[valid+1:,x1] = np.nan\n\n    # Showing data\n    ax.imshow(data, extent=[0,53,0,7], zorder=10, vmin=-1, vmax=1,\n              cmap=\"RdYlBu_r\", origin=\"lower\", alpha=.75)\n# 軸と年数とデータを渡せばヒートマップを作ってくれるメソッドです．","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-22T03:31:53.861823Z","iopub.execute_input":"2022-02-22T03:31:53.862285Z","iopub.status.idle":"2022-02-22T03:31:53.882467Z","shell.execute_reply.started":"2022-02-22T03:31:53.862232Z","shell.execute_reply":"2022-02-22T03:31:53.881783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It appears that walmarts are closed on Chirstmas day. The highest demand day of all the data was on Sunday March 6th, 2016. What happened on this day you may ask... well the [Seventh Democratic presidential candidates debate hosted by CNN and held in Flint, Michigan](https://www.onthisday.com/date/2016/march/6)... I doubt that impacted sales though :D","metadata":{}},{"cell_type":"markdown","source":"walmartはクリスマスにしまっていることがわかりました．  \nまた、与えられたデータの期間の中でもっとも商品の需要があった日は2016/3/6の日曜日です．\n","metadata":{}},{"cell_type":"code","source":"print('The lowest sale date was:', past_sales.sum(axis=1).sort_values().index[0])\nprint('The highest sale date was:', past_sales.sum(axis=1).sort_values(ascending=False).index[0])","metadata":{"execution":{"iopub.status.busy":"2022-02-22T03:31:53.885521Z","iopub.execute_input":"2022-02-22T03:31:53.885744Z","iopub.status.idle":"2022-02-22T03:31:54.014586Z","shell.execute_reply.started":"2022-02-22T03:31:53.885701Z","shell.execute_reply":"2022-02-22T03:31:54.013613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nsscale = StandardScaler()\npast_sales.index = pd.to_datetime(past_sales.index)\nfor i in stv['cat_id'].unique():\n    fig, axes = plt.subplots(3, 1, figsize=(15, 5))\n    items_col = [c for c in past_sales.columns if i in c]\n    sales2013 = past_sales.loc[past_sales.index.isin(pd.date_range('31-Dec-2012',\n                                                                   periods=371))][items_col].mean(axis=1)\n    vals = np.hstack(sscale.fit_transform(sales2013.values.reshape(-1, 1)))\n    calmap(axes[0], 2013, vals.reshape(53,7).T)\n    sales2014 = past_sales.loc[past_sales.index.isin(pd.date_range('30-Dec-2013',\n                                                                   periods=371))][items_col].mean(axis=1)\n    vals = np.hstack(sscale.fit_transform(sales2014.values.reshape(-1, 1)))\n    calmap(axes[1], 2014, vals.reshape(53,7).T)\n    sales2015 = past_sales.loc[past_sales.index.isin(pd.date_range('29-Dec-2014',\n                                                                   periods=371))][items_col].mean(axis=1)\n    vals = np.hstack(sscale.fit_transform(sales2015.values.reshape(-1, 1)))\n    calmap(axes[2], 2015, vals.reshape(53,7).T)\n    plt.suptitle(i, fontsize=15, x=0.3, y=0.98)\n    plt.tight_layout()\n    plt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-22T03:31:54.019656Z","iopub.execute_input":"2022-02-22T03:31:54.0199Z","iopub.status.idle":"2022-02-22T03:31:56.755584Z","shell.execute_reply.started":"2022-02-22T03:31:54.019855Z","shell.execute_reply":"2022-02-22T03:31:56.754706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ヒートマップを見て気づいたことを列挙したいと思います．\n- やはり土日の売り上げが高いです．\n- 一月の平日が明らかに低いです．\n- ほとんどの場合、火曜から木曜は低いです(ところどころ高いところは祝日の可能性があります．)","metadata":{}},{"cell_type":"markdown","source":"# 10. 販売価格の比較\nWe are given historical sale prices of each item. Lets take a look at our example item from before.\n- It looks to me like the price of this item is growing.\n- Different stores have different selling prices.","metadata":{}},{"cell_type":"markdown","source":"販売価格\nそれぞれの商品の販売単価の推移がデータに含まれています．先ほど選んだ商品について、どのように推移していくかを見ていきます．\n- この商品の販売価格は徐々に上がってきていることがわかる．\n- 店舗ごとに違う値段をつけていることがわかる．","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(15, 5))\nstores = []\nfor store, d in sellp.query('item_id == \"FOODS_3_090\"').groupby('store_id'):\n    d.plot(x='wm_yr_wk',\n          y='sell_price',\n          style='.',\n          color=next(color_cycle),\n          figsize=(15, 5),\n          title='FOODS_3_090 sale price over time',\n         ax=ax,\n          legend=store)\n    stores.append(store)\n    # まず、商品IDで絞り込み、店ごとにgroupbyします．\n    # 販売価格をそれぞれプロットします．\n    plt.legend()\nplt.legend(stores)\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-22T03:31:56.757505Z","iopub.execute_input":"2022-02-22T03:31:56.758085Z","iopub.status.idle":"2022-02-22T03:31:57.927214Z","shell.execute_reply.started":"2022-02-22T03:31:56.75803Z","shell.execute_reply":"2022-02-22T03:31:57.926235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sellp['Category'] = sellp['item_id'].str.split('_', expand=True)[0]\nfig, axs = plt.subplots(1, 3, figsize=(15, 4))\ni = 0\nfor cat, d in sellp.groupby('Category'):\n    ax = d['sell_price'].apply(np.log1p) \\\n        .plot(kind='hist',\n                         bins=20,\n                         title=f'Distribution of {cat} prices',\n                         ax=axs[i],\n                                         color=next(color_cycle))\n    # 販売単価をカテゴリーごとにgroup byする．\n    # 販売単価を自然対数を底にlogをとり、どの金額範囲にどの程度の商品数が分布しているかを表示している．\n    ax.set_xlabel('Log(price)')\n    i += 1\nplt.tight_layout()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-22T03:31:57.928954Z","iopub.execute_input":"2022-02-22T03:31:57.932558Z","iopub.status.idle":"2022-02-22T03:32:24.566925Z","shell.execute_reply.started":"2022-02-22T03:31:57.932498Z","shell.execute_reply":"2022-02-22T03:32:24.56593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 11. 学習＆予測\n- Submit the average value from the past 30 days\n\n先月の平均を予測として算出","metadata":{}},{"cell_type":"code","source":"thirty_day_avg_map = stv.set_index('id')[d_cols[-30:]].mean(axis=1).to_dict()\nfcols = [f for f in ss.columns if 'F' in f]\nfor f in fcols:\n    ss[f] = ss['id'].map(thirty_day_avg_map).fillna(0)\n    \nss.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T03:32:24.568247Z","iopub.execute_input":"2022-02-22T03:32:24.568561Z","iopub.status.idle":"2022-02-22T03:32:28.771843Z","shell.execute_reply.started":"2022-02-22T03:32:24.568514Z","shell.execute_reply":"2022-02-22T03:32:28.77081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## TODO\n- その曜日の平均売り上げに基づいた簡単な予測の作成\n- facebookのprophetを用いたモデル\n- lgbm/xgbでの予想","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \nfrom sklearn import preprocessing, metrics\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM,Dropout\nfrom keras.layers import RepeatVector,TimeDistributed\nfrom numpy import array\nfrom keras.models import Sequential, load_model\n#import utils_paths\nimport re\nfrom tqdm import tqdm\nimport os","metadata":{"execution":{"iopub.status.busy":"2022-02-22T03:32:28.77388Z","iopub.execute_input":"2022-02-22T03:32:28.774242Z","iopub.status.idle":"2022-02-22T03:32:28.78074Z","shell.execute_reply.started":"2022-02-22T03:32:28.774185Z","shell.execute_reply":"2022-02-22T03:32:28.779904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_sales = stv\ncalendar = cal\nsellp = pd.read_csv(f'{INPUT_DIR}/sell_prices.csv')\nsell_prices = sellp\nsubmission_file = ss","metadata":{"execution":{"iopub.status.busy":"2022-02-22T03:32:28.782454Z","iopub.execute_input":"2022-02-22T03:32:28.783214Z","iopub.status.idle":"2022-02-22T03:32:31.760687Z","shell.execute_reply.started":"2022-02-22T03:32:28.783154Z","shell.execute_reply":"2022-02-22T03:32:31.759869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def transform(data):\n    \n    nan_features = ['event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']\n    for feature in nan_features:\n        data[feature].fillna('unknown', inplace = True)\n        \n    cat = ['event_name_1','event_type_1','event_name_2','event_type_2','snap_CA','snap_TX','snap_WI']\n    for feature in cat:\n        encoder = preprocessing.LabelEncoder()\n        data[feature] = encoder.fit_transform(data[feature])\n    \n    return data\n#calendarの何もない日をunknownで埋めて、label encoderによって処理しやすい形に変えた．","metadata":{"execution":{"iopub.status.busy":"2022-02-22T03:32:31.762142Z","iopub.execute_input":"2022-02-22T03:32:31.762416Z","iopub.status.idle":"2022-02-22T03:32:31.769353Z","shell.execute_reply.started":"2022-02-22T03:32:31.762371Z","shell.execute_reply":"2022-02-22T03:32:31.768116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"days = range(1, 1970)\ntime_series_columns = [f'd_{i}' for i in days]\ntransfer_cal = pd.DataFrame(calendar[['event_name_1','event_type_1','event_name_2','event_type_2','snap_CA','snap_TX','snap_WI']].values.T, index=['event_name_1','event_type_1','event_name_2','event_type_2','snap_CA','snap_TX','snap_WI'], columns= time_series_columns)\ntransfer_cal = transfer_cal.fillna(0)\nevent_name_1_se = transfer_cal.loc['event_name_1'].apply(lambda x: x if re.search(\"^\\d+$\", str(x)) else np.nan).fillna(10)\nevent_name_2_se = transfer_cal.loc['event_name_2'].apply(lambda x: x if re.search(\"^\\d+$\", str(x)) else np.nan).fillna(10)\n#↑event nameが入っているものをnanにした後、10にしている．","metadata":{"execution":{"iopub.status.busy":"2022-02-22T03:32:31.770634Z","iopub.execute_input":"2022-02-22T03:32:31.771115Z","iopub.status.idle":"2022-02-22T03:32:32.279916Z","shell.execute_reply.started":"2022-02-22T03:32:31.771044Z","shell.execute_reply":"2022-02-22T03:32:32.279085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"calendar['date'] = pd.to_datetime(calendar['date'])\ncalendar = calendar[calendar['date']>= '2016-1-27']  #reduce memory\n#使うデータを少なくします．\ncalendar= transform(calendar)\n# Attempts to convert events into time series data.\ntransfer_cal = pd.DataFrame(calendar[['event_name_1','event_type_1','event_name_2','event_type_2','snap_CA','snap_TX','snap_WI']].values.T,\n                            index=['event_name_1','event_type_1','event_name_2','event_type_2','snap_CA','snap_TX','snap_WI'])\ntransfer_cal","metadata":{"execution":{"iopub.status.busy":"2022-02-22T03:32:32.281387Z","iopub.execute_input":"2022-02-22T03:32:32.281667Z","iopub.status.idle":"2022-02-22T03:32:32.331344Z","shell.execute_reply.started":"2022-02-22T03:32:32.281622Z","shell.execute_reply":"2022-02-22T03:32:32.330335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"price_fea = calendar[['wm_yr_wk','date']].merge(sell_prices, on = ['wm_yr_wk'], how = 'left')\n#販売価格に対して、calendarを結合した．\nprice_fea['id'] = price_fea['item_id']+'_'+price_fea['store_id']+'_validation'\ndf = price_fea.pivot('id','date','sell_price')\n#各商品を一行にして、商品の値段の移り変わりを表した．","metadata":{"execution":{"iopub.status.busy":"2022-02-22T03:32:32.332908Z","iopub.execute_input":"2022-02-22T03:32:32.333342Z","iopub.status.idle":"2022-02-22T03:32:37.238912Z","shell.execute_reply.started":"2022-02-22T03:32:32.333166Z","shell.execute_reply":"2022-02-22T03:32:37.23817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"price_df = train_sales.merge(df,on=['id'],how= 'left').iloc[:,-145:]\nprice_df.index = train_sales.id\nprice_df.head()\n#train salesに値段の移り変わりをのデータをjoinした．","metadata":{"execution":{"iopub.status.busy":"2022-02-22T03:32:37.240449Z","iopub.execute_input":"2022-02-22T03:32:37.240749Z","iopub.status.idle":"2022-02-22T03:32:37.512254Z","shell.execute_reply.started":"2022-02-22T03:32:37.240703Z","shell.execute_reply":"2022-02-22T03:32:37.511424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"days = range(1, 1913 + 1)\ntime_series_columns = [f'd_{i}' for i in days]\ntime_series_data = train_sales[time_series_columns]  #Get time series data\n#train_salesから売り上げデータのみを抽出","metadata":{"execution":{"iopub.status.busy":"2022-02-22T03:32:37.513503Z","iopub.execute_input":"2022-02-22T03:32:37.513963Z","iopub.status.idle":"2022-02-22T03:32:37.660224Z","shell.execute_reply.started":"2022-02-22T03:32:37.513913Z","shell.execute_reply":"2022-02-22T03:32:37.659343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#show last 28days\nfigsize = (25, 5)\ntime_series_data.iloc[15, -28:].plot(figsize=figsize)\n#15行目の最新28日の売り上げデータを表示している．\nplt.grid()\n#The last 28 days\nprint(time_series_data.iloc[0, 1885:].shape)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T03:32:37.661576Z","iopub.execute_input":"2022-02-22T03:32:37.661901Z","iopub.status.idle":"2022-02-22T03:32:37.934901Z","shell.execute_reply.started":"2022-02-22T03:32:37.661854Z","shell.execute_reply":"2022-02-22T03:32:37.934014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"商品の価格と売り上げの関係性をみる","metadata":{}},{"cell_type":"code","source":"def min_max(df):\n    return (df-df.mean())/df.std()  #scale","metadata":{"execution":{"iopub.status.busy":"2022-02-22T03:32:37.939477Z","iopub.execute_input":"2022-02-22T03:32:37.941919Z","iopub.status.idle":"2022-02-22T03:32:37.949508Z","shell.execute_reply.started":"2022-02-22T03:32:37.941852Z","shell.execute_reply":"2022-02-22T03:32:37.948564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(2,6):\n    price_sell = calendar.merge(sell_prices[sell_prices.item_id=='HOBBIES_1_00'+str(i)][sell_prices.store_id=='CA_1'], on = ['wm_yr_wk'], how = 'left')\n    #商品の絞り込みを行う\n    fig =plt.figure(figsize= (20, 5))\n    ax = fig.add_subplot(1, 1, 1)\n    ax.plot(min_max(time_series_data.iloc[i].values))\n    ax.plot(min_max(price_sell.sell_price),'-o')\n    plt.legend(['sale','price'])\n    ax.set_title(str(i))\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Sales')","metadata":{"execution":{"iopub.status.busy":"2022-02-22T03:32:37.955666Z","iopub.execute_input":"2022-02-22T03:32:37.958478Z","iopub.status.idle":"2022-02-22T03:32:45.998272Z","shell.execute_reply.started":"2022-02-22T03:32:37.958416Z","shell.execute_reply":"2022-02-22T03:32:45.997288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"商品の価格と売り上げの間には明確な関係が無いように感じる．","metadata":{}},{"cell_type":"markdown","source":"売り上げとイベントの関係性をみる．","metadata":{}},{"cell_type":"code","source":"for i in range(5,10):\n    fig =plt.figure(figsize= (20, 5))\n    ax = fig.add_subplot(1, 1, 1)\n    #ax.bar(x = range(len(transfer_cal.loc['snap_WI'][1500:1800].values)),height = transfer_cal.loc['snap_TX'][1500:1800].values,label='snap_TX',facecolor='red')\n    ax.plot(time_series_data.iloc[i, 500:800].values,label='sales')\n    #5番目から10番目までの商品について、売上とイベントの相関を示している．\n    ax.bar(x = range(300),height = event_name_1_se[500:800].values*0.05*time_series_data.iloc[i, 500:800].values.max(),label='type_1',facecolor='black',width=1.2)\n    ax.bar(x = range(300),height = event_name_2_se[500:800].values*0.05*time_series_data.iloc[i, 500:800].values.max(),label='type_2',facecolor='orange',width=1.2)\n    plt.legend(['sale','event_1','event_2'])\n    ax.set_title(str(i))\n\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Sales')","metadata":{"execution":{"iopub.status.busy":"2022-02-22T03:32:45.9996Z","iopub.execute_input":"2022-02-22T03:32:45.999939Z","iopub.status.idle":"2022-02-22T03:32:55.343088Z","shell.execute_reply.started":"2022-02-22T03:32:45.999887Z","shell.execute_reply":"2022-02-22T03:32:55.339342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"単体の商品にまで絞り込んでしまうと相関性が見辛い","metadata":{}},{"cell_type":"markdown","source":"それぞれのカテゴリー配下の商品の売上を比べてみる．\nそれぞれのカテゴリー配下の売上の分散をみる必要がある．","metadata":{}},{"cell_type":"code","source":"for i in train_sales.cat_id.unique():\n    fig =plt.figure(figsize= (20, 5))\n    for j in range(10):\n        ax = fig.add_subplot(1, 1, 1)\n        ax.plot(train_sales[train_sales.cat_id==i].iloc[j, :][time_series_columns].values)\n        #売上データからカテゴリーidで絞り込む\n        #絞り込まれた商品データの上から10個の商品の値段の移り変わりを\n        ax.set_title(str(i))\n        ax.set_xlabel('Time')\n        ax.set_ylabel('Sales')","metadata":{"execution":{"iopub.status.busy":"2022-02-22T03:32:55.347287Z","iopub.execute_input":"2022-02-22T03:32:55.349445Z","iopub.status.idle":"2022-02-22T03:32:58.553522Z","shell.execute_reply.started":"2022-02-22T03:32:55.349389Z","shell.execute_reply":"2022-02-22T03:32:58.552784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"同じカテゴリーの商品でも売上の変化が激しいものとどうではいものが存在するように見える．\nしかし、これをそれぞれの商品のmin-maxで[1, 0]にすると似た形なる可能性もある．","metadata":{}},{"cell_type":"code","source":"for i in train_sales.dept_id.unique():\n    fig =plt.figure(figsize= (20, 5))\n    for j in range(10):\n        ax = fig.add_subplot(1, 1, 1)\n        ax.plot(train_sales[train_sales.dept_id==i].iloc[j, :][time_series_columns].values)\n        ax.set_title(str(i))\n        ax.set_xlabel('Time')\n        ax.set_ylabel('Sales')\n#商品のカテゴリーをもう少し詳しく分けた部門について、上のカテゴリーと同じように売上の変化グラフをだす．\n#部門ごとにみることによって、カテゴリー毎の売上のばらつきがどの部門の商品に起因したものであったのかがわ","metadata":{"execution":{"iopub.status.busy":"2022-02-22T03:32:58.554997Z","iopub.execute_input":"2022-02-22T03:32:58.555427Z","iopub.status.idle":"2022-02-22T03:33:03.950061Z","shell.execute_reply.started":"2022-02-22T03:32:58.555375Z","shell.execute_reply":"2022-02-22T03:33:03.947336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in train_sales.store_id.unique():\n    fig =plt.figure(figsize= (20, 5))\n    for j in range(10):\n        ax = fig.add_subplot(1, 1, 1)\n        ax.plot(train_sales[train_sales.store_id==i].iloc[j, :][time_series_columns].values)\n        ax.set_title(str(i))\n        ax.set_xlabel('Time')\n        ax.set_ylabel('Sales')\n#店毎に売上の変化を見ている．\n","metadata":{"execution":{"iopub.status.busy":"2022-02-22T03:33:03.951541Z","iopub.execute_input":"2022-02-22T03:33:03.951987Z","iopub.status.idle":"2022-02-22T03:33:10.41156Z","shell.execute_reply.started":"2022-02-22T03:33:03.951815Z","shell.execute_reply":"2022-02-22T03:33:10.410747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"これらをみることによって、dept_idによって売上数は大きく変化することがわかった．","metadata":{}},{"cell_type":"markdown","source":"## LSTM modelの作成\n- lstmとdense層で構成します\n- inputに使うのは二つの特徴量のみで、日数は56日間のみにします．","metadata":{}},{"cell_type":"code","source":"\nX = []   #build a data with two features(salse and event1)\n# 売上データとevent1のデータのみを格納する．\nfor i in tqdm(range(time_series_data.shape[0])):#30490列について、プログレスバー付きの繰り返しを行う\n    X.append([list(t) for t in zip(transfer_cal.loc['event_name_1'][-(100+28):-(28)],\n                                   transfer_cal.loc['event_type_1'][-(100+28):-(28)],\n                                   transfer_cal.loc['event_name_2'][-(100+28):-(28)],     #emmmm.....Those features didn't work for me...\n                                   transfer_cal.loc['event_type_2'][-(100+28):-(28)],\n                                   transfer_cal.loc['snap_CA'][-(100+28):-(28)],\n                                   transfer_cal.loc['snap_TX'][-(100+28):-(28)],\n                                   transfer_cal.loc['snap_WI'][-(100+28):-(28)],\n                                   price_df.iloc[i][-(100+28):-(28)],\n                                   time_series_data.iloc[i][-100:])]) \n    #for t in zip(A, B, C, ...)によって、A, B, C, ...の要素それぞれの抽出を順番に行なっている\n    #transfer colはtrainをlabel encodingしたもの\n    #price dfは商品毎の値段の移り変わりを表したもの\n    #time_series_dataはその商品の売上を表したもの\n    #行列の形は次のようになっている．\n    #まず、商品毎の軸が存在する\n    #商品毎に次のデータが入れられる\n    #日付毎に行列が用意される\n    #その日付のカレンダー情報、その日の価格、売上が入れられる．\n    \n    \nX = np.asarray(X, dtype=np.float32)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T03:33:10.414609Z","iopub.execute_input":"2022-02-22T03:33:10.414863Z","iopub.status.idle":"2022-02-22T03:34:53.441833Z","shell.execute_reply.started":"2022-02-22T03:33:10.414817Z","shell.execute_reply":"2022-02-22T03:34:53.44094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transfer_cal.loc['event_name_1'][-(100+28):-(28)]","metadata":{"execution":{"iopub.status.busy":"2022-02-22T03:34:53.443788Z","iopub.execute_input":"2022-02-22T03:34:53.444133Z","iopub.status.idle":"2022-02-22T03:34:53.452341Z","shell.execute_reply.started":"2022-02-22T03:34:53.444066Z","shell.execute_reply":"2022-02-22T03:34:53.451325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transfer_cal.head(50)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T03:34:53.453719Z","iopub.execute_input":"2022-02-22T03:34:53.454207Z","iopub.status.idle":"2022-02-22T03:34:53.492493Z","shell.execute_reply.started":"2022-02-22T03:34:53.454001Z","shell.execute_reply":"2022-02-22T03:34:53.49173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"データが大きいため、正規化をする必要があります．\nlow, highという変数は、train dataを普通のサイズに戻す時の為に保持しています．","metadata":{}},{"cell_type":"code","source":"\ndef Normalize(list):\n    list = np.array(list)\n    #配列の状態から行列に変換\n    low, high = np.percentile(list, [0, 97.5])\n    delta = high - low\n    if delta != 0:\n        for i in range(0, len(list)):\n            list[i] = (list[i]-low)/delta\n    return  list,low,high\n#[0 1]に変換\n\ndef FNoramlize(list,low,high):\n    delta = high - low\n    if delta != 0:\n        for i in range(0, len(list)):\n            list[i] = list[i]*delta + low\n    return list\n#normalizeされた値を元に戻す\n\ndef Normalize2(list,low,high):\n    list = np.array(list)\n    delta = high - low\n    if delta != 0:\n        for i in range(0, len(list)):\n            list[i] = (list[i]-low)/delta\n    return  list\n","metadata":{"execution":{"iopub.status.busy":"2022-02-22T03:34:53.493813Z","iopub.execute_input":"2022-02-22T03:34:53.494232Z","iopub.status.idle":"2022-02-22T03:34:53.504386Z","shell.execute_reply.started":"2022-02-22T03:34:53.494059Z","shell.execute_reply":"2022-02-22T03:34:53.503638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"![](https://ae01.alicdn.com/kf/H0d1d47c633fb4964804a53d6ad248967T.jpg)\ntimedistributed layerはone to manyもしくはmany to manyの構造を加えます．これによって、modelの次元を増やします．","metadata":{}},{"cell_type":"markdown","source":"# timedistributed layer\n入力されたシーケンスの各時刻に同様のネットワーク構造を付加できるラッパー [公式document](https://keras.io/ja/layers/wrappers/)  \nこのラッパーによって、入力の全ての時間スライスにレイヤーを適用できる．  \n入力は少なくとも三次元である必要があり、indexの次元は時間次元と見なされる．\n\n公式document上では、32個のサンプルを持つバッチを考えている．各サンプルでは16次元で構成される10個のベクトルをもつ．  \nこのバッチの入力のshapeは(32, 10, 16)となる．  \nこの時10個のタイムスタンプのレイヤーそれぞれにdenseを適用するためにTimeDistributedを利用できる．\n\n今回の例では30490個のサンプルを持つ．各サンプルは9次元で構成される56個のベクトルを持っている．  \n今回の場合のshapeは(30490, 56, 9)となる．  \nよって、56のタイムスタンプレイヤーそれぞれにdenseを適用するためにtimeDistriibutedを利用することができる．\n\n","metadata":{}},{"cell_type":"markdown","source":"## repeat vector\nn回入力を繰り返します．\n\n*    That is, one output for each LSTM at each input sequence time step rather than one output for each LSTM for the whole input sequence.\n\n*    An output for each step of the input sequence gives the decoder access to the intermediate representation of the input sequence each step. This may or may not be useful. Providing the final LSTM output at the end of the input sequence may be more logical as it captures information about the entire input sequence, ready to map to or calculate an output.\n","metadata":{}},{"cell_type":"code","source":"np.random.seed(7)\n\n ## 最新の56日間を訓練データとする\nif __name__ == '__main__':\n    n_steps = 28\n    train_n,train_low,train_high = Normalize(X[:,-(n_steps*2):,:])\n    #Xのデータのうち、56日間のみのデータを抽出、それをNormalizeする．\n    X_train = train_n[:,-28*2:-28,:]\n    #56日前から28日前までを選択\n    y = train_n[:,-28:,8]  #ここには全種類の商品の28日間の売上の情報のみ抽出\n    # reshape from [samples, timesteps] into [samples, timesteps, features]\n    n_features = 9\n    n_out_seq_length =28\n    num_y = 1\n    X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], n_features))\n    y = y.reshape((y.shape[0], y.shape[1], 1))\n    print(X_train.shape)\n    # define model\n\n    model = Sequential()\n\n    \n    model.add(LSTM(128, activation='relu', input_shape=(28, n_features),return_sequences=False))\n    model.add(RepeatVector(n_out_seq_length))\n    model.add(LSTM(32, activation='relu',return_sequences=True))\n    model.add(Dropout(0.1))  \n    model.add(TimeDistributed(Dense(num_y)))   # num_y means the shape of y,in some problem(like translate), it can be many.\n                                                #In that case, you should set the  activation= 'softmax'\n    model.compile(optimizer='adam', loss='mse')\n    # demonstrate prediction\n    model.fit(X_train, y, epochs=10, batch_size=1000)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T03:41:07.740715Z","iopub.execute_input":"2022-02-22T03:41:07.741048Z","iopub.status.idle":"2022-02-22T03:41:29.49922Z","shell.execute_reply.started":"2022-02-22T03:41:07.740994Z","shell.execute_reply":"2022-02-22T03:41:29.498513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## submission.csvの作成","metadata":{}},{"cell_type":"code","source":"num = 30490 # number of traindata\nx_input = array(X_train[:,-n_steps*1:])\nx_input = x_input.reshape((num, n_steps*1, n_features))\n#x_trainはもともと未来28日分までのデータを持っている．それをinputデータとした\nprint(x_input.shape)\n#x_input = Normalize2(x_input,train_low,train_high)\nyhat = model.predict(x_input[:,-n_steps:], verbose=0)\nx_input=np.concatenate((x_input[:,:,8].reshape(x_input.shape[0],x_input.shape[1]),yhat.astype(np.float32).reshape(x_input.shape[0],x_input.shape[1])),axis=1).reshape((x_input.shape[0],x_input.shape[1]+28,1))\n#print(yhat)\nprint(x_input.shape)\nx_input = FNoramlize(x_input,train_low,train_high)\nx_input = np.rint(x_input)\n#整数にしている\nforecast = pd.DataFrame(x_input.reshape(x_input.shape[0],x_input.shape[1])).iloc[:,-28:]\nforecast.columns = [f'F{i}' for i in range(1, forecast.shape[1] + 1)]\nforecast[forecast < 0] =0\nforecast.head()\nvalidation_ids = train_sales['id'].values\nevaluation_ids = [i.replace('validation', 'evaluation') for i in validation_ids]\nids = np.concatenate([validation_ids, evaluation_ids])\npredictions = pd.DataFrame(ids, columns=['id'])\nforecast = pd.concat([forecast]*2).reset_index(drop=True)\npredictions = pd.concat([predictions, forecast], axis=1)\npredictions.to_csv('submission.csv', index=False)  #Generate the csv file.","metadata":{"execution":{"iopub.status.busy":"2022-02-22T03:41:29.50104Z","iopub.execute_input":"2022-02-22T03:41:29.501322Z","iopub.status.idle":"2022-02-22T03:41:40.347908Z","shell.execute_reply.started":"2022-02-22T03:41:29.501275Z","shell.execute_reply":"2022-02-22T03:41:40.347093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-22T03:41:40.349761Z","iopub.execute_input":"2022-02-22T03:41:40.350218Z","iopub.status.idle":"2022-02-22T03:41:40.383629Z","shell.execute_reply.started":"2022-02-22T03:41:40.350169Z","shell.execute_reply":"2022-02-22T03:41:40.382858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stv.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-22T03:35:26.007469Z","iopub.execute_input":"2022-02-22T03:35:26.007907Z","iopub.status.idle":"2022-02-22T03:35:26.041906Z","shell.execute_reply.started":"2022-02-22T03:35:26.007712Z","shell.execute_reply":"2022-02-22T03:35:26.041102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}