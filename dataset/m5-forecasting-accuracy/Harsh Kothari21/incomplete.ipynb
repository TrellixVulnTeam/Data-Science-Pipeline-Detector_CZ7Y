{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nimport warnings\nfrom itertools import cycle\n\nwarnings.filterwarnings('ignore')\nplt.style.use('fivethirtyeight')\npd.set_option('max_columns', 50)\ncolor_cycle = cycle(plt.rcParams['axes.prop_cycle'].by_key()['color'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read in the data\nINPUT_DIR = '../input/m5-forecasting-accuracy'\ncal = pd.read_csv(f'{INPUT_DIR}/calendar.csv')\nstv = pd.read_csv(f'{INPUT_DIR}/sales_train_validation.csv')\nss = pd.read_csv(f'{INPUT_DIR}/sample_submission.csv')\nsellp = pd.read_csv(f'{INPUT_DIR}/sell_prices.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# What exactly are we trying to predict?\nWe are trying for forecast sales for 28 forecast days. The sample submission has the following format:\n\nThe columns represent 28 forecast days. We will fill these forecast days with our predictions.\nThe rows each represent a specific item. This id tells us the item type, state, and store. We don't know what these items are exactly.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ss.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ss.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are given historic sales data in the `sales_train_validation` dataset.\n\n- rows exist in this dataset for days d_1 to d_1913. We are given the department, category, state, and store id of the item.\n- d_1914 - d_1941 represents the `validation` rows which we will predict in stage 1\n- d_1942 - d_1969 represents the `evaluation` rows which we will predict for the final competition standings.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"stv.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stv.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualizing the data for a single item\n- Lets take a random item that sell a lot and see how it's sales look across the training data.\n- FOODS_3_090_CA_3_validation sells a lot\n- Note there are days where it appears the item is unavailable and sales flatline","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"stv.sum(axis=1, skipna=True).sort_values(ascending=False).head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stv.iloc[8412].id","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This proves `FOODS_3_090_CA_3_validation` sells a lot","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#date columns from the dataframe\nd_cols = [d for d in stv.columns if 'd_' in d]\n\nstv.loc[stv['id'] == 'FOODS_3_090_CA_3_validation'] \\\n    .set_index('id')[d_cols] \\\n    .T \\\n    .plot(figsize=(15,5))\nplt.legend('')\nplt.title('FOODS_3_090_CA_3 sales by \"d\" number');\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Note there are days where it appears the item is unavailable and sales flatline","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Merging the data with real dates\nWe are given a calendar with additional information about past and future dates.\nThe calendar data can be merged with our days data\nFrom this we can find weekly and annual trends","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cal.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Merge calendar on our items' data\n\n#setting up item's data\nexample = stv.loc[stv['id'] == 'FOODS_3_090_CA_3_validation'][d_cols].T\nexample = example.rename(columns={8412:'FOODS_3_090_CA_3'}) # Name it correctly\nexample = example.reset_index().rename(columns={'index': 'd'}) # make the index \"d\"\n\n#merge operation\nexample = example.merge(cal, how='left', validate='1:1')\nexample.set_index('date')['FOODS_3_090_CA_3'] \\\n    .plot(figsize=(15, 5),\n          title='FOODS_3_090_CA_3 sales by actual sale dates', color='darkred')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"One example doen't seem enough!, let's take some other examples","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Select more top selling examples\nexample2 = stv.loc[stv['id'] == 'HOBBIES_1_234_CA_3_validation'][d_cols].T\nexample2 = example2.rename(columns={6324:'HOBBIES_1_234_CA_3'}) # Name it correctly\nexample2 = example2.reset_index().rename(columns={'index': 'd'}) # make the index \"d\"\nexample2 = example2.merge(cal, how='left', validate='1:1')\n\nexample3 = stv.loc[stv['id'] == 'HOUSEHOLD_1_118_CA_3_validation'][d_cols].T\nexample3 = example3.rename(columns={6776:'HOUSEHOLD_1_118_CA_3'}) # Name it correctly\nexample3 = example3.reset_index().rename(columns={'index': 'd'}) # make the index \"d\"\nexample3 = example3.merge(cal, how='left', validate='1:1')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Sales broken down by time variables\nNow that we have our example item lets see how it sells by:\n- Day of the week\n- Month\n- Year","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"example.groupby('wday').mean()['FOODS_3_090_CA_3']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's plot these values to graph to get better understanding.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"examples = ['FOODS_3_090_CA_3','HOBBIES_1_234_CA_3','HOUSEHOLD_1_118_CA_3']\nexample_df = [example, example2, example3]\n\nfor i in [0, 1, 2]:\n    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 3))\n    example_df[i].groupby('wday').mean()[examples[i]] \\\n        .plot(kind='line',\n              title='average sale: day of week',\n              color=next(color_cycle),\n              lw=5,\n              ax=ax1)\n    example_df[i].groupby('month').mean()[examples[i]] \\\n        .plot(kind='line',\n              title='average sale: month',\n              lw=5,\n              color=next(color_cycle),\n              ax=ax2)\n    example_df[i].groupby('year').mean()[examples[i]] \\\n        .plot(kind='line',\n              lw=5,\n              color=next(color_cycle),\n              title='average sale: year',\n              ax=ax3)\n    fig.suptitle(f'Trends for item: {examples[i]}',\n                 size=20,\n                 y=1.1)\n    plt.tight_layout()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Lets look at a lot of different items!\n- Lets put it all together to plot 20 different items and their sales\n- Some observations from these plots:\n    - It is common to see an item unavailable for a period of time.\n    - Some items only sell 1 or less in a day, making it very hard to predict.\n    - Other items show spikes in their demand (super bowl sunday?) possibly the \"events\" provided to us could help with these.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create examples and storing on  dataframe\n\ntwenty_examples = stv.sample(20, random_state=529) \\\n        .set_index('id')[d_cols] \\\n    .T \\\n    .merge(cal.set_index('d')['date'],\n           left_index=True,\n           right_index=True,\n            validate='1:1') \\\n    .set_index('date')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.style.use('seaborn')\n#plot the twenty_examples\n\nfig, axs = plt.subplots(10, 2, figsize=(15, 20))\naxs = axs.flatten()\nax_idx = 0\nfor item in twenty_examples.columns:\n    twenty_examples[item].plot(title=item,\n                              color=next(color_cycle),\n                              ax=axs[ax_idx])\n    ax_idx += 1\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Combined Sales over Time by Type\n- We have several item types:\n    - Hobbies\n    - Household\n    - Foods\n- Lets plot the total demand over time for each type","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"stv['cat_id'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.style.use('fivethirtyeight')\nstv.groupby('cat_id').count()['id'] \\\n    .sort_values() \\\n    .plot(kind='barh', figsize=(15, 5), title='Count of Items by Category')\nplt.ylabel('')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"past_sales = stv.set_index('id')[d_cols] \\\n    .T \\\n    .merge(cal.set_index('d')['date'],\n           left_index=True,\n           right_index=True,\n            validate='1:1') \\\n    .set_index('date')\n\n\nfor i in stv['cat_id'].unique():\n    items_col = [c for c in past_sales.columns if i in c]\n    past_sales[items_col] \\\n        .sum(axis=1) \\\n        .plot(figsize=(15, 5),\n              alpha=0.8,\n              title='Total Sales by Item Type')\nplt.legend(stv['cat_id'].unique())\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Rollout of items being sold.\n- We can see the some items come into supply that previously didn't exist. Similarly some items stop being sold completely.\n- Lets plot the sales, but only count if item is selling or not selling (0 -> not selling, >0 -> selling)\n- This plot shows us that many items are being slowly introduced into inventory, so many of them will not register a sale at the beginning of the provided data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"past_sales_clipped = past_sales.clip(0, 1)\nfor i in stv['cat_id'].unique():\n    items_col = [c for c in past_sales.columns if i in c]\n    (past_sales_clipped[items_col] \\\n        .mean(axis=1) * 100) \\\n        .plot(figsize=(15, 5),\n              alpha=0.8,\n              title='Inventory Sale Percentage by Date',\n              style='.')\nplt.ylabel('% of Inventory with at least 1 sale')\nplt.legend(stv['cat_id'].unique(), fontsize=13)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It appears that walmarts are closed on Chirstmas day. The highest demand day of all the data was on Sunday March 6th, 2016. What happened on this day you may ask... well [the Seventh Democratic presidential candidates debate hosted by CNN and held in Flint, Michigan](https://www.onthisday.com/date/2016/march/6)... I doubt that impacted sales though :D","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('The lowest sale date was:', past_sales.sum(axis=1).sort_values().index[0],\n     'with', past_sales.sum(axis=1).sort_values().values[0], 'sales')\nprint('The highest sale date was:', past_sales.sum(axis=1).sort_values(ascending=False).index[0],\n     'with', past_sales.sum(axis=1).sort_values(ascending=False).values[0], 'sales')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"past_sales.sum(axis=1).sort_values().plot()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Sales Heatmap Calendar","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Polygon\nfrom datetime import datetime\nfrom dateutil.relativedelta import relativedelta\n\n\ndef calmap(ax, year, data):\n    ax.tick_params('x', length=0, labelsize=\"medium\", which='major')\n    ax.tick_params('y', length=0, labelsize=\"x-small\", which='major')\n\n    # Month borders\n    xticks, labels = [], []\n    start = datetime(year,1,1).weekday()\n    for month in range(1,13):\n        first = datetime(year, month, 1)\n        last = first + relativedelta(months=1, days=-1)\n\n        y0 = first.weekday()\n        y1 = last.weekday()\n        x0 = (int(first.strftime(\"%j\"))+start-1)//7\n        x1 = (int(last.strftime(\"%j\"))+start-1)//7\n\n        P = [ (x0,   y0), (x0,    7),  (x1,   7),\n              (x1,   y1+1), (x1+1,  y1+1), (x1+1, 0),\n              (x0+1,  0), (x0+1,  y0) ]\n        xticks.append(x0 +(x1-x0+1)/2)\n        labels.append(first.strftime(\"%b\"))\n        poly = Polygon(P, edgecolor=\"black\", facecolor=\"None\",\n                       linewidth=1, zorder=20, clip_on=False)\n        ax.add_artist(poly)\n    \n    ax.set_xticks(xticks)\n    ax.set_xticklabels(labels)\n    ax.set_yticks(0.5 + np.arange(7))\n    ax.set_yticklabels([\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"])\n    ax.set_title(\"{}\".format(year), weight=\"semibold\")\n    \n    # Clearing first and last day from the data\n    valid = datetime(year, 1, 1).weekday()\n    data[:valid,0] = np.nan\n    valid = datetime(year, 12, 31).weekday()\n    # data[:,x1+1:] = np.nan\n    data[valid+1:,x1] = np.nan\n    \n    ax.imshow(data, extent=[0,53,0,7], zorder=10, vmin=-1, vmax=1,\n              cmap=\"RdYlBu_r\", origin=\"lower\", alpha=.75)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nsscale = StandardScaler()\npast_sales.index = pd.to_datetime(past_sales.index)\nfor i in stv['cat_id'].unique():\n    fig, axes = plt.subplots(3, 1, figsize=(20, 8))\n    items_col = [c for c in past_sales.columns if i in c]\n    sales2013 = past_sales.loc[past_sales.index.isin(pd.date_range('31-Dec-2012',\n                                                                   periods=371))][items_col].mean(axis=1)\n    vals = np.hstack(sscale.fit_transform(sales2013.values.reshape(-1, 1)))\n    calmap(axes[0], 2013, vals.reshape(53,7).T)\n    sales2014 = past_sales.loc[past_sales.index.isin(pd.date_range('30-Dec-2013',\n                                                                   periods=371))][items_col].mean(axis=1)\n    vals = np.hstack(sscale.fit_transform(sales2014.values.reshape(-1, 1)))\n    calmap(axes[1], 2014, vals.reshape(53,7).T)\n    sales2015 = past_sales.loc[past_sales.index.isin(pd.date_range('29-Dec-2014',\n                                                                   periods=371))][items_col].mean(axis=1)\n    vals = np.hstack(sscale.fit_transform(sales2015.values.reshape(-1, 1)))\n    calmap(axes[2], 2015, vals.reshape(53,7).T)\n    plt.suptitle(i, fontsize=30, x=0.4, y=1.01)\n    plt.tight_layout()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some interesting things to note from these heatmaps:\n\n- Food tends to have lower number of purchases as the month goes on. Could this be because people get their paychecks early in the month?\n- Household and Hobby items sell much less in January - after the Holiday season is over.\n- Cleary weekends are more popular shopping days regardless of the item category.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# A simple submission\n- Submit the average value from the past 30 days","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"thirty_day_avg_map = stv.set_index('id')[d_cols[-30:]].mean(axis=1).to_dict()\nfcols = [f for f in ss.columns if 'F' in f]\nfor f in fcols:\n    ss[f] = ss['id'].map(thirty_day_avg_map).fillna(0)\n    \nss.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}