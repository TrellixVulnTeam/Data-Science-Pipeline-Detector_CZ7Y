{"cells":[{"metadata":{},"cell_type":"markdown","source":"### *Please help by upvoting this kernel to keep motivate me. *","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Exploratory Data Analysis\n\nFeature Enginering is a crucial part in machine learning.\nBefore start predicting we need to analyze the trend and select features based on the observations.\nFor that purpose we will start with Exploratory data analysis.","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')\nplt.rcParams['figure.figsize'] = (20,5)  #defualt figure size\n\ninput_path = \"../input/m5-forecasting-accuracy/\"\n\nsell_prices = pd.read_csv(input_path+'sell_prices.csv')\ncalendar = pd.read_csv(input_path+'calendar.csv')\nsales = pd.read_csv(input_path+'sales_train_validation.csv')\nsample_output = pd.read_csv(input_path+'sample_submission.csv')\n#print(sell_prices.head())\n#print(calendar.head())\n#print(sales.head())\n#print(sample_output.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Data discription we can find in the data section of the problem statement, \nSo, we are not analyzing data again. As per the data discription only event related feilds having missing data.\nTo start with, we will use item catogery visulizations. That will be an effective representaton of the whole dataset.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Sales Catogery\nsales.groupby('cat_id').count()['id'].sort_values().plot(kind='barh',figsize=(15,2), title='Sales By Catogory',width=0.5)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Sales By Department\nsales.groupby('dept_id').count()['id'].sort_values().plot(kind='barh',figsize=(15,3), title='Sales By Department')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Sales By State\nsales.groupby('state_id').count()['id'].sort_values().plot(kind='barh',figsize=(15,2), title='Sales By State')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"date_columns = [c for c in sales.columns if 'd_' in c] # select date columns\ngouped_by_cat_totals = sales.groupby(['cat_id']).sum().T  #get sum and trasnpose\n#print(gouped_by_cat_totals.columns)\ngouped_by_cat_totals.plot(figsize=(20,5),title=\"Total Sales By Catogory\")\ncal_columns = ['d','month']\nmonthPosition = np.arange(3,2000,30) #Roughly\nfor xc in monthPosition:\n    plt.axvline(x=xc, color='k', linestyle='--')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Merge the Calandar data with the sales data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cal_columns = ['date','d','month','year','wday','event_type_1','event_type_2']\ncalendar_selected = calendar[cal_columns].set_index('d')\ntotal_sales_OverCalendar = pd.concat([calendar_selected,gouped_by_cat_totals],axis=1,sort=False)\nprint(total_sales_OverCalendar['event_type_1'].unique())\nprint(total_sales_OverCalendar['event_type_2'].unique())\ntotal_sales_OverCalendar.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total_sales_OverCalendar['dayOfYear'] = total_sales_OverCalendar['date'].str.slice(5,10)\ndef plot_pivoted_year(data,name,num):\n    plt.figure(figsize=(20,10))\n    plt.subplot(3,1,num)\n    plt.title(name+\" Sales Over ther Year\")\n    pivoted = data.pivot_table(index='dayOfYear',columns='year',values=name)\n    plt.grid()\n    plt.plot(pivoted)\n    plt.legend()\n    plt.show()\n    \nplot_pivoted_year(total_sales_OverCalendar,'FOODS',1)\nplot_pivoted_year(total_sales_OverCalendar,'HOBBIES',2)\nplot_pivoted_year(total_sales_OverCalendar,'HOUSEHOLD',3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Yearly growth\ngouped_yearly = total_sales_OverCalendar.groupby('year')['FOODS','HOBBIES','HOUSEHOLD'].mean().T\ngouped_yearly.plot(kind='bar',title='Total Average Sales by year',figsize=(10,5))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can clearly see yearly similar trend on each catogory. We can see total sales growth in each year.\n2016 we have only half of the years data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Monthly trend analysis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"total_sales_OverCalendar['dayOfMonth'] = total_sales_OverCalendar['date'].str.slice(8,10)\ndef plot_pivoted_month(data,name,num):\n    plt.figure(figsize=(20,10))\n    plt.subplot(3,1,num)\n    plt.title(name+\" Sales Over Month\")\n    pivoted = data.pivot_table(index='dayOfMonth',columns='month',values=name)\n    plt.grid()\n    plt.plot(pivoted)\n    plt.legend()\n    plt.show()\n    \nplot_pivoted_month(total_sales_OverCalendar,'FOODS',1)\nplot_pivoted_month(total_sales_OverCalendar,'HOBBIES',2)\nplot_pivoted_month(total_sales_OverCalendar,'HOUSEHOLD',3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Monthly growth\ngouped_monthly = total_sales_OverCalendar.groupby('month')['FOODS','HOBBIES','HOUSEHOLD'].mean().T\ngouped_monthly.plot(kind='bar',title='Total Average Sales by Month',figsize=(10,5))\nplt.legend(loc='best')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Events more like to be annual. We will analyze event effect on Sales on following section ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def plotSalesAndEvents(data,eventData,col_name):\n    eventData.plot(kind='bar',figsize=(20,5),title='Event count vs Sales('+col_name+')',stacked=True)\n    data[col_name].plot(secondary_y=True,figsize=(20,5),linewidth=4)\n    plt.grid()\n    plt.show()\n    \ndef getEventData(data,eventType):\n    eventData = data[data[eventType].notnull()].pivot_table(index='month',columns=eventType,values='wday',aggfunc=len)\n    eventData = eventData.fillna(0)\n    eventData = eventData.reset_index('month')\n    eventData = eventData.set_index('month')\n    return eventData\n    \n#Chose complete year data. 2011 and 2016 we don;t have whole year data.\ncomplete_year_data = total_sales_OverCalendar[ (total_sales_OverCalendar['year']>2011) & (total_sales_OverCalendar['year']<2016)]\ngouped_monthly = complete_year_data.groupby('month')['FOODS','HOBBIES','HOUSEHOLD'].mean()\ngouped_monthly = gouped_monthly.reset_index('month')\ndata_2012 = total_sales_OverCalendar[total_sales_OverCalendar['year']==2012]\neventData = getEventData(data_2012,'event_type_1')\nplotSalesAndEvents(gouped_monthly,eventData,'FOODS')\nplotSalesAndEvents(gouped_monthly,eventData,'HOBBIES')\nplotSalesAndEvents(gouped_monthly,eventData,'HOUSEHOLD')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Events can affect sales. Above graphs we can see less number of event seasons people tend to buy more FOOD and household items. People tend to buy HOBIIES catogry when their are more events.\n\nWe can clearly see that month 6 having highest sales on HOBIIES catogory where we have highest number of Sporting events for the same month.\nMonth 8 having highes FOOD and HOUSEHOLD sales where lowest events occur for the same month","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Time Series Data Analysis\n\nTo get an idea of the modeling, we will try to pridict result using Time series.\nHere, I'm tring to predict total sales for food catogory. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_timeseries_stat(timeseries):\n    rollingMean = timeseries.rolling(window=30,center=False).mean()\n    rollingStd = timeseries.rolling(window=30,center=False).std()\n    plt.figure(figsize=(20,8))\n    ori = plt.plot(timeseries,color='blue',label='Original')\n    mean = plt.plot(rollingMean,color='red',label='Rolling Mean')\n    std = plt.plot(rollingStd,color='black',label='Rolling Std')\n    plt.legend(loc='best')\n    plt.show(block=False)\n\nplot_timeseries_stat(total_sales_OverCalendar['FOODS'])\nplot_timeseries_stat(total_sales_OverCalendar['HOUSEHOLD'])\nplot_timeseries_stat(total_sales_OverCalendar['HOBBIES'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Modeling start with ARIMA","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Making time series stationary\nfrom statsmodels.tsa.stattools import adfuller, acf, pacf\nfrom statsmodels.tsa.arima_model import ARIMA\nimport math\n\ndef test_stationarityDF(timeseries): ##Dickey-Fuller Test\n    dftest = adfuller(timeseries,autolag='AIC')\n    dfoutput=pd.Series(dftest[0:4],index=['Test Statistic','p-value','#Lags Used','No of Observesations Used'])\n    \n    for key,value in dftest[4].items():\n        dfoutput['Critical Value (%s)'%key] = value\n    print(dfoutput)\n\nfood_Series = total_sales_OverCalendar['FOODS']\nfood_Series.fillna(food_Series.mean(),inplace=True)\ntest_stationarityDF(food_Series)\n\nmovingAverage = food_Series.rolling(window=30).mean()\nmovingSTD = food_Series.rolling(window=30).std()\nplt.figure(figsize=(20,6))\nplt.plot(food_Series)\nplt.plot(movingAverage,color='red')\nplt.plot(movingSTD,color='black')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can observe that Test Statistic is not below the 1% of the Critical value. So,Series is not stationary.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Make stationarry\nfoodSeriesDiff = food_Series-movingAverage\nplt.figure(figsize=(20,5))\nplt.plot(foodSeriesDiff)\nplt.show()\npd.set_option('display.float_format', '{:.5f}'.format)\nfoodSeriesDiff.fillna(foodSeriesDiff.mean(),inplace=True)\ntest_stationarityDF(foodSeriesDiff)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can observe that Test Statistic is way less than 1% of critical value. So, we can conculde that the above Series is 99% stationary.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(np.arange(0,31,1),acf(foodSeriesDiff,nlags=30))\nplt.axhline(y=0,linestyle='--',color='gray')\nplt.axhline(y=-7.96/np.sqrt(len(foodSeriesDiff)),linestyle='--',color='gray')\nplt.axhline(y=7.96/np.sqrt(len(foodSeriesDiff)),linestyle='--',color='gray')\nplt.title('Autocorrelation Function')\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Correlation function cross upper confident value between 1 and 2. Hence chose 2 as p for ARIMA","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(np.arange(0,31,1),pacf(foodSeriesDiff,nlags=30))\nplt.axhline(y=0,linestyle='--',color='gray')\nplt.axhline(y=-7.96/np.sqrt(len(foodSeriesDiff)),linestyle='--',color='gray')\nplt.axhline(y=7.96/np.sqrt(len(foodSeriesDiff)),linestyle='--',color='gray')\nplt.title('Partial Autocorrelation Function')\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Partial Autocorrelation function drop to 0 when value is between 1 and 2. choose 2 as q value ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model = ARIMA(foodSeriesDiff,order=(2,2,0))\nresults_ARIMA = model.fit(disp=-1)\nplt.figure(figsize=(20,5))\nplt.plot(foodSeriesDiff)\nplt.plot(results_ARIMA.fittedvalues,color='red')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results_ARIMA.plot_predict('d_1800','d_1900',dynamic=False,ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plot a portion of data to clear visulization\nplt.figure(figsize=(20,5))\nori = plt.plot(foodSeriesDiff.iloc[30:90],label='Original Diff')\n#plt.plot(results_ARIMA.fittedvalues.iloc[0:60],color='red')\n#plt.plot(results_ARIMA.fittedvalues.iloc[0:60].cumsum(),color='black')\n##ARIMA order is 2. show results lags 2 values\n#shfited['predicShfited2'] = pd.Series(results_ARIMA.fittedvalues,copy=True)\nshfited = pd.DataFrame({'predicShfited2':pd.Series(results_ARIMA.fittedvalues,copy=True),'day':foodSeriesDiff.index[0:1967]})\nshfited = shfited.set_index('day')\npre = plt.plot(shfited['predicShfited2'].iloc[30:90],color='green',label='Predicted Diff')\n#pre = plt.plot(results_ARIMA.fittedvalues.iloc[0:60],color='green',label='Predicted Diff')\nplt.legend(loc='best')\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Prediction","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions_ARIMA_final = pd.Series(food_Series.at['d_2'],index=food_Series.index)\nshfited.loc['d_1'] = 0\nshfited.loc['d_2'] = 0\nmovingAverage.fillna(0)\npredictVsActual = pd.DataFrame({'actual':food_Series,'diffMean':foodSeriesDiff,\n                                'predictDiffOri':shfited['predicShfited2'],\n                                'predictDiff':shfited['predicShfited2'],\n                                'base':movingAverage})\npredictVsActual['predict'] = predictVsActual.loc[:,['predictDiff','base']].sum(axis=1)\npredictVsActual['error'] = predictVsActual['actual'] - predictVsActual['predict']\nplt.figure(figsize=(20,5))\nplt.plot(predictVsActual['actual'].iloc[1800:1950],label='Actual')\nplt.plot(predictVsActual['predict'].iloc[1800:1950],label='Predicted')\n#plt.plot(predictVsActual[['actual','predict']])\nplt.legend(loc='best')\nplt.title('Original vs predicted. RMSE: %4f'%np.sqrt(sum(predictVsActual['error']**2)/len(predictVsActual)))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"ARIMA model is based on time series values. But we have more features than the time shift effect.\nAs a example, events, day of week, month can be considered as few more additional features.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Feature Selection for Complex Models\n\n\nWill update this kernel on below section on upcomming days.\n\n1) Feature Selection\n\n2) Various models\n\n3) Evaluate Results","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}