{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Indholdsfortegnelse\n- [Indledning](#Indledning)\n- [Indlæsning af Data og EDA](2#Indl%C3%A6sning-af-data-og-EDA)\n- [Modeller](#Modeller)\n  - [Baseline: en simpel tilgang](#1.-Baseline:-en-simpel-tilgang)\n  - [Moving Average](#2.-Moving-Average)\n    - [SMA](#Simple-moving-average)\n    - [CMA](#Cumulative-Moving-Average)\n    - [EMA](#Exponential-Moving-Average)\n  - [Random Forest](#3.-Random-forest)\n    - [Feature engineering](#Feature-engineering)\n    - [Prediction](#Prediction)\n  - [Sarimax](#4.-Sarimax)\n    - [Trend og seasonality](#Trend-og-seasonality)\n    - [Parameteranalyse](#Parameteranalyse)\n    - [In sample prediction](#In-sample-prediction)\n    - [Out of sample forecast](#Out-of-sample-forecast)\n  - [Prophet](#5.-Prophet)\n- [Scorer og tidsforbrug](#Scorer-og-tidsforbrug)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# **Indledning**\n”M5 Forecasting – Accuracy” er en konkurrence på www.kaggle.com, hvor interesserede får stillet et datasæt for butikken Walmart’s salg over en årrække til rådighed, og ud fra dette skal forsøge at forudsige 28 dages salg. Dataen indeholder bl.a. information om salg pr. dag, vare, kategori, butik, stat mm., som deltagere kan inddrage i deres analyser og undersøgelser.\n\nPå baggrund af en indledende analyse og undersøgelse af opgaven samt data, er vores problemstilling følgende:\n\n    ”Hvordan kan vi udnytte og implementere det, som vi har lært i undervisningen, til at forudsige Walmart’s salg i 28 dage?\n\nPå grund af køretiden på nogle af modellerne, har vi i flere tilfælde valgt at kompakte hele datasættet til et gennemsnit af alt salget - velviden at vi mister en masse finesser i datasættet på den bekostning.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# **Indlæsning af data og EDA**","execution_count":null},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# initial imports\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Indlæs datoerne som datasættet er spredt ud over\nDette er for senere at kunne omdanne d_1, d_2 osv til rigtige datoer.","execution_count":null},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"calendar_df = pd.read_csv('../input/m5-forecasting-accuracy/calendar.csv', parse_dates=['date'], usecols=['date','d'])\n\n# udvælger kun de datoer som ligger i sales_train_validation\n\ncalendar_stv = calendar_df[:1913] \ncalendar_stv.info()\ndel calendar_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## indlæs datasættet: sales_train_validation","execution_count":null},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"sales_train_validation = pd.read_csv('../input/m5-forecasting-accuracy/sales_train_validation.csv', index_col='id')\nsales_train_validation.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Hvad indeholder det?\nog hvordan er data fordelt?","execution_count":null},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"plt.style.use('ggplot')\nplt.rcParams['figure.figsize'] = [10, 5]\nax = sales_train_validation.groupby(['store_id'])['cat_id'].value_counts().plot(kind='bar', title=\"observationer i datasæt fordelt på store og kategory\")\nax.set_ylabel('# observationer')\nax.set_xlabel('Store - kategory')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Der er 10 butikker, 3 vare kategorier med et ens antal varer i:\n- Foods, med 1437 varer\n- Household, med 1047 varer\n- Hobbies, med 565 varer\n\n30490 rækker i datasættet svarer overens med:\n`antal_butikker * ( antal_varer_foods + antal_varer_household + antal_varer_hobbies )`","execution_count":null},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"plt.style.use('ggplot')\nplt.rcParams['figure.figsize'] = [6, 2.5]\nax = sales_train_validation.groupby('state_id')['store_id'].nunique().plot(kind='bar', title=\"Butikker per stat\")\nax.set_ylabel('# butikker')\nax.set_xlabel('Stat')\nax.set_ylim(bottom=2)\nplt.yticks(np.arange(2,5,1))\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Der er ialt 10 butikker - staten `CA` har en butik mere end de andre to stater","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Det samlede salg per stat ","execution_count":null},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"aggregate_state_sum = sales_train_validation.groupby(by=['state_id'],axis=0).sum()\naggregate_state_sum.columns = calendar_stv['date']\nagg_state_sum_trans = aggregate_state_sum.transpose()\ndel aggregate_state_sum","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"from_year = '2015'\nplt.style.use('ggplot')\nplt.rcParams['figure.figsize'] = [20, 10]\nplt.rcParams['lines.linewidth'] = 2\nax = agg_state_sum_trans[from_year:].plot(title=\"Summeret salg per stat fra {}\".format(from_year))\nax.set_ylabel('Solgte enheder')\nplt.show()\ndel agg_state_sum_trans","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`CA` sælger flere enheder end de andre to stater. `TX` og `WI` følges nogenlunde ad.\n- Husk at staten `CA` har en butik mere end de andre!\n- Hvad sker der hvis man laver en graf med det gennemsnitlige salg?","execution_count":null},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"aggregate_state_mean = sales_train_validation.groupby(by=['state_id'],axis=0).mean()\naggregate_state_mean.columns = calendar_stv['date']\nagg_state_mean_trans = aggregate_state_mean.transpose()\ndel aggregate_state_mean\nplt.style.use('ggplot')\nplt.rcParams['figure.figsize'] = [20, 10]\nplt.rcParams['lines.linewidth'] = 2\nax = agg_state_mean_trans['2015':].plot(title=\"Gennemsnitlig salg per stat\")\nax.set_ylabel('Solgte enheder')\nplt.show()\ndel agg_state_mean_trans","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Når man kigger på salgsgennemsnittet følges de 3 stater mere ad, men `CA` er stadig umiddelbart førende.\n- Måske er stat en faktor der kan hjælpe når der skal trænes en model.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Salget per butik i en stat","execution_count":null},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"\naggregate_state_mean = sales_train_validation.groupby(by=['state_id', 'store_id'],axis=0).mean()\naggregate_state_mean.columns = calendar_stv['date']\nagg_state_mean_trans = aggregate_state_mean.transpose()\ndel aggregate_state_mean\nplt.style.use('ggplot')\nplt.rcParams['figure.figsize'] = [25, 20]\nplt.rcParams['lines.linewidth'] = 2\nfig,ax = plt.subplots(3,1)\nfor i, state in enumerate(['CA','TX','WI'], start=0):\n    ax[i].plot(agg_state_mean_trans['2015':][state])\n    ax[i].set_title(\"Gennemsnitlig salg per butik i {}\".format(state))\n    ax[i].set_ylabel('Solgte enheder')\n    i = i+1\nplt.show()\ndel agg_state_mean_trans","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I CA er der især forskel på gns salget på butikkerne med størt og mindst salg. Det kan variere med ca 2 enheder. I de andre stater følges butikkerne mere ad, men der er stadig forskelle.\n- Det tyder på at 'store_id' måske kan indeholde nogle info vi kan bruge.\n- `store_id` i sig selv er betydningsløst, men værdien kan måske indirekte sige noget om hvor stor et opland de enkelte butikker skal betjene.\n  - Hvis en butik har en stor befolkning i nærområdet, er det oplagt at de folk benytter den butik.\n  - Men ved at kigge på `store_id` i sig selv, er det ikke noget vi kan afgøre.\n- `state_id` og `store_id` er to forskellige måder at opgøre geografisk placering på. `store_id` må antages at være mere præcist at bruge end `state_id`","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Fordeling af varekategorier i de 3 stater","execution_count":null},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"aggregate_state_category = sales_train_validation.groupby(by=['state_id', 'cat_id'],axis=0).sum()\naggregate_state_category.columns = calendar_stv['date']\nagg_state_trans = aggregate_state_category.transpose()\ndel aggregate_state_category\nfig,ax = plt.subplots(3,1)\nplt.style.use('ggplot')\nplt.rcParams['figure.figsize'] = [30, 25]\nplt.rcParams['lines.linewidth'] = 2\n#ax.legend()\nax[0].plot(agg_state_trans['CA']['2015':])\nax[0].set_title('CA')\nax[0].legend(('FOODS', 'HOBBIES', 'HOUSEHOLD'), loc='upper left')\nax[1].plot(agg_state_trans['TX']['2015':])\nax[1].set_title('TX')\nax[1].legend(('FOODS', 'HOBBIES', 'HOUSEHOLD'), loc='upper left')\nax[2].plot(agg_state_trans['WI']['2015':])\nax[2].set_title('WI')\nax[2].legend(('FOODS', 'HOBBIES', 'HOUSEHOLD'), loc='upper left')\nplt.show()\ndel agg_state_trans","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Alle 3 stater følger samme møster:\n- størst salg af `Foods`\n- `Household` kommer på andenpladsen\n- `Hobbies` har mindst salg\n\nVare kategori kan have betydning for salg. Dette er dog ikke specielt overraskende, det virker logisk at der indkøbes flere varer i `FOODS`-kategorien, end i `HOUSEHOLD` og `HOBBIES`.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Salget på underkategorier ","execution_count":null},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"\ndef plot_state_category(sales_train_validation, calendar_dates, state, category='ALL', start_time='2015'):\n    sales_state_category = sales_train_validation.loc[sales_train_validation['state_id'] == state ]\n    if category != 'ALL' :\n        sales_state_category = sales_state_category.loc[sales_state_category['cat_id'] == category]\n    aggregate_ssc = sales_state_category.groupby(by=['dept_id'],axis=0).mean()\n\n    aggregate_ssc.columns = calendar_dates['date']\n\n    agg_ssc_trans = aggregate_ssc.transpose()\n    plt.style.use('ggplot')\n    plt.rcParams['figure.figsize'] = [25, 12]\n    plt.rcParams['lines.linewidth'] = 2\n    ax = agg_ssc_trans[start_time:].plot(title=\"MEANed numbers State: {}, Category: {}\".format(state, category))\n    ax.set_ylabel('Units sold')\n    plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"plot_state_category(sales_train_validation, calendar_stv, 'CA', category='FOODS', start_time='2013')","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"plot_state_category(sales_train_validation, calendar_stv, 'TX', category='FOODS',start_time='2013')","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"plot_state_category(sales_train_validation, calendar_stv, 'WI', category='FOODS', start_time='2013')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Sub kategorierne under foods ser også ud til at kunne vise nogle tendenser i antallet solgt under hver kategori.\n- Overkategorien `cat_id` virker pludselig overføldig - underkategorierne indeholde samme information og mere endnu","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 25. december","execution_count":null},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"light_sales = sales_train_validation.drop(['item_id','dept_id','cat_id','store_id'], axis=1)\nlight_sales = light_sales.groupby('state_id').mean()\nlight_sales.columns = calendar_stv['date']\nlight_s_t = light_sales.transpose()","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"plt.style.use('ggplot')\nplt.rcParams['figure.figsize'] = [15, 10]\nfig, ax = plt.subplots(2,2)\nax[0][0].plot(light_s_t['20-12-2012':'31-12-2012'])\nax[0][0].set_title('2012')\nax[0][1].plot(light_s_t['20-12-2013':'31-12-2013'])\nax[0][1].set_title('2013')\nax[1][0].plot(light_s_t['20-12-2014':'31-12-2014'])\nax[1][0].set_title('2014')\nax[1][1].plot(light_s_t['20-12-2015':'31-12-2015'])\nax[1][1].set_title('2015')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Den 25. december hvert år er der intet salg på nogen kategorier - Amerikanerne fejrer jul, butikkerne er formentlig lukkede.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Trend\nEr der en stigende eller faldende tendens i det samlede datasæt?","execution_count":null},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"sales_mean = sales_train_validation.mean()\nsales_mean.index = calendar_stv['date']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy import stats\nx= np.arange(0,len(sales_mean))\nslope, intercept, r_value, p_value, std_err = stats.linregress(x, sales_mean.values)\nplt.style.use('ggplot')\nplt.rcParams['figure.figsize'] = [10, 5]\nplt.plot(sales_mean.index, sales_mean.values, 'o', label='original data')\nplt.plot(sales_mean.index, intercept + slope*x, 'g', label='trend linje')\nplt.legend()\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Som man kan ane på den blå linje, er der en opadgående tendens på gennemsnittet af alt salget.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Ugentlige udsving","execution_count":null},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"store_dept = sales_train_validation.groupby(by= ['cat_id'], axis=0).mean()\nstore_dept.columns = calendar_stv['date']\nstore_trans = store_dept.transpose()\ndel store_dept","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"weekends = ['01-03-2015','01-04-2015','01-10-2015','01-11-2015','01-17-2015', '01-18-2015','01-24-2015', '01-25-2015', '01-31-2015', \n            '02-01-2015', '02-07-2015', '02-08-2015', '02-14-2015', '02-15-2015', '02-21-2015', '02-22-2015', '02-28-2015', \n            '03-01-2015', '03-07-2015', '03-08-2015', '03-14-2015', '03-15-2015', '03-21-2015', '03-22-2015', '03-28-2015',  '03-29-2015']","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"plt.style.use('ggplot')\nplt.rcParams['figure.figsize'] = [25, 5]\nax = store_trans['01-01-2015':'04-02-2015'].plot(title=\"Gns. salg 3 måneder jan-mar 2015\")\nax.set_ylabel('# enheder')\nax.vlines(weekends, 0, 2.5, colors=['y','c'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I ovenstående graf vises det gennemsnitlige salg på kategorierne. Hver lørdag/søndan er markeret med en vertikal streg.\n- Gennemsnitssalget stiger typisk hver weekend. ","execution_count":null},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"weekends= ['06-02-2012','06-02-2012','06-09-2012','06-10-2012', '06-16-2012','06-17-2012','06-23-2012', '06-24-2012', '06-30-2012', \n           '07-01-2012','07-07-2012','07-08-2012','07-14-2012', '07-15-2012','07-21-2012','07-22-2012', '07-28-2012', '07-29-2012', \n           '08-04-2012','08-05-2012','08-11-2012', '08-12-2012','08-18-2012','08-19-2012', '08-25-2012', '08-26-2012', '09-01-2012','09-02-2012'\n            ]\nplt.style.use('ggplot')\nplt.rcParams['figure.figsize'] = [25, 5]\nax = store_trans['06-01-2012':'09-02-2012'].plot(title=\"Gns. salg 3 måneder jun-aug 2012\")\nax.set_ylabel('# enheder')\nax.vlines(weekends, 0, 2.5, colors=['y','c'])\nplt.show()\ndel weekends","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Der er et lille opsving d. 3. juli (muligvis i forbindelse med 4. juli)\n- Men ellers ser vi generelt samme tendens, nemlig at der indkøbes mere i weekenderne","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Opsamling på EDA\n\nVed at undersøge dataet er vi kommet frem til dette:\n- Store_id er en mere finmasket version af state_id, vi vil kunne fjerne state_id fra et træningssæt da featuren er redundant\n- dept_id er en mere finmasket version af cat_id, vi vil kunne fjerne cat_id fra et træningssæt, da featuren er redundant\n- datasættet har en stigende trend\n- Der er periodevise udsving i salget, der topper i weekenderne og er lavet midt på ugen.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# **Modeller**","execution_count":null},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"#Indlæser data mm.\nsales = pd.read_csv('../input/m5-forecasting-accuracy/sales_train_validation.csv', index_col='id')\nsample = pd.read_csv('../input/m5-forecasting-accuracy/sample_submission.csv', index_col='id')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **1. Baseline: en simpel tilgang**\nNår man har læst opgaven grundigt, renset sin data og gjort sig nogle overvejelser om fremgangsmåde, vil man ofte forsøge at lave en form for baseline. En baseline er en meget simpel løsning/model, der kan bruges som udgangspunkt til mere komplekse løsninger; ikke i den forstand, at du nødvendigvis vil bygge videre på baselinen, men du kan bruge dens resultater til at sammenligne med de løsninger, du sidenhen udvikler, da disse gerne skulle give bedre resultater end baselinen.\n\nDa opgaven kort sagt går ud på at forudsige 28 dages salg for diverse artikler i Walmart, ville en meget simpel baseline være at bruge de sidste 28 dages salg som vores forudsigelse/predictions. Rent matematisk kan det opstilles således: \n\n$ŷ_t=y_{t-28}$\n\n$ŷ_t$ er den værdi, som vi forsøger at udregne/forudsige, og $y_t$ er den faktiske værdi. Det vil sige, at for hver værdi som vi forudsiger, så tager vi den faktiske værdi 28 dage forinden, ergo bruger vi de sidste 28 dages salg som vores forudsigelse.\n","execution_count":null},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"#De sidste 28 dages salg\nlastsales = sales.iloc[:,-28:]","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"#Laver et array fra \"F1\" til \"F28\" som er det påkrævede navneformat for kolonner\nf_list = []\nfor num in np.arange(1,29):\n    f_list.append(\"F{}\".format(num))\n\n#Sætter kolonnenavnene for de sidste 28 dages salg til det påkrævede navneformat\nlastsales.columns = f_list","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"#Tilføjer det tomme evalueringsdata til vores salgsdata for at opnå det ønskede format\nemptyevaluation = sample.tail(30490)\nsubmission = pd.concat([lastsales, emptyevaluation])\n\nsubmission.to_csv('baseline.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##  **2. Moving Average**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"En tilgang til data-analyse er at tage gennemsnittet af en delmængde af datasættet, også kaldet moving average. Man arbejder typisk med et ”vindue”, i dette tilfælde et vindue på et vist antal dage, hvor man så udregner gennemsnittet af værdierne inden for vinduet. Hvis man f.eks. arbejder med et vindue på syv dage, så vil den første værdi blive udregnet ud fra gennemsnittet af de syv første dage. Herefter rykker man vinduet en dag, således at man for næste dag får en gennemsnitsværdi, der nu ikke længere kigger på værdien for første dag, men fra anden dag og syv dage frem. Sådan fortsætter man gennem hele datasættet, indtil alle gennemsnitsværdier er udregnet.\n\nDet er typisk brugbart til at udjævne uregelmæssige/kortsigtede tendenser og i stedet fokusere på langsigtede tendenser, hvilket navnet ”average” også indikerer.\n\n\nDer findes forskellige former for moving averages. I denne opgave har vi besluttet os for at kigge på tre af de mest udbredte og populære former, nemlig simple moving average (SMA), cumulative moving average (CMA) og exponential moving average (EMA).\n\nDet skal noteres, at dette ikke som sådan er en model/classifier, som forudsiger værdier. Vi har dog vurderet, at dette ville være en god forlængelse af baselinen til at undersøge, om gennemsnitsværdier og forskellige vægtninger af disse giver et mere præcist bud på udvikling.\n\nVi bruger Python-bibloteket ”pandas”, som stiller udregningsmetoder for alle disse til rådighed.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Simple moving average\nVed simple moving average (SMA) har man et vindue på et fast/permanent antal dage. Hvis man f.eks. arbejder med en time series og med et vindue på syv dage, så vil den første værdi blive udregnet ud fra gennemsnittet af de syv første dage. Herefter rykker man vinduet en dag, således at man for næste dag får en gennemsnitsværdi, der nu ikke længere kigger på værdien for første dag, men fra anden dag og syv dage frem. Sådan fortsætter man gennem hele datasættet, indtil alle gennemsnitsværdier er udregnet.\n\nI pandas kan man udregne SMA ved hjælp af funktionen .rolling(), som er tilgængelig for DataFrames. Her definerer man sit vindue vha. parametret ”window”. Vi har prøvet med forskellige værdier mellem 1 og 7, og det er med netop et vindue på 1, at det mest præcise resultat opnås. Funktionen bruges sammen med .mean()-metoden til at få gennemsnittet af hvert vindue. \n\nEt vindue på 1 svarer essentielt til at værdierne for de sidste 28 dage, altså giver det samme resultat som vores baseline gør. Et løbende gennemsnit af delsættene har altså ikke være været en forbedring fra vores baseline.\n\nDet kunne muligvis have forbedret vores resultat at bruge en større del af datasættet end blot de sidste 28 dage.","execution_count":null},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"#De sidste 28 dages salg\nlastsales = sales.iloc[:,-28:]","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"#Vi gør her brug .rolling()-funktionen, som blev introduceret ovenfor.\nSMA = lastsales.rolling(window=1, axis=1).mean()","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"#Laver et array fra \"F1\" til \"F28\" som er det påkrævede navneformat for kolonner\nf_list = []\nfor num in np.arange(1,29):\n    f_list.append(\"F{}\".format(num))\n\n#Sætter kolonnenavnene for de sidste 28 dages salg til det påkrævede navneformat\nSMA.columns = f_list","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"#Tilføjer det tomme evalueringsdata til vores salgsdata for at opnå det ønskede format\nemptyevaluation = sample.tail(30490)\nsubmission = pd.concat([SMA, emptyevaluation])","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"submission.to_csv('SMA.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Cumulative Moving Average\n\nDen anden form for moving average, som vi har besluttet os for at afprøve, er cumulative moving average (CMA). Her definerer man ikke et vindue som sådan, men har mulighed for at specificere, hvor mange værdier, der som minimum skal inkluderes til den første (og sågar også alle andre) gennemsnitsudregning. Når næste gennemsnitsudregning skal udføres, så bruger man stadig den første værdi, som nu ville ligge uden for vinduet i SMA, dvs. at man altid bruger alle tidligere værdier i udregningen. Ved den allersidste gennemsnitsudregning bruger man altså alle værdier.\n\nI pandas udregnes CMA ved hjælp af .expanding()-funktionen. Denne har parameteret min_periods, som bruges til at definere det minimum antal værdier, der skal bruges til gennemsnitsudregningerne.\n\nHer har vi valgt at bruge hele datasættet for at få så præcist et kumulativt gennemsnit som muligt. Vi har sat min_periods til 7 dage, da vi i vores data-undersøgelse fandt frem til ugentlig udsving i weekenderne, hvorfor det er relevant at have en hel uges data med som minimum.\n\nDen kumulative tilgang forbedrede ikke resultatet ift. SMA.","execution_count":null},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"#Vi gør her brug .expanding()-funktionen, som blev introduceret ovenfor.\nsales_CMA = sales.expanding(min_periods=7, axis=1).mean()","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"#Vi tager CMA-værdierne for de sidste 28 dage\nCMA = sales_CMA.iloc[:,-28:]","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"#Laver et array fra \"F1\" til \"F28\" som er det påkrævede navneformat for kolonner\nf_list = []\nfor num in np.arange(1,29):\n    f_list.append(\"F{}\".format(num))\n\n#Sætter kolonnenavnene for de sidste 28 dages salg til det påkrævede navneformat\nCMA.columns = f_list","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"#Tilføjer det tomme evalueringsdata til vores salgsdata for at opnå det ønskede format\nemptyevaluation = sample.tail(30490)\nsubmission = pd.concat([CMA, emptyevaluation])","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"submission.to_csv('CMA.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Exponential Moving Average\n\nExponential moving average (EMA) tildeler større vægt til nyere værdier end gamle værdier, dvs. at jo ældre datoen for en observeret værdi er, des mindre vil den påvirke gennemsnitsudregningen. Derfor kan man også antage, at den vil give et mere præcist bud end CMA-fremgangen.\n\nI pandas er det muligt at udregne EMA vha. .ewm()-funktionen. Særligt for den er, at man skal sætte en såkaldt ”lagged” værdi/variabel, altså en variabel som indeholder værdierne for tidligere tidspunkter. I .ewm()-funktionen er det f.eks. muligt at sætte en lagged variabel vha. parametret span. Da vi som beskrevet tidligere har observeret en ugentlig trend i datasættet, ville det være oplagt at sætte parametret til syv; derved lægges der særligt vægt på de syv foregående dage i gennemsnitsudregningerne, hvor den nyeste dag af de syv igen får tildelt særlig vægt, og den ældste af de syv dage får tildelt mindst vægt (ud af de syv).\n\nDen eksponentielle tilgang resulterede i en væsentlig forbedring fra CMA, men forbedrede ikke resultat fra ift. SMA. ","execution_count":null},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"#Vi gør her brug .ewm()-funktionen, som blev introduceret ovenfor.\nsales_EMA = sales.ewm(span=7, axis=1).mean()","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"#Vi tager EMA-værdierne for de sidste 28 dage\nEMA = sales_EMA.iloc[:,-28:]","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"#Laver et array fra \"F1\" til \"F28\" som er det påkrævede navneformat for kolonner\nf_list = []\nfor num in np.arange(1,29):\n    f_list.append(\"F{}\".format(num))\n\n#Sætter kolonnenavnene for de sidste 28 dages salg til det påkrævede navneformat\nEMA.columns = f_list","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"#Tilføjer det tomme evalueringsdata til vores salgsdata for at opnå det ønskede format\nemptyevaluation = sample.tail(30490)\nsubmission = pd.concat([EMA, emptyevaluation])","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"submission.to_csv('EMA.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **3. Random forest**\n\nRandom forest er en af de mest kendte algoritmer. Den gør brug af det der kaldes decision trees. Det vil sige en data struktur hvor den ud fra noget input kan løbe igennem forgreninger for at finde det korrekte svar.\n\nNår man taler om random forest, taler man om mange forskellige decisions trees oprettet på baggrunden en masse random valgt subsets af data. Man løber så noget input igennem alle disse træer og ser hvad output\nalle træerne til sammen hælder mest til.\n\nEn random forest er kendt for at være let at arbejde med, intuitiv og forholdsvist præcis til klassificerings problemer. Den kan dog også bruges som en regressor som er det vi vil se på nu.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Vi indlæser dataen til pandas dataframe, og tager datoerne ud for calendar.csv filen","execution_count":null},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sales_train_validation.csv')\ncalendar = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/calendar.csv', parse_dates=['date'], usecols=['date', 'd'])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature engineering\nVi er nød til at omdanne de forskellige salg om til et let læsbar format for en random forest. Vi antager at X (features) modellen skal have er month, year og day. Alle repræsenteret som heltal. Y-værdien (target) er det salg vi har fået den pågældende dag. Det vil sige at for at vores model kan predicte et salg, er det eneste den skal have en dato. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"For at simplificere brugen af Random Forest gør vi her brug af et gennemsnit for alt i sales_train_validation sættet. \n\nEn anden mulighed er at predicte hvert salg for hvert item i sales_train_validation. Dette er også kravet for selve konkurrencen. Da vi dog vil sammenligne forskellige algoritmer og se hvordan de performer på samme dataset, vælger vi her at gøre det for gennemsnittet af alt salg. ","execution_count":null},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"sales = pd.read_csv(\"/kaggle/input/m5-forecasting-accuracy/sales_train_validation.csv\")\ncalendar_stv = pd.read_csv(\"/kaggle/input/m5-forecasting-accuracy/calendar.csv\", parse_dates=['date'])\n# .mean() fjerner alle kolonner hvor det ikke er muligt at tage et gennemsnit på. \nsales_mean = sales.mean()\nsales_mean.index = calendar_stv['date'][:1913]\n# Vi transposer for at få vendt dataframen om. Dette gør selve datoen til index. \nsales_mean_trans = sales_mean.transpose()\nactual = pd.DataFrame(data=sales_mean_trans)\nactual.columns = ['y']\n# Vi deler index op i tre kolonner ud fra måned, dag og år. \nactual['month'] = actual.index.month\nactual['year'] = actual.index.year\nactual['day'] = actual.index.day","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Prediction","execution_count":null},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"# For at vi kan teste hvor godt modellen performer er vi nød til at tage de sidste 28 dage fra træningssættet og gemme de sidste 28 dage i et test set. \ntraining = actual[:-28]\ntest = actual[-28:]","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\ny = training['y'].values\nX = training.drop('y', axis=1)\nX_test = test.drop('y', axis=1)\nY_test = test['y'].values","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"%%time\nrf = RandomForestRegressor()\nmodel = rf.fit(X, y)\ny_pred = model.predict(X_test)\ny_pred","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For at se hvor godt modellen perfomer laver vi her en RMSE (Root mean squared error). ","execution_count":null},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"np.mean((Y_test - y_pred)**2)**0.5","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%% md\n"}},"cell_type":"markdown","source":"## **4. Sarimax**\nSeasonal AutoRegressive Integrated Moving Average with eXogenous regressors model\n- Vi har ikke brugt X'et :)\n\n### Trend og seasonality\nHvis man skal bruge AR og MA komponenterne af Sarimax, skal vi først sørge for at data er stationary.\nVi afdækkede en trend i data under EDA, hvilket betyder at vi lige skal lave lidt arbejde på datasættet før vi kan hoppe ud i nogle predictions.","execution_count":null},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from statsmodels.tsa.statespace.sarimax import SARIMAX","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"calendar_df = pd.read_csv('../input/m5-forecasting-accuracy/calendar.csv', parse_dates=['date'], usecols=['date','d'])\nsample_sub = pd.read_csv('../input/m5-forecasting-accuracy/sample_submission.csv', index_col='id')\n\n# udvælger kun de datoer som ligger i sales_train_validation\n\ncalendar_stv = calendar_df[:1913]\nsales = pd.read_csv('../input/m5-forecasting-accuracy/sales_train_validation.csv', index_col='id')\ndel calendar_df","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"sales_mean = sales.mean()\nsales_mean.index = calendar_stv['date']","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"x= np.arange(0,len(sales_mean))\nslope, intercept, r_value, p_value, std_err = stats.linregress(x, sales_mean.values)\nplt.style.use('ggplot')\nplt.rcParams['figure.figsize'] = [10, 5]\nplt.plot(sales_mean.index, sales_mean.values, 'o', label='original data')\nplt.plot(sales_mean.index, intercept + slope*x, 'g', label='trend linje')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%% md\n"}},"cell_type":"markdown","source":"Ved at bruge diff() på datasættet kan vi fjerne den opadgående trend","execution_count":null},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"diffed = sales_mean.diff().dropna()\nx= np.arange(0,len(diffed))\nslope, intercept, r_value, p_value, std_err = stats.linregress(x, diffed.values)\nplt.style.use('ggplot')\nplt.rcParams['figure.figsize'] = [10, 5]\nplt.plot(diffed.index, diffed.values, 'o', label='diffed data')\nplt.plot(diffed.index, intercept + slope*x, 'g', label='trend linje')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ved yderligere at bruge diff(7) på datasættet bearbejder vi også sæsonkomponenten (vi kunne se et gentagende mønster på 7 dage under EDA)","execution_count":null},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"diffed_season = diffed.diff(7).dropna()\nx= np.arange(0,len(diffed_season))\nslope, intercept, r_value, p_value, std_err = stats.linregress(x, diffed_season.values)\nplt.style.use('ggplot')\nplt.rcParams['figure.figsize'] = [10, 5]\nplt.plot(diffed_season.index, diffed_season.values, 'o', label='diffed data')\nplt.plot(diffed_season.index, intercept + slope*x, 'g', label='trend linje')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"from statsmodels.tsa.stattools import adfuller\nresults = adfuller(sales_mean_trans)\nprint('ingen dif')\nprint('  ADF Statistic: {}'.format(results[0]))\nprint('  p-value: {}'.format(results[1]))\nresults = adfuller(diffed)\nprint('diffed')\nprint('  ADF Statistic: {}'.format(results[0]))\nprint('  p-value: {}'.format(results[1]))\nresults = adfuller(diffed_season)\nprint('diffed+seasonal diffed 7 dage')\nprint('  ADF Statistic: {}'.format(results[0]))\nprint('  p-value: {}'.format(results[1]))\nresults = adfuller(diffed.diff(30).dropna())\nprint('diffed+seasonal diffed 30 dage')\nprint('  ADF Statistic: {}'.format(results[0]))\nprint('  p-value: {}'.format(results[1]))\nresults = adfuller(diffed.diff(365).dropna())\nprint('diffed+seasonal diffed 365 dage')\nprint('  ADF Statistic: {}'.format(results[0]))\nprint('  p-value: {}'.format(results[1]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hvis p-værdien er under 0.05 indenholder datasættet ikke nogen trend længere\n- uden at diffe data er p-value 0.5, så her tyder det også statistisk på at der er en trend.\n- ved første diff ryger vi pænt under 0.5\nDa jeg ved at der er en sæson komponent i data, plotter jeg mine acf og pacf med det diffede sæt","execution_count":null},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfig,(ax1, ax2) = plt.subplots(2,1, figsize=(20,10))\nplot_acf(diffed, zero=False, ax=ax1, lags=21)\nplot_pacf(diffed, zero=False, ax=ax2, lags=21)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Diffet datasæt - generel trend er fjernet\n- Ved at kigge på ACF tyder det på at en mønster gentages for hver 7 dage\n- Vi prøver at fjerne det mønster:","execution_count":null},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"diffed_7 = diffed.diff(7).dropna()\nfig,(ax1, ax2) = plt.subplots(2,1, figsize=(20,10))\nplot_acf(diffed_7, zero=False, ax=ax1, lags=21)\nplot_pacf(diffed_7, zero=False, ax=ax2, lags=21)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"på nuværende tidspunkt er datasættet blevet diffet en gang og en gang med en sæson på 7.\n\n### Parameteranalyse\n\nSARIMAX består af parametrene (p,d,q)(P,D,Q)s\n- d og D og s værdierne kender vi:\n  - d=1 da datasættet skal diffes for at fjerne trenden\n  - D=1 fordi vi har obseveret seasonality\n  - s=7 fordi vi har sat seasonality til en uge\n- På acf-grafen kan man umiddelbart aflæse antallet af lags der skal bruges i MA(q)\n  - q=2\n- på pacf-grafen kan man umiddelbart aflæse antallet af lags på AR(p)\n  - p=4\n- Så mangler vi P og Q værdierne:","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Nu plotter vi acf og pacf med lags der svarer til intevaller på 7 dage","execution_count":null},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"fig,(ax1, ax2) = plt.subplots(2,1, figsize=(20,10))\nnormal_lags = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18]\nlags7 =  [element * 7 for element in normal_lags]\n\nplot_acf(diffed_7, lags=lags7, ax=ax1)\nplot_pacf(diffed_7, lags=lags7, ax=ax2)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"De samme principper gælder som ved p,q værdierne: P aflæses på pacf og Q aflæses på acf\n- Q = 1, lag 0 sæller ikke med som en faktor\n- P = 5,6 måske 9 \n  - Grafen er mere tvetydig for aflæsning","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### In sample prediction","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nmodel = SARIMAX(sales_mean, order=(4,1,2), seasonal_order=(5,1,1,7))\nresults = model.fit()\nforecast = results.get_prediction(start=-28)\nmean_forecast_is = forecast.predicted_mean\nconfidence_intervals = forecast.conf_int()\n\n#confidence_intervals\nlower_limits = confidence_intervals.loc[:,'lower y']\nupper_limits = confidence_intervals.loc[:,'upper y']\nplt.figure(figsize=(20,10))\n#Plot prediction\nplt.plot(sales_mean[-35:].index, sales_mean[-35:].values, label='sales_mean')\nplt.plot(mean_forecast_is.index,\n         mean_forecast_is.values,\n         color='red',\n         label='forecast')\n#shade uncertainty area\nplt.fill_between(mean_forecast_is.index, lower_limits, upper_limits, color='pink')\nplt.legend()\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#mean_forecast\nRMSE_insample = np.mean((sales_mean[-28:] - mean_forecast_is)**2)**0.5\nprint(\"Sarimax(4,1,2)(5,1,1)7\\n - RMSE score: {}\\n - fitting Tid: {}\".format(RMSE_insample, '60 sekunder'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Out of sample forecast\nJeg skal have splittet mit data op så jeg kan foretage en form for kontrol med kvaliteten af data:\n- `train` indeholder alt data - minus de sidste 28 dage\n- `test` indeholder kun de sidste 28 dage\n\nJeg bruger `train` til at fitte min model og predicte de næste 28 dage frem.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test = sales_mean[-28:]\ntrain = sales_mean[:-28]","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"%%time\nmodel = SARIMAX(train, order=(4,1,2), seasonal_order=(5,1,1,7))\nresults = model.fit()\nprint(results.aic, results.bic)","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"print(results.summary())","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"forecast = results.get_prediction(start='28-03-2016', end='24-04-2016', dynamic=True)\nmean_forecast = forecast.predicted_mean\nconfidence_intervals = forecast.conf_int()\n\n#confidence_intervals\nlower_limits = confidence_intervals.loc[:,'lower y']\nupper_limits = confidence_intervals.loc[:,'upper y']\nplt.figure(figsize=(20,10))\n#Plot prediction\nplt.plot(train[-20:].index, train[-20:].values, label='Train data')\nplt.plot(test.index, test.values, label='Test data')\nplt.plot(mean_forecast.index,\n         mean_forecast.values,\n         color='red',\n         label='forecast')\n#shade uncertainty area\nplt.fill_between(mean_forecast.index, lower_limits, upper_limits, color='pink')\nplt.legend()\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Umiddelbart på grafen ser det nogen lunde ud. Den røde streg - vores prediction - ligger relativt tæt på test.\n","execution_count":null},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"#mean_forecast\nRMSE = np.mean((test - mean_forecast)**2)**0.5\nprint(\"Sarimax(4,1,2)(5,1,1)7\\n - RMSE score: {}\\n - fitting Tid: {}\".format(RMSE, '60 sekunder'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"En sammenligning af RMSE scoren på out of sample og in sample viser at modellen er nogenlunde stabil - den var forventeligt lidt bedre på insample data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('RMSE in sample:     {}\\nRMSE out of sample: {}'.format(RMSE_insample, RMSE))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **5. Prophet**\n\nHer ses et eksempel på brug af prophet. Prophet er en machine learning algoritme udviklet af Facebook til at lave timeseries forecasts.\n\nMålet med denne algoritme er, at sænke kompleksiteten for timeseries forecasting og gøre brug af den syntaks der allerede eksistere i sklearn (.fit(), .predict())\n\nProphet er en algoritme der grundlæggende har følgende formel: y(t) = g(t) + s(t) + h(t) + e(t).\n\nHvor y(t) er det pågældende tid vi forudsiger.\n\ng(t) er trenden.\n\ns(t) er seasonality.\n\nh(t) er holiday.\n\ne(t) er errors vi ikke kan tage højde for.\n\nDet er derfor flere forskellige modeller, der udgør prophet. Det er altså det der kaldes en additive model. \n\nProphets hovedfokus er indenfor den branchen facebook er i, det vil sige menneskeskabt data. Derfor følger prophet pr. default nogle standard værdier der er typisk for et menneske (5. dages arbejdsuge, forskellige ferier osv.). Da det er en machine learning algoritme giver det selvfølgelig mening, at kigge på at hypertune alle de mulige parametre. \n\nNedestående er et eksempel på en implementering af prophet.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Vi starter med at rense dataen, og tage højde for de events der er placeret i calendar. ","execution_count":null},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"sales = pd.read_csv(\"/kaggle/input/m5-forecasting-accuracy/sales_train_validation.csv\")\ncalendar = pd.read_csv(\"/kaggle/input/m5-forecasting-accuracy/calendar.csv\", parse_dates=['date'])\nsales_mean = sales.mean()\nsales_mean.index = calendar['date'][:1913]\nsales_mean_trans = sales_mean.transpose()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Prohpet har mulighed for at bruge events / holidays. I dette tilfælde tager vi alle de datoer hvor der er et event på. Denne dataframe kan sættes som parametre når modellen skal predicte.","execution_count":null},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"df_ev_1 = pd.DataFrame({'holiday': 'Event 1', 'ds': calendar[~calendar['event_name_1'].isna()]['date']})\ndf_ev_2 = pd.DataFrame({'holiday': 'Event 2', 'ds': calendar[~calendar['event_name_2'].isna()]['date']})\nholidays = pd.concat((df_ev_1, df_ev_2))\ndel df_ev_1\ndel df_ev_2\ndel calendar","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Prophet kræver at dataframen har et hvis format. Dataen skal have to kolonner, hhv. en ds-kolonne for DateStamp og en y kolonne for dit target. ","execution_count":null},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"ds = sales_mean_trans.index.values\ny = sales_mean_trans.values\ntest = pd.DataFrame(columns=['ds', 'y'])\ntest['ds'] = ds\ntest['y'] = y","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Vi træner på de kendte data undtagen de sidste 28 dage. Disse dage bruger vi til at teste hvor godt vores model scorer tilsidst.","execution_count":null},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"training = test[:-28]\nactual = test[-28:]","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"%%time\nfrom fbprophet import Prophet\n# Vi sender holidays ind som parametre.\nmodel = Prophet(holidays = holidays)\nmodel.fit(training)\nforecast = model.make_future_dataframe(periods=28, include_history=False)\nforecast = model.predict(forecast)","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"y_pred = forecast['yhat']\ny_actual = actual['y']\ny_actual = y_actual.reset_index()['y']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Vi lavere en RMSE for at se hvor godt vores model performet. ","execution_count":null},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"np.mean((y_actual - y_pred)**2)**0.5","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Scorer og tidsforbrug**\n- Tidsforbruget er fitting på gennemsnitssalget\n- RMSE score ","execution_count":null},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"score_dict = {\n    'naive t-28': [0.10098, 2.0e-3],\n    'Moving Average': [1.0942477597466052e-15, 0.0002],\n    'SARIMAX': [0.07044, 60],\n    'Prophet': [0.08670, 3.47],\n    'Random Forrest': [0.35538,0.326]\n}\n\nscores = pd.DataFrame(data=score_dict)\nscores.index = ['RMSE', 'seconds']\n#scores","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"x = np.arange(3)\nfig, (ax,a2) = plt.subplots(2,1,figsize=(10,10))\nax.barh(scores.columns,scores.loc['seconds'])\nax.set_title('Tidsforbrug')\nax.set_xlabel('Sekunder')\na2.barh(scores.columns, scores.loc['RMSE'])\na2.set_title('RMSE')\na2.set_xlabel('Score')\nfig.tight_layout(pad=3.0)\nplt.show()\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}