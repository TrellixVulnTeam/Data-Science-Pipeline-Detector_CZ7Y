{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nimport seaborn as sns\nfrom matplotlib import colors\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import Normalize\n\nfrom itertools import cycle\npd.set_option('max_columns', 50)\nplt.style.use('bmh')\ncolor_pal = plt.rcParams['axes.prop_cycle'].by_key()['color']\ncolor_cycle = cycle(plt.rcParams['axes.prop_cycle'].by_key()['color'])\n\n\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\n\nimport pywt\nfrom statsmodels.robust import mad\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_dir = '../input/m5-forecasting-accuracy/'\n\nsales_train_validation = pd.read_csv(input_dir+'sales_train_validation.csv')\ncalendar = pd.read_csv(input_dir+'calendar.csv')\nsales_prices = pd.read_csv(input_dir+'sell_prices.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train_validation.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"calendar.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sales_prices.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###  Downcasting","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"def reduce_mem_usage(df, verbose=True):\n\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n\n    # 计算当前占用的内存 \n    start_mem = df.memory_usage(deep=True).sum() / 1024**2\n\n    # 循环每一列\n    for col in df.columns:\n\n        # 获取每一列的数据类型\n        col_type = df[col].dtypes\n\n        # 如果数据类型属于上面定义的类型之\n        if col_type in numerics:\n\n            # 计算该列数据的最小值和最大值 用于我们指定相应的数据类型 \n            c_min = df[col].min()\n            c_max = df[col].max()\n\n            # 如果 该列的数据类型属于 int 类型，然后进行判断\n            if str(col_type)[:3] == 'int':\n                # 如果 该列最小的值 大于int8类型的最小值，并且最大值小于int8类型的最大值，则采用int8类型 \n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n\n                # 同上\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n\n                # 同上\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n\n                # 同上\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n\n            # 否则 则采用 float 的处理方法       \n            else:\n\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage(deep=True).sum() / 1024**2\n\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sales_bd = np.round(sales_train_validation.memory_usage().sum()/(1024*1024),1)\ncalendar_bd = np.round(calendar.memory_usage().sum()/(1024*1024),1)\nprices_bd = np.round(sales_prices.memory_usage().sum()/(1024*1024),1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sales_train_validation = reduce_mem_usage(sales_train_validation)\nsales_prices = reduce_mem_usage(sales_prices)\ncalendar = reduce_mem_usage(calendar)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sales_ad = np.round(sales_train_validation.memory_usage().sum()/(1024*1024),1)\ncalendar_ad = np.round(calendar.memory_usage().sum()/(1024*1024),1)\nprices_ad = np.round(sales_prices.memory_usage().sum()/(1024*1024),1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# memory   # no melt","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# memory  # melt","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"memory = pd.DataFrame({\n    'DataFrame':['sale_train_val','sale_prices','calendar'],\n    'Before memory reducing':[sales_bd,prices_bd,calendar_bd],\n    'After memory reducing':[sales_ad,prices_ad,calendar_ad],\n\n})\n\nmemory = pd.melt(memory,id_vars='DataFrame',var_name='Status',value_name='Memory(MB)')\nmemory.sort_values('Memory(MB)',inplace=True)\nfig = px.bar(memory,x='DataFrame',y='Memory(MB)',color='Status',barmode='group',text='Memory(MB)')\nfig.update_traces(textposition='outside')\nfig.update_layout(template='seaborn',title='Effect of Downcasting')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Sample sales data","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"ids = sorted(list(sales_train_validation['id']))\n\nd_cols = [c for c in sales_train_validation.columns if 'd_' in c ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"x_1 = sales_train_validation.loc[sales_train_validation['id'] == ids[2]].set_index('id')[d_cols].values[0] \nx_2 = sales_train_validation.loc[sales_train_validation['id'] == ids[6]].set_index('id')[d_cols].values[0]\nx_3 = sales_train_validation.loc[sales_train_validation['id'] == ids[7]].set_index('id')[d_cols].values[0]\n\nfig = make_subplots(rows=3,cols=1)\n\nfig.add_traces(go.Scatter(x=np.arange(len(x_1)),y=x_1,showlegend=False,mode='lines',name='First sample',marker=dict(color=next(color_cycle)))\n               ,rows=1,cols=1)\n\nfig.add_traces(go.Scatter(x=np.arange(len(x_2)),y=x_2,showlegend=False,mode='lines',name='Second sample',marker=dict(color=next(color_cycle)))\n                ,rows=2,cols=1)\n\nfig.add_traces(go.Scatter(x=np.arange(len(x_3)),y=x_3,showlegend=False,mode='lines',name='Third sample',marker=dict(color=next(color_cycle)))\n                ,rows=3,cols=1)\n\nfig.update_layout(height =1200,width=800,title_text='Sample sales')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"可以看出这几个商品的销售数据非常不稳定，有的时候销售量甚至为0，可能意味着该商品在某天不可用。","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Sample sales snippets","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"x_1 = sales_train_validation.loc[sales_train_validation['id'] == ids[2]].set_index('id')[d_cols].values[0][0:90] \nx_2 = sales_train_validation.loc[sales_train_validation['id'] == ids[6]].set_index('id')[d_cols].values[0][90:180] \nx_3 = sales_train_validation.loc[sales_train_validation['id'] == ids[7]].set_index('id')[d_cols].values[0][1800:] \n\nfig = make_subplots(rows=3,cols=1)\n\nfig.add_traces(go.Scatter(x=np.arange(len(x_1)),y=x_1,showlegend=False,mode='lines',name='First sample',marker=dict(color=next(color_cycle)))\n               ,rows=1,cols=1) # 指定绘制的子图\n\nfig.add_traces(go.Scatter(x=np.arange(len(x_2)),y=x_2,showlegend=False,mode='lines',name='Second sample',marker=dict(color=next(color_cycle)))\n                ,rows=2,cols=1)\n\nfig.add_traces(go.Scatter(x=np.arange(len(x_3)),y=x_3,showlegend=False,mode='lines',name='Third sample',marker=dict(color=next(color_cycle)))\n                ,rows=3,cols=1)\n\nfig.update_layout(height =1200,width=800,title_text='Sample sales')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"当放大其中某个片段时，我们也可以看出来销售数据的不稳定，某一天的销售量很好，某一天甚至几天的销售量都很低","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### seasonal_decompose\n\n* https://zhuanlan.zhihu.com/p/127032260\n\n* https://github.com/statsmodels/statsmodels/issues/3085\n\n通过Python中的seasonal_decompose函数可以提取序列的趋势、季节和随机效应。对于非平稳的时间序列，可以通过对趋势和季节性进行建模并将它们从模型中剔除，从而将非平稳的数据转换为平稳数据，并对其残差进行进一步的分析。","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"# from statsmodels.tsa.seasonal import seasonal_decompose","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# new_x1=sales_train_validation.loc[sales_train_validation['id'] == ids[2]].set_index('id')[d_cols][0:90]\n# new_x1.T","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# decomposition = seasonal_decompose(new_x1,freq=12)\n# trend = decomposition.trend #趋势效应\n# seasonal = decomposition.seasonal #季节效应\n# residual = decomposition.resid #随机效应\n# plt.subplot(411)\n# plt.plot(new_x1, label=u'原始数据')\n# plt.legend(loc='best')\n# plt.subplot(412)\n# plt.plot(trend, label=u'趋势')\n# plt.legend(loc='best')\n# plt.subplot(413)\n# plt.plot(seasonal,label=u'季节性')\n# plt.legend(loc='best')\n# plt.subplot(414)\n# plt.plot(residual, label=u'残差')\n# plt.legend(loc='best')\n# plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 小波去噪","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"当遇到销售数据非常不稳定时，我们需要某种“去噪”技术来发现销售数据的潜在趋势并作出预测。\n\n\n现在，我将展示如何消除这些不稳定的销售价格，以提取潜在的趋势。这种方法可能会丢失原始时间序列中的一些信息，但在提取有关时间序列趋势的某些特征时，它可能是有用的。小波去噪（通常用于电信号）是一种去除时间序列中不必要噪声的方法。这种方法计算称为“小波系数”的系数。这些系数决定了保留哪些信息（信号）和丢弃哪些信息（噪声）。\n\n我们利用平均绝对偏差（MAD）值来理解销售中的随机性，从而确定时间序列中小波系数的最小阈值。我们从小波中过滤出低系数，然后从剩余的系数中重建销售数据，就这样，我们成功地从销售数据中去除了噪声。\n\n参考：https://www.kaggle.com/theoviel/denoising-with-direct-wavelet-transform","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"def maddest(d, axis=None):\n    return np.mean(np.absolute(d - np.mean(d, axis)), axis)\n\ndef denoise_signal(x, wavelet='db4', level=1):\n    coeff = pywt.wavedec(x, wavelet, mode=\"per\")\n    sigma = (1/0.6745) * maddest(coeff[-level])\n\n    uthresh = sigma * np.sqrt(2*np.log(len(x)))\n    coeff[1:] = (pywt.threshold(i, value=uthresh, mode='hard') for i in coeff[1:])\n\n    return pywt.waverec(coeff, wavelet, mode='per')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"进行去噪","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"y_w1 = denoise_signal(x_1)\ny_w2 = denoise_signal(x_2)\ny_w3 = denoise_signal(x_3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"下图中：红色图形表示原始销售额，绿色图形表示去噪后的销售额。","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"fig,ax=plt.subplots(nrows=3,ncols=2,figsize=(30,20))\n\nax[0,0].plot(x_1,color='seagreen',marker='o')\nax[0,0].set_title('Original Sales',fontsize=24)\nax[0,0].plot(y_w1,color='red',marker='.')\nax[0,0].set_title('Original Sales',fontsize=24)\nax[0,1].plot(y_w1,color='red',marker='.')\nax[0,1].set_title('Original Sales',fontsize=24)\n\nax[1,0].plot(x_2,color='seagreen',marker='o')\nax[1,0].set_title('Original Sales',fontsize=24)\nax[1,0].plot(y_w2,color='red',marker='.')\nax[1,0].set_title('Original Sales',fontsize=24)\nax[1,1].plot(y_w2,color='red',marker='.')\nax[1,1].set_title('Original Sales',fontsize=24)\n\nax[2,0].plot(x_3,color='seagreen',marker='o')\nax[2,0].set_title('Original Sales',fontsize=24)\nax[2,0].plot(y_w3,color='red',marker='.')\nax[2,0].set_title('Original Sales',fontsize=24)\nax[2,1].plot(y_w3,color='red',marker='.')\nax[2,1].set_title('Original Sales',fontsize=24)\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Average smoothing 均值平滑\n\n均值平滑是一种相对简单的去噪时间序列数据的方法。在这种方法中，我们取一个固定大小的“窗口”（如10）。我们首先将窗口放在时间序列的开始处（前十个元素），然后计算该部分的平均值。我们现在在时间序列中向前移动一个特定的“步幅”，计算新窗口的平均值并重复这个过程，直到到达时间序列的末尾。然后，我们计算出的所有平均值被连接到一个新的时间序列中，形成去噪后的销售数据。","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"def average_smoothing(signal, kernel_size=3, stride=1):\n    sample = [0]*(kernel_size-stride) # 通过 len(y_a1) 可以发现与原始数据同长度\n    start = 0\n    end = kernel_size\n    while end <= len(signal):\n        start = start + stride\n        end = end + stride\n        sample.extend([np.mean(signal[start:end])])\n    return np.array(sample)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"y_a1 = average_smoothing(x_1)\ny_a2 = average_smoothing(x_2)\ny_a3 = average_smoothing(x_3)\n\nfig = make_subplots(rows=3, cols=1)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(len(x_1)), mode='lines+markers', y=x_1, marker=dict(color=\"lightskyblue\"), showlegend=False,\n               name=\"Original sales\"),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(len(x_1)), y=y_a1, mode='lines', marker=dict(color=\"navy\"), showlegend=False,\n               name=\"Denoised sales\"),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(len(x_2)), mode='lines+markers', y=x_2, marker=dict(color=\"thistle\"), showlegend=False),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(len(x_2)), y=y_a2, mode='lines', marker=dict(color=\"indigo\"), showlegend=False),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(len(x_3)), mode='lines+markers', y=x_3, marker=dict(color=\"mediumaquamarine\"), showlegend=False),\n    row=3, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(len(x_3)), y=y_a3, mode='lines', marker=dict(color=\"darkgreen\"), showlegend=False),\n    row=3, col=1\n)\n\n\nfig.update_layout(height=1200, width=800, title_text=\"Original (pale) vs. Denoised (dark) signals\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"在上面的图表中，暗线图表示去噪后的销售额，浅线图表示原始销售额。我们可以看到，平均平滑在发现数据中的宏观趋势和模式方面也很不错。\n\n但是一般来说，小波去噪在寻找销售数据趋势方面显然更有效。尽管如此，平均平滑或“滚动平均”也可以用于计算建模的有用特征。","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"fig,ax = plt.subplots(nrows=3,ncols=2,figsize=(30,20))\n\nax[0,0].plot(x_1,color='seagreen',marker='o')\nax[0,0].set_title('Original Sales',fontsize=24)\nax[0,1].plot(y_a1,color='red',marker='x')\nax[0,1].set_title('Original Sales',fontsize=24)\n\nax[1,0].plot(x_2,color='seagreen',marker='o')\nax[1,0].set_title('Original Sales',fontsize=24)\nax[1,1].plot(y_a2,color='red',marker='.')\nax[1,1].set_title('Original Sales',fontsize=24)\n\n\nax[2,0].plot(x_3,color='seagreen',marker='o')\nax[2,0].set_title('Original Sales',fontsize=24)\nax[2,1].plot(y_a3,color='red',marker='.')\nax[2,1].set_title('Original Sales',fontsize=24)\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Stores and states\n\n接下来，我将查看不同商店和州的销售数据，以便获得一些有用的见解。","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"sales_train_validation.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sales_prices.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"calendar.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 查看不同时间商品的销量情况","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"past_sales = sales_train_validation.set_index('id')[d_cols] \\\n            .T \\\n            .merge(calendar.set_index('d')['date'],left_index =True,right_index=True,).set_index('date')\n\nstore_list = sales_prices['store_id'].unique()\nmeans = []\n\nfig = go.Figure()\n\nfor s_id in  store_list:\n    store_items = [s for s in past_sales.columns if s_id in s]\n    data = past_sales[store_items].sum(axis=1).rolling(90).mean()\n    means.append(np.mean(past_sales[store_items].sum(axis=1)))\n    fig.add_trace(go.Scatter(x=data.index, y=data, name=s_id))\n                 \nfig.update_layout(yaxis_title='Sales',xaxis_title='Time',title='Rolling average Sales vs Time (per store)')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"fig = go.Figure()\nfor s_id in  store_list:\n    store_items = [s for s in past_sales.columns if s_id in s]\n    data = past_sales[store_items].sum(axis=1).rolling(90).mean()\n    fig.add_trace(go.Box(x=[s_id]*len(data), y=data, name=s_id))\nfig.update_layout(yaxis_title='Sales',xaxis_title='Store',title='Rolling average Sales vs Store (per store)')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 各个商店所对应产品销售均值比较","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"fig = go.Figure()\n\n# method 1\ndf = pd.DataFrame(np.transpose([means,store_list]))\ndf.columns=['Mean sales','Store name']\npx.bar(df, y=\"Mean sales\", x=\"Store name\", color=\"Store name\", title=\"Mean sales vs. Store name\")\n\n# method 2\n# for i in np.transpose([means, store_list]):\n#     fig.add_trace(go.Bar(y=[i[0]], x=[i[1]], marker = dict(color = next(color_cycle)),name=i[1]))\n# fig.update_layout(yaxis_title=\"Mean Sales\", xaxis_title=\"Store name\", title=\"Mean Sales vs. Store name\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"可以看出 CA_3 的平均销量最好，CA_4 平均销量最差","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### 不同时间下，不同产品数量的变化","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"cat_id_list = sales_train_validation['cat_id'].unique()\n\nfig= go.Figure()\n\nfor cat_id in  cat_id_list:\n    store_items = [s for s in past_sales.columns if cat_id in s]\n    data = past_sales[store_items].sum(axis=1)\n    fig.add_trace(go.Scatter(x=data.index,y=data,name=cat_id))\nfig.update_layout(xaxis_title='time',yaxis_title='cat_id sales',title='cat_id Sales vs Time (per cat_id)')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"商品在不同时间下出现的百分比次数","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"past_sales_clipped = past_sales.clip(0, 1)\n\nfig = go.Figure()\nfor cat_id in  cat_id_list:\n    store_items = [s for s in past_sales_clipped.columns if cat_id in s]\n    data = past_sales_clipped[store_items].mean(axis=1) * 100\n    fig.add_trace(go.Scatter(x=data.index,y=data,name=cat_id,mode='markers'))\nfig.update_layout(xaxis_title='time',yaxis_title='% of Inventory with at least 1 sale',title='Inventory Sale Percentage by Date')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print('The lowest sale date was:', past_sales.sum(axis=1).sort_values().index[0],\n     'with', past_sales.sum(axis=1).sort_values().values[0], 'sales')\nprint('The lowest sale date was:', past_sales.sum(axis=1).sort_values(ascending=False).index[0],\n     'with', past_sales.sum(axis=1).sort_values(ascending=False).values[0], 'sales')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"可以看出在不同时间下，商品的数量也不一样，也就是说存在着有的商品退出了市场","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Modeling 开始建立模型","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### ARIMA \n\nARIMA 的英文全称是 Auto Regressive Integrated Moving Average 模型，中文叫 __差分自回归滑动平均模型__ ，也叫求合自回归滑动平均模型。相比于 ARMA，ARIMA 多了一个差分的过程，作用是对不平稳数据进行差分平稳，在差分平稳后再进行建模。ARIMA 的原理和 ARMA 模型一样。相比于 ARMA(p,q) 的两个阶数，ARIMA 是一个三元组的阶数 (p,d,q)，称为 ARIMA(p,d,q) 模型。其中 d 是差分阶数。\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### SARIMAX\n\n具有季节回归模型的季节性自回归综合移动平均线\n\nhttps://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.sarimax.SARIMAX.html","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"import statsmodels.api as sm\nimport statsmodels.formula.api as smf\nimport statsmodels.tsa.api as smt\nimport itertools\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nimport statsmodels\n\nfrom tqdm import tqdm","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 划分训练集和测试集","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"store_sum = sales_train_validation.groupby(['store_id']).sum().T.reset_index(drop = True)\nstore_sum.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_datasets= store_sum.iloc[0:70]\nval_datasets= store_sum.iloc[70:100]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### seasonal_decompose\n\n季节分解显示了一个时间序列在多大程度上表现出季节性和趋势性。这是一个很好的方式来考虑有多少销售是由于季节性，趋势，或一次性事件，如假日。\n\n现在我们将使用每年一次的频率，直到我们做更详细的分析。","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"from statsmodels.tsa.seasonal import seasonal_decompose","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"weeks_per_year = 365\n\ntime_series = store_sum[\"CA_1\"]\nsj_sc = seasonal_decompose(time_series, freq= weeks_per_year)\nsj_sc.plot()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"绘制其中某个商品在不同地区下的销售图像","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"fig = make_subplots(3,1)\n\nfig.add_trace(go.Scatter(x=np.arange(70),y=train_datasets.iloc[:,0],marker=dict(color='seagreen'),name='Train'),row=1,col=1)\nfig.add_trace(go.Scatter(x=np.arange(70,100),y=val_datasets.iloc[:,0],marker=dict(color='red'),name='Val'),row=1,col=1)\n\nfig.add_trace(go.Scatter(x=np.arange(70),y=train_datasets.iloc[:,4],marker=dict(color='seagreen'), showlegend=False),row=2,col=1)\nfig.add_trace(go.Scatter(x=np.arange(70,100),y=val_datasets.iloc[:,4],marker=dict(color='red'), showlegend=False),row=2,col=1)\n\nfig.add_trace(go.Scatter(x=np.arange(70),y=train_datasets.iloc[:,7],marker=dict(color='seagreen'), showlegend=False),row=3,col=1)\nfig.add_trace(go.Scatter(x=np.arange(70,100),y=val_datasets.iloc[:,7],marker=dict(color='red'), showlegend=False),row=3,col=1)\n\nfig.update_layout(title='Sales volume of a commodity')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"store_col = [0,4,7]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def sarima_train_test(t_series, p = 2, d = 1, r = 2, NUM_TO_FORECAST = 56, do_plot_results = True):\n    \n    NUM_TO_FORECAST = NUM_TO_FORECAST  # Similar to train test splits.\n    dates = np.arange(t_series.shape[0])\n    \n    model = SARIMAX(t_series, order = (p, d, r), trend = 'c')\n    results = model.fit()\n#     results.plot_diagnostics(figsize=(12, 8))\n#     plt.show()\n\n    forecast = results.get_prediction(start = - NUM_TO_FORECAST)\n    mean_forecast = forecast.predicted_mean\n    conf_int = forecast.conf_int()\n\n    print(mean_forecast.shape)\n\n    # Plot the forecast\n#     plt.figure(figsize=(6,4))\n#     plt.plot(dates[-NUM_TO_FORECAST:],\n#             mean_forecast.values,\n#             color = 'red',\n#             label = 'forecast')\n\n\n#     plt.plot(dates[-NUM_TO_FORECAST:],\n#             t_series.iloc[-NUM_TO_FORECAST:],\n#             color = 'blue',\n#             label = 'actual')\n#     plt.legend()\n#     plt.title('Predicted vs. Actual Values')\n#     plt.show()\n    \n    residuals = results.resid # 模型的残差\n    mae_sarima = np.mean(np.abs(residuals))\n    print('Mean absolute error: ', mae_sarima)\n    print(results.summary())\n    return mean_forecast","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"predictions = []\n\nfor col in store_col:\n    predictions.append(sarima_train_test(train_datasets.iloc[:,col],NUM_TO_FORECAST=28))\n\npredictions = np.array(predictions).reshape((-1, 28))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"pred_1 = predictions[0]\npred_2 = predictions[1]\npred_3 = predictions[2]\n\nfig = make_subplots(rows=3, cols=1)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70), mode='lines', y=train_datasets.iloc[:,0].values, marker=dict(color=\"dodgerblue\"),\n               name=\"Train\"),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=val_datasets.iloc[:,0].values, mode='lines', marker=dict(color=\"darkorange\"),\n               name=\"Val\"),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=pred_1, mode='lines', marker=dict(color=\"seagreen\"),\n               name=\"Pred\"),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70), mode='lines', y=train_datasets.iloc[:,4].values, marker=dict(color=\"dodgerblue\"), showlegend=False),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=val_datasets.iloc[:,4].values, mode='lines', marker=dict(color=\"darkorange\"), showlegend=False),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=pred_2, mode='lines', marker=dict(color=\"seagreen\"), showlegend=False,\n               name=\"Denoised signal\"),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70), mode='lines', y=train_datasets.iloc[:,7].values, marker=dict(color=\"dodgerblue\"), showlegend=False),\n    row=3, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=val_datasets.iloc[:,7].values, mode='lines', marker=dict(color=\"darkorange\"), showlegend=False),\n    row=3, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=pred_3, mode='lines', marker=dict(color=\"seagreen\"), showlegend=False,\n               name=\"Denoised signal\"),\n    row=3, col=1\n)\n\nfig.update_layout(height=1200, width=800, title_text=\"ARIMA\")\nfig.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}