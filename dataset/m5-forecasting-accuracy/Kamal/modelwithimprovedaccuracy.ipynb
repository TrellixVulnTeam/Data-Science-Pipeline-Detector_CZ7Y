{"cells":[{"metadata":{},"cell_type":"markdown","source":"<hr>\n1. Thanks to Konstantin Yakovlev for excellent Kernal, where i got the help\n2. We have done some improvement as per previous model and accuracy have been increase to .47450\n3. Please upvote the Kernal in case it seems helpful for you all"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# General imports\nimport numpy as np\nimport pandas as pd\nimport os, sys, gc, time, warnings, pickle, psutil, random\n\n# custom imports\nfrom multiprocessing import Pool        # Multiprocess Runs\n\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"\ndef seed_everything(seed=0):\n    random.seed(seed)\n    np.random.seed(seed)\n\n    \n## Multiprocess Runs\ndef df_parallelize_run(func, t_split):\n    num_cores = np.min([N_CORES,len(t_split)])\n    pool = Pool(num_cores)\n    df = pd.concat(pool.map(func, t_split), axis=1)\n    pool.close()\n    pool.join()\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef get_data_by_store(store):\n    \n    # Read and contact basic feature\n    df = pd.concat([pd.read_pickle(BASE),\n                    pd.read_pickle(PRICE).iloc[:,2:],\n                    pd.read_pickle(CALENDAR).iloc[:,2:]],\n                    axis=1)\n    \n    df = df[df['store_id']==store]\n    df2 = pd.read_pickle(MEAN_ENC)[mean_features]\n    df2 = df2[df2.index.isin(df.index)]\n    \n    df3 = pd.read_pickle(LAGS).iloc[:,3:]\n    df3 = df3[df3.index.isin(df.index)]\n    \n    df = pd.concat([df, df2], axis=1)\n    del df2 # to not reach memory limit \n    \n    df = pd.concat([df, df3], axis=1)\n    del df3 # to not reach memory limit \n    \n    # Create features list\n    features = [col for col in list(df) if col not in remove_features]\n    df = df[['id','d',TARGET]+features]\n    \n    # Skipping first n rows\n    df = df[df['d']>=START_TRAIN].reset_index(drop=True)\n    \n    return df, features\n\n# Recombine Test set after training\ndef get_base_test():\n    base_test = pd.DataFrame()\n\n    for store_id in STORES_IDS:\n        temp_df = pd.read_pickle('test_'+store_id+'.pkl')\n        temp_df['store_id'] = store_id\n        base_test = pd.concat([base_test, temp_df]).reset_index(drop=True)\n    \n    return base_test\n\n\ndef make_lag(LAG_DAY):\n    lag_df = base_test[['id','d',TARGET]]\n    col_name = 'sales_lag_'+str(LAG_DAY)\n    lag_df[col_name] = lag_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(LAG_DAY)).astype(np.float16)\n    return lag_df[[col_name]]\n\n\ndef make_lag_roll(LAG_DAY):\n    shift_day = LAG_DAY[0]\n    roll_wind = LAG_DAY[1]\n    lag_df = base_test[['id','d',TARGET]]\n    col_name = 'rolling_mean_tmp_'+str(shift_day)+'_'+str(roll_wind)\n    lag_df[col_name] = lag_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(shift_day).rolling(roll_wind).mean())\n    return lag_df[[col_name]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"sell_prices.csv"},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### Model params\nimport lightgbm as lgb\nlgb_params = {\n                    'boosting_type': 'gbdt',\n                    'objective': 'tweedie',\n                    'tweedie_variance_power': 1.1,\n                    'metric': 'rmse',\n                    'subsample': 0.5,\n                    'subsample_freq': 1,\n                    'learning_rate': 0.03,\n                    'num_leaves': 2**11-1,\n                    'min_data_in_leaf': 2**12-1,\n                    'feature_fraction': 0.5,\n                    'max_bin': 100,\n                    'n_estimators': 1400,\n                    'boost_from_average': False,\n                    'verbose': -1,\n                } \n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#prices.info(memory_usage=\"deep\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#prices.memory_usage(deep=True) * 1e-6","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#prices.memory_usage(deep=True).sum() * 1e-6","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### Vars\n#################################################################################\nVER = 2                          # Our model version\nSEED = 42                        \nseed_everything(SEED)            \nlgb_params['seed'] = SEED       \nN_CORES = psutil.cpu_count()     \n\n#LIMITS and const\nTARGET      = 'sales'           \nSTART_TRAIN = 0                  \nEND_TRAIN   = 1913               \nP_HORIZON   = 28                \nUSE_AUX     = True               \n\n#FEATURES to remove\n## These features lead to overfit\nremove_features = ['id','state_id','store_id',\n                   'date','wm_yr_wk','d',TARGET]\nmean_features   = ['enc_cat_id_mean','enc_cat_id_std',\n                   'enc_dept_id_mean','enc_dept_id_std',\n                   'enc_item_id_mean','enc_item_id_std'] \n\n\nORIGINAL = '../input/m5-forecasting-accuracy/'\nBASE     = '../input/featureextraction/grid_part_1.pkl'\nPRICE    = '../input/featureextraction/grid_part_2.pkl'\nCALENDAR = '../input/featureextraction/grid_part_3.pkl'\nLAGS     = '../input/lag-rollingfeature/lags_df_28.pkl'\nMEAN_ENC = '../input/other-features/mean_encoding_df.pkl'\n# AUX(pretrained) Models paths\nAUX_MODELS = '../input/m5modelwithestimator1300/'\n\n\n\nSTORES_IDS = pd.read_csv(ORIGINAL+'sales_train_validation.csv')['store_id']\nSTORES_IDS = list(STORES_IDS.unique())\n\n\n\n\n#SPLITS for lags creation\nSHIFT_DAY  = 28\nN_LAGS     = 15\nLAGS_SPLIT = [col for col in range(SHIFT_DAY,SHIFT_DAY+N_LAGS)]\nROLS_SPLIT = []\nfor i in [1,7,14]:\n    for j in [7,14,30,60]:\n        ROLS_SPLIT.append([i,j])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"STORES_IDS","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We can see above table is taking a lot of space which is not at all good for further computation.Let us identify some columns which are consuming most of memory and space**"},{"metadata":{},"cell_type":"markdown","source":"**To make this easier to read, we convert all values to megabytes**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Model Training","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nif USE_AUX:\n    lgb_params['n_estimators'] = 2\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### Train Models\n#################################################################################\nfor store_id in STORES_IDS:\n    print('Train', store_id)\n    \n    # Get grid for current store\n    grid_df, features_columns = get_data_by_store(store_id)\n    \n    train_mask = grid_df['d']<=END_TRAIN\n    valid_mask = train_mask&(grid_df['d']>(END_TRAIN-P_HORIZON))\n    preds_mask = grid_df['d']>(END_TRAIN-100)\n    \n\n    train_data = lgb.Dataset(grid_df[train_mask][features_columns], \n                       label=grid_df[train_mask][TARGET])\n    train_data.save_binary('train_data.bin')\n    train_data = lgb.Dataset('train_data.bin')\n    \n    valid_data = lgb.Dataset(grid_df[valid_mask][features_columns], \n                       label=grid_df[valid_mask][TARGET])\n    \n    grid_df = grid_df[preds_mask].reset_index(drop=True)\n    keep_cols = [col for col in list(grid_df) if '_tmp_' not in col]\n    grid_df = grid_df[keep_cols]\n    grid_df.to_pickle('test_'+store_id+'.pkl')\n    del grid_df\n    \n    # Launch seeder again to make lgb training 100% deterministic\n    # with each \"code line\" np.random \"evolves\" \n    # so we need (may want) to \"reset\" it\n    seed_everything(SEED)\n    estimator = lgb.train(lgb_params,\n                          train_data,\n                          valid_sets = [valid_data],\n                          verbose_eval = 100,\n                          )\n    \n    model_name = 'lgb_model_'+store_id+'_v'+str(VER)+'.bin'\n    pickle.dump(estimator, open(model_name, 'wb'))\n\n    !rm train_data.bin\n    del train_data, valid_data, estimator\n    gc.collect()\n    \n    # \"Keep\" models features for predictions\n    MODEL_FEATURES = features_columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#size=os.path.getsize(\"../input/m5-custom-features/mean_encoding_df.pkl\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nall_preds = pd.DataFrame()\nbase_test = get_base_test()\nmain_time = time.time()\n\nfor PREDICT_DAY in range(1,29):    \n    print('Predict | Day:', PREDICT_DAY)\n    start_time = time.time()\n\n    grid_df = base_test.copy()\n    grid_df = pd.concat([grid_df, df_parallelize_run(make_lag_roll, ROLS_SPLIT)], axis=1)\n        \n    for store_id in STORES_IDS:\n        \n        model_path = 'lgb_model_'+store_id+'_v'+str(VER)+'.bin' \n        if USE_AUX:\n            model_path = AUX_MODELS + model_path\n        \n        estimator = pickle.load(open(model_path, 'rb'))\n        \n        day_mask = base_test['d']==(END_TRAIN+PREDICT_DAY)\n        store_mask = base_test['store_id']==store_id\n        \n        mask = (day_mask)&(store_mask)\n        base_test[TARGET][mask] = estimator.predict(grid_df[mask][MODEL_FEATURES])\n    \n   \n    temp_df = base_test[day_mask][['id',TARGET]]\n    temp_df.columns = ['id','F'+str(PREDICT_DAY)]\n    if 'id' in list(all_preds):\n        all_preds = all_preds.merge(temp_df, on=['id'], how='left')\n    else:\n        all_preds = temp_df.copy()\n        \n    print('#'*10, ' %0.2f min round |' % ((time.time() - start_time) / 60),\n                  ' %0.2f min total |' % ((time.time() - main_time) / 60),\n                  ' %0.2f day sales |' % (temp_df['F'+str(PREDICT_DAY)].sum()))\n    del temp_df\n    \nall_preds = all_preds.reset_index(drop=True)\nall_preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nsubmission = pd.read_csv(ORIGINAL+'sample_submission.csv')[['id']]\nsubmission = submission.merge(all_preds, on=['id'], how='left').fillna(0)\nsubmission.to_csv('submission_v'+str(VER)+'.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}