{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport plotly_express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport gc\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom lightgbm import LGBMRegressor\nimport joblib\nimport datetime as dt\n\nfrom sklearn.metrics import mean_squared_error\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt\nfrom statsmodels.tsa.arima_model import ARIMA","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Import Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"calendar = pd.read_csv('../input/m5-forecasting-accuracy/calendar.csv')\nsales_train_eval = pd.read_csv('../input/m5-forecasting-accuracy/sales_train_evaluation.csv')\nsales_train = pd.read_csv('../input/m5-forecasting-accuracy/sales_train_validation.csv')\nsample_sub = pd.read_csv('../input/m5-forecasting-accuracy/sample_submission.csv')\nsell_prices = pd.read_csv('../input/m5-forecasting-accuracy/sell_prices.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Seperate Date Variables and others \ndate_vars = sales_train.columns[6:] \nother_vars = sales_train.columns[:6]\nprint(date_vars,other_vars)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Downcast Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Downcast in order to save memory\ndef downcast(df):\n    cols = df.dtypes.index.tolist() #numbers of col: ['d_1814', 'd_1815',\n    types = df.dtypes.values.tolist() #datatype in numbers of col: [dtype('int64'), dtype('int64'), \n    #-> for every column,their datatypes are all int64\n    for i in range(len(types)): \n        if 'int' in str(types[i]):\n            if df[cols[i]].min() > np.iinfo(np.int8).min and df[cols[i]].max() < np.iinfo(np.int8).max:\n                df[cols[i]] = df[cols[i]].astype(np.int8)\n            elif df[cols[i]].min() > np.iinfo(np.int16).min and df[cols[i]].max() < np.iinfo(np.int16).max:\n                df[cols[i]] = df[cols[i]].astype(np.int16)\n            elif df[cols[i]].min() > np.iinfo(np.int32).min and df[cols[i]].max() < np.iinfo(np.int32).max:\n                df[cols[i]] = df[cols[i]].astype(np.int32)\n            else:\n                df[cols[i]] = df[cols[i]].astype(np.int64)\n                \n        elif 'float' in str(types[i]): \n            if df[cols[i]].min() > np.finfo(np.float16).min and df[cols[i]].max() < np.finfo(np.float16).max:\n                df[cols[i]] = df[cols[i]].astype(np.float16)\n            elif df[cols[i]].min() > np.finfo(np.float32).min and df[cols[i]].max() < np.finfo(np.float32).max:\n                df[cols[i]] = df[cols[i]].astype(np.float32)\n            else:\n                df[cols[i]] = df[cols[i]].astype(np.float64)\n                \n        elif types[i] == np.object: #can be dates or categories \n            if cols[i] == 'date':\n                df[cols[i]] = pd.to_datetime(df[cols[i]], format='%Y-%m-%d')\n            else:\n                df[cols[i]] = df[cols[i]].astype('category')\n    return df  \n            \n\nsales_train = downcast(sales_train)\nsell_prices = downcast(sell_prices)\ncalendar = downcast(calendar)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"enumerate: can contain both index and the values in a columns ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Unpivot the data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# sales_df = sales_train.melt(id_vars = other_vars, value_vars = date_vars, var_name = \"Date\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. ### Merge the data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Join calendar data \n\n# calendar_to_join = calendar[['date', 'd','wm_yr_wk']]\n# sales_df = pd.merge(sales_df, calendar_to_join, left_on = 'Date'  , right_on = 'd', suffixes=('_sales', '_cal')).drop(['Date'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sales_df = pd.merge(sales_df, sell_prices, on=['store_id','item_id','wm_yr_wk'], how='left') ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sales_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## EDA","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observation:\n\n* 30490 total products\n* 1913 days","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Only preserve sales for days(d_1, d_2, ...)\ndf_day = sales_train.copy()\ndf_day.drop(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], axis=1, inplace = True)\ndf_day.describe()\ndf_day.loc['Ttl_Daily_Sales'] = df_day.sum() #add up the sum of the total daily sales for all items\n\nplt.figure(figsize=(12,8))\nsns.distplot(df_day.loc[\"Ttl_Daily_Sales\"], bins=50, kde=False)\nplt.title(\"Ttl Daily Sales Histogram\", fontsize=14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#find the 10th lowest sales \ndf_day_trans = df_day.transpose()\nsmallest = df_day_trans.nsmallest(10, 'Ttl_Daily_Sales')\nsmallest['Ttl_Daily_Sales']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observation: there are some days that have extremely low sales ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### State level performance","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Row number of each state\nstate_pie = sales_train['state_id'].value_counts().rename_axis('state_name').reset_index(name='counts')\n\n#percentage of row numbers of each state\nprint(sales_train['state_id'].value_counts(normalize=True)) \n\nplt.pie(state_pie['counts'], labels= state_pie['state_name'], autopct = '%1.1f%%')\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observe monthly sales across states","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"state = sales_train.groupby('state_id', axis = 0).sum()\nstate\nstate_trans = state.transpose()\nstate_trans.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"state_trans['date'] = pd.date_range(start='1/29/2011', periods= len(state_trans), freq='D')\nstate_trans.set_index('date', drop = True, inplace = True)\nstate_trans.sort_index(inplace=True)\nstate_month = state_trans.groupby(pd.Grouper(freq='1M')).sum()\nstate_month.head(20)\nstate_month.plot(title = \"Monthly Sales accross States\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Method 2\n\nstate_trans['date'] = pd.date_range(start='1/29/2011', periods= len(state_trans), freq='D')\nstate_trans.set_index('date', drop = True, inplace = True)\nstate_trans.sort_index(inplace=True)\nstate_trans.index.to_period(\"M\")\nstate_month.plot(title = \"Monthly Sales accross States\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Store level performance","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Sales of each store\nstore_pie = sales_train.groupby('store_id').sum().T\nstore_pie.loc['ttl_sales'] = store_pie.sum()\n\nplt.pie(store_pie.loc['ttl_sales'],labels = ['CA_1', 'CA_2', 'CA_3', 'CA_4', 'TX_1', 'TX_2', 'TX_3', 'WI_1', 'WI_2',\n       'WI_3'], autopct = '%1.1f%%')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"store = sales_train.groupby('store_id', axis = 0).sum().reset_index().set_index('store_id').T\nstore['date'] = pd.date_range(start='1/29/2011', periods= len(store), freq='D')\nstore.set_index('date', drop = True, inplace = True)\nstore.sort_index(inplace=True)\nstore_month = store.groupby(pd.Grouper(freq = '1M')).sum()\nstore_month.head(20)\nstore_month.plot(title = \"Monthly Sales accross Store\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Seasonality ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"series = state_month['CA']\nresult = seasonal_decompose(series, model='additive')\nfig, axes = plt.subplots(ncols=1, nrows=4, sharex=True, figsize=(10,8))\nprint(result.trend.plot(ax=axes[0]))\nprint(result.seasonal.plot(ax=axes[1]))\nprint(result.resid.plot(ax=axes[2]))\nprint(result.observed.plot(ax=axes[3]))\naxes[0].set_ylabel('trend')\naxes[1].set_ylabel('seasonal')\naxes[2].set_ylabel('resid')\naxes[3].set_ylabel('observed')\n\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Seperate training and validation dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset = sales_train[date_vars[-100: -30]]\nval_dataset = sales_train_eval[date_vars[-30:]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train_eval.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_dataset.columns)\nprint(val_dataset.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = make_subplots(rows = 2, cols = 1)\n\n\n#first product sales from 1814 - 1883\nfig.add_trace(\n    go.Scatter(x=np.arange(70), mode='lines', y=train_dataset.loc[0].values,  marker=dict(color=\"dodgerblue\"),showlegend=False, \n               name=\"Original signal\"),\n    row=1, col=1\n)\n\n#first product sales from 1884 - 1913\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=val_dataset.loc[0].values, mode='lines', marker=dict(color=\"darkorange\"), showlegend=False,\n               name=\"Denoised signal\"),\n    row=1, col=1\n)\n\n\n\n#second product sales from 1814 - 1883\nfig.add_trace(\n    go.Scatter(x=np.arange(70), mode='lines', y=train_dataset.loc[1].values,  marker=dict(color=\"dodgerblue\"),showlegend=False, \n               name=\"Original signal\"),\n    row=2, col=1\n)\n\n#second product sales from 1884 - 1913\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=val_dataset.loc[1].values, mode='lines', marker=dict(color=\"darkorange\"), showlegend=False,\n               name=\"Denoised signal\"),\n    row=2, col=1\n)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Moving average prediction","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def moving_average(days, train_dataset):\n    predictions = []\n    for i in range(days):\n        if i == 0:\n            predictions.append(np.mean(train_dataset[train_dataset.columns[-28:]].values, axis=1))\n            #when i is 0, average the 30 days sales for every product respectively\n\n        elif i < 28 and i > 0:\n            predictions.append(\n                            (np.sum(train_dataset[train_dataset.columns[-28+i:]].values,axis =1) \n                          + np.sum(predictions[:i], axis=0)) /28  )\n                                \n            #if i is i, we calculate the latest 29 days of average sales, and average it with predictions \n        elif i >= 28:\n            predictions.append(np.mean([predictions[i-28:i]], axis=0))\n        \n    predictions_array = np.transpose(np.array([row.tolist() for row in predictions]))\n    \n    return predictions_array","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# y_1 = val_dataset[val_dataset.columns[-30:]]\n# RMSE = mean_squared_error(y_1, predictions_1)\n# RMSE","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Try out first submission ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Validation dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"validation = sales_train[date_vars[-28:]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_val = moving_average(28, validation)\nsample_sub.iloc[30490:,1:] = pred_val","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_sub","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Validation dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluation = sales_train_eval[sales_train_eval.columns[-28:]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_eval = moving_average(28, evaluation)\nsample_sub.iloc[:30490,1:] = pred_eval","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_sub","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"put the prediction in sample_sub","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"filename = 'M5_2.csv'\n\nsample_sub.to_csv(filename,index=False)\n\nprint('Saved file: ' + filename)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_sub","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'M5_2.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Explonential smoothing model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def Explo_smoothing(train_data):\n    exp_predictions = []\n    for rows in range(len(train_data)): \n        fit1 = ExponentialSmoothing(train_data.iloc[rows].values, seasonal_periods=28).fit(smoothing_level = 0.2)\n        exp_predictions.append(fit1.forecast(28))\n    return exp_predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_exp = Explo_smoothing(validation)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_evl_exp = Explo_smoothing(evaluation)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_sub.iloc[:30490,1:] = pred_evl_exp\nsample_sub.iloc[30490:,1:] = pred_exp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_sub","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"filename = 'M5_3.csv'\n\nsample_sub.to_csv(filename,index=False)\n\nprint('Saved file: ' + filename)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'M5_3.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}