{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib as plt\nimport gc\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"dates = pd.read_csv(\"/kaggle/input/m5-forecasting-accuracy/calendar.csv\")\ndata = pd.read_csv(\"/kaggle/input/m5-forecasting-accuracy/sales_train_validation.csv\")\nsale_data = pd.read_csv(\"/kaggle/input/m5-forecasting-accuracy/sell_prices.csv\")\nsubmission = pd.read_csv(\"/kaggle/input/m5-forecasting-accuracy/sample_submission.csv\")\n\nprint(dates.head())\nprint(data.head())\nprint(sale_data.head())\nprint(submission.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# First we want to see how sales are changing over time\nThis shows that sales are growing over time, but there also looks like there is some cyclic increase and decrease.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"last_date = 1913\n\noriginal_features = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']\n\ntotal_sales = data.mean()\nx_data = np.array([int(x[2:]) for x in total_sales.index])\ny_data = np.array(total_sales.array)\nm, b = np.polyfit(x_data, y_data, 1)\n\nplt.pyplot.scatter(x_data, total_sales, s=1)\nplt.pyplot.plot([0, last_date], [b, m*last_date + b], linewidth=3)\nprint(\"Gradient: \", m, \"Intercept:\", b)\n\nplt.pyplot.figure()\nplt.pyplot.scatter(x_data[400:1250], total_sales.iloc[400:1250], s=5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Seeing how this is per state\nThe data shows that most sales are highly similar.\n* CA the highest sales.\n* TX had the lowest sales, and slowest growth.\n* WI has the fastest growing sales.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"state_groups = data.groupby('state_id')\nstate_data = state_groups.mean()\nprint(state_data)\nx_data = range(1, last_date+1)\n\nfor i, g in enumerate(state_groups.groups.keys()):\n    y_data = [state_data['d_' + str(x)].iloc[i] for x in x_data]\n    plt.pyplot.scatter(x_data, y_data, s=5, alpha=0.3)\n    m, b = np.polyfit(x_data, y_data, 1)\n    print(\"Group \" + str(i) + \":\", g, \"Gradient:\", m, \"Intercept:\", b)\n    plt.pyplot.plot([0, last_date], [b, m*last_date + b], linewidth=4, label=g)\nplt.pyplot.legend(loc=\"upper left\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Now we look at category\nThe data shows that categorical spending in food is drastically different. Hobbies and household sales are similar.\n* FOOD the highest sales.\n* HOBBIES had the lowest sales, and slowest growth.\n* HOUSEHOLD has the fastest growing sales (barely).\n\nThere isn't much significance found in breaking down categories by state.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"category_groups = data.groupby('cat_id')\ncategory_data = category_groups.mean()\n\nx_data = range(1, last_date+1)\n\nfor i, c in enumerate(category_groups.groups.keys()):\n    y_data = [category_data['d_' + str(x)].iloc[i] for x in x_data]\n    plt.pyplot.scatter(x_data, y_data, s=5, alpha=0.3)\n    m, b = np.polyfit(x_data, y_data, 1)\n    print(\"Category \", c, \"Gradient:\", m, \"Intercept:\", b)\n    plt.pyplot.plot([0, last_date], [b, m*last_date + b], linewidth=4, label=c)\nplt.pyplot.legend(loc=\"upper left\")\n\ncs_group = data.groupby(['state_id','cat_id']).mean().reset_index()\ncolours = ['blue', 'darkorange', 'green']\nfor state in state_groups.groups.keys():\n    plt.pyplot.figure(figsize=(20, 12))\n    for i, c in enumerate(category_groups.groups.keys()):\n        plt.pyplot.subplot(int('33' + str(i+1)))\n        y_data = cs_group[(cs_group.state_id == state) & (cs_group.cat_id == c)].iloc[0].tail(last_date).to_list()\n        plt.pyplot.scatter(x_data, y_data, s=5, alpha=0.3, c=colours[i])\n        \n        m, b = np.polyfit(x_data, y_data, 1)\n        print(\"State: \" + state, \"Category: \" + c, \"Gradient:\", m, \"Intercept:\", b)\n        plt.pyplot.plot([0, last_date], [b, m*last_date + b], linewidth=4, label=c, c=colours[i])\n        plt.pyplot.legend(loc=\"upper left\")\n        plt.pyplot.title(state)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Breakdown by store\n* Stores are clearly very different.\n* Unable to ignore stores\n* **Something weird happened after day 500 for WI_1 and WI_2**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ss_group = data.groupby(['state_id','store_id']).mean().reset_index()\n\nfor state in state_groups.groups.keys():\n    store_group = ss_group[ss_group.state_id == state].groupby('store_id')\n    plt.pyplot.figure(figsize=(16, 4))\n    for i, s in enumerate(store_group.groups.keys()):\n        y_data = ss_group[(ss_group.store_id == s)].iloc[0].tail(last_date).to_list()\n        plt.pyplot.scatter(x_data, y_data, s=6, alpha=0.4)\n        \n        m, b = np.polyfit(x_data, y_data, 1)\n        print(\"State: \" + state, \"Store: \" + s, \"Gradient:\", m, \"Intercept:\", b)\n        plt.pyplot.plot([0, last_date], [b, m*last_date + b], linewidth=4, label=s)\n        plt.pyplot.legend(loc=\"upper left\")\n        plt.pyplot.title(state)\n\n# Displaying anomalies\nplt.pyplot.figure(figsize=(16, 4))\nplt.pyplot.subplot(121)\ny_data = ss_group[(ss_group.store_id == 'WI_1')].iloc[0].tail(last_date).to_list()\nplt.pyplot.scatter(x_data, y_data, s=6, alpha=0.4, label='WI_1')\nplt.pyplot.legend(loc=\"upper left\")\nplt.pyplot.title(\"WI_1 Anomaly at day 700\")\n\nplt.pyplot.subplot(122)\ny_data = ss_group[(ss_group.store_id == 'WI_2')].iloc[0].tail(last_date).to_list()\nplt.pyplot.scatter(x_data, y_data, s=6, alpha=0.4, label='WI_2',c='darkorange')\nplt.pyplot.legend(loc=\"upper left\")\nplt.pyplot.title(\"WI_2 Anomaly at day 500\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Category by store\n* All stores are different enough to justify adding storeId as a parameter. \n* This should mean state_id doesn't really need to affect the model.\n* Anomaly in CA_2 detected too","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"store_group = data.groupby('store_id')\nsc_group = data.groupby(['store_id', 'cat_id']).mean().reset_index()\n\nprev_state = ''\nstate_count = 1\nfor store in store_group.groups.keys():\n    if (prev_state != store[0:2]):\n            plt.pyplot.figure(figsize=(16, 8))\n            prev_state = store[0:2]\n            state_count = 1\n    plt.pyplot.subplot(int('22' + str(state_count)))\n    state_count += 1\n    for i, c in enumerate(category_groups.groups.keys()):\n        \n        y_data = sc_group[(sc_group.store_id == store) & (sc_group.cat_id == c)].iloc[0].tail(last_date).to_list()\n        plt.pyplot.scatter(x_data, y_data, s=6, alpha=0.4)\n        \n        m, b = np.polyfit(x_data, y_data, 1)\n        plt.pyplot.plot([0, last_date], [b, m*last_date + b], linewidth=4, label=c)\n        plt.pyplot.legend(loc=\"upper left\")\n        plt.pyplot.title(store)\n        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Anomalies\n* WI_1 in FOODS\n* WI_2 in FOODS\n* WI_2 in HOUSEHOLD\n* CA_2 in FOODS","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.pyplot.figure(figsize=(16, 8))\nplt.pyplot.subplot(221)\n\ny_data = sc_group[(sc_group.store_id == 'WI_1') & (sc_group.cat_id == 'FOODS')].iloc[0].tail(last_date-700).to_list()\nplt.pyplot.scatter(range(700, last_date), y_data, s=6, alpha=0.4, label='FOODS')\nplt.pyplot.legend(loc=\"upper left\")\nplt.pyplot.title(\"WI_1 Anomaly at day 700\")\n\nplt.pyplot.subplot(222)\ny_data = sc_group[(sc_group.store_id == 'WI_2') & (sc_group.cat_id == 'FOODS')].iloc[0].tail(last_date-550).to_list()\nplt.pyplot.scatter(range(550, last_date), y_data, s=6, alpha=0.4, label='FOODS')\nplt.pyplot.legend(loc=\"upper left\")\nplt.pyplot.title(\"WI_2 Anomaly at day 500\")\n\nplt.pyplot.subplot(223)\ny_data = sc_group[(sc_group.store_id == 'WI_2') & (sc_group.cat_id == 'HOUSEHOLD')].iloc[0].tail(last_date-550).to_list()\nplt.pyplot.scatter(range(550, last_date), y_data, s=6, alpha=0.4, label='HOUSEHOLD')\nplt.pyplot.legend(loc=\"upper left\")\nplt.pyplot.title(\"WI_2 Anomaly at day 500\")\n\nplt.pyplot.subplot(224)\ny_data = sc_group[(sc_group.store_id == 'CA_2') & (sc_group.cat_id == 'FOODS')].iloc[0].tail(last_date-1600).to_list()\nplt.pyplot.scatter(range(1600, last_date), y_data, s=6, alpha=0.4, label='FOODS')\nplt.pyplot.legend(loc=\"upper left\")\nplt.pyplot.title(\"CA_2 Anomaly at day 1600\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Differences in day of the week\n* Generally Sat or Sun has the most sales. Rarely Fridays.\n* Generally it lowers throughout the middle of the week.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Don't warn us for too many figures\nplt.rcParams.update({'figure.max_open_warning': 0})\nstore_group = data.groupby('store_id')\nsc_group = data.groupby(['store_id', 'cat_id']).mean().reset_index()\n\nprev_state = ''\nstate_count = 1\ndays = ['Sat', 'Sun', 'Mon', 'Tue', 'Wed', 'Thu', 'Fri']\nfor store in store_group.groups.keys():\n    if (prev_state != store[0:2]):\n            prev_state = store[0:2]\n            state_count = 1\n    state_count += 1\n    for i, c in enumerate(category_groups.groups.keys()):\n        y = pd.DataFrame()\n        y_data = pd.Series(sc_group[(sc_group.store_id == store) & (sc_group.cat_id == c)].iloc[0].tail(last_date).to_list())\n        for j in range(0, len(days)):\n            y[days[j]] = y_data[y_data.index % len(days) == j].reset_index(drop=True)\n        y.plot.box()\n        plt.pyplot.title(store + \" - \" + c)\n        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Differences in months\nNo significant trend shown for month of the year or day of the month","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"m_data = total_sales.tail(last_date)\nmdates = dates.set_index('d')\nmonth_alloc = pd.concat([pd.Series([mdates.loc[d, 'month'] for d in total_sales.index]), m_data.reset_index(drop=True)], axis=1)\nmonth_grp = month_alloc.groupby(0)\n\nplt.pyplot.figure(figsize=(12,4))\nplt.pyplot.scatter(month_grp.groups.keys(), month_grp.mean(), label = \"mean\")\nplt.pyplot.scatter(month_grp.groups.keys(), month_grp.median(), label = \"median\")\nplt.pyplot.scatter(month_grp.groups.keys(), month_grp.max(), label = \"max\")\nplt.pyplot.scatter(month_grp.groups.keys(), month_grp.min(), label = \"min\")\nplt.pyplot.title(\"Mean sales per month\")\nplt.pyplot.legend(loc=\"upper left\")\nmonth_alloc = pd.concat([pd.Series([mdates.loc[d, 'date'][-2:] for d in total_sales.index]), m_data.reset_index(drop=True)], axis=1)\nmonth_grp = month_alloc.groupby(0)\n\nplt.pyplot.figure(figsize=(12,4))\nplt.pyplot.scatter(month_grp.groups.keys(), month_grp.mean(), label = \"mean\")\nplt.pyplot.scatter(month_grp.groups.keys(), month_grp.median(), label = \"median\")\nplt.pyplot.scatter(month_grp.groups.keys(), month_grp.max(), label = \"max\")\nplt.pyplot.scatter(month_grp.groups.keys(), month_grp.min(), label = \"min\")\nplt.pyplot.title(\"Mean sales per month day\")\nplt.pyplot.legend(loc=\"upper left\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_data = category_groups.mean()\nfor cat in category_groups.groups.keys():   \n    m_data = cat_data.loc[cat].tail(last_date)\n    mdates = dates.set_index('d')\n    month_alloc = pd.concat([pd.Series([mdates.loc[d, 'month'] for d in total_sales.index]), m_data.reset_index(drop=True)], axis=1)\n    month_grp = month_alloc.groupby(0)\n\n    plt.pyplot.figure(figsize=(16,4))\n    plt.pyplot.subplot(121)\n    plt.pyplot.scatter(month_grp.groups.keys(), month_grp.mean(), label=\"mean\")\n    plt.pyplot.scatter(month_grp.groups.keys(), month_grp.median(), label=\"median\")\n    plt.pyplot.scatter(month_grp.groups.keys(), month_grp.max(), label=\"max\")\n    plt.pyplot.scatter(month_grp.groups.keys(), month_grp.min(), label=\"min\")\n    plt.pyplot.title(\"Sales per month for \" + cat)\n    plt.pyplot.legend(loc=\"upper left\")\n\n    month_alloc = pd.concat([pd.Series([mdates.loc[d, 'date'][-2:] for d in total_sales.index]), m_data.reset_index(drop=True)], axis=1)\n    month_grp = month_alloc.groupby(0)\n\n    plt.pyplot.subplot(122)\n    plt.pyplot.title(\"Sales per month day \" + cat)\n    plt.pyplot.scatter(month_grp.groups.keys(), month_grp.mean(), label=\"mean\")\n    plt.pyplot.scatter(month_grp.groups.keys(), month_grp.median(), label=\"median\")\n    plt.pyplot.scatter(month_grp.groups.keys(), month_grp.max(), label=\"max\")\n    plt.pyplot.scatter(month_grp.groups.keys(), month_grp.min(), label=\"min\")\n    plt.pyplot.title(\"Sales per month day \" + cat)\n    plt.pyplot.legend(loc=\"upper left\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Let's just see how a few products behave\nThis has an example of:\n1. Frequently purchased items\n2. Rarely purchased items\n3. Items with anomalies","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"p1 = data[data.id == 'FOODS_2_001_CA_3_validation'].transpose().tail(98).reset_index(drop=True)\nfig = plt.pyplot.figure(figsize=(16, 4))\nax = fig.gca()\nax.set_xticks(range(0, 98, 7))\nplt.pyplot.scatter(p1.index, p1)\n\np2 = data[data.id == 'HOBBIES_2_100_WI_3_validation'].transpose().tail(98).reset_index(drop=True)\nfig = plt.pyplot.figure(figsize=(16, 4))\nax = fig.gca()\nax.set_xticks(range(0, 98, 7))\nplt.pyplot.scatter(p2.index, p2)\n\np3 = data[data.id == 'HOUSEHOLD_1_100_TX_2_validation'].transpose().tail(175).reset_index(drop=True)\nfig = plt.pyplot.figure(figsize=(16, 4))\nax = fig.gca()\nax.set_xticks(range(0, 175, 7))\nplt.pyplot.scatter(p3.index, p3)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Now with sales price","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"wk_map = {}\nfor day in data.columns:\n    if day.startswith('d_'):\n        wk_map[day] = dates[dates.d == day].wm_yr_wk.iloc[0]\n        wk_map[int(day[2:])] = wk_map[day]\n\nfor day in range(last_date, last_date+48):\n    wk_map[day] = dates[dates.d == 'd_'+str(day)].wm_yr_wk.iloc[0]\n\nmean_price_lookup = sale_data.groupby(['item_id', 'store_id']).mean().sell_price\nsales_lookup = sale_data.set_index(['item_id', 'store_id', 'wm_yr_wk']).sort_index()\n\nprint()\nx_data = range(0, last_date)\nys = [sales_lookup.loc['HOBBIES_1_001','CA_1', wk_map[x+1]] if (('HOBBIES_1_001','CA_1', wk_map[x+1]) in sales_lookup.index) else 2.5 for x in x_data]\n\ny = data[data.id == 'HOBBIES_1_001_CA_1_validation'].iloc[0].tail(-6)\nplt.pyplot.figure(figsize=(12,4))\nplt.pyplot.subplot(121)\nplt.pyplot.scatter(x_data, y)\nplt.pyplot.subplot(122)\nplt.pyplot.scatter(x_data, ys)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creating a model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"To create this model, a basic model can be made by training each `id` in combination with `weekday`.  \nWe must also account for the anomalies so some data cleanup must be performed.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_int = 28\nlast_date = 1913\ntrain_data = data.rename(mapper=(lambda x: int(x[2:]) if x[0:2] == 'd_' else x),axis='columns').iloc[:,0:-pred_int]\ndef evaluate(preds, data):\n    def calc_err(row):\n        item_id = row.iloc[0]\n        used_data = row.tail(last_date)\n        actual = used_data.tail(pred_int).to_list()\n        \n        hist_err = used_data.head(-pred_int).diff().tail(-1).pow(2).sum()\n        pred = preds[preds.id == item_id].iloc[0, :].tail(-1).to_list()\n        pred_err = sum([(pred[i] - actual[i])**2 for i in range(0,pred_int)])\n        \n        den = hist_err/(last_date-pred_int-1)\n        rmsse = np.sqrt((1/pred_int)*pred_err/den)\n        if (int(row.name) % 305 == 0):\n            print('.',end='')\n        return [item_id, rmsse]\n    print('.'*25)\n    print('.'*50)\n    print('.'*100)\n    result = data.apply(calc_err, result_type='expand',axis=1).rename(columns={0: 'id', 1: 'err'})\n    print()\n    print(\"Mean error: \", result.err.mean())\n    print(\"Median error: \", result.err.median())\n    print(\"Largest errors:\")\n    print(result.nlargest(5, 'err'))\n    return result\n\n# evaluate(submission, data)\ntrain_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = 6\ndef train(estimator, data):\n    print(\"*\"*100)\n    res = data.apply(estimator, axis=1, result_type='expand') \\\n            .rename(mapper=(lambda n: 'F' + str(n+1)), axis=1)\n    print()\n    formatted = pd.concat([data.id, res], axis=1)\n    return formatted\n\ndef basic_model(row):\n    return [np.round(row.tail(-features).mean())]*pred_int\n\n# preds = train(basic_model, train_data)\n# res = evaluate(preds, data)\n# res","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Create linear model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n\ndef simple_linear_model(row):\n    data = row.tail(-features)\n    rid = row.iloc[0]\n    \n    days = data.index.to_list()\n    wkday1 = [np.sin(2*np.pi*x/7) for x in days]\n    wkday2 = [np.cos(2*np.pi*x/7) for x in days]\n    wkday3 = [np.sin(2*np.pi*x/14) for x in days]\n    wkday4 = [np.cos(2*np.pi*x/14) for x in days]\n    wkday5 = [np.sin(2*np.pi*x/3.5) for x in days]\n    wkday6 = [np.cos(2*np.pi*x/3.5) for x in days]\n#     wkday5 = [np.sin(2*np.pi*x/14) for x in days]\n#     wkday4 = [np.cos(2*np.pi*x/14) for x in days]\n\n    X_train = np.column_stack((wkday1, wkday2))\n    Y = data\n    regressor = LinearRegression().fit(X_train, Y)\n    \n    plt.pyplot.figure(figsize=(24, 4))\n    plt.pyplot.plot(days, Y)\n    plt.pyplot.plot(days, regressor.predict(X_train))\n    \n#     days = range(days[-1]+1, days[-1]+pred_int+1)\n    wkday1 = [np.sin(2*np.pi*x/7) for x in days]\n    wkday2 = [np.cos(2*np.pi*x/7) for x in days]\n    X_pred = np.column_stack((wkday1, wkday2))\n    print(regressor.intercept_)\n    reg2 = LinearRegression().fit(np.column_stack((wkday1, wkday2)), Y - regressor.intercept_)\n    \n    plt.pyplot.figure(figsize=(24, 4))\n    plt.pyplot.plot(days, Y - regressor.intercept_)\n    plt.pyplot.plot(days, regressor.predict(np.column_stack((wkday1, wkday2))))\n    return regressor.predict(X_pred)\n   \n# preds = train(simple_linear_model, train_data)\n# preds\nx = simple_linear_model(train_data.iloc[0, -300:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def linear_model(row):\n    data = row.tail(-features)\n    rid = row.iloc[0]\n    item_id = row.iloc[1]\n    store_id = row.iloc[4]\n    days = len(data)\n    \n    if (rid.startswith('HOUSEHOLD') and rid.endswith('WI_2_validation')):\n        data = data.tail(-700)\n    elif (rid.startswith('FOODS') and rid.endswith('WI_2_validation')):\n        data = data.tail(-500)\n    elif (rid.startswith('FOODS') and rid.endswith('WI_1_validation')):\n        data = data.tail(-500)\n    elif (rid.startswith('FOODS') and rid.endswith('CA_2_validation')):\n        data = data.tail(320)\n\n    changes = (data.diff(1) != 0).astype('int').cumsum()\n    zeroes = changes.groupby(changes).size()\n    zgroup = zeroes.where(zeroes > 28).dropna().index.to_list()\n\n    points = changes[~changes.isin(zgroup)].index.to_list()\n    # If less than 3 weeks of data to use OR last data points were more than 3 weeks ago.\n    if (len(points) < 21 or points[-1] <= days - 21):\n        if (len(points) < 2):\n            return row.tail(14).to_list()*(pred_int//14)\n        if (points[-1] > days - 21):\n            first_used_date = [x for x in points if x > days-21][0]\n            return [row.tail(-first_used_date+1).mean()]*pred_int\n        else:\n            return row.tail(14).to_list()*(pred_int//14)\n    \n    maxp = mean_price_lookup.loc[item_id, store_id]*1.75\n    prices = [sales_lookup.loc[(wk_map[p+1], item_id, store_id)] if (wk_map[p+1], item_id, store_id) in sales_lookup.index else maxp for p in points]\n    wkday1 = [np.sin(2*np.pi*x/7) for x in points]\n    wkday2 = [np.cos(2*np.pi*x/7) for x in points]\n    \n    X_train = np.column_stack((points, wkday1, wkday2, prices))\n    Y = data[points]\n    regressor = LinearRegression().fit(X_train, Y)\n    \n    test_points = range(points[-1]+1, points[-1]+pred_int+1)\n    test_prices = [sales_lookup.loc[(wk_map[p+1], item_id, store_id)] if (wk_map[p+1], item_id, store_id) in sales_lookup.index else maxp for p in test_points]\n    test_wkday1 = [np.sin(2*np.pi*x/7) for x in test_points]\n    test_wkday2 = [np.cos(2*np.pi*x/7) for x in test_points]\n    X_pred = np.column_stack((test_points, test_wkday1, test_wkday2, test_prices))\n    \n    if (int(row.name) % 305 == 0):\n        print(\"*\", end='')\n    return regressor.predict(X_pred)\n\n# preds = train(linear_model, train_data)\n# print()\n# print(train_data)\n# res = evaluate(preds, data)\n# res","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# This definitely did not do what I wanted it to do:\nHere we learn to **ALWAYS** visualise predictions when possible","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x = simple_linear_model(train_data.iloc[0, -300:])\n# plt.pyplot.plot(range(0,28), data.iloc[5,-28:])\n# plt.pyplot.plot(range(0,28), preds.iloc[5,-28:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num = 200\nanomalies = ['HOUSEHOLD_1_032_TX_1_validation', 'HOUSEHOLD_1_020_CA_3_validation', 'HOUSEHOLD_1_020_CA_3_validation', 'FOODS_3_827_CA_4_validation', 'HOUSEHOLD_1_400_CA_4_validation']\nfor rid in anomalies:\n    plt.pyplot.figure(figsize=(16, 4))\n    p = train_data[train_data.id == rid].iloc[0].tail(num)\n    p2 = data[data.id == rid].iloc[0].tail(num)\n    plt.pyplot.subplot(121)\n    plt.pyplot.scatter(p.index.to_list(), p)\n    plt.pyplot.subplot(122)\n    plt.pyplot.scatter(p2.index.to_list(), p2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Using Autocorrelation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x_corr = range(0, data.iloc[0, -28:].size)\nplt.pyplot.bar(x_corr, [data.iloc[0,features:].astype(float).autocorr(lag=i) for i in x_corr])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tsa.statespace.sarimax import SARIMAX\ndef sarimax_model(row):\n    data = row.tail(-features)\n    rid = row.iloc[0]\n    model = SARIMAX(data.astype(float), order=(1, 1, 1), seasonal_order=(1, 1, 0, 8))\n    result = model.fit()\n    if (int(row.name) % 305 == 0):\n        print(\"*\", end='')\n    return result.forecast(28)\n\ndef test_sarimax_model(row):\n    data = row.tail(-features)\n    rid = row.iloc[0]\n    model = SARIMAX(data.astype(float), order=(1, 1, 1), seasonal_order=(1, 1, 0, 8))\n    result = model.fit()\n    return (result.fittedvalues, result)\n\npoints = train_data.iloc[0, -300:].reset_index(drop=True)\nresult, model = test_sarimax_model(points)\nfcast = model.forecast(steps=28)\nplt.pyplot.figure(figsize=(24,4))\nplt.pyplot.plot(result.index.to_list(), result)\nplt.pyplot.plot(points.index.to_list(), points)\nplt.pyplot.plot(fcast.index.to_list(), fcast)\nactual = data.iloc[0,-28:].reset_index(drop=True)\n\nprint(\"Test errors: \")\nprint([sum([(result.loc[i] - points.loc[i-1])**2 for i in range(9,300)]), sum([(points.loc[i] - points.mean())**2 for i in range(9,300)])])\nprint(\"Forecast errors\")\nprint([sum([(actual[i] - fcast.reset_index(drop=True)[i])**2 for i in range(0, 28)]), sum([(actual[i] - points.mean())**2 for i in range(0, 28)])])\n\n# predictions = train(sarimax_model, train_data)\n# THIS is wayyyy too slow.\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Try lightGBM model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"del category_groups\ndel category_data\ndel sc_group\ndel state_groups\ndel ss_group\ndel sales_lookup\ndel month_alloc\ndel mean_price_lookup\ndel wk_map\ndel total_sales\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgb\nfrom sklearn.metrics import mean_squared_error \n\ncols = ['id', 'store_id', 'item_id'] + ['d_' + str(day) for day in range(1, 1914)]\ndef get_prediction_subset(dataset):\n    cols = ['id', 'store_id', 'item_id'] + ['d_' + str(day) for day in range(1, 1914)]\n    dataset = pd.melt(dataset, id_vars=['id', 'store_id', 'item_id'], value_name='demand', var_name='day')\n    dataset = pd.merge(dataset, dates[['d', 'wm_yr_wk', 'wday']], how = 'left', left_on = ['day'], right_on = ['d'])\n    dataset = pd.merge(dataset, sale_data[['store_id', 'item_id', 'wm_yr_wk', 'sell_price']], how = 'left', on=['store_id', 'item_id', 'wm_yr_wk'])\n    dataset.drop(['store_id', 'item_id', 'd'], axis=1, inplace=True)\n    dataset['day'] = dataset['day'].map(lambda x: int(x[2:]))\n    dataset['id'] = dataset['id'].astype('category')\n    print(dataset)\n    \n    x_train = dataset[dataset['day'] <= 1885][['id', 'day', 'wm_yr_wk', 'wday', 'sell_price', 'demand']]\n    y_train = x_train['demand']\n    x_train.drop('demand', axis=1, inplace=True)\n    x_val = dataset[(dataset['day'] > 1885)][['id', 'day', 'wm_yr_wk', 'wday', 'sell_price', 'demand']]\n    y_val = x_val['demand']\n    x_val.drop('demand', axis=1, inplace=True)\n    \n    train_set = lgb.Dataset(x_train, y_train, categorical_feature=['id', 'wday'])\n    val_set = lgb.Dataset(x_val, y_val, categorical_feature=['id', 'wday'])\n    \n    params = {\n        'boosting_type': 'gbdt',\n        'metric': 'rmse',\n        'objective': 'regression',\n        'n_jobs': -1,\n        'seed': 236,\n        'learning_rate': 0.1,\n        'bagging_fraction': 0.75,\n        'bagging_freq': 10, \n        'colsample_bytree': 0.75 \\\n    }\n    model = lgb.train(params, \n                      train_set, \n                      num_boost_round = 2500, \n                      early_stopping_rounds = 50, \n                      valid_sets = [train_set, val_set], \n                      verbose_eval = 100)\n    val_pred = model.predict(x_val)\n    val_score = np.sqrt(mean_squared_error(val_pred, y_val))\n    print(val_score)\n    \n    x_val['preds'] = pd.Series(val_pred, index=x_val.index)\n    preds = x_val[['id', 'preds']]\n    \n    result = pd.DataFrame(x_val.groupby('id')\\\n        .apply(lambda x: x.preds.to_list())).reset_index(level=0)\\\n        .apply(lambda x: [x.iloc[0]] + x.iloc[1], axis=1, result_type='expand')\\\n        .rename(mapper=lambda x: 'F'+str(x) if x > 0 else 'id', axis=1)\n    \n    return result\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result = get_prediction_subset(data[cols].iloc[0:10000, :])\nfor low, high in [(10000, 17000), (17000, 24000), (24000, 30490)]:\n    result = pd.concat([result, get_prediction_subset(data[cols].iloc[low:high, :])], ignore_index=True)\nresult","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result.to_csv('lgbm.csv', index=False)\nresult","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final = train(linear_model, data.rename(mapper=(lambda x: int(x[2:]) if x[0:2] == 'd_' else x),axis='columns'))\nfinal_sub = pd.concat([final, submission.tail(30490)])\nfinal_sub.to_csv('linear2.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_sub","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}