{"cells":[{"metadata":{},"cell_type":"markdown","source":"# M5 Forecasting Challenge\n"},{"metadata":{},"cell_type":"markdown","source":"## 1. Introduction\n\nThe goal of this notebook is to give a brief overview of M5 Forecasting competition.\n\n> Note: This is one of the two complementary competitions that together comprise the M5 forecasting challenge.\n\n1. **This Competition:** The objective of this competition is to estimate as precisely as possible, the point forecasts of the unit sales of various products sold in the USA by Walmart? \n\n    * Metric Used for evalueation: **Weighted Root Mean Squared Scaled Error** (RMSSE)\n\n\n2. **Second Competition:** The objective of this competition in to estimate the uncertainty distribution of the realized values of the above competition.\n\n    * Metric Used for evalueation: **Weighted Scaled Pinball Loss** (WSPL)"},{"metadata":{},"cell_type":"markdown","source":"## 2. Data Overview\n\nIn the challenge, you are predicting item sales at stores in various locations for two 28-day time periods. Information about the data is found in the M5 Participants Guide.\n\n### Files\n* `calendar.csv` - Contains information about the dates on which the products are sold.\n* `sales_train_validation.csv` - Contains the historical daily unit sales data per product and store [d_1 - d_1913]\n* `sample_submission.csv` - The correct format for submissions. Reference the Evaluation tab for more info.\n* `sell_prices.csv` - Contains information about the price of the products sold per store and date.\n\n> Note: `sales_train_evaluation.csv` not available yet\n* `sales_train_evaluation.csv` - Available once month before competition deadline. Will include sales [d_1 - d_1941]\n\n\nhttps://storage.googleapis.com/kaggle-forum-message-attachments/772349/15032/M5-Competitors-Guide-Final-10-March-2020.pdf"},{"metadata":{},"cell_type":"markdown","source":"## 3. Peek of the input data folder"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 4 data files available for now in this competition"},{"metadata":{},"cell_type":"markdown","source":"## 4. Importing important packages and libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom pandas_profiling import ProfileReport\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom itertools import cycle\npd.set_option('max_columns', 50)\nplt.style.use('bmh')\ncolor_pal = plt.rcParams['axes.prop_cycle'].by_key()['color']\ncolor_cycle = cycle(plt.rcParams['axes.prop_cycle'].by_key()['color'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5. Loading Data"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"INPUT_DIR_PATH = '../input/m5-forecasting-accuracy/'\n\nsell_prices_df = pd.read_csv(INPUT_DIR_PATH + 'sell_prices.csv')\ncalendar_df = pd.read_csv(INPUT_DIR_PATH + 'calendar.csv')\nsales_train_validation_df = pd.read_csv(INPUT_DIR_PATH + 'sales_train_validation.csv')\nsubmission_df = pd.read_csv(INPUT_DIR_PATH + 'sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6. Peek of the data\n\n### 6.1. Shape of data files"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Shape of sell_prices_df is: {sell_prices_df.shape}')\nprint(f'Shape of calendar_df is: {calendar_df.shape}')\nprint(f'Shape of sales_train_validation_df is: {sales_train_validation_df.shape}')\nprint(f'Shape of submission_df is: {submission_df.shape}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 6.2. Head of data files\n\n* **sell_prices_df**"},{"metadata":{"trusted":true},"cell_type":"code","source":"sell_prices_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Note: sell_prices_df contains information about the price of the products sold per store and date."},{"metadata":{},"cell_type":"markdown","source":"* **calendar_df**"},{"metadata":{"trusted":true},"cell_type":"code","source":"calendar_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Note: calender_df contains information about the dates on which the products are sold."},{"metadata":{},"cell_type":"markdown","source":"* **sales_train_validation_df**"},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train_validation_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Note: We are given historic sales data in the `sales_train_validation` dataset.\n    * rows exist in this dataset for days d_1 to d_1913. We are given the department, category, state, and store id of the item.\n    * d_1914 - d_1941 represents the `validation` rows which we will predict in stage 1\n    * d_1942 - d_1969 represents the `evaluation` rows which we will predict for the final competition standings."},{"metadata":{},"cell_type":"markdown","source":"* **submission_df**"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"submission_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Note: Brief overview of submission file\n    * The submission file has 29 columns, col1 for id and the remaining 28 columns represent the 28 forecast days.\n    * Each represent a specific item. This id tells us the item type, state, and store. We don't know what these items are exactly."},{"metadata":{},"cell_type":"markdown","source":"### 6.3. Profiling of each DataFrame\n\nFor profiling i have used [pandas-profiling](https://github.com/pandas-profiling/pandas-profiling) library\n\n* **sell_prices_df profile**"},{"metadata":{"trusted":true},"cell_type":"code","source":"spd_profile = ProfileReport(sell_prices_df, title='sell_prices_df Profiling Report', html={'style':{'full_width':True}})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"spd_profile.to_file(output_file=\"spd_profile.html\")\nspd_profile.to_notebook_iframe()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Note: sell_prices_df contains following interesting information:\n    * store_id: there are 10 different store_id, which shows data is collected from 10 different stores\n    * item_id: there are 3049 different item_ids, which shows 3049 different items are collected for forecasting. (Also item_id has lots of unique values that's why it has `HIGH CARDINALITY`)\n    * There are no missing or null values in this dataframe\n    * The corelation heat map shows there is no or 0 corelation between variables."},{"metadata":{},"cell_type":"markdown","source":"> * **calendar_df profile**"},{"metadata":{"trusted":true},"cell_type":"code","source":"cd_profile = ProfileReport(calendar_df, title='calendar_df Profiling Report', html={'style':{'full_width':True}})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cd_profile.to_file(output_file=\"cd_profile.html\")\ncd_profile.to_notebook_iframe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Note: calendar_df contains following interesting information:\n    * date: there are 1969 different dates, which shows this df has data for 1969 different dates.\n    * d: there are 1969 different d values (Also d has lots of unique values that's why it has `HIGH CARDINALITY`)\n    * event_name and event_type columns denotes special promotional events thus event_name_1, event_type_1, event_name_2, event_type_2 contains a lot of missing values.\n    * date features has very corelation and features snap_CA, snap_TX and snap_WI also show some corelation between them.\n    "},{"metadata":{},"cell_type":"markdown","source":"> * **sales_train_validation_df profile**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# using minimal=True to avoid heavy computation\n# stvd_profile = ProfileReport(sales_train_validation_df, title='sales_train_validation_df Profiling Report', html={'style':{'full_width':True}}, minimal=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# stvd_profile.to_file(output_file=\"stvd_profile.html\")\n# stvd_profile.to_notebook_iframe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 6.4. Plotting sales of 10 random items"},{"metadata":{"trusted":true},"cell_type":"code","source":"# selecting 10 random rows from dataframe\nstvd10 = sales_train_validation_df.sample(n = 10)\nd_cols = [c for c in stvd10.columns if 'd_' in c] # sales data columns\nstvd10 = stvd10.set_index('id')[d_cols].T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(40, 40))\nplt.subplots_adjust(top=1.2, hspace = 0.8)\nfor i,item_id in enumerate(list(stvd10.columns)):\n    plt.subplot(5, 2, i + 1)\n    stvd10[item_id].plot(figsize=(20, 12),\n          title=f'{item_id} sales by \"d\" number',\n          color=next(color_cycle))\n    plt.grid(False)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The above plot shows that there is lots of variation in item sales, this effect can be seasonal or due to some particular events on high sale days."},{"metadata":{},"cell_type":"markdown","source":"## 7. Merging DataFrames \n\n#### 7.1. Merging Calendar df with sales_train_validation_df\n\nMerging only few important columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"cal = calendar_df[['d','date','event_name_1', 'event_type_1', 'event_name_2', 'event_type_2',\n       'snap_CA', 'snap_TX', 'snap_WI']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stvd_1 = sales_train_validation_df.set_index('id')[d_cols].T\n# rename id column to 'd', to perform merge operation\nstvd_1 = stvd_1.reset_index().rename(columns={'index': 'd'})\n# merging df cal and sales_train_validation_df on 'd'\nstvd_merged = stvd_1.merge(cal, how='left', validate='1:1')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 8. Plotting sales of above 10 items on actual dates."},{"metadata":{"trusted":true},"cell_type":"code","source":"stvd10.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# rename id column to 'd', to perform merge operation\nstvd10 = stvd10.reset_index().rename(columns={'index': 'd'})\nstvd10 = stvd10.merge(cal, how='left', validate='1:1')\nstvd10_date = stvd10.set_index('date')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stvd10_date.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(40, 40))\nplt.subplots_adjust(top=1.2, hspace = 0.8)\nfor i,item_id in enumerate(list(stvd10_date.columns[1:11])):\n    plt.subplot(5, 2, i + 1)\n    stvd10_date[item_id].plot(figsize=(20, 12),\n          title=f'{item_id} sales by \"d\" number',\n          color=next(color_cycle))\n    plt.tight_layout()\n    plt.grid(False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Observations:\n    - It is common to see an item unavailable for a period of time.\n    - Some items only sell 1 or less in a day, making it very hard to predict.\n    - Other items show spikes in their demand (super bowl sunday?) possibly the \"events\" provided to us could help with these."},{"metadata":{},"cell_type":"markdown","source":"## 9. Simple Submission\n\nsimply setting last 30 days sales."},{"metadata":{"trusted":true},"cell_type":"code","source":"last_thirty_day_avg_sales = sales_train_validation_df.set_index('id')[d_cols[-30:]].mean(axis=1).to_dict()\nfcols = [f for f in submission_df.columns if 'F' in f]\nfor f in fcols:\n    submission_df[f] = submission_df['id'].map(last_thirty_day_avg_sales).fillna(0)\n    \nsubmission_df.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## TODO:\n# 1. Analyze sales of items by item_types i.e `Hobbies`, `Household`, `Foods`.\n# 2. Analyze store wise sale of an item datewise\n# 3. Analyze weekly trends or may be monthly.\n# 4. Create new features\n# 5. Try different models","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}