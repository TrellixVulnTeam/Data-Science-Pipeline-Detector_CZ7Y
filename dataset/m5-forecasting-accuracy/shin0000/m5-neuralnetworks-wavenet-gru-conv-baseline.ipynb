{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Sorry for not being good at English.\n\nI can't improve my score using various neural networks. Maybe this is a lack of my knowledge. Then, I decided to publish my notebook. I hope that this can helps someone.\n\nIn this notebook, I used sales_train_evaluation.csv file. So, you can run these codes to submit files. Please upvote if you find this useful. And, I'm a begginer and I'll be happy with your feedbacks. Thank you.\n\nPlease read my other notebook [M5 LightGBM baseline](https://www.kaggle.com/shin0000/m5-lightgbm-baseline) if you want to know about LightGBM baseline.\n\nI checked this notebook running in local environment. It took a lot of time.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Import modules","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"You may look Modeling Section if you want to select models.","execution_count":null},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy as sp\nimport os\nimport gc","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Making Data","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"base_dir = '../m5-forecasting-accuracy/'\ntrain_dir = os.path.join(base_dir, 'sales_train_evaluation.csv')\ntest_dir = os.path.join(base_dir, 'sample_submission.csv')\ncalendar_dir = os.path.join(base_dir, 'calendar.csv')\nprice_dir = os.path.join(base_dir, 'sell_prices.csv')\nsub_dir = os.path.join(base_dir, 'sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_train = pd.read_csv(train_dir)\ndf_test = pd.read_csv(test_dir)\ndf_calendar = pd.read_csv(calendar_dir)\ndf_price = pd.read_csv(price_dir)\ndf_sub = pd.read_csv(sub_dir)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def making_train_data(df_train):\n    print(\"processing train data\")\n    df_train_after = pd.melt(df_train, id_vars=['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], var_name='days', value_name='demand')\n    df_train_after['days'] = df_train_after['days'].map(lambda x: int(x[2:]))\n    df_train_after = df_train_after.drop(['id'], axis=1)\n    df_train_after = reduce_mem_usage(df_train_after)\n    gc.collect()\n    return df_train_after","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def making_test_data(df_test):\n    print(\"processing test data\")\n    df_test['item_id'] = df_test['id'].map(lambda x: x[:-16])\n    df_test['dept_id'] = df_test['item_id'].map(lambda x: x[:-4])\n    df_test['cat_id'] = df_test['dept_id'].map(lambda x: x[:-2])\n    df_test['store_id'] = df_test['id'].map(lambda x: x[-15:-11])\n    df_test['state_id'] = df_test['store_id'].map(lambda x: x[:-2])\n    df_test['va_or_ev'] = df_test['id'].map(lambda x: x[-10:])\n    df_test_val = df_test.loc[df_test['va_or_ev'] == 'validation', :]\n    df_test_ev = df_test.loc[df_test['va_or_ev'] == 'evaluation', :]\n    df_test_val_after = pd.melt(df_test_val, id_vars=['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'va_or_ev'], var_name='days', value_name='demand')\n    df_test_ev_after = pd.melt(df_test_ev, id_vars=['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'va_or_ev'], var_name='days', value_name='demand')\n    df_test_after = pd.concat([df_test_val_after, df_test_ev_after])\n    df_test_after['days'] = df_test_after['days'].map(lambda x: int(x[1:]))\n    df_test_after.loc[df_test_after['va_or_ev']=='evaluation', ['days']] += 28\n    df_test_after['days'] += 1913\n    df_test_after = df_test_after.drop(['va_or_ev'], axis=1)\n    df_test_after = df_test_after.drop(['id'], axis=1)\n    df_test_after = reduce_mem_usage(df_test_after)\n    return df_test_after","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def making_train_test_data(df_train ,df_test):\n    df_train = making_train_data(df_train)\n    df_test = making_test_data(df_test)\n    print(\"processing train test data\")\n    max_train_days = df_train['days'].max()\n    min_test_days = df_test['days'].min()\n    shift_data = 6\n    df_test = pd.concat([df_train.loc[max_train_days - 28 * shift_data <= df_train['days'], :], df_test.loc[df_test['days'] > max_train_days, :]]).reset_index(drop=True)\n    \n    shift_days_set = [28, 30, 32, 34]\n    for i in shift_days_set:\n        df_train['pos_demand_{}day'.format(i)] = df_train.groupby(['item_id', 'store_id'])['demand'].transform(lambda x: x.shift(i))\n        df_test['pos_demand_{}day'.format(i)] = df_test.groupby(['item_id', 'store_id'])['demand'].transform(lambda x: x.shift(i))\n        gc.collect()\n        \n    rolling_days_set = [7, 14, 28]\n    for i in rolling_days_set:\n        \n        df_train['demand_{}day_mean'.format(i)] = df_train.groupby(['item_id', 'store_id'])['demand'].transform(lambda x: x.shift(28).rolling(i).mean())\n        df_train['demand_{}day_max'.format(i)] = df_train.groupby(['item_id', 'store_id'])['demand'].transform(lambda x: x.shift(28).rolling(i).max())\n        df_train['demand_{}day_min'.format(i)] = df_train.groupby(['item_id', 'store_id'])['demand'].transform(lambda x: x.shift(28).rolling(i).min())\n        \n        df_test['demand_{}day_mean'.format(i)] = df_test.groupby(['item_id', 'store_id'])['demand'].transform(lambda x: x.shift(28).rolling(i).mean())\n        df_test['demand_{}day_max'.format(i)] = df_test.groupby(['item_id', 'store_id'])['demand'].transform(lambda x: x.shift(28).rolling(i).max())\n        df_test['demand_{}day_min'.format(i)] = df_test.groupby(['item_id', 'store_id'])['demand'].transform(lambda x: x.shift(28).rolling(i).min())\n        \n        \n        df_train = reduce_mem_usage(df_train)\n        df_test = reduce_mem_usage(df_test)\n        gc.collect()\n    \n    df_test = df_test.loc[df_test['days'] >= min_test_days, :]\n    df_test = reduce_mem_usage(df_test)\n    gc.collect()\n\n    \n    return df_train, df_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def making_calendar_data(df_calendar):\n    df_calendar = reduce_mem_usage(df_calendar)\n    gc.collect()\n    print(\"processing calendar data\")\n    df_calendar['days'] = df_calendar['d'].map(lambda x: int(x[2:]))\n    event_type = {np.nan: 1, 'Sporting': 2, 'Cultural': 3, 'National': 5, 'Religious': 7}\n    df_calendar['event_type_1'] = df_calendar['event_type_1'].map(event_type)\n    df_calendar['event_type_2'] = df_calendar['event_type_2'].map(event_type)\n    df_calendar['event_type'] = df_calendar['event_type_1'] * df_calendar['event_type_2']\n    le = LabelEncoder()\n    le.fit(df_calendar['event_type'])\n    df_calendar['event_type'] = le.transform(df_calendar['event_type'])\n    df_calendar['cal_day'] = pd.to_datetime(df_calendar['date']).dt.day.astype(np.int8)\n    df_calendar['cal_week'] = pd.to_datetime(df_calendar['date']).dt.week.astype(np.int8)\n    df_calendar = df_calendar.drop(['event_type_1', 'event_type_2', 'event_name_1', 'event_name_2', 'd', 'weekday', 'date'], axis=1)\n#     df_calendar['event_type_1day_ago'] = df_calendar['event_type'].shift(1)\n#     df_calendar['event_type_1day_after'] = df_calendar['event_type'].shift(-1)\n    df_calendar = reduce_mem_usage(df_calendar)\n    gc.collect()\n    return df_calendar","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def making_price_data(df_price):\n    df_price = reduce_mem_usage(df_price)\n    gc.collect()\n    print(\"processing price data\")\n    \n    shift_days_set = [28, 30, 32, 34]\n    for i in shift_days_set:\n        df_price['pos_price_{}day'.format(i)] = df_price.groupby(['item_id', 'store_id'])['sell_price'].transform(lambda x: x.shift(i))\n    gc.collect()\n    \n#     rolling_days_set = [7, 14, 28]\n#     for i in rolling_days_set:\n#         df_price['price_{}day_mean'.format(i)] = df_price.groupby(['item_id', 'store_id'])['sell_price'].transform(lambda x: x.shift(28).rolling(i).mean())\n#         df_price['price_{}day_std'.format(i)] = df_price.groupby(['item_id', 'store_id'])['sell_price'].transform(lambda x: x.shift(28).rolling(i).std())\n#         df_price = reduce_mem_usage(df_price)\n#         gc.collect()\n    return df_price","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def concat_data(df_train, df_test, df_calendar, df_price):\n    df_train, df_test = making_train_test_data(df_train ,df_test)\n    df_calendar = making_calendar_data(df_calendar)\n    df_price = making_price_data(df_price)\n    print(\"concat data\")\n    df_train = pd.merge(df_train, df_calendar, on='days', how='left')\n    df_test = pd.merge(df_test, df_calendar, on='days', how='left')\n    df_train = pd.merge(df_train, df_price, on=['wm_yr_wk', 'store_id', 'item_id'], how='left')\n    df_test = pd.merge(df_test, df_price, on=['wm_yr_wk', 'store_id', 'item_id'], how='left')\n    df_train = df_train.drop(['wm_yr_wk'], axis=1)\n    df_test = df_test.drop(['wm_yr_wk'], axis=1)\n    del df_calendar, df_price\n    gc.collect()\n    df_train = reduce_mem_usage(df_train)\n    df_test = reduce_mem_usage(df_test)\n    gc.collect()\n    return df_train, df_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def labeling_data(df_train, df_test, df_calendar, df_price):\n    df_train, df_test = concat_data(df_train, df_test, df_calendar, df_price)\n    print(\"labeling data\")\n    label_columns = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']\n    for c in label_columns:\n        le  = LabelEncoder()\n        le.fit(df_train[c])\n        df_train[c] = le.transform(df_train[c])\n        df_test[c] = le.transform(df_test[c])\n        if c != 'item_id':\n            print(le.classes_)\n    \n    df_train = reduce_mem_usage(df_train)\n    df_test = reduce_mem_usage(df_test)\n    gc.collect()\n    \n    return df_train, df_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"df_train, df_test = labeling_data(df_train, df_test, df_calendar, df_price)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"for c in df_train.columns:\n    print(c)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_train = df_train.fillna(-10)\ndf_test = df_test.fillna(-10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modeling","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"In this section, you can choose models such as wavenet, and models using gru or conv layer.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"from tensorflow.keras.layers import *\nfrom tensorflow.keras.optimizers import *\nfrom tensorflow.keras.callbacks import *\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.metrics import mean_squared_error\n\ndef make_wavenet():\n\n    def wave_block(x, filters, kernel_size, n):\n        dilation_rates = [2**i for i in range(n)]\n        x = Conv1D(filters = filters,\n                  kernel_size = 1,\n                  padding = 'same')(x)\n        res_x = x\n        for dilation_rate in dilation_rates:\n            tanh_out = Conv1D(filters = filters,\n                            kernel_size = kernel_size,\n                            padding = 'same', \n                            activation = 'tanh', \n                            dilation_rate = dilation_rate)(x)\n            sigm_out = Conv1D(filters = filters,\n                            kernel_size = kernel_size,\n                            padding = 'same',\n                            activation = 'sigmoid', \n                            dilation_rate = dilation_rate)(x)\n            x = Multiply()([tanh_out, sigm_out])\n            x = Conv1D(filters = filters,\n                      kernel_size = 1,\n                      padding = 'same')(x)\n            res_x = Add()([res_x, x])\n        return res_x\n\n    inp = Input(shape=(timesteps, n_used_features))\n\n    x = wave_block(inp, 16, 3, 12)\n    x = wave_block(x, 32, 3, 8)\n    x = Flatten()(x)\n    x = Dropout(0.2)(x)\n\n    out = Dense(1, activation = 'relu')(x)\n\n    model = Model(inputs=inp, outputs=out)\n\n    # opt = SGD(lr=0.001, momentum=0.0, decay=0.0, nesterov=False)\n    # opt = RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n    opt = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n    #   opt = tfa.optimizers.SWA(opt)\n\n    model.compile(optimizer=opt, loss='mse')\n\n    return model\n\ndef make_gru():\n    inp = Input(shape=(timesteps, n_used_features))\n    x = GRU(16, activation='relu', return_sequences=True)(inp)\n    x = GRU(32, activation='relu', return_sequences=False)(x)\n    x = Dense(256, activation='relu')(x)\n    out = Dense(1, activation='relu')(x)\n    model = Model(inputs=[inp], outputs=[out])\n    \n    optim = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n    \n    model.compile(optimizer=optim, loss='mse')\n    \n    return model\n\ndef make_conv():\n    inp = Input(shape=(timesteps, n_used_features))\n    x = Conv1D(16, 3, activation='relu')(inp)\n    x = Conv1D(32, 3, activation='relu')(x)\n    x = Conv1D(64, 3, activation='relu')(x)\n    x = Flatten()(x)\n    x = Dense(256, activation='relu')(x)\n    x = Dropout(0.2)(x)\n    out = Dense(1, activation='relu')(x)\n    model = Model(inputs=[inp], outputs=[out])\n    \n    optim = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n    \n    model.compile(optimizer=optim, loss='mse')\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def scheduler(epoch):\n    lr = 0.001\n    if epoch < 10:\n        return lr\n    elif epoch < 20:\n        return lr / 3\n    elif epoch < 30:\n        return lr / 10\n    elif epoch < 50:\n        return lr / 5\n    elif epoch < 60:\n        return lr / 10\n    elif epoch < 70:\n        return lr / 50\n    else:\n        return lr / 100","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_all = pd.concat([df_train, df_test])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def engineer_data(df_i, aborted_time):\n    added_features = []\n    for c in ['item_id', 'dept_id']:\n        m_d = 'mean_demand_groupby_{}'.format(c)\n#         s_d = 'std_demand_groupby_{}'.format(c)\n        m_p = 'mean_price_groupby_{}'.format(c)\n#         s_p = 'std_price_groupby_{}'.format(c)\n        added_features.append(m_d)\n#         added_features.append(s_d)\n        added_features.append(m_p)\n#         added_features.append(s_p)\n        \n        df_i['mean_demand_groupby_{}'.format(c)] = df_i.loc[df_i['days'] < aborted_time, [c, 'demand']].groupby(c).transform('mean')\n#         df_i['std_demand_groupby_{}'.format(c)] = df_i.loc[df_i['days'] < aborted_time, [c, 'demand']].groupby(c).transform('std')\n        df_i['mean_price_groupby_{}'.format(c)] = df_i.loc[df_i['days'] < aborted_time, [c, 'sell_price']].groupby(c).transform('mean')\n#         df_i['std_price_groupby_{}'.format(c)] = df_i.loc[df_i['days'] < aborted_time, [c, 'sell_price']].groupby(c).transform('std')\n    return df_i, added_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def normalize_data(df_i, used_features, aborted_time, used_time):\n    print('used_features:\\n{}'.format(used_features))\n    mask = (aborted_time <= df_i['days']) & (df_i['days'] < used_time)\n    mx = df_i[mask].dropna().max()\n    mn = df_i[mask].dropna().min()\n    df_i.loc[:, used_features] = (df_i.loc[:, used_features] - mn) / (mx - mn)\n    df_i = reduce_mem_usage(df_i)\n    return df_i","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def make_data(df_i_j, item_id):\n    train_position = n_train_data_per_item * item_id\n    valid_position = n_valid_data_per_item * item_id\n    test_position = n_test_data_per_item * item_id\n    for e, i in enumerate(range(test_start_index, train_finish_index)):\n        if train_start_index <= i < train_finish_index:\n            X_train[train_position+n_train_data_per_item-e-1, :, :] = df_i_j.iloc[-timesteps-i: -i].loc[:, used_features].values\n            y_train[train_position+n_train_data_per_item-e-1] = df_i_j.iloc[-i]['demand']\n            option_train[train_position+n_train_data_per_item-e-1] = df_i_j.iloc[-i][option_features]\n        elif valid_start_index <= i < valid_finish_index:\n            X_valid[valid_position+n_valid_data_per_item-e-1, :, :] = df_i_j.iloc[-timesteps-i: -i].loc[:, used_features].values\n            y_valid[valid_position+n_valid_data_per_item-e-1] = df_i_j.iloc[-i]['demand']\n            option_valid[valid_position+n_valid_data_per_item-e-1] = df_i_j.iloc[-i][option_features]\n        elif test_start_index <= i < test_finish_index:\n            X_test[test_position+n_test_data_per_item-e-1, :, :] = df_i_j.iloc[-timesteps-i: -i].loc[:, used_features].values\n            y_test[test_position+n_test_data_per_item-e-1] = df_i_j.iloc[-i]['demand']\n            option_test[test_position+n_test_data_per_item-e-1] = df_i_j.iloc[-i][option_features]\n        else:\n            print('error')\n            break","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nclass ScoreCallback(Callback):\n    def __init__(self, X_valid, y_valid):\n        self.X_valid = X_valid\n        self.y_valid = y_valid\n\n    def on_epoch_end(self, epoch, logs):\n        y_valid_pred = self.model.predict(self.X_valid)\n        print('   RMSE score {:.4f}'.format(np.sqrt(mean_squared_error(self.y_valid, y_valid_pred))))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this code, you have to fix make_wavenet if you want to change models.","execution_count":null},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"gc.collect()\nfrom tqdm import tqdm\nfrom tensorflow.keras.models import load_model\n\ndrop_features = ['snap_CA', 'snap_TX', 'snap_WI', 'dept_id', 'cat_id', 'state_id', 'cal_day', 'cal_week', 'wday', 'year', 'month', 'event_type']\nfundamental_used_features = [x for x in df_all.columns if x not in drop_features]\noption_features = ['store_id', 'item_id', 'days']\ntarget_features = ['demand']\nn_fundamental_used_features = len(fundamental_used_features)\nn_option_features = len(option_features)\nstore_ids = df_all['store_id'].unique()\nitem_ids = df_all['item_id'].unique()\n\ntest_finish_index = 28 * 2 + 1\ntest_start_index = 1\nvalid_finish_index = 28 * 4 + 1\nvalid_start_index = test_finish_index\ntrain_finish_index = 28 * 16 + 1\ntrain_start_index = valid_finish_index\nused_time = 1969 - train_start_index - 1\naborted_time = 1969 - train_finish_index - 1\ntimesteps = 7\nn_items = 3049\n\ndf_total_predict = pd.DataFrame(columns=['store_id', 'item_id', 'days', 'demand'])\n\nn_train_data_per_item = train_finish_index - train_start_index\nn_valid_data_per_item = valid_finish_index - valid_start_index \nn_test_data_per_item = test_finish_index - test_start_index\n\nn_train_data = n_train_data_per_item * n_items\nn_valid_data = n_valid_data_per_item * n_items\nn_test_data = n_test_data_per_item * n_items\n\nfor i in store_ids:\n    print('finish {} / 10'.format(i+1))\n\n    df_i = df_all.loc[df_all['store_id'] == i, :]\n    df_i, added_features = engineer_data(df_i, aborted_time)\n    used_features = list(set(fundamental_used_features + added_features) - set(option_features) - set(target_features))\n    n_used_features = len(used_features)\n    df_i = normalize_data(df_i, used_features, aborted_time, used_time)\n    \n    X_train = np.zeros((n_train_data, timesteps, n_used_features))\n    X_valid = np.zeros((n_valid_data, timesteps, n_used_features))\n    X_test = np.zeros((n_test_data, timesteps, n_used_features))\n    y_train = np.zeros((n_train_data, ))\n    y_valid = np.zeros((n_valid_data, ))\n    y_test = np.zeros((n_test_data, ))\n    option_train = np.zeros((n_train_data, n_option_features))\n    option_valid = np.zeros((n_valid_data, n_option_features))\n    option_test = np.zeros((n_test_data, n_option_features))\n    \n    \n    for j in tqdm(item_ids):\n        df_i_j = df_i.loc[df_i['item_id'] == j, :].drop(drop_features, axis=1).fillna(-1)\n        make_data(df_i_j, j)\n            \n    model = make_wavenet() #you can choose the model (gru, conv, wavenet)\n    model.summary()\n    lrsc = LearningRateScheduler(scheduler)\n    sc = ScoreCallback(X_valid, y_valid)\n    mc = ModelCheckpoint('./models/best_model_store{}.h5'.format(i), monitor='val_loss', verbose=0, save_best_only=True, mode='auto', period=1)\n    model.fit(X_train, y_train, epochs=30, batch_size=1000, validation_data=(X_valid, y_valid), callbacks=[lrsc, sc, mc])\n    model = load_model('./models/best_model_store{}.h5'.format(i))\n    y_valid_pred = model.predict(X_valid)\n    y_test_pred = model.predict(X_test)\n    \n    plt.figure(figsize=(20, 5))\n    plt.title('store_{}_valid'.format(i))\n    plt.plot(y_valid)\n    plt.plot(y_valid_pred)\n    \n    plt.figure(figsize=(20, 5))\n    plt.title('store_{}_test'.format(i))\n    plt.plot(y_test_pred)\n    \n    plt.show()\n    \n    df_predict = pd.DataFrame()\n    df_predict['store_id'] = option_test[:, 0].astype('int8')\n    df_predict['item_id'] = option_test[:, 1].astype('int16')\n    df_predict['days'] = option_test[:, 2].astype('int16')\n    df_predict['demand'] = y_test_pred\n    df_total_predict = pd.concat([df_total_predict, df_predict]).reset_index(drop=True)\n    del X_train, X_valid, X_test, y_train, y_valid, y_test, option_train, option_valid, option_test, model\n    gc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Making Submission","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"df_sub_before = df_test.loc[:, ['store_id', 'item_id', 'days']]\ndf_sub_before= pd.merge(df_sub_before, df_total_predict, left_on=['item_id', 'store_id', 'days'], right_on=['item_id', 'store_id', 'days'], how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_sub = pd.read_csv(sub_dir)\ndf_sub_base = pd.read_csv(sub_dir)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def making_submission(df_sub, df_sub_before, df_sub_base):\n    df_sub['va_or_ev'] = df_sub['id'].map(lambda x: x[-10:])\n    df_sub_val = df_sub.loc[df_sub['va_or_ev'] == 'validation', :]\n    df_sub_ev = df_sub.loc[df_sub['va_or_ev'] == 'evaluation', :]\n    df_sub_val = df_sub_val.melt(id_vars=['id', 'va_or_ev'], var_name='days', value_name='demand').drop(['va_or_ev'], axis=1)\n    df_sub_ev = df_sub_ev.melt(id_vars=['id', 'va_or_ev'], var_name='days', value_name='demand').drop(['va_or_ev'], axis=1)\n    num_va = df_sub_val.shape[0]\n    num_ev = df_sub_ev.shape[0]\n    df_sub_val['demand'] = df_sub_before['demand'][:num_va].values\n    df_sub_ev['demand'] = df_sub_before['demand'][num_va:].values\n    df_sub_val = df_sub_val.pivot(index='id', columns='days', values='demand').reset_index()\n    df_sub_ev = df_sub_ev.pivot(index='id', columns='days', values='demand').reset_index()\n    df_sub_after = pd.concat([df_sub_val, df_sub_ev])\n    df_sub_columns = ['id'] + ['F{}'.format(i+1) for i in range(28)]\n    df_sub = df_sub_after.loc[:, df_sub_columns]\n    df_sub.columns = df_sub_columns\n    df_sub = pd.merge(df_sub_base['id'], df_sub, on='id', how='left')\n    return df_sub","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_sub = making_submission(df_sub, df_sub_before, df_sub_base)\ndf_sub.to_csv('./my_GRU_submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_sub","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}