{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"I'm sorry for not being good at English.\n\nThis is my first public notebook and I am beginner. Surly, it includes many defects. Thank you for your feedback!.\n\nKaggle初心者ですが、初めてノートブックを公開します。数多い欠陥があると思われますが、ご了承ください。 コメントなどで改善点などあれば書いてくれると嬉しいです！\n\n\nI decide to publish notebook because I don't have much time to spend this competition to prepare for regular examination, and I'm happy that this notebook helps someone.\n\nM5コンペティションに掛けれる時間がテスト勉強に取られ、ほとんどできなくなってしまうと思ったので、誰かのお役に立てればと思い、ノートブックを公開することにしました。\n\n\nIn this notebook, I predict demand using lightgbm model for every store.\n\nこのノートブックでは、店舗毎にLightGBMのモデルを用いて需要を予測しています。\n\nI validate score using out of hold, not cross validation.\n\n検証として、クロスバリデーションではなく、アウトオブホールドを使っています。(時系列データに対する検証方法がわからなかったのと、計算時間を考慮した結果)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Import Modules","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"In this Section, I import necessary modules.\n\n必要なモジュールをインポートしていきます。","execution_count":null},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy as sp\nimport os\nimport gc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Making Data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"In Making Data Section, I made features using fundamental method such as lag features, date features, rolling mean and etc.\n\nラグ特徴量、日付、移動平均などの特徴量を作成していきます。","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"base_dir = '/kaggle/input/m5-forecasting-accuracy/'\ntrain_dir = os.path.join(base_dir, 'sales_train_evaluation.csv')\ntest_dir = os.path.join(base_dir, 'sample_submission.csv')\ncalendar_dir = os.path.join(base_dir, 'calendar.csv')\nprice_dir = os.path.join(base_dir, 'sell_prices.csv')\nsub_dir = os.path.join(base_dir, 'sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.read_csv(train_dir)\ndf_test = pd.read_csv(test_dir)\ndf_calendar = pd.read_csv(calendar_dir)\ndf_price = pd.read_csv(price_dir)\ndf_sub = pd.read_csv(sub_dir)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def making_train_data(df_train):\n    print(\"processing train data\")\n    df_train_after = pd.melt(df_train, id_vars=['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], var_name='days', value_name='demand')\n    df_train_after['days'] = df_train_after['days'].map(lambda x: int(x[2:]))\n    df_train_after = df_train_after.drop(['id'], axis=1)\n    df_train_after = reduce_mem_usage(df_train_after)\n    gc.collect()\n    return df_train_after","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def making_test_data(df_test):\n    print(\"processing test data\")\n    df_test['item_id'] = df_test['id'].map(lambda x: x[:-16])\n    df_test['dept_id'] = df_test['item_id'].map(lambda x: x[:-4])\n    df_test['cat_id'] = df_test['dept_id'].map(lambda x: x[:-2])\n    df_test['store_id'] = df_test['id'].map(lambda x: x[-15:-11])\n    df_test['state_id'] = df_test['store_id'].map(lambda x: x[:-2])\n    df_test['va_or_ev'] = df_test['id'].map(lambda x: x[-10:])\n    df_test_val = df_test.loc[df_test['va_or_ev'] == 'validation', :]\n    df_test_ev = df_test.loc[df_test['va_or_ev'] == 'evaluation', :]\n    df_test_val_after = pd.melt(df_test_val, id_vars=['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'va_or_ev'], var_name='days', value_name='demand')\n    df_test_ev_after = pd.melt(df_test_ev, id_vars=['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'va_or_ev'], var_name='days', value_name='demand')\n    df_test_after = pd.concat([df_test_val_after, df_test_ev_after])\n    df_test_after['days'] = df_test_after['days'].map(lambda x: int(x[1:]))\n    df_test_after.loc[df_test_after['va_or_ev']=='evaluation', ['days']] += 28\n    df_test_after['days'] += 1913\n    df_test_after = df_test_after.drop(['va_or_ev'], axis=1)\n    df_test_after = df_test_after.drop(['id'], axis=1)\n    df_test_after = reduce_mem_usage(df_test_after)\n    return df_test_after","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def making_train_test_data(df_train ,df_test):\n    df_train = making_train_data(df_train)\n    df_test = making_test_data(df_test)\n    print(\"processing train test data\")\n    max_train_days = df_train['days'].max()\n    min_test_days = df_test['days'].min()\n    shift_data = 6\n    df_test = pd.concat([df_train.loc[max_train_days - 28 * shift_data <= df_train['days'], :], df_test.loc[df_test['days'] > max_train_days, :]]).reset_index(drop=True)\n    \n#     shift_days_set = [28, 29, 30]\n#     for i in shift_days_set:\n#         df_train['demand_{}day_ago'.format(i)] = df_train.groupby(['item_id', 'store_id'])['demand'].transform(lambda x: x.shift(i))\n#         df_test['demand_{}day_ago'.format(i)] = df_test.groupby(['item_id', 'store_id'])['demand'].transform(lambda x: x.shift(i))\n#         gc.collect()\n        \n    rolling_days_set = [2, 3, 5, 7, 14, 28, 56, 140]\n    for i in rolling_days_set:\n        df_train['demand_{}day_mean'.format(i)] = df_train.groupby(['item_id', 'store_id'])['demand'].transform(lambda x: x.shift(28).rolling(i).mean())\n#         df_train['demand_{}day_max'.format(i)] = df_train.groupby(['item_id', 'store_id'])['demand'].transform(lambda x: x.shift(28).rolling(i).max())\n        \n        df_test['demand_{}day_mean'.format(i)] = df_test.groupby(['item_id', 'store_id'])['demand'].transform(lambda x: x.shift(28).rolling(i).mean())\n#         df_test['demand_{}day_max'.format(i)] = df_test.groupby(['item_id', 'store_id'])['demand'].transform(lambda x: x.shift(28).rolling(i).max())\n        df_train = reduce_mem_usage(df_train)\n        df_test = reduce_mem_usage(df_test)\n        gc.collect()\n    \n    df_test = df_test.loc[df_test['days'] >= min_test_days, :]\n    df_test = reduce_mem_usage(df_test)\n    gc.collect()\n    \n    return df_train, df_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def making_calendar_data(df_calendar):\n    df_calendar = reduce_mem_usage(df_calendar)\n    gc.collect()\n    print(\"processing calendar data\")\n    df_calendar['days'] = df_calendar['d'].map(lambda x: int(x[2:]))\n    event_type = {np.nan: 1, 'Sporting': 2, 'Cultural': 3, 'National': 5, 'Religious': 7}\n    df_calendar['event_type_1'] = df_calendar['event_type_1'].map(event_type)\n    df_calendar['event_type_2'] = df_calendar['event_type_2'].map(event_type)\n    df_calendar['event_type'] = df_calendar['event_type_1'] * df_calendar['event_type_2']\n    le = LabelEncoder()\n    le.fit(df_calendar['event_type'])\n    df_calendar['event_type'] = le.transform(df_calendar['event_type'])\n    df_calendar = df_calendar.drop(['event_type_1', 'event_type_2', 'event_name_1', 'event_name_2', 'd', 'weekday', 'date', 'year'], axis=1)\n#     df_calendar['event_type_1day_ago'] = df_calendar['event_type'].shift(1)\n#     df_calendar['event_type_1day_after'] = df_calendar['event_type'].shift(-1)\n    df_calendar = reduce_mem_usage(df_calendar)\n    gc.collect()\n    return df_calendar","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def making_price_data(df_price):\n    df_price = reduce_mem_usage(df_price)\n    gc.collect()\n    print(\"processing price data\")\n#     shift_days_set = [28, 35, 42]\n#     for i in shift_days_set:\n#         df_price['price_{}day_ago'.format(i)] = df_price.groupby(['item_id', 'store_id'])['sell_price'].transform(lambda x: x.shift(i))\n#     gc.collect()\n    \n    rolling_days_set = [28, 140]\n    for i in rolling_days_set:\n        df_price['price_{}day_mean'.format(i)] = df_price.groupby(['item_id', 'store_id'])['sell_price'].transform(lambda x: x.shift(28).rolling(i).mean())\n        df_price['price_{}day_max'.format(i)] = df_price.groupby(['item_id', 'store_id'])['sell_price'].transform(lambda x: x.shift(28).rolling(i).max())\n        df_price['price_{}day_min'.format(i)] = df_price.groupby(['item_id', 'store_id'])['sell_price'].transform(lambda x: x.shift(28).rolling(i).min())\n        df_price = reduce_mem_usage(df_price)\n        gc.collect()\n    return df_price","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def concat_data(df_train, df_test, df_calendar, df_price):\n    df_train, df_test = making_train_test_data(df_train ,df_test)\n    df_calendar = making_calendar_data(df_calendar)\n    df_price = making_price_data(df_price)\n    print(\"concat data\")\n    df_train = pd.merge(df_train, df_calendar, on='days', how='left')\n    df_test = pd.merge(df_test, df_calendar, on='days', how='left')\n    df_train = pd.merge(df_train, df_price, on=['wm_yr_wk', 'store_id', 'item_id'], how='left')\n    df_test = pd.merge(df_test, df_price, on=['wm_yr_wk', 'store_id', 'item_id'], how='left')\n    df_train = df_train.drop(['wm_yr_wk'], axis=1)\n    df_test = df_test.drop(['wm_yr_wk'], axis=1)\n    del df_calendar, df_price\n    gc.collect()\n    df_train = reduce_mem_usage(df_train)\n    df_test = reduce_mem_usage(df_test)\n    gc.collect()\n    return df_train, df_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def labeling_data(df_train, df_test, df_calendar, df_price):\n    df_train, df_test = concat_data(df_train, df_test, df_calendar, df_price)\n    print(\"labeling data\")\n    label_columns = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']\n    for c in label_columns:\n        le  = LabelEncoder()\n        le.fit(df_train[c])\n        df_train[c] = le.transform(df_train[c])\n        df_test[c] = le.transform(df_test[c])\n        if c != 'item_id':\n            print(le.classes_)\n    \n    df_train = reduce_mem_usage(df_train)\n    df_test = reduce_mem_usage(df_test)\n    gc.collect()\n    \n    return df_train, df_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train, df_test = labeling_data(df_train, df_test, df_calendar, df_price)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for c in df_train.columns:\n    print(c)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modeling","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"I predict demand using lightgbm model for every store.\n\n店舗毎にLightGBMのモデルを用いて需要を予測しています。","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nimport lightgbm as lgbm\n\ndef metric(y_true, y_pred):\n    return np.mean(np.sum(np.abs(y_true - y_pred), axis=0)/np.sum(y_true, axis=0))\n\ndef get_feature_importances(data, store_id, y_valid, y_valid_pred, used_features):\n    train_features = used_features\n    imp_df = pd.DataFrame()\n    imp_df[\"importance_gain_{}\".format(store_id)] = lgb.feature_importance(importance_type='gain')\n    imp_df[\"importance_split_{}\".format(store_id)] = lgb.feature_importance(importance_type='split')\n    imp_df[\"valid_rmse_{}\".format(store_id)] = mean_squared_error(y_valid, y_valid_pred, squared=False)\n    imp_df[\"valid_wrmse_{}\".format(store_id)] = metric(y_valid, y_valid_pred)\n    return imp_df\n\ntotal_imp_df = pd.DataFrame()\ndf_sub_ensemble = df_test.loc[:, ['item_id', 'store_id', 'days', 'demand']]\ndf_sub_ensemble['demand_model'] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['snap'] = 0\ndf_test['snap'] = 0\nused_features = [c for c in df_train.columns if c not in ['demand', 'item_id', 'store_id', 'state_id', 'days', 'snap_CA', 'snap_TX', 'snap_WI']]\ntotal_imp_df[\"feature\"] = used_features\nstore_id_list = df_train['store_id'].unique()\nfor store_id in store_id_list:\n    print('store_id {}/10'.format(store_id + 1))\n    \n    df_train['snap'] = 0\n    df_test['snap'] = 0\n    if 0 <= store_id <= 3:\n        df_train['snap'] = df_train['snap_CA']\n        df_test['snap'] = df_test['snap_CA']\n    elif 4 <= store_id <= 6:\n        df_train['snap'] = df_train['snap_TX']\n        df_test['snap'] = df_test['snap_TX']\n    else:\n        df_train['snap'] = df_train['snap_WI']\n        df_test['snap'] = df_test['snap_WI']\n        \n    train_index = (df_train['days'] < 1913 - 28) & (df_train['store_id'] == store_id)\n    valid_index = (1913 - 28 <= df_train['days']) & (df_train['store_id'] == store_id)\n    test_index = (df_test['store_id'] == store_id)\n    \n    X_train = df_train.loc[train_index, used_features].values\n    y_train = df_train.loc[train_index, 'demand'].values\n\n    X_valid = df_train.loc[valid_index, used_features].values\n    y_valid = df_train.loc[valid_index, 'demand'].values\n\n    X_test = df_test.loc[test_index, used_features].values\n\n    lgb_params = {\n        'objective': 'poisson',\n        'num_iterations' : 2000,\n        'boosting_type': 'gbdt',\n        'metric': 'rmse',\n        'n_jobs': -1,\n        'seed': 42,\n        'learning_rate': 0.075,\n        'bagging_fraction': 0.75,\n        'bagging_freq': 10,\n        'colsample_bytree': 0.75\n                  }\n\n    train_data = lgbm.Dataset(X_train, y_train)\n    valid_data = lgbm.Dataset(X_valid, y_valid)\n\n    lgb = lgbm.train(lgb_params, train_data, valid_sets=[train_data, valid_data], early_stopping_rounds=10, verbose_eval=20)\n    \n    y_valid_pred = lgb.predict(X_valid, num_iteration=lgb.best_iteration)\n    y_test_pred = lgb.predict(X_test, num_iteration=lgb.best_iteration)\n    df_sub_ensemble.loc[test_index, ['demand_model']] = y_test_pred\n    \n    print(metric(y_valid, y_valid_pred))\n    \n    imp_df = get_feature_importances(df_train, store_id, y_valid, y_valid_pred, used_features)\n    total_imp_df = pd.concat([total_imp_df, imp_df], axis=1, sort=False)\n    \n    del X_train, X_valid, X_test, y_train, y_valid, lgb\n    gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_imp = pd.DataFrame(columns=['features', 'importance_gain', 'importance_split', 'valid_rmse', 'valid_wrmse'])\ndf_imp[\"features\"] = used_features\ndf_imp['importance_gain'] = 0\ndf_imp['importance_split'] = 0\ndf_imp['valid_rmse'] = 0\ndf_imp['valid_wrmse'] = 0\nn_stores = len(store_id_list)\nfor store_id in store_id_list:\n    df_imp['importance_gain'] += total_imp_df['importance_gain_{}'.format(store_id)].values / n_stores\n    df_imp['importance_split'] += total_imp_df['importance_split_{}'.format(store_id)].values / n_stores\n    df_imp['valid_rmse'] += total_imp_df['valid_rmse_{}'.format(store_id)].values / n_stores\n    df_imp['valid_wrmse'] += total_imp_df['valid_wrmse_{}'.format(store_id)].values / n_stores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_imp.sort_values(by='importance_gain', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Making Ensemble Submission","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"In this Section, I used to predict test data using many models.　Now, It isn't necessary.\n\nこのセクションでは、様々なモデルを用いてアンサンブルの結果を一つにまとめていました。その名残です。","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sub_before = df_test.loc[:, ['days', 'demand']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sub_before['demand'] = df_sub_ensemble['demand_model']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Making Submission","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"In this Section, I remake original submission format.\n\n最終的なサブミッションを元々のサブミッションの形式と同じになるように作り直しています。","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sub = pd.read_csv(sub_dir)\ndf_sub_base = pd.read_csv(sub_dir)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def making_submission(df_sub, df_sub_before, df_sub_base):\n    df_sub['va_or_ev'] = df_sub['id'].map(lambda x: x[-10:])\n    df_sub_val = df_sub.loc[df_sub['va_or_ev'] == 'validation', :]\n    df_sub_ev = df_sub.loc[df_sub['va_or_ev'] == 'evaluation', :]\n    df_sub_val = df_sub_val.melt(id_vars=['id', 'va_or_ev'], var_name='days', value_name='demand').drop(['va_or_ev'], axis=1)\n    df_sub_ev = df_sub_ev.melt(id_vars=['id', 'va_or_ev'], var_name='days', value_name='demand').drop(['va_or_ev'], axis=1)\n    num_va = df_sub_val.shape[0]\n    num_ev = df_sub_ev.shape[0]\n    df_sub_val['demand'] = df_sub_before['demand'][:num_va].values\n    df_sub_ev['demand'] = df_sub_before['demand'][num_va:].values\n    df_sub_val = df_sub_val.pivot(index='id', columns='days', values='demand').reset_index()\n    df_sub_ev = df_sub_ev.pivot(index='id', columns='days', values='demand').reset_index()\n    df_sub_after = pd.concat([df_sub_val, df_sub_ev])\n    df_sub_columns = ['id'] + ['F{}'.format(i+1) for i in range(28)]\n    df_sub = df_sub_after.loc[:, df_sub_columns]\n    df_sub.columns = df_sub_columns\n    df_sub = pd.merge(df_sub_base['id'], df_sub, on='id', how='left')\n    return df_sub","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sub = making_submission(df_sub, df_sub_before, df_sub_base)\ndf_sub.to_csv('./my_submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sub","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}