{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\nimport pandas as pd\nimport numpy as np\nimport dask.dataframe as dd\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_rows', 500)\nimport matplotlib.pyplot as plt\nimport lightgbm as lgb\nimport statsmodels.api as sm\nimport matplotlib.pyplot as pyplot\nfrom sklearn import preprocessing, metrics\nimport pickle\nimport gc\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nfrom sklearn.model_selection import train_test_split\nimport datetime\nimport random\nimport warnings","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Permutation test\nThis test can be used to determine whether feature is important for the model or not. \nThis could be achieved by random permutation of feature. Main idea here - if model predicts well with permuted feature values, it means that feature is not important. \n\nBut there are two problems here:\n\nFirst - as permutation is RANDOM, there could be luck and model could stil predicts well after permutation even if feature is importand and so we could throw away important feature.\n\nSecond - by whitch amount score should decline to say that feature is not important?  \n\nThis questions could be adressed by using Statcistics.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Wilcoxon test\nThe Wilcoxon signed-rank test is a non-parametric statistical hypothesis test used to compare two related samples, matched samples, or repeated measurements on a single sample to assess whether their population mean ranks differ (i.e. it is a paired difference test).A Wilcoxon signed-rank test is a nonparametric test that can be used to determine whether two dependent samples were selected from populations having the same distribution.\n\nAnother words, we could use this test to determine whether resulted score after permutation is statisticaly the same as before? If same - the feature is not important, if differ - the feature is important!\n\nTo proceed with this test we need at least 20 samples (scores) before permutation and 20 scores after permutation.\nThis could be achieved by random sampling from the predicted values by model, so we will use ShuffleSplit from sklearn to avhieve it","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Overfitting features\n\nAnother problem in feature selection - there could feture that is important for the train part of the data but not important for the test part - it's because based on this feature model is overfiting on the thain part. The combination of Permutation-Wilcoxon test could by used here to determine such features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"## All features for the model and all this features we will tested\n\nMODEL_FEATURES = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'event_name_1',\n       'event_type_1', 'event_name_2', 'event_type_2', 'snap_CA', 'snap_TX',\n       'snap_WI', 'sell_price', 'year', 'quarter', 'month', 'week', 'day',\n       'dayofweek', 'dayofyear', 'weekday', 'weekofyear', 'is_weekend',\n       'lag_28', 'lag_29', 'lag_30', 'lag_31', 'lag_32', 'lag_33', 'lag_34',\n       'lag_35', 'lag_36', 'lag_37', 'lag_38', 'lag_39', 'lag_40',\n       'rolling_mean_7', 'rolling_std_7', 'rolling_mean_14', 'rolling_std_14',\n       'rolling_mean_30', 'rolling_std_30', 'rolling_mean_60',\n       'rolling_std_60', 'rolling_mean_180', 'rolling_std_180',\n       'rolling_mean_tmp_1_7', 'rolling_mean_tmp_1_14',\n       'rolling_mean_tmp_1_30', 'rolling_mean_tmp_1_60',\n       'rolling_mean_tmp_7_7', 'rolling_mean_tmp_7_14',\n       'rolling_mean_tmp_7_30', 'rolling_mean_tmp_7_60',\n       'rolling_mean_tmp_14_7', 'rolling_mean_tmp_14_14',\n       'rolling_mean_tmp_14_30', 'rolling_mean_tmp_14_60', 'price_momentum',\n       'price_momentum_m', 'price_momentum_y', 'lag_price_t1',\n       'price_change_t1', 'rolling_price_max_t28', 'price_change_t28',\n       'rolling_price_std_t7', 'diff_with_mean_price_by_cat',\n       'diff_with_mean_price_by_dept', 'diff_with_mean_price_by_store',\n       'diff_with_max_price_by_cat', 'diff_with_max_price_by_dept',\n       'diff_with_max_price_by_store', 'diff_with_min_price_by_cat',\n       'diff_with_min_price_by_dept', 'diff_with_min_price_by_store']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_base_test(pred=False):\n    base_test = pd.DataFrame()\n    \n    if pred:\n        name = '/kaggle/input/dark-magic-baseline/test_'\n    else:\n        name = '/kaggle/input/dark-magic-baseline/valid_'\n    \n    for store_id in STORES_IDS:\n        temp_df = pd.read_pickle(name+str(store_id)+'.pkl')\n        temp_df['store_id'] = store_id\n        base_test = pd.concat([base_test, temp_df]).reset_index(drop=True)\n    \n    return base_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def rmse(preds, true):\n    return np.sqrt(metrics.mean_squared_error(preds, true))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Get Permutation - Wilcoxon results for the train part","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"STORES_IDS = ['CA_1', 'CA_2', 'CA_3', 'CA_4', 'TX_1', 'TX_2', 'TX_3', 'WI_1', 'WI_2', 'WI_3']\nSTORES_IDS = ['CA_1', 'TX_1', 'WI_1'] #to speed-up\nTARGET = 'demand'\nfrom sklearn.model_selection import ShuffleSplit\nfrom scipy.stats import mannwhitneyu\nfrom scipy.stats import wilcoxon\n\n# random sampling\nss = ShuffleSplit(n_splits=20, test_size=0.25, random_state=42)\n\ndata = get_base_test()\ndata['store_id'] = data['store_id'].astype('category')\ndata = data[data.store_id.isin(STORES_IDS)]\ndata = data[data.date<='2016-03-27'] #train part\ndata['preds'] = np.float16(0)\ndata = data.reset_index()\n\nestimators = {}\n# get base predictions for the train part w/o permutation\nfor store_id in STORES_IDS:\n    model_path = '/kaggle/input/dark-magic-baseline/lgb_model_'+str(store_id)+'.bin' \n    estimator = pickle.load(open(model_path, 'rb'))\n    estimators[store_id] = estimator\n\nfor store_id in STORES_IDS:\n    mask = data.store_id==store_id\n    data['preds'][mask] = estimators[store_id].predict(data[mask][MODEL_FEATURES])\n\n# get 20 random samples\nbase_line = []\nfor _, test_index in ss.split(data):\n    tmp = data[data.index.isin(test_index)]\n    base_line.append(np.sqrt(metrics.mean_squared_error(tmp['preds'], tmp['demand'])))\n\nprint('Starting permutation test for the train part')\n\nfeature_results = {}\n\nfor feature in MODEL_FEATURES:\n    \n    df = data.copy().reset_index()\n        \n    df[feature] = np.random.permutation(df[feature])\n    if feature in ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'event_name_1',\n       'event_type_1', 'event_name_2', 'event_type_2', 'snap_CA', 'snap_TX',\n       'snap_WI',]:\n            df[feature] = df[feature].astype('category') #to avoid error\n        \n    df['sflpred'] = 0.0\n    \n    #get predictions after permutation\n    for store_id in STORES_IDS:\n        mask = data.store_id==store_id\n        df['sflpred'][mask] = estimators[store_id].predict(df[mask][MODEL_FEATURES])\n    \n    # get 20 random samples\n    shufled_pred_for_feature = []\n    for _, test_index in ss.split(df):\n        tmp1 = df[df.index.isin(test_index)]\n        shufled_pred_for_feature.append(np.sqrt(metrics.mean_squared_error(tmp1['sflpred'], tmp1['demand'])))\n        \n    \n    stat, pvalue = wilcoxon(shufled_pred_for_feature, base_line)\n    alpha = 0.05\n    if pvalue > alpha:\n        res = 'Not Significant' # same distribution => no impact\n    else:\n        res = 'Significant' # different distribution => have impact\n\n    print('[Score baseline=%.4f Score shfl=%.4f]' % (sum(base_line)/len(base_line),  sum(shufled_pred_for_feature)/len(shufled_pred_for_feature)),\n    '[Statistics=%.4f, p=%.4f]' % (stat, pvalue), f'[{res}]', f'[{feature}]')\n\n    feature_results[feature] = res","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Get Permutation - Wilcoxon results for the test part","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"STORES_IDS = ['CA_1', 'CA_2', 'CA_3', 'CA_4', 'TX_1', 'TX_2', 'TX_3', 'WI_1', 'WI_2', 'WI_3']\nSTORES_IDS = ['CA_1', 'TX_1', 'WI_1'] #to speed-up\nTARGET = 'demand'\nfrom sklearn.model_selection import ShuffleSplit\nfrom scipy.stats import mannwhitneyu\nfrom scipy.stats import wilcoxon\n\nss = ShuffleSplit(n_splits=20, test_size=0.25, random_state=42)\n\ndata = get_base_test()\ndata['store_id'] = data['store_id'].astype('category')\ndata = data[data.store_id.isin(STORES_IDS)]\ndata = data[data.date>'2016-03-27']\ndata['preds'] = np.float16(0)\ndata = data.reset_index()\n\nestimators = {}\n\nfor store_id in STORES_IDS:\n    model_path = '/kaggle/input/dark-magic-baseline/lgb_model_'+str(store_id)+'.bin' \n    estimator = pickle.load(open(model_path, 'rb'))\n    estimators[store_id] = estimator\n\nfor store_id in STORES_IDS:\n    mask = data.store_id==store_id\n    data['preds'][mask] = estimators[store_id].predict(data[mask][MODEL_FEATURES])\n\nbase_line = []\nfor _, test_index in ss.split(data):\n    tmp = data[data.index.isin(test_index)]\n    base_line.append(np.sqrt(metrics.mean_squared_error(tmp['preds'], tmp['demand'])))\n\nprint('Starting permutation test for validation part')\n\nfeature_pred_results = {}\n\nfor feature in MODEL_FEATURES:#['item_id']: \n    \n    df = data.copy().reset_index()\n    \n#     if df[feature].dtypes.name != 'category':\n    df[feature] = np.random.permutation(df[feature])\n    if feature in ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'event_name_1',\n       'event_type_1', 'event_name_2', 'event_type_2', 'snap_CA', 'snap_TX',\n       'snap_WI',]:\n            df[feature] = df[feature].astype('category')\n        \n    df['sflpred'] = 0.0\n    \n    for store_id in STORES_IDS:\n        mask = df.store_id==store_id\n        df['sflpred'][mask] = estimators[store_id].predict(df[mask][MODEL_FEATURES])\n    \n    shufled_pred_for_feature = []\n    \n    for _, test_index in ss.split(df):\n        tmp1 = df[df.index.isin(test_index)]\n        shufled_pred_for_feature.append(np.sqrt(metrics.mean_squared_error(tmp1['sflpred'], tmp1['demand'])))\n        \n        \n    stat, pvalue = wilcoxon(shufled_pred_for_feature, base_line)\n    alpha = 0.05\n    if pvalue > alpha:\n        res = 'Not Significant' # same distribution => no impact\n    else:\n        res = 'Significant' # different distribution => have impact\n\n    print('[Score baseline=%.4f Score shfl=%.4f]' % (sum(base_line)/len(base_line),  sum(shufled_pred_for_feature)/len(shufled_pred_for_feature)),\n    '[Statistics=%.4f, p=%.4f]' % (stat, pvalue), f'[{res}]', f'[{feature}]')\n\n    feature_pred_results[feature] = res","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Determine overfitting features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for feat in feature_pred_results.keys():\n    tr = feature_results[feat]\n    pr = feature_pred_results[feat]\n    print(f'train [{tr}] pred [{pr}] [{feat}]')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}