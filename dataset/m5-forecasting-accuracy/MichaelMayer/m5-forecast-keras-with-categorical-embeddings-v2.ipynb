{"cells":[{"metadata":{},"cell_type":"markdown","source":"# M5 Forecast: Keras with Categorical Embeddings V2\n\nThis notebook tries to model expected sales of product groups. Since many of the features are categorical, we use this example to show how embedding layers make life easy when dealing with categoric inputs for neural nets by skipping the step of making dummy variables by hand.\n\nData preprocessing and feature engineering is very similar (but not identical) to this [R kernel](https://www.kaggle.com/mayer79/m5-forecast-keras-embeddings-with-r) and uses ideas from the two excellent kernels [Very fst Model](https://www.kaggle.com/ragnar123/very-fst-model) and [M5 ForecasteR](https://www.kaggle.com/kailex/m5-forecaster-0-57330).\n\nTo gain an extra 3 GB of RAM, we do not use GPU acceleration for the training."},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport gc\nimport os\nfrom tqdm.notebook import tqdm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load data"},{"metadata":{"trusted":true},"cell_type":"code","source":"path = \"../input/m5-forecasting-accuracy\"\n\ncalendar = pd.read_csv(os.path.join(path, \"calendar.csv\"))\nselling_prices = pd.read_csv(os.path.join(path, \"sell_prices.csv\"))\nsample_submission = pd.read_csv(os.path.join(path, \"sample_submission.csv\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales = pd.read_csv(os.path.join(path, \"sales_train_validation.csv\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Describe and prepare data\n\nWe will now go through all data sets and prepare them for modelling."},{"metadata":{},"cell_type":"markdown","source":"### Calendar data\n\nFor each date (covering both training and test data), we have access to useful calendar information."},{"metadata":{"trusted":true},"cell_type":"code","source":"calendar.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i, var in enumerate([\"year\", \"weekday\", \"month\", \"event_name_1\", \"event_name_2\", \n                         \"event_type_1\", \"event_type_2\", \"snap_CA\", \"snap_TX\", \"snap_WI\"]):\n    plt.figure()\n    g = sns.countplot(calendar[var])\n    g.set_xticklabels(g.get_xticklabels(), rotation=45)\n    g.set_title(var)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OrdinalEncoder\n\ndef prep_calendar(df):\n    df = df.drop([\"date\", \"weekday\"], axis=1)\n    df = df.assign(d = df.d.str[2:].astype(int))\n    df = df.fillna(\"missing\")\n    cols = list(set(df.columns) - {\"wm_yr_wk\", \"d\"})\n    df[cols] = OrdinalEncoder(dtype=\"int\").fit_transform(df[cols])\n    df = reduce_mem_usage(df)\n    return df\n\ncalendar = prep_calendar(calendar)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"calendar.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Notes for modeling\n\n**Features** deemed to be useful:\n\n- \"wday\", \"year\", \"month\" -> integer coding & embedding\n- \"event_name_1\", \"event_type_1\" -> integer coding & embedding\n- \"snap_XX\" -> numeric (they are dummies)\n\n**Reshape required**: No\n\n**Merge key(s)**: \"d\", \"wm_yr_wk\""},{"metadata":{},"cell_type":"markdown","source":"### Selling prices\n\nContains selling prices for each store_id, item_id_wm_yr_wk combination."},{"metadata":{"trusted":true},"cell_type":"code","source":"selling_prices.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Derive some time related features:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def prep_selling_prices(df):\n    gr = df.groupby([\"store_id\", \"item_id\"])[\"sell_price\"]\n    df[\"sell_price_rel_diff\"] = gr.pct_change()\n    df[\"sell_price_roll_sd7\"] = gr.transform(lambda x: x.rolling(7).std())\n    df[\"sell_price_cumrel\"] = (gr.shift(0) - gr.cummin()) / (1 + gr.cummax() - gr.cummin())\n    df = reduce_mem_usage(df)\n    return df\n\nselling_prices = prep_selling_prices(selling_prices)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"selling_prices.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Notes for modeling\n\n**Features**:\n\n- sell_price and derived features -> numeric\n\n**Reshape**: No\n\n**Merge key(s)**: to sales data by store_id, item_id, wm_yr_wk (through calendar data)"},{"metadata":{},"cell_type":"markdown","source":"### Sales data\n\nContains the number of sold items (= our response) as well as some categorical features."},{"metadata":{"trusted":true},"cell_type":"code","source":"sales.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i, var in enumerate([\"state_id\", \"store_id\", \"cat_id\", \"dept_id\"]):\n    plt.figure()\n    g = sns.countplot(sales[var])\n    g.set_xticklabels(g.get_xticklabels(), rotation=45)\n    g.set_title(var)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales.item_id.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Reshaping\n\nWe now reshape the data from wide to long, using \"id\" as fixed and swapping \"d_x\" columns. Along this process, we also add structure for submission data and reduce data size."},{"metadata":{"trusted":true},"cell_type":"code","source":"def reshape_sales(df, drop_d = None):\n    if drop_d is not None:\n        df = df.drop([\"d_\" + str(i + 1) for i in range(drop_d)], axis=1)\n    df = df.assign(id=df.id.str.replace(\"_validation\", \"\"))\n    df = df.reindex(columns=df.columns.tolist() + [\"d_\" + str(1913 + i + 1) for i in range(2 * 28)])\n    df = df.melt(id_vars=[\"id\", \"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"],\n                 var_name='d', value_name='demand')\n    df = df.assign(d=df.d.str[2:].astype(\"int16\"))\n    return df\n\nsales = reshape_sales(sales, 1000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Distribution of the response"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(sales[\"demand\"][sales[\"demand\"] <= 10]);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Add time-lagged features\n\nAdd some of the derived features from kernel https://www.kaggle.com/ragnar123/very-fst-model."},{"metadata":{"trusted":true},"cell_type":"code","source":"def prep_sales(df):\n    df['lag_t28'] = df.groupby(['id'])['demand'].transform(lambda x: x.shift(28))\n    df['rolling_mean_t7'] = df.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(7).mean())\n    df['rolling_mean_t30'] = df.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(30).mean())\n    df['rolling_mean_t60'] = df.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(60).mean())\n    df['rolling_mean_t90'] = df.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(90).mean())\n    df['rolling_mean_t180'] = df.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(180).mean())\n    df['rolling_std_t7'] = df.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(7).std())\n    df['rolling_std_t30'] = df.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(30).std())\n\n    # Remove rows with NAs except for submission rows. rolling_mean_t180 was selected as it produces most missings\n    df = df[(df.d >= 1914) | (pd.notna(df.rolling_mean_t180))]\n    df = reduce_mem_usage(df)\n\n    return df\n\nsales = prep_sales(sales)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Notes for modeling\n\n**Features**\n- \"dept_id\", \"item_id\", \"store_id\": Integer coding & embedding\n- lagged features derived from response: Numeric\n\n**Reshape**:\n- Reshape days as \"d\" from wide to long -> \"demand\" will be response variable\n\n**Merges**:\n1. Join calendar features by \"d\"\n2. Join selling prices by \"store_id\", \"item_id\" and (\"wm_yr_wk\" from calendar)\n\nComment: Submission dates: \"d_1914\" - \"d_1969\""},{"metadata":{},"cell_type":"markdown","source":"### Combine data sources"},{"metadata":{"trusted":true},"cell_type":"code","source":"sales = sales.merge(calendar, how=\"left\", on=\"d\")\ngc.collect()\nsales.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales = sales.merge(selling_prices, how=\"left\", on=[\"wm_yr_wk\", \"store_id\", \"item_id\"])\nsales.drop([\"wm_yr_wk\"], axis=1, inplace=True)\ngc.collect()\nsales.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del selling_prices","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Prepare data for Keras interface"},{"metadata":{},"cell_type":"markdown","source":"### Ordinal encoding of remaining categoricals"},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_id_cols = [\"item_id\", \"dept_id\", \"store_id\", \"cat_id\", \"state_id\"]\ncat_cols = cat_id_cols + [\"wday\", \"month\", \"year\", \"event_name_1\", \n                          \"event_type_1\", \"event_name_2\", \"event_type_2\"]\n\n# In loop to minimize memory use\nfor i, v in tqdm(enumerate(cat_id_cols)):\n    sales[v] = OrdinalEncoder(dtype=\"int\").fit_transform(sales[[v]])\n\nsales = reduce_mem_usage(sales)\nsales.head()\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Impute numeric columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_cols = [\"sell_price\", \"sell_price_rel_diff\", \"sell_price_roll_sd7\", \"sell_price_cumrel\",\n            \"lag_t28\", \"rolling_mean_t7\", \"rolling_mean_t30\", \"rolling_mean_t60\", \n            \"rolling_mean_t90\", \"rolling_mean_t180\", \"rolling_std_t7\", \"rolling_std_t30\"]\nbool_cols = [\"snap_CA\", \"snap_TX\", \"snap_WI\"]\ndense_cols = num_cols + bool_cols\n\n# Need to do column by column due to memory constraints\nfor i, v in tqdm(enumerate(num_cols)):\n    sales[v] = sales[v].fillna(sales[v].median())\n    \nsales.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Separate submission data and reconstruct id columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"test = sales[sales.d >= 1914]\ntest = test.assign(id=test.id + \"_\" + np.where(test.d <= 1941, \"validation\", \"evaluation\"),\n                   F=\"F\" + (test.d - 1913 - 28 * (test.d > 1941)).astype(\"str\"))\ntest.head()\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Make training data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Input dict for training with a dense array and separate inputs for each embedding input\ndef make_X(df):\n    X = {\"dense1\": df[dense_cols].to_numpy()}\n    for i, v in enumerate(cat_cols):\n        X[v] = df[[v]].to_numpy()\n    return X\n\n# Submission data\nX_test = make_X(test)\n\n# One month of validation data\nflag = (sales.d < 1914) & (sales.d >= 1914 - 28)\nvalid = (make_X(sales[flag]),\n         sales[\"demand\"][flag])\n\n# Rest is used for training\nflag = sales.d < 1914 - 28\nX_train = make_X(sales[flag])\ny_train = sales[\"demand\"][flag]\n                             \ndel sales, flag\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.unique(X_train[\"state_id\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## The model"},{"metadata":{},"cell_type":"markdown","source":"### Imports"},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow.keras as keras\n\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.layers import Dense, Input, Embedding, Dropout, concatenate, Flatten\nfrom tensorflow.keras.models import Model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Architecture with embeddings"},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model(lr=0.002):\n    tf.random.set_seed(173)\n\n    tf.keras.backend.clear_session()\n    gc.collect()\n\n    # Dense input\n    dense_input = Input(shape=(len(dense_cols), ), name='dense1')\n\n    # Embedding input\n    wday_input = Input(shape=(1,), name='wday')\n    month_input = Input(shape=(1,), name='month')\n    year_input = Input(shape=(1,), name='year')\n    event_name_1_input = Input(shape=(1,), name='event_name_1')\n    event_type_1_input = Input(shape=(1,), name='event_type_1')\n    event_name_2_input = Input(shape=(1,), name='event_name_2')\n    event_type_2_input = Input(shape=(1,), name='event_type_2')\n    item_id_input = Input(shape=(1,), name='item_id')\n    dept_id_input = Input(shape=(1,), name='dept_id')\n    store_id_input = Input(shape=(1,), name='store_id')\n    cat_id_input = Input(shape=(1,), name='cat_id')\n    state_id_input = Input(shape=(1,), name='state_id')\n\n    wday_emb = Flatten()(Embedding(7, 1)(wday_input))\n    month_emb = Flatten()(Embedding(12, 1)(month_input))\n    year_emb = Flatten()(Embedding(6, 1)(year_input))\n    event_name_1_emb = Flatten()(Embedding(31, 1)(event_name_1_input))\n    event_type_1_emb = Flatten()(Embedding(5, 1)(event_type_1_input))\n    event_name_2_emb = Flatten()(Embedding(5, 1)(event_name_2_input))\n    event_type_2_emb = Flatten()(Embedding(5, 1)(event_type_2_input))\n\n    item_id_emb = Flatten()(Embedding(3049, 3)(item_id_input))\n    dept_id_emb = Flatten()(Embedding(7, 1)(dept_id_input))\n    store_id_emb = Flatten()(Embedding(10, 1)(store_id_input))\n    cat_id_emb = Flatten()(Embedding(3, 1)(cat_id_input))\n    state_id_emb = Flatten()(Embedding(3, 1)(state_id_input))\n\n    # Combine dense and embedding parts and add dense layers. Exit on linear scale.\n    x = concatenate([dense_input, wday_emb, month_emb, year_emb, \n                     event_name_1_emb, event_type_1_emb, \n                     event_name_2_emb, event_type_2_emb, \n                     item_id_emb, dept_id_emb, store_id_emb,\n                     cat_id_emb, state_id_emb])\n    x = Dense(150, activation=\"tanh\")(x)\n    x = Dense(75, activation=\"tanh\")(x)\n    x = Dense(10, activation=\"tanh\")(x)\n    outputs = Dense(1, activation=\"linear\", name='output')(x)\n\n    inputs = {\"dense1\": dense_input, \"wday\": wday_input, \"month\": month_input, \"year\": year_input, \n              \"event_name_1\": event_name_1_input, \"event_type_1\": event_type_1_input,\n              \"event_name_2\": event_name_2_input, \"event_type_2\": event_type_2_input,\n              \"item_id\": item_id_input, \"dept_id\": dept_id_input, \"store_id\": store_id_input, \n              \"cat_id\": cat_id_input, \"state_id\": state_id_input}\n\n    # Connect input and output\n    model = Model(inputs, outputs)\n\n    model.compile(loss=keras.losses.mean_squared_error,\n                  metrics=[\"mse\"],\n                  optimizer=keras.optimizers.RMSprop(learning_rate=lr))\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = create_model(0.0002)\nmodel.summary()\nkeras.utils.plot_model(model, 'model.png', show_shapes=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Calculate derivatives and fit model"},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(X_train, \n                    y_train,\n                    batch_size=10000,\n                    epochs=30,\n                    shuffle=True,\n                    validation_data=valid)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Plot the evaluation metrics over epochs"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'valid'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history.history[\"val_loss\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"model.save('model.h5')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = model.predict(X_test, batch_size=10000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test[\"demand\"] = pred.clip(0)\nsubmission = test.pivot(index=\"id\", columns=\"F\", values=\"demand\").reset_index()[sample_submission.columns]\nsubmission = sample_submission[[\"id\"]].merge(submission, how=\"left\", on=\"id\")\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission[sample_submission.id==\"FOODS_1_001_TX_2_validation\"].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"submission.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":4}