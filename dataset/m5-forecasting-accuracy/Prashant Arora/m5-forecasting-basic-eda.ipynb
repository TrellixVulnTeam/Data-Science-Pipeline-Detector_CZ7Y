{"cells":[{"metadata":{},"cell_type":"markdown","source":"Lets have a look over the data first and then perform the basic EDA.","execution_count":null},{"metadata":{"id":"j0_Pe5rO6mSu","trusted":false},"cell_type":"code","source":"import pandas as pd\ncalendar = pd.read_csv(\"../input/m5-forecasting-accuracy/calendar.csv\")\ncalendar.fillna(\"missing\")\n\nsell_prices  =pd.read_csv(\"../input/m5-forecasting-accuracy/sell_prices.csv\")\nsales_train_validation = pd.read_csv(\"../input/m5-forecasting-accuracy/sales_train_validation.csv\")","execution_count":null,"outputs":[]},{"metadata":{"id":"rjr1efoZ8auW","outputId":"04d6939a-edcf-4b9f-cd65-00d685708a6d","trusted":false},"cell_type":"code","source":"calendar","execution_count":null,"outputs":[]},{"metadata":{"id":"inPEKyhz8ufA","outputId":"b408a90e-b9fc-4a0e-f190-94429e7818d0","trusted":false},"cell_type":"code","source":"sell_prices","execution_count":null,"outputs":[]},{"metadata":{"id":"S6hUpl0M8w56","outputId":"5d589ab0-904d-4fda-eaf1-2ea23d9fa3d5","trusted":false},"cell_type":"code","source":"sales_train_validation","execution_count":null,"outputs":[]},{"metadata":{"id":"Sbab8kVsyvhe","trusted":false},"cell_type":"code","source":"sales_train_validation['updated_ID'] = sales_train_validation.id.apply(lambda x:\"_\".join(x.split(\"_\")[:-1]))","execution_count":null,"outputs":[]},{"metadata":{"id":"qHjUyd1zZXef","outputId":"35ab8fd8-0957-4ebd-87ce-82d03db9e40b","trusted":false},"cell_type":"code","source":"sales_train_validation[['dept_id',\"cat_id\",\"store_id\",\"item_id\"]].nunique()","execution_count":null,"outputs":[]},{"metadata":{"id":"LBZP81aE3rrz"},"cell_type":"markdown","source":" Note: This notebook is almost a duplicate of [this kernel](https://www.kaggle.com/tarunpaparaju/m5-competition-eda-models). I have just created this learning purpose . You can have look at the original kernel too.","execution_count":null},{"metadata":{"id":"yDjv7IG5t6n-","outputId":"6896f7c3-81ca-49ab-cbab-ce722d704b5d","trusted":false},"cell_type":"code","source":"# reference from https://www.kaggle.com/tarunpaparaju/m5-competition-eda-models\nimport os\nimport gc\nimport time\nimport math\nimport datetime\nfrom math import log, floor\nfrom sklearn.neighbors import KDTree\n\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom sklearn.utils import shuffle\nfrom tqdm.notebook import tqdm as tqdm\n\nimport seaborn as sns\nfrom matplotlib import colors\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import Normalize\n\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\n\nimport pywt\nfrom statsmodels.robust import mad\n\nimport scipy\nimport statsmodels\nfrom scipy import signal\nimport statsmodels.api as sm\nfrom fbprophet import Prophet\nfrom scipy.signal import butter, deconvolve\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"id":"Y4-5XjGYvH7v","outputId":"a34276b4-6ca5-4021-abf5-03c983641f98","trusted":false},"cell_type":"code","source":"ids = sorted(list(set(sales_train_validation['id'])))\nd_cols = [c for c in sales_train_validation.columns if 'd_' in c]\nx_1 = sales_train_validation.loc[sales_train_validation['id'] == ids[2]].set_index('id')[d_cols].values[0]\nx_2 = sales_train_validation.loc[sales_train_validation['id'] == ids[1]].set_index('id')[d_cols].values[0]\nx_3 = sales_train_validation.loc[sales_train_validation['id'] == ids[17]].set_index('id')[d_cols].values[0]\n\nfig = make_subplots(rows=3, cols=1)\n\nfig.add_trace(go.Scatter(x=np.arange(len(x_1)), y=x_1, showlegend=False,\n                    mode='lines', name=\"First sample\",\n                         marker=dict(color=\"red\")),\n             row=1, col=1)\n\nfig.add_trace(go.Scatter(x=np.arange(len(x_2)), y=x_2, showlegend=False,\n                    mode='lines', name=\"Second sample\",\n                         marker=dict(color=\"violet\")),\n             row=2, col=1)\n\nfig.add_trace(go.Scatter(x=np.arange(len(x_3)), y=x_3, showlegend=False,\n                    mode='lines', name=\"Third sample\",\n                         marker=dict(color=\"dodgerblue\")),\n             row=3, col=1)\n\nfig.update_layout(height=1200, width=800, title_text=\"Sample sales\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"Rm86kG_Rxirh"},"cell_type":"markdown","source":"Above plot represents sales of 3 randomly choosen stores. We can see that these sales are so inconsistent and hence we can infer that sales of any store depends upon various other features as we see on some days these sales have gone down to 0 , may be there was no stock or some kind of problem in that area the store didn't open or even the customers liked the product so much that they have stocked it in there house as they want to go out.","execution_count":null},{"metadata":{"id":"ga-spydCxUa9","outputId":"fa4c07b3-cac1-4bbc-8039-21bf6776ba34","trusted":false},"cell_type":"code","source":"ids = sorted(list(set(sales_train_validation['id'])))\nd_cols = [c for c in sales_train_validation.columns if 'd_' in c]\nx_1 = sales_train_validation.loc[sales_train_validation['id'] == ids[2]].set_index('id')[d_cols].values[0][:90]\nx_2 = sales_train_validation.loc[sales_train_validation['id'] == ids[1]].set_index('id')[d_cols].values[0][0:90]\nx_3 = sales_train_validation.loc[sales_train_validation['id'] == ids[17]].set_index('id')[d_cols].values[0][0:90]\nfig = make_subplots(rows=3, cols=1)\n\nfig.add_trace(go.Scatter(x=np.arange(len(x_1)), y=x_1, showlegend=False,\n                    mode='lines+markers', name=\"First sample\",\n                         marker=dict(color=\"mediumseagreen\")),\n             row=1, col=1)\n\nfig.add_trace(go.Scatter(x=np.arange(len(x_2)), y=x_2, showlegend=False,\n                    mode='lines+markers', name=\"Second sample\",\n                         marker=dict(color=\"violet\")),\n             row=2, col=1)\n\nfig.add_trace(go.Scatter(x=np.arange(len(x_3)), y=x_3, showlegend=False,\n                    mode='lines+markers', name=\"Third sample\",\n                         marker=dict(color=\"dodgerblue\")),\n             row=3, col=1)\n\nfig.update_layout(height=1200, width=800, title_text=\"Sample sales snippets\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"YSh49AEcyjT5"},"cell_type":"markdown","source":"In the above plots i have just plotted graph of sales for first 90 days of same store which i had used in the previous graph. As you can see , the sales are very volatile as stated before too.","execution_count":null},{"metadata":{"id":"JaQ0vG_G1j-v"},"cell_type":"markdown","source":"# Denoising Data","execution_count":null},{"metadata":{"id":"Wazu6U8a11jl"},"cell_type":"markdown","source":"Denoising is a technique to find trend in time-series data , although it may loose some useful information in the data but it helps us to understand the trend more better.","execution_count":null},{"metadata":{"id":"VI_zi_gw1nvz"},"cell_type":"markdown","source":"## Wavelet Denoising","execution_count":null},{"metadata":{"id":"UlByEpaK3jYV"},"cell_type":"markdown","source":"Wavelet denoising (usually used with electric signals) is a way to remove the unnecessary noise from a time series. This method calculates coefficients called the \"wavelet coefficients\". These coefficients decide which pieces of information to keep (signal) and which ones to discard (noise).\n\nWe make use of the MAD (mean absolute deviation) value to understand the randomness in the sales and accordingly decide the minimum threshold for the wavelet coefficients in the time series. We filter out the low coefficients from the wavelets and reconstruct the sales data from the remaining coefficients and that's it; we have successfully removed noise from the sales data.","execution_count":null},{"metadata":{"id":"EajBvuUZy1AR","trusted":false},"cell_type":"code","source":"def maddest(d, axis=None):\n    return np.mean(np.absolute(d - np.mean(d, axis)), axis)\n\ndef denoise_signal(x, wavelet='db4', level=1):\n    coeff = pywt.wavedec(x, wavelet, mode=\"per\")\n    sigma = (1/0.6745) * maddest(coeff[-level])\n\n    uthresh = sigma * np.sqrt(2*np.log(len(x)))\n    coeff[1:] = (pywt.threshold(i, value=uthresh, mode='hard') for i in coeff[1:])\n\n    return pywt.waverec(coeff, wavelet, mode='per')","execution_count":null,"outputs":[]},{"metadata":{"id":"RyMNNpr_zrpr","outputId":"4db9e5cb-dbc9-478b-ba08-f4b499bdf533","trusted":false},"cell_type":"code","source":"y_w1 = denoise_signal(x_1)\ny_w2 = denoise_signal(x_2)\ny_w3 = denoise_signal(x_3)\n\n\nfig = make_subplots(rows=3, cols=1)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(len(x_1)), mode='lines+markers', y=x_1, marker=dict(color=\"mediumaquamarine\"), showlegend=False,\n               name=\"Original signal\"),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(len(x_1)), y=y_w1, mode='lines', marker=dict(color=\"darkgreen\"), showlegend=False,\n               name=\"Denoised signal\"),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(len(x_2)), mode='lines+markers', y=x_2, marker=dict(color=\"thistle\"), showlegend=False),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(len(x_2)), y=y_w2, mode='lines', marker=dict(color=\"purple\"), showlegend=False),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(len(x_3)), mode='lines+markers', y=x_3, marker=dict(color=\"lightskyblue\"), showlegend=False),\n    row=3, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(len(x_3)), y=y_w3, mode='lines', marker=dict(color=\"navy\"), showlegend=False),\n    row=3, col=1\n)\n\nfig.update_layout(height=1200, width=800, title_text=\"Original (pale) vs. Denoised (dark) sales\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"eCwOWgmY1QKq","outputId":"2a65cafc-19c5-4cae-f31c-f2216ed7db2a","trusted":false},"cell_type":"code","source":"fig, ax = plt.subplots(nrows=3, ncols=2, figsize=(30, 20))\n\nax[0, 0].plot(x_1, color='seagreen', marker='o') \nax[0, 0].set_title('Original Sales', fontsize=24)\nax[0, 1].plot(y_w1, color='red', marker='.') \nax[0, 1].set_title('After Wavelet Denoising', fontsize=24)\n\nax[1, 0].plot(x_2, color='seagreen', marker='o') \nax[1, 0].set_title('Original Sales', fontsize=24)\nax[1, 1].plot(y_w2, color='red', marker='.') \nax[1, 1].set_title('After Wavelet Denoising', fontsize=24)\n\nax[2, 0].plot(x_3, color='seagreen', marker='o') \nax[2, 0].set_title('Original Sales', fontsize=24)\nax[2, 1].plot(y_w3, color='red', marker='.') \nax[2, 1].set_title('After Wavelet Denoising', fontsize=24)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"kB1FgQyk4Lhu"},"cell_type":"markdown","source":"The above graph shows the actual time series graph on the left side and on right side we have shown wavelet denoised time series data, just used to extract trends from the original data.","execution_count":null},{"metadata":{"id":"yUKptYTX1ttJ"},"cell_type":"markdown","source":"## Average Smoothing","execution_count":null},{"metadata":{"id":"svHObUcc4ykt"},"cell_type":"markdown","source":"A different technique to extract trends from our data.Both the Wavelet and Avg. smoothing techniques provides different trends in our data , but to use them as data to train model on , is not a good choice as they may lack some important details about the data which we might require for better performance.","execution_count":null},{"metadata":{"id":"0atdhoVM1Qpr","trusted":false},"cell_type":"code","source":"def average_smoothing(signal, kernel_size=5, stride=1):\n    sample = []\n    start = 0\n    end = kernel_size\n    while end <= len(signal):\n        start = start + stride\n        end = end + stride\n        sample.extend(np.ones(end - start)*np.mean(signal[start:end]))\n    return np.array(sample)","execution_count":null,"outputs":[]},{"metadata":{"id":"q0PARo7U15nq","outputId":"4c778364-a3e3-4bbd-fd17-a13fe0039d1c","trusted":false},"cell_type":"code","source":"y_a1 = average_smoothing(x_1)\ny_a2 = average_smoothing(x_2)\ny_a3 = average_smoothing(x_3)\n\nfig = make_subplots(rows=3, cols=1)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(len(x_1)), mode='lines+markers', y=x_1, marker=dict(color=\"lightskyblue\"), showlegend=False,\n               name=\"Original sales\"),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(len(x_1)), y=y_a1, mode='lines', marker=dict(color=\"navy\"), showlegend=False,\n               name=\"Denoised sales\"),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(len(x_2)), mode='lines+markers', y=x_2, marker=dict(color=\"thistle\"), showlegend=False),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(len(x_2)), y=y_a2, mode='lines', marker=dict(color=\"indigo\"), showlegend=False),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(len(x_3)), mode='lines+markers', y=x_3, marker=dict(color=\"mediumaquamarine\"), showlegend=False),\n    row=3, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(len(x_3)), y=y_a3, mode='lines', marker=dict(color=\"darkgreen\"), showlegend=False),\n    row=3, col=1\n)\n\nfig.update_layout(height=1200, width=800, title_text=\"Original (pale) vs. Denoised (dark) signals\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"GtBVHj762AWX","outputId":"70dba3ee-1959-42bf-b169-3047b36c8b66","trusted":false},"cell_type":"code","source":"fig, ax = plt.subplots(nrows=3, ncols=3, figsize=(30, 20))\n\nax[0, 0].plot(x_1, color='seagreen', marker='o') \nax[0, 0].set_title('Original Sales', fontsize=24)\nax[0, 1].plot(y_w1, color='red', marker='.') \nax[0, 1].set_title('After Wavelet Denoising', fontsize=24)\nax[0, 2].plot(y_a1, color='red', marker='.') \nax[0, 2].set_title('After Average_smoothing', fontsize=24)\n\n\nax[1, 0].plot(x_2, color='seagreen', marker='o') \nax[1, 0].set_title('Original Sales', fontsize=24)\nax[1, 1].plot(y_w2, color='red', marker='.') \nax[1, 1].set_title('After Wavelet Denoising', fontsize=24)\nax[1, 2].plot(y_a2, color='red', marker='.') \nax[1, 2].set_title('After Average Smoothing', fontsize=24)\n\nax[2, 0].plot(x_3, color='seagreen', marker='o') \nax[2, 0].set_title('Original Sales', fontsize=24)\nax[2, 1].plot(y_w3, color='red', marker='.') \nax[2, 1].set_title('After Wavelet Denoising', fontsize=24)\nax[2, 2].plot(y_a3, color='red', marker='.') \nax[2, 2].set_title('After Average Smoothing', fontsize=24)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"KBExDosS5VHa"},"cell_type":"markdown","source":"here , i have plotted the actual data, wavelet denoised trend data, and average smoothed trend data. Just to show there representations with each other. ","execution_count":null},{"metadata":{"id":"UupQPmVA6z7P","trusted":false},"cell_type":"code","source":"def max_val_smoothing(signal, kernel_size=5, stride=1):\n    sample = []\n    start = 0\n    end = kernel_size\n    while end <= len(signal):\n        start = start + stride\n        end = end + stride\n        sample.extend(np.ones(end - start)*np.ma(signal[start:end]))\n    return np.array(sample)","execution_count":null,"outputs":[]},{"metadata":{"outputId":"0ba1cd7a-8dad-49aa-9f53-b81b300a8e9f","id":"6UA4Cbs-6z7f","trusted":false},"cell_type":"code","source":"y_a1 = average_smoothing(x_1)\ny_a2 = average_smoothing(x_2)\ny_a3 = average_smoothing(x_3)\n\nfig = make_subplots(rows=3, cols=1)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(len(x_1)), mode='lines+markers', y=x_1, marker=dict(color=\"lightskyblue\"), showlegend=False,\n               name=\"Original sales\"),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(len(x_1)), y=y_a1, mode='lines', marker=dict(color=\"navy\"), showlegend=False,\n               name=\"Denoised sales\"),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(len(x_2)), mode='lines+markers', y=x_2, marker=dict(color=\"thistle\"), showlegend=False),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(len(x_2)), y=y_a2, mode='lines', marker=dict(color=\"indigo\"), showlegend=False),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(len(x_3)), mode='lines+markers', y=x_3, marker=dict(color=\"mediumaquamarine\"), showlegend=False),\n    row=3, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(len(x_3)), y=y_a3, mode='lines', marker=dict(color=\"darkgreen\"), showlegend=False),\n    row=3, col=1\n)\n\nfig.update_layout(height=1200, width=800, title_text=\"Original (pale) vs. Denoised (dark) signals\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"DNZCxa1I2RBp"},"cell_type":"markdown","source":"The above plot shows details the same way  as we get from  average smoothing , but just a small difference is that we have used max value of the window instead of the mean value to plot the graph , showing a bit of different trend front the averge one. ","execution_count":null},{"metadata":{"id":"ZziX-Jbr-_mF","trusted":false},"cell_type":"code","source":"past_sales = sales_train_validation.set_index('id')[d_cols] \\\n    .T \\\n    .merge(calendar.set_index('d')['date'],\n           left_index=True,\n           right_index=True,\n            validate='1:1') \\\n    .set_index('date')\n\nstore_list = sell_prices['store_id'].unique()\n","execution_count":null,"outputs":[]},{"metadata":{"id":"Fbbaf9wd59gc","outputId":"ec7b61bc-2eb1-4ba0-b86d-6e474ee34ec8","trusted":false},"cell_type":"code","source":"means = []\nfig = go.Figure()\nfor s in store_list:\n  store_items = [c for c in past_sales.columns if s in c]\n  data = past_sales[store_items].sum(axis=1).rolling(90).mean()\n  means.append(np.mean(past_sales[store_items].sum(axis=1)))\n  fig.add_trace(go.Scatter(x=np.arange(len(data)), y=data, name=s))\n    \nfig.update_layout(yaxis_title=\"Sales\", xaxis_title=\"Time\", title=\"Rolling Average Sales vs. Time (per store)\")","execution_count":null,"outputs":[]},{"metadata":{"id":"NuwamzYm29c0"},"cell_type":"markdown","source":"Here we have plotted rolling means of 90 days of the sales based of different store present in clifornia , texas and winsonica locations.\nShowing various trends in there sales. \n\nWe cans see that CA-3 has maintained high sales over the given period of time , but it looks like a kind of period graph which after a time gets reduced in sales but then again it heads up to overcome the previous top mark. \n\nAn all time increase and maintained sales have been marked in WI_2. there is no such time they have got there sales reduced to much.","execution_count":null},{"metadata":{"id":"aMFVStTPB0Ll","outputId":"5e5bf260-7840-459d-d597-b5b1bc561629","trusted":false},"cell_type":"code","source":"fig = go.Figure()\n\nfor i, s in enumerate(store_list):\n  store_items = [c for c in past_sales.columns if s in c]\n  data = past_sales[store_items].sum(axis=1).rolling(90).mean()\n  fig.add_trace(go.Box(x=[s]*len(data), y=data, name=s))\n    \nfig.update_layout(yaxis_title=\"Sales\", xaxis_title=\"Time\", title=\"Rolling Average Sales vs. Store name \")","execution_count":null,"outputs":[]},{"metadata":{"id":"hctipytR6L8A"},"cell_type":"markdown","source":"The box plot shows the sales ditribution in different Stores provided in the dataset. We can infer that california sales vs stores have the highest variance and california has also maintained the highest overall mean sale. Where other Texas and Winsonica have maintained there sales with no so much varation and also each store in Texas have similar sales ,whereas in Winsonica there is high varaince in sales of each store","execution_count":null},{"metadata":{"id":"CuWsJ4PdB5dO","outputId":"bba99195-4014-4748-e7b6-e741f649d27d","trusted":false},"cell_type":"code","source":"df = pd.DataFrame(np.transpose([means, store_list]))\ndf.columns = [\"Mean sales\", \"Store name\"]\npx.bar(df, y=\"Mean sales\", x=\"Store name\", color=\"Store name\", title=\"Mean sales vs. Store name\")","execution_count":null,"outputs":[]},{"metadata":{"id":"ugPkE9IE6_ug"},"cell_type":"markdown","source":"The baar plot shows all sales of different stores over a period of time. ","execution_count":null},{"metadata":{"id":"BOSyaoGNDY-k","outputId":"208f5d07-000c-4870-d53e-4e818cc4825d","trusted":false},"cell_type":"code","source":"greens = [\"darkgreen\", \"mediumseagreen\", \"seagreen\", \"green\"]\nstore_list = sell_prices['store_id'].unique()\nfig = go.Figure()\nmeans = []\nstores = []\nfor i, s in enumerate(store_list):\n    if \"tx\" in s or \"TX\" in s:\n        store_items = [c for c in past_sales.columns if s in c]\n        data = past_sales[store_items].sum(axis=1).rolling(90).mean()\n        means.append(np.mean(past_sales[store_items].sum(axis=1)))\n        stores.append(s)\n        fig.add_trace(go.Scatter(x=np.arange(len(data)), y=data, name=s, marker=dict(color=greens[i%len(greens)])))\n        \nfig.update_layout(yaxis_title=\"Sales\", xaxis_title=\"Time\", title=\"Rolling Average Sales vs. Time (Texas)\")","execution_count":null,"outputs":[]},{"metadata":{"id":"fOvP8dhnx4AR","outputId":"ff8f00cd-85a5-415d-9638-3f5474caf733","trusted":false},"cell_type":"code","source":"fig = go.Figure()\n\nfor i, s in enumerate(store_list):\n    if \"tx\" in s or \"TX\" in s:\n        store_items = [c for c in past_sales.columns if s in c]\n        data = past_sales[store_items].sum(axis=1).rolling(90).mean()\n        fig.add_trace(go.Box(x=[s]*len(data), y=data, name=s, marker=dict(color=greens[i%len(greens)])))\n    \nfig.update_layout(yaxis_title=\"Sales\", xaxis_title=\"Time\", title=\"Rolling Average Sales vs. Store name (Texas)\")","execution_count":null,"outputs":[]},{"metadata":{"id":"zpSagepHx4bJ","outputId":"3e68e6d4-ea94-4d5d-cdd7-0055af83417c","trusted":false},"cell_type":"code","source":"df = pd.DataFrame(np.transpose([means, stores]))\ndf.columns = [\"Mean sales\", \"Store name\"]\npx.bar(df, y=\"Mean sales\", x=\"Store name\", color=\"Store name\", title=\"Mean sales vs. Store name\", color_continuous_scale=greens)\n\n\nfig = go.Figure(data=[\n    go.Bar(name='', x=stores, y=means, marker={'color' : greens})])\n\nfig.update_layout(title=\"Mean sales vs. Store name (Texas)\", yaxis=dict(title=\"Mean sales\"), xaxis=dict(title=\"Store name\"))\nfig.update_layout(barmode='group')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"5YwAjsiC8DDA"},"cell_type":"markdown","source":"In the above plots ,  we can see how the sales in 3 different store in texas variate over time , there distributions in overall texas area. We can see that in the some initial time period when the sales of one store were increasing ,the other were side by side to each other .It is like , the crowd in region of store 1 liked the product when the people near other stores didn't. And after some time it got reversed. THe sales of store 1 went down to be almost same as the other two.\n\nWe conclude that store 2 TX_2 has the maximum sales and TX_1 has the minimum sales.","execution_count":null},{"metadata":{"id":"co8JegYL-2H0"},"cell_type":"markdown","source":"Note : The same can be also be done for other cities too and details analytics and there trends can be extracted.","execution_count":null},{"metadata":{"id":"o0nSCfvP-qQd","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}