{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Importing Data","metadata":{}},{"cell_type":"code","source":"import pandas as pd\ncal = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/calendar.csv')\nstval = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sales_train_validation.csv')\nprice = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sell_prices.csv') ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stval.info()","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cal.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"price.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Downcasting (Reducing Size)","metadata":{}},{"cell_type":"code","source":"# Taken from https://www.kaggle.com/gemartin/load-data-reduce-memory-usage\nimport numpy as np\ndef reduce_mem_usage(df):\n   \n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reduce_mem_usage(stval)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reduce_mem_usage(price)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reduce_mem_usage(cal)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Re-Structuring and Merging Data\n\nFor convenience of work and subsequent merging of data with other dataframes \"price\" and \"cal\", the melt() function is used to unpivot \"sales\" dataframe from wide format to long format.","metadata":{}},{"cell_type":"code","source":"sales = pd.melt(stval, id_vars=['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], var_name='d', value_name='demand').dropna()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sales = pd.merge(sales, cal, on='d', how='left')\nsales = pd.merge(sales, price, on=['store_id','item_id','wm_yr_wk'], how='left') ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sales = sales.drop(['d','event_name_1','event_type_1','event_name_2','event_type_2','snap_CA','snap_TX','snap_WI','wm_yr_wk','weekday'], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sales = sales.dropna()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For working in details, we will work with data for only one food item FOODS_3_555 from location CA_1.","metadata":{}},{"cell_type":"code","source":"sale_food_ca = sales[sales['id'] == 'FOODS_3_555_CA_1_validation']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del sales, price, cal #free some space","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sale_food_ca['date'] = pd.to_datetime(sale_food_ca['date'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train-Test Split\n\nThe last transaction date found in validation data set is 24th April 2016. For last 28 days (spanning from 28th March 2016 till 24th April 2016), all records will go to the test dataset. Except those records, all records from the start till 27th March 2016 will be used for training. ","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.rc('xtick', labelsize=16) \nplt.rc('ytick', labelsize=16) \nparams = {'legend.fontsize': 16,'legend.handlelength': 2}\nplt.rcParams.update(params)\n\ntrain = sale_food_ca[sale_food_ca['date'] <= '2016-03-27']\ntest = sale_food_ca[(sale_food_ca['date'] > '2016-03-27') & (sale_food_ca['date'] <= '2016-04-24')]\n\nfig, ax = plt.subplots(figsize=(25,5))\ntrain.plot(x='date',y='demand',label='Train',ax=ax)\ntest.plot(x='date',y='demand',label='Test',ax=ax)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Check for Stationarity (ADH Test, KPSS Test) and Granger Causality\n\nWe will use Augmented Dickeyâ€“Fuller test which tests the null hypothesis that a unit root is present in an autoregressive model. If data has no unit root then it is stationary. If the data is stationary, then only we can apply time series on it. If non-stationary, we need to make it stationary first through differencing.","metadata":{}},{"cell_type":"code","source":"# Taken from https://gist.github.com/Deffro/e54bdb90dd1c1392fc85e1db1cfbab7d\n\nimport pandas as pd\nfrom statsmodels.tsa.stattools import adfuller\n\ndef adf_test(series,title=''):\n    \"\"\"\n    Pass in a time series and an optional title, returns an ADF report\n    \"\"\"\n    print('Augmented Dickey-Fuller Test: {}'.format(title))\n    result = adfuller(series.dropna(),autolag='AIC') \n    \n    labels = ['ADF test statistic','p-value','# lags used','# observations']\n    out = pd.Series(result[0:4],index=labels)\n\n    for key,val in result[4].items():\n        out['critical value ({})'.format(key)]=val\n        \n    print(out.to_string())          \n    \n    if result[1] <= 0.05:\n        print(\"Strong evidence against the null hypothesis\")\n        print(\"Reject the null hypothesis\")\n        print(\"Data has no unit root and is stationary\")\n    else:\n        print(\"Weak evidence against the null hypothesis\")\n        print(\"Fail to reject the null hypothesis\")\n        print(\"Data has a unit root and is non-stationary\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Augmented Dickey-Fuller Test\nadf_test(train[['date','demand']]['demand'],title='Demand')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We observe, the data is stationary and no differencing is required. In the ADF test, the null hypothesis is the time series has a unit root (so data is non-stationary). Since we have got p-Value < 0.05 we reject the null hypothesis.","metadata":{}},{"cell_type":"code","source":"# KPSS Test\n\nfrom statsmodels.tsa.stattools import kpss\nresult = kpss(train[['date','demand']]['demand'].values, regression='c')\nprint(\"KPSS Statistic: {}\".format(result[0]))\nprint(\"P-Value: {}\".format(result[1]))\nfor key, value in result[3].items():\n    print(\"Critial Values: {}, {}\".format(key,value))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"KPSS test is used for testing a null hypothesis that an observable time series is stationary around a deterministic trend against the alternative of a unit root. The p-value interpretation is just the opposite of ADH test. So here, we see a p-value > 0.05","metadata":{}},{"cell_type":"code","source":"# Granger Causality Test\n\nfrom statsmodels.tsa.stattools import grangercausalitytests\ngrangercausalitytests(train[['demand','sell_price']], maxlag=3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here, we are looking whether one time series for demand is useful in forecasting the sell_price. We should look for very low p-values (<0.05) here. With maxlag = 3, we see that p-values are getting lowered down but still it is >0.05 and hence, one time series is not useful in forecasting the other.","metadata":{}},{"cell_type":"markdown","source":"# Lag Plots\n\nA lag plot is a kind of a scatter plot which is used for visualizing whether a dataset is random (stochastic) or not over a particular lag (time). Here, we will check randomness for demand over time and randomness of sell_price over time. Though, according to task, we are supposed to forecast demand over the time.","metadata":{}},{"cell_type":"code","source":"# Lag PLots\n\nfrom pandas.plotting import lag_plot\nfig, ax = plt.subplots(2,3,figsize=(15,5))\nlag_plot(train['demand'], lag=1, ax=ax[0][0])\nlag_plot(train['demand'], lag=30, ax=ax[0][1])\nlag_plot(train['demand'], lag=60, ax=ax[0][2])\nlag_plot(train['sell_price'], lag=1, ax=ax[1][0])\nlag_plot(train['sell_price'], lag=30, ax=ax[1][1])\nlag_plot(train['sell_price'], lag=60, ax=ax[1][2])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We notice that demand is following quite a random pattern whereas sell_price is quite linear with a positive (upward) slope, which is obvious, because sell_price is supposed to hike over the period of time.","metadata":{}},{"cell_type":"markdown","source":"# ACF and PACF Plots","metadata":{}},{"cell_type":"code","source":"import pandas as pd\npredictions = pd.DataFrame()\npredictions['date'] = test['date']\nstats = pd.DataFrame(columns=['Model Name','Execution Time','RMSE'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from statsmodels.graphics.tsaplots import plot_acf\nfig, ax = plt.subplots(figsize=(15,3))\nplot_acf(sale_food_ca['demand'].tolist(), lags=50, ax=ax)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from statsmodels.graphics.tsaplots import plot_pacf\nfig, ax = plt.subplots(figsize=(15,3))\nplot_pacf(sale_food_ca['demand'].tolist(), lags=50, ax=ax)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From both the autocorrelation plot and partial autocorrelation plots, it is observed that after every 7 lags there is a peak. This indicates a high correlation between the demand of food item on the present day and the demand of it before 7 days. So, if we use MA model, we can set the window size=7 and if we use AR or ARIMA model, we can set the span=7. ","metadata":{}},{"cell_type":"markdown","source":"# Different Time Series Modelling\n\nHere, we will use moving average, exponential smoothing, non-seasonal ARIMA, seasonal ARIMA and seasonal ARIMAX models. The moving average model is just used for basic observation purpose. Rest of the models' performances are compared based on total time taken for execution and Root Mean Squared Error (RMSE). First we will use the training data set to train it with various time series models. After training, we will use forecast(x) with x=28 days.\n\n# (1) Simple Moving Average and Expanding Moving Average Models","metadata":{}},{"cell_type":"code","source":"# Moving Average model with window size 7 \ny = train[['date','demand']]\ny = y.set_index('date')\ny['MA7'] = y.rolling(window=7).mean() \ny.plot(figsize=(15,4))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Moving Average model with expanding window size 2\ny1 = train[['date','demand']]\ny1 = y1.set_index('date')\ny1['demand'].expanding(min_periods=2).mean().plot(figsize=(15,4))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# (2) Simple/ Double/ Triple Exponential Smooting Models","metadata":{}},{"cell_type":"code","source":"import time\nfrom statsmodels.tsa.holtwinters import SimpleExpSmoothing\nfrom sklearn.metrics import mean_squared_error\n\nt0 = time.time()\nmodel_name='Simple Exponential Smoothing'\nspan = 7\nalpha = 2/(span+1)\n#train\nsimpleExpSmooth_model = SimpleExpSmoothing(train['demand']).fit(smoothing_level=alpha,optimized=False)\nt1 = time.time()-t0\n#predict\npredictions[model_name] = simpleExpSmooth_model.forecast(28).values\nfig, ax = plt.subplots(figsize=(25,5))\ntrain[-28:].plot(x='date',y='demand',label='Train',ax=ax, marker='o', color='blue')\ntest.plot(x='date',y='demand',label='Test',ax=ax, marker='o', color='red');\npredictions.plot(x='date',y=model_name,label=model_name,ax=ax, marker='x', color='green');\n#evaluate\nscore = np.sqrt(mean_squared_error(predictions[model_name].values, test['demand']))\nprint('RMSE for {}: {:.4f}'.format(model_name,score))\n\nstats = stats.append({'Model Name':model_name, 'Execution Time':t1, 'RMSE':score},ignore_index=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see a linear trend which is not matching the curvy spikes present in original forecasting trend followed by the test data. So we will try applying double exponential smoothing next.","metadata":{}},{"cell_type":"code","source":"from statsmodels.tsa.holtwinters import ExponentialSmoothing\n\nt0 = time.time()\nmodel_name='Double Exponential Smoothing'\n#train\ndoubleExpSmooth_model = ExponentialSmoothing(train['demand'],trend='add',seasonal_periods=7).fit()\nt1 = time.time()-t0\n#predict\npredictions[model_name] = doubleExpSmooth_model.forecast(28).values\nfig, ax = plt.subplots(figsize=(25,5))\ntrain[-28:].plot(x='date',y='demand',label='Train',ax=ax, marker='o', color='blue')\ntest.plot(x='date',y='demand',label='Test',ax=ax, marker='o', color='red');\npredictions.plot(x='date',y=model_name,label=model_name,ax=ax, marker='x', color='green');\n#evaluate\nscore = np.sqrt(mean_squared_error(predictions[model_name].values, test['demand']))\nprint('RMSE for {}: {:.4f}'.format(model_name,score))\n\nstats = stats.append({'Model Name':model_name, 'Execution Time':t1, 'RMSE':score},ignore_index=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Even with double exponential smoothing also, scenario remains the same. It is still linear. So we will try using triple exponential smoothing next.","metadata":{}},{"cell_type":"code","source":"t0 = time.time()\nmodel_name='Triple Exponential Smoothing'\n#train\ntripleExpSmooth_model = ExponentialSmoothing(train['demand'],trend='add',seasonal='add',seasonal_periods=7).fit()\nt1 = time.time()-t0\n#predict\npredictions[model_name] = tripleExpSmooth_model.forecast(28).values\nfig, ax = plt.subplots(figsize=(25,4))\ntrain[-28:].plot(x='date',y='demand',label='Train',ax=ax, marker='o', color='blue')\ntest.plot(x='date',y='demand',label='Test',ax=ax, marker='o', color='red');\npredictions.plot(x='date',y=model_name,label=model_name,ax=ax, marker='x', color='green');\n#evaluate\nscore = np.sqrt(mean_squared_error(predictions[model_name].values, test['demand']))\nprint('RMSE for {}: {:.4f}'.format(model_name,score))\n\nstats = stats.append({'Model Name':model_name, 'Execution Time':t1, 'RMSE':score},ignore_index=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we can see the linear shape has got converted to curve with the spikes following original forecasting trend of the test data set. Also, we can observe a substantial reduction in RMSE while using with Triple Exponential Smoothing (4.2348). So we are stopping here experimenting with the exponential smoothing model.","metadata":{}},{"cell_type":"markdown","source":"# (3) Non-Seasonal ARIMA Model","metadata":{}},{"cell_type":"code","source":"!pip install pmdarima","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pmdarima import auto_arima\nt0 = time.time()\nmodel_name='ARIMA'\narima_model = auto_arima(train['demand'], start_p=0, start_q=0,\n                          max_p=20, max_q=5,\n                          seasonal=False,\n                          d=None, trace=True,random_state=12345,\n                          error_action='ignore',   \n                          suppress_warnings=True,  \n                          stepwise=True)\narima_model.summary() ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train\narima_model.fit(train['demand'])\nt1 = time.time()-t0\n#predict\npredictions[model_name] = arima_model.predict(n_periods=28)\nfig, ax = plt.subplots(figsize=(25,5))\ntrain[-28:].plot(x='date',y='demand',label='Train',ax=ax, marker='o', color='blue')\ntest.plot(x='date',y='demand',label='Test',ax=ax, marker='o', color='red');\npredictions.plot(x='date',y=model_name,label=model_name,ax=ax, marker='x', color='green');\n#evaluate\nscore = np.sqrt(mean_squared_error(predictions[model_name].values, test['demand']))\nprint('RMSE for {}: {:.4f}'.format(model_name,score))\n\nstats = stats.append({'Model Name':model_name, 'Execution Time':t1, 'RMSE':score},ignore_index=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see a moderately fitting curve roughly following the forecasting trend shown by the test data, but non-seasonal ARIMA model is having RMSE greater than what we have got with the triple exponential model (5.2752 is > 4.2348). So the performance has not improved using non-seasonal ARIMA. ","metadata":{}},{"cell_type":"markdown","source":"# (4) Seasonal ARIMA Model","metadata":{}},{"cell_type":"code","source":"t0 = time.time()\nmodel_name='SARIMA'\nsarima_model = auto_arima(train['demand'], start_p=0, start_q=0,\n                          max_p=20, max_q=5,\n                          seasonal=True, m=7,\n                          d=None, trace=True,random_state=12345,\n                          out_of_sample_size=28,\n                          error_action='ignore',   \n                          suppress_warnings=True,  \n                          stepwise=True)\nsarima_model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train\nsarima_model.fit(train['demand'])\nt1 = time.time()-t0\n#predict\npredictions[model_name] = sarima_model.predict(n_periods=28)\nfig, ax = plt.subplots(figsize=(25,4))\ntrain[-28:].plot(x='date',y='demand',label='Train',ax=ax, marker='o', color='blue')\ntest.plot(x='date',y='demand',label='Test',ax=ax, marker='o', color='red');\npredictions.plot(x='date',y=model_name,label=model_name,ax=ax, marker='x', color='green');\n#evaluate\nscore = np.sqrt(mean_squared_error(predictions[model_name].values, test['demand']))\nprint('RMSE for {}: {:.4f}'.format(model_name,score))\n\nstats = stats.append({'Model Name':model_name, 'Execution Time':t1, 'RMSE':score},ignore_index=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Seasonal ARIMA is fitting much better than non-seasonal ARIMA. Also, we are getting reduced RMSE with seasonal ARIMA (4.6820) compared to what we had got using non-seasonal ARIMA (5.2752).  ","metadata":{}},{"cell_type":"markdown","source":"# (5) Seasonal ARIMAX Model\n\nIt is SARIMA model with eXogenous regressors. These regressors introduce the external features which can influence a time series.","metadata":{}},{"cell_type":"code","source":"t0 = time.time()\nmodel_name='SARIMAX'\nsarimax_model = auto_arima(train['demand'], start_p=0, start_q=0,\n                          max_p=20, max_q=5,\n                          seasonal=True, m=7,\n                          exogenous = train[['sell_price']].values,\n                          d=None, trace=True,random_state=2020,\n                          out_of_sample_size=28,\n                          error_action='ignore',   \n                          suppress_warnings=True,  \n                          stepwise=True)\nsarimax_model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train\nsarimax_model.fit(train['demand'])\nt1 = time.time()-t0\n#predict\npredictions[model_name] = sarimax_model.predict(n_periods=28)\nfig, ax = plt.subplots(figsize=(25,4))\ntrain[-28:].plot(x='date',y='demand',label='Train',ax=ax, marker='o', color='blue')\ntest.plot(x='date',y='demand',label='Test',ax=ax, marker='o', color='red');\npredictions.plot(x='date',y=model_name,label=model_name,ax=ax, marker='x', color='green');\n#evaluate\nscore = np.sqrt(mean_squared_error(predictions[model_name].values, test['demand']))\nprint('RMSE for {}: {:.4f}'.format(model_name,score))\n\nstats = stats.append({'Model Name':model_name, 'Execution Time':t1, 'RMSE':score},ignore_index=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that SARIMAX curve is fitting the best compared to previously plotted non-seasonal and seasonal ARIMA curves. Also, here we have got the lowest RMSE (4.4632) compared to what we had got earlier using non-seasonal ARIMA (5.2752) and seasonal ARIMA (4.6820). However, RMSE with triple exponential smoothing was lower that what we've got using SARIMAX.","metadata":{}},{"cell_type":"markdown","source":"# Model Performance Comparison","metadata":{}},{"cell_type":"code","source":"# RMSE Comparison\nstats.plot(kind='line',x='Model Name', y='RMSE', figsize=(12,4), title=\"RMSE Comparison for Different TS Models\")\nplt.xticks(rotation='vertical')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Execution Time Comparison\nstats.plot(kind='bar',x='Model Name', y='Execution Time', figsize=(12,4), title=\"Execution Time Comparison for Different TS Models\")\nplt.xticks(rotation='vertical')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion:\nWe observe that Triple Exponential Smoothing model has the lowest RMSE and the lowest execution time. Hence, it can be concluded as the best performing model in this scenario.\n\n**Acknowledgement:** I am grossly indebted to [Dimitrios Effrosynidis](https://www.kaggle.com/deffro) for his various time series related works on medium and github.","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}