{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom scipy.special import inv_boxcox\nfrom typing import Tuple\nimport lightgbm as lgb\nfrom datetime import timedelta\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\nFIGURE_SIZE = (20, 10)\nplt.rcParams['axes.grid'] = True\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load Data\n\nWe will load the data for the M5 competition and then take a subsample. All of the techniques described below can be scaled to the full data set and used in any statisical or machine learning model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_pickle('/kaggle/input/m5-full-training-dataset/m5_competition_data.pkl')\n\n# Get sample ids\nnp.random.seed(1985)\nsample_ids = np.random.choice(data['id'].unique(), 50)\ndata = data.loc[data['id'].isin(sample_ids)]\n\n# Look at the first few observations of the sample\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Transformations for Time Series\n\nIn many situations it is necessary for us to trasform a time series dataset before using any statistical or machine learning models. The need for transformations arise when the time series data we have are not stationary or when we want to eliminate trends or cycles in the data and study what is left after these transformations.\n\nAfter using a transformation it is important to apply the inverse of that transformation to the data in order get to back to the original scale. This notebook will show the different types of transformations available to time series and how to apply it to our data.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Power Transformations\n\nA simple but effective form of transformations are power transformations. They are an effective way to stabilze variance over time. The most common power transformations are:\n\n1. Square root -> $\\sqrt(x)$\n2. Cube root -> $(x) ^ {\\frac{1}{3}}$\n3. Log -> $\\log(x)$\n4. Box-cox transformations\n\nThe Box Cox transformation is an exponent, lambda (λ), which varies from -5 to 5. All values of λ are considered and the optimal value for your data is selected; The “optimal value” is the one which results in the best approximation of a normal distribution curve. The transformation of Y has the form:\n\n$$y(\\lambda) = \\begin{cases} \\dfrac{y ^ \\lambda - 1}{\\lambda} & \\text{if } \\lambda \\neq 0 \\\\ \\log(y) &  \\text{if } \\lambda = 0 \\end{cases}$$\n\nThe case for the Box-cox transformation will be different when we apply because accoring to the documentation: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.boxcox.html. This function will also return a maximized $\\lambda$ values for the series.\n\nLet's look at some example and the inverse transformations back to the original data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define power trnnsformations and their inverses\n# Square root transformation\ndef square_root_transformation(x: pd.Series) -> pd.Series:\n    return np.sqrt(x)\n\ndef square_root_inverse_transformation(x: pd.Series) -> pd.Series:\n    return np.square(x)\n\n# Cube root transformation\ndef cube_root_transformation(x: pd.Series) -> pd.Series:\n    return x ** (1 / 3)\n\ndef cube_root_inverse_transformation(x: pd.Series) -> pd.Series:\n    return x ** 3\n\n# Log transformation\ndef log_transformation(x: pd.Series) -> pd.Series:\n    # Function np.log1p = log(x + 1)\n    return np.log1p(x)\n\ndef log_inverse_transformation(x: pd.Series) -> pd.Series:\n    # Function np.expm1(x) = exp(x) - 1\n    return np.expm1(x)\n\n# Box-cox transformation\ndef box_cox_transformation(x: pd.Series) -> Tuple[np.array, float]:\n    x_transformed, lambda_value = stats.boxcox(x)\n    return x_transformed, lambda_value\n    \ndef box_cox_inverse_transformation(x: pd.Series, lambda_value: float) -> pd.Series:\n    return inv_boxcox(x, lambda_value)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Square Root\n# Apply transformation to the data\ndata['square_root_transformation_demand'] = data.groupby('id')['demand'].apply(lambda x: square_root_transformation(x))\n\n# Apply the inverse square root transformation\ndata['square_root_inv_transformation_demand'] = data.groupby('id')['square_root_transformation_demand'].apply(lambda x: square_root_inverse_transformation(x))\n\n# Cube Root\ndata['cube_root_transformation_demand'] = data.groupby('id')['demand'].apply(lambda x: cube_root_transformation(x))\n\n# Apply the inverse square root transformation\ndata['cube_root_inv_transformation_demand'] = data.groupby('id')['cube_root_transformation_demand'].apply(lambda x: cube_root_inverse_transformation(x))\n\n# Log Root\ndata['log_transformation_demand'] = data.groupby('id')['demand'].apply(lambda x: log_transformation(x))\n\n# Apply the inverse square root transformation\ndata['log_inv_transformation_demand'] = data.groupby('id')['log_transformation_demand'].apply(lambda x: log_inverse_transformation(x))\n\n# Box-cox transformation\nbox_cox_data = []\nbox_cox_inverse_transform_lambda_map = {}\nfor group, group_df in data.groupby('id'):\n    box_cox_transformed_data, lambda_value = box_cox_transformation(group_df['demand'] + 1)\n    group_df['box_cox_transformation_demand'] = box_cox_transformed_data\n    box_cox_data.append(group_df)\n    box_cox_inverse_transform_lambda_map.update({group: lambda_value})\n    \nbox_cox_data = pd.concat(box_cox_data)\n\n# Apply inverse transformation\nall_power_transformed_data = []\nfor group, group_df in box_cox_data.groupby('id'):\n    lambda_value = box_cox_inverse_transform_lambda_map.get(group)\n    group_df['box_cox_inv_transformation_demand'] = box_cox_inverse_transformation(group_df['box_cox_transformation_demand'], lambda_value) - 1\n    all_power_transformed_data.append(group_df)\n    \nall_power_transformed_data = pd.concat(all_power_transformed_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_transformations(df: pd.DataFrame, transformation: str) -> None:\n    # Get axes for multiple plots\n    fig, axes = plt.subplots(nrows=1, ncols=3)\n    \n    # Original data\n    df.set_index('date')['demand'].plot(figsize=FIGURE_SIZE, ax=axes[0], color='blue')\n    \n    # Transformed data\n    transformed_column_name = f'{transformation}_transformation_demand'\n    df.set_index('date')[transformed_column_name].plot(figsize=FIGURE_SIZE, ax=axes[1], color='red')\n    \n    # Inverse Transformed data\n    inverse_transformed_data = f'{transformation}_inv_transformation_demand'\n    df.set_index('date')[inverse_transformed_data].plot(figsize=FIGURE_SIZE, ax=axes[2], color='orange')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get a single id so we can take a look at some plots\nsingle_id = 'FOODS_1_073_TX_1_validation'\nsingle_id_data = all_power_transformed_data.loc[all_power_transformed_data['id'] == single_id]\nsingle_id_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_transformations(single_id_data, 'square_root')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_transformations(single_id_data, 'cube_root')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_transformations(single_id_data, 'log')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_transformations(single_id_data, 'box_cox')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Differencing\n\nDifferencing is one of the most powerful transformation for time series and has not really been discussed in this competition with the exception of some comments around ARIMA models. ARIMA models implement differencing automatically provided the I input to the model is greater than zero. A difference transform is a simple way for removing a systematic structure from the time series.\n\nFor example, a trend can be removed by subtracting the previous value from each value in the series. This is called first order differencing. The process can be repeated (e.g. difference the differenced series) to remove second order trends, and so on.\n\nDifferencing is tricky to implement because the inverse operation of differencing is the cumulative sum. This is not straightforward as a transformation because when we apply the inverse of the differenced data to our predictions we have to add in the last known observation of our series in order to get the correct transformation.\n\nLet's go over an example and see how we can use this in the competition.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Before we apply our transformation let's make sure the data is sorted\ndata = data.sort_values(['id', 'date'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['differenced_trasnformation_demand'] = data.groupby('id')['demand'].diff().values\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The `diff` function in pandas will give us what we want. Also notice above that the first value is `NaN`. That is because we do not have any infomation before that and therefore there is nothing to take a difference of. For the completness to see if our inverse works we can just fill the `NaN` values with the value from that date.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data['differenced_demand_filled'] = np.where(pd.isnull(data['differenced_trasnformation_demand']), data['demand'], data['differenced_trasnformation_demand'])\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's take the cumulative sum of the `difference_demand_filled` to see that it is equal to the original `demand` variable. Once we verify this we can see how we can incorporate this into our models.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data['differenced_inv_transformation_demand'] = data.groupby('id')['differenced_demand_filled'].cumsum()\nnp.testing.assert_array_equal(data['demand'].values, data['differenced_inv_transformation_demand'].values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The testing shows that the arrays are equal so we have the correct inverse transformation. Also, above we only did first order differencing but depending on the time series we encounter we can take higher order differencing. So for example, 2nd order differencing is the difference of the differenced values and so on. We are also not limited to the previous observation to use as a difference. We can use values from 7 days, 28 days, etc. for differencing.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"single_id = 'FOODS_1_073_TX_1_validation'\nsingle_id_data = data.loc[data['id'] == single_id]\nsingle_id_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot of differenced data\nsingle_id_data.set_index('date')['differenced_trasnformation_demand'].plot(figsize=FIGURE_SIZE)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Applying the Differenced Transformation to Our Models\n\nLet's look at a very simple LightGBM model. We will only use temporal features. The idea is to get the concept of transformations into our models. Once we have a general framework down to apply these transformations we can scale up with features and other concepts such as cross validation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_temporal_features(data: pd.DataFrame) -> pd.DataFrame:\n    # Temporal features\n    data['date'] = pd.to_datetime(data['date'])\n    data['year'] = data['date'].dt.year\n    data['month'] = data['date'].dt.month\n    data['week'] = data['date'].dt.week\n    data['day'] = data['date'].dt.day\n    data['dayofweek'] = data['date'].dt.dayofweek\n    data['quarter'] = data['date'].dt.quarter\n    data['week_of_month'] = data['day'].apply(lambda x: np.ceil(x / 7)).astype(np.int8)\n    data['is_weekend'] = (data['dayofweek'] > 5).astype(np.int8)\n    \n    return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = build_temporal_features(data)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cutoff_date = data['date'].max() - timedelta(days=28)\n\nfeature_columns = [\n    'year',\n    'month',\n    'week',\n    'day',\n    'dayofweek',\n    'quarter',\n    'week_of_month',\n    'is_weekend'\n]\n\ntarget_column = ['differenced_trasnformation_demand']\n\nidentifier_columns = ['id', 'date', 'demand']\n\nX_train = data.loc[data['date'] <= cutoff_date]\nX_valid = data.loc[data['date'] > cutoff_date]\n\n# Filter X_train, X_valid\nX_train = X_train[feature_columns + identifier_columns + target_column]\nX_valid = X_valid[feature_columns + identifier_columns + target_column]\n\n# Drop values where we do not have a target value\nX_train = X_train.dropna()\n\n# Define target\ny_train, y_valid = X_train['differenced_trasnformation_demand'].values, X_valid['differenced_trasnformation_demand'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n    'boosting_type': 'gbdt',\n    'objective': 'regression',\n    'boost_from_average': False,\n    'verbose': -1,\n} \n\nlgb_train_data, lgb_valid_data = lgb.Dataset(X_train[feature_columns], y_train), lgb.Dataset(X_valid[feature_columns], y_valid)\n\nmodel = lgb.train(params, lgb_train_data, 200)\nX_valid['y_preds'] = model.predict(X_valid[feature_columns])\nX_valid.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Applying to predictions\n\nWe need to extract the last known value for each series from the training data and append it to our predictions so we can take a cumulative sum.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"keep_columns = ['id', 'date', 'y_preds']\n\nX_train_last = X_train.groupby('id').last().reset_index()\n\n# We need the same columns to concatenate\nX_train_last = X_train_last.rename(columns={'demand': 'y_preds'})\nX_train_last = X_train_last[keep_columns]\n\nX_valid = X_valid[keep_columns]\n\npredictions = pd.concat([X_train_last, X_valid], axis=0)\npredictions = predictions.sort_values(['id', 'date'])\npredictions.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions['y_preds'] = predictions.groupby('id')['y_preds'].cumsum()\npredictions = predictions.reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def mask_first(x: pd.Series) -> np.array:\n    result = np.ones_like(x)\n    result[0] = 0\n    return result\n\nmask = predictions.groupby(['id'])['id'].transform(mask_first).astype(bool)\npredictions = predictions.loc[mask]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Other Transformations\n\nAs a conclusion to this notebook, I just wanted to list a few other types of transformations that we can consider:\n\n1. Standing Scaling - standard scaling is available in `sklearn` and has a very nice implementation\n2. Normalization - a rescaling of the data from 0 to 1. `sklearn` also has an implemenation of this called `MinMaxScaler`. The default range is (0, 1)\n\n\nThank you for reading this notebook. Any comments or feedback would be greatly appreciated!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}