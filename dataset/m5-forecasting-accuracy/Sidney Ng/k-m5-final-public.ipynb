{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Run for start_train=700, 950, 1200; 1350, 1500 then Average\n## Loss fn = Poisson; Magic No *1.00 (None)","execution_count":null},{"metadata":{"id":"KLQTVo-VwWH9","outputId":"6c303e6b-c307-4f53-9c7e-6861eb2407c7","trusted":true},"cell_type":"code","source":"!pip install fastai2 ","execution_count":null,"outputs":[]},{"metadata":{"id":"5mcUiwfIKj2V","trusted":true},"cell_type":"code","source":"\n## Kaggle paths ##########################################\n\npath = \"/kaggle/input/m5-forecasting-accuracy\"\npath_fe =\"/kaggle/input/m5-simple-fe-eval\"\npath_lag =\"/kaggle/input/m5-lags-features-eval\"\npath_o = '../output/tabular_pred'\n\n#PATHS for Features\nORIGINAL = path\n\n\n#test_path  = f'{path}/dm_files'\nBASE     = f'{path_fe}/grid_part_1.pkl'\nPRICE    = f'{path_fe}/grid_part_2.pkl'\nCALENDAR = f'{path_fe}/grid_part_3.pkl'\nLAGS     = f'{path_lag}/lags_df_28.pkl'\nCS   = f'{path_lag}/cumsum.pkl'\n\n#########################################################\n\n# AUX(pretrained) Models paths\nAUX_MODELS = path","execution_count":null,"outputs":[]},{"metadata":{"id":"P4aJFe8TSmE4","trusted":true},"cell_type":"code","source":"%reload_ext autoreload\n%autoreload 2","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"id":"zP7ZZLWCJqd_","trusted":true},"cell_type":"code","source":"import fastai2\nfrom fastai2.tabular.all import *\nfrom fastai2.basics import *\nfrom fastai2.callback.all import *\n\n# General imports\nimport numpy as np\nimport pandas as pd\nimport os, sys, gc, time, warnings, pickle, psutil, random\n\n# custom imports\nfrom multiprocessing import Pool        # Multiprocess Runs\n\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"id":"a7KOp4VUkoWz","trusted":true},"cell_type":"code","source":"def seed_everything(seed=0):\n    random.seed(seed)\n    np.random.seed(seed)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"oB7jhU8xkyf0","trusted":true},"cell_type":"code","source":"# Read data\ndef get_data_by_store(store, START_TRAIN):\n    print('Start train at Day ', START_TRAIN, '; End train at Day ', END_TRAIN)\n    # Read and contact basic feature\n    df = pd.concat([pd.read_pickle(BASE),\n                    pd.read_pickle(PRICE).iloc[:,2:],\n                    pd.read_pickle(CALENDAR).iloc[:,2:]],\n                    axis=1)\n    \n    # Leave only relevant store\n    df = df[df['store_id']==store]\n    df = df[df['d']>=START_TRAIN]\n\n\n\n    # keep 'd' to select by days\n    lag_feat = ['sales_lag_28', 'sales_lag_56', 'sales_lag_84', 'sales_lag_168', 'sales_lag_364',\n              'roll_mean_lag_28_7', 'roll_mean_lag_28_14', 'roll_mean_lag_28_28', \n            'roll_mean_lag_56_7', 'roll_mean_lag_56_14', 'roll_mean_lag_56_28', \n            'roll_mean_lag_84_7', 'roll_mean_lag_84_14', 'roll_mean_lag_84_28', \n            'roll_mean_lag_168_7', 'roll_mean_lag_168_14', 'roll_mean_lag_168_28',\n            'roll_mean_lag_364_7', 'roll_mean_lag_364_14', 'roll_mean_lag_364_28',\n            ]\n\n    #df3 = pd.read_pickle(LAGS).iloc[:, 3:]\n    df3 = pd.read_pickle(LAGS)[lag_feat]\n    df3 = df3[df3.index.isin(df.index)]\n\n  \n    df = pd.concat([df, df3], axis=1)\n    del df3 # to not reach memory limit \n    gc.collect()\n    \n    \n    cum_feat = [ 'sales_lag_28_cum']  # 'price_sales',\n    df4 = pd.read_pickle(CS)[cum_feat]\n    #df4 = pd.read_pickle(CS).iloc[:, 3:]\n    df4 = df4[df4.index.isin(df.index)]\n\n  \n    df = pd.concat([df, df4], axis=1)\n    del df4 # to not reach memory limit \n    gc.collect()\n    \n\n    # Create features list\n    features = [col for col in list(df) if col not in remove_features]\n    #df = df[['id','d',TARGET]+features]\n    df = df[features]\n    \n    # Skipping first n rows\n    df = df[df['d']>=START_TRAIN].reset_index(drop=True)\n    \n    return df, features","execution_count":null,"outputs":[]},{"metadata":{"id":"f2hWebdlEkWx"},"cell_type":"markdown","source":"# Create fast.ai databunch + Learner","execution_count":null},{"metadata":{"id":"YGxiXhb3D9YI","trusted":true},"cell_type":"code","source":"def create_dbunch(train_df):\n    global df_test, submit_store, y_max\n\n    dep_var = TARGET\n\n\n    print('max of sales_lag_28_cum=',  train_df.sales_lag_28_cum.max() ) # train_df.ps_lag_1.max()\n\n\n    feats = list(train_df)\n    for feat in feats:\n      if '_lag_' in feat:\n        train_df[feat]=train_df[feat].fillna(0.0)  #inplace can't work\n\n    #train_df['log_sales']= np.log1p(train_df.sales.values)  #convert to log then can fit\n    #train_df[TARGET]= train_df.sales.values  #try no Log\n    train_df[TARGET]= train_df['sales']*train_df['sell_price']  #tgt=price*sales\n    \n    print('max, Target; max, min Sales ',train_df[TARGET].max(), train_df['sales'].max(), train_df['sales'].min())\n    y_max = 1.1 * train_df[TARGET].max()\n    #print('train_df null=', train_df.isnull().sum())\n\n    all_vars = train_df.columns.tolist()\n    all_vars.remove(dep_var)\n    #all_vars.remove('weekday')\n\n    cat_vars = ['item_id', 'dept_id', 'cat_id', 'd', 'price_nunique', 'item_nunique',  \n                'event_name_1', 'event_name_2', 'event', 'snap_CA', 'snap_TX', 'snap_WI', 'release',\n                  'tm_d', 'tm_w', 'tm_m', 'tm_y', 'tm_wm', 'tm_dw', 'tm_w_end',  ]\n\n    xtra_vars = ['sales', 'id' ] # move 'id' here, else embedding ERROR\n    cont_vars = [col for col in all_vars if col not in cat_vars+xtra_vars]\n\n\n    train_mask = train_df['d']<= END_TRAIN  # all data b4 end of trg set\n    df = train_df[train_mask][cat_vars + cont_vars + [dep_var]].copy()\n\n    preds_mask = train_df['d']> (START_PRED - 400) # need 364+28days b4 predict to calc lag_roll\n    df_test = train_df[preds_mask][cat_vars + cont_vars + [dep_var]].copy()  #add Target to test for recursive predict\n\n    submit_mask = train_df['d']== START_PRED  #mask 1-store all 3049 products, for only 1 day\n    submit_store = train_df[submit_mask][['id']].copy()\n\n    #procs=[FillMissing, Categorify, Normalize]\n    procs=[Categorify, Normalize]\n\n    cut = df['d'][(df['d'] >= (END_TRAIN - P_HORIZON) )].index.min() # find smallest index\n    last = df['d'][(df['d'] == END_TRAIN )].index.max() #find biggest trg index\n\n    valid_idx = list(range(cut, last))  \n    print(cut, last)\n    #print (valid_idx)\n\n\n    dls = TabularDataLoaders.from_df(df, path=path, procs=procs, cat_names=cat_vars, cont_names=cont_vars, \n                   y_names=TARGET, valid_idx=valid_idx, bs=2048)\n\n    dls.show_batch()\n    return dls","execution_count":null,"outputs":[]},{"metadata":{"id":"onam2yXlEE8q","trusted":true},"cell_type":"code","source":"def create_learner(dls):\n     \n    #y_max = 800.0  #use 1.2*y_max\n    print('set y_max at ', y_max)\n\n    learn = tabular_learner(dls,  loss_func= nn.PoissonNLLLoss(log_input=False), layers=[500, 100], ps=[0.001, 0.01], \n                            emb_drop=0.04, y_range=[0.0, y_max], path=path_o) #define path for Kaggle \n    \n    \n    print('Loss fn= ', learn.loss_func)\n    #learn.model\n\n    learn.lr_find(end_lr=8)\n    learn.fit_one_cycle(10, max_lr=5e-3, wd=0.01,  cbs=SaveModelCallback() ) \n    print('show sample result:')\n    learn.show_results()\n      \n    return learn","execution_count":null,"outputs":[]},{"metadata":{"id":"R1_Qvb9Y2VW9","trusted":true},"cell_type":"code","source":"def predict_store(learn):\n    global preds, tgt\n\n    dl = learn.dls.test_dl(df_test) #can provide Tgt y or not\n    preds, tgt = learn.get_preds(dl=dl)\n    #test_preds = (np.expm1(preds)).numpy().squeeze()  #inv log(x)-1.0\n    test_preds = preds.numpy().squeeze() \n    \n    #df_test['sales_p']=test_preds\n    df_test['pricesales_p']=test_preds\n    df_test['sales_p']= df_test['pricesales_p'] / df_test['sell_price']\n\n    for day_id in range(1, 29):\n    #for day_id in range(29, 57):\n      submit_mask = df_test['d']== (START_PRED -1 + day_id) # 1942 - 1\n      submit_store[f'F{day_id}'] = df_test[submit_mask][['sales_p']].values\n\n    submit_store.to_pickle(f'{path_o}/{store_id}_pred.pkl')\n\n    return","execution_count":null,"outputs":[]},{"metadata":{"id":"HAVDcAOCmgLb","outputId":"e6ba5e86-dad6-45be-a232-776b28612da6","trusted":true},"cell_type":"code","source":"VER = 1                          # Our model version\nSEED = 42                        # We want all things\nseed_everything(SEED)            # to be as deterministic \n#lgb_params['seed'] = SEED        # as possible\nN_CORES = psutil.cpu_count()     # Available CPU cores\n\n#TARGET      = 'sales'            # Our target 'sales'\nTARGET      = 'pricesales'\n\n#remove_features = ['id','state_id','store_id', 'date','wm_yr_wk','d', TARGET]\nremove_features = ['state_id','store_id', 'date', 'wm_yr_wk', ]\n                        \n\n#STORES_IDS = ['CA_1', 'CA_2', 'CA_3', 'CA_4', 'TX_1', 'TX_2', 'TX_3', 'WI_1', 'WI_2', 'WI_3']\nSTORES_IDS = ['CA_1',  ]\n\n\n#LIMITS and const\nSTART_TRAIN = 1200                  # We can skip some rows (Nans/faster training)\nEND_TRAIN   = 1941               # End day of our train set\nSTART_PRED  = 1942        # sid --> Decouple start_pred & end_train; Gap !!\nP_HORIZON   = 28                 # Prediction horizon\nUSE_AUX     = False               # Use or not pretrained models\n\n","execution_count":null,"outputs":[]},{"metadata":{"id":"l5Vk1c4KnD08"},"cell_type":"markdown","source":"# Train Models + Predict by Store (n*lag_28)","execution_count":null},{"metadata":{"id":"SzO7villnHo8","outputId":"d69a5688-0c03-4136-82d3-0c1a31aabe05","trusted":true},"cell_type":"code","source":"for store_id in STORES_IDS:\n    print('Training Store ', store_id)\n    \n    # Get grid for current store\n    grid_df, features_columns = get_data_by_store(store_id, START_TRAIN)\n    \n    \n    ## Create databunch\n    dbunch = create_dbunch(grid_df)\n\n    del grid_df\n    gc.collect()\n\n    # Launch seeder again to make training 100% deterministic\n    seed_everything(SEED)\n\n    learner = create_learner(dbunch)\n\n    predict_store(learner)\n      ","execution_count":null,"outputs":[]},{"metadata":{"id":"dSCRnLSrsvES","outputId":"61b6d93b-71ad-4617-f090-9055f43b5c83","trusted":true},"cell_type":"code","source":"#df_test\ndf_test[(df_test['d']>=1907) & (df_test['item_id']=='FOODS_3_827')]  #.iloc[:, 20:]","execution_count":null,"outputs":[]},{"metadata":{"id":"dAG8mZc3w4Lr","outputId":"0a2a9e7e-edb4-48f4-fbec-31aaa14245dd","trusted":true},"cell_type":"code","source":"tst_feat = list(df_test)\nlen(tst_feat), tst_feat","execution_count":null,"outputs":[]},{"metadata":{"id":"MQdg1sUSoy8t"},"cell_type":"markdown","source":"# Submission","execution_count":null},{"metadata":{"id":"LoA7Pp9fFCc3","outputId":"3e80c069-3672-4b68-f0ed-cafd190bc35d","trusted":true},"cell_type":"code","source":"#path_o = f'{path}/tabular_pred'\nall_preds = pd.DataFrame()\n\nfor store_id in STORES_IDS:\n  temp_df = pd.read_pickle(f'{path_o}/{store_id}_pred.pkl')\n  if 'id' in list(all_preds):\n    #all_preds = all_preds.merge(temp_df, on=['id'], how='left')\n    all_preds = pd.concat([all_preds, temp_df], axis=0, sort=False)\n  else:\n    all_preds = temp_df.copy()\n\n  del temp_df\n    \nall_preds = all_preds.reset_index(drop=True)\nall_preds","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# magic number 1.00 (None)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"feats = list(all_preds)\nfeats.remove('id')\nfor feat in feats :\n    all_preds[feat] = np.round(all_preds[feat].values * 1.00, 4)    ","execution_count":null,"outputs":[]},{"metadata":{"id":"gUrfr1rao0ll","trusted":true},"cell_type":"code","source":"sample = pd.read_csv(ORIGINAL+'/sample_submission.csv')\nsubm_eval = sample[sample['id'].str.contains(\"validation\")].copy()  #validation is now dummy\nsubmission = pd.concat([subm_eval, all_preds ], axis=0, sort=False) \nsubmission.to_csv('submission.csv', index=False)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"Wd-tTaW_ZgwX","outputId":"b0de6ec0-9ff2-4cdd-877a-109bb5da0f8f","trusted":true},"cell_type":"code","source":"submission.id.nunique()","execution_count":null,"outputs":[]},{"metadata":{"id":"mbIAnnkSLIio","outputId":"c4306935-e375-4a59-b487-179be22d4e5c","trusted":true},"cell_type":"code","source":"# no rows = 3049 x # stores + 30490 dummy stores\nsubmission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission[submission['id'].str.contains(\"evaluation\")]","execution_count":null,"outputs":[]},{"metadata":{"id":"KeuVhWwlHb_q","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}