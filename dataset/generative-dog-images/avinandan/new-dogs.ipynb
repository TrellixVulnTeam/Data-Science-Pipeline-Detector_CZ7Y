{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#for (root, dirs, files) in os.walk('../input'):\n#    print(root, dirs)\nwidth = 64\nheight = 64\nchannel = 3\ndogs = []\nepochs = 200\nbatch = 128\nlog_interval = 20","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nfrom PIL import Image\n#import matplotlib.pyplot as plt\nimport xml.etree.ElementTree as ET\n\nfor breed in os.listdir('../input/annotation/Annotation'):\n    for dog in (os.listdir('../input/annotation/Annotation/'+breed)):\n        annot = '../input/annotation/Annotation/'+breed+'/'+dog\n        fullyQualifiedDog = '../input/all-dogs/all-dogs/'+dog\n        try: img = Image.open(fullyQualifiedDog+'.jpg')\n        except: continue\n       \n        file = open(annot, 'r')\n        root = ET.parse(file).getroot()\n        \n        for obj in root.findall('object'):\n            bndbox = obj.find('bndbox')\n            xmin = bndbox.find('xmin').text\n            xmax = bndbox.find('xmax').text\n            ymin = bndbox.find('ymin').text\n            ymax = bndbox.find('ymax').text\n            cropImg = img.crop((int(xmin), int(ymin), int(xmax), int(ymax)))\n            resizedImg = cropImg.resize((width,height), Image.ANTIALIAS)\n            dogs.append(np.asarray(resizedImg))\ndogs = np.asarray(dogs)\nprint(dogs.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense, BatchNormalization, Reshape, Flatten, Activation, Dropout, Conv2D, Conv2DTranspose\nfrom keras.layers.advanced_activations import LeakyReLU\nfrom keras.optimizers import Adam\nfrom keras.initializers import RandomNormal\n\n\ndef conv_generator():\n    model = Sequential()\n    model.add(Dense(units=4*4*512, input_shape=(4096,), kernel_initializer=RandomNormal(stddev=0.02, mean=0.0)))\n    model.add(Reshape(target_shape=(4,4,512)))\n    model.add(BatchNormalization(momentum=0.5))\n    model.add(Activation('relu'))\n    model.add(Dropout(0.2))\n    model.add(Conv2DTranspose(filters=512, kernel_size=(3,3), strides=(2,2), padding='same', data_format='channels_last', kernel_initializer=RandomNormal(stddev=0.02, mean=0.0)))\n    model.add(BatchNormalization(momentum=0.5))\n    model.add(Activation('relu'))\n    model.add(Dropout(0.2))\n    model.add(Conv2DTranspose(filters=256, kernel_size=(3,3), strides=(2,2), padding='same', data_format='channels_last', kernel_initializer=RandomNormal(stddev=0.02, mean=0.0)))\n    model.add(BatchNormalization(momentum=0.5))\n    model.add(Activation('relu'))\n    model.add(Dropout(0.2))\n    model.add(Conv2DTranspose(filters=128, kernel_size=(3,3), strides=(2,2), padding='same', data_format='channels_last', kernel_initializer=RandomNormal(stddev=0.02, mean=0.0)))\n    model.add(BatchNormalization(momentum=0.5))\n    model.add(Activation('relu'))\n    model.add(Dropout(0.2))\n    model.add(Conv2DTranspose(filters=3, kernel_size=(3,3), strides=(2,2), padding='same', data_format='channels_last', kernel_initializer=RandomNormal(stddev=0.02, mean=0.0)))\n    model.add(Activation('tanh'))\n    return model\n\ndef generator():\n    model = Sequential()\n    n_nodes = 256\n    model.add(Dense(n_nodes, input_shape=(100,)))\n    model.add(LeakyReLU(alpha=0.2))\n    model.add(BatchNormalization())\n    model.add(Dense(512))\n    model.add(LeakyReLU(alpha=0.2))\n    model.add(BatchNormalization())\n    model.add(Dense(1024))\n    model.add(LeakyReLU(alpha=0.2))\n    model.add(BatchNormalization())\n    model.add(Dense(width*height*channel, activation='tanh'))\n    model.add(Reshape((width, height, channel)))\n    return model\n\ndef discriminator():\n    model = Sequential()\n    model.add(Flatten(input_shape=(width, height, channel)))\n    model.add(Dense((width*height*channel), input_shape=(width, height, channel)))\n    model.add(LeakyReLU(alpha=0.2))\n    model.add(Dense(int((width*height*channel)/2)))\n    model.add(LeakyReLU(alpha=0.2))\n    model.add(Dense(1, activation='sigmoid'))\n    return model\n\ndef conv_discriminator():\n    model = Sequential()\n    model.add(Conv2D(filters=64, kernel_size=(5,5), strides=(2,2), padding='same', data_format='channels_last', kernel_initializer=RandomNormal(stddev=0.02, mean=0.0)))\n    model.add(BatchNormalization(momentum=0.5))\n    model.add(LeakyReLU(alpha=0.2))\n    model.add(Dropout(0.3))\n    model.add(Conv2D(filters=128, kernel_size=(5,5), strides=(2,2), padding='same', data_format='channels_last', kernel_initializer=RandomNormal(stddev=0.02, mean=0.0)))\n    model.add(BatchNormalization(momentum=0.5))\n    model.add(LeakyReLU(alpha=0.2))\n    model.add(Dropout(0.3))\n    model.add(Conv2D(filters=256, kernel_size=(5,5), strides=(2,2), padding='same', data_format='channels_last', kernel_initializer=RandomNormal(stddev=0.02, mean=0.0)))\n    model.add(BatchNormalization(momentum=0.5))\n    model.add(LeakyReLU(alpha=0.2))\n    model.add(Dropout(0.5))\n    model.add(Conv2D(filters=1, kernel_size=(4,4), strides=(1,1), padding='same'))\n    model.add(Flatten())\n    model.add(Dense(1, activation='sigmoid'))\n    return model\n\nG = conv_generator()\nG.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0002, beta_1=0.5))\nG.summary()\n\nD = conv_discriminator()\nD.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0002, beta_1=0.5), metrics=['accuracy'])\n\ndef G_D():\n    D.trainable = False\n    model = Sequential()\n    model.add(G)\n    model.add(D)\n    return model\n\nG_D = G_D()\nG_D.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0002, beta_1=0.5))\nG_D.summary()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n\n\ndef plot_img():\n    samples = 16\n    noises = np.random.uniform(-1, 1, size=(samples, 4096))\n    images = G.predict(noises)\n    plt.figure(figsize=(8,8))\n    \n    for i in range(16):\n        plt.subplot(4, 4, i+1)\n        plt.axis('off')\n        pltImg = ((images[i] + 1)*127.5).astype(np.uint8)\n        plt.imshow(images[i])\n    plt.show()\n\ndef train():\n    batches = int(len(dogs)/batch)\n    for cnt in range(epochs):\n        for batchCnt in range(batches):\n            for _ in range(2):\n                indx = batchCnt*batch\n                good_imgs = dogs[indx:indx+batch]\n                good_imgs = good_imgs.astype('float32')\n                good_imgs = (good_imgs - 127.5)/127.5\n\n                gen_noise = np.random.uniform(-1, 1, size=(batch, 4096))\n                keras.backend.get_session().run(tf.global_variables_initializer())\n                synthetic_imgs = G.predict(gen_noise)\n                \n                y_real = np.ones((batch, 1))\n                y_real = y_real - 0.3 + np.random.rand(batch, 1)*0.3\n                y_fake = np.zeros((batch, 1))\n                y_fake = y_fake + np.random.rand(batch, 1)*0.3\n                D.trainable = True\n                d_loss_real = D.train_on_batch(good_imgs, y_real)\n                d_loss_fake = D.train_on_batch(synthetic_imgs, y_fake)\n                d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n            D.trainable = False\n            noise = np.random.uniform(-1, 1, size=(batch, 4096))\n            y_mislead = y_real\n            g_loss = G_D.train_on_batch(noise, y_mislead)\n        print ('epoch: %d, [Discriminator :: d_loss: %f], [ Generator :: loss: %f]' % (cnt, d_loss[0], g_loss))\n    plot_img()\ntrain()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}