{"cells":[{"metadata":{},"cell_type":"markdown","source":"## import packages"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport tensorflow as tf\n# tf.enable_eager_execution()  # 可以实现立即输出，不用再麻烦地给Session()\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras import layers\nfrom tensorflow import keras\nfrom PIL import Image\n%matplotlib inline\nfrom imageio import imread, imsave, mimsave\nimport cv2\nimport glob\nimport shutil\nimport xml.etree.ElementTree as ET # xml parser used during pre-processing stage\nimport time # time the execution of codeblocks\nimport xml.dom.minidom # for printing the annotation xml nicely\nprint(os.listdir(\"../input\"))\ntf.__version__\n\n\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## define "},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"BATCH_SIZE = 100\nz_dim = 1000\nWIDTH = 64\nHEIGHT = 64\nLAMBDA = 10\nDIS_ITERS = 3 # 5\nDEPTH = 3\nOUTPUT_DIR = '../samples_dog' \nModel_Path = '../checkpoint'\nDIRout = '../output_images'\n    \nif os.path.exists(DIRout):\n    shutil.rmtree(DIRout)\nif not os.path.exists(DIRout):\n    os.mkdir(DIRout)\nX = tf.placeholder(dtype=tf.float32, shape=[BATCH_SIZE, HEIGHT, WIDTH, 3], name='X')\nnoise = tf.placeholder(dtype=tf.float32, shape=[BATCH_SIZE, z_dim], name='noise')\nis_training = tf.placeholder(dtype=tf.bool, name='is_training')\ndef lrelu(x, leak=0.2):\n    return tf.maximum(x, leak * x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## read images"},{"metadata":{"trusted":true},"cell_type":"code","source":"def crop_images(image_dir, annotation_dir):\n    IMAGES = os.listdir(image_dir)\n    breeds = os.listdir(annotation_dir)  #狗狗种类\n    idxIn = 0; namesIn = []\n    imagesIn = np.zeros((25000,64,64,3))\n\n    # CROP WITH BOUNDING BOXES TO GET DOGS ONLY\n    # iterate through each directory in annotation\n    for breed in breeds:\n        # iterate through each file in the directory\n        for dog in os.listdir(os.path.join(annotation_dir,breed)):\n            try: img = Image.open(image_dir + '/'+dog+'.jpg') \n            except: continue           \n            # Element Tree library allows for parsing xml and getting specific tag values    \n            tree = ET.parse(annotation_dir+'/'+breed+'/'+dog)\n            # take a look at the print out of an xml previously to get what is going on\n            root = tree.getroot() # <annotation>\n            objects = root.findall('object') # <object>\n            for o in objects:\n                bndbox = o.find('bndbox') # <bndbox>\n                xmin = int(bndbox.find('xmin').text) # <xmin>\n                ymin = int(bndbox.find('ymin').text) # <ymin>\n                xmax = int(bndbox.find('xmax').text) # <xmax>\n                ymax = int(bndbox.find('ymax').text) # <ymax>\n                w = np.min((xmax - xmin, ymax - ymin))\n                img2 = img.crop((xmin, ymin, xmin+w, ymin+w))\n                img2 = img2.resize((64,64), Image.ANTIALIAS)\n                imagesIn[idxIn,:,:,:] = np.asarray(img2)\n                namesIn.append(breed)\n                idxIn += 1   \n#     print(idxIn)\n    \n    return imagesIn, namesIn, idxIn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess_image(images,idxIn):\n    images = images[:idxIn,:,:,:] / 255.\n    images = (images[:idxIn,:,:,:] - 0.5)*2\n    return images","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image_dir = '../input/all-dogs/all-dogs'\nannotation_dir = '../input/annotation/Annotation'\nimagesIn,namesIn, idxIn = crop_images(image_dir, annotation_dir)\nprint(idxIn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = np.random.randint(0,idxIn,25)\n\nfor k in range(5):\n    plt.figure(figsize=(15,3))\n    for j in range(5):\n        plt.subplot(1,5,j+1)\n        img = Image.fromarray( imagesIn[x[k*5+j],:,:,:].astype('uint8') )\n        plt.axis('off')\n        plt.title(namesIn[x[k*5+j]].split('-')[1],fontsize=11)\n        plt.imshow(img)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imagesIn = preprocess_image(imagesIn, idxIn)\nimagesIn = imagesIn[:idxIn,:,:,:]\nimageNums = imagesIn.shape[0]\nprint(imageNums)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def gernerate_shuffle_index(epoches, image_nums):\n    shuffle_indice = np.random.permutation(np.arange(image_nums))\n    for i in range(epoches-1):\n        permu = np.random.permutation(np.arange(image_nums))\n        shuffle_indice = np.concatenate((shuffle_indice, permu),axis=0)\n    return shuffle_indice","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_batch_image(shuffle_indice, begin, end):\n    batch_index = shuffle_indice[begin:end]\n    batch_image = np.array([imagesIn[i,:,:,:] for i in batch_index])\n    return batch_image","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def discriminator(image, reuse=None, is_training=is_training):\n    momentum = 0.9\n    with tf.variable_scope('discriminator', reuse=reuse):\n        h0 = lrelu(tf.layers.conv2d(image, kernel_size=5, filters=32, strides=2, padding='same'))\n        \n        h1 = lrelu(tf.layers.conv2d(h0, kernel_size=5, filters=64, strides=2, padding='same'))\n        \n        h2 = lrelu(tf.layers.conv2d(h1, kernel_size=5, filters=128, strides=2, padding='same'))\n        \n        h3 = lrelu(tf.layers.conv2d(h2, kernel_size=5, filters=256, strides=2, padding='same'))\n        \n        h4 = tf.contrib.layers.flatten(h3)\n        h4 = tf.layers.dense(h4, units=1)\n        return h4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def generator(z, is_training=is_training):\n    momentum = 0.9\n    with tf.variable_scope('generator', reuse=None):\n        d = 8\n        h0 = tf.layers.dense(z, units=d * d * 128)\n        h0 = tf.reshape(h0, shape=[-1, d, d, 128])\n        h0 = tf.nn.relu(tf.contrib.layers.batch_norm(h0, is_training=is_training, decay=momentum))\n        \n        h1 = tf.layers.conv2d_transpose(h0, kernel_size=5, filters=64, strides=2, padding='same')\n        h1 = tf.nn.relu(tf.contrib.layers.batch_norm(h1, is_training=is_training, decay=momentum))\n        \n        h2 = tf.layers.conv2d_transpose(h1, kernel_size=5, filters=32, strides=2, padding='same')\n        h2 = tf.nn.relu(tf.contrib.layers.batch_norm(h2, is_training=is_training, decay=momentum))\n        \n#         h3 = tf.layers.conv2d_transpose(h2, kernel_size=5, filters=32, strides=2, padding='same')\n#         h3 = tf.nn.relu(tf.contrib.layers.batch_norm(h3, is_training=is_training, decay=momentum))\n        \n        h4 = tf.layers.conv2d_transpose(h2, kernel_size=5, filters=3, strides=2, padding='same', activation=tf.nn.tanh, name='g')\n        return h4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g = generator(noise)\nd_real = discriminator(X)\nd_fake = discriminator(g, reuse=True)\n\nloss_d_real = -tf.reduce_mean(d_real)\nloss_d_fake = tf.reduce_mean(d_fake)\nloss_g = -tf.reduce_mean(d_fake)\nloss_d = loss_d_real + loss_d_fake\n\nalpha = tf.random_uniform(shape=[BATCH_SIZE, 1, 1, 1], minval=0., maxval=1.)\ninterpolates = alpha * X + (1 - alpha) * g\ngrad = tf.gradients(discriminator(interpolates, reuse=True), [interpolates])[0]\nslop = tf.sqrt(tf.reduce_sum(tf.square(grad), axis=[1]))\ngp = tf.reduce_mean((slop - 1.) ** 2)\nloss_d += LAMBDA * gp\n\nvars_g = [var for var in tf.trainable_variables() if var.name.startswith('generator')]\nvars_d = [var for var in tf.trainable_variables() if var.name.startswith('discriminator')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\nwith tf.control_dependencies(update_ops):\n    optimizer_d = tf.train.AdamOptimizer(learning_rate=0.0002, beta1=0.5).minimize(loss_d, var_list=vars_d)\n    optimizer_g = tf.train.AdamOptimizer(learning_rate=0.0002, beta1=0.5).minimize(loss_g, var_list=vars_g)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def montage(images):    \n    if isinstance(images, list):\n        images = np.array(images)\n    img_h = images.shape[1]\n    img_w = images.shape[2]\n    n_plots = int(np.ceil(np.sqrt(images.shape[0])))\n    if len(images.shape) == 4 and images.shape[3] == 3:\n        m = np.ones(\n            (images.shape[1] * n_plots + n_plots + 1,\n             images.shape[2] * n_plots + n_plots + 1, 3)) * 0.5\n    elif len(images.shape) == 4 and images.shape[3] == 1:\n        m = np.ones(\n            (images.shape[1] * n_plots + n_plots + 1,\n             images.shape[2] * n_plots + n_plots + 1, 1)) * 0.5\n    elif len(images.shape) == 3:\n        m = np.ones(\n            (images.shape[1] * n_plots + n_plots + 1,\n             images.shape[2] * n_plots + n_plots + 1)) * 0.5\n    else:\n        raise ValueError('Could not parse image shape of {}'.format(images.shape))\n    for i in range(n_plots):\n        for j in range(n_plots):\n            this_filter = i * n_plots + j\n            if this_filter < images.shape[0]:\n                this_img = images[this_filter]\n                m[1 + i + i * img_h:1 + i + (i + 1) * img_h,\n                  1 + j + j * img_w:1 + j + (j + 1) * img_w] = this_img\n    return m","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EPOCHES = 300\nIterations = np.floor(EPOCHES * imageNums / BATCH_SIZE)\nShuffle_Indice = gernerate_shuffle_index(EPOCHES, imageNums)\nIterations = int(Iterations)\nprint(type(Iterations))\nprint(Iterations) \nprint(Shuffle_Indice.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sess = tf.Session()\nsess.run(tf.global_variables_initializer())\nz_samples = np.random.uniform(-1.0, 1.0, [BATCH_SIZE, z_dim]).astype(np.float32)\nsamples = []\nloss = {'d': [], 'g': []} \nfor i in tqdm(range(60000)):\n    for j in range(DIS_ITERS):\n        n = np.random.uniform(-1.0, 1.0, [BATCH_SIZE, z_dim]).astype(np.float32)\n        batch_begin = i*BATCH_SIZE\n        batch_end = (i+1)*BATCH_SIZE\n        batch_images = get_batch_image(Shuffle_Indice,batch_begin, batch_end)            \n        _, d_ls = sess.run([optimizer_d, loss_d], feed_dict={X: batch_images, noise: n, is_training: True})\n\n    _, g_ls = sess.run([optimizer_g, loss_g], feed_dict={X: batch_images, noise: n, is_training: True})\n\n    loss['d'].append(d_ls)\n    loss['g'].append(g_ls)\n\n    if i % 500 == 0:\n        print(i, d_ls, g_ls)\n        # 这边用来输出的图像不是随机产生的，是固定的，所以会发现生成的图都差不多hh\n        gen_imgs = sess.run(g, feed_dict={noise: z_samples, is_training: False})\n        gen_imgs = (gen_imgs + 1) / 2\n        imgs = [img[:, :, :] for img in gen_imgs]\n        gen_imgs = montage(imgs)\n        plt.axis('off')\n        plt.imshow(gen_imgs)\n        plt.show()\n        samples.append(gen_imgs)\nplt.plot(loss['d'], label='Discriminator')\nplt.plot(loss['g'], label='Generator')\nplt.legend(loc='upper right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n = 10000\nbatch = 100\nfor i in tqdm(range(0, n, batch)):\n    z = np.random.normal(0,1,size=(batch, z_dim))\n    gen_imgs = sess.run(g, feed_dict={noise: z, is_training: False})\n    gen_imgs = (gen_imgs + 1) / 2\n    imgs = [img[:, :, :] for img in gen_imgs]\n    for index, img in enumerate (imgs):\n        img = (img *255.).astype('uint8')\n        imsave(os.path.join(DIRout, 'image_' + str(i+index+1).zfill(5) + '.png'), img)\n        if (i + index +1) == n:\n            break\nprint(len(os.listdir(DIRout)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if os.path.exists('images.zip'):\n    os.remove('images.zip')\nshutil.make_archive('images', 'zip', DIRout)\n# shutil.rmtree(DIRout)\nprint(\"<END>\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}