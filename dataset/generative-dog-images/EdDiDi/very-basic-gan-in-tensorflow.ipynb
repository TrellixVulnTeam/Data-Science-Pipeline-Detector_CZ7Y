{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"## This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport os\n\nfrom PIL import Image\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nprint(os.listdir(\"../input\"))\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"\n#os.listdir(\"../input/all-dogs/all-dogs/\")\n#os.listdir(\"../input/annotation/Annotation/\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"im = Image.open(\"../input/all-dogs/all-dogs/\"+os.listdir(\"../input/all-dogs/all-dogs/\")[100])\nim=im.resize((64, 64 ))\nnp_im = np.array(im)\nprint(np_im.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.imshow(np_im)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def makeMasterArray(dim):\n    masterLst=[]\n    for pic in os.listdir(\"../input/all-dogs/all-dogs/\"):\n        im = Image.open(\"../input/all-dogs/all-dogs/\"+pic)\n        im=im.resize((dim, dim))\n        np_im = np.array(im)\n        masterLst.append(np_im)\n    return np.array(masterLst)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"masterDogArray=makeMasterArray(64)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def conv(x,w,b,stride,name):\n    with tf.variable_scope('conv'):\n        return tf.nn.conv2d(x,filter=w,strides=[1,stride,stride,1],padding='SAME',name=name)+b\n\ndef deconv(x,w,b,shape,stride,name):\n    with tf.variable_scope('deconv'):\n        return tf.nn.conv2d_transpose(x,filter=w,output_shape=shape,strides=[1,stride,stride,1],padding='SAME',name=name)+ b\n\ndef lrelu(x,alpha=0.2):\n    with tf.variable_scope('leakyReLU'):\n        return tf.maximum(x,alpha*x)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef generator(X,batch_size=64):\n    \n    with tf.variable_scope('generator'):\n        K=64\n        L=32\n        M=16\n        \n        W1=tf.get_variable('G_W1',[100,16*16*K],initializer=tf.random_normal_initializer(stddev=0.1))\n        B1=tf.get_variable('G_B1',[16*16*K],initializer=tf.constant_initializer())\n        \n        W2=tf.get_variable('G_W2',[4,4,M,K],initializer=tf.random_normal_initializer(stddev=0.1))\n        B2=tf.get_variable('G_B2',[M],initializer=tf.constant_initializer())\n        \n        W3=tf.get_variable('G_W3',[4,4,3,M],initializer=tf.random_normal_initializer(stddev=0.1))\n        B3=tf.get_variable('G_B3',[3],initializer=tf.constant_initializer())\n        \n        X=lrelu(tf.matmul(X,W1)+B1)\n        X=tf.reshape(X,[batch_size,16,16,K])\n        \n        deconv1=deconv(X,W2,B2,shape=[batch_size,32,32,M],stride=2,name='deconv1')\n        \n        bn1=tf.contrib.layers.batch_norm(deconv1)\n        \n        deconv22=deconv(tf.nn.dropout(lrelu(bn1),0.4),W3,B3,shape=[batch_size,64,64,3],stride=2,name='deconv2')\n        \n        return deconv22\n\ndef discriminator(X,reuse=False):\n    with tf.variable_scope('discriminator'):\n        if reuse:\n            tf.get_variable_scope().reuse_variables()\n        K=16\n        M=32\n        N=64\n        \n        W1=tf.get_variable('D_W1',[4,4,3,K],initializer=tf.random_normal_initializer(stddev=0.1))\n        B1=tf.get_variable('D_B1',[K],initializer=tf.constant_initializer())\n        \n        W2=tf.get_variable('D_W2',[4,4,K,M],initializer=tf.random_normal_initializer(stddev=0.1))\n        B2=tf.get_variable('D_B2',[M],initializer=tf.constant_initializer())\n        \n        W3=tf.get_variable('D_W3',[16*16*M,N],initializer=tf.random_normal_initializer(stddev=0.1))\n        B3=tf.get_variable('D_B3',[N],initializer=tf.constant_initializer())\n        \n        W4=tf.get_variable('D_W4',[N,1],initializer=tf.random_normal_initializer(stddev=0.1))\n        B4=tf.get_variable('D_B4',[1],initializer=tf.constant_initializer())\n        \n        X=tf.reshape(X,[-1,64,64,3],'reshape')\n        conv1=conv(X,W1,B1,stride=2,name='conv1')\n        bn1=tf.contrib.layers.batch_norm(conv1)\n        \n        conv2=conv(tf.nn.dropout(lrelu(bn1),0.4),W2,B2,stride=2,name='conv2')\n        bn2=tf.contrib.layers.batch_norm(conv2)\n        \n        flat=tf.reshape(tf.nn.dropout(lrelu(bn2),0.4),[-1,16*16*M],name='flat')\n        dense=lrelu(tf.matmul(flat,W3)+B3)\n        logits=tf.matmul(dense,W4)+B4\n        prob=tf.nn.sigmoid(logits)\n\n        return prob,logits\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef train(batch_size):\n    with tf.variable_scope('placeholder'):\n        X=tf.placeholder(tf.float32,[None,64,64,3])\n        z=tf.placeholder(tf.float32,[None,100])\n        G=generator(z,batch_size)\n        D_real,D_real_logits=discriminator(X,reuse=False)\n        D_fake,D_fake_logits=discriminator(G,reuse=True)\n    \n    with tf.variable_scope('D_loss'):\n        d_loss_real=tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_real_logits,labels=tf.ones_like(D_real_logits)))\n        d_loss_fake=tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_fake_logits,labels=tf.zeros_like(D_fake_logits)))\n        d_loss=d_loss_real+d_loss_fake\n    \n    with tf.variable_scope('G_loss'):\n        g_loss=tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_fake_logits,labels=tf.ones_like(D_fake_logits)))*1000\n        tvar=tf.trainable_variables()\n        dvar=[var for var in tvar if 'discriminator' in var.name]\n        gvar=[var for var in tvar if 'generator' in var.name]\n        \n        \n    with tf.variable_scope('train'):\n        d_train_step=tf.train.AdamOptimizer().minimize(d_loss,var_list=dvar)\n        g_train_step=tf.train.AdamOptimizer().minimize(g_loss,var_list=gvar)\n        \n    sess=tf.Session()\n    init=tf.global_variables_initializer()\n    sess.run(init)\n\n    for iters in range(40):\n        for batchIdx in range(int(masterDogArray.shape[0]/batch_size)):\n            batch_X=masterDogArray[batchIdx*batch_size:(batchIdx+1)*batch_size]\n            batch_noise=np.random.uniform(-1.0,1.0,[batch_size,100])\n            _,d_loss_print=sess.run([d_train_step,d_loss],feed_dict={X:batch_X,z:batch_noise})\n            _,g_loss_print=sess.run([g_train_step,g_loss],feed_dict={z:batch_noise})\n            \n            #print('g_loss: %f, d_loss: %f' %(g_loss_print,d_loss_print))\n        samples=sess.run(G,feed_dict={z:np.random.uniform(-1.0,1.0,[batch_size,100])})\n\n        plt.imshow(samples[0])\n        plt.show()\n            ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntrain(64)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}