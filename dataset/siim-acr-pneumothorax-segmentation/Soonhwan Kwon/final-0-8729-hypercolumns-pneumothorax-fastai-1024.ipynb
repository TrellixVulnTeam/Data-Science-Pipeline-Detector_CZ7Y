{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Overview\nThe primary goal of this competition is identification and segmentation of chest radiographic images with pneumothorax. In this kernel a U-net based approach is used, which provides end-to-end framework for image segmentation. In prior image segmentation competitions ([Airbus Ship Detection Challenge](https://www.kaggle.com/c/airbus-ship-detection/discussion) and [TGS Salt Identification Challenge](https://www.kaggle.com/c/tgs-salt-identification-challenge)), U-net based model architecture has demonstrated supperior performence, and top solutions are based on it. The current competition is similar to TGS Salt Identification Challenge in terms of identifying the correct mask based on visual inspection of images. Therefore, I have tried a technique that was extremely effective in Satl competition - [Hypercolumns](https://towardsdatascience.com/review-hypercolumn-instance-segmentation-367180495979).\n\nAs a starting point [this public kernel](https://www.kaggle.com/mnpinto/pneumothorax-fastai-u-net) is used, and the following things are added (see text below for mode details):\n* Hypercolumns\n* Gradient accumulation\n* TTA based on horizontal flip\n* Noise removal (if the predicted mask contains too few pixels, it is assumed to be empty)\n* Image equilibration"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":false},"cell_type":"code","source":"%reload_ext autoreload\n%autoreload 2\n%matplotlib inline\nimport os\n# os.environ['CUDA_VISIBLE_DEVICES']='0,1,2,3'\nos.environ['CUDA_VISIBLE_DEVICES']='0'\nimport sys\nsys.path.insert(0, '../input/siim-acr-pneumothorax-segmentation')\n\nimport fastai\nfrom fastai.vision import *\nfrom mask_functions import *\nfrom fastai.callbacks import SaveModelCallback\nimport gc\nfrom sklearn.model_selection import KFold\nfrom PIL import Image\n\nfastai.__version__","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false},"cell_type":"markdown","source":"The original images, provided in this competition, have 1024x1024 resolution. To prevent additional overhead on image loading, the datasets composed of 128x128 and 256x256 scaled down images are prepared separately and used as an input. Check [this keknel](https://www.kaggle.com/iafoss/data-repack-and-image-statistics) for more details on image rescaling and mask generation. Also In that kernel I apply image normalization based on histograms (exposure.equalize_adapthist) that provides some improvement of image appearance as well as a small boost of the model performance. The corresponding pixel statistics are computed in the kernel."},{"metadata":{"_kg_hide-output":false,"trusted":false},"cell_type":"code","source":"sz = 1024\nbs = 2\nn_acc = 64//bs #gradinet accumulation steps\nnfolds = 4\nSEED = 2019\n\n#eliminate all predictions with a few (noise_th) pixesls\nnoise_th = 50.0*(sz/128.0)**2 #threshold for the number of predicted pixels\nbest_thr0 = 0.2 #preliminary value of the threshold for metric calculation\n\nif sz == 256:\n    stats = ([0.540,0.540,0.540],[0.264,0.264,0.264])\n    TRAIN = '../input/siimacr-pneumothorax-segmentation-data-256/train'\n    TEST = '../input/siimacr-pneumothorax-segmentation-data-256/test'\n    MASKS = '../input/siimacr-pneumothorax-segmentation-data-256/masks'\nelif sz == 128:\n    stats = ([0.615,0.615,0.615],[0.291,0.291,0.291])\n    TRAIN = '../input/siimacr-pneumothorax-segmentation-data-128/train'\n    TEST = '../input/siimacr-pneumothorax-segmentation-data-128/test'\n    MASKS = '../input/siimacr-pneumothorax-segmentation-data-128/masks'\nelif sz ==512:\n#     mean: 0.5292001691897411 , std: 0.2588056436836543\n#     mean: 0.5265718130841701 , std: 0.2589975116265956\n    stats = ([0.5292,0.5292,0.5292],[0.2588,0.2588,0.2588])\n    TRAIN = '../input/siimacr-pneumothorax-segmentation-data-512/train'\n    TEST = '../input/siimacr-pneumothorax-segmentation-data-512/test'\n    MASKS = '../input/siimacr-pneumothorax-segmentation-data-512/masks'\nelif sz ==1024:\n    \n#     mean: 0.521344684992593 , std: 0.2542653719896922\n#     mean: 0.5188847691560139 , std: 0.2546409761578348\n    stats = ([0.521,0.521,0.521],[0.254,0.254,0.254])\n    TRAIN = '../input/siimacr-pneumothorax-segmentation-data-1024/train'\n    TEST = '../input/siimacr-pneumothorax-segmentation-data-1024/test'\n    MASKS = '../input/siimacr-pneumothorax-segmentation-data-1024/masks'\n\n# copy pretrained weights for resnet34 to the folder fastai will search by default\nPath('/tmp/.cache/torch/checkpoints/').mkdir(exist_ok=True, parents=True)\n# !cp '../input/resnet34/resnet34.pth' '/tmp/.cache/torch/checkpoints/resnet34-333f7ec4.pth'\n\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    #tf.set_random_seed(seed)\nseed_everything(SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from fastai.vision.learner import create_head, cnn_config, num_features_model, create_head#, Hook\nfrom fastai.callbacks.hooks import model_sizes, hook_outputs, dummy_eval, Hook, _hook_inner\nfrom fastai.vision.models.unet import _get_sfs_idxs, UnetBlock\n\nclass Hcolumns(nn.Module):\n    def __init__(self, hooks:Collection[Hook], nc:Collection[int]=None):\n        super(Hcolumns,self).__init__()\n        self.hooks = hooks\n        self.n = len(self.hooks)\n        self.factorization = None \n        if nc is not None:\n            self.factorization = nn.ModuleList()\n            for i in range(self.n):\n                self.factorization.append(conv2d(nc[i],nc[-1],1,bias=True))\n        \n    def forward(self, x:Tensor):\n        n = len(self.hooks)\n        out = [F.interpolate(self.hooks[i].stored if self.factorization is None\n                    else self.factorization[i](self.hooks[i].stored), scale_factor=2**(self.n-i),\n                    mode='bilinear',align_corners=False) for i in range(self.n)] + [x]\n        return torch.cat(out, dim=1)\n\nclass DynamicUnet_Hcolumns(SequentialEx):\n    \"Create a U-Net from a given architecture.\"\n    def __init__(self, encoder:nn.Module, n_classes:int, blur:bool=False, blur_final=True, \n                 self_attention:bool=False,\n                 y_range:Optional[Tuple[float,float]]=None,\n                 last_cross:bool=True, bottle:bool=False, **kwargs):\n        imsize = (sz,sz)\n        sfs_szs = model_sizes(encoder, size=imsize)\n        sfs_idxs = list(reversed(_get_sfs_idxs(sfs_szs)))\n        self.sfs = hook_outputs([encoder[i] for i in sfs_idxs])\n        x = dummy_eval(encoder, imsize).detach()\n\n        ni = sfs_szs[-1][1]\n        middle_conv = nn.Sequential(conv_layer(ni, ni*2, **kwargs),\n                                    conv_layer(ni*2, ni, **kwargs)).eval()\n        x = middle_conv(x)\n        layers = [encoder, batchnorm_2d(ni), nn.ReLU(), middle_conv]\n\n        self.hc_hooks = []\n        hc_c = []\n        self.hc_hooks.append(Hook(layers[-1], _hook_inner, detach=False))\n        hc_c.append(x.shape[1])\n        \n        for i,idx in enumerate(sfs_idxs):\n            not_final = i!=len(sfs_idxs)-1\n            up_in_c, x_in_c = int(x.shape[1]), int(sfs_szs[idx][1])\n            do_blur = blur and (not_final or blur_final)\n            sa = self_attention and (i==len(sfs_idxs)-3)\n            unet_block = UnetBlock(up_in_c, x_in_c, self.sfs[i], final_div=not_final, blur=blur, self_attention=sa,\n                                   **kwargs).eval()\n            layers.append(unet_block)\n            x = unet_block(x)\n            self.hc_hooks.append(Hook(layers[-1], _hook_inner, detach=False))\n            hc_c.append(x.shape[1])\n\n        ni = x.shape[1]\n        if imsize != sfs_szs[0][-2:]: layers.append(PixelShuffle_ICNR(ni, **kwargs))\n        if last_cross:\n            layers.append(MergeLayer(dense=True))\n            ni += in_channels(encoder)\n            layers.append(res_block(ni, bottle=bottle, **kwargs))\n        hc_c.append(ni)\n        layers.append(Hcolumns(self.hc_hooks, hc_c))\n        layers += [conv_layer(ni*len(hc_c), 1, ks=1, use_activ=False, **kwargs)]\n        if y_range is not None: layers.append(SigmoidRange(*y_range))\n        super().__init__(*layers)\n\n    def __del__(self):\n        if hasattr(self, \"sfs\"): self.sfs.remove()\n            \ndef unet_learner(data:DataBunch, arch:Callable, pretrained:bool=True, blur_final:bool=True,\n                 norm_type:Optional[NormType]=NormType, split_on:Optional[SplitFuncOrIdxList]=None, blur:bool=False,\n                 self_attention:bool=False, y_range:Optional[Tuple[float,float]]=None, last_cross:bool=True,\n                 bottle:bool=False, cut:Union[int,Callable]=None, hypercolumns=True, **learn_kwargs:Any)->Learner:\n    \"Build Unet learner from `data` and `arch`.\"\n    meta = cnn_config(arch)\n    body = create_body(arch, pretrained, cut)\n    M = DynamicUnet_Hcolumns if hypercolumns else DynamicUnet\n#     model = to_device(M(body, n_classes=data.c, blur=blur, blur_final=blur_final,\n#           self_attention=self_attention, y_range=y_range, norm_type=norm_type, last_cross=last_cross,\n#           bottle=bottle), data.device)\n    model = to_device(M(body, n_classes=data.c, blur=blur, blur_final=blur_final,\n          self_attention=self_attention, y_range=y_range, norm_type=norm_type, last_cross=last_cross,\n          bottle=bottle),data.device)\n    learn = Learner(data, model, **learn_kwargs)\n    learn.split(ifnone(split_on, meta['split']))\n    if False:\n        learn.model = torch.nn.DataParallel(learn.model)\n    \n    if pretrained: learn.freeze()\n    apply_init(model[2], nn.init.kaiming_normal_)\n    return learn","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":false},"cell_type":"code","source":"# Setting div=True in open_mask\nclass SegmentationLabelList(SegmentationLabelList):\n    def open(self, fn): return open_mask(fn, div=True)\n    \nclass SegmentationItemList(SegmentationItemList):\n    _label_cls = SegmentationLabelList\n\n# Setting transformations on masks to False on test set\ndef transform(self, tfms:Optional[Tuple[TfmList,TfmList]]=(None,None), **kwargs):\n    if not tfms: tfms=(None,None)\n    assert is_listy(tfms) and len(tfms) == 2\n    self.train.transform(tfms[0], **kwargs)\n    self.valid.transform(tfms[1], **kwargs)\n    kwargs['tfm_y'] = False # Test data has no labels\n    if self.test: self.test.transform(tfms[1], **kwargs)\n    return self\nfastai.data_block.ItemLists.transform = transform","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model"},{"metadata":{},"cell_type":"markdown","source":"The model used in this kernel is based on U-net like architecture with ResNet34 encoder. To boost the model performance, Hypercolumns are incorporated into DynamicUnet fast.ai class (see code below). The idea of Hypercolumns is schematically illustrated in the following figure. ![](https://i.ibb.co/3y7f8rj/Hypercolumns1.png)\nEach upscaling block is connected to the output layer through linear resize to the original image size. So the final image is produced based on concatenation of U-net output with resized outputs of intermediate layers. These skip-connections provide a shortcut for gradient flow improving model performance and convergence speed. Since intermediate layers have many channels, their upscaling and use as an input for the final layer would introduce a significant overhead in terms the computational time and memory. Therefore, 1x1 convolutions are applied (factorization) before the resize to reduce the number of channels.\nFurther details on Hypercolumns can be found [here](http://home.bharathh.info/pubs/pdfs/BharathCVPR2015.pdf) and [here](https://towardsdatascience.com/review-hypercolumn-instance-segmentation-367180495979)."},{"metadata":{},"cell_type":"markdown","source":"Accumulation of gradients to overcome the problem of too small batches. The code is mostly based on [this post](https://forums.fast.ai/t/accumulating-gradients/33219/25) with slight adjustment to work with mean reduction."},{"metadata":{"trusted":false},"cell_type":"code","source":"class AccumulateOptimWrapper(OptimWrapper):\n    def step(self):          pass\n    def zero_grad(self):      pass\n    def real_step(self):      super().step()\n    def real_zero_grad(self): super().zero_grad()\n        \ndef acc_create_opt(self, lr:Floats, wd:Floats=0.):\n        \"Create optimizer with `lr` learning rate and `wd` weight decay.\"\n        self.opt = AccumulateOptimWrapper.create(self.opt_func, lr, self.layer_groups,\n                                         wd=wd, true_wd=self.true_wd, bn_wd=self.bn_wd)\nLearner.create_opt = acc_create_opt   \n\n@dataclass\nclass AccumulateStep(LearnerCallback):\n    \"\"\"\n    Does accumlated step every nth step by accumulating gradients\n    \"\"\"\n    def __init__(self, learn:Learner, n_step:int = 1):\n        super().__init__(learn)\n        self.n_step = n_step\n\n    def on_epoch_begin(self, **kwargs):\n        \"init samples and batches, change optimizer\"\n        self.acc_batches = 0\n        \n    def on_batch_begin(self, last_input, last_target, **kwargs):\n        \"accumulate samples and batches\"\n        self.acc_batches += 1\n        #print(f\"At batch {self.acc_batches}\")\n        \n    def on_backward_end(self, **kwargs):\n        \"step if number of desired batches accumulated, reset samples\"\n        if (self.acc_batches % self.n_step) == self.n_step - 1:\n            for p in (self.learn.model.parameters()):\n                if p.requires_grad: p.grad.div_(self.acc_batches)\n    \n            #print(f\"Stepping at batch: {self.acc_batches}\")\n            self.learn.opt.real_step()\n            self.learn.opt.real_zero_grad()\n            self.acc_batches = 0\n    \n    def on_epoch_end(self, **kwargs):\n        \"step the rest of the accumulated grads\"\n        if self.acc_batches > 0:\n            for p in (self.learn.model.parameters()):\n                if p.requires_grad: p.grad.div_(self.acc_batches)\n            self.learn.opt.real_step()\n            self.learn.opt.real_zero_grad()\n            self.acc_batches = 0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A slight modification of the default dice metric to make it comparable with the competition metric: dice is computed for each image independently, and dice of empty image with zero prediction is 1. Also I use noise removal and similar threshold as in my prediction pipline."},{"metadata":{"trusted":false},"cell_type":"code","source":"def dice(input:Tensor, targs:Tensor, iou:bool=False, eps:float=1e-8)->Rank0Tensor:\n    \"Dice coefficient metric for binary target. If iou=True, returns iou metric, classic for segmentation problems.\"\n    n = targs.shape[0]\n    if mode =='bce':\n        input = torch.softmax(input, dim=1)[:,1,...].view(n,-1)\n    else:\n        input = torch.sigmoid(input).view(n, -1)\n    input = (input > best_thr0).long()\n    input[input.sum(-1) < noise_th,...] = 0.0 \n    #input = input.argmax(dim=1).view(n,-1)\n    targs = targs.view(n,-1)\n    intersect = (input * targs).sum(-1).float()\n    union = (input+targs).sum(-1).float()\n    if not iou: return ((2.0*intersect + eps) / (union+eps)).mean()\n    else: return ((intersect + eps) / (union - intersect + eps)).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#dice for threshold selection\ndef dice_overall(preds, targs):\n    n = preds.shape[0]\n    preds = preds.view(n, -1)\n    targs = targs.view(n, -1)\n    intersect = (preds * targs).sum(-1).float()\n    union = (preds+targs).sum(-1).float()\n    u0 = union==0\n    intersect[u0] = 1\n    union[u0] = 2\n    return (2. * intersect / union)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The following function generates predictions with using flip TTA (average the result for the original image and a flipped one)."},{"metadata":{"trusted":false},"cell_type":"code","source":"# Prediction with flip TTA\ndef pred_with_flip(learn:fastai.basic_train.Learner,\n                   ds_type:fastai.basic_data.DatasetType=DatasetType.Valid):\n    #get prediction\n    preds, ys = learn.get_preds(ds_type)\n    if False:\n        preds = preds[:,1,...]\n    else:\n        preds = torch.sigmoid(preds)\n    #add fiip to dataset and get prediction\n    learn.data.dl(ds_type).dl.dataset.tfms.append(flip_lr())\n    preds_lr, ys = learn.get_preds(ds_type)\n    del learn.data.dl(ds_type).dl.dataset.tfms[-1]\n    if False:\n        preds_lr = preds_lr[:,1,...]\n    else:\n        preds_lr = torch.sigmoid(preds_lr)\n    ys = ys.squeeze()\n    preds = 0.5*(preds + torch.flip(preds_lr,[-1]))\n    del preds_lr\n    gc.collect()\n    torch.cuda.empty_cache()\n    return preds, ys","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"! ls ../input/ids-for-test/items_test_dgx01_1024","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data"},{"metadata":{"trusted":false},"cell_type":"code","source":"def get_data(fold):\n    kf = KFold(n_splits=nfolds, shuffle=True, random_state=SEED)\n    valid_idx = list(kf.split(list(range(len(Path(TRAIN).ls())))))[fold][1]\n    segs = pickle.load(open('../input/folder-list/cv_list','rb'))\n    \n    # Create databunch\n    data = (#SegmentationItemList.from_folder(TRAIN)\n            segs\n            .split_by_idx(valid_idx)\n            .label_from_func(lambda x : str(x).replace('train', 'masks'), classes=[0,1])\n            .add_test(pickle.load(open('../input/ids-for-test/items_test_dgx01_1024','rb')), label=None)\n#             .add_test(Path(TEST).ls(), label=None)\n            .transform(get_transforms(), size=sz, tfm_y=True)\n            .databunch(path=Path('.'), bs=bs)\n            .normalize(stats))\n    return data\n\n# Display some images with masks\nget_data(0).show_batch()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Training"},{"metadata":{},"cell_type":"markdown","source":"Expand the following cell to see the model printout. The model is based on Unet like architecture with ResNet34 based pretrained encoder. The upscaling is based on [pixel shuffling technique](https://arxiv.org/pdf/1609.05158.pdf). On the top, hypercolumns are added to provide additional skip-connections between the upscaling blocks and the output."},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":false},"cell_type":"code","source":"unet_learner(get_data(0), models.resnet34, metrics=[dice]).model","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"mode = 'bce_soft_dice'\nif mode == 'weighted_bce':\n    def criterion_pixel(logit_pixel, truth_pixel):\n        logit = logit_pixel.view(-1)\n        truth = truth_pixel.view(-1).float()\n#         print(logit.shape, truth.shape)\n        assert(logit.shape == truth.shape)\n\n        loss = F.binary_cross_entropy_with_logits(logit, truth, reduction='none')\n        if 0:\n            loss = loss.mean()\n        if 1:\n            pos = (truth > 0.5).float()\n            neg = (truth < 0.5).float()\n            pos_weight = pos.sum().item() + 1e-12\n            neg_weight = neg.sum().item() + 1e-12\n            loss = (0.25* pos*loss/pos_weight + 0.75*neg*loss/neg_weight).sum()\n        return loss\nelif mode == 'bce_soft_dice':\n    def soft_dice_criterion(logit, truth, weight=[0.5, 0.5]):\n        batch_size = len(logit)\n        probability = torch.sigmoid(logit)\n        p = probability.view(batch_size, -1)\n        t = truth.view(batch_size, -1)\n        w = truth.detach()\n        w = w*(weight[1] - weight[0])+weight[0]\n        \n        p = w*(p*2-1) # convert to [0,1] --> [-1,1]\n        t = w*(t*2-1)\n        \n        intersection = (p*t).sum(-1)\n        union = (p*p).sum(-1) + (t*t).sum(-1)\n        dice = 1 - 2* intersection/union\n        \n        loss = dice\n        return loss\n        \n    def criterion_pixel(logit_pixel, truth_pixel):\n        batch_size = len(logit_pixel)\n        logit = logit_pixel.view(batch_size, -1)\n        truth = truth_pixel.view(batch_size,-1).float()\n        assert(logit.shape==truth.shape)\n        \n        loss = soft_dice_criterion(logit, truth)\n        \n        loss1 = loss.mean()\n        \n        logit = logit_pixel.view(-1)\n        truth = truth_pixel.view(-1).float()\n#         print(logit.shape, truth.shape)\n        assert(logit.shape == truth.shape)\n\n        loss = F.binary_cross_entropy_with_logits(logit, truth, reduction='none')\n        loss2 = loss.mean()\n        loss = (loss1 + loss2) /2\n        return loss\n    \nelif mode == 'lovasz':\n    def lovasz_loss(logit, truth, margin=[1,5]):\n        def compute_lovasz_gradient(truth):\n            truth_sum = truth.sum()\n            intersection = truth_sum - truth.cumsum(0)\n            union = truth_sum + (1-truth).cumsum(0)\n            jaccard = 1. - intersection / union\n            T = len(truth)\n            jaccard[1:T] = jaccard[1:T] - jaccard[0:T-1]\n\n            gradient = jaccard\n            return gradient\n\n        def lovasz_hinge_one(logit, truth):\n            m = truth.detach()\n            m = m*(margin[1] - margin[0])+margin[0]\n\n            truth = truth.float()\n            sign = 2. * truth-1\n            hinge = (m - logit * sign)\n            hinge, permutation = torch.sort(hinge, dim=0, descending= True)\n            hinge = F.relu(hinge)\n\n            truth = truth[permutation.data]\n            gradient = compute_lovasz_gradient(truth)\n\n            loss = torch.dot(hinge, gradient)\n            return loss\n        \n        lovasz_one = lovasz_hinge_one\n        \n        batch_size = len(truth)\n        loss = torch.zeros(batch_size).cuda()\n        for b in range(batch_size):\n            l, t = logit[b].view(-1), truth[b].view(-1)\n            loss[b] = lovasz_one(l, t)\n        return loss\n\n    def criterion_pixel(logit_pixel, truth_pixel):\n        batch_size = len(logit_pixel)\n        logit = logit_pixel.view(batch_size, -1)\n        truth = truth_pixel.view(batch_size,-1).float()\n        assert(logit.shape==truth.shape)\n\n        loss = lovasz_loss(logit, truth)\n\n        loss = loss.mean()\n        return loss\nelif mode == 'bce_sigmoid':\n    def criterion_pixel(logit_pixel, truth_pixel):\n        logit = logit_pixel.view(-1)\n        truth = truth_pixel.view(-1).float()\n#         print(logit.shape, truth.shape)\n        assert(logit.shape == truth.shape)\n\n        loss = F.binary_cross_entropy_with_logits(logit, truth, reduction='none')\n        loss = loss.mean()\n        return loss        \nelse:\n    mode = None\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"gc.collect()\ntorch.cuda.empty_cache()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"scores, best_thrs = [],[]\n# first phase for bce then lovasz??\nfold = 0\nif True:\n# for fold in range(nfolds):\n    print('fold: ', fold)\n    data = get_data(fold)\n    learn = unet_learner(data, models.resnet34, metrics=[dice])\n    print(mode)\n    if mode is not None:\n        learn.loss_func = criterion_pixel\n    else:\n        mode = 'bce'\n    \n    epochpost=12\n    \n    learn.clip_grad(1.0);\n    \n    #fit the decoder part of the model keeping the encode frozen\n    lr = 7e-3\n    epochpre = 6\n#     learn.load('resnet34_imsizefixed_img{}_fold{}_lr{}_epochpre{}_epochpost{}'.format(sz,fold,lr,epochpre,epochpost))\n    learn.fit_one_cycle(epochpre, lr, callbacks = [AccumulateStep(learn,n_acc)])\n    \n    #fit entire model with saving on the best epoch\n    learn.unfreeze()\n#     learn.fit_one_cycle(12, slice(lr/80, lr/2), callbacks = [AccumulateStep(learn,n_acc)])\n    learn.fit_one_cycle(epochpost, slice(lr/80, lr/2), callbacks = [\n        AccumulateStep(learn,n_acc), \n        SaveModelCallback(learn, \n                          monitor='dice', \n                          mode='max',\n                          name='resnet34_imsizefixed_{}_best_img{}_fold{}_lr{}_epochpre{}_epochpost{}'.format(mode,sz,fold,lr,epochpre,epochpost))])\n    learn.save('resnet34_imsizefixed_{}_img{}_fold{}_lr{}_epochpre{}_epochpost{}'.format(mode,sz,fold,lr,epochpre,epochpost));\n    if False:\n        #prediction on val and test sets\n        preds, ys = pred_with_flip(learn)\n        pt, _ = pred_with_flip(learn,DatasetType.Test)\n\n        if fold == 0: preds_test = pt\n        else: preds_test += pt\n\n        #remove noise\n        preds[preds.view(preds.shape[0],-1).sum(-1) < noise_th,...] = 0.0\n\n        #optimal threshold \n        #the best way would be collecting all oof predictions followed by single threshold calculation\n        #however, it requres too much RAM for high image resolution\n        dices = []\n        thrs = np.arange(0.01, 1, 0.01)\n        for th in progress_bar(thrs):\n            preds_m = (preds>th).long()\n            dices.append(dice_overall(preds_m, ys).mean())\n        dices = np.array(dices)    \n        scores.append(dices.max())\n        best_thrs.append(thrs[dices.argmax()])\n\n        if fold != nfolds-1: del preds, ys\n    gc.collect()\n    torch.cuda.empty_cache()\n    \n# preds_test /= nfolds","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# ph2 \nscores, best_thrs = [],[]\n# first phase for bce then lovasz??\nfold = 0\nif True:\n# for fold in range(nfolds):\n    print('fold: ', fold)\n    data = get_data(fold)\n    learn = unet_learner(data, models.resnet34, metrics=[dice])\n    print(mode)\n    if mode is not None:\n        learn.loss_func = criterion_pixel\n    else:\n        mode = 'bce'\n    \n    epochpost=12\n    \n    learn.clip_grad(1.0);\n    \n    #fit the decoder part of the model keeping the encode frozen\n    lr = 7e-3\n    epochpre = 6\n    learn.load('resnet34_imsizefixed_{}_img{}_fold{}_lr{}_epochpre{}_epochpost{}'.format(mode,sz,fold,lr,epochpre,epochpost))\n#     learn.fit_one_cycle(epochpre, lr, callbacks = [AccumulateStep(learn,n_acc)])\n    \n    #fit entire model with saving on the best epoch\n    learn.unfreeze()\n#     learn.fit_one_cycle(12, slice(lr/80, lr/2), callbacks = [AccumulateStep(learn,n_acc)])\n    learn.fit_one_cycle(epochpost, slice(lr/80, lr/2), callbacks = [\n        AccumulateStep(learn,n_acc), \n        SaveModelCallback(learn, \n                          monitor='dice', \n                          mode='max',\n                          name='resnet34_imsizefixed_finetune_{}_best_img{}_fold{}_lr{}_epochpre{}_epochpost{}'.format(mode,sz,fold,lr,epochpre,epochpost))])\n    learn.save('resnet34_imsizefixed_finetune_{}_img{}_fold{}_lr{}_epochpre{}_epochpost{}'.format(mode,sz,fold,lr,epochpre,epochpost));\n    if False:\n        #prediction on val and test sets\n        preds, ys = pred_with_flip(learn)\n        pt, _ = pred_with_flip(learn,DatasetType.Test)\n\n        if fold == 0: preds_test = pt\n        else: preds_test += pt\n\n        #remove noise\n        preds[preds.view(preds.shape[0],-1).sum(-1) < noise_th,...] = 0.0\n\n        #optimal threshold \n        #the best way would be collecting all oof predictions followed by single threshold calculation\n        #however, it requres too much RAM for high image resolution\n        dices = []\n        thrs = np.arange(0.01, 1, 0.01)\n        for th in progress_bar(thrs):\n            preds_m = (preds>th).long()\n            dices.append(dice_overall(preds_m, ys).mean())\n        dices = np.array(dices)    \n        scores.append(dices.max())\n        best_thrs.append(thrs[dices.argmax()])\n\n        if fold != nfolds-1: del preds, ys\n    gc.collect()\n    torch.cuda.empty_cache()\n    \n# preds_test /= nfolds","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# ph3\nscores, best_thrs = [],[]\n# first phase for bce then lovasz??\nfold = 0\nif True:\n# for fold in range(nfolds):\n    print('fold: ', fold)\n    data = get_data(fold)\n    learn = unet_learner(data, models.resnet34, metrics=[dice])\n    print(mode)\n    if mode is not None:\n        learn.loss_func = criterion_pixel\n    else:\n        mode = 'bce'\n    \n    epochpost=12\n    \n    learn.clip_grad(1.0);\n    \n    #fit the decoder part of the model keeping the encode frozen\n    lr = 7e-3\n    epochpre = 6\n    learn.load('resnet34_imsizefixed_finetune_{}_best_img{}_fold{}_lr{}_epochpre{}_epochpost{}'.format(mode,sz,fold,lr,epochpre,epochpost))\n#     learn.fit_one_cycle(epochpre, lr, callbacks = [AccumulateStep(learn,n_acc)])\n    \n    #fit entire model with saving on the best epoch\n    learn.unfreeze()\n    \n    \n#     learn.fit_one_cycle(12, slice(lr/80, lr/2), callbacks = [AccumulateStep(learn,n_acc)])\n    learn.fit_one_cycle(epochpost, slice(lr/80, lr/4), callbacks = [\n        AccumulateStep(learn,n_acc), \n        SaveModelCallback(learn, \n                          monitor='dice', \n                          mode='max',\n                          name='resnet34_imsizefixed_finetune_0.25_{}_best_img{}_fold{}_lr{}_epochpre{}_epochpost{}'.format(mode,sz,fold,lr,epochpre,epochpost))])\n    learn.save('resnet34_imsizefixed_finetune_0.25_{}_img{}_fold{}_lr{}_epochpre{}_epochpost{}'.format(mode,sz,fold,lr,epochpre,epochpost));\n    if False:\n        #prediction on val and test sets\n        preds, ys = pred_with_flip(learn)\n        pt, _ = pred_with_flip(learn,DatasetType.Test)\n\n        if fold == 0: preds_test = pt\n        else: preds_test += pt\n\n        #remove noise\n        preds[preds.view(preds.shape[0],-1).sum(-1) < noise_th,...] = 0.0\n\n        #optimal threshold \n        #the best way would be collecting all oof predictions followed by single threshold calculation\n        #however, it requres too much RAM for high image resolution\n        dices = []\n        thrs = np.arange(0.01, 1, 0.01)\n        for th in progress_bar(thrs):\n            preds_m = (preds>th).long()\n            dices.append(dice_overall(preds_m, ys).mean())\n        dices = np.array(dices)    \n        scores.append(dices.max())\n        best_thrs.append(thrs[dices.argmax()])\n\n        if fold != nfolds-1: del preds, ys\n    gc.collect()\n    torch.cuda.empty_cache()\n    \n# preds_test /= nfolds","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# ph4\nscores, best_thrs = [],[]\n# first phase for bce then lovasz??\nfold = 0\nif True:\n# for fold in range(nfolds):\n    print('fold: ', fold)\n    data = get_data(fold)\n    learn = unet_learner(data, models.resnet34, metrics=[dice])\n    print(mode)\n    if mode is not None:\n        learn.loss_func = criterion_pixel\n    else:\n        mode = 'bce'\n    \n    epochpost=12\n    \n    learn.clip_grad(1.0);\n    \n    #fit the decoder part of the model keeping the encode frozen\n    lr = 7e-3\n    epochpre = 6\n    learn.load('resnet34_imsizefixed_finetune_0.25_{}_best_img{}_fold{}_lr{}_epochpre{}_epochpost{}'.format(mode,sz,fold,lr,epochpre,epochpost))\n#     learn.fit_one_cycle(epochpre, lr, callbacks = [AccumulateStep(learn,n_acc)])\n    \n    #fit entire model with saving on the best epoch\n    learn.unfreeze()\n#     learn.fit_one_cycle(12, slice(lr/80, lr/2), callbacks = [AccumulateStep(learn,n_acc)])\n    learn.fit_one_cycle(epochpost, slice(lr/80, lr/8), callbacks = [\n        AccumulateStep(learn,n_acc), \n        SaveModelCallback(learn, \n                          monitor='dice', \n                          mode='max',\n                          name='resnet34_imsizefixed_finetune_0.125_{}_best_img{}_fold{}_lr{}_epochpre{}_epochpost{}'.format(mode,sz,fold,lr,epochpre,epochpost))])\n    learn.save('resnet34_imsizefixed_finetune_0.125_{}_img{}_fold{}_lr{}_epochpre{}_epochpost{}'.format(mode,sz,fold,lr,epochpre,epochpost));\n    if False:\n        #prediction on val and test sets\n        preds, ys = pred_with_flip(learn)\n        pt, _ = pred_with_flip(learn,DatasetType.Test)\n\n        if fold == 0: preds_test = pt\n        else: preds_test += pt\n\n        #remove noise\n        preds[preds.view(preds.shape[0],-1).sum(-1) < noise_th,...] = 0.0\n\n        #optimal threshold \n        #the best way would be collecting all oof predictions followed by single threshold calculation\n        #however, it requres too much RAM for high image resolution\n        dices = []\n        thrs = np.arange(0.01, 1, 0.01)\n        for th in progress_bar(thrs):\n            preds_m = (preds>th).long()\n            dices.append(dice_overall(preds_m, ys).mean())\n        dices = np.array(dices)    \n        scores.append(dices.max())\n        best_thrs.append(thrs[dices.argmax()])\n\n        if fold != nfolds-1: del preds, ys\n    gc.collect()\n    torch.cuda.empty_cache()\n    \n# preds_test /= nfolds","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"learn.recorder.metrics","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# for submit pres_test probabilities\nscores, best_thrs = [],[]\n# if True:\nfor fold in range(nfolds):\n    print('fold: ', fold)\n    data = get_data(fold)\n    learn = unet_learner(data, models.resnet34, metrics=[dice])\n    print(mode)\n    if mode is not None:\n        learn.loss_func = criterion_pixel\n    else:\n        mode = 'bce'\n    \n    epochpost=12\n    \n    learn.clip_grad(1.0);\n    \n    #fit the decoder part of the model keeping the encode frozen\n    lr = 7e-3\n    epochpre = 6\n    learn.load('resnet34_imsizefixed_finetune_0.125_{}_best_img{}_fold{}_lr{}_epochpre{}_epochpost{}'.format(mode,sz,fold,lr,epochpre,epochpost))\n#     learn.fit_one_cycle(epochpre, lr, callbacks = [AccumulateStep(learn,n_acc)])\n    if False:\n        #fit entire model with saving on the best epoch\n        learn.unfreeze()\n    #     learn.fit_one_cycle(12, slice(lr/80, lr/2), callbacks = [AccumulateStep(learn,n_acc)])\n        learn.fit_one_cycle(epochpost, slice(lr/80, lr/2), callbacks = [\n            AccumulateStep(learn,n_acc), \n            SaveModelCallback(learn, \n                              monitor='dice', \n                              mode='max',\n                              name='resnet34_imsizefixed_{}_best_img{}_fold{}_lr{}_epochpre{}_epochpost{}'.format(mode,sz,fold,lr,epochpre,epochpost))])\n        learn.save('resnet34_imsizefixed_{}_img{}_fold{}_lr{}_epochpre{}_epochpost{}'.format(mode,sz,fold,lr,epochpre,epochpost));\n\n        #prediction on val and test sets\n        preds, ys = pred_with_flip(learn)\n    if True:\n        pt, _ = pred_with_flip(learn,DatasetType.Test)\n\n        if fold == 0: preds_test = pt\n        else: preds_test += pt\n    if False:\n        #remove noise\n        preds[preds.view(preds.shape[0],-1).sum(-1) < noise_th,...] = 0.0\n\n        #optimal threshold \n        #the best way would be collecting all oof predictions followed by single threshold calculation\n        #however, it requres too much RAM for high image resolution\n        dices = []\n        thrs = np.arange(0.01, 1, 0.01)\n        for th in progress_bar(thrs):\n            preds_m = (preds>th).long()\n            dices.append(dice_overall(preds_m, ys).mean())\n        dices = np.array(dices)    \n        scores.append(dices.max())\n        best_thrs.append(thrs[dices.argmax()])\n\n        if fold != nfolds-1: del preds, ys\n    gc.collect()\n    torch.cuda.empty_cache()\n    \npreds_test /= nfolds","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"if False:\n    torch.save(preds_test, 'preds_test_resnet34_imsizefixed_finetune_0.125_{}_best_img{}_all_lr{}_epochpre{}_epochpost{}'.format(mode,sz,lr,epochpre,epochpost))\n# preds_test = torch.load('preds_test_resnet34_imsizefixed_finetune_0.125_{}_best_img{}_all_lr{}_epochpre{}_epochpost{}'.format(mode,sz,lr,epochpre,epochpost))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# for submit and postprocessing\nimport cv2\nsz = 1024\nnoise_th = noise_th_multiplier*(sz/128.0)**2 #threshold for the number of predicted pixels\nbest_thr = 0.21\ncell_prob_thr = 0.35\n# Generate rle encodings (images are first converted to the original size)\npreds_t = (preds_test>best_thr).long().numpy()\nrles = []\nall_probs = []\nfor i,p in enumerate(preds_t):\n    if(p.sum() > 0):\n        pred_image = preds_test[i].numpy().T\n        im = PIL.Image.fromarray((p.T*255).astype(np.uint8)).resize((1024,1024))\n        im = np.asarray(im)\n        num_component, component = cv2.connectedComponents(im)\n        im_temp = np.zeros((1024,1024),np.float32)\n        num = 0\n        \n        cell_probs = []\n        for c in range(1, num_component):\n            p = (component==c )\n            each_probs = np.mean(pred_image[p])\n            cell_probs.append(each_probs)\n            \n            \n            print('cell probs {}'.format(each_probs))\n            if each_probs > cell_prob_thr :\n                im_temp[p] = 255\n                num +=1\n            else:\n                print('drop {}'.format(p.sum()))\n        all_probs.append(cell_probs)\n        rles.append(mask2rle(im_temp, 1024, 1024))\n        \n    else: rles.append('-1')\n    \nsub_df = pd.DataFrame({'ImageId': ids_test, 'EncodedPixels': rles})\nsub_df.to_csv('submission_probpost{}_singlemask_best0.8729_without_noiseremoval_correct.csv'.format(cell_prob_thr,best_thr), index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ======dgx01 bce+dice\n# fold0\n# [[tensor(0.7610)],\n#  [tensor(0.8041)],\n#  [tensor(0.8097)],\n#  [tensor(0.7132)],\n#  [tensor(0.7936)],\n#  [tensor(0.8077)],\n#  [tensor(0.8111)],\n#  [tensor(0.8253)],\n#  [tensor(0.8110)],\n#  [tensor(0.8325)],\n#  [tensor(0.8410)],\n#  [tensor(0.8405)]]\n\n# fold0 ph2\n# [[tensor(0.8200)],\n#  [tensor(0.7529)],\n#  [tensor(0.8317)],\n#  [tensor(0.8283)],\n#  [tensor(0.8111)],\n#  [tensor(0.8371)],\n#  [tensor(0.8340)],\n#  [tensor(0.8387)],\n#  [tensor(0.8224)],\n#  [tensor(0.8353)],\n#  [tensor(0.8444)],\n#  [tensor(0.8385)]]\n\n# fold0 ph3\n# [[tensor(0.8403)],\n#  [tensor(0.8375)],\n#  [tensor(0.8047)],\n#  [tensor(0.7944)],\n#  [tensor(0.8317)],\n#  [tensor(0.8130)],\n#  [tensor(0.8389)],\n#  [tensor(0.8421)],\n#  [tensor(0.8400)],\n#  [tensor(0.8402)],\n#  [tensor(0.8451)],\n#  [tensor(0.8448)]]\n\n# fold0 ph4\n# [[tensor(0.8448)],\n#  [tensor(0.8384)],\n#  [tensor(0.8219)],\n#  [tensor(0.8246)],\n#  [tensor(0.8281)],\n#  [tensor(0.8332)],\n#  [tensor(0.8408)],\n#  [tensor(0.8447)],\n#  [tensor(0.8437)],\n#  [tensor(0.8442)],\n#  [tensor(0.8433)],\n#  [tensor(0.8420)]]\n\n# fold0 ph5 0.8453\n# [[tensor(0.8453)],\n#  [tensor(0.8339)],\n#  [tensor(0.8289)],\n#  [tensor(0.8268)],\n#  [tensor(0.8417)],\n#  [tensor(0.8382)],\n#  [tensor(0.8432)],\n#  [tensor(0.8388)],\n#  [tensor(0.8426)],\n#  [tensor(0.8427)],\n#  [tensor(0.8444)],\n#  [tensor(0.8447)]]\n\n# fold0 ph6 0.8451\n# [[tensor(0.8398)],\n#  [tensor(0.8412)],\n#  [tensor(0.8351)],\n#  [tensor(0.8346)],\n#  [tensor(0.8438)],\n#  [tensor(0.8391)],\n#  [tensor(0.8430)],\n#  [tensor(0.8423)],\n#  [tensor(0.8451)],\n#  [tensor(0.8428)],\n#  [tensor(0.8448)],\n#  [tensor(0.8440)]]\n\n# fold1\n# [[tensor(0.7996)],\n#  [tensor(0.8182)],\n#  [tensor(0.7909)],\n#  [tensor(0.7418)],\n#  [tensor(0.8085)],\n#  [tensor(0.8155)],\n#  [tensor(0.8028)],\n#  [tensor(0.8173)],\n#  [tensor(0.8403)],\n#  [tensor(0.8244)],\n#  [tensor(0.8425)],\n#  [tensor(0.8436)]]\n\n# fold1 ph2\n# [[tensor(0.8474)],\n#  [tensor(0.8165)],\n#  [tensor(0.8410)],\n#  [tensor(0.8238)],\n#  [tensor(0.8356)],\n#  [tensor(0.8425)],\n#  [tensor(0.8482)],\n#  [tensor(0.8215)],\n#  [tensor(0.8382)],\n#  [tensor(0.8492)],\n#  [tensor(0.8492)],\n#  [tensor(0.8510)]]\n\n# fold1 ph3\n# [[tensor(0.8461)],\n#  [tensor(0.8470)],\n#  [tensor(0.8275)],\n#  [tensor(0.8319)],\n#  [tensor(0.8418)],\n#  [tensor(0.8248)],\n#  [tensor(0.8291)],\n#  [tensor(0.8524)],\n#  [tensor(0.8506)],\n#  [tensor(0.8439)],\n#  [tensor(0.8526)],\n#  [tensor(0.8527)]]\n\n# fold1 ph4\n# [[tensor(0.8517)],\n#  [tensor(0.8512)],\n#  [tensor(0.8448)],\n#  [tensor(0.8524)],\n#  [tensor(0.8482)],\n#  [tensor(0.8367)],\n#  [tensor(0.8540)],\n#  [tensor(0.8519)],\n#  [tensor(0.8509)],\n#  [tensor(0.8552)],\n#  [tensor(0.8544)],\n#  [tensor(0.8537)]]\n\n# fold1 ph5\n# [[tensor(0.8536)],\n#  [tensor(0.8385)],\n#  [tensor(0.8545)],\n#  [tensor(0.8497)],\n#  [tensor(0.8496)],\n#  [tensor(0.8483)],\n#  [tensor(0.8480)],\n#  [tensor(0.8496)],\n#  [tensor(0.8518)],\n#  [tensor(0.8554)],\n#  [tensor(0.8553)],\n#  [tensor(0.8537)]]\n\n# fold2\n# [[tensor(0.8269)],\n#  [tensor(0.7563)],\n#  [tensor(0.7875)],\n#  [tensor(0.8114)],\n#  [tensor(0.7880)],\n#  [tensor(0.8278)],\n#  [tensor(0.8184)],\n#  [tensor(0.8305)],\n#  [tensor(0.8379)],\n#  [tensor(0.8347)],\n#  [tensor(0.8409)],\n#  [tensor(0.8393)]]\n\n# fold2 ph2\n# [[tensor(0.8361)],\n#  [tensor(0.8081)],\n#  [tensor(0.8213)],\n#  [tensor(0.8044)],\n#  [tensor(0.8368)],\n#  [tensor(0.8188)],\n#  [tensor(0.8359)],\n#  [tensor(0.8319)],\n#  [tensor(0.8367)],\n#  [tensor(0.8410)],\n#  [tensor(0.8401)],\n#  [tensor(0.8415)]]\n\n# fold2 ph3\n# [[tensor(0.8411)],\n#  [tensor(0.8151)],\n#  [tensor(0.8408)],\n#  [tensor(0.8419)],\n#  [tensor(0.8409)],\n#  [tensor(0.8407)],\n#  [tensor(0.8375)],\n#  [tensor(0.8343)],\n#  [tensor(0.8399)],\n#  [tensor(0.8429)],\n#  [tensor(0.8410)],\n#  [tensor(0.8424)]]\n\n# fold2 ph4\n# [[tensor(0.8432)],\n#  [tensor(0.8428)],\n#  [tensor(0.8133)],\n#  [tensor(0.8381)],\n#  [tensor(0.8412)],\n#  [tensor(0.8365)],\n#  [tensor(0.8448)],\n#  [tensor(0.8374)],\n#  [tensor(0.8430)],\n#  [tensor(0.8452)],\n#  [tensor(0.8417)],\n#  [tensor(0.8451)]]\n\n# fold2 ph5\n# [[tensor(0.8452)],\n#  [tensor(0.8385)],\n#  [tensor(0.8380)],\n#  [tensor(0.8391)],\n#  [tensor(0.8430)],\n#  [tensor(0.8331)],\n#  [tensor(0.8442)],\n#  [tensor(0.8430)],\n#  [tensor(0.8444)],\n#  [tensor(0.8433)],\n#  [tensor(0.8449)],\n#  [tensor(0.8448)]]\n\n# fold3\n# [[tensor(0.8108)],\n#  [tensor(0.7294)],\n#  [tensor(0.8016)],\n#  [tensor(0.8030)],\n#  [tensor(0.8100)],\n#  [tensor(0.7928)],\n#  [tensor(0.8057)],\n#  [tensor(0.8155)],\n#  [tensor(0.8287)],\n#  [tensor(0.8241)],\n#  [tensor(0.8257)],\n#  [tensor(0.8202)]]\n\n# fold3 ph2\n# [[tensor(0.8275)],\n#  [tensor(0.8160)],\n#  [tensor(0.8226)],\n#  [tensor(0.8029)],\n#  [tensor(0.7808)],\n#  [tensor(0.8048)],\n#  [tensor(0.8250)],\n#  [tensor(0.8262)],\n#  [tensor(0.8279)],\n#  [tensor(0.8321)],\n#  [tensor(0.8344)],\n#  [tensor(0.8348)]]\n\n# fold3 ph3\n# [[tensor(0.8338)],\n#  [tensor(0.8211)],\n#  [tensor(0.8198)],\n#  [tensor(0.8046)],\n#  [tensor(0.8250)],\n#  [tensor(0.8252)],\n#  [tensor(0.8140)],\n#  [tensor(0.8227)],\n#  [tensor(0.8303)],\n#  [tensor(0.8326)],\n#  [tensor(0.8336)],\n#  [tensor(0.8318)]]\n\n# fold3 ph4\n# [[tensor(0.8298)],\n#  [tensor(0.8266)],\n#  [tensor(0.8264)],\n#  [tensor(0.7987)],\n#  [tensor(0.8187)],\n#  [tensor(0.8204)],\n#  [tensor(0.8256)],\n#  [tensor(0.8313)],\n#  [tensor(0.8289)],\n#  [tensor(0.8313)],\n#  [tensor(0.8330)],\n#  [tensor(0.8338)]]\n\n# fold3 ph5\n# [[tensor(0.8344)],\n#  [tensor(0.8274)],\n#  [tensor(0.8188)],\n#  [tensor(0.8253)],\n#  [tensor(0.8288)],\n#  [tensor(0.8286)],\n#  [tensor(0.8355)],\n#  [tensor(0.8326)],\n#  [tensor(0.8348)],\n#  [tensor(0.8340)],\n#  [tensor(0.8353)],\n#  [tensor(0.8333)]]\n\n# fold3 ph6\n# [[tensor(0.8337)],\n#  [tensor(0.8282)],\n#  [tensor(0.8276)],\n#  [tensor(0.8180)],\n#  [tensor(0.8307)],\n#  [tensor(0.8309)],\n#  [tensor(0.8319)],\n#  [tensor(0.8292)],\n#  [tensor(0.8302)],\n#  [tensor(0.8328)],\n#  [tensor(0.8314)],\n#  [tensor(0.8315)]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ======bce+dice resnet50 3e-4\n# fold 0 0.8277\n# [[tensor(0.7856)],\n#  [tensor(0.7795)],\n#  [tensor(0.7898)],\n#  [tensor(0.7569)],\n#  [tensor(0.8013)],\n#  [tensor(0.8114)],\n#  [tensor(0.8205)],\n#  [tensor(0.8277)],\n#  [tensor(0.8241)],\n#  [tensor(0.8254)],\n#  [tensor(0.8259)],\n#  [tensor(0.8224)]]\n\n# fold 0 ph2 0.8300\n# epoch\ttrain_loss\tvalid_loss\tdice\ttime\n# 0\t0.004460\t0.005886\t0.821651\t51:52\n# 1\t0.005969\t0.005893\t0.801328\t51:46\n# 2\t0.007122\t0.006117\t0.804395\t51:43\n# 3\t0.006183\t0.006231\t0.812023\t51:40\n# 4\t0.005936\t0.006027\t0.811091\t51:38\n# 5\t0.003460\t0.006016\t0.782531\t51:38\n# 6\t0.004310\t0.005997\t0.804455\t51:17\n# 7\t0.003708\t0.006566\t0.819687\t51:17\n# 8\t0.003695\t0.006318\t0.824498\t51:17\n# 9\t0.003874\t0.006612\t0.821759\t51:19\n# 10\t0.002778\t0.006484\t0.822675\t51:18\n# 11\t0.002100\t0.006779\t0.830086\t51:18\n# Better model found at epoch 0 with dice value: 0.8216508626937866.\n# Better model found at epoch 8 with dice value: 0.8244979977607727.\n# Better model found at epoch 11 with dice value: 0.8300861120223999\n\n# fold1 0.8309\n# [[tensor(0.7777)],\n#  [tensor(0.7778)],\n#  [tensor(0.7457)],\n#  [tensor(0.8054)],\n#  [tensor(0.8236)],\n#  [tensor(0.7926)],\n#  [tensor(0.8081)],\n#  [tensor(0.8243)],\n#  [tensor(0.8281)],\n#  [tensor(0.8304)],\n#  [tensor(0.8269)],\n#  [tensor(0.8309)]]\n\n# fold1 ph2 0.8346\n# [[tensor(0.8257)],\n#  [tensor(0.8350)],\n#  [tensor(0.8087)],\n#  [tensor(0.8362)],\n#  [tensor(0.8161)],\n#  [tensor(0.8340)],\n#  [tensor(0.8198)],\n#  [tensor(0.8336)],\n#  [tensor(0.8326)],\n#  [tensor(0.8346)],\n#  [tensor(0.8343)],\n#  [tensor(0.8325)]]\n\n# fold2 0.8271\n# [[tensor(0.7998)],\n#  [tensor(0.7681)],\n#  [tensor(0.8180)],\n#  [tensor(0.8195)],\n#  [tensor(0.7847)],\n#  [tensor(0.8117)],\n#  [tensor(0.7417)],\n#  [tensor(0.8149)],\n#  [tensor(0.8271)],\n#  [tensor(0.8257)],\n#  [tensor(0.8244)],\n#  [tensor(0.8193)]]\n\n# fold3 0.8178\n# [[tensor(0.7780)],\n#  [tensor(0.7779)],\n#  [tensor(0.7817)],\n#  [tensor(0.7963)],\n#  [tensor(0.7897)],\n#  [tensor(0.7657)],\n#  [tensor(0.8178)],\n#  [tensor(0.8102)],\n#  [tensor(0.8022)],\n#  [tensor(0.8110)],\n#  [tensor(0.8114)],\n#  [tensor(0.8144)]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fold 0\nepoch\ttrain_loss\tvalid_loss\tdice\ttime\n0\t0.005329\t0.008973\t0.820277\t1:30:01\n1\t0.004792\t0.009235\t0.824711\t1:29:40\n2\t0.004611\t0.010663\t0.825052\t1:29:36\n3\t0.004171\t0.011810\t0.812811\t1:29:34\n4\t0.003634\t0.009718\t0.824433\t1:29:29\n5\t0.005561\t0.010448\t0.813646\t1:29:27\n6\t0.004434\t0.008877\t0.822849\t1:29:31\n7\t0.004877\t0.009485\t0.822946\t1:29:29\n8\t0.002459\t0.010549\t0.820190\t1:29:31\n9\t0.003624\t0.009636\t0.822931\t1:29:29\n10\t0.003978\t0.011270\t0.822492\t1:29:30\n11\t0.002737\t0.011042\t0.822096\t1:29:34\nBetter model found at epoch 0 with dice value: 0.8202774524688721.\nBetter model found at epoch 1 with dice value: 0.8247113227844238.\nBetter model found at epoch 2 with dice value: 0.8250521421432495.\n\n\nfold:  1\n\nepoch\ttrain_loss\tvalid_loss\tdice\ttime\n0\t0.007127\t0.008310\t0.836103\t1:30:32\n1\t0.004536\t0.009290\t0.831446\t1:29:56\n2\t0.004288\t0.009331\t0.831958\t1:29:53\n3\t0.004563\t0.009137\t0.833080\t1:29:45\n4\t0.004158\t0.010182\t0.832579\t1:29:41\n5\t0.003921\t0.009543\t0.833295\t1:29:43\n6\t0.004194\t0.009401\t0.835947\t1:29:44\n7\t0.004584\t0.009832\t0.837901\t1:29:45\n8\t0.004033\t0.010654\t0.831863\t1:29:45\n9\t0.004907\t0.010494\t0.835302\t1:29:44\n10\t0.002935\t0.010810\t0.831416\t1:29:44\n11\t0.002860\t0.010834\t0.831417\t1:29:42\nBetter model found at epoch 0 with dice value: 0.8361029624938965.\nBetter model found at epoch 7 with dice value: 0.8379009366035461.\n\nfold:  2\nepoch\ttrain_loss\tvalid_loss\tdice\ttime\n0\t0.014262\t0.014819\t0.687674\t1:22:04\n1\t0.011007\t0.009866\t0.806199\t1:21:13\n2\t0.012556\t0.010506\t0.805177\t1:20:59\n3\t0.010263\t0.008242\t0.813777\t1:20:57\n4\t0.006284\t0.007857\t0.818966\t1:21:05\n5\t0.009344\t0.007730\t0.818302\t1:21:46\nepoch\ttrain_loss\tvalid_loss\tdice\ttime\n0\t0.009150\t0.008387\t0.822880\t1:29:24\n1\t0.007065\t0.007723\t0.819144\t1:29:20\n2\t0.004986\t0.009361\t0.814605\t1:29:31\n3\t0.007759\t0.008976\t0.820553\t1:29:37\n4\t0.008809\t0.008335\t0.822547\t1:28:23\n5\t0.006168\t0.008478\t0.823449\t1:28:11\n6\t0.005340\t0.008670\t0.822819\t1:28:15\n7\t0.004951\t0.007760\t0.822989\t1:28:07\n8\t0.004814\t0.007861\t0.827206\t1:28:12\n9\t0.009067\t0.008105\t0.827544\t1:28:07\n10\t0.005683\t0.008157\t0.827294\t1:28:08\n11\t0.005105\t0.008057\t0.823705\t1:27:51\nBetter model found at epoch 0 with dice value: 0.8228797912597656.\nBetter model found at epoch 5 with dice value: 0.8234494924545288.\nBetter model found at epoch 8 with dice value: 0.8272055983543396.\nBetter model found at epoch 9 with dice value: 0.8275436758995056.\nfold:  3\nepoch\ttrain_loss\tvalid_loss\tdice\ttime\n0\t0.026892\t0.014963\t0.770922\t1:20:51\n1\t0.013744\t0.011118\t0.786418\t1:20:20\n2\t0.007804\t0.011807\t0.792785\t1:20:09\n3\t0.006909\t0.009720\t0.807148\t1:20:02\n4\t0.011643\t0.009819\t0.808711\t1:20:11\n5\t0.005560\t0.009124\t0.808388\t1:21:24\nepoch\ttrain_loss\tvalid_loss\tdice\ttime\n0\t0.006412\t0.009343\t0.812534\t1:29:12\n1\t0.005806\t0.009915\t0.804407\t1:29:03\n2\t0.007079\t0.011265\t0.800056\t1:29:24\n3\t0.008566\t0.009434\t0.805174\t1:29:20\n4\t0.006620\t0.010455\t0.807575\t1:28:20\n5\t0.006316\t0.010035\t0.805124\t1:28:00\n6\t0.005538\t0.009238\t0.814438\t1:28:01\n7\t0.007481\t0.009090\t0.810495\t1:27:56\n8\t0.004882\t0.009584\t0.815818\t1:27:58\n9\t0.003599\t0.009886\t0.813863\t1:27:56\n10\t0.004064\t0.009721\t0.815348\t1:27:52\n11\t0.004554\t0.009581\t0.818450\t1:27:44\nBetter model found at epoch 0 with dice value: 0.8125340342521667.\nBetter model found at epoch 6 with dice value: 0.8144382238388062.\nBetter model found at epoch 8 with dice value: 0.8158184885978699.\nBetter model found at epoch 11 with dice value: 0.8184497952461243.\n-------------------512 finetune 0.25 RESNEXT101-32X8D\nfold:  0\nepoch\ttrain_loss\tvalid_loss\tdice\ttime\n0\t0.005299\t0.008649\t0.825610\t1:30:29\n1\t0.004567\t0.008544\t0.826394\t1:29:59\n2\t0.003797\t0.009225\t0.822685\t1:29:55\n3\t0.003726\t0.009869\t0.821343\t1:29:49\n4\t0.003118\t0.008614\t0.828068\t1:29:44\n5\t0.004983\t0.009363\t0.822326\t1:29:49\n6\t0.003822\t0.009243\t0.824851\t1:29:50\n7\t0.004120\t0.009700\t0.821603\t1:29:48\n8\t0.002501\t0.010308\t0.820887\t1:29:50\n9\t0.003941\t0.010059\t0.822769\t1:29:47\n10\t0.004392\t0.010589\t0.821599\t1:29:49\n11\t0.003021\t0.010419\t0.822123\t1:29:49\nBetter model found at epoch 0 with dice value: 0.8256103992462158.\nBetter model found at epoch 1 with dice value: 0.8263944387435913.\nBetter model found at epoch 4 with dice value: 0.8280683159828186.\n\n\n\nfold:  2\nepoch\ttrain_loss\tvalid_loss\tdice\ttime\n0\t0.005731\t0.008329\t0.828296\t1:30:30\n1\t0.004366\t0.008295\t0.825781\t1:29:55\n2\t0.005894\t0.009523\t0.823959\t1:29:50\n3\t0.004642\t0.010283\t0.820261\t1:29:43\n4\t0.003910\t0.009004\t0.824968\t1:29:39\n5\t0.005872\t0.008353\t0.831194\t1:29:42\n6\t0.004779\t0.009164\t0.827115\t1:29:48\n7\t0.003729\t0.009589\t0.827741\t1:29:42\n8\t0.002382\t0.010495\t0.825872\t1:29:42\n9\t0.003647\t0.009949\t0.827074\t1:29:42\n10\t0.003826\t0.010135\t0.827529\t1:29:39\n11\t0.003138\t0.010171\t0.827706\t1:29:43\nBetter model found at epoch 0 with dice value: 0.8282960653305054.\nBetter model found at epoch 5 with dice value: 0.8311936259269714.\nfold3\nepoch\ttrain_loss\tvalid_loss\tdice\ttime\n0\t0.005510\t0.009959\t0.814834\t1:30:27\n1\t0.004696\t0.009779\t0.815714\t1:29:54\n2\t0.004126\t0.009552\t0.818221\t1:29:51\n3\t0.004121\t0.010156\t0.817459\t1:29:44\n4\t0.004757\t0.010107\t0.818174\t1:29:38\n5\t0.003463\t0.009922\t0.815573\t1:29:41\n6\t0.004251\t0.010353\t0.814511\t1:29:43\n7\t0.003629\t0.010751\t0.815229\t1:29:42\n8\t0.003579\t0.011269\t0.812502\t1:29:44\n9\t0.004614\t0.011579\t0.811707\t1:29:42\n10\t0.003638\t0.010986\t0.816639\t1:29:42\n11\t0.004797\t0.011186\t0.818167\t1:29:40\nBetter model found at epoch 0 with dice value: 0.8148338198661804.\nBetter model found at epoch 1 with dice value: 0.8157141804695129.\nBetter model found at epoch 2 with dice value: 0.8182209134101868.\n\n\n\n\nfold:  3\nepoch\ttrain_loss\tvalid_loss\tdice\ttime\n0\t0.005536\t0.010181\t0.815544\t1:29:43\n1\t0.005007\t0.010228\t0.813226\t1:29:43\n2\t0.004829\t0.009476\t0.805355\t1:29:38\n3\t0.005007\t0.010039\t0.811850\t1:29:34\n4\t0.005185\t0.011137\t0.808189\t1:29:30\n5\t0.003862\t0.010142\t0.815380\t1:29:32\n6\t0.004465\t0.009667\t0.816439\t1:29:30\n7\t0.004076\t0.012367\t0.808578\t1:29:27\n8\t0.003637\t0.010428\t0.818723\t1:29:32\n9\t0.004116\t0.011323\t0.813577\t1:29:30\n10\t0.003371\t0.011082\t0.817705\t1:29:28\n11\t0.004306\t0.011553\t0.816912\t1:29:37\nBetter model found at epoch 0 with dice value: 0.8155437707901001.\nBetter model found at epoch 6 with dice value: 0.8164392709732056.\nBetter model found at epoch 8 with dice value: 0.8187227249145508.","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"fastai","language":"python","name":"fastai"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"}},"nbformat":4,"nbformat_minor":1}