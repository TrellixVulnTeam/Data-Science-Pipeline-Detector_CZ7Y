{"cells":[{"metadata":{"_uuid":"6e1ab15f588fc6c54baacf68d87eb97b054a197b"},"cell_type":"markdown","source":"In this notebook, I \n- visualize the dataset. \n- some basic image enhancement techniques using skiimage.\n- Many images have multiple annotations. I show how to select according to value counts and visualize it \n\n\nYou may find the following links useful\n- [EDA](https://www.kaggle.com/peterchang77/exploratory-data-analysis)\n\nI will keep updating the kernel as i explore further."},{"metadata":{"_uuid":"c2a0e0a494e7da38a56a05d1849d7a0a07f3ccde"},"cell_type":"markdown","source":"# Loading required libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import os\nimport sys\nimport random\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom skimage.transform import resize\nfrom skimage.morphology import label\nfrom skimage.feature import hog\nfrom skimage import exposure\nfrom keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\nfrom skimage.feature import canny\nfrom skimage.filters import sobel\nfrom skimage.morphology import watershed\nfrom scipy import ndimage as ndi\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom skimage.segmentation import mark_boundaries\nfrom scipy import signal\nimport cv2\nimport glob, pylab, pandas as pd\nimport pydicom, numpy as np\nimport tqdm\nimport gc\ngc.enable()\nimport glob\n\nfrom skimage.transform import resize\nfrom skimage.morphology import label\nfrom skimage.feature import hog\nfrom skimage import exposure\nfrom keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\nfrom skimage.feature import canny\nfrom skimage.filters import sobel\nfrom skimage.morphology import watershed\nfrom scipy import ndimage as ndi\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom skimage.segmentation import mark_boundaries\n\nimport sys\nsys.path.insert(0, '../input/siim-acr-pneumothorax-segmentation/')\nfrom mask_functions import rle2mask","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.listdir('../input')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_files = glob.glob('../input/siim-acr-pneumothorax-segmentation/sample images/*.dcm')\nlen(sample_files)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6b89fc0c491ff50b02a282d174a29d2f7e295bba"},"cell_type":"markdown","source":"# Some utility functions"},{"metadata":{"trusted":true,"_uuid":"3ef5bea28573c765d8a276d5f1d45c87c8d0611f"},"cell_type":"code","source":"df = pd.read_csv('../input/siim-acr-pneumothorax-segmentation/sample images/train-rle-sample.csv',header=None)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"96a60d1855f762f06e6d1cb0fea3f2bc669cd04b"},"cell_type":"code","source":"import cv2\nfrom IPython.display import display, Image\ndef cvshow(image, format='.png', rate=255 ):\n    decoded_bytes = cv2.imencode(format, image*rate)[1].tobytes()\n    display(Image(data=decoded_bytes))\n    return","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0fd256eaf1b2c71d96129d40f4a3ad7846462ba6"},"cell_type":"markdown","source":"# Visualizing Sample Images"},{"metadata":{"trusted":true,"_uuid":"dc5383cdd25f85df09013e16b53187d0a213ed33"},"cell_type":"code","source":"j = 0\nnImg = 10\nimg_ar = np.empty(0)\nwhile img_ar.shape[0]!=nImg:\n#     dcm_file = '../input/../input/sample images/%s.dcm' % patientId\n    dcm_file = sample_files[j]\n    dcm_data = pydicom.read_file(dcm_file)\n    img = np.expand_dims(dcm_data.pixel_array,axis=0)    \n    if j==0:\n        img_ar = img\n    elif (j%100==0):\n        print(j,'images loaded')\n    else:\n        img_ar = np.concatenate([img_ar,img],axis=0)\n    j += 1\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3c780929c955aa651635a5cac816f299a30ad11e"},"cell_type":"code","source":"def imgtile(imgs,tile_w):\n    assert imgs.shape[0]%tile_w==0,\"'imgs' cannot divide by 'th'.\"\n    r=imgs.reshape((-1,tile_w)+imgs.shape[1:])\n    return np.hstack(np.hstack(r))\n\n#usage\ntiled = imgtile(img_ar,5)\n# cvshow(tiled)\ntiled.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3ed919d542159a5100da180ea9754ca0d565e2c0"},"cell_type":"code","source":"cvshow(cv2.resize( tiled, (1024,512), interpolation=cv2.INTER_LINEAR ))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These are some of the traditional ways to enhance an image. I used these techniques a lot but now deep learning has made them obsolete. You can find some visualization below.\n\n## Rescaling Intensity\nUsing this function we can stretch/shrink its intensity levels.\n\n## Contrast Limited Adaptive Histogram Equalization (CLAHE).\n\nAn algorithm for local contrast enhancement, that uses histograms computed\nover different tile regions of the image. Local details can therefore be\nenhanced even in regions that are darker or lighter than most of the image.\n\n## Logarithmic Correction\n\nThis function transforms the input image pixelwise according to the\nequation $O = gain*log(1 + I)$ after scaling each pixel to the range 0 to 1.\n\n## Gamma Correction\n\nAlso known as Power Law Transform.\nThis function transforms the input image pixelwise according to the\nequation $O = I*gamma$ after scaling each pixel to the range 0 to 1.\n\n\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# simple ways to enhance the image\n\nplt.figure(figsize=(30,15))\nplt.subplots_adjust(bottom=0.2, top=0.7, hspace=0)  #adjust this to change vertical and horiz. spacings..\nnImg = 3  #no. of images to process\nfor j in range(nImg):\n    q = j+1\n    img = np.array(pydicom.read_file(sample_files[j]).pixel_array)\n    \n#     # Contrast stretching\n    p2, p97 = np.percentile(img, (2, 97))\n    img_rescale = exposure.rescale_intensity(img, in_range=(p2, p97))\n    \n    # Equalization\n    img_eq = exposure.equalize_hist(img)\n\n    # Adaptive Equalization\n    img_adapteq = exposure.equalize_adapthist(img)\n    img_adjustlog = exposure.adjust_log(img)\n    img_adjustgamma = exposure.adjust_gamma(img)\n    \n    plt.subplot(nImg,7,q*7-6)\n    plt.imshow(img, cmap=plt.cm.bone)\n    plt.title('Original Image')\n    \n    \n    plt.subplot(nImg,7,q*7-5)    \n    plt.imshow(img_rescale, cmap=plt.cm.bone)\n    plt.title('Contrast stretching')\n    \n    \n    plt.subplot(nImg,7,q*7-4)\n    plt.imshow(img_eq, cmap=plt.cm.bone)\n    plt.title('Equalization')\n    \n    \n    plt.subplot(nImg,7,q*7-3)\n    plt.imshow(img_adapteq, cmap=plt.cm.bone)\n    plt.title('Adaptive Equalization')\n    \n    \n    plt.subplot(nImg,7,q*7-2)\n    plt.imshow(img_adjustgamma, cmap=plt.cm.bone)\n    plt.title('Adjust Gamma')\n    \n    plt.subplot(nImg,7,q*7-2)\n    plt.imshow(img_adjustlog, cmap=plt.cm.bone)\n    plt.title('Adjust Log')\n\n\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To download images you need to create an account on Google Cloud Products (GCP) and go through a series of messy process.. Providing the data on kaggle would have been so much easier!! "},{"metadata":{"trusted":true},"cell_type":"code","source":"train_glob = '../input/siim-acr-pneumothorax-segmentation-data/pneumothorax/dicom-images-train/*/*/*.dcm'\ntest_glob = '../input/siim-acr-pneumothorax-segmentation-data/pneumothorax/dicom-images-test/*/*/*.dcm'\ntrain_fns = sorted(glob.glob(train_glob))\ntest_fns = sorted(glob.glob(test_glob))\ndf_mask = pd.read_csv('../input/siim-acr-pneumothorax-segmentation-data/pneumothorax/train-rle.csv', index_col='ImageId')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(df_mask),len(set(df_mask.index.values))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_mask_dist = df_mask.copy(deep=True)\ndf_mask_dist.reset_index(inplace=True)\n\ndf = pd.DataFrame(df_mask_dist.ImageId.value_counts()).reset_index()\ndf.columns = ['ImageId','counts']\n\ndf_mask_dist = pd.merge(df_mask_dist,df,on='ImageId')\ndf_mask_dist.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_mask_dist.counts.plot.hist()\nplt.xlabel('Number of Annotations')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_image_data(im_path,im_type,df_mask=df_mask):\n    if im_type=='train':\n        return pydicom.dcmread(im_path).pixel_array","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_fn_id = [val.split('/')[-1][:-4] for val in train_fns]\nprint(len(set(train_fn_id).intersection(set(df_mask_dist.ImageId))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"masks_count_gt_one = set(df_mask_dist[df_mask_dist.counts>4].ImageId.values)\nprint('Number of masks with annotations greater than 5:',(df_mask_dist.counts>5).sum())\ncommon_masks =  list(set(train_fn_id).intersection(masks_count_gt_one))\nprint(len(common_masks))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nImg = 12  #no. of images that you want to display\nnp.random.seed(42)\nsel_train_fns = [train_fns[train_fn_id.index(c_m)] for c_m in common_masks]\n\n_train_ids = list(sel_train_fns)\nnp.random.shuffle(_train_ids)\n_train_ids = _train_ids[:nImg]\ntile_size = (256, 256)\nn = 4\nalpha = 0.25\n\nm = int(np.ceil(len(_train_ids) * 1.0 / n))\ncomplete_image = np.zeros((m*(tile_size[0]+2), n*(tile_size[1]+2), 1), dtype=np.uint8)\ncomplete_image_masked = np.zeros((m*(tile_size[0]+2), n*(tile_size[1]+2), 1), dtype=np.uint8)\n\ncounter = 0\nfor i in range(m):\n    ys = i*(tile_size[1] + 2)\n    ye = ys + tile_size[1]\n    for j in range(n):\n        xs = j*(tile_size[0] + 2)\n        xe = xs + tile_size[0]\n        if counter == len(_train_ids):\n            break\n        image_path = _train_ids[counter]; counter+=1\n        img = get_image_data(image_path, 'train')\n        \n        num_annot = len(df_mask.loc[image_path.split('/')[-1][:-4]].values)\n#         print('num_annot',num_annot)\n        for i in range(num_annot):\n            mask =  rle2mask(df_mask.loc[image_path.split('/')[-1][:-4]].iloc[i][0], 1024, 1024).T\n            \n        mask = mask.astype(np.uint8)\n        \n        img_masked =  cv2.addWeighted(img, alpha, mask, 1 - alpha,0)\n#         img_masked = cv2.bitwise_and(img, img, mask=mask)\n\n        img = cv2.resize(img, dsize=tile_size)\n        img_masked = cv2.resize(img_masked, dsize=tile_size)\n        \n        img_masked = cv2.putText(img_masked, image_path.split('/')[-1][:-4], (5,img.shape[0] - 5), cv2.FONT_HERSHEY_PLAIN, 1.5, (0, 255, 0), thickness=2)\n        complete_image_masked[ys:ye, xs:xe, :] = img_masked[:,:,None]\n        \n    if counter == len(_train_ids):\n        break    \n    \nm = complete_image.shape[0] / (tile_size[0] + 2)\nk = 8\nn = int(np.ceil(m / k))\nfor i in range(n):\n    plt.figure(figsize=(20, 20))\n    ys = i*(tile_size[0] + 2)*k\n    ye = min((i+1)*(tile_size[0] + 2)*k, complete_image.shape[0])\n    plt.imshow(complete_image_masked[ys:ye,:,0],cmap='bone')\n    plt.title(\"Training dataset: Lighter Color depicts mask\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}