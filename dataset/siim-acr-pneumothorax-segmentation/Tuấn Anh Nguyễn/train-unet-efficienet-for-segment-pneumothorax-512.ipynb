{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Phân đoạn Pneumothorax (tràn khí màng phổi)\n\n## Tổng quan bài toán\n\nTrong dự án này, sẽ phải phân đoạn được đâu là phần tràn khí màng phổi từ hình ảnh X-quang đã được gán nhãn trước.\n\nDữ liệu bao gồm có 12955 hình X-quang từ 12047 bệnh nhân và nhãn tương ứng (có những hình ảnh có nhãn, có những hình ảnh không có nhãn tức là không bị chứng tràn khí màng phổi)."},{"metadata":{},"cell_type":"markdown","source":"## Mục tiêu\n\n* Đào tạo mô hình segment dựa trên Unet với các backbone:\n    * Resnet50\n    * Efficientnet\n    * Xceptionnet\n* Thử nghiệm các phương pháp augumentation trên hình ảnh X-Quang\n* Đánh giá mô hình:\n    * Trên metrics dice score\n    * Đánh giá cho từng mô hình với các backbone khác nhau"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# import libraries\nimport numpy as np\nimport pandas as pd\nfrom glob import glob\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib.image as image\nfrom tqdm.notebook import tqdm\nimport glob\nimport pydicom\nimport sys\nimport os\n\nprint(os.listdir(\"../input/siim-acr-pneumothorax-segmentation\"))\nprint()\nsys.path.insert(0, '../input/siim-acr-pneumothorax-segmentation')\n\nfrom mask_functions import rle2mask\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# install libraries for augumentation image and\n# visualize architecture model Unet\n!pip install albumentations\n!pip install torchsummary","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torchvision\nimport torch.nn as nn \nfrom torch.utils.data import DataLoader, Dataset\n\nfrom torchsummary import summary\nimport torchvision.models as models\nimport torchvision.transforms as T\nimport albumentations as A\n\nfrom torch.autograd import Variable","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load information for dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load information for dataset\ntrain_df = pd.read_csv(\"../input/segment-data/train_info_split.csv\")\nval_df = pd.read_csv(\"../input/segment-data/val_info_split.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load all path for file .dcm\nfile_path = \"../input/siim-acr-pneumothorax-segmentation-data/dicom-images-train/*/*/*.dcm\"\nfile_paths = glob.glob(file_path)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Visualize image origin and mask with file .dcm"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualize image mask for file .dcm\nimage_id_arr = train_df[\"ImageId\"].unique()\n\nfor index, image_id in enumerate(image_id_arr):\n    index_ = list(filter(lambda x: image_id in file_paths[x], range(len(file_paths))))\n    dataset = pydicom.dcmread(file_paths[index_[0]])\n    image_data = dataset.pixel_array\n    \n    record_arr = train_df[train_df[\"ImageId\"]==image_id]\n    # Visualize patient has multi segment\n    if len(record_arr) >= 2:\n        fig, (ax1, ax2) = plt.subplots(1, 2)\n        fig.set_figheight(15)\n        fig.set_figwidth(15)\n        ax1.imshow(image_data, cmap=plt.cm.bone)\n        ax2.imshow(image_data, cmap=plt.cm.bone)\n        mask = np.zeros((1024, 1024))\n        for _, row in record_arr.iterrows():\n            if row[\"EncodedPixels\"] != ' -1':\n                mask_ = rle2mask(row[\"EncodedPixels\"], 1024, 1024).T\n                mask[mask_==255] = 255\n        \n        ax2.imshow(mask, alpha=0.3, cmap=\"Blues\")    \n        break","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Khởi tạo kiến trúc mô hình"},{"metadata":{},"cell_type":"markdown","source":"### Các hàm trợ giúp"},{"metadata":{"trusted":true},"cell_type":"code","source":"def toTensor(np_array, axis=(2,0,1)):\n    return torch.tensor(np_array).permute(axis)\n\ndef toNumpy(tensor, axis=(1,2,0)):\n    return tensor.detach().cpu().permute(axis).numpy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Tạo Data Loader"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualize image mask for file .dcm\nimage_id_arr = train_df[\"ImageId\"].unique()\n\nfor index, image_id in enumerate(image_id_arr):\n    index_ = list(filter(lambda x: image_id in file_paths[x], range(len(file_paths))))\n    dataset = pydicom.dcmread(file_paths[index_[0]])\n    image_data = dataset.pixel_array\n    \n    record_arr = train_df[train_df[\"ImageId\"]==image_id]\n    # Visualize patient has multi segment\n    if len(record_arr) >= 2:\n        fig, (ax1, ax2) = plt.subplots(1, 2)\n        fig.set_figheight(15)\n        fig.set_figwidth(15)\n        ax1.imshow(image_data, cmap=plt.cm.bone)\n        ax2.imshow(image_data, cmap=plt.cm.bone)\n        mask = np.zeros((1024, 1024))\n        for _, row in record_arr.iterrows():\n            if row[\"EncodedPixels\"] != ' -1':\n                mask_ = rle2mask(row[\"EncodedPixels\"], 1024, 1024).T\n                mask[mask_==255] = 255\n        \n        ax2.imshow(mask, alpha=0.3, cmap=\"Blues\")    \n        break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_infor(df):\n    infor = []\n    image_id_arr = df[\"ImageId\"].unique()\n    for index, image_id in tqdm(enumerate(image_id_arr)):\n        index_ = list(filter(lambda x: image_id in file_paths[x], range(len(file_paths))))\n        full_image_path = file_paths[index_[0]]\n\n        # Get all segment encode\n        record_arr = train_df[train_df[\"ImageId\"]==image_id]\n        encode_pixels = []\n        for _, row in record_arr.iterrows():\n            encode_pixels.append(row[\"EncodedPixels\"])\n\n        infor.append({\n            \"key\": image_id,\n            \"file_path\": full_image_path,\n            \"mask\": encode_pixels\n        })\n    return infor\n\nprint(\"Loading information for training set \\n\")\ntrain_infor = get_infor(train_df)\nprint(\"Loading information for validation set \\n\")\nval_infor = get_infor(val_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import cv2\n\nclass MaskDataset(Dataset):\n    def __init__(self, df, img_info, transforms=None):\n        self.df = df\n        self.img_info = img_info\n        self.transforms = transforms\n        \n    def __getitem__(self, idx):\n        img_path = self.img_info[idx][\"file_path\"]\n        key = self.img_info[idx][\"key\"]\n        \n        # load image data\n        dataset = pydicom.dcmread(img_path)\n        img = dataset.pixel_array\n        img = cv2.resize(img, dsize=(512, 512), interpolation=cv2.INTER_CUBIC)\n        \n        mask_arr = self.img_info[idx][\"mask\"]\n        \n        mask = np.zeros((512, 512))\n        \n        for item in mask_arr:\n            if item != \" -1\":\n                mask_ = rle2mask(item, 1024, 1024).T\n                mask_ = cv2.resize(mask_, dsize=(512, 512), interpolation=cv2.INTER_CUBIC)\n                mask[mask_==255] = 255\n        \n        if self.transforms:\n            sample = {\n                \"image\": img,\n                \"mask\": mask\n            }\n            sample = self.transforms(**sample)\n            img = sample[\"image\"]\n            mask = sample[\"mask\"]\n\n        # to Tensor\n        mask = np.expand_dims(mask, axis=-1)/255.0\n        mask = toTensor(mask).float()\n        \n        img = np.expand_dims(img, axis=-1)/255.0\n        img = toTensor(img).float()\n        \n        return img, mask\n            \n    def __len__(self):\n        return len(self.img_info)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_transform = A.Compose([\n    A.HorizontalFlip(),\n    A.OneOf([\n        A.RandomContrast(),\n        A.RandomGamma(),\n        A.RandomBrightness(),\n        ], p=0.3),\n    A.OneOf([\n        A.ElasticTransform(alpha=120, sigma=120 * 0.05, alpha_affine=120 * 0.03),\n        A.GridDistortion(),\n        A.OpticalDistortion(distort_limit=2, shift_limit=0.5),\n        ], p=0.3),\n    A.ShiftScaleRotate(),\n])\n\ntrain_dataset = MaskDataset(train_df, train_infor, train_transform)\ntrain_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, drop_last=True)\n\nval_dataset = MaskDataset(val_df, val_infor)\nval_loader = DataLoader(val_dataset, batch_size=1, shuffle=True, drop_last=True)\n\nnumber_visualize = 1\nfor img, mask in train_dataset:\n    if number_visualize > 5:\n        break\n    img = toNumpy(img)[:,:,0]\n    mask = toNumpy(mask)[:,:,0]\n\n    fig, (ax1, ax2) = plt.subplots(1, 2)\n    fig.set_figheight(15)\n    fig.set_figwidth(15)\n    ax1.imshow(img, cmap=plt.cm.bone)\n    ax2.imshow(img, cmap=plt.cm.bone)\n    ax2.imshow(mask, alpha=0.3, cmap=\"Blues\")\n    number_visualize += 1\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Định nghĩa hàm đo dice score và tính toán dice loss"},{"metadata":{},"cell_type":"markdown","source":"Đọc trong: [jeremyjordan semantic-segmentation](https://www.jeremyjordan.me/semantic-segmentation/)"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install torchsummary\nimport os\nimport cv2\nimport pandas\nimport numpy as np\nfrom tqdm import tqdm\nfrom glob import glob\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom random import choice, choices, shuffle\n\nimport torch\nimport torchvision\nimport torch.nn as nn \nfrom torch.utils.data import DataLoader, Dataset\n\nfrom torchsummary import summary\nimport torchvision.models as models\nimport torchvision.transforms as T\nfrom sklearn.model_selection import train_test_split\nfrom random import randint\nimport albumentations as A\nfrom PIL import Image\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def Deconv(n_input, n_output, k_size=4, stride=2, padding=1):\n    Tconv = nn.ConvTranspose2d(\n        n_input, n_output,\n        kernel_size=k_size,\n        stride=stride, padding=padding,\n        bias=False)\n    block = [\n        Tconv,\n        nn.BatchNorm2d(n_output),\n        nn.LeakyReLU(inplace=True),\n    ]\n    return nn.Sequential(*block)\n        \n\ndef Conv(n_input, n_output, k_size=4, stride=2, padding=0, bn=False, dropout=0):\n    conv = nn.Conv2d(\n        n_input, n_output,\n        kernel_size=k_size,\n        stride=stride,\n        padding=padding, bias=False)\n    block = [\n        conv,\n        nn.BatchNorm2d(n_output),\n        nn.LeakyReLU(0.2, inplace=True),\n        nn.Dropout(dropout)\n    ]\n    return nn.Sequential(*block)\n\ndef get_layer_efficientnet(efficientnet, start_index, end_index, x):\n    for index in range(start_index, end_index):\n        x = efficientnet._blocks[index](x)\n    \n    return x\n\nclass Unet(nn.Module):\n    def __init__(self, efficientnet):\n        super().__init__()\n        \n        # get some layer from efficientnet\n        self.efficientnet = efficientnet\n        self._conv_stem = efficientnet._conv_stem\n        self._bn0 = efficientnet._bn0\n        \n        self._conv_head = efficientnet._conv_head\n        self._bn1 = efficientnet._bn1\n        self._avg_pooling = efficientnet._avg_pooling\n        self._dropout = efficientnet._dropout\n        \n        self.tanh = nn.Tanh()\n        self.sigmoid = nn.Sigmoid()\n        \n        \n        # convolution layer, use to reduce the number of channel => reduce weight number\n        self.conv_6 = Conv(384, 192, 1, 1, 0)\n        self.conv_5 = Conv(160, 80, 1, 1, 0)\n        self.conv_4 = Conv(80, 40, 1, 1, 0)\n        self.conv_3 = Conv(48, 24, 1, 1, 0)\n        self.conv_2 = Conv(32, 16, 1, 1, 0)\n        self.conv_1 = Conv(64, 32, 1, 1, 0)\n        self.conv_0 = Conv(1, 1, 1, 1, 0)\n        \n        # deconvolution layer\n        self.deconv6 = Deconv(320, 192, 3, 1, 1)\n        self.deconv5 = Deconv(192, 80, 3, 2, 0)\n        self.deconv4 = Deconv(80, 40, 5, 2, 1)\n        self.deconv3 = Deconv(40, 24, 5, 2, 1)\n        self.deconv2 = Deconv(24, 16, 5, 2, 1)\n        self.deconv1 = Deconv(16, 32, 1, 1, 0)\n        self.deconv0 = Deconv(32, 1, 4, 2, 0)\n        \n        \n    def forward(self, x):\n        \n        # down sample\n        x = self._conv_stem(x)\n        x = self._bn0(x)\n        skip_1 = x\n        \n        x = get_layer_efficientnet(self.efficientnet, 0, 1, x)\n        skip_2 = x\n        x = get_layer_efficientnet(self.efficientnet, 1, 3, x)\n        skip_3 = x\n        x = get_layer_efficientnet(self.efficientnet, 3, 5, x)\n        skip_4 = x\n        \n        x = get_layer_efficientnet(self.efficientnet, 5, 8, x)\n        skip_5 = x\n        \n        x = get_layer_efficientnet(self.efficientnet, 8, 15, x)\n        skip_6 = x\n        \n        x = get_layer_efficientnet(self.efficientnet, 15, 16, x)\n        x7 = x\n        \n        # up sample\n        x6 = self.deconv6(x7)\n        x6 = torch.cat([x6, skip_6], dim=1)\n        x6 = self.conv_6(x6)\n        \n        x5 = self.deconv5(x6)\n        x5 = torch.cat([x5, skip_5], dim=1)\n        x5 = self.conv_5(x5)\n        \n        x4 = self.deconv4(x5)\n        x4 = torch.cat([x4, skip_4], dim=1)\n        x4 = self.conv_4(x4)\n        \n        x3 = self.deconv3(x4)\n        x3 = torch.cat([x3, skip_3], dim=1)\n        x3 = self.conv_3(x3)\n        \n        x2 = self.deconv2(x3)\n        x2 = torch.cat([x2, skip_2], dim=1)\n        x2 = self.conv_2(x2)\n        \n        x1 = self.deconv1(x2)\n        x1 = torch.cat([x1, skip_1], dim=1)\n        x1 = self.conv_1(x1)\n        \n        x0 = self.deconv0(x1)\n        x0 = self.conv_0(x0)\n        x0 = self.sigmoid(x0)\n        return x0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install torchsummary\nfrom torchsummary import summary\n\npackage_path = '../input/efficientnet/EfficientNet-PyTorch-master/'\nsys.path.append(package_path)\nfrom efficientnet_pytorch import EfficientNet\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nmodel_ft = EfficientNet.from_name('efficientnet-b0')\nmodel_ft.load_state_dict(torch.load('../input/efficientnet-pytorch/efficientnet-b0-08094119.pth'))\nmodel_ft._conv_stem = nn.Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n\nmodel_ft.to(device)\nmodel = Unet(model_ft)\nmodel.to(device)\n\n# for i, child in enumerate(model.children()):\n#     if i <= 7:\n#         for param in child.parameters():\n#             param.requires_grad = False\n\nprint(summary(model,input_size=(1,512,512)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ALPHA = 0.8\nGAMMA = 2\nimport torch.nn.functional as F\n\ndef dice_score(inputs, targets, smooth=1):\n    #flatten label and prediction tensors\n    inputs = inputs.view(-1)\n    targets = targets.view(-1)\n\n    intersection = (inputs * targets).sum()                            \n    dice_score = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  \n    \n    return dice_score\n\ndef get_dice_loss(inputs, targets, smooth=1):\n    #flatten label and prediction tensors\n    inputs = inputs.view(-1)\n    targets = targets.view(-1)\n\n    intersection = (inputs * targets).sum()                            \n    dice_loss = 1 - (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  \n    \n    return dice_loss\n\ndef get_focal_loss(inputs, targets, alpha=0.8, gamma=2, smooth=1):       \n    #flatten label and prediction tensors\n    inputs = inputs.view(-1)\n    targets = targets.view(-1)\n\n    #first compute binary cross-entropy \n    BCE = F.binary_cross_entropy(inputs, targets, reduction='mean')\n    BCE_EXP = torch.exp(-BCE)\n    focal_loss = alpha * (1-BCE_EXP)**gamma * BCE\n    \n    return focal_loss\n\ndef combo_loss(inputs, targets):\n    dice_loss = get_dice_loss(inputs, targets)\n    BCE = F.binary_cross_entropy(inputs, targets, reduction='mean')\n    focal_loss = get_focal_loss(inputs, targets)\n    \n    return 1*dice_loss + 4*focal_loss + 3*BCE","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def dice_score_validation(inputs, targets, smooth=1):\n    #flatten label and prediction tensors\n    inputs = inputs.view(-1)\n    targets = targets.view(-1)\n    \n    if np.all(inputs.numpy() == 0) and np.all(targets.numpy() == 0):\n        dice_score = 1\n        \n        return dice_score\n\n    intersection = (inputs * targets).sum()                            \n    dice_score = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  \n    \n    return dice_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import csv\n\n# train_params = [param for param in model.parameters() if param.requires_grad]\n# optimizer = torch.optim.Adam(train_params, lr=0.001, betas=(0.9, 0.99))\n\n# start_epochs = 281\n# end_epochs = 381\n# path_model = \"../input/segment-data/model_unet_efficienet_epoch_280.0_train_dice_score_0.5216_val_dice_score_0.3641.pth\"\n# model.load_state_dict(torch.load(path_model))\n# model.train()\n# saved_dir = \"model\"\n# os.makedirs(saved_dir, exist_ok=True)\n# path_csv_history = \"./model/history.csv\"\n\n# with open(path_csv_history, mode='w') as csv_file:\n#     fieldnames = ['epoch', 'train_dice_score', 'val_dice_score']\n#     writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n\n#     writer.writeheader()\n#     for epoch in range(start_epochs, end_epochs):\n#         number_iter = 1\n#         train_dice_score = []\n#         with tqdm(train_loader, unit=\"batch\") as tepoch:\n#             for imgs, masks in tepoch:\n#                 tepoch.set_description(f\"Epoch {epoch}, interation {number_iter}\")\n#                 optimizer.zero_grad()\n#                 imgs_gpu = imgs.to(device)\n#                 outputs = model(imgs_gpu)\n#                 # outputs = torch.sigmoid(outputs)\n#                 masks = masks.to(device)\n\n#                 dice_scores = dice_score(outputs, masks)\n#                 loss = combo_loss(outputs, masks)\n                \n#                 loss.backward()\n#                 optimizer.step()\n#                 tepoch.set_postfix(loss=loss.item(), \n#                                    dice_score=dice_scores.item())\n#                 number_iter += 1\n                \n#         train_dice_score = []\n#         train_losses = []\n#         with torch.no_grad():\n#             for imgs, masks in tqdm(train_loader):\n#                 imgs_gpu = imgs.to(device)\n#                 outputs = model(imgs_gpu)\n#                 # outputs = torch.sigmoid(outputs) \n#                 masks = masks.to(device)\n                \n#                 dice_scores = dice_score(outputs, masks)\n#                 loss = combo_loss(outputs, masks)\n                \n#                 train_dice_score.extend([dice_scores.item()])\n#                 train_losses.extend([loss.item()])\n#             train_dice_score = np.mean(np.array(train_dice_score))\n#             train_losses = np.mean(np.array(train_losses))\n        \n#         val_dice_score = []\n#         val_losses = []\n#         with torch.no_grad():\n#             for imgs, masks in tqdm(val_loader):\n#                 imgs_gpu = imgs.to(device)\n#                 outputs = model(imgs_gpu)\n#                 # outputs = torch.sigmoid(outputs) \n#                 masks = masks.to(device)\n                \n#                 dice_scores = dice_score(outputs, masks)\n#                 loss = combo_loss(outputs, masks)\n                \n#                 val_dice_score.extend([dice_scores.item()])\n#                 val_losses.extend([loss.item()])\n#             val_dice_score = np.mean(np.array(val_dice_score))\n#             val_losses = np.mean(np.array(val_losses))\n#             print(f\"Epoch {epoch}, Train loss {train_losses:0.4f}, train dice score: {train_dice_score:0.4f}, Validation loss {val_losses:0.4f}, validation dice score: {val_dice_score:0.4f}\")\n#             path = \"./model/model_unet_efficienet_epoch_{:.1f}_train_dice_score_{:0.4f}_val_dice_score_{:0.4f}.pth\".format(epoch, train_dice_score, val_dice_score)\n#             torch.save(model.state_dict(), path)\n        \n#             writer.writerow({\n#                 \"epoch\": epoch,\n#                 \"train_dice_score\": train_dice_score,\n#                 \"val_dice_score\": val_dice_score\n#             })","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test model\nimport csv\n\ntrain_params = [param for param in model.parameters() if param.requires_grad]\noptimizer = torch.optim.Adam(train_params, lr=0.001, betas=(0.9, 0.99))\n\npath_model = \"../input/segment-data/model_unet_efficienet_epoch_280.0_train_dice_score_0.5216_val_dice_score_0.3641.pth\"\nmodel.load_state_dict(torch.load(path_model))\n\n\n\ntrain_dice_score = []\nwith torch.no_grad():\n    for imgs, masks in tqdm(train_loader):\n        imgs_gpu = imgs.to(device)\n        outputs = model(imgs_gpu)\n        masks = masks.to(device)\n        \n        dice_scores = dice_score_validation(outputs, masks)\n        train_dice_score.extend([dice_scores.item()])\n\n    train_dice_score = np.mean(np.array(train_dice_score))\n\n    val_dice_score = []\n    \nwith torch.no_grad():\n    for imgs, masks in tqdm(val_loader):\n        imgs_gpu = imgs.to(device)\n        outputs = model(imgs_gpu)\n        masks = masks.to(device)\n        \n        dice_scores = dice_score_validation(outputs, masks)\n        val_dice_score.extend([dice_scores.item()])\n\nval_dice_score = np.mean(np.array(val_dice_score))\nprint(f\"train dice score: {train_dice_score:0.4f}, validation dice score: {val_dice_score:0.4f}\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# visualize \nnumber = 0\nwith torch.no_grad():\n    for imgs, masks in val_loader:\n        imgs_gpu = imgs.to(device)\n        outputs = model(imgs_gpu)\n        # outputs = torch.sigmoid(outputs) \n        outputs = torch.round(outputs) * 255\n        masks = masks.to(device)\n        for index in range(10):\n            img_origin = np.reshape(imgs_gpu[index].cpu().numpy(), (512, 512))\n            pred_ = np.reshape(outputs[index].cpu().numpy(), (512, 512))\n            mask_ = np.reshape(masks[index].cpu().numpy()*255, (512, 512))\n            if np.all(mask_==0):\n                continue\n            fig, (ax1, ax2, ax3) = plt.subplots(1, 3)\n            fig.set_figheight(15)\n            fig.set_figwidth(15)\n            ax1.imshow(img_origin, cmap=plt.cm.bone)\n            ax2.imshow(img_origin, cmap=plt.cm.bone)\n            ax2.imshow(pred_, alpha=0.3, cmap=\"Blues\")\n            ax3.imshow(img_origin, cmap=plt.cm.bone)\n            ax3.imshow(mask_, alpha=0.3, cmap=\"Blues\")\n            number += 1\n        if number == 100:\n            break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}