{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Introduction\n\n이전 커널 [(Finding Pneumo: EDA and UNet Starter Code)](https://www.kaggle.com/ekhtiar/finding-pneumo-eda-and-unet-starter-code)에서 우리는 데이터 EDA를 진행하였고 작동하지 않은 UNet 모델을 학습했습니다. 이번 커널에서는 일부 기흉 사례를 식별하는 ResNet 모델을 만듭니다.\n\n**이 커널에서는 누출이 기흉 환자를 구할 수 없기 때문에, 어떠한 누출도 사용하지 않을 겁니다.**\n저자의 목표는 데이터 세트와 다른 데이터 세트에 일반적인 접근 방식과 모델을 식별하는 것입니다. 이를 위해 커널의 요약 섹션에서 ResUNet 교육 과정을 문서화 하겠습니다."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Basic imports for the entire Kernel\nimport numpy as np\nimport pandas as pd\n# imports for loading data\nimport pydicom\nfrom glob import glob\nfrom tqdm import tqdm\n# import mask function\nimport sys\nsys.path.insert(0, '../input/siim-acr-pneumothorax-segmentation')\nfrom mask_functions import rle2mask, mask2rle\n# plotting function\nfrom matplotlib import pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load rles\nrles_df = pd.read_csv('../input/siim-train-test/siim/train-rle.csv')\n# the second column has a space at the start, so manually giving column name\nrles_df.columns = ['ImageId', 'EncodedPixels']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def dicom_to_dict(dicom_data, file_path, rles_df, encoded_pixels=True):\n    \"\"\"Parse DICOM dataset and returns a dictonary with relevant fields.\n\n    Args:\n        dicom_data (dicom): chest x-ray data in dicom format.\n        file_path (str): file path of the dicom data.\n        rles_df (pandas.core.frame.DataFrame): Pandas dataframe of the RLE.\n        encoded_pixels (bool): if True we will search for annotation.\n        \n    Returns:\n        dict: contains metadata of relevant fields.\n    \"\"\"\n    \n    data = {}\n    \n    # Parse fields with meaningful information\n    data['patient_name'] = dicom_data.PatientName\n    data['patient_id'] = dicom_data.PatientID\n    data['patient_age'] = int(dicom_data.PatientAge)\n    data['patient_sex'] = dicom_data.PatientSex\n    data['pixel_spacing'] = dicom_data.PixelSpacing\n    data['file_path'] = file_path\n    data['id'] = dicom_data.SOPInstanceUID\n    \n    # look for annotation if enabled (train set)\n    if encoded_pixels:\n        encoded_pixels_list = rles_df[rles_df['ImageId']==dicom_data.SOPInstanceUID]['EncodedPixels'].values\n       \n        pneumothorax = False\n        for encoded_pixels in encoded_pixels_list:\n            if encoded_pixels != ' -1':\n                pneumothorax = True\n        \n        # get meaningful information (for train set)\n        data['encoded_pixels_list'] = encoded_pixels_list\n        data['has_pneumothorax'] = pneumothorax\n        data['encoded_pixels_count'] = len(encoded_pixels_list)\n        \n    return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a list of all the files\ntrain_fns = sorted(glob('../input/siim-train-test/siim/dicom-images-train/*/*/*.dcm'))\n# parse train DICOM dataset\ntrain_metadata_df = pd.DataFrame()\ntrain_metadata_list = []\nfor file_path in tqdm(train_fns):\n    dicom_data = pydicom.dcmread(file_path)\n    train_metadata = dicom_to_dict(dicom_data, file_path, rles_df)\n    train_metadata_list.append(train_metadata)\ntrain_metadata_df = pd.DataFrame(train_metadata_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a list of all the files\ntest_fns = sorted(glob('../input/siim-train-test/siim/dicom-images-test/*/*/*.dcm'))\n# parse test DICOM dataset\ntest_metadata_df = pd.DataFrame()\ntest_metadata_list = []\nfor file_path in tqdm(test_fns):\n    dicom_data = pydicom.dcmread(file_path)\n    test_metadata = dicom_to_dict(dicom_data, file_path, rles_df, encoded_pixels=False)\n    test_metadata_list.append(test_metadata)\ntest_metadata_df = pd.DataFrame(test_metadata_list)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Lung Segmentation\n저자의 마지막 EDA 노트북과 discussion 섹션에서 [Dr. Konya](https://www.kaggle.com/sandorkonya)는 멋진 공헌을 했습니다. 그가 언급한 매우 중요하고 분명한 점은 이 대회의 사용 사례가 인정되는 한 기흉이 폐에만 나타날 것이기 때문에 이 대회에서 폐 분할을 수행하기 위해 기존 모델을 사용해아한다는 것입니다.\n\n저자는 Github에서 훌륭한 업적을 발견했습니다. 그 중 하나는 폐 분할을 위한 [imlab-uiip](https://github.com/imlab-uiip/lung-segmentation-2d)입니다. 그들은 또한 Github 저장소에 사전 훈련된 모델을 업로드 했습니다. 이 섹션에서는 이 사전에 학습된 모델을 사용하여 이 대회의 데이터에 대한 마스크를 만듭니다. 또한, 이번 대회에서 제공하는 RLE 기능으로 마스크를 통합하여 향후 사례에서 쉽게 사용할 수 있도록 만듭니다.\n\n그 모델을 한 번 돌려봤는데, 폐 분할의 정확성이 완벽하지 않았습니다. 하지만 이 모델은 이미지에서 원하지 않는 부분을 자동으로 자르는데 충분할 거 같습니다. 토론에서 [Lung segmentation Dataset of the training images by a radiologist](https://www.kaggle.com/c/siim-acr-pneumothorax-segmentation/discussion/100864)와 [Dr. Konya](https://www.kaggle.com/sandorkonya)는 경쟁에서 우리에게 중요한 부분만 잘라 내기 위해 Dr. Konya가 한 주석을 중요하게 집었습니다. 이 모델을 사용하여 이를 자동화 하고 세분화를 위한 경계 상자를 얻으려고합니다. 이러한 방식으로 데이터에서 많은 노이즈를 제거하고 모델의 정확도를 높일 수 있습니다."},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport cv2\nfrom skimage import morphology, io, color, exposure, img_as_float, transform","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_dir = '../input/lung-segmentation-for-siimacr-pneumothorax/trained_model.hdf5'\nlung_seg_model = tf.keras.models.load_model(model_dir, custom_objects=None, compile=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_lung_seg_tensor(file_path, batch_size, seg_size, n_channels):\n    \n        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n        # Initialization\n        X = np.empty((batch_size, seg_size, seg_size, n_channels))\n\n        # Process Image\n        pixel_array = pydicom.read_file(file_path).pixel_array\n        image_resized = cv2.resize(pixel_array, (seg_size, seg_size))\n        image_resized = exposure.equalize_hist(image_resized)\n        image_resized = np.array(image_resized, dtype=np.float64)\n        image_resized -= image_resized.mean()\n        image_resized /= image_resized.std()\n        # Store Image\n        X[0,] = np.expand_dims(image_resized, axis=2)\n\n        return X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_small_regions(img, size):\n    \"\"\"Morphologically removes small (less than size) connected regions of 0s or 1s.\"\"\"\n    img = morphology.remove_small_objects(img, size)\n    img = morphology.remove_small_holes(img, size)\n    return img","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def bounding_box(img):\n    # return max and min of a mask \n    rows = np.any(img, axis=1)\n    cols = np.any(img, axis=0)\n    rmin, rmax = np.where(rows)[0][[0, -1]]\n    cmin, cmax = np.where(cols)[0][[0, -1]]\n\n    return rmin, rmax, cmin, cmax","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_lung_seg_rle(metadata_df, seg_size):\n\n    processed_images = []\n\n    for id, row in metadata_df.iterrows():\n        # get image in the 4d tensor\n        img = get_lung_seg_tensor(row['file_path'],1,seg_size,1)\n        # get segmented mask\n        seg_mask = lung_seg_model.predict(img).reshape((seg_size,seg_size))\n        # only take above .5\n        seg_mask = seg_mask > 0.5\n        # remove small region\n        seg_mask = remove_small_regions(seg_mask, 0.02 * np.prod(seg_size))\n        processed_img = {}\n        processed_img['id'] = row['id']\n        processed_img['lung_mask'] = mask2rle(seg_mask*255, seg_size, seg_size)\n        processed_img['rmin'], processed_img['rmax'], processed_img['cmin'], processed_img['cmax'] = bounding_box(seg_mask)\n        processed_images.append(processed_img)\n    \n    return pd.DataFrame(processed_images)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seg_size = 256","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"저자는 사전에 학습된 모델의 데이터 세트와 모든 사람의 삶을 편하게 하기 위해 사전 처리 된 폐 분할 마스크를 [Lung Segmentation For SIIM-ACR Pneumothorax\n](https://www.kaggle.com/ekhtiar/lung-segmentation-for-siimacr-pneumothorax)에 만들어 놓았습니다."},{"metadata":{"trusted":true},"cell_type":"code","source":"#try:\n#    train_lung_mask_df = pd.read_csv('../input/lung-segmentation-for-siimacr-pneumothorax/train_lung_mask.csv')\n#except FileNotFoundError:\ntrain_lung_mask_df = get_lung_seg_rle(train_metadata_df, seg_size)\ntrain_lung_mask_df.to_csv('./train_lung_mask.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#try:\n#    test_lung_mask_df = pd.read_csv('../input/lung-segmentation-for-siimacr-pneumothorax/test_lung_mask.csv')\n#except FileNotFoundError:\ntest_lung_mask_df = get_lung_seg_rle(test_metadata_df, seg_size)\ntest_lung_mask_df.to_csv('./test_lung_mask.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### proof of concept\n우리의 segmentation을 확인하고 그것이 잘 돌아가는지 시각적으로 확인해봅시다."},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_lung_seg(file_path, mask_encoded_list, lung_mask, rmin, rmax, cmin, cmax):\n    \n    pixel_array = pydicom.dcmread(file_path).pixel_array\n    \n    # use the masking function to decode RLE\n    mask_decoded_list = [rle2mask(mask_encoded, 1024, 1024).T for mask_encoded in mask_encoded_list]\n    lung_mask_decoded = cv2.resize(rle2mask(lung_mask, 256, 256), (1024,1024))\n    rmin, rmax, cmin, cmax =  rmin * 4, rmax * 4, cmin * 4, cmax * 4 \n    \n    fig, ax = plt.subplots(nrows=1, ncols=3, sharey=True, figsize=(20,10))\n    \n    \n    ax[0].imshow(pixel_array, cmap=plt.cm.bone)\n    ax[0].imshow(lung_mask_decoded, alpha=0.3, cmap=\"Blues\")\n    ax[0].set_title('Xray with Lung Mask')\n    \n    ax[1].imshow(pixel_array[rmin:rmax+1,cmin:cmax+1], cmap=plt.cm.bone)\n    ax[1].set_title('Cropped Xray')\n   \n    ax[2].imshow(lung_mask_decoded, cmap='Blues')\n    for mask_decoded in mask_decoded_list:\n        ax[2].imshow(mask_decoded, alpha=0.3, cmap=\"Reds\")\n    ax[2].set_title('Lung Mask with Pneumothorax')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_lm_metadata_df = pd.concat([train_metadata_df, train_lung_mask_df.drop('id',axis=1)], axis=1)\ntest_lm_metadata_df = pd.concat([test_metadata_df, test_lung_mask_df.drop('id',axis=1)], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i, r in train_lm_metadata_df[train_lm_metadata_df['has_pneumothorax']==True][:10].iterrows():\n    file_path = r['file_path']\n    encoded_pixels_list = r['encoded_pixels_list']\n    lung_mask = r['lung_mask']\n    rmin = r['rmin'] \n    rmax = r['rmax']\n    cmin = r['cmin']\n    cmax = r['cmax']\n    \n    plot_lung_seg(file_path, encoded_pixels_list, lung_mask, rmin, rmax, cmin, cmax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"우리 모델이 안되는 것보다 잘 한 결과물을 더 많이 얻는 걸 확인할 수 있습니다. 비록 우리의 폐 분할이 완벽하지 않지만 이미지의 자르기 버전은 사용하는데 유망해 보입니다. 다음 커널에서 Dr. Konya가 손수 만든 어노테이션과 얼마나 가까워졌는지 비교해보도록 하겠습니다."},{"metadata":{},"cell_type":"markdown","source":"## ResUNet\n\n이 섹션에서는 UNet 대신 ResUNet을 사용하여 기흉을 예측합니다. 이 CNN 아키텍처를 제안한 원문 논문은 [ResUNet-a: a deep learning framework for semantic segmentation of remotely sensed data](https://arxiv.org/abs/1904.00592) 입니다. 만약 당신이 논문을 읽는다면, 이 네트워크에 대한 자세한 정보를 얻을 수 있습니다. 그러나 참고할 수 있도록, 아래 모델의 아키텍처를 첨부합니다.\n\n![ResUNet Architecture](https://raw.githubusercontent.com/nikhilroxtomar/Deep-Residual-Unet/master/images/arch.png)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# defining configuration parameters\nimg_size = 512 # image resize size\nbatch_size = 8\n# batch size for training unet\nk_size = 3 # kernel size 3x3\nval_size = .20 # split of training set between train and validation set\nno_pneumo_drop = 0 # dropping some data to balance the class a little bit better","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# imports for building the network\nfrom tensorflow import reduce_sum\nfrom tensorflow.keras.backend import pow\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Conv2D, MaxPool2D, UpSampling2D, Concatenate, Add, Flatten\nfrom tensorflow.keras.losses import binary_crossentropy\nfrom sklearn.model_selection import train_test_split\nimport cv2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Data Generator\n\n데이터를 우리 모델에 입력하기 위해 사용자 지정 데이터 생성기를 만듭니다. 생성기를 사용하면 데이터를 한 번에 모두 메모리에 로드하는 대신 점진적으로 로드할 수 있습니다. 사용자 지정 생성기를 사용하면 데이터를 로드하는 동안 더 많은 사용자 지정에 맞출 수 있습니다. 모델이 GPU에서 처리되고 있기 때문에 사용자 지정 생성기를 통해 이미지를 사전에 처리할 수 있습니다. 현재는 다중 프로세서를 활용하여 사전 처리를 병렬화 할 수도 있습니다."},{"metadata":{"trusted":true},"cell_type":"code","source":"class DataGenerator(tf.keras.utils.Sequence):\n    def __init__(self, file_path_list, labels, batch_size=32, \n                 img_size=256, channels=1, shuffle=True):\n        self.file_path_list = file_path_list\n        self.labels = labels\n        self.batch_size = batch_size\n        self.img_size = img_size\n        self.channels = channels\n        self.shuffle = shuffle\n        self.on_epoch_end()\n    \n    def __len__(self):\n        'denotes the number of batches per epoch'\n        return int(np.floor(len(self.file_path_list)) / self.batch_size)\n    \n    def __getitem__(self, index):\n        'generate one batch of data'\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n        # get list of IDs\n        file_path_list_temp = [self.file_path_list[k] for k in indexes]\n        # generate data\n        X, y = self.__data_generation(file_path_list_temp)\n        # return data \n        return X, y\n    \n    def on_epoch_end(self):\n        'update ended after each epoch'\n        self.indexes = np.arange(len(self.file_path_list))\n        if self.shuffle:\n            np.random.shuffle(self.indexes)\n            \n    def __data_generation(self, file_path_list_temp):\n        'generate data containing batch_size samples'\n        X = np.empty((self.batch_size, self.img_size, self.img_size, self.channels))\n        y = np.empty((self.batch_size, self.img_size, self.img_size, self.channels))\n        \n        for idx, file_path in enumerate(file_path_list_temp):\n            \n            id = file_path.split('/')[-1][:-4]\n            rle = self.labels.get(id)\n            image = pydicom.read_file(file_path).pixel_array\n            image_resized = cv2.resize(image, (self.img_size, self.img_size))\n            image_resized = np.array(image_resized, dtype=np.float64)\n            \n            X[idx,] = np.expand_dims(image_resized, axis=2)\n            \n            # if there is no mask create empty mask\n            # notice we are starting of with 1024 because we need to use the rle2mask function\n            if rle is None:\n                mask = np.zeros((1024, 1024))\n            else:\n                if len(rle) == 1:\n                    mask = rle2mask(rle[0], 1024, 1024).T\n                else: \n                    mask = np.zeros((1024, 1024))\n                    for r in rle:\n                        mask =  mask + rle2mask(r, 1024, 1024).T\n                        \n            mask_resized = cv2.resize(mask, (self.img_size, self.img_size))\n            y[idx,] = np.expand_dims(mask_resized, axis=2)\n            \n        # normalize \n        X = X / 255\n        y = (y > 0).astype(int)\n            \n        return X, y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"masks = {}\nfor index, row in train_metadata_df[train_metadata_df['has_pneumothorax']==1].iterrows():\n    masks[row['id']] = list(row['encoded_pixels_list'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bad_data = train_metadata_df[train_metadata_df['encoded_pixels_count']==0].index\nnew_train_metadata_df = train_metadata_df.drop(bad_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_data = new_train_metadata_df[new_train_metadata_df['has_pneumothorax'] == False].sample(no_pneumo_drop).index\nnew_train_metadata_df = new_train_metadata_df.drop(drop_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split the training data into train and validation set (stratified)\nX_train, X_val, y_train, y_val = train_test_split(new_train_metadata_df.index, new_train_metadata_df['has_pneumothorax'].values, test_size=val_size, random_state=42)\nX_train, X_val = new_train_metadata_df.loc[X_train]['file_path'].values, new_train_metadata_df.loc[X_val]['file_path'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {'img_size': img_size,\n          'batch_size': batch_size,\n          'channels': 1,\n          'shuffle': True}\n\n# Generators\ntraining_generator = DataGenerator(X_train, masks, **params)\nvalidation_generator = DataGenerator(X_val, masks, **params)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"우리는 다음과 같은 방법으로 생성기 클래스가 작동하고 올바른 데이터를 시각적으로 전달하고 있는지 확인할 수 있습니다."},{"metadata":{"trusted":true},"cell_type":"code","source":"x, y = training_generator.__getitem__(0)\nprint(x.shape, y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure()\nfig.subplots_adjust(hspace=0.4, wspace=0.4)\nax = fig.add_subplot(1, 2, 1)\nax.imshow(x[6].reshape(img_size, img_size), cmap=plt.cm.bone)\nax = fig.add_subplot(1, 2, 2)\nax.imshow(np.reshape(y[6], (img_size, img_size)), cmap=\"gray\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### ResUNet TensorFlow Keras Implementation\n\nResUNet 모델을 구축해 보겠습니다.사실 저자는 [Github](https://github.com/nikhilroxtomar/Deep-Residual-Unet)에서 ResUNet의 멋진 구현을 찾았고 아래 커널에서 사용했습니다."},{"metadata":{"trusted":true},"cell_type":"code","source":"def bn_act(x, act=True):\n    'batch normalization layer with an optinal activation layer'\n    x = tf.keras.layers.BatchNormalization()(x)\n    if act == True:\n        x = tf.keras.layers.Activation('relu')(x)\n    return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def conv_block(x, filters, kernel_size=3, padding='same', strides=1):\n    'convolutional layer which always uses the batch normalization layer'\n    conv = bn_act(x)\n    conv = Conv2D(filters, kernel_size, padding=padding, strides=strides)(conv)\n    return conv","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def stem(x, filters, kernel_size=3, padding='same', strides=1):\n    conv = Conv2D(filters, kernel_size, padding=padding, strides=strides)(x)\n    conv = conv_block(conv, filters, kernel_size, padding, strides)\n    shortcut = Conv2D(filters, kernel_size=1, padding=padding, strides=strides)(x)\n    shortcut = bn_act(shortcut, act=False)\n    output = Add()([conv, shortcut])\n    return output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def residual_block(x, filters, kernel_size=3, padding='same', strides=1):\n    res = conv_block(x, filters, k_size, padding, strides)\n    res = conv_block(res, filters, k_size, padding, 1)\n    shortcut = Conv2D(filters, kernel_size, padding=padding, strides=strides)(x)\n    shortcut = bn_act(shortcut, act=False)\n    output = Add()([shortcut, res])\n    return output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def upsample_concat_block(x, xskip):\n    u = UpSampling2D((2,2))(x)\n    c = Concatenate()([u, xskip])\n    return c","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def ResUNet(img_size):\n    f = [16, 32, 64, 128, 256, 512, 1024, 2048] * 32\n    inputs = Input((img_size, img_size, 1))\n    \n    ## Encoder\n    e0 = inputs\n    e1 = stem(e0, f[0])\n    e2 = residual_block(e1, f[1], strides=2)\n    e3 = residual_block(e2, f[2], strides=2)\n    e4 = residual_block(e3, f[3], strides=2)\n    e5 = residual_block(e4, f[4], strides=2)\n    e6 = residual_block(e5, f[5], strides=2)\n    e7 = residual_block(e6, f[6], strides=2)\n    \n    ## Bridge\n    b0 = conv_block(e7, f[6], strides=1)\n    b1 = conv_block(b0, f[6], strides=1)\n    \n    ## Decoder\n    u1 = upsample_concat_block(b1, e6)\n    d1 = residual_block(u1, f[6])\n    \n    u2 = upsample_concat_block(d1, e5)\n    d2 = residual_block(u2, f[3])\n    \n    u3 = upsample_concat_block(d2, e4)\n    d3 = residual_block(u3, f[2])\n    \n    u4 = upsample_concat_block(d3, e3)\n    d4 = residual_block(u4, f[1])\n    \n    u5 = upsample_concat_block(d4, e2)\n    d5 = residual_block(u5, f[1])\n    \n    u6 = upsample_concat_block(d5, e1)\n    d6 = residual_block(u6, f[1])\n    \n    outputs = tf.keras.layers.Conv2D(1, (1, 1), padding=\"same\", activation=\"sigmoid\")(d6)\n    model = tf.keras.models.Model(inputs, outputs)\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def dsc(y_true, y_pred):\n    smooth = 1.\n    y_true_f = Flatten()(y_true)\n    y_pred_f = Flatten()(y_pred)\n    intersection = reduce_sum(y_true_f * y_pred_f)\n    score = (2. * intersection + smooth) / (reduce_sum(y_true_f) + reduce_sum(y_pred_f) + smooth)\n    return score\n\ndef dice_loss(y_true, y_pred):\n    loss = 1 - dsc(y_true, y_pred)\n    return loss\n\ndef bce_dice_loss(y_true, y_pred):\n    loss = binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n    return loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = ResUNet(img_size)\nadam = tf.keras.optimizers.Adam(lr = 0.01, epsilon = 0.1)\nmodel.compile(optimizer=adam, loss=bce_dice_loss, metrics=[dsc])\n#model.summary() # print out the architecture of our network","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load a pre trained model here if you wish\n# model.load_weights('../input/resunet-e200-s256/ResUNet.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# running more epoch to see if we can get better results\nhistory = model.fit_generator(generator=training_generator, validation_data=validation_generator, epochs=10, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save('./ResUNet.h5')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Plotting Model Training History\n\n이 섹션에서는 단순한 non-flashy matplotlib를 사용하여 모델의 성능을 표시합니다."},{"metadata":{"trusted":true},"cell_type":"code","source":"# list all data in history\nprint(history.history.keys())\n# summarize history for accuracy\nplt.figure(figsize=(20,5))\nplt.subplot(1,2,1)\nplt.plot(history.history['dsc'])\nplt.plot(history.history['val_dsc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\n\n# summarize history for loss\nplt.subplot(1,2,2)\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Checking out our model\n\n우리는 아래와 같은 방법으로 모델에 대해 어떻게 작동하는지 시각적으로 조사할 수 있습니다."},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_train(img, mask, pred):\n    \n    fig, ax = plt.subplots(nrows=1, ncols=3, sharey=True, figsize=(15,5))\n    \n    ax[0].imshow(img, cmap=plt.cm.bone)\n    ax[0].set_title('Chest X-Ray')\n    \n    ax[1].imshow(mask, cmap=plt.cm.bone)\n    ax[1].set_title('Mask')\n    \n    ax[2].imshow(pred, cmap=plt.cm.bone)\n    ax[2].set_title('Pred Mask')\n    \n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets loop over the predictions and print some good-ish results\ncount = 0\nfor i in range(0,50):\n    if count <= 50:\n        x, y = validation_generator.__getitem__(i)\n        predictions = model.predict(x)\n        for idx, val in enumerate(x):\n            #if y[idx].sum() > 0 and count <= 15: \n                img = np.reshape(x[idx]* 255, (img_size, img_size))\n                mask = np.reshape(y[idx]* 255, (img_size, img_size))\n                pred = np.reshape(predictions[idx], (img_size, img_size))\n                pred = pred > 0.5\n                pred = pred * 255\n                plot_train(img, mask, pred)\n                count += 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Making Predictions\n\n이 섹션에서는 모델을 사용하여 예측하고 누출(leak)을 이용하지 않고 제출물을 만듭니다."},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_test_tensor(file_path, batch_size, img_size, channels):\n    \n        X = np.empty((batch_size, img_size, img_size, channels))\n\n        # Store sample\n        pixel_array = pydicom.read_file(file_path).pixel_array\n        image_resized = cv2.resize(pixel_array, (img_size, img_size))\n        image_resized = np.array(image_resized, dtype=np.float64)\n        image_resized -= image_resized.mean()\n        image_resized /= image_resized.std()\n        X[0,] = np.expand_dims(image_resized, axis=2)\n\n        return X","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"이 대외에서 우리는 아주 작고 작은 예측을 많이 할 것입니다. 지역이 매우 작은 경우 일반적으로 제거하는 것이 좋은 정책입니다. 아래의 기능은 이를 수행하는 데 도움이 됩니다."},{"metadata":{"trusted":true},"cell_type":"code","source":"from skimage import morphology\n\ndef remove_small_regions(img, size):\n    \"\"\"Morphologically removes small (less than size) connected regions of 0s or 1s.\"\"\"\n    img = morphology.remove_small_objects(img, size)\n    img = morphology.remove_small_holes(img, size)\n    return img","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"기흉을 예측하고 식별하기 위해 방금 구축한 모델을 사용하겠습니다."},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = []\n\nfor i, row in test_metadata_df.iterrows():\n\n    test_img = get_test_tensor(test_metadata_df['file_path'][i],1,img_size,1)\n    \n    pred_mask = model.predict(test_img).reshape((img_size,img_size))\n    prediction = {}\n    prediction['ImageId'] = str(test_metadata_df['id'][i])\n    pred_mask = cv2.resize(pred_mask.astype('float32'), (1024, 1024))\n    pred_mask = (pred_mask > .5).astype(int)\n    pred_mask = remove_small_regions(pred_mask, 0.02 * np.prod(1024))\n    \n    if pred_mask.sum() < 1:\n        prediction['EncodedPixels']=  -1\n    else:\n        prediction['EncodedPixels'] = mask2rle(pred_mask.T * 255, 1024, 1024)\n        \n    submission.append(prediction)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df = pd.DataFrame(submission)\nsubmission_df = submission_df[['ImageId','EncodedPixels']]\n# check out some predictions and see if it looks good\nsubmission_df[ submission_df['EncodedPixels'] != -1].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df.to_csv('./submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}