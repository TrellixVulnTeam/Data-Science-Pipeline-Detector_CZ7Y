{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nimport os\nimport torch\nimport copy\nimport torch.utils.data\nimport matplotlib.patches as patches\nfrom torchvision import transforms\nimport collections\nimport random\nfrom tqdm import tqdm_notebook as tqdm\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def rle2mask(rle, width, height):\n    mask= np.zeros(width* height)\n    array = np.asarray([int(x) for x in rle.split()])\n    starts = array[0::2]\n    lengths = array[1::2]\n    current_position = 0\n    for index, start in enumerate(starts):\n        current_position += start\n        mask[current_position:current_position+lengths[index]] = 1\n        current_position += lengths[index]\n    return mask.reshape(width, height)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prepare Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"class SIIMDataset(torch.utils.data.Dataset):\n    def __init__(self, df_path, img_dir):\n        self.df = pd.read_csv(df_path, nrows=100)\n        self.height = 1024\n        self.width = 1024\n        self.image_dir = img_dir\n        self.image_info = collections.defaultdict(dict)\n        self.df = self.df.drop_duplicates('ImageId', keep='last').reset_index(drop=True)\n\n        counter = 0\n        for index, row in tqdm(self.df.iterrows(), total=len(self.df)):\n            image_id = row['ImageId']\n            image_path = os.path.join(self.image_dir, image_id)\n            if os.path.exists(image_path + '.png') and row[\" EncodedPixels\"].strip() != \"-1\":\n                self.image_info[counter][\"image_id\"] = image_id\n                self.image_info[counter][\"image_path\"] = image_path\n                self.image_info[counter][\"annotations\"] = row[\" EncodedPixels\"].strip()\n                counter += 1\n\n    def __getitem__(self, idx):\n        img_path = self.image_info[idx][\"image_path\"]\n        img = Image.open(img_path + '.png').convert(\"RGB\")\n        width, height = img.size\n        img = img.resize((self.width, self.height), resample=Image.BILINEAR)\n        info = self.image_info[idx]\n\n        mask = rle2mask(info['annotations'], width, height)\n        mask = Image.fromarray(mask.T)\n        mask = mask.resize((self.width, self.height), resample=Image.BILINEAR)\n        mask = np.expand_dims(mask, axis=0)\n\n        pos = np.where(np.array(mask)[0, :, :])\n        xmin = np.min(pos[1])\n        xmax = np.max(pos[1])\n        ymin = np.min(pos[0])\n        ymax = np.max(pos[0])\n\n        boxes = torch.as_tensor([[xmin, ymin, xmax, ymax]], dtype=torch.float32)\n        labels = torch.ones((1,), dtype=torch.int64)\n        masks = torch.as_tensor(mask, dtype=torch.uint8)\n        image_id = torch.tensor([idx])\n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n        iscrowd = torch.zeros((1,), dtype=torch.int64)\n\n        target = {}\n        target[\"boxes\"] = boxes\n        target[\"labels\"] = labels\n        target[\"masks\"] = masks\n        target[\"image_id\"] = image_id\n        target[\"area\"] = area\n        target[\"iscrowd\"] = iscrowd\n        return transforms.ToTensor()(img), target\n\n    def __len__(self):\n        return len(self.image_info)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = SIIMDataset(\"../input/siim-dicom-images/train-rle.csv\", \"../input/siim-png-images/input/train_png/\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plotter(img, target):\n    fig,ax = plt.subplots(1)\n    ax.imshow(transforms.ToPILImage()(img))\n    ax.imshow(transforms.ToPILImage()(target[\"masks\"][0].cpu() * 255), alpha=0.5)\n    rect = patches.Rectangle((target[\"boxes\"][0][0].item(),\n                             target[\"boxes\"][0][1].item()),\n                             target[\"boxes\"][0][2].item() - target[\"boxes\"][0][0].item(),\n                             target[\"boxes\"][0][3].item() - target[\"boxes\"][0][1].item(),\n                             linewidth=1,edgecolor='r',facecolor='none')\n    ax.add_patch(rect)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img, target = dataset[0]\nplotter(img, target)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Horizontal Flip"},{"metadata":{"trusted":true},"cell_type":"code","source":"def horizontal_flip(image, target):\n    img = copy.deepcopy(image)\n    tgt = copy.deepcopy(target)\n    height, width = img.shape[-2:]\n    img = img.flip(-1)\n    bbox = tgt[\"boxes\"]\n    bbox[:, [0, 2]] = width - bbox[:, [2, 0]]\n    tgt[\"boxes\"] = bbox\n    tgt[\"masks\"] = tgt[\"masks\"].flip(-1)\n    return img, tgt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_img, _target = horizontal_flip(img, target)\nplotter(_img, _target)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Vertical Flip"},{"metadata":{"trusted":true},"cell_type":"code","source":"def vertical_flip(image, target):\n    img = copy.deepcopy(image)\n    tgt = copy.deepcopy(target)\n    height, width = img.shape[-2:]\n    img = img.flip(1)\n    bbox = tgt[\"boxes\"]\n    bbox[:, [1, 3]] = height - bbox[:, [3, 1]]\n    tgt[\"boxes\"] = bbox\n    tgt[\"masks\"] = tgt[\"masks\"].flip(1)\n    return img, tgt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_img, _target = vertical_flip(img, target)\nplotter(_img, _target)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Rotation"},{"metadata":{"trusted":true},"cell_type":"code","source":"def rotation(image, target, degrees):\n    img = copy.deepcopy(image)\n    tgt = copy.deepcopy(target)\n    height, width = img.shape[-2:]\n    img = transforms.ToTensor()(transforms.ToPILImage()(img).rotate(degrees))\n    rotated_mask = transforms.ToTensor()(transforms.ToPILImage()(tgt[\"masks\"]).rotate(degrees))\n    \n    pos = np.where(np.array(rotated_mask)[0, :, :])\n    xmin = np.min(pos[1])\n    xmax = np.max(pos[1])\n    ymin = np.min(pos[0])\n    ymax = np.max(pos[0])\n\n    bbox = torch.as_tensor([[xmin, ymin, xmax, ymax]], dtype=torch.float32)\n    \n    tgt[\"boxes\"] = bbox\n    tgt[\"masks\"] = rotated_mask\n    return img, tgt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_img, _target = rotation(img, target, 30)\nplotter(_img, _target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_img, _target = rotation(img, target, 90)\nplotter(_img, _target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_img, _target = rotation(img, target, 120)\nplotter(_img, _target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_img, _target = rotation(img, target, 270)\nplotter(_img, _target)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These are just some of the augmentations you can do to possibly get a better result. How you integrate these to Dataset Class you ask? It's not very difficult. I'll leave it as an exercise to the reader! ;)"},{"metadata":{},"cell_type":"markdown","source":"If you want more image+mask augmentations, write in comments and i would try my best to implement them!"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}