{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Exploratory Data Analysis of Pneumothorax dataset\n* [1. Introduction](#section1)  \n* [2. What is Pneumothorax and how can data science help?](#section2)  \n* [3. The dataset and how to decode](#section3)  \n* [4. Exploring the train metadata](#section4)  \n    * [4.1. Gender and Pneumothorax distribution](#section4a)  \n    * [4.2. All patients age distribution](#section4b)\n    * [4.3. The presence of Pneumothorax at particular ages and genders](#section4c)\n    * [4.4. The importance of view positions on an x-ray image](#section4d)\n* [5. The run-length-encoding](#section5)  \n    * [5.1 Mask functions](#section5a)\n* [6. Analyzing the masks](#section6)\n    * [6.1 The affected area and the view position](#section6a)\n    * [6.2 The location of Pneumothorax](#section6b)\n* [7. Conclusion](#section7)\n\n<a class=\"anchor\" id=\"section1\"></a> \n## 1. Introduction \n\nThe idea of using data science to help in identifying diseases is respectable, a really noble goal. This contest is about identifying Pneumothorax disease in chest x-rays. As a junior data scientist, I will present here in detail my curiosity-driven explorations about the dataset and the project, hoping it will be helpful for other juniors.\n\nFirst, we need some domain knowledge.\n\n<a class=\"anchor\" id=\"section2\"></a>\n## 2. What is Pneumothorax and how can data science help? \n\nPneumothorax refers to the presence of air in the pleural space, between the lung and the chest wall. This causes collapsed lung, because thus the dilation of the chest wall would not involve the dilation of the lung. Shortness of breath, pain in the chest can be symptoms, the level of severity can vary. (https://www.mountnittany.org/articles/healthsheets/5473), \n[here](https://www.mayoclinic.org/diseases-conditions/pneumothorax/symptoms-causes/syc-20350367) \n[or here](https://radiopaedia.org/articles/pneumothorax?lang=us) They say, this disease can be identified from chest x-ray images, and here comes in the data science.\n\n<img src=\"https://www.fairview.org/hlimg/krames/344230.jpg\" width=\"300px\">\n\nUsing a dataset with x-ray images and their diagnosis with the exact place of the air in pleura, a model could be trained to recognize Pneumothorax. Additional information about the patients could also be useful when analyzing an x-ray image.\n\n<a class=\"anchor\" id=\"section3\"></a>\n## 3. The dataset and how to decode \n\nOriginally the creators of the contest had the idea of downloading the data from Google Cloud Healthcare API, to learn a bit about accessing real-world data. Here are the [instructions](https://www.kaggle.com/c/siim-acr-pneumothorax-segmentation/overview/siim-cloud-healthcare-api-tutorial). Thanks for [Jesper](https://www.kaggle.com/jesperdramsch), the data is easily accessible [here](https://www.kaggle.com/jesperdramsch/siim-acr-pneumothorax-segmentation-data)\n\nThe main folder contains two subfolders: train and test, and a csv file with the train labels.\nThe train and test folders contain DICOM (Digital Imaging and Communications in Medicine) formatted files, for some reason every single image is embedded in two more folders. We are happy for our data, but new challenges appear: the CSV content seems complicated and the dcm file can not be opened easily.","metadata":{}},{"cell_type":"code","source":"from glob import glob\nimport os\nimport pandas as pd\n\n#checnking the input files\nprint(os.listdir(\"../input/siim-acr-pneumothorax-segmentation-data\"))\n\n#reading all dcm files into train and text\ntrain = sorted(glob(\"../input/siim-acr-pneumothorax-segmentation-data/pneumothorax/dicom-images-train/*/*/*.dcm\"))\ntest = sorted(glob(\"../input/siim-acr-pneumothorax-segmentation-data/pneumothorax/dicom-images-test/*/*/*.dcm\"))\nprint(\"train files: \", len(train))\nprint(\"test files: \", len(test))\n\npd.reset_option('max_colwidth')\n\n#reading the csv\nprint(\"the csv with the labels: -1 means no Pneumothorax, othervise there is an encoding for the place of Pneumothorax\")\nmasks = pd.read_csv(\"../input/siim-acr-pneumothorax-segmentation-data/pneumothorax/train-rle.csv\", delimiter=\",\")\nmasks.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-21T01:47:21.498027Z","iopub.execute_input":"2022-03-21T01:47:21.498421Z","iopub.status.idle":"2022-03-21T01:48:39.454188Z","shell.execute_reply.started":"2022-03-21T01:47:21.498361Z","shell.execute_reply":"2022-03-21T01:48:39.45342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pydicom\nimport matplotlib.pyplot as plt\n\n#displaying the image\nimg = pydicom.read_file(train[0]).pixel_array\nplt.imshow(img, cmap='bone')\nplt.grid(False)\n\n#displaying metadata\ndata = pydicom.dcmread(train[0])\nprint(data)","metadata":{"execution":{"iopub.status.busy":"2022-03-21T01:48:39.455999Z","iopub.execute_input":"2022-03-21T01:48:39.456499Z","iopub.status.idle":"2022-03-21T01:48:39.927985Z","shell.execute_reply.started":"2022-03-21T01:48:39.456449Z","shell.execute_reply":"2022-03-21T01:48:39.927086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a class=\"anchor\" id=\"section4\"></a>\n## 4. Exploring the train metadata ","metadata":{}},{"cell_type":"code","source":"#dataframe to ease the access\npatients = []\nmissing = 0\n\npd.reset_option('max_colwidth')\n\nfor t in train:\n    data = pydicom.dcmread(t)\n    patient = {}\n    patient[\"UID\"] = data.SOPInstanceUID\n    try:\n        encoded_pixels = masks[masks[\"ImageId\"] == patient[\"UID\"]].values[0][1]\n        patient[\"EncodedPixels\"] = encoded_pixels\n    except:\n        missing = missing + 1\n    patient[\"Age\"] = data.PatientAge\n    patient[\"Sex\"] = data.PatientSex\n    patient[\"Modality\"] = data.Modality\n    patient[\"BodyPart\"] = data.BodyPartExamined\n    patient[\"ViewPosition\"] = data.ViewPosition\n    patient[\"path\"] = \"../input/siim-acr-pneumothorax-segmentation-data/pneumothorax/dicom-images-train/\" + data.StudyInstanceUID + \"/\" + data.SeriesInstanceUID + \"/\" + data.SOPInstanceUID + \".dcm\"\n    patients.append(patient)\n\nprint(\"missing labels: \", missing)\n#pd.set_option('display.max_colwidth', -1)\ndf_patients = pd.DataFrame(patients, columns=[\"UID\", \"EncodedPixels\", \"Age\", \"Sex\", \"Modality\", \"BodyPart\", \"ViewPosition\", \"path\"])\nprint(\"images with labels: \", df_patients.shape[0])\ndf_patients.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-21T01:48:39.929159Z","iopub.execute_input":"2022-03-21T01:48:39.929455Z","iopub.status.idle":"2022-03-21T01:50:24.248557Z","shell.execute_reply.started":"2022-03-21T01:48:39.929409Z","shell.execute_reply":"2022-03-21T01:50:24.247794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib as mpl\nimport numpy as np\n\n#gender\nmen = df_patients[df_patients[\"Sex\"] == \"M\"].shape[0]\nwomen = df_patients.shape[0] - men\nprint(men, women)\n\n\n#illness\nhealthy = df_patients[df_patients[\"EncodedPixels\"] == \" -1\"].shape[0]\nill = df_patients.shape[0] - healthy\nprint(healthy, ill)\n\n#gender + illness\nmen_h = df_patients[(df_patients[\"Sex\"] == \"M\") & (df_patients[\"EncodedPixels\"] == \" -1\")].shape[0]\nmen_ill = men - men_h\nwomen_h = df_patients[(df_patients[\"Sex\"] == \"F\") & (df_patients[\"EncodedPixels\"] == \" -1\")].shape[0]\nwomen_ill = women - women_h\nprint(men_h, men_ill, women_h, women_ill)\n\nperc = [str(round(men_ill/107.12, 1)) + \"% \\n ill\", \"healthy \\n\" + str(round(men_h/107.12, 1)) + \"%\", \"healthy \\n\" + str(round(women_h/107.12, 1)) + \"%\",str(round(women_ill/107.12, 1)) + \"% \\n ill\"]\n\nfig, ax = plt.subplots(1, 3, figsize=(15, 5))\n\nfig.suptitle(\"4.1 Gender and Pneumothorax distributions\", fontsize=24, y=1.1)\n\nmpl.rcParams['font.size'] = 12.0\n\n#circle for donut chart\ncircle0 = plt.Circle( (0,0), 0.6, color = 'white')\ncircle1 = plt.Circle( (0,0), 0.4, color = 'white')\ncircle2 = plt.Circle( (0,0), 0.6, color = 'white')\n\n#men women\nax[0].pie([men, women], labels=[\"men\", \"women\"], colors=[\"#42A5F5\", \"#E57373\"], autopct='%1.1f%%', pctdistance=0.8, startangle=90)\nax[0].add_patch(circle0)\nax[0].axis('equal')\n\n#gender healthy\nmypie, _ = ax[2].pie([men, women], radius=1.3, labels=[\"men\", \"women\"], colors=[\"#42A5F5\", \"#E57373\"], startangle=90)\nplt.setp( mypie, width=0.3, edgecolor='white')\n\nmypie2, _ = ax[2].pie([ men_ill, men_h, women_h, women_ill], radius = 1.3 - 0.3, labels=perc, labeldistance=0.61,\n                      colors = [\"#FFB74D\", \"#9CCC65\", \"#9CCC65\", \"#FFB74D\"], startangle=90)\nplt.setp( mypie2, width=0.4, edgecolor='white')\nplt.margins(0,0)\n\n#healthy ill\nax[1].pie([healthy, ill], labels=[\"healthy\", \"ill\"], colors=[\"#9CCC65\", \"#FFB74D\"], autopct='%1.1f%%', pctdistance=0.8, startangle=135)\nax[1].add_patch(circle2)\nax[1].axis('equal')  \n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-21T01:50:24.249965Z","iopub.execute_input":"2022-03-21T01:50:24.250519Z","iopub.status.idle":"2022-03-21T01:50:24.595615Z","shell.execute_reply.started":"2022-03-21T01:50:24.250223Z","shell.execute_reply":"2022-03-21T01:50:24.594746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n#group into bins the same aged men and women with histogram --> all of them and ill of them\n\n#convert he Age column to int\ndf_patients[\"Age\"] = pd.to_numeric(df_patients[\"Age\"])\n\nsorted_ages = np.sort(df_patients[\"Age\"].values)\nprint(sorted_ages)","metadata":{"execution":{"iopub.status.busy":"2022-03-21T01:50:24.602032Z","iopub.execute_input":"2022-03-21T01:50:24.602481Z","iopub.status.idle":"2022-03-21T01:50:24.636228Z","shell.execute_reply.started":"2022-03-21T01:50:24.602316Z","shell.execute_reply":"2022-03-21T01:50:24.6351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nplt.style.use('seaborn-whitegrid')\nplt.figure(figsize=(17, 5))\nplt.hist(sorted_ages[:-2], bins=[i for i in range(100)])\nplt.title(\"4.2 All patients age histogram\", fontsize=18, pad=10)\nplt.xlabel(\"age\", labelpad=10)\nplt.xticks([i*10 for i in range(11)])\nplt.ylabel(\"count\", labelpad=10)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-21T01:50:24.642059Z","iopub.execute_input":"2022-03-21T01:50:24.64248Z","iopub.status.idle":"2022-03-21T01:50:25.666531Z","shell.execute_reply.started":"2022-03-21T01:50:24.642328Z","shell.execute_reply":"2022-03-21T01:50:25.665679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#calculating all and ill men and women histograms\nbins = [i for i in range(100)]\nplt.style.use('seaborn-whitegrid')\n\nall_men = np.histogram(df_patients[df_patients[\"Sex\"] == \"M\"][\"Age\"].values, bins=bins)[0]\nall_women = np.histogram(df_patients[df_patients[\"Sex\"] == \"F\"][\"Age\"].values, bins=bins)[0]\n\nill_men = np.histogram(df_patients[(df_patients[\"Sex\"] == \"M\") & (df_patients[\"EncodedPixels\"] != ' -1')][\"Age\"].values, bins=bins)[0]\nill_women = np.histogram(df_patients[(df_patients[\"Sex\"] == \"F\") & (df_patients[\"EncodedPixels\"] != ' -1')][\"Age\"].values, bins=bins)[0]\n\nfig, axes = plt.subplots(ncols=2, sharey=True, figsize=(17, 16))\n\nfig.suptitle(\"4.3 The presence of Pneumothorax at particular ages and genders\", fontsize=22, y=0.96)\n\naxes[0].margins(x=0.1, y=0.01)\nm1 = axes[0].barh(bins[:-1], all_men, color='#90CAF9')\nm2 = axes[0].barh(bins[:-1], ill_men, color='#0D47A1')\naxes[0].set_title('Men', fontsize=18, pad=15)\naxes[0].invert_xaxis()\naxes[0].set(yticks=[i*5 for i in range(20)])\naxes[0].tick_params(axis=\"y\", labelsize=14)\naxes[0].yaxis.tick_right()\naxes[0].xaxis.tick_top()\naxes[0].legend((m1[0], m2[0]), ('healthy', 'with Pneumothorax'), loc=2, prop={'size': 16})\n\nlocs = axes[0].get_xticks()\n\naxes[1].margins(y=0.01)\nw1 = axes[1].barh(bins[:-1], all_women, color='#EF9A9A')\nw2 = axes[1].barh(bins[:-1], ill_women, color='#B71C1C')\naxes[1].set_title('Women', fontsize=18, pad=15)\naxes[1].xaxis.tick_top()\naxes[1].set_xticks(locs)\naxes[1].legend((w1[0], w2[0]), ('healthy', 'with Pneumothorax'), prop={'size': 17})\n\n#for i, v in enumerate(depos[\"ItemViewCount\"].values):\n   #print(i, v)\n    #axes[1].text(int(v) + 3, int(i)-0.25, str(v))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-21T01:50:25.668383Z","iopub.execute_input":"2022-03-21T01:50:25.668901Z","iopub.status.idle":"2022-03-21T01:50:27.231875Z","shell.execute_reply.started":"2022-03-21T01:50:25.668852Z","shell.execute_reply":"2022-03-21T01:50:27.231144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It is striking that many 16-year-old boys suffer from Pneumothorax, even more than the 64 years old men. The result seems unexpected, but if we read more about the disease, it turns out that the [young, tall and thin men](https://edmonton.ctvnews.ca/tall-thin-young-man-you-could-suffer-from-a-collapsed-lung-1.1234203) are more vulnerable. Young women in their twenties don't seem to be an exception. However this information is based only on a small amount of data.\n\nNext let's check the other extracted attributes: Modality, BodyPart, ViewPosition.","metadata":{}},{"cell_type":"code","source":"bodypart = df_patients[\"BodyPart\"].values\nprint(\"Body parts:\", list(set(bodypart)))\n\nmodality = df_patients[\"Modality\"].values\nprint(\"Modality:\", list(set(modality)))\n\nview = list(df_patients[\"ViewPosition\"].values)\nprint(\"View Positions: \", list(set(view)))\n\npa = view.count(\"PA\")\nap = view.count(\"AP\")\nprint(pa, ap)","metadata":{"execution":{"iopub.status.busy":"2022-03-21T01:50:27.233263Z","iopub.execute_input":"2022-03-21T01:50:27.233768Z","iopub.status.idle":"2022-03-21T01:50:27.248513Z","shell.execute_reply.started":"2022-03-21T01:50:27.233709Z","shell.execute_reply":"2022-03-21T01:50:27.247786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"basic_palette = sns.color_palette()\nplt.style.use('seaborn-whitegrid')\nplt.pie([pa, ap], labels = [\"PA\", \"AP\"], colors=[basic_palette[-2], basic_palette[4]], autopct='%1.1f%%', startangle=70)\nplt.title(\"Occurrences of View positions\", fontsize=16)","metadata":{"execution":{"iopub.status.busy":"2022-03-21T01:50:27.250005Z","iopub.execute_input":"2022-03-21T01:50:27.250303Z","iopub.status.idle":"2022-03-21T01:50:27.347314Z","shell.execute_reply.started":"2022-03-21T01:50:27.250257Z","shell.execute_reply":"2022-03-21T01:50:27.346538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we expected, all the BodyPart fields are chests, the modality is CR, which means Computed Radiography (I think). \n\n<a class=\"anchor\" id=\"section4d\"></a>\n### 4.4 The importance of view positions on an x-ray image \n\nThe view position can be AP or PA. These refer to the way of x-ray in the body, [based on this source](https://www.quora.com/What-is-the-difference-between-an-AP-and-a-PA-view-of-an-X-ray)\n* PA: passes from posterior of the body to anterior --> getting better anterior shadings\n* AP: passes from anterior of the body to posterior --> getting better posterior shaginds\n\n\nThey say usually AP view is used for x-rays, but in the case of the chest x-rays are rather taken from the PA view. If the health level of the patient does not allow to do PA, AP can also help. It would be interesting to check the Pneumothorax severity level in relation to view position too. But first, we have to decode the run-length-encoding (RLE) present int the train csv file. ","metadata":{}},{"cell_type":"markdown","source":"<a class=\"anchor\" id=\"section5\"></a>\n## 5. The run-length-encoding \n\nSo as we could see, the train csv contains the image id and the rle-encodings of the place of Pneumothorax. But what does this mean? Run-length-encoding (RLE) is a simple lossless compression method, it replaces data sequences with identical values (run) with the respective value stored once and the length of the run. It can be useful when the data contains relatively long sequences. \n\nWe have to work with **relative** RLE, which means, that the pixel locations are measured from the previous run. For example, '1 3 10 5' implies pixels 1,2,3 are to be included in the mask, as well as 14,15,16,17,18, according to [Robin Schwob](https://www.kaggle.com/schwobr) in this [discussion](https://www.kaggle.com/c/siim-acr-pneumothorax-segmentation/discussion/98397#latest-567740).\n\n<a class=\"anchor\" id=\"section5a\"></a>\n### 5.1 Mask functions\nFortunately there are decoding and encoding functions inculded to the sample data: mask_functions.py. As I would like to analyze the train data rle labels, I will use only the rle2mask function. Let's check how it is working.","metadata":{}},{"cell_type":"code","source":"#mask functions from sample dataset\nimport numpy as np\n\ndef mask2rle(img, width, height):\n    rle = []\n    lastColor = 0;\n    currentPixel = 0;\n    runStart = -1;\n    runLength = 0;\n\n    for x in range(width):\n        for y in range(height):\n            currentColor = img[x][y]\n            if currentColor != lastColor:\n                if currentColor == 255:\n                    runStart = currentPixel;\n                    runLength = 1;\n                else:\n                    rle.append(str(runStart));\n                    rle.append(str(runLength));\n                    runStart = -1;\n                    runLength = 0;\n                    currentPixel = 0;\n            elif runStart > -1:\n                runLength += 1\n            lastColor = currentColor;\n            currentPixel+=1;\n\n    return \" \".join(rle)\n\ndef rle2mask(rle, width, height):\n    mask= np.zeros(width* height)\n    array = np.asarray([int(x) for x in rle.split()])\n    starts = array[0::2]\n    lengths = array[1::2]\n\n    current_position = 0\n    for index, start in enumerate(starts):\n        current_position += start\n        mask[current_position:current_position+lengths[index]] = 255\n        current_position += lengths[index]\n\n    return mask.reshape(width, height)","metadata":{"execution":{"iopub.status.busy":"2022-03-21T01:50:27.351635Z","iopub.execute_input":"2022-03-21T01:50:27.353969Z","iopub.status.idle":"2022-03-21T01:50:27.372911Z","shell.execute_reply.started":"2022-03-21T01:50:27.353913Z","shell.execute_reply":"2022-03-21T01:50:27.372142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here are two examples of the encoded pixels. The rle2mask function returns a filter which has the same size as the original image. Some rotation and mirroring were needed, to place the mask correctly.","metadata":{}},{"cell_type":"code","source":"df_pneumo = df_patients[df_patients[\"EncodedPixels\"] != ' -1']\n\n#print(df_pneumo.values[3][2], df_pneumo.values[3][3])\n\nmask = rle2mask(df_pneumo.values[3][1], 1024, 1024)\nmask = np.rot90(mask, 3) #rotating three times 90 to the right place\nmask = np.flip(mask, axis=1)\nimg = pydicom.read_file(df_pneumo.values[3][-1]).pixel_array\n\nfig = plt.figure(figsize=(15, 10))\na = fig.add_subplot(1, 3, 1)\nplt.imshow(img, cmap='bone') #original x-ray\na.set_title(\"Original x-ray image\")\nplt.grid(False)\nplt.axis(\"off\")\n\na = fig.add_subplot(1, 3, 2)\nimgplot = plt.imshow(mask, cmap='binary')\n\na.set_title(\"The mask\")\nplt.grid(False)\nplt.xticks([])\nplt.yticks([])\n\na = fig.add_subplot(1, 3, 3)\nplt.imshow(img, cmap='bone')\nplt.imshow(mask, cmap='binary', alpha=0.3)\na.set_title(\"Mask on the x-ray: air in the pleura\")\n\nplt.axis(\"off\")\n\nplt.grid(False)\n\nmask = rle2mask(df_pneumo.values[6][1], 1024, 1024)\nmask = np.rot90(mask, 3) #rotating three times 90 to the right place\nmask = np.flip(mask, axis=1)\nimg = pydicom.read_file(df_pneumo.values[6][-1]).pixel_array\n\nfig = plt.figure(figsize=(15, 10))\na = fig.add_subplot(1, 3, 1)\nplt.imshow(img, cmap='bone') #original x-ray\na.set_title(\"Original x-ray image\")\nplt.grid(False)\nplt.axis(\"off\")\n\na = fig.add_subplot(1, 3, 2)\nimgplot = plt.imshow(mask, cmap='binary')\n\na.set_title(\"The mask\")\nplt.grid(False)\nplt.xticks([])\nplt.yticks([])\n\na = fig.add_subplot(1, 3, 3)\nplt.imshow(img, cmap='bone')\nplt.imshow(mask, cmap='binary', alpha=0.3)\na.set_title(\"Mask on the x-ray: air in the pleura\")\n\nplt.axis(\"off\")\n\nplt.grid(False)","metadata":{"execution":{"iopub.status.busy":"2022-03-21T01:50:27.378232Z","iopub.execute_input":"2022-03-21T01:50:27.381121Z","iopub.status.idle":"2022-03-21T01:50:28.609631Z","shell.execute_reply.started":"2022-03-21T01:50:27.381065Z","shell.execute_reply":"2022-03-21T01:50:28.608852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that we can decode the rle, we can analyze some aspects of the masks separately and together with the metadata. Let's start with the area of the air in the chest, which could measure the severity of the disease.\n\n<a class=\"anchor\" id=\"section6\"></a>\n## 6. Analyzing the masks\n\nAs the masks are created from pixels, we can eaisly count them to calculate the area. As we can see the majority of ill patients suffer from a smaller Pneumothorax, and some patients have a severely collapsed lung. Note: we didn't take in consideration the overlapping masks, just the masked pixels, and at some images the scale may differ too.","metadata":{}},{"cell_type":"code","source":"area = []\npos = []\npa_area = []\nap_area = []\n\nc = 0\n\nfor p in df_pneumo.values:\n    try:\n        mask = rle2mask(p[1], 1024, 1024)\n        pixels = np.count_nonzero(mask)\n        area.append(pixels)\n        pos.append(p[6])\n        if p[6] == \"AP\":\n            ap_area.append(pixels)\n        else:\n            pa_area.append(pixels)\n    except:\n        c = c + 1\n\nprint(\"missing labels\", c)\nprint(\"all area\", np.sort(np.array(area)))\n#print(\"ap area\", np.sort(np.array(ap_area)))\nprint(\"pa area\", np.sort(np.array(pa_area)))","metadata":{"execution":{"iopub.status.busy":"2022-03-21T01:50:28.61107Z","iopub.execute_input":"2022-03-21T01:50:28.611555Z","iopub.status.idle":"2022-03-21T01:50:38.382584Z","shell.execute_reply.started":"2022-03-21T01:50:28.611508Z","shell.execute_reply":"2022-03-21T01:50:38.381176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.style.use('seaborn-whitegrid')\nplt.figure(figsize=(17, 5))\nplt.hist(area, bins=[i*500 for i in range(340)])\nplt.title(\"The affected area by Pneumothorax\", fontsize=18, pad=10)\nplt.xlabel(\"area [pixels]\", labelpad=10)\n#plt.xticks([i*10 for i in range(1000)])\nplt.ylabel(\"count of patient in groups\", labelpad=10)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-21T01:50:38.383988Z","iopub.execute_input":"2022-03-21T01:50:38.384285Z","iopub.status.idle":"2022-03-21T01:50:39.344146Z","shell.execute_reply.started":"2022-03-21T01:50:38.384233Z","shell.execute_reply":"2022-03-21T01:50:39.343364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a class=\"anchor\" id=\"section6a\"></a>\n### 6.1 The affected area and the view position\n\nAs mentioned before, let's see if the \"AP\" positioned x-ray images has a more serious Pneumothorax? On the plots below can be seen that this supposition tend to be true. The x-ray images with the biggest masks are made in AP position, howewer, this view position occurs fewer times than PA.","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(figsize=(17, 5))\nbasic_palette = sns.color_palette()\n\nax1 = plt.subplot2grid((1, 3), (0, 0), colspan=2)\n\nsns.boxplot(x=area, y=pos, palette={\"AP\": basic_palette[4], \"PA\": basic_palette[-2]})#, height=[0.6, 0.4])\nax1.set_xlabel(\"area [pixels]\", fontsize=14, labelpad=10)\nax1.set_ylabel(\"view position\", fontsize=14, labelpad=10)\nax1.set_title(\"6.1 Affected area vs view position\", fontsize=18, pad=10)\n\nax2 = plt.subplot2grid((1, 3), (0, 2))\n\nax2.pie([pa, ap], labels = [\"PA\", \"AP\"], colors=[basic_palette[-2], basic_palette[4]], autopct='%1.1f%%', startangle=70)\nax2.set_title(\"Occurrences of View positions\", fontsize=18)\n","metadata":{"execution":{"iopub.status.busy":"2022-03-21T01:50:39.345483Z","iopub.execute_input":"2022-03-21T01:50:39.345902Z","iopub.status.idle":"2022-03-21T01:50:39.708859Z","shell.execute_reply.started":"2022-03-21T01:50:39.345853Z","shell.execute_reply":"2022-03-21T01:50:39.707936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a class=\"anchor\" id=\"section6b\"></a>\n### 6.2 The location of Pneumothorax\n\nTo get a general insight into the location of Pneumothorax, we can create some \"lung heatmaps\" for  AP and PA views. Of course, every patient is different, thus the size of the lungs and their place on the x-ray image can be different. However, a heatmap may give an overview.","metadata":{}},{"cell_type":"code","source":"c = 0\nap_sum = np.array([[0 for i in range(1024)] for j in range(1024)])\npa_sum = np.array([[0 for i in range(1024)] for j in range(1024)])\n\nap = 0\npa = 0\n\nfor p in df_pneumo.values:\n    try :\n        mask = rle2mask(p[1], 1024, 1024)\n        mask = np.rot90(mask, 3) #rotating three times 90 to the right place\n        mask = np.flip(mask, axis=1)\n        if p[6] == 'AP':\n            ap_sum = ap_sum + mask\n            ap = ap + 1\n        else:\n            pa_sum = pa_sum + mask\n            pa = pa + 1\n    except:\n        c = c + 1","metadata":{"execution":{"iopub.status.busy":"2022-03-21T01:50:39.71366Z","iopub.execute_input":"2022-03-21T01:50:39.714106Z","iopub.status.idle":"2022-03-21T01:50:54.68665Z","shell.execute_reply.started":"2022-03-21T01:50:39.713945Z","shell.execute_reply":"2022-03-21T01:50:54.685842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In this kernel I implement Unet Plus Plus with EfficientNet Encoder. Unet Plus Plus introduce intermediate layers to skip connections of U-Net, which naturally form multiple new up-sampling paths from different depths, ensembling U-Nets of various receptive fields. This results in far better performance than traditional Unet. \nFor more details please [refer] .( \"UNet++: A Nested U-Net Architecture for Medical Image Segmentation\" )\n\n- I have made some changes to the augmentation part. \n- I have added TTA.\n","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(figsize=(17, 5))\nbasic_palette = sns.color_palette()\n\nax1 = plt.subplot2grid((1, 3), (0, 0))\nax1.imshow(pa_sum, cmap='magma_r')\nax1.set_title(\"All PA positioned masks\", fontsize=18, pad=15)\n\n#colorbar\nmaxval = np.max(pa_sum)\ncmap = plt.get_cmap('magma_r', maxval)\n\nnorm = mpl.colors.Normalize()\nsm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\nsm.set_array([])\n\nticks=[1/(maxval+1)/2, 0.5, 1-1/(maxval+1)/2]\n\ncb_1 = plt.colorbar(sm,ticks=[0, 1], fraction=0.046, ax=ax1)#ticks and boundaries\ncb_1.ax.set_yticklabels([\"no Pneu\", str(int(maxval))]) #label of colormap\ncb_1.ax.yaxis.set_label_position('left')\n\nplt.grid(False)\nplt.xticks([])\nplt.yticks([])\n\nax2 = plt.subplot2grid((1, 3), (0, 1))\nax2.pie([pa, ap], labels = [\"PA\", \"AP\"], colors=[basic_palette[-2], basic_palette[4]], autopct='%1.1f%%', startangle=70)\nax2.set_title(\"Occurrences of View positions\", fontsize=18)\n\nax3 = plt.subplot2grid((1, 3), (0, 2))\nax3.imshow(ap_sum, cmap='magma_r')\nax3.set_title(\"All AP positioned masks\", fontsize=18, pad=15)\n\nmaxval = np.max(ap_sum)\ncmap2 = plt.get_cmap('magma_r', maxval)\n\nnorm = mpl.colors.Normalize()\nsm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\nsm.set_array([])\n\ncb_2 = plt.colorbar(sm,ticks=[0, 1], fraction=0.046, ax=ax3)#ticks and boundaries\ncb_2.ax.set_yticklabels([\"no Pneu\", str(int(maxval))]) #label of colormap\ncb_2.ax.yaxis.set_label_position('left')\n\nplt.grid(False)\nplt.xticks([])\nplt.yticks([])","metadata":{"execution":{"iopub.status.busy":"2022-03-21T01:50:54.688274Z","iopub.execute_input":"2022-03-21T01:50:54.688707Z","iopub.status.idle":"2022-03-21T01:50:55.488789Z","shell.execute_reply.started":"2022-03-21T01:50:54.688657Z","shell.execute_reply":"2022-03-21T01:50:55.48805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It seems that in both cases the top left corner of the lung is the most common location. In the PA cases, the x-ray images create a cleaner view, the lungs are outlined, while the general lungs of AP cases are noisier and have orange tone. From the colorbar can be read the maximum number of overlapped pixels.\n\n<a class=\"anchor\" id=\"section7\"></a>\n## 7. Conclusion\n\nSo these were some brief explorations to get a bit more insight into the dataset and its metadata. We have seen the gender and age distributions of healthy and ill people, we discovered more about the x-ray image view positions and about the affected areas. Hopefully, it was useful to understand a bit the nature of the disease and of the affected people.","metadata":{}},{"cell_type":"markdown","source":"# Loading Libraries","metadata":{"_uuid":"1c5ef627f3327353ea3df4223e85d9eebd1e598c"}},{"cell_type":"code","source":"!pip install albumentations > /dev/null\n!pip install -U efficientnet==0.0.4\nimport numpy as np\nimport pandas as pd\nimport gc\nimport keras\n\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-white')\nimport seaborn as sns\nsns.set_style(\"white\")\n\nfrom sklearn.model_selection import train_test_split,StratifiedKFold\n\nfrom skimage.transform import resize\nimport tensorflow as tf\nimport keras.backend as K\nfrom keras.losses import binary_crossentropy\n\nfrom keras.preprocessing.image import load_img\nfrom keras import Model\nfrom keras.callbacks import  ModelCheckpoint\nfrom keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, concatenate, Dropout,BatchNormalization\nfrom keras.layers import Conv2D, Concatenate, MaxPooling2D\nfrom keras.layers import UpSampling2D, Dropout, BatchNormalization\nfrom tqdm import tqdm_notebook\nfrom keras import initializers\nfrom keras import regularizers\nfrom keras import constraints\nfrom keras.utils import conv_utils\nfrom keras.utils.data_utils import get_file\nfrom keras.engine.topology import get_source_inputs\nfrom keras.engine import InputSpec\nfrom keras import backend as K\nfrom keras.layers import LeakyReLU\nfrom keras.layers import ZeroPadding2D\nfrom keras.losses import binary_crossentropy\nimport keras.callbacks as callbacks\nfrom keras.callbacks import Callback\nfrom keras.applications.xception import Xception\nfrom keras.layers import multiply\n\n\nfrom keras import optimizers\nfrom keras.legacy import interfaces\nfrom keras.utils.generic_utils import get_custom_objects\n\nfrom keras.engine.topology import Input\nfrom keras.engine.training import Model\nfrom keras.layers.convolutional import Conv2D, UpSampling2D, Conv2DTranspose\nfrom keras.layers.core import Activation, SpatialDropout2D\nfrom keras.layers.merge import concatenate\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers.pooling import MaxPooling2D\nfrom keras.layers import Input,Dropout,BatchNormalization,Activation,Add\nfrom keras.regularizers import l2\nfrom keras.layers.core import Dense, Lambda\nfrom keras.layers.merge import concatenate, add\nfrom keras.layers import GlobalAveragePooling2D, Reshape, Dense, multiply, Permute\nfrom keras.optimizers import SGD\nfrom keras.preprocessing.image import ImageDataGenerator\n\nimport glob\nimport shutil\nimport os\nimport random\nfrom PIL import Image\n\nseed = 10\nnp.random.seed(seed)\nrandom.seed(seed)\nos.environ['PYTHONHASHSEED'] = str(seed)\nnp.random.seed(seed)\ntf.set_random_seed(seed)\n    \n%matplotlib inline","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2022-03-21T01:50:55.490285Z","iopub.execute_input":"2022-03-21T01:50:55.490761Z","iopub.status.idle":"2022-03-21T01:51:12.837456Z","shell.execute_reply.started":"2022-03-21T01:50:55.490716Z","shell.execute_reply":"2022-03-21T01:51:12.8367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading Training Set Data\nAs mentioned earlier, I don't have international credit card so, I am using the data from this [kernel](https://www.kaggle.com/iafoss/data-repack-and-image-statistics)","metadata":{}},{"cell_type":"code","source":"!mkdir masks\n!unzip -q ../input/data-repack-and-image-statistics/masks.zip -d masks \n!mkdir train\n!unzip -q ../input/data-repack-and-image-statistics/train.zip -d train \n!mkdir test\n!unzip -q ../input/data-repack-and-image-statistics/test.zip -d test ","metadata":{"execution":{"iopub.status.busy":"2022-03-21T01:51:12.839187Z","iopub.execute_input":"2022-03-21T01:51:12.839659Z","iopub.status.idle":"2022-03-21T01:51:27.398139Z","shell.execute_reply.started":"2022-03-21T01:51:12.839603Z","shell.execute_reply":"2022-03-21T01:51:27.397283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pneumothorax as percentage of mask","metadata":{}},{"cell_type":"code","source":"all_mask_fn = glob.glob('./masks/*')\nmask_df = pd.DataFrame()\nmask_df['file_names'] = all_mask_fn\nmask_df['mask_percentage'] = 0\nmask_df.set_index('file_names',inplace=True)\nfor fn in all_mask_fn:\n    mask_df.loc[fn,'mask_percentage'] = np.array(Image.open(fn)).sum()/(256*256*255) #255 is bcz img range is 255\n    \nmask_df.reset_index(inplace=True)\nsns.distplot(mask_df.mask_percentage)\nmask_df['labels'] = 0\nmask_df.loc[mask_df.mask_percentage>0,'labels'] = 1","metadata":{"execution":{"iopub.status.busy":"2022-03-21T01:51:27.401536Z","iopub.execute_input":"2022-03-21T01:51:27.401809Z","iopub.status.idle":"2022-03-21T01:51:37.501264Z","shell.execute_reply.started":"2022-03-21T01:51:27.40176Z","shell.execute_reply":"2022-03-21T01:51:37.500509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_train_fn = glob.glob('./train/*')\ntotal_samples = len(all_train_fn)\nidx = np.arange(total_samples)\ntrain_fn,val_fn = train_test_split(all_train_fn,stratify=mask_df.labels,test_size=0.30,random_state=10)\n\nprint('No. of train files:', len(train_fn))\nprint('No. of val files:', len(val_fn))\n\nmasks_train_fn = [fn.replace('./train','./masks') for fn in train_fn]    \nmasks_val_fn = [fn.replace('./train','./masks') for fn in val_fn]","metadata":{"execution":{"iopub.status.busy":"2022-03-21T01:51:37.505361Z","iopub.execute_input":"2022-03-21T01:51:37.507658Z","iopub.status.idle":"2022-03-21T01:51:37.649677Z","shell.execute_reply.started":"2022-03-21T01:51:37.5076Z","shell.execute_reply":"2022-03-21T01:51:37.648972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir ./keras_im_train\ntrain_dir = './keras_im_train'\nfor full_fn in train_fn:\n    fn = full_fn.split('/')[-1]\n    shutil.move(full_fn,os.path.join(train_dir,fn))\n    \n!mkdir ./keras_mask_train\ntrain_dir = './keras_mask_train'\nfor full_fn in masks_train_fn:\n    fn = full_fn.split('/')[-1]\n    shutil.move(full_fn,os.path.join(train_dir,fn))\n    \n!mkdir ./keras_im_val\ntrain_dir = './keras_im_val'\nfor full_fn in val_fn:\n    fn = full_fn.split('/')[-1]\n    shutil.move(full_fn,os.path.join(train_dir,fn))\n    \n!mkdir ./keras_mask_val\ntrain_dir = './keras_mask_val'\nfor full_fn in masks_val_fn:\n    fn = full_fn.split('/')[-1]\n    shutil.move(full_fn,os.path.join(train_dir,fn))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-21T01:51:37.653646Z","iopub.execute_input":"2022-03-21T01:51:37.65575Z","iopub.status.idle":"2022-03-21T01:51:41.09474Z","shell.execute_reply.started":"2022-03-21T01:51:37.655686Z","shell.execute_reply":"2022-03-21T01:51:41.09372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_im_path,train_mask_path = './keras_im_train','./keras_mask_train'\nh,w,batch_size = 256,256,16\n\nval_im_path,val_mask_path = './keras_im_val','./keras_mask_val'\n\nclass DataGenerator(keras.utils.Sequence):\n    'Generates data for Keras'\n    def __init__(self, train_im_path=train_im_path,train_mask_path=train_mask_path,\n                 augmentations=None, batch_size=batch_size,img_size=256, n_channels=3, shuffle=True):\n        'Initialization'\n        self.batch_size = batch_size\n        self.train_im_paths = glob.glob(train_im_path+'/*')\n        \n        self.train_im_path = train_im_path\n        self.train_mask_path = train_mask_path\n\n        self.img_size = img_size\n        \n        self.n_channels = n_channels\n        self.shuffle = shuffle\n        self.augment = augmentations\n        self.on_epoch_end()\n\n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        return int(np.ceil(len(self.train_im_paths) / self.batch_size))\n\n    def __getitem__(self, index):\n        'Generate one batch of data'\n        # Generate indexes of the batch\n        indexes = self.indexes[index*self.batch_size:min((index+1)*self.batch_size,len(self.train_im_paths))]\n\n        # Find list of IDs\n        list_IDs_im = [self.train_im_paths[k] for k in indexes]\n\n        # Generate data\n        X, y = self.data_generation(list_IDs_im)\n\n        if self.augment is None:\n            return X,np.array(y)/255\n        else:            \n            im,mask = [],[]   \n            for x,y in zip(X,y):\n                augmented = self.augment(image=x, mask=y)\n                im.append(augmented['image'])\n                mask.append(augmented['mask'])\n            return np.array(im),np.array(mask)/255\n\n    def on_epoch_end(self):\n        'Updates indexes after each epoch'\n        self.indexes = np.arange(len(self.train_im_paths))\n        if self.shuffle == True:\n            np.random.shuffle(self.indexes)\n\n    def data_generation(self, list_IDs_im):\n        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n        # Initialization\n        X = np.empty((len(list_IDs_im),self.img_size,self.img_size, self.n_channels))\n        y = np.empty((len(list_IDs_im),self.img_size,self.img_size, 1))\n\n        # Generate data\n        for i, im_path in enumerate(list_IDs_im):\n            \n            im = np.array(Image.open(im_path))\n            mask_path = im_path.replace(self.train_im_path,self.train_mask_path)\n            \n            mask = np.array(Image.open(mask_path))\n            \n            \n            if len(im.shape)==2:\n                im = np.repeat(im[...,None],3,2)\n\n#             # Resize sample\n            X[i,] = cv2.resize(im,(self.img_size,self.img_size))\n\n            # Store class\n            y[i,] = cv2.resize(mask,(self.img_size,self.img_size))[..., np.newaxis]\n            y[y>0] = 255\n\n        return np.uint8(X),np.uint8(y)","metadata":{"execution":{"iopub.status.busy":"2022-03-21T01:51:41.096547Z","iopub.execute_input":"2022-03-21T01:51:41.096856Z","iopub.status.idle":"2022-03-21T01:51:41.120084Z","shell.execute_reply.started":"2022-03-21T01:51:41.096806Z","shell.execute_reply":"2022-03-21T01:51:41.119116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2\nfrom albumentations import (\n    Compose, HorizontalFlip, CLAHE, HueSaturationValue,\n    RandomBrightness, RandomContrast, RandomGamma,OneOf,\n    ToFloat, ShiftScaleRotate,GridDistortion, ElasticTransform, JpegCompression, HueSaturationValue,\n    RGBShift, RandomBrightness, RandomContrast, Blur, MotionBlur, MedianBlur, GaussNoise,CenterCrop,\n    IAAAdditiveGaussianNoise,GaussNoise,OpticalDistortion,RandomSizedCrop\n)\n\nAUGMENTATIONS_TRAIN = Compose([\n    HorizontalFlip(p=0.5),\n    OneOf([\n        RandomContrast(),\n        RandomGamma(),\n        RandomBrightness(),\n         ], p=0.3),\n    OneOf([\n        ElasticTransform(alpha=120, sigma=120 * 0.05, alpha_affine=120 * 0.03),\n        GridDistortion(),\n        OpticalDistortion(distort_limit=2, shift_limit=0.5),\n        ], p=0.3),\n    RandomSizedCrop(min_max_height=(176, 256), height=h, width=w,p=0.25),\n    ToFloat(max_value=1)\n],p=1)\n\n\nAUGMENTATIONS_TEST = Compose([\n    ToFloat(max_value=1)\n],p=1)","metadata":{"execution":{"iopub.status.busy":"2022-03-21T01:51:41.121468Z","iopub.execute_input":"2022-03-21T01:51:41.121903Z","iopub.status.idle":"2022-03-21T01:51:41.320144Z","shell.execute_reply.started":"2022-03-21T01:51:41.121858Z","shell.execute_reply":"2022-03-21T01:51:41.319497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train Set Images with Masks","metadata":{}},{"cell_type":"code","source":"a = DataGenerator(batch_size=64,shuffle=False)\nimages,masks = a.__getitem__(0)\nmax_images = 64\ngrid_width = 16\ngrid_height = int(max_images / grid_width)\nfig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width, grid_height))\n\nfor i,(im, mask) in enumerate(zip(images,masks)):\n    ax = axs[int(i / grid_width), i % grid_width]\n    ax.imshow(im.squeeze(), cmap=\"bone\")\n    ax.imshow(mask.squeeze(), alpha=0.5, cmap=\"Reds\")    \n    ax.axis('off')\nplt.suptitle(\"Chest X-rays, Red: Pneumothorax.\")","metadata":{"execution":{"iopub.status.busy":"2022-03-21T01:51:41.321791Z","iopub.execute_input":"2022-03-21T01:51:41.322289Z","iopub.status.idle":"2022-03-21T01:51:45.052841Z","shell.execute_reply.started":"2022-03-21T01:51:41.322077Z","shell.execute_reply":"2022-03-21T01:51:45.052161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Images after Augmentations","metadata":{}},{"cell_type":"code","source":"a = DataGenerator(batch_size=64,augmentations=AUGMENTATIONS_TRAIN,shuffle=False)\nimages,masks = a.__getitem__(0)\nmax_images = 64\ngrid_width = 16\ngrid_height = int(max_images / grid_width)\nfig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width, grid_height))\n\nfor i,(im, mask) in enumerate(zip(images,masks)):\n    ax = axs[int(i / grid_width), i % grid_width]\n    ax.imshow(im[:,:,0], cmap=\"bone\")\n    ax.imshow(mask.squeeze(), alpha=0.5, cmap=\"Reds\")    \n    ax.axis('off')\nplt.suptitle(\"Chest X-rays, Red: Pneumothorax.\")","metadata":{"execution":{"iopub.status.busy":"2022-03-21T01:51:45.054226Z","iopub.execute_input":"2022-03-21T01:51:45.054686Z","iopub.status.idle":"2022-03-21T01:51:49.606908Z","shell.execute_reply.started":"2022-03-21T01:51:45.054641Z","shell.execute_reply":"2022-03-21T01:51:49.606156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Calculating IOU","metadata":{"_uuid":"7e07a9adca04095f690733343ab30cdc8f3f95a2"}},{"cell_type":"code","source":"# https://www.kaggle.com/cpmpml/fast-iou-metric-in-numpy-and-tensorflow\ndef get_iou_vector(A, B):\n    # Numpy version    \n    batch_size = A.shape[0]\n    metric = 0.0\n    for batch in range(batch_size):\n        t, p = A[batch], B[batch]\n        true = np.sum(t)\n        pred = np.sum(p)\n        \n        # deal with empty mask first\n        if true == 0:\n            metric += (pred == 0)\n            continue\n        \n        # non empty mask case.  Union is never empty \n        # hence it is safe to divide by its number of pixels\n        intersection = np.sum(t * p)\n        union = true + pred - intersection\n        iou = intersection / union\n        \n        # iou metrric is a stepwise approximation of the real iou over 0.5\n        iou = np.floor(max(0, (iou - 0.45)*20)) / 10\n        \n        metric += iou\n        \n    # teake the average over all images in batch\n    metric /= batch_size\n    return metric\n\n\ndef my_iou_metric(label, pred):\n    # Tensorflow version\n    return tf.py_func(get_iou_vector, [label, pred > 0.5], tf.float64)","metadata":{"_uuid":"45410fdbd533d546a578d9dc29982a1657b8dfa9","execution":{"iopub.status.busy":"2022-03-21T01:51:49.608347Z","iopub.execute_input":"2022-03-21T01:51:49.60879Z","iopub.status.idle":"2022-03-21T01:51:49.619449Z","shell.execute_reply.started":"2022-03-21T01:51:49.608745Z","shell.execute_reply":"2022-03-21T01:51:49.618723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Defining Dice Loss","metadata":{}},{"cell_type":"code","source":"def dice_coef(y_true, y_pred):\n    y_true_f = K.flatten(y_true)\n    y_pred = K.cast(y_pred, 'float32')\n    y_pred_f = K.cast(K.greater(K.flatten(y_pred), 0.5), 'float32')\n    intersection = y_true_f * y_pred_f\n    score = 2. * K.sum(intersection) / (K.sum(y_true_f) + K.sum(y_pred_f))\n    return score\n\ndef dice_loss(y_true, y_pred):\n    smooth = 1.\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = y_true_f * y_pred_f\n    score = (2. * K.sum(intersection) + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n    return 1. - score\n\ndef bce_dice_loss(y_true, y_pred):\n    return binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n\ndef bce_logdice_loss(y_true, y_pred):\n    return binary_crossentropy(y_true, y_pred) - K.log(1. - dice_loss(y_true, y_pred))","metadata":{"_uuid":"d88d952ca7c27fd0e2411eb63ec16fd25cb8ebec","execution":{"iopub.status.busy":"2022-03-21T01:51:49.620912Z","iopub.execute_input":"2022-03-21T01:51:49.621447Z","iopub.status.idle":"2022-03-21T01:51:49.634955Z","shell.execute_reply.started":"2022-03-21T01:51:49.621398Z","shell.execute_reply":"2022-03-21T01:51:49.634293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Define Learning Rate Scheduler\nusing cosine annealing for learning rate","metadata":{}},{"cell_type":"code","source":"class SnapshotCallbackBuilder:\n    def __init__(self, nb_epochs, nb_snapshots, init_lr=0.1):\n        self.T = nb_epochs\n        self.M = nb_snapshots\n        self.alpha_zero = init_lr\n\n    def get_callbacks(self, model_prefix='Model'):\n\n        callback_list = [\n            callbacks.ModelCheckpoint(\"./keras.model\",monitor='val_my_iou_metric', \n                                   mode = 'max', save_best_only=True, verbose=1),\n            swa,\n            callbacks.LearningRateScheduler(schedule=self._cosine_anneal_schedule)\n        ]\n\n        return callback_list\n\n    def _cosine_anneal_schedule(self, t):\n        cos_inner = np.pi * (t % (self.T // self.M))  # t - 1 is used when t has 1-based indexing.\n        cos_inner /= self.T // self.M\n        cos_out = np.cos(cos_inner) + 1\n        return float(self.alpha_zero / 2 * cos_out)","metadata":{"_uuid":"98c8b0aff0989293ce43592a82eb3c5dcf8bbee9","execution":{"iopub.status.busy":"2022-03-21T01:51:49.638484Z","iopub.execute_input":"2022-03-21T01:51:49.63876Z","iopub.status.idle":"2022-03-21T01:51:49.650186Z","shell.execute_reply.started":"2022-03-21T01:51:49.638698Z","shell.execute_reply":"2022-03-21T01:51:49.649069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Useful Model Blocks","metadata":{"_uuid":"dc9e6a9ab02ceeb1cb2491568870ab1bd881b5f2"}},{"cell_type":"code","source":"def convolution_block(x, filters, size, strides=(1,1), padding='same', activation=True):\n    x = Conv2D(filters, size, strides=strides, padding=padding)(x)\n    x = BatchNormalization()(x)\n    if activation == True:\n        x = LeakyReLU(alpha=0.1)(x)\n    return x\n\ndef residual_block(blockInput, num_filters=16):\n    x = LeakyReLU(alpha=0.1)(blockInput)\n    x = BatchNormalization()(x)\n    blockInput = BatchNormalization()(blockInput)\n    x = convolution_block(x, num_filters, (3,3) )\n    x = convolution_block(x, num_filters, (3,3), activation=False)\n    x = Add()([x, blockInput])\n    return x","metadata":{"_uuid":"a8e87341f77e48a6c54de11f8dc72f50f43b8637","execution":{"iopub.status.busy":"2022-03-21T01:51:49.651882Z","iopub.execute_input":"2022-03-21T01:51:49.652171Z","iopub.status.idle":"2022-03-21T01:51:49.662363Z","shell.execute_reply.started":"2022-03-21T01:51:49.652124Z","shell.execute_reply":"2022-03-21T01:51:49.66137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Defining UEfficientNet Model","metadata":{"_uuid":"8f464edae7c4c308bc70dbc50140ee1c6dee6470"}},{"cell_type":"markdown","source":"![image](https://miro.medium.com/max/1316/1*ExIkm6cImpPgpetFW1kwyQ.png)","metadata":{}},{"cell_type":"markdown","source":"As mentioned above, this model uses pretrained EfficientNetB4 model as encoder. I use Residual blocks in the decoder part.","metadata":{"_uuid":"dd410e24ea4348c42c272e9bcbda0ffee316092c"}},{"cell_type":"code","source":"from efficientnet import EfficientNetB4\n\ndef UEfficientNet(input_shape=(None, None, 3),dropout_rate=0.1):\n\n    backbone = EfficientNetB4(weights='imagenet',\n                            include_top=False,\n                            input_shape=input_shape)\n    input = backbone.input\n    start_neurons = 8\n\n    conv4 = backbone.layers[342].output\n    conv4 = LeakyReLU(alpha=0.1)(conv4)\n    pool4 = MaxPooling2D((2, 2))(conv4)\n    pool4 = Dropout(dropout_rate)(pool4)\n    \n     # Middle\n    convm = Conv2D(start_neurons * 32, (3, 3), activation=None, padding=\"same\",name='conv_middle')(pool4)\n    convm = residual_block(convm,start_neurons * 32)\n    convm = residual_block(convm,start_neurons * 32)\n    convm = LeakyReLU(alpha=0.1)(convm)\n    \n    deconv4 = Conv2DTranspose(start_neurons * 16, (3, 3), strides=(2, 2), padding=\"same\")(convm)\n    deconv4_up1 = Conv2DTranspose(start_neurons * 16, (3, 3), strides=(2, 2), padding=\"same\")(deconv4)\n    deconv4_up2 = Conv2DTranspose(start_neurons * 16, (3, 3), strides=(2, 2), padding=\"same\")(deconv4_up1)\n    deconv4_up3 = Conv2DTranspose(start_neurons * 16, (3, 3), strides=(2, 2), padding=\"same\")(deconv4_up2)\n    uconv4 = concatenate([deconv4, conv4])\n    uconv4 = Dropout(dropout_rate)(uconv4) \n    \n    uconv4 = Conv2D(start_neurons * 16, (3, 3), activation=None, padding=\"same\")(uconv4)\n    uconv4 = residual_block(uconv4,start_neurons * 16)\n#     uconv4 = residual_block(uconv4,start_neurons * 16)\n    uconv4 = LeakyReLU(alpha=0.1)(uconv4)  #conv1_2\n    \n    deconv3 = Conv2DTranspose(start_neurons * 8, (3, 3), strides=(2, 2), padding=\"same\")(uconv4)\n    deconv3_up1 = Conv2DTranspose(start_neurons * 8, (3, 3), strides=(2, 2), padding=\"same\")(deconv3)\n    deconv3_up2 = Conv2DTranspose(start_neurons * 8, (3, 3), strides=(2, 2), padding=\"same\")(deconv3_up1)\n    conv3 = backbone.layers[154].output\n    uconv3 = concatenate([deconv3,deconv4_up1, conv3])    \n    uconv3 = Dropout(dropout_rate)(uconv3)\n    \n    uconv3 = Conv2D(start_neurons * 8, (3, 3), activation=None, padding=\"same\")(uconv3)\n    uconv3 = residual_block(uconv3,start_neurons * 8)\n#     uconv3 = residual_block(uconv3,start_neurons * 8)\n    uconv3 = LeakyReLU(alpha=0.1)(uconv3)\n\n    deconv2 = Conv2DTranspose(start_neurons * 4, (3, 3), strides=(2, 2), padding=\"same\")(uconv3)\n    deconv2_up1 = Conv2DTranspose(start_neurons * 4, (3, 3), strides=(2, 2), padding=\"same\")(deconv2)\n    conv2 = backbone.layers[92].output\n    uconv2 = concatenate([deconv2,deconv3_up1,deconv4_up2, conv2])\n        \n    uconv2 = Dropout(0.1)(uconv2)\n    uconv2 = Conv2D(start_neurons * 4, (3, 3), activation=None, padding=\"same\")(uconv2)\n    uconv2 = residual_block(uconv2,start_neurons * 4)\n#     uconv2 = residual_block(uconv2,start_neurons * 4)\n    uconv2 = LeakyReLU(alpha=0.1)(uconv2)\n    \n    deconv1 = Conv2DTranspose(start_neurons * 2, (3, 3), strides=(2, 2), padding=\"same\")(uconv2)\n    conv1 = backbone.layers[30].output\n    uconv1 = concatenate([deconv1,deconv2_up1,deconv3_up2,deconv4_up3, conv1])\n    \n    uconv1 = Dropout(0.1)(uconv1)\n    uconv1 = Conv2D(start_neurons * 2, (3, 3), activation=None, padding=\"same\")(uconv1)\n    uconv1 = residual_block(uconv1,start_neurons * 2)\n#     uconv1 = residual_block(uconv1,start_neurons * 2)\n    uconv1 = LeakyReLU(alpha=0.1)(uconv1)\n    \n    uconv0 = Conv2DTranspose(start_neurons * 1, (3, 3), strides=(2, 2), padding=\"same\")(uconv1)   \n    uconv0 = Dropout(0.1)(uconv0)\n    uconv0 = Conv2D(start_neurons * 1, (3, 3), activation=None, padding=\"same\")(uconv0)\n    uconv0 = residual_block(uconv0,start_neurons * 1)\n#     uconv0 = residual_block(uconv0,start_neurons * 1)\n    uconv0 = LeakyReLU(alpha=0.1)(uconv0)\n    \n    uconv0 = Dropout(dropout_rate/2)(uconv0)\n    output_layer = Conv2D(1, (1,1), padding=\"same\", activation=\"sigmoid\")(uconv0)    \n    \n    model = Model(input, output_layer)\n    model.name = 'u-xception'\n\n    return model","metadata":{"_uuid":"95fee2ea26eb7d84ebbf325a2de0128814675e7c","execution":{"iopub.status.busy":"2022-03-21T01:51:49.663835Z","iopub.execute_input":"2022-03-21T01:51:49.664338Z","iopub.status.idle":"2022-03-21T01:51:49.697568Z","shell.execute_reply.started":"2022-03-21T01:51:49.664288Z","shell.execute_reply":"2022-03-21T01:51:49.696768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"K.clear_session()\nimg_size = 256\nmodel = UEfficientNet(input_shape=(img_size,img_size,3),dropout_rate=0.5)","metadata":{"_uuid":"2382b088c8a6be16490354ebd386120a9ced414d","execution":{"iopub.status.busy":"2022-03-21T01:51:49.699254Z","iopub.execute_input":"2022-03-21T01:51:49.699699Z","iopub.status.idle":"2022-03-21T01:52:15.758313Z","shell.execute_reply.started":"2022-03-21T01:51:49.699639Z","shell.execute_reply":"2022-03-21T01:52:15.757441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-03-21T01:52:15.763768Z","iopub.execute_input":"2022-03-21T01:52:15.764028Z","iopub.status.idle":"2022-03-21T01:52:15.853069Z","shell.execute_reply.started":"2022-03-21T01:52:15.763983Z","shell.execute_reply":"2022-03-21T01:52:15.8524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Stochastic Weight Averaging\nI have found SWA to give better results. Please check out the paper for more info.","metadata":{}},{"cell_type":"code","source":"class SWA(keras.callbacks.Callback):\n    \n    def __init__(self, filepath, swa_epoch):\n        super(SWA, self).__init__()\n        self.filepath = filepath\n        self.swa_epoch = swa_epoch \n    \n    def on_train_begin(self, logs=None):\n        self.nb_epoch = self.params['epochs']\n        print('Stochastic weight averaging selected for last {} epochs.'\n              .format(self.nb_epoch - self.swa_epoch))\n        \n    def on_epoch_end(self, epoch, logs=None):\n        \n        if epoch == self.swa_epoch:\n            self.swa_weights = self.model.get_weights()\n            \n        elif epoch > self.swa_epoch:    \n            for i in range(len(self.swa_weights)):\n                self.swa_weights[i] = (self.swa_weights[i] * \n                    (epoch - self.swa_epoch) + self.model.get_weights()[i])/((epoch - self.swa_epoch)  + 1)  \n\n        else:\n            pass\n        \n    def on_train_end(self, logs=None):\n        self.model.set_weights(self.swa_weights)\n        print('Final model parameters set to stochastic weight average.')\n        self.model.save_weights(self.filepath)\n        print('Final stochastic averaged weights saved to file.')","metadata":{"_uuid":"3d7a02b2dca5074cbd7025947d031b134a17ff54","execution":{"iopub.status.busy":"2022-03-21T01:52:15.857329Z","iopub.execute_input":"2022-03-21T01:52:15.857562Z","iopub.status.idle":"2022-03-21T01:52:15.87333Z","shell.execute_reply.started":"2022-03-21T01:52:15.857521Z","shell.execute_reply":"2022-03-21T01:52:15.87244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(loss=bce_dice_loss, optimizer='adam', metrics=[my_iou_metric])","metadata":{"execution":{"iopub.status.busy":"2022-03-21T01:52:15.87792Z","iopub.execute_input":"2022-03-21T01:52:15.878158Z","iopub.status.idle":"2022-03-21T01:52:15.996548Z","shell.execute_reply.started":"2022-03-21T01:52:15.878112Z","shell.execute_reply":"2022-03-21T01:52:15.995811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training Begins","metadata":{}},{"cell_type":"code","source":"epochs = 70\nsnapshot = SnapshotCallbackBuilder(nb_epochs=epochs,nb_snapshots=1,init_lr=1e-3)\nbatch_size = 16\nswa = SWA('./keras_swa.model',67)\nvalid_im_path,valid_mask_path = './keras_im_val','./keras_mask_val'\n# Generators\ntraining_generator = DataGenerator(augmentations=AUGMENTATIONS_TRAIN,img_size=img_size)\nvalidation_generator = DataGenerator(train_im_path = valid_im_path ,\n                                     train_mask_path=valid_mask_path,augmentations=AUGMENTATIONS_TEST,\n                                     img_size=img_size)\n\nhistory = model.fit_generator(generator=training_generator,\n                            validation_data=validation_generator,                            \n                            use_multiprocessing=False,\n                            epochs=epochs,verbose=2,\n                            callbacks=snapshot.get_callbacks())","metadata":{"_uuid":"f1773642758da7b4480e0e48c045bd01ea3684ae","scrolled":true,"execution":{"iopub.status.busy":"2022-03-21T02:01:37.608135Z","iopub.execute_input":"2022-03-21T02:01:37.60849Z","iopub.status.idle":"2022-03-21T09:14:52.632086Z","shell.execute_reply.started":"2022-03-21T02:01:37.608432Z","shell.execute_reply":"2022-03-21T09:14:52.631268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(16,4))\nplt.subplot(1,2,1)\nplt.plot(history.history['my_iou_metric'][1:])\nplt.plot(history.history['val_my_iou_metric'][1:])\nplt.ylabel('iou')\nplt.xlabel('epoch')\nplt.legend(['train','Validation'], loc='upper left')\n\nplt.title('model IOU')\n\nplt.subplot(1,2,2)\nplt.plot(history.history['loss'][1:])\nplt.plot(history.history['val_loss'][1:])\nplt.ylabel('val_loss')\nplt.xlabel('epoch')\nplt.legend(['train','Validation'], loc='upper left')\nplt.title('model loss')\ngc.collect()","metadata":{"_uuid":"42e9ef3c4e0a2bb2539e5e51740ba6bfc092d37c","execution":{"iopub.status.busy":"2022-03-21T09:14:53.653403Z","iopub.execute_input":"2022-03-21T09:14:53.65387Z","iopub.status.idle":"2022-03-21T09:14:54.837088Z","shell.execute_reply.started":"2022-03-21T09:14:53.653679Z","shell.execute_reply":"2022-03-21T09:14:54.836335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load best model or swa model if not available\ntry:\n    print('using swa weight model')\n    model.load_weights('./keras_swa.model')\nexcept Exception as e:\n    print(e)\n    model.load_weights('./keras.model')","metadata":{"_uuid":"c824f6bca47f051500966c433ce7fb5a9528f6d7","execution":{"iopub.status.busy":"2022-03-21T09:14:54.841231Z","iopub.execute_input":"2022-03-21T09:14:54.843373Z","iopub.status.idle":"2022-03-21T09:14:55.306388Z","shell.execute_reply.started":"2022-03-21T09:14:54.843321Z","shell.execute_reply":"2022-03-21T09:14:55.305625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predict the validation set to do a sanity check\nAgain plot some sample images including the predictions.","metadata":{"_uuid":"0f168318eadb324daa8c020f0e3e0a24d82a464f"}},{"cell_type":"code","source":"def predict_result(model,validation_generator,img_size): \n    # TBD predict both orginal and reflect x\n    preds_test1 = model.predict_generator(validation_generator).reshape(-1, img_size, img_size)\n    return preds_test1","metadata":{"_uuid":"fc4d63ca6c7e6c13e4cfb988a554199712486af2","execution":{"iopub.status.busy":"2022-03-21T09:14:55.309619Z","iopub.execute_input":"2022-03-21T09:14:55.30992Z","iopub.status.idle":"2022-03-21T09:14:55.317477Z","shell.execute_reply.started":"2022-03-21T09:14:55.309875Z","shell.execute_reply":"2022-03-21T09:14:55.31682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"validation_generator = DataGenerator(train_im_path = valid_im_path ,\n                                     train_mask_path=valid_mask_path,augmentations=AUGMENTATIONS_TEST,\n                                     img_size=img_size,shuffle=False)\n\nAUGMENTATIONS_TEST_FLIPPED = Compose([\n    HorizontalFlip(p=1),\n    ToFloat(max_value=1)\n],p=1)\n\nvalidation_generator_flipped = DataGenerator(train_im_path = valid_im_path ,\n                                     train_mask_path=valid_mask_path,augmentations=AUGMENTATIONS_TEST_FLIPPED,\n                                     img_size=img_size,shuffle=False)\n\npreds_valid_orig = predict_result(model,validation_generator,img_size)\npreds_valid_flipped = predict_result(model,validation_generator_flipped,img_size)\npreds_valid_flipped = np.array([np.fliplr(x) for x in preds_valid_flipped])\npreds_valid = 0.5*preds_valid_orig + 0.5*preds_valid_flipped","metadata":{"_uuid":"16cbfe2fee11a8b13b96ce78161ce19b5e5a0c46","execution":{"iopub.status.busy":"2022-03-21T09:14:55.319486Z","iopub.execute_input":"2022-03-21T09:14:55.319969Z","iopub.status.idle":"2022-03-21T09:15:20.214599Z","shell.execute_reply.started":"2022-03-21T09:14:55.319921Z","shell.execute_reply":"2022-03-21T09:15:20.213718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"valid_fn = glob.glob('./keras_mask_val/*')\ny_valid_ori = np.array([cv2.resize(np.array(Image.open(fn)),(img_size,img_size)) for fn in valid_fn])\nassert y_valid_ori.shape == preds_valid.shape","metadata":{"execution":{"iopub.status.busy":"2022-03-21T09:15:20.216101Z","iopub.execute_input":"2022-03-21T09:15:20.21642Z","iopub.status.idle":"2022-03-21T09:15:20.718131Z","shell.execute_reply.started":"2022-03-21T09:15:20.216373Z","shell.execute_reply":"2022-03-21T09:15:20.717182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Plot some predictions for validation set images","metadata":{"_uuid":"423b3268c580dc1eae84f54deeeb0f691eff6028"}},{"cell_type":"code","source":"threshold_best = 0.5\nmax_images = 64\ngrid_width = 16\ngrid_height = int(max_images / grid_width)\nfig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width, grid_height))\n\nvalidation_generator = DataGenerator(train_im_path = valid_im_path ,\n                                     train_mask_path=valid_mask_path,augmentations=AUGMENTATIONS_TEST,\n                                     img_size=img_size,batch_size=64,shuffle=False)\n\nimages,masks = validation_generator.__getitem__(0)\nfor i,(im, mask) in enumerate(zip(images,masks)):\n    pred = preds_valid[i]\n    ax = axs[int(i / grid_width), i % grid_width]\n    ax.imshow(im[...,0], cmap=\"bone\")\n    ax.imshow(mask.squeeze(), alpha=0.5, cmap=\"Reds\")    \n    ax.imshow(np.array(np.round(pred > threshold_best), dtype=np.float32), alpha=0.5, cmap=\"Greens\")\n    ax.axis('off')\nplt.suptitle(\"Green:Prediction , Red: Pneumothorax.\")","metadata":{"_uuid":"40c263765ac6d53a8c0c1361ff1e6f061eecf825","execution":{"iopub.status.busy":"2022-03-21T09:15:20.71997Z","iopub.execute_input":"2022-03-21T09:15:20.720498Z","iopub.status.idle":"2022-03-21T09:15:24.964964Z","shell.execute_reply.started":"2022-03-21T09:15:20.720297Z","shell.execute_reply":"2022-03-21T09:15:24.964169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# src: https://www.kaggle.com/aglotero/another-iou-metric\ndef iou_metric(y_true_in, y_pred_in, print_table=False):\n    labels = y_true_in\n    y_pred = y_pred_in\n    \n    true_objects = 2\n    pred_objects = 2\n\n    intersection = np.histogram2d(labels.flatten(), y_pred.flatten(), bins=(true_objects, pred_objects))[0]\n\n    # Compute areas (needed for finding the union between all objects)\n    area_true = np.histogram(labels, bins = true_objects)[0]\n    area_pred = np.histogram(y_pred, bins = pred_objects)[0]\n    area_true = np.expand_dims(area_true, -1)\n    area_pred = np.expand_dims(area_pred, 0)\n\n    # Compute union\n    union = area_true + area_pred - intersection\n\n    # Exclude background from the analysis\n    intersection = intersection[1:,1:]\n    union = union[1:,1:]\n    union[union == 0] = 1e-9\n\n    # Compute the intersection over union\n    iou = intersection / union\n\n    # Precision helper function\n    def precision_at(threshold, iou):\n        matches = iou > threshold\n        true_positives = np.sum(matches, axis=1) == 1   # Correct objects\n        false_positives = np.sum(matches, axis=0) == 0  # Missed objects\n        false_negatives = np.sum(matches, axis=1) == 0  # Extra objects\n        tp, fp, fn = np.sum(true_positives), np.sum(false_positives), np.sum(false_negatives)\n        return tp, fp, fn\n\n    # Loop over IoU thresholds\n    prec = []\n    if print_table:\n        print(\"Thresh\\tTP\\tFP\\tFN\\tPrec.\")\n    for t in np.arange(0.5, 1.0, 0.05):\n        tp, fp, fn = precision_at(t, iou)\n        if (tp + fp + fn) > 0:\n            p = tp / (tp + fp + fn)\n        else:\n            p = 0\n        if print_table:\n            print(\"{:1.3f}\\t{}\\t{}\\t{}\\t{:1.3f}\".format(t, tp, fp, fn, p))\n        prec.append(p)\n    \n    if print_table:\n        print(\"AP\\t-\\t-\\t-\\t{:1.3f}\".format(np.mean(prec)))\n    return np.mean(prec)\n\ndef iou_metric_batch(y_true_in, y_pred_in):\n    batch_size = y_true_in.shape[0]\n    metric = []\n    for batch in range(batch_size):\n        value = iou_metric(y_true_in[batch], y_pred_in[batch])\n        metric.append(value)\n    return np.mean(metric)\n\nvalid_fn = glob.glob('./keras_mask_val/*')\ny_valid_ori = np.array([cv2.resize(np.array(Image.open(fn)),(img_size,img_size)) for fn in valid_fn])\nassert y_valid_ori.shape == preds_valid.shape","metadata":{"execution":{"iopub.status.busy":"2022-03-21T09:15:24.966487Z","iopub.execute_input":"2022-03-21T09:15:24.967056Z","iopub.status.idle":"2022-03-21T09:15:25.569046Z","shell.execute_reply.started":"2022-03-21T09:15:24.967003Z","shell.execute_reply":"2022-03-21T09:15:25.568034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Scoring for last model\nthresholds = np.linspace(0.2, 0.9, 31)\nious = np.array([iou_metric_batch(y_valid_ori, np.int32(preds_valid > threshold)) for threshold in tqdm_notebook(thresholds)])","metadata":{"execution":{"iopub.status.busy":"2022-03-21T09:15:25.570885Z","iopub.execute_input":"2022-03-21T09:15:25.571562Z","iopub.status.idle":"2022-03-21T09:18:41.550995Z","shell.execute_reply.started":"2022-03-21T09:15:25.571315Z","shell.execute_reply":"2022-03-21T09:18:41.550233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"threshold_best_index = np.argmax(ious) \niou_best = ious[threshold_best_index]\nthreshold_best = thresholds[threshold_best_index]\n\nplt.plot(thresholds, ious)\nplt.plot(threshold_best, iou_best, \"xr\", label=\"Best threshold\")\nplt.xlabel(\"Threshold\")\nplt.ylabel(\"IoU\")\nplt.title(\"Threshold vs IoU ({}, {})\".format(threshold_best, iou_best))\nplt.legend()","metadata":{"execution":{"iopub.status.busy":"2022-03-21T09:18:41.552503Z","iopub.execute_input":"2022-03-21T09:18:41.553029Z","iopub.status.idle":"2022-03-21T09:18:41.820497Z","shell.execute_reply.started":"2022-03-21T09:18:41.552978Z","shell.execute_reply":"2022-03-21T09:18:41.819615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_images = 64\ngrid_width = 16\ngrid_height = int(max_images / grid_width)\nfig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width, grid_height))\n\nvalidation_generator = DataGenerator(train_im_path = valid_im_path ,\n                                     train_mask_path=valid_mask_path,augmentations=AUGMENTATIONS_TEST,\n                                     img_size=img_size,batch_size=64,shuffle=False)\n\nimages,masks = validation_generator.__getitem__(0)\nfor i,(im, mask) in enumerate(zip(images,masks)):\n    pred = preds_valid[i]\n    ax = axs[int(i / grid_width), i % grid_width]\n    ax.imshow(im[...,0], cmap=\"bone\")\n    ax.imshow(mask.squeeze(), alpha=0.5, cmap=\"Reds\")    \n    ax.imshow(np.array(np.round(pred > threshold_best), dtype=np.float32), alpha=0.5, cmap=\"Greens\")\n    ax.axis('off')\nplt.suptitle(\"Green:Prediction , Red: Pneumothorax.\")","metadata":{"execution":{"iopub.status.busy":"2022-03-21T09:18:41.822031Z","iopub.execute_input":"2022-03-21T09:18:41.822717Z","iopub.status.idle":"2022-03-21T09:18:46.092649Z","shell.execute_reply.started":"2022-03-21T09:18:41.822665Z","shell.execute_reply":"2022-03-21T09:18:46.090819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test Set Prediction","metadata":{"_uuid":"4a0123d4cbd90c49c822cf5dc545a30f4a9eb456"}},{"cell_type":"code","source":"test_fn = glob.glob('./test/*')\nx_test = [cv2.resize(np.array(Image.open(fn)),(img_size,img_size)) for fn in test_fn]\nx_test = np.array(x_test)\nx_test = np.array([np.repeat(im[...,None],3,2) for im in x_test])\nprint(x_test.shape)\npreds_test_orig = model.predict(x_test,batch_size=batch_size)\n\nx_test = np.array([np.fliplr(x) for x in x_test])\npreds_test_flipped = model.predict(x_test,batch_size=batch_size)\npreds_test_flipped = np.array([np.fliplr(x) for x in preds_test_flipped])\n\npreds_test = 0.5*preds_test_orig + 0.5*preds_test_flipped\n\n# del x_test; gc.collect()","metadata":{"_uuid":"bd6ce9b4d5fc80a2502a43e80299d628fb5ffc42","execution":{"iopub.status.busy":"2022-03-21T09:18:46.094103Z","iopub.execute_input":"2022-03-21T09:18:46.094585Z","iopub.status.idle":"2022-03-21T09:19:45.604804Z","shell.execute_reply.started":"2022-03-21T09:18:46.094536Z","shell.execute_reply":"2022-03-21T09:19:45.603979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Some Test Set Predictions","metadata":{"_uuid":"834f8f03bccb46931ce8e8afb0f26af1658a3985"}},{"cell_type":"code","source":"max_images = 64\ngrid_width = 16\ngrid_height = int(max_images / grid_width)\nfig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width, grid_height))\n# for i, idx in enumerate(index_val[:max_images]):\nfor i, idx in enumerate(test_fn[:max_images]):\n    img = x_test[i]\n    pred = preds_test[i].squeeze()\n    ax = axs[int(i / grid_width), i % grid_width]\n    ax.imshow(img, cmap=\"Greys\")\n    ax.imshow(np.array(np.round(pred > threshold_best).T, dtype=np.float32), alpha=0.5, cmap=\"Reds\")\n    ax.axis('off')","metadata":{"_uuid":"b63b8f23f51fb7030cbe6f10d38b186044dc7d4e","execution":{"iopub.status.busy":"2022-03-21T09:21:03.272358Z","iopub.execute_input":"2022-03-21T09:21:03.272697Z","iopub.status.idle":"2022-03-21T09:21:06.505991Z","shell.execute_reply.started":"2022-03-21T09:21:03.272644Z","shell.execute_reply":"2022-03-21T09:21:06.505191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\nsys.path.insert(0, '../input/siim-acr-pneumothorax-segmentation')\n\nfrom mask_functions import rle2mask,mask2rle\nimport pdb\n\n# Generate rle encodings (images are first converted to the original size)\nrles = []\ni,max_img = 1,10\nplt.figure(figsize=(16,4))\nfor p in tqdm_notebook(preds_test):\n    p = p.squeeze()\n    im = cv2.resize(p,(1024,1024))\n    im = im > threshold_best\n#     zero out the smaller regions.\n    if im.sum()<1024*2:\n        im[:] = 0\n    im = (im.T*255).astype(np.uint8)  \n    rles.append(mask2rle(im, 1024, 1024))\n    i += 1\n    if i<max_img:\n        plt.subplot(1,max_img,i)\n        plt.imshow(im)\n        plt.axis('off')","metadata":{"_uuid":"388d2d738bd15cc4b7259d1ec41df6e4eede94e7","execution":{"iopub.status.busy":"2022-03-21T09:24:43.860767Z","iopub.execute_input":"2022-03-21T09:24:43.861075Z","iopub.status.idle":"2022-03-21T10:09:42.419735Z","shell.execute_reply.started":"2022-03-21T09:24:43.861023Z","shell.execute_reply":"2022-03-21T10:09:42.419072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ids = [o.split('/')[-1][:-4] for o in test_fn]\nsub_df = pd.DataFrame({'ImageId': ids, 'EncodedPixels': rles})\nsub_df.loc[sub_df.EncodedPixels=='', 'EncodedPixels'] = '-1'\nsub_df.head()","metadata":{"_uuid":"3d85641bbb14e796c7f47a6122f2b9ed2c97a46f","execution":{"iopub.status.busy":"2022-03-21T10:09:50.717119Z","iopub.execute_input":"2022-03-21T10:09:50.717444Z","iopub.status.idle":"2022-03-21T10:09:50.743368Z","shell.execute_reply.started":"2022-03-21T10:09:50.717391Z","shell.execute_reply":"2022-03-21T10:09:50.742735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_df.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-03-21T10:10:05.408583Z","iopub.execute_input":"2022-03-21T10:10:05.40888Z","iopub.status.idle":"2022-03-21T10:10:05.636182Z","shell.execute_reply.started":"2022-03-21T10:10:05.40883Z","shell.execute_reply":"2022-03-21T10:10:05.63504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_df.tail(10)","metadata":{"execution":{"iopub.status.busy":"2022-03-21T10:10:08.496513Z","iopub.execute_input":"2022-03-21T10:10:08.496817Z","iopub.status.idle":"2022-03-21T10:10:08.512193Z","shell.execute_reply.started":"2022-03-21T10:10:08.496761Z","shell.execute_reply":"2022-03-21T10:10:08.51129Z"},"trusted":true},"execution_count":null,"outputs":[]}]}