{"cells":[{"metadata":{"_uuid":"a518ba2a-3260-4b4e-84c7-db73270b50ca","_cell_guid":"118a866a-08d5-4559-829d-98af32fb7b14","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os\nimport glob\nimport tensorflow as tf\nimport skimage\nfrom PIL import Image\nfrom tensorflow import keras\nfrom tensorflow.keras.models import *\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.optimizers import *\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, TensorBoard, CSVLogger\nimport matplotlib.pyplot as plt\nfrom skimage.exposure import equalize_adapthist\nfrom cv2 import equalizeHist","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7314e53a-9988-4259-9e61-8e367d26f1ac","_cell_guid":"12834748-4843-4c69-aec7-66d9fca4021d","trusted":true},"cell_type":"markdown","source":"## Data Generators","execution_count":null},{"metadata":{"_uuid":"6ad20d14-1220-46c1-8a07-d8dc8a06fa07","_cell_guid":"702d80ce-5da4-4b80-b0a1-813942d19477","trusted":true},"cell_type":"code","source":"IMAGES_PATH = \"/kaggle/input/saving-infected-images-and-masks-as-jpg/train/\"\nMASKS_PATH = \"/kaggle/input/saving-infected-images-and-masks-as-jpg/train/\"\n\n# IMAGE_SIZE = (224, 224)\nIMAGE_SIZE = (512, 512)\nCLASS_MODE = None\nCOLOR_MODE = 'grayscale'\nEPOCHS = 50\nBATCH_SIZE = 8\n# BATCH_SIZE = 1\nSEED = 1337","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.listdir(IMAGES_PATH)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = glob.glob(f'{IMAGES_PATH}/images/*.jpg')\ntest_mask_path = glob.glob(f'{MASKS_PATH}/masks/*.jpg')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from skimage import io\ntest_img = np.array(Image.open(test[0]).convert('L'))\ntest_mask = np.array(Image.open(test_mask_path[0]).convert('L'))\n# test_img = io.imread(test[0])\n# test_mask = io.imread(test_mask_path[0])\n\nplt.imshow(test_img, cmap='gray')\n# plt.imshow(test_img + test_mask * 0.1, cmap='gray')\n# plt.imshow(test_mask, cmap='gray')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(test_img.astype(np.uint8), cmap='gray')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"equalized_test_img = equalize_adapthist(np.array(test_img) * 1./255)\nplt.imshow(equalized_test_img, cmap='gray')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# equalized_test_img = test_img * 1./255\nequalized_test_img = equalizeHist(test_img)\nplt.imshow(equalized_test_img,  cmap='gray')\nprint(equalized_test_img.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def apply_adaptive_histogram_equalization(img):\n    img = equalizeHist(img.astype(np.uint8))\n    img = np.expand_dims(img.astype(np.float32), -1)\n    return img","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c1fd6b2b-63db-4362-b287-13da1756465f","_cell_guid":"fdb8fcbb-dea9-4982-8169-095a17111e78","trusted":true},"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n\n# VI Note: use the same seed for image_datagen and mask_datagen to ensure the transformation for image and mask is the same\ndata_gen_args = dict(rescale=1./255, \n                     preprocessing_function=apply_adaptive_histogram_equalization,\n                     rotation_range=10,\n                     shear_range=0.2,\n                    width_shift_range=0.01,\n                    horizontal_flip=True,\n                     validation_split=0.2)\n\n# So our usage here is as data loader instead of loading everything in RAM, not data augmentation\nmask_gen_args = dict(rescale=1./255,\n                    rotation_range=10,\n                     shear_range=0.2,\n                    width_shift_range=0.01,\n                    horizontal_flip=True,\n                    validation_split=0.2)  # to make it binary, add: preprocessing_function=apply_adaptive_equalization,\n\nimage_datagen = ImageDataGenerator(**data_gen_args)\nmask_datagen  = ImageDataGenerator(**mask_gen_args) \n\n# Provide the same seed and keyword arguments to the fit and flow methods\n\nimage_generator = image_datagen.flow_from_directory(\n    IMAGES_PATH,\n    class_mode=CLASS_MODE,\n    classes=['images'],\n    seed=SEED,\n    batch_size=BATCH_SIZE,\n    color_mode=COLOR_MODE,\n    target_size=IMAGE_SIZE,\n    subset='training'\n)\n\nmask_generator = mask_datagen.flow_from_directory(\n    MASKS_PATH,\n    classes=['masks'],\n    class_mode=CLASS_MODE,\n    seed=SEED,\n    batch_size=BATCH_SIZE,\n    color_mode=COLOR_MODE,\n    target_size=IMAGE_SIZE,\n    subset='training'\n)\n\n# combine generators into one which yields image and masks\ntrain_generator = zip(image_generator, mask_generator)\n\n      \n\nval_image_generator = image_datagen.flow_from_directory(\n    IMAGES_PATH,\n    class_mode=CLASS_MODE,\n    classes=['images'],\n    color_mode=COLOR_MODE,\n    seed=SEED,\n    batch_size=BATCH_SIZE,\n    target_size=IMAGE_SIZE,\n    subset='validation'\n)\n\nval_mask_generator = mask_datagen.flow_from_directory(\n    MASKS_PATH,\n    classes=['masks'],\n    class_mode=CLASS_MODE,\n    seed=SEED,\n    batch_size=BATCH_SIZE,\n    color_mode=COLOR_MODE,\n    target_size=IMAGE_SIZE,\n    subset='validation'\n)\n\n# combine generators into one which yields image and masks\nval_generator = zip(val_image_generator, val_mask_generator)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d0888413-3fb1-4722-a7c3-e3f71ee92ebb","_cell_guid":"4d5a7077-1218-4d07-9879-fae3a1b65567","trusted":true},"cell_type":"code","source":"\ndef train_generator_fn():\n    for (img,mask) in train_generator:\n        yield (img, mask)        \n\n\ndef val_generator_fn():\n    for (img,mask) in val_generator:\n        yield (img, mask)  \n        \n        \nprint(f\"Number of training examples: {len(os.listdir(os.path.join(IMAGES_PATH, 'images')))}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(x, m) = next(train_generator_fn())\n# plt.imshow(np.squeeze(x[1]), cmap='gray')\nplt.imshow(np.squeeze(x[1]) + np.squeeze(m[1]) * 0.5, cmap='gray')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(np.squeeze(m[1]), cmap='gray')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d36c4c2a-811c-4697-966e-841c544cb906","_cell_guid":"327dc4be-89e0-48b1-a558-54cd09191ef4","trusted":true},"cell_type":"markdown","source":"## Model","execution_count":null},{"metadata":{"_uuid":"a6bb1809-4fed-4f1d-b7b4-328dab6893c0","_cell_guid":"fe116650-6b3d-4fb9-a38f-314543bda5a9","trusted":true},"cell_type":"code","source":"# Evaluation metric for the competition.\nfrom keras.losses import binary_crossentropy\n\ndef dice_coef(y_true, y_pred, smooth=1):\n    y_true_f = tf.keras.layers.Flatten()(y_true)\n    y_pred_f = tf.keras.layers.Flatten()(y_pred)\n    intersection = tf.reduce_sum(y_true_f * y_pred_f)\n    return (2. * intersection + smooth) / (tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) + smooth)\n\n\ndef dice_coef_loss(y_true, y_pred):\n    return 1.0 - dice_coef(y_true, y_pred)\n\ndef bce_dice_loss(y_true, y_pred):\n    return binary_crossentropy(y_true, y_pred) + dice_coef_loss(y_true, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://www.kaggle.com/cpmpml/fast-iou-metric-in-numpy-and-tensorflow\nfrom tensorflow.keras import backend as K\ndef get_iou_vector(A, B):\n    # Numpy version\n    B = K.cast(B, 'float32')\n    batch_size = A.shape[0]\n    if batch_size is None:\n      batch_size = 0\n    metric = 0.0\n    for batch in range(batch_size):\n        t, p = A[batch], B[batch]\n        true = np.sum(t)\n        pred = np.sum(p)\n\n        # deal with empty mask first\n        if true == 0:\n            pred_batch_size = pred / ( p.shape[0] * p.shape[1] )\n            if pred_batch_size > 0.03:\n               pred_batch_size = 1 \n            metric +=  1 - pred_batch_size\n            continue\n        \n        # non empty mask case.  Union is never empty \n        # hence it is safe to divide by its number of pixels\n\n        intersection = np.sum(t * p)\n        union = true + pred - intersection\n        iou = intersection / union\n        \n        # iou metrric is a stepwise approximation of the real iou over 0.5\n        iou = np.floor(max(0, (iou - 0.45)*20)) / 10\n        \n        metric += iou\n        \n    # teake the average over all images in batch\n    metric /= batch_size\n    return metric\n\n\ndef my_iou_metric(label, pred):\n    # Tensorflow version\n    return tf.py_function(get_iou_vector, [label, pred > 0.5], tf.float64)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8d13a63c-403e-458b-a2aa-044b001d8bbc","_cell_guid":"70b7c647-cae2-46b0-af05-50af055aa55c","trusted":true},"cell_type":"markdown","source":"### Model Hyper-Parameters","execution_count":null},{"metadata":{"_uuid":"97436540-9005-4b0b-a519-f2800dcb0f69","_cell_guid":"ad01d743-aad7-46f2-81ed-01bc5d8fe4a2","trusted":true},"cell_type":"code","source":"factor = 0.5\nlr_patience = 3\nlr_cooldown = 1\nlearning_rate = 1e-4\nloss_function = bce_dice_loss","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a9c2d3db-907f-4900-a145-aeef852bf60a","_cell_guid":"b84b3fe2-9d8f-4e29-b1f4-8ccacb9fa792","trusted":true},"cell_type":"code","source":"def create_dir(dirname):\n    try:\n        os.makedirs(dirname)\n        print(f\"Directory '{dirname}' created.\") \n    except FileExistsError:\n        print(f\"Directory '{dirname}' already exists.\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c4a0ae94-997d-4149-baaf-27d7131230f1","_cell_guid":"6f264b54-984d-4080-b11f-5139eb8eae19","trusted":true},"cell_type":"code","source":"models_dir = '/kaggle/working/models/'\ncreate_dir(models_dir)\n\nmodel_name = f'M-epochs-{EPOCHS}-lr-{learning_rate}-reduce-{factor}-each-{lr_patience}-loss-{loss_function.__name__}'\nmodel_path = os.path.join(models_dir, model_name)\nbest_model_path = os.path.join(model_path, 'best')\nmodel_epochs_path = os.path.join(model_path, 'epochs')\nmodel_logs_path = os.path.join(model_path, 'logs')\n\ncreate_dir(best_model_path)\ncreate_dir(model_epochs_path)\ncreate_dir(model_logs_path)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"50bb12c1-251c-42fe-96ca-034c4d05b1b9","_cell_guid":"097a5bb3-c94c-40c9-8289-d48aeded3faa","trusted":true},"cell_type":"code","source":"# def unet(n_classes, input_size = (*IMAGE_SIZE, 1), flat=False):\n#     inputs = Input(input_size)\n#     conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\n#     conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n#     pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n    \n#     conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n#     conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\n#     pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n    \n#     conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n#     conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n#     pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n    \n#     conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n#     conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\n#     drop4 = Dropout(0.5)(conv4)\n#     pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n\n#     conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n#     conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n#     drop5 = Dropout(0.5)(conv5)\n\n#     up6 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\n#     merge6 = concatenate([drop4,up6], axis = 3)\n#     conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\n#     conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\n\n#     up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\n#     merge7 = concatenate([conv3,up7], axis = 3)\n#     conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\n#     conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n\n#     up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n#     merge8 = concatenate([conv2,up8], axis = 3)\n#     conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\n#     conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\n\n#     up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\n#     merge9 = concatenate([conv1,up9], axis = 3)\n#     conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)\n#     conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n#     #conv9 = Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n#     #conv10 = Conv2D(n_classes, (1,1), activation = 'softmax')(conv9)\n#     conv10 = Conv2D(n_classes, (1,1), padding='same')(conv9)\n# #     if flat:\n# #       output_layer = Reshape((256*256,n_classes))(conv10)\n# #     else:\n#     output_layer = conv10\n#     output_layer = Activation('sigmoid')(output_layer)\n     \n#     model = Model(inputs,output_layer)\n\n\n#     return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def unet(n_classes, input_size = (*IMAGE_SIZE, 1), flat=False):\n    inputs = Input(input_size)\n    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\n    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n    \n    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\n    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n    \n    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n    \n    conv4 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n    conv4 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\n    drop4 = Dropout(0.5)(conv4)\n    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n    \n    conv44 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n    conv44 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv44)\n    drop44 = Dropout(0.5)(conv44)\n    pool44 = MaxPooling2D(pool_size=(2, 2))(drop44)\n\n    conv5 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool44)\n    conv5 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n    drop5 = Dropout(0.5)(conv5)\n    \n    up66 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\n    merge66to4 = concatenate([drop44,up66], axis = 3)\n    conv66 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge66to4)\n    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv66)\n\n    up6 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv66))\n    merge6to4 = concatenate([drop4,up6], axis = 3)\n    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6to4)\n    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\n\n    up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\n    merge7to3 = concatenate([conv3,up7], axis = 3)\n    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7to3)\n    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n\n    up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n    merge8to2 = concatenate([conv2,up8], axis = 3)\n    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8to2)\n    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\n\n    up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\n    merge9to1 = concatenate([conv1,up9], axis = 3)\n    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9to1)\n    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n    \n    \n    conv10 = Conv2D(n_classes, (1,1), padding='same')(conv9)\n    output_layer = conv10\n    output_layer = Activation('sigmoid')(output_layer)\n     \n    model = Model(inputs,output_layer)\n\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3a44e402-7e53-4624-a2a6-26a4ad36ceb6","_cell_guid":"05233905-477b-4aab-8e95-a62336d93e80","trusted":true},"cell_type":"code","source":"model = unet(n_classes=1)  # n_classes=1 not 2 -> see last layer output\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2e5cd82f-8615-4a40-b3f6-2b438b88d480","_cell_guid":"894643a3-894f-4d11-8e67-daca0046a228","trusted":true},"cell_type":"code","source":"from tensorflow.keras.metrics import MeanIoU\n\n# Conclusion: Don't use dice_coef_loss as a loss function in its own.\n# very high starting LR -> bad results\n# model.compile(optimizer = Adam(0.1), loss=dice_coef_loss, metrics=[dice_coef, 'accuracy', MeanIoU(num_classes=2)])\n# model.compile(optimizer = Adam(0.01), loss=dice_coef_loss, metrics=[dice_coef, 'accuracy', MeanIoU(num_classes=2)])\n# model.compile(optimizer = Adam(lr = 0.0001), loss=dice_coef_loss, metrics=[dice_coef, 'accuracy', MeanIoU(num_classes=2)])\n# \n\n\n# Good starting loss\n# model.compile(optimizer = Adam(lr = 1e-5), loss=BinaryCrossentropy(), metrics=[dice_coef, 'accuracy', MeanIoU(num_classes=2)])\n# ReduceLROnPlateau(factor=0.5, patience=5, min_lr=1e-6, verbose=1)\n# Epoch 00050: val_loss did not improve from 0.03727\n# 267/267 [==============================] - 79s 297ms/step - loss: 0.0280 - dice_coef: 0.2126 - accuracy: 0.9859 - mean_io_u_1: 0.4959 - val_loss: 0.0514 - val_dice_coef: 0.1394 - val_accuracy: 0.9808 - val_mean_io_u_1: 0.4963\n# Best: \n# at epoch 11:\n# 0.9856 - mean_io_u_1: 0.4959 - val_loss: 0.0373 - val_dice_coef: 0.0971 - val_accuracy: 0.9858 - val_mean_io_u_1: 0.4963\n\n\n\n\n# ---------------------------------------\n# learning_rate = 1e-3\n# model.compile(optimizer = Adam(lr = learning_rate), loss=BinaryCrossentropy(), metrics=[dice_coef, 'accuracy', MeanIoU(num_classes=2)])\n# ReduceLROnPlateau(factor=0.5, patience=10, min_lr=1e-6, verbose=1)\n\n# Epoch 00050: val_loss did not improve from 0.03023\n# 267/267 [==============================] - 79s 298ms/step - loss: 0.0360 - dice_coef: 0.1134 - accuracy: 0.9850 - mean_io_u_2: 0.4958 - val_loss: 0.0357 - val_dice_coef: 0.1118 - val_accuracy: 0.9856 - val_mean_io_u_2: 0.4964\n# Best:  \n# Epoch 00046: val_loss improved from 0.03032 to 0.03023, saving model to models/best/best_pneumorhorax_dice.h5\n# 267/267 [==============================] - 80s 300ms/step - loss: 0.0371 - dice_coef: 0.1073 - accuracy: 0.9852 - mean_io_u_2: 0.4957 - val_loss: 0.0302 - val_dice_coef: 0.1034 - val_accuracy: 0.9864 - val_mean_io_u_2: 0.4966\n# Best second training\n# Epoch 00070: val_loss improved from 0.02977 to 0.02966, saving model to models/best/best_pneumorhorax_dice.h5\n# 267/267 [==============================] - 83s 311ms/step - loss: 0.0248 - dice_coef: 0.2727 - accuracy: 0.9870 - mean_io_u_2: 0.4960 - val_loss: 0.0297 - val_dice_coef: 0.1973 - val_accuracy: 0.9866 - val_mean_io_u_2: 0.4966\n# last in second\n# Epoch 00100: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n# 267/267 [==============================] - 82s 305ms/step - loss: 0.0108 - dice_coef: 0.6561 - accuracy: 0.9906 - mean_io_u_2: 0.5046 - val_loss: 0.0446 - val_dice_coef: 0.2282 - val_accuracy: 0.9848 - val_mean_io_u_2: 0.4972\n# model.compile(optimizer = Adam(lr = 1e-4), loss=BinaryCrossentropy(), metrics=[dice_coef, 'accuracy', MeanIoU(num_classes=2)])\n# ---------------------------------------\n# Conclusion: start with loss 1e-4 or 1e-5\n# Patience of 10 is very big, try 5, maybe you should use cooldown=1, becuase after reducing lr the val get small\n# ---------------------------------------\n\n# best learning rate to start with 1e-4\nmodel.compile(optimizer = Adam(lr = learning_rate), loss=loss_function, metrics=[dice_coef, 'accuracy', MeanIoU(num_classes=2), my_iou_metric])\n\n\n# model.compile(optimizer = Adam(lr = 1e-5), loss=BinaryCrossentropy(), metrics=[dice_coef, 'accuracy', MeanIoU(num_classes=2)])\n# model.compile(optimizer = Adam(lr = 1e-4), metrics = ['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fc743db5-0502-4ce6-9ae9-cc05204de222","_cell_guid":"a0782c4e-d8e3-4b0c-aeaa-0adc071d8064","trusted":true},"cell_type":"code","source":"print(model_name)\n\ncallbacks = [\n    ModelCheckpoint(os.path.join(best_model_path, 'best_pneumorhorax_dice.h5'), monitor='val_loss',verbose=1, save_best_only=True),\n    ModelCheckpoint(filepath=os.path.join(model_epochs_path, 'model.{epoch:02d}-{val_loss:.2f}.h5'), save_freq='epoch', period=10),\n    ReduceLROnPlateau(factor=factor, patience=lr_patience, min_lr=1e-6, verbose=1, cooldown=lr_cooldown),\n    TensorBoard(model_logs_path), \n    CSVLogger(os.path.join(model_path, \"model_history_log.csv\"), append=True)\n]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6ffe0559-1d42-486b-a79b-5290fa97c4d5","_cell_guid":"0cc7bea6-c1aa-4971-a579-42c65124db88","trusted":true},"cell_type":"code","source":"# history = model.fit_generator(train_generator_fn(),\n#                     validation_data=val_generator_fn(),\n#                     steps_per_epoch=len(image_generator),\n#                     validation_steps=len(val_image_generator),\n#                     epochs=EPOCHS,\n#                     callbacks=callbacks)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Resume Learning","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\npretrained_model_path = \"/kaggle/input/pneumorhorax-model/epoch33best_pneumorhorax_dice.h5\"\npretrained_model = tf.keras.models.load_model(pretrained_model_path,\n                                       custom_objects={\n                                           'dice_coef': dice_coef,\n                                           'bce_dice_loss': bce_dice_loss\n                                       }\n                                      )\npretrained_model.compile(optimizer = Adam(lr = 2e-5), loss=bce_dice_loss, metrics=[dice_coef, 'accuracy', MeanIoU(num_classes=2), my_iou_metric])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = pretrained_model.fit_generator(train_generator_fn(),\n                    validation_data=val_generator_fn(),\n                    steps_per_epoch=len(image_generator),\n                    validation_steps=len(val_image_generator),\n                    epochs=EPOCHS,\n                    callbacks=callbacks)\n\n# 267/267 [==============================] - 333s 1s/step - loss: 0.7828 - dice_coef: 0.2802 - accuracy: 0.9793 - mean_io_u_4: 0.4965 - my_iou_metric: 0.0228 - val_loss: 0.7841 - val_dice_coef: 0.2771 - val_accuracy: 0.9759 - val_mean_io_u_4: 0.4969 - val_my_iou_metric: 0.0160","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9aa54f01-61e2-4218-ab21-80ab13422fee","_cell_guid":"2c0b376f-aec6-4c51-a104-380e749fac60","trusted":true},"cell_type":"markdown","source":"### Plot Using History","execution_count":null},{"metadata":{"_uuid":"fd641102-0844-48ae-a139-0ed3cd79efe4","_cell_guid":"938fca2a-2b6e-49a9-91a7-94ad5211310c","trusted":true},"cell_type":"code","source":"def plot_learning_metrics(history_model):\n\n    plt.plot(history_model.history['loss'], label='loss')\n    plt.plot(history_model.history['val_loss'], label = 'val_loss')\n\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n#     plt.ylim([0.5, 1])\n    plt.legend(loc='lower right')\n\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b523e943-9d4c-4bff-a49b-a14a12736dc2","_cell_guid":"45c3c9f2-27d2-4447-90f6-a7aa886ffba5","trusted":true},"cell_type":"code","source":"def plot_dice_history(history_model):\n\n    plt.plot(history_model.history['dice_coef'], label='dice_coef')\n    plt.plot(history_model.history['val_dice_coef'], label = 'val_dice_coef')\n    plt.xlabel('Epoch')\n    plt.ylabel('Dice Coef')\n#     plt.ylim([0.5, 1])\n    plt.legend(loc='lower right')\n\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ed364ff4-8eed-45f0-a847-2fe7c0733705","_cell_guid":"8c4369b8-e08d-4573-a286-7600bb8b7c4d","trusted":true},"cell_type":"code","source":"plot_learning_metrics(history)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3a6f40b6-dee0-4561-a1c9-7befedd6bfba","_cell_guid":"54b19514-4d7f-4b60-9b92-63457fdd470e","trusted":true},"cell_type":"code","source":"plot_dice_history(history)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f01d2254-ecb7-4a2d-abbf-304fa01dbf55","_cell_guid":"35a9057f-ad11-4240-8243-35a8bc37f6ae","trusted":true},"cell_type":"code","source":"import pickle \ntry: \n    os.makedirs(\"/kaggle/working/models/history/\")\nexcept FileExistsError: \n    print(\"exists\")\n\nwith open(\"/kaggle/working/models/history/train_history_dict\", 'wb') as file_pi:\n        pickle.dump(history.history, file_pi)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"07ecfa61-d423-4ae9-be54-98c7b948eaa5","_cell_guid":"3f50df69-a4e4-480a-9603-86f0bf6db608","trusted":true},"cell_type":"markdown","source":"## Testing","execution_count":null},{"metadata":{"_uuid":"122f4899-74c6-4997-9564-3c1169054074","_cell_guid":"f263ca44-1f88-4bf4-af39-70453c7be9d8","trusted":true},"cell_type":"markdown","source":"### Loading The Testing Images","execution_count":null},{"metadata":{"_uuid":"60d4418d-8555-438d-a55d-a426cba7c68c","_cell_guid":"eb9b7b20-379e-4be5-81f1-f6fe55bb2c89","trusted":true},"cell_type":"code","source":"import glob\ntest_images_paths = glob.glob(\"/kaggle/input/siim-acr-pneumothorax-segmentation/stage_2_images/*.dcm\")\ntest_images_paths[:5]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8ca9de6a-0e32-4d21-b8cc-d0aaefdec6ef","_cell_guid":"ad187276-e99e-4c8c-8ac9-84b6d35713f3","trusted":true},"cell_type":"code","source":"def get_image_name_from_path(image_path):\n    image_name_with_extesion = image_path.rsplit('/', maxsplit=1)[-1]\n    image_name_without_extension = image_name_with_extesion.split('.')[0]\n    return image_name_without_extension\n\n# get_image_name_from_path('/kaggle/input/siim-acr-pneumothorax-segmentation/stage_2_images/ID_9979c1b39.dcm')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"193224b8-d606-4709-a4b7-8696deb82425","_cell_guid":"e29cdd09-9090-4e8e-b5b7-02e95ad3bffa","trusted":true},"cell_type":"code","source":"test_images_names = list(map(get_image_name_from_path, test_images_paths))\ntest_images_names[:5]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1bb5a7a5-3623-4d51-aa13-67263a078a40","_cell_guid":"dfbec8cd-0e0a-469c-bdfe-eef076f94df7","trusted":true},"cell_type":"code","source":"import pandas as pd \n\ntest_df = pd.DataFrame(dict(ImageId=test_images_names,\n               ImagePath=test_images_paths,\n               EncodedPixels=-1))\n\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"337787bc-8b99-4bf7-a0b5-3a92f6aaf876","_cell_guid":"3df14a6d-746a-49dd-ba83-8c33f591f3fe","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport pydicom\nimport cv2\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom keras_preprocessing.image.dataframe_iterator import DataFrameIterator\n\n\n\nclass DCMDataFrameIterator(DataFrameIterator):\n    def __init__(self, *arg, **kwargs):\n        # add dcm as a valid file\n        white_list_formats_with_dcm = list(self.white_list_formats) + ['dcm']\n        self.white_list_formats = tuple(white_list_formats_with_dcm) \n        \n        super(DCMDataFrameIterator, self).__init__(*arg, **kwargs)\n        \n        self.dataframe = kwargs['dataframe']\n        self.x = self.dataframe[kwargs['x_col']]\n        self.y = self.dataframe[kwargs['y_col']]\n        self.color_mode = kwargs['color_mode']\n        self.target_size = kwargs['target_size']\n\n    def _get_batches_of_transformed_samples(self, indices_array):\n        # get batch of images\n        batch_x = np.array([self.read_dcm_as_array(dcm_path, self.target_size, color_mode=self.color_mode)\n                            for dcm_path in self.x.iloc[indices_array]])\n\n        batch_y = np.array(self.y.iloc[indices_array].astype(np.uint8))  # astype because y was passed as str\n\n        # transform images\n        if self.image_data_generator is not None:\n            for i, (x, y) in enumerate(zip(batch_x, batch_y)):\n                transform_params = self.image_data_generator.get_random_transform(x.shape)\n                batch_x[i] = self.image_data_generator.apply_transform(x, transform_params)\n                # you can change y here as well, eg: in semantic segmentation you want to transform masks as well \n                # using the same image_data_generator transformations.\n\n        return batch_x, batch_y\n\n    @staticmethod\n    def read_dcm_as_array(dcm_path, target_size=(256, 256), color_mode='rgb'):\n        image_array = pydicom.dcmread(dcm_path).pixel_array\n        image_array = cv2.resize(image_array, target_size, interpolation=cv2.INTER_NEAREST)  #this returns a 2d array\n        image_array = np.expand_dims(image_array, -1)\n        if color_mode == 'rgb':\n            image_array = cv2.cvtColor(image_array, cv2.COLOR_GRAY2RGB)\n        return image_array","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"31f41a55-c8d3-46d5-9fce-65e989770374","_cell_guid":"73dbe54e-5629-4936-bb55-6e998f9432e0","trusted":true},"cell_type":"code","source":"test_datagen = ImageDataGenerator(rescale=1./255)\n\n# Using the testing generator to evaluate the model after training\ntest_augmentation_parameters = dict(\n    rescale=1.0/255.0\n)\n\ntest_consts = {\n    'batch_size': 1,  # should be 1 in testing\n    'class_mode': CLASS_MODE,\n    'color_mode': COLOR_MODE,\n    'target_size': IMAGE_SIZE,  # resize input images\n    'shuffle': False\n}\n\ntest_augmenter = ImageDataGenerator(**test_augmentation_parameters)\n\ntest_generator = DCMDataFrameIterator(dataframe=test_df,\n                             x_col='image_path',\n                             y_col='target',\n                             image_data_generator=None,\n                             **test_consts)\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cac36b9a-a957-4d3b-b2cb-11ae89483005","_cell_guid":"a2605c13-1197-4c18-89b0-6da1f74b3e10","trusted":true},"cell_type":"markdown","source":"### Loading The Model","execution_count":null},{"metadata":{"_uuid":"c2bac5ca-2ac8-431e-8282-c33cedb838af","_cell_guid":"abaa08aa-c100-46af-b09c-90fa3dfe8304","trusted":true},"cell_type":"code","source":"# new_model = tf.keras.models.load_model('models/interrupted_model.h5',\n#                                        custom_objects={\n#                                            'dice_coef': dice_coef,\n#                                            'dice_coef_loss': dice_coef_loss\n#                                        }\n#                                       )\n\nnew_model = tf.keras.models.load_model('models/interrupted_model.h5',\n                                       compile=False)\n\n# Check its architecture\nnew_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3eddc453-4c89-4c7e-ad37-688986e85683","_cell_guid":"8872bdd5-6668-4ded-b8b9-1b4f9cd075e6","trusted":true},"cell_type":"code","source":"predict = model.predict_generator(test_generator)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f9b2d921-019a-46b1-9ac1-6af9914f56dc","_cell_guid":"77c0b480-5963-4fad-8bf5-c6663385004f","trusted":true},"cell_type":"markdown","source":"### Predict The Mask of Each Image","execution_count":null},{"metadata":{"_uuid":"ea20be61-9f18-44e6-a84e-a49a7ee43f10","_cell_guid":"54ccf7b8-de22-4565-b23f-24f87d6f2063","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7218a80a-b9df-46a2-946d-aefbd8634e4a","_cell_guid":"3f3aada4-2ee7-4d6d-aff8-74e047d8c7c6","trusted":true},"cell_type":"markdown","source":"### Submitting The Testing Results","execution_count":null},{"metadata":{"_uuid":"d9f75f7a-24b4-4eaf-867e-745282242f59","_cell_guid":"06dd0fcc-f4f6-45cd-bee8-bef097cbe1a5","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}