{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Heyyy Everyyoneeee !! *** pleasee UPVOTE if you liked it OR if you have a good heart *** 🥺 🥺 🥺\n\n<img src=\"https://ih1.redbubble.net/image.1600796187.0360/mp,840x830,matte,f8f8f8,t-pad,1000x1000,f8f8f8.jpg\" width=\"600\" class=\"center\"/>","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-14T11:51:13.802716Z","iopub.execute_input":"2021-06-14T11:51:13.803282Z","iopub.status.idle":"2021-06-14T11:51:13.810386Z","shell.execute_reply.started":"2021-06-14T11:51:13.803229Z","shell.execute_reply":"2021-06-14T11:51:13.809638Z"}}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2021-06-17T12:09:04.984943Z","iopub.execute_input":"2021-06-17T12:09:04.985598Z","iopub.status.idle":"2021-06-17T12:09:05.006613Z","shell.execute_reply.started":"2021-06-17T12:09:04.985493Z","shell.execute_reply":"2021-06-17T12:09:05.005145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport matplotlib.pyplot as plt\nimport keras\nimport statsmodels.api as sm","metadata":{"execution":{"iopub.status.busy":"2021-06-17T12:09:05.008693Z","iopub.execute_input":"2021-06-17T12:09:05.009465Z","iopub.status.idle":"2021-06-17T12:09:13.701482Z","shell.execute_reply.started":"2021-06-17T12:09:05.009411Z","shell.execute_reply":"2021-06-17T12:09:13.700184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Wont need this buuutt in casee !!!\nsample_submission = pd.read_csv(\"../input/mlb-player-digital-engagement-forecasting/example_sample_submission.csv\")\n# players = pd.read_csv(\"../input/mlb-player-digital-engagement-forecasting/players.csv\")\n# seasons = pd.read_csv(\"../input/mlb-player-digital-engagement-forecasting/seasons.csv\")\n# awards = pd.read_csv(\"../input/mlb-player-digital-engagement-forecasting/awards.csv\")\n# teams = pd.read_csv(\"../input/mlb-player-digital-engagement-forecasting/teams.csv\")\n# train = pd.read_csv(\"../input/mlb-player-digital-engagement-forecasting/train.csv\")\n# example_test = pd.read_csv(\"../input/mlb-player-digital-engagement-forecasting/example_test.csv\")\n\n## Merged data !!\ndata = pd.read_pickle('../input/mlb-player-digital-engagement-merged-data/player_engagement_with_info.pkl')","metadata":{"execution":{"iopub.status.busy":"2021-06-17T12:09:13.706559Z","iopub.execute_input":"2021-06-17T12:09:13.706994Z","iopub.status.idle":"2021-06-17T12:09:28.679415Z","shell.execute_reply.started":"2021-06-17T12:09:13.706955Z","shell.execute_reply":"2021-06-17T12:09:28.678122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head(2)","metadata":{"execution":{"iopub.status.busy":"2021-06-17T12:09:28.6815Z","iopub.execute_input":"2021-06-17T12:09:28.681951Z","iopub.status.idle":"2021-06-17T12:09:28.75985Z","shell.execute_reply.started":"2021-06-17T12:09:28.681901Z","shell.execute_reply":"2021-06-17T12:09:28.758742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Looking at individual player 🎭 !!\n\nplayer_1 = data[data['playerId']== 628317]\n\nplayer_1.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-17T12:09:28.761515Z","iopub.execute_input":"2021-06-17T12:09:28.762143Z","iopub.status.idle":"2021-06-17T12:09:31.568095Z","shell.execute_reply.started":"2021-06-17T12:09:28.76209Z","shell.execute_reply":"2021-06-17T12:09:31.567083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Null percentage !! 💀 💀 💀 !!\n## most columns have null values and its not because of source issue, ist trully because the individual haven't score or performed !!\nplayer_1.isnull().sum()/player_1.shape[0]","metadata":{"execution":{"iopub.status.busy":"2021-06-17T12:09:31.569476Z","iopub.execute_input":"2021-06-17T12:09:31.569793Z","iopub.status.idle":"2021-06-17T12:09:31.58687Z","shell.execute_reply.started":"2021-06-17T12:09:31.569758Z","shell.execute_reply":"2021-06-17T12:09:31.585687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Filling missing values !! to thoesss columns whicch seem relevant to MEE for any type of player  , it can change for you 🤗 🤗 🤗\n\n# lossStreakTeam :-\n## lenght of losses by it's team, there it's null place cane filled with '0.'\n# player_1[~player_1['lossStreakTeam'].isnull()].head()\ndata['lossStreakTeam'].fillna(0.0, inplace=True)\n\n# winStreakTeam :-\n## lenght of win by it's team, there it's null place cane filled with '0.'\n# player_1[~player_1['winStreakTeam'].isnull()].head()\ndata['winStreakTeam'].fillna(0.0, inplace=True)\n\n# winPctTeam :-\n## lenght of winPctTeam by it's team, there it's null place cane filled with '0.'\n# player_1[~player_1['winPctTeam'].isnull()]['winPctTeam'].describe()\ndata['winPctTeam'].fillna(0.0, inplace=True)\n\n# wildCardRankTeam :-\n## wildCardRankTeam by it's team, there it's null place cane filled with '0.'\n# player_1[~player_1['wildCardRankTeam'].isnull()].head()\ndata['wildCardRankTeam'].fillna(0.0, inplace=True)\n\n# leagueRankTeam :-\n## leagueRankTeam by it's team, there it's null place cane filled with '0.'\n# player_1[~player_1['leagueRankTeam'].isnull()].head()\ndata['leagueRankTeam'].fillna(0.0, inplace=True)\n\n# divisionRankTeam :-\n## divisionRankTeam by it's team, there it's null place cane filled with '0.'\n# player_1[~player_1['divisionRankTeam'].isnull()].head()\ndata['divisionRankTeam'].fillna(0.0, inplace=True)\n\n# leagueRankTeam :-\n## leagueRankTeam by it's team, there it's null place cane filled with '0.'\n# player_1[~player_1['wildCardRankTeam'].isnull()].head()\ndata['winStreakTeam'].fillna(0.0, inplace=True)\n\n# noHitter :-\n## noHitter , there it's null place cane filled with '0.'\n# player_1[~player_1['noHitter'].isnull()].head()\ndata['noHitter'].fillna(0.0, inplace=True)\n\n# blownSaves :-\n## blownSaves Binary, 1 if credited with blown save, 0 will signify no saves in place of nan !!!\n# player_1[~player_1['blownSaves'].isnull()].head()\ndata['blownSaves'].fillna(0.0, inplace=True)\n\n# saves :-\n## saves Binary, 1 if credited with save, 0 will signify no saves in place of nan !!!\n# player_1[~player_1['saves'].isnull()].head()\ndata['saves'].fillna(0.0, inplace=True)\n\n# battersFaced :-\n## battersFaced game total batter faced !!, 0 will signify no batter faced\n# player_1[~player_1['battersFaced'].isnull()].head()\ndata['battersFaced'].fillna(0.0, inplace=True)\n\n# earnedRuns :-\n## Game total earned runs allowed.\n# player_1[~player_1['earnedRuns'].isnull()].head()\ndata['earnedRuns'].fillna(0.0, inplace=True)\n\n# caughtStealing :-\n## caughtStealing game total caught Stealing\n# player_1[~player_1['caughtStealing'].isnull()].head()\ndata['caughtStealing'].fillna(0.0, inplace=True)\n\n# strikeOuts :-\n## Game total strike outs.\n# player_1[~player_1['strikeOuts'].isnull()].head()\ndata['strikeOuts'].fillna(0.0, inplace=True)\n\n# homeRuns :-\n## Game total homeRuns.\n# player_1[~player_1['homeRuns'].isnull()].head()\ndata['homeRuns'].fillna(0.0, inplace=True)\n\n# runsScored :-\n## Game total runscored.\n# player_1[~player_1['runsScored'].isnull()].head()\ndata['runsScored'].fillna(0.0, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-17T12:09:31.594031Z","iopub.execute_input":"2021-06-17T12:09:31.59433Z","iopub.status.idle":"2021-06-17T12:09:31.726608Z","shell.execute_reply.started":"2021-06-17T12:09:31.594296Z","shell.execute_reply":"2021-06-17T12:09:31.725381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Removing all irrelevant or incompatable ( ** according to me ** ) columns having NAN 😭 😭 !!\ndata.dropna(axis=1, inplace=True)\n\nplayer_1 = data[data['playerId']== 628317]\nplayer_1.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-17T12:09:31.728314Z","iopub.execute_input":"2021-06-17T12:09:31.728667Z","iopub.status.idle":"2021-06-17T12:09:35.601427Z","shell.execute_reply.started":"2021-06-17T12:09:31.728635Z","shell.execute_reply":"2021-06-17T12:09:35.60044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Removing all constant, quasi constant columsn or dublicate columns\n\n## Constant columns for specific players ~ most probably will stay constant wrt other players 😰 😰 😰 😰!!\n# playerId : wont be removed !!\n# playerName\n# DOB\n# birthCity\n# birthCountry\n# primaryPositionName\n# homeRuns\n# caughtStealing\n# blownSaves\n# noHitter\n# inSeason: 2\n# year: 4\n\n\nfor col in player_1.columns:\n#     print(col, player_1[col].nunique())\n    if player_1[col].nunique()==1:\n        if col != 'playerId':\n            data.drop(col, axis=1, inplace =True)\n            \n## Updating the individual player's data !!            \nplayer_1 = data[data['playerId']== 628317]\nplayer_1.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-17T12:09:35.603006Z","iopub.execute_input":"2021-06-17T12:09:35.603331Z","iopub.status.idle":"2021-06-17T12:09:39.328732Z","shell.execute_reply.started":"2021-06-17T12:09:35.603301Z","shell.execute_reply":"2021-06-17T12:09:39.327759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Removing dublicate columns !!\n## dailyDataDate\n## engagementMetricsDate\ndata.drop(['dailyDataDate','engagementMetricsDate'], axis=1, inplace =True)\n\n## string columns to integer !!\n## inSeason\n## seasonPart\n\n# Get one hot encoding of columns inSeason & seasonPart\none_hot_inSeason = pd.get_dummies(data['inSeason'])\none_hot_seasonPart = pd.get_dummies(data['seasonPart'])\n# Drop column B as it is now encoded\ndata = data.drop(['inSeason','seasonPart'],axis = 1)\n# Join the encoded df\ndata = data.join(one_hot_inSeason)\ndata = data.join(one_hot_seasonPart)\n\n## Updating the individual player's data !!            \nplayer_1 = data[data['playerId']== 628317]\nplayer_1.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-17T12:09:39.330068Z","iopub.execute_input":"2021-06-17T12:09:39.330728Z","iopub.status.idle":"2021-06-17T12:09:41.26978Z","shell.execute_reply.started":"2021-06-17T12:09:39.330668Z","shell.execute_reply":"2021-06-17T12:09:41.268215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Saved for later usage and since we know !! submission is kinda weird in the competition !!\n# data.to_pickle(\"mlb_data_cleaned.pkl\")\n# ##\n# data = pd.read_pickle('../input/mlb-player-digital-engagement-merged-data/player_engagement_with_info_cleaned.pkl')","metadata":{"execution":{"iopub.status.busy":"2021-06-17T12:09:41.271783Z","iopub.execute_input":"2021-06-17T12:09:41.27233Z","iopub.status.idle":"2021-06-17T12:09:41.277641Z","shell.execute_reply.started":"2021-06-17T12:09:41.272281Z","shell.execute_reply":"2021-06-17T12:09:41.276179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Lets Start with IDK maybe EDA or visual of a single player,🤣 🤣 🤣 🤣 !!!\n## Updating the individual player's data !!            \nplayer_1 = data[data['playerId']== 628317]\nplayer_1.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-17T12:09:41.28102Z","iopub.execute_input":"2021-06-17T12:09:41.28151Z","iopub.status.idle":"2021-06-17T12:09:41.327705Z","shell.execute_reply.started":"2021-06-17T12:09:41.281461Z","shell.execute_reply":"2021-06-17T12:09:41.326628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Date Range !!\n## 2 year 5 months !!\nprint('Date range : '+ str(player_1['date'].min()) +' to ', player_1['date'].max())\n","metadata":{"execution":{"iopub.status.busy":"2021-06-17T12:09:41.329283Z","iopub.execute_input":"2021-06-17T12:09:41.329622Z","iopub.status.idle":"2021-06-17T12:09:41.336679Z","shell.execute_reply.started":"2021-06-17T12:09:41.329588Z","shell.execute_reply":"2021-06-17T12:09:41.335834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Visualizing plots of first 5 players !!\n\nfor num, ID in enumerate(data['playerId'].unique()):\n    if num > 2:\n        break\n    player = data[data['playerId']== ID]\n    ## Visualizinggg targets !!\n    player.set_index('date', inplace=True)\n    player['target1'].plot.line(mark_right=False, figsize=(20,5), grid=True)\n    player['target2'].plot.line(mark_right=False, figsize=(20,5), grid=True)\n    player['target3'].plot.line(mark_right=False, figsize=(20,5), grid=True)\n    player['target4'].plot.line(mark_right=False, figsize=(20,5), grid=True)\n    plt.legend(loc=\"upper left\")\n    plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2021-06-17T12:09:41.337863Z","iopub.execute_input":"2021-06-17T12:09:41.338184Z","iopub.status.idle":"2021-06-17T12:09:42.861498Z","shell.execute_reply.started":"2021-06-17T12:09:41.338155Z","shell.execute_reply":"2021-06-17T12:09:42.86009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## We can sseee SEASONALITY !!!!!!!!","metadata":{"execution":{"iopub.status.busy":"2021-06-17T12:09:42.863367Z","iopub.execute_input":"2021-06-17T12:09:42.863736Z","iopub.status.idle":"2021-06-17T12:09:42.868479Z","shell.execute_reply.started":"2021-06-17T12:09:42.863702Z","shell.execute_reply":"2021-06-17T12:09:42.867169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## lets check till what date we need to forecast !! \n\n# During the Training phase of the competition, this unseen test set is comprised of data for the month of \n# May 2021 and the set of active players this year.\n# During the Evaluation phase, the test set will be a future in-season range of approximately one month.\n\n# sample_submission = pd.read_csv(\"../input/mlb-player-digital-engagement-forecasting/example_sample_submission.csv\")\n\n# sample_submission['playerId'] = sample_submission['date_playerId'].str.rsplit('_').apply(lambda x: int(x[-1]))\n# sample_submission['date'] = sample_submission['date'].apply(lambda x:pd.to_datetime(pd.Series([str(x)])))\n# sample_submission.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-17T12:09:42.870623Z","iopub.execute_input":"2021-06-17T12:09:42.871166Z","iopub.status.idle":"2021-06-17T12:09:42.882827Z","shell.execute_reply.started":"2021-06-17T12:09:42.871117Z","shell.execute_reply":"2021-06-17T12:09:42.881652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## lets seee what correlats the most with the targets : \nplayer_1 = data[data['playerId']== 628317]\n\n## HMMMMMMMMMMM ~~~\nplayer_1[player_1.columns[:]].corr()['target2'][:].sort_values(ascending=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-17T12:09:42.884326Z","iopub.execute_input":"2021-06-17T12:09:42.884632Z","iopub.status.idle":"2021-06-17T12:09:42.915865Z","shell.execute_reply.started":"2021-06-17T12:09:42.884604Z","shell.execute_reply":"2021-06-17T12:09:42.914221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Classical Time Series Forecasting Methods !!","metadata":{"execution":{"iopub.status.busy":"2021-06-16T08:19:41.91866Z","iopub.execute_input":"2021-06-16T08:19:41.919061Z","iopub.status.idle":"2021-06-16T08:19:41.922498Z","shell.execute_reply.started":"2021-06-16T08:19:41.919028Z","shell.execute_reply":"2021-06-16T08:19:41.921763Z"}}},{"cell_type":"markdown","source":"## 1. The Seasonal Autoregressive Integrated Moving-Average with Exogenous Regressors (SARIMAX)","metadata":{}},{"cell_type":"code","source":"# The Seasonal Autoregressive Integrated Moving-Average with Exogenous Regressors (SARIMAX) is an extension of the SARIMA model that also includes the modeling of exogenous variables.\n# Trying SARIMAX !!\nplot_df = player_1.set_index(['date'])['target1']\n\n# Using the “sm.tsa.seasonal_decompose” command from the pylab library we can decompose the time-series into \n# three distinct components: trend, seasonality, and noise.\n\nfrom pylab import rcParams\nimport itertools\n\np = d = q = range(0, 2)\npdq = list(itertools.product(p, d, q))\nseasonal_pdq = [(x[0], x[1], x[2], 12) for x in list(itertools.product(p, d, q))]\n\nfor param in pdq:\n    for param_seasonal in seasonal_pdq:\n        try:\n            mod = sm.tsa.statespace.SARIMAX(y,order=param,seasonal_order=param_seasonal,enforce_stationarity=False,enforce_invertibility=False)\n            results = mod.fit()\n            print('ARIMA{}x{}12 - AIC:{}'.format(param,param_seasonal,results.aic))\n        except: \n            continue\n            \nmod = sm.tsa.statespace.SARIMAX(plot_df,\n                                order=(0, 0, 1),\n                                seasonal_order=(1, 1, 1, 12),\n                                enforce_stationarity=False,\n                                enforce_invertibility=False)\nresults = mod.fit()\npred_uc = results.get_forecast(steps=30)\n\nforecast = pred_uc.predicted_mean\n\nforecast_data = plot_df.append(forecast)\n# forecast_data = forecast_data.to_frame('target1')\n\nforecast_data[-180:].plot.line(mark_right=False, figsize=(20,5), grid=True)\nplt.legend(loc=\"upper left\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-17T12:09:42.92Z","iopub.execute_input":"2021-06-17T12:09:42.9204Z","iopub.status.idle":"2021-06-17T12:09:45.308485Z","shell.execute_reply.started":"2021-06-17T12:09:42.920359Z","shell.execute_reply":"2021-06-17T12:09:45.306682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. The Autoregressive Integrated Moving Average (ARIMA) ","metadata":{}},{"cell_type":"code","source":"# The Autoregressive Integrated Moving Average (ARIMA) method models the next step in the sequence as a linear function \n# of the differenced observations and residual errors at prior time steps.\n\n# ARIMA example\n\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom random import random\n# contrived dataset\ndata_ply = player_1.set_index(['date'])['target1']\n# fit model\nmodel = ARIMA(data_ply, order=(1, 1, 1))\nmodel_fit = model.fit()\n# make prediction\nyhat = model_fit.predict(len(data_ply), len(data_ply)+30, typ='levels')\n# print(yhat)\n\nforecast_data_ply = plot_df.append(yhat)\n# forecast_data_ply = forecast_data_ply.to_frame('target1')\n\nforecast_data_ply[-180:].plot.line(mark_right=False, figsize=(20,5), grid=True)\nplt.legend(loc=\"upper left\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-17T12:09:45.31202Z","iopub.execute_input":"2021-06-17T12:09:45.312501Z","iopub.status.idle":"2021-06-17T12:09:45.778442Z","shell.execute_reply.started":"2021-06-17T12:09:45.312462Z","shell.execute_reply":"2021-06-17T12:09:45.777409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Moving Average","metadata":{}},{"cell_type":"code","source":"# Statistical approaches to Forecast : Moving Average\nimport datetime\n# plot_df = player_1.set_index(['date'])['target1']\nseries = player_1['target1']\ntime = player_1['date']\n\nsplit_time = time.shape[0]-365\ntime_train = time[:split_time]\nx_train = series[:split_time]\ntime_valid = time[split_time:]\nx_valid = series[split_time:]\n\ndef plot_series(time, series, format=\"-\", start=0, end=None):\n    plt.plot(time[start:end], series[start:end], format)\n    plt.xlabel(\"Time\")\n    plt.ylabel(\"Value\")\n    plt.grid(True)\n    \ndef moving_average_forecast(series, window_size):\n    \"\"\"Forecasts the mean of the last few values.\n     If window_size=1, then this is equivalent to naive forecast\"\"\"\n    forecast = []\n    for time in range(len(series) - window_size):\n        forecast.append(series[time:time + window_size].mean())\n    return np.array(forecast)\n\nmoving_avg = moving_average_forecast(series,50)[split_time - 50:]\n\nforecast_data_ply = plot_df.append((pd.DataFrame(moving_avg)).set_index(\n    pd.Series([(pd.to_datetime('2021-05-01') + datetime.timedelta(days=i)).strftime(\"%Y-%m-%d\") for i in range(365)]))[0])\n\nforecast_data_ply[:].plot.line(mark_right=False, figsize=(20,5), grid=True)\nplt.legend(loc=\"upper left\")\nplt.show()\n\n# plt.figure(figsize=(10, 6))\n# plot_series(time_valid, x_valid)\n# plot_series(time_valid, moving_avg)\n\n# # print(keras.metrics.mean_squared_error(x_valid, moving_avg).numpy())\n# # print(keras.metrics.mean_absolute_error(x_valid, moving_avg).numpy())","metadata":{"execution":{"iopub.status.busy":"2021-06-17T12:09:45.779845Z","iopub.execute_input":"2021-06-17T12:09:45.780185Z","iopub.status.idle":"2021-06-17T12:09:46.193615Z","shell.execute_reply.started":"2021-06-17T12:09:45.780152Z","shell.execute_reply":"2021-06-17T12:09:46.192463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. The Seasonal Autoregressive Integrated Moving-Average (SARIMA)","metadata":{}},{"cell_type":"code","source":"# The Seasonal Autoregressive Integrated Moving Average (SARIMA) method models the next step in the sequence as a linear function \n# of the differenced observations, errors, differenced seasonal observations, and seasonal errors at prior time steps.\n# SARIMA example\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom random import random\n# contrived data_plyset\ndata_ply = player_1.set_index(['date'])['target1']\n# fit model\nmodel = SARIMAX(data_ply, order=(1,1, 1), seasonal_order=(0, 0, 0, 0))\nmodel_fit = model.fit(disp=False)\n# make prediction\nyhat = model_fit.predict(len(data_ply), len(data_ply)+30)\n\nforecast_data_ply = plot_df.append(yhat)\n# forecast_data_ply = forecast_data_ply.to_frame('target1')\n\nforecast_data_ply[-410:].plot.line(mark_right=False, figsize=(20,5), grid=True)\nplt.legend(loc=\"upper left\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-17T12:09:46.19519Z","iopub.execute_input":"2021-06-17T12:09:46.195571Z","iopub.status.idle":"2021-06-17T12:09:46.814177Z","shell.execute_reply.started":"2021-06-17T12:09:46.195535Z","shell.execute_reply":"2021-06-17T12:09:46.813012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5. Holt Winter’s Exponential Smoothing (HWES)","metadata":{}},{"cell_type":"code","source":"# Holt Winter’s Exponential Smoothing (HWES)\n\n# The Holt Winter’s Exponential Smoothing (HWES) also called the Triple Exponential Smoothing method models \n# the next time step as an exponentially weighted linear function of observations at prior time steps, \n# taking trends and seasonality into account.\n\n# The method is suitable for univariate time series with trend and/or seasonal components.\n\nfrom statsmodels.tsa.holtwinters import ExponentialSmoothing\nfrom random import random\n\nplot_df = player_1.set_index(['date'])['target1']\ndata_ply = player_1.set_index(['date'])['target1']\n# contrived data_plyset\n# data_ply = [x + random() for x in range(1, 100)]\n# fit model\nmodel = ExponentialSmoothing(data_ply ,seasonal_periods=7 ,trend='add', seasonal='add',)\nmodel_fit = model.fit()\n# make prediction\nyhat = model_fit.predict(len(data_ply), len(data_ply)+30)\nforecast_data_ply = plot_df.append(yhat)\n# forecast_data_ply = forecast_data_ply.to_frame('target1')\n\nforecast_data_ply[-180:].plot.line(mark_right=False, figsize=(20,5), grid=True)\nplt.legend(loc=\"upper left\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-17T12:09:46.815937Z","iopub.execute_input":"2021-06-17T12:09:46.816285Z","iopub.status.idle":"2021-06-17T12:09:47.487186Z","shell.execute_reply.started":"2021-06-17T12:09:46.816251Z","shell.execute_reply":"2021-06-17T12:09:47.48576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6. Simple Exponential Smoothing (SES)","metadata":{}},{"cell_type":"code","source":"# The Simple Exponential Smoothing (SES) method models the next time step as an exponentially weighted linear function \n# of observations at prior time steps.\n\nfrom statsmodels.tsa.holtwinters import SimpleExpSmoothing\nfrom random import random\n# contrived data_plyset\ndata_ply = player_1.set_index(['date'])['target1']\n# fit model\nmodel = SimpleExpSmoothing(data_ply)\nmodel_fit = model.fit()\n# make prediction\nyhat = model_fit.predict(len(data_ply), len(data_ply)+30)\nforecast_data_ply = plot_df.append(yhat)\n# forecast_data_ply = forecast_data_ply.to_frame('target1')\n\nforecast_data_ply[-180:].plot.line(mark_right=False, figsize=(20,5), grid=True)\nplt.legend(loc=\"upper left\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-17T12:09:47.489195Z","iopub.execute_input":"2021-06-17T12:09:47.489654Z","iopub.status.idle":"2021-06-17T12:09:47.781647Z","shell.execute_reply.started":"2021-06-17T12:09:47.489606Z","shell.execute_reply":"2021-06-17T12:09:47.780583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#### Hmmmmmmmmmmmmm : looks like non of them did well !! LOL 🤒 😭\n\ndata = pd.read_pickle('../input/mlb-player-digital-engagement-merged-data/player_engagement_with_info_cleaned.pkl')","metadata":{"execution":{"iopub.status.busy":"2021-06-17T12:09:47.783279Z","iopub.execute_input":"2021-06-17T12:09:47.7836Z","iopub.status.idle":"2021-06-17T12:09:51.272142Z","shell.execute_reply.started":"2021-06-17T12:09:47.783562Z","shell.execute_reply":"2021-06-17T12:09:51.27119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"####                                                        Lets try \n\n<img src=\"https://i.redd.it/xwgslh4ioqq61.jpg\" width=\"500\" class=\"center\"/>","metadata":{"execution":{"iopub.status.busy":"2021-06-16T12:14:56.135162Z","iopub.execute_input":"2021-06-16T12:14:56.135611Z","iopub.status.idle":"2021-06-16T12:14:56.14186Z","shell.execute_reply.started":"2021-06-16T12:14:56.135566Z","shell.execute_reply":"2021-06-16T12:14:56.140636Z"}}},{"cell_type":"code","source":"## getting mean of each player based on there target inform of list !!\nimport random\n\nmean_player = {}\nfor Id in data['playerId'].unique().tolist():\n    mean_player[Id] = {}\n    mean_player[Id]['target1'] = random.uniform(data[data['playerId']== Id]['target1'].quantile(0.25),\n                     data[data['playerId']== Id]['target1'].quantile(0.5))\n    mean_player[Id]['target2'] = random.uniform(data[data['playerId']== Id]['target2'].quantile(0.25),\n                     data[data['playerId']== Id]['target2'].quantile(0.5))\n    mean_player[Id]['target3'] = random.uniform(data[data['playerId']== Id]['target3'].quantile(0.25),\n                     data[data['playerId']== Id]['target3'].quantile(0.5))\n    mean_player[Id]['target4'] = random.uniform(data[data['playerId']== Id]['target4'].quantile(0.25),\n                     data[data['playerId']== Id]['target4'].quantile(0.5))\n\nmean_player[656669]\n\n# mean_player = {}\n# for Id in data['playerId'].unique().tolist():\n#     mean_player[Id] = {}\n#     mean_player[Id]['target1'] = data[data['playerId']== Id]['target1'].quantile(0.75)\n#     mean_player[Id]['target2'] = data[data['playerId']== Id]['target2'].quantile(0.75)\n#     mean_player[Id]['target3'] = data[data['playerId']== Id]['target3'].quantile(0.75)\n#     mean_player[Id]['target4'] = data[data['playerId']== Id]['target4'].quantile(0.75)\n\n# mean_player[656669]","metadata":{"execution":{"iopub.status.busy":"2021-06-17T12:09:51.273588Z","iopub.execute_input":"2021-06-17T12:09:51.274113Z","iopub.status.idle":"2021-06-17T12:11:30.030646Z","shell.execute_reply.started":"2021-06-17T12:09:51.274079Z","shell.execute_reply":"2021-06-17T12:11:30.029483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(mean_player), data['playerId'].nunique()","metadata":{"execution":{"iopub.status.busy":"2021-06-17T12:11:30.032444Z","iopub.execute_input":"2021-06-17T12:11:30.032801Z","iopub.status.idle":"2021-06-17T12:11:30.091109Z","shell.execute_reply.started":"2021-06-17T12:11:30.032741Z","shell.execute_reply":"2021-06-17T12:11:30.090204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # COMMENTING SINCE I HAVE SAVED THE OUTPUT !!\n\n# sub_dict = {}\n# ext_date = pd.Series([(pd.to_datetime('2021-05-01') + datetime.timedelta(days=i)).strftime(\"%Y-%m-%d\") for i in range(90)])\n# split_time = 851 # player_1['date'].shape[0]-365\n\n# no_days = 90\n\n# for player in data['playerId'].unique():\n#     player_data = data[data['playerId']==player][['target1', 'target2', 'target3', 'target4','year','date']]\n#     player_data_cut = player_data[player_data['date']>'2021-04-20']\n    \n#     ## we know the avilable data ranges from :2018-01-01 to  2021-04-30\n    \n#     model_fit = ExponentialSmoothing(player_data['target1']).fit()\n#     target1_df = pd.DataFrame({'target1': model_fit.predict(player_1.shape[0], player_1.shape[0]+no_days)}).reset_index(drop=True)\n#     target1_df['date'] = ext_date\n#     target1_df = player_data_cut[['date','target1']].append(target1_df).reset_index(drop=True)\n    \n#     model_fit = ExponentialSmoothing(player_data['target2']).fit()\n#     target2_df = pd.DataFrame({'target2': model_fit.predict(player_1.shape[0], player_1.shape[0]+no_days)}).reset_index(drop=True)\n#     target2_df['date'] = ext_date\n#     target2_df = player_data_cut[['date','target2']].append(target2_df).reset_index(drop=True)\n    \n#     model_fit = ExponentialSmoothing(player_data['target3']).fit()\n#     target3_df = pd.DataFrame({'target3': model_fit.predict(player_1.shape[0], player_1.shape[0]+no_days)}).reset_index(drop=True)\n#     target3_df['date'] = ext_date\n#     target3_df = player_data_cut[['date','target3']].append(target3_df).reset_index(drop=True)\n    \n#     model_fit = ExponentialSmoothing(player_data['target4']).fit()\n#     target4_df = pd.DataFrame({'target4': model_fit.predict(player_1.shape[0], player_1.shape[0]+no_days)}).reset_index(drop=True)\n#     target4_df['date'] = ext_date\n#     target4_df = player_data_cut[['date','target4']].append(target4_df).reset_index(drop=True)\n    \n#     # Lets make dict which will help us in this weird submission style 🤬 🤬 🤬 !!\n#     sub_dict[player] = {}\n#     for date in target1_df['date'][:-1]:\n#         sub_dict[player][date] ={}\n#         sub_dict[player][date]['target1'] = target1_df[target1_df['date']==date]['target1'].values[0]\n#         sub_dict[player][date]['target2'] = target2_df[target1_df['date']==date]['target2'].values[0]\n#         sub_dict[player][date]['target3'] = target3_df[target1_df['date']==date]['target3'].values[0]\n#         sub_dict[player][date]['target4'] = target4_df[target1_df['date']==date]['target4'].values[0]\n        ","metadata":{"execution":{"iopub.status.busy":"2021-06-17T12:11:30.094072Z","iopub.execute_input":"2021-06-17T12:11:30.094525Z","iopub.status.idle":"2021-06-17T12:11:30.099608Z","shell.execute_reply.started":"2021-06-17T12:11:30.094484Z","shell.execute_reply":"2021-06-17T12:11:30.098406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sub_dict[596071]['2021-04-30']","metadata":{"execution":{"iopub.status.busy":"2021-06-17T12:11:30.101105Z","iopub.execute_input":"2021-06-17T12:11:30.101468Z","iopub.status.idle":"2021-06-17T12:11:30.117011Z","shell.execute_reply.started":"2021-06-17T12:11:30.101431Z","shell.execute_reply":"2021-06-17T12:11:30.116052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import numpy as np\n\n# Save\n# np.save('sub_dict_ExponentialSmoothing.npy', sub_dict) \n\n# # Load\n# sub_dict = np.load('../input/mlb-player-digital-engagement-merged-data/sub_dict_ExponentialSmoothing.npy',allow_pickle='TRUE').item()\n# sub_dict[628317]['2021-05-01']\n","metadata":{"execution":{"iopub.status.busy":"2021-06-17T12:11:30.11849Z","iopub.execute_input":"2021-06-17T12:11:30.118799Z","iopub.status.idle":"2021-06-17T12:11:30.127445Z","shell.execute_reply.started":"2021-06-17T12:11:30.118771Z","shell.execute_reply":"2021-06-17T12:11:30.126424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Submission !! 😇 😇 😇 ##\n## Function to create processed data that will be submitted !!\n\n# def process_pred(data):\n#     data = data.reset_index()\n#     data['date_formated'] = data['date'].apply(lambda x:pd.to_datetime(pd.Series([str(x)])))\n#     data['date_formated'] = data['date_formated'].apply(lambda x: x.strftime(\"%Y-%m-%d\") )\n    \n#     data['playerId'] = data['date_playerId'].str.rsplit('_').apply(lambda x: int(x[-1]))\n\n#     for player, date in zip(data['playerId'], data['date_formated']):\n#         indexes = (data[(data['playerId'] == player) & (data['date_formated'] == date)].index).tolist()\n        \n#         data.loc[indexes,'target1'] = sub_dict[player][date]['target1']\n#         data.loc[indexes,'target2'] = sub_dict[player][date]['target2']\n#         data.loc[indexes,'target3'] = sub_dict[player][date]['target3']\n#         data.loc[indexes,'target4'] = sub_dict[player][date]['target4']\n    \n#     data = data.set_index('date', drop = True)\n#     data = data.drop(['playerId','date_formated'], axis=1)\n#     return data\n\ndef process_pred(data):\n    data = data.reset_index()\n    data['playerId'] = data['date_playerId'].str.rsplit('_').apply(lambda x: int(x[-1]))\n\n    for ply in data['playerId'].unique().tolist():\n        indexes = (data[data['playerId'] == ply].index).tolist()\n        data.loc[indexes,'target1'] = mean_player[ply]['target1']\n        data.loc[indexes,'target2'] = mean_player[ply]['target2']\n        data.loc[indexes,'target3'] = mean_player[ply]['target3']\n        data.loc[indexes,'target4'] = mean_player[ply]['target4']\n    \n    data = data.set_index('date', drop = True)\n    data = data.drop(['playerId'], axis=1)\n    return data","metadata":{"execution":{"iopub.status.busy":"2021-06-17T12:11:30.128994Z","iopub.execute_input":"2021-06-17T12:11:30.129346Z","iopub.status.idle":"2021-06-17T12:11:30.143788Z","shell.execute_reply.started":"2021-06-17T12:11:30.129313Z","shell.execute_reply":"2021-06-17T12:11:30.142722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Checking the process_pred function !!\nsample_submission = pd.read_csv(\"../input/mlb-player-digital-engagement-forecasting/example_sample_submission.csv\")\n\nprocess_pred(sample_submission.set_index('date', drop = True)).head()","metadata":{"execution":{"iopub.status.busy":"2021-06-17T12:11:30.145253Z","iopub.execute_input":"2021-06-17T12:11:30.145985Z","iopub.status.idle":"2021-06-17T12:11:33.064471Z","shell.execute_reply.started":"2021-06-17T12:11:30.145947Z","shell.execute_reply":"2021-06-17T12:11:33.063446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Shitty submission typeee !!\n\nimport sys\nif 'kaggle_secrets' in sys.modules:  # only run while on Kaggle\n    import mlb","metadata":{"execution":{"iopub.status.busy":"2021-06-17T12:11:33.065748Z","iopub.execute_input":"2021-06-17T12:11:33.066056Z","iopub.status.idle":"2021-06-17T12:11:33.090461Z","shell.execute_reply.started":"2021-06-17T12:11:33.066026Z","shell.execute_reply":"2021-06-17T12:11:33.089531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Execute this only for one !!\n\nenv = mlb.make_env()\niter_test = env.iter_test()","metadata":{"execution":{"iopub.status.busy":"2021-06-17T12:11:33.091756Z","iopub.execute_input":"2021-06-17T12:11:33.092078Z","iopub.status.idle":"2021-06-17T12:11:33.097258Z","shell.execute_reply.started":"2021-06-17T12:11:33.092047Z","shell.execute_reply":"2021-06-17T12:11:33.095981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Hereeee wee Ggooooooooo !!!!! 🌊 🚀 🚀 🚀\n\nfor (test_df, sample_prediction_df) in iter_test:\n    sample_prediction_df = process_pred(sample_prediction_df)\n    env.predict(sample_prediction_df)","metadata":{"execution":{"iopub.status.busy":"2021-06-17T12:11:33.098714Z","iopub.execute_input":"2021-06-17T12:11:33.099115Z","iopub.status.idle":"2021-06-17T12:11:48.60817Z","shell.execute_reply.started":"2021-06-17T12:11:33.098997Z","shell.execute_reply":"2021-06-17T12:11:48.606787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_prediction_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-17T12:11:48.609729Z","iopub.execute_input":"2021-06-17T12:11:48.610112Z","iopub.status.idle":"2021-06-17T12:11:48.624964Z","shell.execute_reply.started":"2021-06-17T12:11:48.610071Z","shell.execute_reply":"2021-06-17T12:11:48.623558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Since you have reached this far -- Upvoteeee !! yeah i am a simp for upvotes <3","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}