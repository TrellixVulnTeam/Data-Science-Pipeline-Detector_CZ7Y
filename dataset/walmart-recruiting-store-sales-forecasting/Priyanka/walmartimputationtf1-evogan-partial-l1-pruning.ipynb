{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"!pip install tensorflow==1.15.0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\n%matplotlib inline\n\nimport tensorflow as tf\nfrom tqdm import tqdm\nprint(tf.__version__)\n\n# Make numpy values easier to read.\nnp.set_printoptions(precision=3, suppress=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = \"/kaggle/input/walmart-recruiting-store-sales-forecasting/\"\ndataset = pd.read_csv(path + \"train.csv.zip\", names=['Store','Dept','Date','weeklySales','isHoliday'],sep=',', header=0)\nfeatures = pd.read_csv(path + \"features.csv.zip\",sep=',', header=0,\n                       names=['Store','Date','Temperature','Fuel_Price','MarkDown1','MarkDown2','MarkDown3','MarkDown4',\n                              'MarkDown5','CPI','Unemployment','IsHoliday']).drop(columns=['IsHoliday'])\nstores = pd.read_csv(path + \"stores.csv\", names=['Store','Type','Size'],sep=',', header=0)\ndataset = dataset.merge(stores, how='left').merge(features, how='left')\n\ndataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales = dataset.groupby(['Dept', 'Date', 'Store'])['weeklySales'].sum().unstack()\nsales","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Sales_complete is the complete subset of sales data without any missing value\n\n\n\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ndata = scaler.fit_transform(sales).astype(np.float32)\nsales_scaled = pd.DataFrame(data=data, columns=sales.columns, index=sales.index)\n\nsales_complete = sales_scaled[sales_scaled.isna().sum(axis=1) == 0]\nprint(sales_complete.shape)\nsales_complete\n'''\nsales_complete = sales[sales.isna().sum(axis=1) == 0]\nprint(sales_complete.shape)\nsales_complete","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_new=sales_complete.iloc[:4416,:42].copy()\nsales_new.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_new = sales_new.to_numpy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split Data into 6 pieces\nfull_size = len(data_new)\nn_subsets = 6\nsub_size = full_size // n_subsets\n\nsubsets = []\nfor k in range(6):\n    subsets.append(data_new[k*sub_size:(k+1)*sub_size, k*3:(k*3)+12])\n    \nprint(\"Split data into\", n_subsets, \"subsets. Size of one subset:\", sub_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Apply GAIN on the first dataset\nData = subsets[0]\n# Data = raw_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#%% System Parameters\n# 1. Mini batch size\nmb_size = 128\n# 2. Missing rate\np_miss = 0.2\n# 3. Hint rate\np_hint = 0.9\n# 4. Loss Hyperparameters\nalpha = 10\n# 5. Train Rate\ntrain_rate = 0.8\n\n# Parameters\nNo = len(Data)\nDim = len(Data[0,:])\n\n# Hidden state dimensions\nH_Dim1 = Dim\nH_Dim2 = Dim\nk=0.99 #pruning percentage\nprint(Dim)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# regularization\n#regularizer = tf.contrib.layers.l2_regularizer(scale=0.001)\nregularizer = tf.contrib.layers.l1_regularizer(scale=0.5)\n#regularizer=tf.contrib.layers.l1_l2_regularizer(scale_l1=0.001,scale_l2=0.001,scope=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Normalization (0 to 1)\ndef normalization(Data,Dim=12):\n    Min_Val = np.zeros(Dim)\n    Max_Val = np.zeros(Dim)\n\n    for i in range(Dim):\n        Min_Val[i] = np.min(Data[:,i])\n        Data[:,i] = Data[:,i] - np.min(Data[:,i])\n        Max_Val[i] = np.max(Data[:,i])\n        Data[:,i] = Data[:,i] / (np.max(Data[:,i]) + 1e-6)    \n\n    #%% Missing introducing\n    p_miss_vec = p_miss * np.ones((Dim,1)) \n   \n    Missing = np.zeros((No,Dim))\n\n    for i in range(Dim):\n        A = np.random.uniform(0., 1., size = [len(Data),])\n        B = A > p_miss_vec[i]\n        Missing[:,i] = 1.*B\n    return Data, Missing","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#%% Train Test Division    \ndef train_test(Data,Missing):\n    idx = np.random.permutation(No)\n\n    Train_No = int(No * train_rate)\n    Test_No = No - Train_No\n    \n    # Train / Test Features\n    trainX = Data[idx[:Train_No],:]\n    testX = Data[idx[Train_No:],:]\n\n    # Train / Test Missing Indicators\n    trainM = Missing[idx[:Train_No],:]\n    testM = Missing[idx[Train_No:],:]\n    \n    return trainX,testX,trainM,testM,Train_No,Test_No","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#%% Necessary Functions\n\n# 1. Xavier Initialization Definition\ndef xavier_init(size):\n    in_dim = size[0]\n    xavier_stddev = 1. / tf.sqrt(in_dim / 2.)\n    return tf.random_normal(shape = size, stddev = xavier_stddev)\n    \n# Hint Vector Generation\ndef sample_M(m, n, p):\n    A = np.random.uniform(0., 1., size = [m, n])\n    B = A > p\n    C = 1.*B\n    return C","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#%% GAIN Architecture   \n   \n#%% 1. Input Placeholders\n# 1.1. Data Vector\ndef input_placeholder():\n    \n    X = tf.placeholder(tf.float32, shape = [None, Dim])\n    # 1.2. Mask Vector \n    M = tf.placeholder(tf.float32, shape = [None, Dim])\n    # 1.3. Hint vector\n    H = tf.placeholder(tf.float32, shape = [None, Dim])\n    # 1.4. X with missing values\n    New_X = tf.placeholder(tf.float32, shape = [None, Dim])\n    \n    return X,M,H,New_X\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#%% 2. Discriminator\nD_W1 = tf.Variable(xavier_init([Dim*2, H_Dim1]), name=\"D_W1\")     # Data + Hint as inputs\ntf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, D_W1)\nD_b1 = tf.Variable(tf.zeros(shape = [H_Dim1]), name=\"D_b1\")\n\n#D_W2 = tf.Variable(xavier_init([H_Dim1, H_Dim2]), name=\"D_W2\")\nD_W2 = tf.Variable(xavier_init([12, 12]), name=\"D_W2\")\ntf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, D_W2)\n#D_b2 = tf.Variable(tf.zeros(shape = [H_Dim2]), name=\"D_b2\")\nD_b2 = tf.Variable(tf.zeros(shape = [12]), name=\"D_b2\")\n\n#D_W3 = tf.Variable(xavier_init([H_Dim2, Dim]), name=\"D_W3\")\nD_W3 = tf.Variable(xavier_init([12, Dim]), name=\"D_W3\")\ntf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, D_W3)\nD_b3 = tf.Variable(tf.zeros(shape = [Dim]), name=\"D_b3\")       # Output is multi-variate\n\ntheta_D = [D_W1, D_W2, D_W3, D_b1, D_b2, D_b3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#%% 3. Generator\nG_W1 = tf.Variable(xavier_init([Dim*2, H_Dim1]), name=\"G_W1\")     # Data + Mask as inputs (Random Noises are in Missing Components)\ntf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, G_W1)\nG_b1 = tf.Variable(tf.zeros(shape = [H_Dim1]), name=\"G_b1\")\n\n#G_W2 = tf.Variable(xavier_init([H_Dim1, H_Dim2]), name=\"G_W2\")\nG_W2 = tf.Variable(xavier_init([12, 12]), name=\"G_W2\")\ntf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, G_W2)\n#G_b2 = tf.Variable(tf.zeros(shape = [H_Dim2]), name=\"G_b2\")\nG_b2 = tf.Variable(tf.zeros(shape = [12]), name=\"G_b2\")\n\n#G_W3 = tf.Variable(xavier_init([H_Dim2, Dim]), name=\"G_W3\")\nG_W3 = tf.Variable(xavier_init([12, Dim]), name=\"G_W3\")\ntf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, G_W3)\nG_b3 = tf.Variable(tf.zeros(shape = [Dim]), name=\"G_b3\")\n\ntheta_G = [G_W1, G_W2, G_W3, G_b1, G_b2, G_b3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#%% GAIN Function\n\n#%% 1. Generator\ndef generator(new_x,m):\n    inputs = tf.concat(axis = 1, values = [new_x,m])  # Mask + Data Concatenate\n    G_h1 = tf.nn.relu(tf.matmul(inputs, G_W1) + G_b1)\n    G_h2 = tf.nn.relu(tf.matmul(G_h1, G_W2) + G_b2)   \n    G_prob = tf.nn.sigmoid(tf.matmul(G_h2, G_W3) + G_b3) # [0,1] normalized Output\n    \n    return G_prob\n    \n#%% 2. Discriminator\ndef discriminator(new_x, h):\n    inputs = tf.concat(axis = 1, values = [new_x,h])  # Hint + Data Concatenate\n    D_h1 = tf.nn.relu(tf.matmul(inputs, D_W1) + D_b1)  \n    D_h2 = tf.nn.relu(tf.matmul(D_h1, D_W2) + D_b2)\n    D_logit = tf.matmul(D_h2, D_W3) + D_b3\n    D_prob = tf.nn.sigmoid(D_logit)  # [0,1] Probability Output\n    \n    return D_prob","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#%% 3. Other functions\n# Random sample generator for Z\ndef sample_Z(m, n):\n    return np.random.uniform(0., 0.01, size = [m, n])        \n\n# Mini-batch generation\ndef sample_idx(m, n):\n    A = np.random.permutation(m)\n    idx = A[:n]\n    return idx","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#%% Structure\nX,M,H,New_X=input_placeholder()\n# Generator\n\nG_sample = generator(New_X,M)\n\n# Combine with original data\nHat_New_X = New_X * M + G_sample * (1-M)\n\n# Discriminator\nD_prob = discriminator(Hat_New_X, H)\n\n#%% Loss\nD_loss1 = -tf.reduce_mean(M * tf.log(D_prob + 1e-8) + (1-M) * tf.log(1. - D_prob + 1e-8)) \nG_loss1 = -tf.reduce_mean((1-M) * tf.log(D_prob + 1e-8))\nMSE_train_loss = tf.reduce_mean((M * New_X - M * G_sample)**2) / tf.reduce_mean(M)\n\nD_loss = D_loss1\nG_loss = G_loss1 + alpha * MSE_train_loss \n\nreg_variables = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\nreg_term = tf.contrib.layers.apply_regularization(regularizer, reg_variables)\nD_loss += reg_term\n\n#%% MSE Performance metric\nMSE_test_loss = tf.reduce_mean(((1-M) * X - (1-M)*G_sample)**2) / tf.reduce_mean(1-M)\n\n#%% Solver\nD_solver = tf.train.AdamOptimizer().minimize(D_loss, var_list=theta_D)\nG_solver = tf.train.AdamOptimizer().minimize(G_loss, var_list=theta_G)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm\nimport matplotlib\nimport pickle\nimport matplotlib.pyplot as plt\nfrom tensorflow.contrib.model_pruning.python import pruning\nfrom tensorflow.contrib.model_pruning.python.layers import layers\nimport time\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Data,Missing=normalization(Data)\ntrainX,testX,trainM,testM,Train_No,Test_No=train_test(Data,Missing)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Sessions\n\nsess = tf.Session()\nsess.run(tf.global_variables_initializer())\n\n#%% Iterations\ntrain_losses = []\ntest_losses = []\n\n#%% Start Iterations\nt=time.time()\nfor it in tqdm(range(1000)):    \n    \n    #%% Inputs\n    mb_idx = sample_idx(Train_No, mb_size)\n    X_mb = trainX[mb_idx,:]  \n    \n    Z_mb = sample_Z(mb_size, Dim) \n#     M_mb = trainM[mb_idx,:]  \n    M_mb = trainM[:mb_size, :]\n    H_mb1 = sample_M(mb_size, Dim, 1-p_hint)\n    H_mb = M_mb * H_mb1\n    \n    New_X_mb = M_mb * X_mb + (1-M_mb) * Z_mb  # Missing Data Introduce\n    \n    _, D_loss_curr = sess.run([D_solver, D_loss1], feed_dict = {M: M_mb, New_X: New_X_mb, H: H_mb})\n    _, G_loss_curr, MSE_train_loss_curr, MSE_test_loss_curr = sess.run([G_solver, G_loss1, MSE_train_loss, MSE_test_loss],\n                                                                       feed_dict = {X: X_mb, M: M_mb, New_X: New_X_mb, H: H_mb})\n            \n        \n    #%% Intermediate Losses\n#     if it % 100 == 0:\n#         print('Iter: {}'.format(it))\n#         print('Train_loss: {:.4}'.format(np.sqrt(MSE_train_loss_curr)))\n#         print('Test_loss: {:.4}'.format(np.sqrt(MSE_test_loss_curr)))\n#         print()\n    train_losses.append(np.sqrt(MSE_train_loss_curr))\n    test_losses.append(np.sqrt(MSE_test_loss_curr))\nt1=time.time()    \nprint(\"Time cost before pruning: \",t1-t)\n#%% Final Loss\n   \nZ_mb = sample_Z(Test_No, Dim) \nM_mb = testM\nX_mb = testX\n        \nNew_X_mb = M_mb * X_mb + (1-M_mb) * Z_mb  # Missing Data Introduce\n    \nMSE_final, Sample = sess.run([MSE_test_loss, G_sample], feed_dict = {X: testX, M: testM, New_X: New_X_mb})\n        \nprint('Final Test RMSE: ' + str(np.sqrt(MSE_final)))\n#print(\"Sparsity of layers\", sess.run(tf.contrib.model_pruning.get_weight_sparsity()))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"parameter=[]\nbaseline=[]\n\nD_W1_values = sess.run(D_W1)\nD_W2_values = sess.run(D_W2)\nD_W3_values = sess.run(D_W3)\n\nG_W1_values = sess.run(G_W1)\nG_W2_values = sess.run(G_W2)\nG_W3_values = sess.run(G_W3)\n\nD_b1_values = sess.run(D_b1)\nD_b2_values = sess.run(D_b2)\nD_b3_values = sess.run(D_b3)\n\nG_b1_values = sess.run(G_b1)\nG_b2_values = sess.run(G_b2)\nG_b3_values = sess.run(G_b3)\nVal_list1=[D_W1_values,D_W2_values,D_W3_values,G_W1_values,G_W2_values,G_W3_values]\nVal_list_bias=[D_b1_values,D_b2_values,D_b3_values,G_b1_values,G_b2_values,G_b3_values]\n\ntotal=0\nbelow_threshold=0\nfor weights in Val_list1:\n    r,c=weights.shape[:2]\n    total+=r*c\n    #below_threshold+=len(weights[np.where(np.abs(weights)<0.001)])\n    below_threshold+=len(weights[np.where(np.abs(weights)==0)])\nb1=total\np1=total-below_threshold\nsparsity=((total-below_threshold)/total)*100\nparameter.append(p1)\nbaseline.append(b1)\nprint(parameter)\nprint(baseline)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import rankdata\n#k=0.99\n\nNEW_WEIGHTS = \"GAIN1\"+str(k)+\".pickle\"\n  \n#print(NEW_WEIGHTS)\nwith open (NEW_WEIGHTS, 'wb') as f:\n    for weights in Val_list1:\n        x=(rankdata(np.abs(weights),method='dense') - 1).astype('float32').reshape(weights.shape)\n        lower_bound_rank = np.ceil(np.max(x)*k).astype('float32')\n        weights[x<=lower_bound_rank]=0\n        pickle.dump(weights, f)\n    for biases in Val_list_bias:\n        pickle.dump(biases, f)\n    \n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## GAN2 with Pruning"},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.reset_default_graph()\nData = subsets[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open('/kaggle/working/'+NEW_WEIGHTS, 'rb') as f:\n    D2_W1_values = pickle.load(f).astype('float32')\n    D2_W2_values = pickle.load(f).astype('float32')\n    D2_W3_values = pickle.load(f).astype('float32')\n    G2_W1_values = pickle.load(f).astype('float32')\n    G2_W2_values = pickle.load(f).astype('float32')\n    G2_W3_values = pickle.load(f).astype('float32')\n    D2_b1_values = pickle.load(f).astype('float32')\n    D2_b2_values = pickle.load(f).astype('float32')\n    D2_b3_values = pickle.load(f).astype('float32')\n    G2_b1_values = pickle.load(f).astype('float32')\n    G2_b2_values = pickle.load(f).astype('float32')\n    G2_b3_values = pickle.load(f).astype('float32')\nG2_W1=tf.get_variable(\"G2_W1\", initializer=G2_W1_values)\ntf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, G2_W1)\nG2_b1 =tf.get_variable(\"G2_b1\", initializer=G2_b1_values)\nG2_W21=tf.get_variable(\"G2_W21\", initializer=G2_W2_values)\ntf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, G2_W21)\n\nG2_W22=tf.Variable(xavier_init([12, 5]), name=\"G2_W2\")\ntf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, G2_W22)\nG2_b21=tf.get_variable(\"G2_b21\", initializer=G2_b2_values)\n\nG2_b22=tf.Variable(xavier_init([1, 5]), name=\"G2_b22\")\n\nG2_W31=tf.get_variable(\"G2_W31\", initializer=G2_W3_values)\ntf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, G2_W31)\n\nG2_W32=tf.Variable(xavier_init([5, 12]), name=\"G2_W32\")\ntf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, G2_W32)\nG2_b3=tf.get_variable(\"G2_b3\", initializer=G2_b3_values)\n\ntheta_G2 = [G2_W22, G2_W32, G2_b22,G2_W31,G2_W32,G2_b3]\n\nD2_W1 = tf.get_variable(\"D2_W1\", initializer=D2_W1_values)\ntf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, D2_W1)\nD2_b1 =tf.get_variable(\"D2_b1\", initializer=D2_b1_values)\n\nD2_W21=tf.get_variable(\"D2_W21\", initializer=D2_W2_values)\ntf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, D2_W21)\n\nD2_W22=tf.Variable(xavier_init([12, 5]), name=\"D2_W2\")\ntf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, D2_W22)\nD2_b21=tf.get_variable(\"D2_b21\", initializer=D2_b2_values)\n\nD2_b22=tf.Variable(xavier_init([1, 5]), name=\"D2_b22\")\n\nD2_W31=tf.get_variable(\"D2_W31\", initializer=D2_W3_values)\ntf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, D2_W31)\n\nD2_W32=tf.Variable(xavier_init([5, 12]), name=\"D2_W32\")\ntf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, D2_W32)\nD2_b3=tf.get_variable(\"D2_b3\", initializer=D2_b3_values)\n\ntheta_D2 = [ D2_W22, D2_W32, D2_b22,D2_W31,D2_W32,D2_b3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"  #%% 1. Generator\ndef generator(new_x,m):\n    inputs = tf.concat(axis = 1, values = [new_x,m])  # Mask + Data Concatenate\n    G2_h1 = tf.nn.relu(tf.matmul(inputs, G2_W1) + G2_b1)\n    G2_h2 = tf.nn.relu(tf.concat([(tf.matmul(G2_h1, G2_W21) + G2_b21),(tf.matmul(G2_h1, G2_W22) + G2_b22)],1))   \n    G_prob = tf.nn.sigmoid(tf.matmul(G2_h2,tf.concat((G2_W31,G2_W32),0)) + G2_b3) # [0,1] normalized Output\n    \n    return G_prob\n    \n  #%% 2. Discriminator\ndef discriminator(new_x, h):\n    inputs = tf.concat(axis = 1, values = [new_x,h])  # Hint + Data Concatenate\n    D2_h1 = tf.nn.relu(tf.matmul(inputs, D2_W1) + D2_b1)  \n    D2_h2 = tf.nn.relu(tf.concat([(tf.matmul(D2_h1, D2_W21) + D2_b21),(tf.matmul(D2_h1, D2_W22) + D2_b22)],1)) \n    D2_logit = tf.matmul(D2_h2, tf.concat((D2_W31,D2_W32),0)) + D2_b3\n    D_prob = tf.nn.sigmoid(D2_logit)  # [0,1] Probability Output\n    \n    return D_prob","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#%% Structure\nX,M,H,New_X=input_placeholder()\n# Generator\nG_sample = generator(New_X,M)\n#print(\"G_sample\",G_sample)\n# Combine with original data\nHat_New_X = New_X * M + G_sample * (1-M)\n# Discriminator\nD_prob = discriminator(Hat_New_X, H)\n#print(\"D_prob\",D_prob)\n#%% Loss\nD_loss1 = -tf.reduce_mean(M * tf.log(D_prob + 1e-8) + (1-M) * tf.log(1. - D_prob + 1e-8)) \nG_loss1 = -tf.reduce_mean((1-M) * tf.log(D_prob + 1e-8))\nMSE_train_loss = tf.reduce_mean((M * New_X - M * G_sample)**2) / tf.reduce_mean(M)\n\nD_loss = D_loss1\nG_loss = G_loss1 + alpha * MSE_train_loss \n\nreg_variables1 = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\nreg_term1 = tf.contrib.layers.apply_regularization(regularizer, reg_variables1)\nD_loss += reg_term1\n\n#%% MSE Performance metric\nMSE_test_loss = tf.reduce_mean(((1-M) * X - (1-M)*G_sample)**2) / tf.reduce_mean(1-M)\n\n#%% Solver\nD_solver = tf.train.AdamOptimizer().minimize(D_loss, var_list=theta_D2)\nG_solver = tf.train.AdamOptimizer().minimize(G_loss, var_list=theta_G2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Data,Missing=normalization(Data)\ntrainX,testX,trainM,testM,Train_No,Test_No=train_test(Data,Missing)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Sessions\nsess = tf.Session()\nsess.run(tf.global_variables_initializer())\n\n#%% Iterations\ntrain_losses2 = []\ntest_losses2 = []\n\n  #%% Start Iterations\nfor it in tqdm(range(1000)):    \n    \n    #%% Inputs\n    mb_idx = sample_idx(Train_No, mb_size)\n    X_mb = trainX[mb_idx,:]  \n   \n    Z_mb = sample_Z(mb_size, Dim) \n#     M_mb = trainM[mb_idx,:]  \n    M_mb = trainM[:mb_size, :]\n    H_mb1 = sample_M(mb_size, Dim, 1-p_hint)\n    H_mb = M_mb * H_mb1\n    \n    New_X_mb = M_mb * X_mb + (1-M_mb) * Z_mb  # Missing Data Introduce\n    \n    _, D_loss_curr = sess.run([D_solver, D_loss1], feed_dict = {M: M_mb, New_X: New_X_mb, H: H_mb})\n    _, G_loss_curr, MSE_train_loss_curr, MSE_test_loss_curr = sess.run([G_solver, G_loss1, MSE_train_loss, MSE_test_loss],\n                                                                      feed_dict = {X: X_mb, M: M_mb, New_X: New_X_mb, H: H_mb})\n            \n        \n    train_losses2.append(np.sqrt(MSE_train_loss_curr))\n    test_losses2.append(np.sqrt(MSE_test_loss_curr))\n    \n#%% Final Loss\n    \nZ_mb = sample_Z(Test_No, Dim) \nM_mb = testM\nX_mb = testX\n        \nNew_X_mb = M_mb * X_mb + (1-M_mb) * Z_mb  # Missing Data Introduce\n    \nMSE_final, Sample = sess.run([MSE_test_loss, G_sample], feed_dict = {X: testX, M: testM, New_X: New_X_mb})\n        \nprint('Final Test RMSE: ' + str(np.sqrt(MSE_final)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## GAN3 with pruning"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Store trained weights and biases\n\nD2_W1_values = sess.run(D2_W1)\nD2_b1_values = sess.run(D2_b1)\nD2_W21_values=sess.run(D2_W21)\nD2_W22_values=sess.run(D2_W22)\nD2_b21_values=sess.run(D2_b21)\nD2_b22_values=sess.run(D2_b22)\nD2_W31_values=sess.run(D2_W31)\nD2_W32_values=sess.run(D2_W32)\nD2_b3_values=sess.run(D2_b3)\n\nG2_W1_values = sess.run(G2_W1)\nG2_b1_values = sess.run(G2_b1)\nG2_W21_values=sess.run(G2_W21)\nG2_W22_values=sess.run(G2_W22)\nG2_b21_values=sess.run(G2_b21)\nG2_b22_values=sess.run(G2_b22)\nG2_W31_values=sess.run(G2_W31)\nG2_W32_values=sess.run(G2_W32)\nG2_b3_values=sess.run(G2_b3)\n\nVal_list2=[D2_W1_values,D2_W21_values,D2_W22_values,D2_W31_values,D2_W32_values,G2_W1_values,G2_W21_values,G2_W22_values,G2_W31_values,G2_W32_values]\nVal_list2_bias=[D2_b1_values,D2_b21_values,D2_b22_values,D2_b3_values,G2_b1_values,G2_b21_values,G2_b22_values,G2_b3_values ]\ntotal=0\nbelow_threshold=0\nfor weights in Val_list2:\n    r,c=weights.shape[:2]\n    total+=r*c\n    #below_threshold+=len(weights[np.where(np.abs(weights)<0.001)])\n    below_threshold+=len(weights[np.where(np.abs(weights)==0)])\np2=total-below_threshold\nb2=total\nsparsity=((total-below_threshold)/total)*100\nparameter.append(p2)\nbaseline.append(b1+b2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import rankdata\n#k=0.99\n\nNEW_WEIGHTS = \"GAIN2\"+str(k)+\".pickle\"\n  \n#print(NEW_WEIGHTS)\nwith open (NEW_WEIGHTS, 'wb') as f:\n    for weights in Val_list2:\n        x=(rankdata(np.abs(weights),method='dense') - 1).astype('float32').reshape(weights.shape)\n        lower_bound_rank = np.ceil(np.max(x)*k).astype('float32')\n        weights[x<=lower_bound_rank]=0\n        pickle.dump(weights, f)\n    for biases in Val_list2_bias:\n        pickle.dump(biases, f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.reset_default_graph()\nData = subsets[2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open('/kaggle/working/'+NEW_WEIGHTS, 'rb') as f:\n    D3_W1_values = pickle.load(f).astype('float32')\n    D3_W21_values = pickle.load(f).astype('float32')\n    D3_W22_values = pickle.load(f).astype('float32')\n    D3_W31_values = pickle.load(f).astype('float32')\n    D3_W32_values = pickle.load(f).astype('float32')\n    G3_W1_values = pickle.load(f).astype('float32')\n    G3_W21_values = pickle.load(f).astype('float32')\n    G3_W22_values = pickle.load(f).astype('float32')\n    G3_W31_values = pickle.load(f).astype('float32')\n    G3_W32_values = pickle.load(f).astype('float32')\n    D3_b1_values = pickle.load(f).astype('float32')\n    D3_b21_values = pickle.load(f).astype('float32')\n    D3_b22_values = pickle.load(f).astype('float32')\n    D3_b3_values = pickle.load(f).astype('float32')\n    G3_b1_values = pickle.load(f).astype('float32')\n    G3_b21_values = pickle.load(f).astype('float32')\n    G3_b22_values = pickle.load(f).astype('float32')\n    G3_b3_values = pickle.load(f).astype('float32')\nG3_W1=tf.get_variable(\"G3_W1\", initializer=G3_W1_values)\ntf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, G3_W1)\nG3_b1 =tf.get_variable(\"G3_b1\", initializer=G3_b1_values)\n\nG3_W21=tf.get_variable(\"G3_W21\", initializer=G3_W21_values)\ntf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, G3_W21)\n\nG3_W22=tf.get_variable(\"G3_W22\", initializer=G3_W22_values)\ntf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, G3_W22)\n\n\nG3_W23=tf.Variable(xavier_init([12, 5]), name=\"G3_W23\")\ntf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, G3_W23)\n\nG3_b21=tf.get_variable(\"G3_b21\", initializer=G3_b21_values)\nG3_b22=tf.get_variable(\"G3_b22\", initializer=G3_b22_values)\n\nG3_b23=tf.Variable(xavier_init([1, 5]), name=\"G3_b23\")\n\n\nG3_W31=tf.get_variable(\"G3_W31\", initializer=G3_W31_values)\ntf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, G3_W31)\n\nG3_W32=tf.get_variable(\"G3_W32\", initializer=G3_W32_values)\ntf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, G3_W32)\n\n\nG3_W33=tf.Variable(xavier_init([5, 12]), name=\"G3_W33\")\ntf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, G3_W33)\n\nG3_b3=tf.get_variable(\"G3_b3\", initializer=G3_b3_values)\n\ntheta_G3 = [G3_W23, G3_W33, G3_b23,G3_W31,G3_W32,G3_W33,G3_b3]\n\n\nD3_W1 = tf.get_variable(\"D3_W1\", initializer=D3_W1_values)\ntf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, D3_W1)\nD3_b1 =tf.get_variable(\"D3_b1\", initializer=D3_b1_values)\n\nD3_W21=tf.get_variable(\"D3_W21\", initializer=D3_W21_values)\ntf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, D3_W21)\nD3_W22=tf.get_variable(\"D3_W22\", initializer=D3_W22_values)\ntf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, D3_W22)\nD3_W23=tf.Variable(xavier_init([12, 5]), name=\"D3_W23\")\ntf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, D3_W23)\nD3_b21=tf.get_variable(\"D3_b21\", initializer=D3_b21_values)\nD3_b22=tf.get_variable(\"D3_b22\", initializer=D3_b22_values)\n\nD3_b23=tf.Variable(xavier_init([1, 5]), name=\"D3_b23\")\n\nD3_W31=tf.get_variable(\"D3_W31\", initializer=D3_W31_values)\ntf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, D3_W31)\nD3_W32=tf.get_variable(\"D3_W32\", initializer=D3_W32_values)\ntf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, D3_W32)\nD3_W33=tf.Variable(xavier_init([5, 12]), name=\"D3_W33\")\ntf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, D3_W33)\nD3_b3=tf.get_variable(\"D3_b3\", initializer=D3_b3_values)\ntheta_D3 = [ D3_W23, D3_W33, D3_b23,D3_W31,D3_W32,D3_W33,D3_b3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#%% GAIN Function\n\n\n#%% 1. Generator\ndef generator(new_x,m):\n    inputs = tf.concat(axis = 1, values = [new_x,m])  # Mask + Data Concatenate\n    G3_h1 = tf.nn.relu(tf.matmul(inputs, G3_W1) + G3_b1)\n    G3_h2 = tf.nn.relu(tf.concat([(tf.matmul(G3_h1, G3_W21) + G3_b21),(tf.matmul(G3_h1, G3_W22) + G3_b22),\n                                 (tf.matmul(G3_h1, G3_W23) + G3_b23)],1))   \n    G_prob = tf.nn.sigmoid(tf.matmul(G3_h2,tf.concat((G3_W31,G3_W32,G3_W33),0)) + G3_b3) # [0,1] normalized Output\n    \n    return G_prob\n    \n#%% 2. Discriminator\ndef discriminator(new_x, h):\n    inputs = tf.concat(axis = 1, values = [new_x,h])  # Hint + Data Concatenate\n    D3_h1 = tf.nn.relu(tf.matmul(inputs, D3_W1) + D3_b1)  \n    D3_h2 = tf.nn.relu(tf.concat([(tf.matmul(D3_h1, D3_W21) + D3_b21),(tf.matmul(D3_h1, D3_W22) + D3_b22),\n                                 (tf.matmul(D3_h1, D3_W23) + D3_b23)],1)) \n    D3_logit = tf.matmul(D3_h2, tf.concat((D3_W31,D3_W32,D3_W33),0)) + D3_b3\n    D_prob = tf.nn.sigmoid(D3_logit)  # [0,1] Probability Output\n    \n    return D_prob","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#%% Structure\nX,M,H,New_X=input_placeholder()\n# Generator\nG_sample = generator(New_X,M)\n#print(\"G_sample\",G_sample)\n# Combine with original data\nHat_New_X = New_X * M + G_sample * (1-M)\n\n# Discriminator\nD_prob = discriminator(Hat_New_X, H)\n#print(\"D_prob\",D_prob)\n#%% Loss\nD_loss1 = -tf.reduce_mean(M * tf.log(D_prob + 1e-8) + (1-M) * tf.log(1. - D_prob + 1e-8)) \nG_loss1 = -tf.reduce_mean((1-M) * tf.log(D_prob + 1e-8))\nMSE_train_loss = tf.reduce_mean((M * New_X - M * G_sample)**2) / tf.reduce_mean(M)\n\nD_loss = D_loss1\nG_loss = G_loss1 + alpha * MSE_train_loss \n\nreg_variables3 = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\nreg_term3 = tf.contrib.layers.apply_regularization(regularizer, reg_variables3)\nD_loss += reg_term3\n\n#%% MSE Performance metric\nMSE_test_loss = tf.reduce_mean(((1-M) * X - (1-M)*G_sample)**2) / tf.reduce_mean(1-M)\n\n#%% Solver\nD_solver = tf.train.AdamOptimizer().minimize(D_loss, var_list=theta_D3)\nG_solver = tf.train.AdamOptimizer().minimize(G_loss, var_list=theta_G3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Data,Missing=normalization(Data)\ntrainX,testX,trainM,testM,Train_No,Test_No=train_test(Data,Missing)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Sessions\nimport time\nsess = tf.Session()\nsess.run(tf.global_variables_initializer())\n\n#%% Iterations\ntrain_losses3 = []\ntest_losses3 = []\nt2=time.time()\n#%% Start Iterations\nfor it in tqdm(range(1000)):    \n    \n    #%% Inputs\n    mb_idx = sample_idx(Train_No, mb_size)\n    X_mb = trainX[mb_idx,:]  \n    \n    Z_mb = sample_Z(mb_size, Dim) \n#     M_mb = trainM[mb_idx,:]  \n    M_mb = trainM[:mb_size, :]\n    H_mb1 = sample_M(mb_size, Dim, 1-p_hint)\n    H_mb = M_mb * H_mb1\n    \n    New_X_mb = M_mb * X_mb + (1-M_mb) * Z_mb  # Missing Data Introduce\n    \n    _, D_loss_curr = sess.run([D_solver, D_loss1], feed_dict = {M: M_mb, New_X: New_X_mb, H: H_mb})\n    _, G_loss_curr, MSE_train_loss_curr, MSE_test_loss_curr = sess.run([G_solver, G_loss1, MSE_train_loss, MSE_test_loss],\n                                                                       feed_dict = {X: X_mb, M: M_mb, New_X: New_X_mb, H: H_mb})\n            \n        \n    #%% Intermediate Losses\n#     if it % 100 == 0:\n#         print('Iter: {}'.format(it))\n#         print('Train_loss: {:.4}'.format(np.sqrt(MSE_train_loss_curr)))\n#         print('Test_loss: {:.4}'.format(np.sqrt(MSE_test_loss_curr)))\n#         print()\n    train_losses3.append(np.sqrt(MSE_train_loss_curr))\n    test_losses3.append(np.sqrt(MSE_test_loss_curr))\n    \n#%% Final Loss\nt3=time.time()    \nZ_mb = sample_Z(Test_No, Dim) \nM_mb = testM\nX_mb = testX\n        \nNew_X_mb = M_mb * X_mb + (1-M_mb) * Z_mb  # Missing Data Introduce\n    \nMSE_final, Sample = sess.run([MSE_test_loss, G_sample], feed_dict = {X: testX, M: testM, New_X: New_X_mb})\n        \nprint('Final Test RMSE: ' + str(np.sqrt(MSE_final)))\nprint(\"Time cost: \",t3-t2)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## GAN4 with pruning"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Store trained weights and biases\nD3_W1_values = sess.run(D3_W1)\nD3_b1_values = sess.run(D3_b1)\nD3_W21_values = sess.run(D3_W21)\nD3_W22_values = sess.run(D3_W22)\nD3_W23_values = sess.run(D3_W23)\nD3_b21_values = sess.run(D3_b21)\nD3_b22_values = sess.run(D3_b22)\nD3_b23_values = sess.run(D3_b23)\nD3_W31_values = sess.run(D3_W31)\nD3_W32_values = sess.run(D3_W32)\nD3_W33_values = sess.run(D3_W33)\nD3_b3_values = sess.run(D3_b3)\n\nG3_W1_values = sess.run(G3_W1)\nG3_b1_values = sess.run(G3_b1)\nG3_W21_values = sess.run(G3_W21)\nG3_W22_values = sess.run(G3_W22)\nG3_W23_values = sess.run(G3_W23)\nG3_b21_values = sess.run(G3_b21)\nG3_b22_values = sess.run(G3_b22)\nG3_b23_values = sess.run(G3_b23)\nG3_W31_values = sess.run(G3_W31)\nG3_W32_values = sess.run(G3_W32)\nG3_W33_values = sess.run(G3_W33)\nG3_b3_values = sess.run(G3_b3)\n\nVal_list3=[D3_W1_values,D3_W21_values,D3_W22_values,D3_W23_values,D3_W31_values,D3_W32_values,D3_W33_values,G3_W1_values,\n          G3_W21_values,G3_W22_values,G3_W23_values,G3_W31_values,G3_W32_values,G3_W33_values]\nVal_list3_bias=[D3_b1_values,D3_b21_values,D3_b22_values,D3_b23_values,D3_b3_values,G3_b1_values,\n          G3_b21_values,G3_b22_values,G3_b23_values,G3_b3_values]\n\ntotal=0\nbelow_threshold=0\nfor weights in Val_list3:\n    r,c=weights.shape[:2]\n    total+=r*c\n    #below_threshold+=len(weights[np.where(np.abs(weights)<0.001)])\n    below_threshold+=len(weights[np.where(np.abs(weights)==0)])\np3=total-below_threshold\nb3=total\nsparsity=((total-below_threshold)/total)*100\nparameter.append(p3)\nbaseline.append(b1+b2+b3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import rankdata\n#k=0.99\n\nNEW_WEIGHTS = \"GAIN3\"+str(k)+\".pickle\"\n  \n#print(NEW_WEIGHTS)\nwith open (NEW_WEIGHTS, 'wb') as f:\n    for weights in Val_list3:\n        x=(rankdata(np.abs(weights),method='dense') - 1).astype('float32').reshape(weights.shape)\n        lower_bound_rank = np.ceil(np.max(x)*k).astype('float32')\n        weights[x<=lower_bound_rank]=0\n        pickle.dump(weights, f)\n    for biases in Val_list3_bias:\n        pickle.dump(biases, f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.reset_default_graph()\nData = subsets[3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open('/kaggle/working/'+NEW_WEIGHTS, 'rb') as f:\n    D4_W1_values = pickle.load(f).astype('float32')\n    D4_W21_values = pickle.load(f).astype('float32')\n    D4_W22_values = pickle.load(f).astype('float32')\n    D4_W23_values = pickle.load(f).astype('float32')\n    D4_W31_values = pickle.load(f).astype('float32')\n    D4_W32_values = pickle.load(f).astype('float32')\n    D4_W33_values = pickle.load(f).astype('float32')\n    G4_W1_values = pickle.load(f).astype('float32')\n    G4_W21_values = pickle.load(f).astype('float32')\n    G4_W22_values = pickle.load(f).astype('float32')\n    G4_W23_values = pickle.load(f).astype('float32')\n    G4_W31_values = pickle.load(f).astype('float32')\n    G4_W32_values = pickle.load(f).astype('float32')\n    G4_W33_values = pickle.load(f).astype('float32')\n    D4_b1_values = pickle.load(f).astype('float32')\n    D4_b21_values = pickle.load(f).astype('float32')\n    D4_b22_values = pickle.load(f).astype('float32')\n    D4_b23_values = pickle.load(f).astype('float32')\n    D4_b3_values = pickle.load(f).astype('float32')\n    G4_b1_values = pickle.load(f).astype('float32')\n    G4_b21_values = pickle.load(f).astype('float32')\n    G4_b22_values = pickle.load(f).astype('float32')\n    G4_b23_values = pickle.load(f).astype('float32')\n    G4_b3_values = pickle.load(f).astype('float32')\n\nG4_W1=tf.get_variable(\"G4_W1\", initializer=G4_W1_values)\ntf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, G4_W1)\nG4_b1 =tf.get_variable(\"G4_b1\", initializer=G4_b1_values)\n\nG4_W21=tf.get_variable(\"G4_W21\", initializer=G4_W21_values)\ntf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, G4_W21)\nG4_W22=tf.get_variable(\"G4_W22\", initializer=G4_W22_values)\ntf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, G4_W22)\nG4_W23=tf.get_variable(\"G4_W23\", initializer=G4_W23_values)\ntf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, G4_W23)\nG4_W24=tf.Variable(xavier_init([12, 5]), name=\"G4_W24\")\ntf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, G4_W24)\nG4_b21=tf.get_variable(\"G4_b21\", initializer=G4_b21_values)\nG4_b22=tf.get_variable(\"G4_b22\", initializer=G4_b22_values)\nG4_b23=tf.get_variable(\"G4_b23\", initializer=G4_b23_values)\nG4_b24=tf.Variable(xavier_init([1, 5]), name=\"G4_b24\")\n\n\nG4_W31=tf.get_variable(\"G4_W31\", initializer=G4_W31_values)\ntf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, G4_W31)\nG4_W32=tf.get_variable(\"G4_W32\", initializer=G4_W32_values)\ntf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, G4_W32)\nG4_W33=tf.get_variable(\"G4_W33\", initializer=G4_W33_values)\ntf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, G4_W33)\nG4_W34=tf.Variable(xavier_init([5, 12]), name=\"G4_W34\")\ntf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, G4_W34)\nG4_b3=tf.get_variable(\"G4_b3\", initializer=G4_b3_values)\n\ntheta_G4 = [G4_W24, G4_W34, G4_b24,G4_W31,G4_W32,G4_W33,G4_W34,G4_b3]\n\nD4_W1 = tf.get_variable(\"D4_W1\", initializer=D4_W1_values)\ntf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, D4_W1)\nD4_b1 =tf.get_variable(\"D4_b1\", initializer=D4_b1_values)\n\nD4_W21=tf.get_variable(\"D4_W21\", initializer=D4_W21_values)\ntf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, D4_W21)\nD4_W22=tf.get_variable(\"D4_W22\", initializer=D4_W22_values)\ntf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, D4_W22)\nD4_W23=tf.get_variable(\"D4_W23\", initializer=D4_W23_values)\ntf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, D4_W23)\n\nD4_W24=tf.Variable(xavier_init([12, 5]), name=\"D4_W24\")\ntf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, D4_W24)\nD4_b21=tf.get_variable(\"D4_b21\", initializer=D4_b21_values)\nD4_b22=tf.get_variable(\"D4_b22\", initializer=D4_b22_values)\nD4_b23=tf.get_variable(\"D4_b23\", initializer=D4_b23_values)\nD4_b24=tf.Variable(xavier_init([1, 5]), name=\"D4_b24\")\n\nD4_W31=tf.get_variable(\"D4_W31\", initializer=D4_W31_values)\ntf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, D4_W31)\nD4_W32=tf.get_variable(\"D4_W32\", initializer=D4_W32_values)\ntf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, D4_W32)\nD4_W33=tf.get_variable(\"D4_W33\", initializer=D4_W33_values)\ntf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, D4_W33)\nD4_W34=tf.Variable(xavier_init([5, 12]), name=\"D4_W34\")\ntf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, D4_W34)\nD4_b3=tf.get_variable(\"D4_b3\", initializer=D4_b3_values)\ntheta_D4 = [ D4_W24, D4_W34, D4_b24,D4_W31,D4_W32,D4_W33,D4_W34,D4_b3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#%% 1. Generator\ndef generator(new_x,m):\n    inputs = tf.concat(axis = 1, values = [new_x,m])  # Mask + Data Concatenate\n    G4_h1 = tf.nn.relu(tf.matmul(inputs, G4_W1) + G4_b1)\n    G4_h2 = tf.nn.relu(tf.concat([(tf.matmul(G4_h1, G4_W21) + G4_b21),(tf.matmul(G4_h1, G4_W22) + G4_b22),\n                                 (tf.matmul(G4_h1, G4_W23) + G4_b23),(tf.matmul(G4_h1, G4_W24) + G4_b24)],1))   \n    G_prob = tf.nn.sigmoid(tf.matmul(G4_h2,tf.concat((G4_W31,G4_W32,G4_W33,G4_W34),0)) + G4_b3) # [0,1] normalized Output\n    \n    return G_prob\n    \n#%% 2. Discriminator\ndef discriminator(new_x, h):\n    inputs = tf.concat(axis = 1, values = [new_x,h])  # Hint + Data Concatenate\n    D4_h1 = tf.nn.relu(tf.matmul(inputs, D4_W1) + D4_b1)  \n    D4_h2 = tf.nn.relu(tf.concat([(tf.matmul(D4_h1, D4_W21) + D4_b21),(tf.matmul(D4_h1, D4_W22) + D4_b22),\n                                 (tf.matmul(D4_h1, D4_W23) + D4_b23),(tf.matmul(D4_h1, D4_W24) + D4_b24)],1)) \n    D4_logit = tf.matmul(D4_h2, tf.concat((D4_W31,D4_W32,D4_W33,D4_W34),0)) + D4_b3\n    D_prob = tf.nn.sigmoid(D4_logit)  # [0,1] Probability Output\n    \n    return D_prob","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#%% Structure\nX,M,H,New_X=input_placeholder()\n# Generator\nG_sample = generator(New_X,M)\n#print(\"G_sample\",G_sample)\n# Combine with original data\nHat_New_X = New_X * M + G_sample * (1-M)\n\n# Discriminator\nD_prob = discriminator(Hat_New_X, H)\n#print(\"D_prob\",D_prob)\n#%% Loss\nD_loss1 = -tf.reduce_mean(M * tf.log(D_prob + 1e-8) + (1-M) * tf.log(1. - D_prob + 1e-8)) \nG_loss1 = -tf.reduce_mean((1-M) * tf.log(D_prob + 1e-8))\nMSE_train_loss = tf.reduce_mean((M * New_X - M * G_sample)**2) / tf.reduce_mean(M)\n\nD_loss = D_loss1\nG_loss = G_loss1 + alpha * MSE_train_loss \n\nreg_variables4 = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\nreg_term4 = tf.contrib.layers.apply_regularization(regularizer, reg_variables4)\nD_loss += reg_term4\n#%% MSE Performance metric\nMSE_test_loss = tf.reduce_mean(((1-M) * X - (1-M)*G_sample)**2) / tf.reduce_mean(1-M)\n\n#%% Solver\nD_solver = tf.train.AdamOptimizer().minimize(D_loss, var_list=theta_D4)\nG_solver = tf.train.AdamOptimizer().minimize(G_loss, var_list=theta_G4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Data,Missing=normalization(Data)\ntrainX,testX,trainM,testM,Train_No,Test_No=train_test(Data,Missing)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Sessions\nsess = tf.Session()\nsess.run(tf.global_variables_initializer())\n\n#%% Iterations\ntrain_losses4 = []\ntest_losses4 = []\nt4=time.time()\n#%% Start Iterations\nfor it in tqdm(range(1000)):    \n    \n    #%% Inputs\n    mb_idx = sample_idx(Train_No, mb_size)\n    X_mb = trainX[mb_idx,:]  \n    \n    Z_mb = sample_Z(mb_size, Dim) \n#     M_mb = trainM[mb_idx,:]  \n    M_mb = trainM[:mb_size, :]\n    H_mb1 = sample_M(mb_size, Dim, 1-p_hint)\n    H_mb = M_mb * H_mb1\n    \n    New_X_mb = M_mb * X_mb + (1-M_mb) * Z_mb  # Missing Data Introduce\n    \n    _, D_loss_curr = sess.run([D_solver, D_loss1], feed_dict = {M: M_mb, New_X: New_X_mb, H: H_mb})\n    _, G_loss_curr, MSE_train_loss_curr, MSE_test_loss_curr = sess.run([G_solver, G_loss1, MSE_train_loss, MSE_test_loss],\n                                                                       feed_dict = {X: X_mb, M: M_mb, New_X: New_X_mb, H: H_mb})\n            \n        \n    #%% Intermediate Losses\n#     if it % 100 == 0:\n#         print('Iter: {}'.format(it))\n#         print('Train_loss: {:.4}'.format(np.sqrt(MSE_train_loss_curr)))\n#         print('Test_loss: {:.4}'.format(np.sqrt(MSE_test_loss_curr)))\n#         print()\n    train_losses4.append(np.sqrt(MSE_train_loss_curr))\n    test_losses4.append(np.sqrt(MSE_test_loss_curr))\n    \n#%% Final Loss\nt5=time.time()    \nZ_mb = sample_Z(Test_No, Dim) \nM_mb = testM\nX_mb = testX\n        \nNew_X_mb = M_mb * X_mb + (1-M_mb) * Z_mb  # Missing Data Introduce\n    \nMSE_final, Sample = sess.run([MSE_test_loss, G_sample], feed_dict = {X: testX, M: testM, New_X: New_X_mb})\n\nprint('Final Test RMSE after pruning: ' + str(np.sqrt(MSE_final)))\n        \n#print('Final Test RMSE: ' + str(np.sqrt(MSE_final)))\nprint(\"Time cost after pruning: \",t5-t4)\n#sess.close()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## GAN5 with pruning"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Store trained weights and biases\nD4_W1_values = sess.run(D4_W1)\nD4_b1_values = sess.run(D4_b1)\nD4_W21_values = sess.run(D4_W21)\nD4_W22_values = sess.run(D4_W22)\nD4_W23_values = sess.run(D4_W23)\nD4_W24_values = sess.run(D4_W24)\nD4_b21_values = sess.run(D4_b21)\nD4_b22_values = sess.run(D4_b22)\nD4_b23_values = sess.run(D4_b23)\nD4_b24_values = sess.run(D4_b24)\nD4_W31_values = sess.run(D4_W31)\nD4_W32_values = sess.run(D4_W32)\nD4_W33_values = sess.run(D4_W33)\nD4_W34_values = sess.run(D4_W34)\nD4_b3_values = sess.run(D4_b3)\n\nG4_W1_values = sess.run(G4_W1)\nG4_b1_values = sess.run(G4_b1)\nG4_W21_values = sess.run(G4_W21)\nG4_W22_values = sess.run(G4_W22)\nG4_W23_values = sess.run(G4_W23)\nG4_W24_values = sess.run(G4_W24)\nG4_b21_values = sess.run(G4_b21)\nG4_b22_values = sess.run(G4_b22)\nG4_b23_values = sess.run(G4_b23)\nG4_b24_values = sess.run(G4_b24)\nG4_W31_values = sess.run(G4_W31)\nG4_W32_values = sess.run(G4_W32)\nG4_W33_values = sess.run(G4_W33)\nG4_W34_values = sess.run(G4_W34)\nG4_b3_values = sess.run(G4_b3)\n\nVal_list4=[D4_W1_values,D4_W21_values,D4_W22_values,D4_W23_values,D4_W24_values,D4_W31_values,D4_W32_values,D4_W33_values,D4_W34_values,G4_W1_values,\n          G4_W21_values,G4_W22_values,G4_W23_values,G4_W24_values,G4_W31_values,G4_W32_values,G4_W33_values,G4_W34_values]\nVal_list4_bias=[D4_b1_values,D4_b21_values,D4_b22_values,D4_b23_values,D4_b24_values,D4_b3_values,G4_b1_values,\n          G4_b21_values,G4_b22_values,G4_b23_values,G4_b24_values,G4_b3_values]\n\ntotal=0\nbelow_threshold=0\nfor weights in Val_list4:\n    r,c=weights.shape[:2]\n    total+=r*c\n    #below_threshold+=len(weights[np.where(np.abs(weights)<0.001)])\n    below_threshold+=len(weights[np.where(np.abs(weights)==0)])\np4=total-below_threshold\nb4=total\n\nparameter.append(p4)\nbaseline.append(b1+b2+b3+b4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.reset_default_graph()\nData = subsets[4]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import rankdata\n#k=0.99\n\nNEW_WEIGHTS = \"GAIN4\"+str(k)+\".pickle\"\n  \n#print(NEW_WEIGHTS)\nwith open (NEW_WEIGHTS, 'wb') as f:\n    for weights in Val_list4:\n        x=(rankdata(np.abs(weights),method='dense') - 1).astype('float32').reshape(weights.shape)\n        lower_bound_rank = np.ceil(np.max(x)*k).astype('float32')\n        weights[x<=lower_bound_rank]=0\n        pickle.dump(weights, f)\n    for biases in Val_list4_bias:\n        pickle.dump(biases, f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open('/kaggle/working/'+NEW_WEIGHTS, 'rb') as f:\n    D4_W1_values = pickle.load(f).astype('float32')\n    D4_W21_values = pickle.load(f).astype('float32')\n    D4_W22_values = pickle.load(f).astype('float32')\n    D4_W23_values = pickle.load(f).astype('float32')\n    D4_W24_values = pickle.load(f).astype('float32')\n    D4_W31_values = pickle.load(f).astype('float32')\n    D4_W32_values = pickle.load(f).astype('float32')\n    D4_W33_values = pickle.load(f).astype('float32')\n    D4_W34_values = pickle.load(f).astype('float32')\n    G4_W1_values = pickle.load(f).astype('float32')\n    G4_W21_values = pickle.load(f).astype('float32')\n    G4_W22_values = pickle.load(f).astype('float32')\n    G4_W23_values = pickle.load(f).astype('float32')\n    G4_W24_values = pickle.load(f).astype('float32')\n    G4_W31_values = pickle.load(f).astype('float32')\n    G4_W32_values = pickle.load(f).astype('float32')\n    G4_W33_values = pickle.load(f).astype('float32')\n    G4_W34_values = pickle.load(f).astype('float32')\n    D4_b1_values = pickle.load(f).astype('float32')\n    D4_b21_values = pickle.load(f).astype('float32')\n    D4_b22_values = pickle.load(f).astype('float32')\n    D4_b23_values = pickle.load(f).astype('float32')\n    D4_b24_values = pickle.load(f).astype('float32')\n    D4_b3_values = pickle.load(f).astype('float32')\n    G4_b1_values = pickle.load(f).astype('float32')\n    G4_b21_values = pickle.load(f).astype('float32')\n    G4_b22_values = pickle.load(f).astype('float32')\n    G4_b23_values = pickle.load(f).astype('float32')\n    G4_b24_values = pickle.load(f).astype('float32')\n    G4_b3_values = pickle.load(f).astype('float32')\n\nG5_W1=tf.get_variable(\"G5_W1\", initializer=G4_W1_values)\ntf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, G5_W1)\nG5_b1 =tf.get_variable(\"G5_b1\", initializer=G4_b1_values)\n\nG5_W21=tf.get_variable(\"G5_W21\", initializer=G4_W21_values)\ntf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, G5_W21)\nG5_W22=tf.get_variable(\"G5_W22\", initializer=G4_W22_values)\ntf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, G5_W22)\nG5_W23=tf.get_variable(\"G5_W23\", initializer=G4_W23_values)\ntf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, G5_W23)\nG5_W24=tf.get_variable(\"G5_W24\", initializer=G4_W24_values)\ntf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, G5_W23)\nG5_W25=tf.Variable(xavier_init([12, 5]), name=\"G5_W25\")\ntf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, G5_W25)\nG5_b21=tf.get_variable(\"G5_b21\", initializer=G4_b21_values)\nG5_b22=tf.get_variable(\"G5_b22\", initializer=G4_b22_values)\nG5_b23=tf.get_variable(\"G5_b23\", initializer=G4_b23_values)\nG5_b24=tf.get_variable(\"G5_b24\", initializer=G4_b24_values)\nG5_b25=tf.Variable(xavier_init([1, 5]), name=\"G5_b25\")\n\n\nG5_W31=tf.get_variable(\"G5_W31\", initializer=G4_W31_values)\ntf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, G5_W31)\nG5_W32=tf.get_variable(\"G5_W32\", initializer=G4_W32_values)\ntf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, G5_W32)\nG5_W33=tf.get_variable(\"G5_W33\", initializer=G4_W33_values)\ntf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, G5_W33)\nG5_W34=tf.get_variable(\"G5_W34\", initializer=G4_W34_values)\ntf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, G5_W34)\nG5_W35=tf.Variable(xavier_init([5, 12]), name=\"G5_W35\")\ntf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, G5_W35)\nG5_b3=tf.get_variable(\"G5_b3\", initializer=G4_b3_values)\n\ntheta_G5 = [G5_W25, G5_W35, G5_b25,G5_W31,G5_W32,G5_W33,G5_W34,G5_W35,G5_b3]\n\n\nD5_W1 = tf.get_variable(\"D5_W1\", initializer=D4_W1_values)\ntf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, D5_W1)\nD5_b1 =tf.get_variable(\"D5_b1\", initializer=D4_b1_values)\n\nD5_W21=tf.get_variable(\"D5_W21\", initializer=D4_W21_values)\ntf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, D5_W21)\nD5_W22=tf.get_variable(\"D5_W22\", initializer=D4_W22_values)\ntf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, D5_W22)\nD5_W23=tf.get_variable(\"D5_W23\", initializer=D4_W23_values)\ntf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, D5_W23)\nD5_W24=tf.get_variable(\"D5_W24\", initializer=D4_W24_values)\ntf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, D5_W24)\nD5_W25=tf.Variable(xavier_init([12, 5]), name=\"D5_W25\")\ntf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, D5_W25)\nD5_b21=tf.get_variable(\"D5_b21\", initializer=D4_b21_values)\nD5_b22=tf.get_variable(\"D5_b22\", initializer=D4_b22_values)\nD5_b23=tf.get_variable(\"D5_b23\", initializer=D4_b23_values)\nD5_b24=tf.get_variable(\"D5_b24\", initializer=D4_b24_values)\nD5_b25=tf.Variable(xavier_init([1, 5]), name=\"D5_b25\")\n\nD5_W31=tf.get_variable(\"D5_W31\", initializer=D4_W31_values)\ntf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, D5_W31)\nD5_W32=tf.get_variable(\"D5_W32\", initializer=D4_W32_values)\ntf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, D5_W32)\nD5_W33=tf.get_variable(\"D5_W33\", initializer=D4_W33_values)\ntf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, D5_W33)\nD5_W34=tf.get_variable(\"D5_W34\", initializer=D4_W34_values)\ntf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, D5_W34)\nD5_W35=tf.Variable(xavier_init([5, 12]), name=\"D5_W35\")\ntf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, D5_W35)\nD5_b3=tf.get_variable(\"D5_b3\", initializer=D4_b3_values)\ntheta_D5 = [ D5_W25, D5_W35, D5_b25,D5_W31,D5_W32,D5_W33,D5_W34,D5_W35,D5_b3]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#%% 1. Generator\ndef generator(new_x,m):\n    inputs = tf.concat(axis = 1, values = [new_x,m])  # Mask + Data Concatenate\n    G5_h1 = tf.nn.relu(tf.matmul(inputs, G5_W1) + G5_b1)\n    G5_h2 = tf.nn.relu(tf.concat([(tf.matmul(G5_h1, G5_W21) + G5_b21),(tf.matmul(G5_h1, G5_W22) + G5_b22),\n                                 (tf.matmul(G5_h1, G5_W23) + G5_b23),(tf.matmul(G5_h1, G5_W24) + G5_b24),\n                                 (tf.matmul(G5_h1, G5_W25) + G5_b25)],1))   \n    G_prob = tf.nn.sigmoid(tf.matmul(G5_h2,tf.concat((G5_W31,G5_W32,G5_W33,G5_W34,G5_W35),0)) + G5_b3) # [0,1] normalized Output\n    \n    return G_prob\n    \n#%% 2. Discriminator\ndef discriminator(new_x, h):\n    inputs = tf.concat(axis = 1, values = [new_x,h])  # Hint + Data Concatenate\n    D5_h1 = tf.nn.relu(tf.matmul(inputs, D5_W1) + D5_b1)  \n    D5_h2 = tf.nn.relu(tf.concat([(tf.matmul(D5_h1, D5_W21) + D5_b21),(tf.matmul(D5_h1, D5_W22) + D5_b22),\n                                 (tf.matmul(D5_h1, D5_W23) + D5_b23),(tf.matmul(D5_h1, D5_W24) + D5_b24),\n                                 (tf.matmul(D5_h1, D5_W25) + D5_b25)],1)) \n    D5_logit = tf.matmul(D5_h2, tf.concat((D5_W31,D5_W32,D5_W33,D5_W34,D5_W35),0)) + D5_b3\n    D_prob = tf.nn.sigmoid(D5_logit)  # [0,1] Probability Output\n    \n    return D_prob","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#%% Structure\nX,M,H,New_X=input_placeholder()\n# Generator\nG_sample = generator(New_X,M)\n#print(\"G_sample\",G_sample)\n# Combine with original data\nHat_New_X = New_X * M + G_sample * (1-M)\n\n# Discriminator\nD_prob = discriminator(Hat_New_X, H)\n#print(\"D_prob\",D_prob)\n#%% Loss\nD_loss1 = -tf.reduce_mean(M * tf.log(D_prob + 1e-8) + (1-M) * tf.log(1. - D_prob + 1e-8)) \nG_loss1 = -tf.reduce_mean((1-M) * tf.log(D_prob + 1e-8))\nMSE_train_loss = tf.reduce_mean((M * New_X - M * G_sample)**2) / tf.reduce_mean(M)\n\nD_loss = D_loss1\nG_loss = G_loss1 + alpha * MSE_train_loss \n\nreg_variables5 = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\nreg_term5 = tf.contrib.layers.apply_regularization(regularizer, reg_variables5)\nD_loss += reg_term5\n#%% MSE Performance metric\nMSE_test_loss = tf.reduce_mean(((1-M) * X - (1-M)*G_sample)**2) / tf.reduce_mean(1-M)\n\n#%% Solver\nD_solver = tf.train.AdamOptimizer().minimize(D_loss, var_list=theta_D5)\nG_solver = tf.train.AdamOptimizer().minimize(G_loss, var_list=theta_G5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Data,Missing=normalization(Data)\ntrainX,testX,trainM,testM,Train_No,Test_No=train_test(Data,Missing)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Sessions\nsess = tf.Session()\nsess.run(tf.global_variables_initializer())\n\n#%% Iterations\ntrain_losses5 = []\ntest_losses5 = []\nt6=time.time()\n#%% Start Iterations\nfor it in tqdm(range(1000)):    \n    \n    #%% Inputs\n    mb_idx = sample_idx(Train_No, mb_size)\n    X_mb = trainX[mb_idx,:]  \n    \n    Z_mb = sample_Z(mb_size, Dim) \n#     M_mb = trainM[mb_idx,:]  \n    M_mb = trainM[:mb_size, :]\n    H_mb1 = sample_M(mb_size, Dim, 1-p_hint)\n    H_mb = M_mb * H_mb1\n    \n    New_X_mb = M_mb * X_mb + (1-M_mb) * Z_mb  # Missing Data Introduce\n    \n    _, D_loss_curr = sess.run([D_solver, D_loss1], feed_dict = {M: M_mb, New_X: New_X_mb, H: H_mb})\n    _, G_loss_curr, MSE_train_loss_curr, MSE_test_loss_curr = sess.run([G_solver, G_loss1, MSE_train_loss, MSE_test_loss],\n                                                                       feed_dict = {X: X_mb, M: M_mb, New_X: New_X_mb, H: H_mb})\n            \n        \n    #%% Intermediate Losses\n#     if it % 100 == 0:\n#         print('Iter: {}'.format(it))\n#         print('Train_loss: {:.4}'.format(np.sqrt(MSE_train_loss_curr)))\n#         print('Test_loss: {:.4}'.format(np.sqrt(MSE_test_loss_curr)))\n#         print()\n    train_losses5.append(np.sqrt(MSE_train_loss_curr))\n    test_losses5.append(np.sqrt(MSE_test_loss_curr))\n    \n#%% Final Loss\nt7=time.time()    \nZ_mb = sample_Z(Test_No, Dim) \nM_mb = testM\nX_mb = testX\n        \nNew_X_mb = M_mb * X_mb + (1-M_mb) * Z_mb  # Missing Data Introduce\n    \nMSE_final, Sample = sess.run([MSE_test_loss, G_sample], feed_dict = {X: testX, M: testM, New_X: New_X_mb})\n        \nprint('Final Test RMSE: ' + str(np.sqrt(MSE_final)))\nprint(\"Time cost: \",t7-t6)\n#sess.close()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## GAN6 after pruning"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Store trained weights and biases\nD5_W1_values = sess.run(D5_W1)\nD5_b1_values = sess.run(D5_b1)\nD5_W21_values = sess.run(D5_W21)\nD5_W22_values = sess.run(D5_W22)\nD5_W23_values = sess.run(D5_W23)\nD5_W24_values = sess.run(D5_W24)\nD5_W25_values = sess.run(D5_W25)\nD5_b21_values = sess.run(D5_b21)\nD5_b22_values = sess.run(D5_b22)\nD5_b23_values = sess.run(D5_b23)\nD5_b24_values = sess.run(D5_b24)\nD5_b25_values = sess.run(D5_b25)\nD5_W31_values = sess.run(D5_W31)\nD5_W32_values = sess.run(D5_W32)\nD5_W33_values = sess.run(D5_W33)\nD5_W34_values = sess.run(D5_W34)\nD5_W35_values = sess.run(D5_W35)\nD5_b3_values = sess.run(D5_b3)\n\nG5_W1_values = sess.run(G5_W1)\nG5_b1_values = sess.run(G5_b1)\nG5_W21_values = sess.run(G5_W21)\nG5_W22_values = sess.run(G5_W22)\nG5_W23_values = sess.run(G5_W23)\nG5_W24_values = sess.run(G5_W24)\nG5_W25_values = sess.run(G5_W25)\nG5_b21_values = sess.run(G5_b21)\nG5_b22_values = sess.run(G5_b22)\nG5_b23_values = sess.run(G5_b23)\nG5_b24_values = sess.run(G5_b24)\nG5_b25_values = sess.run(G5_b25)\nG5_W31_values = sess.run(G5_W31)\nG5_W32_values = sess.run(G5_W32)\nG5_W33_values = sess.run(G5_W33)\nG5_W34_values = sess.run(G5_W34)\nG5_W35_values = sess.run(G5_W35)\nG5_b3_values = sess.run(G5_b3)\n\nVal_list5=[D5_W1_values,D5_W21_values,D5_W22_values,D5_W23_values,D5_W24_values,D5_W25_values,D5_W31_values,D5_W32_values,D5_W33_values,D5_W34_values,D5_W35_values,G5_W1_values,\n          G5_W21_values,G5_W22_values,G5_W23_values,G5_W24_values,G5_W25_values,G5_W31_values,G5_W32_values,G5_W33_values,G5_W34_values,G5_W35_values]\nVal_list5_bias=[D5_b1_values,D5_b21_values,D5_b22_values,D5_b23_values,D5_b24_values,D5_b25_values,D5_b3_values,G5_b1_values,\n          G5_b21_values,G5_b22_values,G5_b23_values,G5_b24_values,G5_b25_values,G5_b3_values]\n\ntotal=0\nbelow_threshold=0\nfor weights in Val_list5:\n    r,c=weights.shape[:2]\n    total+=r*c\n    #below_threshold+=len(weights[np.where(np.abs(weights)<0.001)])\n    below_threshold+=len(weights[np.where(np.abs(weights)==0)])\np5=total-below_threshold\nb5=total\n\nparameter.append(p5)\nbaseline.append(b1+b2+b3+b4+b5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.reset_default_graph()\nData = subsets[5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import rankdata\n#k=0.99\n\nNEW_WEIGHTS = \"GAIN5\"+str(k)+\".pickle\"\n  \n#print(NEW_WEIGHTS)\nwith open (NEW_WEIGHTS, 'wb') as f:\n    for weights in Val_list5:\n        x=(rankdata(np.abs(weights),method='dense') - 1).astype('float32').reshape(weights.shape)\n        lower_bound_rank = np.ceil(np.max(x)*k).astype('float32')\n        weights[x<=lower_bound_rank]=0\n        pickle.dump(weights, f)\n    for biases in Val_list5_bias:\n        pickle.dump(biases, f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open('/kaggle/working/'+NEW_WEIGHTS, 'rb') as f:\n    D5_W1_values = pickle.load(f).astype('float32')\n    D5_W21_values = pickle.load(f).astype('float32')\n    D5_W22_values = pickle.load(f).astype('float32')\n    D5_W23_values = pickle.load(f).astype('float32')\n    D5_W24_values = pickle.load(f).astype('float32')\n    D5_W25_values = pickle.load(f).astype('float32')\n    D5_W31_values = pickle.load(f).astype('float32')\n    D5_W32_values = pickle.load(f).astype('float32')\n    D5_W33_values = pickle.load(f).astype('float32')\n    D5_W34_values = pickle.load(f).astype('float32')\n    D5_W35_values = pickle.load(f).astype('float32')\n    G5_W1_values = pickle.load(f).astype('float32')\n    G5_W21_values = pickle.load(f).astype('float32')\n    G5_W22_values = pickle.load(f).astype('float32')\n    G5_W23_values = pickle.load(f).astype('float32')\n    G5_W24_values = pickle.load(f).astype('float32')\n    G5_W25_values = pickle.load(f).astype('float32')\n    G5_W31_values = pickle.load(f).astype('float32')\n    G5_W32_values = pickle.load(f).astype('float32')\n    G5_W33_values = pickle.load(f).astype('float32')\n    G5_W34_values = pickle.load(f).astype('float32')\n    G5_W35_values = pickle.load(f).astype('float32')\n    D5_b1_values = pickle.load(f).astype('float32')\n    D5_b21_values = pickle.load(f).astype('float32')\n    D5_b22_values = pickle.load(f).astype('float32')\n    D5_b23_values = pickle.load(f).astype('float32')\n    D5_b24_values = pickle.load(f).astype('float32')\n    D5_b25_values = pickle.load(f).astype('float32')\n    D5_b3_values = pickle.load(f).astype('float32')\n    G5_b1_values = pickle.load(f).astype('float32')\n    G5_b21_values = pickle.load(f).astype('float32')\n    G5_b22_values = pickle.load(f).astype('float32')\n    G5_b23_values = pickle.load(f).astype('float32')\n    G5_b24_values = pickle.load(f).astype('float32')\n    G5_b25_values = pickle.load(f).astype('float32')\n    G5_b3_values = pickle.load(f).astype('float32')\n\nG6_W1=tf.get_variable(\"G6_W1\", initializer=G5_W1_values)\ntf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, G6_W1)\nG6_b1 =tf.get_variable(\"G6_b1\", initializer=G5_b1_values)\n\nG6_W21=tf.get_variable(\"G6_W21\", initializer=G5_W21_values)\ntf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, G6_W21)\nG6_W22=tf.get_variable(\"G6_W22\", initializer=G5_W22_values)\ntf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, G6_W22)\nG6_W23=tf.get_variable(\"G6_W23\", initializer=G5_W23_values)\ntf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, G6_W23)\nG6_W24=tf.get_variable(\"G6_W24\", initializer=G5_W24_values)\ntf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, G6_W23)\nG6_W25=tf.get_variable(\"G6_W25\", initializer=G5_W25_values)\ntf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, G6_W25)\nG6_W26=tf.Variable(xavier_init([12, 5]), name=\"G6_W26\")\ntf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, G6_W26)\nG6_b21=tf.get_variable(\"G6_b21\", initializer=G5_b21_values)\nG6_b22=tf.get_variable(\"G6_b22\", initializer=G5_b22_values)\nG6_b23=tf.get_variable(\"G6_b23\", initializer=G5_b23_values)\nG6_b24=tf.get_variable(\"G6_b24\", initializer=G5_b24_values)\nG6_b25=tf.get_variable(\"G6_b25\", initializer=G5_b25_values)\nG6_b26=tf.Variable(xavier_init([1, 5]), name=\"G6_b26\")\n\n\nG6_W31=tf.get_variable(\"G6_W31\", initializer=G5_W31_values)\ntf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, G6_W31)\nG6_W32=tf.get_variable(\"G6_W32\", initializer=G5_W32_values)\ntf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, G6_W32)\nG6_W33=tf.get_variable(\"G6_W33\", initializer=G5_W33_values)\ntf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, G6_W33)\nG6_W34=tf.get_variable(\"G6_W34\", initializer=G5_W34_values)\ntf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, G6_W34)\nG6_W35=tf.get_variable(\"G6_W35\", initializer=G5_W35_values)\ntf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, G6_W35)\nG6_W36=tf.Variable(xavier_init([5, 12]), name=\"G6_W36\")\ntf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, G6_W36)\nG6_b3=tf.get_variable(\"G6_b3\", initializer=G5_b3_values)\n\ntheta_G6 = [G6_W26, G6_W36, G6_b26,G6_W31,G6_W32,G6_W33,G6_W34,G6_W35,G6_W36,G6_b3]\n\nD6_W1 = tf.get_variable(\"D6_W1\", initializer=D5_W1_values)\ntf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, D6_W1)\nD6_b1 =tf.get_variable(\"D6_b1\", initializer=D5_b1_values)\n\nD6_W21=tf.get_variable(\"D6_W21\", initializer=D5_W21_values)\ntf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, D6_W21)\nD6_W22=tf.get_variable(\"D6_W22\", initializer=D5_W22_values)\ntf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, D6_W22)\nD6_W23=tf.get_variable(\"D6_W23\", initializer=D5_W23_values)\ntf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, D6_W23)\nD6_W24=tf.get_variable(\"D6_W24\", initializer=D5_W24_values)\ntf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, D6_W24)\nD6_W25=tf.get_variable(\"D6_W25\", initializer=D5_W25_values)\ntf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, D6_W25)\nD6_W26=tf.Variable(xavier_init([12, 5]), name=\"D6_W26\")\ntf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, D6_W26)\nD6_b21=tf.get_variable(\"D6_b21\", initializer=D5_b21_values)\nD6_b22=tf.get_variable(\"D6_b22\", initializer=D5_b22_values)\nD6_b23=tf.get_variable(\"D6_b23\", initializer=D5_b23_values)\nD6_b24=tf.get_variable(\"D6_b24\", initializer=D5_b24_values)\nD6_b25=tf.get_variable(\"D6_b25\", initializer=D5_b25_values)\nD6_b26=tf.Variable(xavier_init([1, 5]), name=\"D6_b26\")\n\nD6_W31=tf.get_variable(\"D6_W31\", initializer=D5_W31_values)\ntf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, D6_W31)\nD6_W32=tf.get_variable(\"D6_W32\", initializer=D5_W32_values)\ntf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, D6_W32)\nD6_W33=tf.get_variable(\"D6_W33\", initializer=D5_W33_values)\ntf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, D6_W33)\nD6_W34=tf.get_variable(\"D6_W34\", initializer=D5_W34_values)\ntf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, D6_W34)\nD6_W35=tf.get_variable(\"D6_W35\", initializer=D5_W35_values)\ntf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, D6_W35)\nD6_W36=tf.Variable(xavier_init([5, 12]), name=\"D6_W36\")\ntf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, D6_W36)\nD6_b3=tf.get_variable(\"D6_b3\", initializer=D5_b3_values)\ntheta_D6 = [ D6_W26, D6_W36, D6_b26,D6_W31,D6_W32,D6_W33,D6_W34,D6_W35,D6_W36,D6_b3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#%% 1. Generator\ndef generator(new_x,m):\n    inputs = tf.concat(axis = 1, values = [new_x,m])  # Mask + Data Concatenate\n    G6_h1 = tf.nn.relu(tf.matmul(inputs, G6_W1) + G6_b1)\n    G6_h2 = tf.nn.relu(tf.concat([(tf.matmul(G6_h1, G6_W21) + G6_b21),(tf.matmul(G6_h1, G6_W22) + G6_b22),\n                                 (tf.matmul(G6_h1, G6_W23) + G6_b23),(tf.matmul(G6_h1, G6_W24) + G6_b24),\n                                 (tf.matmul(G6_h1, G6_W25) + G6_b25),(tf.matmul(G6_h1, G6_W26) + G6_b26)],1))   \n    G_prob = tf.nn.sigmoid(tf.matmul(G6_h2,tf.concat((G6_W31,G6_W32,G6_W33,G6_W34,G6_W35,G6_W36),0)) + G6_b3) # [0,1] normalized Output\n    \n    return G_prob\n    \n#%% 2. Discriminator\ndef discriminator(new_x, h):\n    inputs = tf.concat(axis = 1, values = [new_x,h])  # Hint + Data Concatenate\n    D6_h1 = tf.nn.relu(tf.matmul(inputs, D6_W1) + D6_b1)  \n    D6_h2 = tf.nn.relu(tf.concat([(tf.matmul(D6_h1, D6_W21) + D6_b21),(tf.matmul(D6_h1, D6_W22) + D6_b22),\n                                 (tf.matmul(D6_h1, D6_W23) + D6_b23),(tf.matmul(D6_h1, D6_W24) + D6_b24),\n                                 (tf.matmul(D6_h1, D6_W25) + D6_b25),(tf.matmul(D6_h1, D6_W26) + D6_b26)],1)) \n    D6_logit = tf.matmul(D6_h2, tf.concat((D6_W31,D6_W32,D6_W33,D6_W34,D6_W35,D6_W36),0)) + D6_b3\n    D_prob = tf.nn.sigmoid(D6_logit)  # [0,1] Probability Output\n    \n    return D_prob","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#%% Structure\nX,M,H,New_X=input_placeholder()\n# Generator\nG_sample = generator(New_X,M)\n#print(\"G_sample\",G_sample)\n# Combine with original data\nHat_New_X = New_X * M + G_sample * (1-M)\n\n# Discriminator\nD_prob = discriminator(Hat_New_X, H)\n#print(\"D_prob\",D_prob)\n#%% Loss\nD_loss1 = -tf.reduce_mean(M * tf.log(D_prob + 1e-8) + (1-M) * tf.log(1. - D_prob + 1e-8)) \nG_loss1 = -tf.reduce_mean((1-M) * tf.log(D_prob + 1e-8))\nMSE_train_loss = tf.reduce_mean((M * New_X - M * G_sample)**2) / tf.reduce_mean(M)\n\nD_loss = D_loss1\nG_loss = G_loss1 + alpha * MSE_train_loss \n\nreg_variables6 = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\nreg_term6 = tf.contrib.layers.apply_regularization(regularizer, reg_variables6)\nD_loss += reg_term6\n#%% MSE Performance metric\nMSE_test_loss = tf.reduce_mean(((1-M) * X - (1-M)*G_sample)**2) / tf.reduce_mean(1-M)\n\n#%% Solver\nD_solver = tf.train.AdamOptimizer().minimize(D_loss, var_list=theta_D6)\nG_solver = tf.train.AdamOptimizer().minimize(G_loss, var_list=theta_G6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Data,Missing=normalization(Data)\ntrainX,testX,trainM,testM,Train_No,Test_No=train_test(Data,Missing)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Sessions\nsess = tf.Session()\nsess.run(tf.global_variables_initializer())\n\n#%% Iterations\ntrain_losses6 = []\ntest_losses6 = []\nt8=time.time()\n#%% Start Iterations\nfor it in tqdm(range(1000)):    \n    \n    #%% Inputs\n    mb_idx = sample_idx(Train_No, mb_size)\n    X_mb = trainX[mb_idx,:]  \n    \n    Z_mb = sample_Z(mb_size, Dim) \n#     M_mb = trainM[mb_idx,:]  \n    M_mb = trainM[:mb_size, :]\n    H_mb1 = sample_M(mb_size, Dim, 1-p_hint)\n    H_mb = M_mb * H_mb1\n    \n    New_X_mb = M_mb * X_mb + (1-M_mb) * Z_mb  # Missing Data Introduce\n    \n    _, D_loss_curr = sess.run([D_solver, D_loss1], feed_dict = {M: M_mb, New_X: New_X_mb, H: H_mb})\n    _, G_loss_curr, MSE_train_loss_curr, MSE_test_loss_curr = sess.run([G_solver, G_loss1, MSE_train_loss, MSE_test_loss],\n                                                                       feed_dict = {X: X_mb, M: M_mb, New_X: New_X_mb, H: H_mb})\n            \n        \n    #%% Intermediate Losses\n#     if it % 100 == 0:\n#         print('Iter: {}'.format(it))\n#         print('Train_loss: {:.4}'.format(np.sqrt(MSE_train_loss_curr)))\n#         print('Test_loss: {:.4}'.format(np.sqrt(MSE_test_loss_curr)))\n#         print()\n    train_losses6.append(np.sqrt(MSE_train_loss_curr))\n    test_losses6.append(np.sqrt(MSE_test_loss_curr))\n    \n#%% Final Loss\nt9=time.time()    \nZ_mb = sample_Z(Test_No, Dim) \nM_mb = testM\nX_mb = testX\n        \nNew_X_mb = M_mb * X_mb + (1-M_mb) * Z_mb  # Missing Data Introduce\n    \nMSE_final, Sample = sess.run([MSE_test_loss, G_sample], feed_dict = {X: testX, M: testM, New_X: New_X_mb})\n        \nprint('Final Test RMSE: ' + str(np.sqrt(MSE_final)))\nprint(\"Time cost: \",t9-t8)\n#sess.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Store trained weights and biases\nD6_W1_values = sess.run(D6_W1)\nD6_b1_values = sess.run(D6_b1)\nD6_W21_values = sess.run(D6_W21)\nD6_W22_values = sess.run(D6_W22)\nD6_W23_values = sess.run(D6_W23)\nD6_W24_values = sess.run(D6_W24)\nD6_W25_values = sess.run(D6_W25)\nD6_W26_values = sess.run(D6_W26)\nD6_b21_values = sess.run(D6_b21)\nD6_b22_values = sess.run(D6_b22)\nD6_b23_values = sess.run(D6_b23)\nD6_b24_values = sess.run(D6_b24)\nD6_b25_values = sess.run(D6_b25)\nD6_b26_values = sess.run(D6_b26)\nD6_W31_values = sess.run(D6_W31)\nD6_W32_values = sess.run(D6_W32)\nD6_W33_values = sess.run(D6_W33)\nD6_W34_values = sess.run(D6_W34)\nD6_W35_values = sess.run(D6_W35)\nD6_W36_values = sess.run(D6_W36)\nD6_b3_values = sess.run(D6_b3)\n\nG6_W1_values = sess.run(G6_W1)\nG6_b1_values = sess.run(G6_b1)\nG6_W21_values = sess.run(G6_W21)\nG6_W22_values = sess.run(G6_W22)\nG6_W23_values = sess.run(G6_W23)\nG6_W24_values = sess.run(G6_W24)\nG6_W25_values = sess.run(G6_W25)\nG6_W26_values = sess.run(G6_W26)\nG6_b21_values = sess.run(G6_b21)\nG6_b22_values = sess.run(G6_b22)\nG6_b23_values = sess.run(G6_b23)\nG6_b24_values = sess.run(G6_b24)\nG6_b25_values = sess.run(G6_b25)\nG6_b26_values = sess.run(G6_b26)\nG6_W31_values = sess.run(G6_W31)\nG6_W32_values = sess.run(G6_W32)\nG6_W33_values = sess.run(G6_W33)\nG6_W34_values = sess.run(G6_W34)\nG6_W35_values = sess.run(G6_W35)\nG6_W36_values = sess.run(G6_W36)\nG6_b3_values = sess.run(G6_b3)\n\nVal_list6=[D6_W1_values,D6_W21_values,D6_W22_values,D6_W23_values,D6_W24_values,D6_W25_values,D6_W26_values,D6_W31_values,D6_W32_values,D6_W33_values,D6_W34_values,D6_W35_values,D6_W36_values,G6_W1_values,\n          G6_W21_values,G6_W22_values,G6_W23_values,G6_W24_values,G6_W25_values,G6_W26_values,G6_W31_values,G6_W32_values,G6_W33_values,G6_W34_values,G6_W35_values,G6_W36_values]\nVal_list6_bias=[D6_b1_values,D6_b21_values,D6_b22_values,D6_b23_values,D6_b24_values,D6_b25_values,D6_b26_values,D6_b3_values,G6_b1_values,\n          G6_b21_values,G6_b22_values,G6_b23_values,G6_b24_values,G6_b25_values,G6_b26_values,G6_b3_values]\n\ntotal=0\nbelow_threshold=0\nfor weights in Val_list6:\n    r,c=weights.shape[:2]\n    total+=r*c\n    #below_threshold+=len(weights[np.where(np.abs(weights)<0.001)])\n    below_threshold+=len(weights[np.where(np.abs(weights)==0)])\np6=total-below_threshold\nb6=total\n\nparameter.append(p6)\nbaseline.append(b1+b2+b3+b4+b5+b6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(parameter)\nprint(baseline)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}