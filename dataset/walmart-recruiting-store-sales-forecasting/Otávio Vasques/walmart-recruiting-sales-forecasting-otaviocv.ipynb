{"cells":[{"metadata":{"_uuid":"8e4c4494-da51-4a6b-8af2-807e1d008363","_cell_guid":"282d3a37-6619-4ec6-88b2-b3ff2fa8ae64","trusted":true},"cell_type":"markdown","source":"# Walmart Recruitng: Store Sales Forecasting\nTechnical exercise for Zé Delivery selection process. <img style=\"padding-left:20px; width:75px; display: inline;\" src=\"https://courier-images-web.imgix.net/static/img/small-logo.png?auto=compress,format&fit=max&w=undefined&h=undefined&dpr=2&fm=png\">\n\n***Otávio Vasques*** | September 2020\n\n[Github](https://github.com/otaviocv) | [LinkedIn](https://www.linkedin.com/in/otaviocv/) | [Email](mailto:otaviocv.deluqui@gmail.com)\n\n","execution_count":null},{"metadata":{"_uuid":"c077df18-8244-4fe1-91e9-38e8ad6736e0","_cell_guid":"429cd688-c001-4642-b118-b16837bce142","trusted":true},"cell_type":"markdown","source":"## Introduction\n\nThe problem we want to solve is the Sales Values for Walmart stores and departments, it is a *regression* problem. We want to forecast future sales values based on the store configuration, environmental variables regarding a given store, not just previous historical data.\n\nMy solution will follow a very standard outline: a basic exploratory data analysis for data understanding, to capture particular characteristics of features, identify which feature engineering techniques will be required and understand what will be the information available at the prediction environment. The next step is to make a dummy baseline to establish the worst case scenario. Then I will build a basic benchmark model with a very simple linear model with regularization to really define the reference I will build upon. The last modeling step will be a more powerful and complex model that has a huge adoption among the machine learning practitioners due to its amazing features: XGBoost.\n\nThese three \"models\" will be then compared in a results section and a final conclusions section will end my analysis and summarize all my thoughts.\n\n\n### Challenges\n\n**Stores and Departments**\n\nPredictions are divided in subcategories within the stores, their departments. This is a problem because all feature values are related explicitly with a store and not just a department. It will be necessary to include this distinction in the prediction process.\n\n**Time Axis**\n\nThe data is naturally indexed in time, which leaves space for multiple strategies based on time series feature extraction and time series models such as the AR, MA and ARIMA models.\n\n\n### Approaches and solution candidates\n\nMy solution will follow three basic steps with minor feature engineering. As my first step I will make a basic, very grounded benchmark based on mean and standard values to establish our starting point. In a real world scenario, where the team would be first worried with integrations and data collection, this could be a dummy model to fill the gaps while a more proper solution is being developed.\n\nMy second step will be a linear model, more specific an Elastic Net, the linear model with two regularization terms, L1 and L2. This is a very established and well known solution, with very low resistance even within the most conservative teams. This will serve as our real baseline model which we will improve from.\n\nMy third step will be the Kaggle standard for data without a particular information structure (such as images or audio data): tree based gradient boosting. My choice is the XGBoost implementation which provides multiple features for controlling overfitting, trees growth and has a great implementation for high performance prediction scenarios.\n\nI will make use of an explanation technique called [SHAP](https://github.com/slundberg/shap), based on game theory. It is very handful for understanding local contributions of some features. It works really well for linear models and tree based models.\n\n### Metrics and perfomance measures\n\nThe competition metric is a *Weighted Mean Absolute Error* where the holidays weight 5 times the usual error. This means that predicting sales for holidays is very important. I will also use the classic *Mean Absolute Error* for a general comparison and understand general effects of the models without taking too much attention on the holidays.\n\n### Data\n\nData description ([link](https://www.kaggle.com/c/walmart-recruiting-store-sales-forecasting/data)). **This is just a copy and paste from the data description page of the original problem for easier reference**.\n\n**stores**\n\nThis file contains anonymized information about the 45 stores, indicating the type and size of store.\n\n**feaures**\n\nThis file contains additional data related to the store, department, and regional activity for the given dates. It contains the following fields:\n\n    Store - the store number\n    Date - the week\n    Temperature - average temperature in the region\n    Fuel_Price - cost of fuel in the region\n    MarkDown1-5 - anonymized data related to promotional markdowns that Walmart is running. MarkDown data is only available after Nov 2011, and is not available for all stores all the time. Any missing value is marked with an NA.\n    CPI - the consumer price index\n    Unemployment - the unemployment rate\n    IsHoliday - whether the week is a special holiday week\n\nFor convenience, the four holidays fall within the following weeks in the dataset (not all holidays are in the data):\n\n* Super Bowl: 12-Feb-10, 11-Feb-11, 10-Feb-12, 8-Feb-13\n* Labor Day: 10-Sep-10, 9-Sep-11, 7-Sep-12, 6-Sep-13\n* Thanksgiving: 26-Nov-10, 25-Nov-11, 23-Nov-12, 29-Nov-13\n* Christmas: 31-Dec-10, 30-Dec-11, 28-Dec-12, 27-Dec-13\n\n**train**\n\nThis is the historical training data, which covers to 2010-02-05 to 2012-11-01. Within this file you will find the following fields:\n\n    Store - the store number\n    Dept - the department number\n    Date - the week\n    Weekly_Sales -  sales for the given department in the given store\n    IsHoliday - whether the week is a special holiday week\n\n**test**\n\nThis file is identical to train.csv, except we have withheld the weekly sales. You must predict the sales for each triplet of store, department, and date in this file.","execution_count":null},{"metadata":{"_uuid":"30fb0b52-cf89-41b3-9bbb-aff59d80894e","_cell_guid":"326da925-0f7d-44f5-89e3-db2de70377d1","trusted":true},"cell_type":"markdown","source":"### Imports","execution_count":null},{"metadata":{"_uuid":"959f824a-1069-4c7c-9f21-e67bf6f1a9f5","_cell_guid":"1eb0fc48-47f9-4f5f-b4e7-48d59824365f","trusted":true},"cell_type":"code","source":"import os\nimport itertools\nfrom tqdm.notebook import tqdm\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.metrics import mean_absolute_error\n\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\nfrom sklearn.model_selection import KFold, GridSearchCV, ParameterGrid\nimport xgboost as xgb\nimport catboost as cat\n\nimport shap\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"95d6baf8-f5cd-4023-bb24-d6c8d7619bd7","_cell_guid":"8b0b3ebd-60e3-4a94-a03e-ba3dec554edb","trusted":true},"cell_type":"markdown","source":"### Parameters","execution_count":null},{"metadata":{"_uuid":"bfe7890e-32ce-4453-9464-c1532de912d9","_cell_guid":"6a2f3351-eeec-497d-9ae6-2c688ace54bc","trusted":true},"cell_type":"code","source":"data_path = '/kaggle/input/walmart-recruiting-store-sales-forecasting/'","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"40a14257-f435-4e31-9fdd-d43be3fa5d15","_cell_guid":"e9ddaa29-0314-4c14-b4c6-8cff3a6a1cf0","trusted":true},"cell_type":"markdown","source":"### Understanding Kaggle environment","execution_count":null},{"metadata":{"_uuid":"95fc868b-1371-4518-bb64-bc355d0a0138","_cell_guid":"3a84cbc1-5832-4294-83bb-3d9eca356291","trusted":true},"cell_type":"code","source":"!ls {data_path}","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7057090d-57c8-458a-8c06-92f62d5ece4e","_cell_guid":"bfb1834d-e2f0-449a-afc9-582ec6fabb5a","trusted":true},"cell_type":"code","source":"for dirname, _, filenames in os.walk(data_path):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1bd61737-c147-4c5b-ab46-f52c7582c19d","_cell_guid":"ee96a0eb-9382-4473-b3d1-f7d61defe7b0","trusted":true},"cell_type":"code","source":"!pwd","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0774985c-a71e-4bdd-9659-e0065fe06816","_cell_guid":"9fe8fbe3-9f09-42b0-85ca-40dbe3b4b48a","trusted":true},"cell_type":"markdown","source":"### Data","execution_count":null},{"metadata":{"_uuid":"dfe9fe1f-5f7d-4393-baef-95ad8a6dbda1","_cell_guid":"6f46e653-ba2f-4851-b6c6-8391562fefbb","trusted":true},"cell_type":"code","source":"stores = pd.read_csv(data_path + \"stores.csv\")\nfeatures = pd.read_csv(data_path + \"features.csv.zip\")\ntrain_data = pd.read_csv(data_path + \"train.csv.zip\")\ntest_data = pd.read_csv(data_path + \"test.csv.zip\")\nsample_submission = pd.read_csv(data_path + \"sampleSubmission.csv.zip\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"796748a9-1bb0-4fa3-a4ed-3db664ab5994","_cell_guid":"7f0eb97d-55b0-432d-b8db-d6bd0c03f698","trusted":true},"cell_type":"code","source":"features.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e41486f4-ebb8-40f6-a94b-4c94a36da17c","_cell_guid":"31e4e987-e386-4f05-a9a6-611f6a1171a4","trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e41486f4-ebb8-40f6-a94b-4c94a36da17c","_cell_guid":"31e4e987-e386-4f05-a9a6-611f6a1171a4","trusted":true},"cell_type":"code","source":"test_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All data has been loaded correctly from the zip files.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Holiays\n\n* Super Bowl: 12-Feb-10, 11-Feb-11, 10-Feb-12, 8-Feb-13\n* Labor Day: 10-Sep-10, 9-Sep-11, 7-Sep-12, 6-Sep-13\n* Thanksgiving: 26-Nov-10, 25-Nov-11, 23-Nov-12, 29-Nov-13\n* Christmas: 31-Dec-10, 30-Dec-11, 28-Dec-12, 27-Dec-13\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"holidays = pd.to_datetime([\"2010-02-12\", \"2011-02-11\", \"2012-02-10\", \"2013-02-08\", \"2010-09-10\", \"2011-09-09\", \"2012-09-07\", \"2013-09-13\",\n                           \"2010-11-26\", \"2011-11-25\", \"2012-11-23\", \"2013-11-29\", \"2010-12-31\", \"2011-12-30\", \"2012-12-28\", \"2013-12-27\"])\nholidays_dict = {\n    \"2010-02-12\": \"Super Bowl\",\n    \"2011-02-11\": \"Super Bowl\",\n    \"2012-02-10\": \"Super Bowl\",\n    \"2013-02-08\": \"Super Bowl\",\n    \"2010-09-10\": \"Labor Day\",\n    \"2011-09-09\": \"Labor Day\",\n    \"2012-09-07\": \"Labor Day\",\n    \"2013-09-13\": \"Labor Day\",\n    \"2010-11-26\": \"Thanksgiving\",\n    \"2011-11-25\": \"Thanksgiving\",\n    \"2012-11-23\": \"Thanksgiving\",\n    \"2013-11-29\": \"Thanksgiving\",\n    \"2010-12-31\": \"Christmas\",\n    \"2011-12-30\": \"Christmas\",\n    \"2012-12-28\": \"Christmas\",\n    \"2013-12-27\": \"Christmas\"\n}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Weighted Mean Absolute Error Metric function\n\nJust checking how to use the scikit mean absolute error with weights.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_true = np.array([1, 2, 3, 4, 5, 6])\ny_pred = np.array([2 ,3, 0, 5, 6, 1]) # 1 + 1 + 3 + 1 + 1 + 5 = 12\n                                      # 1 + 1 + 15 + 1 + 1 + 25 = 44\nweights = np.array([1, 1, 5, 1, 1, 5]) # 1 + 1 + 5 + 1 + 1 + 5 = 14\n\nassert mean_absolute_error(y_true, y_pred) == 2\nassert mean_absolute_error(y_true, y_pred, sample_weight=weights) == 44/14","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Plot Holidays Function","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(5,1))\n\ndef plot_holidays(holidays, ax=plt, color=\"red\", linestyle=\"--\", linewidth=\"0.5\", **kwargs):\n    for hday in holidays:\n        ax.axvline(x=hday, color=color, linestyle=linestyle, linewidth=linewidth, **kwargs)\n        \nplot_holidays(holidays, ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0cff5d86-88ec-42d8-942a-892281e7f80f","_cell_guid":"10518c8d-49ca-4fea-a71b-2ee108b939fc","trusted":true},"cell_type":"markdown","source":"## Exploratory Data Anlysis\n\nIn this section, I will make my best effort to understand and gain intuition about data. This will help my future modeling decisions and will provide material to understand the models' behavior.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Stores","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"stores.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stores.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stores.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stores.Type.value_counts(normalize=True, dropna=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(8, 3), dpi=140)\nsns.distplot(stores.Size);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stores.Size.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 45 stores, every one of them has a *type* and a *size*. The meaning of these variables aren't provided. We can just say that the type is one of three values, A, B and C. The size ranges from ~63 825 to 220000.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### NANS Viz","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(1, 2, figsize=(10, 5), dpi=150)\nsns.heatmap(features.sort_values(\"Store\").isna(), ax=ax[0], cmap=\"plasma\", cbar=None, yticklabels=False);\nax[0].set_title(\"NANs sorted by store\")\nsns.heatmap(features.sort_values(\"Date\").isna(), ax=ax[1], cmap=\"plasma\", yticklabels=False);\nax[1].set_title(\"NANs sorted by date\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features.isna().mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Almost half of the MarkDown values are missing. What does this mean? The data description says that this data is only available after Nov. 2011. Since 2011 falls relatively in the middle of our time window, almost half of these values are missing. A dedicated approach for these values will be necessary for the linear model.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Features\n\nThis is the main table. The information in this table has two main indexes that will match the prediction dataset, the week of prediction, and the store number. All values from these tables are the same for a given store in a given week, there is still the department distinction in the train data. I will have to figure out how to include this department's information in our final models.\n\nAll feature values, both from the train and test datasets, are mixed in a single table. In a future section, we will need to split train features, train target, test features, and test target.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"features.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Time Axis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.Date = pd.to_datetime(train_data.Date)\ntest_data.Date = pd.to_datetime(test_data.Date)\nfeatures.Date = pd.to_datetime(features.Date)\nfeatures_dates = features.groupby(\"Date\", as_index=False).agg({\"Store\": \"nunique\"})\n#features_dates.Date = pd.to_datetime(features_dates.Date)\nfeatures_dates.plot(x=\"Date\", y=\"Store\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Min: \", features_dates.Date.min(), \"Max: \", features_dates.Date.max())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Temperature","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(12, 4), dpi=150)\n\nfor year in range(features_dates.Date.min().year, features_dates.Date.max().year+1):\n    plot_data = features[features.Date.dt.year == year].groupby(\"Date\", as_index=False).agg({\"Temperature\": \"mean\"})\n    ax.plot(plot_data.Date.dt.dayofyear, plot_data.Temperature, label=year)\n\nax.legend()\nax.set_title(\"Temperature over Time\")\nax.set_ylabel(\"Temperature [°F]\")\nax.set_xlabel(\"Day of year\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The problem description doesn't state if the stores are inside or restricted to the US. Assuming that the Walmart stores are located in the US, or in Northern Hemisphere, the graph show the expected temperature curve for the Northern Hemisphere.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Unemployment","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(12, 4), dpi=150)\n\nfor year in range(features_dates.Date.min().year, features_dates.Date.max().year +1):\n    plot_data = features[features.Date.dt.year == year].groupby(\"Date\", as_index=False).agg({\"Unemployment\": \"mean\"})\n    ax.plot(plot_data.Date.dt.dayofyear, plot_data.Unemployment, label=year)\n\nax.legend()\nax.set_title(\"Unemployment over Time\")\nax.set_ylabel(\"Unemployment\")\nax.set_xlabel(\"Day of year\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is clear from the graph that the unemployment is constant for all stores and is measured every 3 months.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"features.groupby([\"Date\", \"Store\"], as_index=False).agg({\"Unemployment\": \"nunique\"}).Unemployment.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Fuel Price","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(12, 4), dpi=150)\n\nfor year in range(features_dates.Date.min().year, features_dates.Date.max().year+1):\n    plot_data = features[features.Date.dt.year == year].groupby(\"Date\", as_index=False).agg({\"Fuel_Price\": \"mean\"})\n    ax.plot(plot_data.Date.dt.dayofyear, plot_data.Fuel_Price, label=year)\n\nax.legend()\nax.set_title(\"Fuel Price over Time\")\nax.set_ylabel(\"Fuel Price [USD]\") # I'm assuming US Dollars\nax.set_xlabel(\"Day of year\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Consumer Price Index\n\n> The Consumer Price Index (CPI) is a measure of the average change over time in the prices paid by urban consumers for a market basket of consumer goods and services. Indexes are available for the U.S. and various geographic areas. Average price data for select utility, automotive fuel, and food items are also available.\n\nFrom https://www.bls.gov/cpi/","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(12, 4), dpi=150)\n\nfor year in range(features_dates.Date.min().year, features_dates.Date.max().year+1):\n    plot_data = features[features.Date.dt.year == year].groupby(\"Date\", as_index=False).agg({\"CPI\": \"mean\"})\n    ax.plot(plot_data.Date.dt.dayofyear, plot_data.CPI, label=year)\n    ax.axhline(y=plot_data.CPI.max(), color=\"grey\", linestyle=\"--\", linewidth=0.2)\n\nax.legend()\nax.set_title(\"Consumer Price Index over Time\")\nax.set_ylabel(\"Consumer Price Index\")\nax.set_xlabel(\"Day of year\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All the plots above show the common values across stores and are fixed for each date. My choice to plot the years in the same axis was to identify some kind of seasonality but apart from temperature, there is no clear seasonality in each of the other features.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Markdowns\n\nAfter a quick search on the internet on \"What is a Marketing Markdown?\" I'm not sure exactly what kind of Markdowns Walmart refers to, and therefore it is unclear what the numbers mean for each case. Probably they refer to the amount of money spent on the MarkDown type for the given store but there isn't a way to be sure about this.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(12, 4), dpi=150)\n\nfor i in range(1, 6):\n    plot_data = features.groupby(\"Date\", as_index=False).agg({\"MarkDown\" + str(i): \"mean\"})\n    ax.plot(plot_data.Date, plot_data[\"MarkDown\" + str(i)], label=\"MarkDown\" + str(i))\n    #ax.axhline(y=plot_data.CPI.max(), color=\"grey\", linestyle=\"--\", linewidth=0.2)\n\nax.legend()\nax.set_title(\"Markdowns over Time\")\nax.set_ylabel(\"Markdowns\")\nax.set_xlabel(\"Day of year\");\nplot_holidays(holidays[holidays > pd.to_datetime(\"2011-11-01\")], ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is clear that some kinds of MarkDowns are just used in some specific circumstances like Markdown 3 (green), it stays flat the entire 2012 year. It is also clear that they heavily used Markdowns the week before or at holiday weeks to improve sales. One way to check their relationship is to plot the correlation matrix between markdowns to understand a bit more how their combined usage behaves.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"markdown_corr = np.zeros((5,5))\nmarkdown_cols = [\"MarkDown\" + str(i) for i in range(1, 6)]\nnan_mask = features.loc[:, markdown_cols].isnull().sum(axis=1) > 0\nfiltered_markdowns = features.loc[~nan_mask, markdown_cols]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(filtered_markdowns.corr());","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"MarkDown 1 and 4 are used a bit more combined than other pairs.","execution_count":null},{"metadata":{"_uuid":"1c652dc3-44df-4b5d-a24b-7f8c886ccbca","_cell_guid":"4c30e0a4-5683-4f56-9e65-be539d220419","trusted":true},"cell_type":"markdown","source":"### Target(s) Observation\n\nSo far I have analyzed the feature values by itself, without considering their relationship with sales values. This next session we explore a bit the sales values itself and some relations with feature values.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(12, 4), dpi=130)\n\n\nscatter_data = train_data.groupby([\"Date\", \"Dept\"], as_index=False).Weekly_Sales.agg(\"median\")\npalette = sns.color_palette(\"hls\", scatter_data.Dept.nunique())\ncolor_pallete = {d: palette[i] for i, d in enumerate(scatter_data.Dept.unique())}\nxjitter = np.random.randint(0, 5, size=len(scatter_data)).astype(\"timedelta64[D]\") # points are falling in the same spot, let's given them a little jitter no more than 5 days\ncolor = scatter_data.Dept.apply(lambda x: color_pallete.get(x))\nax.scatter(scatter_data.Date + xjitter, scatter_data.Weekly_Sales, s=0.3, color=color);\nax.set_ylim(0,80000);\nax.set_facecolor('black')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bla = sns.palplot(palette)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Not much of a useful plot, there are a lot of departments and it is hard to identify each of them.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Homegeinity between stores and departments\n\nThere a bunch of stores and departments. We know that there are 45 stores and they have different size some questions can be posed:\n* How many departments are there?\n* Do store sales are affected by the Store size?\n    * Bigger Stores sells more?\n* Are departments sell about the same in all stores? Are there stores that some department are preferred than in others?\n* Do the department respect any rule of sasonality?\n\n\n*Let's answer all this questions!*","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Departments**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.groupby(\"Store\").agg({\"Dept\": \"nunique\"}).sort_values(\"Dept\", ascending=False).describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f ,ax = plt.subplots(figsize=(12, 2), dpi=180)\ntrain_data.groupby(\"Store\").agg({\"Dept\": \"nunique\"}).sort_values(\"Dept\", ascending=False).plot.bar(ax=ax);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 79 different departments. All stores have at least 61 departments. At least 75% have 74 departments or more. Do they all have the same departments? Which stores have which departments?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dept_fill_matrix = train_data.pivot_table(index=\"Store\", columns=\"Dept\", values=\"Date\", aggfunc=\"count\")\ndept_fill_matrix_mask = dept_fill_matrix > 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(12, 6), dpi=160)\nsns.heatmap(dept_fill_matrix_mask);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Stores have pretty much the same departments. It is clear that all stores have all departments from 1 to 18 and there some *rare* departments such as 43 adn 61.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Do store sales are affected by the Store size?**\n\nLet's take a look to their total year sales and compare to their sizes.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"year_sales = train_data.groupby([\"Store\", train_data.Date.dt.year]).agg({\"Weekly_Sales\": \"sum\"}).reset_index()\nyear_sales = year_sales.merge(stores, on=\"Store\", how=\"left\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, axs = plt.subplots(3, 1, figsize=(10, 15), dpi=120)\ncolor_pallete = sns.color_palette(\"plasma\", year_sales.Store.nunique())\n\nfor i, year in enumerate(range(features_dates.Date.min().year, features_dates.Date.max().year)):\n    ax = axs[i]\n    data = year_sales[year_sales.Date == year]\n    colors = data.Store.apply(lambda s: color_pallete[s-1])\n    ax.scatter(data.Size, data.Weekly_Sales, color=colors)\n    ax.set_title(year)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The store size affects sales and, in general, a bigger store sells more.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Department Stores Sales**\n\nLet's remake the Stores Department Matrix but this time let's put the average sales value in color.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dept_sales_matrix = train_data.pivot_table(index=\"Store\", columns=\"Dept\", values=\"Weekly_Sales\", aggfunc=\"mean\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns_sort = dept_sales_matrix.sum(axis=0).sort_values(ascending=False).index.values\nrows_sort = dept_sales_matrix.sum(axis=1).sort_values(ascending=False).index.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(12, 6), dpi=160)\nsns.heatmap(dept_sales_matrix.loc[rows_sort, columns_sort], ax=ax, cmap=\"plasma\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nWe can see that the upper left corner gets a lot brighter after sorting values. There are some departments much bigger than others and there are stores much bigger than others. This will guide us when interpreting the department's impact on the models' behavior. Let's see how departments behave with time. More than just the variation with time I want to identify which departments oscillate the most on holidays.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dept_sales_over_time = train_data.groupby([\"Date\", \"Dept\"]).agg({\"Weekly_Sales\": \"sum\"}).reset_index()\ndept_sales_avg = dept_sales_over_time.groupby(\"Dept\", as_index=False).agg(dept_mean=pd.NamedAgg(column='Weekly_Sales', aggfunc='mean'))\ndept_sales_over_time = dept_sales_over_time.merge(dept_sales_avg, on=\"Dept\")\ndept_sales_over_time[\"variance\"] = (dept_sales_over_time[\"Weekly_Sales\"] - dept_sales_over_time[\"dept_mean\"])\ndept_sales_over_time[\"proportional_variance\"] = dept_sales_over_time[\"variance\"]/dept_sales_over_time[\"dept_mean\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dept_sales_over_time","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(3, 1, figsize=(12, 9), dpi=160)\ndept_sales_over_time.groupby(\"Dept\").plot(x=\"Date\", y=\"Weekly_Sales\", ax=ax[0]);\nax[0].set_title(\"Absolute Values\")\nax[0].legend([]);\nplot_holidays(holidays, ax[0])\n\ndept_sales_over_time.groupby(\"Dept\").plot(x=\"Date\", y=\"variance\", ax=ax[1]);\nax[1].set_title(\"Absolute Variance\")\nax[1].legend([]);\nplot_holidays(holidays, ax[1])\n\ndept_sales_over_time.groupby(\"Dept\").plot(x=\"Date\", y=\"proportional_variance\", ax=ax[2]);\nax[2].set_title(\"Proportional Variance\")\nax[2].legend([]);\nax[2].set_ylim([-20, 20])\nplot_holidays(holidays, ax[2])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Dispersion Analysis**\n\nSo far all my plots aggregate sales values in some way, either by summing or taking averages. This procedure masks the natural dispersion of sales values. By sneak peaking some other Kaggle kernels I found [this](https://www.kaggle.com/simonstochholm/walmart-sales-forecast-gammel) very interesting box plot view of sales values for stores and departments. I will reproduce these plots below.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(14,5), dpi=120)\nsns.boxplot(train_data[\"Store\"], train_data[\"Weekly_Sales\"],showfliers=False);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(14,5), dpi=120)\nsns.boxplot(train_data[\"Dept\"], train_data[\"Weekly_Sales\"],showfliers=False);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see from the Store's box plots that the sales values have an asymmetric distribution, with compact values to the left of the median and the peak values stretching to the 4th quantile bar. The same is not as present in the Departments plot, in the Departments plot we can see more balanced distributions and a greater difference between departments.\n\n### EDA Conclusions and general thoughts\n\n* Environmental features such as Temperature, CPI, Unemployment Rate, and Fuel Price are common to all departments inside a single store, they will not help much in the department distinctions. They will help to build a base notion of sale values.\n* Almost half of Markdown values are missing. The markdown values will require special attention for filling nans.\n* Store Size matters\n* Department type matters\n* Holidays play a huge role in increasing sales\n* Almost all stores have almost all departments. The departments with lower frequency will have their generalization compromised due to fewer data.\n\nCharacteristics related to the store and departments seem to be more important to environmental features.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Datasets and splits\n\nTo proceed with thee models I will build my canonical dataset that will be used for all models. They may introduce particular feature engineering techniques but the train, test, optimization, and validation datasets will be the same for all models. Since the names train and test are already taken by the Kaggle competition and we still have to make discussions in the notebook I will split the training dataset into two subsets: the optimization dataset, that will be used for cross-validation and hyperparameter optimization, and the validation dataset, that will be used to compare all the models and serve as the common ground for all of our results.\n\nMy final set of feature attributes will be:\n* The Store attributes\n    * Store ID\n    * Store Type\n    * Store Size\n* The features attributes\n    * Temperature\n    * Fuel_Price\n    * Markdows\n    * CPI\n    * Unemployment\n    * Holiday Flag (it is always possible to know beforehand if a given week will contain a Holiday)\n* Holiday Type (This is an implicit feature that may be helpful)\n* Date features (basic decomposition of the date as an attempt of extracting time-related behavior)\n    * Year\n    * Month\n    * Day\n    * Weekday (Monday, Tuesday, Wednesday, etc.)\n    * Week of Year\n    * Day of year\n\nSince this a time-based problem, it makes the most sense to split our training dataset based on a time split. I will take about the first 80% (the stores and departments will make this an uneven split) as our optimization dataset and the last 20% as the validation dataset.\n\nA final submission with our best model will be made just as a sanity check to confirm that our conclusions and discussions in the notebook are correct.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"all_features = features.merge(stores, on=\"Store\")\nall_features[\"HolidayType\"] = all_features.Date.apply(lambda x: holidays_dict.get(x.date().isoformat(), \"Not Holiday\"))\nall_features[\"Year\"] = all_features.Date.dt.year\nall_features[\"Month\"] = all_features.Date.dt.month\nall_features[\"Day\"] = all_features.Date.dt.day\nall_features[\"DayOfWeek\"] = all_features.Date.dt.dayofweek\nall_features[\"WeekOfYear\"] = all_features.Date.dt.isocalendar().week\nall_features[\"DayOfYear\"] = all_features.Date.dt.dayofyear","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full_train_data = train_data.drop(\"IsHoliday\", axis=1).merge(all_features, on=[\"Store\", \"Date\"])\nfull_test_data = test_data.drop(\"IsHoliday\", axis=1).merge(all_features, on=[\"Store\", \"Date\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_ids = full_test_data.Store.astype(str) + '_' + full_test_data.Dept.astype(str) + '_' + full_test_data.Date.astype(str)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full_train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full_test_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Min: \", full_train_data.Date.min(), \"Max: \", full_train_data.Date.max())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target = \"Weekly_Sales\"\ncontinuous_features = [\"Temperature\", \"Fuel_Price\", \"MarkDown1\", \"MarkDown2\", \"MarkDown3\", \"MarkDown4\", \"MarkDown5\",\n                       \"CPI\", \"Unemployment\", \"Size\", \"Year\", \"Day\", \"Month\", \"WeekOfYear\", \"DayOfYear\"]\ncategorical_features = [\"Store\", \"Dept\", \"IsHoliday\", \"Type\", \"HolidayType\", \"DayOfWeek\"]\nfeatures = continuous_features + categorical_features\ndrop_features = [\"Date\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"split_date = (full_train_data.Date.min() + (full_train_data.Date.max() - full_train_data.Date.min()) * 0.8)\nsplit_date","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimization_data = full_train_data[full_train_data.Date < split_date]\noptimization_target = optimization_data[target]\noptimization_features = optimization_data[features]\nvalidation_data = full_train_data[full_train_data.Date >= split_date]\nvalidation_target = validation_data[target]\nvalidation_features = validation_data[features]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Optimization proportion: {len(optimization_data)/len(full_train_data)}\")\nprint(f\"Validation proportion: {len(validation_data)/len(full_train_data)}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A tiny bit more for validation but it is ok.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Optimization shapes: \", optimization_data.shape, optimization_features.shape, optimization_target.shape)\nprint(\"Validation shapes: \", validation_data.shape, validation_features.shape, validation_target.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Two columns missing in the features: Date and Weekly Sales.\n\n*Let's build some models!*","execution_count":null},{"metadata":{"_uuid":"d8e92960-e020-4841-a22b-8e1c35eb93ed","_cell_guid":"d447b6b5-66db-46ab-b87c-36e806b22b55","trusted":true},"cell_type":"markdown","source":"## Dummy Benchmark (Mean and Standard deviation)\n\nThis section will serve as our very basic baseline, it will serve to guide us with basic boundaries and ensure that we are not making any weird errors in the true models. The Kaggle leaderboard states that the All Zeros benchmark in the test set is **22265.71813**. I will make two basic dummy models:\n* Make a general Mean prediction.\n* Make a more specialized Mean prediction by using averages for stores and departments.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### General Mean Dummy Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(1, 1, figsize=(10, 3), dpi=120)\nsns.distplot(optimization_target, label=\"Optimization\")\nsns.distplot(validation_target, label=\"Validation\")\nax.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimization_target.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"validation_target.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Not exactly sure if this negative values of sales are wrong or just under profit operations. Since I have not found any explicit disclaimer in the data description neither any discussion in the Kaggle discussion board I will keep these values in my solution.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_mean = optimization_target.mean()\npred_mean","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_mean_pred_train = np.zeros(len(optimization_data))\ny_mean_pred_train[:] = pred_mean\n\ny_mean_pred = np.zeros(len(validation_data))\ny_mean_pred[:] = pred_mean","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimization_weights = optimization_data[\"IsHoliday\"].apply(lambda x: 5 if x else 1).values\nweights = validation_data[\"IsHoliday\"].apply(lambda x: 5 if x else 1).values\nweights","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimization_performance_dummy_1 = mean_absolute_error(optimization_target, y_mean_pred_train, sample_weight=optimization_weights)\noptimization_performance_dummy_1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"validation_performance_dummy_1 = mean_absolute_error(validation_target, y_mean_pred, sample_weight=weights)\nvalidation_performance_dummy_1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Stores and Departments Mean","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"stores_and_deps_mean = optimization_data.groupby([\"Store\", \"Dept\"], as_index=False).agg({\"Weekly_Sales\": \"mean\"})\nstores_and_deps_mean","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stores_and_dept_preds_train = optimization_data.loc[:, [\"Store\", \"Dept\"]].merge(stores_and_deps_mean, on=[\"Store\", \"Dept\"], how=\"left\").Weekly_Sales\nstores_and_dept_preds_train[stores_and_dept_preds_train.isna()] = pred_mean\n\nstores_and_dept_preds = validation_data.loc[:, [\"Store\", \"Dept\"]].merge(stores_and_deps_mean, on=[\"Store\", \"Dept\"], how=\"left\").Weekly_Sales\nstores_and_dept_preds[stores_and_dept_preds.isna()] = pred_mean","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimization_performance_dummy_2 = mean_absolute_error(optimization_target, stores_and_dept_preds_train, sample_weight=optimization_weights)\noptimization_performance_dummy_2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"validation_performance_dummy_2 = mean_absolute_error(validation_target, stores_and_dept_preds, sample_weight=weights)\nvalidation_performance_dummy_2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_absolute_error(validation_target, stores_and_dept_preds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Submission","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"stores_and_dept_preds_test = test_data.loc[:, [\"Store\", \"Dept\", \"Date\"]].merge(stores_and_deps_mean, on=[\"Store\", \"Dept\"], how=\"left\")\nstores_and_dept_preds_test.loc[stores_and_dept_preds_test.Weekly_Sales.isnull(), \"Weekly_Sales\"] = pred_mean\ntest_ids = stores_and_dept_preds_test.Store.astype(str) + '_' + stores_and_dept_preds_test.Dept.astype(str) + '_' + stores_and_dept_preds_test.Date.astype(str)\nsample_submission['Id'] = test_ids.values\nsample_submission['Weekly_Sales'] = stores_and_dept_preds_test.Weekly_Sales.values\nsample_submission.to_csv('submission_dummy_2.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Comments","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Wow! This is an astonishing result! By comparing directly to the leaderboard this would be a top 20 submission! It is clear that there are just a few entries for each store and department pairs and would not generalize over time. But it is still impressive how better the solution got by just restricting the mean.\n\nThis makes clear that department and store identification plays a huge role in the predictions. If this was a model to be applied to new stores, stores absent from our data, the department configuration of the store is critical information and we would not be able to use the store id number. Since there is no explicit recommendation on the problem description and a bunch of solutions makes use of this information I will keep this in my solution.\n\n> Just for the record, the following code:\n``` python\nstores_and_dept_preds_test = test_data.loc[:, [\"Store\", \"Dept\", \"Date\"]].merge(stores_and_deps_mean, on=[\"Store\", \"Dept\"], how=\"left\")\nstores_and_dept_preds_test.loc[stores_and_dept_preds_test.Weekly_Sales.isnull(), \"Weekly_Sales\"] = pred_mean\ntest_ids = stores_and_dept_preds_test.Store.astype(str) + '_' + stores_and_dept_preds_test.Dept.astype(str) + '_' + stores_and_dept_preds_test.Date.astype(str)\nsample_submission['Id'] = test_ids.values\nsample_submission['Weekly_Sales'] = stores_and_dept_preds_test.Weekly_Sales.values\nsample_submission.to_csv('submission_dummy_2.csv',index=False)\n```\n>Yields public and private scores of 5351.79373 and 5103.70395 respectively. This would rank 437 in the private leaderboard. Amazing result, in my opinion, given that we just have taken the stores and department means.\n\n*Let's build some real models!*","execution_count":null},{"metadata":{"_uuid":"ba535ccb-ccc4-444b-9ea0-56a99f9d7dc8","_cell_guid":"3e9e6fcb-9bab-4851-b522-72a9041b8f3b","trusted":true},"cell_type":"markdown","source":"## Linear Model\n\nMy choice of a linear model is based on its simplicity. Linear models are well known for a long time and yield pretty solid results as the first attempt. Without much effort, with a very simple (simple in this case meaning fast and reliable) model, it is possible to deliver value and result in a problem. Given these comments I will make a linear model as my first model, it will serve as our *real* benchmark.\n\nLinear models require special treatments for some features, One Hot Encoding and Scaling, and therefore the total number of features will increase. Due to this new number of features regularization terms will play an essential role in the final model. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Preprocessing\n\nAs stated above we got some categorical features and some continuous features. Categorical features will need labeling and one-hot encoding, continuous features will require scaling. We will also try to make some kind of nan handling for markdowns and others.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Contnuous features:\", continuous_features)\nprint(\"Categorical features:\", categorical_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimization_data.loc[:, categorical_features].dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"linear_preprocessor = ColumnTransformer([\n    \n    ('scaled_continous',\n     Pipeline([\n         ('imputer', SimpleImputer()), # This is not strictly necessary for well behaved features but it will serve as a general protection under bad data.\n         ('scaler', StandardScaler())\n     ]),\n    ['Temperature', 'Fuel_Price', 'CPI', 'Unemployment', 'Size', 'Year', 'Day', 'Month', 'WeekOfYear', 'DayOfYear']\n    ),\n    \n    ('markdowns',\n     Pipeline([\n         ('imputer', SimpleImputer(strategy=\"constant\", fill_value=0)), # Since markdowns has a lot of missing values and change a lot over time I will simply fill it with zeros\n         ('scaler', StandardScaler())\n     ]),\n     ['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']\n    ),\n    \n    (\"categorical\",\n     Pipeline([\n         (\"one_hot\", OneHotEncoder(handle_unknown='ignore'))\n     ]),\n     (['Store', 'Dept', 'Type', 'HolidayType', 'DayOfWeek'])\n    ),\n    \n    (\"others\",\n     \"passthrough\",\n     ['IsHoliday'] # IsHoliday is not actually categorical, it isn't necessary to put it in a OneHotEncoder\n    )\n    \n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"linear_preprocessor.fit_transform(optimization_features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"linear_estimator = ElasticNet()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Complete Pipeline","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"linear_model = Pipeline([\n    ('preprocessor', linear_preprocessor),\n    ('estimator', linear_estimator)\n])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Hyperparameters","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"linear_hyperparameters = {\n    \"estimator__l1_ratio\": [0.2, 0.5, 0.8, 1],\n    \"estimator__alpha\": [1e-1, 1, 1e1, 1e2, 1e3, 1e4, 1e5]\n}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Splits","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"kf = KFold(5)\nsplits = kf.split(optimization_features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Full Optimization","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"linear_optimizer = GridSearchCV(\n    linear_model,\n    linear_hyperparameters,\n    scoring=\"neg_mean_absolute_error\",\n    cv=5,\n    n_jobs=4,\n    return_train_score=True,\n    verbose=10\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"linear_optimizer.fit(optimization_features, optimization_target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"linear_optimizer.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(linear_optimizer.cv_results_).sort_values(by=\"rank_test_score\").loc[:, [\"mean_test_score\", \"std_test_score\", \"mean_train_score\", \"std_train_score\"]].head(8)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The best model after a hyperparameter search is a Lasso Regression with a penalty coefficient of 10. There is a small difference between train and test scores which are, compared with the absolute value of the error, fine. One problem with my optimization setup is the holiday weights, the optimizer is not taking into account that holidays weight 5 times more in the final score than regular days, this way some other solution that performs better in these cases will be ignored.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Evaluation\n\nHolidays worth 5 times a regular prediction. Let's take this into account.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_linear_pred_train = linear_optimizer.predict(optimization_features)\ny_linear_pred = linear_optimizer.predict(validation_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimization_performance_linear = mean_absolute_error(optimization_target, y_linear_pred_train, sample_weight=optimization_weights)\noptimization_performance_linear","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"validation_performance_linear = mean_absolute_error(validation_target, y_linear_pred, sample_weight=weights)\nvalidation_performance_linear","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_absolute_error(optimization_target, y_linear_pred_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_absolute_error(validation_target, y_linear_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Submission","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test_ids = full_test_data.Store.astype(str) + '_' + full_test_data.Dept.astype(str) + '_' + full_test_data.Date.astype(str)\nsample_submission[\"Id\"] = test_ids\ny_linear_test = linear_optimizer.predict(full_test_data.loc[:, features])\nsample_submission[\"Weekly_Sales\"] = y_linear_test\nsample_submission.to_csv('submission_linear.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> The submission results for the linear model are 11891.07291 and 11520.97285 for the public and private leaderboards respectivelly.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Explanation","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Preparations for explanation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"best_model = linear_optimizer.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"linear_preprocessed_feature_names = []\nfor transformation, transformer, columns in best_model.named_steps[\"preprocessor\"].transformers_:\n    if transformer == \"passthrough\":\n        linear_preprocessed_feature_names += columns\n        continue\n    last_transformer = transformer.steps[-1]\n    if last_transformer[0] == \"scaler\":\n        linear_preprocessed_feature_names += columns\n    if last_transformer[0] == \"one_hot\":\n        categories = last_transformer[1].categories_\n        for column, category in zip(columns, categories):\n            linear_preprocessed_feature_names += [column + \"_\" + str(i) for i in category]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(linear_preprocessed_feature_names)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### SHAP","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"optimization_features_linear_preprocessed = best_model.named_steps[\"preprocessor\"].transform(optimization_features).toarray()\nvalidation_features_linear_preprocessed = best_model.named_steps[\"preprocessor\"].transform(validation_features).toarray()\nlinear_explainer = shap.LinearExplainer(best_model.named_steps[\"estimator\"],\n                                        optimization_features_linear_preprocessed,\n                                        feature_perturbation=\"interventional\")\nlinear_shap_values = linear_explainer.shap_values(validation_features_linear_preprocessed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shap.summary_plot(linear_shap_values, validation_features_linear_preprocessed, linear_preprocessed_feature_names)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The summary plot confirms most of our ideas so far. Size is the most important variable, indicating that the store size matters. We can see that, after Size up to Year, Department variables appear with a single red dot, indicating the 1 value from the one-hot encoding. If we recover our column sort from the Department X Stores sales matrix we can check what is our prior position to these departments positions:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"columns_sort","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"They apper exactly in the same position up to the 13 label, in the 9th position! The first inversion happens with the swap of 94 and 8 labels.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Regression Weights","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"linear_model_weights_df = pd.DataFrame({\"column\": linear_preprocessed_feature_names, \"weights\": best_model.named_steps[\"estimator\"].coef_}).sort_values(\"weights\", ascending=False).reset_index(drop=True)\nlinear_model_weights_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(15, 3), dpi=200)\nlinear_model_weights_df[~(linear_model_weights_df[\"weights\"].abs() < 100)].plot.bar(x=\"column\", y=\"weights\", ax=ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"One cool feature of SHAP analysis for Linear Models is that, since SHAP takes into account the mean average value, we get some interesting sortings. If we just looked at the weights sorting we would just see *Size* after a bunch of department categories. With SHAP it is possible to identify that there is a big mass distributed over the variable values.","execution_count":null},{"metadata":{"_uuid":"fc6a2956-a7bd-412f-b5bc-979b866e4c34","_cell_guid":"3cfc6c52-6a15-48df-9b4d-92403399c109","trusted":true},"cell_type":"markdown","source":"## Gradient Boosting Model (XGBoost)\n\nXGBoost is a way more complex model, this requires a detailed usage of its API to take out the most it can offer.[](http://)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Preprocessing\n\nSince we will use the tree-based XGBoost, gbtree booster option, we won't need scaling preprocessing either one-hot encoders. But, since we are removing the one-hot encoder, we have to turn the string categorical features back again to some interpretable data type, we will perform this operation through an OrdinalEncoder. I will also not care about nan handling since XGBoost has a default tree path for missing data, every sample with missing will reach a leaf and these samples in the training dataset will be accounted for in the loss function.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_preprocessor = ColumnTransformer([\n    \n    ('labeler',\n     OrdinalEncoder(),\n    ['WeekOfYear', 'Type', 'HolidayType']\n    ),\n    \n    (\"others\",\n     \"passthrough\",\n     ['Temperature','Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5',\n      'CPI', 'Unemployment', 'Size', 'Year', 'Day', 'Month', 'DayOfYear', 'Store',\n      'Dept', 'IsHoliday', 'DayOfWeek']\n    )\n    \n])\n\nxgb_preprocessed_feature_names = ['WeekOfYear', 'Type', 'HolidayType'] + ['Temperature','Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5',\n      'CPI', 'Unemployment', 'Size', 'Year', 'Day', 'Month', 'DayOfYear', 'Store',\n      'Dept', 'IsHoliday', 'DayOfWeek']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_preprocessed_feature_names","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since I will try to make use of the learning API of xgboost it will be necessary to decouple preprocessing from the actual training, this opens space for data leakage problems. Since the only preprocessing step that I'm performing is OrdinalEncoder, that could be done beforehand without any specific knowledge of the sample, there will be no problem to preprocess all data and set them ready to get into the model.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### DMatrices","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"optimization_features_xgb_preprocessed = xgb_preprocessor.fit_transform(optimization_features)\nvalidation_features_xgb_preprocessed = xgb_preprocessor.transform(validation_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dmatrix_optimization = xgb.DMatrix(optimization_features_xgb_preprocessed, optimization_target)\ndmatrix_validation = xgb.DMatrix(validation_features_xgb_preprocessed, validation_target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dmatrix_optimization.num_col()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Hyperparameters\n\nAs mentioned before my choice for the base estimator will be trees (booster is gbtree). Boosting methods are extremely susceptible to overfitting. Optimizing hyperparameters for a model as complex as XGBoost is a hard task and requires a lot of effort and computation time. With my previous experiences in mind, I will set some general hyperparameters that certainly will help to reduce overfitting. These parameters are:\n* Subsample (0.5)\n    * Half of the dataset will be sampled for each boosting round\n* Colsample by tree (0.8)\n    * 80% of the features will be selected for each boosting round. Since I will also pick a colsample by level we must not set this number too low or no features will be available for some deep trees.\n* Colsample by level (0.95)\n    * Every new tree level we reduce the number of features by 5%, this will be around 1 feature for the first level.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"base_xgb_params = {\n    \"objective\": \"reg:squarederror\",\n    \"booster\": \"gbtree\",\n    \"subsample\": 0.5,\n    \"colsample_bytree\": 0.8,\n    \"colsample_bylevel\": 0.95,\n    \"num_parallel_tree\": 3, # Forests for the Win! This parameter controls the number of simultaneous tree trained at each boosting round.\n    \"eval_metric\": \"mae\"\n}\n\nxgb_hyperparameters = {\n    \"max_depth\": [3, 5, 10],\n    \"reg_alpha\": [1e1, 1e3, 1e5],\n    \"reg_lambda\": [1e2, 1e3, 1e4, 1e5], # let's choose some heavy regularization parameters\n}\n\nparameter_grid = [dict(**base_xgb_params, **i) for i in list(ParameterGrid(xgb_hyperparameters))]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Booster","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"results = []\nfor params in tqdm(parameter_grid[:]):\n    print(params)\n    xgb_cv_results = xgb.cv(params, dmatrix_optimization, num_boost_round=20, nfold=5, early_stopping_rounds=1, verbose_eval=False)\n    rounds = len(xgb_cv_results)\n    performance_values = xgb_cv_results.iloc[-1, :].to_dict()\n    print(performance_values)\n    result_dict = dict(**params, **performance_values, rounds=rounds, params=params)\n    results.append(result_dict)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see from the logs that the difference between test and train splits are very close to each other, indicating that the new tree being added have some generalization power. We still can overfit hyperparameters, but keeping the differences low and the std deviation of folds low gives us a track on building a good boosting model.\n\nI've tried other hyperparameter grids, with deeper trees and more boosting rounds, but after checking the submission results they clearly indicated overfitted models.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"results_df = pd.DataFrame(results).sort_values(\"test-mae-mean\")\nbest_model = results_df.iloc[0, :]\nbest_params = best_model[\"params\"]\n\nresults_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_params","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_estimator = xgb.train(best_params, dmatrix_optimization, num_boost_round=best_model[\"rounds\"], verbose_eval=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_xgb_preds_train = xgb_estimator.predict(dmatrix_optimization)\ny_xgb_preds = xgb_estimator.predict(dmatrix_validation)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimization_performance_xgboost = mean_absolute_error(optimization_target, y_xgb_preds_train, sample_weight=optimization_weights)\noptimization_performance_xgboost","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"validation_performance_xgboost = mean_absolute_error(validation_target, y_xgb_preds, sample_weight=weights)\nvalidation_performance_xgboost","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_absolute_error(optimization_target, y_xgb_preds_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_absolute_error(validation_target, y_xgb_preds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Submission","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test_ids = full_test_data.Store.astype(str) + '_' + full_test_data.Dept.astype(str) + '_' + full_test_data.Date.astype(str)\nsample_submission['Id'] = test_ids.values\ny_xgb_test = xgb_estimator.predict(xgb.DMatrix(xgb_preprocessor.transform(full_test_data.loc[:, features])))\nsample_submission['Weekly_Sales'] = y_xgb_test\nsample_submission.to_csv('submission_xgboost.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> The submissions results are 6364.36197 and 6111.85871 for the private and public leaderboard respectvely.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### SHAP Explanations","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model_bytearray = xgb_estimator.save_raw()[4:]\ndef myfun(self=None):\n    return model_bytearray\n\nxgb_estimator.save_raw = myfun","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"explainer = shap.TreeExplainer(xgb_estimator, feature_perturbation='tree_path_dependent')\nshap_values = explainer.shap_values(validation_features_xgb_preprocessed, check_additivity=False) # The additivty test is failing for less than 1e3 of difference.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shap.summary_plot(shap_values, validation_features_xgb_preprocessed, xgb_preprocessed_feature_names)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Again, departments, sizes, and stores appear as top features. Since we don't have an OneHotEncoder it is a bit harder to read the department's impact, but we can see that bigger values of department ids yield bigger SHAP values. The top departments from the linear models were 92, 95, 38, 72, 90, 40,  2, 91, 13,  8, 94,  4, 93, confirming that some departments are really important, especially the ones with high id values. Let's take a detailed look at the department's impact.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"shap.dependence_plot(\"Dept\", shap_values, validation_features_xgb_preprocessed, xgb_preprocessed_feature_names)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see the vertical bars indicating SHAP value for each department value. We can see again that departments around 90 have the most impact. We can also see that the high department number associated with the store size really contributes to high sales values.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## CatBoost\n\nAs we have seen in the previous section our heuristic model based on stores and department is really good! We got a real effort on training our xgboost model to beat it, and just by a small margin. Given that these two variables encode so much information I will make one last model that brings a key feature: categorical encoding. CatBoost is a really nice, very mature, [boosting model from Yandex](https://github.com/catboost/catboost), it has a categorical encoding technique that already encodes target information, this will be a combination of a powerful tree-based boosting algorithm with the information hidden in the stores and departments variables.\n\nI will not perform any kind of hyperparameter optimization and will tell CatBoost to encode just the stores' and departments' variables. I expect to beat our heuristic model by a significant margin. Due to this additional category encoding technique, that is based on target values, it introduces a permutation and sampling technique across boosting rounds to reduced overfitting, this increases significantly the amount of computational time necessary to train the model.\n\nSince I'm trying to capture the information encoded on target and departments I will only use the top 3 variables from XGBoost.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reduced_features = ['Size', 'Store', 'Dept',]\ncat_cat_feaures = ['Store', 'Dept']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_estimator = cat.CatBoostRegressor(cat_features=cat_cat_feaures)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_estimator.fit(optimization_features.loc[:, reduced_features], optimization_target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_estimator.tree_count_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_cat_preds_train = cat_estimator.predict(optimization_features.loc[:, reduced_features])\ny_cat_preds = cat_estimator.predict(validation_features.loc[:, reduced_features])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimization_performance_cat = mean_absolute_error(optimization_target, y_cat_preds_train, sample_weight=optimization_weights)\noptimization_performance_cat","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"validation_performance_cat = mean_absolute_error(validation_target, y_cat_preds, sample_weight=weights)\nvalidation_performance_cat","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_absolute_error(validation_target, y_cat_preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_absolute_error(optimization_target, y_cat_preds_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Submission","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test_ids = full_test_data.Store.astype(str) + '_' + full_test_data.Dept.astype(str) + '_' + full_test_data.Date.astype(str)\nsample_submission['Id'] = test_ids.values\ny_cat_test = cat_estimator.predict(full_test_data.loc[:, reduced_features])\nsample_submission['Weekly_Sales'] = y_cat_test\nsample_submission.to_csv('submission_cat.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> The submissions results are 6347.62468 and 6115.17444 for the private and public leaderboard respectvely.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Results\n\nHere is our summary of performance results:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"performances = pd.DataFrame(\n    [[optimization_performance_dummy_1, optimization_performance_dummy_2, optimization_performance_linear, optimization_performance_xgboost, optimization_performance_cat],\n     [validation_performance_dummy_1, validation_performance_dummy_2, validation_performance_linear, validation_performance_xgboost, validation_performance_cat]],\n    columns = [\"Dummy Model 1\", \"Dummy Model 2\", \"Linear Model\", \"XGBoost\", \"CatBoost\"],\n    index = [\"Optimization\", \"Validation\"]\n)\nperformances","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Units in USD*\n\nAll three real models beat up the dummy model by a significant margin. The result from the expected path is an improvement from USD 7989.3774 from our linear model to a USD 3337.6414 from our XGBoost model, an improvement of approximately 58%. The CatBoost model with only three features got a very similar result. The really unexpected result is our Dummy Model 2, based on the combined mean of stores and departments, none of the models was able to beat it.\n\nThe dummy model had the best overall performance even in the submission leaderboard.\n\nThe explanations plots and analysis confirm some of the observations made in the EDA such as:\n* Store Size is important.\n* Departments and store configuration are some of the most important things to predict sales values.\n* Not just store size, but department size also matters.\n* Since the competition metric was not directly optimized, IsHoliday played a minor role.\n\nThe heuristic model based on stores and departments means was the best model. The best machine learning model was XGBoost.\n\nAlthough holidays worth 5 times more their frequency is really, almost vanishing the effect of the weights, minor losses were observed for all the models.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Sample visualization\n\nAs a sanity check let's pick a store at random and look at the sum of predicted values over departments and compare all models.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"store_candidate = stores.sample(1).iloc[0, 0]\nstore_candidate\n\noptimization_preds_df = pd.DataFrame({\n    \"Store\": optimization_data.Store.values,\n    \"Dept\": optimization_data.Dept.values,\n    \"Date\": optimization_data.Date.values,\n    \"Weekly_Sales\": optimization_data.Weekly_Sales.values,\n    \"dummy_1\": y_mean_pred_train,\n    \"dummy_2\": stores_and_dept_preds_train,\n    \"linear\": y_linear_pred_train,\n    \"xgboost\": y_xgb_preds_train,\n    \"catboost\": y_cat_preds_train,\n})\n\nvalidation_preds_df = pd.DataFrame({\n    \"Store\": validation_data.Store.values,\n    \"Dept\": validation_data.Dept.values,\n    \"Date\": validation_data.Date.values,\n    \"Weekly_Sales\": validation_data.Weekly_Sales.values,\n    \"dummy_1\": y_mean_pred,\n    \"dummy_2\": stores_and_dept_preds,\n    \"linear\": y_linear_pred,\n    \"xgboost\": y_xgb_preds,\n    \"catboost\": y_cat_preds,\n})\n\ntest_preds_df = pd.DataFrame({\n    \"Store\": full_test_data.Store.values,\n    \"Dept\": full_test_data.Dept.values,\n    \"Date\": full_test_data.Date.values,\n    \"dummy_2\": stores_and_dept_preds_test.Weekly_Sales.values,\n    \"linear\": y_linear_test,\n    \"xgboost\": y_xgb_test,\n    \"catboost\": y_cat_test,\n})\n\nsummary_optimization = optimization_preds_df[optimization_preds_df.Store == store_candidate].groupby([\"Date\"], as_index=False).sum()\nsummary_validation = validation_preds_df[validation_preds_df.Store == store_candidate].groupby([\"Date\"], as_index=False).sum()\nsummary_test = test_preds_df[test_preds_df.Store == store_candidate].groupby([\"Date\"], as_index=False).sum()\n\nplots = [\"Weekly_Sales\", \"dummy_1\", \"dummy_2\", \"linear\", \"xgboost\", \"catboost\"]\ncolors = {\n    \"Weekly_Sales\": \"red\",\n    \"dummy_1\": \"green\",\n    \"dummy_2\": \"orange\",\n    \"linear\": \"brown\",\n    \"xgboost\": \"blue\",\n    \"catboost\": \"cyan\"\n}\n\nlinestyle = {\n    \"Weekly_Sales\": \"-.\"\n}\n\nf, ax = plt.subplots(figsize=(12, 4), dpi=130)\nfor p in plots:\n    kwargs = {\n        \"label\": p,\n        \"color\": colors.get(p),\n        \"linewidth\": 0.8,\n        \"linestyle\": linestyle.get(p, \"-\")\n    }\n    ax.plot(summary_optimization.Date, summary_optimization[p], **kwargs)\n    ax.plot(summary_validation.Date, summary_validation[p], **kwargs)\n    if p != \"Weekly_Sales\" and p != \"dummy_1\":\n        ax.plot(summary_test.Date, summary_test[p], **kwargs)\n\nax.axvspan(optimization_data.Date.min(), optimization_data.Date.max(), color='grey',alpha=0.14)\nax.axvspan(validation_data.Date.min(), validation_data.Date.max(), color='green',alpha=0.14)\nax.axvspan(test_preds_df.Date.min(), test_preds_df.Date.max(), color='red',alpha=0.14)\nax.set_title(\"Store \" + str(store_candidate))\nax.legend(loc=\"upper right\")\nplot_holidays(holidays, ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The dummy model presents now its weakeness.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Conclusions\n\nWe get by the end of the notebook with a very unusual situation where a heuristic model is the best candidate so far. Before discussing which model to pick I believe it is important to discuss deployment scenarios and application purposes of these models.\n\nThis is a Kaggle competition focused on the Walmart selection process. There is no reference in the problem statement of the real purpose of the model. Given that we don't have a specific context of usage we can list possible situations:\n\n**New Stores Placement**\n\nOne possible application for this model is to investigate which factors impact sales. With this short analysis, it was clear that some factors such as departments' configuration and store size impact sales. This analysis could be used to guide store changes or guide new store openings, fake feature values could be extracted from possible new store locations and inputted to the model to predict the future \"store value\". This application neglects the time index present on the data.\n\n**Sales Forecast**\n\nThis is the title of the competition and the most probable use case. The model can be placed in a system that will take the store ids and departments and forecast sale values for better storage planning and the anticipation of critical events. It is important to notice that all of our models, including our superpower dummy model, depends on the previous historical data for the store, none of our models would perform well on new stores (with new ids). It is important to recognize that all models developed in this analysis are tightly coupled with the stores' ids. If we wanted to apply this model to new stores we would have to think carefully about how to use the store id information.\n\nNone of our models are suitable for the first case.\n\nFor the second case, a time of experimentation would put the generalization power of XGBoost model at proof. I am inclined, with the current results, to pick the XGBoost model with a clear generalization, but it is really impressive what the Dummy Model is capable of. In a context where shadow predictions and a period of experimentation is not allowed, the Dummy Model could be a reasonable solution while we tweak the other model until better performance is reached. It is clear that the dummy model generalization is limited, we can check this on the predictions plot.\n\nDepartment Store Number, Size and Store Number encode most of the information, our CatBoost model, with only the top three XGBoost model features, had similar performance. This opens space for the question \"What kind of model generalization are we searching for?\", since the department number and store id play a so crucial role, what is exactly our predictions? Are we really learning where sales values come from? Are we capturing external effects from holidays and temperature or just memorizing which stores and departments sell more?\n\nIf the answer to the last question is yes, we have a very solid explanation of why our dummy model is so good. To understand better this results we really need to understand how this model would be used and what is its main objective.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Further Work\n\nWe got curious results and the analysis opened a lot of future work paths. I will list some of these paths here:\n* Understand why the dummy model was so good.\n    * Powerful models weren't able to beat it, why?\n    * We can make use of more advanced features of the boosting models and train them so they can beat our dummy model.\n* Detailed usage of XGBoost boosting rounds\n    * Clip new tress to avoid overfitting\n    * Test the DART booster\n* Better explore the target encoding from CatBoost. The dummy model is a very simple way to do this, a more powerful approach could lead to better results.\n* Explore the time dimension.\n    * No attention was given to feature engineering based on time, there is a role new world of time series methods that can enhance our models.\n    * We can combine the multiple kinds of feature extraction and the different time series modeling technique to enhance our results.\n* Optimize models with the desired metric.\n    * All optimizations ware made with the standard MSE. Dedicated optimization using the desired metric could select a better model for the problem purposes.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Thank you for your time.\n\nSincerely,\n\nOtávio Vasques","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}