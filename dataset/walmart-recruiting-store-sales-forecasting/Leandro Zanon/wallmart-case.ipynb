{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt \nimport numpy as np\nfrom sklearn.preprocessing import OneHotEncoder\nfrom xgboost import XGBRegressor","metadata":{"execution":{"iopub.status.busy":"2021-09-09T01:07:13.283434Z","iopub.execute_input":"2021-09-09T01:07:13.283858Z","iopub.status.idle":"2021-09-09T01:07:13.724435Z","shell.execute_reply.started":"2021-09-09T01:07:13.28377Z","shell.execute_reply":"2021-09-09T01:07:13.723458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Get data","metadata":{}},{"cell_type":"code","source":"DATA_PATH = '../input/walmart-recruiting-store-sales-forecasting/'\n\nstores = pd.read_csv(DATA_PATH + 'stores.csv')\nfeatures = pd.read_csv(DATA_PATH + 'features.csv.zip')\nfeatures['Date'] = pd.to_datetime(features['Date'])\n\n\ntrain = pd.read_csv(DATA_PATH + 'train.csv.zip')\ntrain['Date'] = pd.to_datetime(train['Date'])\n\ntest = pd.read_csv(DATA_PATH + 'test.csv.zip')\ntest['Date'] = pd.to_datetime(test['Date'])\n","metadata":{"execution":{"iopub.status.busy":"2021-09-09T01:10:37.691883Z","iopub.execute_input":"2021-09-09T01:10:37.69224Z","iopub.status.idle":"2021-09-09T01:10:38.164713Z","shell.execute_reply.started":"2021-09-09T01:10:37.69221Z","shell.execute_reply":"2021-09-09T01:10:38.163582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stores.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-09-09T01:10:39.319427Z","iopub.execute_input":"2021-09-09T01:10:39.319807Z","iopub.status.idle":"2021-09-09T01:10:39.340909Z","shell.execute_reply.started":"2021-09-09T01:10:39.319773Z","shell.execute_reply":"2021-09-09T01:10:39.339858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-09-09T01:10:40.215379Z","iopub.execute_input":"2021-09-09T01:10:40.215774Z","iopub.status.idle":"2021-09-09T01:10:40.23037Z","shell.execute_reply.started":"2021-09-09T01:10:40.215744Z","shell.execute_reply":"2021-09-09T01:10:40.228906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-09-09T01:10:41.086796Z","iopub.execute_input":"2021-09-09T01:10:41.087164Z","iopub.status.idle":"2021-09-09T01:10:41.1056Z","shell.execute_reply.started":"2021-09-09T01:10:41.087131Z","shell.execute_reply":"2021-09-09T01:10:41.104872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Join all data for train and test set","metadata":{}},{"cell_type":"code","source":"all_data = train.merge(stores, how = 'left')\\\n     .merge(features, how = 'left')\n\n\nall_test_data = test.merge(stores, how = 'left')\\\n     .merge(features, how = 'left')\n        ","metadata":{"execution":{"iopub.status.busy":"2021-09-09T01:10:42.535566Z","iopub.execute_input":"2021-09-09T01:10:42.535945Z","iopub.status.idle":"2021-09-09T01:10:42.791437Z","shell.execute_reply.started":"2021-09-09T01:10:42.535911Z","shell.execute_reply":"2021-09-09T01:10:42.790472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_data.head(5)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2021-09-09T01:10:42.933405Z","iopub.execute_input":"2021-09-09T01:10:42.933756Z","iopub.status.idle":"2021-09-09T01:10:42.956753Z","shell.execute_reply.started":"2021-09-09T01:10:42.933713Z","shell.execute_reply":"2021-09-09T01:10:42.955488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA\n\nThere are 45 distinct Stores and 81 distinct Departments. Combined we have 3331 (not all stores have all departments) different times series. \n\nPS: Although times series statistical modeling requeires autocorrelation and partial auto-correlation functions (ACF and PACF) to set a SARIMA family models parameters, wouldn't be so smart fitting a SARIMA model for each one of 3.331 times series due this amount of granularity. So, for now on I will evaluate the mean weekly sales to summarize all times series as one, so the EDA can be more effective and we keep some of statistical rigors.","metadata":{}},{"cell_type":"code","source":"print(all_data[['Store']].drop_duplicates().count())\nprint(all_data[['Dept']].drop_duplicates().count())\nprint(all_data[['Store', 'Dept']].drop_duplicates().count())","metadata":{"execution":{"iopub.status.busy":"2021-09-09T01:10:43.798344Z","iopub.execute_input":"2021-09-09T01:10:43.798759Z","iopub.status.idle":"2021-09-09T01:10:43.891761Z","shell.execute_reply.started":"2021-09-09T01:10:43.798708Z","shell.execute_reply":"2021-09-09T01:10:43.890623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15,3))\nall_data.groupby('Date')['Weekly_Sales'].mean().plot()\nplt.title('Average weekly Sales of the company across all stores in train dataset timeframe', fontsize=18)\nplt.ylabel('Sales', fontsize=16)\nplt.xlabel('Date', fontsize=16);","metadata":{"execution":{"iopub.status.busy":"2021-09-09T01:10:44.168525Z","iopub.execute_input":"2021-09-09T01:10:44.168976Z","iopub.status.idle":"2021-09-09T01:10:44.585359Z","shell.execute_reply.started":"2021-09-09T01:10:44.168939Z","shell.execute_reply":"2021-09-09T01:10:44.58396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_data.agg({'Date': [min, max]}).T\n","metadata":{"execution":{"iopub.status.busy":"2021-09-09T01:10:44.587025Z","iopub.execute_input":"2021-09-09T01:10:44.587358Z","iopub.status.idle":"2021-09-09T01:10:44.606782Z","shell.execute_reply.started":"2021-09-09T01:10:44.587323Z","shell.execute_reply":"2021-09-09T01:10:44.605558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Using mean of departments and stores weekly sales (from February 2010 until October 2012) plotted above something comes to mind when we see those spikes at the end of the years (2010, 2011), this could be a yearly seasonality, maybe due to Black Friday, Christmas and all the commemorative dates effects; As soon as we expect different behavior between normal days and commemorative ones it should be used as feature to out modeling.\n\nSo let's build some features from Date column to use as features:","metadata":{}},{"cell_type":"code","source":"def get_date_features(df: pd.DataFrame) -> pd.DataFrame:\n    '''\n    Inputs:\n        - df: pandas DataFrame; it must have a column named Date and it must be datetime type\n    Outputs:\n        - Year: year extracted from Date column;\n        - Month: month extracted from Date column; \n        - WeekOfYear: week number extracted from Date column;\n        \n    Example of usage:\n        > print(df)\n            |Date      |\n            |2010-02-05|\n\n        > df2 = get_date_features(df)\n        > print(df2)\n            |Date      |Year|Month|Day|WeekOfYear|\n            |2010-02-05|2010|2    |5  |6         |\n    '''\n    df['Year'] = df.Date.dt.year\n    df['Month'] = df.Date.dt.month\n    df['Day'] = df.Date.dt.day\n    df['WeekOfYear'] = df.Date.dt.isocalendar().week.astype(int)\n    \n    return df\n\nall_data = get_date_features(all_data)\nall_test_data = get_date_features(all_test_data)","metadata":{"execution":{"iopub.status.busy":"2021-09-09T01:10:45.303216Z","iopub.execute_input":"2021-09-09T01:10:45.30357Z","iopub.status.idle":"2021-09-09T01:10:45.551158Z","shell.execute_reply.started":"2021-09-09T01:10:45.303538Z","shell.execute_reply":"2021-09-09T01:10:45.550053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Looking for yearly seasonalities","metadata":{}},{"cell_type":"code","source":"mean_weekly_sales_2010 = all_data[all_data.Year==2010].groupby('WeekOfYear')['Weekly_Sales'].mean()\nmean_weekly_sales_2011 = all_data[all_data.Year==2011].groupby('WeekOfYear')['Weekly_Sales'].mean()\nmean_weekly_sales_2012 = all_data[all_data.Year==2012].groupby('WeekOfYear')['Weekly_Sales'].mean()\n\nplt.figure(figsize=(22,8))\nplt.plot(mean_weekly_sales_2010.index, mean_weekly_sales_2010.values)\nplt.plot(mean_weekly_sales_2011.index, mean_weekly_sales_2011.values)\nplt.plot(mean_weekly_sales_2012.index, mean_weekly_sales_2012.values)\n\nplt.xticks(np.arange(1, 53, step=1), fontsize=16)\nplt.yticks( fontsize=16)\nplt.xlabel('Week of Year', fontsize=20, labelpad=20)\nplt.ylabel('Sales', fontsize=20, labelpad=20)\n\nplt.title(\"Average Weekly Sales - By Year\", fontsize=24)\nplt.legend(['2010', '2011', '2012'], fontsize=20);","metadata":{"execution":{"iopub.status.busy":"2021-09-09T01:10:46.054174Z","iopub.execute_input":"2021-09-09T01:10:46.05453Z","iopub.status.idle":"2021-09-09T01:10:46.942567Z","shell.execute_reply.started":"2021-09-09T01:10:46.0545Z","shell.execute_reply":"2021-09-09T01:10:46.941584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Behavior above can be described as yearly seasonality, because 2010, 2011 and 2012 times series \"walk\" together up and down through weeks of years. ","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(15,3))\nall_data[all_data['Year']==2010].groupby('Month').mean()['Weekly_Sales'].plot()\nall_data[all_data['Year']==2011].groupby('Month').mean()['Weekly_Sales'].plot()\nall_data[all_data['Year']==2012].groupby('Month').mean()['Weekly_Sales'].plot()\nplt.title('Average weekly Sales of the company in each year', fontsize=18)\nplt.legend(['2010', '2011', '2012'], loc='best', fontsize=16)\nplt.ylabel('Sales', fontsize=16)\nplt.xlabel('Months', fontsize=16);","metadata":{"execution":{"iopub.status.busy":"2021-09-09T01:10:46.944229Z","iopub.execute_input":"2021-09-09T01:10:46.944901Z","iopub.status.idle":"2021-09-09T01:10:47.317981Z","shell.execute_reply.started":"2021-09-09T01:10:46.944845Z","shell.execute_reply":"2021-09-09T01:10:47.317216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Same as we saw by year but here Months are on x axis. It shows the same pattern for all three of the times series.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(15,3))\nall_data[all_data['Type']=='A'].groupby('Month').mean()['Weekly_Sales'].plot()\nall_data[all_data['Type']=='B'].groupby('Month').mean()['Weekly_Sales'].plot()\nall_data[all_data['Type']=='C'].groupby('Month').mean()['Weekly_Sales'].plot()\nplt.title('Average weekly Sales of the company by type of the store', fontsize=18)\nplt.legend(['Type A', 'Type B', 'Type C'], loc='best', fontsize=16)\nplt.ylabel('Sales', fontsize=16)\nplt.xlabel('Months', fontsize=16);","metadata":{"execution":{"iopub.status.busy":"2021-09-09T01:10:47.519909Z","iopub.execute_input":"2021-09-09T01:10:47.520457Z","iopub.status.idle":"2021-09-09T01:10:48.080373Z","shell.execute_reply.started":"2021-09-09T01:10:47.520408Z","shell.execute_reply":"2021-09-09T01:10:48.079088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For each type of the store, there is the same pattern of ups and downs by month for Type A and Type B Stores, but it happens in different levels, in other words, the three times series are randomly distributed in different levels.\nSo we expect the months affects differently by Type of the Stores, so we should build our model considering Type as feature too.","metadata":{}},{"cell_type":"markdown","source":"Below we see MarkDown columns has >60% of missing data, so I decided not filling it and not using due to lack of information.","metadata":{}},{"cell_type":"code","source":"all_data.isna().mean()","metadata":{"execution":{"iopub.status.busy":"2021-09-09T01:10:48.495217Z","iopub.execute_input":"2021-09-09T01:10:48.495583Z","iopub.status.idle":"2021-09-09T01:10:48.557668Z","shell.execute_reply.started":"2021-09-09T01:10:48.495553Z","shell.execute_reply":"2021-09-09T01:10:48.556707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_data.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-09-09T01:10:48.825581Z","iopub.execute_input":"2021-09-09T01:10:48.825959Z","iopub.status.idle":"2021-09-09T01:10:48.849005Z","shell.execute_reply.started":"2021-09-09T01:10:48.825928Z","shell.execute_reply":"2021-09-09T01:10:48.848253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"Select numerical columns as raw features and encode categorical columns to be used as features. Also set Weekly_sales as target column.","metadata":{}},{"cell_type":"code","source":"numeric_cols = ['Size',\n       'Temperature', 'Fuel_Price', 'CPI', 'Unemployment', 'Year', 'Month',\n       'WeekOfYear']\n\ncategorical_cols = ['IsHoliday', 'Type']\n\ntarget_col = 'Weekly_Sales'","metadata":{"execution":{"iopub.status.busy":"2021-09-09T01:10:49.763702Z","iopub.execute_input":"2021-09-09T01:10:49.764256Z","iopub.status.idle":"2021-09-09T01:10:49.768703Z","shell.execute_reply.started":"2021-09-09T01:10:49.764211Z","shell.execute_reply":"2021-09-09T01:10:49.767897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Below we have OneHotEncoder for IsHoliday and Type columns, I choosed this encoding because those as nominal categories and there is not a lot categories by each column (IsHoliday has 2 and Type has 3).\n\nPS: we have to do same transformation on train and test to fit and predict properly.","metadata":{}},{"cell_type":"code","source":"encoder_train = OneHotEncoder(sparse=False, handle_unknown='ignore').fit(all_data[categorical_cols])\nencoded_cols_train = list(encoder_train.get_feature_names(categorical_cols))\n\n\n\nencoder_test = OneHotEncoder(sparse=False, handle_unknown='ignore').fit(all_test_data[categorical_cols])\nencoded_cols_test = list(encoder_test.get_feature_names(categorical_cols))\n\n\nall_data[encoded_cols_train] = encoder_train.transform(all_data[categorical_cols])\nall_test_data[encoded_cols_test] = encoder_test.transform(all_test_data[categorical_cols])","metadata":{"execution":{"iopub.status.busy":"2021-09-09T01:10:50.431637Z","iopub.execute_input":"2021-09-09T01:10:50.432009Z","iopub.status.idle":"2021-09-09T01:10:51.663047Z","shell.execute_reply.started":"2021-09-09T01:10:50.431979Z","shell.execute_reply":"2021-09-09T01:10:51.662089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modeling","metadata":{}},{"cell_type":"code","source":"X = all_data[numeric_cols + encoded_cols_train]\nX_test = all_test_data[numeric_cols + encoded_cols_test]","metadata":{"execution":{"iopub.status.busy":"2021-09-09T01:10:51.66451Z","iopub.execute_input":"2021-09-09T01:10:51.664823Z","iopub.status.idle":"2021-09-09T01:10:51.753523Z","shell.execute_reply.started":"2021-09-09T01:10:51.664791Z","shell.execute_reply":"2021-09-09T01:10:51.751936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Set 25 trees, each tree with 4 nodes depth. Simple to be baseline model and also not overfitting","metadata":{}},{"cell_type":"code","source":"model = XGBRegressor(random_state=42, \n                     n_estimators= 25, \n                     max_depth=4)\n","metadata":{"execution":{"iopub.status.busy":"2021-09-09T01:10:51.755723Z","iopub.execute_input":"2021-09-09T01:10:51.756239Z","iopub.status.idle":"2021-09-09T01:10:51.761496Z","shell.execute_reply.started":"2021-09-09T01:10:51.756191Z","shell.execute_reply":"2021-09-09T01:10:51.760425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nmodel.fit(X, all_data[target_col])","metadata":{"execution":{"iopub.status.busy":"2021-09-09T01:10:51.794361Z","iopub.execute_input":"2021-09-09T01:10:51.794742Z","iopub.status.idle":"2021-09-09T01:10:54.885712Z","shell.execute_reply.started":"2021-09-09T01:10:51.794696Z","shell.execute_reply":"2021-09-09T01:10:54.88467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = model.predict(X)","metadata":{"execution":{"iopub.status.busy":"2021-09-09T01:10:54.887152Z","iopub.execute_input":"2021-09-09T01:10:54.887637Z","iopub.status.idle":"2021-09-09T01:10:55.002508Z","shell.execute_reply.started":"2021-09-09T01:10:54.887585Z","shell.execute_reply":"2021-09-09T01:10:55.001621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds_test = model.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-09-09T01:10:55.004273Z","iopub.execute_input":"2021-09-09T01:10:55.004903Z","iopub.status.idle":"2021-09-09T01:10:55.039856Z","shell.execute_reply.started":"2021-09-09T01:10:55.00486Z","shell.execute_reply":"2021-09-09T01:10:55.038951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\n\ndef rmse(y: float, y_hat: float) -> float:\n    '''\n    This functions returns RMSE considering two vectors\n    '''\n    return mean_squared_error(y, y_hat, squared=False)","metadata":{"execution":{"iopub.status.busy":"2021-09-09T01:10:55.041526Z","iopub.execute_input":"2021-09-09T01:10:55.042149Z","iopub.status.idle":"2021-09-09T01:10:55.04799Z","shell.execute_reply.started":"2021-09-09T01:10:55.042109Z","shell.execute_reply":"2021-09-09T01:10:55.046907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rmse(all_data[target_col], preds)","metadata":{"execution":{"iopub.status.busy":"2021-09-09T01:10:55.049505Z","iopub.execute_input":"2021-09-09T01:10:55.049911Z","iopub.status.idle":"2021-09-09T01:10:55.07203Z","shell.execute_reply.started":"2021-09-09T01:10:55.049877Z","shell.execute_reply":"2021-09-09T01:10:55.070465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"21549 as baseline RMSE. There is a lot to be done to squeeze this error.\n\nBelow I show feature importances to understand what's helping (and what's not) out modeling have this performance and drive us to collect and build new features in v2 of this model.","metadata":{}},{"cell_type":"code","source":"importance_df = pd.DataFrame({\n    'feature': X.columns,\n    'importance': model.feature_importances_\n}).sort_values('importance', ascending=False)","metadata":{"execution":{"iopub.status.busy":"2021-09-09T01:10:55.073269Z","iopub.execute_input":"2021-09-09T01:10:55.07355Z","iopub.status.idle":"2021-09-09T01:10:55.090573Z","shell.execute_reply.started":"2021-09-09T01:10:55.073523Z","shell.execute_reply":"2021-09-09T01:10:55.089429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"importance_df.head(10)","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nplt.figure(figsize=(10,6))\nplt.title('Feature Importance')\nsns.barplot(data=importance_df.head(10), x='importance', y='feature');","metadata":{"execution":{"iopub.status.busy":"2021-09-09T01:10:55.091998Z","iopub.execute_input":"2021-09-09T01:10:55.0923Z","iopub.status.idle":"2021-09-09T01:10:55.470202Z","shell.execute_reply.started":"2021-09-09T01:10:55.092271Z","shell.execute_reply":"2021-09-09T01:10:55.469081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Apperently Size feature is the most important during modeling, econometric data helps (but not as much as Size) maybe because it is not a particularity of Wallmart sales but just a \"temperature\" of the economy so it might be more inferential feature than predictive one. Maybe collecting, for example, the amount of money used on each promotional events could help us better on predictive side.","metadata":{}},{"cell_type":"markdown","source":"Also I always do a small KFold to make sure our model can generalize so we can trust it when it is deployed and it needs to predict data it has never get **any contact**.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import KFold\n\ndef train_and_evaluate(X_train, train_targets, X_val, val_targets, **params):\n    model = XGBRegressor(random_state=42, n_jobs=-1, **params)\n    model.fit(X_train, train_targets)\n    train_rmse = rmse(model.predict(X_train), train_targets)\n    val_rmse = rmse(model.predict(X_val), val_targets)\n    return model, train_rmse, val_rmse","metadata":{"execution":{"iopub.status.busy":"2021-09-09T01:10:55.63869Z","iopub.execute_input":"2021-09-09T01:10:55.639067Z","iopub.status.idle":"2021-09-09T01:10:55.645495Z","shell.execute_reply.started":"2021-09-09T01:10:55.639035Z","shell.execute_reply":"2021-09-09T01:10:55.644243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kfold = KFold(n_splits=5)\n\ntargets = all_data[target_col].copy()","metadata":{"execution":{"iopub.status.busy":"2021-09-09T01:10:56.553025Z","iopub.execute_input":"2021-09-09T01:10:56.553367Z","iopub.status.idle":"2021-09-09T01:10:56.558012Z","shell.execute_reply.started":"2021-09-09T01:10:56.553338Z","shell.execute_reply":"2021-09-09T01:10:56.557207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models = []\n\nfor train_idxs, val_idxs in kfold.split(X):\n    X_train, train_targets = X.iloc[train_idxs], targets.iloc[train_idxs]\n    X_val, val_targets = X.iloc[val_idxs], targets.iloc[val_idxs]\n    model, train_rmse, val_rmse = train_and_evaluate(X_train, \n                                                     train_targets, \n                                                     X_val, \n                                                     val_targets, \n                                                     max_depth=4, \n                                                     n_estimators=20)\n    models.append(model)\n    print('Train RMSE: {}, Validation RMSE: {}'.format(train_rmse, val_rmse))","metadata":{"execution":{"iopub.status.busy":"2021-09-09T01:10:57.159799Z","iopub.execute_input":"2021-09-09T01:10:57.160158Z","iopub.status.idle":"2021-09-09T01:11:07.171088Z","shell.execute_reply.started":"2021-09-09T01:10:57.160126Z","shell.execute_reply":"2021-09-09T01:11:07.170221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we see above, performance in train and validation set are not really different for each fold and also does not varies a lot considering all folds, so it is an okay modeling to be generalized.","metadata":{}},{"cell_type":"markdown","source":"## Things to try for v2 of this modeling","metadata":{}},{"cell_type":"markdown","source":"Something I would do on v2 of this model is consider lag of 1 week sales to be a feature. A important characteristics of times series analysis is autoregressive data, it means data in time t is auto-dependent of its own data in time t-1 (classical AR(p) models are Y_t = mu + theta * Y_t-1 + error). This could increase our predictive perfomance comparing to this MVP.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}