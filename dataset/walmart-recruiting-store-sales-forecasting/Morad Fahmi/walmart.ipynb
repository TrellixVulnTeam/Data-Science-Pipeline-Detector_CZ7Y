{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Import labraries","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#Importation des librairies\nimport seaborn as sns; sns.set(style=\"ticks\", color_codes=True)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor, RandomForestRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.feature_selection import RFE\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM, Dropout, GRU, Bidirectional\nfrom keras.optimizers import SGD\nimport math\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import mean_squared_error\nimport matplotlib.pyplot as plt\n#from sklearn.utils import check_arrays\nfrom math import sqrt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lire les données\ndf_train = pd.read_csv(\"/kaggle/input/walmart-recruiting-store-sales-forecasting/train.csv.zip\")\ndf_test = pd.read_csv(\"/kaggle/input/walmart-recruiting-store-sales-forecasting/test.csv.zip\")\ndf_store = pd.read_csv(\"/kaggle/input/walmart-recruiting-store-sales-forecasting/stores.csv\")\ndf_features = pd.read_csv(\"/kaggle/input/walmart-recruiting-store-sales-forecasting/features.csv.zip\")\n#df_train = df_train.merge(df_store, on='Store', how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visiulaser les données\ndf_features.head()\ndf_features.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fonction pour la création des shémas\ndef scatter(column):\n    plt.figure()\n    plt.scatter(df_fusion[column] , df_train['Weekly_Sales'])\n    plt.ylabel('weeklySales')\n    plt.xlabel(column)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# La création de l'erreur WMAE\ndef WMAE(dataset, real, predicted):\n    weights = dataset.IsHoliday.apply(lambda x: 5 if x else 1)\n    return np.round(np.sum(weights*abs(real-predicted))/(np.sum(weights)), 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# La créastion de l'erreur RMSE\ndef rmse(val_y, y_pred) :\n    return sqrt(mean_squared_error(val_y, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fusionner les données\ndf = pd.merge(df_features,df_store,on='Store',how='inner')\ndf_fusion = pd.merge(df,df_train,on=['Store','Date','IsHoliday']) \n# L'utilisation des 3 critères 'Store','Date','IsHoliday' en même temps aide à optimiser l'utilisation \n#de la RAM sinon Kaggle donne l'erreur 'Your notebook tried to allocate more memory than is available. It has restarted.'\n \ndf_fusion.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_fusion.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.jointplot(x=\"Store\", y = \"Weekly_Sales\", data = df_fusion)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_fusion.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Elimination des valeurs des Weekly_Sales qui sont inf à 0\n#df_fusion.drop(df_fusion[df_fusion['Weekly_Sales'] < 0].index, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_fusion.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Préparation des données pour la méthodes LSTM où nous allons traiter les magazins individuelement \nL = []\nfor i in range(45) :\n    L += [df_fusion[df_fusion['Store'] == i+1 ]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"L[0].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#for i in range(45) :\n#    print('the store ',i,'has :', L[i].shape[0],'lines')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndf_fusion['Date'] = pd.to_datetime(df_fusion['Date'])\ndf_fusion['Year'] = pd.to_datetime(df_fusion['Date']).dt.year\ndf_fusion['Month'] = pd.to_datetime(df_fusion['Date']).dt.month\ndf_fusion['Week'] = pd.to_datetime(df_fusion['Date']).dt.week\ndf_fusion['Day'] = pd.to_datetime(df_fusion['Date']).dt.day\ndf_fusion.replace({'A': 1, 'B': 2,'C':3},inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# La valeur à prédir\ny = df_fusion.Weekly_Sales\n# Identifier les critères que l'alghoritme doit prendre en compte\nfeatures = ['Dept', 'IsHoliday', 'Size','Temperature','Fuel_Price','CPI','Unemployment','Type']\n# Au début je n'ai pas pu utliser ces critères 'Temperature','Fuel_Price','CPI','Unemployment'. En les ajoutant \n#j'ai pu optimiser le 'mean absolute error' par 630 dans la méthode 'RandomForestRegressor'\n#et 100 dans la méthode 'DecisionTreeRegressor' et 650 dans la méthode 'XGBRegressor'\n\nX = df_fusion[features]\n            \ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Les shémats\nscatter('Fuel_Price')\nscatter('Size')\nscatter('CPI')\nscatter('Type')\nscatter('IsHoliday')\nscatter('Unemployment')\nscatter('Temperature')\nscatter('Store')\nscatter('Dept')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weekly_sales_2010 = df_fusion[df_fusion.Year==2010]['Weekly_Sales'].groupby(df_fusion['Week']).mean()\nweekly_sales_2011 = df_fusion[df_fusion.Year==2011]['Weekly_Sales'].groupby(df_fusion['Week']).mean()\nweekly_sales_2012 = df_fusion[df_fusion.Year==2012]['Weekly_Sales'].groupby(df_fusion['Week']).mean()\nplt.figure(figsize=(20,8))\nsns.lineplot(weekly_sales_2010.index, weekly_sales_2010.values)\nsns.lineplot(weekly_sales_2011.index, weekly_sales_2011.values)\nsns.lineplot(weekly_sales_2012.index, weekly_sales_2012.values)\nplt.grid()\nplt.xticks(np.arange(1, 53, step=1))\nplt.legend(['2010', '2011', '2012'], loc='best', fontsize=16)\nplt.title('Les ventes hebdomadaires pour chaque année', fontsize=18)\nplt.ylabel('Ventes', fontsize=16)\nplt.xlabel('Semaine', fontsize=16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"_kg_hide-output":false,"trusted":true},"cell_type":"code","source":"# Méthode Machine Learning\n# Méthode RandomForestRegressor\niowa_model = RandomForestRegressor(random_state=1)\niowa_model.fit(train_X, train_y)\nval_predictions = iowa_model.predict(val_X)\nval_mae = mean_absolute_error(val_predictions, val_y)\nval_mse = mean_squared_error(val_y, val_predictions)\nval_rmse = rmse(val_y, val_predictions)\nval_r2 = r2_score(val_y, val_predictions)\nval_wmae = WMAE(val_X, val_y, val_predictions)\nprint('Erreur MAE RandomForestRegressor: ', val_mae)\nprint('Erreur MSE RandomForestRegressor: ', val_mse)\nprint('Erreur RMSE RandomForestRegressor: ', val_rmse)\nprint('Erreur R2 RandomForestRegressor: ', val_r2)\nprint('Erreur WMAE RandomForestRegressor: ', val_wmae)\n\n\n# Méthode DecisionTreeRegressor\n\niowa_model_tree = DecisionTreeRegressor(random_state=1)\niowa_model_tree.fit(train_X, train_y)\nval_predictions_tree = iowa_model_tree.predict(val_X)\nval_mae_tree = mean_absolute_error(val_predictions_tree, val_y)\nval_mse_tree = mean_squared_error(val_y, val_predictions_tree)\nval_rmse_tree = rmse(val_y, val_predictions_tree)\nval_r2_tree = r2_score(val_y, val_predictions_tree)\nval_wmae_tree = WMAE(val_X, val_y, val_predictions_tree)\nprint('Erreur MAE DecisionTreeRegressor: ', val_mae_tree)\nprint('Erreur MSE DecisionTreeRegressor: ', val_mse_tree)\nprint('Erreur RMSE DecisionTreeRegressor: ', val_rmse_tree)\nprint('Erreur R2 DecisionTreeRegressor: ', val_r2_tree)\nprint('Erreur WMAE DecisionTreeRegressor: ', val_wmae_tree)\n\n\n#Méthode XGBRegressor\nfrom xgboost import XGBRegressor\nmy_model = XGBRegressor(n_estimators=500)\nmy_model.fit(train_X, train_y, \n             early_stopping_rounds=5, \n             eval_set=[(val_X, val_y)], \n             verbose=False)\nval_predictions_XG = my_model.predict(val_X)\nval_mae_XG = mean_absolute_error(val_predictions_XG, val_y)\nval_mse_XG = mean_squared_error(val_y, val_predictions_XG)\nval_rmse_XG = rmse(val_y, val_predictions_XG)\nval_r2_XG = r2_score(val_y, val_predictions_XG)\nval_wmae_XG = WMAE(val_X, val_y, val_predictions_XG)\nprint('Erreur MAE XGBRegressor: ', val_mae_XG)\nprint('Erreur MSE XGBRegressor: ', val_mse_XG)\nprint('Erreur RMSE XGBRegressor: ', val_rmse_XG)\nprint('Erreur R2 XGBRegressor: ', val_r2_XG)\nprint('Erreur WMAE XGBRegressor: ', val_wmae_XG)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Mise à l'échelle de la dataset\n#sc = MinMaxScaler(feature_range=(0,1))\n#training_set_scaled = sc.fit_transform(train_X)\n#val_X = sc.fit_transform(val_X)\n#print(val_X)\n#print(training_set_scaled)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# LSTM en utilisant tous les données\n#X_train, y_train = np.array(training_set_scaled), np.array(train_y)\n#val_X, val_y = np.array(val_X), np.array(val_y)\n\n#val_X = np.reshape(val_X, (val_X.shape[0],val_X.shape[1],1))\n#X_train = np.reshape(X_train, (X_train.shape[0],X_train.shape[1],1))\n#print(X_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#LSTM en utilisant que les weekly_sales\nW = L[1].Weekly_Sales\nX1_train, X1_test = train_test_split(W,test_size = 0.2)\n\n\nX1_train = X1_train.values.reshape(-1,1)\nsc = MinMaxScaler(feature_range=(0,1))\ntraining_set_scaled = sc.fit_transform(X1_train)\n\n\nX_train = []\ny_train = []\n# Nous avons pris une longueur sup à 52 pour que le programme soit capable d'identifier plus précisement les variations\n# des ventes durant toute l'année\nfor i in range(60,7000):\n    X_train.append(training_set_scaled[i-60:i,0])\n    y_train.append(training_set_scaled[i,0])\n\nX_train, y_train = np.array(X_train), np.array(y_train)\nX_train = np.reshape(X_train, (X_train.shape[0],X_train.shape[1],1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"regressor = Sequential()\n# Première couche LSTM \nregressor.add(LSTM(units=200, return_sequences=True, input_shape=(X_train.shape[1],1)))\nregressor.add(Dropout(0.2))\n# 2éme couche\nregressor.add(LSTM(units=200, return_sequences=True))\nregressor.add(Dropout(0.2))\n# 3éme couche\nregressor.add(LSTM(units=200, return_sequences=True))\nregressor.add(Dropout(0.2))\n# 4éme couche\nregressor.add(LSTM(units=200))\nregressor.add(Dropout(0.2))\n# Couche de sortie\nregressor.add(Dense(units=1))\n\n# Compilation du RNN\nregressor.compile(optimizer='rmsprop',loss='mean_squared_error')\n# Adaptation\nregressor.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#val_X = sc.fit_transform(val_X)\nX1_test = X1_test.values.reshape(-1,1)\nX1_test = sc.fit_transform(X1_test)\n\nX_test = []\ny_test = []\nfor i in range(60,400):\n    X_test.append(X1_test[i-60:i,0])\n    y_test.append(X1_test[i,0])\nX_test = np.array(X_test)\ny_test = np.array(y_test)\nX_test = np.reshape(X_test, (X_test.shape[0],X_test.shape[1],1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_prediction_LSTM = regressor.predict(X_test)\nval_y = np.array(y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nval_mae_LSTM = mean_absolute_error(val_prediction_LSTM, y_test)\nval_mse_LSTM = mean_squared_error(val_y, val_prediction_LSTM)\nval_rmse_LSTM = rmse(val_y, val_prediction_LSTM)\nval_r2_LSTM = r2_score(val_y, val_prediction_LSTM)\nprint('Erreur MAE LSTM: ', val_mae_LSTM)\nprint('Erreur MSE LSTM: ', val_mse_LSTM)\nprint('Erreur RMSE LSTM: ', val_rmse_LSTM)\nprint('Erreur R2 LSTM: ', val_r2_LSTM)\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}