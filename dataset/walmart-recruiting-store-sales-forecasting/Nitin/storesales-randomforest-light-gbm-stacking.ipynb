{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Import Libraries, Datasets & Declare Functions"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Import libraries | Standard\nimport pandas as pd\nimport numpy as np\nimport os\nimport datetime\nimport warnings\nfrom time import time\n\n# Import libraries | Visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\n\n# Import libraries | Sk-learn\nfrom sklearn.preprocessing import MinMaxScaler, RobustScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, mean_squared_log_error\nfrom sklearn.metrics.scorer import make_scorer\nfrom sklearn.linear_model import LinearRegression, Lasso, ElasticNet\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.model_selection import KFold, cross_val_score, GridSearchCV\n\nimport xgboost as xgb\nfrom lightgbm import LGBMRegressor\n\n# Pretty display for notebooks\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"pd.set_option('display.max_columns', None)  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#pd.set_option('display.max_rows', None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"warnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def distribution(data, features, transformed = False):\n    \"\"\"\n    Visualization code for displaying distributions of features\n    \"\"\"\n    \n    # Create figure\n    fig = plt.figure(figsize = (11,5));\n\n    # Skewed feature plotting\n    for i, feature in enumerate(features):\n        ax = fig.add_subplot(1, 2, i+1)\n        ax.hist(data[feature], bins = 25, color = '#00A0A0')\n        ax.set_title(\"'%s' Feature Distribution\"%(feature), fontsize = 14)\n        ax.set_xlabel(\"Value\")\n        ax.set_ylabel(\"Number of Records\")\n        ax.set_ylim((0, 2000))\n        ax.set_yticks([0, 500, 1000, 1500, 2000])\n        ax.set_yticklabels([0, 500, 1000, 1500, \">2000\"])\n\n    # Plot aesthetics\n    if transformed:\n        fig.suptitle(\"Log-transformed Distributions of Continuous Data Features\", \\\n            fontsize = 16, y = 1.03)\n    else:\n        fig.suptitle(\"Distributions of Continuous Data Features\", \\\n            fontsize = 16, y = 1.03)\n\n    fig.tight_layout()\n    fig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def eval_train_predict(learner, sample_size, train_X, train_y, test_X, test_y, transform_y, log_constant): \n    '''\n    inputs:\n       - learner: the learning algorithm to be trained and predicted on\n       - sample_size: the size of samples (number) to be drawn from training set       \n       - train_X: features training set\n       - train_y: sales training set\n       - test_X: features testing set\n       - test_y: sales testing set\n    '''\n    \n    results = {}\n    \n    # Fit the learner to the training data\n    start = time() # Get start time\n    learner = learner.fit(train_X[:sample_size], train_y[:sample_size])\n    end = time() # Get end time\n    \n    # Calculate the training time\n    results['time_train'] = end - start\n        \n    # Get the predictions on the test set(X_test),\n    start = time() # Get start time\n    predictions = learner.predict(test_X)\n    end = time() # Get end time\n    \n    # Calculate the total prediction time\n    results['time_pred'] = end - start\n            \n    # Compute Weighted Mean Absolute Error on Test Set\n    if transform_y == 'log':\n        results['WMAE'] = weighted_mean_absolute_error(np.exp(test_y) - 1 - log_constant, \n                                                       np.exp(predictions) - 1 - log_constant, \n                                                       compute_weights(test_X['IsHoliday']))\n    else:\n        results['WMAE'] = weighted_mean_absolute_error(test_y, predictions, compute_weights(test_X['IsHoliday']))\n                   \n    # Success\n    print(\"{} trained on {} samples.\".format(learner.__class__.__name__, sample_size))\n        \n    # Return the results\n    return results","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def eval_visualize(results):\n    \"\"\"\n    Visualization code to display results of various learners.\n    \n    inputs:\n      - learners: a list of supervised learners\n      - stats: a list of dictionaries of the statistic results from 'train_predict()'\n    \"\"\"\n  \n    # Create figure\n    fig, ax = plt.subplots(1, 3, figsize = (18,8))\n\n    # Constants\n    bar_width = 0.1\n    colors = ['#A00000','#00A0A0','#00A000','#E3DAC9','#555555', '#87CEEB']\n    metrics = ['time_train', 'time_pred', 'WMAE']\n    \n    # Super loop to plot four panels of data\n    for k, learner in enumerate(results.keys()):\n        for j, metric in enumerate(metrics):\n            # Creative plot code\n            ax[j%3].bar(0+k*bar_width, results[learner][0][metric], width = bar_width, color = colors[k])\n            ax[j%3].set_xlabel(\"Models\")\n            ax[j%3].set_xticklabels([''])\n                \n    # Add unique y-labels\n    ax[0].set_ylabel(\"Time (in seconds)\")\n    ax[1].set_ylabel(\"Time (in seconds)\")\n    ax[2].set_ylabel(\"WMAE\")\n    \n    # Add titles\n    ax[0].set_title(\"Model Training\")\n    ax[1].set_title(\"Model Predicting\")\n    ax[2].set_title(\"WMAE on Testing Set\")\n \n    # Create patches for the legend\n    patches = []\n    for i, learner in enumerate(results.keys()):\n        patches.append(mpatches.Patch(color = colors[i], label = learner))\n    plt.legend(handles = patches, bbox_to_anchor = (-.80, 2.43), \\\n               loc = 'upper center', borderaxespad = 0., ncol = 3, fontsize = 'x-large')\n    \n    # Aesthetics\n    plt.suptitle(\"Performance Metrics for Supervised Learning Models\", fontsize = 16, y = 1.10)\n    plt.tight_layout()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def train_predict(learner, train_X, train_y, test_X, test_y, transform_y, log_constant, verbose=0): \n    '''\n    inputs:\n       - learner: the learning algorithm to be trained and predicted on\n       - train_X: features training set\n       - train_y: sales training set\n       - test_X: features testing set\n       - test_y: sales testing set\n    '''\n    \n    results = {}\n    \n    # Fit the learner to the training data\n    start = time() # Get start time\n    learner = learner.fit(train_X, train_y)\n    end = time() # Get end time\n    \n    # Calculate the training time\n    results['time_train'] = end - start\n        \n    # Get the predictions on the test set(X_test),\n    start = time() # Get start time\n    predictions = learner.predict(test_X)\n    end = time() # Get end time\n    \n    # Calculate the total prediction time\n    results['time_pred'] = end - start\n            \n    # Compute Weighted Mean Absolute Error on Test Set\n    if transform_y == 'log':\n        results['WMAE'] = weighted_mean_absolute_error(np.exp(test_y) - 1 - log_constant, \n                                                       np.exp(predictions) - 1 - log_constant, \n                                                       compute_weights(test_X['IsHoliday']))\n    else:\n        results['WMAE'] = weighted_mean_absolute_error(test_y, predictions, compute_weights(test_X['IsHoliday']))\n    \n\n    #Extract the feature importances\n    importances = learner.feature_importances_\n\n    # Success\n    print(\"Learner Name :\", learner.__class__.__name__)\n    print(\"Training     :\", round(results['time_train'],2), \"secs /\", len(train_y), \"records\")\n    print(\"Predicting   :\", round(results['time_pred'],2), \"secs /\", len(test_y), \"records\")\n    print(\"Weighted MAE :\", round(results['WMAE'],2))\n\n    if verbose == 1:\n        # Plot\n        print(\"\\n<Feature Importance>\\n\")\n        feature_plot(importances, train_X, train_y, 10)\n\n        print(\"\\n<Feature Weightage>\\n\")\n        topk = len(train_X.columns)\n        indices = np.argsort(importances)[::-1]\n        columns = train_X.columns.values[indices[:topk]]\n        values = importances[indices][:topk]\n\n        for i in range(topk):\n            print('\\t' + columns[i] + (' ' * (15 - len(columns[i])) + ': ' + str(values[i])))\n            \n        print(\"\\n<Learner Params>\\n\", model.get_params())\n    \n    # Return the model & predictions\n    return (learner, predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def feature_plot(importances, train_X, train_y, topk=5):\n    \n    # Display the most important features\n    indices = np.argsort(importances)[::-1]\n    columns = train_X.columns.values[indices[:topk]]\n    values = importances[indices][:topk]\n\n    # Creat the plot\n    fig = plt.figure(figsize = (18,5))\n    plt.title(\"Normalized Weights for First \" + str(topk) + \" Most Predictive Features\", fontsize = 16)\n    plt.bar(np.arange(topk), values, width = 0.6, align=\"center\", color = '#00A000', \\\n          label = \"Feature Weight\")\n    plt.bar(np.arange(topk) - 0.3, np.cumsum(values), width = 0.2, align = \"center\", color = '#00A0A0', \\\n          label = \"Cumulative Feature Weight\")\n    plt.xticks(np.arange(topk), columns)\n    plt.xlim((-0.5, 9.5))\n    plt.ylabel(\"Weight\", fontsize = 12)\n    plt.xlabel(\"Feature\", fontsize = 12)\n    \n    plt.legend(loc = 'upper left')\n    plt.tight_layout()\n    plt.show()  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def reduce_mem_usage(df, verbose=True):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"    \n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))    \n    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def compute_weights(holidays):\n    return holidays.apply(lambda x: 1 if x==0 else 5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def weighted_mean_absolute_error(pred_y, test_y, weights):\n    return 1/sum(weights) * sum(weights * abs(test_y - pred_y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\ncnt = 0\nenv = 'Outside Kaggle'\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        cnt += 1\n        print(os.path.join(dirname, filename))\n        \nif cnt > 0:\n    env = 'Kaggle Kernel'","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print('Environment:', env)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Read input files\nif env == 'Kaggle Kernel':\n    features = pd.read_csv(\"/kaggle/input/walmart-recruiting-store-sales-forecasting/features.csv\")\n    stores = pd.read_csv(\"/kaggle/input/walmart-recruiting-store-sales-forecasting/stores.csv\")\n    train = pd.read_csv(\"/kaggle/input/walmart-recruiting-store-sales-forecasting/train.csv\")\n    test = pd.read_csv(\"/kaggle/input/walmart-recruiting-store-sales-forecasting/test.csv\")\nelse:    \n    features = pd.read_csv(\"data/features.csv\")\n    stores = pd.read_csv(\"data/stores.csv\")\n    train = pd.read_csv(\"data/train.csv\")\n    test = pd.read_csv(\"data/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"train=reduce_mem_usage(train)\ntest=reduce_mem_usage(test)"},{"metadata":{},"cell_type":"markdown","source":"# Data Exploration"},{"metadata":{},"cell_type":"markdown","source":"## 1. Stores Data"},{"metadata":{"trusted":false},"cell_type":"code","source":"stores.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"stores.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"stores.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#missing data\ntotal = stores.isnull().sum().sort_values(ascending=False)\npercent = (stores.isnull().sum()/stores.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"stores['Size'].groupby(stores['Type']).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Create figure\n#plt.figure()\n#plt.scatter(stores['Type'], stores['Store'])\n#plt.ylabel('Store ID')\n#plt.xlabel('Store Type')\n\nfig, ax = plt.subplots(1, 2, figsize = (15,6))\nax[0].bar(stores['Type'].unique(), stores['Size'].groupby(stores['Type']).count())\nax[0].set_ylabel('# of Stores')\nax[0].set_xlabel('Store Type')\nax[0].yaxis.grid(True, linewidth=0.3)\n\nax[1].scatter(stores['Type'], stores['Size'])\nax[1].scatter(stores['Type'].unique(), stores['Size'].groupby(stores['Type']).mean()) #Store Type Average Store Size Vs \nax[1].set_ylabel('Store Size (Total / Average)')\nax[1].set_xlabel('Store Type')\nax[1].yaxis.grid(True, linewidth=0.3)\n\n#plt.figure(figsize=(6,6))\n#plt.yticks(np.arange(len(features_missing)),features_missing.index,rotation='horizontal')\n#plt.xlabel('fraction of rows with missing data')\n#plt.barh(np.arange(len(features_missing)), features_missing)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"stores[(stores['Size'] < 40000) & (~stores['Type'].isin(['C']))]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Explore Weekly Sales - histogram\nsns.distplot(stores['Size'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### Takeaways: \n1. Column TYPE is a candidate for one-hot encoding. \n2. Most stores are of TYPE='A'. Only a few stores are of TYPE='C'.\n3. TYPE columns seem to be linked to Store Size. Average store size of TYPE 'A' is ~ 175k, TYPE 'B' is ~ 100k and TYPE 'C' is ~40k\n4. Four stores [3, 5, 33 & 36] whose size is < 40k, seem to have been incorrectly tagged as Types A & B"},{"metadata":{},"cell_type":"markdown","source":"## 2. Features Data"},{"metadata":{"trusted":false},"cell_type":"code","source":"features.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"features.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"features.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#missing data\ntotal = features.isnull().sum().sort_values(ascending=False)\npercent = (features.isnull().sum()/features.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Distribution of NaNs for all columns\nfeatures_missing = features.isna().sum()/len(features) * 100","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(6,6))\nplt.yticks(np.arange(len(features_missing)),features_missing.index,rotation='horizontal')\nplt.xlabel('fraction of rows with missing data')\nplt.barh(np.arange(len(features_missing)), features_missing)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"fig, ax = plt.subplots(2, 2, figsize = (15,12))\n\n# Plot 1: Year Vs # of Records\nax[0,0].barh(features['Date'].str.slice(start=0, stop=4).unique(), \n          features['Date'].str.slice(start=0, stop=4).value_counts())\nax[0,0].set_xlabel('# of Records')\nax[0,0].set_ylabel('Year')\nax[0,0].yaxis.grid(True, linewidth=0.3)\n\n# Plot 2: Month Vs # of Records with Missing Values - Unemployment\nax[1,0].barh(features['Date'].str.slice(start=0, stop=7)[features['Unemployment'].isna()].unique(), \n          features['Date'].str.slice(start=0, stop=7)[features['Unemployment'].isna()].value_counts())\nax[1,0].set_xlabel('# of Records with Missing Values - Unemployment')\nax[1,0].set_ylabel('Month')\nax[1,0].yaxis.grid(True, linewidth=0.3)\n\n# Plot 3: Month Vs # of Records with Missing Values - CPI\nax[1,1].barh(features['Date'].str.slice(start=0, stop=7)[features['CPI'].isna()].unique(), \n          features['Date'].str.slice(start=0, stop=7)[features['CPI'].isna()].value_counts())\nax[1,1].set_xlabel('# of Records with Missing Values - CPI')\nax[1,1].set_ylabel('Month')\nax[1,1].yaxis.grid(True, linewidth=0.3)\n\n#plt.figure(figsize=(6,6))\n#plt.yticks(np.arange(len(features_missing)),features_missing.index,rotation='horizontal')\n#plt.xlabel('fraction of rows with missing data')\n#plt.barh(np.arange(len(features_missing)), features_missing)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"holidays = ['2010-02-12', '2011-02-11', '2012-02-10', '2013-02-08', #Super Bowl\n           '2010-09-10', '2011-09-09', '2012-09-07', '2013-02-06',  #Labor Day\n           '2010-11-26', '2011-11-25', '2012-11-23', '2013-11-29',  #Thanksgiving\n           '2010-12-31', '2011-12-30', '2012-12-28', '2013-12-27']  #Christmas","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Validate Holidays\nfeatures['IsHoliday'][features['Date'].isin(holidays)].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"features['Date'][features['IsHoliday'].isin([1])][~features['Date'].isin(holidays)].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"features[['CPI','Unemployment']].groupby([features['Store'], features['Date'].str.slice(start=0, stop=7)]).mean().head(84)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"features.groupby(features['Date'].str.slice(start=0, stop=7))['MarkDown1','MarkDown2','MarkDown3','MarkDown4','MarkDown5'].count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Explore Distribution\ndistribution(features, ['CPI','Unemployment'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Explore Distribution\ndistribution(features, ['Temperature','Fuel_Price'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Explore Distribution\ndistribution(features, ['MarkDown1','MarkDown2'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Explore Distribution\ndistribution(features, ['MarkDown3','MarkDown4'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Explore Distribution\ndistribution(features, ['MarkDown5'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### Takeaways: \n1. Data requires pre-processing\n2. Column(s) ISHOLIDAY has been validated\n3. Column(s) UNEMPLOYMENT & CPI have missing values for May, Jun & Jul 2013. For these columns as the values dont change significantly month on month, value from Apr 2013 would be propogated over for each store. \n4. Column(s) MARKDOWN* have missing values for 2010 (entire year) and 2011 (until Nov). Additionally, there are missing values for other other dates as well. \n5. CPI and UNEMPLOYMENT value are a bit skewed. MARKDOWN* columns are skewed. "},{"metadata":{},"cell_type":"markdown","source":"## 3. Train Data"},{"metadata":{"trusted":false},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Explore Date Range\ntrain['Date'].str.slice(start=0, stop=4).value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Validate Holidays\ntrain['IsHoliday'][train['Date'].isin(holidays)].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train['Date'][train['IsHoliday'].isin([1])][~train['Date'].isin(holidays)].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Explore Distribution\ndistribution(train, ['Weekly_Sales'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train['Store'][train['Weekly_Sales'] < 0].count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_outliers = pd.merge(train, stores, how='left', on=['Store'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Average Weekly Sales by Store Type\ntrain_outliers.groupby(['Type'])['Weekly_Sales'].mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Average Weekly Sales for possibly misclassified Stores\ntrain_outliers = train_outliers[train_outliers['Store'].isin([3,5,33,36])]\ntrain_outliers.groupby(['Store','Type'])['Weekly_Sales'].mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Average Weekly Sales by Store Type\nfig, ax = plt.subplots(1, 2, figsize = (15,6))\nax[0].bar(train_outliers['Type'].unique(), train_outliers.groupby(['Type'])['Weekly_Sales'].mean())\nax[0].set_ylabel('Average Weekly Sales')\nax[0].set_xlabel('Store Type')\nax[0].yaxis.grid(True, linewidth=0.3)\n\nax[1].bar([3,5,33,36], train_outliers.groupby(['Store','Type'])['Weekly_Sales'].mean())\nax[1].set_ylabel('Average Weekly Sales')\nax[1].set_xlabel('Store ID')\nax[1].yaxis.grid(True, linewidth=0.3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_outliers = None","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### Takeaways: \n1. Column DATE is non-numeric and is a candidate for pre-processing.\n2. 1285 records with Weekly Sales < 0\n3. Data spans years 2010, 2011 and 2012\n4. As suspected above, four stores [3, 5, 33 & 36] seem to have incorrectly classified as Type A & B. Average Weekly Sales for these stores is in line with the average for Type C. Hence, these would need to be reclassified as Type C."},{"metadata":{},"cell_type":"markdown","source":"## 4. Test Data"},{"metadata":{"trusted":false},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"test.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"test.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"test['Date'].str.slice(start=0, stop=4).value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Validate Holidays\ntest['IsHoliday'][test['Date'].isin(holidays)].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"test['Date'][test['IsHoliday'].isin([1])][~test['Date'].isin(holidays)].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### Takeaways: \n1. Column DATE is non-numeric and is a candidate for pre-processing.\n2. Data spans years 2012 and 2013"},{"metadata":{},"cell_type":"markdown","source":"# Data Pre-Processing"},{"metadata":{},"cell_type":"markdown","source":"## 1. Missing/Incorrect Values"},{"metadata":{},"cell_type":"markdown","source":"### Stores Data | Correct Type for 4 stores"},{"metadata":{"trusted":false},"cell_type":"code","source":"stores[stores['Store'].isin([3,5,33,36])].index","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"stores.iat[2, 1] = stores.iat[4, 1] = stores.iat[32, 1] = stores.iat[35, 1] = 'C'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Features Data | Negative values for MarkDowns:"},{"metadata":{"trusted":false},"cell_type":"code","source":"features['MarkDown1'] = features['MarkDown1'].apply(lambda x: 0 if x < 0 else x)\nfeatures['MarkDown2'] = features['MarkDown2'].apply(lambda x: 0 if x < 0 else x)\nfeatures['MarkDown3'] = features['MarkDown3'].apply(lambda x: 0 if x < 0 else x)\nfeatures['MarkDown4'] = features['MarkDown4'].apply(lambda x: 0 if x < 0 else x)\nfeatures['MarkDown5'] = features['MarkDown5'].apply(lambda x: 0 if x < 0 else x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Features Data | NaN values for multiple columns:"},{"metadata":{},"cell_type":"markdown","source":"#### Columns: CPI and Unemployment\nAs noted above, columns are missing values for 3 months May, Jun & Jul 2013. Values from Apr 2019 would be propogated to records with missing values. "},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\n# For each Store, propogate values of CPI & Unemployment to the rows with NaN values\nfor i in range(len(features)):\n\n    if features.iloc[i]['Date'] == '2013-04-26':\n        CPI_new = features.iloc[i]['CPI']\n        Unemployment_new = features.iloc[i]['Unemployment']\n    \n    if np.isnan(features.iloc[i]['CPI']):\n        features.iat[i, 9] = CPI_new\n        features.iat[i, 10] = Unemployment_new","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Columns: MarkDown1, MarkDown2, MarkDown3, MarkDown4 & MarkDown5\nAs noted above, columns MARKDOWN* are missing values for the whole of 2010 and 2011 (upto Nov). For each store, 2012 values would be copied over to records with missing values. Also, to facilitate the copy, new columns WEEK and YEAR would be derived from DATE."},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\n# For each date, retrive the corresponding week number\nfeatures['Week'] = 0\n\nfor i in range(len(features)):\n    features.iat[i, 12] = datetime.date(int(features.iloc[i]['Date'][0:4]), \n                                        int(features.iloc[i]['Date'][5:7]), \n                                        int(features.iloc[i]['Date'][8:10])).isocalendar()[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"features['Year'] = features['Date'].str.slice(start=0, stop=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#missing data for 2012 & 2013\ntotal = features[features['Year'].isin(['2012','2013'])].isnull().sum().sort_values(ascending=False)\npercent = (features[features['Year'].isin(['2012','2013'])].isnull().sum()/\n           features[features['Year'].isin(['2012','2013'])].isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\n# For 2010 & 2011 records, for each store, copy over MarkDown values from 2012\n\n# Iterate through stores\nfor i in range(1, len(features['Store'].unique())):\n    \n    # For 2010, iterate through weeks 5 thru 52\n    for j in range(5, 52):\n        idx = features.loc[(features.Year == '2010') & (features.Store == i) & (features.Week == j),['Date']].index[0]\n        \n        features.iat[idx, 4] = features.loc[(features.Year == '2012') & (features.Store == i) & (features.Week == j),['MarkDown1']].values[0]\n        features.iat[idx, 5] = features.loc[(features.Year == '2012') & (features.Store == i) & (features.Week == j),['MarkDown2']].values[0]\n        features.iat[idx, 6] = features.loc[(features.Year == '2012') & (features.Store == i) & (features.Week == j),['MarkDown3']].values[0]\n        features.iat[idx, 7] = features.loc[(features.Year == '2012') & (features.Store == i) & (features.Week == j),['MarkDown4']].values[0]\n        features.iat[idx, 8] = features.loc[(features.Year == '2012') & (features.Store == i) & (features.Week == j),['MarkDown5']].values[0]\n        \n    # For 2011, iterate through weeks 1 thru 44\n    for j in range(1, 44):\n        idx = features.loc[(features.Year == '2011') & (features.Store == i) & (features.Week == j),['Date']].index[0]\n        \n        features.iat[idx, 4] = features.loc[(features.Year == '2012') & (features.Store == i) & (features.Week == j),['MarkDown1']].values[0]\n        features.iat[idx, 5] = features.loc[(features.Year == '2012') & (features.Store == i) & (features.Week == j),['MarkDown2']].values[0]\n        features.iat[idx, 6] = features.loc[(features.Year == '2012') & (features.Store == i) & (features.Week == j),['MarkDown3']].values[0]\n        features.iat[idx, 7] = features.loc[(features.Year == '2012') & (features.Store == i) & (features.Week == j),['MarkDown4']].values[0]\n        features.iat[idx, 8] = features.loc[(features.Year == '2012') & (features.Store == i) & (features.Week == j),['MarkDown5']].values[0]        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"features.drop(columns=['Year'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Now fill all the missing MarkDown values with 0\nfeatures.fillna(0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train Data | Negative Values for Weekly Sales"},{"metadata":{"trusted":false},"cell_type":"code","source":"train['Weekly_Sales'] = train['Weekly_Sales'].apply(lambda x: 0 if x < 0 else x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Merge Datasets"},{"metadata":{},"cell_type":"markdown","source":"### Merge the following datasets:\n1. Stores + Features + Train\n2. Stores + Features + Test\n3. Remove duplicate columns from each dataset"},{"metadata":{"trusted":false},"cell_type":"code","source":"train = pd.merge(train, stores, how='left', on=['Store'])\ntrain = pd.merge(train, features, how='left', on=['Store','Date'])\n\ntest = pd.merge(test, stores, how='left', on=['Store'])\ntest = pd.merge(test, features, how='left', on=['Store','Date'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train['Store'][train['IsHoliday_x'] != train['IsHoliday_y']].count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"test['Store'][test['IsHoliday_x'] != test['IsHoliday_y']].count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train.drop(columns=['IsHoliday_y'], axis=1, inplace=True)\ntest.drop(columns=['IsHoliday_y'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train.rename(columns={'IsHoliday_x': 'IsHoliday'}, inplace=True)\ntest.rename(columns={'IsHoliday_x': 'IsHoliday'}, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Feature Engineering"},{"metadata":{},"cell_type":"markdown","source":"### Column #1: IsHoliday\nColumn has boolean values and would ned converted to numeric. "},{"metadata":{"trusted":false},"cell_type":"code","source":"train['IsHoliday'] = train['IsHoliday'].apply(lambda x: 1 if x==True else 0)\ntest['IsHoliday'] = test['IsHoliday'].apply(lambda x: 1 if x==True else 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Column #2: Type\nColumn is categorical and would be converted to numeric via one-hot encoding. "},{"metadata":{"trusted":false},"cell_type":"code","source":"train = pd.get_dummies(train, columns=['Type'])\ntest = pd.get_dummies(test, columns=['Type'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Column #3: Week\nNew numeric column being created to replace YEAR. "},{"metadata":{"trusted":false},"cell_type":"code","source":"train['Week'] = test['Week'] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\n# For each date, retrive the corresponding week number\nfor i in range(len(train)):\n    train.iat[i, 15] = datetime.date(int(train.iloc[i]['Date'][0:4]), \n                                     int(train.iloc[i]['Date'][5:7]), \n                                     int(train.iloc[i]['Date'][8:10])).isocalendar()[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\n# For each date, retrive the corresponding week number\nfor i in range(len(test)):\n    test.iat[i, 14] = datetime.date(int(test.iloc[i]['Date'][0:4]), \n                                    int(test.iloc[i]['Date'][5:7]), \n                                    int(test.iloc[i]['Date'][8:10])).isocalendar()[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Create checkpoint\ntrain.to_csv('train_prescaled.csv', index=False)\ntest.to_csv('test_prescaled.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Restore checkpoint\ntrain = pd.read_csv(\"train_prescaled.csv\")\ntest = pd.read_csv(\"test_prescaled.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Create Submission dataframe\nsubmission = test[['Store', 'Dept', 'Date']].copy()\nsubmission['Id'] = submission['Store'].map(str) + '_' + submission['Dept'].map(str) + '_' + submission['Date'].map(str)\nsubmission.drop(['Store', 'Dept', 'Date'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train['Year'] = train['Date'].str.slice(start=0, stop=4)\ntest['Year'] = test['Date'].str.slice(start=0, stop=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Drop non-numeric columns\ntrain.drop(columns=['Date'], axis=1, inplace=True)\ntest.drop(columns=['Date'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Log Transform Skewed Features"},{"metadata":{"trusted":false},"cell_type":"code","source":"skewed = ['MarkDown1','MarkDown2','MarkDown3','MarkDown4','MarkDown5']\ntrain[skewed] = train[skewed].apply(lambda x: np.log(x + 1))\ntest[skewed] = test[skewed].apply(lambda x: np.log(x + 1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"MarkDown1_min = abs(min(train['MarkDown1'].min(),test['MarkDown1'].min()))\nMarkDown2_min = abs(min(train['MarkDown2'].min(),test['MarkDown2'].min()))\nMarkDown3_min = abs(min(train['MarkDown3'].min(),test['MarkDown3'].min()))\nMarkDown4_min = abs(min(train['MarkDown4'].min(),test['MarkDown4'].min()))\nMarkDown5_min = abs(min(train['MarkDown5'].min(),test['MarkDown5'].min()))"},{"metadata":{},"cell_type":"markdown","source":"train['MarkDown1'] = train['MarkDown1'].apply(lambda x: np.log(x + 1 + MarkDown1_min))\ntrain['MarkDown2'] = train['MarkDown2'].apply(lambda x: np.log(x + 1 + MarkDown2_min))\ntrain['MarkDown3'] = train['MarkDown3'].apply(lambda x: np.log(x + 1 + MarkDown3_min))\ntrain['MarkDown4'] = train['MarkDown4'].apply(lambda x: np.log(x + 1 + MarkDown4_min))\ntrain['MarkDown5'] = train['MarkDown5'].apply(lambda x: np.log(x + 1 + MarkDown5_min))\n\ntest['MarkDown1'] = test['MarkDown1'].apply(lambda x: np.log(x + 1 + MarkDown1_min))\ntest['MarkDown2'] = test['MarkDown2'].apply(lambda x: np.log(x + 1 + MarkDown2_min))\ntest['MarkDown3'] = test['MarkDown3'].apply(lambda x: np.log(x + 1 + MarkDown3_min))\ntest['MarkDown4'] = test['MarkDown4'].apply(lambda x: np.log(x + 1 + MarkDown4_min))\ntest['MarkDown5'] = test['MarkDown5'].apply(lambda x: np.log(x + 1 + MarkDown5_min))"},{"metadata":{"trusted":false},"cell_type":"code","source":"log_constant = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train['Weekly_Sales'] = train['Weekly_Sales'].apply(lambda x: np.log(x + 1 + log_constant))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"distribution(train, ['Weekly_Sales'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Analyze Feature Correlation"},{"metadata":{"trusted":false},"cell_type":"code","source":"colormap = plt.cm.RdBu\ncorr = train.astype(float).corr()\n\nplt.figure(figsize=(14,12))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.set(font_scale=0.9)\nsns.heatmap(round(corr,2),linewidths=0.1,vmax=1.0, square=True, \n            cmap=colormap, linecolor='white', annot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"corr_cutoff = 0.8\ncolumns = np.full((corr.shape[0],), True, dtype=bool)\n\nfor i in range(corr.shape[0]):\n    for j in range(i+1, corr.shape[0]):\n        if corr.iloc[i,j] >= corr_cutoff:\n            if columns[j]:\n                columns[j] = False\n                \nselected_columns = train.columns[columns]\nhighcorr_columns = train.columns.difference(selected_columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"highcorr_columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train.drop(columns=highcorr_columns, axis=1, inplace=True)\ntest.drop(columns=highcorr_columns, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### Takeaway: \n1. MarkDown4 and Type_A are highly correlated to other existing features and have been dropped. "},{"metadata":{},"cell_type":"markdown","source":"### Split Training dataset into Train & Validation"},{"metadata":{"trusted":false},"cell_type":"code","source":"train_X, val_X, train_y, val_y = train_test_split(train.drop('Weekly_Sales', axis = 1), \n                                                  train['Weekly_Sales'], \n                                                  test_size = 0.2, \n                                                  random_state = 0)\n\n# Show the results of the split\nprint(\"Training set has {} samples.\".format(train_X.shape[0]))\nprint(\"Validation set has {} samples.\".format(val_X.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Validate shape\ntrain_X.shape, train_y.shape, val_X.shape, val_y.shape, test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Scale Datasets"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Initialize a scaler, then apply it to the features\nscaler = MinMaxScaler() # default=(0, 1)\n\nnumerical = ['Store', 'Dept', 'IsHoliday', 'Size', 'Temperature', 'Fuel_Price', \n             'CPI', 'Unemployment', 'Week', 'Type_B', 'Type_C',\n             'MarkDown1','MarkDown2','MarkDown3','MarkDown4','MarkDown5']\n\ntrain_scaled = pd.DataFrame(data = train_X)\ntrain_scaled[numerical] = scaler.fit_transform(train_X[numerical])\n\n# Show an example of a record with scaling applied\ndisplay(train_scaled.head(n = 5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"val_scaled = pd.DataFrame(data = val_X)\nval_scaled[numerical] = scaler.transform(val_X[numerical])\n\n# Show an example of a record with scaling applied\ndisplay(val_scaled.head(n = 5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"test_scaled = pd.DataFrame(data = test)\ntest_scaled[numerical] = scaler.transform(test[numerical])\n\n# Show an example of a record with scaling applied\ndisplay(test_scaled.head(n = 5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Free up memory\ntrain = test = features = stores = None","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Create checkpoint\ntrain_scaled.to_csv('train_X_scaled.csv', index=False)\nval_scaled.to_csv('val_X_scaled.csv', index=False)\ntrain_y.to_csv('train_y.csv', index=False, header=['Weekly_Sales'])\nval_y.to_csv('val_y.csv', index=False, header=['Weekly_Sales'])\ntest_scaled.to_csv('test_X_scaled.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Restore checkpoint\ntrain_scaled = pd.read_csv(\"train_X_scaled.csv\")\nval_scaled = pd.read_csv(\"val_X_scaled.csv\")\ntrain_y = pd.read_csv(\"train_y.csv\")\nval_y = pd.read_csv(\"val_y.csv\")\ntest_scaled = pd.read_csv(\"test_X_scaled.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Reduce memory usage\n#train_scaled=reduce_mem_usage(train_scaled)\n#test_scaled=reduce_mem_usage(test_scaled)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_X = train_scaled\nval_X = val_scaled","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"train_scaled.drop(columns=['Temperature', 'Fuel_Price'], axis=1, inplace=True)\ntest_scaled.drop(columns=['Temperature', 'Fuel_Price'], axis=1, inplace=True)"},{"metadata":{},"cell_type":"markdown","source":"train_y = train_scaled['Weekly_Sales']\ntrain_X = train_scaled.drop('Weekly_Sales', axis = 1)\n\nval_y = val_scaled['Weekly_Sales']\nval_X = val_scaled.drop('Weekly_Sales', axis = 1)"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Free up memory\ntrain_scaled = val_scaled = None","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Convert Dataframe to Series\ntrain_y = train_y.iloc[:,0]\nval_y = val_y.iloc[:,0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Modelling"},{"metadata":{},"cell_type":"markdown","source":"### Select and evaluate candidate models"},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\n# Initialize base models\nmodel_A = LinearRegression()\nmodel_B = ElasticNet(random_state=1)\nmodel_C = RandomForestRegressor(random_state=1)\nmodel_D = GradientBoostingRegressor(random_state=1)\nmodel_E = xgb.XGBRegressor()\nmodel_F = LGBMRegressor(random_state=1)\n\nsamples = len(train_y) # 100% of training set\n\n# Collect results on the learners\nresults = {}\nfor model in [model_A, model_B, model_C, model_D, model_E, model_F]:\n    model_name = model.__class__.__name__\n    results[model_name] = {}\n    for i, samples in enumerate([samples]):\n        results[model_name][i] = eval_train_predict(model, samples, train_X, train_y, val_X, val_y, 'log', log_constant)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Evaluate Metrics\neval_visualize(results)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"results","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### Takeaway: With respect to WMAE, Random Forest and Light GBM have turned out to be the top performing base models and would be further evaluated."},{"metadata":{},"cell_type":"markdown","source":"### Evaluate Random Forest (Ensemble)"},{"metadata":{},"cell_type":"markdown","source":"#### Default Model"},{"metadata":{"trusted":false},"cell_type":"code","source":"model_rf_base = RandomForestRegressor(random_state=42, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"model_rf_base, pred_y_rf_val = train_predict(model, train_X, train_y, val_X, val_y, 'log', log_constant, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"pred_y_rf_test = model_rf_base.predict(test_scaled)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"param_grid = { \n    'n_estimators': [10, 50, 100, 150],\n    'max_features': [None, 'auto'],\n    'bootstrap': [True, False],\n    'max_depth':[None],\n    'random_state': [42], \n    'verbose': [1]\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#%%time\n#CV = GridSearchCV(estimator=model_rf_base, param_grid=param_grid, cv=2, verbose=1)\n#CV.fit(train_X, train_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#CV.best_params_ # latest","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Using best params from GridSearch\n#model.set_params(**CV.best_params_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Tuned Model"},{"metadata":{"trusted":false},"cell_type":"code","source":"model = RandomForestRegressor(random_state=42, \n                              n_estimators=150, \n                              bootstrap=True, \n                              max_features=None, \n                              max_depth=None, \n                              min_samples_leaf=1,\n                              min_samples_split=3,\n                              verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"model, pred_y_rf_val = train_predict(model, train_X, train_y, val_X, val_y, 'log', log_constant, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"pred_y_rf_test = model.predict(test_scaled)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Evaluate Light GBM (Boosting)"},{"metadata":{},"cell_type":"markdown","source":"#### Default Model"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Default model\nmodel = LGBMRegressor()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"model, pred_y_lgbm_val = train_predict(model, train_X, train_y, val_X, val_y, 'log', log_constant, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"param_grid = {\n    'boosting_type': ['gbdt'], \n    'objective': ['regression'],\n    'random_state': [42],\n    'min_data_in_leaf':[3],\n    'min_depth':[2],\n    'learning_rate': [0.3],\n    #'n_estimators': [1000, 3000],\n    'n_estimators': [3000],\n    #'num_leaves': [60, 70, 80],\n    'max_bin': [150,200,255,300]\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\n#CV_lgbm = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, verbose=1, scoring='neg_mean_absolute_error')\n#CV = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, verbose=1)\n#CV.fit(train_X, train_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#print(\"Best parameter (CV score=%0.3f):\" % CV.best_score_)\n#print(CV.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Using best params from GridSearch\n#model.set_params(**CV.best_params_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Tuned Model"},{"metadata":{},"cell_type":"markdown","source":"model = LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n                      importance_type='split', learning_rate=0.3, max_depth=-1,\n                      min_child_samples=5, min_child_weight=0.001, min_data_in_leaf=2,\n                      min_depth=3, min_split_gain=0.0, n_estimators=3000, n_jobs=-1,\n                      num_leaves=80, objective='regression', random_state=42,\n                      reg_alpha=0.1, reg_lambda=2, silent=True, subsample=1.0,\n                      subsample_for_bin=200000, subsample_freq=0,\n                      verbose=1)\n#Weighted MAE : 1275.72"},{"metadata":{},"cell_type":"markdown","source":"model = LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n                      importance_type='split', learning_rate=0.3, max_depth=-1,\n                      min_child_samples=5, min_child_weight=0.001, min_data_in_leaf=2,\n                      min_depth=3, min_split_gain=0.0, n_estimators=3000, n_jobs=-1,\n                      num_leaves=80, objective='regression', random_state=42,\n                      reg_alpha=0.1, reg_lambda=2, silent=True, subsample=1.0,\n                      subsample_for_bin=200000, subsample_freq=0,\n                      verbose=1)\n#Weighted MAE : 1324.72"},{"metadata":{"trusted":false},"cell_type":"code","source":"model = LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n       importance_type='split', learning_rate=0.3, max_bin=150,\n       max_depth=-1, min_child_samples=5, min_child_weight=0.001,\n       min_data_in_leaf=3, min_depth=2, min_split_gain=0.0,\n       n_estimators=3000, n_jobs=-1, num_leaves=80, objective='regression',\n       random_state=42, reg_alpha=0.1, reg_lambda=2, silent=True,\n       subsample=1.0, subsample_for_bin=200000, subsample_freq=0,\n       verbose=1)\n#Weighted MAE : 1238.72","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"model, pred_y_lgbm_val = train_predict(model, train_X, train_y, val_X, val_y, 'log', log_constant, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"pred_y_lgbm_test = model.predict(test_scaled)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Stacking"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Blend the results of the two regressors and save the prediction to a CSV file.\npred_y_val = ((np.exp(pred_y_rf_val) - 1 - log_constant) * 0.7) + ((np.exp(pred_y_lgbm_val) - 1 - log_constant) * 0.3)\npred_y = ((np.exp(pred_y_rf_test) - 1 - log_constant) * 0.7) + ((np.exp(pred_y_lgbm_test) - 1 - log_constant) * 0.3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"val_y = np.exp(val_y) - 1 - log_constant","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# make predictions\nprint(\"Weighted Mean Absolute Error: \", weighted_mean_absolute_error(pred_y_val, val_y, compute_weights(val_X['IsHoliday'])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"submission['Weekly_Sales'] = pred_y","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"submission[['Id','Weekly_Sales']].to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"val_X.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"tmp = pd.DataFrame(scaler.inverse_transform(val_X), columns = val_X.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"tmp = tmp.assign(weekly_sales=val_y.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"tmp = pd.concat([tmp, pd.DataFrame(pred_y_val)], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"tmp.head(5000).to_csv('tmp5000.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"tmp.to_csv('tmp5000.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"conda_pytorch_p36","language":"python","name":"conda_pytorch_p36"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"nbformat":4,"nbformat_minor":1}