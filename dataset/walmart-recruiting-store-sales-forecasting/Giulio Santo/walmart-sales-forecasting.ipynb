{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"#Data\nimport pandas as pd\nimport numpy as np\n\n#Sklearn Libraries\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import make_scorer\nfrom sklearn.linear_model import Lasso\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.feature_selection import SelectKBest, f_regression\nfrom sklearn.model_selection import KFold, GridSearchCV, RandomizedSearchCV, train_test_split\n\n\n#LIghtGBM\nimport lightgbm as lgb\n\n#Category Encoders\nfrom category_encoders.leave_one_out import LeaveOneOutEncoder\nfrom category_encoders.james_stein import JamesSteinEncoder\nfrom category_encoders.target_encoder import TargetEncoder\nfrom category_encoders.m_estimate import MEstimateEncoder\nfrom category_encoders.cat_boost import CatBoostEncoder\nfrom category_encoders.ordinal import OrdinalEncoder\nfrom category_encoders.woe import WOEEncoder\n\n#SHAP\nimport shap\n\n#Plots\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\nsns.set_style('darkgrid')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <a class=\"anchor\">Table of Content</a>\n\n[1. Import data](#import)\n   \n[2. Explanatory Data Analysis](#eda)\n   \n[3. Searching for Outliers](#outliers)\n\n[4. Feature Engineering](#fe)\n\n[5. Machine Learning Modeling](#ML)\n  - [5.1. Defining Metric](#me_metric)\n  - [5.2. Checking Correlations](#me_corr)\n  - [5.3. Choosing Best Encoder](#me_encod)\n  - [5.4. Choosing Best Model](#me_models)\n  - [5.5. Shapley Values](#me_shap)\n  \n[6. Submission](#sub)"},{"metadata":{},"cell_type":"markdown","source":"### <a class=\"anchor\" id=\"import\">1. Import Data</a>"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"features_df = pd.read_csv(\"../input/walmart-recruiting-store-sales-forecasting/features.csv.zip\", compression='zip')\nstores_df = pd.read_csv(\"../input/walmart-recruiting-store-sales-forecasting/stores.csv\")\ntest_df = pd.read_csv(\"../input/walmart-recruiting-store-sales-forecasting/test.csv.zip\", compression='zip')\ntrain_df = pd.read_csv(\"../input/walmart-recruiting-store-sales-forecasting/train.csv.zip\", compression='zip')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Let's creat a dataframe that will be used for the modeling task!**"},{"metadata":{"trusted":true},"cell_type":"code","source":"modeling_df = pd.merge(train_df, stores_df, on='Store', how='inner')\nmodeling_df = pd.merge(modeling_df, \n                       features_df,\n                       on=['Store', 'Date','IsHoliday'],\n                       how='inner')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"modeling_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <a class=\"anchor\" id=\"eda\">2. Explanatory Data Analysis</a>"},{"metadata":{},"cell_type":"markdown","source":"**Let's firt take a quick look at how the data is distributed!**"},{"metadata":{"trusted":true},"cell_type":"code","source":"modeling_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"modeling_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(100*(modeling_df.isnull().sum()/modeling_df.shape[0])).rename(columns={0:'Percentual of Missing Values (%)'})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**It is interesting to notice that all Markdown features contain more than 64% of missing values.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"modeling_df.groupby(['Store','Dept'])['Date'].count().reset_index().set_index(['Store','Dept']).describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We can see from the above dataframe that not all Departments contain 143 weeks. In fact, there is at least one department with only 1 week available.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(30,10));\n\nplt.subplot(2,1,1);\nweekly_plot = modeling_df.sort_values(by='Date',ascending=True).groupby('Date')['Weekly_Sales'].sum().reset_index();\nweekly_plot['Date'] = pd.to_datetime(weekly_plot['Date']);\nsns.lineplot(x=\"Date\", y=\"Weekly_Sales\", markers=True, dashes=False, data=weekly_plot, color='darkorange');\nplt.title(\"Total Weekly Sales\", fontsize=20, fontweight=\"bold\");\nplt.xticks(rotation=20, fontweight='bold', fontsize=16);\nplt.yticks(fontweight='bold', fontsize=16);\nplt.xlabel(\"Date (Week)\", fontsize=18, fontweight='bold');\nplt.ylabel(\"Total Number of Sales\", fontsize=18, fontweight='bold');\n\nplt.subplot(2,1,2);\nweekly_plot = modeling_df.sort_values(by='Date',ascending=True).groupby('Date')['Weekly_Sales'].mean().reset_index();\nweekly_plot['Date'] = pd.to_datetime(weekly_plot['Date']);\nsns.lineplot(x=\"Date\", y=\"Weekly_Sales\", markers=True, dashes=False, data=weekly_plot, color='darkorange');\nplt.title(\"Average Weekly Sales\", fontsize=20, fontweight=\"bold\");\nplt.xticks(rotation=20, fontweight='bold', fontsize=16);\nplt.yticks(fontweight='bold', fontsize=16);\nplt.xlabel(\"Date (Week)\", fontsize=18, fontweight='bold');\nplt.ylabel(\"Average Number of Sales\", fontsize=18, fontweight='bold');\n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(30,10));\nweekly_store_plot = modeling_df.sort_values(by='Date',ascending=True).groupby(['Date','Store'])['Weekly_Sales'].mean().reset_index();\nweekly_store_plot['Date'] = pd.to_datetime(weekly_store_plot['Date']);\n\nsns.lineplot(x=\"Date\", y=\"Weekly_Sales\",hue='Store', markers=True, dashes=False, data=weekly_store_plot);\nplt.title(\"Average Sales By Week for each Store\", fontsize=20, fontweight=\"bold\");\nplt.xticks(rotation=20, fontweight='bold', fontsize=16);\nplt.yticks(fontweight='bold', fontsize=16);\nplt.legend(fontsize=14);\nplt.xlabel(\"Date (Week)\", fontsize=18, fontweight='bold');\nplt.ylabel(\"Average Number of Sales\", fontsize=18, fontweight='bold');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Some conclusions so far:\n\n- We can clearly notice the seasonality peaks. In the original dataset, the holidays are marked on a single day, i.e., in 2010, for example, Christmas was on dezember 31. However, the figures above show that the effects on sales caused by the hollidays can occur days before and even last for several days. For this reason, I will be adding for each holiday 4 additional days: 2 before and 2 after the holiday's original date. (This is done in the function create_holidays).\n\n- It is very clear that there is a huge difference in sales for each store. We have stores with an average sales of 10000 and stores with an average sale of around 30000."},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_holidays(x):\n    \"\"\"\n    This function defines each holiday based on its corresponding dates.\n    Notice that 4 additional days are included for each holiday: 2 days before\n    and 2 days after the original date.\n    \n    Arguments:\n        x: an input date\n    \n    Output:\n        the corresponding date\n    \"\"\"\n    if x in ['2010-02-10','2010-02-11','2010-02-12','2010-02-13','2010-02-14',\n             '2011-02-09','2011-02-10','2011-02-11','2011-02-12','2011-02-13',\n             '2012-02-08','2012-02-09','2012-02-10','2012-02-11','2012-02-12',\n             '2013-02-06','2013-02-07','2013-02-08','2013-02-09','2013-02-10']:\n        \n        return 'super_bowl'\n    \n    elif x in ['2010-09-08','2010-09-09','2010-09-10','2010-09-11','2010-09-12',\n               '2011-09-09','2011-09-10','2011-09-11','2011-09-12','2011-09-13',\n               '2012-09-05','2012-09-06','2012-09-07','2012-09-08','2012-09-09',\n               '2013-09-04','2013-09-05','2013-09-06','2013-09-07','2013-09-08']:\n        \n        return 'labor_day'\n    \n    elif x in ['2010-11-24','2010-11-25','2010-11-26','2010-11-27','2010-11-28',\n               '2011-11-23','2011-11-24','2011-11-25','2011-11-26','2011-11-27',\n               '2012-11-21','2012-11-22','2012-11-23','2012-11-24','2012-11-25',\n               '2013-11-27','2013-11-28','2013-11-29','2013-11-30','2013-12-01']:\n        \n        return 'thanksgiving'\n    \n    elif x in ['2010-12-29','2010-12-30','2010-12-31','2010-12-31','2010-12-31',\n               '2011-12-30','2011-12-30','2011-12-30','2011-12-30','2011-12-30',\n               '2012-12-28','2012-12-28','2012-12-28','2012-12-28','2012-12-28',\n               '2013-12-27','2013-12-27','2013-12-27','2013-12-27','2013-12-27']:\n        \n        return 'christmas'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def update_isholiday(x):\n    \"\"\"\n    This function is used for update the IsHoliday field\n    from the original dataframe.\n    \n    Arguments:\n        x: a holiday produced by the function create_holidays\n    \n    Output:\n        1 if a holiday, 0 otherwise.\n    \"\"\"\n    if x is not None:\n        return 1\n    else:\n        return 0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Updating IsHoliday and Creating a Holiday feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"modeling_df['holiday'] = modeling_df['Date'].apply(create_holidays)\nmodeling_df['IsHoliday'] = modeling_df['holiday'].apply(update_isholiday)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"modeling_df[modeling_df['IsHoliday']==1].head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Let's see the impact of each holiday in each store and in overall sales"},{"metadata":{"trusted":true},"cell_type":"code","source":"store_avg_sales_holidays = modeling_df[modeling_df['IsHoliday'] == 1][['Date','Store','Dept','Weekly_Sales']]\nstore_avg_sales_not_holidays = modeling_df[modeling_df['IsHoliday'] == 0][['Date','Store','Dept','Weekly_Sales']]\n\nstore_avg_sales_holidays = store_avg_sales_holidays.groupby('Store').mean().reset_index()\nstore_avg_sales_not_holidays = store_avg_sales_not_holidays.groupby('Store').mean().reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,10));\nplt.subplot(1,2,1);\nsns.barplot(x=\"Weekly_Sales\",\n            y=\"Store\",\n            data=store_avg_sales_holidays,\n            color='darkmagenta',\n            order=store_avg_sales_holidays.sort_values(by='Weekly_Sales',ascending=False)['Store'],\n            orient='h');\nplt.title(\"Average Sales Per Store on Holidays\", fontsize=16, fontweight=\"bold\");\nplt.ylabel(\"Store\", fontsize=18, fontweight='bold');\nplt.xlabel(\"Average Sales\", fontsize=18, fontweight='bold');\nplt.legend(fontsize=16);\n\nplt.subplot(1,2,2);\nsns.barplot(x=\"Weekly_Sales\",\n            y=\"Store\",\n            data=store_avg_sales_not_holidays,\n            color='darkmagenta',\n            order=store_avg_sales_not_holidays.sort_values(by='Weekly_Sales',ascending=False)['Store'],\n            orient='h');\nplt.title(\"Average Sales Per Store on Regular Days\", fontsize=16, fontweight=\"bold\");\nplt.ylabel(\"Store\", fontsize=18, fontweight='bold');\nplt.xlabel(\"Average Sales\", fontsize=18, fontweight='bold');\nplt.legend(fontsize=16);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"average_sales_holidays = modeling_df.groupby(['Store','holiday'])['Weekly_Sales'].mean().reset_index()\n\nplt.figure(figsize=(10,30))\nsns.barplot(x=\"Weekly_Sales\",\n            y=\"Store\",\n            data=average_sales_holidays,\n            color='sandybrown',\n            hue='holiday',\n            order=average_sales_holidays.groupby('Store').mean().reset_index().sort_values(by='Weekly_Sales', ascending=False)['Store'],\n            orient='h');\nplt.title(\"Average Sales Per Store for Each Holiday\", fontsize=20, fontweight=\"bold\");\nplt.ylabel(\"Store\", fontsize=18, fontweight='bold');\nplt.xlabel(\"Average Sales\", fontsize=18, fontweight='bold');\nplt.legend(fontsize=16);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### We can see that store 20 is the one with the highest average of sales, while store 33 is the one with the lowest one. Besides, clearly each holiday have a different impact in sales: while thanksgiving appears to be the holiday with highest impact, on averagem christmas appears to be the one with the lowest. However, this is not true for every store: in store 7, for example, christmas appears as an important sales driver. For that reason, one can expect that the holidays feature is an important one to predict sales for each store. "},{"metadata":{},"cell_type":"markdown","source":"### <a class=\"anchor\" id=\"outliers\">3. Searching for Outliers</a>"},{"metadata":{},"cell_type":"markdown","source":"#### In this section, I will be searching for outliers in the following features: size, temperature, fuel_price, cpi and unemployment. Notice that the feature Weekly_Sales can contain outliers itself. However, since this is a time-series variable, highly influenced by sasonality, finding outliers it is not a trivial task and could result in model bias. Therefore, I will be only looking at the features variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"outliers_df = modeling_df[modeling_df['IsHoliday']==0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,5));\n\nplt.subplot(1,5,1);\nsns.boxplot(x=outliers_df[\"Size\"].values, orient='v');\nplt.title(\"Size\");\nplt.subplot(1,5,2);\nsns.boxplot(x=outliers_df[\"Temperature\"].values, orient='v');\nplt.title(\"Temperature\");\nplt.subplot(1,5,3);\nsns.boxplot(x=outliers_df[\"Fuel_Price\"].values, orient='v');\nplt.title(\"Fuel Price\");\nplt.subplot(1,5,4);\nsns.boxplot(x=outliers_df[\"CPI\"].values, orient='v');\nplt.title(\"CPI\");\nplt.subplot(1,5,5);\nsns.boxplot(x=outliers_df[\"Unemployment\"].values, orient='v');\nplt.title(\"Unemployment\");\n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** From the boxplot above, there is no apparent reason for removing any data.**"},{"metadata":{},"cell_type":"markdown","source":"### <a class=\"anchor\" id=\"fe\">4. Feature Engineering</a>"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"In this section, I will be engineering features related to date. Instead of using the date itself and applying some kind of encoder to it, I will be making a cyclical transformation and select the best approach through feature selection.\n\nA cyclical transformation of a certain date corresponds to its convertion into a sinusoidal (or cosinusoidal) function. The advantage of that is the ability of adding a new information to the date feature: the knowledge that monday is closer to sunday than it is to wednesday, for example;\n\nIf we want to conver the hours of a day into a cyclical variable, for example, we would need to do the following:\n\n$cyclical\\_hour = sin(\\frac{2 \\cdot \\pi \\cdot hour}{24})$\n\nSome interesting articles about this subject can be found here:\n\n[1] https://www.kaggle.com/avanwyk/encoding-cyclical-features-for-deep-learning\n\n[2] https://datascience.stackexchange.com/questions/5990/what-is-a-good-way-to-transform-cyclic-ordinal-attributes"},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_cyclical_dates():\n    \"\"\"\n    This function creates a dataframe of cyclical dates\n    \n    Output:\n        a dataframe with dates and its cyclical transformations.\n    \"\"\"\n    df = pd.DataFrame({\"Date\": pd.date_range('2000-01-01', '2050-12-31')})\n\n    df['day_of_year'] = df['Date'].dt.dayofyear\n    df['day_of_month'] = df['Date'].dt.day\n    df['day_of_week'] = df['Date'].dt.dayofweek\n    df['week_of_year'] = df['Date'].dt.weekofyear\n    df['month_of_year'] = df['Date'].dt.month\n    df['year'] = df['Date'].dt.year\n\n    df = pd.merge(df,\n                  df.groupby('year')['day_of_year'].count(). \\\n                      reset_index().rename(columns={'day_of_year':'num_days_year'}),\n                  on='year',\n                  how='inner')\n    df = pd.merge(df,\n                  df.groupby(['year','month_of_year'])['day_of_month'].count(). \\\n                      reset_index().rename(columns={'day_of_month':'num_days_month'}),\n                  on=['year','month_of_year'],\n                  how='inner')\n\n    df['sine_day_of_month'] = np.sin((2*np.pi*df['day_of_month'])/df['num_days_month'])\n    df['sine_day_of_year'] = np.sin((2*np.pi*df['day_of_year'])/df['num_days_year'])\n    df['sine_day_of_week'] = np.sin((2*np.pi*df['day_of_week'])/7)\n    df['sine_week_of_year'] = np.sin((2*np.pi*df['week_of_year'])/52)\n    df['sine_month_of_year'] = np.sin((2*np.pi*df['month_of_year'])/12)\n    \n    df['Date'] = df['Date'].apply(lambda x: x.strftime(\"%Y-%m-%d\"))\n    \n    return df[['Date','day_of_year',\n               'day_of_month','day_of_week',\n               'week_of_year','month_of_year',\n               'year','sine_day_of_month',\n               'sine_day_of_year','sine_day_of_week',\n               'sine_week_of_year','sine_month_of_year']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cyclical_df = create_cyclical_dates()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cyclical_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,5))\n\nplt.subplot(1,5,1);\nplt.plot(cyclical_df['sine_day_of_month']);\nplt.xlim([0, 100]);\nplt.title(\"Day of Month\");\n\nplt.subplot(1,5,2);\nplt.plot(cyclical_df['sine_day_of_year']);\nplt.xlim([0, 1000]);\nplt.title(\"Day of Year\");\n\nplt.subplot(1,5,3);\nplt.plot(cyclical_df['sine_day_of_week']);\nplt.xlim([0, 100]);\nplt.title(\"Day of Week\");\n\nplt.subplot(1,5,4);\nplt.plot(cyclical_df['sine_week_of_year']);\nplt.xlim([0, 1000]);\nplt.title(\"Week of Year\");\n\nplt.subplot(1,5,5);\nplt.plot(cyclical_df['sine_month_of_year']);\nplt.xlim([0, 1000]);\nplt.title(\"Month of Year\");\n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"modeling_df = pd.merge(modeling_df,\n                       cyclical_df,\n                       on='Date',\n                       how='inner')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"modeling_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <a class=\"anchor\" id=\"fe\">5. Machine Learning Modeling</a>"},{"metadata":{},"cell_type":"markdown","source":"#### <a class=\"anchor\" id=\"me_metric\">5.1. Defining Metric</a>\n\nThe first thing that I will be doing is defining the validation metric as proposed by Kaggle. The metric used for evaluation in the competition is the Weighted Mean Absolute Error (WMAE), as follows:\n\n$$WMAE = \\frac{1}{\\sum w_{i}} \\sum_{i=1}^{n} w_{i}|y_{i} - \\hat{y_{i}}|$$\n\nwhere:\n\n- $n$ is the number of rows\n- $\\hat{y_{i}}$ is the predicted sales\n- $y_{i}$ is the actual sales\n- $w_{i}$ are weights. $w$ = 5 if the week is a holiday week, 1 otherwise\n\n**Notice: I have included more days in each holiday (4 days for each), such that the metric woudn't match the Kaggle's validation. Therefore, I will be considering only the default dates in the validation function.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def holiday_weights(x):\n    \"\"\"\n    This function returns the holiday weights according\n    to the metric definitions.\n    \n    Arguments:\n        x: a date input\n        \n    Output:\n        Either 5 or 1, depending on the date.\n    \"\"\"\n    if x in ['2010-02-12','2011-02-11','2012-02-10','2013-02-08',\n             '2010-09-10','2011-09-11','2012-09-07','2013-09-06',\n             '2010-11-26','2011-11-25','2012-11-23','2013-11-29',\n             '2010-12-31','2011-12-30','2012-12-28','2013-12-27']:\n\n        return 5\n    else:\n        return 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def weighted_mean_absolute_error(y_true, y_pred, W):\n    \"\"\"\n    This function defines the Weighted Mean Absolute Error\n    \n    Arguments:\n        y_true: the true output value array\n        y_pred: the prediction array\n        W: the correspondig weights\n    \"\"\"\n    return np.sum(W*np.abs(np.subtract(np.squeeze(y_true.values),\n                                       np.squeeze(y_pred))))/np.sum(W)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Let's first verify the features most correlated with the output (Weekly Sales)"},{"metadata":{},"cell_type":"markdown","source":"#### <a class=\"anchor\" id=\"me_corr\">5.2. Checking Correlations</a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_diagonal_correlation_matrix(X,y):\n    \"\"\"\n    This functions plots a diagonal correlation matrix.\n    Ref: https://seaborn.pydata.org/examples/many_pairwise_correlations.html\n    \n    Arguments:\n        df: the input dataframe\n    \"\"\"\n    \n    #Compute Correlations\n    corr = pd.concat([X,y],axis=1).corr()\n\n    # Generate a mask for the upper triangle\n    mask = np.triu(np.ones_like(corr, dtype=np.bool))\n\n    # Set up the matplotlib figure\n    f, ax = plt.subplots(figsize=(20, 20))\n\n    # Generate a custom diverging colormap\n    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n    # Draw the heatmap with the mask and correct aspect ratio\n    sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n                square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True)\n    \n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"modeling_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = modeling_df.drop(columns=['Weekly_Sales',\n                              'MarkDown1',\n                              'MarkDown2',\n                              'MarkDown3',\n                              'MarkDown4',\n                              'MarkDown5'])\ny = modeling_df[['Weekly_Sales']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder = JamesSteinEncoder()\nX_corr = encoder.fit_transform(X,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_diagonal_correlation_matrix(X_corr,y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### From the correlation plot, we can see that the variables that are most correlated with the sales in each store are the following:\n\n- Store and Department\n- Date\n- Store type\n- Store Size\n- Holiday"},{"metadata":{},"cell_type":"markdown","source":"#### <a class=\"anchor\" id=\"me_encod\">5.3. Choosing Best Encoder</a>"},{"metadata":{},"cell_type":"markdown","source":"#### Let's try different types of encoders to see which one performs best. For this task, I will be building a simple Random Forest Regressor, with default tunning. The idea here is just to see the performance of each encoder."},{"metadata":{"trusted":true},"cell_type":"code","source":"def compare_encoders(encoder_list, X_train, X_test, y_test):\n    \"\"\"\n    This function compare different type of encoders available\n    in the library category_encoder.\n    \n    Arguments:\n        encoder_list: a list of encoders to be comapred\n        X_train: training features\n        y_train: training target\n    \n    Output:\n        y_test: validation target\n    \"\"\"\n    for encoder in encoder_list:\n        \n        lr_pipe = Pipeline([('encoder',encoder),\n                            ('scaler',StandardScaler()),\n                            ('clf',RandomForestRegressor(n_jobs=-1))])\n        \n        #Create Weights for validation metric\n        W = X_test['Date'].apply(holiday_weights).values\n        \n        #Remove Date and year and fit model\n        lr_pipe.fit(X_train, np.squeeze(y_train))\n        lr_pred = lr_pipe.predict(X_test)\n        \n        #Print encoder and validation metric\n        score = weighted_mean_absolute_error(y_test, lr_pred, W=W)\n        \n        print(\"{} wmae-score: {}\".format(str(encoder).split('(')[0],score))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import make_scorer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder_list = [OrdinalEncoder(),\n                TargetEncoder(),\n                MEstimateEncoder(),\n                JamesSteinEncoder(),\n                LeaveOneOutEncoder(),\n                CatBoostEncoder()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X['holiday'] = X['holiday'].fillna('not_holiday')\nX_train, X_test, y_train, y_test = train_test_split(X, y,\n                                                    test_size=0.33)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"compare_encoders(encoder_list=encoder_list,\n                 X_train=X_train,\n                 X_test=X_test,\n                 y_test=y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### <a class=\"anchor\" id=\"me_models\">5.4. Choosing Best Model</a>"},{"metadata":{"trusted":true},"cell_type":"code","source":" def plot_feature_importances(model, X , num = 20):\n    \"\"\"\n    This function plots the Feature Importances.\n    \n    Arguments:\n        model: the trained tree-based model\n        X: the training features\n        num: the number of features to be displayed\n    \"\"\"\n    feature_imp = pd.DataFrame({'Value':model.feature_importances_,\n                                'Feature':X.columns})\n    \n    plt.figure(figsize=(15, 10))\n    sns.barplot(x=\"Value\",\n                y=\"Feature\",\n                data=feature_imp.sort_values(by=\"Value\",ascending=False)[0:num],\n                color='darkred')\n    plt.title('Features Importance', fontsize=18, fontweight='bold')\n    plt.yticks(fontsize=14, fontweight='bold')\n    plt.tight_layout()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Random Forest Regressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"def random_forest_pipeline(encoder,\n                           n_estimators,\n                           max_depth,\n                           min_samples_split,\n                           min_samples_leaf,\n                           use_selectkbest = False,\n                           k = None):\n    \"\"\"\n    Creates a Random Forest regressor pipeline with the following steps:\n    \n    1) Encoder\n    2) Scaler\n    3) Random Forest Regressor\n    \"\"\"\n    \n    if use_selectkbest:\n        rf_pipe = Pipeline([('encoder',encoder),\n                            ('scaler',StandardScaler()),\n                            ('kbest',SelectKBest(f_regression)),\n                            ('clf',RandomForestRegressor(n_jobs=-1))])\n        param_grid = {\n        'clf__n_estimators':n_estimators,\n        'clf__max_depth':max_depth,\n        'clf__min_samples_split':min_samples_split,\n        'clf__min_samples_leaf':min_samples_leaf,\n        'kbest__k':k\n        }\n        \n    else:\n        rf_pipe = Pipeline([('encoder',encoder),\n                            ('scaler',StandardScaler()),\n                            ('clf',RandomForestRegressor(n_jobs=-1))])      \n        param_grid = {\n        'clf__n_estimators':n_estimators,\n        'clf__max_depth':max_depth,\n        'clf__min_samples_split':min_samples_split,\n        'clf__min_samples_leaf':min_samples_leaf\n        }\n        \n    return rf_pipe, param_grid","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_encoder = MEstimateEncoder()\nrf_n_estimators = [10]\nrf_max_depth = [None, 2, 4, 8, 10]\nrf_min_samples_split = [2, 4, 8, 10, 20]\nrf_min_samples_leaf = [1, 2, 4, 8, 10]\n\nrf_pipe, rf_grid = random_forest_pipeline(encoder = rf_encoder,\n                                          n_estimators = rf_n_estimators,\n                                          max_depth = rf_max_depth,\n                                          min_samples_split = rf_min_samples_split,\n                                          min_samples_leaf = rf_min_samples_leaf)\n\nrf_gd = RandomizedSearchCV(estimator=rf_pipe,\n                           param_distributions=rf_grid,\n                           scoring='neg_mean_absolute_error',\n                           cv=3,\n                           n_iter=10,\n                           n_jobs=-1,\n                           verbose=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_gd.fit(X_train, y_train);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_gd.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_pred = rf_gd.predict(X_test)\nscore = weighted_mean_absolute_error(y_test,\n                                     rf_pred,\n                                     W = X_test['Date'].apply(holiday_weights).values)\nprint(\"wmae-score: {}\".format(score))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_feature_importances(model=rf_gd.best_estimator_['clf'],\n                         X=X_train,\n                         num=20);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**I will be selecting just the top features and reapplying a grid search for parameters**"},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_encoder = MEstimateEncoder()\nrf_n_estimators = [10]\nrf_max_depth = [None, 2, 4, 8, 10]\nrf_min_samples_split = [2, 4, 8, 10, 20]\nrf_min_samples_leaf = [1, 2, 4, 8, 10]\nrf_k = list(range(1,X_train.shape[1]))\n\nrf_pipe_2, rf_grid_2 = random_forest_pipeline(encoder = rf_encoder,\n                                          n_estimators = rf_n_estimators,\n                                          max_depth = rf_max_depth,\n                                          min_samples_split = rf_min_samples_split,\n                                          min_samples_leaf = rf_min_samples_leaf,\n                                          use_selectkbest=True,\n                                          k=rf_k)\n\nrf_gd_2 = RandomizedSearchCV(estimator=rf_pipe_2,\n                           param_distributions=rf_grid_2,\n                           scoring='neg_mean_absolute_error',\n                           cv=3,\n                           n_iter=10,\n                           n_jobs=-1,\n                           verbose=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_gd_2.fit(X_train, np.squeeze(y_train.values));\nrf_pred_2 = rf_gd_2.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_gd_2.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = weighted_mean_absolute_error(y_test,\n                                     rf_pred_2,\n                                     W = X_test['Date'].apply(holiday_weights).values)\n\nprint(\"wmae-score: {}\".format(score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Polynomial Regression - Lasso"},{"metadata":{"trusted":true},"cell_type":"code","source":"def non_linear_multiple_regressor_pipeline(encoder,\n                                           max_poly_degree,\n                                           alpha,\n                                           use_selectkbest = False,\n                                           k = None):                                           \n    \"\"\"\n    Creates a non linear multiple regressor pipeline with the following steps:\n    \n    1) Encoder\n    2) Polynomial Transformer\n    3) Scaler\n    3) Lasso Regressor\n    \n    Arguments:\n        encoder: the encoder object\n        max_poly_degree: the maximum degree of the polynomial\n        alpha: penalty value for l1 regularization\n    \n    Output:\n        pipe: the training pipeline\n        param_grid: the param grid for training the pipeline\n    \"\"\"\n    \n    if use_selectkbest:\n        pipe = Pipeline([(\"encoder\",encoder),\n                         (\"polynomial\",PolynomialFeatures()),\n                         ('scaler',StandardScaler()),\n                         ('kbest',SelectKBest(f_regression)),\n                         ('clf',Lasso())])\n        param_grid = {\n        'polynomial__degree':list(range(1,max_poly_degree)),\n        'clf__alpha':list(alpha),\n        'clf__fit_intercept':[True],\n        'clf__normalize':[False],\n        'kbest__k':list(k)\n        }\n        \n    else:\n        pipe = Pipeline([(\"encoder\",encoder),\n                         (\"polynomial\",PolynomialFeatures()),\n                         ('scaler',StandardScaler()),\n                         ('clf',Lasso())])\n        param_grid = {\n        'polynomial__degree':list(range(1,max_poly_degree)),\n        'clf__alpha':list(alpha),\n        'clf__fit_intercept':[True],\n        'clf__normalize':[False]\n        }\n\n    return pipe, param_grid","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_encoder = MEstimateEncoder()\nlr_max_poly_degree = 3\nlr_alpha = [0.00000001, 0.001, 0.1, 1, 2, 4, 10]\nlr_k = list(range(1,X_train.shape[1]))\n\nlr_pipe, lr_grid = non_linear_multiple_regressor_pipeline(\n                                          encoder = lr_encoder,\n                                          max_poly_degree = lr_max_poly_degree,\n                                          alpha = lr_alpha,\n                                          use_selectkbest=True,\n                                          k=lr_k)\n\nlr_gd = RandomizedSearchCV(estimator=lr_pipe,\n                           param_distributions=lr_grid,\n                           scoring='neg_mean_absolute_error',\n                           cv=3,\n                           n_iter=10,\n                           n_jobs=-1,\n                           verbose=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_gd.fit(X_train, np.squeeze(y_train.values));\nlr_pred = lr_gd.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = weighted_mean_absolute_error(y_test, lr_pred, W = X_test['Date'].apply(holiday_weights).values)\nprint(\"wmae-score: {}\".format(score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Light GBM - An approach considering the Markdown features"},{"metadata":{"trusted":true},"cell_type":"code","source":"def lgbm_pipeline(encoder,\n                  n_estimators,\n                  max_depth,\n                  min_child_samples,\n                  boosting_type):   \n\n    \"\"\"\n    Creates a Light GBM regressor pipeline with the following steps:\n    \n    1) Econder\n    2) Scaler\n    3) LightGBM Regressor\n    \n    Arguments:\n        n_estimators: the number of estimators.\n        max_depth: the maximum tree depth.\n        min_child_samples: the minimum number of samples accepted in a leaf.\n        boosting_type: defines the type of algorithm you want to run, default=gdbt\n                       - gbdt: traditional Gradient Boosting Decision Tree\n                       - rf: random forest\n                       - dart: Dropouts meet Multiple Additive Regression Trees (Recommended)\n                       - goss: Gradient-based One-Side Sampling\n                       \n    Output:\n        pipe: the training pipeline\n        param_grid: the param grid for training the pipeline\n    \"\"\"\n    \n    if use_selectkbest:\n        pipe = Pipeline([(\"encoder\",encoder),\n                         (\"scaler\",StandardScaler()),\n                         ('kbest',SelectKBest(f_regression, k=k)),\n                         (\"clf\", lgb.LGBMRegressor(n_jobs=-1))])\n    else:\n        pipe = Pipeline([(\"encoder\",JamesSteinEncoder()),\n                         (\"scaler\",StandardScaler()),\n                         (\"clf\", lgb.LGBMRegressor(n_jobs=-1))])\n    \n    return pipe, param_grid","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### LightGBM Without Markdown Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"lgbm_encoder = MEstimateEncoder()\nlgbm_n_estimators = [5000]\nlgbm_max_depth = [None, 2, 4, 8, 10]\nlgbm_min_child_samples = [2, 4, 8, 10, 20]\nlgbm_boosting_type = ['dart','gbdt','rf']\n\nlgbm_pipe, lgbm_grid = lgbm_pipeline(\n                            encoder=lgbm_encoder,\n                            n_estimators=lgbm_n_estimators,\n                            max_depth=lgbm_max_depth,\n                            min_child_samples=lgbm_min_child_samples,\n                            boosting_type=lgbm_boosting_type\n                        )\n\nlgbm_gd = RandomizedSearchCV(estimator=lgbm_pipe,\n                             param_distributions=lgbm_grid,\n                             scoring='neg_mean_absolute_error',\n                             cv=3,\n                             n_iter=2,\n                             n_jobs=-1,\n                             verbose=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgbm_gd.fit(X_train, np.squeeze(y_train.values));\nlgbm_pred = lgbm_gd.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgbm_gd.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = weighted_mean_absolute_error(y_test, lgbm_pred, W = X_test['Date'].apply(holiday_weights).values)\nprint(\"wmae-score: {}\".format(score))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_feature_importances(model=lgbm_gd.best_estimator_['clf'],\n                         X=X_train,\n                         num=30);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Including Markdown Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = modeling_df.drop(columns=['Weekly_Sales'])\ny = modeling_df[['Weekly_Sales']]\n\nX['holiday'] = X['holiday'].fillna('not_holiday')\nX_train, X_test, y_train, y_test = train_test_split(X, y,\n                                                    test_size=0.33)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgbm_encoder = MEstimateEncoder()\nlgbm_n_estimators = [5000]\nlgbm_max_depth = [None, 2, 4, 8, 10]\nlgbm_min_child_samples = [2, 4, 8, 10, 20]\nlgbm_boosting_type = ['dart','gbdt','rf']\n\nlgbm_pipe_2, lgbm_grid_2 = lgbm_pipeline(\n                            encoder=lgbm_encoder,\n                            n_estimators=lgbm_n_estimators,\n                            max_depth=lgbm_max_depth,\n                            min_child_samples=lgbm_min_child_samples,\n                            boosting_type=lgbm_boosting_type\n                        )\n\nlgbm_gd_2 = RandomizedSearchCV(estimator=lgbm_pipe,\n                             param_distributions=lgbm_grid,\n                             scoring='neg_mean_absolute_error',\n                             cv=3,\n                             n_iter=2,\n                             n_jobs=-1,\n                             verbose=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgbm_gd_2.fit(X_train, np.squeeze(y_train.values));\nlgbm_pred_2 = lgbm_gd_2.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgbm_gd_2.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = weighted_mean_absolute_error(y_test, lgbm_pred_2, W = X_test['Date'].apply(holiday_weights).values)\nprint(\"wmae-score: {}\".format(score))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_feature_importances(model=lgbm_gd_2.best_estimator_['clf'],\n                         X=X_train,\n                         num=30);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Clearly the best model was the LightGBM Regressor not considering the Markdown features!"},{"metadata":{},"cell_type":"markdown","source":"#### <a class=\"anchor\" id=\"me_shap\">5.4. Shapley Values</a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder = MEstimateEncoder()\nplot_df = encoder.fit_transform(X_test,\n                                y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"explainer = shap.TreeExplainer(lgbm_gd.best_estimator_['clf'])\nshap_values = explainer.shap_values(plot_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_df.columns = X_test.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shap.summary_plot(shap_values, plot_df,max_display=20);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shap.summary_plot(shap_values, plot_df,max_display=20, plot_type=\"bar\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <a class=\"anchor\" id=\"sub\">6. Submission</a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"validation_data = test_df.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_data_transformations(df):\n    \"\"\"\n    This function makes all the necessary transformations\n    for predicting the testing data.\n    \"\"\"\n    df = pd.merge(df, stores_df, on='Store', how='inner')\n    \n    df = pd.merge(df, \n                  features_df,\n                  on=['Store', 'Date','IsHoliday'],\n                  how='inner')\n\n    df = pd.merge(df,\n                  cyclical_df,\n                  on='Date',\n                  how='inner')\n\n    df['holiday'] = df['Date'].apply(create_holidays)\n    df['IsHoliday'] = df['holiday'].apply(update_isholiday)\n\n    df['holiday'] = df['holiday'].fillna('not_holiday')\n\n    df.drop(columns=['MarkDown1',\n                     'MarkDown2',\n                     'MarkDown3',\n                     'MarkDown4',\n                     'MarkDown5'],inplace=True)\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"validation_data = make_data_transformations(validation_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"validation_pred = lgbm_gd.best_estimator_.predict(validation_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_submission_df(validation_data, y_pred):\n    \"\"\"\n    This functiont takes the testing data and its \n    correspondig predictions and create a submission\n    dataframe.\n    \n    Arguments:\n        validation_data: the validation_data\n        y_pred: the validation_data predictions\n        \n    Output:\n        submission_df: the submission dataframe\n    \"\"\"\n    submission_df = pd.read_csv(\"../input/walmart-recruiting-store-sales-forecasting/sampleSubmission.csv.zip\", compression='zip')\n    \n    validation_data['Id'] = validation_data['Store'].astype(str) + \"_\" + \\\n                            validation_data['Dept'].astype(str) + \"_\" + \\\n                            validation_data[\"Date\"]\n    \n    validation_data['Weekly_Sales'] = y_pred\n    \n    submission_df = pd.merge(submission_df,\n                             validation_data[['Id','Weekly_Sales']],\n                             on='Id',\n                             how='inner')\n    \n    submission_df.drop(columns='Weekly_Sales_x', inplace = True)\n    submission_df.rename(columns={'Weekly_Sales_y':'Weekly_Sales'}, inplace=True)\n    \n    return submission_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df = make_submission_df(validation_data=validation_data,\n                                   y_pred=validation_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df.to_csv('submission.csv')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}