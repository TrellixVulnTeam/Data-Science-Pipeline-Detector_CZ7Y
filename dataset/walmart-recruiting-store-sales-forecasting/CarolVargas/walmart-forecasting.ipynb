{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Bibliotecas","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\n\n\nfrom matplotlib.pyplot import plot, figure\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport math\nimport arrow\nfrom datetime import datetime as dt\nimport statsmodels.api as sm\nimport seaborn as sns\nfrom scipy import stats\nimport scipy\nimport scipy.stats\nimport pylab \nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Define o tamanho das figuras\nmatplotlib.rcParams['figure.figsize'] = (18.0, 8.0)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-10T02:19:07.890114Z","iopub.execute_input":"2022-06-10T02:19:07.890646Z","iopub.status.idle":"2022-06-10T02:19:10.265437Z","shell.execute_reply.started":"2022-06-10T02:19:07.890544Z","shell.execute_reply":"2022-06-10T02:19:10.263629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Modelo para prever as vendas semanais para diferentes departamentos de diversas lojas de um grande varejista","metadata":{}},{"cell_type":"markdown","source":"## Load e Join dos dados","metadata":{}},{"cell_type":"code","source":"features_data = pd.read_csv('../input/walmart-recruiting-store-sales-forecasting/features.csv.zip')\nstore_data = pd.read_csv('../input/walmart-recruiting-store-sales-forecasting/stores.csv')\ntest_data = pd.read_csv('../input/walmart-recruiting-store-sales-forecasting/test.csv.zip')\ntrain_data = pd.read_csv('../input/walmart-recruiting-store-sales-forecasting/train.csv.zip')\n\n# Indicadora de base de teste\ntest_data['is_test'] = 1\ntrain_data['is_test'] = 0\n\n# Junta as bases\ndata = pd.DataFrame(train_data.append(test_data)).sort_values('Date')\n\n# Junta as features\ndata = pd.merge(data, features_data.drop('IsHoliday', axis=1), how=\"left\", on=['Store','Date'])\ndata = pd.merge(data, store_data, how=\"left\", on=['Store'])","metadata":{"execution":{"iopub.status.busy":"2022-06-10T02:19:10.268082Z","iopub.execute_input":"2022-06-10T02:19:10.270881Z","iopub.status.idle":"2022-06-10T02:19:12.207251Z","shell.execute_reply.started":"2022-06-10T02:19:10.270825Z","shell.execute_reply":"2022-06-10T02:19:12.205944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Construção de novas features","metadata":{}},{"cell_type":"code","source":"data['IsHoliday'] = [ '1' if value == True else '0' for row, value in enumerate( data.IsHoliday ) ]\n\n# Converte o campo Date de string para tipo Data\ndata[\"Date2\"] = [ dt.strptime( value, '%Y-%m-%d') for row, value in enumerate(data.Date) ] \n\n# Função para retornar a semana do mes\ndef week_number_of_month(date_value):\n    if( dt.strftime( date_value , '%b' ) == \"Jan\" ):\n        out = date_value.isocalendar()[1]\n    else:\n        out = date_value.isocalendar()[1] - date_value.replace(day=1).isocalendar()[1] + 1\n    return (out)\n\n# Variáveis adicionais\ndata[\"week_month\"] = [ week_number_of_month(value) for row, value in enumerate(data[\"Date2\"])   ] \ndata[\"month\"] = [ dt.strftime( value , '%m-%b' ) for row, value in enumerate(data.Date2) ]\n\ndata2 = data","metadata":{"execution":{"iopub.status.busy":"2022-06-10T02:19:12.208945Z","iopub.execute_input":"2022-06-10T02:19:12.209427Z","iopub.status.idle":"2022-06-10T02:19:30.566467Z","shell.execute_reply.started":"2022-06-10T02:19:12.209381Z","shell.execute_reply":"2022-06-10T02:19:30.565533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Ajustes para vendas negativas","metadata":{}},{"cell_type":"markdown","source":"Olhando para o resumo descritivo da tabela abaixo, podemos ver que, para algumas observações, os valores das vendas são negativos. Pela natureza do dado, assumimos que esse valor não pode ser zero. Faremos a sua substituição pela média de vendas para o mesmo departamento e semana do mês.","metadata":{}},{"cell_type":"code","source":"print(data.describe())","metadata":{"execution":{"iopub.status.busy":"2022-06-10T02:19:30.568763Z","iopub.execute_input":"2022-06-10T02:19:30.569175Z","iopub.status.idle":"2022-06-10T02:19:31.177224Z","shell.execute_reply.started":"2022-06-10T02:19:30.569143Z","shell.execute_reply":"2022-06-10T02:19:31.176204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Backup antes do join\ndata = data2\n\n# Cálculo da média por loja, departamento e semana do mês\ndf_medias_dept_week = data.groupby([ 'Store', 'Dept', 'week_month' ]).mean(['Weekly_Sales'])['Weekly_Sales'].reset_index().rename( columns={'Weekly_Sales': 'Weekly_Sales_Mean_week'} )\n\n# Cálculo da média por loja e departamento\ndf_medias_dept = data.groupby([ 'Store', 'Dept']).mean(['Weekly_Sales'])['Weekly_Sales'].reset_index().rename( columns={'Weekly_Sales': 'Weekly_Sales_Mean'} )\n\n# Salva uma cópia das vendas originais\ndata['Weekly_Sales_Original'] = data['Weekly_Sales']\n\n# Join da base de vendas com as médias\ndata = data.merge( df_medias_dept_week, how='left', on = ['Store', 'Dept', 'week_month'],  )\\\n           .merge( df_medias_dept, how='left', on = ['Store', 'Dept'] )\n\n# Substituição das vendas zeradas ou negativas pela média \n# Pode ser que o departamento nao tenha informacao para aquela semana, entao substitui pela media do departamento da loja\ndata.Weekly_Sales = [ data.Weekly_Sales_Mean_week[row] if value < 0 else value for row, value in enumerate(data.Weekly_Sales) ]\ndata.Weekly_Sales = [ data.Weekly_Sales_Mean[row] if value < 0 else value for row, value in enumerate(data.Weekly_Sales) ]\n\n\n# 194 valores de Weekly_Sales ainda permanecem negativos, entao excluí da base\ndata.Weekly_Sales = [ np.nan if value <= 0 else value for row, value in enumerate(data.Weekly_Sales) ]\ndata = data[ (( ~(np.isnan(data.Weekly_Sales))) & (data.is_test == 0)) | (data.is_test == 1) ]\n\n# Deleta as colunas de médias utilizadas para a substituição\ndata = data.drop(['Weekly_Sales_Mean', 'Weekly_Sales_Mean_week'], axis = 1)\n\n# Redefine as bases de teste e treino\ntest_data = data[ data.is_test == 1 ]\ntrain_data = data[ data.is_test == 0 ]","metadata":{"execution":{"iopub.status.busy":"2022-06-10T02:19:31.178827Z","iopub.execute_input":"2022-06-10T02:19:31.179979Z","iopub.status.idle":"2022-06-10T02:19:33.211297Z","shell.execute_reply.started":"2022-06-10T02:19:31.179916Z","shell.execute_reply":"2022-06-10T02:19:33.210242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note que, em geral, as substituições afetam somente alguns dias, lojas e departamentos específicos, essa diferença não é significativa olhando para o macro (gráfico abaixo).","metadata":{}},{"cell_type":"code","source":"df_soma = train_data.groupby('Date').sum()\n\nplt.plot( df_soma.Weekly_Sales )\nplt.title(\"Vendas Ajustada vs Vendas Original\")\nplt.plot( df_soma.Weekly_Sales_Original )\nplt.legend([\"Vendas Ajustadas\", \"Vendas Originais\"])","metadata":{"execution":{"iopub.status.busy":"2022-06-10T02:19:33.212558Z","iopub.execute_input":"2022-06-10T02:19:33.213019Z","iopub.status.idle":"2022-06-10T02:19:35.069963Z","shell.execute_reply.started":"2022-06-10T02:19:33.212985Z","shell.execute_reply":"2022-06-10T02:19:35.069143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Dados faltantes","metadata":{}},{"cell_type":"markdown","source":"Apliquei o mesmo processo de substituição dos dados faltantes feito acima para as covariáveis MarkDown1 a MarkDown5, pois essas variáveis apresentaram uma alta taxa de missings (tabela abaixo) e, por isso, o processo de substituição é mais adequado; outra alternativa seria a remoção delas do modelo. Além disso, retirei as observações nulas de CPI e Unemployment, pois representam um percentual baixo.","metadata":{}},{"cell_type":"code","source":"### Verifição dos nulos\ncolunas = data.columns.to_list()\nprint(\n    pd.DataFrame(\n        { 'nulos' : [ 1-( len(data[i].dropna())/len(data)) for i in  colunas], \n          'coluna' : colunas \n        }\n    )\n)","metadata":{"execution":{"iopub.status.busy":"2022-06-10T02:19:35.071193Z","iopub.execute_input":"2022-06-10T02:19:35.072188Z","iopub.status.idle":"2022-06-10T02:19:35.377449Z","shell.execute_reply.started":"2022-06-10T02:19:35.072149Z","shell.execute_reply":"2022-06-10T02:19:35.376142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Processo de substituição dos nulos\ncols_replace = ['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']\n\ndf_medias_store_MarkDown = train_data.groupby('Store').mean()[cols_replace].reset_index()\ndf_medias_geral_MarkDown = train_data.mean()[cols_replace]\n\n\ndata = data.merge( df_medias_store_MarkDown, how='left', on = ['Store'], suffixes=('', '_Store_Mean')  )\n\n# Substitui dados nulos pela média da loja\ndata.MarkDown1 = [ data.MarkDown1_Store_Mean[row] if np.isnan(value) else value for row, value in enumerate(data.MarkDown1) ]\ndata.MarkDown2 = [ data.MarkDown2_Store_Mean[row] if np.isnan(value) else value for row, value in enumerate(data.MarkDown2) ]\ndata.MarkDown3 = [ data.MarkDown3_Store_Mean[row] if np.isnan(value) else value for row, value in enumerate(data.MarkDown3) ]\ndata.MarkDown4 = [ data.MarkDown4_Store_Mean[row] if np.isnan(value) else value for row, value in enumerate(data.MarkDown4) ]\ndata.MarkDown5 = [ data.MarkDown5_Store_Mean[row] if np.isnan(value) else value for row, value in enumerate(data.MarkDown5) ]\n\n# Se ainda tiver nulos, substitui pela média geral\ndata.MarkDown1 = [ df_medias_geral_MarkDown[0] if np.isnan(value) else value for row, value in enumerate(data.MarkDown1) ]\ndata.MarkDown2 = [ df_medias_geral_MarkDown[1] if np.isnan(value) else value for row, value in enumerate(data.MarkDown2) ]\ndata.MarkDown3 = [ df_medias_geral_MarkDown[2] if np.isnan(value) else value for row, value in enumerate(data.MarkDown3) ]\ndata.MarkDown4 = [ df_medias_geral_MarkDown[3] if np.isnan(value) else value for row, value in enumerate(data.MarkDown4) ]\ndata.MarkDown5 = [ df_medias_geral_MarkDown[4] if np.isnan(value) else value for row, value in enumerate(data.MarkDown5) ]\n\n# Deleta as médias utilizadas para a substituição\ndata = data.drop(['MarkDown1_Store_Mean', 'MarkDown2_Store_Mean', 'MarkDown3_Store_Mean', 'MarkDown4_Store_Mean', 'MarkDown5_Store_Mean'], 1)\n\n# Deleta os nulos das colunas CPI e Unemployment\ndata = data.dropna( subset=['CPI', 'Unemployment'], how = 'any' )\n\n# Redefine as bases de teste e treino\ntest_data = data[ data.is_test == 1 ]\ntrain_data = data[ data.is_test == 0 ]","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-10T02:19:35.378926Z","iopub.execute_input":"2022-06-10T02:19:35.379291Z","iopub.status.idle":"2022-06-10T02:22:05.849844Z","shell.execute_reply.started":"2022-06-10T02:19:35.379262Z","shell.execute_reply":"2022-06-10T02:22:05.84891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Escolha da Técnica de Modelagem","metadata":{}},{"cell_type":"markdown","source":"Há pelo menos duas abordagens de modelagem que pode ser seguido com o trabalho:\n  - **Séries Temporais:** Modelagem da variável \"Venda\" como uma variável aleatória dependente do tempo, identificando o seu comportamento ao longo do tempo e ajustando um modelo de previsão. Um modelo clássico seria o ARIMA ou o SARIMA (considera períodos sazonais)\n  \n  - **Modelos de regressão:** Uma abordagem via regressão incorpora dados explicativos à variável resposta (vendas) através de uma função de ligação. Através da identificação da distribuição da variável resposta, podemos aplicar um MLG (modelo linear geral) para explicar a venda futura.\n  \n  \nComo possuímos algumas variáveis explicativas, como indicador de feriado, identificação da loja e departamento da venda, por simplicidade e menor tempo de entrega, optei por seguir com a modelagem via regressão.\n\nNo gráfico de linhas da série das vendas semanais, conseguimos ver períodos com maiores e menores valores absolutos de venda. \n\nA influência de feriados, dos meses de novembro e dezembro e da semana do mês, nas vendas, ficam evidentes nos gráficos abaixo.","metadata":{}},{"cell_type":"code","source":"# Gráfico da Série de Vendas\nweekly_sales = train_data.groupby([\"Date\",\"IsHoliday\"]).sum().reset_index().sort_values(\"Date\")\n#weekly_sales[\"is_holiday\"] = [\"Sim\" if value > 0 else \"Nao\" for row, value in enumerate( weekly_sales['IsHoliday'].astype(int) )]\n\n# Tema do gráfico \nsns.set_theme(style='darkgrid')\n\n# Divide a figura em 2 gráficos\nplt.figure(1)\n\n#\nplt.subplot(211)\nsns.lineplot(data = train_data, y=\"Weekly_Sales\", x = \"Date\").set(title = \"Vendas Totais\")\n\nplt.subplot(212)\nsns.lineplot(data = weekly_sales, y=\"Weekly_Sales\", x = \"Date\", hue = \"IsHoliday\").set(title = \"Vendas com Feriados\")","metadata":{"execution":{"iopub.status.busy":"2022-06-10T02:22:05.851359Z","iopub.execute_input":"2022-06-10T02:22:05.852204Z","iopub.status.idle":"2022-06-10T02:22:22.924563Z","shell.execute_reply.started":"2022-06-10T02:22:05.852163Z","shell.execute_reply":"2022-06-10T02:22:22.923599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Também existem picos de vendas nas semanas onde não se apresentam feriados, mas sabemos que feriados importantes, como o Natal e a comemoração do dia dos Namorados, acabam afetando as vendas do mês como um todo.","metadata":{}},{"cell_type":"code","source":"# Meses com feriados\nprint(train_data[ train_data.IsHoliday == '1']['month'].unique())","metadata":{"execution":{"iopub.status.busy":"2022-06-10T02:22:22.927648Z","iopub.execute_input":"2022-06-10T02:22:22.92806Z","iopub.status.idle":"2022-06-10T02:22:23.012518Z","shell.execute_reply.started":"2022-06-10T02:22:22.928024Z","shell.execute_reply":"2022-06-10T02:22:23.010886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Tema do gráfico \n#sns.set_theme(style='darkgrid')\nsns.set_theme(style=\"ticks\")\n\n# Divide a figura em 2 gráficos\nplt.figure(1)\n\n#\nplt.subplot(311)\nsns.boxplot( x=weekly_sales[\"Weekly_Sales\"], y = weekly_sales[\"IsHoliday\"], whis=[0, 100], width=.6, palette=\"vlag\", orient = 'h')\n\n# Add in points to show each observation\n#sns.stripplot(y=weekly_sales[\"is_holiday\"], x=weekly_sales[\"Weekly_Sales\"], size=4, color=\".3\", linewidth=0)\n\nplt.subplot(312)\nsns.boxplot( x= \"Weekly_Sales\", y = \"month\", data = train_data.sort_values(\"month\"), whis=[0, 100], width=.6, palette=\"vlag\", orient = 'h')\n\n# Add in points to show each observation\n# sns.stripplot(y=weekly_sales[\"month\"], x=weekly_sales[\"Weekly_Sales\"], size=4, color=\".3\", linewidth=0)\n\nplt.subplot(313)\nsns.boxplot( x = \"Weekly_Sales\", y = \"week_month\", data = train_data, whis=[0, 100], width=.6, palette=\"vlag\", orient = 'h')\n\n# Add in points to show each observation\n# sns.stripplot(y=weekly_sales[\"week_month\"], x=weekly_sales[\"Weekly_Sales\"], size=4, color=\".3\", linewidth=0)","metadata":{"execution":{"iopub.status.busy":"2022-06-10T02:22:23.014256Z","iopub.execute_input":"2022-06-10T02:22:23.015047Z","iopub.status.idle":"2022-06-10T02:22:24.930355Z","shell.execute_reply.started":"2022-06-10T02:22:23.015008Z","shell.execute_reply":"2022-06-10T02:22:24.928859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note que os valores das vendas tem comportamentos diferentes para cada mês. Os meses do meio do mês vem de uma crescente de vendas, com janeiro sendo o mês com menores vendas, e começa a cair tendo uma segunda crescente nos últimos dois meses do ano. Mesmo dezembro sendo o mês com feriados importantes, como o Natal, é o mês de novembro que possui o maior pico de vendas.\n\nEm relação à variável weekly_month, não vemos comportamentos muito diferentes de vendas entre as semanas, mas temos um maior pico de vendas na quarta semana.","metadata":{}},{"cell_type":"markdown","source":"## Identificação da função de distribuição de probabilidades das vendas","metadata":{}},{"cell_type":"markdown","source":"A partir daqui, várias possibilidades foram testadas. \nOlhando para o histograma abaixo, percebemos que os dados estão muito inflacionados em valores baixos e com concentrações baixíssimas em valores muito altos. ","metadata":{}},{"cell_type":"code","source":"sns.histplot( x = 'Weekly_Sales', data = train_data, bins=60)\\\n.set(title = 'Densidade dos Valore Reais vs Predito')","metadata":{"execution":{"iopub.status.busy":"2022-06-10T02:22:24.932149Z","iopub.execute_input":"2022-06-10T02:22:24.932648Z","iopub.status.idle":"2022-06-10T02:22:25.283021Z","shell.execute_reply.started":"2022-06-10T02:22:24.932602Z","shell.execute_reply":"2022-06-10T02:22:25.281848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Poderíamos tentar ajustar uma distribuição gamma, que tem sua função densidade de probabilidade assimétrica, assim como a distribuição dos dados desse caso. Foi utilizada a função GLM no pacote scipy para tentar ajustar um modelo de regressão gamma, mas como é uma distribuição com três parâmetros a serem estimados, acaba sendo computacionalmente e o cluster do Kaggle não aguentou, restartando todas as vezes que tentei rodar.\n\nMinha segunda opção foi tentar ajustar um modelo de regressão linear, mas como existem muitos valores próximos de zero nas vendas semanais, obtive muitas previsões negativas. \n\nPensando que poderíamos reduzir a variabilidade dos dados, decidi aplicar a transformação de BoxCox na variável resposta e tentar ajustar um modelo de regressão linear para essa variável transformada.","metadata":{}},{"cell_type":"markdown","source":"### Transformação de Boxcox","metadata":{}},{"cell_type":"markdown","source":"A transformação de Box-Cox se dá pela seguinte expessão:\n\n$$ y = \\dfrac{x^\\lambda - 1}{\\lambda}, \\text{para } \\lambda \\neq 0 $$\n\nNesse caso, x é o campo Weekly_Sales e y será a variável para a qual ajustaremos o modelo.\n\nO parâmetro $\\lambda$ é otimizado de forma que y se aproxime de uma distribuição normal.\n\nO nosso valor predito voltará à escala original pela transformação inversa:\n\n$$ x = (y\\lambda + 1)^\\frac{1}{\\lambda} $$","metadata":{}},{"cell_type":"code","source":"data[\"Weekly_Sales_Boxcox\"] = np.nan\ndata[\"Boxcox_lambda\"] = np.nan\n\n## Aplicando tranformação de boxcox para normalização das vendas\nsoma_lambda = 0 # np.mean(train_data[\"Weekly_Sales\"])\ntrain_data[\"Weekly_Sales_Boxcox\"], train_data[\"Boxcox_lambda\"] = stats.boxcox( (train_data[\"Weekly_Sales\"] + soma_lambda ) )\ndata[\"Weekly_Sales_Boxcox\"][data.is_test == 0], data[\"Boxcox_lambda\"][data.is_test == 0] = stats.boxcox( (data[data.is_test == 0] [\"Weekly_Sales\"] + soma_lambda ) )","metadata":{"execution":{"iopub.status.busy":"2022-06-10T02:22:25.284365Z","iopub.execute_input":"2022-06-10T02:22:25.284706Z","iopub.status.idle":"2022-06-10T02:22:26.228291Z","shell.execute_reply.started":"2022-06-10T02:22:25.284678Z","shell.execute_reply":"2022-06-10T02:22:26.227083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Parte 1 - Ajuste de um único modelo geral (considera as lojas e departamentos como variável aleatória)","metadata":{}},{"cell_type":"markdown","source":"## 1.1 - Análise de correlação/multicolinearidade para a pré-seleção de variáveis","metadata":{"execution":{"iopub.status.busy":"2022-06-09T03:01:36.052752Z","iopub.execute_input":"2022-06-09T03:01:36.053356Z","iopub.status.idle":"2022-06-09T03:01:36.05902Z","shell.execute_reply.started":"2022-06-09T03:01:36.053307Z","shell.execute_reply":"2022-06-09T03:01:36.057762Z"}}},{"cell_type":"markdown","source":"A variável MarkDown1 ficou fortemente correlacionada com a MarkDown4 e Size; assim, ela foi descartada como input do modelo. \nNote que covariáveis com correlação alta prejudica o estimdador dos parâmetros e pode viesar o resultado final.","metadata":{}},{"cell_type":"code","source":"df_corr = train_data.corr()\nprint( df_corr[df_corr>.5] )","metadata":{"execution":{"iopub.status.busy":"2022-06-10T02:22:26.230101Z","iopub.execute_input":"2022-06-10T02:22:26.230519Z","iopub.status.idle":"2022-06-10T02:22:26.74431Z","shell.execute_reply.started":"2022-06-10T02:22:26.230488Z","shell.execute_reply":"2022-06-10T02:22:26.743372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.2 - Ajuste do Modelo","metadata":{}},{"cell_type":"code","source":"cols_dummies = ['Type', 'month', 'week_month', 'IsHoliday', 'Store', 'Dept']\n\nx_train = pd.get_dummies( \n                train_data.drop(['Date', 'Weekly_Sales', 'Weekly_Sales_Original', 'is_test','Date2', 'Weekly_Sales_Boxcox','Weekly_Sales_Boxcox','Boxcox_lambda'], axis=1),\n                columns = cols_dummies, \n                drop_first=True\n            )\nx_train['const'] = 1\n#x_train = sm.add_constant(x_train)\n\ny_train = train_data['Weekly_Sales_Boxcox']\n\nx_test = pd.get_dummies( \n                test_data.drop(['Date', 'Weekly_Sales', 'Weekly_Sales_Original', 'is_test','Date2'], axis=1), \n                columns = cols_dummies, \n                drop_first=True\n            )\nx_test['const'] = 1\nx_test = sm.add_constant(x_test)\n\n# Ajusta as colunas do x_teste para ficar igual a x_train\nx_train_names = x_train.columns\nx_test_names = x_test.columns\nmissing_names = list(x_train_names[ ~(x_train_names.isin(x_test_names)) ])\nfor i in (missing_names):\n    x_test[i]  = 0\nx_test = x_test[x_train_names]","metadata":{"execution":{"iopub.status.busy":"2022-06-10T02:22:26.746807Z","iopub.execute_input":"2022-06-10T02:22:26.747237Z","iopub.status.idle":"2022-06-10T02:22:27.6186Z","shell.execute_reply.started":"2022-06-10T02:22:26.747202Z","shell.execute_reply":"2022-06-10T02:22:27.617567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.3 Modelo e seleção das variáveis","metadata":{}},{"cell_type":"markdown","source":"Uma das formas de seleção das variáveis resposta para o modelo de regressão é pelo método de stepwise. Como não encontrei um pacote em python para realizar esse método, e por questões práticas de entrega do case, parti para uma seleção manual, onde comecei retirando a variável menos significativa e ajustando um novo modelo às restantes, até que só sobrassem variáveis significativas. Dessa forma, foram excluídas do modelo final as variáveis Markdown1 e a CPI.\n\nPelos dados apresentarem muita variabilidade, considerei um nível de 10% de signifância para o p-valor do teste dos coeficientes, pois poderíamos acabar rejeitando alguma variável explicativa importante para o modelo caso tivesse considerado um nível de significância de 5%.\n\nAbaixo é mostrado o summary do modelo final.","metadata":{}},{"cell_type":"code","source":"features_drop = ['CPI', 'MarkDown1']\nresults = sm.OLS(y_train, x_train.drop( features_drop, axis = 1 ) ).fit()\nresults.summary()","metadata":{"execution":{"iopub.status.busy":"2022-06-10T02:22:27.620001Z","iopub.execute_input":"2022-06-10T02:22:27.620353Z","iopub.status.idle":"2022-06-10T02:22:41.402872Z","shell.execute_reply.started":"2022-06-10T02:22:27.62032Z","shell.execute_reply":"2022-06-10T02:22:41.40122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note que temos um valor de R2 ajustado muito bom, que nos diz o quão bom o modelo está ajustado aos dados. Quanto mais próximo de um esse coeficiente está, melhor o ajuste.","metadata":{}},{"cell_type":"markdown","source":"## 1.4 - Valor Ajustado","metadata":{}},{"cell_type":"markdown","source":"Aqui fazemos a predição para a variável transformada e invertemos a transformação para obter a variável na escala original.","metadata":{}},{"cell_type":"code","source":"# Predição para a variável transformada\nWeekly_Sales_Boxcox_Predict = results.predict(x_train.drop( features_drop , axis = 1) )\nBoxcox_lambda = min(train_data.Boxcox_lambda)\n\n# Predição com escala dos dados originais\nWeekly_Sales_Predict = ( ((Weekly_Sales_Boxcox_Predict * Boxcox_lambda) + 1) ** (1/Boxcox_lambda) )\n\nWeekly_Sales = train_data.Weekly_Sales\nWeekly_Sales_Boxcox = train_data.Weekly_Sales_Boxcox\n\ndf_final = train_data\ndf_final['Weekly_Sales_Predict'] = Weekly_Sales_Predict","metadata":{"execution":{"iopub.status.busy":"2022-06-10T02:22:41.404623Z","iopub.execute_input":"2022-06-10T02:22:41.405675Z","iopub.status.idle":"2022-06-10T02:22:42.369003Z","shell.execute_reply.started":"2022-06-10T02:22:41.405624Z","shell.execute_reply":"2022-06-10T02:22:42.367632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.5 - Comparação da distribuição real versus ajustada","metadata":{}},{"cell_type":"code","source":"# plt.hist( Weekly_Sales_Predict, bins=50, density = True)\n# plt.hist( Weekly_Sales, bins=50, density = True )\n\n### Modelo Ajustado\n\ndf_comparar_filtro_vendas =  \\\npd.DataFrame( {\n    'tipo': ['predict'] * len(Weekly_Sales_Predict),\n    'y' : train_data.Weekly_Sales_Predict,\n    'date': train_data.Date\n}).append(\npd.DataFrame( {\n    'tipo': ['y_value'] * len(y_train),\n    'y' : train_data.Weekly_Sales,\n    'date': train_data.Date\n})  \n).reset_index()\n\nplt.figure(1)\n\nplt.subplot(311)\n# Plotando a densidade das distribuições de Weekly_Sales e de Weekly_Sales_Predict (Limitados por Weekly_Sales)\nsns.kdeplot( x = 'y', hue = 'tipo', data = df_comparar_filtro_vendas)\\\n .set(title = 'Densidade dos Valore Reais vs Predito', xlim=(0,40000))\n\nplt.subplot(312)\nsns.histplot( x = 'y', hue = 'tipo', data = df_comparar_filtro_vendas)\\\n.set(title = 'Densidade dos Valore Reais vs Predito', xlim=(0,40000))\n\nplt.subplot(313)\nsns.lineplot(  y = df_comparar_filtro_vendas.y, x = df_comparar_filtro_vendas.date, hue = df_comparar_filtro_vendas.tipo)\\\n.set(title = 'Valore Reais vs Predito')","metadata":{"execution":{"iopub.status.busy":"2022-06-10T02:22:42.370747Z","iopub.execute_input":"2022-06-10T02:22:42.371554Z","iopub.status.idle":"2022-06-10T02:23:20.793633Z","shell.execute_reply.started":"2022-06-10T02:22:42.37152Z","shell.execute_reply":"2022-06-10T02:23:20.792377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Como vemos no terceiro gráfico, os valores preditos ficam um pouco abaixo dos valores reais. Isso pode estar acontecendo por causa da inflação de valores baixos. \n\nNa literatura existem distribuições específicas para dados inflacionados, mas não conheço nenhum pacote em python com essa solução.","metadata":{}},{"cell_type":"markdown","source":"## 1.6 - Gráfico da série de vendas para algumas lojas e departamentos","metadata":{}},{"cell_type":"markdown","source":"Abaixo plotei alguns gráficos de dos valores reais vs valores ajustados para alguns departamentos da loja 3.\nVemos que o modelo não consegue ter uma boa previsão para os picos de venda.","metadata":{}},{"cell_type":"code","source":"df_comparar_lojas =  \\\npd.DataFrame( {\n    'Store': train_data.Store,\n    'Dept': train_data.Dept,\n    'tipo': ['predict'] * len(Weekly_Sales_Predict),\n    'y' : train_data.Weekly_Sales_Predict,\n    'date': train_data.Date\n}).append(\npd.DataFrame( {\n    'Store': train_data.Store,\n    'Dept': train_data.Dept,\n    'tipo': ['y_value'] * len(y_train),\n    'y' : train_data.Weekly_Sales,\n    'date': train_data.Date\n})  \n).reset_index()\n\ndef serie_venda_loja_departamento(loja, departamento):\n    df_comparar_lojas_filtro = df_comparar_lojas[ (df_comparar_lojas.Store == loja) & (df_comparar_lojas.Dept == departamento) ]\n    sns.lineplot(data = df_comparar_lojas_filtro, y=\"y\", x = \"date\", hue = \"tipo\").set(title = \"Vendas: Loja \" + str(loja) + \". Departamento: \" + str(departamento))\n\n# plt.figure(2)\n# for i in range(9):\n#     plt.subplot(3,3,i+1)\n#     serie_venda_loja_departamento(i+1, 10)\n    \n# plt.figure(2)\n# for i in range(15):\n#     plt.subplot(5,3,i+1)\n#     serie_venda_loja_departamento(i+1, 2)\n    \n    \nplt.figure(2)\nfor i in range(15):\n    plt.subplot(5,3,i+1)\n    serie_venda_loja_departamento(i+1, 3)","metadata":{"execution":{"iopub.status.busy":"2022-06-10T02:23:20.795255Z","iopub.execute_input":"2022-06-10T02:23:20.79561Z","iopub.status.idle":"2022-06-10T02:23:44.519753Z","shell.execute_reply.started":"2022-06-10T02:23:20.795572Z","shell.execute_reply":"2022-06-10T02:23:44.518499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.7 - Análise de diagnóstico (erro)","metadata":{}},{"cell_type":"markdown","source":"Abaixo temos alguns gráficos de análise dos resíduos do modelo. Pelo histrograma (gráfico do meio) já podemos ver que os erros não aparentam ter uma distribuição normal; além disso, vemos pelo qq-plot que os resíduos das observações extremas se afastam da distribuição teórica da normal. Tudo isso nos indica que os resíduos desse modelo não têm distribuição normal e, por isso, a regressão linear não seria o melhor modelo de ajuste aos nossos dados, mas não podemos dizer que seria um modelo totalmente descartável, pois note que, apesar de não conseguir se ajustar aos picos da série, tem um ajuste bom para o comportamento geral dela.","metadata":{}},{"cell_type":"code","source":"erro_boxcox = Weekly_Sales_Boxcox_Predict - Weekly_Sales_Boxcox\nerro_predict = Weekly_Sales_Predict - Weekly_Sales\n\nplt.figure(1)\n\nplt.subplot(131)\nsns.scatterplot(Weekly_Sales_Boxcox_Predict, y_train)\\\n.set(title = 'Valor predito vs Valor observado')\n\nplt.subplot(132)\nsns.histplot( erro_boxcox )\\\n.set(title = 'Histograma dos Erros')\n\nplt.subplot(133)\nstats.probplot(erro_boxcox, dist=\"norm\", plot=pylab)\nplt.title( 'QQ-Plot dos erros (normais)' )\n\n# plt.subplot(224)\n# stats.probplot(erro_boxcox[ (erro_boxcox >= -40000) & (erro_boxcox <= 40000) ], dist=\"norm\", plot=pylab)\n# plt.title('QQ-Plot dos erros (normais) s/ outlier')","metadata":{"execution":{"iopub.status.busy":"2022-06-10T02:23:44.521543Z","iopub.execute_input":"2022-06-10T02:23:44.522049Z","iopub.status.idle":"2022-06-10T02:23:48.801917Z","shell.execute_reply.started":"2022-06-10T02:23:44.522004Z","shell.execute_reply":"2022-06-10T02:23:48.800751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.8 - Previsão","metadata":{}},{"cell_type":"code","source":"\n# for i in x_train.columns:\n#     x_test[i].astype = x_train[i].astype\n\ny_future = results.predict( x_test.drop( features_drop , axis = 1) )\ny_future = ( ((y_future * Boxcox_lambda) + 1) ** (1/Boxcox_lambda) )\ndata['Weekly_Sales'][data.is_test == 1] = y_future\n\nsns.lineplot( y = data.Weekly_Sales, x = data.Date, hue = data.is_test)","metadata":{"execution":{"iopub.status.busy":"2022-06-10T02:23:48.803412Z","iopub.execute_input":"2022-06-10T02:23:48.803797Z","iopub.status.idle":"2022-06-10T02:24:01.837544Z","shell.execute_reply.started":"2022-06-10T02:23:48.803744Z","shell.execute_reply":"2022-06-10T02:24:01.83661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Parte 2 - Um modelo para cada loja e departamento","metadata":{}},{"cell_type":"markdown","source":"Como vimos que o modelo geral não consegue se ajustar muito bem à variabilidade dos dados, mesmo tendo um bom R2, parti para o ajuste de um modelo para cada loja e departamento.\n\nA ideia de que um departamento pode ter valores de venda muito diferentes de outro pode explicar a dificuldade do modelo em prever isso, mesmo as variáveis departamento e loja sendo variáveis explicativas do modelo.\n\nPara o ajuste desses modelos, assim como para o modelo geral, desconsiderei a variável CPI como significativa. Note que o ideal seria ter o método de seleção para cada modelo.\n\nPara avaliarmos o ajuste dos modelos individuais, plotei, aleatoriamente, gráficos de valores reais vs valores ajustados.\n\nNote que um modelo para cada loja e departamento se ajusta muito melhor aos dados do que um modelo geral. O modelo individual consegue prever o comportamento cíclico de alguns departamentos e até tendência de crescimento de alguns outros. A previsão futura para alguns acaba tendo picos e extrapolando os valores da série histórica, para isso poderia ser adotada uma política de identificação dessas previsões extremas e ajustar um limite igual ao máximo da série histórica ou a alguma outra melhor política para o negócio.\n","metadata":{}},{"cell_type":"code","source":"# Variáveis para armazenar as métricas dos modelos\n\nBoxcox_lambda = min(train_data.Boxcox_lambda)\npercent_plots = 0.006\n\n## Tabela para armazenar os parâmetros de cada modelo\ndf_parameters = pd.DataFrame({\n    'Store' : np.nan,\n    'Dept': np.nan,\n    'mse_model' : np.nan,\n    'mse_resid' : np.nan,\n    'mse_total' : np.nan,\n    'rsquared' : np.nan,\n    'rsquared_adj' : np.nan\n}, index = ['index'] ).dropna()\n\n\nfor row_store, value_store in enumerate(test_data.Store.unique()):\n    for row_dept, value_dept in enumerate(test_data[test_data.Store == value_store].Dept.unique()):\n        \n        \n        try:\n\n            #print(\"Loja: \" + str(value_store) + \". Dept: \" +  str(value_dept))\n            \n\n            ## Define os dados para ajustar o modelo \n            cols_dummies = ['Type', 'month', 'week_month', 'IsHoliday']\n            \n            df_data_store_dept = data[ (data.Store ==value_store) & (data.Dept == value_dept) ].reset_index().drop(['index'], 1)\n            df_data_store_dept2 = pd.get_dummies( \n                                    df_data_store_dept.drop(['Date','Store', 'Dept', 'Weekly_Sales', 'Weekly_Sales_Original','Date2', 'Weekly_Sales_Boxcox', 'Boxcox_lambda'  ], axis=1),\n                                    columns = cols_dummies, \n                                    drop_first=True\n                                )\n            \n            df_train_store_dept = df_data_store_dept2[ df_data_store_dept2.is_test == 0 ]\n            df_test_store_dept = df_data_store_dept2[ df_data_store_dept2.is_test == 1 ]\n\n\n            x_train = df_train_store_dept\n            x_train['const'] = 1\n            #x_train = sm.add_constant(x_train)\n\n            y_train = df_data_store_dept['Weekly_Sales_Boxcox'][df_data_store_dept.is_test == 0]\n            \n            x_test = df_test_store_dept\n            x_test['const'] = 1\n            #x_test = sm.add_constant(x_test)\n            \n            # Ajusta as colunas do x_teste para ficar igual a x_train\n            x_train_names = x_train.columns\n            x_test_names = x_test.columns\n            missing_names = list(x_train_names[ ~(x_train_names.isin(x_test_names)) ])\n            for i in (missing_names):\n                x_test[i]  = 0\n            x_test = x_test[x_train_names]\n            \n            ## Ajuste do modelo\n            query = 'results_'+ str(value_store) + '_'+ str(value_dept) + '  = sm.OLS( y_train, x_train ).fit() '\n            exec(query)\n\n            ## Salva as métricas\n            df_parameters = \\\n            df_parameters.append(\n                    pd.DataFrame({\n                        'Store' : value_store,\n                        'Dept': value_dept,\n                        'mse_model' : eval('results_'+ str(value_store) + '_'+ str(value_dept) + '.mse_model'),\n                        'mse_resid' : eval('results_'+ str(value_store) + '_'+ str(value_dept) + '.mse_resid'),\n                        'mse_total' : eval('results_'+ str(value_store) + '_'+ str(value_dept) + '.mse_total'),\n                        'rsquared' :  eval('results_'+ str(value_store) + '_'+ str(value_dept) + '.rsquared'),\n                        'rsquared_adj' : eval('results_'+ str(value_store) + '_'+ str(value_dept) + '.rsquared_adj')\n                    },  index = ['index'] )\n                )\n            \n            #Gráfico\n            r_ajustado = eval('results_'+ str(value_store) + '_'+ str(value_dept) + '.rsquared_adj')\n            \n            if ( (np.random.uniform() <= percent_plots) & (r_ajustado > 0) & ( r_ajustado < 1 ) ):\n               \n                exec('y_predict = results_'+ str(value_store) + '_'+ str(value_dept) + '.predict(x_train)')\n                exec('y_predict_test = results_'+ str(value_store) + '_'+ str(value_dept) + '.predict(x_test)')\n                \n                y_predict = ( ((y_predict * Boxcox_lambda) + 1) ** (1/Boxcox_lambda) )\n                y_predict_test = ( ((y_predict_test * Boxcox_lambda) + 1) ** (1/Boxcox_lambda) )\n                \n                y_train2 = df_data_store_dept['Weekly_Sales'][df_data_store_dept.is_test == 0]\n\n\n                df_comparar_lojas = pd.DataFrame({\n                        'tipo': ['predict'] * len(y_predict),\n                        'y' : y_predict,\n                        'date': df_data_store_dept['Date'][df_data_store_dept.is_test == 0]\n                    })\\\n                .append(\n                    pd.DataFrame({\n                        'tipo': ['y_value'] * len(y_train2),\n                        'y' : y_train2,\n                        'date': df_data_store_dept['Date'][df_data_store_dept.is_test == 0]\n                    })\n                )\\\n                .append(\n                        pd.DataFrame({\n                            'tipo': ['future'] * len(y_predict_test),\n                            'y' : y_predict_test,\n                            'date': df_data_store_dept['Date'][df_data_store_dept.is_test == 1]\n                        })\n                ).reset_index().drop(['index'], 1)\n                \n                print(eval('results_'+ str(value_store) + '_'+ str(value_dept) + '.rsquared'))\n                sns.lineplot(data = df_comparar_lojas, y=\"y\", x = \"date\", hue = \"tipo\").set(title = \"Vendas: Loja \" + str(value_store) + \". Departamento: \" + str(value_dept) )\n                plt.show()\n\n        except Exception as erro_except:\n            print(erro_except)\n                \n## Score                 \nq1 = np.percentile(df_parameters.rsquared_adj , 25 )\nq2 = np.percentile(df_parameters.rsquared_adj , 50 )\nq3 = np.percentile(df_parameters.rsquared_adj , 75 )\nq4 = np.percentile(df_parameters.rsquared_adj , 90 )\n\ndf_parameters['score'] = [ 'D' if value < q1 else 'C' if value < q2 else 'B' if value < q3 else 'A' if value < q4 else 'A+' if value >= q4 else 'NA' for row, value in enumerate(df_parameters.rsquared_adj) ]\n","metadata":{"execution":{"iopub.status.busy":"2022-06-10T02:24:01.839399Z","iopub.execute_input":"2022-06-10T02:24:01.840734Z","iopub.status.idle":"2022-06-10T02:25:33.117271Z","shell.execute_reply.started":"2022-06-10T02:24:01.840683Z","shell.execute_reply":"2022-06-10T02:25:33.116158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_parameters","metadata":{"execution":{"iopub.status.busy":"2022-06-10T02:25:33.119094Z","iopub.execute_input":"2022-06-10T02:25:33.119975Z","iopub.status.idle":"2022-06-10T02:25:33.145531Z","shell.execute_reply.started":"2022-06-10T02:25:33.119925Z","shell.execute_reply":"2022-06-10T02:25:33.144607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}