{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install tensorflow==1.15.0","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\n%matplotlib inline\n\nimport tensorflow as tf\nfrom tqdm import tqdm\nprint(tf.__version__)\n\n# Make numpy values easier to read.\nnp.set_printoptions(precision=3, suppress=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = \"/kaggle/input/walmart-recruiting-store-sales-forecasting/\"\ndataset = pd.read_csv(path + \"train.csv.zip\", names=['Store','Dept','Date','weeklySales','isHoliday'],sep=',', header=0)\nfeatures = pd.read_csv(path + \"features.csv.zip\",sep=',', header=0,\n                       names=['Store','Date','Temperature','Fuel_Price','MarkDown1','MarkDown2','MarkDown3','MarkDown4',\n                              'MarkDown5','CPI','Unemployment','IsHoliday']).drop(columns=['IsHoliday'])\nstores = pd.read_csv(path + \"stores.csv\", names=['Store','Type','Size'],sep=',', header=0)\ndataset = dataset.merge(stores, how='left').merge(features, how='left')\n\ndataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales = dataset.groupby(['Dept', 'Date', 'Store'])['weeklySales'].sum().unstack()\n\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ndata = scaler.fit_transform(sales).astype(np.float32)\nsales_scaled = pd.DataFrame(data=data, columns=sales.columns, index=sales.index)\n\nsales_complete = sales_scaled[sales_scaled.isna().sum(axis=1) == 0]\nprint(sales_complete.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_new = sales_complete.to_numpy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#%% System Parameters\n# 1. Mini batch size\nmb_size = 128\n# 2. Missing rate\n#p_miss = 0.2\n# 3. Hint rate\np_hint = 0.9\n# 4. Loss Hyperparameters\nalpha = 10\n# 5. Train Rate\ntrain_rate = 0.8\n\n# Parameters\nNo, Dim = sales_complete.shape\n\n# Hidden state dimensions\nH_Dim1 = Dim\nH_Dim2 = Dim\n\nprint(Dim)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Prepare the data set\nmiss_rate = 0.2\n\ndef binary_sampler(p, rows, cols):\n    unif_random_matrix = np.random.uniform(0.0, 1.0, size=[rows, cols])\n    binary_random_matrix = (unif_random_matrix < p).astype(np.float32)\n    return binary_random_matrix\n\ndata_m = binary_sampler(1-miss_rate, No, Dim)\nmiss_data_x = sales_complete.copy().to_numpy()\nmiss_data_x[data_m == 0] = 123.456 # np.nan will create error in X*M\ntrain_dataset = tf.data.Dataset.from_tensor_slices(miss_data_x).batch(mb_size, drop_remainder=True)\nmaskset = tf.data.Dataset.from_tensor_slices(data_m).batch(mb_size, drop_remainder=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Missing = data_m","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def normalization (data):\n    '''Normalize data in [0, 1] range.\n\n    Args:\n    - data: original data\n\n    Returns:\n    - norm_data: normalized data\n    - norm_parameters: min_val, max_val for each feature for renormalization\n    '''\n\n    # Parameters\n    _, dim = data.shape\n    norm_data = data.copy()\n\n    # MixMax normalization\n    min_val = np.zeros(dim)\n    max_val = np.zeros(dim)\n\n    # For each dimension\n    for i in range(dim):\n        min_val[i] = np.nanmin(norm_data[:,i])\n        norm_data[:,i] = norm_data[:,i] - np.nanmin(norm_data[:,i])\n        max_val[i] = np.nanmax(norm_data[:,i])\n        norm_data[:,i] = norm_data[:,i] / (np.nanmax(norm_data[:,i]) + 1e-6)   \n\n    # Return norm_parameters for renormalization\n    norm_parameters = {'min_val': min_val,\n                     'max_val': max_val}\n\n    return norm_data, norm_parameters","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"norm_data,norm_parameters=normalization(data_new)\nnorm_data_x = np.nan_to_num(norm_data, 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"norm_parameters","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#%% Train Test Division    \n   \nidx = np.random.permutation(No)\n#idx=list(idx)\n\nTrain_No = int(No * train_rate)\nTest_No = No - Train_No\nbatch_idx=idx[:Train_No]\n\n# Train / Test Features\ntrainX = norm_data_x #use entire dataset for training\ntestX = norm_data_x #supply the entire data as test data \n\n# Train / Test Missing Indicators\ntrainM = Missing\ntestM = Missing #supply entire missing mask","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 1. Xavier Initialization Definition\ndef xavier_init(size):\n    in_dim = size[0]\n    xavier_stddev = 1. / tf.sqrt(in_dim / 2.)\n    return tf.random_normal(shape = size, stddev = xavier_stddev)\n    \n# Hint Vector Generation\ndef sample_M(m, n, p):\n    A = np.random.uniform(0., 1., size = [m, n])\n    B = A > p\n    C = 1.*B\n    return C","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#%% GAIN Architecture   \n   \n#%% 1. Input Placeholders\n# 1.1. Data Vector\nX = tf.placeholder(tf.float32, shape = [None, Dim])\n# 1.2. Mask Vector \nM = tf.placeholder(tf.float32, shape = [None, Dim])\n# 1.3. Hint vector\nH = tf.placeholder(tf.float32, shape = [None, Dim])\n# 1.4. X with missing values\nNew_X = tf.placeholder(tf.float32, shape = [None, Dim])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#%% 2. Discriminator\nD_W1 = tf.Variable(xavier_init([Dim*2, H_Dim1]), name=\"D_W1\")     # Data + Hint as inputs\nD_b1 = tf.Variable(tf.zeros(shape = [H_Dim1]), name=\"D_b1\")\n\nD_W2 = tf.Variable(xavier_init([H_Dim1, H_Dim2]), name=\"D_W2\")\n#D_W2 = tf.Variable(xavier_init([11, 38]), name=\"D_W2\")\nD_b2 = tf.Variable(tf.zeros(shape = [H_Dim2]), name=\"D_b2\")\n#D_b2 = tf.Variable(tf.zeros(shape = [38]), name=\"D_b2\")\n\nD_W3 = tf.Variable(xavier_init([H_Dim2, Dim]), name=\"D_W3\")\n#D_W3 = tf.Variable(xavier_init([38, Dim]), name=\"D_W3\")\nD_b3 = tf.Variable(tf.zeros(shape = [Dim]), name=\"D_b3\")       # Output is multi-variate\n\ntheta_D = [D_W1, D_W2, D_W3, D_b1, D_b2, D_b3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#%% 3. Generator\nG_W1 = tf.Variable(xavier_init([Dim*2, H_Dim1]), name=\"G_W1\")     # Data + Mask as inputs (Random Noises are in Missing Components)\nG_b1 = tf.Variable(tf.zeros(shape = [H_Dim1]), name=\"G_b1\")\n\nG_W2 = tf.Variable(xavier_init([H_Dim1, H_Dim2]), name=\"G_W2\")\n#G_W2 = tf.Variable(xavier_init([11, 38]), name=\"G_W2\")\nG_b2 = tf.Variable(tf.zeros(shape = [H_Dim2]), name=\"G_b2\")\n#G_b2 = tf.Variable(tf.zeros(shape = [38]), name=\"G_b2\")\n\nG_W3 = tf.Variable(xavier_init([H_Dim2, Dim]), name=\"G_W3\")\n#G_W3 = tf.Variable(xavier_init([38, Dim]), name=\"G_W3\")\nG_b3 = tf.Variable(tf.zeros(shape = [Dim]), name=\"G_b3\")\n\ntheta_G = [G_W1, G_W2, G_W3, G_b1, G_b2, G_b3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#%% GAIN Function\n\n#%% 1. Generator\ndef generator(new_x,m):\n    inputs = tf.concat(axis = 1, values = [new_x,m])  # Mask + Data Concatenate\n    G_h1 = tf.nn.relu(tf.matmul(inputs, G_W1) + G_b1)\n    G_h2 = tf.nn.relu(tf.matmul(G_h1, G_W2) + G_b2)   \n    G_prob = tf.nn.sigmoid(tf.matmul(G_h2, G_W3) + G_b3) # [0,1] normalized Output\n    \n    return G_prob\n    \n#%% 2. Discriminator\ndef discriminator(new_x, h):\n    inputs = tf.concat(axis = 1, values = [new_x,h])  # Hint + Data Concatenate\n    D_h1 = tf.nn.relu(tf.matmul(inputs, D_W1) + D_b1)  \n    D_h2 = tf.nn.relu(tf.matmul(D_h1, D_W2) + D_b2)\n    D_logit = tf.matmul(D_h2, D_W3) + D_b3\n    D_prob = tf.nn.sigmoid(D_logit)  # [0,1] Probability Output\n    \n    return D_prob","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#%% 3. Other functions\n# Random sample generator for Z\ndef sample_Z(m, n):\n    return np.random.uniform(0., 0.01, size = [m, n])        \n\n# Mini-batch generation\ndef sample_idx(m, n):\n    A = np.random.permutation(m)\n    idx = A[:n]\n    return idx","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def renormalization (norm_data, norm_parameters):\n    '''Renormalize data from [0, 1] range to the original range.\n\n    Args:\n    - norm_data: normalized data\n    - norm_parameters: min_val, max_val for each feature for renormalization\n\n    Returns:\n    - renorm_data: renormalized original data\n    '''\n\n    min_val = norm_parameters['min_val']\n    max_val = norm_parameters['max_val']\n\n    _, dim = norm_data.shape\n    renorm_data = norm_data.copy()\n\n    for i in range(dim):\n        renorm_data[:,i] = renorm_data[:,i] * (max_val[i] + 1e-6)   \n        renorm_data[:,i] = renorm_data[:,i] + min_val[i]\n\n    return renorm_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def rounding (imputed_data, data_x):\n    '''Round imputed data for categorical variables.\n\n    Args:\n    - imputed_data: imputed data\n    - data_x: original data with missing values\n\n    Returns:\n    - rounded_data: rounded imputed data\n    '''\n\n    _, dim = data_x.shape\n    rounded_data = imputed_data.copy()\n\n    for i in range(dim):\n        temp = data_x[~np.isnan(data_x[:, i]), i]\n        # Only for the categorical variable\n        if len(np.unique(temp)) < 100:\n              rounded_data[:, i] = np.round(rounded_data[:, i])\n\n    return rounded_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#%% Structure\n# Generator\nG_sample = generator(New_X,M)\n\n# Combine with original data\nHat_New_X = New_X * M + G_sample * (1-M)\n\n# Discriminator\nD_prob = discriminator(Hat_New_X, H)\n\n#%% Loss\nD_loss1 = -tf.reduce_mean(M * tf.log(D_prob + 1e-8) + (1-M) * tf.log(1. - D_prob + 1e-8)) \nG_loss1 = -tf.reduce_mean((1-M) * tf.log(D_prob + 1e-8))\nMSE_train_loss = tf.reduce_mean((M * New_X - M * G_sample)**2) / tf.reduce_mean(M)\n\nD_loss = D_loss1\nG_loss = G_loss1 + alpha * MSE_train_loss \n\n#%% MSE Performance metric\nMSE_test_loss = tf.reduce_mean(((1-M) * X - (1-M)*G_sample)**2) / tf.reduce_mean(1-M)\n\n#%% Solver\nD_solver = tf.train.AdamOptimizer().minimize(D_loss, var_list=theta_D)\nG_solver = tf.train.AdamOptimizer().minimize(G_loss, var_list=theta_G)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Sessions\nimport time\nsess1 = tf.Session()\nsess1.run(tf.global_variables_initializer())\n\n#%% Iterations\ntrain_losses = []\ntest_losses = []\n\n#%% Start Iterations\nt=time.time()\nfor it in tqdm(range(1000)):    \n    \n    #%% Inputs\n    mb_idx = sample_idx(Train_No, mb_size)\n    X_mb = trainX[mb_idx,:]  \n    #print(X_mb.shape)\n    \n    Z_mb = sample_Z(mb_size, Dim) \n#     M_mb = trainM[mb_idx,:]  \n    M_mb = trainM[:mb_size, :]\n    #print(M_mb.shape)\n    H_mb1 = sample_M(mb_size, Dim, 1-p_hint)\n    H_mb = M_mb * H_mb1 #Hint matrix\n    \n    New_X_mb = M_mb * X_mb+(1-M_mb) * Z_mb  # random value z in inserted in place of missing Data\n    #print(\"Missing data shape\",New_X_mb.shape)\n    \n    _, D_loss_curr = sess1.run([D_solver, D_loss1], feed_dict = {M: M_mb, New_X: New_X_mb, H: H_mb})\n    _, G_loss_curr, MSE_train_loss_curr, MSE_test_loss_curr = sess1.run([G_solver, G_loss1, MSE_train_loss, MSE_test_loss],\n                                                                       feed_dict = {X: X_mb, M: M_mb, New_X: New_X_mb, H: H_mb})\n    #print('Train loss: ',np.sqrt(MSE_train_loss_curr), 'Test loss: ', np.sqrt(MSE_test_loss_curr))\n    train_losses.append(np.sqrt(MSE_train_loss_curr))\n    test_losses.append(np.sqrt(MSE_test_loss_curr))\nprint(\"Test dataset imputation\")    \n#%% Final Loss\nt_final=time.time()    \nZ_mb = sample_Z(len(norm_data), Dim) \nM_mb = testM\nX_mb = norm_data\n        \nNew_X_mb = M_mb * X_mb + (1-M_mb) * Z_mb  # Missing Data Introduce\n    \n    \nMSE_final,imputed_data = sess1.run([MSE_test_loss,G_sample], feed_dict = {X:X_mb , M: testM, New_X: New_X_mb})\nimputed_data = testM * norm_data + (1-testM) * imputed_data\n\n # Renormalization\nimputed_data = renormalization(imputed_data, norm_parameters)  \n        \n# Rounding\n#data=data.to_numpy()\nimputed_data = rounding(imputed_data, data_new)  \nprint('Final Test RMSE: ' + str(np.sqrt(MSE_final)))\nprint('Time cost: ',t_final-t)\n#sess1.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_new.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# How to split into 6 subsets with ~50% overlap?\n# Subset 1: 0 - 11\n# Subset 2: 6 - 17\n# Subset 3: 12 - 23\n# Subset 4: 18 - 29\n# Subset 5: 24 - 35\n# Subset 6: 30 - 41\n# Remove 42 - 44\n\n# 12 features for each subset, and 6 features overlap\n# Number of instances: 4417 / 6 ~= 700\n\n# All is needed: 1. split data_new and Missing into 6 subsets. 2. repeat the above tests.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}