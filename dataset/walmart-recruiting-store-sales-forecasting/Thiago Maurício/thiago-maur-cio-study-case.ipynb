{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom scipy import stats\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor, RandomForestRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.feature_selection import RFE\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import KFold\n\nsns.set(rc=({'figure.figsize':(11,15)}))\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#-- importing files\ndb_features = pd.read_csv('/kaggle/input/walmart-recruiting-store-sales-forecasting/features.csv.zip')\ndb_sampleSubmission = pd.read_csv('/kaggle/input/walmart-recruiting-store-sales-forecasting/sampleSubmission.csv.zip')\ndb_stores = pd.read_csv('/kaggle/input/walmart-recruiting-store-sales-forecasting/stores.csv')\ndb_test = pd.read_csv('/kaggle/input/walmart-recruiting-store-sales-forecasting/test.csv.zip')\ndb_train = pd.read_csv('/kaggle/input/walmart-recruiting-store-sales-forecasting/train.csv.zip')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Analyzing Features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#-- printing head\ndb_features.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#-- checking db dimension\nprint(f'Rows: {db_features.shape[0]}')\nprint(f'\\nColumns: {db_features.shape[1]}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#-- checking features type\ndb_features.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Date field was not recognized as data, let's convert it","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#-- converting date field\ndb_features['Date'] = db_features['Date'].apply(pd.to_datetime)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#-- checking convertion\ndb_features.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Date field converted","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#-- checking missing values\ndb_features.isnull().sum().sort_values(ascending=False).to_frame() / len(db_features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> MarkDownn fields are filled after Nov 2011, that's something we must check after. On the other hand, there are a lot of missing values. Unemployment and CPI have missing values, but we can just remove the lines.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#-- checking data\ndb_features.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Features seems to be consistent","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#-- printing date values\nprint(f\"Min Date value: {min(db_features['Date'])}\")\nprint(f\"\\nMax Date value: {max(db_features['Date'])}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#-- checking % of registers before Nov 2011\nprint(\"Values which will be removed, considering markdown availability: \" +\n      f\"{len(db_features[db_features['Date'] < '2011-12-01']) / len(db_features):.2f}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Analyzing Stores","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#-- printing head\ndb_stores.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#-- checking db dimension\nprint(f'Rows: {db_stores.shape[0]}')\nprint(f'\\nColumns: {db_stores.shape[1]}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#-- checking features type\ndb_stores.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#-- checking missing values\ndb_stores.isnull().sum().sort_values(ascending=False).to_frame() / len(db_stores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#-- checking data\ndb_stores.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#-- counting stores types\ndb_stores['Type'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Analyzing Train","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#-- printing head\ndb_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#-- checking db dimension\nprint(f'Rows: {db_train.shape[0]}')\nprint(f'\\nColumns: {db_train.shape[1]}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#-- checking features type\ndb_train.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Same issue, date field is been considerde as object. Let's convert it","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#-- converting date field\ndb_train['Date'] = db_train['Date'].apply(pd.to_datetime)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#-- checking convertion\ndb_train.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Convertion OK!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#-- checking missing values\ndb_train.isnull().sum().sort_values(ascending=False).to_frame() / len(db_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#-- checking data\ndb_train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#-- grouping stores\ndb_train_g = db_train.groupby(['Store', 'Dept'])['Dept'].count().to_frame().rename(columns={'Dept':'count'})\ndb_train_g.reset_index(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#-- checking number of departments by Store\ndb_train_g['Store'].value_counts().to_frame().sort_index()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Number of department are not the same by store","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#-- merging db_train + db_store\ndb_train_store = pd.merge(left=db_train, right=db_stores, on='Store', how='left')\ndb_train_store.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#-- grouping weekly sales by store type\ndb_train_store_g = db_train_store.groupby(['Date', 'Type'])['Weekly_Sales'].sum().reset_index()\ndb_train_store_g.index = db_train_store_g['Date']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#-- ploting seasonal for stores A\nresult_a = seasonal_decompose(db_train_store_g[db_train_store_g['Type'] == 'A']['Weekly_Sales'], model='additive')\n\nfig, (ax1,ax2,ax3, ax4) = plt.subplots(4,1, figsize=(12,8))\nresult_a.observed.plot(ax=ax1)\nresult_a.trend.plot(ax=ax2)\nresult_a.seasonal.plot(ax=ax3)\nresult_a.resid.plot(ax=ax4)\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#-- ploting seasonal for stores B\nresult_b = seasonal_decompose(db_train_store_g[db_train_store_g['Type'] == 'B']['Weekly_Sales'], model='additive')\n\nfig, (ax1,ax2,ax3, ax4) = plt.subplots(4,1, figsize=(12,8))\nresult_b.observed.plot(ax=ax1)\nresult_b.trend.plot(ax=ax2)\nresult_b.seasonal.plot(ax=ax3)\nresult_b.resid.plot(ax=ax4)\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#-- ploting seasonal for stores C\nresult_c = seasonal_decompose(db_train_store_g[db_train_store_g['Type'] == 'C']['Weekly_Sales'], model='additive')\n\nfig, (ax1,ax2,ax3, ax4) = plt.subplots(4,1, figsize=(12,8))\nresult_c.observed.plot(ax=ax1)\nresult_c.trend.plot(ax=ax2)\nresult_c.seasonal.plot(ax=ax3)\nresult_c.resid.plot(ax=ax4)\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Analyzig the plots above, we can notice a difference in the total sell by store type. \n- All kind of stores presents a tendence to increase sales. \n- We can notice sazonality too, as black fridays and periods before Christmas.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#-- function to define stationarity\ndef estacionario(x):\n    \"\"\"Função para avaliar se os dados são estacionários\"\"\"\n    \n    if adfuller(x)[1] <= 0.05:\n        print('Conjunto de dados é estacionário')\n        print(f'Número de lags utilizado: {adfuller(x)[2]}')\n    else:\n        print('Conjuntos de dados não é estacionário')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#-- function to define normality\ndef normal(x):\n    \"\"\"Função para avaliar normalidade\"\"\"\n    p_normal = stats.shapiro(x)[1]\n    \n    if p_normal >= 0.05:\n        print(f'Dados seguem uma distribuição normal - p_value = {p_normal:.2}')\n    else:\n        print(f'Dados não seguem uma distribuição normal - p_value = {p_normal:.2}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Irei testar se os dados são estacionários","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#-- stationarity test\nestacionario(db_train_store_g[db_train_store_g['Type'] == 'A']['Weekly_Sales'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#-- stationarity test\nestacionario(db_train_store_g[db_train_store_g['Type'] == 'B']['Weekly_Sales'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#-- stationarity test\nestacionario(db_train_store_g[db_train_store_g['Type'] == 'C']['Weekly_Sales'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Only C stores are not stacionary. This is important if I'll use ARIMA's model.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"> I'll test normality into the data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#-- normality teste\nnormal(db_train_store_g[db_train_store_g['Type'] == 'A']['Weekly_Sales'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#-- normality teste\nnormal(db_train_store_g[db_train_store_g['Type'] == 'B']['Weekly_Sales'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#-- normality teste\nnormal(db_train_store_g[db_train_store_g['Type'] == 'C']['Weekly_Sales'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The databases are not normal, so I can not calculate confidential interval.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Merging dbs","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#-- merging train_stores + features\ndb_train_store_features = pd.merge(left=db_train_store, right=db_features, on=['Store', 'Date'], how='left', suffixes=('_train', '_features'))\ndb_train_store_features.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"db_train_store_features.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#-- creating db bi store type\ntype_a = db_train_store_features.loc[db_train_store_features['Type'] == 'A']\ntype_b = db_train_store_features.loc[db_train_store_features['Type'] == 'B']\ntype_c = db_train_store_features.loc[db_train_store_features['Type'] == 'C']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#-- EDA\n_ = sns.pairplot(db_train_store_features_n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With the figure above, it is possible to verify:\n- some unmployment rate has more weekly sales, temperature and store size","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#-- checking correlation between features\n_ = plt.clf()\n_ = plt.style.use('fivethirtyeight')\n_ = font_opts = {'fontsize':15, 'fontweight':'bold'}\n_ = plt.figure(figsize=(20,10))\n\nx = sns.heatmap(\n    db_train_store_features.corr(), \n    annot=db_train_store_features.corr(), \n    fmt='.2f', \n    annot_kws={'fontsize':10, 'fontweight':'bold'},\n    cmap='RdPu'\n)\n\n_ = plt.title(\"Correlation Matrix\\n\", **font_opts)\n_ = plt.xticks(**font_opts)\n_ = plt.yticks(**font_opts)\n\n\n_ = plt.tight_layout();\n_ = plt.plot();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Store size plays a little role in weekly sales","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## MLI - BaseLine","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"- The data set has missing values and I didn't test for outliers (due to restrict time to solve this case). So, I'm gonna use random forest model, because it is not affect by those issues.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#-- converting strings to number\ndb_train_store_features.Type = db_train_store_features.Type.map({'A':1, 'B':2, 'C':3})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#-- selecting features\nX_train = db_train_store_features[['Store','Dept','IsHoliday_train','Size', 'Type']]\nY_train = db_train_store_features['Weekly_Sales']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#-- creating function to ml\ndef random_forest(n_estimators, max_depth):\n    resultado = []\n    for estimator in n_estimators:\n        for depth in max_depth:\n            wmaes = []\n            for i in range(1,5):\n                print('k:', i, ', n_estimators:', estimator, ', max_depth:', depth)\n                x_train, x_test, y_train, y_test = train_test_split(X_train, Y_train, test_size=0.3, random_state=0)\n                ml_rf = RandomForestRegressor(n_estimators=estimator, max_depth=depth, random_state=0)\n                ml_rf.fit(x_train, y_train)\n                predicted = ml_rf.predict(x_test)\n                wmaes.append(wmae(x_test, y_test, predicted))\n            print('WMAE:', np.mean(wmaes))\n            resultado.append({'Max_Depth': depth, 'Estimators': estimator, 'WMAE': np.mean(wmaes)})\n    return pd.DataFrame(resultado)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#-- creating fuction to calculate error\ndef wmae(db, y, y_h):\n    weights = db.IsHoliday_train.apply(lambda x: 5 if x else 1)\n    return np.round(np.sum(weights*abs(y-y_h))/(np.sum(weights)), 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#-- applying ml\nn_estimators = [56, 58, 60]\nmax_depth = [25, 27, 30]\n\nrandom_forest(n_estimators, max_depth)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature engineering","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#-- creating columns based on date, I'll test if those columns will improve models evaluation\ndb_train_store_features['week'] = db_train_store_features['Date'].dt.week\ndb_train_store_features['month'] = db_train_store_features['Date'].dt.month\ndb_train_store_features['day_year'] = db_train_store_features['Date'].dt.dayofyear","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#-- selecting features\nX_train = db_train_store_features[['Store','Dept','IsHoliday_train','Size', 'Type', 'week', 'month', 'day_year']]\nY_train = db_train_store_features['Weekly_Sales']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## MLII","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#-- running second model\nn_estimators = [56, 58, 60]\nmax_depth = [25, 27, 30]\n\nrandom_forest(n_estimators, max_depth)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- This model, with featuring engineering was considered improved in comparison with first model. So, I'll use it.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#-- defining model to use in test db\nml_rf = RandomForestRegressor(n_estimators=60, max_depth=27, max_features=6, min_samples_split=3, min_samples_leaf=1, random_state=0)\nml_rf.fit(X_train, Y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Applying into test db","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#-- printing head\ndb_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#-- checking data\ndb_test.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#-- converting date field\ndb_test.Date = db_test.Date.apply(pd.to_datetime)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#-- checking convertion\ndb_test.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#-- merging db_train + db_store\ndb_train_store = pd.merge(left=db_train, right=db_stores, on='Store', how='left')\ndb_train_store.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#-- merging dbs\ndb_test_store = pd.merge(left=db_test, right=db_stores, on='Store', how='left' )\ndb_test_store.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#-- merging dbs\ndb_test_store_features = pd.merge(left=db_test_store, right=db_features, on=['Store', 'Date'], how='left', suffixes=('_train', '_features'))\ndb_test_store_features.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#-- converting\ndb_test_store_features.Type = db_test_store_features.Type.map({'A':1, 'B':2, 'C':3})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#-- creating same features\ndb_test_store_features['week'] = db_test_store_features['Date'].dt.week\ndb_test_store_features['month'] = db_test_store_features['Date'].dt.month\ndb_test_store_features['day_year'] = db_test_store_features['Date'].dt.dayofyear","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#-- selecting features\nX_test = db_test_store_features[['Store','Dept','IsHoliday_train','Size', 'Type', 'week', 'month', 'day_year']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#-- running prediction\npredict = ml_rf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#-- print prediction\nprint(predict)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Creating file to save results","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#-- concat store + dept + date\ndb_test_store_features['Id'] = db_test_store_features['Store'].astype(str) + '_' + db_test_store_features['Dept'].astype(str) + '_' + db_test_store_features['Date'].astype(str)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#-- concat ID + Prediction\ndb_Submission = pd.concat([db_test_store_features['Id'], pd.DataFrame(predict)], axis=1)\ndb_Submission.columns = ['Id', 'Weekly_Sales']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#-- saving results into .csv\ndb_Submission.to_csv('walmart_thiago_mauricio.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conclusions\n1. some unmployment rate has more weekly sales, temperature and store size\n2. store size has a little influence in weekly sales\n3. week, month and day of the year have a strong impact in model's prediction","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Next steps\n1. Fill missing values\n2. User more featuring engineering as: difference between weekly sales and moving average\n3. Use cross validation\n4. Calculate feature importance\n5. Test a different algorithm\n6. Test splitting data by Store Type (A, B and C) ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}