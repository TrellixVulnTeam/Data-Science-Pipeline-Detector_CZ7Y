{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Modelo de Previsão de Demanda\n**Autora:** Gué Cardoso\n\nSabemos que num mercado competitivo, é sempre importante estarmos à frente dos nossos concorrentes. E para garantir certa vantagem, seria de bom tom **alinhar a necessidades dos clientes**, **estoque** e **demanda de mercado**. Aí que os modelos de previsão de demanda entram. Tome como exemplo o Natal ou até mesmo as semanas que antecedem esta data. Com base só na nossa experiência, sabemos que algo muda ali, que o mercado fica muito mais aquecido e a procura por certos produtos aumentam consideravelmente. \n\n\nE ao invés de tomarmos decisões estratégicas baseada nos *achismos* ou as experiências vividas no passar dos anos, não seria mais seguro entender sua base histórica e criar mecanismos que auxiliem na tomada de decisão de forma a impactar no resultado final? O Forecasting não só é útil na parte de controle de estoque, mas também pode trazer uma visão sobre comportamento de compra, informações sobre a jornada de compra dos clientes.\n\n\nNas próximas linhas será apresentada uma solução simples para o problema de Store Sales Forecasting tendo como base 45 lojas do Walmart alocadas em diferentes regiões, cada uma contendo seus departamentos, suas promoções, seus tamanhos, etc. O desafio? Criar predições de vendas semanais.\n\n\nA ideia inicial foi de criar um **modelo base** e a partir dele utiliar técnicas de **feature engineering** para melhorar tal modelo, pensando num esquema de CRISP-DM","metadata":{}},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-30T04:52:49.184594Z","iopub.execute_input":"2022-05-30T04:52:49.184984Z","iopub.status.idle":"2022-05-30T04:52:49.192429Z","shell.execute_reply.started":"2022-05-30T04:52:49.184952Z","shell.execute_reply":"2022-05-30T04:52:49.191434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Imports das bibliotecas que serão utilizadas","metadata":{}},{"cell_type":"code","source":"import pickle\nimport numpy as np\nimport pandas as pd\npd.options.display.float_format = '{:,.2f}'.format\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.style.use('Solarize_Light2') #plt.style.available\n%matplotlib inline\n\n\nfrom sklearn import metrics \nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split \n\npath = \"../input/walmart-recruiting-store-sales-forecasting\"\n","metadata":{"execution":{"iopub.status.busy":"2022-05-30T04:52:52.474929Z","iopub.execute_input":"2022-05-30T04:52:52.475356Z","iopub.status.idle":"2022-05-30T04:52:52.485137Z","shell.execute_reply.started":"2022-05-30T04:52:52.475323Z","shell.execute_reply":"2022-05-30T04:52:52.484392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Definição de funções \n* **TODO** criar arquivo .py com todas as funções, importar junto com as outras bibliotecas e chamar as funções no pipeline","metadata":{}},{"cell_type":"code","source":"def read_create_merge_data(filelist):\n    '''\n    função que faz a leitura dos dados do diretório\n    e cria um dataframe único fazendo merging dos dados\n    de loja, treino e features\n    '''\n    data = pd.read_csv(filelist[0])\n    aux =  pd.read_csv(filelist[1])\n    data = data.merge(aux, how='left', on='Store')\n    \n    aux =  pd.read_csv(filelist[2])\n    data = data.merge(aux, how='left', on=['Store','Date','IsHoliday'])\n    \n    return data\n\n\ndef df_filter(dataframe, col_name,col_value):\n    '''função para filtro do dataframe'''\n    return dataframe[dataframe[col_name]==col_value].reset_index(drop=True)\n\n\ndef create_rf_model(X,y):\n    #Create model\n    regr = RandomForestRegressor(n_estimators=100, random_state=42)\n    regr.fit(X_train,y_train)\n    return regr\n\n\ndef save_model(pkl_filename, rf_model):\n    '''função para salvar o modelo em disco'''\n    # saving the trained model to disk \n    pickle.dump(rf_model, open(pkl_filename, 'wb'))\n    return \"Saved model to disk\"\n\n\ndef treat_data(df):\n    '''\n    função para extrair ano, mes, dia e semana do campo data\n    '''\n    df[['year','month','day']] = df['Date'].str.split('-',expand=True)\n    df['year'] = df['year'].astype(int)\n    df['month'] = df['month'].astype(int)\n    df['day'] = df['day'].astype(int)\n    df['week'] = (((df['month']-1) * 30) + df['day'])//7\n    return df\n\n\n\ndef plot_heatmap(dataframe):\n    sns.heatmap(dataframe.corr(),annot=True,cmap='winter_r',linewidths=0.2) \n    fig=plt.gcf()\n    fig.set_size_inches(13,9)\n    return plt.show()\n\n\ndef plot_bar(dataframe, xCol_name, yCol_name):\n    plt.figure(figsize=(20,8))\n    sns.barplot(x=xCol_name, y=yCol_name, data=dataframe)\n    plt.grid()\n    plt.title('Média '+str(yCol_name), fontsize=18)\n    plt.ylabel(xCol_name, fontsize=16)\n    plt.xlabel(yCol_name, fontsize=16)\n    return plt.show()\n\n\ndef get_holidays_df():\n    '''cria dataframe com os feriados fornecidos'''\n    holidays = {\n    'Super Bowl': ['2010-02-12', '2011-02-11', '2012-02-10', '2013-02-08'],\n    'Labor Day': ['2010-09-10', '2011-09-09', '2012-09-07', '2013-09-06'],\n    'Thanksgiving': ['2010-11-26', '2011-11-25', '2012-11-23', '2013-11-29'],\n    'Christmas': ['2010-12-31', '2011-12-30', '2012-12-28', '2013-12-27']\n    }\n    holidays_df = pd.DataFrame(holidays)\n    holidays_df = pd.melt(holidays_df)\n\n    holidays_df.columns = ['Holiday','Date']\n    return holidays_df\n\n\ndef get_holidays_near_features(df):\n    # TODO Deixar parametrizado (coluna e tamanho janela)\n    '''\n    função que pega semanas antes do feriado\n    '''\n    \n    df['Holiday_Christmas_2'] = df['Holiday_Christmas']\\\n                            + df['Holiday_Christmas'].shift(-1)\\\n                            + df['Holiday_Christmas'].shift(-2)\\\n                            + df['Holiday_Christmas'].shift(-3)\n    df['Holiday_Christmas_2'] = df['Holiday_Christmas_2'].fillna(0)\n\n    df['Holiday_Labor Day2'] = df['Holiday_Labor Day']\\\n                                + df['Holiday_Labor Day'].shift(-1)\\\n                                + df['Holiday_Labor Day'].shift(-2)\\\n                                + df['Holiday_Labor Day'].shift(-3)\n    df['Holiday_Labor Day2'] = df['Holiday_Labor Day2'].fillna(0)\n\n    df['Holiday_Super Bowl2'] = df['Holiday_Super Bowl']\\\n                                + df['Holiday_Super Bowl'].shift(-1)\\\n                                + df['Holiday_Super Bowl'].shift(-2)\\\n                                + df['Holiday_Super Bowl'].shift(-3)\n    df['Holiday_Super Bowl2'] = df['Holiday_Super Bowl2'].fillna(0)\n\n    df['Holiday_Thanksgiving2'] = df['Holiday_Thanksgiving']\\\n                                + df['Holiday_Thanksgiving'].shift(-1)\\\n                                + df['Holiday_Thanksgiving'].shift(-2)\\\n                                + df['Holiday_Thanksgiving'].shift(-3)\n    df['Holiday_Thanksgiving2'] = df['Holiday_Thanksgiving2'].fillna(0)\n\n    return df\n\n\ndef get_shift_sales(df, weeks):\n    '''\n    função que faz shift das vendas em N semanas e obem estes valores\n    retorna o dataframe add tais dados em novas colunas\n    '''\n    # Pivot Data to get each Weekly_Sales in each Store/Dept by Date\n    df_aux = pd.pivot_table(df, values='Weekly_Sales', index=['Date'],columns=['Store', 'Dept'])\n    # Shifting all Weekly_Sales by N weeks and melting data\n    df_aux = pd.melt(df_aux.shift(weeks).fillna(-1), ignore_index=False)\n    df_aux.columns = ['Store','Dept','Weekly_Sales_aux']\n    # Reseting index to get Data as collumn\n    df_aux = df_aux.reset_index()\n    df_aux = df.merge(df_aux, on = ['Date','Store','Dept'], how = 'left')\n    return df_aux['Weekly_Sales_aux']\n\n\ndef linreg(Y):\n    \"\"\"\n    funçao para achar a, b na função y = ax + b tal que a raiz da média\n    da distância entre a linha de tendência e os pontos originais é minimizada    \n    \"\"\"\n    X = range(len(Y))\n    N = len(X)\n    Sx = Sy = Sxx = Syy = Sxy = 0.0\n    for x, y in zip(X, Y):\n        Sx = Sx + x\n        Sy = Sy + y\n        Sxx = Sxx + x*x\n        Syy = Syy + y*y\n        Sxy = Sxy + x*y\n    det = Sxx * N - Sx * Sx\n    return [(Sxy * N - Sy * Sx)/det, (Sxx * Sy - Sx * Sxy)/det]\n\n\ndef get_regression_linear_pred(df, cols):\n    '''\n    fução que faz a reg linear nas colunas e retorna a, b e predict\n    '''\n    # Applying Linear Regression in last 2 years\n    df_aux = df[cols].apply(lambda x: linreg(x.values),axis=1)\n    df_aux = pd.DataFrame(df_aux.tolist())\n\n    \n    #Geting weekly Sales 1 Year Ago, a and b\n    df_aux.columns = ['Weekly_Sales_1ya_a','Weekly_Sales_1ya_b']\n    df_aux['Linear_Pred_1ya'] = (df_aux['Weekly_Sales_1ya_a'] * len(cols)) + df_aux['Weekly_Sales_1ya_b']\n    \n    return df_aux\n\n\n\ndef evaluate_model(y_test,y_pred):\n    df_pred = pd.DataFrame({'y_true':y_test,'y_pred':y_pred})\n    df_pred.plot()\n    ","metadata":{"execution":{"iopub.status.busy":"2022-05-30T04:52:56.979563Z","iopub.execute_input":"2022-05-30T04:52:56.980188Z","iopub.status.idle":"2022-05-30T04:52:57.013376Z","shell.execute_reply.started":"2022-05-30T04:52:56.980137Z","shell.execute_reply":"2022-05-30T04:52:57.012588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Criação de um modelo inicial (CRISP-DM)\n* Com base na matriz de correlação, escolhi as primeiras features que entrarão no modelo inicial. Nos gráficos de barras de vendas por Loja e por Departamento já é possível ver que as vendas não estão igualmente distribuídas. Existe uma variação ali dependendo da loja/dept, mas seria o ideal criar 1 modelo para cada um dos 99 departamentos existentes?","metadata":{}},{"cell_type":"code","source":"## Leitura, criação do dataframe unindo dados dos arquivos\nfilelist = [path+'/train.csv.zip',path+'/stores.csv',path+'/features.csv.zip']\ndf = read_create_merge_data(filelist)\ndisplay(df.head())\n\n\n## Gera matriz de correlação\nprint('\\n\\n')\nplot_heatmap(df)\n\n\n## Gera gráfico de barras\nprint('\\n\\n')\nplot_bar(df,'Store','Weekly_Sales')\nplot_bar(df,'Dept','Weekly_Sales')\n","metadata":{"execution":{"iopub.status.busy":"2022-05-30T04:53:00.131566Z","iopub.execute_input":"2022-05-30T04:53:00.131984Z","iopub.status.idle":"2022-05-30T04:53:16.151629Z","shell.execute_reply.started":"2022-05-30T04:53:00.13195Z","shell.execute_reply":"2022-05-30T04:53:16.150825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Seleção das colunas de interesse\ndata = df[['Store','Dept', 'IsHoliday', 'Size','Unemployment',\"Weekly_Sales\"]]\n\n## seleção de features\nX = data.drop(['Weekly_Sales'], axis=1)\n## seleão do target\ny = pd.DataFrame(data['Weekly_Sales'])\n\n## separação em treino e teste\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 42)\nregr_model = create_rf_model(X,y)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-30T04:53:16.152993Z","iopub.execute_input":"2022-05-30T04:53:16.153579Z","iopub.status.idle":"2022-05-30T04:54:08.161048Z","shell.execute_reply.started":"2022-05-30T04:53:16.153547Z","shell.execute_reply":"2022-05-30T04:54:08.160228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## TODO: Ver qual a melhor métrica a ser utilizada para avaliar o modelo\ny_pred = regr_model.predict(X_test)\n\nprint(\"MAE\" , metrics.mean_absolute_error(y_test, y_pred))\nprint(\"MSE\" , metrics.mean_squared_error(y_test, y_pred))\nprint(\"RMSE\" , np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\nprint(\"R2\" , metrics.explained_variance_score(y_test, y_pred))\nprint(\"MAPE\" ,metrics.mean_absolute_percentage_error(y_test, y_pred))\n\n","metadata":{"execution":{"iopub.status.busy":"2022-05-30T04:54:08.162162Z","iopub.execute_input":"2022-05-30T04:54:08.16249Z","iopub.status.idle":"2022-05-30T04:54:13.549735Z","shell.execute_reply.started":"2022-05-30T04:54:08.162462Z","shell.execute_reply":"2022-05-30T04:54:13.548747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"serie = df[['Weekly_Sales']]\nserie['pred'] = regr_model.predict(X)\nserie.plot()\n\n\nplt.figure()\nfeat_importances = pd.Series(regr_model.feature_importances_, index=X.columns)\nfeat_importances.nlargest(10).plot(kind='barh')","metadata":{"execution":{"iopub.status.busy":"2022-05-30T04:55:22.113407Z","iopub.execute_input":"2022-05-30T04:55:22.113808Z","iopub.status.idle":"2022-05-30T04:55:29.325195Z","shell.execute_reply.started":"2022-05-30T04:55:22.113777Z","shell.execute_reply":"2022-05-30T04:55:29.32418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Engineering e novo modelo","metadata":{}},{"cell_type":"code","source":"## leitura dos dados\ndf = pd.read_csv(path+\"/train.csv.zip\")\n\n## extração dia/ano/mes/semana do campo data e união de dataframes\ndf = treat_data(df)\ndf = df.merge(get_holidays_df(),how='left').fillna(0)\n\n\n# One Hot Encode\ndf = df.join(pd.get_dummies(df['Holiday'], prefix='Holiday'))\ndf = df.drop('Holiday',axis=1)\n\n\n## semanas antes do feriado\ndf = get_holidays_near_features(df)\n\n\n## Tempo minimo de predição\ndf['Weekly_Sales_48'] = get_shift_sales(df, 48)\n## 1 ano antes do tempo minimo para predição\ndf['Weekly_Sales_100'] = get_shift_sales(df, 100)\n## 1 ano antes da semana a ser estimada\ndf['Weekly_Sales_52'] = get_shift_sales(df, 52)\n## 2 anos antes da semana a ser estimada\ndf['Weekly_Sales_104'] = get_shift_sales(df, 104)\n\n\n## regressão linear para achar pontos a, b e predict\ndf[['Weekly_Sales_1ya_a',\n    'Weekly_Sales_1ya_b',\n    'Linear_Pred_1ya']] = get_regression_linear_pred(df, ['Weekly_Sales_104','Weekly_Sales_52'])\ndf.head()\n","metadata":{"execution":{"iopub.status.busy":"2022-05-30T04:55:29.327231Z","iopub.execute_input":"2022-05-30T04:55:29.327671Z","iopub.status.idle":"2022-05-30T04:55:41.499237Z","shell.execute_reply.started":"2022-05-30T04:55:29.327629Z","shell.execute_reply":"2022-05-30T04:55:41.498178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n## separa em treino e teste (1 ano para test)\ndf_train = df[df['Date'] < '2011-10-26']\ndf_test = df[df['Date'] >= '2011-10-26']\n\ny_train = df_train['Weekly_Sales']\nX_train = df_train.drop(['Weekly_Sales','Date'],axis=1)\n\ny_test = df_test['Weekly_Sales']\nX_test = df_test.drop(['Weekly_Sales','Date'],axis=1)\n\nregr = RandomForestRegressor(random_state=0)\nregr.fit(X_train, y_train)\n\ny_pred = regr.predict(X_test)\n\nevaluate_model(y_test,y_pred)","metadata":{"execution":{"iopub.status.busy":"2022-05-30T04:59:59.289901Z","iopub.execute_input":"2022-05-30T04:59:59.290288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ndf_train2 = df_train[:90]\ndf_test2 = df_test[:53]\n\ny_train2 = df_train2['Weekly_Sales']\nX_train2 = df_train2.drop(['Weekly_Sales','Date'],axis=1)\n\ny_test2 = df_test2['Weekly_Sales']\nX_test2 = df_test2.drop(['Weekly_Sales','Date'],axis=1)\n\nregr2 = RandomForestRegressor(random_state=0)\nregr2.fit(X_train2, y_train2)\n\ny_pred2 = regr2.predict(X_test2)\ndf_pred2 = pd.DataFrame({'y_true':y_test2.values,'y_pred':y_pred2})\ndf_pred2.plot()\ndf_pred2.index = y_test2.index\n\nplt.figure()\ndf_plot = df_train2.append(df_test2)\ndf_plot['pred'] = df_pred2['y_pred']\ndf_plot[['Weekly_Sales','pred']].plot()\n\nplt.figure()\nfeat_importances = pd.Series(regr2.feature_importances_, index=X_train.columns)\nfeat_importances.nlargest(30).plot(kind='barh')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## leitura dos dados\ndf_sub = pd.read_csv(path+\"/test.csv.zip\")\n\n## extração dia/ano/mes/semana do campo data e união de dataframes\ndf_sub = treat_data(df_sub)\ndf_sub = df_sub.merge(get_holidays_df(),how='left').fillna(0)\ndf_sub = df_sub.drop('Date', axis=1)\n\n# One Hot Encode\ndf_sub = df_sub.join(pd.get_dummies(df_sub['Holiday'], prefix='Holiday'))\ndf_sub = df_sub.drop('Holiday',axis=1)\ndf_sub.head()\n\ndf_sub['Holiday_Labor Day'] = 0\n\n## semanas antes do feriado\ndf_sub = get_holidays_near_features(df_sub)\n\n\n## Tempo minimo de predição\ndf_sub['Weekly_Sales_48'] = 0\n## 1 ano antes do tempo minimo para predição\ndf_sub['Weekly_Sales_100'] = 0\n## 1 ano antes da semana a ser estimada\ndf_sub['Weekly_Sales_52'] = 0\n## 2 anos antes da semana a ser estimada\ndf_sub['Weekly_Sales_104'] = 0\n\n\n## regressão linear para achar pontos a, b e predict\ndf_sub[['Weekly_Sales_1ya_a',\n    'Weekly_Sales_1ya_b',\n    'Linear_Pred_1ya']] = get_regression_linear_pred(df_sub, ['Weekly_Sales_104','Weekly_Sales_52'])\n\n\ndf_sub.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-30T05:03:51.077602Z","iopub.execute_input":"2022-05-30T05:03:51.077982Z","iopub.status.idle":"2022-05-30T05:03:51.099209Z","shell.execute_reply.started":"2022-05-30T05:03:51.077953Z","shell.execute_reply":"2022-05-30T05:03:51.09849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.read_csv(path+'/sampleSubmission.csv.zip')\nsubmission['Weekly_Sales']=regr2.predict(df_sub)\nsubmission.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-30T05:03:37.874016Z","iopub.execute_input":"2022-05-30T05:03:37.874443Z","iopub.status.idle":"2022-05-30T05:03:38.611949Z","shell.execute_reply.started":"2022-05-30T05:03:37.87441Z","shell.execute_reply":"2022-05-30T05:03:38.61063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('submission_gac.csv')","metadata":{"execution":{"iopub.status.busy":"2022-05-30T05:03:21.842668Z","iopub.execute_input":"2022-05-30T05:03:21.84306Z","iopub.status.idle":"2022-05-30T05:03:22.35542Z","shell.execute_reply.started":"2022-05-30T05:03:21.843028Z","shell.execute_reply":"2022-05-30T05:03:22.354082Z"},"trusted":true},"execution_count":null,"outputs":[]}]}