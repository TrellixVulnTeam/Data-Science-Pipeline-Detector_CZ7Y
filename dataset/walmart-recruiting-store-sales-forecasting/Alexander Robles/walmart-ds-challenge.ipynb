{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Walmart Store Sales Prediction"},{"metadata":{},"cell_type":"markdown","source":"# 1- Load data and import packages"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import datetime\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.graph_objs as go\n\nfrom sklearn.ensemble import RandomForestRegressor,AdaBoostRegressor\nfrom sklearn.model_selection import train_test_split\n\n# Importing everything from forecasting quality metrics\nfrom sklearn.metrics import r2_score, median_absolute_error, mean_absolute_error\nfrom sklearn.metrics import median_absolute_error, mean_squared_error, mean_squared_log_error\n\nimport warnings\nwarnings.filterwarnings(\"ignore\") # ignoring annoying warnings\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Read data\nfeatures = pd.read_csv('../input/walmart-recruiting-store-sales-forecasting/features.csv.zip')\ntrain = pd.read_csv('../input/walmart-recruiting-store-sales-forecasting/train.csv.zip')\ntest = pd.read_csv('../input/walmart-recruiting-store-sales-forecasting/test.csv.zip')\nstores = pd.read_csv('../input/walmart-recruiting-store-sales-forecasting/stores.csv')\nsample_submission = pd.read_csv('../input/walmart-recruiting-store-sales-forecasting/sampleSubmission.csv.zip')\nfeatures.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Check columns type from data"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(features.dtypes, columns=['Type'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(train.dtypes, columns=['Type'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(stores.dtypes, columns=['Type'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2- Exploratory Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Merge Feature and Store\nfeat_str = features.merge(stores, how='inner', on='Store')\nfeat_str.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_to = pd.merge(train, feat_str)\ntrain_to.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check **null values** in each column"},{"metadata":{"trusted":true},"cell_type":"code","source":"null_columns = (train_to.isnull().sum(axis = 0)/len(train_to)).sort_values(ascending=False).index\nnull_data = pd.concat([\n    train_to.isnull().sum(axis = 0),\n    (train_to.isnull().sum(axis = 0)/len(train_to)).sort_values(ascending=False),\n    train_to.loc[:, train_to.columns.isin(list(null_columns))].dtypes], axis=1)\nnull_data = null_data.rename(columns={0: '# null', \n                                      1: '% null', \n                                      2: 'type'}).sort_values(ascending=False, by = '% null')\nnull_data = null_data[null_data[\"# null\"]!=0]\nnull_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_to = pd.merge(test, feat_str)\ntest_to.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"null_test = (test_to.isnull().sum(axis = 0)/len(test_to)).sort_values(ascending=False).index\nnull_data_test = pd.concat([\n    test_to.isnull().sum(axis = 0),\n    (test_to.isnull().sum(axis = 0)/len(test_to)).sort_values(ascending=False),\n    test_to.loc[:, test_to.columns.isin(list(null_test))].dtypes], axis=1)\nnull_data_test = null_data_test.rename(columns={0: '# null', \n                                      1: '% null', \n                                      2: 'type'}).sort_values(ascending=False, by = '% null')\nnull_data_test = null_data_test[null_data_test[\"# null\"]!=0]\nnull_data_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del features, train, stores, test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**As you can see from above the date column is an object but we need to convert to \"date\" form:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split date into year, month, day, days\ntrain = train_to.copy()\ntest = test_to.copy()\n\ntrain['Date'] = pd.to_datetime(train['Date'])\ntrain['Year'] = pd.to_datetime(train['Date']).dt.year\ntrain['Month'] = pd.to_datetime(train['Date']).dt.month\ntrain['Week'] = pd.to_datetime(train['Date']).dt.week\ntrain['Day'] = pd.to_datetime(train['Date']).dt.day\ntrain.replace({'A': 1, 'B': 2,'C':3},inplace=True)\n\ntest['Date'] = pd.to_datetime(test['Date'])\ntest['Year'] = pd.to_datetime(test['Date']).dt.year\ntest['Month'] = pd.to_datetime(test['Date']).dt.month\ntest['Week'] = pd.to_datetime(test['Date']).dt.week\ntest['Day'] = pd.to_datetime(test['Date']).dt.day\ntest.replace({'A': 1, 'B': 2,'C':3},inplace=True)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Plot heatmap correlation:"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_heat = train.drop('Date', axis=1)\ncorr = train_heat.corr()\nf, ax = plt.subplots(figsize=(20, 12))\nsns.heatmap(corr, annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Plot time series from Weekly Sales"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pre-processing for week data\nweekly_sales_2010 = train[train.Year==2010]['Weekly_Sales'].groupby(train['Week']).mean()\nweekly_sales_2011 = train[train.Year==2011]['Weekly_Sales'].groupby(train['Week']).mean()\nweekly_sales_2012 = train[train.Year==2012]['Weekly_Sales'].groupby(train['Week']).mean()\n\n# Plot\nfig = go.Figure(\n    [\n        go.Scatter(x = weekly_sales_2010.index, y = weekly_sales_2010.values, mode = 'markers+lines', name=\"2010\"),\n        go.Scatter(x = weekly_sales_2011.index, y = weekly_sales_2011.values, mode = 'markers+lines', name=\"2011\"),\n        go.Scatter(x = weekly_sales_2012.index, y = weekly_sales_2012.values, mode = 'markers+lines', name=\"2012\"),\n    ]\n)\n\nfig.update_layout(title='Average Weekly Sales - Per Year',\n                   plot_bgcolor='rgb(230, 230,230)',\n                   showlegend=True, template=\"plotly_white\")\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3- ML Models and forecasting\n\nThis competition is evaluated on **the weighted mean absolute error (WMAE):**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Error metric\ndef WMAE(dataset, real, predicted):\n    weights = dataset.IsHoliday.apply(lambda x: 5 if x else 1)\n    return np.round(np.sum(weights*abs(real-predicted))/(np.sum(weights)), 2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Drop some columns that are useless"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.drop(['CPI','Unemployment','MarkDown1','MarkDown2','MarkDown3', 'MarkDown4','MarkDown5', 'Date', 'Temperature', 'Fuel_Price'],axis=1)\ntest = test.drop(['CPI','Unemployment','MarkDown1','MarkDown2','MarkDown3', 'MarkDown4','MarkDown5', 'Date', 'Temperature', 'Fuel_Price'],axis=1)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Random Forest\nBuild a simple random forest model and infer in test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"t1 = datetime.datetime.now()\nX = train.loc[:, train.columns != 'Weekly_Sales']\ny = train.loc[:, train.columns == 'Weekly_Sales']\nRF = RandomForestRegressor(n_estimators=58, max_depth=27, max_features=6, min_samples_split=3, min_samples_leaf=1)\nRF.fit(X, y)\nt2 = datetime.datetime.now()\nprint(t2-t1)\ntest['Weekly_Sales'] = RF.predict(test)\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission['Weekly_Sales'] = test['Weekly_Sales']\nsample_submission.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}