{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport time\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns; sns.set(style=\"ticks\", color_codes=True)\n\nfrom sklearn.metrics import mean_absolute_error as MAE, mean_squared_error as MSE\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.feature_selection import RFE\n\nimport matplotlib.pyplot as plt\nfrom IPython.display import display","metadata":{"execution":{"iopub.status.busy":"2021-05-27T10:49:41.346103Z","iopub.execute_input":"2021-05-27T10:49:41.346433Z","iopub.status.idle":"2021-05-27T10:49:41.825253Z","shell.execute_reply.started":"2021-05-27T10:49:41.346375Z","shell.execute_reply":"2021-05-27T10:49:41.824338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Data Loading**","metadata":{}},{"cell_type":"code","source":"Datasets = dict()\nfor ds in ['train', 'test']:\n    dataset = pd.read_csv(f\"../input/{ds}.csv.zip\", sep=',', header=0,\n                          names=['Store', 'Dept', 'Date', 'weeklySales', 'isHoliday'] if ds=='train'\n                           else ['Store', 'Dept', 'Date', 'isHoliday'])\n    features = pd.read_csv(\"../input/features.csv.zip\", sep=',', header=0,\n                           names=['Store', 'Date', 'Temperature', 'Fuel_Price', \n                                  'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', \n                                  'CPI', 'Unemployment', 'IsHoliday']).drop(columns=['IsHoliday'])\n    stores = pd.read_csv(\"../input/stores.csv\", names=['Store', 'Type', 'Size'], sep=',', header=0)\n    dataset = dataset.merge(stores, how='left').merge(features, how='left')\n\n    dataset['Date'] = pd.to_datetime(dataset['Date'])\n    # dataset[\"isTomorrowHoliday\"] = dataset[\"isHoliday\"].shift(-1).fillna(False)\n    display(dataset.head())\n    \n    Datasets[ds] = dataset","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.status.busy":"2021-05-27T10:49:41.826743Z","iopub.execute_input":"2021-05-27T10:49:41.82699Z","iopub.status.idle":"2021-05-27T10:49:42.596553Z","shell.execute_reply.started":"2021-05-27T10:49:41.826943Z","shell.execute_reply":"2021-05-27T10:49:42.595439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Datasets['train'].dtypes","metadata":{"execution":{"iopub.status.busy":"2021-05-27T10:49:42.597931Z","iopub.execute_input":"2021-05-27T10:49:42.59829Z","iopub.status.idle":"2021-05-27T10:49:42.606482Z","shell.execute_reply.started":"2021-05-27T10:49:42.598225Z","shell.execute_reply":"2021-05-27T10:49:42.605443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def describe_missing_values(df: pd.DataFrame):\n    miss_val = df.isnull().sum()\n    miss_val_percent = 100 * df.isnull().sum() / len(df)\n    miss_val_table = pd.concat([miss_val, miss_val_percent], axis=1)\n    miss_val_table_ren_columns = miss_val_table.rename(\n        columns = {0: 'Missing Values', \n                   1: '% of Total Values',}\n    )\n    miss_val_table_ren_columns = miss_val_table_ren_columns[\n        miss_val_table_ren_columns.iloc[:,1] != 0\n    ].sort_values('% of Total Values', ascending=False).round(1)\n    \n    print(f\"Dataframe has {df.shape[1]} columns,\")\n    print(f\"\\t\\t {miss_val_table_ren_columns.shape[0]} columns that have missing values.\")\n\n    return miss_val_table_ren_columns\n\n\ndef visualize_distribution_of_missing_values(df: pd.DataFrame):\n    df_nan_check = df.isna().sum().sort_values()\n    df_nan_check = df_nan_check.to_dict()\n    df_not_nan = []\n\n    nan_cols = 0\n\n    for key, value in df_nan_check.items():\n        df_nan_check[key] = int(value/len(df)*100)\n        if df_nan_check[key] >= 80:\n            nan_cols += 1\n        else:\n            df_not_nan.append(key)\n\n    # Visualize\n    plt.figure(figsize=(9, 6))\n    plt.suptitle('Distribution of Empty Values', fontsize=19)\n    plt.bar(df_nan_check.keys(), df_nan_check.values())\n    plt.xticks(rotation=69)\n    plt.show()\n    \n\nfor ds in ['train', 'test']:\n    print(f'\\n\\n{ds}-set:')\n    print(describe_missing_values(Datasets[ds]))\n    # visualize_distribution_of_missing_values(dataset)","metadata":{"execution":{"iopub.status.busy":"2021-05-27T10:49:42.60844Z","iopub.execute_input":"2021-05-27T10:49:42.608783Z","iopub.status.idle":"2021-05-27T10:49:43.044197Z","shell.execute_reply.started":"2021-05-27T10:49:42.608717Z","shell.execute_reply":"2021-05-27T10:49:43.043169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Data Exploration**","metadata":{"_uuid":"53ed17a3087c8d1b282d852e55a4843e23136ec6"}},{"cell_type":"code","source":"def scatter(dataset, column):\n    plt.figure()\n    plt.scatter(dataset[column] , dataset['weeklySales'], alpha=0.169)\n    plt.ylabel('weeklySales')\n    plt.xlabel(column)","metadata":{"_uuid":"d5d0f48bb70f7676dc1268eadcfac31703a852ea","execution":{"iopub.status.busy":"2021-05-27T10:49:43.045552Z","iopub.execute_input":"2021-05-27T10:49:43.045902Z","iopub.status.idle":"2021-05-27T10:49:43.051769Z","shell.execute_reply.started":"2021-05-27T10:49:43.045828Z","shell.execute_reply":"2021-05-27T10:49:43.050694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in ['Fuel_Price', 'Size', 'CPI', 'Type', 'isHoliday', 'Unemployment', 'Temperature', 'Store', 'Dept']:\n    scatter(Datasets['train'], col)","metadata":{"_uuid":"24f3db6eaa3103816f2316c3c52480124a888a96","execution":{"iopub.status.busy":"2021-05-27T10:49:43.053331Z","iopub.execute_input":"2021-05-27T10:49:43.053636Z","iopub.status.idle":"2021-05-27T10:50:21.292114Z","shell.execute_reply.started":"2021-05-27T10:49:43.053587Z","shell.execute_reply":"2021-05-27T10:50:21.290075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(18, 14))\ncorr = Datasets['train'].corr()\nc = plt.pcolor(corr)\nplt.yticks(np.arange(0.5, len(corr.index), 1), corr.index)\nplt.xticks(np.arange(0.5, len(corr.columns), 1), corr.columns, rotation=45)\nfig.colorbar(c)","metadata":{"_uuid":"7de400728cdc665aefedd7ba506cffbbf6dfe87d","execution":{"iopub.status.busy":"2021-05-27T10:50:21.293428Z","iopub.execute_input":"2021-05-27T10:50:21.293724Z","iopub.status.idle":"2021-05-27T10:50:22.580196Z","shell.execute_reply.started":"2021-05-27T10:50:21.293677Z","shell.execute_reply":"2021-05-27T10:50:22.579157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Data Manipulation**","metadata":{"_uuid":"45bb9a93aa88a169dae9dc805525a1f56f2afeec"}},{"cell_type":"code","source":"for ds in Datasets.keys():\n    # make holidays more specific\n    Datasets[ds]['Holiday_Type'] = None\n    Datasets[ds].loc[(Datasets[ds]['isHoliday']==True) & \n                     (Datasets[ds]['Date'].dt.month==2), 'Holiday_Type'] = 'Super_Bowl'\n    Datasets[ds].loc[(Datasets[ds]['isHoliday']==True) & \n                     (Datasets[ds]['Date'].dt.month==9), 'Holiday_Type'] = 'Labor_Day'\n    Datasets[ds].loc[(Datasets[ds]['isHoliday']==True) & \n                     (Datasets[ds]['Date'].dt.month==11), 'Holiday_Type'] = 'Thanksgiving' \n    Datasets[ds].loc[(Datasets[ds]['isHoliday']==True) & \n                     (Datasets[ds]['Date'].dt.month==12), 'Holiday_Type'] = 'Christmax'\n    Datasets[ds].drop(columns=['isHoliday'], inplace=True)\n    \n    # 1-hot encoding for categorical features\n    Datasets[ds] = pd.get_dummies(Datasets[ds], columns=[\"Type\", \"Holiday_Type\"])\n    \n    # data imputation\n    Datasets[ds].fillna(value=0, inplace=True)\n    display(Datasets[ds].head())","metadata":{"_uuid":"1586a44ad77b30ff62ac07810796b391bc1096de","execution":{"iopub.status.busy":"2021-05-27T10:50:22.581708Z","iopub.execute_input":"2021-05-27T10:50:22.582057Z","iopub.status.idle":"2021-05-27T10:50:23.078345Z","shell.execute_reply.started":"2021-05-27T10:50:22.582002Z","shell.execute_reply":"2021-05-27T10:50:23.077352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Datasets['train'].Store.value_counts(sort=False)","metadata":{"execution":{"iopub.status.busy":"2021-05-27T10:50:23.07961Z","iopub.execute_input":"2021-05-27T10:50:23.080027Z","iopub.status.idle":"2021-05-27T10:50:23.091109Z","shell.execute_reply.started":"2021-05-27T10:50:23.079797Z","shell.execute_reply":"2021-05-27T10:50:23.090245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Modeling**","metadata":{"_uuid":"352d8fad9967d928067c9623ff8cb73f20748689"}},{"cell_type":"code","source":"STORE_ID = 44","metadata":{"execution":{"iopub.status.busy":"2021-05-27T10:50:23.09254Z","iopub.execute_input":"2021-05-27T10:50:23.092805Z","iopub.status.idle":"2021-05-27T10:50:23.097433Z","shell.execute_reply.started":"2021-05-27T10:50:23.092758Z","shell.execute_reply":"2021-05-27T10:50:23.096588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Facebook Prophet","metadata":{}},{"cell_type":"code","source":"holidays = pd.DataFrame({\n    'holiday': ['Super_Bowl']*4 + ['Labor_Day']*4 + ['Thanksgiving']*4 + ['Christmas']*4,\n    'ds': pd.to_datetime(['12-02-2010', '11-02-2011', '10-02-2012', '08-02-2013',\n                          '10-09-2010', '09-09-2011', '07-09-2012', '06-09-2013',\n                          '26-10-2010', '25-10-2011', '23-10-2012', '29-10-2013',\n                          '31-12-2010', '30-12-2011', '28-12-2012', '27-12-2013',]),\n    'lower_window': 0,\n    'upper_window': 1,\n})","metadata":{"execution":{"iopub.status.busy":"2021-05-27T10:50:23.09863Z","iopub.execute_input":"2021-05-27T10:50:23.098908Z","iopub.status.idle":"2021-05-27T10:50:23.115402Z","shell.execute_reply.started":"2021-05-27T10:50:23.098858Z","shell.execute_reply":"2021-05-27T10:50:23.114481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_train, data_test = Datasets['train'].copy(), Datasets['test'].copy()\ndata_train.rename(columns={'Date': 'ds', 'weeklySales': 'y'}, inplace=True)\ndata_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-27T10:50:23.116937Z","iopub.execute_input":"2021-05-27T10:50:23.11735Z","iopub.status.idle":"2021-05-27T10:50:23.230581Z","shell.execute_reply.started":"2021-05-27T10:50:23.117203Z","shell.execute_reply":"2021-05-27T10:50:23.229677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from fbprophet import Prophet\n\nModels = dict()\n\nfor name, group in data_train.groupby([\"Store\", \"Dept\"]):\n    \n    if name[0] != STORE_ID:\n        continue\n        \n    data_grouped = group.drop(columns=[\"Store\", \"Dept\"])\n    print(f\"\\n Training Facebook's Prophet for store={name[0]}, dept={name[1]} with {len(data_grouped)} samples ...\")\n    if len(data_grouped) < 3:\n        print(f\"\\t\\t Number of samples must be larger than 2 !!!\")\n        Models[name] = [None, np.mean(group['y'])]\n        continue\n    \n    # Creating model\n    model = Prophet(\n        growth='linear', # linear or logistic\n        changepoints=None, # list of dates at which to include potential changepoints\n        n_changepoints=11, # number of potential changepoints\n        changepoint_range=0.69, # proportion of history in which trend changepoints will be estimated\n        yearly_seasonality='auto',\n        weekly_seasonality='auto',\n        daily_seasonality='auto',\n        holidays=holidays,\n        seasonality_mode='additive',\n        seasonality_prior_scale=6.9,\n        holidays_prior_scale=6.9,\n        changepoint_prior_scale=0.169,\n        mcmc_samples=0, # if > 0: Bayesian inference with number of MCMC samples, else: MAP estimation\n        interval_width=0.69, # width of the uncertainty intervals provided for the forecast\n        uncertainty_samples=690 # number of simulated draws used to estimate uncertainty intervals\n    )\n\n    for col in ['Size', 'CPI', 'Unemployment']:\n        model.add_regressor(name=col, prior_scale=None, standardize='auto', mode='additive')\n        \n    # Training model        \n    t1 = time.time()\n    model.fit(data_grouped)\n    t2 = time.time()\n    print(f\"\\t\\t ... in {round(t2-t1, 3)} seconds\")\n\n    Models[name] = [model, np.mean(group['y'])]","metadata":{"execution":{"iopub.status.busy":"2021-05-27T10:50:23.231715Z","iopub.execute_input":"2021-05-27T10:50:23.231986Z","iopub.status.idle":"2021-05-27T10:50:27.860901Z","shell.execute_reply.started":"2021-05-27T10:50:23.231939Z","shell.execute_reply":"2021-05-27T10:50:27.85992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_test.rename(columns={'Date': 'ds'}, inplace=True)\nfor col in data_train.columns:\n    if col in ['ds', 'y']:\n        continue\n    if col not in list(data_test.columns):\n        data_test[col] = 0\n\nresult = []\nfor name, group in data_test.groupby([\"Store\", \"Dept\"]):\n\n    if name[0] != STORE_ID:\n        continue\n        \n    data_grouped = group.drop(columns=[\"Store\", \"Dept\"])\n    print(f\"\\n Predicting Facebook's Prophet for store={name[0]}, dept={name[1]} with {len(data_grouped)} samples ...\")\n    if name not in list(Models.keys()):\n        forecast = group.copy()\n        forecast['yhat'] = np.mean(data_train.y)\n    else:\n        model, mean_value = Models[name]\n        if model is None:\n            forecast = group.copy()\n            forecast['yhat'] = mean_value\n        else:\n            try:\n                t1 = time.time()\n                forecast = model.predict(df=data_grouped)\n                t2 = time.time()\n                print(f\"\\t ... in {round(t2-t1, 3)} seconds\")\n                forecast[\"Store\"] = name[0]\n                forecast[\"Dept\"] = name[1]\n            except Exception as e:\n                print(e)\n                forecast = group.copy()\n                forecast['yhat'] = mean_value\n            \n    # 1st-date: Friday, 05-02-2010\n    # Models[name].plot_components(forecast, weekly_start=5, yearly_start=31+5)\n    forecast['yhat'][forecast['yhat']<0] = 20\n    result.append(forecast[['Store', 'Dept', 'ds', 'yhat']])","metadata":{"execution":{"iopub.status.busy":"2021-05-27T10:50:27.862177Z","iopub.execute_input":"2021-05-27T10:50:27.862437Z","iopub.status.idle":"2021-05-27T10:51:38.031154Z","shell.execute_reply.started":"2021-05-27T10:50:27.862389Z","shell.execute_reply":"2021-05-27T10:51:38.030268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = pd.concat(result, axis=0, ignore_index=True)\nresult.rename(columns={'yhat': 'Weekly_Sales'}, inplace=True)\nresult['Id'] = result['Store'].apply(str) + '_' + result['Dept'].apply(str) + '_' + result['ds'].dt.strftime('%Y-%m-%d')\n\nresult[['Id', 'Weekly_Sales']].to_csv(f'submission_Prophet_store={STORE_ID}.csv', index=False)\n\ndisplay(result.head())","metadata":{"execution":{"iopub.status.busy":"2021-05-27T10:51:38.032304Z","iopub.execute_input":"2021-05-27T10:51:38.03252Z","iopub.status.idle":"2021-05-27T10:51:38.141708Z","shell.execute_reply.started":"2021-05-27T10:51:38.032487Z","shell.execute_reply":"2021-05-27T10:51:38.141099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **SARIMAX** - **S**easonal **A**uto**R**egressive **I**ntegrated **M**oving **A**verage with e**X**ogenous regressors","metadata":{}},{"cell_type":"code","source":"data_train, data_test = Datasets['train'].copy(), Datasets['test'].copy()\n\ndata_train.set_index(keys='Date', drop=True, inplace=True)\ndata_test.set_index(keys='Date', drop=True, inplace=True)\n\nfor col in data_train.columns:\n    if col in ['weeklySales']:\n        continue\n    if col not in list(data_test.columns):\n        data_test[col] = 0\n\ndisplay(data_train[(data_train.Store==1) & (data_train.Dept==1)].tail())\ndisplay(data_test[(data_test.Store==1) & (data_test.Dept==1)].head())","metadata":{"execution":{"iopub.status.busy":"2021-05-27T10:51:38.14253Z","iopub.execute_input":"2021-05-27T10:51:38.142852Z","iopub.status.idle":"2021-05-27T10:51:38.237348Z","shell.execute_reply.started":"2021-05-27T10:51:38.142818Z","shell.execute_reply":"2021-05-27T10:51:38.236711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from statsmodels.tsa.statespace.sarimax import SARIMAX\n\nsteps = -1\nModels = dict()\nresults = list()\n\nfor name, group_test in data_test.groupby([\"Store\", \"Dept\"]):\n        \n    if name[0] != STORE_ID:\n        continue\n    \n    print(f\"\\n SARIMAX for store={name[0]}, dept={name[1]}\")\n    \n    test_size = len(group_test)\n    if test_size < 1:\n        continue\n    \n    # preparing data\n    group = data_train[(data_train[\"Store\"]==name[0]) & (data_train[\"Dept\"]==name[1])]\n    train_size = len(group)\n    if train_size < 1:\n        predictions = group_test.copy()\n        predictions['Weekly_Sales'] = np.mean(data_train['weeklySales'])\n    else:\n        data_grouped = group.copy()\n        data_grouped.drop(columns=[\"Store\", \"Dept\"], inplace=True)\n        data_grouped.index = pd.DatetimeIndex(data=data_grouped.index.values,\n                                              freq=data_grouped.index.inferred_freq)\n        features_imposed = data_grouped['weeklySales'].shift(steps)\n        features_exposed = data_grouped.drop(columns=['weeklySales'])\n        seasonal_order = 52 if train_size>52 else 1\n\n        try:\n            # Creating model\n            model = SARIMAX(endog=features_imposed, \n                            exog=features_exposed, \n                            order=(1, 0, 0), # p,d,q - number of AR parameters, differences, and MA parameters\n                            seasonal_order=(0, 1, 0, seasonal_order), # P,D,Q,s - AR parameters, differences, MA parameters, and periodicity\n                            trend='ct', # c: const - t: time\n                            enforce_invertibility=False, \n                            enforce_stationarity=False)\n\n            # Training model\n            print(f\"\\t Training with {train_size} samples ...\")\n            t1 = time.time()\n            forecaster = model.fit()\n            t2 = time.time()\n            print(f\"\\t\\t ... in {round(t2-t1, 3)} seconds\")\n            # print(forecaster.summary())\n\n            # Models[name] = forecaster\n\n            # Predicting\n            features = group_test.copy()\n            features.drop(columns=[\"Store\", \"Dept\"], inplace=True)\n            features.index = pd.DatetimeIndex(data=features.index.values,\n                                              freq=features.index.inferred_freq)\n\n            print(f\"\\t Predicting with {test_size} samples ...\")\n            t1 = time.time()\n            predictions = forecaster.predict(start=train_size, \n                                             end=train_size+test_size-1, \n                                             exog=features)\n            t2 = time.time()\n            print(f\"\\t\\t ... in {round(t2-t1, 3)} seconds\")\n\n            predictions = predictions.to_frame(name='Weekly_Sales')\n            predictions[\"Store\"] = name[0]\n            predictions[\"Dept\"] = name[1]\n\n        except Exception as e:\n            print(e)\n            predictions = group_test.copy()\n            predictions['Weekly_Sales'] = np.mean(group['weeklySales'])\n               \n    predictions = predictions[[\"Store\", \"Dept\", 'Weekly_Sales']]\n    predictions.reset_index(inplace=True)\n    predictions['Date'] = group_test.index\n    predictions['Weekly_Sales'][predictions['Weekly_Sales']<0] = np.mean(group['weeklySales'])\n    \n    assert len(predictions)==test_size, f\"[xxx] #predictions = {len(predictions)} != {test_size}\"\n    results.append(predictions)","metadata":{"execution":{"iopub.status.busy":"2021-05-27T10:51:38.238243Z","iopub.execute_input":"2021-05-27T10:51:38.238551Z","iopub.status.idle":"2021-05-27T11:01:30.797792Z","shell.execute_reply.started":"2021-05-27T10:51:38.23851Z","shell.execute_reply":"2021-05-27T11:01:30.797089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = pd.concat(results, axis=0, ignore_index=True)\n# result.to_csv('test.csv', index=False)\nresult['Date'] = pd.to_datetime(result.Date, format='%Y-%m-%d %H:%M:%S')\nresult['Id'] = result['Store'].astype(int).apply(str) + '_' + result['Dept'].astype(int).apply(str) + '_' + result['Date'].dt.strftime('%Y-%m-%d')\nresult[['Id', 'Weekly_Sales']].to_csv(f'submission_SARIMAX_store={STORE_ID}.csv', index=False)\n\ndisplay(result.head())","metadata":{"execution":{"iopub.status.busy":"2021-05-27T11:01:30.799234Z","iopub.execute_input":"2021-05-27T11:01:30.799456Z","iopub.status.idle":"2021-05-27T11:01:30.868163Z","shell.execute_reply.started":"2021-05-27T11:01:30.799418Z","shell.execute_reply":"2021-05-27T11:01:30.867529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}