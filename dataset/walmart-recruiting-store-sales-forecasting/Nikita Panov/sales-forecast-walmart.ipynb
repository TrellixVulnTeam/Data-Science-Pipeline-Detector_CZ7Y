{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom matplotlib.gridspec import GridSpec\nimport seaborn as sns\nfrom scipy import stats\nfrom scipy.special import boxcox1p\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\n\nfrom xgboost import XGBRegressor\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# tslearn - time series clusters\n!pip install tslearn\nfrom tslearn.clustering import TimeSeriesKMeans\n\nfrom pandasql import sqldf\npysqldf = lambda q: sqldf(q, globals())\n\n\nfrom sklearn import decomposition, datasets\nfrom sklearn import tree\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import StandardScaler","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**IMPORTING CSVS**"},{"metadata":{"trusted":true},"cell_type":"code","source":"features = pd.read_csv('../input/walmart-recruiting-store-sales-forecasting/features.csv.zip')\ntrain = pd.read_csv('../input/walmart-recruiting-store-sales-forecasting/train.csv.zip')\nstores = pd.read_csv('../input/walmart-recruiting-store-sales-forecasting/stores.csv')\ntest = pd.read_csv('../input/walmart-recruiting-store-sales-forecasting/test.csv.zip')\nsample_submission = pd.read_csv('../input/walmart-recruiting-store-sales-forecasting/sampleSubmission.csv.zip')\n\nprint(\"import done\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**DEFINITIONS**"},{"metadata":{"trusted":true},"cell_type":"code","source":"sep = '--------------------------------------------------------'\n\nd = globals()\ndf_list = [v for k, v in d.items() if isinstance(v, pd.DataFrame)]\ndf_dict = {k: v for k, v in d.items() if isinstance(v, pd.DataFrame)}\n\ndef quick_look(df):\n    print(df.head())\n    print(\"shape\")\n    print(df.shape)\n    print(\"info\")\n    print(df.info())\n    return\n\ndef WMAE(dataset, real, predicted):\n    weights = dataset.IsHoliday.apply(lambda x: 5 if x else 1)\n    return np.round(np.sum(weights*abs(real-predicted))/(np.sum(weights)), 2)\n\ndef add_value_labels(ax, spacing=5):\n    \"\"\"Add labels to the end of each bar in a bar chart.\n\n    Arguments:\n        ax (matplotlib.axes.Axes): The matplotlib object containing the axes\n            of the plot to annotate.\n        spacing (int): The distance between the labels and the bars.\n    \"\"\"\n\n    # For each bar: Place a label\n    for rect in ax.patches:\n        # Get X and Y placement of label from rect.\n        y_value = rect.get_height()\n        x_value = rect.get_x() + rect.get_width() / 2\n\n        # Number of points between bar and label. Change to your liking.\n        space = spacing\n        # Vertical alignment for positive values\n        va = 'bottom'\n\n        # If value of bar is negative: Place label below bar\n        if y_value < 0:\n            # Invert space to place label below\n            space *= -1\n            # Vertically align label at top\n            va = 'top'\n\n        # Use Y value as label and format number with one decimal place\n        label = \"{:.1f}\".format(y_value)\n\n        # Create annotation\n        ax.annotate(\n            label,                      # Use `label` as label\n            (x_value, y_value),         # Place label at end of the bar\n            xytext=(0, space),          # Vertically shift label by `space`\n            textcoords=\"offset points\", # Interpret `xytext` as offset in points\n            ha='center',                # Horizontally center label\n            va=va)                      # Vertically align label differently for\n                                        # positive and negative values.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"VARIABLE INVESTIGATION"},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in df_dict:\n    print(i+sep)\n    quick_look(eval(i))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# stores2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pysqldf(\"\"\"\n\n# SELECT DISTINCT DATE \n#     ,SUBSTR(date,0,11) as date2 ,CAST(SUBSTR(date,6,2) as int ) as month ,WEEK\n# FROM store_fact str\n# WHERE CAST(SUBSTR(date,6,2) as int )  = 7\n# \"\"\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# type dummy\n# stores = pd.get_dummies(stores, columns=['Type'])\n\nstores.Type = stores.Type.apply(lambda x: 3 if x == 'A' else(2 if x == 'B' else 1))\n\n# store over time\nstore_fact = features.merge(stores, how='inner', on='Store')\n\n# time variables\nstore_fact.Date = pd.to_datetime(store_fact.Date)\ntrain.Date = pd.to_datetime(train.Date)\ntest.Date = pd.to_datetime(test.Date)\n\n\nstore_fact['Week'] = store_fact.Date.dt.week \nstore_fact['Year'] = store_fact.Date.dt.year\nstore_fact['WeekYear'] = store_fact.Year*100+store_fact.Week\n\n# ADD pre-superbowl\nstore_fact.loc[(store_fact.Week==5), 'IsHoliday'] = True\n\n# ADD EASTER\nstore_fact.loc[(store_fact.Year==2010) & (store_fact.Week==13), 'IsHoliday'] = True\nstore_fact.loc[(store_fact.Year==2011) & (store_fact.Week==16), 'IsHoliday'] = True\nstore_fact.loc[(store_fact.Year==2012) & (store_fact.Week==14), 'IsHoliday'] = True\nstore_fact.loc[(store_fact.Year==2013) & (store_fact.Week==13), 'IsHoliday'] = True\n\n# ADD INDEPENDENCE DAY\nstore_fact.loc[(store_fact.Week==26), 'IsHoliday'] = True\n\n\n# ADD WEEK OF MONTH\nstore_fact = pysqldf(\n\"\"\"\nwith cal1 as \n    (SELECT distinct date ,SUBSTR(date,0,11) as date2 ,CAST(SUBSTR(date,6,2) as int ) as month ,CAST(SUBSTR(date,9,2) as int ) as days ,year\n        ,case when CAST(SUBSTR(date,6,2) as int) = 12 then 0 else IsHoliday end as IsHoliday \n        ,case when CAST(SUBSTR(date,6,2) as int) = 12 then IsHoliday\n           else 0 end as IsHoliday_Dec\n    FROM store_fact\n    )\n,cal_dim as \n    (select *\n        ,rank() OVER ( PARTITION BY month2 ORDER BY date) AS week_of_month\n    from\n        (select * \n            ,CASE WHEN month = 2 and days >= 26 THEN year*100 + month+1\n                when days >= 28 THEN year*100 + month+1 \n                ELSE year*100 + month end AS month2\n            ,lag(IsHoliday, -1, 0) over (order by date) AS IsHoliday_lag\n            ,lag(IsHoliday_Dec, -1, 0) over (order by date) AS IsHoliday_Dec_lag\n        from cal1\n        )\n    ORDER BY date\n    )\nselect store ,str.Date ,Temperature ,Fuel_Price\n    ,MarkDown1,MarkDown2,MarkDown3,MarkDown4,MarkDown5\n    ,CPI,Unemployment\n    ,Size\n    ,Type --Type_A,Type_B,Type_C\n    ,str.Week,str.Year,str.WeekYear\n    ,cal.IsHoliday \n    ,cal.IsHoliday_Dec\n    ,cal.IsHoliday_lag ,cal.IsHoliday_Dec_lag\n    ,cal.week_of_month\nfrom store_fact str\nleft join cal_dim cal\non str.date = cal.date\n\"\"\")\n\n# pd.set_option('display.max_rows', test.shape[0]+1)\n# print(test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pysqldf(\n\"\"\" \nselect\ndistinct date ,SUBSTR(date,0,11) as date2 ,CAST(SUBSTR(date,6,2) as int ) as month ,CAST(SUBSTR(date,9,2) as int ) as days ,year\n        ,case when CAST(SUBSTR(date,6,2) as int) = 12 then 0 else IsHoliday end as IsHoliday \n        ,case when CAST(SUBSTR(date,6,2) as int) = 12 and year = 2010 and CAST(SUBSTR(date,9,2) as int ) in (24,31) then 1\n           when CAST(SUBSTR(date,6,2) as int) = 12 and year = 2011 and CAST(SUBSTR(date,9,2) as int ) in (23,30) then 1\n           when CAST(SUBSTR(date,6,2) as int) = 12 and year = 2012 and CAST(SUBSTR(date,9,2) as int ) in (21,28) then 1\n           else 0 end as IsHoliday_Dec ,week\n    FROM store_fact\n   where CAST(SUBSTR(date,6,2) as int) = 12\n\"\"\")\n\n# pysqldf(\n# \"\"\" \n\n#         SELECT\n#             Store, \n#             Dept, \n#             Week, \n#             Weekly_Sales,\n#             case \n#                 when Week = 52 then lag(Weekly_Sales) over(partition by Store, Dept) \n#             end as last_sales\n#         from Final where Week>48\n#         \"\"\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"CALENDAR ALTERATIONS"},{"metadata":{"trusted":true},"cell_type":"code","source":"train=train.drop(columns=['IsHoliday'])\ntest=test.drop(columns=['IsHoliday'])\n\ntrain['Store_Dept'] = train['Store'].apply(str) + '_' + train['Dept'].apply(str)\ntest['Store_Dept'] = test['Store'].apply(str) + '_' + test['Dept'].apply(str)\n\nstore_fact.Date = pd.to_datetime(store_fact.Date)\n\n# train2 - full data\ntrain2 = train.merge(store_fact \n                           ,how='inner'\n                           ,on=['Store','Date']).sort_values(by=['Store','Dept','Date']).reset_index(drop=True)\ntest2 = test.merge(store_fact \n                           ,how='inner'\n                           ,on=['Store','Date']).sort_values(by=['Store','Dept','Date']).reset_index(drop=True)\ntrain2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# STORE CLUSTER\nweekly_summary = train2.groupby(['Store','WeekYear']).sum()['Weekly_Sales'].reset_index()\ndf = weekly_summary.pivot(index='WeekYear', columns='Store', values='Weekly_Sales').reset_index().rename_axis(None, axis=1).drop(columns=['WeekYear'])\nX = df.transpose().values\n\ndistortions = []\nK = range(1,10)\nfor k in K:\n    model = TimeSeriesKMeans(n_clusters=k, metric=\"euclidean\", max_iter=10, n_init=2)\n    model.fit(X)\n    distortions.append(model.inertia_)\nmodel.inertia_\n\nplt.figure(figsize=(5,5))\nplt.plot(K, distortions, 'bx-')\nplt.xlabel('k')\nplt.ylabel('Distortion')\nplt.title('The Elbow Method showing the optimal k')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Elbow - choosing 4 clusters, but after analysis changed to 6"},{"metadata":{"trusted":true},"cell_type":"code","source":"# model = TimeSeriesKMeans(n_clusters=5, metric=\"euclidean\", max_iter=10, n_init=2)\nmodel = TimeSeriesKMeans(n_clusters=6, metric=\"euclidean\", max_iter=10, n_init=2)\nmodel.fit(X)\n\n# build helper df to map metrics to their cluster labels\ndf_scluster = pd.DataFrame(list(zip(df.columns, model.labels_)), columns=['Store', 'cluster'])\ndf_scluster.head()\n\nstores_cluster = stores.merge(df_scluster, how='inner', on='Store')\nstores_cluster = pd.get_dummies(stores_cluster, columns=['cluster'])\n\nfilter_col = [col for col in stores_cluster if (col.startswith('Type_') or col.startswith('cluster_'))]\n\n# Plot Heatmap from variable's correlation\nplt.figure(figsize=(5,5))\nm = stores_cluster[filter_col].corr()\nnp.fill_diagonal(m.values, np.nan)\nsns.heatmap(m, cmap='seismic', annot=True, fmt='.2f', annot_kws={\"size\": 9})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see some correlation between type of store (likely to be: Hiper, Super & Convenience formats) and the store clusters.\nBut the clusters clearly add something different into the mix."},{"metadata":{"trusted":true},"cell_type":"code","source":"# DEPARTMENT CLUSTER\nweekly_summary = train2.groupby(['Dept','WeekYear']).sum()['Weekly_Sales'].reset_index()\ndf = weekly_summary.pivot(index='WeekYear', columns='Dept', values='Weekly_Sales').reset_index().rename_axis(None, axis=1).drop(columns=['WeekYear'])\n\nX = df.fillna(0).transpose().values\n\ndistortions = []\nK = range(1,10)\nfor k in K:\n    model = TimeSeriesKMeans(n_clusters=k, metric=\"euclidean\", max_iter=10, n_init=2)\n    model.fit(X)\n    distortions.append(model.inertia_)\nmodel.inertia_\n\nplt.figure(figsize=(5,5))\nplt.plot(K, distortions, 'bx-')\nplt.xlabel('k')\nplt.ylabel('Distortion')\nplt.title('The Elbow Method showing the optimal k')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Elbow - choosing 5 clusters"},{"metadata":{"trusted":true},"cell_type":"code","source":"# model = TimeSeriesKMeans(n_clusters=4, metric=\"euclidean\", max_iter=10, n_init=2)\nmodel = TimeSeriesKMeans(n_clusters=6, metric=\"euclidean\", max_iter=10, n_init=2)\nmodel.fit(X)\n\n# # build helper df to map metrics to their cluster labels\ndf_dcluster = pd.DataFrame(list(zip(df.columns, model.labels_)), columns=['Dept', 'dcluster'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Adding a store/department cluster - these ended up not being used"},{"metadata":{"trusted":true},"cell_type":"code","source":"# STORE / DEPARTMENT CLUSTER\nweekly_summary = train2.groupby(['Store_Dept','WeekYear']).sum()['Weekly_Sales'].reset_index()\ndf = weekly_summary.pivot(index='WeekYear', columns='Store_Dept', values='Weekly_Sales').reset_index().rename_axis(None, axis=1).drop(columns=['WeekYear'])\n\nX = df.fillna(0).transpose().values\n\ndistortions = []\nK = range(1,10)\nfor k in K:\n    model = TimeSeriesKMeans(n_clusters=k, metric=\"euclidean\", max_iter=10, n_init=2)\n    model.fit(X)\n    distortions.append(model.inertia_)\nmodel.inertia_\n\nplt.figure(figsize=(5,5))\nplt.plot(K, distortions, 'bx-')\nplt.xlabel('k')\nplt.ylabel('Distortion')\nplt.title('The Elbow Method showing the optimal k')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Elbow - choosing 5 clusters"},{"metadata":{"trusted":true},"cell_type":"code","source":"# model = TimeSeriesKMeans(n_clusters=5, metric=\"euclidean\", max_iter=10, n_init=2)\nmodel = TimeSeriesKMeans(n_clusters=7, metric=\"euclidean\", max_iter=10, n_init=2)\nmodel.fit(X)\n\n# # build helper df to map metrics to their cluster labels\ndf_sdcluster = pd.DataFrame(list(zip(df.columns, model.labels_)), columns=['Store_Dept', 'sdcluster'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# visualize clusters \ntrain3 = train2.merge(df_scluster, how='inner', on='Store')\ntrain3 = train3.merge(df_dcluster, how='inner', on='Dept')\ntrain3 = train3.merge(df_sdcluster, how='inner', on='Store_Dept')\ntrain3.to_csv('cluster_test.csv',index=False)\n#viz made in powerbi (adjusted store clusters)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def random_forest(n_estimators, max_depth):\n    result = []\n    for estimator in n_estimators:\n        for depth in max_depth:\n            wmaes_cv = []\n            for i in range(1,3):\n                print('k:', i, ', n_estimators:', estimator, ', max_depth:', depth)\n                x_train, x_test, y_train, y_test = train_test_split(X_train, Y_train, test_size=0.3)\n                RF = RandomForestRegressor(n_estimators=estimator, max_depth=depth)\n                RF.fit(x_train, y_train)\n                predicted = RF.predict(x_test)\n                wmaes_cv.append(WMAE(x_test, y_test, predicted))\n            print('WMAE:', np.mean(wmaes_cv))\n            result.append({'Max_Depth': depth, 'Estimators': estimator, 'WMAE': np.mean(wmaes_cv)})\n    return pd.DataFrame(result)\n\ndef random_forest_II(n_estimators, max_depth, max_features):\n    result = []\n    for feature in max_features:\n        wmaes_cv = []\n        for i in range(1,3):\n            print('k:', i, ', max_features:', feature)\n            x_train, x_test, y_train, y_test = train_test_split(X_train, Y_train, test_size=0.3)\n            RF = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth, max_features=feature)\n            RF.fit(x_train, y_train)\n            predicted = RF.predict(x_test)\n            wmaes_cv.append(WMAE(x_test, y_test, predicted))\n        print('WMAE:', np.mean(wmaes_cv))\n        result.append({'Max_Feature': feature, 'WMAE': np.mean(wmaes_cv)})\n    return pd.DataFrame(result)\n\ndef random_forest_III(n_estimators, max_depth, max_features, min_samples_split, min_samples_leaf):\n    result = []\n    for split in min_samples_split:\n        for leaf in min_samples_leaf:\n            wmaes_cv = []\n            for i in range(1,3):\n                print('k:', i, ', min_samples_split:', split, ', min_samples_leaf:', leaf)\n                x_train, x_test, y_train, y_test = train_test_split(X_train, Y_train, test_size=0.3)\n                RF = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth, max_features=max_features, \n                                           min_samples_leaf=leaf, min_samples_split=split)\n                RF.fit(x_train, y_train)\n                predicted = RF.predict(x_test)\n                wmaes_cv.append(WMAE(x_test, y_test, predicted))\n            print('WMAE:', np.mean(wmaes_cv))\n            result.append({'Min_Samples_Leaf': leaf, 'Min_Samples_Split': split, 'WMAE': np.mean(wmaes_cv)})\n    return pd.DataFrame(result)\n\ndef XGBRegressor_I(n_estimators, early_stopping_rounds ,learning_rate):\n    result = []\n    for estimator in n_estimators:\n        for rounds in early_stopping_rounds:\n            for rates in learning_rate:\n                wmaes_cv = []\n                for i in range(1,3):\n                    print('k:', i, ', n_estimators:', estimator, ', early_stopping_rounds:', rounds, ', learning_rate:', rates)\n                    x_train, x_test, y_train, y_test = train_test_split(X_train, Y_train, test_size=0.3)\n\n                    XGBR = XGBRegressor(n_estimators=estimator, early_stopping_rounds=rounds ,learning_rate=rates)\n                    XGBR.fit(x_train, y_train, eval_set=[(x_test, y_test)], verbose=False)\n\n                    predicted = XGBR.predict(x_test)\n                    wmaes_cv.append(WMAE(x_test, y_test, predicted))\n                print('WMAE:', np.mean(wmaes_cv))\n                result.append({'Early Stopping Rounds': rounds, 'Estimators': estimator, 'WMAE': np.mean(wmaes_cv)})\n    return pd.DataFrame(result)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"var_list = ['Dept','IsHoliday' ,'IsHoliday_Dec','Week','Year','Type','Size','cluster','dcluster']\n\nX_train = train3[var_list]\nY_train = train3['Weekly_Sales']\n\nn_estimators = [25,50,75,125]\nmax_depth = [20,25]\nrandom_forest(n_estimators, max_depth)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"var_list = ['Dept','IsHoliday' ,'IsHoliday_Dec','Week','Year','Type','Size','cluster','dcluster']\n\nX_train = train3[var_list]\nY_train = train3['Weekly_Sales']\n\nn_estimators = [125]\nmax_depth = [24,26]\nrandom_forest(n_estimators, max_depth)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"var_list = ['Dept','IsHoliday' ,'IsHoliday_Dec','Week','Year','Type','Size','cluster','dcluster']\n\nX_train = train3[var_list]\nY_train = train3['Weekly_Sales']\n\nn_estimators = [125]\nmax_depth = [27]\nrandom_forest(n_estimators, max_depth)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"var_list =  ['Store','Dept','IsHoliday' ,'IsHoliday_Dec','Week','Year','Type','Size']\n\nX_train = train3[var_list]\nY_train = train3['Weekly_Sales']\n\nn_estimators = [25,50,75,125]\nmax_depth = [20,25]\nrandom_forest(n_estimators, max_depth)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"var_list =  ['Store','Dept','IsHoliday' ,'IsHoliday_Dec','Week','Year','Type','Size']\n\nX_train = train3[var_list]\nY_train = train3['Weekly_Sales']\n\nn_estimators = [75]\nmax_depth = [24,26]\nrandom_forest(n_estimators, max_depth)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"var_list =  ['Store','Dept','IsHoliday' ,'IsHoliday_Dec','Week','Year','Type','Size']\n\nX_train = train3[var_list]\nY_train = train3['Weekly_Sales']\n\nn_estimators = [75]\nmax_depth = [22,23]\nrandom_forest(n_estimators, max_depth)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"var_list = ['Dept','IsHoliday' ,'IsHoliday_Dec','Week','Year','Type','Size','cluster','dcluster']\nX_train = train3[var_list]\nY_train = train3['Weekly_Sales']\nmax_features = ['auto' ,'sqrt', 'log2', 5, 7]\nrandom_forest_II(n_estimators=125, max_depth=26, max_features=max_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"var_list =  ['Store','Dept','IsHoliday' ,'IsHoliday_Dec','Week','Year','Type','Size']\nX_train = train3[var_list]\nY_train = train3['Weekly_Sales']\nmax_features = ['auto' ,'sqrt', 'log2', 5, 7]\nrandom_forest_II(n_estimators=75, max_depth=23, max_features=max_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"var_list = ['Dept','IsHoliday' ,'IsHoliday_Dec','Week','Year','Type','Size','cluster','dcluster']\nX_train = train3[var_list]\nY_train = train3['Weekly_Sales']\n\nmin_samples_split = [2, 3, 4]\nmin_samples_leaf = [1, 2, 3]\nrandom_forest_III(n_estimators=125, max_depth=26, max_features=7, \n                  min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"var_list =  ['Store','Dept','IsHoliday' ,'IsHoliday_Dec','Week','Year','Type','Size']\nX_train = train3[var_list]\nY_train = train3['Weekly_Sales']\n\nmin_samples_split = [2, 3, 4]\nmin_samples_leaf = [1, 2, 3]\nrandom_forest_III(n_estimators=75, max_depth=23, max_features='auto', \n                  min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"var_list =  ['Store','Dept','IsHoliday' ,'IsHoliday_Dec','Week','Year','Type','Size','cluster','dcluster']\nX_train = train3[var_list]\nY_train = train3['Weekly_Sales']\n\n\nn_est = [3000]\nesr = [5]\nlrs = [0.1,0.5,0.8]\nXGBRegressor_I(n_est,esr,lrs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"var_list =  ['Store','Dept','IsHoliday' ,'IsHoliday_Dec','Week','Year','Type','Size','cluster','dcluster']\nX_train = train3[var_list]\nY_train = train3['Weekly_Sales']\n\n\nn_est = [4000]\nesr = [5]\nlrs = [0.1,0.5]\nXGBRegressor_I(n_est,esr,lrs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"var_list = ['Dept','IsHoliday' ,'IsHoliday_Dec','Week','Year','Type','Size','cluster','dcluster']\n\nX_train = train3[var_list]\nY_train = train3['Weekly_Sales']\n\nRF = RandomForestRegressor(n_estimators=125, max_depth=26, max_features=7, min_samples_split=4, min_samples_leaf=1)\nRF.fit(X_train, Y_train)\n\ntest3 = test2.merge(df_scluster, how='left', on='Store')\ntest3 = test3.merge(df_dcluster, how='left', on='Dept')\n# test3 = test3.merge(df_sdcluster, how='left', on='Store_Dept')\n\nX_test = test3[var_list]\n\n# X_test.fillna(7, inplace=True)\npredict = RF.predict(X_test)\nFinal = test3[['Store', 'Dept', 'Week']]\nFinal['Weekly_Sales'] = predict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"var_list = ['Store','Dept','IsHoliday' ,'IsHoliday_Dec','Week','Year','Type','Size']\n\nX_train = train3[var_list]\nY_train = train3['Weekly_Sales']\n\nRF = RandomForestRegressor(n_estimators=75, max_depth=23, max_features='auto', min_samples_split=3, min_samples_leaf=1)\nRF.fit(X_train, Y_train)\n\ntest3 = test2.merge(df_scluster, how='left', on='Store')\ntest3 = test3.merge(df_dcluster, how='left', on='Dept')\n# test3 = test3.merge(df_sdcluster, how='left', on='Store_Dept')\n\nX_test = test3[var_list]\n\n# X_test.fillna(7, inplace=True)\npredict = RF.predict(X_test)\nFinal_rf2 = test3[['Store', 'Dept', 'Week']]\nFinal_rf2['Weekly_Sales'] = predict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"estimator = 3000\nrounds = 5\nrates = 0.5\n\nvar_list = ['Store','Dept','IsHoliday' ,'IsHoliday_Dec','Week','Year','Type','Size','cluster','dcluster']\n\nX_train = train3[var_list]\nY_train = train3['Weekly_Sales']\n\nXGBR = XGBRegressor(n_estimators=estimator, early_stopping_rounds=rounds ,learning_rate=rates)\nXGBR.fit(X_train, Y_train, verbose=False)\n      \ntest3 = test2.merge(df_scluster, how='left', on='Store')\ntest3 = test3.merge(df_dcluster, how='left', on='Dept')\n\nX_test = test3[var_list]\npredict = XGBR.predict(X_test)\n\nFinal_XGBR = test3[['Store', 'Dept', 'Week']]\nFinal_XGBR['Weekly_Sales'] = predict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"estimator = 4000\nrounds = 5\nrates = 0.5\n\nvar_list = ['Store','Dept','IsHoliday' ,'IsHoliday_Dec','Week','Year','Type','Size','cluster','dcluster']\n\nX_train = train3[var_list]\nY_train = train3['Weekly_Sales']\n\nXGBR = XGBRegressor(n_estimators=estimator, early_stopping_rounds=rounds ,learning_rate=rates)\nXGBR.fit(X_train, Y_train, verbose=False)\n      \ntest3 = test2.merge(df_scluster, how='left', on='Store')\ntest3 = test3.merge(df_dcluster, how='left', on='Dept')\n\nX_test = test3[var_list]\npredict = XGBR.predict(X_test)\n\nFinal_XGBR2 = test3[['Store', 'Dept', 'Week']]\nFinal_XGBR2['Weekly_Sales'] = predict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Final_adj = pysqldf(\"\"\"\n#     with trans as \n#         (select cluster ,dcluster ,year ,sum(Weekly_Sales) as Weekly_Sales\n#         from train3 where week <= 43 and week >=32\n#         group by cluster ,dcluster ,year)\n\n#     ,lkp as \n#         (select distinct cluster,store ,dept ,dcluster from train3)\n\n#     ,final_pred as\n#         (SELECT a.Store ,a.Dept ,a.Week ,(a.Weekly_Sales*0.25 + b.Weekly_Sales*0.25 + c.Weekly_Sales*0.25 + d.Weekly_Sales*0.25) as Weekly_Sales\n#          from Final a\n#          left join  Final_XGBR b\n#          on a.Store = b.Store\n#          and a.Dept = b.Dept\n#          and a.Week = b.Week\n#          left join  Final_rf2 c\n#          on a.Store = c.Store\n#          and a.Dept = c.Dept\n#          and a.Week = c.Week\n#          left join  Final_XGBR2 d\n#          on a.Store = d.Store\n#          and a.Dept = d.Dept\n#          and a.Week = d.Week\n#         )\n#     SELECT fin.Store ,fin.Dept ,Week ,Weekly_Sales\n#          ,case when Week = 52 and last_sales > 2*Weekly_Sales then Weekly_Sales+(2.5/7)*last_sales \n#             when week > 43 and week <=51 and growth > 1.01 then 1.01*Weekly_Sales \n#             when week > 43 and week <=51 and growth < 0.99 then 0.99*Weekly_Sales \n#             when week > 43 and week <=51 and growth is null then 1*Weekly_Sales \n#             when week > 43 and week <=51 then growth*Weekly_Sales \n#              else Weekly_Sales \n#         end as Weekly_Sales_Adjusted\n#     from\n#         (SELECT Store ,Dept ,Week ,Weekly_Sales\n#             ,case when Week = 52 then lag(Weekly_Sales) over(partition by Store, Dept)  end as last_sales\n#         from final_pred\n#         ) fin\n#     left join lkp on fin.store = lkp.store and fin.dept = lkp.dept\n#     --\n#     left join \n#         (select a.cluster ,a.dcluster ,a.Weekly_Sales/b.Weekly_Sales as growth\n#         from trans a\n#         inner join trans b\n#         on a.cluster= b.cluster \n#         and a.dcluster = b.dcluster\n#         where a.year = 2012 and b.year = 2011\n#         ) b\n#     on lkp.dcluster = b.dcluster\n#     and lkp.cluster = b.cluster\n#     left join (select store ,dept ,min(Weekly_Sales) as Min_Weekly_Sales from train3 group by store ,dept) c\n#     on fin.Dept = c.Dept\n#     and fin.store = c.store \n# \"\"\")\n\n# sample_submission['Weekly_Sales'] = Final_adj['Weekly_Sales_Adjusted']\n# sample_submission.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from xgboost import XGBRegressor\n\n# x_train, x_test, y_train, y_test = train_test_split(X_train, Y_train, test_size=0.3)\n\n# my_model = XGBRegressor()\n# # Add silent=True to avoid printing out updates with each cycle\n# my_model.fit(x_train, y_train, verbose=False)\n\n# # make predictions\n# # predicted = my_model.predict(x_test)\n\n# from sklearn.metrics import mean_absolute_error\n# print(\"WMAE: \" + str(WMAE(x_test, y_test, predicted)))\n\n# XGBR = XGBRegressor(n_estimators=1000, learning_rate=0.05)\n# XGBR.fit(x_train, y_train, early_stopping_rounds=1, eval_set=[(x_test, y_test)], verbose=False)\n    \n# # my_model = XGBRegressor(n_estimators=1000, learning_rate=0.05)\n# # my_model.fit(x_train, y_train, early_stopping_rounds=5, \n# #              eval_set=[(x_test, y_test)], verbose=False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# #Retrodiction of Markdown data\n# store_fact = store_fact.fillna(0)\n\n# store_fact_temp  = pysqldf(\"\"\"\n#     select a.Store \n#         ,case when a.MarkDown1 =  0 then b.MarkDown1 else a.MarkDown1 end as MarkDown1\n#         ,case when a.MarkDown2 =  0 then b.MarkDown2 else a.MarkDown2 end as MarkDown2\n#         ,case when a.MarkDown3 =  0 then b.MarkDown3 else a.MarkDown3 end as MarkDown3\n#         ,case when a.MarkDown4 =  0 then b.MarkDown4 else a.MarkDown4 end as MarkDown4\n#         ,case when a.MarkDown5 =  0 then b.MarkDown5 else a.MarkDown5 end as MarkDown5\n#         ,WeekYear\n#     from store_fact a\n#     left join \n#         (select Week,Year-1 as Year,Store ,MarkDown1,MarkDown2,MarkDown3,MarkDown4,MarkDown5\n#         from store_fact a\n#         where year = 2012\n#         union \n#         select Week,Year-2 as Year,Store ,MarkDown1,MarkDown2,MarkDown3,MarkDown4,MarkDown5\n#         from store_fact a\n#         where year = 2012\n#         ) b\n#     on a.Store =  b.Store\n#     and a.Week =  b.Week\n#     and a.Year =  b.Year\n#     \"\"\")\n# store_fact['MarkDown1'] = store_fact_temp['MarkDown1']\n# store_fact['MarkDown2'] = store_fact_temp['MarkDown2']\n# store_fact['MarkDown3'] = store_fact_temp['MarkDown3']\n# store_fact['MarkDown4'] = store_fact_temp['MarkDown4']\n# store_fact['MarkDown5'] = store_fact_temp['MarkDown5']","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}