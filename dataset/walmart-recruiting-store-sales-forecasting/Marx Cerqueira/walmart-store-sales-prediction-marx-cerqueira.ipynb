{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Walmart Store Sales  Predicion**\n**made by: Marx Cerqueira**","metadata":{}},{"cell_type":"markdown","source":"<img src=\"https://i.ytimg.com/vi/XRRu9cea1sg/maxresdefault.jpg\" alt=\"some text\" width=500 height=400 align=\"left\">\n","metadata":{"cell_style":"split"}},{"cell_type":"markdown","source":"This project is an end-to-end Data Science project with a regression adapted for time series as solution was created four machine learning models to forecast the weekly sales. Predictions can be accessed by users through a submission csv at the end.","metadata":{}},{"cell_type":"markdown","source":"**In this notebook, I have included the following contents:**\n\n**Table of Contents**\n\n* [1 Project Solution Planning](#section-one)\n* [2 Business Problem](#section-two)\n* [3 Imports](#section-three)\n* [4 Loading Data](#section-four)\n* [5 Data Description](#section-five)\n    - [5.1 Rename Columns](#subsection-five-one)\n    - [5.2 Data Dimension](#subsection-five-two)\n    - [5.3 Data Types](#subsection-five-three)\n    - [5.4 Check NA Values](#subsection-five-four)\n    - [5.5 Fillout NAs](#subsection-five-five)\n    - [5.6 Change dtypes](#subsection-five-six)\n    - [5.7 Descriptive Statistics](#subsection-five-seven)\n* [6 Feature Engineering](#section-six)\n    - [6.1 Hypothesis Mindmap](#subsection-six-one)\n    - [6.2 Hypothesis](#subsection-six-two)\n    - [6.3 Final Hypothesis List](#subsection-six-three)\n    - [6.4 Feature Engineering](#subsection-six-four)\n* [7 Variable Filtering](#section-seven)\n* [8 Exploratory Data Analysis (EDA)](#section-eight)\n    - [8.1 Univariate Analysis](#subsection-eight-one)\n    - [8.2 Bivariate Analysis](#subsection-eight-two)\n    - [8.3 Multivariate Analysis](#subsection-eight-three)\n* [9 Data Preparation](#section-nine)\n    - [9.1 Split dataframe into traning and validation](#subsection-nine-one)\n    - [9.2 Checking Features Outliers Presence](#subsection-nine-two)\n    - [9.3 Feature Rescaling](#subsection-nine-three)\n    - [9.4 Feature Transformation](#subsection-nine-four)\n    - [9.5 Apply Transformations on Validation Dataset](#subsection-nine-five)\n* [10 Feature Selection](#section-ten)\n* [11 Machine Learning Models](#section-eleven)\n    - [11.1 Average Model](#subsection-eleven-one)\n    - [11.2 Linear Regression Model](#subsection-eleven-two)\n    - [11.3 Linear Regression Model - Lasso](#subsection-eleven-three)\n    - [11.4 Random Forest](#subsection-eleven-four)\n    - [11.5 XGBoost Regressor](#subsection-eleven-five)\n    - [11.6 LightGBM Regressor](#subsection-eleven-six)\n    - [11.7 Compare Models Performance](#subsection-eleven-seven)\n* [12 Hyperparameter Fine tunning](#section-twelve)\n    - [12.1 Random Search](#subsection-twelve-one)\n    - [12.2 Final Model](#subsection-twelve-two)\n* [13 Error Interpretation](#section-thirteen)\n    - [13.1 Business Performance - Store Granularity](#subsection-thirteen-one)\n    - [13.2 Business Performance - Department Granularity](#subsection-thirteen-two)\n    - [13.3 Total Performance](#subsection-thirteen-one)\n    - [13.4 Machine Learning Performance](#subsection-thirteen-two)\n* [14 Model Submission](#section-fourteen)\n    - [14.1 Load Model and Scalers](#subsection-thirteen-one)\n    - [14.2 Data ETL](#subsection-thirteen-two)\n* [15 Conclusion](#section-fifteen)\n* [16 Next Steps](#section-sixteen)","metadata":{}},{"cell_type":"markdown","source":"## 1 PROJECT SOLUTION PLANNING\n<a id=\"section-one\"></a>","metadata":{"heading_collapsed":true}},{"cell_type":"markdown","source":"### 1.1 Input","metadata":{"ExecuteTime":{"end_time":"2021-04-07T12:44:04.386844Z","start_time":"2021-04-07T12:44:04.384084Z"},"hidden":true}},{"cell_type":"markdown","source":"1. Business problem\n    - The CFO wanted to reinvest in all stores, therefore, he need to know how much revenue each store will bring so he can invest it now.\n    \n2. Datasets:\n\n    - **stores.csv**\n    - **train.csv**\n    - **test.csv**\n    - **features.csv**\n","metadata":{"hidden":true}},{"cell_type":"markdown","source":"### 1.2 Output\n","metadata":{"hidden":true}},{"cell_type":"markdown","source":"1. Deliverables:\n\n- Model's performance and results report with the following topics:\n    - What's the weekly sales in dollars of each store and department?\n    - Predictions will be available through a csv where stakeholders can access the predictions\n             \n2. Business Report with all insights","metadata":{"hidden":true}},{"cell_type":"markdown","source":"### 1.3 Tasks","metadata":{"hidden":true}},{"cell_type":"markdown","source":"**Project Development Method**\n\nThe project was developed based on the CRISP-DS (Cross-Industry Standard Process - Data Science, a.k.a. CRISP-DM) project management method, with the following steps:\n\n- Project Planning\n- Business Understanding;\n- Data Collection;\n- Data Cleaning;\n- Exploratory Data Analysis (EDA);\n- Data Preparation;\n- Machine Learning Modelling and fine-tuning;\n- Model and Business performance evaluation / Results;\n- Model deployment.\n","metadata":{"hidden":true}},{"cell_type":"markdown","source":"<img src=\"https://www.researchgate.net/profile/Youssef-Tounsi-2/publication/341627969/figure/fig1/AS:903550875996160@1592434724532/CRISP-DM-data-mining-framework.png\" alt=\"some text\" width=500 height=400 align=\"left\">\n","metadata":{"ExecuteTime":{"end_time":"2022-05-25T20:26:12.404036Z","start_time":"2022-05-25T20:26:12.399247Z"},"hidden":true}},{"cell_type":"markdown","source":"## 2 BUSINESS PROBLEM\n<a id=\"section-two\"></a>","metadata":{"ExecuteTime":{"end_time":"2021-09-28T11:49:15.556175Z","start_time":"2021-09-28T11:49:15.550801Z"},"heading_collapsed":true}},{"cell_type":"markdown","source":"Walmart Stores Sales\n\n- A private multinational retail corporation that operates a chain of hypermarkets.\n\n- Walmart owns hypermarkets (also called supercenters), discount department stores, and grocery stores from the United States\n\n- Business Model: Product sales.","metadata":{"hidden":true}},{"cell_type":"markdown","source":"The problem:\n- The CFO wanted to reinvest in all stores, therefore, he need to know how much revenue each store will bring so he can invest it now.","metadata":{"hidden":true}},{"cell_type":"markdown","source":"Goal:\n- Predict the weekly sales of all stores.","metadata":{"hidden":true}},{"cell_type":"markdown","source":"Deliverables:\n- Model's performance and results report with the following topics:\n    - What's the weekly sales in dollars of each store and department?\n    - Predictions will be available through a csv file where stakeholders can access the prediction by a smartphone","metadata":{"hidden":true}},{"cell_type":"markdown","source":"## 3 IMPORTS\n<a id=\"section-three\"></a>","metadata":{}},{"cell_type":"code","source":"pip install inflection","metadata":{"execution":{"iopub.status.busy":"2022-05-30T16:14:42.656674Z","iopub.execute_input":"2022-05-30T16:14:42.657222Z","iopub.status.idle":"2022-05-30T16:14:57.19107Z","shell.execute_reply.started":"2022-05-30T16:14:42.657175Z","shell.execute_reply":"2022-05-30T16:14:57.18973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import math\nimport pandas                as pd\nimport numpy                 as np\nimport seaborn               as sns\nimport matplotlib.pyplot     as plt\nimport datetime\nimport inflection\nimport warnings\nimport random\nimport pickle\nimport json\n\nimport xgboost               as xgb\nimport lightgbm              as lgbm\n    \nfrom pandas.api.types        import is_string_dtype, is_numeric_dtype\nfrom matplotlib              import gridspec\nfrom scipy                   import stats as ss\nfrom sklearn.preprocessing   import RobustScaler, MinMaxScaler, LabelEncoder\nfrom sklearn.ensemble        import RandomForestRegressor\nfrom sklearn.metrics         import mean_absolute_error, mean_squared_error\nfrom sklearn.linear_model    import LinearRegression, Lasso\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom boruta                  import BorutaPy\n\nfrom IPython.core.display    import HTML\nfrom IPython.display         import Image\n\n# Versão da Linguagem Python\nfrom platform                import python_version\nprint('Versão da Linguagem Python Usada Neste Jupyter Notebook:', python_version())\nwarnings.filterwarnings( 'ignore' )","metadata":{"ExecuteTime":{"end_time":"2022-05-30T10:44:11.812151Z","start_time":"2022-05-30T10:44:10.035394Z"},"execution":{"iopub.status.busy":"2022-05-30T16:14:57.193618Z","iopub.execute_input":"2022-05-30T16:14:57.19436Z","iopub.status.idle":"2022-05-30T16:14:58.774656Z","shell.execute_reply.started":"2022-05-30T16:14:57.194313Z","shell.execute_reply":"2022-05-30T16:14:58.773941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.1 Helper Functions","metadata":{}},{"cell_type":"code","source":"def jupyter_settings():\n    %matplotlib inline\n    %pylab inline\n    \n    plt.style.use( 'bmh' )\n    plt.rcParams['figure.figsize'] = [25, 12]\n    plt.rcParams['font.size'] = 24\n    \n    display( HTML( '<style>.container { width:100% !important; }</style>') )\n    pd.options.display.max_columns = None\n    pd.options.display.max_rows = None\n    pd.set_option( 'display.expand_frame_repr', False )\n    \n    sns.set()\n    \ndef cramer_v( x, y ):\n    cm = pd.crosstab( x, y ).values # Confusion Matrix\n    n = cm.sum()\n    r, k = cm.shape\n    \n    chi2 = ss.chi2_contingency( cm )[0]\n    chi2corr = max( 0, chi2 - (k-1)*(r-1)/(n-1) )\n    \n    kcorr = k - (k-1)**2/(n-1)\n    rcorr = r - (r-1)**2/(n-1)\n    \n    return np.sqrt( (chi2corr/n) / ( min( kcorr-1, rcorr-1 ) ) )\n\ndef mean_absolute_percentage_error( y, yhat ):\n    y, yhat = np.array(y), np.array(yhat)\n    return np.mean( np.abs( ( y-yhat ) / y ))\n\ndef mean_percentage_error( y, yhat ):\n    return np.mean( ( y - yhat ) / y )\n\n# Define the function to evaluate the models\ndef weighted_mean_absolute_error(df, y, yhat):\n    weights = df.is_holiday.apply(lambda x: 5 if x else 1)\n    return np.round(np.sum(weights*abs(y-yhat))/(np.sum(weights)), 2)\n\ndef ml_error( df,model_name, y, yhat):\n    mae = mean_absolute_error( y,yhat )\n    mape = mean_absolute_percentage_error( y,yhat )\n    rmse = np.sqrt(mean_squared_error( y,yhat ))\n    WMAE = weighted_mean_absolute_error(df, y, yhat)\n    \n    return pd.DataFrame( {'Model Name': model_name,\n                          'MAE': mae,\n                          'RMSE': rmse,\n                          'WMAE': WMAE}, index=[0])\n\n# time-series cross validation implementation\ndef cross_validation( x_training, kfold, model_name, model, verbose=False ):\n    mae_list = []\n    mape_list = []\n    rmse_list = []\n    WMAE_list = []\n     \n    for k in reversed( range( 1, kfold+1 ) ): #k-fold implementation\n        if verbose:\n            print( '\\nKFold Number: {}'.format( k ) )\n        # start and end date for validation \n        start_date_validation = x_training['date'].max() - datetime.timedelta( weeks=k*22) #primeira semanada da venda realizada\n        end_date_validation = x_training['date'].max() - datetime.timedelta( weeks=(k-1)*22) #ultima semana\n\n        # filtering dataset\n        training = x_training[x_training['date'] < start_date_validation]\n        validation = x_training[(x_training['date'] >= start_date_validation) & (x_training['date'] <= end_date_validation)]\n\n        # training and validation dataset\n        # training\n        xtraining = training.drop( ['date', 'weekly_sales'], axis=1 ) \n        ytraining = training['weekly_sales']\n\n        # validation\n        xvalidation = validation.drop( ['date', 'weekly_sales'], axis=1 )\n        yvalidation = validation['weekly_sales']\n\n        # model\n        m = model.fit( xtraining, ytraining )\n\n        # prediction\n        yhat = m.predict(xvalidation)\n\n        # performance\n        m_result = ml_error( xvalidation, model_name, np.expm1( yvalidation ), np.expm1( yhat ) )\n\n        # store performance of each kfold iteration\n        mae_list.append(  m_result['MAE'] )\n        rmse_list.append( m_result['RMSE'] )\n        WMAE_list.append( m_result['WMAE'])\n\n    return pd.DataFrame( {'Model Name': model_name,\n                          'MAE CV':  np.round( np.mean( mae_list ), 2 ).astype( str )  + ' +/- ' + np.round( np.std( mae_list ), 2 ).astype( str ),\n                          'RMSE CV': np.round( np.mean( rmse_list ), 2 ).astype( str ) + ' +/- ' + np.round( np.std( rmse_list ), 2 ).astype( str ),\n                          'WMAE CV': np.round( np.mean( WMAE_list ), 2 ).astype( str ) + ' +/- ' + np.round( np.std( WMAE_list ), 2 ).astype( str )}, index=[0] )\n","metadata":{"ExecuteTime":{"end_time":"2022-05-30T10:44:46.680472Z","start_time":"2022-05-30T10:44:46.598151Z"},"execution":{"iopub.status.busy":"2022-05-30T16:14:58.77611Z","iopub.execute_input":"2022-05-30T16:14:58.776772Z","iopub.status.idle":"2022-05-30T16:14:58.833514Z","shell.execute_reply.started":"2022-05-30T16:14:58.776738Z","shell.execute_reply":"2022-05-30T16:14:58.832535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"jupyter_settings()","metadata":{"ExecuteTime":{"end_time":"2022-05-30T10:44:50.479457Z","start_time":"2022-05-30T10:44:50.453397Z"},"execution":{"iopub.status.busy":"2022-05-30T16:15:01.275933Z","iopub.execute_input":"2022-05-30T16:15:01.276744Z","iopub.status.idle":"2022-05-30T16:15:01.291316Z","shell.execute_reply.started":"2022-05-30T16:15:01.276684Z","shell.execute_reply":"2022-05-30T16:15:01.290273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4 LOADING DATA\n<a id=\"section-four\"></a>","metadata":{}},{"cell_type":"code","source":"#project home path for importing files\n# home_path = '/home/marxcerqueira/repos/ze-delivery-prediction-case/'\nhome_kaggle = '../input/'\n\n#loading datasets available for this project\ndf_sales_raw  = pd.read_csv(home_kaggle + 'dataset/train.csv', low_memory = False)\ndf_test_raw   = pd.read_csv(home_kaggle + 'dataset/test.csv', low_memory = False)\ndf_features   = pd.read_csv(home_kaggle + 'dataset/features.csv', low_memory = False)\ndf_stores     = pd.read_csv(home_kaggle + 'dataset/stores.csv', low_memory = False)","metadata":{"ExecuteTime":{"end_time":"2022-05-30T10:45:00.039714Z","start_time":"2022-05-30T10:44:59.844089Z"},"execution":{"iopub.status.busy":"2022-05-30T16:15:10.461047Z","iopub.execute_input":"2022-05-30T16:15:10.461456Z","iopub.status.idle":"2022-05-30T16:15:10.956143Z","shell.execute_reply.started":"2022-05-30T16:15:10.461425Z","shell.execute_reply":"2022-05-30T16:15:10.955311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#first look at the dataframes\ndf_sales_raw.head()","metadata":{"ExecuteTime":{"end_time":"2022-05-29T14:02:05.474817Z","start_time":"2022-05-29T14:02:05.447787Z"},"code_folding":[0],"execution":{"iopub.status.busy":"2022-05-30T16:15:21.432318Z","iopub.execute_input":"2022-05-30T16:15:21.433036Z","iopub.status.idle":"2022-05-30T16:15:21.455644Z","shell.execute_reply.started":"2022-05-30T16:15:21.432996Z","shell.execute_reply":"2022-05-30T16:15:21.454557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#first look at the dataframes\ndf_features.head()","metadata":{"ExecuteTime":{"end_time":"2022-05-29T14:02:09.571246Z","start_time":"2022-05-29T14:02:09.56139Z"},"code_folding":[0],"execution":{"iopub.status.busy":"2022-05-30T16:15:23.626067Z","iopub.execute_input":"2022-05-30T16:15:23.627051Z","iopub.status.idle":"2022-05-30T16:15:23.645075Z","shell.execute_reply.started":"2022-05-30T16:15:23.627007Z","shell.execute_reply":"2022-05-30T16:15:23.644366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#first look at the dataframes\ndf_stores.head()","metadata":{"ExecuteTime":{"end_time":"2022-05-29T14:02:11.275841Z","start_time":"2022-05-29T14:02:11.270338Z"},"code_folding":[0],"execution":{"iopub.status.busy":"2022-05-30T16:15:25.840185Z","iopub.execute_input":"2022-05-30T16:15:25.8412Z","iopub.status.idle":"2022-05-30T16:15:25.85227Z","shell.execute_reply.started":"2022-05-30T16:15:25.841147Z","shell.execute_reply":"2022-05-30T16:15:25.851208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Datasets Feature Description**\n\n**stores.csv**\n\nThis file contains anonymized information about the 45 stores, indicating the type and size of store.\n\n**train.csv**\n\nThis is the historical training data, which covers to 2010-02-05 to 2012-11-01. Within this file you will find the following fields:\n\n- Store - the store number\n- Dept - the department number\n- Date - the week\n- Weekly_Sales -  sales for the given department in the given store\n- IsHoliday - whether the week is a special holiday week\n\n**test.csv**\n\nThis file is identical to **train.csv**, except we have withheld the weekly sales. You must predict the sales for each triplet of store, department, and date in this file.\n\n**features.csv**\n\nThis file contains additional data related to the store, department, and regional activity for the given dates. It contains the following fields:\n\n- Store - the store number\n- Date - the week\n- Temperature - average temperature in the region\n- Fuel_Price - cost of fuel in the region\n- MarkDown1-5 - anonymized data related to promotional markdowns that Walmart is running. MarkDown data is only available after Nov 2011, and is not available for all stores all the time. Any missing value is marked with an NA.\n- CPI - the consumer price index\n- Unemployment - the unemployment rate\n- IsHoliday - whether the week is a special holiday week\n\nFor convenience, the four holidays fall within the following weeks in the dataset (not all holidays are in the data):\n\n- Super Bowl: 12-Feb-10, 11-Feb-11, 10-Feb-12, 8-Feb-13\n- Labor Day: 10-Sep-10, 9-Sep-11, 7-Sep-12, 6-Sep-13\n- Thanksgiving: 26-Nov-10, 25-Nov-11, 23-Nov-12, 29-Nov-13\n- Christmas: 31-Dec-10, 30-Dec-11, 28-Dec-12, 27-Dec-13","metadata":{}},{"cell_type":"markdown","source":"### 4.1 Merge datasets","metadata":{}},{"cell_type":"markdown","source":"The datasets keys here can be 'Store', 'Dept' and 'IsHoliday'.","metadata":{}},{"cell_type":"code","source":"# merge datasets into one\ndf_store_feature = df_features.merge(df_stores, on = 'Store', how = 'left')\n\n# main dataframe for exploring\ndf0 = df_sales_raw.merge(df_store_feature, on = ['Store', 'Date', 'IsHoliday'], how = 'left').sort_values(['Store','Dept','Date'])","metadata":{"ExecuteTime":{"end_time":"2022-05-30T09:00:19.099738Z","start_time":"2022-05-30T09:00:18.898429Z"},"execution":{"iopub.status.busy":"2022-05-30T16:17:33.422621Z","iopub.execute_input":"2022-05-30T16:17:33.423269Z","iopub.status.idle":"2022-05-30T16:17:33.86179Z","shell.execute_reply.started":"2022-05-30T16:17:33.42322Z","shell.execute_reply":"2022-05-30T16:17:33.860831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#take a first look at the main dataset after merges\ndf0.head()","metadata":{"ExecuteTime":{"end_time":"2022-05-29T14:03:56.996464Z","start_time":"2022-05-29T14:03:56.977096Z"},"code_folding":[0],"execution":{"iopub.status.busy":"2022-05-30T16:17:36.387804Z","iopub.execute_input":"2022-05-30T16:17:36.388239Z","iopub.status.idle":"2022-05-30T16:17:36.409559Z","shell.execute_reply.started":"2022-05-30T16:17:36.388186Z","shell.execute_reply":"2022-05-30T16:17:36.408436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5 DATA DESCRIPTION\n<a id=\"section-five\"></a>","metadata":{}},{"cell_type":"code","source":"#Copy dataset\ndf1 = df0.copy()","metadata":{"ExecuteTime":{"end_time":"2022-05-30T09:01:35.471244Z","start_time":"2022-05-30T09:01:35.423156Z"},"execution":{"iopub.status.busy":"2022-05-30T16:21:32.1561Z","iopub.execute_input":"2022-05-30T16:21:32.156633Z","iopub.status.idle":"2022-05-30T16:21:32.250632Z","shell.execute_reply.started":"2022-05-30T16:21:32.156599Z","shell.execute_reply":"2022-05-30T16:21:32.249606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5.1 Rename Columns\n<a id=\"subsection-five-one\"></a>","metadata":{}},{"cell_type":"code","source":"cols_old = df1.columns\n\nsnakecase = lambda x: inflection.underscore(x)\n\ncols_new = list( map( snakecase, cols_old ) )\n\n#Rename Columns\ndf1.columns = cols_new","metadata":{"ExecuteTime":{"end_time":"2022-05-30T09:01:36.794218Z","start_time":"2022-05-30T09:01:36.78674Z"},"execution":{"iopub.status.busy":"2022-05-30T16:21:33.731131Z","iopub.execute_input":"2022-05-30T16:21:33.731553Z","iopub.status.idle":"2022-05-30T16:21:33.737786Z","shell.execute_reply.started":"2022-05-30T16:21:33.731517Z","shell.execute_reply":"2022-05-30T16:21:33.736939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#checking cols transformation\ndf1.columns","metadata":{"ExecuteTime":{"end_time":"2022-05-29T14:05:03.367651Z","start_time":"2022-05-29T14:05:03.363566Z"},"scrolled":true,"execution":{"iopub.status.busy":"2022-05-30T16:21:35.641537Z","iopub.execute_input":"2022-05-30T16:21:35.642198Z","iopub.status.idle":"2022-05-30T16:21:35.648814Z","shell.execute_reply.started":"2022-05-30T16:21:35.642164Z","shell.execute_reply":"2022-05-30T16:21:35.648037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5.2 Data Dimension\n<a id=\"subsection-five-two\"></a>","metadata":{}},{"cell_type":"code","source":"# checking data dimesions to see if we have enough computational power\nprint( 'Number of Rows: {}'.format( df1.shape[0] ) )\nprint( 'Number of Cols: {}'.format( df1.shape[1] ) )","metadata":{"ExecuteTime":{"end_time":"2022-05-29T16:59:37.494251Z","start_time":"2022-05-29T16:59:37.484025Z"},"execution":{"iopub.status.busy":"2022-05-30T16:21:37.745509Z","iopub.execute_input":"2022-05-30T16:21:37.746209Z","iopub.status.idle":"2022-05-30T16:21:37.752018Z","shell.execute_reply.started":"2022-05-30T16:21:37.746171Z","shell.execute_reply":"2022-05-30T16:21:37.750933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5.3 Data Types\n<a id=\"subsection-five-three\"></a>","metadata":{}},{"cell_type":"code","source":"# checking features dtypes\ndf1.dtypes","metadata":{"ExecuteTime":{"end_time":"2022-05-29T14:04:15.177804Z","start_time":"2022-05-29T14:04:15.172249Z"},"execution":{"iopub.status.busy":"2022-05-30T16:21:39.082245Z","iopub.execute_input":"2022-05-30T16:21:39.082908Z","iopub.status.idle":"2022-05-30T16:21:39.091054Z","shell.execute_reply.started":"2022-05-30T16:21:39.08287Z","shell.execute_reply":"2022-05-30T16:21:39.090044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5.4 Check NA values\n<a id=\"subsection-five-four\"></a>","metadata":{}},{"cell_type":"code","source":"# checking NA values sum and its percentagem from total number of rows\nmissing_count = df1.isnull().sum() # the count of missing values\nvalue_count = df1.isnull().count() # the count of all values\n\nmissing_percentage = round(missing_count/value_count *100, 2) # the percentage of missing values\nmissing_df = pd.DataFrame({'missing values count': missing_count, 'percentage': missing_percentage})\nmissing_df","metadata":{"ExecuteTime":{"end_time":"2022-05-29T14:04:17.567843Z","start_time":"2022-05-29T14:04:17.323022Z"},"execution":{"iopub.status.busy":"2022-05-30T16:21:40.978643Z","iopub.execute_input":"2022-05-30T16:21:40.979463Z","iopub.status.idle":"2022-05-30T16:21:41.202608Z","shell.execute_reply.started":"2022-05-30T16:21:40.979419Z","shell.execute_reply":"2022-05-30T16:21:41.201739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# missing na chart\nbarchart = missing_df.plot.bar(y='percentage')\nfor index, percentage in enumerate( missing_percentage ):\n    barchart.text( index, percentage, str(percentage)+'%')","metadata":{"ExecuteTime":{"end_time":"2022-05-28T00:14:46.488569Z","start_time":"2022-05-28T00:14:46.154423Z"},"code_folding":[0],"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-05-30T16:21:47.24676Z","iopub.execute_input":"2022-05-30T16:21:47.247192Z","iopub.status.idle":"2022-05-30T16:21:47.700725Z","shell.execute_reply.started":"2022-05-30T16:21:47.247146Z","shell.execute_reply":"2022-05-30T16:21:47.699546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Markdown 1-5 columns have NAs, all other columns are complete.\n\n- It contains lots of missing values, more than 64% of NAs in each markdown column. \n\n- They correspond to the promotional activities being carried out at different stores. \n\n- The promotional markdowns only started after November 2011 and is not running all the times at all the stores. So this makes sense why these columns have lot of NAs values. \n\n- Let's perform the exploratory data analysis and study their relationship with the weekly sales and then we will decide about these columns and the missing values","metadata":{}},{"cell_type":"markdown","source":"### 5.5 Fillout NA\n<a id=\"subsection-five-five\"></a>","metadata":{}},{"cell_type":"code","source":"# replacing NAs with 0\ndf1 = df1.fillna(0)","metadata":{"ExecuteTime":{"end_time":"2022-05-30T09:01:41.479095Z","start_time":"2022-05-30T09:01:41.371997Z"},"execution":{"iopub.status.busy":"2022-05-30T16:21:52.262333Z","iopub.execute_input":"2022-05-30T16:21:52.263132Z","iopub.status.idle":"2022-05-30T16:21:52.393825Z","shell.execute_reply.started":"2022-05-30T16:21:52.263087Z","shell.execute_reply":"2022-05-30T16:21:52.392771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- replacing NAs with 0 even though this action will create more bias to the model\n- during the next cicle of CRIPS we will take a deep look into it.","metadata":{}},{"cell_type":"markdown","source":"### 5.6 Change Types\n<a id=\"subsection-five-six\"></a>","metadata":{}},{"cell_type":"code","source":"#converting feature 'date' to datetime\ndf1['date'] = pd.to_datetime( df1[ 'date' ] )","metadata":{"ExecuteTime":{"end_time":"2022-05-30T09:01:42.946805Z","start_time":"2022-05-30T09:01:42.891017Z"},"execution":{"iopub.status.busy":"2022-05-30T16:21:54.047781Z","iopub.execute_input":"2022-05-30T16:21:54.048199Z","iopub.status.idle":"2022-05-30T16:21:54.12528Z","shell.execute_reply.started":"2022-05-30T16:21:54.048164Z","shell.execute_reply":"2022-05-30T16:21:54.12454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1.dtypes # checking datatypes transformation","metadata":{"ExecuteTime":{"end_time":"2022-05-29T18:14:02.467777Z","start_time":"2022-05-29T18:14:02.456013Z"},"execution":{"iopub.status.busy":"2022-05-30T16:21:56.006324Z","iopub.execute_input":"2022-05-30T16:21:56.007089Z","iopub.status.idle":"2022-05-30T16:21:56.015217Z","shell.execute_reply.started":"2022-05-30T16:21:56.00705Z","shell.execute_reply":"2022-05-30T16:21:56.0141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5.7 Descriptive Statistics\n<a id=\"subsection-five-seven\"></a>","metadata":{}},{"cell_type":"markdown","source":"- It's usefull to get the first knowledge of the business problem over the features and we can detect some data erros","metadata":{}},{"cell_type":"code","source":"# separate numerical and categorical attributes\nnum_attributes = df1.select_dtypes( include = 'number')\ncate_attributes = df1.select_dtypes( include = 'object')","metadata":{"ExecuteTime":{"end_time":"2022-05-28T00:14:46.861777Z","start_time":"2022-05-28T00:14:46.858942Z"},"execution":{"iopub.status.busy":"2022-05-30T16:22:00.763972Z","iopub.execute_input":"2022-05-30T16:22:00.764368Z","iopub.status.idle":"2022-05-30T16:22:00.786938Z","shell.execute_reply.started":"2022-05-30T16:22:00.764337Z","shell.execute_reply":"2022-05-30T16:22:00.78609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 5.7.1 Numerical Attributes","metadata":{}},{"cell_type":"code","source":"# Central Tendency - Mean, median\nct1 = pd.DataFrame( num_attributes.apply( np.mean ) ).T\nct2 = pd.DataFrame( num_attributes.apply( np.median ) ).T\n\n# Dispersion - std, min, max, range, skew, kurtoisis\nd1 = pd.DataFrame(num_attributes.apply( np.std )).T\nd2 = pd.DataFrame(num_attributes.apply( min )).T\nd3 = pd.DataFrame(num_attributes.apply( max )).T\nd4 = pd.DataFrame(num_attributes.apply( lambda x: x.max() - x.min() )).T\nd5 = pd.DataFrame(num_attributes.apply( lambda x: x.skew() )).T\nd6 = pd.DataFrame(num_attributes.apply( lambda x: x.kurtosis() )).T\n\n#concatenate\nm1 = pd.concat( [d2, d3, d4, ct1, ct2, d1, d5, d6] ).T.reset_index()\n\n\nm1.columns = ['attributes', 'min', 'max', 'range', 'mean', 'median', 'std', 'skew', 'kurtosis']\nm1","metadata":{"ExecuteTime":{"end_time":"2022-05-28T00:14:48.031606Z","start_time":"2022-05-28T00:14:46.946237Z"},"code_folding":[0],"execution":{"iopub.status.busy":"2022-05-30T16:22:03.250585Z","iopub.execute_input":"2022-05-30T16:22:03.250998Z","iopub.status.idle":"2022-05-30T16:22:05.104556Z","shell.execute_reply.started":"2022-05-30T16:22:03.250966Z","shell.execute_reply":"2022-05-30T16:22:05.103547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check numerical features distribution\nnum_attributes.hist(bins = 50);\nplt.style.use('tableau-colorblind10');","metadata":{"ExecuteTime":{"end_time":"2022-05-28T00:14:50.263686Z","start_time":"2022-05-28T00:14:48.033685Z"},"code_folding":[0],"execution":{"iopub.status.busy":"2022-05-30T16:22:05.106406Z","iopub.execute_input":"2022-05-30T16:22:05.106881Z","iopub.status.idle":"2022-05-30T16:22:08.631095Z","shell.execute_reply.started":"2022-05-30T16:22:05.106836Z","shell.execute_reply":"2022-05-30T16:22:08.630191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- histograms were take into account to check how features distributions behave","metadata":{}},{"cell_type":"markdown","source":"#### 5.7.2 Categorical Attributes","metadata":{}},{"cell_type":"code","source":"# check unique values of categorical features\ncate_attributes.apply( lambda x: x.unique().shape[0])","metadata":{"ExecuteTime":{"end_time":"2022-05-28T00:14:50.292099Z","start_time":"2022-05-28T00:14:50.265944Z"},"execution":{"iopub.status.busy":"2022-05-30T16:22:09.031737Z","iopub.execute_input":"2022-05-30T16:22:09.032177Z","iopub.status.idle":"2022-05-30T16:22:09.072129Z","shell.execute_reply.started":"2022-05-30T16:22:09.03214Z","shell.execute_reply":"2022-05-30T16:22:09.071055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cate_attributes.type.value_counts()","metadata":{"ExecuteTime":{"end_time":"2022-05-28T00:14:50.311688Z","start_time":"2022-05-28T00:14:50.29411Z"},"execution":{"iopub.status.busy":"2022-05-30T16:22:11.122168Z","iopub.execute_input":"2022-05-30T16:22:11.122865Z","iopub.status.idle":"2022-05-30T16:22:11.180132Z","shell.execute_reply.started":"2022-05-30T16:22:11.122824Z","shell.execute_reply":"2022-05-30T16:22:11.179121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot boxplots of categorical features against target variable\naux1 = df1[(df1['type'] != '0') & (df1['weekly_sales'] > 0)]\n\nplt.subplot (1, 3, 1)\nsns.boxplot(x='type', y= 'weekly_sales', data=aux1);","metadata":{"ExecuteTime":{"end_time":"2022-05-28T00:14:52.673979Z","start_time":"2022-05-28T00:14:50.313295Z"},"cell_style":"center","code_folding":[],"execution":{"iopub.status.busy":"2022-05-30T16:22:12.413622Z","iopub.execute_input":"2022-05-30T16:22:12.414027Z","iopub.status.idle":"2022-05-30T16:22:17.249991Z","shell.execute_reply.started":"2022-05-30T16:22:12.413993Z","shell.execute_reply":"2022-05-30T16:22:17.248998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- A boxplot is a graph that gives you a good indication of how the values in the data are spread out\n- boxplots may seem primitive in comparison to a histogram or density plot, they have the advantage of taking up less space, which is useful when comparing distributions between many groups or datasets.","metadata":{}},{"cell_type":"markdown","source":"## 6 FEATURE ENGINEERING\n<a id=\"section-six\"></a>","metadata":{}},{"cell_type":"markdown","source":"- in this sessions we created a hypothese mindmap to help us to create hipothesis and after that feature engineering","metadata":{}},{"cell_type":"code","source":"df2 = df1.copy()","metadata":{"ExecuteTime":{"end_time":"2022-05-30T09:01:51.297583Z","start_time":"2022-05-30T09:01:51.282226Z"},"execution":{"iopub.status.busy":"2022-05-30T16:22:17.25149Z","iopub.execute_input":"2022-05-30T16:22:17.25182Z","iopub.status.idle":"2022-05-30T16:22:17.276394Z","shell.execute_reply.started":"2022-05-30T16:22:17.25179Z","shell.execute_reply":"2022-05-30T16:22:17.275463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 6.1 Hypothesis Mindmap","metadata":{}},{"cell_type":"markdown","source":"Created based on what affects the business problem:\n\n1) Phenomeno: What phenomenon am I modeling?\n\n2) Agents: Who are the agents that act on the phenomenon of interest? (all entities that impact the phenomenon)\n\n3) Agent attributes: what is the description of the agents? (ex: client is age, salary, profession, etc.)\n\n4) List of Hypotheses: Hypotheses to validate with the data\n\n\n- insights are generated in two ways: surprise and belief contravention\n- Hypotheses are bets, they must be written as a statement in relation to the response variable.\n- It is not a cause and effect relationship, but a correlation","metadata":{"ExecuteTime":{"end_time":"2021-08-16T19:46:11.70147Z","start_time":"2021-08-16T19:46:11.694778Z"}}},{"cell_type":"code","source":"# Hypothesis Mindmap to help us to create business hypothesis\nImage(home_kaggle + 'images2/DAILY_STORE_SALES.png')","metadata":{"ExecuteTime":{"end_time":"2022-05-28T00:14:52.991806Z","start_time":"2022-05-28T00:14:52.985015Z"},"code_folding":[0],"execution":{"iopub.status.busy":"2022-05-30T16:22:54.631242Z","iopub.execute_input":"2022-05-30T16:22:54.631676Z","iopub.status.idle":"2022-05-30T16:22:54.652423Z","shell.execute_reply.started":"2022-05-30T16:22:54.631634Z","shell.execute_reply":"2022-05-30T16:22:54.651458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 6.2 Creating Hypothesis","metadata":{}},{"cell_type":"markdown","source":"#### **6.2.1 Store Hypothesis**","metadata":{}},{"cell_type":"markdown","source":"**1.** Stores with more employees should sell more.\n\n**2.** Stores with greater inventory capacity should sell more.\n\n**3.** Larger stores should sell more.\n\n**4.** Stores with larger assortments should sell more.\n\n**5.** Type A stores should sell more\n\n**6.** Stores with more departments should sell more","metadata":{}},{"cell_type":"markdown","source":"#### **6.2.2 Product Hypothesis**","metadata":{}},{"cell_type":"markdown","source":"**1.** Stores that invest more in Marketing should sell more.\n\n**2.** Stores with more product exposure should sell more.\n\n**3.** Stores with lower priced products should sell more.\n\n**5.** Stores with more aggressive promotions (bigger discounts) should sell more.\n\n**6.** Stores with longer active promotions should sell more.","metadata":{"ExecuteTime":{"end_time":"2021-03-01T00:59:13.716271Z","start_time":"2021-03-01T00:59:13.710287Z"}}},{"cell_type":"markdown","source":"#### **6.2.3 Time Hypothesis**","metadata":{}},{"cell_type":"markdown","source":"**1.** Stores during the Christmas holiday should sell more.\n\n**2.** Stores should sell more over the years.\n\n**3.** Stores should sell more in the second half of the year.\n\n**4.** Stores should sell more after the 2nd week each month.\n\n**5.** Stores should sell less on weekends.\n\n**6.** Stores should sell more during holidays.","metadata":{"ExecuteTime":{"end_time":"2021-03-01T01:06:06.66812Z","start_time":"2021-03-01T01:06:06.663158Z"}}},{"cell_type":"markdown","source":"#### **6.2.4 Macroeconomics**","metadata":{}},{"cell_type":"markdown","source":"**1.** Places with lower temperatures sell more\n\n**2.** Locations with lower gas prices sell more.\n\n**3.** Places with higher unemployment rate sell less\n\n**4.** Places with a high consumer confidence index sell more","metadata":{"ExecuteTime":{"end_time":"2021-03-01T01:06:06.66812Z","start_time":"2021-03-01T01:06:06.663158Z"}}},{"cell_type":"markdown","source":"### 6.3 Final Hypothesis List","metadata":{}},{"cell_type":"markdown","source":"- Prioritization based on available features in the dataset\n- The hypothesis list below will be validated during Exploratory Data Analysis","metadata":{"ExecuteTime":{"end_time":"2022-05-28T00:14:52.995402Z","start_time":"2022-05-28T00:14:52.993139Z"}}},{"cell_type":"markdown","source":"**1.** Larger stores should sell more.\n\n**2.** Type A stores should sell more.\n\n**3.** Stores with more departments should sell more.\n\n**4.** Stores with more aggressive promotions (bigger discounts) should sell more. (markdows)\n\n**5.** Stores during the Christmas holiday should sell more.\n\n**6.** Stores should sell more over the years.\n\n**7.** Stores should sell more in the second half of the year.\n\n**8.** Stores should sell more after the 2nd week each month.\n\n**9.** Places with lower temperatures sell more\n\n**10.** Locations with lower gas prices sell more.\n\n**11.** Places with higher unemployment rate sell less\n\n**12.** Places with a high consumer confidence index sell more","metadata":{"ExecuteTime":{"end_time":"2021-03-02T15:24:47.343762Z","start_time":"2021-03-02T15:24:47.296549Z"}}},{"cell_type":"markdown","source":"### 6.4 Feature Engineering","metadata":{}},{"cell_type":"code","source":"# year\ndf2['year'] = df2['date'].dt.year\n\n# month\ndf2['month'] = df2['date'].dt.month\n\n# day\ndf2['day'] = df2['date'].dt.day\n\n# week of year\ndf2['week_of_year'] = df2['date'].dt.isocalendar().week.astype('int64')\n\n# year week\ndf2['year_week'] = df2['date'].dt.strftime( '%Y-%W' )\n\n# year quarter\ndf2['quarter'] = df2['date'].dt.to_period('Q')","metadata":{"ExecuteTime":{"end_time":"2022-05-30T09:02:07.736208Z","start_time":"2022-05-30T09:02:05.824751Z"},"execution":{"iopub.status.busy":"2022-05-30T16:23:39.947589Z","iopub.execute_input":"2022-05-30T16:23:39.948508Z","iopub.status.idle":"2022-05-30T16:23:43.583325Z","shell.execute_reply.started":"2022-05-30T16:23:39.948452Z","shell.execute_reply":"2022-05-30T16:23:43.58235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 7 VARIABLE FILTERING \n<a id=\"section-seven\"></a>","metadata":{}},{"cell_type":"code","source":" df3 = df2.copy()","metadata":{"ExecuteTime":{"end_time":"2022-05-30T09:02:08.272039Z","start_time":"2022-05-30T09:02:08.209711Z"},"execution":{"iopub.status.busy":"2022-05-30T16:23:43.58486Z","iopub.execute_input":"2022-05-30T16:23:43.585194Z","iopub.status.idle":"2022-05-30T16:23:43.680666Z","shell.execute_reply.started":"2022-05-30T16:23:43.585156Z","shell.execute_reply":"2022-05-30T16:23:43.679605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"the motivation behind variable filtering is business constraints","metadata":{}},{"cell_type":"markdown","source":"### 7.1 Rows Filtering","metadata":{}},{"cell_type":"markdown","source":"- Firstly I do the rows filtering because it reduces the dataset volume, increasing the processing performance;\n- We are removing weekly sales with negative values since it will not increase the bias of the model that much;\n- During the next CRISP cicle we can work on negative sales and ask the business teams for some business constraints","metadata":{}},{"cell_type":"code","source":"# checking the impact of removing negative sales\n# percentage of register that we will deleted with this action\ndf3[df3['weekly_sales']< 0].shape[0]/df3.shape[0] * 100","metadata":{"ExecuteTime":{"end_time":"2022-05-29T14:06:05.636592Z","start_time":"2022-05-29T14:06:05.630296Z"},"execution":{"iopub.status.busy":"2022-05-30T16:23:45.053117Z","iopub.execute_input":"2022-05-30T16:23:45.053512Z","iopub.status.idle":"2022-05-30T16:23:45.063951Z","shell.execute_reply.started":"2022-05-30T16:23:45.053481Z","shell.execute_reply":"2022-05-30T16:23:45.063005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df3 = df3[df3['weekly_sales']>= 1]","metadata":{"ExecuteTime":{"end_time":"2022-05-30T09:02:11.305575Z","start_time":"2022-05-30T09:02:11.243125Z"},"execution":{"iopub.status.busy":"2022-05-30T16:23:46.497297Z","iopub.execute_input":"2022-05-30T16:23:46.497862Z","iopub.status.idle":"2022-05-30T16:23:46.559637Z","shell.execute_reply.started":"2022-05-30T16:23:46.497825Z","shell.execute_reply":"2022-05-30T16:23:46.558669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 8 EXPLORATORY DATA ANALYSIS (EDA)\n<a id=\"section-eight\"></a>","metadata":{}},{"cell_type":"markdown","source":"How do the variables impact the phenomenon, in this case weekly sales?\n\nHow strong is this impact?\n\nIt serves to measure the impact of features in relation to the response variable (target)\n\n3 goals:\n- gain business experience\n- validate business hypotheses (insights)\n- elect variables that are important to the model","metadata":{}},{"cell_type":"code","source":"df4 = df3.copy()","metadata":{"ExecuteTime":{"end_time":"2022-05-30T09:02:27.630898Z","start_time":"2022-05-30T09:02:27.603359Z"},"execution":{"iopub.status.busy":"2022-05-30T16:23:49.053878Z","iopub.execute_input":"2022-05-30T16:23:49.0544Z","iopub.status.idle":"2022-05-30T16:23:49.085041Z","shell.execute_reply.started":"2022-05-30T16:23:49.054369Z","shell.execute_reply":"2022-05-30T16:23:49.084328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 8.1 Univariate Analysis","metadata":{}},{"cell_type":"markdown","source":"#### 8.1.1 Response Variable (target)","metadata":{}},{"cell_type":"code","source":"# plot target variable distribution\nfig = plt.figure( figsize = (14, 6), constrained_layout=True)\nsns.distplot(df4['weekly_sales'], kde = False);\nplt.style.use('tableau-colorblind10');","metadata":{"ExecuteTime":{"end_time":"2022-05-29T14:06:12.800758Z","start_time":"2022-05-29T14:06:12.462341Z"},"execution":{"iopub.status.busy":"2022-05-30T16:23:51.068058Z","iopub.execute_input":"2022-05-30T16:23:51.068597Z","iopub.status.idle":"2022-05-30T16:23:51.521635Z","shell.execute_reply.started":"2022-05-30T16:23:51.068566Z","shell.execute_reply":"2022-05-30T16:23:51.520668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- It is not close to a normal distribution. \n- Skew far to the right.\n- The more normal the response variable, the better the algorithm will perform. We might have to use log transformation later on it","metadata":{}},{"cell_type":"markdown","source":"#### 8.1.2 Numerical Variable","metadata":{}},{"cell_type":"code","source":"num_attributes = df4.select_dtypes( include = 'number')","metadata":{"ExecuteTime":{"end_time":"2022-05-29T14:06:22.923578Z","start_time":"2022-05-29T14:06:22.921194Z"},"execution":{"iopub.status.busy":"2022-05-30T16:23:54.738431Z","iopub.execute_input":"2022-05-30T16:23:54.739099Z","iopub.status.idle":"2022-05-30T16:23:54.775206Z","shell.execute_reply.started":"2022-05-30T16:23:54.739057Z","shell.execute_reply":"2022-05-30T16:23:54.774356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# histogram for numerical features\nnum_attributes.hist(bins = 45);\nplt.style.use('tableau-colorblind10');","metadata":{"ExecuteTime":{"end_time":"2022-05-29T14:06:27.751069Z","start_time":"2022-05-29T14:06:25.162245Z"},"execution":{"iopub.status.busy":"2022-05-30T16:23:56.168995Z","iopub.execute_input":"2022-05-30T16:23:56.17004Z","iopub.status.idle":"2022-05-30T16:24:00.585626Z","shell.execute_reply.started":"2022-05-30T16:23:56.170001Z","shell.execute_reply":"2022-05-30T16:24:00.584592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Overall:** All variables don't follow a normal distribution\n\n - Store: There are specific stores that have less weekly sales points than others;\n - dept: There are specific departaments that have less weekly sales points than others;\n - temperature: THe closest to a normal distribuition, a little skewed to the left (-)\n - week_of_year: some especifics weeks have more sales data points\n - month: more sales data points in months 4 and 7\n - year: more sales data points in the year 2011","metadata":{}},{"cell_type":"markdown","source":"#### 8.1.3 Categorical Variable","metadata":{}},{"cell_type":"code","source":"cate_attributes = df4.select_dtypes( include = 'object')","metadata":{"ExecuteTime":{"end_time":"2022-05-29T14:06:51.489097Z","start_time":"2022-05-29T14:06:51.481438Z"},"execution":{"iopub.status.busy":"2022-05-30T16:24:06.065948Z","iopub.execute_input":"2022-05-30T16:24:06.066378Z","iopub.status.idle":"2022-05-30T16:24:06.082305Z","shell.execute_reply.started":"2022-05-30T16:24:06.066338Z","shell.execute_reply":"2022-05-30T16:24:06.081494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# type\nplt.subplot(2, 2, 1)\nsns.countplot(data = df4, x = df4['type'])\n\nplt.subplot(2, 2, 2)\nsns.kdeplot(data = df4, x = df4[df4['type'] == 'A']['weekly_sales'], shade = True)\nsns.kdeplot(data = df4, x = df4[df4['type'] == 'B']['weekly_sales'], shade = True)\nsns.kdeplot(data = df4, x = df4[df4['type'] == 'C']['weekly_sales'], shade = True);\n\nplt.subplot(2, 2, 3)\nsns.boxplot(x='type', y= 'weekly_sales', data=df4);\n\nplt.subplot(2, 2, 4)\nsns.boxplot(x='type', y= 'weekly_sales', data=df4, showfliers=False);\nplt.style.use('tableau-colorblind10');","metadata":{"ExecuteTime":{"end_time":"2022-05-28T00:15:08.933414Z","start_time":"2022-05-28T00:14:59.872446Z"},"code_folding":[0],"execution":{"iopub.status.busy":"2022-05-30T16:24:07.029599Z","iopub.execute_input":"2022-05-30T16:24:07.03003Z","iopub.status.idle":"2022-05-30T16:24:23.743142Z","shell.execute_reply.started":"2022-05-30T16:24:07.029997Z","shell.execute_reply":"2022-05-30T16:24:23.742136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- More sales datapoint in stores of type A\n- Little contribution of the types of stores in relation to the response variable, since the distributions are overlapped.\n- The median of A is the highest and C is the lowest","metadata":{}},{"cell_type":"markdown","source":"### 8.2 Bivariate Analysis","metadata":{}},{"cell_type":"markdown","source":"#### H1. Larger stores should sell more.\n\n**TRUE** Stores with more sizes have higher sales record","metadata":{}},{"cell_type":"code","source":"# compare the size and type of store with weekly sales\naux = df4[['weekly_sales', 'type', 'size']].groupby(['type', 'size']).mean().reset_index()\n\nsns.barplot(x = 'size', y = 'weekly_sales', data = aux, hue = 'type')\nplt.xticks(rotation = 75);\nplt.style.use('tableau-colorblind10');","metadata":{"ExecuteTime":{"end_time":"2022-05-28T00:15:09.980001Z","start_time":"2022-05-28T00:15:08.935371Z"},"code_folding":[],"execution":{"iopub.status.busy":"2022-05-30T16:24:23.744721Z","iopub.execute_input":"2022-05-30T16:24:23.74514Z","iopub.status.idle":"2022-05-30T16:24:25.228184Z","shell.execute_reply.started":"2022-05-30T16:24:23.745108Z","shell.execute_reply":"2022-05-30T16:24:25.227284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# scatterplot from df4\nsns.scatterplot(df4['size'], df4['weekly_sales']);","metadata":{"ExecuteTime":{"end_time":"2022-05-28T00:15:10.676818Z","start_time":"2022-05-28T00:15:09.981429Z"},"cell_style":"split","execution":{"iopub.status.busy":"2022-05-30T16:24:26.285181Z","iopub.execute_input":"2022-05-30T16:24:26.285792Z","iopub.status.idle":"2022-05-30T16:24:27.335707Z","shell.execute_reply.started":"2022-05-30T16:24:26.285749Z","shell.execute_reply":"2022-05-30T16:24:27.334647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# scatterplot aggregated sizes\nsns.scatterplot(aux['size'], aux['weekly_sales']);","metadata":{"ExecuteTime":{"end_time":"2022-05-28T00:15:10.848587Z","start_time":"2022-05-28T00:15:10.678217Z"},"cell_style":"split","execution":{"iopub.status.busy":"2022-05-30T16:24:27.33746Z","iopub.execute_input":"2022-05-30T16:24:27.337981Z","iopub.status.idle":"2022-05-30T16:24:27.642393Z","shell.execute_reply.started":"2022-05-30T16:24:27.337936Z","shell.execute_reply":"2022-05-30T16:24:27.641212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# type and size boxplot\nplt.subplot(1, 2, 1)\nsns.boxplot(x='type', y='size', data=df4)\nplt.style.use('tableau-colorblind10');\n\nplt.subplot(1, 2, 2)\nsns.boxplot(x='type', y= 'weekly_sales', data=df4, showfliers=False);\nplt.style.use('tableau-colorblind10');","metadata":{"ExecuteTime":{"end_time":"2022-05-28T00:15:15.612289Z","start_time":"2022-05-28T00:15:10.850221Z"},"code_folding":[0],"execution":{"iopub.status.busy":"2022-05-30T16:24:27.643972Z","iopub.execute_input":"2022-05-30T16:24:27.644838Z","iopub.status.idle":"2022-05-30T16:24:37.03856Z","shell.execute_reply.started":"2022-05-30T16:24:27.644802Z","shell.execute_reply":"2022-05-30T16:24:37.037518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#check how different type stores performed over years\naux2 = df4[['weekly_sales', 'type', 'year_week']].groupby(['type', 'year_week']).mean().reset_index()\naux2.pivot( index = 'year_week', columns = 'type', values = 'weekly_sales').plot();\n","metadata":{"ExecuteTime":{"end_time":"2022-05-28T00:15:15.93565Z","start_time":"2022-05-28T00:15:15.618949Z"},"code_folding":[0],"execution":{"iopub.status.busy":"2022-05-30T16:24:37.040315Z","iopub.execute_input":"2022-05-30T16:24:37.040792Z","iopub.status.idle":"2022-05-30T16:24:37.466498Z","shell.execute_reply.started":"2022-05-30T16:24:37.040747Z","shell.execute_reply":"2022-05-30T16:24:37.46528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# correlation between size and weekly sales\nsns.heatmap(aux.corr(method= 'pearson'), annot= True);","metadata":{"ExecuteTime":{"end_time":"2022-05-28T00:15:16.098271Z","start_time":"2022-05-28T00:15:15.937185Z"},"code_folding":[0],"execution":{"iopub.status.busy":"2022-05-30T16:24:37.467978Z","iopub.execute_input":"2022-05-30T16:24:37.468433Z","iopub.status.idle":"2022-05-30T16:24:37.752548Z","shell.execute_reply.started":"2022-05-30T16:24:37.46839Z","shell.execute_reply":"2022-05-30T16:24:37.751499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- By boxplot, we can infer that type A store is the largest store and C is the smallest\n- There is no overlapped area in size among A, B, and C. Type is the best predictor for Size\n- Stores with more sizes have higher sales record (The order of median of size and median of sales is the same)","metadata":{}},{"cell_type":"markdown","source":"#### H2. Type A stores should sell more.\n**TRUE** Type A Stores sell more over time, but because they are strongly correlated with size (hypothersis number 1)","metadata":{"ExecuteTime":{"end_time":"2021-03-03T21:50:48.312678Z","start_time":"2021-03-03T21:50:48.309686Z"},"heading_collapsed":true}},{"cell_type":"markdown","source":"#### H3. Stores with more departments should sell more.\n**TRUE** Stores with more department tend to sell more.","metadata":{}},{"cell_type":"code","source":"# compare the number of departments of each store with weekly sales\naux = df4[['store', 'dept', 'weekly_sales']].groupby(['store']).agg(n_store_dept = ('dept', 'nunique'),\n                                                                    weekly_sales = ('weekly_sales', 'mean')).reset_index()\n\n# compare the store number with weekly sales\naux2 = df4[['store', 'weekly_sales']].groupby(['store']).mean().reset_index()\n\nplt.subplot(2, 1, 1)\nsns.barplot(x= 'n_store_dept', y= 'weekly_sales', data= aux);\nplt.style.use('tableau-colorblind10');\n\nplt.subplot(2, 1, 2)\nsns.barplot(x= 'store', y= 'weekly_sales', data= aux2);\nplt.style.use('tableau-colorblind10');","metadata":{"ExecuteTime":{"end_time":"2022-05-28T00:15:17.110284Z","start_time":"2022-05-28T00:15:16.100601Z"},"code_folding":[0],"execution":{"iopub.status.busy":"2022-05-30T16:24:37.753821Z","iopub.execute_input":"2022-05-30T16:24:37.75417Z","iopub.status.idle":"2022-05-30T16:24:39.037505Z","shell.execute_reply.started":"2022-05-30T16:24:37.754138Z","shell.execute_reply":"2022-05-30T16:24:39.036499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checking each department individualy by type\nf, ax = plt.subplots(figsize=(10, 50))\nsns.boxplot(x='weekly_sales', y= 'dept', data=df4, showfliers=False, hue=\"type\",orient=\"h\");\nplt.style.use('tableau-colorblind10');","metadata":{"ExecuteTime":{"end_time":"2022-05-28T00:15:22.052992Z","start_time":"2022-05-28T00:15:17.111682Z"},"code_folding":[0],"execution":{"iopub.status.busy":"2022-05-30T16:24:39.039708Z","iopub.execute_input":"2022-05-30T16:24:39.040049Z","iopub.status.idle":"2022-05-30T16:24:48.39887Z","shell.execute_reply.started":"2022-05-30T16:24:39.04002Z","shell.execute_reply":"2022-05-30T16:24:48.397936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Each department shows the different level of sales\n- Department may be the powerful variable to predict sales\n- When department and type of store are considered together, generally department in A type shows the highest sales record\n- Type and department may have the interaction effect\n- There are missing some departaments (eg: 15, 73..)","metadata":{}},{"cell_type":"markdown","source":"#### H4. Stores with more aggressive promotions (bigger discounts) should sell more. (markdows)\n\n**FALSE** Not very clear the influence of markdows in the weekly sales. Next CRISP cycle we will do a more in-depth analysis\n","metadata":{}},{"cell_type":"code","source":"# compare weekly sales with all mark downs features\naux = df4[['weekly_sales', 'mark_down1', 'mark_down2', 'mark_down3', 'mark_down4', 'mark_down5']].groupby(df4['week_of_year']).mean()\n\nplt.figure(figsize=(20,8))\n\nsns.lineplot(aux.index, aux.weekly_sales.values)\nsns.lineplot(aux.index, aux.mark_down1.values)\nsns.lineplot(aux.index, aux.mark_down2.values)\nsns.lineplot(aux.index, aux.mark_down3.values)\nsns.lineplot(aux.index, aux.mark_down4.values)\nsns.lineplot(aux.index, aux.mark_down5.values)\n\nplt.grid()\n\nplt.xticks(np.arange(1, 53, step=1))\nplt.legend(['weekly_sales', 'mark_down1', 'mark_down2', 'mark_down3', 'mark_down4', 'mark_down5'], loc='best', fontsize=16)\nplt.title('Average Weekly Sales - Mark Downs', fontsize=18)\nplt.ylabel('Sales', fontsize=16)\nplt.xlabel('Week', fontsize=16)\nplt.show()","metadata":{"ExecuteTime":{"end_time":"2022-05-28T00:15:24.063154Z","start_time":"2022-05-28T00:15:22.054314Z"},"code_folding":[0],"execution":{"iopub.status.busy":"2022-05-30T16:24:48.40018Z","iopub.execute_input":"2022-05-30T16:24:48.400553Z","iopub.status.idle":"2022-05-30T16:24:51.836114Z","shell.execute_reply.started":"2022-05-30T16:24:48.40052Z","shell.execute_reply":"2022-05-30T16:24:51.835157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### H5. Stores during the Christmas holiday week should sell more.\n**TRUE** The highest point of weekly sales happen during the week 50 and 51","metadata":{}},{"cell_type":"code","source":"# plot sales week by week of the year for each year\nweekly_sales_2010 = df4[df4['year']==2010]['weekly_sales'].groupby(df4['week_of_year']).mean()\nweekly_sales_2011 = df4[df4['year']==2011]['weekly_sales'].groupby(df4['week_of_year']).mean()\nweekly_sales_2012 = df4[df4['year']==2012]['weekly_sales'].groupby(df4['week_of_year']).mean()\n\nplt.figure(figsize=(20,8))\n\nsns.lineplot(weekly_sales_2010.index, weekly_sales_2010.values)\nsns.lineplot(weekly_sales_2011.index, weekly_sales_2011.values)\nsns.lineplot(weekly_sales_2012.index, weekly_sales_2012.values)\n\nplt.grid()\n\nplt.xticks(np.arange(1, 53, step=1))\nplt.legend(['2010', '2011', '2012'], loc='best', fontsize=16)\nplt.title('Average Weekly Sales - Per Year', fontsize=18)\nplt.ylabel('Sales', fontsize=16)\nplt.xlabel('Week', fontsize=16)\nplt.show()","metadata":{"ExecuteTime":{"end_time":"2022-05-28T00:15:28.330049Z","start_time":"2022-05-28T00:15:24.06446Z"},"code_folding":[0],"execution":{"iopub.status.busy":"2022-05-30T16:24:51.837358Z","iopub.execute_input":"2022-05-30T16:24:51.837687Z","iopub.status.idle":"2022-05-30T16:24:59.673508Z","shell.execute_reply.started":"2022-05-30T16:24:51.837657Z","shell.execute_reply":"2022-05-30T16:24:59.672429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# compare weekly sales with holidays\naux = df4[['is_holiday', 'weekly_sales']].groupby('is_holiday').mean().reset_index();\n\nfig = plt.figure(figsize = (18,12))\nplt.subplot(211)\nsns.barplot(data = aux, x= 'is_holiday', y= 'weekly_sales');\n\naux2 = df4[['month','is_holiday', 'weekly_sales']].groupby(['month','is_holiday']).mean().reset_index();\n\nplt.subplot(212)\nsns.barplot(data = aux2, x= 'month', y= 'weekly_sales', hue= 'is_holiday');\nplt.xticks(rotation = 45);","metadata":{"ExecuteTime":{"end_time":"2022-05-28T00:15:28.697137Z","start_time":"2022-05-28T00:15:28.33141Z"},"code_folding":[0],"execution":{"iopub.status.busy":"2022-05-30T16:24:59.675027Z","iopub.execute_input":"2022-05-30T16:24:59.676067Z","iopub.status.idle":"2022-05-30T16:25:00.2855Z","shell.execute_reply.started":"2022-05-30T16:24:59.676023Z","shell.execute_reply":"2022-05-30T16:25:00.284481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- by thanksgiving and Christmas the sales rise up by a huge margin in all the years\n- sales increase with holidays\n- Sales are high in the weeks leading up to Christmas","metadata":{}},{"cell_type":"markdown","source":"#### H6. Stores should sell more over the years.\n**FALSE** the weekly sales average is maintained over the years. However, it tends to decay a little but because 2012 christimas is not computed yet.","metadata":{}},{"cell_type":"code","source":"# average weekly sales over the years\naux  = df4[['year', 'is_holiday','weekly_sales']].groupby(['year', 'is_holiday']).mean().reset_index()\naux2 = df4[['year','weekly_sales']].groupby(['year']).mean().reset_index()\n\nplt.subplot(3, 1, 1)\nsns.barplot(data = aux2, x = 'year', y= 'weekly_sales');\n\n\nplt.subplot(3, 1, 2)\nsns.barplot(data = aux, x = 'year', y= 'weekly_sales', hue= 'is_holiday');\n\nplt.subplot(3, 1, 3)\nsns.regplot(data = aux, x= 'year', y= 'weekly_sales');","metadata":{"ExecuteTime":{"end_time":"2022-05-28T00:15:29.225546Z","start_time":"2022-05-28T00:15:28.699739Z"},"code_folding":[0],"execution":{"iopub.status.busy":"2022-05-30T16:25:00.286622Z","iopub.execute_input":"2022-05-30T16:25:00.28697Z","iopub.status.idle":"2022-05-30T16:25:01.008935Z","shell.execute_reply.started":"2022-05-30T16:25:00.286938Z","shell.execute_reply":"2022-05-30T16:25:01.007901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# correlation between year and weekly sales\nsns.heatmap(aux.corr(method= 'pearson'), annot= True);","metadata":{"ExecuteTime":{"end_time":"2022-05-28T00:15:29.411238Z","start_time":"2022-05-28T00:15:29.227057Z"},"code_folding":[0],"execution":{"iopub.status.busy":"2022-05-30T16:25:01.010169Z","iopub.execute_input":"2022-05-30T16:25:01.010557Z","iopub.status.idle":"2022-05-30T16:25:01.325987Z","shell.execute_reply.started":"2022-05-30T16:25:01.010512Z","shell.execute_reply":"2022-05-30T16:25:01.324819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **H7.** Stores should sell more in the second half of the year.\n**TRUE** Stores sell more in the second half of the year, mainly because of the last quarter which include thanks giving and christimas sales","metadata":{"ExecuteTime":{"end_time":"2021-03-05T01:04:05.743322Z","start_time":"2021-03-05T01:04:05.727364Z"}}},{"cell_type":"code","source":"# month sales performance by year\naux = df4[['year', 'month','weekly_sales']].groupby(['year', 'month']).mean().reset_index()\nsns.barplot(data = aux, x = 'month', y= 'weekly_sales', hue= 'year');","metadata":{"ExecuteTime":{"end_time":"2022-05-28T00:15:29.932972Z","start_time":"2022-05-28T00:15:29.415777Z"},"code_folding":[0],"execution":{"iopub.status.busy":"2022-05-30T16:25:01.327742Z","iopub.execute_input":"2022-05-30T16:25:01.328123Z","iopub.status.idle":"2022-05-30T16:25:01.928005Z","shell.execute_reply.started":"2022-05-30T16:25:01.328089Z","shell.execute_reply":"2022-05-30T16:25:01.92691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# aggregate month related to sales\nplt.subplot(131)\naux = df4[['month','weekly_sales']].groupby(['month']).mean().reset_index()\nsns.barplot(data = aux, x = 'month', y= 'weekly_sales');\n\nplt.subplot(132)\nsns.regplot(data = aux, x= 'month', y= 'weekly_sales');\n\nplt.subplot(133)\nsns.heatmap(aux.corr(method= 'pearson'), annot = True);","metadata":{"ExecuteTime":{"end_time":"2022-05-28T00:15:30.624425Z","start_time":"2022-05-28T00:15:29.934429Z"},"code_folding":[0],"execution":{"iopub.status.busy":"2022-05-30T16:25:01.931697Z","iopub.execute_input":"2022-05-30T16:25:01.932298Z","iopub.status.idle":"2022-05-30T16:25:02.827439Z","shell.execute_reply.started":"2022-05-30T16:25:01.932248Z","shell.execute_reply":"2022-05-30T16:25:02.826333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sales performance by quarter\naux2010 = df4[df4['year']==2010][['weekly_sales', 'quarter']].groupby('quarter').mean().reset_index()\naux2011 = df4[df4['year']==2011][['weekly_sales', 'quarter']].groupby('quarter').mean().reset_index()\naux2012 = df4[df4['year']==2012][['weekly_sales', 'quarter']].groupby('quarter').mean().reset_index()\n\nplt.subplot(131)\nsns.barplot(data = aux2010, x = 'quarter', y = 'weekly_sales')\nplt.subplot(132)\nsns.barplot(data = aux2011, x = 'quarter', y = 'weekly_sales')\nplt.subplot(133)\nsns.barplot(data = aux2012, x = 'quarter', y = 'weekly_sales');","metadata":{"ExecuteTime":{"end_time":"2022-05-28T00:15:31.09646Z","start_time":"2022-05-28T00:15:30.62757Z"},"code_folding":[0],"execution":{"iopub.status.busy":"2022-05-30T16:25:02.82868Z","iopub.execute_input":"2022-05-30T16:25:02.828986Z","iopub.status.idle":"2022-05-30T16:25:03.408266Z","shell.execute_reply.started":"2022-05-30T16:25:02.828958Z","shell.execute_reply":"2022-05-30T16:25:03.407133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **H8.** Stores should sell more after the 2nd week each month.\n**FALSE** Sales are almost the same.","metadata":{}},{"cell_type":"code","source":"# plot to check sales after and before 2 weeks of each month\naux = df4[['day', 'weekly_sales']].groupby('day').mean().reset_index()\naux['before_after'] = aux['day'].apply(lambda x: 'before_15_days' if x <= 15 else\n                                                  'after_15_days')\n\nsns.barplot(data = aux, x= 'before_after', y= 'weekly_sales');","metadata":{"ExecuteTime":{"end_time":"2022-05-28T00:15:31.296565Z","start_time":"2022-05-28T00:15:31.098822Z"},"code_folding":[0],"execution":{"iopub.status.busy":"2022-05-30T16:25:03.409763Z","iopub.execute_input":"2022-05-30T16:25:03.41022Z","iopub.status.idle":"2022-05-30T16:25:03.708918Z","shell.execute_reply.started":"2022-05-30T16:25:03.410168Z","shell.execute_reply":"2022-05-30T16:25:03.707717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **H9.** Places with lower temperatures sell more\n**FALSE** Temperature seems to have no relationship with weekly sales","metadata":{}},{"cell_type":"code","source":"# relationship between temperature and sales over the years\naux = df4[['temperature', 'weekly_sales', 'year_week']].groupby('year_week').mean().reset_index()\n\nsns.lineplot(data = aux, x = 'year_week', y = 'weekly_sales')\nplt.xticks(rotation = 90);\nax2 = plt.twinx()\nsns.lineplot(data=aux,x = 'year_week', y = 'temperature', color=\"r\", ax=ax2);","metadata":{"ExecuteTime":{"end_time":"2022-05-28T00:15:35.048624Z","start_time":"2022-05-28T00:15:31.298324Z"},"code_folding":[0],"execution":{"iopub.status.busy":"2022-05-30T16:25:03.71033Z","iopub.execute_input":"2022-05-30T16:25:03.710815Z","iopub.status.idle":"2022-05-30T16:25:09.266285Z","shell.execute_reply.started":"2022-05-30T16:25:03.710768Z","shell.execute_reply":"2022-05-30T16:25:09.263222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# temperature x weekly sales\nsns.scatterplot(x = df4.temperature, y = df4.weekly_sales);","metadata":{"ExecuteTime":{"end_time":"2022-05-28T00:15:35.791192Z","start_time":"2022-05-28T00:15:35.050573Z"},"code_folding":[0],"execution":{"iopub.status.busy":"2022-05-30T16:25:09.267552Z","iopub.execute_input":"2022-05-30T16:25:09.267889Z","iopub.status.idle":"2022-05-30T16:25:10.587676Z","shell.execute_reply.started":"2022-05-30T16:25:09.267859Z","shell.execute_reply":"2022-05-30T16:25:10.586561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- There seems to be no relatiobship between the temperature in the region and weekly sales of the stores. \n- At low and very high temperatures the sales seems to dip a bit but in general there doesn't exist a clear relationship","metadata":{}},{"cell_type":"markdown","source":"#### **H10.** Locations with lower gas prices sell more.\n**FALSE** There are no clear reletionship!","metadata":{}},{"cell_type":"code","source":"# relationship between temperature and sales over the years\naux = df4[['fuel_price', 'weekly_sales', 'year_week']].groupby('year_week').mean().reset_index()\n\nsns.lineplot(data = aux, x = 'year_week', y = 'weekly_sales')\nplt.xticks(rotation = 90);\nax2 = plt.twinx()\nsns.lineplot(data=aux,x = 'year_week', y = 'fuel_price', color=\"r\", ax=ax2);","metadata":{"ExecuteTime":{"end_time":"2022-05-28T00:15:39.316614Z","start_time":"2022-05-28T00:15:35.79301Z"},"code_folding":[0],"execution":{"iopub.status.busy":"2022-05-30T16:25:10.589156Z","iopub.execute_input":"2022-05-30T16:25:10.589542Z","iopub.status.idle":"2022-05-30T16:25:16.152268Z","shell.execute_reply.started":"2022-05-30T16:25:10.589496Z","shell.execute_reply":"2022-05-30T16:25:16.151224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# temperature x weekly sales\nsns.scatterplot(x = df4.fuel_price, y = df4.weekly_sales);","metadata":{"ExecuteTime":{"end_time":"2022-05-28T00:15:40.027198Z","start_time":"2022-05-28T00:15:39.31809Z"},"code_folding":[0],"execution":{"iopub.status.busy":"2022-05-30T16:25:16.153867Z","iopub.execute_input":"2022-05-30T16:25:16.154402Z","iopub.status.idle":"2022-05-30T16:25:17.27128Z","shell.execute_reply.started":"2022-05-30T16:25:16.154359Z","shell.execute_reply":"2022-05-30T16:25:17.270533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Between fuel price and the sales there doesn't seem to exist any clear relationship","metadata":{}},{"cell_type":"markdown","source":"#### **H11.** Places with higher unemployment rate sell less\n**FALSE** There are no clear reletionship!","metadata":{}},{"cell_type":"code","source":"# relationship between Unemployment rate and sales over the years\naux = df4[['unemployment', 'weekly_sales', 'year_week']].groupby('year_week').mean().reset_index()\n\nsns.lineplot(data = aux, x = 'year_week', y = 'weekly_sales')\nplt.xticks(rotation = 90);\nax2 = plt.twinx()\nsns.lineplot(data=aux,x = 'year_week', y = 'unemployment', color=\"r\", ax=ax2);","metadata":{"ExecuteTime":{"end_time":"2022-05-28T00:15:43.227217Z","start_time":"2022-05-28T00:15:40.028469Z"},"code_folding":[0],"execution":{"iopub.status.busy":"2022-05-30T16:25:29.314463Z","iopub.execute_input":"2022-05-30T16:25:29.315096Z","iopub.status.idle":"2022-05-30T16:25:34.859677Z","shell.execute_reply.started":"2022-05-30T16:25:29.315052Z","shell.execute_reply":"2022-05-30T16:25:34.858674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Unemployment rate x weekly sales\nsns.scatterplot(x = df4.unemployment, y = df4.weekly_sales);","metadata":{"ExecuteTime":{"end_time":"2022-05-28T00:15:43.912275Z","start_time":"2022-05-28T00:15:43.229103Z"},"code_folding":[0],"execution":{"iopub.status.busy":"2022-05-30T16:25:34.86162Z","iopub.execute_input":"2022-05-30T16:25:34.862252Z","iopub.status.idle":"2022-05-30T16:25:35.918196Z","shell.execute_reply.started":"2022-05-30T16:25:34.862205Z","shell.execute_reply":"2022-05-30T16:25:35.917235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **H12.** Places with a high consumer confidence index sell more\n**FALSE** There are no clear relationship","metadata":{}},{"cell_type":"code","source":"# relationship between CPI and sales over the years\naux = df4[['cpi', 'weekly_sales', 'year_week']].groupby('year_week').mean().reset_index()\n\nsns.lineplot(data = aux, x = 'year_week', y = 'weekly_sales')\nplt.xticks(rotation = 90);\nax2 = plt.twinx()\nsns.lineplot(data=aux,x = 'year_week', y = 'cpi', color=\"r\", ax=ax2);","metadata":{"ExecuteTime":{"end_time":"2022-05-28T00:15:47.19755Z","start_time":"2022-05-28T00:15:43.914463Z"},"code_folding":[0],"execution":{"iopub.status.busy":"2022-05-30T16:25:35.919799Z","iopub.execute_input":"2022-05-30T16:25:35.920363Z","iopub.status.idle":"2022-05-30T16:25:41.432776Z","shell.execute_reply.started":"2022-05-30T16:25:35.920323Z","shell.execute_reply":"2022-05-30T16:25:41.431816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# CPI x weekly sales\nsns.scatterplot(x = df4.cpi, y = df4.weekly_sales);","metadata":{"ExecuteTime":{"end_time":"2022-05-28T00:15:47.859934Z","start_time":"2022-05-28T00:15:47.199109Z"},"code_folding":[0],"execution":{"iopub.status.busy":"2022-05-30T16:25:41.434658Z","iopub.execute_input":"2022-05-30T16:25:41.435089Z","iopub.status.idle":"2022-05-30T16:25:42.47982Z","shell.execute_reply.started":"2022-05-30T16:25:41.435055Z","shell.execute_reply":"2022-05-30T16:25:42.47895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **8.2.13 Hypothesis summarize**","metadata":{}},{"cell_type":"code","source":"# Hypothesis Summary to select feature relevance to the model\nsummary = pd.DataFrame({'Hypothesis':['Larger stores should sell more.',\n                                      'Type A stores should sell more.',\n                                      'Stores with more departments should sell more.',\n                                      'Stores with more aggressive promotions (bigger discounts) should sell more. (markdows)',\n                                      'Stores during the Christmas holiday should sell more.',\n                                      'Stores should sell more over the years.',\n                                      'Stores should sell more in the second half of the year.',\n                                      'Stores should sell more after the 2nd week each month.',\n                                      'Places with lower temperatures sell more',\n                                      'Locations with lower gas prices sell more.',\n                                      'Places with higher unemployment rate sell less,',\n                                      'Places with a high consumer confidence index sell more.',\n                                     ],\n                        'True / False':['True', 'True', 'True', 'False', 'True', 'False', 'True', 'False', 'False',\n                                        'False','False', 'False'],\n                        'Relevance':['High', 'Medium', 'Low', 'Medium', 'Low', 'Low', 'High', 'Low', 'Low', \n                                     'Low', 'Low', 'Low']}, \n                        index=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12])\nsummary","metadata":{"ExecuteTime":{"end_time":"2022-05-28T00:15:47.871055Z","start_time":"2022-05-28T00:15:47.861361Z"},"code_folding":[0],"execution":{"iopub.status.busy":"2022-05-30T16:25:42.481168Z","iopub.execute_input":"2022-05-30T16:25:42.481529Z","iopub.status.idle":"2022-05-30T16:25:42.497503Z","shell.execute_reply.started":"2022-05-30T16:25:42.481496Z","shell.execute_reply":"2022-05-30T16:25:42.496806Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 8.3 Multivariate Analysis","metadata":{}},{"cell_type":"markdown","source":"- Checking the relationship between the different columns numerically to see how they correlate with the weekly sales in order to confirm the inferences we have evaluated in the above EDA.","metadata":{}},{"cell_type":"code","source":"# correlation among all variables\ncorrelation = (num_attributes.corr( method = 'pearson' ))\nsns.heatmap( correlation, annot = True );","metadata":{"ExecuteTime":{"end_time":"2022-05-28T00:15:49.184258Z","start_time":"2022-05-28T00:15:47.87291Z"},"code_folding":[0],"execution":{"iopub.status.busy":"2022-05-30T16:26:23.171848Z","iopub.execute_input":"2022-05-30T16:26:23.173169Z","iopub.status.idle":"2022-05-30T16:26:25.420614Z","shell.execute_reply.started":"2022-05-30T16:26:23.173075Z","shell.execute_reply":"2022-05-30T16:26:25.419867Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_corr(col):\n    a = correlation[col].sort_values(ascending=False).to_frame()\n    a.columns = ['']\n    a.drop(col, axis=0, inplace=True)\n    plot = sns.heatmap( a, annot=True, cmap='Blues').set_title(col);\n    \n    return plot\n\nplot_corr('weekly_sales');","metadata":{"ExecuteTime":{"end_time":"2022-05-28T00:15:49.401033Z","start_time":"2022-05-28T00:15:49.185879Z"},"code_folding":[0],"execution":{"iopub.status.busy":"2022-05-30T16:26:36.590487Z","iopub.execute_input":"2022-05-30T16:26:36.591354Z","iopub.status.idle":"2022-05-30T16:26:36.962952Z","shell.execute_reply.started":"2022-05-30T16:26:36.591307Z","shell.execute_reply":"2022-05-30T16:26:36.961827Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 9 DATA PREPARATION\n<a id=\"section-nine\"></a>","metadata":{}},{"cell_type":"code","source":"# copy of dataset\ndf5 = df4.copy()","metadata":{"ExecuteTime":{"end_time":"2022-05-30T09:02:54.009905Z","start_time":"2022-05-30T09:02:53.972659Z"},"execution":{"iopub.status.busy":"2022-05-30T16:26:40.978641Z","iopub.execute_input":"2022-05-30T16:26:40.979053Z","iopub.status.idle":"2022-05-30T16:26:41.00855Z","shell.execute_reply.started":"2022-05-30T16:26:40.97902Z","shell.execute_reply":"2022-05-30T16:26:41.0073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The rescaling methods applied below are based on the features distribution shape and boxplot outlier analysis.\n\n- Standard Scaler: applied on variables with a distribution shape similar to a normal distribution;\n- Min-Max Scaler: applied on variables with low outliers influence;\n- Robust Scaler: applied on variables with high outliers influence.","metadata":{}},{"cell_type":"markdown","source":"### 9.1 Split dataframe into training and validation","metadata":{}},{"cell_type":"markdown","source":"- Here we are going to split the dataframe into train and validation (Proportion 85/15)\n- We are doing that before data preparation to avoid data leakeage.\n- Data preparation will be applied into train dataset (fit_transform), and after that it will be applied in the validation dataset (only transform).\n- The same will be done in the test data at the end of modelling.\n- Using temporal variable is a more reliable way of splitting datasets whenever the dataset includes the date variable, and we want to predict something in the future that depends on date","metadata":{}},{"cell_type":"code","source":"# checking the max date in the dataframe\ndf5[['store', 'date']].groupby('store').max().reset_index()['date'][0] ","metadata":{"ExecuteTime":{"end_time":"2022-05-29T14:09:45.92322Z","start_time":"2022-05-29T14:09:45.890578Z"},"execution":{"iopub.status.busy":"2022-05-30T16:26:42.587589Z","iopub.execute_input":"2022-05-30T16:26:42.58801Z","iopub.status.idle":"2022-05-30T16:26:42.612444Z","shell.execute_reply.started":"2022-05-30T16:26:42.587978Z","shell.execute_reply":"2022-05-30T16:26:42.611331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checking 22 weeks before\ndf5[['store', 'date']].groupby('store').max().reset_index()['date'][0] - datetime.timedelta( weeks = 22 )","metadata":{"ExecuteTime":{"end_time":"2022-05-29T14:09:47.321124Z","start_time":"2022-05-29T14:09:47.29861Z"},"execution":{"iopub.status.busy":"2022-05-30T16:26:44.223634Z","iopub.execute_input":"2022-05-30T16:26:44.224052Z","iopub.status.idle":"2022-05-30T16:26:44.245111Z","shell.execute_reply.started":"2022-05-30T16:26:44.224019Z","shell.execute_reply":"2022-05-30T16:26:44.244427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Spliting dataframe into train and validation. \n# Validation will have the last 22 weeks of sales which represents 16% of the data\n# starting at 12-05-25 until the last day of sales\n\ndf5[['store', 'date']].groupby('store').max().reset_index()['date'][0] - datetime.timedelta( weeks = 22 )\n\n# Train dataset\nX_train = df5[df5['date'] < '2012-05-25']\ny_train = X_train['weekly_sales']\n\n# Validation dataset\nX_validation = df5[df5['date'] >= '2012-05-25']\ny_validation = X_validation['weekly_sales']\n\nprint( 'Training Min Date: {}'.format( X_train['date'].min() ) )\nprint( 'Training Max Date: {}'.format( X_train['date'].max() ) )\n\nprint( '\\nValidation Min Date: {}'.format( X_validation['date'].min() ) )\nprint( 'Validation Max Date: {}'.format( X_validation['date'].max() ) )","metadata":{"ExecuteTime":{"end_time":"2022-05-30T09:02:55.404055Z","start_time":"2022-05-30T09:02:55.321484Z"},"execution":{"iopub.status.busy":"2022-05-30T16:26:45.493255Z","iopub.execute_input":"2022-05-30T16:26:45.49388Z","iopub.status.idle":"2022-05-30T16:26:45.597459Z","shell.execute_reply.started":"2022-05-30T16:26:45.49384Z","shell.execute_reply":"2022-05-30T16:26:45.596526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check the proportion of validation data datapoints\nX_validation.shape[0]/df5.shape[0] * 100","metadata":{"ExecuteTime":{"end_time":"2022-05-29T18:14:48.246025Z","start_time":"2022-05-29T18:14:48.236542Z"},"execution":{"iopub.status.busy":"2022-05-30T16:26:49.77785Z","iopub.execute_input":"2022-05-30T16:26:49.778246Z","iopub.status.idle":"2022-05-30T16:26:49.785262Z","shell.execute_reply.started":"2022-05-30T16:26:49.778215Z","shell.execute_reply":"2022-05-30T16:26:49.78426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 9.2 Checking features outliers presence","metadata":{}},{"cell_type":"markdown","source":"In the plot below:\n- plot boxplots features in order to check outliers presence\n\n- 'temperature', 'fuel_price', 'mark_down1', 'mark_down2', 'mark_down3', 'mark_down4', 'mark_down5', 'cpi', 'unemployment', 'size', 'year', 'month', 'day', 'week_of_year'","metadata":{}},{"cell_type":"code","source":"# plot boxplots features in order to check outliers presence\nplt.subplot(6, 4, 9)\nsns.boxplot(df5['temperature'])\n\nplt.subplot(6, 4, 10)\nsns.boxplot(df5['fuel_price'])\n\nplt.subplot(6, 4, 11)\nsns.boxplot(df5['mark_down1'])\n\nplt.subplot(6, 4, 12)\nsns.boxplot(df5['mark_down2'])\n\nplt.subplot(6, 4, 13)\nsns.boxplot(df5['mark_down3'])\n\nplt.subplot(6, 4, 14)\nsns.boxplot(df5['mark_down4'])\n\nplt.subplot(6, 4, 15)\nsns.boxplot(df5['mark_down5']);\n\nplt.subplot(6, 4, 16)\nsns.boxplot(df5['cpi']);\n\nplt.subplot(6, 4, 17)\nsns.boxplot(df5['unemployment']);\n\nplt.subplot(6, 4, 18)\nsns.boxplot(df5['size']);\n\nplt.subplot(6, 4, 19)\nsns.boxplot(df5['year']);\n\nplt.subplot(6, 4, 20)\nsns.boxplot(df5['month']);\n\nplt.subplot(6, 4, 21)\nsns.boxplot(df5['day']);\n\nplt.subplot(6, 4, 22)\nsns.boxplot(df5['week_of_year']);\n\nfig.tight_layout()","metadata":{"ExecuteTime":{"end_time":"2022-05-29T14:14:05.630868Z","start_time":"2022-05-29T14:14:03.531098Z"},"code_folding":[0],"execution":{"iopub.status.busy":"2022-05-30T16:27:00.088397Z","iopub.execute_input":"2022-05-30T16:27:00.089442Z","iopub.status.idle":"2022-05-30T16:27:03.13166Z","shell.execute_reply.started":"2022-05-30T16:27:00.089398Z","shell.execute_reply":"2022-05-30T16:27:03.130621Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 9.3 Feature Normalization","metadata":{"ExecuteTime":{"end_time":"2021-03-06T22:29:36.132679Z","start_time":"2021-03-06T22:29:36.129657Z"}}},{"cell_type":"markdown","source":"**None of the attributes follow a normal distribution**","metadata":{}},{"cell_type":"markdown","source":"### 9.4 Feature Rescaling","metadata":{}},{"cell_type":"code","source":"rs = RobustScaler()  #selection of the rescaling method is due to outliers\n                     #same formula as minmaxScaler, but uses interquatis itervals as range, being robust to outliers\n\nmms = MinMaxScaler() #super sensitive to outliers\n\n#features for Robust Scalers\n#temperature\nX_train['temperature'] = rs.fit_transform( X_train[['temperature']].values ) \n# pickle.dump(rs, open(home_path+'/parameters/temperature_scaler.pkl', 'wb'))\n\n# mark_down1\nX_train['mark_down1'] = rs.fit_transform( X_train[['mark_down1']].values ) \n# pickle.dump(rs, open(home_path+'/parameters/mark_down1_scaler.pkl', 'wb'))\n\n# mark_down2\nX_train['mark_down2'] = rs.fit_transform( X_train[['mark_down2']].values ) \n# pickle.dump(rs, open(home_path+'/parameters/mark_down2_scaler.pkl', 'wb'))\n\n# mark_down3\nX_train['mark_down3'] = rs.fit_transform( X_train[['mark_down3']].values ) \n# pickle.dump(rs, open(home_path+'/parameters/mark_down3_scaler.pkl', 'wb'))\n\n# mark_down4\nX_train['mark_down4'] = rs.fit_transform( X_train[['mark_down4']].values ) \n# pickle.dump(rs, open(home_path+'/parameters/mark_down4_scaler.pkl', 'wb'))\n\n# mark_down5\nX_train['mark_down5'] = rs.fit_transform( X_train[['mark_down5']].values ) \n# pickle.dump(rs, open(home_path+'/parameters/mark_down5_scaler.pkl', 'wb'))\n\n# unemployment\nX_train['unemployment'] = rs.fit_transform( X_train[['unemployment']].values ) \n# pickle.dump(rs, open(home_path+'/parameters/unemployment_scaler.pkl', 'wb'))\n\n#Features for MinMaxScaler\n# fuel_price\nX_train['fuel_price'] = mms.fit_transform( X_train[['fuel_price']].values )\n# pickle.dump(mms, open(home_path + '/parameters/fuel_price_scaler.pkl', 'wb'))\n\n# cpi\nX_train['cpi'] = mms.fit_transform( X_train[['cpi']].values )\n# pickle.dump(mms, open(home_path + '/parameters/cpi_scaler.pkl', 'wb'))\n\n# size\nX_train['size'] = mms.fit_transform( X_train[['size']].values )\n# pickle.dump(mms, open(home_path + '/parameters/size_scaler.pkl', 'wb'))\n\n# year\nX_train['year'] = mms.fit_transform( X_train[['year']].values )\n# pickle.dump(mms, open(home_path + '/parameters/year_scaler.pkl', 'wb'))","metadata":{"ExecuteTime":{"end_time":"2022-05-30T09:02:59.00411Z","start_time":"2022-05-30T09:02:58.593907Z"},"execution":{"iopub.status.busy":"2022-05-30T16:27:13.609892Z","iopub.execute_input":"2022-05-30T16:27:13.6103Z","iopub.status.idle":"2022-05-30T16:27:13.8154Z","shell.execute_reply.started":"2022-05-30T16:27:13.610267Z","shell.execute_reply":"2022-05-30T16:27:13.814542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 9.5 Feature Transformation","metadata":{}},{"cell_type":"markdown","source":"#### Encoding","metadata":{"ExecuteTime":{"end_time":"2021-03-06T23:23:33.191471Z","start_time":"2021-03-06T23:23:33.188481Z"}}},{"cell_type":"code","source":"# is_holiday\nX_train['is_holiday'] = X_train['is_holiday'].apply(lambda x: 1 if x == True else 0)\n\n# type - Label Encoder\nle = LabelEncoder()\nX_train['type'] = le.fit_transform( X_train['type'] )\n# pickle.dump(le, open(home_path + '/parameters/type_scaler.pkl', 'wb'))","metadata":{"ExecuteTime":{"end_time":"2022-05-30T09:03:11.394019Z","start_time":"2022-05-30T09:03:11.244615Z"},"scrolled":true,"execution":{"iopub.status.busy":"2022-05-30T16:27:18.007191Z","iopub.execute_input":"2022-05-30T16:27:18.008069Z","iopub.status.idle":"2022-05-30T16:27:18.327395Z","shell.execute_reply.started":"2022-05-30T16:27:18.008009Z","shell.execute_reply":"2022-05-30T16:27:18.326645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Response (Target) Variable Transformation","metadata":{}},{"cell_type":"code","source":"X_train['weekly_sales'] = np.log1p( X_train['weekly_sales'] )\nsns.distplot(X_train['weekly_sales']);","metadata":{"ExecuteTime":{"end_time":"2022-05-30T09:03:14.074611Z","start_time":"2022-05-30T09:03:12.872659Z"},"execution":{"iopub.status.busy":"2022-05-30T16:27:19.272515Z","iopub.execute_input":"2022-05-30T16:27:19.273064Z","iopub.status.idle":"2022-05-30T16:27:21.088617Z","shell.execute_reply.started":"2022-05-30T16:27:19.273018Z","shell.execute_reply":"2022-05-30T16:27:21.087555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- The log transformation is, arguably, the most popular among the different types of transformations used to transform skewed data to approximately conform to normality","metadata":{}},{"cell_type":"markdown","source":"#### Nature Transformation","metadata":{}},{"cell_type":"code","source":"# month\nX_train['month_sin'] = X_train['month'].apply( lambda x: np.sin( x * ( 2. * np.pi/12 ) ) )\nX_train['month_cos'] = X_train['month'].apply( lambda x: np.cos( x * ( 2. * np.pi/12 ) ) )\n\n# day \nX_train['day_sin'] = X_train['day'].apply( lambda x: np.sin( x * ( 2. * np.pi/30 ) ) )\nX_train['day_cos'] = X_train['day'].apply( lambda x: np.cos( x * ( 2. * np.pi/30 ) ) )\n\n# week_of_year\nX_train['week_of_year_sin'] = X_train['week_of_year'].apply( lambda x: np.sin( x * ( 2. * np.pi/52 ) ) )\nX_train['week_of_year_cos'] = X_train['week_of_year'].apply( lambda x: np.cos( x * ( 2. * np.pi/52 ) ) )","metadata":{"ExecuteTime":{"end_time":"2022-05-30T09:03:18.612641Z","start_time":"2022-05-30T09:03:16.681856Z"},"execution":{"iopub.status.busy":"2022-05-30T16:27:21.090584Z","iopub.execute_input":"2022-05-30T16:27:21.091197Z","iopub.status.idle":"2022-05-30T16:27:25.544611Z","shell.execute_reply.started":"2022-05-30T16:27:21.09115Z","shell.execute_reply":"2022-05-30T16:27:25.543548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- for features that have cyclic behavior: it repeats over time","metadata":{}},{"cell_type":"code","source":"#checking dataframe after scaling to see if everything went through\nX_train.head()","metadata":{"ExecuteTime":{"end_time":"2022-05-29T14:18:35.483669Z","start_time":"2022-05-29T14:18:35.464281Z"},"execution":{"iopub.status.busy":"2022-05-30T16:27:25.546292Z","iopub.execute_input":"2022-05-30T16:27:25.546689Z","iopub.status.idle":"2022-05-30T16:27:25.578919Z","shell.execute_reply.started":"2022-05-30T16:27:25.546655Z","shell.execute_reply":"2022-05-30T16:27:25.577738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 9.6 Apply Transformations in the Validation dataset","metadata":{}},{"cell_type":"code","source":"# loading scalers\n\n#Robust Scaler \ntemperature_scaler  = pickle.load(open(home_kaggle + 'parameters/temperature_scaler.pkl', 'rb'))\nmark_down1_scaler   = pickle.load(open(home_kaggle + 'parameters/mark_down1_scaler.pkl', 'rb'))\nmark_down2_scaler   = pickle.load(open(home_kaggle + 'parameters/mark_down2_scaler.pkl', 'rb'))\nmark_down3_scaler   = pickle.load(open(home_kaggle + 'parameters/mark_down3_scaler.pkl', 'rb'))\nmark_down4_scaler   = pickle.load(open(home_kaggle + 'parameters/mark_down4_scaler.pkl', 'rb'))\nmark_down5_scaler   = pickle.load(open(home_kaggle + 'parameters/mark_down5_scaler.pkl', 'rb'))\nunemployment_scaler = pickle.load(open(home_kaggle + 'parameters/unemployment_scaler.pkl', 'rb'))\n\n#MinMax Scaler \nfuel_price_scaler   = pickle.load(open(home_kaggle + 'parameters/fuel_price_scaler.pkl', 'rb'))\ncpi_scaler          = pickle.load(open(home_kaggle + 'parameters/cpi_scaler.pkl', 'rb'))\nsize_scaler         = pickle.load(open(home_kaggle + 'parameters/size_scaler.pkl', 'rb'))\nyear_scaler         = pickle.load(open(home_kaggle + 'parameters/year_scaler.pkl', 'rb'))\n\n#Label enconder\ntype_scaler         = pickle.load(open(home_kaggle + 'parameters/type_scaler.pkl', 'rb'))","metadata":{"ExecuteTime":{"end_time":"2022-05-30T09:03:23.175823Z","start_time":"2022-05-30T09:03:23.164115Z"},"execution":{"iopub.status.busy":"2022-05-30T16:27:26.641057Z","iopub.execute_input":"2022-05-30T16:27:26.642161Z","iopub.status.idle":"2022-05-30T16:27:26.687451Z","shell.execute_reply.started":"2022-05-30T16:27:26.642118Z","shell.execute_reply":"2022-05-30T16:27:26.686589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Applying all transformations on validation dataset\n#Validation dataset features transform - Robust Scaler\nX_validation['temperature'] = temperature_scaler.transform( X_validation[['temperature']].values ) \nX_validation['mark_down1']  = mark_down1_scaler.transform( X_validation[['mark_down1']].values ) \nX_validation['mark_down2']  = mark_down2_scaler.transform( X_validation[['mark_down2']].values ) \nX_validation['mark_down3']  = mark_down3_scaler.transform( X_validation[['mark_down3']].values ) \nX_validation['mark_down4']  = mark_down4_scaler.transform( X_validation[['mark_down4']].values ) \nX_validation['mark_down5']  = mark_down5_scaler.transform( X_validation[['mark_down5']].values ) \nX_validation['unemployment']= unemployment_scaler.transform( X_validation[['unemployment']].values ) \n\n##Validation dataset features transform - MinMaxScaler\nX_validation['fuel_price'] = fuel_price_scaler.transform( X_validation[['fuel_price']].values )\nX_validation['cpi']        = cpi_scaler.transform( X_validation[['cpi']].values )\nX_validation['size']       = size_scaler.transform( X_validation[['size']].values )\nX_validation['year']       = year_scaler.transform( X_validation[['year']].values )\n\n##Validation dataset features transform - Label Enconder\nX_validation['type'] = type_scaler.transform( X_validation['type'] )\n\n# is_holiday\nX_validation['is_holiday'] = X_validation['is_holiday'].apply(lambda x: 1 if x == True else 0)\n\n# target variable\nX_validation['weekly_sales'] = np.log1p( X_validation['weekly_sales'] )\n\n##Validation dataset features transform - Natural Transformations\n# month\nX_validation['month_sin'] = X_validation['month'].apply( lambda x: np.sin( x * ( 2. * np.pi/12 ) ) )\nX_validation['month_cos'] = X_validation['month'].apply( lambda x: np.cos( x * ( 2. * np.pi/12 ) ) )\n\n# day \nX_validation['day_sin'] = X_validation['day'].apply( lambda x: np.sin( x * ( 2. * np.pi/30 ) ) )\nX_validation['day_cos'] = X_validation['day'].apply( lambda x: np.cos( x * ( 2. * np.pi/30 ) ) )\n\n# week_of_year\nX_validation['week_of_year_sin'] = X_validation['week_of_year'].apply( lambda x: np.sin( x * ( 2. * np.pi/52 ) ) )\nX_validation['week_of_year_cos'] = X_validation['week_of_year'].apply( lambda x: np.cos( x * ( 2. * np.pi/52 ) ) )","metadata":{"ExecuteTime":{"end_time":"2022-05-30T09:03:34.428791Z","start_time":"2022-05-30T09:03:33.873854Z"},"execution":{"iopub.status.busy":"2022-05-30T16:27:30.857414Z","iopub.execute_input":"2022-05-30T16:27:30.857932Z","iopub.status.idle":"2022-05-30T16:27:31.843588Z","shell.execute_reply.started":"2022-05-30T16:27:30.857892Z","shell.execute_reply":"2022-05-30T16:27:31.842621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# new y_train with weekly sales rescaled\ny_validation = X_validation['weekly_sales']\n\ny_train = X_train['weekly_sales'] ","metadata":{"ExecuteTime":{"end_time":"2022-05-30T09:03:37.738819Z","start_time":"2022-05-30T09:03:37.733148Z"},"execution":{"iopub.status.busy":"2022-05-30T16:27:33.200028Z","iopub.execute_input":"2022-05-30T16:27:33.201106Z","iopub.status.idle":"2022-05-30T16:27:33.205429Z","shell.execute_reply.started":"2022-05-30T16:27:33.20106Z","shell.execute_reply":"2022-05-30T16:27:33.204749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 10 FEATURE SELECTION\n<a id=\"section-ten\"></a>","metadata":{}},{"cell_type":"markdown","source":"- Selecting the most relevant features that describes our dataset (phenomenon). removing collinear features, its explain the same part of the phenomenon.\n\n- Always prefer the simplest model! occla's razors idea","metadata":{}},{"cell_type":"code","source":"df6 = X_train.copy()","metadata":{"ExecuteTime":{"end_time":"2022-05-30T09:03:43.555349Z","start_time":"2022-05-30T09:03:43.502066Z"},"execution":{"iopub.status.busy":"2022-05-30T16:27:43.127316Z","iopub.execute_input":"2022-05-30T16:27:43.127768Z","iopub.status.idle":"2022-05-30T16:27:43.232417Z","shell.execute_reply.started":"2022-05-30T16:27:43.127731Z","shell.execute_reply":"2022-05-30T16:27:43.231546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# deleting features after feature engineering derivation and transformations. Deleting original variables.\ncols_drop = ['week_of_year', 'day', 'month', 'year_week', 'quarter']\ndf6 = df6.drop(cols_drop, axis = 1)","metadata":{"ExecuteTime":{"end_time":"2022-05-30T09:03:46.993774Z","start_time":"2022-05-30T09:03:46.953792Z"},"execution":{"iopub.status.busy":"2022-05-30T16:27:45.058268Z","iopub.execute_input":"2022-05-30T16:27:45.059335Z","iopub.status.idle":"2022-05-30T16:27:45.083386Z","shell.execute_reply.started":"2022-05-30T16:27:45.059296Z","shell.execute_reply":"2022-05-30T16:27:45.082362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#double checking dtypes before run models\ndf6.dtypes","metadata":{"ExecuteTime":{"end_time":"2022-05-29T14:20:36.228475Z","start_time":"2022-05-29T14:20:36.224198Z"},"execution":{"iopub.status.busy":"2022-05-30T16:27:46.301627Z","iopub.execute_input":"2022-05-30T16:27:46.302015Z","iopub.status.idle":"2022-05-30T16:27:46.309726Z","shell.execute_reply.started":"2022-05-30T16:27:46.301984Z","shell.execute_reply":"2022-05-30T16:27:46.308913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 10.1 Boruta as a Feature Selector","metadata":{}},{"cell_type":"markdown","source":"- I ran boruta localy due to the computational power required for it, the lines below are commented out so the algorithm doesn't run every time.","metadata":{}},{"cell_type":"code","source":"# # creating training and test dataset for Boruta, because it can't be a dataframe type\n# X_train_n = df6.drop( ['date', 'weekly_sales'], axis=1 ).values\n# y_train_n = y_train.values.ravel()\n\n# # Define RandomForestRegressor\n# rf = RandomForestRegressor( n_jobs=-1 )\n\n# # Define Boruta\n# boruta = BorutaPy( rf, n_estimators='auto', verbose=2, random_state=42 ).fit( X_train_n, y_train_n )","metadata":{"ExecuteTime":{"end_time":"2022-05-28T00:15:56.127368Z","start_time":"2022-05-28T00:15:56.125013Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 10.2 Best Features from Boruta","metadata":{}},{"cell_type":"code","source":"# cols_selected = boruta.support_.tolist()\n\n# X_train_fs = df6.drop(['date', 'weekly_sales'], axis = 1)\n# cols_selected_boruta = X_train_fs.iloc[ :, cols_selected].columns.tolist()\n\n# # Not selected boruta features\n# cols_not_selected_boruta = np.setdiff1d(X_train_fs.columns, cols_selected_boruta)","metadata":{"ExecuteTime":{"end_time":"2022-05-28T00:15:56.130934Z","start_time":"2022-05-28T00:15:56.128865Z"},"code_folding":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 10.3 Best Features from Random Forest","metadata":{}},{"cell_type":"code","source":"X_train = df6.drop( ['date', 'weekly_sales'], axis=1 ).copy()\ny_train = df6['weekly_sales'].copy()","metadata":{"ExecuteTime":{"end_time":"2022-05-29T17:01:44.094215Z","start_time":"2022-05-29T17:01:44.056713Z"},"execution":{"iopub.status.busy":"2022-05-30T16:28:01.906949Z","iopub.execute_input":"2022-05-30T16:28:01.907812Z","iopub.status.idle":"2022-05-30T16:28:02.009036Z","shell.execute_reply.started":"2022-05-30T16:28:01.907764Z","shell.execute_reply":"2022-05-30T16:28:02.008033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train random forest classifier\nrf = RandomForestRegressor(n_estimators = 200, n_jobs =-1, random_state = 42)\nrf.fit(X_train, y_train)\n\n# feature importance data frame\nfeat_imp = pd.DataFrame({'feature': X_train.columns,\n                        'feature_importance': rf.feature_importances_})\\\n                        .sort_values('feature_importance', ascending=False)\\\n                        .reset_index(drop=True)\n\n\n# plot feature importance\nplt.subplots(figsize=(12,6))\nsns.barplot(x='feature_importance', y='feature', data=feat_imp, orient='h', color='royalblue')\\\n    .set_title('Feature Importance');","metadata":{"ExecuteTime":{"end_time":"2022-05-28T01:30:56.377398Z","start_time":"2022-05-28T01:29:23.028538Z"},"scrolled":true,"execution":{"iopub.status.busy":"2022-05-30T16:28:03.730753Z","iopub.execute_input":"2022-05-30T16:28:03.731414Z","iopub.status.idle":"2022-05-30T16:32:15.66742Z","shell.execute_reply.started":"2022-05-30T16:28:03.731378Z","shell.execute_reply":"2022-05-30T16:32:15.666463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 10.4 Manual Feature Selection","metadata":{}},{"cell_type":"markdown","source":"- Features was selected based on the junction of Boruta algorithm, EDA and feature importance by random forest;","metadata":{}},{"cell_type":"code","source":"cols_selected_boruta = ['store',\n'dept',\n'is_holiday',\n'type',\n'cpi',\n'size',\n'month_cos',\n'week_of_year_cos']\n# columns to add\nfeat_to_add = ['date', 'weekly_sales']\n\n# final features\n\ncols_selected_boruta_full = cols_selected_boruta.copy()\ncols_selected_boruta_full.extend( feat_to_add )","metadata":{"ExecuteTime":{"end_time":"2022-05-30T09:04:02.325891Z","start_time":"2022-05-30T09:04:02.313311Z"},"execution":{"iopub.status.busy":"2022-05-30T16:32:15.669452Z","iopub.execute_input":"2022-05-30T16:32:15.669948Z","iopub.status.idle":"2022-05-30T16:32:15.676232Z","shell.execute_reply.started":"2022-05-30T16:32:15.669901Z","shell.execute_reply":"2022-05-30T16:32:15.675514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# cols_selected_boruta = ['store',\n# 'dept',\n# 'is_holiday',\n# 'temperature',\n# 'fuel_price',\n# 'mark_down3',\n# 'cpi',\n# 'unemployment',\n# 'type',\n# 'size',\n# 'month_cos',\n# 'day_sin',\n# 'day_cos',\n# 'week_of_year_sin',\n# 'week_of_year_cos']\n# # columns to add\n# feat_to_add = ['date', 'weekly_sales']\n\n# # final features\n\n# cols_selected_boruta_full = cols_selected_boruta.copy()\n# cols_selected_boruta_full.extend( feat_to_add )","metadata":{"ExecuteTime":{"end_time":"2022-05-28T00:15:56.189683Z","start_time":"2022-05-28T00:15:56.186301Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(data = cols_selected_boruta, columns = ['feature_selected'])","metadata":{"ExecuteTime":{"end_time":"2022-05-28T00:15:56.197558Z","start_time":"2022-05-28T00:15:56.192186Z"},"execution":{"iopub.status.busy":"2022-05-30T16:32:15.677539Z","iopub.execute_input":"2022-05-30T16:32:15.677872Z","iopub.status.idle":"2022-05-30T16:32:15.696409Z","shell.execute_reply.started":"2022-05-30T16:32:15.677842Z","shell.execute_reply":"2022-05-30T16:32:15.695379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 11 MACHINE LEARNING ALGORITHM MODELS\n<a id=\"section-eleven\"></a>","metadata":{}},{"cell_type":"markdown","source":"**Five different algorithms are going to be used to predict the target variable:**\n\n- **Average:** averaging model is the model we use most in everyday life, it will always predict the average. It is useful as it is a comparative basis for implementing other models\n\n- **Logistic Regression:** uses a complex cost function, which can be defined as the Sigmoid function. The output of the classification is based on the probability score between 0 and 1 of the input being in one class or another according to a threshold\n\n- **Random Forest:** it is a tree based model build with multiple ensamble decision trees created with the bagging method. Then, all the classifiers take a weighted vote on their predictions. Since the algorithm goal is not trying to find a linear function to describe the event, it works for problems with more complex behaviour\n\n- **XGBoost:** it is also a tree based model but they are built in a different way. While Random Forests builds each tree independently, XGBoost builds one tree at the time learning with its predecessor. Therefore, this algorithm doesn't combine results at the end of the process by taking majority votes, it combines the results along the way\n\n- **LightGBM:** is a gradient boosting framework that uses tree based learning algorithms. It is designed to be distributed and efficient with the following advantages:\n\n - Faster training speed and higher efficiency.\n - Lower memory usage.\n - Better accuracy.\n - Support of parallel, distributed, and GPU learning.\n - Capable of handling large-scale data.","metadata":{}},{"cell_type":"markdown","source":"- if we have multiple models to choose from, we should choose the least complex model as they generalize learning better.","metadata":{"ExecuteTime":{"end_time":"2022-05-26T21:41:32.935721Z","start_time":"2022-05-26T21:41:32.92389Z"}}},{"cell_type":"code","source":"# Applying selected features by boruta on train and validation datasets\nx_train = X_train[ cols_selected_boruta ].copy() #selecting only the columns selected by boruta\nx_validation = X_validation[ cols_selected_boruta ].copy()\n\n# Time Series Data Preparation for cross-validation\nx_training = df6[ cols_selected_boruta_full ].copy()","metadata":{"ExecuteTime":{"end_time":"2022-05-30T09:04:10.081061Z","start_time":"2022-05-30T09:04:10.001634Z"},"execution":{"iopub.status.busy":"2022-05-30T16:32:15.699085Z","iopub.execute_input":"2022-05-30T16:32:15.699609Z","iopub.status.idle":"2022-05-30T16:32:15.777613Z","shell.execute_reply.started":"2022-05-30T16:32:15.699559Z","shell.execute_reply":"2022-05-30T16:32:15.776865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 11.1 Average Model","metadata":{}},{"cell_type":"code","source":"aux1 = x_validation.copy()\naux1['weekly_sales'] = y_validation.copy()\n\n# Predictions\naux2 = aux1[['store', 'weekly_sales']].groupby('store').mean().reset_index().rename(columns = {'weekly_sales': 'predictions'})\naux1 = pd.merge( aux1, aux2, how= 'left', on='store')\nyhat_baseline = aux1['predictions']\n\n# Performance\nbaseline_result = ml_error( aux1 ,'Average Model', np.expm1( y_validation ), np.expm1( yhat_baseline ))\nbaseline_result","metadata":{"ExecuteTime":{"end_time":"2022-05-29T18:15:29.373319Z","start_time":"2022-05-29T18:15:29.310403Z"},"execution":{"iopub.status.busy":"2022-05-30T16:32:15.778838Z","iopub.execute_input":"2022-05-30T16:32:15.779291Z","iopub.status.idle":"2022-05-30T16:32:15.879142Z","shell.execute_reply.started":"2022-05-30T16:32:15.77926Z","shell.execute_reply":"2022-05-30T16:32:15.878169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 11.2 Linear Regression Model","metadata":{}},{"cell_type":"code","source":"# Model\nlr = LinearRegression().fit(x_train, y_train)\n\n# Prediction \nyhat_lr = lr.predict( x_validation )\n\n# Performance\nlr_result = ml_error( x_validation,'Linear Regression', np.expm1(y_validation), np.expm1(yhat_lr))\nlr_result","metadata":{"ExecuteTime":{"end_time":"2022-05-29T16:54:57.372578Z","start_time":"2022-05-29T16:54:57.218017Z"},"execution":{"iopub.status.busy":"2022-05-30T16:32:15.880546Z","iopub.execute_input":"2022-05-30T16:32:15.880988Z","iopub.status.idle":"2022-05-30T16:32:16.128351Z","shell.execute_reply.started":"2022-05-30T16:32:15.880944Z","shell.execute_reply":"2022-05-30T16:32:16.127454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Linear Regression Model - Cross Validation","metadata":{}},{"cell_type":"code","source":"lr_result_cv = cross_validation( x_training, 5, 'Linear Regression', lr, verbose=False )\nlr_result_cv","metadata":{"ExecuteTime":{"end_time":"2022-05-29T16:55:05.963599Z","start_time":"2022-05-29T16:55:05.459475Z"},"execution":{"iopub.status.busy":"2022-05-30T16:32:16.129526Z","iopub.execute_input":"2022-05-30T16:32:16.129866Z","iopub.status.idle":"2022-05-30T16:32:17.001193Z","shell.execute_reply.started":"2022-05-30T16:32:16.129835Z","shell.execute_reply":"2022-05-30T16:32:17.000049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 11.3 Linear Regression Regularized Model - Lasso","metadata":{}},{"cell_type":"code","source":"# model\nlrr = Lasso( alpha=0.01 ).fit( x_train, y_train )\n\n# prediction\nyhat_lrr = lrr.predict( x_validation )\n\n# performance\nlrr_result = ml_error( x_validation,'Linear Regression - Lasso', np.expm1( y_validation ), np.expm1( yhat_lrr ) )\nlrr_result","metadata":{"ExecuteTime":{"end_time":"2022-05-29T16:55:10.623047Z","start_time":"2022-05-29T16:55:10.497229Z"},"execution":{"iopub.status.busy":"2022-05-30T16:32:17.007705Z","iopub.execute_input":"2022-05-30T16:32:17.011157Z","iopub.status.idle":"2022-05-30T16:32:17.229326Z","shell.execute_reply.started":"2022-05-30T16:32:17.011072Z","shell.execute_reply":"2022-05-30T16:32:17.228198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Linear Regression Regularized Model - Lasso","metadata":{}},{"cell_type":"code","source":"lrr_result_cv = cross_validation( x_training, 5, 'Linear Regression Regularized Model - Lasso', lrr, verbose=False )\nlrr_result_cv","metadata":{"ExecuteTime":{"end_time":"2022-05-29T16:55:14.102422Z","start_time":"2022-05-29T16:55:13.578739Z"},"scrolled":true,"execution":{"iopub.status.busy":"2022-05-30T16:32:17.231958Z","iopub.execute_input":"2022-05-30T16:32:17.232522Z","iopub.status.idle":"2022-05-30T16:32:18.128676Z","shell.execute_reply.started":"2022-05-30T16:32:17.232458Z","shell.execute_reply":"2022-05-30T16:32:18.127882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 11.4 Random Forest Regressor","metadata":{}},{"cell_type":"code","source":"# model\nrf = RandomForestRegressor( n_estimators = 100, n_jobs =-1, random_state=7 ).fit( x_train, y_train )\n\n# prediction\nyhat_rf = rf.predict( x_validation )\n\n# performance\nrf_result = ml_error( x_validation,'Random Forest Regressor', np.expm1( y_validation ), np.expm1( yhat_rf ) )\nrf_result","metadata":{"ExecuteTime":{"end_time":"2022-05-29T17:06:07.027435Z","start_time":"2022-05-29T17:05:51.562809Z"},"execution":{"iopub.status.busy":"2022-05-30T16:32:18.132597Z","iopub.execute_input":"2022-05-30T16:32:18.13294Z","iopub.status.idle":"2022-05-30T16:33:05.10118Z","shell.execute_reply.started":"2022-05-30T16:32:18.132912Z","shell.execute_reply":"2022-05-30T16:33:05.100276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Random Forest Regressor - Cross Validation","metadata":{}},{"cell_type":"code","source":"rf_result_cv = cross_validation( x_training, 5, 'Random Forest Regressor', rf, verbose=False )\nrf_result_cv","metadata":{"ExecuteTime":{"end_time":"2022-05-29T17:06:46.24113Z","start_time":"2022-05-29T17:06:11.967294Z"},"execution":{"iopub.status.busy":"2022-05-30T16:33:05.102701Z","iopub.execute_input":"2022-05-30T16:33:05.103322Z","iopub.status.idle":"2022-05-30T16:34:43.213122Z","shell.execute_reply.started":"2022-05-30T16:33:05.103287Z","shell.execute_reply":"2022-05-30T16:34:43.212159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 11.5 XGBoost Regressor","metadata":{"heading_collapsed":true}},{"cell_type":"code","source":"# model\nmodel_xgb = xgb.XGBRegressor( objective='reg:squarederror',\n                              n_estimators = 100, random_state=7).fit( x_train, y_train )\n\n# prediction\nyhat_xgb = model_xgb.predict( x_validation )\n\n# performance\nxgb_result = ml_error(x_validation ,'XGBoost Regressor', np.expm1( y_validation ), np.expm1( yhat_xgb ) )\nxgb_result","metadata":{"ExecuteTime":{"end_time":"2022-05-29T14:32:47.414642Z","start_time":"2022-05-29T14:32:43.756773Z"},"hidden":true,"execution":{"iopub.status.busy":"2022-05-30T16:34:43.214657Z","iopub.execute_input":"2022-05-30T16:34:43.21569Z","iopub.status.idle":"2022-05-30T16:34:53.614533Z","shell.execute_reply.started":"2022-05-30T16:34:43.21565Z","shell.execute_reply":"2022-05-30T16:34:53.613724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### XGBoost Regressor - Cross Validation","metadata":{"hidden":true}},{"cell_type":"code","source":"xgb_result_cv = cross_validation( x_training, 5, 'XGBoost Regressor', model_xgb, verbose=False )\nxgb_result_cv","metadata":{"ExecuteTime":{"end_time":"2022-05-29T14:33:11.879838Z","start_time":"2022-05-29T14:33:02.897259Z"},"hidden":true,"execution":{"iopub.status.busy":"2022-05-30T16:34:53.615789Z","iopub.execute_input":"2022-05-30T16:34:53.616071Z","iopub.status.idle":"2022-05-30T16:35:18.852375Z","shell.execute_reply.started":"2022-05-30T16:34:53.616045Z","shell.execute_reply":"2022-05-30T16:35:18.851303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 11.6 LightGBM Regressor","metadata":{"heading_collapsed":true}},{"cell_type":"code","source":"# model\nmodel_lgbm = lgbm.LGBMRegressor(n_estimators = 100, n_jobs =-1, random_state=7).fit( x_train, y_train )\n\n# prediction\nyhat_lgbm = model_lgbm.predict( x_validation )\n\n# performance\nlgbm_result = ml_error(x_validation ,'LightGBM Regressor', np.expm1( y_validation ), np.expm1( yhat_lgbm ) )\nlgbm_result","metadata":{"ExecuteTime":{"end_time":"2022-05-29T14:43:27.529195Z","start_time":"2022-05-29T14:43:26.529269Z"},"hidden":true,"execution":{"iopub.status.busy":"2022-05-30T16:35:18.853903Z","iopub.execute_input":"2022-05-30T16:35:18.854602Z","iopub.status.idle":"2022-05-30T16:35:20.122795Z","shell.execute_reply.started":"2022-05-30T16:35:18.854559Z","shell.execute_reply":"2022-05-30T16:35:20.121478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### LightGBM Regressor - Cross Validation","metadata":{"hidden":true}},{"cell_type":"code","source":"lgbm_result_cv = cross_validation( x_training, 5, 'LightGBM Regressor', model_lgbm, verbose=False )\nlgbm_result_cv","metadata":{"ExecuteTime":{"end_time":"2022-05-29T14:41:45.546173Z","start_time":"2022-05-29T14:41:43.636572Z"},"hidden":true,"execution":{"iopub.status.busy":"2022-05-30T16:35:20.123999Z","iopub.execute_input":"2022-05-30T16:35:20.124358Z","iopub.status.idle":"2022-05-30T16:35:23.862589Z","shell.execute_reply.started":"2022-05-30T16:35:20.124326Z","shell.execute_reply":"2022-05-30T16:35:23.861694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 11.7 Compare Model's Performance","metadata":{}},{"cell_type":"markdown","source":"#### 11.7.1 Single Performance - 1 fold","metadata":{}},{"cell_type":"code","source":"results = pd.concat( [baseline_result, lr_result, lrr_result, rf_result, xgb_result, lgbm_result ] ).set_index('Model Name')\nresults.sort_values('RMSE')","metadata":{"ExecuteTime":{"end_time":"2022-05-29T15:07:58.383335Z","start_time":"2022-05-29T15:07:58.376424Z"},"scrolled":true,"execution":{"iopub.status.busy":"2022-05-30T16:35:23.863812Z","iopub.execute_input":"2022-05-30T16:35:23.864148Z","iopub.status.idle":"2022-05-30T16:35:23.881817Z","shell.execute_reply.started":"2022-05-30T16:35:23.864118Z","shell.execute_reply":"2022-05-30T16:35:23.880813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 11.7.2 Real Performance - Cross Validation - 5 folds","metadata":{}},{"cell_type":"code","source":"results_cv = pd.concat([lr_result_cv , lrr_result_cv , rf_result_cv, xgb_result_cv, lgbm_result_cv]).set_index('Model Name')\nresults_cv","metadata":{"ExecuteTime":{"end_time":"2022-05-29T15:09:40.335234Z","start_time":"2022-05-29T15:09:40.317405Z"},"scrolled":true,"execution":{"iopub.status.busy":"2022-05-30T16:35:23.883227Z","iopub.execute_input":"2022-05-30T16:35:23.884327Z","iopub.status.idle":"2022-05-30T16:35:23.898187Z","shell.execute_reply.started":"2022-05-30T16:35:23.884276Z","shell.execute_reply":"2022-05-30T16:35:23.897178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- The choosen model was the one with the best performance with crossvalidation -> **Random Forest Regressor**","metadata":{}},{"cell_type":"code","source":"# Blocked time series k fold Cross-validation strategy used in the models\nImage(home_kaggle + 'images2/blocked-time-series-kfold.png')","metadata":{"execution":{"iopub.status.busy":"2022-05-30T16:35:23.899586Z","iopub.execute_input":"2022-05-30T16:35:23.900448Z","iopub.status.idle":"2022-05-30T16:35:23.925432Z","shell.execute_reply.started":"2022-05-30T16:35:23.900385Z","shell.execute_reply":"2022-05-30T16:35:23.924647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 12 HYPERPARAMETERS FINE TUNING\n<a id=\"section-twelve\"></a>","metadata":{}},{"cell_type":"markdown","source":"### 12.1 Random Search","metadata":{}},{"cell_type":"code","source":"# param = {\n#     'n_estimators': [300, 400] ,\n#     'max_features': ['auto'],\n#     'max_depth': [21,25,31],\n#     'min_samples_split':[5, 7] ,\n#     'min_samples_leaf': [1, 2, 5],\n#         }\n\n# MAX_EVAL = 5; #quantas iterações temos","metadata":{"ExecuteTime":{"end_time":"2022-05-29T17:45:17.798144Z","start_time":"2022-05-29T17:45:17.789267Z"},"execution":{"iopub.status.busy":"2022-05-30T16:35:23.926624Z","iopub.execute_input":"2022-05-30T16:35:23.927362Z","iopub.status.idle":"2022-05-30T16:35:23.932389Z","shell.execute_reply.started":"2022-05-30T16:35:23.92733Z","shell.execute_reply":"2022-05-30T16:35:23.931532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# final_result = pd.DataFrame()\n\n# for i in range(MAX_EVAL):\n#     # choose values for parameters randomly\n#     hp = { k: random.sample(v, 1)[0] for k, v in param.items() }\n#     print(hp)\n    \n#     # model\n#     model_rf = RandomForestRegressor(n_estimators = hp['n_estimators'],\n#                                   max_features = hp['max_features'],\n#                                   max_depth = hp['max_depth'],\n#                                   min_samples_split = hp['min_samples_split'],\n#                                   min_samples_leaf = hp['min_samples_leaf'],\n#                                   random_state = 7,\n#                                   n_jobs = -1)\n    \n#     # performance\n#     result = cross_validation( x_training, 5, 'Random Forest Regressor', model_rf, verbose = True)\n#     final_result = pd.concat([final_result, result])\n\n# final_result","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 12.2 Final Model","metadata":{}},{"cell_type":"code","source":"param_tuned = {\n    'n_estimators':300,\n    'max_features': 'auto',\n    'min_samples_split': 5,\n    'min_samples_leaf': 1 ,\n        }","metadata":{"ExecuteTime":{"end_time":"2022-05-30T09:04:35.37979Z","start_time":"2022-05-30T09:04:35.376469Z"},"execution":{"iopub.status.busy":"2022-05-30T16:45:42.972566Z","iopub.execute_input":"2022-05-30T16:45:42.972994Z","iopub.status.idle":"2022-05-30T16:45:42.978185Z","shell.execute_reply.started":"2022-05-30T16:45:42.972958Z","shell.execute_reply":"2022-05-30T16:45:42.977417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model\nmodel_rf_tuned = RandomForestRegressor(n_estimators = param_tuned['n_estimators'],\n                                  max_features      = param_tuned['max_features'],\n                                  min_samples_split = param_tuned['min_samples_split'],\n                                  min_samples_leaf  = param_tuned['min_samples_leaf'],\n                                  n_jobs = -1,\n                                  random_state = 7).fit(x_train, y_train)\n\n#prediction\nyhat_rf_tuned = model_rf_tuned.predict(x_validation)\n\n# performance\nrf_tuned_result = ml_error( x_validation,'Random Forest Regressor', np.expm1(y_validation) , np.expm1(yhat_rf_tuned))\nrf_tuned_result","metadata":{"ExecuteTime":{"end_time":"2022-05-30T09:05:29.831309Z","start_time":"2022-05-30T09:04:45.532277Z"},"scrolled":true,"execution":{"iopub.status.busy":"2022-05-30T16:45:52.815147Z","iopub.execute_input":"2022-05-30T16:45:52.815571Z","iopub.status.idle":"2022-05-30T16:47:53.639789Z","shell.execute_reply.started":"2022-05-30T16:45:52.815536Z","shell.execute_reply":"2022-05-30T16:47:53.639032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 13 ERROR INTERPRETATION\n<a id=\"section-thirteen\"></a>","metadata":{}},{"cell_type":"code","source":"# selecting validation dataframe to evaluate error \ndf13 = X_validation[cols_selected_boruta_full]\n\n# rescale\ndf13['weekly_sales'] = np.expm1(df13['weekly_sales'])\ndf13['predictions'] = np.expm1(yhat_rf_tuned)","metadata":{"ExecuteTime":{"end_time":"2022-05-30T09:08:34.787554Z","start_time":"2022-05-30T09:08:34.760212Z"},"execution":{"iopub.status.busy":"2022-05-30T16:47:53.640997Z","iopub.execute_input":"2022-05-30T16:47:53.641668Z","iopub.status.idle":"2022-05-30T16:47:53.654218Z","shell.execute_reply.started":"2022-05-30T16:47:53.641635Z","shell.execute_reply":"2022-05-30T16:47:53.653006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checking new error features\ndf13.head()","metadata":{"ExecuteTime":{"end_time":"2022-05-30T09:08:36.198703Z","start_time":"2022-05-30T09:08:36.176829Z"},"execution":{"iopub.status.busy":"2022-05-30T16:47:53.655817Z","iopub.execute_input":"2022-05-30T16:47:53.656367Z","iopub.status.idle":"2022-05-30T16:47:53.676553Z","shell.execute_reply.started":"2022-05-30T16:47:53.656317Z","shell.execute_reply":"2022-05-30T16:47:53.675624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 13.1 Business Performance - Store Granularity","metadata":{}},{"cell_type":"markdown","source":"- In this section we are evaluating bussiness peformance for each store. Looking at the sales for the next 22 weeks","metadata":{}},{"cell_type":"code","source":"# sum of prediction\ndf131 = df13[['store', 'predictions']].groupby('store').sum().reset_index()\n\n# MAE and MAPE\ndf13_aux1 = df13[['store', 'weekly_sales', 'predictions']].groupby('store').apply(lambda x: mean_absolute_error(x['weekly_sales'], x['predictions']) ).reset_index().rename(columns = {0: 'MAE'})\n\ndf13_aux2 = df13[['store', 'weekly_sales', 'predictions']].groupby('store').apply(lambda x: mean_absolute_percentage_error(x['weekly_sales'], x['predictions']) ).reset_index().rename(columns = {0: 'MAPE'})\n\ndf13_aux3 = df13[['store', 'weekly_sales', 'predictions']].groupby('store').apply(lambda x: weighted_mean_absolute_error(df13 ,x['weekly_sales'], x['predictions']) ).reset_index().rename(columns = {0: 'WMAE'})\n\n\n# merge\ndf13_aux4 = pd.merge(df13_aux1, df13_aux2, how = 'inner', on = 'store')\ndf13_aux5 = pd.merge(df13_aux4, df13_aux3, how = 'inner', on = 'store')\n\ndf132 = pd.merge(df131, df13_aux5, how = 'inner', on = 'store')\n\n# Scenerios\n\ndf132['worst_scenario'] = df132['predictions'] - df132['MAE']\ndf132['best_scenario'] = df132['predictions'] + df132['MAE']\n\n\ndf132 = df132[['store', 'predictions', 'worst_scenario', 'best_scenario', 'MAE', 'MAPE', 'WMAE']]","metadata":{"ExecuteTime":{"end_time":"2022-05-30T09:13:16.848974Z","start_time":"2022-05-30T09:13:16.051081Z"},"execution":{"iopub.status.busy":"2022-05-30T16:47:53.678152Z","iopub.execute_input":"2022-05-30T16:47:53.678698Z","iopub.status.idle":"2022-05-30T16:47:55.485601Z","shell.execute_reply.started":"2022-05-30T16:47:53.678662Z","shell.execute_reply":"2022-05-30T16:47:55.484656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# worst and best scenarios\ndf132.sort_values('WMAE', ascending = True).head().style.format({'predictions': '${0:,.2f}', 'worst_scenario': '${:,.2f}',  'best_scenario': '${0:,.2f}', 'MAE': '${0:,.2f}', 'MAPE': '{:.2%}'})","metadata":{"ExecuteTime":{"end_time":"2022-05-30T09:13:24.967301Z","start_time":"2022-05-30T09:13:24.892276Z"},"scrolled":true,"execution":{"iopub.status.busy":"2022-05-30T16:47:55.487042Z","iopub.execute_input":"2022-05-30T16:47:55.487518Z","iopub.status.idle":"2022-05-30T16:47:55.566422Z","shell.execute_reply.started":"2022-05-30T16:47:55.487458Z","shell.execute_reply":"2022-05-30T16:47:55.565351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# worst and best scenarios\ndf132.sort_values('WMAE', ascending = False).head().style.format({'predictions': '${0:,.2f}', 'worst_scenario': '${:,.2f}',  'best_scenario': '${0:,.2f}', 'MAE': '${0:,.2f}', 'MAPE': '{:.2%}'})","metadata":{"ExecuteTime":{"end_time":"2022-05-30T09:13:36.367986Z","start_time":"2022-05-30T09:13:36.343983Z"},"execution":{"iopub.status.busy":"2022-05-30T16:47:55.567937Z","iopub.execute_input":"2022-05-30T16:47:55.568406Z","iopub.status.idle":"2022-05-30T16:47:55.580158Z","shell.execute_reply.started":"2022-05-30T16:47:55.568372Z","shell.execute_reply":"2022-05-30T16:47:55.579199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# scatterplot\nsns.scatterplot(x = 'store', y = 'WMAE', data = df132 );","metadata":{"ExecuteTime":{"end_time":"2022-05-30T09:13:51.632801Z","start_time":"2022-05-30T09:13:51.454578Z"},"execution":{"iopub.status.busy":"2022-05-30T16:47:55.581769Z","iopub.execute_input":"2022-05-30T16:47:55.582461Z","iopub.status.idle":"2022-05-30T16:47:55.867797Z","shell.execute_reply.started":"2022-05-30T16:47:55.582424Z","shell.execute_reply":"2022-05-30T16:47:55.867112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 13.2 Business Performance - Department Granularity","metadata":{}},{"cell_type":"code","source":"# sum of prediction\ndf10 = df13[['store', 'dept', 'predictions']].groupby(['store','dept']).sum().reset_index()\n\n# MAE and MAPE\ndf10_aux1 = df13[['store', 'dept', 'weekly_sales', 'predictions']].groupby(['store','dept']).apply(lambda x: mean_absolute_error(x['weekly_sales'], x['predictions']) ).reset_index().rename(columns = {0: 'MAE'})\n\ndf10_aux2 = df13[['store', 'dept', 'weekly_sales', 'predictions']].groupby(['store','dept']).apply(lambda x: mean_absolute_percentage_error(x['weekly_sales'], x['predictions']) ).reset_index().rename(columns = {0: 'MAPE'})\n\ndf10_aux3 = df13[['store', 'dept', 'weekly_sales', 'predictions']].groupby(['store','dept']).apply(lambda x: weighted_mean_absolute_error(df13 ,x['weekly_sales'], x['predictions']) ).reset_index().rename(columns = {0: 'WMAE'})\n\n\n# merge\ndf10_aux4 = pd.merge(df10_aux1, df10_aux2, how = 'inner', on = ['store','dept'])\ndf10_aux5 = pd.merge(df10_aux4, df10_aux3, how = 'inner', on = ['store','dept'])\n\ndf12 = pd.merge(df10, df10_aux5, how = 'inner', on = ['store','dept'])\n\n# Scenerios\n\ndf12['worst_scenario'] = df12['predictions'] - df12['MAE']\ndf12['best_scenario'] = df12['predictions'] + df12['MAE']\n\n\ndf12 = df12[['store','dept', 'predictions', 'worst_scenario', 'best_scenario', 'MAE', 'MAPE', 'WMAE']]","metadata":{"ExecuteTime":{"end_time":"2022-05-30T09:15:24.411685Z","start_time":"2022-05-30T09:14:31.805589Z"},"execution":{"iopub.status.busy":"2022-05-30T16:47:55.869011Z","iopub.execute_input":"2022-05-30T16:47:55.869733Z","iopub.status.idle":"2022-05-30T16:49:58.369124Z","shell.execute_reply.started":"2022-05-30T16:47:55.869697Z","shell.execute_reply":"2022-05-30T16:49:58.368105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# scatterplot\nsns.scatterplot(x = 'store', y = 'WMAE', data = df12 );","metadata":{"ExecuteTime":{"end_time":"2022-05-30T09:15:24.666988Z","start_time":"2022-05-30T09:15:24.413111Z"},"execution":{"iopub.status.busy":"2022-05-30T16:49:58.370985Z","iopub.execute_input":"2022-05-30T16:49:58.371441Z","iopub.status.idle":"2022-05-30T16:49:58.620771Z","shell.execute_reply.started":"2022-05-30T16:49:58.371394Z","shell.execute_reply":"2022-05-30T16:49:58.619976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 13.3 Total Performance","metadata":{}},{"cell_type":"code","source":"# Total Walmart sales predictions for the next 22 weeks\ndf133 = df132[['predictions', 'worst_scenario', 'best_scenario']].apply( lambda x: np.sum( x ), axis=0 ).reset_index().rename( columns={'index': 'Scenario', 0:'Values'} )\ndf133['Values'] = df133['Values'].map( 'R$ {:,.2f}'.format )\ndf133","metadata":{"ExecuteTime":{"end_time":"2022-05-30T09:15:52.297596Z","start_time":"2022-05-30T09:15:52.285045Z"},"execution":{"iopub.status.busy":"2022-05-30T16:49:58.622784Z","iopub.execute_input":"2022-05-30T16:49:58.623733Z","iopub.status.idle":"2022-05-30T16:49:58.638999Z","shell.execute_reply.started":"2022-05-30T16:49:58.623695Z","shell.execute_reply":"2022-05-30T16:49:58.637926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 13.4 Machine Learning Performance ","metadata":{}},{"cell_type":"code","source":"# defining error and error rate\ndf13['error'] = df13['weekly_sales'] - df13['predictions']\ndf13['error_rate'] = df13['predictions'] / df13['weekly_sales']","metadata":{"ExecuteTime":{"end_time":"2022-05-30T09:16:35.095665Z","start_time":"2022-05-30T09:16:35.068066Z"},"execution":{"iopub.status.busy":"2022-05-30T16:49:58.640706Z","iopub.execute_input":"2022-05-30T16:49:58.641302Z","iopub.status.idle":"2022-05-30T16:49:58.648653Z","shell.execute_reply.started":"2022-05-30T16:49:58.641269Z","shell.execute_reply":"2022-05-30T16:49:58.647849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ML error analysis\nplt.subplot( 2, 2, 1 )\nsns.lineplot( x='date', y='weekly_sales', data=df13, label='SALES' )\nsns.lineplot( x='date', y='predictions', data=df13, label='PREDICTIONS' )\n\nplt.subplot( 2, 2, 2 )\nsns.lineplot( x='date', y='error_rate', data=df13 )\nplt.axhline( 1, linestyle='--', color = 'red')\n\nplt.subplot( 2, 2, 3 )\nsns.distplot( df13['error'] )\n\nplt.subplot( 2, 2, 4 )\nsns.scatterplot( df13['predictions'], df13['error'] );\nplt.style.use('tableau-colorblind10');","metadata":{"ExecuteTime":{"end_time":"2022-05-30T09:17:27.326451Z","start_time":"2022-05-30T09:17:19.495963Z"},"code_folding":[0],"execution":{"iopub.status.busy":"2022-05-30T16:49:58.650375Z","iopub.execute_input":"2022-05-30T16:49:58.650739Z","iopub.status.idle":"2022-05-30T16:50:14.120224Z","shell.execute_reply.started":"2022-05-30T16:49:58.650708Z","shell.execute_reply":"2022-05-30T16:50:14.119207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- The first graph above shows us the predictions (orage line) which tends to follow the sales values (blue line) over the last 22 weeks of sales, meaning that the model predicted well the sales, so it is following a good pattern.\n\n- The second graph shows us the error rate against the sales. The error rate is the ratio between prediction values and observed values. The model does have some high rates, it could performe better.\n\n- One of the premises for a good machine learning model is to have a normal-shaped distribution of residuals with mean zero. In the third graph, we can observe that the errors are centered around zero, and its distribution resembles a normal, bell-shaped curve.\n\n- The last graph is a scatterplot with predictions plotted against the error for each sales day. Ideally, we would have all data points concentrated within a \"tube\" since it represents low error variance across all values that sales prediction can assume. In our case some predictions have high error values, so we can stress more the model or create better features to get better performance.","metadata":{}},{"cell_type":"markdown","source":"## 14 MODEL SUBMISSION\n<a id=\"section-fourteen\"></a>","metadata":{}},{"cell_type":"code","source":"model_rf_tuned","metadata":{"ExecuteTime":{"end_time":"2022-05-30T09:24:59.564487Z","start_time":"2022-05-30T09:24:59.537991Z"},"scrolled":true,"execution":{"iopub.status.busy":"2022-05-30T16:50:16.606203Z","iopub.execute_input":"2022-05-30T16:50:16.606638Z","iopub.status.idle":"2022-05-30T16:50:16.615933Z","shell.execute_reply.started":"2022-05-30T16:50:16.606593Z","shell.execute_reply":"2022-05-30T16:50:16.615087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save Trained Model\n# pickle.dump(model_rf_tuned, open(home_path + 'model/model_walmart.pkl', 'wb'))","metadata":{"ExecuteTime":{"end_time":"2022-05-30T09:25:48.696424Z","start_time":"2022-05-30T09:25:11.927308Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 14.1 Loading Model and Scalers","metadata":{}},{"cell_type":"code","source":"#loading scalers\n#Robust Scaler \ntemperature_scaler  = pickle.load(open(home_kaggle + 'parameters/temperature_scaler.pkl', 'rb'))\nmark_down1_scaler   = pickle.load(open(home_kaggle + 'parameters/mark_down1_scaler.pkl', 'rb'))\nmark_down2_scaler   = pickle.load(open(home_kaggle + 'parameters/mark_down2_scaler.pkl', 'rb'))\nmark_down3_scaler   = pickle.load(open(home_kaggle + 'parameters/mark_down3_scaler.pkl', 'rb'))\nmark_down4_scaler   = pickle.load(open(home_kaggle + 'parameters/mark_down4_scaler.pkl', 'rb'))\nmark_down5_scaler   = pickle.load(open(home_kaggle + 'parameters/mark_down5_scaler.pkl', 'rb'))\nunemployment_scaler = pickle.load(open(home_kaggle + 'parameters/unemployment_scaler.pkl', 'rb'))\n\n#MinMax Scaler \nfuel_price_scaler   = pickle.load(open(home_kaggle + 'parameters/fuel_price_scaler.pkl', 'rb'))\ncpi_scaler          = pickle.load(open(home_kaggle + 'parameters/cpi_scaler.pkl', 'rb'))\nsize_scaler         = pickle.load(open(home_kaggle + 'parameters/size_scaler.pkl', 'rb'))\nyear_scaler         = pickle.load(open(home_kaggle + 'parameters/year_scaler.pkl', 'rb'))\n\n#Label enconder\ntype_scaler         = pickle.load(open(home_kaggle + 'parameters/type_scaler.pkl', 'rb'))","metadata":{"ExecuteTime":{"end_time":"2022-05-30T10:53:52.321155Z","start_time":"2022-05-30T10:53:52.298188Z"},"execution":{"iopub.status.busy":"2022-05-30T16:50:30.096722Z","iopub.execute_input":"2022-05-30T16:50:30.09756Z","iopub.status.idle":"2022-05-30T16:50:30.122803Z","shell.execute_reply.started":"2022-05-30T16:50:30.097522Z","shell.execute_reply":"2022-05-30T16:50:30.121999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# loading trained model\nmodel = pickle.load(open(home_kaggle + 'models/model_walmart.pkl', 'rb'))","metadata":{"ExecuteTime":{"end_time":"2022-05-30T10:54:27.00151Z","start_time":"2022-05-30T10:54:16.827042Z"},"execution":{"iopub.status.busy":"2022-05-30T16:50:34.328719Z","iopub.execute_input":"2022-05-30T16:50:34.329618Z","iopub.status.idle":"2022-05-30T16:51:04.286994Z","shell.execute_reply.started":"2022-05-30T16:50:34.329573Z","shell.execute_reply":"2022-05-30T16:51:04.285092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 14.2 Data ETL","metadata":{}},{"cell_type":"code","source":"# defining data pipeline\ndef data_merge(df_test, df_features, df_stores):\n    # merge datasets into one\n    df_store_feature = df_features.merge(df_stores, on = 'Store', how = 'left')\n    \n    # main dataframe for exploring\n    df_test = df_test.merge(df_store_feature, on = ['Store', 'Date', 'IsHoliday'], how = 'left').sort_values(['Store','Dept','Date'])\n    return df_test\n\ndef data_cleaning(df1):\n    ### Rename Columns\n    cols_old = df1.columns\n    snakecase = lambda x: inflection.underscore(x)\n    \n    cols_new = list( map( snakecase, cols_old ) )\n    #Rename Columns\n    df1.columns = cols_new\n    \n    # replacing NAs with 0\n    df1 = df1.fillna(0)\n\n    #converting feature 'date' to datetime\n    df1['date'] = pd.to_datetime( df1[ 'date' ] )\n    \n    return df1\n\ndef data_feature_engineering(df2):\n    # year\n    df2['year'] = df2['date'].dt.year\n\n    # month\n    df2['month'] = df2['date'].dt.month\n    \n    # day\n    df2['day'] = df2['date'].dt.day\n    \n    # week of year\n    df2['week_of_year'] = df2['date'].dt.isocalendar().week.astype('int64')\n    \n    # year week\n    df2['year_week'] = df2['date'].dt.strftime( '%Y-%W' )\n    \n    # year quarter\n    df2['quarter'] = df2['date'].dt.to_period('Q')\n    \n    return df2\n\ndef data_preparation(df5):\n    # Applying all transformations on validation dataset\n    #Validation dataset features transform - Robust Scaler\n    df5['temperature'] = temperature_scaler.transform( df5[['temperature']].values ) \n    df5['mark_down1']  = mark_down1_scaler.transform( df5[['mark_down1']].values ) \n    df5['mark_down2']  = mark_down2_scaler.transform( df5[['mark_down2']].values ) \n    df5['mark_down3']  = mark_down3_scaler.transform( df5[['mark_down3']].values ) \n    df5['mark_down4']  = mark_down4_scaler.transform( df5[['mark_down4']].values ) \n    df5['mark_down5']  = mark_down5_scaler.transform( df5[['mark_down5']].values ) \n    df5['unemployment']= unemployment_scaler.transform( df5[['unemployment']].values ) \n    \n    ##Validation dataset features transform - MinMaxScaler\n    df5['fuel_price'] = fuel_price_scaler.transform( df5[['fuel_price']].values )\n    df5['cpi']        = cpi_scaler.transform( df5[['cpi']].values )\n    df5['size']       = size_scaler.transform( df5[['size']].values )\n    df5['year']       = year_scaler.transform( df5[['year']].values )\n    \n    ##Validation dataset features transform - Label Enconder\n    df5['type'] = type_scaler.transform( df5['type'] )\n    \n    # is_holiday\n    df5['is_holiday'] = df5['is_holiday'].apply(lambda x: 1 if x == True else 0)\n    \n    ##Validation dataset features transform - Natural Transformations\n    # month\n    df5['month_sin'] = df5['month'].apply( lambda x: np.sin( x * ( 2. * np.pi/12 ) ) )\n    df5['month_cos'] = df5['month'].apply( lambda x: np.cos( x * ( 2. * np.pi/12 ) ) )\n    \n    # day \n    df5['day_sin'] = df5['day'].apply( lambda x: np.sin( x * ( 2. * np.pi/30 ) ) )\n    df5['day_cos'] = df5['day'].apply( lambda x: np.cos( x * ( 2. * np.pi/30 ) ) )\n    \n    # week_of_year\n    df5['week_of_year_sin'] = df5['week_of_year'].apply( lambda x: np.sin( x * ( 2. * np.pi/52 ) ) )\n    df5['week_of_year_cos'] = df5['week_of_year'].apply( lambda x: np.cos( x * ( 2. * np.pi/52 ) ) )\n    \n    cols_selected = ['store','dept','is_holiday','type','cpi','size','month_cos','week_of_year_cos']\n    return df5[cols_selected]\n\ndef get_prediction(model, original_data, test_data):\n    # prediction\n    pred = model.predict(test_data)\n    \n    # join pred into the original data so people can undestandt it the new table with prediction column\n    original_data['weekly_sales'] = np.expm1(pred)\n    \n    #creatomg id column\n    original_data['id'] = original_data['Store'].astype(str) + '_' +  original_data['Dept'].astype(str) + '_' +  original_data['Date'].astype(str)\n    original_data = original_data[['id', 'weekly_sales']].copy()\n    return original_data","metadata":{"ExecuteTime":{"end_time":"2022-05-30T11:00:10.823223Z","start_time":"2022-05-30T11:00:10.812089Z"},"code_folding":[1,9,26,47,86],"execution":{"iopub.status.busy":"2022-05-30T17:37:42.349107Z","iopub.execute_input":"2022-05-30T17:37:42.350966Z","iopub.status.idle":"2022-05-30T17:37:42.382913Z","shell.execute_reply.started":"2022-05-30T17:37:42.350883Z","shell.execute_reply":"2022-05-30T17:37:42.381829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Applying ETL into test dataset\ndf1_test = data_merge(df_test_raw, df_features, df_stores)\ndf2_test = data_cleaning(df1_test)\ndf3_test = data_feature_engineering(df2_test)\ndf4_test = data_preparation(df3_test)\ndf_submission = get_prediction(model, df_test_raw, df4_test)","metadata":{"ExecuteTime":{"end_time":"2022-05-30T11:00:14.619889Z","start_time":"2022-05-30T11:00:12.710573Z"},"scrolled":true,"execution":{"iopub.status.busy":"2022-05-30T16:51:04.325573Z","iopub.execute_input":"2022-05-30T16:51:04.325946Z","iopub.status.idle":"2022-05-30T16:51:09.503447Z","shell.execute_reply.started":"2022-05-30T16:51:04.325914Z","shell.execute_reply":"2022-05-30T16:51:09.502092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Final result sampĺe\ndf_submission.head()","metadata":{"ExecuteTime":{"end_time":"2022-05-30T11:05:10.59619Z","start_time":"2022-05-30T11:05:10.585654Z"},"scrolled":true,"execution":{"iopub.status.busy":"2022-05-30T17:38:25.697046Z","iopub.execute_input":"2022-05-30T17:38:25.697849Z","iopub.status.idle":"2022-05-30T17:38:25.711875Z","shell.execute_reply.started":"2022-05-30T17:38:25.697813Z","shell.execute_reply":"2022-05-30T17:38:25.710537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# final result to csvhttps://www.linkedin.com/in/marxcerqueira/\ndf_submission.to_csv('./submission.csv',index=False )","metadata":{"ExecuteTime":{"end_time":"2022-05-30T11:05:06.284992Z","start_time":"2022-05-30T11:05:05.919455Z"},"execution":{"iopub.status.busy":"2022-05-30T16:51:48.124149Z","iopub.execute_input":"2022-05-30T16:51:48.124817Z","iopub.status.idle":"2022-05-30T16:51:48.567154Z","shell.execute_reply.started":"2022-05-30T16:51:48.124778Z","shell.execute_reply":"2022-05-30T16:51:48.565568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 15 CONCLUSION","metadata":{}},{"cell_type":"markdown","source":"- In this project, all necessary steps to deploy a complete Data Science project to production were taken. Using one CRISP-DM project management methodology cycle, a satisfactory model performance was obtained by using the Random Forest algorithm to predict sales revenue for Walmart stores and its departments up to 22 weeks in advance, and useful business information was generated during the exploratory data analysis section. Due to this, the project met the criteria of finding a suitable solution for the company's stakeholders to access sales predictions on a csv taht could later be deployed.","metadata":{}},{"cell_type":"markdown","source":"## 16 NEXT STEPS","metadata":{}},{"cell_type":"markdown","source":"- In the next cycle of CRISP, we shoud create specific models for stores that were more difficult to make predictions;\n- Try to create new features for these stores.\n- Put more stress on machine learning models. \n- Improve the predictive model using the ensembling method to combine models and come with better model.\n- Create a anomaly detection for better understanding of outliers and sales during holidays and weeks before holidays\n- Time Series Analysis\n- Deploy the model so it can be access from a smartfone","metadata":{}}]}