{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Author: Dwi Hadyan Harsono\n* [Github source](https://github.com/dwihdyn/ds-exploration/blob/main/p2/retail-simple.ipynb) ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"## Most beginner-friendly notebook & straight-to-the-point (no fuss & no lengthy charts)\n- Obtain data\n- Scrub data : to make it all numerical & model-friendly\n- Explore data : correlation check with weekly_sales\n- Model : RandomForestClassifier\n- Interpret : predicting the future sales","metadata":{"execution":{"iopub.status.busy":"2021-08-05T03:47:05.347641Z","iopub.status.idle":"2021-08-05T03:47:05.348146Z"}}},{"cell_type":"markdown","source":"> Beginner friendly, as this helps you on getting the concept of data science full-cycle (once you understand, youre able to treat this as stepping-stone to improve the model accuracy)","metadata":{}},{"cell_type":"markdown","source":"# Obtain","metadata":{}},{"cell_type":"code","source":"# load necessary packages\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\") # ignoring annoying warnings\n\n# load data\ndf_features = pd.read_csv('../input/walmart-recruiting-store-sales-forecasting/features.csv.zip', parse_dates=['Date']) # parse_date to ensure Date in 'datetime64' format\ndf_sales = pd.read_csv('../input/walmart-recruiting-store-sales-forecasting/train.csv.zip', parse_dates=['Date'])\ndf_stores = pd.read_csv('../input/walmart-recruiting-store-sales-forecasting/stores.csv')\ndf_sales_answer = pd.read_csv('../input/walmart-recruiting-store-sales-forecasting/test.csv.zip', parse_dates=['Date'])\nsample_submission = pd.read_csv('../input/walmart-recruiting-store-sales-forecasting/sampleSubmission.csv.zip')\n","metadata":{"execution":{"iopub.status.busy":"2021-08-24T01:23:36.854054Z","iopub.execute_input":"2021-08-24T01:23:36.854485Z","iopub.status.idle":"2021-08-24T01:23:38.613761Z","shell.execute_reply.started":"2021-08-24T01:23:36.854398Z","shell.execute_reply":"2021-08-24T01:23:38.612596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Scrub\n- combine all datasets into one\n- then separate into train inclusive(2010-02-05 until 2012-10-26) and test (2012-11-02 until 2013-07-26)\n- convert all columns to numerical","metadata":{}},{"cell_type":"code","source":"# combine all 4 training dataset into one. from largest to smallest\n\n# merge two sales into one for now\nsales_answer = pd.merge(df_sales ,df_sales_answer, how='outer', on=['Store', 'Dept', 'Date', 'IsHoliday'])\n\n# merge features & stores on 'store' key\nsales_feat = pd.merge(sales_answer ,df_features, how='outer', on=['Store', 'Date', 'IsHoliday'])\n\n# merge sales_feat & sales on 'Store' and 'Date' key\ndf_all = pd.merge(sales_feat, df_stores, how='outer', on='Store')","metadata":{"execution":{"iopub.status.busy":"2021-08-24T01:23:38.615272Z","iopub.execute_input":"2021-08-24T01:23:38.615633Z","iopub.status.idle":"2021-08-24T01:23:39.234196Z","shell.execute_reply.started":"2021-08-24T01:23:38.615604Z","shell.execute_reply":"2021-08-24T01:23:39.233234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def multipledummies(df, non_numerical_columns):\n    ''' Input the whole dataframe & name of non-numerical columns, output is clean dataframe that all is in numerical format'''\n\n    for i in non_numerical_columns:\n\n        # convert to numerical using get_dummies\n        one_hot = pd.get_dummies(df[i], prefix=i)\n\n        # append new numerical column to main df\n        df = df.join(one_hot)\n\n        # drop that non-numerical column\n        df.drop(i, axis = 1, inplace=True)\n\n    return df","metadata":{"execution":{"iopub.status.busy":"2021-08-24T01:23:39.235848Z","iopub.execute_input":"2021-08-24T01:23:39.236144Z","iopub.status.idle":"2021-08-24T01:23:39.244041Z","shell.execute_reply.started":"2021-08-24T01:23:39.236116Z","shell.execute_reply":"2021-08-24T01:23:39.243154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# convert Date to 'Day, Week, Month' to make it numerical\ndf_all['Day'] = df_all.Date.dt.day\ndf_all['Week'] = df_all.Date.dt.week \ndf_all['Year'] = df_all.Date.dt.year\n","metadata":{"execution":{"iopub.status.busy":"2021-08-24T01:23:39.245468Z","iopub.execute_input":"2021-08-24T01:23:39.245812Z","iopub.status.idle":"2021-08-24T01:23:39.424953Z","shell.execute_reply.started":"2021-08-24T01:23:39.245773Z","shell.execute_reply":"2021-08-24T01:23:39.423778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# convert Type columns to numerical using multipledummies\ndf_all = multipledummies(df_all, ['Type'])\ndf_all.sample(3)\n","metadata":{"execution":{"iopub.status.busy":"2021-08-24T01:23:39.42639Z","iopub.execute_input":"2021-08-24T01:23:39.426715Z","iopub.status.idle":"2021-08-24T01:23:39.738415Z","shell.execute_reply.started":"2021-08-24T01:23:39.426684Z","shell.execute_reply":"2021-08-24T01:23:39.737384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# separate df into data inclusive(2010-02-05 until 2012-10-26) and answer (2012-11-02 until 2013-07-26)\n\ndata_range = (df_all['Date'] >= '2010-02-05') & (df_all['Date'] <= '2012-10-26')\nanswer_range = (df_all['Date'] >= '2012-11-02') & (df_all['Date'] <= '2013-07-26')\n\n\ndf = df_all.loc[data_range]\ndf_answer = df_all.loc[answer_range]\n\n\n# drop date column now since its been segregated properly already\ndf.drop(['Date'], axis=1, inplace=True)\ndf_answer.drop(['Date'], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-08-24T01:23:39.739618Z","iopub.execute_input":"2021-08-24T01:23:39.739917Z","iopub.status.idle":"2021-08-24T01:23:39.840155Z","shell.execute_reply.started":"2021-08-24T01:23:39.739889Z","shell.execute_reply":"2021-08-24T01:23:39.839389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ensure all columns in df is in integer format before done with \"scrub\" section\ndf.info()\ndf_answer.info()\n\n# for multiple null, we check in heatmap first, if weak correlation (between 0.1 & -0.1) with weekly_sales, we drop those column. else we use IterativeImputer() package\n\n# IsHoliday stays boolean object for now. converted in 'Model' section WMAE function","metadata":{"execution":{"iopub.status.busy":"2021-08-24T01:23:39.841495Z","iopub.execute_input":"2021-08-24T01:23:39.842138Z","iopub.status.idle":"2021-08-24T01:23:39.945777Z","shell.execute_reply.started":"2021-08-24T01:23:39.842091Z","shell.execute_reply":"2021-08-24T01:23:39.944527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Explore\n\n- heatmap. drop if corr with targetVariable is between 0.1 & -0.1 ","metadata":{}},{"cell_type":"code","source":"sns.set(style=\"white\")\n\ncorr = df.corr()\n\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\n\nf, ax = plt.subplots(figsize=(20, 15))\n\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\nplt.title('Correlation Matrix', fontsize=18)\n\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=1, vmin=-1, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True, fmt='.2f')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-24T01:23:39.94859Z","iopub.execute_input":"2021-08-24T01:23:39.949034Z","iopub.status.idle":"2021-08-24T01:23:41.628733Z","shell.execute_reply.started":"2021-08-24T01:23:39.948988Z","shell.execute_reply":"2021-08-24T01:23:41.627614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# drop CPI, Unemployment & all markdowns1-5, as it is : weak correlation to weekly_sales AND too much Null data\ndf.drop(['CPI', 'Unemployment', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5'], axis=1, inplace=True)\ndf_answer.drop(['CPI', 'Unemployment', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5'], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-08-24T01:23:41.630379Z","iopub.execute_input":"2021-08-24T01:23:41.630684Z","iopub.status.idle":"2021-08-24T01:23:41.658985Z","shell.execute_reply.started":"2021-08-24T01:23:41.630654Z","shell.execute_reply":"2021-08-24T01:23:41.658192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model\n- Since competition error measurement is WMAE (given in competition, and not the usual RMSE from GridSearchCV), we cant use GridSearchCV or RandomSearchCV to fine-tune our model\n- for the sake of siimplicity, we'll just jump right to model building.\n- once you grasp the concept, you may tweak & google around to improve this","metadata":{}},{"cell_type":"code","source":"# WMAE function as error measurement (lower the better)\n\ndef WMAE(dataset, real, predicted):\n    ''' Input df, real value , predicted value. Output the error value. lower the value, more accurate our model is '''\n\n    # weight allocation on IsHoliday\n    weights = dataset.IsHoliday.apply(lambda x : 5 if x else 1)\n\n    # WMSE formula\n    return np.round(np.sum(weights * abs(real - predicted)) / (np.sum(weights)), 2)","metadata":{"execution":{"iopub.status.busy":"2021-08-24T01:23:41.659995Z","iopub.execute_input":"2021-08-24T01:23:41.660416Z","iopub.status.idle":"2021-08-24T01:23:41.665162Z","shell.execute_reply.started":"2021-08-24T01:23:41.660376Z","shell.execute_reply":"2021-08-24T01:23:41.664433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# prep data\n\nX_train = df.drop(['Weekly_Sales'], axis = 1)\nY_train = df['Weekly_Sales']\n","metadata":{"execution":{"iopub.status.busy":"2021-08-24T01:23:41.666258Z","iopub.execute_input":"2021-08-24T01:23:41.666687Z","iopub.status.idle":"2021-08-24T01:23:41.696612Z","shell.execute_reply.started":"2021-08-24T01:23:41.666657Z","shell.execute_reply":"2021-08-24T01:23:41.6956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model building & training\nfrom sklearn.ensemble import RandomForestRegressor\n# from sklearn.model_selection import train_test_split\n\nRF = RandomForestRegressor(n_estimators=58, max_depth=27, max_features=12, min_samples_split=4, min_samples_leaf=1)\nRF.fit(X_train, Y_train)","metadata":{"execution":{"iopub.status.busy":"2021-08-24T01:23:41.697979Z","iopub.execute_input":"2021-08-24T01:23:41.698547Z","iopub.status.idle":"2021-08-24T01:26:01.222867Z","shell.execute_reply.started":"2021-08-24T01:23:41.69851Z","shell.execute_reply":"2021-08-24T01:26:01.221971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get prediction answer\n\nX_test = df_answer.drop(['Weekly_Sales'], axis = 1)\npredict = RF.predict(X_test)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-08-24T01:26:01.223926Z","iopub.execute_input":"2021-08-24T01:26:01.22436Z","iopub.status.idle":"2021-08-24T01:26:03.3802Z","shell.execute_reply.started":"2021-08-24T01:26:01.22433Z","shell.execute_reply":"2021-08-24T01:26:03.379412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# iNterpret\n\n- convert output to csv that to be submitted","metadata":{}},{"cell_type":"code","source":"sample_submission['Weekly_Sales'] = predict\nsample_submission","metadata":{"execution":{"iopub.status.busy":"2021-08-24T01:26:03.381179Z","iopub.execute_input":"2021-08-24T01:26:03.381487Z","iopub.status.idle":"2021-08-24T01:26:03.395197Z","shell.execute_reply.started":"2021-08-24T01:26:03.381459Z","shell.execute_reply":"2021-08-24T01:26:03.394406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# export file to csv\nsample_submission = sample_submission.set_index('Id')\nsample_submission.to_csv('walmart_v1.csv', sep=',')","metadata":{"execution":{"iopub.status.busy":"2021-08-24T01:26:03.396174Z","iopub.execute_input":"2021-08-24T01:26:03.396644Z","iopub.status.idle":"2021-08-24T01:26:03.78138Z","shell.execute_reply.started":"2021-08-24T01:26:03.396614Z","shell.execute_reply":"2021-08-24T01:26:03.780627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}