{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom category_encoders.binary import BinaryEncoder\n\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.ensemble import RandomForestRegressor\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.metrics import mean_absolute_error","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-30T15:08:02.476314Z","iopub.execute_input":"2022-05-30T15:08:02.476742Z","iopub.status.idle":"2022-05-30T15:08:02.482384Z","shell.execute_reply.started":"2022-05-30T15:08:02.47671Z","shell.execute_reply":"2022-05-30T15:08:02.481742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Introdução","metadata":{}},{"cell_type":"markdown","source":"Neste notebook, atacarei o desafio [Walmart Recruiting - Store Sales Forecasting](https://www.kaggle.com/competitions/walmart-recruiting-store-sales-forecasting/overview) do Kaggle como parte do processo seletivo para o Zé Delivery. Esse desafio consiste em realizar predições de quanto cada departamento de cada loja irá vender em cada semana, com base em vendas passadas e informações sobre as lojas e os locais em que elas estão.\n\nDurante a análise e modelagem, procurarei motivar cada passo dado, trazendo fontes e explicações sobre os conceitos utilizados sempre que possível. Gráficos serão usados para melhor visualização dos dados.\n\nVale apontar que as técnicas aqui utilizadas serão por vezes simplificadas para que o desafio possa ser terminado em um tempo razoável. Os pontos fracos dessas simplificações serão apontados e as ações que eu tomaria na \"situação ideal\" serão descritas na seção \"O que eu faria para fazer um modelo melhor?\" ao fim do notebook.\n\nAs predições nos dados de teste serão submetidas ao Kaggle, com seu score privado sendo revelado ao fim do notebook. As aleatoriedades serão eliminadas a partir da escolha de random states fixos de forma que o notebook possa ser 100% reproduzido.","metadata":{}},{"cell_type":"markdown","source":"**Obs: usarei os termos \"feature\", \"atributo\" e \"coluna\" sempre me referindo à mesma coisa: as diferentes classes de dados usadas na modelagem do problema. Acho útil colocar isso aqui para evitar confusão ao longo do texto**","metadata":{}},{"cell_type":"markdown","source":"# Carregamento e limpeza de dados","metadata":{}},{"cell_type":"markdown","source":"Nessa seção carergarei os dados e farei pequenas modificações e adequações para facilitar a codagem","metadata":{}},{"cell_type":"code","source":"# Carregando os arquivos providos pelo Kaggle\nfeatures_raw = pd.read_csv('../input/walmart-recruiting-store-sales-forecasting/features.csv.zip')\ntrain_raw = pd.read_csv('../input/walmart-recruiting-store-sales-forecasting/train.csv.zip')\ntest_raw = pd.read_csv('../input/walmart-recruiting-store-sales-forecasting/test.csv.zip')\nstores_raw = pd.read_csv('../input/walmart-recruiting-store-sales-forecasting/stores.csv')\nsample_submission = pd.read_csv('../input/walmart-recruiting-store-sales-forecasting/sampleSubmission.csv.zip')","metadata":{"execution":{"iopub.status.busy":"2022-05-30T15:08:02.543154Z","iopub.execute_input":"2022-05-30T15:08:02.543817Z","iopub.status.idle":"2022-05-30T15:08:02.980465Z","shell.execute_reply.started":"2022-05-30T15:08:02.543777Z","shell.execute_reply":"2022-05-30T15:08:02.979576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Aqui temos os dados \"crus\", ou seja, diretamente como foram fornecidos. Eles são separados em \"treino\", que serão os dados que serão analisados para gerar insights para a modelagem e que também servirão para treinar os modelos de machine learning, e \"teste\", que serão os dados para os quais queremos fazer predições.\n\nÉ importante que a análise e modelagem seja feito apenas no conjunto de treino, com o teste sendo exclusivamente para realizar a predição. Isso impede que tenhamos um resultado demasiadamente otimista e terminemos com um modelo sem real poder de predizer nada. \n\nUsar o teste para realizar a modelagem seria como fazer as provas já sabendo as respostas: iria bem na prova, mas seria incapaz de extrapolar para uma situação real que fuja daquelas respostas \"decoradas\". Essa metodologia de separação entre treino e teste é essencial para o sucesso dos modelos (embora existam outras para outras situações).","metadata":{}},{"cell_type":"markdown","source":"Com os dados crus carregados, precisamos juntar as informações contidas nas diferentes tabelas fornecidas","metadata":{}},{"cell_type":"code","source":"# Fazendo o merge dos dados\nsf = stores_raw.merge(features_raw, on='Store')\ntrain = train_raw.merge(sf, on=['Store', 'Date', 'IsHoliday'])\ntest = test_raw.merge(sf, on=['Store', 'Date', 'IsHoliday']).sort_values(by = ['Store','Dept','Date']).reset_index(drop=True) # Para submeter o arquivo final, temos que ter os dados de teste na mesma ordem que na tabela sample_submission","metadata":{"execution":{"iopub.status.busy":"2022-05-30T15:08:02.982318Z","iopub.execute_input":"2022-05-30T15:08:02.982646Z","iopub.status.idle":"2022-05-30T15:08:03.235087Z","shell.execute_reply.started":"2022-05-30T15:08:02.982619Z","shell.execute_reply":"2022-05-30T15:08:03.234092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-30T15:08:03.236275Z","iopub.execute_input":"2022-05-30T15:08:03.236781Z","iopub.status.idle":"2022-05-30T15:08:03.256222Z","shell.execute_reply.started":"2022-05-30T15:08:03.236748Z","shell.execute_reply":"2022-05-30T15:08:03.25498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Prefiro escrever as colunas todas em letras minúsculas para escrever os códigos com mais agilidade e portanto farei essa mudança.","metadata":{}},{"cell_type":"code","source":"# Deixar o nome das colunas em letras minúsculas\ntrain.columns = train.columns.str.lower()\ntest.columns = test.columns.str.lower()","metadata":{"execution":{"iopub.status.busy":"2022-05-30T15:08:03.25843Z","iopub.execute_input":"2022-05-30T15:08:03.258842Z","iopub.status.idle":"2022-05-30T15:08:03.267545Z","shell.execute_reply.started":"2022-05-30T15:08:03.258812Z","shell.execute_reply":"2022-05-30T15:08:03.266774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.dtypes","metadata":{"execution":{"iopub.status.busy":"2022-05-30T15:08:03.268619Z","iopub.execute_input":"2022-05-30T15:08:03.26892Z","iopub.status.idle":"2022-05-30T15:08:03.281501Z","shell.execute_reply.started":"2022-05-30T15:08:03.268884Z","shell.execute_reply":"2022-05-30T15:08:03.280899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Vemos acima que os dados da coluna 'Date' não estão gravados no tipo de dado mais adequado (datetime). Portanto, faremos essa transformação\n","metadata":{}},{"cell_type":"code","source":"# Transformar as datas no formato datetime do pandas\ntrain.date = pd.to_datetime(train.date)\ntest.date = pd.to_datetime(test.date)","metadata":{"execution":{"iopub.status.busy":"2022-05-30T15:08:03.282878Z","iopub.execute_input":"2022-05-30T15:08:03.283246Z","iopub.status.idle":"2022-05-30T15:08:03.390209Z","shell.execute_reply.started":"2022-05-30T15:08:03.283209Z","shell.execute_reply":"2022-05-30T15:08:03.389294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Análise exploratória dos dados (EDA)","metadata":{}},{"cell_type":"markdown","source":"Como fica explícito no título, nesta seção exploraremos os dados de forma a gerar insights que possam ajudar na modelagem do problema. A partir dela podemos decidir que atributos serão de fatos importantes, ter ideias de como criar novas features, checar se os dados fornecidos fazem sentido, etc.","metadata":{}},{"cell_type":"markdown","source":"Como está explicitado na descrição do desafio, trabalharemos com as seguintes colunas:\n- Store: identificação de cada uma das diferentes lojas\n- Dept: identificação de cada departamento\n- Date: data\n- Weekly_Sales: vendas semanas. Essa é a nossa coluna alvo, ou seja, a que iremos quere prever\n- IsHoliday: marcador que diz se naquela semana houve um feriado importante (SuperBowl, Labor Day, Thanksgiving e Natal)\n- Type: não foi identificado no desafio a que essa coluna se refere. Isso será abordado a frente\n- Size: tamanho da loja\n- Temperature: temperatura média do local onde a loja está\n- Fuel_Price: preço médio do combustível na localização da loja\n- MarkDown (1 a 5): colunas com dados anonimizados, ou seja, cuja origem não foi explicitada\n- CPI: índice de preço do consumidor médio da localização da loja\n- Unemployment: índice de desemprego médio da região","metadata":{}},{"cell_type":"markdown","source":"O primeiro passo será adicionar uma coluna com a granularidade adequada para as datas. Usaremos semanas, já que os dados são organizados dessa maneira, principalmente nossa variável alvo (weekly sales). Irei fazer uma coluna com o ano também para diferenciar entre semanas de anos diferentes para uma mesma loja.","metadata":{}},{"cell_type":"code","source":"# Granularidade de data\ntrain['year'] = train.date.dt.year\ntrain['week'] = train.date.dt.week \n\ntest['year'] = test.date.dt.year\ntest['week'] = test.date.dt.week ","metadata":{"execution":{"iopub.status.busy":"2022-05-30T15:08:03.391427Z","iopub.execute_input":"2022-05-30T15:08:03.39174Z","iopub.status.idle":"2022-05-30T15:08:03.642789Z","shell.execute_reply.started":"2022-05-30T15:08:03.391712Z","shell.execute_reply":"2022-05-30T15:08:03.642005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Alguns números","metadata":{}},{"cell_type":"markdown","source":"Veremos algumas estatísticas básicas dos dados disponíveis, como média, valores máximos/mínimos, etc. Isso servirá como uma checagem de se os dados fazem sentido de  fato e frequentemente geram insights importantes sobre como realizar a modelagem.","metadata":{}},{"cell_type":"code","source":"train.describe()","metadata":{"execution":{"iopub.status.busy":"2022-05-30T15:08:03.643768Z","iopub.execute_input":"2022-05-30T15:08:03.644323Z","iopub.status.idle":"2022-05-30T15:08:03.911142Z","shell.execute_reply.started":"2022-05-30T15:08:03.644294Z","shell.execute_reply":"2022-05-30T15:08:03.91034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Os valores parecem fazer sentido, já que podemos inferir com certa segurança as unidades usadas para representar as quantidades nas colunas:\n- Temperature: em fahrenheits, indo de -2 (-18°C) a 100(38°C)\n- Fuel price: litros/galão, com 3.36 de média, o que condiz com [fontes](https://www.eia.gov/dnav/pet/hist/LeafHandler.ashx?n=pet&s=emm_epm0_pte_nus_dpg&f=m)\n- CPI: embora esteja um pouco maior que a [média nos EUA para aquele período](https://fred.stlouisfed.org/series/CPIAUCSL), tem valores razoáveis (e temos que lembrar que se referem apenas às áreas onde estão as lojas)\n- Unemployment: porcentagens razoáveis considerando a [média nacional de 9% para aquele período](https://tradingeconomics.com/united-states/unemployment-rate#:~:text=Unemployment%20Rate%20in%20the%20United,percent%20in%20May%20of%201953.)","metadata":{}},{"cell_type":"markdown","source":"### Vendas ao longo do ano","metadata":{}},{"cell_type":"markdown","source":"Vamos plotar os valores médios de vendas semanais para cada ano para ver se verificamos alguma sazonalidade que nos dê algum insight. Isso é especialmente importante devido à grande ênfase dada aos feriados no enunciado deste desafio.","metadata":{}},{"cell_type":"code","source":"# Marcadores para super bowl, labor day, thanksgiving e christmas\nholiday_weeks = train[train.isholiday==True].week.unique()\nholiday_names = ['super bowl', 'labor day', 'thanksgiving', 'christmas']\nholiday_colors = ['c', 'y', 'm', 'r']\nprint(holiday_weeks)","metadata":{"execution":{"iopub.status.busy":"2022-05-30T15:08:03.912241Z","iopub.execute_input":"2022-05-30T15:08:03.912639Z","iopub.status.idle":"2022-05-30T15:08:03.925059Z","shell.execute_reply.started":"2022-05-30T15:08:03.91261Z","shell.execute_reply":"2022-05-30T15:08:03.924029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotar vendas semanais médias para cada ano\n\nyears = [2010, 2011, 2012]\nplt.figure(figsize=(25, 10))\n\n# Plots por ano\nfor year in years:\n    df_grouped = train[train.year == year].groupby('week')['weekly_sales'].mean()\n    sns.lineplot(df_grouped.index, df_grouped.values)\n\n# Marcadores de feriado\nfor week, name, color in zip(holiday_weeks, holiday_names, holiday_colors):\n    plt.axvline(week, color=color, ls='--', label=name)\n    plt.legend()\n\nplt.grid(axis='y')\nplt.legend(['2010', '2011', '2012'] + holiday_names)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-30T15:08:03.92779Z","iopub.execute_input":"2022-05-30T15:08:03.928112Z","iopub.status.idle":"2022-05-30T15:08:04.342222Z","shell.execute_reply.started":"2022-05-30T15:08:03.928085Z","shell.execute_reply":"2022-05-30T15:08:04.341297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Aqui fica claro que, em termos de vendas, Thanksgiving e Natal são os feriados mais importantes, de longe. Vale observar também que as vendas do Natal são feitas na semana anterior. Sendo assim, na seção de engenharia de atributos (feature engineering) criarei 4 colunas novas que poderão auxiliar na modelagem:\n\n- Marcador da semana de Thanksgiving (binário)\n- Marcador da semana pré-Natal (binário)\n- Quantidade de dias até o Thanksgiving\n- Quantidade de dias até o Natal","metadata":{}},{"cell_type":"markdown","source":"# Análise das features","metadata":{}},{"cell_type":"markdown","source":"Em contraste com a seção anterior que deu conta de aspectos mais gerais dos dados, aqui abordaremos os diferentes atributos dos dados separadamente, de forma a gerar insights sobre a utilização deles no treino dos modelos de machine learning.","metadata":{}},{"cell_type":"markdown","source":"### Stores e departments","metadata":{}},{"cell_type":"markdown","source":"Aqui analizaremos o quanto cada loja/departamento são diferentes uns dos outros, de forma a decidir se vale a pena incluir estes atributos na modelagem das vendas. Esta decisão é importante, já que a inclusão de dados categóricos (e não simplesmente numéricos, como \"temperatura\", por exemplo) como estes trazem uma complexidade maior à modelagem, como veremos a frente.","metadata":{}},{"cell_type":"code","source":"# Número de lojas diferentes\ntrain.store.unique().size","metadata":{"execution":{"iopub.status.busy":"2022-05-30T15:08:04.343373Z","iopub.execute_input":"2022-05-30T15:08:04.344111Z","iopub.status.idle":"2022-05-30T15:08:04.35196Z","shell.execute_reply.started":"2022-05-30T15:08:04.344073Z","shell.execute_reply":"2022-05-30T15:08:04.351333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Distribuição de dados entre as lojas\ntrain.store.value_counts(normalize=True).head()","metadata":{"execution":{"iopub.status.busy":"2022-05-30T15:08:04.353402Z","iopub.execute_input":"2022-05-30T15:08:04.353965Z","iopub.status.idle":"2022-05-30T15:08:04.365926Z","shell.execute_reply.started":"2022-05-30T15:08:04.353934Z","shell.execute_reply":"2022-05-30T15:08:04.365196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Número de departamentos diferentes\ntrain.dept.unique().size","metadata":{"execution":{"iopub.status.busy":"2022-05-30T15:08:04.366893Z","iopub.execute_input":"2022-05-30T15:08:04.367309Z","iopub.status.idle":"2022-05-30T15:08:04.383068Z","shell.execute_reply.started":"2022-05-30T15:08:04.36728Z","shell.execute_reply":"2022-05-30T15:08:04.38222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Distribuição de dados entre os departamentos\ntrain.dept.value_counts(normalize=True).head()","metadata":{"execution":{"iopub.status.busy":"2022-05-30T15:08:04.384101Z","iopub.execute_input":"2022-05-30T15:08:04.384418Z","iopub.status.idle":"2022-05-30T15:08:04.398073Z","shell.execute_reply.started":"2022-05-30T15:08:04.384389Z","shell.execute_reply":"2022-05-30T15:08:04.397158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Existe um número considerável de lojas (45) e departamentos (81), com os dados sendo bem distribuídos dentre eles. É importante que tenhamos números relevantes de dados de cada tipo, de modo que nenhum fique subrepresentado e gere dstorções na capacidade de predição do modelo.","metadata":{}},{"cell_type":"code","source":"# Plotar média de vendas por loja\n\nstore_sales = train.groupby('store')['weekly_sales'].mean()\nplt.figure(figsize=(25, 15))\nsns.barplot(store_sales.index, store_sales.values)\nplt.grid(axis='y')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-30T15:08:04.399392Z","iopub.execute_input":"2022-05-30T15:08:04.400188Z","iopub.status.idle":"2022-05-30T15:08:05.105307Z","shell.execute_reply.started":"2022-05-30T15:08:04.400146Z","shell.execute_reply":"2022-05-30T15:08:05.104656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Insight: aparentemente há uma grande disparidade de vendas entre as lojas, o que significa que suas identificações provavelmente serão de grande valor para o modelo de predição de vendas.","metadata":{}},{"cell_type":"code","source":"# Plotar média de vendas por departamento\n\nstore_dpt = train.groupby('dept')['weekly_sales'].mean()\nplt.figure(figsize=(25, 15))\nsns.barplot(store_dpt.index, store_dpt.values)\nplt.grid(axis='y')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-30T15:08:05.106513Z","iopub.execute_input":"2022-05-30T15:08:05.107352Z","iopub.status.idle":"2022-05-30T15:08:06.167053Z","shell.execute_reply.started":"2022-05-30T15:08:05.107302Z","shell.execute_reply":"2022-05-30T15:08:06.166146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"O mesmo vale para os diferentes departamentos.\n\nAlém disso, vemos que temos departamentos com valores de venda semanais BEM baixos, o que podem acabar sendo um estorvo para o aprendizado do modelo por serem tão diferentes dos demais. Embora não vá ser feito neste trabalho, verificarei a quantidade de dados deste tipo para explorar a possibilidade de criação de um modelo espcífico para estes dados.","metadata":{}},{"cell_type":"code","source":"# Departmentoss com vendas BEM baixas\ntrain[train.dept.isin([28, 39, 43, 45, 47, 51, 54, 60, 77, 78])].size / train.size","metadata":{"execution":{"iopub.status.busy":"2022-05-30T15:08:06.168186Z","iopub.execute_input":"2022-05-30T15:08:06.168596Z","iopub.status.idle":"2022-05-30T15:08:06.189049Z","shell.execute_reply.started":"2022-05-30T15:08:06.168566Z","shell.execute_reply":"2022-05-30T15:08:06.188229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Temos uma quantidade razoável de dados deste tipo (5%), o que sinaliza uma possibilidade de tratamento em separado destes dados.","metadata":{}},{"cell_type":"markdown","source":"### O que é 'type'?","metadata":{}},{"cell_type":"markdown","source":"Tirando as colunas anonimizadas (\"markdowns\"), a coluna 'type' é a única que não traz qualquer explicação sobre sua natureza. Aqui exploraremos ela para ver se podemos usá-la de alguma forma.","metadata":{}},{"cell_type":"code","source":"# Representantes de cada type\ntrain.type.value_counts(normalize=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-30T15:08:06.190064Z","iopub.execute_input":"2022-05-30T15:08:06.190345Z","iopub.status.idle":"2022-05-30T15:08:06.245172Z","shell.execute_reply.started":"2022-05-30T15:08:06.190321Z","shell.execute_reply":"2022-05-30T15:08:06.244594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Média semanal de vendas de cada tipo\ntrain.groupby('type')['weekly_sales'].mean()","metadata":{"execution":{"iopub.status.busy":"2022-05-30T15:08:06.246342Z","iopub.execute_input":"2022-05-30T15:08:06.246815Z","iopub.status.idle":"2022-05-30T15:08:06.295565Z","shell.execute_reply.started":"2022-05-30T15:08:06.246785Z","shell.execute_reply":"2022-05-30T15:08:06.294575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Mediana semanal de vendas de cada tipo\ntrain.groupby('type')['weekly_sales'].median()","metadata":{"execution":{"iopub.status.busy":"2022-05-30T15:08:06.296969Z","iopub.execute_input":"2022-05-30T15:08:06.297496Z","iopub.status.idle":"2022-05-30T15:08:06.347225Z","shell.execute_reply.started":"2022-05-30T15:08:06.297451Z","shell.execute_reply":"2022-05-30T15:08:06.34628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As lojas com 'type' A não apenas tem mais representantes como também tem maior média/mediana de vendas semanais. O mesmo ocorre para B em relação a C.\n\nComo A > B > C consistentemente, modelaremos esse comportamento atribuindo valores a cada um dos tipos que obedeçam a essa relação: A = 3, B = 2, C = 1","metadata":{}},{"cell_type":"code","source":"# Mapeando A, B e C em 1, 2 e 3, respectivamente\ntrain['type'] = train.type.map({'A': 3, 'B': 2, 'C': 1})\ntest['type'] = test.type.map({'A': 3, 'B': 2, 'C': 1})","metadata":{"execution":{"iopub.status.busy":"2022-05-30T15:08:06.348147Z","iopub.execute_input":"2022-05-30T15:08:06.348463Z","iopub.status.idle":"2022-05-30T15:08:06.392926Z","shell.execute_reply.started":"2022-05-30T15:08:06.348425Z","shell.execute_reply":"2022-05-30T15:08:06.392026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Acabamos de tomar nossa primeira decisão de modelagem dos dados a partir de insights gerados por análises! Um ponto importante de perceber é que, embora os insights sejam gerados apenas a partir do conjunto de treino, as mesmas modificações são realizadas também no conjunto de teste. Isso porque o teste deverá ter a mesma forma que o treino de forma que o modelo consiga extrapolar o que aprendeu no algoritmo de machine learning.","metadata":{}},{"cell_type":"markdown","source":"### Correlações","metadata":{}},{"cell_type":"markdown","source":"Correlações são relações estatísticas entre diferentes atributos que nos ajudam a entender como elas funcionam entre si. Em poucas palavras, atribuiremos um valor entre -1 e 1 a cada par de atributos que nos responderão a seguinte pergunta:\n\n**Se pegamos dados com valores cada vez maiores para o atributo 1, o que devemos esperar para os respectivos valores do atributo 2?**\n\nValores negativos de correlação indicam que devemos esperar que o atributo 2 tenha valores cada vez menores. O oposto ocorre para números positivos, com suas normas indicando o quão forte é essa relação de crescimento/diminuição. \n\nValores próximos a 0 indicam que os atributos tendem a ser independentes, ou seja, que o comportamento de um atributo não deverá trazer nenhuma informação por si só sobre o comportamento de outro atributo. Eles não estarão correlacionados.\n\nNormalmente a correlação é uma forma simples de se ter um feeling sobre que atributos serão mais ou menos importantes para realizarmos nossas predições. Normas altas de correlação com o atributo alvo (no nosso caso, as vendas semanais) podem indicar que temos aí uma coluna valiosa para nossa modelagem. Por outro lado, correlações altas entre atributos que não sejam o alvo podem sugerir que eles serão redundantes, já que possuem comportamento parecido.\n\nEmbora eu vá usar os resultados da matriz de correlação para tomar algumas decisões, vale apontar que a interação dos atributos dentro dos modelos de machine learning são em geral muito mais complexos do que as captadas por esta simples relação estatística. No fim, só saberemos se um atributo será útil ou não ao modelo através de testes.","metadata":{}},{"cell_type":"code","source":"# Fonte: https://seaborn.pydata.org/examples/many_pairwise_correlations.html\n\nsns.set_theme(style=\"white\")\n\n# Compute the correlation matrix\ncorr = train.drop(columns=['store', 'dept', 'isholiday']).corr() # exclude categorical features from Pearson corr\n\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(22, 18))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, \n            annot=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-30T15:08:06.394651Z","iopub.execute_input":"2022-05-30T15:08:06.39533Z","iopub.status.idle":"2022-05-30T15:08:07.426621Z","shell.execute_reply.started":"2022-05-30T15:08:06.39529Z","shell.execute_reply":"2022-05-30T15:08:07.425698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Correlações altas: \n- markdowns 1 e 4: markdown4 será descartada, já que tem baixíssima correlação com a feature alvo (weekly_sales). \n- fuel_price e year: fuel_price tem uma correlação extremamente baixa com a feature alvo. Além disso, a coluna 'year' ajuda na diferenciação de semanas de mesmo valor. Portanto, fuel_price será descartada.\n\nCorrelações baixas:\n- temperature: tem correlação baixíssima com a feature alvo e será descartada.\n- unemployment e cpi: também têm correlações relativamente baixas mas, por ora, permanecerão, já que intuitivamente parecem tem uma relação maior com vendas do que temperatura, por exemplo.\n\nMarkdowns: \nmarkdowns 1 e 5 têm as maiores correlações com o atributo alvo e suas permanências serão consideradas. Essa decisão dependerá do quão grande será o número de valores nulos nestas colunas, já que fomos advertidos no anunciado do desafio que essas informações estão incompletas.","metadata":{}},{"cell_type":"markdown","source":"**Importante**\n\nComo dito antes, análise de correlação não dão conta da complexidade envolvida nos modelos de machine learning, sendo apenas um primeiro approach. Descartei colunas aqui como uma primeira aposta principalmente para ilustar como um processo de modelagem pode ser guiado por essa ferramenta, mas idealmente voltaria a revisitá-las caso houvesse necessidade de melhoria do modelo.","metadata":{}},{"cell_type":"code","source":"# Tirando as colunas mencionadas acima\ntrain = train.drop(columns=['markdown2', 'markdown3', 'markdown4', 'fuel_price', 'temperature'])\ntest = test.drop(columns=['markdown2', 'markdown3', 'markdown4', 'fuel_price', 'temperature'])","metadata":{"execution":{"iopub.status.busy":"2022-05-30T15:08:07.427728Z","iopub.execute_input":"2022-05-30T15:08:07.428022Z","iopub.status.idle":"2022-05-30T15:08:07.455502Z","shell.execute_reply.started":"2022-05-30T15:08:07.427996Z","shell.execute_reply":"2022-05-30T15:08:07.45453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Agora vamos verificar qual a porcentagem de dados nulos temos nas colunas markdown","metadata":{}},{"cell_type":"code","source":"na_1_size = train[train.markdown1.isna()].size\nna_5_size = train[train.markdown5.isna()].size\ntrain_size = train.size\n\nprint('markdown1: \\n')\nprint(na_1_size / train_size, '% de valores nulos\\n')\n\nprint('markdown5: \\n')\nprint(na_5_size / train_size, '% de valores nulos')","metadata":{"execution":{"iopub.status.busy":"2022-05-30T15:08:07.456819Z","iopub.execute_input":"2022-05-30T15:08:07.457172Z","iopub.status.idle":"2022-05-30T15:08:07.495738Z","shell.execute_reply.started":"2022-05-30T15:08:07.457142Z","shell.execute_reply":"2022-05-30T15:08:07.494821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Nós temos mais de 60% de perda de dados nessas colunas! Por agora irei descartá-las, mas veremos mais a frente que temos maneiras de lidar com a existência de valores nulos em nossos dados.","metadata":{}},{"cell_type":"code","source":"# Retirando o resto das colunas markdown\ntrain = train.drop(columns=['markdown1', 'markdown5'])\ntest = test.drop(columns=['markdown1', 'markdown5'])","metadata":{"execution":{"iopub.status.busy":"2022-05-30T15:08:07.496899Z","iopub.execute_input":"2022-05-30T15:08:07.497288Z","iopub.status.idle":"2022-05-30T15:08:07.515757Z","shell.execute_reply.started":"2022-05-30T15:08:07.497258Z","shell.execute_reply":"2022-05-30T15:08:07.514783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature engineering","metadata":{}},{"cell_type":"markdown","source":"Na seção 'Stores e Departments' decidimos criar novas features novas. Isso será feito agora","metadata":{}},{"cell_type":"code","source":"# Marcadores das semanas the thanksgiving e pré-natal\ntrain['is_thanksgiving'] = (train.week == 47).astype(int)\ntrain['is_pre_xmas'] = (train.week == (52 - 1)).astype(int)\n\ntest['is_thanksgiving'] = (test.week == 47).astype(int)\ntest['is_pre_xmas'] = (test.week == (52 - 1)).astype(int)","metadata":{"execution":{"iopub.status.busy":"2022-05-30T15:08:07.516811Z","iopub.execute_input":"2022-05-30T15:08:07.51711Z","iopub.status.idle":"2022-05-30T15:08:07.529273Z","shell.execute_reply.started":"2022-05-30T15:08:07.517076Z","shell.execute_reply":"2022-05-30T15:08:07.528583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dias para Thanksgiving\ntrain['days_to_thanksgiving'] = (pd.to_datetime(train.year.astype(str) + '-11-24') - pd.to_datetime(train.date)).dt.days\ntest['days_to_thanksgiving'] = (pd.to_datetime(test.year.astype(str) + '-11-24') - pd.to_datetime(test.date)).dt.days\n\n# Dias para natal\ntrain['days_to_xmas'] = (pd.to_datetime(train.year.astype(str) + '-12-24') - pd.to_datetime(train.date)).dt.days\ntest['days_to_xmas'] = (pd.to_datetime(test.year.astype(str) + '-12-24') - pd.to_datetime(test.date)).dt.days","metadata":{"execution":{"iopub.status.busy":"2022-05-30T15:08:07.530303Z","iopub.execute_input":"2022-05-30T15:08:07.530738Z","iopub.status.idle":"2022-05-30T15:08:09.168288Z","shell.execute_reply.started":"2022-05-30T15:08:07.530696Z","shell.execute_reply":"2022-05-30T15:08:09.167344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Não usarei mais os dados de data cheios, portanto descartarei\ntrain = train.drop(columns='date')\ntest = test.drop(columns='date')","metadata":{"execution":{"iopub.status.busy":"2022-05-30T15:08:09.172848Z","iopub.execute_input":"2022-05-30T15:08:09.173199Z","iopub.status.idle":"2022-05-30T15:08:09.21381Z","shell.execute_reply.started":"2022-05-30T15:08:09.173167Z","shell.execute_reply":"2022-05-30T15:08:09.212738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pré-processamento","metadata":{}},{"cell_type":"markdown","source":"A parte de pré-processamento é extremamente importante para que possamos apresentar os dados ao algoritmo de machine learning numa linguagem que ele entenda. Lidaremos com dois tipos de pré-processamento aqui:\n\n- **Preenchimento de valores nulos:** Valores nulos não são aceitos pela grande maioria de algoritmos de machine learning. Sendo assim, identificaremos onde existem valores nulos para que possamos substituí-los por valores númericos. \n\n- **Codificação de variáveis categóricas:** Enquanto colunas como 'week', 'unemployment' e outras têm valores numéricos facilmente interpretados pelo algoritmo de machine learning, as colunas 'store' e 'dept' são nada mais do que códigos de identificação, cujo valor numérico não carrega nenhum valor intrínseco. Afinal, a loja '12' não é maior que a loja '10' ou menor que a loja '15' em nenhum sentido, por exemplo. Desta forma, precisamos transformar essa identificação de lojas e departamentos em uma forma 'legível' para o modelo, o que será feito e explicado em breve\n\n\n\n","metadata":{}},{"cell_type":"markdown","source":"### Preenchimento de valores nulos","metadata":{}},{"cell_type":"code","source":"# Quais colunas do treino têm valores nulos?\ntrain.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2022-05-30T15:08:09.215154Z","iopub.execute_input":"2022-05-30T15:08:09.215482Z","iopub.status.idle":"2022-05-30T15:08:09.231851Z","shell.execute_reply.started":"2022-05-30T15:08:09.215454Z","shell.execute_reply":"2022-05-30T15:08:09.23091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Quais colunas do teste têm valores nulos?\ntest.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2022-05-30T15:08:09.233164Z","iopub.execute_input":"2022-05-30T15:08:09.233434Z","iopub.status.idle":"2022-05-30T15:08:09.242084Z","shell.execute_reply.started":"2022-05-30T15:08:09.23341Z","shell.execute_reply":"2022-05-30T15:08:09.24128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A opção feita aqui é a de substituir os nulos pela média dos valores da respectiva coluna no conjunto de treino, de forma que representem o caso \"mais comum\" o possível e tenham pouco impacto nas decisões do modelo. Em outros tipos de modelo podemos fazer a substitução por \"0\", seguindo o mesmo princípio.","metadata":{}},{"cell_type":"code","source":"# Preencher nulos com a média\ntest['cpi'] = test.cpi.fillna(test.cpi.mean())\ntest['unemployment'] = test.unemployment.fillna(test.unemployment.mean())","metadata":{"execution":{"iopub.status.busy":"2022-05-30T15:08:09.24348Z","iopub.execute_input":"2022-05-30T15:08:09.244267Z","iopub.status.idle":"2022-05-30T15:08:09.252197Z","shell.execute_reply.started":"2022-05-30T15:08:09.244237Z","shell.execute_reply":"2022-05-30T15:08:09.251572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Encoding","metadata":{}},{"cell_type":"markdown","source":"Como foi dito, precisamos lidar com as features categóricas de nosso dataset.\n\nA primeira é mais simples: a coluna 'isholiday' que está como booleana (True/False) pode ser facilmente transformada em binária, que é um formato que o algoritmo de machine learning \"entende\", já que representa simplesmente presença ou ausência. \n\n","metadata":{}},{"cell_type":"code","source":"# Transforming 'isholiday' into integers (0 or 1)\ntrain['isholiday'] = train.isholiday.astype(int)\ntest['isholiday'] = test.isholiday.astype(int)","metadata":{"execution":{"iopub.status.busy":"2022-05-30T15:08:09.253191Z","iopub.execute_input":"2022-05-30T15:08:09.253702Z","iopub.status.idle":"2022-05-30T15:08:09.263329Z","shell.execute_reply.started":"2022-05-30T15:08:09.253671Z","shell.execute_reply":"2022-05-30T15:08:09.262624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As colunas 'store' e 'dept' são mais complicadas, com a primeira tendo 45 classes diferentes e a segunda 81. \n\nUma forma de codificar isso seria usar o método conhecido como [One-Hot-Encoding](https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/), que cria uma coluna binária para cada uma das classes existentes, bastando atribuir o valor 1 para a coluna referente ao valor nominal da classe. Esse seria um método válido, mas tem a desvantagem de criar muitas colunas adicionais de dados, o que traz seus próprio problemas como a famigerada [\"maldição da dimensionalidade\"](https://towardsdatascience.com/the-curse-of-dimensionality-50dc6e49aa1e), que aumenta muito a possibilidade de overfitting nos dados de treino.\n\nUma segunda técnica foi escolhida: mantendo a ideia de transformar dados categóricos em binários, codificamos cada uma das possíveis classes em seu valor binário. Por exemplo, a classe 9 seria codificada como 1001 e assim em diante. Essa metodologia tem a vantagem de só precisar criar o número de colunas referentes à necessidade de representação binária do seu número total de classes: para representar 45 classes precisamos de 6 colunas (45 < 2^6) e para 81 classes precisamos de 7 (81 < 2^7). Esse método é chamado de [Binary Encoding](https://contrib.scikit-learn.org/category_encoders/binary.html).","metadata":{}},{"cell_type":"code","source":"# Separando a feature alvo para treinar o encoder apropriadamente\nX = train.drop(columns=['weekly_sales'])\ny_exp = train.weekly_sales","metadata":{"execution":{"iopub.status.busy":"2022-05-30T15:08:09.26437Z","iopub.execute_input":"2022-05-30T15:08:09.264817Z","iopub.status.idle":"2022-05-30T15:08:09.318014Z","shell.execute_reply.started":"2022-05-30T15:08:09.264776Z","shell.execute_reply":"2022-05-30T15:08:09.317289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Defino o encoder\nbin_enc = BinaryEncoder(cols = ['store', 'dept'])\n\n# Treino e transformação dos dados\nX_exp = bin_enc.fit_transform(X)\nX_test = bin_enc.transform(test)","metadata":{"execution":{"iopub.status.busy":"2022-05-30T15:08:09.319326Z","iopub.execute_input":"2022-05-30T15:08:09.319853Z","iopub.status.idle":"2022-05-30T15:08:11.495863Z","shell.execute_reply.started":"2022-05-30T15:08:09.319824Z","shell.execute_reply":"2022-05-30T15:08:11.49509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_exp.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-30T15:08:11.497291Z","iopub.execute_input":"2022-05-30T15:08:11.497627Z","iopub.status.idle":"2022-05-30T15:08:11.5209Z","shell.execute_reply.started":"2022-05-30T15:08:11.497597Z","shell.execute_reply":"2022-05-30T15:08:11.520022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_exp.columns","metadata":{"execution":{"iopub.status.busy":"2022-05-30T15:08:11.522163Z","iopub.execute_input":"2022-05-30T15:08:11.522802Z","iopub.status.idle":"2022-05-30T15:08:11.529843Z","shell.execute_reply.started":"2022-05-30T15:08:11.522756Z","shell.execute_reply":"2022-05-30T15:08:11.528989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Criando os modelos","metadata":{}},{"cell_type":"markdown","source":"Nesta seção criaremos finalmente os modelos. Primeiramente faremos uma rápida escolha do algorítmo usado, para depois realizarmos uma busca de parâmetros simplificada e finalmente o treino do modelo final e a avaliação da importância das features em suas predições.","metadata":{}},{"cell_type":"markdown","source":"Toda a parte de experimentação desta seção será feita utilizando um conjunto de validação fixo, este criado a partir da seleção aleatória de 15% dos dados de treino. Esses dados não serão usados para o treino do modelo na parte de experimentação, mas serão incluídos no treinamento do modelo final. Vale apontar que uma forma melhor de realizar essa experimentação seria através do uso de Cross Validation, que diminui as chances de overfitting nos dados de treino, mas evitarei a técnica para termos uma experimentação mais rápida.","metadata":{}},{"cell_type":"markdown","source":"### Testando diferentes algoritmos","metadata":{}},{"cell_type":"markdown","source":"Como realizar busca e refinamento de parâmetros para múltiplos algoritmos é um processo demorado, utilizarei uma abordagem simplificada. Testarei alguns poucos algoritmos com suas configurações padrões para, a partir dos resultados, escolher apenas 1 deles e seguir com a experimentação.\n\nA métrica utilizada será a mesma definida pelo enunciado do desafio: WMAE (weighted mean absolute error), que usará a coluna 'isholiday' para atribuir peso 5 às semanas com feriados.","metadata":{}},{"cell_type":"code","source":"# Simples train/valid split\nX_train, X_valid, y_train, y_valid = train_test_split(X_exp, y_exp, random_state=42, test_size=0.15)","metadata":{"execution":{"iopub.status.busy":"2022-05-30T15:08:11.53122Z","iopub.execute_input":"2022-05-30T15:08:11.532244Z","iopub.status.idle":"2022-05-30T15:08:11.656108Z","shell.execute_reply.started":"2022-05-30T15:08:11.5322Z","shell.execute_reply":"2022-05-30T15:08:11.655001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Vamos usar XGBRegressor, LGBMRegressor e RandomForestRegressor como possíveis algoritmos\nregressors = [XGBRegressor(random_state=42), \n              LGBMRegressor(random_state=42), \n              RandomForestRegressor(random_state=42)]\n\nfor reg in regressors:\n    # Fit model\n    model = reg.fit(X_train, y_train)\n    \n    # Predict\n    y_pred_val = model.predict(X_valid)\n    \n    # Definir métrica do desafio: MAE com peso 5 para feriados\n    weights = X_valid.isholiday.apply(lambda x: 5 if x else 1)\n    WMAE = mean_absolute_error(y_valid, y_pred_val, sample_weight=weights)\n    \n    print(f'O WMAE do algoritmo {str(model)} é: {WMAE}')\n","metadata":{"execution":{"iopub.status.busy":"2022-05-30T15:08:11.657224Z","iopub.execute_input":"2022-05-30T15:08:11.657545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Random Forest é o melhor algoritmo de longe! Sendo assim, este será o algoritmo usado para realizar refinamento de parâmetros e gerar o modelo final a ser usado nas predições.","metadata":{}},{"cell_type":"markdown","source":"### Refinamento de hiperparâmetros","metadata":{}},{"cell_type":"markdown","source":"Refinamento de parâmetros será a parte mais computacionalmente pesada deste notebook. Por este motivo, tomarei algumas previdências para simplificar o processo, de forma que ele possa ser feito em um tempo razoável:\n- Será feito um [Grid Search](https://towardsdatascience.com/grid-search-for-model-tuning-3319b259367e) em apenas dois parâmetros da RandomForestRegressor: número de estimators (n_estimators) e profundidade máxima das árvores (max_depth). mais parâmetros para refinamento podem ser encontrados na [documentação da RandomForestRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)\n- Como já foi dito, não usarei Cross Validation, já que seu uso multiplicaria o tempo de execução por um fator igual ao número de folds usados.\n\nO Grid Search será codado com 'for's encadeados, para maior clareza. O uso da classe [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) é recomendada, mas por estarmos usando uma métrica com pesos (WMAE), programar seu uso seria desnecessariamente complicado. Uma forma de fazer isso pode ser encontrada nesta [página do Stack Overflow](https://stackoverflow.com/questions/49581104/sklearn-gridsearchcv-not-using-sample-weight-in-score-function). ","metadata":{}},{"cell_type":"code","source":"# Valores escolhidos por experimentações anteriores\nn_estimators_params = [i for i in range(100, 151, 10)] # Default: 100\nmax_depth_params = [i for i in range(25, 41, 3)] + [None] # Default None: até todas as folhas estarem \"puras\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Coding the simple grid-search\n\nweights = X_valid.isholiday.apply(lambda x: 5 if x else 1)\nbest_est = 100 # Default\nbest_depth = None # Default\nbest_WMAE = 1596.69 # Baseline\n\nfor est in n_estimators_params:\n    for depth in max_depth_params:\n        print(f'Training: n_estimators = {est}, max_depth = {depth}')\n        # Fit model\n        model = RandomForestRegressor(n_estimators=est, max_depth=depth, \n                                      random_state=42).fit(X_train, y_train)\n\n        # Predict\n        y_pred_val = model.predict(X_valid)\n\n        WMAE = mean_absolute_error(y_valid, y_pred_val, sample_weight=weights)\n        \n        if WMAE < best_WMAE:\n            best_est = est\n            best_depth = depth\n            best_WMAE = WMAE\n        \n        print(f'WMAE = {WMAE}\\n')\n\nprint(f'Best params: n_estimators = {best_est}, max_depth = {best_depth}\\n',\n      f'Best WMAE is: {best_WMAE}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Importância das features","metadata":{}},{"cell_type":"markdown","source":"Agora que os parâmetros estão definidos, treinaremos o modelo final e analisaremos as importâncias de cada uma das features.","metadata":{}},{"cell_type":"code","source":"# Treinar com os parâmetros selecionados e com o dataset completo\nmodel = RandomForestRegressor(n_estimators=best_est, \n                              max_depth=best_depth, random_state=42).fit(X_exp, y_exp)\n\n# Gerar dados sobre importancia das features\nfeature_importances = model.feature_importances_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotar importancia das features\n\nplt.figure(figsize=(15, 10))\nsns.barplot(X_exp.columns, feature_importances)\nplt.xticks(rotation=90)\nplt.grid(axis='y')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"É interessante observar o quão importante as features 'dept' são para o modelo, algo que esperávamos a partir da análise exploratória. O mesmo não se repete com a mesma intensidade com as features 'store', o que indica que a diferenciação entre as lojas pode estar vindo de outra fonte, como a feature 'size' (que tem a maior importância de todas).\n\nAs maiores surpresas ficam por conta das features 'cpi' e 'unemployment', que chegaram a ser consideradas para remoção devido às suas baixas correlações com a feature alvo. Isso mostra como análises de correlação nem sempre contam toda a história.\n\nAs features criadas para Natal e Thanksgiving se saíram razoavelmente bem, ao contrário da feature 'isholiday', o que pode indicar ter sido uma boa ideia separar os dois feriados mais importantes dos demais.","metadata":{}},{"cell_type":"markdown","source":"# Predict","metadata":{}},{"cell_type":"markdown","source":"Finalmente, faremos a predição no conjunto de teste e geraremos o .csv que será enviado ao Kaggle para a avaliação final.","metadata":{}},{"cell_type":"code","source":"# Predict no teste\ny_pred_test = model.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Criar arquivo da predição\nsample_submission['Weekly_Sales'] = y_pred_test\nsample_submission.to_csv('submission_order.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# O que eu faria para deixar o modelo melhor?","metadata":{}},{"cell_type":"markdown","source":"Ao lidar com modelo de Machine Learning, nos deparamos frequentemente com a decisão entre melhorar o modelo ou levá-lo a produção quando ele já pode agregar algum valor à empresa. Tomar esse tipo de decisão é por si só uma habilidade a ser desenvolvida pelo cientista de dados em vez de estar sempre na busca infinita de um modelo perfeito, mas decidi que seria uma boa ideia listar aqui algumas melhorias que eu tentaria fazer neste modelo se tivesse mais tempo:\n\n- Revisitaria as features descartadas na seção 'Análise de features' já que correlação não diz tudo sobre possíveis contribuições dos atributos. Inclusive as features 'markdown' poderiam ser reconsideradas, mesmo com tamanha porcentagem de nulos, sendo possível realizar a mesma abordagem de preenchimento feita para as colunas 'cpi' e 'unemployment' do conjunto de teste.\n\n- Exploraria a possibilidade de criação de um modelo em separado para departamentos com vendas semanais muito abaixo das demais.\n\n- Usaria Cross Validation para a realização dos experimentos, que melhoraria em muito os problemas de overfitting do modelo treinado.\n\n- Feriados são parte importante da modelagem e podem ser tratados com mais cuidado. Poderíamos por exemplo reunir mais informações sobre os padrões de consumo dos clientes americanos nestes feriados levando em conta os dias da semana, por exemplo. Esse tipo de conhecimento para além dos dados é bastante importante e exemplifica a importância para a  modelagem do conhecimento que pode ser fornecido por PMs ou outros especialistas da empresa.\n\n- Para a busca de parâmetros eu usaria algoritmos mais complexos e mais assertivos (além de procurar mais parâmetros além dos dois usados como exemplo). Uma combinação que eu gosto de usar é a das excelentes classes [HalvingGridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.HalvingGridSearchCV.html) e [HalvingRandomSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.HalvingRandomSearchCV.html), que usam um número crescente de recursos (dados) para inferir os melhores parâmetros. Primeiro costumo usar o RandomSearch em um espaço de valores mais extenso, de forma a achar uma região geral onde temos bons resultados. Depois uso o GridSearch em volta deste ponto no espaço de parâmetros para fazer um refinamento, chegando geralmente a excelentes valores. A única preocupação que temos que ter é com mínimos locais, que podem levar a overfitting, mas a componente de CrossValidation dos algoritmos ajudam nisso.\n\n- Para o modelo final, frequentemente é uma boa ideia treinar modelos com algoritmos diversos e fazer um \"comitê\" final com os melhores, onde os resultados são médias dos modelos individuais. Pode ser uma boa tentativa para este problema.","metadata":{}},{"cell_type":"markdown","source":"# Conclusão","metadata":{}},{"cell_type":"markdown","source":"Foram utilizadas diversas técnicas de análise e modelagem que, ainda que por vezes simplificadas, mostraram um caminho claro para obtenção de um bom modelo de machine learning para um problema de regressão simples. \n\nMesmo com as simplificações, foi obtido um score privado de 4016.10011 com as predições feitas no conjunto de teste, o que deixaria este modelo na 363ª posição no ranking do Kaggle. Tenho confiança que com as melhorias propostas na seção passada poderíamos ter um modelo ainda melhor.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}