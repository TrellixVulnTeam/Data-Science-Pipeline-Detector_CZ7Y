{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport plotly.express as px\nfrom random import randint\nfrom tpot import TPOTRegressor\nfrom sklearn.model_selection import train_test_split\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Reading Data\nIt reads the data provided by the host","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df_stores = pd.read_csv(\"/kaggle/input/walmart-recruiting-store-sales-forecasting/stores.csv\")\ndf_features = pd.read_csv('/kaggle/input/walmart-recruiting-store-sales-forecasting/features.csv.zip')\ndf_train = pd.read_csv('/kaggle/input/walmart-recruiting-store-sales-forecasting/train.csv.zip')\ndf_test = pd.read_csv('/kaggle/input/walmart-recruiting-store-sales-forecasting/test.csv.zip')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Converting Date\nIt convert to date format in order to handle datetime.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.Date =  pd.to_datetime(df_train.Date) \ndf_test.Date =  pd.to_datetime(df_test.Date) \ndf_features.Date = pd.to_datetime(df_features.Date)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Exploration\nDuring data exploration, I breafly observed data variables distribution","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_features.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Handling Outliers\nIn order to avoid outliers I just clipped the data based on assumption like: negative sales does not make sense at least from the context I had so far on the data provided and considering that probably top and bottom 1% could be treated as outliers","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_clipped = df_train\ndf_train_clipped['Weekly_Sales'] = df_train.Weekly_Sales.clip(0,df_train.Weekly_Sales.max())\ndf_train_clipped.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_features_clipped = df_features\ndf_features_clipped['Temperature'] = df_features.Temperature.clip(df_features.Temperature.quantile(0.01),df_features.Temperature.quantile(0.99))\ndf_features_clipped['Fuel_Price'] = df_features.Fuel_Price.clip(df_features.Fuel_Price.quantile(0.01),df_features.Fuel_Price.quantile(0.99))\ndf_features_clipped['MarkDown1'] = df_features.MarkDown1.clip(df_features.MarkDown1.quantile(0.01),df_features.MarkDown1.quantile(0.99))\ndf_features_clipped['MarkDown2'] = df_features.MarkDown2.clip(df_features.MarkDown2.quantile(0.01),df_features.MarkDown2.quantile(0.99))\ndf_features_clipped['MarkDown3'] = df_features.MarkDown3.clip(df_features.MarkDown3.quantile(0.01),df_features.MarkDown3.quantile(0.99))\ndf_features_clipped['MarkDown4'] = df_features.MarkDown4.clip(df_features.MarkDown4.quantile(0.01),df_features.MarkDown4.quantile(0.99))\ndf_features_clipped['MarkDown5'] = df_features.MarkDown5.clip(df_features.MarkDown5.quantile(0.01),df_features.MarkDown5.quantile(0.99))\ndf_features_clipped['CPI'] = df_features.CPI.clip(df_features.CPI.quantile(0.01),df_features.CPI.quantile(0.99))\ndf_features_clipped['Unemployment'] = df_features.Unemployment.clip(df_features.Unemployment.quantile(0.01),df_features.Unemployment.quantile(0.99))\ndf_features_clipped.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_features_clipped.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Just wondering on the size of sales time series","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"gTrain = df_train.groupby(['Store','Dept']).count()['Date']\ngTrain.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Ploting series\nAgain exploring features time series visualy to have a superficial understanding curves characteristics and on the effect of holidays and markdown action.","execution_count":null},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"#dept = randint(1,45)\ndept = 77\nsel_dept = df_train_clipped['Dept']== dept\ndf = df_train_clipped[sel_dept]\nfig = px.line(df, x='Date', y='Weekly_Sales', color='Store', \n              width=1200, height=800, title='Time Series with Rangeslider')\n\nfig.update_xaxes(rangeslider_visible=True)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"#store = randint(1,45)\nstore = 45\nsel_store = df_train_clipped['Store']== store\ndf = df_train_clipped[sel_store]\nfig = px.line(df, x='Date', y='Weekly_Sales', color='Dept', \n              width=1200, height=800, title='Time Series with Rangeslider')\n\nfig.update_xaxes(rangeslider_visible=True)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"store = randint(1,45)\nsel_store = df_test['Store']== store\ndf1 = df_test[sel_store]\nfig = px.line(df1, x='Date', y='IsHoliday', color='Dept', \n              width=1200, height=800, title='Time Series with Rangeslider')\n\nfig.update_xaxes(rangeslider_visible=True)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"# store = randint(1,45)\n# sel_store = feature['Store']== store\n# df_test = test[sel_store]\nfig = px.line(df_features, x='Date', y='Temperature', color='Store', \n              width=1200, height=800, title='Time Series with Rangeslider')\n\nfig.update_xaxes(rangeslider_visible=True)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Adding a column to point out a specific holiday\nI have decided to build up a variable that no only tell about if it is holiday, but to say specifically which holiday it is.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* Super Bowl: 12-Feb-10, 11-Feb-11, 10-Feb-12, 8-Feb-13\n* Labor Day: 10-Sep-10, 9-Sep-11, 7-Sep-12, 6-Sep-13\n* Thanksgiving: 26-Nov-10, 25-Nov-11, 23-Nov-12, 29-Nov-13\n* Christmas: 31-Dec-10, 30-Dec-11, 28-Dec-12, 27-Dec-13","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def getHoliday(date):\n    if date in ['2010-02-12','2011-02-11','2012-02-10','2013-02-08']:\n        return str('super_bowl')\n    if date in ['2010-09-10','2011-09-09','2012-09-07','2013-09-06']:\n        return str('labor_day')\n    if date in ['2010-11-26','2011-11-25','2012-11-23','2013-11-29']:\n        return str('thx_giving')\n    if date in ['2010-12-31','2011-12-30','2012-12-28','2013-12-13']:\n        return str('xmas')\n    else:\n        return 'not_holiday'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_features_clipped['Holiday'] = df_features_clipped.apply(lambda x: getHoliday(x['Date']),axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_features_clipped.drop(columns='IsHoliday', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"   I also applied NA treatment replacing na by feature median","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_features_clipped.fillna(df_features_clipped.median(), inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, another decision was to make use of week of year representation, e.g. week 26 other than 2013-06-28 and it will be used as a feature","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_features_clipped['Week_of_Year'] = df_features_clipped.Date.dt.week","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_features_clipped.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Putting together features, train and stores data sets.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_feat = pd.merge(df_train_clipped,df_features_clipped, how='inner', on=['Store','Date'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_feat.drop(columns='IsHoliday', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_feat = pd.merge(df_train_feat,df_stores, how='inner', on=['Store'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_feat.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Putting together features, test and stores data sets.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test_feat = pd.merge(df_test,df_features_clipped, how='inner', on=['Store','Date'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test_feat = pd.merge(df_test_feat,df_stores, how='inner', on=['Store'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test_feat.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## ML Modeling\nTo generate ML model, I took advantage TPOT automl framework ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Preparing data to feed framework\n> I decided to select as features to feed the model: Temperature, Fuel Price, Markdown1-5, CPI, Unemployment, Week_of_Year, Size of Store, Store Type, Holiday information, Dept.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def getInputData(df):\n    numeric_cols = ['Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'CPI','Unemployment', 'Week_of_Year','Size']\n    \n    stores_cols = pd.get_dummies(df['Store'])\n    dept_cols = pd.get_dummies(df['Dept'])\n    holiday_cols = pd.get_dummies(df['Holiday'])\n    # weekOfYear_cols = pd.get_dummies(df['Week_of_Year'])\n    type_cols = pd.get_dummies(df['Type'])\n    \n    input_data = pd.concat([df[numeric_cols],\n              stores_cols, dept_cols, holiday_cols, type_cols], axis=1)\n    return input_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_data = getInputData(df_train_feat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target = df_train_feat['Weekly_Sales']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Model selection and training","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# splitting train and test data\nX_train, X_test, y_train, y_test = train_test_split(input_data, target,\n                                                    train_size=0.80, test_size=0.20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This part is responsible for tunning and selecting a model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model = TPOTRegressor(generations=5, population_size=20, \n                                   verbosity=2, n_jobs=3, scoring = 'neg_mean_absolute_error')\nmodel.fit(X_train, y_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(model.score(X_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.export('/kaggle/working/sales_forecast.py')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Predicting for test file provided by challenge host","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_validation = getInputData(df_test_feat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Weekly_Sales = model.predict(X_validation)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.Series(Weekly_Sales)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"keys = ['Store','Dept','Date']\ndf_submission = pd.concat([df_test_feat[keys],pd.Series(Weekly_Sales, name='Weekly_Sales')], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_submission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_submission['Id'] = df_submission.apply(lambda x: '_'.join([str(x['Store']),str(x['Dept']),str(x['Date'])]),axis=1)\ndf_submission['Id'] = df_submission.Id.apply(lambda x: x.split()[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"challenge_file = df_submission[['Id','Weekly_Sales']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"challenge_file","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"challenge_file.to_csv('/kaggle/working/df_submission.csv',index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}