{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\n\nfrom scipy.stats import zscore, boxcox\nfrom sklearn.preprocessing import QuantileTransformer, LabelEncoder, MinMaxScaler\nfrom sklearn.model_selection import RepeatedKFold, cross_val_score\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.linear_model import BayesianRidge\nfrom sklearn.model_selection import RepeatedKFold, cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_absolute_error, make_scorer\nfrom sklearn.ensemble import VotingRegressor\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\n\n## NN ##\nfrom keras.models import Sequential\nfrom keras.layers import Dense as Dense2\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras import regularizers\nfrom keras import callbacks\n\nfrom tensorflow.keras.layers import Dense, Flatten, Conv2D\nfrom tensorflow.keras import Model\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport tensorflow as tf\n#######\n\n\nsns.set_style('darkgrid')\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\npd.options.mode.chained_assignment = None","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Input"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.read_csv('/kaggle/input/walmart-recruiting-store-sales-forecasting/sampleSubmission.csv').head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"X_train = pd.read_csv('/kaggle/input/walmart-recruiting-store-sales-forecasting/train.csv')\nstores = pd.read_csv('/kaggle/input/walmart-recruiting-store-sales-forecasting/stores.csv')\nfeatures = pd.read_csv('/kaggle/input/walmart-recruiting-store-sales-forecasting/features.csv')\nX_train = X_train.merge(stores, on='Store').merge(features.drop('IsHoliday', axis=1), on=['Store', 'Date'])\nX_train.fillna(0, inplace=True) ## only NaN columns are the markdowns. Set to 0 if NaN ##\nX_train['Date'] = pd.to_datetime(X_train['Date'])\n# X_train_no_neg = X_train[X_train['Weekly_Sales'] >= 0].reset_index(drop=True)\n\n# y_train = X_train[['Store', 'Dept', 'Date', 'Weekly_Sales']]\n# X_train.drop('Weekly_Sales', axis=1, inplace=True)\n\n# y_train_no_neg = X_train_no_neg[['Store', 'Dept', 'Date', 'Weekly_Sales']]\n# X_train_no_neg.drop('Weekly_Sales', axis=1, inplace=True)\n\n# print(X_train.shape, X_train_no_neg.shape, (X_train.shape[0] - X_train_no_neg.shape[0]) / X_train.shape[0])\n\nX_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train['Date'].dtype","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = pd.read_csv('/kaggle/input/walmart-recruiting-store-sales-forecasting/test.csv')\nX_test = X_test.merge(stores, on='Store').merge(features.drop('IsHoliday', axis=1), on=['Store', 'Date'])\nX_test.fillna(0, inplace=True) ## only NaN columns are the markdowns. Set to 0 if NaN ##\n\nX_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test['Store'].nunique(), X_test['Dept'].nunique(), X_test['Date'].nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.shape, X_train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train['Year'] = pd.DatetimeIndex(X_train['Date']).year\nX_train['Month'] = pd.DatetimeIndex(X_train['Date']).month\nX_train['woy'] = pd.DatetimeIndex(X_train['Date']).weekofyear\nX_train['quarter'] = pd.DatetimeIndex(X_train['Date']).quarter\n\nX_test['Year'] = pd.DatetimeIndex(X_test['Date']).year\nX_test['Month'] = pd.DatetimeIndex(X_test['Date']).month\nX_test['woy'] = pd.DatetimeIndex(X_test['Date']).weekofyear\nX_test['quarter'] = pd.DatetimeIndex(X_test['Date']).quarter\n\n## for future reference ##\n# df['dow'] = df.index.dayofweek\n# df['doy'] = df.index.dayofyear","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_all['Store'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ## Add 1-year `Weekly_Sales` lag ##\n\n# X_all = pd.concat([X_train, X_test])\n# X_all['Date2'] = pd.to_datetime(X_all['Date'], utc = True)\n# X_all['52_Week_Lag'] = X_all['Date2'] - np.timedelta64(52,'W')\n# X_all_temp = X_all[['Weekly_Sales', 'Date2', 'Store', 'Dept']]\n\n# X_all = X_all.merge(X_all_temp,\n#                     left_on=['Store', 'Dept', '52_Week_Lag'], \n#                     right_on=['Store', 'Dept', 'Date2'],\n#                     how='inner',\n#                     suffixes=('', '_y'))\n# X_all.rename(columns={'Weekly_Sales_y': 'Weekly_Sales_Lag_52_Weeks'}, inplace=True)\n# X_all = X_all[[col for col in X_all.columns if not col.endswith('_y')]]\n\n# drop_cols = ['Date2_y', '1_Year_Lag']\n# X_all.drop(['52_Week_Lag'], axis=1, inplace=True)\n\n# X_all.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# X_train['Date2'] = pd.to_datetime(X_train['Date'], utc = True)\n# X_test['Date2'] = pd.to_datetime(X_test['Date'], utc = True)\n\n# X_train['Weekly_Sales_Lag_52_Weeks'] = X_train.merge(X_all, \n#                                                    left_on=['Store', 'Dept', 'Date2'], \n#                                                    right_on=['Store', 'Dept', 'Date2'],\n#                                                    how='inner')['Weekly_Sales_Lag_52_Weeks']\n# X_test['Weekly_Sales_Lag_52_Weeks'] = X_test.merge(X_all, \n#                                                  left_on=['Store', 'Dept', 'Date2'], \n#                                                  right_on=['Store', 'Dept', 'Date2'],\n#                                                  how='inner')['Weekly_Sales_Lag_52_Weeks']\n\n# X_test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_num = [col for col in X_train.columns if X_train[col].dtype in [float, int]]\nncols = len(cols_num) // 4\nfig, axes = plt.subplots(ncols=ncols, nrows=5, figsize=(30,16))\n\ni = 1\nfor j, col in enumerate(cols_num):\n    sns.distplot(X_train[col], bins=10, ax=axes[i-1][j % ncols])\n\n    if j % ncols == (ncols - 1):\n        i += 1\n        \nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Will not transform the `Store` or `Department` variables, as they're unique identifiers."},{"metadata":{"trusted":true},"cell_type":"code","source":"# sns.set(style=\"ticks\", color_codes=True)\n\n# for col in X_train.columns.drop('Weekly_Sales'):\n#     sns.pairplot(data=X_train,\n#                  y_vars=['Weekly_Sales'],\n#     #              x_vars=['Weekly_Sales_Lag_1_Year', 'quarter', 'woy'],\n#                  x_vars=col,\n#                  hue='Year')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Transformation"},{"metadata":{},"cell_type":"markdown","source":"### Dependent variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train['outlier'] = np.where((zscore(X_train['Weekly_Sales']) <= -2.5) | (zscore(X_train['Weekly_Sales']) >= 2.5), 1, 0)\nnum_outliers = X_train[X_train['outlier'] == 1]['Weekly_Sales'].count()\n\nprint('Number of `Weekly_Sales` outliers: {}\\nPercent outliers: {:.2f}%'.format(num_outliers, num_outliers / X_train.shape[0] * 100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3.26% of the `Weekly_Sales` are outliers. We can either drop the entries to have a better fit, or apply a scaling. Let's try a log scaling."},{"metadata":{"trusted":true},"cell_type":"code","source":"num_neg_sales = X_train[X_train['Weekly_Sales'] < 0].shape[0]\nprint('Number of negative `Weekly_Sales` : {}\\nPercent: {:.2f}%'.format(num_neg_sales, num_neg_sales / X_train.shape[0] * 100))\n\nX_train['Weekly_Sales'] = np.where(X_train['Weekly_Sales'] < 0, 0, X_train['Weekly_Sales'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Negative `Weekly_Sales` does not make sense so replace them with zero."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train['Weekly_Sales_Log'] = np.log1p(X_train['Weekly_Sales'])\nX_train['outlier'] = np.where((zscore(X_train['Weekly_Sales_Log']) <= -2.5) | (zscore(X_train['Weekly_Sales_Log']) >= 2.5), 1, 0)\nnum_outliers = X_train[X_train['outlier'] == 1]['Weekly_Sales_Log'].count()\n\nprint('Number of `Weekly_Sales_Log` outliers: {}\\nPercent outliers: {:.2f}%'.format(num_outliers, num_outliers / X_train.shape[0] * 100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After higher than what there would have been with no outliers. \n\nI'll evaluate models on each transformation of the data to determine which is the best performer."},{"metadata":{"trusted":true},"cell_type":"code","source":"## QuantileTransformer ##\n## https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.QuantileTransformer.html ##\nqt = QuantileTransformer(output_distribution='normal')\n# y_train_qt = y_train.copy()\nX_train['Weekly_Sales_tf'] = qt.fit_transform(np.array(X_train['Weekly_Sales'] + 1).reshape(-1, 1))\nX_train['outlier'] = np.where((zscore(X_train['Weekly_Sales_tf']) <= -2.5) | (zscore(X_train['Weekly_Sales_tf']) >= 2.5), 1, 0)\nnum_outliers = X_train[X_train['outlier'] == 1]['Store'].count()\n\nX_train_tf = X_train[X_train['outlier'] != 1]\n\nprint('QuantileTransformer - Number of `Weekly_Sales` outliers: {}\\nPercent outliers: {:.2f}%'.format(num_outliers, num_outliers / X_train.shape[0] * 100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Add 1-year `Weekly_Sales` lag ##\n\nX_all = pd.concat([X_train, X_test])\nX_all['Date2'] = pd.to_datetime(X_all['Date'], utc = True)\nX_all['52_Week_Lag'] = X_all['Date2'] - np.timedelta64(52,'W')\nX_all_temp = X_all[['Weekly_Sales_tf', 'Date2', 'Store', 'Dept']]\n\nX_all = X_all.merge(X_all_temp,\n                    left_on=['Store', 'Dept', '52_Week_Lag'], \n                    right_on=['Store', 'Dept', 'Date2'],\n                    how='left',\n                    suffixes=('', '_y'))\nX_all.rename(columns={'Weekly_Sales_tf_y': 'Weekly_Sales_tf_Lag_52_Weeks'}, inplace=True)\nX_all = X_all[[col for col in X_all.columns if not col.endswith('_y')]]\n\n## bad solution, fix later ##\n\n## first try to fill missing 52-week lagged values (2,041 ~ 2%) with approximate year-lag values ##\nfor i in [53, 51, 54, 50, 55, 49, 56, 48]:\n    X_all['n_Week_Lag'] = X_all['Date2'] - np.timedelta64(i,'W')\n    X_all = X_all.merge(X_all_temp,\n                    left_on=['Store', 'Dept', 'n_Week_Lag'], \n                    right_on=['Store', 'Dept', 'Date2'],\n                    how='left',\n                    suffixes=('', '_y'))\n    X_all['Weekly_Sales_tf_Lag_52_Weeks'].fillna(X_all['Weekly_Sales_tf_y'], inplace=True)\n    X_all = X_all[[col for col in X_all.columns if not col.endswith('_y')]]\n    \n## ffill remaining. Bad because departments and times aren't aligned (only 571 values --> < 0.5%) ##\nX_all['Weekly_Sales_tf_Lag_52_Weeks'].fillna(method='ffill', inplace=True)\n#############################\n\nX_all.drop(['52_Week_Lag'], axis=1, inplace=True)\n\nX_all[pd.to_datetime(X_all['Date']) >= datetime(2012, 11, 2)].isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_tf['Date2'] = pd.to_datetime(X_train_tf['Date'], utc = True)\nX_test['Date2'] = pd.to_datetime(X_test['Date'], utc = True)\n\nX_train_tf['Weekly_Sales_tf_Lag_52_Weeks'] = X_train_tf.merge(X_all, \n                                                   left_on=['Store', 'Dept', 'Date2'], \n                                                   right_on=['Store', 'Dept', 'Date2'],\n                                                   how='inner')['Weekly_Sales_tf_Lag_52_Weeks']\nX_test['Weekly_Sales_tf_Lag_52_Weeks'] = X_test.merge(X_all, \n                                                 left_on=['Store', 'Dept', 'Date2'], \n                                                 right_on=['Store', 'Dept', 'Date2'],\n                                                 how='inner')['Weekly_Sales_tf_Lag_52_Weeks']\n\nX_train_tf.drop(['Date2', 'outlier'], axis=1, inplace=True)\nX_test.drop('Date2', axis=1, inplace=True)\n\nX_test.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Plotting distributions of different scalings of dependent variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(ncols=5, figsize=(20,8))\nsns.distplot(X_train['Weekly_Sales'], bins=10, ax=axes[0]).set_title('Weekly Sales')\nsns.distplot(X_train['Weekly_Sales_Log'], bins=10, ax=axes[1]).set_title('Log(1+Weekly Sales)')\nsns.distplot(X_train_tf['Weekly_Sales_Log'], bins=10, ax=axes[2]).set_title('Log(1+Weekly Sales)\\nno outliers')\nsns.distplot(X_train['Weekly_Sales_tf'], bins=10, ax=axes[3]).set_title('(1+Weekly Sales)\\nQauntile Transformer')\nsns.distplot(X_train_tf['Weekly_Sales_tf'], bins=10, ax=axes[4]).set_title('(1+Weekly Sales)\\nQauntile Transformer\\nno outliers')\n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Given the original distribution is expoential, the best results are obtained by transforming the data using the `QuantileTransformer` method and removing outliers. The amount of outliers after applying the `QuantileTransformer` method is only 0.98% of the original dataset. This is much better than +3.0% of data the would have needed to be removed without scaling or with the `np.log1p` transformation."},{"metadata":{"trusted":true},"cell_type":"code","source":"## rename columns to remove `tf` ##\n\nX_train_tf['Weekly_Sales'] = X_train['Weekly_Sales_tf']\nX_train_tf['Weekly_Sales_Lag_52_Weeks'] = X_train_tf['Weekly_Sales_tf_Lag_52_Weeks']\nX_train_tf.drop(['Weekly_Sales_tf', 'Weekly_Sales_tf_Lag_52_Weeks'], axis=1, inplace=True)\n\nX_test['Weekly_Sales_Lag_52_Weeks'] = X_test['Weekly_Sales_tf_Lag_52_Weeks']\nX_test.drop(['Weekly_Sales_tf_Lag_52_Weeks'], axis=1, inplace=True)\n\ny_train_qt_tf = X_train_tf[['Store', 'Dept', 'Date', 'Weekly_Sales', 'Weekly_Sales_Lag_52_Weeks']]\nX_train_tf.drop(['Weekly_Sales', 'Weekly_Sales_Log'], axis=1, inplace=True)\ny_train_qt_tf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Independent variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train_tf.shape)\n\nX_train_tf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train_tf['Type'].value_counts(), '\\n', X_test['Type'].value_counts())\nprint(X_train_tf['IsHoliday'].value_counts(), '\\n', X_test['IsHoliday'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Only 3 values for `Type` and 2 for `IsHoliday`, so can use `LabelEncoder` for them. All values in the training set are in the test set as well."},{"metadata":{"trusted":true},"cell_type":"code","source":"lbl_encoder = LabelEncoder()\n\nX_train_tf['IsHoliday'] = X_train_tf['IsHoliday'].replace(True, 5).replace(False, 1).values # go off the custom weighted-mae function\nX_train_tf['Type'] = lbl_encoder.fit_transform(X_train_tf['Type'])\n\nX_test['IsHoliday'] = X_test['IsHoliday'].replace(True, 5).replace(False, 1).values # go off the custom weighted-mae function\nX_test['Type'] = lbl_encoder.transform(X_test['Type'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_tf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_tf.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"def weighted_mae_custom(y_true, y_pred):\n    '''\n    Custom weighting function as specified in the evaluation section.\n    '''\n    weights = X_train_tf['IsHoliday']\n    sample_weights = pd.Series(weights.loc[y_true.index.values].values.reshape(-1)).dropna()\n    return (1.0 / np.sum(sample_weights)) * np.sum(sample_weights * np.abs(y_true - y_pred))\n\nweighted_mae = make_scorer(weighted_mae_custom)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_tf.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params={'rf': {\n#             'n_estimators': [100, 250, 500],\n#             'max_depth': [1, 2, 3, 4],\n#             'max_features': [2, 4, 6, 8]\n            },\n        'knn': {\n#             'n_neighbors': [2, 3, 4],  \n#             'p': [1,2],\n            },\n        'gb': {\n#             'max_depth': [1, 2, 3, 4],\n#             'learning_rate':[1e-3,1e-2,0.1,1]\n            },\n        'lr':{\n            'fit_intercept': [True, False]\n            },\n        'lgbm':{\n#             'learning_rate':[1e-3,1e-2,0.1,1],\n#             'n_estimators': [100, 250, 500],\n            },\n        'xgb':{\n            \n            },\n        }\n        \nmodels = {\n          'lr': LinearRegression(n_jobs=-1),\n          'knn': KNeighborsRegressor(n_jobs=-1),\n          'rf': RandomForestRegressor(random_state=0, n_jobs=-1, n_estimators=100),\n          'gb': GradientBoostingRegressor(random_state=0),\n          'lgbm': LGBMRegressor(random_state=0),\n          'xgb': XGBRegressor(nthread=-1, seed=0),\n         }\n\nbest_params = {}\nbest_models = []\n\nfor name, model in models.items():\n    cv1 = RepeatedKFold(n_splits=2, n_repeats=1,  random_state=0)                       \n    gs_cv = GridSearchCV(model, \n                         params[name], \n                         scoring=weighted_mae,\n                         cv=cv1,\n                         n_jobs=-1,\n                         iid=True,\n                         verbose=2)\n    \n    gs_cv.fit(X_train_tf.drop('Date', axis=1).dropna(), y_train_qt_tf.dropna()['Weekly_Sales'])\n    \n    mean = abs(gs_cv.cv_results_['mean_test_score'][0])\n    std = gs_cv.cv_results_['std_test_score'][0]\n    \n    best_params[name] = gs_cv.best_params_\n    best_models.append(gs_cv.best_estimator_)\n    \n    print(\"Results for {}: {:.4f} ({:.4f}) [{:.4f}, {:.4f}] WMAE\".format(name, \n                                                                         mean,\n                                                                         std,\n                                                                         mean - std,\n                                                                         mean + std))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I didn't tune any hyperparamters, except logistic regression, because of size of data. I left the code there to tune the hyperparameters in case someone else or myself is willing to spend time to tune in the future."},{"metadata":{"trusted":true},"cell_type":"code","source":"## run to get hyperparameters for `VotingRegressor` ##\n\nbest_params_2 = {}\nfor model,params in best_params.items():\n    for param,value in best_params[model].items():\n        best_params_2['{}__{}'.format(model, param)] = [value]\n        \nbest_params_2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def voting_regressor(X, y):\n    # br and lr similar performance, knn slightly worse, rf and gbr perform very badly. Use this to select weighting.\n    v_reg = VotingRegressor(estimators=[('lr', LinearRegression(n_jobs=-1)),\n                                        ('knn', KNeighborsRegressor(n_jobs=-1)),\n                                        ('gb', GradientBoostingRegressor(random_state=0)),\n#                                         ('rf', RandomForestRegressor(random_state=0, n_jobs=-1, n_estimators=100)),\n                                        ('lgbm', LGBMRegressor(random_state=0)),\n                                        ('xgb', XGBRegressor(nthread=-1, seed=0))\n                                       ], \n                            n_jobs=-1,\n                            weights=[0.10, 0.10, 0.20, 0.40, .20] ## specify weight given to each model in prediction (overweight rf & lgbm) ##\n                           )\n    \n    cv1 = RepeatedKFold(n_splits=2, n_repeats=1,  random_state=0)                       \n    gs_cv = GridSearchCV(v_reg, \n                         best_params_2, ## no parameters used in previous implementation, except logistic regression ##\n                         scoring=weighted_mae,\n                         cv=cv1, \n                         n_jobs=-1,\n                         iid=True,\n                         verbose=2)\n    \n    gs_cv.fit(X, y)\n\n    mean = abs(gs_cv.cv_results_['mean_test_score'][0])\n    std = gs_cv.cv_results_['std_test_score'][0]\n    \n    print(\"Results for {}: {:.4f} ({:.4f}) [{:.4f}, {:.4f}] accuracy\".format('VotingRegressor', \n                                                                             mean,\n                                                                             std,\n                                                                             mean - std,\n                                                                             mean + std))\n    \n    return gs_cv.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vot_reg = voting_regressor(X_train_tf.drop('Date', axis=1).dropna(), y_train_qt_tf.dropna()['Weekly_Sales'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"`RandomForestRegressor` does better on it's own that combining the other five models."},{"metadata":{},"cell_type":"markdown","source":"### NN"},{"metadata":{"trusted":true},"cell_type":"code","source":"## takes too long ##","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def weighted_mae_keras(weights):\n    def loss(y_true, y_pred):\n        return (1.0 / np.sum(weights)) * keras.backend.sum(weights * keras.backend.abs(y_true - y_pred))\n    \n    return loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def weighted_mae_keras(y_true, y_pred):\n#     return keras.losses.mean_absolute_error(y_true, y_pred)\n\ndef build_model(X, weights):\n    model = keras.Sequential([\n        layers.Dense(2048, activation=tf.nn.leaky_relu, input_shape=[X.shape[1]]),\n        layers.BatchNormalization(),\n        layers.Dropout(0.2),\n        layers.Dense(1024, activation=tf.nn.leaky_relu),\n        layers.BatchNormalization(),\n        layers.Dropout(0.2),\n        layers.Dense(512, activation=tf.nn.leaky_relu),\n        layers.BatchNormalization(),\n        layers.Dropout(0.2),\n        layers.Dense(128, activation=tf.nn.leaky_relu),\n        layers.BatchNormalization(),\n        layers.Dropout(0.2),\n        layers.Dense(1)\n    ])\n\n    model.compile(\n                  loss=weighted_mae_keras(weights),\n#                   loss='mae',\n                  optimizer='adam',)\n#                   metrics=[weighted_mae_keras(weights)])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weights = X_train_tf['IsHoliday'].values\n\nmodel = build_model(X_train_tf.drop('Date', axis=1).dropna(), weights) ## drop rows with NaN lagged 1-year `Weekly_Sales` ##","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mm_scaler = MinMaxScaler()\nX_train_tf_scaled = pd.DataFrame(mm_scaler.fit_transform(X_train_tf.drop('Date', axis=1).dropna()), columns=X_train_tf.drop('Date', axis=1).columns)\n\nEPOCHS = 1\n\nhistory = model.fit(X_train_tf_scaled[:1000], \n                    y_train_qt_tf.dropna()['Weekly_Sales'][:1000],\n                    epochs=EPOCHS, \n#                     callbacks=[es],\n                    validation_split=0.25,\n                    verbose=1,\n#                     verbose=2,\n                    workers=10,\n                    use_multiprocessing=True\n                   )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# hist = pd.DataFrame(history.history)\n# hist.plot()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## ARIMA"},{"metadata":{"trusted":true},"cell_type":"code","source":"## create 1-year lag value of store sales (can't use 1-week as test set doesn't have any `Weekly_Sales` ##\n## X_train ends at 2012-10-26 and X_test ends at 2013-07-26, so no NaN values for `Weekly_Sales_Lag` in X_test ##","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# X_all = pd.concat([X_train, X_test])\n# X_all.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Predictions ##"},{"metadata":{"trusted":true},"cell_type":"code","source":"best_models[2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.drop('Date', axis=1).isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## QuantileTransformer ##\nX_test['Weekly_Sales'] = qt.inverse_transform(best_models[2].predict(X_test.drop('Date', axis=1)).reshape(-1, 1)) + 1\nX_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nId,Weekly_Sales\n1_1_2012-11-02,0\n'''\ndf_pred = pd.DataFrame(columns=['Id', 'Weekly_Sales'])\ndf_pred['Id'] = X_test['Store'].astype(str) + '_' + X_test['Dept'].astype(str) + '_' + X_test['Date'].astype(str)\ndf_pred['Weekly_Sales'] = X_test['Weekly_Sales']\n\ndf_pred.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_pred.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}