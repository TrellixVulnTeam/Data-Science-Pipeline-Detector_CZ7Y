{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Project Objective\n\nThe main purpose of this project is to correctly predict sales, specially in holiday weeks, despite not having complete/ideal historical data. It is part of tha challenge to model the effects of markdowns on these holiday weeks.\n\nThe data for this project is available in:\nhttps://www.kaggle.com/c/walmart-recruiting-store-sales-forecasting/data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\npd.options.display.max_columns = 30","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Importing files\n\nNext, we will import and analyze all data files in order to evaluate all the information we have. From the \"Data Description\" section from Kaggle, we can see that there are date columns in \"train.csv\", \"test.csv\" and \"features.csv\" files. Hence, we will import the \"Date\" column from these files as “datetime64” data type.\n\n## Train file\n\nThe first file is the train file. Each row of this file represents one day of sale for one department of one store. It also informs how much was sold and if that day is part of a special holiday week.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#importing data\ntrain = pd.read_csv(\"../input/walmart-recruiting-store-sales-forecasting/train.csv.zip\", parse_dates = [\"Date\"])\n\ndef snake_case(dataframe):\n    #convert column names to snake_case\n    dataframe.columns = dataframe.columns.str.lower()\n    dataframe = dataframe.rename(columns = {\"isholiday\" : \"is_holiday\"})\n    return dataframe\n\n#converting column names to snake_case\ntrain = snake_case(train)\n\n#exploring the data\nprint(train.shape)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Test file\n\nIt contains the same columns as the train file, except for \"weekly_sales\", which is our target. The test file is about 27% the train file.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#importing data\ntest = pd.read_csv(\"../input/walmart-recruiting-store-sales-forecasting/test.csv.zip\", parse_dates = [\"Date\"])\n\n#converting column names to snake_case\ntest = snake_case(test)\n\n#exploring the data\nprint(test.shape)\nprint(test.shape[0] / train.shape[0]) #how long is the test dataframe in comparison to train the dataframe\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Features file\n\nThe features are separated from the train and test dataframes. It must be merged together with both test and train dataframes in order to train the model and predict the weekly sales. It will be merged on \"store\" and \"date\" columns.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#importing data\nfeatures = pd.read_csv(\"../input/walmart-recruiting-store-sales-forecasting/features.csv.zip\", parse_dates = [\"Date\"])\n\n#converting column names to snake_case\nfeatures = snake_case(features)\n\n#exploring the data\nprint(features.shape)\nfeatures.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Stores file\n\nCharacteristics of each store are also separeted from the train and test dataframes. Therefore, stores dataframe will also be merged with both of them. They will be merged on \"store\" column.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#importing data\nstores = pd.read_csv(\"../input/walmart-recruiting-store-sales-forecasting/stores.csv\")\n\n#converting column names to snake_case\nstores = snake_case(stores)\n\n#exploring the data\nprint(stores.shape)\nstores.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Sample Submission file\n\nThe sample submission file contains one Id column that is not present in the train dataframe nor in the test dataframe. Hence, the Id column will be created in both test and train dataframes by concatenating the \"store\", \"dept\", and \"date\" with underscores.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#importing data\nsample = pd.read_csv(\"../input/walmart-recruiting-store-sales-forecasting/sampleSubmission.csv.zip\")\n\n#exploring the data\nprint(sample.shape)\nsample.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Checking for duplicated information\n\nWe will check if the data have duplicated information. It is important to avoid addition of new rows when merging the dataframes together. For train and test, we should have one row for each subset \"store-dept-date\". For features, we should have one row for each subset \"store-date\". For stores dataframe, we must have one row for each store. Finally, for sample, it should contain one row for each \"Id\".","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#checking for duplicated. If sum is > than 0, it has duplicated rows.\nprint(\"duplicated in train:\", train.duplicated(subset = [\"store\", \"dept\", \"date\"]).sum())\nprint(\"duplicated in test:\", test.duplicated(subset = [\"store\", \"dept\", \"date\"]).sum())\nprint(\"duplicated in features:\", features.duplicated(subset = [\"store\", \"date\"]).sum())\nprint(\"duplicated in stores:\", stores.duplicated(subset = [\"store\"]).sum())\nprint(\"duplicated in sample:\", sample.duplicated(subset = [\"Id\"]).sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are no duplicated rows and we can proceed to check for missing values.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Checking for missing values\n\nWe will check how much missing data we have and choose the best way to deal with that.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No missing values in the \"train\" dataframe.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No missing values in the \"test\" dataframe.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"stores.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No missing values in the \"stores\" dataframe.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"features.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are missing values in the \"features\" dataframe. We have to deal with them in order to apply machine learning techniques. We will investigate these data further.","execution_count":null},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"#importing data visualisation libraries\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\n\n#plotting missing data\nfig_missing_data, ax_missing_data = plt.subplots(figsize = (18,6))\nsns.heatmap(features.isnull(), ax = ax_missing_data)\n\nax_missing_data.set_title(\"Missing data by column\")\nax_missing_data.set_ylabel(\"Row index\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will have to deal with 7 columns. All five \"markdown\" columns, \"cpi\" column and \"unemployment\" column. We will start dealing \"cpi\" and \"unemployment\", that have much less missing data than any of the five \"markdown\" columns.\n\n## Dealing with \"cpi\" and \"unemployment\" missing values\n\nFrom the \"Data Description\" section from Kaggle, \"cpi\" stands for consumer price index and \"unemployment\" is the unemployment rate. Both vary a little over time, thus, one idea to deal with missing data is to take the mean between the preceding week and the subsequent week and use it to fill the missing values. We will plot both of these columns to check if taking the mean  between the subsequent week and the preceding week is a valid option.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#registering converters to avoid warning\nfrom pandas.plotting import register_matplotlib_converters\nregister_matplotlib_converters()\n\ndef plot_feature(axes_object, dataframe, column_name, store = None):\n    #plot single store\n    if store is not None:\n        dataframe_mask = dataframe[\"store\"] == store\n        \n        hue = None #only one line will be plotted\n        \n        title = \"Store {} - {} data\".format(store, column_name.title())\n    \n    #plot all stores\n    else:\n        # creating mask with only true values\n        dataframe_mask = pd.Series(np.ones(len(dataframe), dtype=bool))\n        \n        hue = dataframe[dataframe_mask][\"store\"] #create one line for each store\n        \n        title = \"All Stores - {} data\".format(column_name.title())\n    \n    #plotting\n    sns.lineplot(dataframe[dataframe_mask][\"date\"], dataframe[dataframe_mask][column_name], hue = hue, ax = axes_object)\n    \n    #setup\n    axes_object.set_title(title)\n    axes_object.set_xlabel(\"Date\")\n    axes_object.set_ylabel(column_name.title())\n    \n    plt.setp(axes_object.xaxis.get_majorticklabels(), rotation=45)\n\n    \n#creating figure with four graphs\nfig_time, axs_time = plt.subplots(2, 2, figsize = (15, 15))\n    \nplot_feature(axs_time[0, 0], features, \"cpi\", store = 1)\nplot_feature(axs_time[1, 0], features, \"unemployment\", store = 1)\nplot_feature(axs_time[0, 1], features, \"cpi\")\nplot_feature(axs_time[1, 1], features, \"unemployment\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We expected a discontinuity in the lines, representing the missing data. All lines representing each store are continuous, thus, the missing values occur in the first dates of each store or in the last dates.\n\nFrom the heatmap we plotted earlier, it seems like both \"cpi\" and \"unemployment\" data are missing in similarly spaced chunks of rows. Since the data from features dataframe is ordered by store and then by date, this could mean that the missing data occurs at the same dates for all stores.\n\nNow we will check if the missing values for \"cpi\" and \"unemployment\" really occur in the beginning or the end of the time series.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking if \"cpi\" and \"unemployment\" data are missing in the same rows\n(features[\"cpi\"].isnull() != features[\"unemployment\"].isnull()).sum() # When summing boolean values, True equals 1 \n                                                                      # and False equals 0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Both \"cpi\" and \"unemployment\" have missing data in the same rows. Thus, we can use either of these columns to create a mask for missing values.","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#creating mask\nmissing_mask = features[\"cpi\"].isnull()\n\n#dates with missing cpi and unemployment data\nmissing_dates = features[missing_mask][\"date\"].unique()\nsorted(missing_dates)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#all dates in the dataframe\nall_dates = features[\"date\"].unique()\nsorted(all_dates)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As expected, the missing data occurs in the last 13 weeks of the time series. It seems that from 2013-05-03 onward, no more \"cpi\" and \"unemployment\" data was collected. We will verify if that happens to every store.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#column that identify if cpi is null\nfeatures[\"cpi_isnull\"] = missing_mask\n\n# pivot table to check if \"cpi\" and \"unemployment\" data are missing in the same dates for all stores\npivot_missing_cpi = features.pivot_table(\"cpi_isnull\", \"date\", \"store\", aggfunc = np.sum)\npivot_missing_cpi","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 45 stores. If the cpi is missing for every store in a specific date (row), the sum of all the values of the row will be 45. Similarly, if all values are False, the sum of all values of that row will be 0.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#counting how many stores have missing \"cpi\" data for each date\ncount_missing_stores = pivot_missing_cpi.sum(axis = 1)\n\n# printing the value counts to be sure that all values will be either 0 (no missing data for all stores) \n# or 45 (all stores missing cpi data)\nprint(count_missing_stores.value_counts())\ncount_missing_stores.tail(15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From above, we can conclude that all stores misses \"cpi\" and \"unemployment\" data from 2013-05-03 onward. If we want to use \"cpi\" and \"unemployment\" as features to predict future sales, the lack of data in the most recent dates can be quite troublesome. For instance, it is likely that the train file contains older sales and the test files contain more recent sales. If that is true, all \"cpi\" and \"unemployment\" missing data will occur in the test file and no missing data will be present in the train file. \n\nAs a consequence, if we want to train and test our model using \"cpi\" and \"unemployment\" data as features, we will have to manually fill the missing values, all of which are present in the test file. Hence, our model will likely perform far worse on the test set in comparison to the training set.\n\nWe have to verify if all missing values for \"cpi\" and \"unemployment\" occur on dates that are present exclusively in the test dataframe.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking how many times \"cpi\" and \"unemployment\" will be missing in each dataframe, after merging with features\nmissing_cpi_train = train[\"date\"].isin(missing_dates).sum()\nmissing_cpi_test = test[\"date\"].isin(missing_dates).sum()\n\nprint(missing_cpi_train, missing_cpi_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Every \"cpi\" and \"unemployment\" missing data will be present on the test dataframe and none on the train dataframe. About one third of the 115064 rows of the test dataframe (38162 rows) will have missing values for \"cpi\" and \"unemployment\" columns. For now, we will fill the missing values with the value from the closest date. But we must remember that dropping these columns entirely may be the best idea.\n\nThe last date with data for \"cpi\" and \"unemployment\" columns were in 2013-04-26. The value from that date will be used to fill the missing values.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#dropping \"CPI_isnull\" column that was used specifically for our previous sanity check\nfeatures = features.drop(\"cpi_isnull\", axis = 1)\n\n#slicing the dataframe with the date of 2013-04-26\nfeatures_2013_04_26 = features[features[\"date\"] == \"2013-04-26\"]\n\n#filling the CPI and Unemployment missing values with the data from the day 2013-04-26 for each store\nfor store in range(1, 46):\n    \n    #values to be used to fill\n    cpi_value =  features_2013_04_26[features_2013_04_26[\"store\"] == store][\"cpi\"].iloc[0]\n    unemployment_value =  features_2013_04_26[features_2013_04_26[\"store\"] == store][\"unemployment\"].iloc[0]\n    \n    #filling the missing values\n    indexes = features[(features[\"store\"] == store) & features[\"cpi\"].isnull()].index\n    features.loc[indexes, \"cpi\"] = cpi_value\n    features.loc[indexes, \"unemployment\"] = unemployment_value","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, we will plot \"cpi\" and \"unemployment\" columns again to see if the data in the last weeks of the time series are now present in the charts and to check if the values seem far from reality.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cpifig_time, axs_time = plt.subplots(2, 2, figsize = (15, 15))\n    \nplot_feature(axs_time[0, 0], features, \"cpi\", store = 1)\nplot_feature(axs_time[1, 0], features, \"unemployment\", store = 1)\nplot_feature(axs_time[0, 1], features, \"cpi\")\nplot_feature(axs_time[1, 1], features, \"unemployment\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using the last known value to fill the missing values seem to be an appropriate aproximation.\n\n## Dealing with \"markdown\" missing values\n\nNow that we have dealt with \"cpi and \"unemployment\" columns, we will deal the remaining five \"markdown\" columns, that have much more missing data than \"cpi\" and \"unemployment\".","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#markdown column names\nmark_cols = [\"markdown{}\".format(count) for count in range(1,6)]\n\n#percentage of missing values\nfeatures[mark_cols].isnull().sum() / len(features[mark_cols])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"More than half of all data is missing. We could drop these columns entirely or try to fill it with a meaningful value. Since \"markdown\" is likely to have an influcente on sales, we will keep these columns for now.\n\nFrom the \"Data Description\" section from Kaggle, the following is explained:\n\n\"MarkDown data is only available after Nov 2011, and is not available for all stores all the time. Any missing value is marked with an NA.\"\n\nFrom that, we can infer that \"markdown\" is probably a value different from zero when data is not available. Since we cannot be sure if NA data is zero or an unknown value different from zero, we will try both approaches and keep the one that gives us the best results.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Filling with zero approach\n\nThe first approach will be to fill all missing data with zeroes.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"features_zero = features.copy()\nfeatures_zero[mark_cols] = features[mark_cols].fillna(0).copy()\nfeatures_zero","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features_zero.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Filling with mean approach\n\nThe second approach will be to fill all missing data with the mean value of the column. From Kaggles's Data Description:\n\n\"...Walmart runs several promotional markdown events throughout the year. These markdowns precede prominent holidays, the four largest of which are the Super Bowl, Labor Day, Thanksgiving, and Christmas.\"\n\nThus, \"markdown\" values may be signicantly different depending if it is a holiday week or not. With this in mind, a better approach would be to calculate two means for each \"markdown\" column. One mean for holiday week and another for not holiday week.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#calculating the mean of \"markdown\" columns\nmarkdown_holiday = features.groupby(\"is_holiday\")[mark_cols].mean()\nmarkdown_holiday","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As expected, \"markdown\" are quite different during holiday season. We will fill with different values for holiday weeks and non holiday weeks.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#identifying the index of holiday rows and non holiday rows\nfeatures_holiday_index = features[features[\"is_holiday\"]].index\nfeatures_not_holiday_index = features[~features[\"is_holiday\"]].index\n\nfeatures_mean = features.copy()\n\n#filling with the appropriate mean\nfeatures_mean.loc[features_holiday_index, mark_cols] = (features_mean.loc[features_holiday_index, \n                                                                          mark_cols\n                                                                         ].fillna(markdown_holiday.iloc[1])) # Holiday\nfeatures_mean.loc[features_not_holiday_index, mark_cols] = (features_mean.loc[features_not_holiday_index, \n                                                                              mark_cols\n                                                                             ].fillna(markdown_holiday.iloc[0])) # Non Holiday\nfeatures_mean","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features_mean.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Merging Data\n\nAll features we treated so far are related to the stores and none are related to the department. Hence, our machine learning model will at first predict the sales at each date for each store. The prediction of sales of the department will be done by calculating the representativity that each department has in each store.\n\nTherefore, the first step we will take before merging the data, is group the sales of the train dataframe and the test dataframe by date and by store. Only then we will merge the features with the train and test dataframe. Finally, the boolean column \"is_holiday\" will be converted to 1 and 0.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#merging features with stores dataframes\nfeatures_zero = pd.merge(features_zero, stores, how = \"left\", on = [\"store\"])\nfeatures_mean = pd.merge(features_mean, stores, how = \"left\", on = [\"store\"])\n\n#function to group train and test dataframes\ndef group_dataframe(dataframe, is_train = True):\n    #grouping dataframe\n    grouped = dataframe.groupby([\"store\", \"date\"]).agg([\"sum\",\"mean\"])\n    \n    if is_train:\n        #selecting the columns with the sum of sales and the mean of is_holiday\n        grouped = grouped.iloc[:,[2,5]]\n\n        #renaming the columns\n        grouped.columns = [\"weekly_sales\", \"is_holiday\"]\n    \n    else:\n        #selecting the column with the mean of is_holiday\n        grouped = grouped.iloc[:,3]\n\n        #drop column level\n        grouped.name = \"is_holiday\"\n        \n    #return dataframe with indexes reset\n    return grouped.reset_index()\n\n# merging features with train and test dataframes. We are leaving \"is_holiday\" column from features\n# out to avoid duplicating columns\nfeature_df_cols = ['store', 'date', 'temperature', 'fuel_price', 'markdown1', 'markdown2',\n                   'markdown3', 'markdown4', 'markdown5', 'cpi', 'unemployment', 'type', 'size']\n\ngrouped_train = group_dataframe(train, True)\ngrouped_test = group_dataframe(test, False)\n\ntrain_zero = pd.merge(grouped_train, features_zero[feature_df_cols], how = \"left\", on = [\"store\", \"date\"])\ntrain_mean = pd.merge(grouped_train, features_mean[feature_df_cols], how = \"left\", on = [\"store\", \"date\"])\n\n#Converting boolean to zero or one\ntrain_zero[\"is_holiday\"] = train_zero[\"is_holiday\"].astype(int)\ntrain_mean[\"is_holiday\"] = train_mean[\"is_holiday\"].astype(int)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploratory analysis\n\nNow we will investigate what features seem to influence the sales. We will use train_zero dataframe in our analysis instead of using both, train_zero and train_mean because they are the same, except for \"markdown\" columns. When investigating \"markdown\" columns we will use both to see how they differ.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating exploratory dataframe\nexploratory = train_zero.copy()\n\n#function to create time related columns\ndef add_time_columns(dataframe):\n    #add time related columns\n    dataframe[\"year\"] = dataframe[\"date\"].dt.year\n    dataframe[\"month\"] = dataframe[\"date\"].dt.month\n    dataframe[\"year_month\"] = dataframe[\"date\"].dt.to_period(\"M\")\n    dataframe[\"week\"] = dataframe[\"date\"].dt.week\n    \n    return dataframe\n\n#creating dummy function to explore categorical columns\ndef add_dummy(column_name, dataframe):\n    # add dummy columns\n    dummy_df = pd.get_dummies(dataframe[column_name], prefix = column_name)\n    dataframe = pd.concat([dataframe, dummy_df], axis = 1)\n    return dataframe\n\n#adding time related columns and dummy columns to dataframes\nexploratory = add_time_columns(exploratory)\nexploratory = add_dummy('type', exploratory)\n\nexploratory.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Firstly, we will evaluate how sales change monthly to see if we can find any pattern.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#monthly sales\nexploratory.groupby(\"year_month\")[\"weekly_sales\"].sum().plot()\nplt.title(\"Monthly Sales\")\nplt.ylabel(\"Revenue\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Clearly, sales are much higher in December in comparison to other months. We will zoom in and plot the dates (weeks) in the x axis to see if this peak happens throught the month or is centered in one week.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#weekly sales\nexploratory.groupby(\"date\")[\"weekly_sales\"].sum().plot()\nplt.title(\"Weekly Sales\")\nplt.ylabel(\"Revenue\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems like sales are higher throughout December and peaks in one week of December. To see if \"is_holiday\" is a useful column to be a feature of our model, we will plot a vertical line for each of these dates and see if they match with the peaks.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#list of holidays from the exploratory set\nholiday_weeks = exploratory.query(\"is_holiday == 1\")[\"date\"].unique()\n\n#weekly sales\nexploratory.groupby(\"date\")[\"weekly_sales\"].sum().plot()\nplt.title(\"Weekly Sales\")\nplt.ylabel(\"Revenue\")\n\n#plot holidays\nfor holiday in holiday_weeks: \n    plt.axvline(holiday, c = \"red\", lw = 0.5 )\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Surprisingly, only Thanksgiving (end of november) is matching a representative peak. The other holidays are not matching a peak. December sales are higher before Christmas week holiday. Hence, \"is_holiday\" column must not correlate well with weekly sales.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#correlation\ncorrelation_table = np.abs(exploratory.corr()[\"weekly_sales\"]).sort_values(ascending = False)\n\n#removing weekly_sales\ncorrelation_table = correlation_table.drop(\"weekly_sales\")\n\nsns.barplot(correlation_table.values, correlation_table.index, orient = \"h\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\"is_holiday\" columns correlates very poorly with \"weekly_sales\". Moreover, it seems that many other columns does not correlate strongly either. The size of the store and its type are the most representative indicators of the sale potential of a store. \n\nHowever, as we saw previously, there is some sazonality in sales and we have no feature to capture that information. Instead of using \"is_holiday\" as a feature to capture sazonality in sales, we will create one column that fits those peaks in sales better. Firstly, we will create one table to see if the strong sales occur in the same week of the year, every year.","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"pd.options.display.max_rows = 150\n#table to see in what week of the year sales are the strongest\nexploratory.groupby(\"date\")[\"weekly_sales\",\"is_holiday\", \"week\"].agg([\"sum\", \"mean\"]).iloc[:, [0,3,5]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Sales are the strongest in weeks 47, 49, 50 and 51, every year. We will create one column called \"is_strong_sales\" to identify these strong sales weeks. This new feature will probably correlate better with \"weekly_sales\" than \"is_holiday\".","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating new column\nexploratory[\"is_strong_sales\"] = 0\n\n# it is 1 if weeks are 47, 49, 50 or 51\nstrong_weeks = [47, 49, 50, 51]\n\nexploratory[\"is_strong_sales\"] = exploratory[\"is_strong_sales\"].mask(exploratory[\"week\"].isin(strong_weeks), 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will plot again our correlation table to see how well our new column perform.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#correlation\ncorrelation_table = np.abs(exploratory.corr()[\"weekly_sales\"]).sort_values(ascending = False)\n\n#removing weekly_sales\ncorrelation_table = correlation_table.drop(\"weekly_sales\")\n\nsns.barplot(correlation_table.values, correlation_table.index, orient = \"h\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we have one new column to prepare our model to sazonality effects. From the figure above, we can identify what is likely our best features so far: \"size\", \"type\", \"is_strong_sales\" and \"markdown\". Now, we must verify if markdown is a better feature by filling N/A with zeroes or by filling with mean values.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#changing the name of exploratory dataframe for standardization\nexploratory_zero = exploratory\n\n#Creating exploratory dataframe using train_mean instead of train_zero\nexploratory_mean = train_mean.copy()\n\n#adding time related columns and dummy columns to dataframes\nexploratory_mean = add_time_columns(exploratory_mean)\nexploratory_mean = add_dummy('type', exploratory_mean)\n\n#creating new column\nexploratory_mean[\"is_strong_sales\"] = 0\n\n# it is 1 if weeks are 47, 49, 50 or 51\nexploratory_mean[\"is_strong_sales\"] = exploratory_mean[\"is_strong_sales\"].mask(exploratory_mean[\"week\"].isin(strong_weeks), 1)\n\nexploratory_mean.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#correlation\ncorrelation_table_mean = np.abs(exploratory_mean.corr()[\"weekly_sales\"]).sort_values(ascending = False)\n\n#removing weekly_sales\ncorrelation_table_mean = correlation_table_mean.drop(\"weekly_sales\")\n\nsns.barplot(correlation_table_mean.values, correlation_table_mean.index, orient = \"h\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using train_mean instead of train_zero, the correlation o \"markdown5\" and \"markdown1\" with weekly_sales increased while the correlation of \"markdown2\", \"markdown3\" and \"markdown4\" decreased. However, the difference was small and we will double check by validating this with a Machine Learning model.\n\n## Using Machine Learning to choose fill N/A method\n\nWe must decide which version of the exploratory file we will use. The zero version or the mean version. For that, we will use both in K-Nearest Neighbors and see which of them perform better.\n\nFirstly, we will normalize the data in order to avoid unnintentionally weighting the features.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def normalize(list_of_columns, dataframe):\n    #normalize dataframe making it range from 0 to 1\n    normal_df = ((dataframe[list_of_columns] - dataframe[list_of_columns].min()) / \n                 (dataframe[list_of_columns].max() - dataframe[list_of_columns].min()))\n\n    return normal_df\n\n#features\nnumerical_cols = ['is_holiday', 'temperature', 'fuel_price', 'markdown1', 'markdown2', 'markdown3', 'markdown4', 'markdown5', \n                  'cpi', 'unemployment', 'size', 'type_A', 'type_B', 'type_C', 'is_strong_sales', 'week', 'month']\n\nnormal_train_zero = normalize(numerical_cols, exploratory_zero)\nnormal_train_mean = normalize(numerical_cols, exploratory_mean)\n\ntarget = exploratory_zero[\"weekly_sales\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will shuffle the rows, cross validate 10-fold and calculate the root mean square error using K-Nearest Neighbors. Each dataframe version (mean or zero) will be repeated twice, once using all features and once using the top 6 features: \"size\", \"type_A\", \"type_B\", \"markdown1\", \"markdown5\" and \"is_strong_sales\". Note that \"store\" was not considered a feature because its number is simply an ID and \"type_C\" was not selected as one of the best features because it is collinear with \"type_A\" and \"type_B\".","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# importing models\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.model_selection import KFold, cross_val_score\n\n#top 6 features\ntop_features = [\"size\", \"type_A\", \"type_B\", \"markdown1\", \"markdown5\", \"is_strong_sales\"]\n\n#models\nknn = KNeighborsRegressor()\nkf = KFold(10, shuffle = True)\n\n#root_mean_squared_error\nknn_zero_all_features_rmse = (-np.mean(cross_val_score(knn, normal_train_zero, target, cv = kf, \n                                                 scoring = \"neg_mean_squared_error\")))**(1/2)\nknn_zero_six_features_rmse = (-np.mean(cross_val_score(knn, normal_train_zero[top_features], target, cv = kf, \n                                                 scoring = \"neg_mean_squared_error\")))**(1/2)\n\nknn_mean_all_features_rmse = (-np.mean(cross_val_score(knn, normal_train_mean, target, cv = kf, \n                                                 scoring = \"neg_mean_squared_error\")))**(1/2)\nknn_mean_six_features_rmse = (-np.mean(cross_val_score(knn, normal_train_mean[top_features], target, cv = kf, \n                                                 scoring = \"neg_mean_squared_error\")))**(1/2)\n\n#zero dataframe\nprint(\"zero version error:\", knn_zero_all_features_rmse, knn_zero_six_features_rmse)\n\n#mean dataframe\nprint(\"mean version error:\", knn_mean_all_features_rmse, knn_mean_six_features_rmse)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As the zero version tend to be slightly better, we will use only it from now on.\n\n## Dummy regressor\n\nOur model must perform better than a minimum benchmark. We will predict the mean value for every store and calculate the error, which will be considered our minimum benchmark. From now on, we will calculate the Weighted Mean Absolute Error, which is the official metric of this challange. The error formula is available in:\n\nhttps://www.kaggle.com/c/walmart-recruiting-store-sales-forecasting/overview/evaluation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Importing dummy regressor and cross_val_predict\nfrom sklearn.dummy import DummyRegressor\nfrom sklearn.model_selection import cross_val_predict\n\n#model\ndr = DummyRegressor()\nkf = KFold(10, shuffle = True)\n\n#predictions\ndummy_predictions = cross_val_predict(dr, normal_train_zero, target, cv = kf)\n\n#function to calculate the Weighted Mean Absolute Error\ndef wmae(predictions, correct_value, is_holiday_column):\n    #size of the series/vector\n    size = len(correct_value)\n\n    #creating series object with weights set to 1\n    weights = pd.Series(np.ones(size), index = correct_value.index).astype(int)\n\n    #changing weights to 5 when it is holiday \n    weights = weights.mask(is_holiday_column == 1, 5)\n\n    #error metric\n    wmae_value = (np.abs(correct_value - predictions) * weights).sum() / weights.sum()\n    \n    return wmae_value\n\n#calculating the error\ndummy_wmae = wmae(dummy_predictions, target, normal_train_zero[\"is_holiday\"])\n\nprint(\"dummy error:\", dummy_wmae)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Predicting sales using Machine Learning\nWe will use \"Random Forests\" model to predict the sales of each store because it can handle well non-linearities in our data and tend to overfit less than \"Decision Trees\". Firstly, we will select the best features to the model. Secondly, we will choose the best parameters by using \"Grid Search\".\n\nFinally, in a next section we will use the sales values we predicted and proportionally distribute them depending on how relevant each department is for each store. With this distribution we can calculate the error of our model.\n\n## Random Forests - Feature Selection\n\nWe will start by selecting the best features for our model. From numerical cols, we have the following available features:\n```python\nnumerical_cols = ['is_holiday', 'temperature', 'fuel_price', 'markdown1', 'markdown2', 'markdown3', 'markdown4', \n                  'markdown5', 'cpi', 'unemployment', 'size', 'type_A', 'type_B', 'type_C', 'is_strong_sales', 'week', \n                  'month']\n```\nWe will use recursive feature elimination in order to optimize the feature selection. Since we are not fixing a random state, the selected features vary. We will run the feature selection 10 times and select the most recurring features.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#importing libraries\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.feature_selection import RFECV\n\n#function to select the best features by recursive feature elimination\ndef select_features(X_train, y_train):\n    #recursive feature elimination in random forests, 10 fold, shuffling rows\n    rfr = RandomForestRegressor(n_estimators = 10)\n    kf = KFold(10, shuffle = True)\n    \n    #fitting recursive feature elimination model\n    selector = RFECV(rfr, cv = kf)\n    selector.fit(X_train,y_train)\n    \n    #best features\n    best_columns = list(X_train.columns[selector.support_])\n    \n    return best_columns\n\n#assigning features and target\nX_train = normal_train_zero\ny_train = target\n\n#dictionary to hold 10 features selection by recursive feature elimination\ndic_features = {}\n\nfor count in range(10):\n    features = select_features(X_train, y_train)\n    dic_features[count] = features","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Resulting dictionary\n\nI saved the dictionary in order to not need to run the code multiple times, since it takes time.\nNow we will count how many times each feature occured and select those that appered often.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#resulting dictionary\ndic_features = {\n                    0: ['temperature', 'fuel_price', 'markdown3', 'cpi', 'unemployment', 'size', 'type_A', 'type_B', \n                        'is_strong_sales', 'week'],\n\n                    1: ['cpi', 'unemployment', 'size', 'is_strong_sales', 'week'],\n\n                    2: ['temperature', 'fuel_price', 'markdown3', 'markdown4', 'cpi', 'unemployment', 'size', 'type_A', \n                        'type_B', 'is_strong_sales', 'week', 'month'],\n\n                    3: ['temperature', 'fuel_price', 'markdown3', 'markdown4', 'cpi', 'unemployment', 'size', 'type_A', \n                        'type_B', 'is_strong_sales', 'week', 'month'],\n\n                    4: ['cpi', 'unemployment', 'size', 'is_strong_sales', 'week'],\n\n                    5: ['cpi', 'unemployment', 'size', 'type_A', 'is_strong_sales', 'week'],\n\n                    6: ['temperature', 'fuel_price', 'markdown3', 'markdown4', 'cpi', 'unemployment', 'size', 'type_A', \n                        'type_B', 'type_C', 'is_strong_sales', 'week', 'month'],\n\n                    7: ['cpi', 'unemployment', 'size', 'is_strong_sales', 'week'],\n\n                    8: ['cpi', 'unemployment', 'size', 'is_strong_sales', 'week'],\n\n                    9: ['temperature', 'fuel_price', 'markdown1', 'markdown3', 'markdown4', 'markdown5', 'cpi', \n                        'unemployment', 'size', 'type_A', 'type_B', 'type_C', 'is_strong_sales', 'week', 'month']\n                }\n\n# one big list to hold features off all runs\nfeatures_list = []\n\nfor run in dic_features:\n    #adding each run to the list\n    features_list += dic_features[run]\n\n#counting how often each feature occured\npd.Series(features_list).value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Every feature that appeared four times or more will be selected. The other will be discarded to avoid overfitting of the model.\nWe already discussed collinearity issues with type_C, so it is no surprise that it was not selected. Markdown 5 and Markdown 1 may also not be important to predict sales. Other columns that did not appear, for instance, \"is_holiday\" may not be a good indicator of sales. The column we created \"is_strong_sales\" and \"week\" may be better than \"is_holiday\" to describe sazonality.\n\n## Tuning Random Forests\nHaving selected the features, now we can select the best performing hyperparameters of the Random Forests model. Again we will run the parameter optimization agorithm 10 times without setting a random_state in order to see the most recurring parameters.\nThe ones most recurring configuration will be the one we will adopt.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#selected features\nfeatures_list = ['temperature', 'fuel_price', 'markdown3', 'markdown4', 'cpi', 'unemployment', 'size', 'type_A', \n                 'type_B', 'is_strong_sales', 'week', 'month']\n\n#importing grid search for model tuning\nfrom sklearn.model_selection import GridSearchCV\n\n#importing again to be able to run this cell even if feature selection cell was not run\nfrom sklearn.ensemble import RandomForestRegressor\n\n#assigning again to be able to run this cell even if feature selection cell was not run\nX_train = normal_train_zero\ny_train = target\n\n#function to select the best parameters of Random Forests\ndef select_hyperparams(features_columns, X_train, y_train):\n    \n    hyperparameters = {\n                        \"n_estimators\": [10],\n                        \"max_depth\": [None, 8, 13, 18],\n                        \"min_samples_leaf\": [1, 4],\n                        \"min_samples_split\": [2, 4, 5, 6, 7]\n                      }\n    \n    rfr = RandomForestRegressor(n_jobs = 4)\n    kf = KFold(10, shuffle = True)\n\n    grid = GridSearchCV(rfr, param_grid = hyperparameters, cv = kf)\n    grid.fit(X_train[features_columns], y_train)\n    best_params = grid.best_params_\n    best_score = grid.best_score_\n    return best_params, best_score\n\n#dictionaries to hold 10 hyperparameters selection and scores\ndic_params = {}\ndic_score = {}\n\nfor count in range(10):\n    best_params, best_score = select_hyperparams(features_list, X_train, y_train)\n    dic_params[count] = best_params\n    dic_score[count] = best_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Resulting dictionary\n\nI saved the dictionary in order to not need to run the code multiple times, since it takes time.\nNow we will see what configuration ocurred most often.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dic_params = {\n                0: {'max_depth': 13, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 10},\n                1: {'max_depth': 18, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 10},\n                2: {'max_depth': 18, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 10},\n                3: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 4, 'n_estimators': 10},\n                4: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 10},\n                5: {'max_depth': 18, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 10},\n                6: {'max_depth': 13, 'min_samples_leaf': 1, 'min_samples_split': 4, 'n_estimators': 10},\n                7: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 10},\n                8: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 4, 'n_estimators': 10},\n                9: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 4, 'n_estimators': 10}\n             }","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The most recurring configuration was max_depth = None, min_sample_leaf = 1 and min_sample_split = 4. Despite the fact we ran the model with 10 estimators, we know that increasing the number of estimators makes the model overfit less. However, this benefit has diminishing returns in expense of time complexity. Hence, we will use 100 estimators.\n\n## Predicting using Random Forests\n\nAfter selecting the features and tuning the Random Forests model, we can finally make predictions.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#importing again to be able to run this cell even if previous cells were not run\nfrom sklearn.ensemble import RandomForestRegressor\n\n#assigning again to be able to run this cell even if previous cells were not run\nX_train = normal_train_zero\ny_train = target\n\nrfr = RandomForestRegressor(n_estimators = 100, max_depth = None, min_samples_leaf = 1, min_samples_split = 4)\nkf = KFold(10, shuffle = True)\n\nrfr_predictions = cross_val_predict(rfr, X_train[features_list], y_train, cv = kf)\nwmae(rfr_predictions, y_train, X_train[\"is_holiday\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is the prediction error of the weekly sales of each store. It is much better than our dummy prediction (wmae: 479000) and it is also better than our k-nearest prediction (wmae: 189000), granted it was not hyperparamer and feature optimized. These error values may vary due to cross validation shuffling.\n\nNext, we need to translate these store predictions to department predictions. We can create new features to each department, and build a whole new model to predict sales of each department or tackle the issue with a simpler solution, like proportionally distributing the predictions of each store considering how representative the department is in that store, for that week of the year.\n\nFor now, we will adopt the latter solution and see if our error is within a desired range.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# creating train dataframe with week column\nreference_train = train.copy()\nreference_train[\"week\"] = reference_train[\"date\"].dt.week\n\n#pivot_table with weekly sales by store and by department\ndept_reference_train = reference_train.pivot_table(\"weekly_sales\", [\"store\", \"week\"], \"dept\", aggfunc = np.sum, fill_value = 0)\n\n#weekly sales by store\nstore_weekly_sales = dept_reference_train.sum(axis = 1)\n\n#finding how representative each department is within a store at a certain week of the year\nproportion_sales = dept_reference_train.div(store_weekly_sales, axis = 0)\n\nproportion_sales.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will create a function to check this table and find how representative a department is in a store and use the apply method on both train and test dataframes. By doing this, we will create a new column with the proportions.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#function that returns the proportion of each department of each store\ndef proportion_by_dept(row):\n    \n    return proportion_sales.loc[(row[\"store\"], row[\"week\"]), row[\"dept\"]]\n\n#copying the train dataframe \ntrain_predictions = train.copy()\n\n#creating the proportion column\ntrain_predictions[\"proportion\"] = reference_train.apply(proportion_by_dept, axis = 1)\ntrain_predictions.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we need to concatenate the values predicted by our model into the grouped_train dataframe, which is the train dataframe but with the weekly_sales values grouped by store.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# adding predictions column to the grouped train dataframe\ngrouped_train_predictions = pd.concat([grouped_train, pd.Series(rfr_predictions)], axis = 1)\ngrouped_train_predictions = grouped_train_predictions.rename(columns = {0:\"store_predictions\"})\ngrouped_train_predictions.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, this dataframe will be merged into train predictions dataframe, in order to bring the store predictions. The values from this column will be multiplied by the values in the proportion column in order to calculate the prediction value of each department of each store.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# adding store predictions column to the train dataframe\ntrain_predictions = pd.merge(train_predictions, grouped_train_predictions[[\"store\", \"date\", \"store_predictions\"]],\n                             on = [\"store\", \"date\"], how = \"left\")\n\n# predict department sales based on the relevance of the deparment in each store\ntrain_predictions[\"predicted_department_sales\"] = train_predictions[\"proportion\"] * train_predictions[\"store_predictions\"]\ntrain_predictions.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The same function used to calculate the error of the grouped train dataframe can be used to calculate the error for each deparment. Since the sales by department is much lower than the sales by store, the error is also much lower.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#weighted error of the department sales predictions\ntrain_wmae = wmae(train_predictions[\"predicted_department_sales\"], train_predictions[\"weekly_sales\"], \n                  train_predictions[\"is_holiday\"])\n\ntrain_wmae","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we will set another minimum benchmark to see how our model compares.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dummy model\ndr = DummyRegressor()\nkf = KFold(10, shuffle = True)\n\n#predictions\ndummy_predictions = cross_val_predict(dr, train[\"is_holiday\"], train[\"weekly_sales\"], cv = kf)\n\n#weighted error of the department sales using the dummy model\ndummy_wmae = wmae(dummy_predictions, train[\"weekly_sales\"], train[\"is_holiday\"])\ndummy_wmae","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our model seems to be significantly better than the dummy benchmark. We well create a dashboard to explore how close our predictions are to the real data.","execution_count":null},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"def plot_results(axes_object, grouped_dataframe, store_id):\n    \n    #plot single store\n    dataframe_mask = grouped_dataframe[\"store\"] == store_id\n\n    hue = None #only one line will be plotted\n\n    title = \"Store {} - prediction vs real data\".format(store_id)\n\n    #plotting target data\n    sns.lineplot(grouped_dataframe[dataframe_mask][\"date\"], grouped_dataframe[dataframe_mask][\"weekly_sales\"], hue = hue,\n                 ax = axes_object, color = \"blue\")\n    \n    #plotting random forests predictions\n    sns.lineplot(grouped_dataframe[dataframe_mask][\"date\"], grouped_dataframe[dataframe_mask][\"store_predictions\"], hue = hue,\n                 ax = axes_object, color = \"green\")\n\n    #setup\n    axes_object.set_title(title)\n    axes_object.set_xlabel(\"Date\")\n    axes_object.set_ylabel(\"Sales value\")\n    axes_object.legend([\"Weekly Sales\", \"Predicted Value\"])\n\n    #rotating the ticks\n    plt.setp(axes_object.xaxis.get_majorticklabels(), rotation=45)\n    plt.tight_layout()\n\ndef create_dashboard(rows, columns, store_list, grouped_dataframe):\n    \n    #creating dashboard with rows x columns graphs\n    fig, axs = plt.subplots(rows, columns, figsize = (18, 18))\n\n    #plotting one graph for each store\n    for count, store_id in enumerate(store_list):\n        row = int(count / columns)\n        column = count % columns\n        plot_results(axs[row, column], grouped_dataframe, store_id)\n\n    plt.show()\n\n#creating a 3x3 dashboard with stores from 1 to 9\ncreate_dashboard(3, 3, range(1,10), grouped_train_predictions)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our predictions seem to be tracking real world data very well. Now we will prepare our test file to submit it to kaggle.\n\n## Preparing the test file\n\nWe have to prepare our test file the same way we did with the train file. Firstly, we will add the features to the test dataframe.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating dataframe with all features\ntest_zero = pd.merge(grouped_test, features_zero[feature_df_cols], how = \"left\", on = [\"store\", \"date\"])\n\n#Converting boolean to zero or one\ntest_zero[\"is_holiday\"] = test_zero[\"is_holiday\"].astype(int)\n\n#adding time related columns and dummy columns to dataframes\ntest_zero = add_time_columns(test_zero)\ntest_zero = add_dummy('type', test_zero)\n\n#creating new column \"is_strong_sales\"\ntest_zero[\"is_strong_sales\"] = 0\ntest_zero[\"is_strong_sales\"] = test_zero[\"is_strong_sales\"].mask(test_zero[\"week\"].isin(strong_weeks), 1)\n\ntest_zero.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Secondly, the numeric features will be normalized.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#normalizing numerical columns\nnormal_test = normalize(numerical_cols, test_zero)\nnormal_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, we are ready to predict the sales in the test file using Random Forests algorithm.\n\n## Predicting sales from the test file\n\nNow we will fit our whole data into the model and predict the weekly sales for each store. After that, we will predict the sales for each department the same way we did with the train dataframe.\n\nThen we are ready to create our submission file.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#assigning again to be able to run this cell even if previous cells were not run\nX_train = normal_train_zero\ny_train = target\nX_test = normal_test\n\n#model with the optmized hyperparameters \nrfr_test = RandomForestRegressor(n_estimators = 100, max_depth = None, min_samples_leaf = 1, min_samples_split = 4)\n\n#fitting the model with our selected features \nrfr_test.fit(X_train[features_list], y_train)\n\n#predicting the test file with our selected features\nrfr_test_predictions = rfr_test.predict(X_test[features_list])\nrfr_test_predictions","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Repeating the same steps from before to predict the sales by department.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#test file with predictions\ntest_predictions = test.copy()\n\n#adding week column\ntest_predictions[\"week\"] = test_predictions[\"date\"].dt.week\n\n#creating the proportion column\ntest_predictions[\"proportion\"] = test_predictions.apply(proportion_by_dept, axis = 1)\n\n# adding predictions column to the grouped test dataframe\ngrouped_test_predictions = pd.concat([grouped_test, pd.Series(rfr_test_predictions)], axis = 1)\ngrouped_test_predictions = grouped_test_predictions.rename(columns = {0:\"store_predictions\"})\n\n# adding store predictions column to the test dataframe\ntest_predictions = pd.merge(test_predictions, grouped_test_predictions[[\"store\", \"date\", \"store_predictions\"]],\n                             on = [\"store\", \"date\"], how = \"left\")\n\n# predict department sales based on the relevance of the deparment in each store\ntest_predictions[\"predicted_department_sales\"] = test_predictions[\"proportion\"] * test_predictions[\"store_predictions\"]\ntest_predictions.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The \"Id\" column and the submission file in the correct format will be created in the next cell.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating Id column\ntest_predictions[['store', 'dept', 'date']] = test_predictions[['store', 'dept', 'date']].astype(str)\ntest_predictions['Id'] = test_predictions[['store', 'dept', 'date']].agg('_'.join, axis=1)\n\n#creating submission file\nmy_sample = test_predictions[[\"Id\", \"predicted_department_sales\"]].copy()\nmy_sample = my_sample.rename(columns = {\"predicted_department_sales\" : \"Weekly_Sales\"})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#saving csv file\nmy_sample.to_csv(\"submission.csv\", index = False)\nmy_sample.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conclusion\n\nThe main goal of this project was to predict the weekly sales of each department of each Walmart store and we have taken lots of steps in order to do that. We started by filling \"not a number values\" with values that seemed to be consistent with real values. Then we engineered new time related features and one feature to identify strong sales periods based on what we learned by exploring the data. After that, we noticed that all features were related to the stores and not to the department of each store. With that in mind, we created a Random Forests model to predict sales of each store in each week. That value was distributed over each department considering how representative they were in the store at a specific time o the year.\n\nWith one single optmized model we were able to achieve a relatively low weighted mean absolute error (Train data: 1409, Kaggle submission: 3448). For further lowering our error we could create new features from existing ones (bining temperature, cpi and unemplyment rate, for instance), we could try different algorithms that were not used (K-Nearest Neighbors, Linear Regression, Neural Networks), optimize the features for them and select the best hyperparameters, finally we could broaden the range of our feature \"is_strong_sales\" in order to reduce overfitting (the current configuration might be too specific for tha train data).\n\nThere are many things to do to improve the model even more. It was a long way in these seven days but plenty was achieved!\n\nThank you for reading!\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}