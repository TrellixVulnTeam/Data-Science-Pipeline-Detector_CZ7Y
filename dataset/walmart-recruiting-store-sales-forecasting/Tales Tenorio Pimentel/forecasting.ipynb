{"cells":[{"metadata":{},"cell_type":"markdown","source":"# <font color=\"navy\">Walmart Recruiting Store Sales Forecasting</font>\nhttps://www.kaggle.com/c/walmart-recruiting-store-sales-forecasting","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Future Sales Forecasting Problem\n### Introduction\nTrying to predict future sales is often realized as a Time Series <br>\nforecasting problem where the seek for Trend and Seasonality are <br>\nthe main tasks. Knowing both of these elements with the assumption <br>\nof steady external factors, it is a good starting point and models <br>\nlike Exponential Smoothing are frequently used. <br>\n<br>\nWhen seasonality is not clear enough, stochastic processes that use <br>\na fixed number of previous time states is used, such as ARIMA. <br>\nHowever this kind of forcasting is likely to propagate errors over <br>\ntime for a long unknown time window. <br>\n\n### My Strategy\nIn order to bring a new contribution, that can be ensembled with <br> \ntraditional strategies to solve this problem, I have made some <br> \nassumptions: <br>\n* Department Weekly Sales depend on the week order within the <br>\nsame month\n* The tuple Week-Month can explain human behavior, shopping is <br>\na human behavior, right?\n* The number of weeks in a month tend to repeat over the years, <br>\ndepending on how much Mondays a month have.\n* The sales variation from the same Store-Dept between Year0 and <br>\nYear1 is almost stable and can be replicated between Year1 and <br>\nYear2.\n* Store-Dept Weekly Sales is correlated to the Size of the Store the <br>\nDept is placed \n\nSo, having made these assumptions, it is reasonable to understand <br>\nthat the Sales from a specific Store-Dept in the next year can be <br>\na function of the same Store-Dept in the current year. Of course <br>\nfurther information should be add in order to improve accuracy. <br>\nTo perform this prediction, a **Linear Regression** is chosen and the <br>\nx1 representing the Weekly_Sales of Store-Dept and x2, the current <br>\nSize of the whole store.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\n\nimport my_dao\nimport time_utils\nimport pretties\nimport stats\nimport process\nimport evaluation\nimport plotter\nimport download\n\nfrom bokeh.plotting import show, output_notebook","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pretties.max_data_frame_columns()\npretties.decimal_notation()\noutput_notebook()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **The dataset**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Some semantic enrichment on data were applied in order to have more <br> \npossibilities to explore data and, eventually, turn them into <br>\nproperly machine learning features. <br>\n<br>\nIn this context, the following fields were built: <br>\n<br>\n\n### Train dataset\n<ul>\n  <li>sales_diff : a number, represents the absolute difference between current and previous week Sales</li>\n  <li>sales_diff_p : a number, represents the relative difference between current and previous week Sales</li>\n  <li>up_diff : a boolean indication whether there was a positive difference in sales_diff</li>\n</ul>\n\n<br>\n\n### Features dataset\n<ul>\n  <li>pre_holiday : a boolean indication whether there is a Holiday in the following week</li>\n  <li>pos_holiday : a boolean indication whether there was a Holiday in the previous week</li>\n  <li>celsius : a number, represents the Temperature in Celsius scale</li>\n  <li>celsius_diff : a number, represents the absolute difference between current and previous week Temperature in Celsius</li>\n  <li>week_n : a number, represents the order of the week whitin its month</li>\n  <li>month_n : a number, represents the month</li>\n    \n</ul>\n\n### The <font color=\"navy\">store_dept</font>  and <font color=\"navy\">wm_date</font> composite key\nAs mentioned in the first section, my strategy uses the tuple <br>\n(Store,Dept) and the element that explains shopping behavior, <br>\nthe tuple (Week_n,Month_n) to group Weekly Sales for each year. <br>\n\nGot confused? <br>\nIt is easy! <br>\n\nGo ahead and check help texts close to outputs. :)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Loading...","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train = my_dao.load_dataset(\"train\")\ntrain = train.groupby(\"store_dept\").apply(process.train_sales_semantic_enrichment)\n\ntest = my_dao.load_dataset(\"test\")\n\nfeat = my_dao.load_features()\nfeat = process.features_semantic_enrichment(feat)\n\nstores = my_dao.load_stores()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Merging train and test datasets with features dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.merge(feat, how=\"left\", left_on=[\"Store\", \"Date\"], right_on=[\"Store\", \"Date\"], suffixes=[\"\", \"_y\"])\ndel train[\"IsHoliday_y\"]\ndel train[\"timestamp_y\"]\ntrain = train.merge(stores, how=\"left\", left_on=[\"Store\"], right_on=[\"Store\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = test.merge(feat, how=\"left\", left_on=[\"Store\", \"Date\"], right_on=[\"Store\", \"Date\"], suffixes=[\"\", \"_y\"])\ndel test[\"IsHoliday_y\"]\ndel test[\"timestamp_y\"]\ntest = test.merge(stores, how=\"left\", left_on=[\"Store\"], right_on=[\"Store\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = ['Date', 'Store', 'Dept', 'Weekly_Sales', 'pre_holiday', 'IsHoliday', 'pos_holiday', 'Fuel_Price', \n        'CPI', 'Unemployment', 'celsius', 'datetime', 'Type', 'sales_diff', 'sales_diff_p',\n        'Size', 'Temperature', 'timestamp', 'store_dept', \"day_n\", \"week_n\", \"month_n\", \"wm_date\", \"up_diff\", \"celsius_diff\", \"year\"]\n\ntrain = train[cols]\nprint(\"Shape: {}\".format(train.shape))\ntrain.sample(6)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Train dataset time interval","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Train\\n\")\nprint(\"Initial date: {}\".format(train[\"Date\"].iloc[0]))\nprint(\"Final date  : {}\".format(train[\"Date\"].iloc[-1]))\nprint(\"Time interval (months): {}\".format(time_utils.time_interval_months(train[\"Date\"])))\nprint(\"Time interval (years) : {}\".format(time_utils.time_interval_months(train[\"Date\"]) / 12))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Test\\n\")\nprint(\"Initial date: {}\".format(test[\"Date\"].iloc[0]))\nprint(\"Final date  : {}\".format(test[\"Date\"].iloc[-1]))\nprint(\"Time interval (months): {}\".format(time_utils.time_interval_months(test[\"Date\"])))\nprint(\"Time interval (years) : {}\".format(time_utils.time_interval_months(test[\"Date\"]) / 12))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train & Validation...\npartitions","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Partition timestamp threshold\nA timestamp threshold to build **train** and **validation** partitions must be set. <br>\nAs the whole train dataset have a 32.7 months interval, I have decided to split fitting/validation into 24 months for training and the rest 8 months for validation.<br>\nThe reason is that there will be 2 entrys for each (Store,Dept,week_n,month_n) composite key for fitting stage. <br>\nFitting data is stored in use_train.<br>\nValidation data is stored in use_valid.<br>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"timestamp_threshold = time_utils.str_datetime_to_timestamp(\"2012-02-01\", \"%Y-%m-%d\") #24 months from the first entry\n\nuse_train = train[train[\"timestamp\"] <= timestamp_threshold]\nuse_valid = train[train[\"timestamp\"] > timestamp_threshold]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Fitting dataset time interval\\n\")\nprint(use_train[\"Date\"].head(1).append(use_train[\"Date\"].tail(1)))\nprint()\nprint(\"Time interval (months): {}\".format(time_utils.time_interval_months(use_train[\"Date\"])))\nprint(\"Time interval (years) : {}\".format(time_utils.time_interval_months(use_train[\"Date\"]) / 12))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Validation dataset time interval\\n\")\nprint(use_valid[\"Date\"].head(1).append(use_valid[\"Date\"].tail(1)))\nprint()\nprint(\"Time interval (months): {}\".format(time_utils.time_interval_months(use_valid[\"Date\"])))\nprint(\"Time interval (years) : {}\".format(time_utils.time_interval_months(use_valid[\"Date\"]) / 12))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Forecasting","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### <font color=\"navy\">Week-Month</font> data","execution_count":null},{"metadata":{"trusted":false},"cell_type":"markdown","source":"### Transformation into WM Data...\nThe input data for fitting stage is described some cells bellow :)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"try:\n    wm_data_train = my_dao.load_week_month_data(\"wm_data_train\")\nexcept FileNotFoundError:\n    wm_data_train = process.wm_data(use_train)\n    my_dao.save_week_month_data(wm_data_train, \"wm_data_train\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wm_data_train = process.format_wm_data_colnames(wm_data_train, \"train\")\nwm_data_train.sample(4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"try:\n    wm_data_valid = my_dao.load_week_month_data(\"wm_data_valid\")\nexcept FileNotFoundError:\n    wm_data_valid = process.wm_data(use_valid)\n    my_dao.save_week_month_data(wm_data_valid, \"wm_data_valid\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wm_data_valid = process.format_wm_data_colnames(wm_data_valid, \"valid\")\nwm_data_valid.sample(4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"markdown","source":"Now that train and validation dataset are placed as tables with <br>\n(Store, Dept, Week_n, Month_n) as its composite key (represented <br>\nby the columns store_dept and wm_date), it is time to merge them <br>\non this key. <br>\nThe **inner** merge is chosen because the sklearn.LinearRegression <br> \nfit method doesn't work with NaN values. <br>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"xy = pd.merge(wm_data_train, wm_data_valid, \n              left_on=[\"wm_date\", \"store_dept\"], right_on=[\"wm_date\", \"store_dept\"], \n              how=\"inner\", suffixes=[\"_train\", \"_valid\"])\n\nxy[\"Store\"] = xy[\"year1_sales_train\"]\nxy[\"Dept\"] = xy[\"year1_size_train\"]\nxy[\"Date\"] = xy[\"Date_train\"]\n\nprint(\"Total groups: \", len(xy.drop_duplicates([\"wm_date\", \"store_dept\"])))\ndisplay(xy.head(4))\nprint(\"Hey, look the table above and check the first two columns (store_dept and wm_date)\")\nprint(\"They mean Store 29, Dept 5, Month_n 7 and 4th week of the month.\")\nprint(\"The following columns are the fields values for each year.\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The input table description","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now that you can see the merged dataset, it is time to explain it. <br>\nAs I said before, each entry of this table represents all possible  <br>\nfield values from a (Store, Department, Week, Month). Well, even  <br>\nthough not all fields are placed there, it was designed to recieve  <br>\nthe ones you want to. <br>\nThis approach started simple, as it should be, there will be used  <br>\nonly two input columns in the fitting stage: a) the Weekly_Sales of  <br>\n(Store,Dept) at each (Week_n, Month_n) time tag and b), the size of  <br>\nthe Store at that time tag. <br>\n<br>\nSo, the Linear Regression fitting stage will be performed using  <br>\nWeekly Sales and store Size, right? But how? <br>\nRemember the train and validation partition split? Right. <br>\nThere were reserved two years for the trainning dataset, so it is  <br>\npossible to train with year0's sales and size, targeting the year1  <br>\nsales. <br>\nThen, evaluate the prediction quality with the third year (year2)  <br>\nthat was reserved for the validation stage. =) <br>\n<br>\nObs.: Unfortunately the MarkDown fields only have valid values from Nov 2011.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\nxy.plot.scatter(\"year0_sales_train\", \"year1_sales_train\", \n                title=\"scatter plot - sales year0 vs sales year1\", \n               ylim=(0, 200000), xlim=(0, 200000), alpha=0.15, figsize=(6,6))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What the cart above tell us? <br>\nDespite some outliers, the majority of the points lies close <br> \nto the 45 degree diagonal, which means that the Weekly Sales <br>\nbetween year0 and year1 tend, in general, to be close one to <br>\nanother.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\nxy.plot.scatter(\"year1_sales_train\", \"year0_sales_valid\", color=\"magenta\",\n                title=\"scatter plot - sales year1 vs sales year2\", \n               ylim=(0, 200000), xlim=(0, 200000), alpha=0.3, figsize=(6,6))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And this one in magenta color? <br>\nIt represents the same previous chart structure, but <bt>\nthis time with Weekly Sales from year1 to year2. <br>\nIn the right end of it the points seem to have a slight <br>\ncurve up, but most os the points tend to have the same <br>\nspread as the chart before.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Ok, let's move forward. <br>\nTime to revome missing data on the second year (year1_train). <br>\nThere are valid values from year0_train but not for all <br>\nyear1_train.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"NAs count on first year\")\nstats.freq(xy[\"year0_sales_train\"].isna())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"NAs count on second year\")\nstats.freq(xy[\"year1_sales_train\"].isna())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"NAs count on third year\")\nstats.freq(xy[\"year0_sales_valid\"].isna())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"not_na_xy = xy[(xy[\"year0_sales_train\"].notna()) & (xy[\"year1_sales_train\"].notna()) & (xy[\"year0_sales_valid\"].notna())]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"key_colnames = [\"Store\", \"Dept\", \"Date\"] #column names need to build submission file","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fitting_cols = [\"year0_sales_train\", \"year0_size_train\"] #first year data\nx = not_na_xy[fitting_cols] \nx.head(4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = not_na_xy[[\"year1_sales_train\"]] #first year target\ny.head(4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Fitting","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"reg = LinearRegression().fit(x, y)\nprint(reg.score(x, y))\nprint(\"Function:\")\n\nprint(\"y = {} * {} + {} * {} + {}\".format(round(reg.coef_[0][0], 3), x.columns[0], round(reg.coef_[0][1], 3), x.columns[1], round(reg.intercept_[0]), 3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Applying to Validation dataset","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Recap! <br>\nNow it's time to apply the Linear Regression function <br>\nfor the second year data to predict the next year sales.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x_valid = not_na_xy#[[\"year1_sales_train\", \"year1_size_train\"] + key_colnames + [\"year1_isholiday_train\"]]\nx_valid.head(4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = reg.predict(x_valid[fitting_cols])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\npd.DataFrame(y_pred)[0].plot.hist(title=\"Validation Predicted\")\ny_valid = not_na_xy[[\"year0_sales_valid\"]]\ny_valid.plot.hist(title=\"Validation Real\", color=\"magenta\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking to both charts above, at least the values dispersion seem to be very close.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Evaluation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x_valid[fitting_cols]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_valid = x_valid.rename({\"year0_isholiday_valid\": \"IsHoliday\", \"year0_sales_valid\": \"Weekly_Sales\"}, axis=1)\n\nsubm = evaluation.build_submission_df(test_df=x_valid[fitting_cols + key_colnames], \n                                      target_predicted=y_pred)\n\nprint(\"Validation prediction evaluation:\\n\")\nprint(evaluation.evaluate(subm, x_valid))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Test\nNow it is time to apply prediction to the test dataset.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"This colnames updating is to move time window in one year.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test_cols = [fitting_col.replace(\"0\", \"1\") for fitting_col in fitting_cols]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.sample(4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test[\"Date\"].head(1).append(test[\"Date\"].tail(1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"try:\n    wm_data_test = my_dao.load_week_month_data(\"wm_data_test\")\nexcept FileNotFoundError:\n    wm_data_test = process.wm_data(test)\n    my_dao.save_week_month_data(wm_data_test, \"wm_data_test\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wm_data_test = process.format_wm_data_colnames(wm_data_test, \"test\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"try:\n    wm_data_train_valid = my_dao.load_week_month_data(\"wm_data_train_valid\")\nexcept FileNotFoundError:\n    wm_data_train_valid = process.wm_data(train)\n    my_dao.save_week_month_data(wm_data_train_valid, \"wm_data_train_valid\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wm_data_train_valid = process.format_wm_data_colnames(wm_data_train_valid, \"train\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xy_test = pd.merge(wm_data_train_valid, wm_data_test, \n                   left_on=[\"wm_date\", \"store_dept\"], right_on=[\"wm_date\", \"store_dept\"], \n                   how=\"right\", suffixes=[\"_train\", \"_test\"])\n\nxy_test.sample(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Filling NaN values\nWhile merging both train and test Week-Month dataset, on the <br>\ncomposite key (Store, Dept, Week_n, Month_n) there were NA <br>\nvalues within Sales and Size of previous year data. <br>\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"NAs frequency over predictive columns\\n\")\nfor test_col in test_cols:\n    print(test_col)\n    print(stats.freq(xy_test[test_col].isna()))\n    print()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 10.45% of missing values that can be filled <br>\nused some ways. <br>\nMaybe we can replace all missing values from a <br>\n(Store, Dept, Week_n, Month_n) with the median of all <br>\nvalues from (Store, Dept), all Week_n and Dept_n. <br>\n<br>\nFirst question: how many (Store, Dept) with missing<br>\ndate are present with any Date?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"not_na_xy_test = xy_test[xy_test[test_cols].notna().all(1)]\nna_xy_test = xy_test[~xy_test.index.isin(not_na_xy_test.index)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stats.freq(na_xy_test[\"store_dept\"].isin(wm_data_train_valid[\"store_dept\"]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are lucky! <br>\n99.70% of all (Store, Dept, Week_n, Month_n) with <br>\nmissing data have at least one entry for valid data <br>\nfor Weekly Sales. <br>\n<br>\nSo, let's take the median of all available Weekly <br>\nSales of this (Store, Dept) to fill missing values! <br>\nThe other 0.30% we can fill with the median of all <br>\nits Store Sales.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"filling","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"na_xy_test_filled = na_xy_test.apply(lambda row : process.dummy_fill_store_dept_median(row, wm_data_train_valid, test_cols), axis=1)\nxy_test = not_na_xy_test.append(na_xy_test_filled)\n\nprint(\"NAs frequency over predictive columns\\n\")\nfor test_col in test_cols:\n    print(test_col)\n    print(stats.freq(xy_test[test_col].isna()))\n    print()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"not_na_xy_test = xy_test[xy_test[test_cols].notna().all(1)]\nna_xy_test = xy_test[~xy_test.index.isin(not_na_xy_test.index)]\n\nna_xy_test_filled = na_xy_test.apply(lambda row : process.dummy_fill_store_median(row, wm_data_train_valid, test_cols), axis=1)\nxy_test = not_na_xy_test.append(na_xy_test_filled)\n\nprint(\"NAs frequency over predictive columns\\n\")\nfor test_col in test_cols:\n    print(test_col)\n    print(stats.freq(xy_test[test_col].isna()))\n    print()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predicting Weekly_Sales for TEST dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test = not_na_xy_test.append(na_xy_test_filled)#[[\"year1_sales_train\", \"year1_size_train\"] + key_colnames + [\"IsHoliday_test\"]]\nx_test.head(4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test_pred = reg.predict(x_test[test_cols])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subm = evaluation.build_submission_df(test_df=x_test, \n                                      target_predicted=y_test_pred,\n                                      store_colname=\"Store_test\", \n                                      dept_colname=\"Dept_test\", \n                                      date_colname=\"Date_test\")\n\n# subm[\"Weekly_Sales\"] = subm[\"Weekly_Sales\"].apply(lambda ws : round(ws, 4))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subm.reset_index().to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"download.create_download_link(subm, \"Submission Download\", \"submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Results\nMy best score at Kaggle submission was <font color=\"navy\">3710.35797</font> which <br>\nwould place me at position <font color=\"navy\">334th</font>, over <font color=\"navy\">688</font> people. <br>\nIt was not expected to be in the Top <font color=\"navy\">48.5%</font> of participants as <br>\nthis approach was designed to be a simple contribution <br> \naiming an ensemble to improve traditional approaches. <br>\n\nConsidering that only two features were used, it can be <br>\nconcluded that this strategy (initial assumptions and data <br>\ntransformations based on Store-Dept-Week_n-Month_n) has a <br>\ngreat potential for ensembling or even be used on its own.<br>\n\nAs this case was required to be done in very short time, <br>\nthere are plenty of work that can be done as next steps. <br>\n\n## Next Steps \n\n### Time Series Similarity\nStore and Departaments that have similar Time Series shapes <br>\nmay me composing a semantic cluster. It is worth trying to <br>\ncheck if a Linear Regression can be fitted for each of these <br>\nclusters. <br>\n\n### Further Human Behavior Modeling\nAs in the initial assumptions I have stated that <br>\n(Week_n-Month_n) can explain human behavior, there may be <br>\nsome other human community behavior that is influenced by <br>\ncommunities' geolocalzation. We don't know this information <br>\nbut fields like CPI, Temperature and Fuel Price indexes may <br>\nbe used for clusterization. And these groups may represent <br>\nclose communities. <br>\n\n### Smoothing Filters and ETS Decomposition \nHodrick-Prescott filter was tried but it can be more explored. <br>\nETS (Error-Trend-Seasonality) was not applied and it could be given <br>\na chance for it. <br>\n\n### Ensemble Application With Traditional Models\nThis (Week_n, Month_) strategy may result into an awesome result if <br> \nused together with Exponential Smothing or ARIMA models. \n\n\n### Tracking Tests Execution\nA Design of Experiment wasn't applied at all, and it could help <br>\nindentify how to better switch between modules, models, parameters, <br>\nfeatures and so on...\n\n### More features\nRemember only two features were used. <br>\nHoliday data must be used due to its wheight relevance. \n","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}