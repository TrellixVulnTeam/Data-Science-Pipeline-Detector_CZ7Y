{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Overview"},{"metadata":{},"cell_type":"markdown","source":"Vamos fazer uma análise das vendas por loja e departamentos do Walmart. "},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport plotly.express as px\nimport matplotlib\nimport time\nimport warnings\nimport matplotlib.pyplot as plt\nwarnings.filterwarnings(\"ignore\")\nplt.style.use('seaborn-darkgrid')\nimport statsmodels.api as sm\nmatplotlib.rcParams['axes.labelsize'] = 20\nmatplotlib.rcParams['xtick.labelsize'] = 12\nmatplotlib.rcParams['ytick.labelsize'] = 12\nmatplotlib.rcParams['text.color'] = 'k'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy import stats\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_validate\nfrom sklearn import preprocessing\nfrom sklearn.metrics import mean_absolute_error, r2_score\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor \nfrom sklearn.model_selection import GridSearchCV","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Vamos separar os data sets e analisar as informações que estão contidas em cada um"},{"metadata":{"trusted":true},"cell_type":"code","source":"filepath = '../input/walmart-recruiting-store-sales-forecasting/'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dados = pd.read_csv(f'{filepath}train.csv.zip', parse_dates=['Date'], compression='zip')\ndados.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lojas = pd.read_csv(f'{filepath}stores.csv')\nlojas.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = pd.read_csv(f'{filepath}features.csv.zip', parse_dates=['Date'], compression='zip')\nfeatures.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predizer = pd.read_csv(f'{filepath}test.csv.zip', parse_dates=['Date'], compression='zip')\npredizer.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Análise de Variáveis"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,8))\nsns.distplot(dados.Weekly_Sales)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,8))\nsns.boxplot(x=\"Type\", y=\"Size\", data=lojas)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As classificações de tipos de lojas parece ser feito de acordo com o tamanho das lojas.\nVamos agrupar as informações de features e das lojas para continuar as análises de variáveis."},{"metadata":{"trusted":true},"cell_type":"code","source":"valores_por_loja_tamanho = dados.merge(lojas, on='Store', how='left')\nlojas_completas = valores_por_loja_tamanho.merge(features, on=['Store','Date'], how='left')\nlojas_completas.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,6))\nsns.boxplot(x=\"Store\", y=\"Weekly_Sales\",hue='IsHoliday_x', data=lojas_completas)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Como nossos dados estão granulares no nível de departamento, vamos agrupar por loja para tentar identificar melhor as tendências"},{"metadata":{"trusted":true},"cell_type":"code","source":"vendas_no_tempo = lojas_completas.groupby(['Date','Type']).mean()['Weekly_Sales'].reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(30,8))\nsns.lineplot(x=\"Date\", y=\"Weekly_Sales\",hue='Type', data=vendas_no_tempo)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Podemos verificar que as lojas de tipo C tem uma variabilidade menor durante o tempo, talvez essas lojas não tenham tanta influência dos feriados, vamos analisar mais a fundo."},{"metadata":{"trusted":true},"cell_type":"code","source":"vendas_agrupadas = lojas_completas.groupby(['Store','Date','Type','IsHoliday_x']).sum()['Weekly_Sales'].reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,6))\nsns.boxplot(x=\"Store\", y=\"Weekly_Sales\",hue='IsHoliday_x', data=vendas_agrupadas)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Podemos identificar que existe alguma flutuação nas vendas em feriados, vamos colocar os feriados fornecidos para identificar possíveis interações."},{"metadata":{"trusted":true},"cell_type":"code","source":"feriados = {\n    'Date' : (pd.to_datetime(['2010-02-12', '2011-02-11', '2012-02-10', '2013-02-08',\n                             '2010-09-10', '2011-09-09', '2012-09-07', '2013-09-06',\n                            '2010-11-26', '2011-11-25', '2012-11-23', '2013-11-29',\n                            '2010-12-31', '2011-12-30', '2012-12-28', '2013-12-27'])),\n    'TypeHoliday' :(['SB', 'SB', 'SB', 'SB',\n                    'Labor', 'Labor', 'Labor', 'Labor',\n                    'Thanksgiving', 'Thanksgiving', 'Thanksgiving', 'Thanksgiving',\n                    'Christmas', 'Christmas', 'Christmas', 'Christmas']) \n\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lojas_feriados = lojas_completas.merge(pd.DataFrame(feriados), on='Date', how='left')\nlojas_feriados['TypeHoliday'].fillna(0,inplace=True)\nlojas_feriados","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Vamos extrair as colunas de mês e ano para identificar se os feriados tem comportamento diferente no tempo"},{"metadata":{"trusted":true},"cell_type":"code","source":"lojas_feriados['Month'] = pd.to_datetime(lojas_feriados.Date).dt.month\nlojas_feriados['Year'] = pd.to_datetime(lojas_feriados.Date).dt.year","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_feriados = lojas_feriados.groupby(['Store','Year','TypeHoliday','Type']).sum()['Weekly_Sales'].reset_index()\nplt.figure(figsize=(15,8))\nsns.boxplot(x=\"Year\", y=\"Weekly_Sales\",hue='TypeHoliday', data=plot_feriados.loc[plot_feriados['TypeHoliday']!=0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,8))\nsns.boxplot(x=\"Type\", y=\"Weekly_Sales\",hue='TypeHoliday', data=plot_feriados.loc[plot_feriados['TypeHoliday']!=0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Podemos ver que os feriados de Ação de Graças tem um valor de vendas superior aos demais feriados. Vamos considerar isso nas predições.\nTambém certificamos que as lojas do tipo C não tem forte influência dos feriados."},{"metadata":{},"cell_type":"markdown","source":"### Tratando MissingValues"},{"metadata":{},"cell_type":"markdown","source":"Antes de começar os testes de modelos, vamos tratar os valores faltantes. \nNota-se que os MarkDowns são os unicos valores faltantes neste dataset, e considerando a documentação e a natureza dessas features vamos fazer uma rápida análise para verificar o comportamento dessas variáveis."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(30,8))\nsns.boxplot(x=\"Store\", y=\"MarkDown1\",hue='IsHoliday_x', data=lojas_feriados)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(30,8))\nsns.boxplot(x=\"Store\", y=\"MarkDown2\",hue='IsHoliday_x', data=lojas_feriados)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(30,8))\nsns.boxplot(x=\"Store\", y=\"MarkDown3\",hue='IsHoliday_x', data=lojas_feriados)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(30,8))\nsns.boxplot(x=\"Store\", y=\"MarkDown4\",hue='IsHoliday_x', data=lojas_feriados)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(30,8))\nsns.boxplot(x=\"Store\", y=\"MarkDown5\",hue='IsHoliday_x', data=lojas_feriados)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Notamos que os MarkDowns tem bastantes outliers por loja, por isso iremos tratar os missing values utilizando as medianas dos valores por loja e para caso tenha feriado ou não na data."},{"metadata":{"trusted":true},"cell_type":"code","source":"filling = lojas_feriados.groupby(['Store','IsHoliday_x']).median()[['MarkDown1','MarkDown2','MarkDown3','MarkDown4','MarkDown5']].reset_index()\nfilling.rename(columns={'MarkDown1':'FMD1','MarkDown2':'FMD2','MarkDown3':'FMD3','MarkDown4':'FMD4','MarkDown5':'FMD5'}, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lojas_preenchidas = lojas_feriados.merge(filling, on=['Store','IsHoliday_x'], how='inner')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lojas_preenchidas.MarkDown1.fillna(lojas_preenchidas['FMD1'],inplace=True)\nlojas_preenchidas.MarkDown2.fillna(lojas_preenchidas['FMD2'],inplace=True)\nlojas_preenchidas.MarkDown3.fillna(lojas_preenchidas['FMD3'],inplace=True)\nlojas_preenchidas.MarkDown4.fillna(lojas_preenchidas['FMD4'],inplace=True)\nlojas_preenchidas.MarkDown5.fillna(lojas_preenchidas['FMD5'],inplace=True)\nlojas_preenchidas.drop(['FMD1','FMD2','FMD3','FMD4','FMD5'], axis=1, inplace=True)\nlojas_preenchidas.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Como verificamos que os feriados tem diferentes pesos nas vendas das lojas, vamos separalos em variáveis para auxiliar as nossas análises."},{"metadata":{"trusted":true},"cell_type":"code","source":"lojas_preenchidas['IsHoliday'] = pd.get_dummies(lojas_preenchidas.IsHoliday_x)[1]\nlojas_preenchidas['SB'] =pd.get_dummies(lojas_preenchidas.TypeHoliday)['SB']\nlojas_preenchidas['Labor'] =pd.get_dummies(lojas_preenchidas.TypeHoliday)['Labor']\nlojas_preenchidas['Thanksgiving'] =pd.get_dummies(lojas_preenchidas.TypeHoliday)['Thanksgiving']\nlojas_preenchidas['Christmas'] =pd.get_dummies(lojas_preenchidas.TypeHoliday)['Christmas']\nlojas_preenchidas['TypeA'] = pd.get_dummies(lojas_preenchidas.Type)['A']\nlojas_preenchidas['TypeB'] = pd.get_dummies(lojas_preenchidas.Type)['B']\nlojas_preenchidas['TypeC'] = pd.get_dummies(lojas_preenchidas.Type)['C']\nlojas_preenchidas.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Com todas as variáveis preenchidas no DataSet, vamos analisar as possíveis correlações entre elas."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(25,20))\nsns.heatmap(lojas_preenchidas.fillna(0).corr(), annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Podemos verificar algumas correlações lógicas entre feriados, mas também podemos notar algumas correlações entre MarkDowns. Vamos retirar algumas variáveis afim de simplificar o modelo e mitigar problemas de multicolinearidade dado que ainda não decidimos qual modelo vamos utilizar em nossas predições."},{"metadata":{},"cell_type":"markdown","source":"Uma ultima análise, vimos que as vendas das lojas C não se comportam da mesma maneira que as demais lojas, portanto vamos verificar como elas se comportam, dividindo a loja pelo seu tamanho. "},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,10))\nlojas_preenchidas['Weekly_Sales_Size'] = lojas_preenchidas['Weekly_Sales']/lojas_preenchidas['Size']\nsns.lineplot(x=\"Date\", y=\"Weekly_Sales_Size\",hue='Type', data=lojas_preenchidas)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As lojas C tem um comportamento bem diferente, o que pode causar alguns problemas em nosso modelo. Vamos manter as variáveis de tipo afim de tentar reduzir esse problema."},{"metadata":{},"cell_type":"markdown","source":"### Seleção de Variáveis."},{"metadata":{},"cell_type":"markdown","source":"Vamos retirar as variáveis com alguma colinearidade com os MarkDonw, além das variáveis categóricas e da Weekly_Sale_Size que serviu apenas para uma análise pontual do comportamento."},{"metadata":{"trusted":true},"cell_type":"code","source":"lojas_limpas = lojas_preenchidas.drop(['MarkDown4','MarkDown5','IsHoliday_y','IsHoliday_x','Type','TypeHoliday','TypeC','Weekly_Sales_Size'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modelo"},{"metadata":{},"cell_type":"markdown","source":"A primeira vista, considerando as vendas durante um determinado período de tempo para prever vendas futuras, foi considerado a utilização da abordagem por <b> Séries Temporais </b>. Para tal deveríamos analisar as vendas no tempo e tentar ajusar um modelo de classe ARIMAX para projetar vendas futuras. \nAnalisando as features, notamos que existem fetures de datas futuras, ou datas as quais iremos estimar, dados estas informações podemos utiliza algum modelo de <b> Regressão/Classificação </b>.\nCaso fossemos utilizar um modelo ARIMAX, tirariamos as variáveis de departamento, buscando analisar apenas as vendas semanais. \nFizemos uma pequena análise das vendas e percebemos que as diferenças entre semanas tem um comportamento estacionário, o que permitiria uma abordagem de séries temporais, o que não foi o caso selecionado."},{"metadata":{},"cell_type":"markdown","source":"## Treino e Teste"},{"metadata":{},"cell_type":"markdown","source":"Mesmo não utilizando a abordagem de séries temporais, vamos fazer uma quebra de treino e teste considerando que nossas vendas se comportam com uma função do tempo, para isso vamos quebrar a série de Teste em <b> Out of Sample </b> e <b> Out of Time </b>"},{"metadata":{},"cell_type":"markdown","source":"Como estamos falando em vendas semanais, vamos separar um mês de vendas para nossa análise out of time."},{"metadata":{"trusted":true},"cell_type":"code","source":"OfT = lojas_limpas.loc[lojas_limpas['Date'] >= '2012-10-05']\nlojas_predict = lojas_limpas.loc[lojas_limpas['Date'] < '2012-10-05']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Agora vamos quebrar nossas variáveis nos X e Y que utilizaremos para treinar os modelos."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Conjunto OfT\nX_OfT = OfT.drop(['Weekly_Sales','Date'], axis=1)\ny_OfT = OfT['Weekly_Sales']\n# Conjunto OfS\nX = lojas_predict.drop(['Weekly_Sales','Date'], axis=1)\ny = lojas_predict['Weekly_Sales']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_OfT.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Para coparar os modelos que iremos ajustar as predições usando a função determinada pelo desafio de WMAE"},{"metadata":{"trusted":true},"cell_type":"code","source":"def WMAE(y_obs, y_pred, flag):\n    peso = flag*5+1*(1-flag)\n    \n    indice = (1/sum(peso))*sum(peso*abs(y_obs-y_pred))\n\n    return indice","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Quebrando os conjuntos de treino e teste"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Formato do conjunto de treino OfS: {X_train.shape}')\nprint(f'Formato do conjunto de teste: {X_test.shape}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Mesma quantidade de variáveis, ok. "},{"metadata":{},"cell_type":"markdown","source":"## Os Modelos"},{"metadata":{},"cell_type":"markdown","source":"Foram escolhidos três modelos para comparar performance e verificar qual seria o modelo adotado para esse caso: LinearRegression, RandomForestRegressor e GradientBoostRegressor. Cada um dos tres modelos tem uma especificidade para como fitam os dados, portanto vamos olhar todos.\nA <b> Regressão Linear </b> nos servirá como baseline para a performance dos modelos."},{"metadata":{},"cell_type":"markdown","source":"Vamos usar o método de CrossValidation para veirificar a coerência dos nossos modelos, assim como sua estabilidade nos conjuntos de dados. \nUsaremos também o GridSearch para vefiricar quais os parâmentros ótimos de cada modelo que estamos utilizando."},{"metadata":{},"cell_type":"markdown","source":"### Regressão Linear"},{"metadata":{"trusted":true},"cell_type":"code","source":"parameters = {'normalize': [False,True], 'fit_intercept': [False,True]} \nmodel = LinearRegression()\n\nSEED = 1988\nnp.random.seed(SEED)\ncv = KFold(10, shuffle=True)\n\n\n\nclf = GridSearchCV(model, parameters, cv=cv, verbose=5, n_jobs=8)\nclf.fit(X_train, y_train)\nclf.best_params_\nclf.predict(X_test)\nwmae = WMAE(y_test, clf.predict(X_test), X_test.IsHoliday)\nr2 = r2_score(y_test, clf.predict(X_test))\nresults = cross_validate(model, X, y, cv=cv, return_train_score = False)\nmedia = results['test_score'].mean()\ndesvio = results['test_score'].std()\n\nprint(\"LinearRegression\")\nprint(\"------------------------------\")\nprint(f'Parametros ótimos = {clf.best_params_}')\nprint(f'Mean: {media*100}')\nprint(f'Accuracy: [{(media-2*desvio)*100} , {(media+2*desvio)*100}]')\nprint(f'WMAE = {wmae} and R-square = {r2}')\nprint(\"------------------------------\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Veremos que o nosso baseline é em 14.910, vamos verificar como os outros modelos se comportam. "},{"metadata":{},"cell_type":"markdown","source":"* ### Random Forest Regressor e Gradient Boosting Regressor"},{"metadata":{},"cell_type":"markdown","source":"Foi feita uma rodada de GridSearch, para determinar os melhores parâmentros dos modelos em questão. Com esses parâmetros calibraremos e faremos as predições e análise dos dados. "},{"metadata":{},"cell_type":"markdown","source":"### Calibrando e Calculando a Efetividade dos Modelos"},{"metadata":{"trusted":true},"cell_type":"code","source":"SEED = 1988\nnp.random.seed(SEED)\nln = LinearRegression(fit_intercept = True, normalize = False)\nrf = RandomForestRegressor(max_depth = 30, max_features =  18, min_samples_leaf = 2 ,n_estimators = 200)\ngbm = GradientBoostingRegressor(max_depth = 5, max_features = 12, min_samples_leaf = 5, n_estimators = 200)\nnp.random.seed(SEED)\n\nln.fit(X_train, y_train)\nln_y_pred = ln.predict(X_test)\nwmae_ln_pred = WMAE(y_test, ln_y_pred, X_test.IsHoliday)\nr2_ln_pred = r2_score(y_test, ln_y_pred)\n\nrf.fit(X_train, y_train)\nrf_y_pred = rf.predict(X_test)\nwmae_rf_pred = WMAE(y_test, rf_y_pred, X_test.IsHoliday)\nr2_rf_pred = r2_score(y_test, rf_y_pred)\n\ngbm.fit(X_train, y_train)\ngbm_y_pred = gbm.predict(X_test)\nwmae_gbm_pred = WMAE(y_test, gbm_y_pred, X_test.IsHoliday)\nr2_gbm_pred = r2_score(y_test, gbm_y_pred)\n\nprint(\"LinearRegression\")\nprint(\"------------------------------\")\nprint(f'WMAE = {wmae_ln_pred} and R-square = {r2_ln_pred}')\nprint(\"------------------------------\")\nprint(\"RandomForest\")\nprint(\"------------------------------\")\nprint(f'WMAE = {wmae_rf_pred} and R-square = {r2_rf_pred}')\nprint(\"------------------------------\")\nprint(\"GradientBoosting\")\nprint(\"------------------------------\")\nprint(f'WMAE = {wmae_gbm_pred} and R-square = {r2_gbm_pred}')\nprint(\"------------------------------\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (20,12))\nplt.scatter(y_test,ln_y_pred,label='LR',marker = 'o',color='r')\nplt.scatter(y_test,rf_y_pred,label='RF',marker = 'o',color='b')\nplt.scatter(y_test,gbm_y_pred,label='GBR',marker = 'o',color='y')\nplt.title('Modelos',fontsize = 25)\nplt.legend(fontsize = 20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"O Modelo <b> Random Forest Regressor </b>, parece mais consistente nesse cenário, vamos analisar a amostra <b>OfT</b>."},{"metadata":{},"cell_type":"markdown","source":"### Teste Out of Time"},{"metadata":{"trusted":true},"cell_type":"code","source":"ln_y_OfT = ln.predict(X_OfT)\nwmae_ln_OfT = WMAE(y_OfT, ln_y_OfT, X_OfT.IsHoliday)\nr2_ln_OfT = r2_score(y_OfT, ln_y_OfT)\n\n\nrf_y_OfT = rf.predict(X_OfT)\nwmae_rf_OfT = WMAE(y_OfT, rf_y_OfT, X_OfT.IsHoliday)\nr2_rf_OfT = r2_score(y_OfT, rf_y_OfT)\n\n\ngbm_y_OfT = gbm.predict(X_OfT)\nwmae_gbm_OfT = WMAE(y_OfT, gbm_y_OfT, X_OfT.IsHoliday)\nr2_gbm_OfT = r2_score(y_OfT, gbm_y_OfT)\n\nprint(\"LinearRegression\")\nprint(\"------------------------------\")\nprint(f'WMAE = {wmae_ln_OfT} and R-square = {r2_ln_OfT}')\nprint(\"------------------------------\")\nprint(\"RandomForest\")\nprint(\"------------------------------\")\nprint(f'WMAE = {wmae_rf_OfT} and R-square = {r2_rf_OfT}')\nprint(\"------------------------------\")\nprint(\"GradientBoosting\")\nprint(\"------------------------------\")\nprint(f'WMAE = {wmae_gbm_OfT} and R-square = {r2_gbm_OfT}')\nprint(\"------------------------------\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (20,12))\nplt.scatter(y_OfT,ln_y_OfT,label='LR',marker = 'o',color='r')\nplt.scatter(y_OfT,rf_y_OfT,label='RF',marker = 'o',color='b')\nplt.scatter(y_OfT,gbm_y_OfT,label='GBR',marker = 'o',color='y')\nplt.title('Modelos',fontsize = 25)\nplt.legend(fontsize = 20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"O modelo <b> Random Forest </b> se mostrou mais consistente nos testes com estes parâmetros. O <b> Gradient Boosting Regressor </b> teve uma efetividade melhor em testes com outros parâmetros, que demandam muito mais do nosso poder computacional, portanto optamos por não utiliza-lo."},{"metadata":{},"cell_type":"markdown","source":"# Predições"},{"metadata":{"trusted":true},"cell_type":"code","source":"predicoes_loja = predizer.merge(features, on=['Store','Date'], how='inner')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lojas_p_features = predicoes_loja.merge(lojas, on='Store', how='inner')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"filling_p = lojas_p_features.groupby(['Store','IsHoliday_x','Type']).median()[['MarkDown1','MarkDown2','MarkDown3','MarkDown4','MarkDown5','CPI','Unemployment']].reset_index()\nfilling_p.rename(columns={'MarkDown1':'FMD1','MarkDown2':'FMD2','MarkDown3':'FMD3','MarkDown4':'FMD4','MarkDown5':'FMD5','CPI': 'C','Unemployment':'UN'}, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lojas_preenchidas_p = lojas_p_features.merge(filling_p, on=['Store','IsHoliday_x','Type'], how='inner')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lojas_preenchidas_p.MarkDown1.fillna(lojas_preenchidas_p['FMD1'],inplace=True)\nlojas_preenchidas_p.MarkDown2.fillna(lojas_preenchidas_p['FMD2'],inplace=True)\nlojas_preenchidas_p.MarkDown3.fillna(lojas_preenchidas_p['FMD3'],inplace=True)\nlojas_preenchidas_p.MarkDown4.fillna(lojas_preenchidas_p['FMD4'],inplace=True)\nlojas_preenchidas_p.MarkDown5.fillna(lojas_preenchidas_p['FMD5'],inplace=True)\nlojas_preenchidas_p.CPI.fillna(lojas_preenchidas_p['C'],inplace=True)\nlojas_preenchidas_p.Unemployment.fillna(lojas_preenchidas_p['UN'],inplace=True)\nlojas_preenchidas_p.drop(['FMD1','FMD2','FMD3','FMD4','FMD5','C','UN'], axis=1, inplace=True)\nlojas_preenchidas_p.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lojas_feriados_p = lojas_preenchidas_p.merge(pd.DataFrame(feriados), on='Date', how='left')\nlojas_feriados_p['TypeHoliday'].fillna(0,inplace=True)\nlojas_feriados_p","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lojas_feriados_p['IsHoliday'] = pd.get_dummies(lojas_feriados_p.IsHoliday_x)[1]\nlojas_feriados_p['SB'] = pd.get_dummies(lojas_feriados_p.TypeHoliday)['SB']\nlojas_feriados_p['Labor'] = 0\nlojas_feriados_p['Thanksgiving'] = pd.get_dummies(lojas_feriados_p.TypeHoliday)['Thanksgiving']\nlojas_feriados_p['Christmas'] = pd.get_dummies(lojas_feriados_p.TypeHoliday)['Christmas']\n\nlojas_feriados_p","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lojas_feriados_p['TypeA'] = pd.get_dummies(lojas_feriados_p.Type)['A']\nlojas_feriados_p['TypeB'] = pd.get_dummies(lojas_feriados_p.Type)['B']\nlojas_feriados_p['TypeC'] = pd.get_dummies(lojas_feriados_p.Type)['C']\nlojas_feriados_p['Month'] = pd.to_datetime(lojas_feriados_p.Date).dt.month\nlojas_feriados_p['Year'] =  pd.to_datetime(lojas_feriados_p.Date).dt.year","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"colunas = X_train.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Aplicando as Predições"},{"metadata":{"trusted":true},"cell_type":"code","source":"lojas_predizer = lojas_feriados_p[colunas]\nlojas_predizer.fillna(0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lojas_preenchidas_p['Weekly_Sales'] = rf.predict(lojas_predizer).round(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicoes_finais = lojas_preenchidas_p[['Store','Dept','Date','Weekly_Sales']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicoes_finais['Id'] = (predicoes_finais[['Store','Dept','Date']].astype('str').apply('_'.join, axis=1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission = (f'{filepath}sampleSubmission.csv.zip')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = predicoes_finais[['Id','Weekly_Sales']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusões"},{"metadata":{},"cell_type":"markdown","source":"Analisamos as variáveis e as possíveis informações para o modelo e levantamos algumas hipóteses e conclusçoes:\n<n>1) Para obter um baixo <b>WMAE</b> consideramos os departamentos como uma variável de alto poder de predição apra o modelo. Para isso assumimos que os números dos departamentos tem consistência entre as lojas, ou seja, o Dept 1 da loja 1 representa o mesmo departamento que o Dept 1 da loja 45. Uma outra forma de modelar seria utilizar a soma total das vendas por lojas e quebrar as vendas por departamento de acordo com a representatividade de cada departamento por loja. Fizemos a simulação e encontramos resultados interessantes, porém um pouco distantes das lojas do Tipo C.\n<n>2) Caso fossesmo tratar com uma predição para lojas futuras, seria interessante não considerar a variável Loja e Departamento nas predições, afim de não viesar os resultados por lojas já conhecidas.\n<n>3) Em caso de predições de vendas futuras, onde não conhecemos as features agregadas, poderiamos buscar a abordagem de séries temporais, e tentar o fit de um modelo <b>ARIMA</b> para os dados,para isso deveríamos usar a diferença de vendas semanais agregadas por loja, dado que essa variávei <b>Weekly_Sales.diff()</b> possui um comportamento estacionário no tmepo."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}