{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1 - Libraries","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom numpy import ravel\nimport seaborn as sns\nimport pickle\nfrom joblib import dump, load\nimport scipy as sp\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score, mean_squared_error, make_scorer\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor, VotingRegressor\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.compose import ColumnTransformer\nfrom skopt import BayesSearchCV\nfrom skopt.space import Real, Categorical, Integer\nimport lightgbm","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2 - Importing DataFrame","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/walmart-recruiting-store-sales-forecasting/train.csv.zip')\ntest = pd.read_csv('/kaggle/input/walmart-recruiting-store-sales-forecasting/test.csv.zip')\nfeatures = pd.read_csv('/kaggle/input/walmart-recruiting-store-sales-forecasting/features.csv.zip')\nstores = pd.read_csv('/kaggle/input/walmart-recruiting-store-sales-forecasting/stores.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3 - Merging DataFrames","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.merge(train, stores, on = 'Store', how = 'left')\ntrain = pd.merge(train, features, on = ['Store', 'Date'], how = 'left')\ntest = pd.merge(test, stores, on = 'Store', how = 'left')\ntest = pd.merge(test, features, on = ['Store', 'Date'], how = 'left')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4 - Exploratory Analysis","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 4.1 - Understanding features and how it might affect the target","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"1. Store: Stores\n2. Dept: Department inside each store, not all the stores have all the departments. \n    - How it migth affect target: if a store do not sells alcoholic beverages, it might have a worst performance on Super bowl in comparison to other Wallmarts stores\n3. Date: Date which starts each week.\n    - How it might affect target: Some people buy all goods once a month when they receive their salary, this statement is more sensitive for some departments then others\n4. Weekly Sales: Amount sold in a week (our target)\n5. Is Holiday: Flag for main holidays in a year (Christmas, ThanksGiving, Super Bowl and Labor Day), I would also add the 4th July when Americans have a 1 week holiday.\n    - How it might affect target: Some departments might sell more on holidays (eg beverages and snacks on Super Bowl, Clothes and Toys on Christmas, food and eletronics on ThanksGiving)\n    - This feature needs to be divided in 4 features (Is Christmas?, Is SuperBowl?, IsLaborDay... to avoid noise on departments\n        - Super Bowl: 12-Feb-10, 11-Feb-11, 10-Feb-12, 8-Feb-13 (Week 6)\n        - Labor Day: 10-Sep-10, 9-Sep-11, 7-Sep-12, 6-Sep-13 (Week 36)\n        - Thanksgiving: 26-Nov-10, 25-Nov-11, 23-Nov-12, 29-Nov-13 (Week 47)\n        - Christmas: 31-Dec-10, 30-Dec-11, 28-Dec-12, 27-Dec-13 (Week 51, 52)\n        - July 4th: Week 27 2010, Week 27 2011, Week 27 2012, Week 27 2013\n- Type: I'm assuming that type is a store class like a rank, type A are the best stores and so on.\n    - How it might affect target: Store with Type A might sell more than store Type B\n- Size: It is the store size\n    - How it might affect target: Big stores might be located in places with high desnsity population and tend to sell more than small stores\n- Temperature: Average temperature per week\n    - How it might affect target: Some departments tends to sell more on summer and other on winter\n- Fuel Price: Average fuel price in the region store is located\n    - How it might affect target: Unless Walmart have gas station department this item should be discarded. According to https://www.usatoday.com/story/money/2016/02/04/walmart-to-operate-own-gas-stations-going-forward/79809480/ Walmart are operating gas station since 2016, once our data is from 2010 untill 2013 therefore Fuel Price will be discarded.\n- Markdowns: Flag for labels as Sales, Clearance, close to expiration and so on\n    - How it might affect target: People might buy goods with discount even if they don't need that thing.\n- CPI: The Consumer Price Index (CPI) is a measure that examines the weighted average of prices of a basket of consumer goods and services, such as transportation, food, and medical care. It is calculated by taking price changes for each item in the predetermined basket of goods and averaging them. Changes in the CPI are used to assess price changes associated with the cost of living. The CPI is one of the most frequently used statistics for identifying periods of inflation or deflation.\n    - How it might affect target: In a scenario of inflation people tends to avoid buy expensive things, so some departments could be affected\n- Unemployment: Rate of unemployment in the region of store.\n    - How it might affect target: People unemployed might buy only essential products and it might affected some departments.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 4.1.1 Analyzing features 'Date' and 'Holiday'","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating new columns with Year, Month and Week of Year\ntrain.Date = pd.to_datetime(train.Date)\ntrain['Year'], train['Month'], train['Week'] = train.Date.dt.year, train.Date.dt.month, train.Date.dt.strftime('%V')\ntest.Date = pd.to_datetime(test.Date)\ntest['Year'], test['Month'], test['Week'] = test.Date.dt.year, test.Date.dt.month, test.Date.dt.strftime('%V')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Grouping Weekly Sales by week and extracting min, max and median values for each week\ntrain_summary = train.groupby(['Week']).agg({'Weekly_Sales': [np.min, np.max, np.mean]})\ntrain_median = train[['Week', 'Weekly_Sales']].groupby('Week').median()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Ploting Min, Max and Median weekly sales trend with holidays highlighted\nplt.plot(train_summary.index, train_summary.Weekly_Sales.amin, color = \"red\", label = \"Min Weekly Sales\")\nplt.plot(train_summary.index, train_summary.Weekly_Sales.amax, color = \"blue\", label = \"Max Weekly Sales\")\nplt.plot(train_median.index, train_median.values, color = 'green', label = \"Median Weekly Sales\")\nplt.axvspan(4.5, 5.5, alpha=0.5, color='yellow')\nplt.axvspan(34.5, 35.5, alpha=0.5, color='red')\nplt.axvspan(45.5, 46.5, alpha=0.5, color='blue')\nplt.axvspan(49.5, 50.5, alpha=0.5, color='green')\nplt.axvspan(26.5, 27.5, alpha=0.5, color='brown')\nplt.rcParams[\"figure.figsize\"] = (18,8)\nplt.fill_between(train_summary.index, train_summary.Weekly_Sales.amin, train_summary.Weekly_Sales.amax, \n                 facecolor = \"grey\", alpha = 0.3)\nplt.rcParams.update({'font.size': 14})\nplt.legend()\nplt.xlabel(\"Weeks over the year\")\nplt.ylabel(\"Weekly Sales US$\")\nplt.title(\"Weekly Sales over the Weeks\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based on the graph above we can see that there is a trend with peak weekly sales on main Holidays. Christmas and ThanksGiving has bigger peaks because those holidays might affect more departments, consumed departments on those holidays are more expensive or due to high clearances and discount on ThanksGiving (people waits throughout the year to spend money on this holiday). In addition we can observe that the 4th July Holiday doesn't seems to have high effect on sales and for this reason it will not be created as feature.\nFor a better analysis the next graph will bring the same information removing Max weekly sales, splitting by Year, and replacing median by mean.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train2010 = train[train.Year == 2010]\ntrain2011 = train[train.Year == 2011]\ntrain2012 = train[train.Year == 2012]\n\ntrain2010mean = train2010[['Weekly_Sales', 'Week']].groupby('Week').mean()\ntrain2011mean = train2011[['Weekly_Sales', 'Week']].groupby('Week').mean().tail(48)\ntrain2012mean = train2012[['Weekly_Sales', 'Week']].groupby('Week').mean().tail(39)\ntrain2010min = train2010[['Weekly_Sales', 'Week']].groupby('Week').min()\ntrain2011min = train2011[['Weekly_Sales', 'Week']].groupby('Week').min().tail(48)\ntrain2012min = train2012[['Weekly_Sales', 'Week']].groupby('Week').min().tail(39)\nplt.plot(train2010mean.index, train2010mean.values, color = \"blue\", label = \"Mean Weekly Sales 2010\")\nplt.plot(train2011mean.index, train2011mean.values, color = \"red\", label = \"Mean Weekly Sales 2011\")\nplt.plot(train2012mean.index, train2012mean.values, color = 'green', label = \"Mean Weekly Sales 2012\")\nplt.plot(train2010min.index, train2010min.values, color = \"blue\", label = \"Min Weekly Sales 2010\")\nplt.plot(train2011min.index, train2011min.values, color = \"red\", label = \"Min Weekly Sales 2011\")\nplt.plot(train2012min.index, train2012min.values, color = 'green', label = \"Min Weekly Sales 2012\")\n\nplt.axvspan(0.5, 1.5, alpha=0.5, color='yellow')\nplt.axvspan(30.5, 31.5, alpha=0.5, color='red')\nplt.axvspan(41.5, 42.5, alpha=0.5, color='blue')\nplt.axvspan(45.5, 46.5, alpha=0.5, color='green')\nplt.rcParams[\"figure.figsize\"] = (18,8)\nplt.rcParams.update({'font.size': 14})\nplt.legend()\nplt.xticks(np.arange(1, 53, step=1))\nplt.xlabel(\"Weeks over the year\")\nplt.ylabel(\"Weekly Sales US$\")\nplt.title(\"Weekly Sales over the Weeks\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Removing the max values we still can observe the peak trend on SuperBowl, ThanksGiving and Christmas. We also can observe that prior to a big holiday there is a negative trend but in general it does not affect the median sale values in comparison to min values. The negative values represents that a store sold less than what it spent with returning product. Negative peak happened prior to labor day and it might be happen cause people buy things, and then discover that during labor day sales the same thing will have a 50% discount, so they give back the product and receive their money back to spend on holiday. I also observed that there are 3 peaks very close to each other from week 13 untill week 16. These 3 peaks represents the Easter Sales, and these Holiday can affect some departments like chocolate, candies and gift and for this reason I'll creat a feature 'IsEaster'.\nOn next graph I'll check how type stores are affected on weekly sales.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_A = train[train.Type == 'A']\ntrain_B = train[train.Type == 'B']\ntrain_C = train[train.Type == 'C']\n\ntrainA_median = train_A[['Week', 'Weekly_Sales']].groupby('Week').median()\ntrainB_median = train_B[['Week', 'Weekly_Sales']].groupby('Week').median()\ntrainC_median = train_C[['Week', 'Weekly_Sales']].groupby('Week').median()\n\nplt.plot(trainA_median.index, trainA_median.values, color = \"red\", label = \"Median Weekly Sales Type A Stores\")\nplt.plot(trainB_median.index, trainB_median.values, color = 'green', label = \"Median Weekly Sales Type B Stores\")\nplt.plot(trainC_median.index, trainC_median.values, color = 'blue', label = \"Median Weekly Sales Type C Stores\")\nplt.axvspan(4.5, 5.5, alpha=0.5, color='yellow')\nplt.axvspan(34.5, 35.5, alpha=0.5, color='red')\nplt.axvspan(45.5, 46.5, alpha=0.5, color='blue')\nplt.axvspan(49.5, 50.5, alpha=0.5, color='green')\nplt.rcParams[\"figure.figsize\"] = (18,8)\nplt.rcParams.update({'font.size': 14})\nplt.legend()\nplt.xlabel(\"Weeks over the year\")\nplt.ylabel(\"Weekly Sales US$\")\nplt.title(\"Median Weekly Sales over the Weeks\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.loc[(train.Year==2010) & (train.Week=='13'), 'IsEaster'] = True\ntrain.loc[(train.Year==2011) & (train.Week=='16'), 'IsEaster'] = True\ntrain.loc[(train.Year==2012) & (train.Week=='14'), 'IsEaster'] = True\ntrain['IsEaster'] = train.IsEaster.apply(lambda x: 1 if x == True else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['IsChristmas'] = train.Week.apply(lambda x: 1 if (x == '51' or x == '52') else 0)\ntrain['IsSuperBowl'] = train.Week.apply(lambda x: 1 if x == '06' else 0)\ntrain['IsLaborDay'] = train.Week.apply(lambda x: 1 if x == '36' else 0)\ntrain['IsThanksGiving'] = train.Week.apply(lambda x: 1 if x == '47' else 0)\ntrain['IsEaster'] = train.IsEaster.apply(lambda x: 1 if x == True else 0)\n\ntrain.drop(['IsHoliday_x'], axis = 1, inplace = True)\ntest.drop(['IsHoliday_x'], axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_superbowl = train[train.IsSuperBowl == 1]\nsuperBowl = train_superbowl[['Dept', 'Weekly_Sales']].groupby('Dept').sum().sort_values(by = 'Weekly_Sales', ascending = False).head(10)\ntrain_christmas = train[train.IsChristmas == 1]\nchristmas = train_christmas[['Dept', 'Weekly_Sales']].groupby('Dept').sum().sort_values(by = 'Weekly_Sales', ascending = False).head(10)\ntrain_thanksgiving = train[train.IsThanksGiving == 1]\nthanksGiving = train_thanksgiving[['Dept', 'Weekly_Sales']].groupby('Dept').sum().sort_values(by = 'Weekly_Sales', ascending = False).head(10)\ntrain_laborday = train[train.IsLaborDay == 1]\nlaborDay = train_laborday[['Dept', 'Weekly_Sales']].groupby('Dept').sum().sort_values(by = 'Weekly_Sales', ascending = False).head(10)\ntrain_easter = train[train.IsEaster == 1]\neaster = train_easter[['Dept', 'Weekly_Sales']].groupby('Dept').sum().sort_values(by = 'Weekly_Sales', ascending = False).head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking top departments for each holiday\nfrom IPython.core.display import HTML\n\ndef show_dataframes(listOfTables):\n    ''' Accepts a list of IpyTable objects and returns a table which contains each IpyTable in a cell\n    '''\n    return HTML(\n        '<table><tr style=\"background-color:white;\">' + \n        ''.join(['<td>' + table.to_html(max_rows=10) + '</td>' for table in listOfTables]) +\n        '</tr></table>'\n    )\n\nshow_dataframes([superBowl, christmas, thanksGiving, laborDay, easter])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based on top 10 departments for each holiday it can be concluded that the holiday influence the way people buy things, each department has it own importance for each holiday, for example Dept 7 may be related to Toys and Gifts beacuse it appears only on ThanksGiving and Crhistmas while Dept 5 may be related to eletronics once it is Top3 on ThanksGiving. On the other hand Dept 1 only shows up on Easter dataframe and may be chocolate department. For these reason 5 new columns were created on the DataFrame to check which Holiday is.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### 4.1.2 - Analyzing feature 'Type'","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"results_type = {}\n\nresults_type['A'] = [train[train.Type == 'A']['Weekly_Sales'].min(), \n                     train[train.Type == 'A']['Weekly_Sales'].median(), \n                     train[train.Type == 'A']['Weekly_Sales'].max()]\nresults_type['B'] = [train[train.Type == 'B']['Weekly_Sales'].min(), \n                     train[train.Type == 'B']['Weekly_Sales'].median(), \n                     train[train.Type == 'B']['Weekly_Sales'].max()]\nresults_type['C'] = [train[train.Type == 'C']['Weekly_Sales'].min(), \n                     train[train.Type == 'C']['Weekly_Sales'].median(), \n                     train[train.Type == 'C']['Weekly_Sales'].max()]\n\nresults_type = pd.DataFrame(results_type)\nresults_type = results_type.T\nresults_type","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based on the table above, stores type A has a better median values, store type B has the max values, but this is being affected by outliers. Let's consider the median values that is not affected by outliers. We may conclude that Type is related to Store performance, but before we state it, let's check if Type has correlation with Size.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,8))\nsns.boxplot(x = \"Type\", y = \"Size\", data=train)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Boxplot above made clear the correlation between Store Size and Type.On the other hand Weekly Sales is related to Store Size as well, on models refining we can decide if we should keep type ou if it's better to remove it.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#transforming Type to numerical values to use on future models\ntrain.Type = train.Type.map({'C': 0, 'B': 1, 'A': 2})\ntest.Type = test.Type.map({'C': 0, 'B': 1, 'A': 2})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Correlation between Store Type and Unemployment rate\nfig, axs = plt.subplots(1,3, figsize = (10,5.5), sharey = True)\naxs[0].boxplot(train[train.Type == 2]['Unemployment'])\naxs[0].set_title('Type A')\naxs[1].boxplot(train[train.Type == 1]['Unemployment'])\naxs[1].set_title('Type B')\naxs[2].boxplot(train[train.Type == 0]['Unemployment'])\naxs[2].set_title('Type C')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based on boxplot above we can state that the store Type might have a correlation with Unemployment rate, values of unemployment for stores type A are packed between 4.5 and 10 with outliers below 4, while the other types has wider values.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 4.1.3 - Analyzing feature 'Size'","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"size_sales = train[['Size', 'Weekly_Sales']].groupby('Size').median()\nfig, axs = plt.subplots(1,1, figsize = (6,4))\naxs.scatter(size_sales.index, size_sales.values)\naxs.set_title('Store Size x Weekly Sales median')\nplt.xlabel('Store Size')\nplt.ylabel('Weekly Sales')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sp.stats.pearsonr(train.Size, train.Weekly_Sales), sp.stats.spearmanr(train.Size, train.Weekly_Sales)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sp.stats.pearsonr(train.Type, train.Weekly_Sales), sp.stats.spearmanr(train.Type, train.Weekly_Sales)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based on the correlation comparison, Store Size is more correlated with Weekly Sales than Type and should be used as feature.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 4.1.4 - Analyzing features 'Temperature' and 'Fuel Price' \n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Trying to understand if there is correlation between Temperature or Fuel Price with Weekly Sales\nfig, axs = plt.subplots(1, 2, figsize = (15,6), sharey = True)\naxs[0].scatter(train.Fuel_Price, train.Weekly_Sales, alpha = 0.05)\naxs[0].set_title('Fuel Price vs Weekly Sales') \naxs[1].scatter(train.Temperature, train.Weekly_Sales, alpha = 0.05)\naxs[1].set_title('Temperature vs Weekly Sales') \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Regarding Fuel Price, there isn't correlation with fuel price and weekly sales, no trends were identifies. Unless Walmart have gas station department this item should be discarded. According to https://www.usatoday.com/story/money/2016/02/04/walmart-to-operate-own-gas-stations-going-forward/79809480/ Walmart are operating gas station since 2016, once our data is from 2010 untill 2012, Fuel Price will be discarded.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.drop('Fuel_Price', axis = 1, inplace = True)\ntest.drop('Fuel_Price', axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Evaluating how the temperature may affect departments\nlow_temp = train[train.Temperature <= 34]\nmedium_temp = train[(train.Temperature > 34) & (train.Temperature <=66)]\nhigh_temp = train[train.Temperature > 67]\n\nlow_temp_df = low_temp[['Dept', 'Weekly_Sales']].groupby('Dept').median().sort_values(by = 'Weekly_Sales', ascending = False).head(10)\nmed_temp_df = medium_temp[['Dept', 'Weekly_Sales']].groupby('Dept').median().sort_values(by = 'Weekly_Sales', ascending = False).head(10)\nhigh_temp_df = high_temp[['Dept', 'Weekly_Sales']].groupby('Dept').median().sort_values(by = 'Weekly_Sales', ascending = False).head(10)\nshow_dataframes([low_temp_df, med_temp_df, high_temp_df])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Temperature has a slight effect on what people buys, for example Dept 72 and 8 is highly influenced by temperature while other departments are not influenced by temperatyre. I'll keep Temperatyre because I believe that this influence could bring a slightly better performance at the end of day","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 4.1.5 - Analyzing features 'Markdow'","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train[['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']].isnull().sum(), train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.Week = train.Week.astype(int)\nplt.figure(figsize=(25,20))\nsns.heatmap(train.fillna(0).corr(), annot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"According to Correlation Matrix, MarkDown3 is higly correlated with Thanks Giving while MarkDown2 has correlation with Christmas Holiday. MarkDown 1, 4 and 5 are correlated with Year. Once I have created the features 'IsChristmas'and 'IsTxsGiving' MarkDown 2 and 3 will be removed. \n\nConsidering that the Year should not define the prediction and in addition due to the high number of missing values for MarkDowns, Mardown 1, 4 and 5 will also be removed.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 4.1.6 - Analyzing Features 'CPI' and 'Unemployment'","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(1,2, figsize = (14,5.5))\naxs[0].boxplot(train.Unemployment)\naxs[0].set_title('Unemployment Boxplot')\naxs[1].boxplot(train.CPI)\naxs[1].set_title('CPI Boxplot')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"good_place = train[train.Unemployment <= 6]\nnormal_place = train[(train.Unemployment > 6) & (train.Unemployment <= 10)]\nbad_place = train[train.Unemployment > 11]\n\nsum_good_place = good_place[['Dept', 'Weekly_Sales']].groupby('Dept').sum().sort_values(by='Weekly_Sales', ascending = False).head(10)\nsum_normal_place = normal_place[['Dept', 'Weekly_Sales']].groupby('Dept').sum().sort_values(by='Weekly_Sales', ascending = False).head(10)\nsum_bad_place = bad_place[['Dept', 'Weekly_Sales']].groupby('Dept').sum().sort_values(by='Weekly_Sales', ascending = False).head(10)\n\nshow_dataframes([sum_good_place, sum_normal_place, sum_bad_place])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"median_goodPlace = train[train.Unemployment < 5]['Weekly_Sales'].median()\nmedian_normalPlace = train[(train.Unemployment <= 10) & (train.Unemployment >= 5)]['Weekly_Sales'].median()\nmedian_badPlace = train[train.Unemployment > 10]['Weekly_Sales'].median()\nprint('Weekly Sales median when unemployment rate is above 10%: U$', median_badPlace)\nprint('Weekly Sales median when unemployment rate is between 5% and 10%: U$', median_normalPlace)\nprint('Weekly Sales median when unemployment rate is below 5%: U$', median_goodPlace)\nprint('''Based on above information, unemployment rate has correlation with target values and must be considered in \nthe model''')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"median_cpiDeflation = train[train.CPI < 140]['Weekly_Sales'].median()\nmedian_cpiNormal = train[(train.CPI <= 220) & (train.CPI >= 140)]['Weekly_Sales'].median()\nmedian_cpiInflation = train[train.CPI > 220]['Weekly_Sales'].median()\nprint('Weekly Sales when CPI is above 220 (inflation): U$', median_cpiInflation)\nprint('Weekly Sales when CPI is between 140 and 220 (Normal): U$', median_cpiNormal)\nprint('Weekly Sales when CPI is below 140 (deflation): U$', median_cpiDeflation)\nprint('''Based on above information, CPI has correlation with target values and must be considered in the model''')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5 - Pre-processing data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 5.1 - Splitting train and test before any transformation that could lead to data leakage","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"treino, teste = train_test_split(train, test_size = 0.2, random_state = 11)\n\ntreino = treino.copy()\nteste = teste.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.2 - Re-scaling values ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_treino = treino.drop(['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'IsHoliday_y', 'Year', \n                        'Month', 'Weekly_Sales', 'Date', 'Type'], axis = 1)\ny_treino = treino.Weekly_Sales\nX_teste = teste.drop(['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'IsHoliday_y', 'Year', \n                        'Month', 'Weekly_Sales', 'Date', 'Type'], axis = 1)\ny_teste = teste.Weekly_Sales","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"minmax = MinMaxScaler()\n\nX_treino.loc[:, :] = minmax.fit_transform(X_treino.loc[:,:])\nX_teste.loc[:,:] = minmax.transform(X_teste.loc[:,:])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 6 - Models","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 6.1 - Evaluation Metrics","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"holiday_week_treino = treino.IsHoliday_y.apply(lambda x: True if x else False)\nholiday_week_teste = treino.IsHoliday_y.apply(lambda x: True if x else False)\n\ndef wmae_train(y_treino, y_pred):\n    sumOfWeights = 0\n    sumOfErrors = 0\n    \n    for i in range(0, len(y_pred)):\n        weight = 0\n        if holiday_week_treino.values[i] == True: \n            weight = 5\n        else:\n            weight = 1\n        \n        errors = abs(y_treino.values[i] - y_pred[i])*weight\n        sumOfWeights += weight \n        sumOfErrors += errors\n\n    return sumOfErrors/sumOfWeights\n\ndef wmae_test(y_teste, y_pred):\n    sumOfWeights = 0\n    sumOfErrors = 0\n    \n    for i in range(0, len(y_pred)):\n        weight = 0\n        if holiday_week_teste.values[i] == True: \n            weight = 5\n        else:\n            weight = 1\n        \n        errors = abs(y_teste.values[i] - y_pred[i])*weight\n        sumOfWeights += weight \n        sumOfErrors += errors\n\n    return sumOfErrors/sumOfWeights","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weight_error = make_scorer(wmae_train, greater_is_better=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As requested, the metric used to evaluate must be the WMAE which set a heavier weight when is holiday, for this reason I created the function wmae_train. I also created the weigh_error to use as scoring parameter on RandomizedSearchCV or GridSearchCV","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 6.2 - Linear Regression\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Once the values are re-scaled I'll not pass any hyperparameter\nlin_reg = LinearRegression().fit(X_treino, y_treino)\n\ny_pred_linreg = lin_reg.predict(X_teste)\n\nWMAE_linreg = wmae_test(y_teste, y_pred_linreg)\n\nr2_linreg = r2_score(y_teste, y_pred_linreg)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Results = {}\n\n\nResults['Linear Regression'] = [WMAE_linreg, r2_linreg]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"LinearRegression\")\nprint(\"---------------------------------------\")\nprint(f'WMAE = {WMAE_linreg:.10} and R-square = {r2_linreg:.4}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(y_pred_linreg, y_teste.values, alpha = 0.05)\nplt.xlabel(\"y predicted by Linear Regression\")\nplt.ylabel(\"y true\")\nplt.title(\"y pred vs y true (Linear Regression)\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6.3 - Decision Tree Regressor","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tree = DecisionTreeRegressor(random_state = 11)\n\nrs_tree = RandomizedSearchCV(estimator = tree,\n                            param_distributions = {'max_depth': np.arange(1, 25),\n                                                   'min_samples_leaf': np.arange(0.00001, 0.3, 0.00001)},\n                            n_iter = 500,\n                            scoring = weight_error,\n                            cv = 5,\n                            random_state = 11)\n\nrs_tree.fit(X_treino, y_treino)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_rs_tree = rs_tree.predict(X_teste)\n\nwmae_rs_tree = wmae_test(y_teste, y_pred_rs_tree)\n\nr2_rs_tree = r2_score(y_teste, y_pred_rs_tree)\n\nResults['Decision Tree Regressor'] = [wmae_rs_tree, r2_rs_tree]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Decision Tree Regressor\")\nprint(\"------------------------------------------------------------\")\nprint(f'Best parameters: {rs_tree.best_params_}')\nprint(\"------------------------------------------------------------\")\nprint(f'WMAE = {wmae_rs_tree:.7} and R-square = {r2_rs_tree:.4}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(y_pred_rs_tree, y_teste.values, alpha = 0.05)\nplt.xlabel(\"y predicted by Decision Tree\")\nplt.ylabel(\"y true\")\nplt.title(\"y pred vs y true (Decision Tree)\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6.4 - Random Forest Regressor","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestRegressor(random_state = 11, n_estimators = 30)\n\nrs_rf = RandomizedSearchCV(estimator = rf,\n                           param_distributions = {'max_depth': np.arange(1,30),\n                                                  'min_samples_leaf': np.arange(0.0001, 0.3, 0.0001)},\n                           n_iter = 30,\n                           scoring = weight_error,\n                           cv = 3,\n                           random_state = 11)\n\nrs_rf.fit(X_treino, y_treino)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_rs_rf = rs_rf.predict(X_teste)\n\nwmae_rs_rf = wmae_test(y_teste, y_pred_rs_rf)\n\nr2_rs_rf = r2_score(y_teste, y_pred_rs_rf)\n\nResults['Random Forest'] = [wmae_rs_rf, r2_rs_rf]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Random Forest Regressor\")\nprint(\"---------------------------------------------------------------\")\nprint(f'Best parameters: {rs_rf.best_params_}')\nprint(\"---------------------------------------------------------------\")\nprint(f'WMAE = {wmae_rs_rf:.7} and R-square = {r2_rs_rf:.4}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(y_pred_rs_rf, y_teste.values, alpha = 0.05)\nplt.xlabel(\"y predicted by Random Forest\")\nplt.ylabel(\"y true\")\nplt.title(\"y pred vs y true (Random Forest)\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6.5 - Ada Boosting Regressor","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tree = DecisionTreeRegressor(min_samples_leaf = 0.00043, max_depth = 22, random_state = 11)\n\nada_tree = AdaBoostRegressor(base_estimator = tree,\n                             n_estimators = 500,\n                             learning_rate = 0.05,\n                             random_state = 11)\n\nada_tree.fit(X_treino, y_treino)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_ada_tree = ada_tree.predict(X_teste)\n\nwmae_ada_tree = wmae_test(y_teste, y_pred_ada_tree)\n\nr2_ada_tree = r2_score(y_teste, y_pred_ada_tree)\n\nResults['Ada Boosting with Decision Tree'] = [wmae_ada_tree, r2_ada_tree]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Ada Boosting with Decision Tree\")\nprint(\"---------------------------------------\")\nprint(f'WMAE = {wmae_ada_tree:.7} and R-square = {r2_ada_tree:.4}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(y_teste.values, y_pred_ada_tree, alpha = 0.2)\nplt.xlabel(\"y true\")\nplt.ylabel(\"y predicted by Ada Boosting with Decision Tree\")\nplt.title(\"y pred vs y true (Ada Boosting)\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6.6 - Gradient Boosting Regressor","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"grad_boost = GradientBoostingRegressor(n_estimators = 500,\n                                       learning_rate = 0.05,\n                                       max_depth = 22, \n                                       min_samples_leaf = 0.00043,\n                                       random_state = 11)\n\ngrad_boost.fit(X_treino, y_treino)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_grad_boost = grad_boost.predict(X_teste)\n\nwmae_grad_boost = wmae_test(y_teste, y_pred_grad_boost)\n\nr2_grad_boost = r2_score(y_teste, y_pred_grad_boost)\n\nResults['Gradient Boosting'] = [wmae_grad_boost, r2_grad_boost]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Gradient Boosting\")\nprint(\"---------------------------------------\")\nprint(f'WMAE = {wmae_grad_boost:.7} and R-square = {r2_grad_boost:.4}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(y_teste.values, y_pred_grad_boost, alpha = 0.2)\nplt.ylabel(\"y predicted by Gradient Boosting\")\nplt.xlabel(\"y true\")\nplt.title(\"y pred vs y true (Gradient Boosting)\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6.7 - LightGBM Regressor","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"gbm = lightgbm.LGBMRegressor()\n\nparam_space = {'num_leaves': Integer(1, 100),\n               'max_depth': Integer(1, 40),\n               'learning_rate': Real(0.05, 0.25),\n               'n_estimators': Integer(50, 500),\n               'min_split_gain': Real(0.001, 0.3),\n               'min_child_samples': Integer(1, 50)}\n\nbayes_gbm = BayesSearchCV(estimator = gbm,\n                          search_spaces = param_space,\n                          n_iter = 200,\n                          cv = 3,\n                          scoring = weight_error, \n                          random_state = 11)\n\nbayes_gbm.fit(X_treino, y_treino)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_gbm_bs = bayes_gbm.predict(X_teste)\n\nwmae_gbm = wmae_test(y_teste, y_pred_gbm_bs)\n\nr2_gbm = r2_score(y_teste, y_pred_gbm_bs)\n\nResults['LightGBM Regressor'] = [wmae_gbm, r2_gbm]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"LightGBM Regressor\")\nprint(\"---------------------------------------\")\nprint(f'WMAE = {wmae_gbm:.7} and R-square = {r2_voting:.4}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(y_teste.values, y_pred_gbm_bs, alpha = 0.2)\nplt.ylabel(\"y predicted by LightGBM Regressor\")\nplt.xlabel(\"y true\")\nplt.title(\"y pred vs y true (LightGBM Regressor)\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 7 - Results","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Results = pd.DataFrame(Results)\nResults = Results.T\nResults","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(y_teste.values, y_pred_ada_tree, alpha = 0.2, color = 'blue', label = 'Ada Boosting')\nplt.scatter(y_teste.values, y_pred_grad_boost, alpha = 0.2, color = 'red', label = 'Gradient Boosting')\nplt.scatter(y_teste.values, y_pred_gbm_bs, alpha = 0.2, color = 'yellow', label = 'LightGBM Regressor')\nplt.ylabel(\"y predicted\")\nplt.xlabel(\"y true\")\nplt.title(\"y pred vs y true\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based on the results table and also according to above graph, lightGBM presented the best results, with WMAE of 1510 and R2 of 0.9818. This model is very robust predicting values below US$ 200.000,00 and has very good performance with higher values (not so robust as AdaBoosting predicting higher values).\nIn addition, lightGBM regressor is way faster to train than other Boosting model because it can paralelize part of prediction.\nBased on my comments, lightGBM is the best model for this case.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 8 - Pipeline","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def prepare_data(dataframe_1, dataframe_2, dataframe_3, var, var2, how):\n    '''This function prepare the dataframe to proceed direct to hold out\n    dataframe_1: Must be train dataframe\n    dataframe_2: Must be stores dataframe\n    dataframe_3: Must be features dataframe\n    var: Feature to be used to merge first and second dataframe (string or list of strings)\n    var2: Feature to be used to merge first and third dataframe (string or list of strings)\n    how: How you want to join dataframes'''\n\n    def merge(dataframe_1, dataframe_2, dataframe_3, var, var2, how):\n        dataframe_1 = pd.merge(dataframe_1, dataframe_2, on = var, how = how)\n        dataframe_1 = pd.merge(dataframe_1, dataframe_3, on = var2, how = how).drop('IsHoliday_x', axis = 1)\n        dataframe_1.columns = ['Store', 'Dept', 'Date', 'Weekly_Sales', 'Type', 'Size', 'Temperature',\n       'Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4',\n       'MarkDown5', 'CPI', 'Unemployment', 'IsHoliday']\n        return dataframe_1\n\n    def split_date(dataframe_1):\n        dataframe_1['Date'] = pd.to_datetime(dataframe_1['Date'])\n        dataframe_1['Year'], dataframe_1['Month'], dataframe_1['Week'] = dataframe_1['Date'].dt.year, dataframe_1['Date'].dt.month, dataframe_1['Date'].dt.strftime('%V')\n        return dataframe_1\n\n    def break_down_holiday(dataframe_1):\n        dataframe_1['IsChristmas'] = dataframe_1['IsHoliday'].apply(lambda x: 1 if (x == '51' or x == '52') else 0)\n        dataframe_1['IsSuperBowl'] = dataframe_1['IsHoliday'].apply(lambda x: 1 if x == '06' else 0)\n        dataframe_1['IsLaborDay'] = dataframe_1['IsHoliday'].apply(lambda x: 1 if x == '36' else 0)\n        dataframe_1['IsThanksGiving'] = dataframe_1['IsHoliday'].apply(lambda x: 1 if x == '47' else 0)\n        return dataframe_1\n    \n    def create_easter(dataframe_1):\n        dataframe_1.loc[(dataframe_1.Year==2010) & (dataframe_1.Week=='13'), 'IsEaster'] = True\n        dataframe_1.loc[(dataframe_1.Year==2011) & (dataframe_1.Week=='16'), 'IsEaster'] = True\n        dataframe_1.loc[(dataframe_1.Year==2012) & (dataframe_1.Week=='14'), 'IsEaster'] = True\n        dataframe_1['IsEaster'] = dataframe_1.IsEaster.apply(lambda x: 1 if x == True else 0)\n        return dataframe_1\n        \n    dataframe_1 = merge(dataframe_1, dataframe_2, dataframe_3, var, var2, how)\n    dataframe_1 = split_date(dataframe_1)\n    dataframe_1 = break_down_holiday(dataframe_1)\n    dataframe_1 = create_easter(dataframe_1)\n    return dataframe_1\n\ndef wmae_test(y_teste, y_pred):\n    sumOfWeights = 0\n    sumOfErrors = 0\n    \n    for i in range(0, len(y_pred)):\n        weight = 0\n        if holiday_week_teste.values[i] == True: \n            weight = 5\n        else:\n            weight = 1\n        \n        errors = abs(y_teste.values[i] - y_pred[i])*weight\n        sumOfWeights += weight \n        sumOfErrors += errors\n\n    return sumOfErrors/sumOfWeights","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_pipe = pd.read_csv('/kaggle/input/walmart-recruiting-store-sales-forecasting/train.csv.zip')\nfeatures_pipe = pd.read_csv('/kaggle/input/walmart-recruiting-store-sales-forecasting/features.csv.zip')\nstores_pipe = pd.read_csv('/kaggle/input/walmart-recruiting-store-sales-forecasting/stores.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_pipe = prepare_data(train_pipe, stores_pipe, features_pipe, 'Store', ['Store', 'Date'], 'left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"treino_pipe, teste_pipe = train_test_split(train_pipe, test_size = 0.2, random_state = 11)\n\ntreino_pipe = treino_pipe.copy()\nteste_pipe = teste_pipe.copy()\n\nX_treino_pipe = treino_pipe.drop(['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'IsHoliday', 'Year', \n                        'Month', 'Weekly_Sales', 'Date', 'Fuel_Price', 'Type'], axis = 1)\ny_treino_pipe = treino_pipe.Weekly_Sales\nX_teste_pipe = teste_pipe.drop(['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'IsHoliday', 'Year', \n                        'Month', 'Weekly_Sales', 'Date', 'Fuel_Price', 'Type'], axis = 1)\ny_teste_pipe = teste_pipe.Weekly_Sales\n\nholiday_week_teste = treino_pipe.IsHoliday.apply(lambda x: True if x else False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_treino_pipe.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipe_gbm = Pipeline(steps = [\n    ('rescale', MinMaxScaler()),\n    ('modelo', lightgbm.LGBMRegressor(num_leaves = 100,\n                                      max_depth = 14,\n                                      learning_rate = 0.2396915023230759,\n                                      n_estimators = 482,\n                                      min_split_gain = 0.17147656915295872,\n                                      min_child_samples = 12))\n])\n\npipe_gbm.fit(X_treino_pipe, y_treino_pipe)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.to_pickle(pipe_gbm, 'pipe_gbm.pkl')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}