{"cells":[{"metadata":{"_uuid":"d789a1c8-0f35-42e3-b615-744e2b96a757","_cell_guid":"86b2656e-0244-400e-853f-d9f7f2fada3c","trusted":true},"cell_type":"code","source":"# Para centralizar as imagens:\nfrom IPython.core.display import display, HTML\nHTML(\"\"\"<style>.output_png{display: table-cell;    text-align: center;    vertical-align: middle;}</style>\"\"\")\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fa7dbbb6-c20e-4590-a77a-3fbb9d2f1777","_cell_guid":"83e4f2af-0e00-40f1-ab61-23648249d7f4","trusted":true},"cell_type":"markdown","source":"# **Resolução de um Problema de Negócio do Walmart**\n\n**Título:** *Store Sales Forecasting* - Previsão de Vendas das Lojas\n\n**Referência:** https://www.kaggle.com/c/walmart-recruiting-store-sales-forecasting/overview/evaluation\n\n**Objetivos:** \n\n1) Prever as vendas das lojas usando dados históricos\n\n2) Estimar o impacto de uma semana com feriado nas vendas semanais\n\n               \n**Métrica:** A métrica de avaliação da eficiência do modelo é:\n\n![\\Large%20WMAE=\\frac{1}{\\sum{w_i}}\\sum_{i=1}^n%20w_i|y_i-\\hat{y}_{i}|](https://latex.codecogs.com/svg.latex?\\Large%20WMAE=\\frac{1}{\\sum{w_i}}\\sum_{i=1}^n%20w_i|y_i-\\hat{y}_{i}|)\nonde:\n* $n$ é o número das linhas\n* $\\hat{y}_{i}$ é a previsão de venda\n* $y^i$ é a venda realizada\n* $w_i$ são pesos. $w = 5$ se a semana tiver feriado, $1$ se não tiver\n\nQuanto menor for o valor de WMAE, mais preciso o modelo é em prever as vendas semanais, especialmente nas semanas com feriado.\n\n## **1) Preparação**\n**Entendendo o Problema**:  \nPrimeiramente, é necessário entender qual é tipo de problema em questão para\naplicar a ferramenta (modelo) mais adequada para obtenção da solução. Então, \nvamos dar uma olhada para os dados disponibilizados e o que se pede:"},{"metadata":{"_uuid":"49e506e7-02d6-4a58-8488-27ba1f68c50b","_cell_guid":"fdbac83a-6509-42d6-8899-07d289f0fe09","trusted":true},"cell_type":"code","source":"# ANÁLISE EXPLORATÓRIA DOS DADOS:\n    # Importando os dados:\nimport pandas as pd\nfeatures = pd.read_csv('../input/walmart-recruiting-store-sales-forecasting/features.csv.zip')\nstores = pd.read_csv('../input/walmart-recruiting-store-sales-forecasting/stores.csv')\ntrain = pd.read_csv('../input/walmart-recruiting-store-sales-forecasting/train.csv.zip')\ntest = pd.read_csv('../input/walmart-recruiting-store-sales-forecasting/test.csv.zip')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"37d9fef9-8564-463a-abb5-4c1f011c105e","_cell_guid":"878086d0-d4b6-4cec-ba24-7dfdb78251e7","trusted":true},"cell_type":"markdown","source":"Exibindo uma pequena amostra dos dados de treino para o modelo (train.csv) e alguns de seus parâmetros descritivos:"},{"metadata":{"_uuid":"847991d9-c906-42c0-bff9-da672df46f10","_cell_guid":"1598eadc-c4be-435f-b0a3-e073fc78558a","trusted":true},"cell_type":"code","source":"pd.concat([train.head(3),train.sample(4),train.tail(3)])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"98bec1bf-c93e-433c-8fbc-851c30dd9d70","_cell_guid":"26c5a07d-3a2b-4e0f-bb0c-f7e2232a9348","trusted":true},"cell_type":"code","source":"    # Análise Estatística Descritiva dos Dados:\ntrain.describe().T","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"abefbc4e-e81d-459f-9b0c-5ce75c56cd31","_cell_guid":"360ada5d-51dc-4712-a26a-b76ca0cf90e5","trusted":true},"cell_type":"markdown","source":"De acordo com o que se pediu (\"[...] project the sales for each department in each store\"), os dados de treino disponibilizam as vendas semanais para cada loja e departamento, que estão enumerados. Três coisas chamam a atenção ao analisar este conjunto de dados: \n* 1ª) O formato de data apresenta o dia de término da semana, que aparenta representar sempre as sextas-feiras. A princípio o ideal seria converter para número da semana, já que os resultados retratam vendas semanais;\n* 2ª) Há ocorrência de vendas negativas nos dados históricos. Isto pode significar que em determinadas semanas houve mais devoluções do que vendas. O que causa estranheza. Por curiosidade, vamos verificar qual a porcentagem desse conjunto de dados que apresenta vendas semanais negativas:"},{"metadata":{"_uuid":"7cf4c0de-d932-4107-aabf-fa15f6df3ad1","_cell_guid":"ac971c90-4e72-4ab9-95f5-b07631747735","trusted":true},"cell_type":"code","source":"round((len(train[train['Weekly_Sales'] < 0])/len(train))*100,2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9411e234-a4c9-4032-ad83-3561093f5879","_cell_guid":"4bfdff29-7930-4151-9f78-bb05aabfa74b","trusted":true},"cell_type":"markdown","source":"0,3% das vendas semanais sendo negativas parece ser razoável.\n* 3ª) Parece haver um grande número de *outliers* nos dados de vendas semanais, já que seu valor máximo distoa muito do intervalo interquartil (onde metade dos dados estão) além de que a média é mais que o dobro da mediana (valores muito grandes estão elevando a média). Para visualizar a presença de *outliers*, faz-se o uso do diagrama de caixa, ou boxplot:"},{"metadata":{"_uuid":"df670e92-39b7-43c4-b566-16c167b581c7","_cell_guid":"2be9e84d-d823-4491-b6f2-2a23c9043d45","trusted":true},"cell_type":"code","source":"import seaborn as sns\nfrom matplotlib import pyplot as plt\n\nplt.figure(figsize=(15,5))\nsns.boxplot(x=train['Weekly_Sales'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"07bd5761-7251-4537-8cf4-8b378867daed","_cell_guid":"9b4ae072-8bb1-4310-aab5-53deb7a8a698","trusted":true},"cell_type":"code","source":"Q1 = train['Weekly_Sales'].quantile(0.25)\nQ3 = train['Weekly_Sales'].quantile(0.75)\nIQR = Q3 - Q1\nNoutliers = (train['Weekly_Sales'] > (Q3 + 1.5 * IQR)).sum()\nprint(round((Noutliers/len(train))*100,1), '% de outliers no dataset')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b9fe0dd9-5808-44b3-b881-44a4ec204c66","_cell_guid":"691c3ed6-6d0b-436b-b0b6-8329a173d7e4","trusted":true},"cell_type":"markdown","source":"Realmente, a hipótese de que existem muitos *outliers* se confirma. Talvez seja ideal removê-los antes de aplicar o modelo, para melhorar sua acurácia. Mas antes vamos ver se eles estão correlacionados com as semanas com feriado:"},{"metadata":{"_uuid":"dd014d78-c38c-416e-8792-fa0f4c07992a","_cell_guid":"06543a0d-447e-4119-9da9-0a87c5c983b8","trusted":true},"cell_type":"markdown","source":"## 2) Análise das Vendas em Semanas com Feriado:"},{"metadata":{"_uuid":"eb14b1cb-91ce-4897-98fc-49814d2771b5","_cell_guid":"e8e7a55f-2dfb-47ab-a230-274c765f5aa3","trusted":true},"cell_type":"code","source":"outliers = train[train['Weekly_Sales'] > (Q3 + 1.5 * IQR)]\nOutliers_Holiday_False_Per100 = int(round(100*outliers[outliers['IsHoliday']==False]['Weekly_Sales'].count()/outliers['Weekly_Sales'].count(),0))\nOutliers_Holiday_True_Per100 = int(round(100*outliers[outliers['IsHoliday']==True]['Weekly_Sales'].count()/outliers['Weekly_Sales'].count(),0))\nHoliday_False_Per100 = int(round(100*train[train['IsHoliday']==False]['Weekly_Sales'].count()/train['Weekly_Sales'].count(),0))\nHoliday_True_Per100 = int(round(100*train[train['IsHoliday']==True]['Weekly_Sales'].count()/train['Weekly_Sales'].count(),0))\n\nprint('Porcentagem de semana com feriado vs semana sem feriado - todo o dataset:',Holiday_True_Per100,'% vs',Holiday_False_Per100,'%')\nprint('Porcentagem de semana com feriado vs semana sem feriado - somente outliers:',Outliers_Holiday_True_Per100,'% vs',Outliers_Holiday_False_Per100,'%')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c1351fec-abe9-4b1a-be93-fdd91e8145cb","_cell_guid":"24b53090-7fbb-46aa-886c-8aa23f50d11b","trusted":true},"cell_type":"markdown","source":"A princípio, os dados dizem que os **picos** de vendas semanais não ocorrem necessariamente em feriados. Agora, vamos verificar a influência dos feriados de outra forma - comparando a média de vendas das semanas com feriado e sem feriado:"},{"metadata":{"_uuid":"fe520359-ef38-42ab-bf93-cdf57a16c80b","_cell_guid":"c4314042-2890-421c-ac9e-9756578fc2d2","trusted":true},"cell_type":"code","source":"NoHoliday_Sales = train[train['IsHoliday']==False]['Weekly_Sales'].mean()\nHoliday_Sales = train[train['IsHoliday']==True]['Weekly_Sales'].mean()\nprint('Em média, semanas com feriados vendem',int(round(100*Holiday_Sales/NoHoliday_Sales - 100,0)),'% a mais que semanas sem feriados.')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6107c3af-7681-4cc0-8440-ca508541677f","_cell_guid":"37249d27-51b6-436d-a048-0258f5814e01","trusted":true},"cell_type":"markdown","source":"Surpreendentemente, este não é um valor expressivo, como se esperaria.\n\nVamos mais a fundo, a fim de identificar a correlação de cada feriado com as médias de vendas semanais para cada ano:"},{"metadata":{"_uuid":"56301ec9-572b-4cea-9b32-4df057bd75a1","_cell_guid":"fb454f9c-59e3-4732-b172-94c3806d4a7a","trusted":true},"cell_type":"code","source":"    # Manipulando as datas para o formato desejado:\ntrain['Date'] = pd.to_datetime(train['Date'])\ntest['Date'] = pd.to_datetime(test['Date'])\n\ntrain.insert(3,'Year',train['Date'].dt.year)\ntest.insert(3,'Year',test['Date'].dt.year)\n\ntrain['Date'] = train['Date'].dt.isocalendar().week\ntest['Date'] = test['Date'].dt.isocalendar().week\n\ntrain = train.rename(columns={'Date': 'WeekNo'})\ntest = test.rename(columns={'Date': 'WeekNo'})\n\n    # Identificando as semanas com feriados:\nHolidays = {'Super Bowl': ['12-Feb-10', '11-Feb-11', '10-Feb-12', '8-Feb-13'],\n            'Labor Day': ['10-Sep-10', '9-Sep-11', '7-Sep-12', '6-Sep-13'],\n            'Thanksgiving': ['26-Nov-10', '25-Nov-11', '23-Nov-12', '29-Nov-13'],\n            'Christmas': ['31-Dec-10', '30-Dec-11', '28-Dec-12', '27-Dec-13']}\nHolidays = pd.DataFrame(Holidays, index=['2010','2011','2012','2013'])\nHolidays['Super Bowl'] = pd.to_datetime(Holidays['Super Bowl'], format='%d-%b-%y')\nHolidays['Labor Day'] = pd.to_datetime(Holidays['Labor Day'], format='%d-%b-%y')\nHolidays['Thanksgiving'] = pd.to_datetime(Holidays['Thanksgiving'], format='%d-%b-%y')\nHolidays['Christmas'] = pd.to_datetime(Holidays['Christmas'], format='%d-%b-%y')\n\nHolidays['Super Bowl'] = Holidays['Super Bowl'].dt.isocalendar().week\nHolidays['Labor Day'] = Holidays['Labor Day'].dt.isocalendar().week\nHolidays['Thanksgiving'] = Holidays['Thanksgiving'].dt.isocalendar().week\nHolidays['Christmas'] = Holidays['Christmas'].dt.isocalendar().week","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3424bc4e-edca-4035-a754-89cb9a0d488f","_cell_guid":"cbfab4b6-9ff8-4ed5-bcef-916d81f6bc7d","trusted":true},"cell_type":"code","source":"# Configurando o gráfico:\nWeekly_Sales_2010 = train[train.Year==2010]['Weekly_Sales'].groupby(train['WeekNo']).mean()\nWeekly_Sales_2011 = train[train.Year==2011]['Weekly_Sales'].groupby(train['WeekNo']).mean()\nWeekly_Sales_2012 = train[train.Year==2012]['Weekly_Sales'].groupby(train['WeekNo']).mean()\nplt.figure(figsize=(18,6))\nsns.lineplot(x=Weekly_Sales_2010.index, y=Weekly_Sales_2010.values)\nsns.lineplot(x=Weekly_Sales_2011.index, y=Weekly_Sales_2011.values)\nsns.lineplot(x=Weekly_Sales_2012.index, y=Weekly_Sales_2012.values)\nplt.grid()\nplt.xticks(range(min(train['WeekNo']), max(train['WeekNo'])+1,1))\nplt.legend(['2010', '2011', '2012'], loc='best', fontsize=16)\nplt.gca().add_artist(plt.legend(['2010', '2011', '2012'], loc='upper left', fontsize=16))\nplt.title('Average Weekly Sales per Year\\n', fontsize=18)\nplt.ylabel('Average Sales', fontsize=16)\nplt.xlabel('Week', fontsize=16)\ni=0\ncolors = ['blue', 'lime', 'magenta', 'cyan']\nfor x in Holidays.iloc[:3,:].drop_duplicates().stack():\n    plt.axvline(x, color = colors[i], label = Holidays.columns[i])\n    plt.legend(loc='upper center',fontsize=16)\n    i = i + 1\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7bbaa427-397d-4643-846e-2c0140100d97","_cell_guid":"78af1502-c2ea-4b13-b6c3-c13c4d62c6f4","trusted":true},"cell_type":"markdown","source":"Através do gráfico percebe-se que os únicos feriados que realmente causaram impacto significativo nas vendas foram o *Thanksgiving* e o Natal. \n\nComo parte da avaliação (\"*Part of the challenge presented by this competition is modeling the effects of markdowns on these holiday weeks in the absence of complete/ideal historical data*\"), vamos verificar e modelar o impacto das promoções nessas datas especiais:"},{"metadata":{"_uuid":"3c36dea4-e583-48a2-b4f6-9b7bcdf482d0","_cell_guid":"54c6606d-900e-4b95-9ac8-f0751ca6d9cb","trusted":true},"cell_type":"markdown","source":"Dando uma olhada no *dataset* de *features* (features.csv):"},{"metadata":{"_uuid":"a4b8a2a9-71dc-4637-b650-d5c4ebd47de8","_cell_guid":"f36a9698-9ca3-4260-8af1-19f1388e4bff","trusted":true},"cell_type":"code","source":"pd.concat([features.head(3),features.sample(4),features.tail(3)])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3e550af6-88cf-4241-876a-917d0082428e","_cell_guid":"0f78886c-6ea4-44af-8183-818a2a259e2f","trusted":true},"cell_type":"markdown","source":"Como foi dito na descrição dos dados, o registro das promoções somente está disponível a partir de novembro de 2011:\n\n\"*MarkDown data is only available after Nov 2011, and is not available for all stores all the time. Any missing value is marked with an NA*\"\n\nAlém disso, percebe-se que o *dataset* de *features* tem dados até 26 de julho de 2013, sendo que os últimos registros do *dataset* de treino ocorrem em 26 de outubro de 2012 (últimos valores de vendas). Então, vamos considerar somente o período entre novembro de 2011 e outubro de 2012 e relacionar com as vendas."},{"metadata":{"_uuid":"2cc218c3-ee90-49b6-9040-bb8628d7322e","_cell_guid":"433b4445-7fd4-45d4-bf3d-e7fa47014fd0","trusted":true},"cell_type":"code","source":"ftrs = features[['Store','Date','MarkDown1','MarkDown2','MarkDown3','MarkDown4','MarkDown5','IsHoliday']]\nIsHoliday = ftrs['IsHoliday']\nftrs = ftrs.drop(columns=['IsHoliday'])\nftrs.insert(1,'IsHoliday',IsHoliday)\n\nMarkDown_Total = features.loc[:,'MarkDown1':'MarkDown5'].sum(axis=1).rename('MarkDowns_Total')\nftrs.insert(2, 'MarkDown_Total', MarkDown_Total)\n\nftrs['Date'] = pd.to_datetime(ftrs['Date'])\nftrs.insert(2,'Year',ftrs['Date'].dt.year)\nftrs = ftrs[(ftrs['Date'] >= (pd.to_datetime('11-Nov-11', format='%d-%b-%y'))) & (ftrs['Year'] >= 2011) & (ftrs['Date'] <= (pd.to_datetime('26-Oct-12', format='%d-%b-%y')))]\nftrs['Date'] = ftrs['Date'].dt.isocalendar().week\nftrs = ftrs.rename(columns={'Date': 'WeekNo'})\nftrs.insert(1,'Store_Week_Year',pd.Series(ftrs['Store'].astype(str) + '_' + ftrs['WeekNo'].astype(str) + '_' + ftrs['Year'].astype(str)))\nftrs = ftrs.drop(columns=['Store','WeekNo','Year'])\n\nStore_Week_Year = pd.Series(train['Store'].astype(str) + '_' + train['WeekNo'].astype(str) + '_' + train['Year'].astype(str),name='Store_Week_Year')\ntrn = pd.concat([Store_Week_Year,train],axis=1)\ntrn = trn.drop(columns=['Store','WeekNo','Year','Dept'])\ntrn = trn.groupby(['Store_Week_Year']).agg({'Weekly_Sales':sum,'IsHoliday':'first'})\n\nMarkDown_Sales = pd.merge(ftrs, trn, on = ['Store_Week_Year','IsHoliday'], how = 'inner')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cfaeeb3d-9119-4326-b95b-bd18b887cd1e","_cell_guid":"74bf0c68-af60-4f55-a4ef-1101c9f8a898","trusted":true},"cell_type":"markdown","source":"Vamos checar se após filtrar o período ainda há valores faltantes no *dataset*:"},{"metadata":{"_uuid":"7796c00d-63af-44a7-bbb1-7292006d9996","_cell_guid":"5b638a66-a7b4-4f5a-9745-f968e9399bfa","trusted":true},"cell_type":"code","source":"NAs = pd.DataFrame(round(MarkDown_Sales.isnull().sum()/MarkDown_Sales.shape[0], 3)*100,columns=['%NA'])\nNAs.T","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4cdb5f61-3450-4f64-9836-a54b9b0bd642","_cell_guid":"9417123c-a6db-4090-aa54-5093d3be6dd7","trusted":true},"cell_type":"code","source":"MDvsSales = MarkDown_Sales.dropna(0)\nDataLoss = MDvsSales.shape[0] - MarkDown_Sales.shape[0]\nDataLossP100 = print('Ao eliminar as linhas com valores nulos a perda de informação é de',-DataLoss,'linhas de',\n                     MarkDown_Sales.shape[0],', o que equivale a',round(1 - MDvsSales.shape[0]/MarkDown_Sales.shape[0],3)*100,'% do dataset.')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a169c6c2-bd99-4c24-bcad-035845d597dc","_cell_guid":"bf2e6941-26f1-4d93-b2f2-81f3ebe2ad0f","trusted":true},"cell_type":"markdown","source":"A descrição do problema enfatiza que os valores NA correspondem a valores faltantes e não possivelmente a um valor nulo, ou zero de promoções/ofertas. Sendo assim, apesar de representarem uma parcela significativa do conjunto de dados, foi escolhido eliminar as linhas com NAs, já que substituir estes valores com a média ou mediana da coluna pode enviesar o resultado."},{"metadata":{"_uuid":"5f15872c-c7e5-4a1c-8e80-d1d7c931a300","_cell_guid":"279e5ccb-cf22-4555-a121-2c9957520822","trusted":true},"cell_type":"code","source":"MDvsSales = MarkDown_Sales.dropna(0)\n\n# Plotando\nfrom scipy import stats as st\nsns.set(font_scale=1.3)\nfig, axes = plt.subplots(6, 3, figsize=(20,20), tight_layout=True)\nfig.suptitle('Markdown-Sales Correlation\\n', fontsize=22)\naxes[0,0].set_title(\"\\n\".join([\"All Weeks\\n\"]), fontsize=16)\naxes[0,1].set_title(\"\\n\".join([\"Only Non-Holidays\\n\"]), fontsize=16)\naxes[0,2].set_title(\"\\n\".join([\"Only Holidays\\n\"]), fontsize=16)\nMD = ['T', 1, 2, 3, 4, 5]\nfor i in range(6):\n    column_name = 'Z_Score_MD'+str(MD[i])\n    MDvsSales.insert(len(MDvsSales.columns),column_name,st.zscore(MDvsSales.iloc[:,i+2]))\n    MDvsSales_NoOut = MDvsSales[MDvsSales[column_name].abs() < 3] # Retirando os outliers\n    \n    data=MDvsSales_NoOut\n    sns.regplot(ax=axes[i,0],data=data,x=data.iloc[:,i+2],y='Weekly_Sales',scatter_kws={\"color\":\"gold\"},line_kws={\"color\":\"black\"})\n    r1, p1 = st.pearsonr(data.iloc[:,i+2], data['Weekly_Sales'])\n    axes[i,0].text(0.45,0.9,'Plot {:n}'.format(i+1),transform=axes[i,0].transAxes)\n    axes[i,0].text(0.83,0.78,'r={:.2f}\\np={:.3f}'.format(r1,p1),transform=axes[i,0].transAxes)\n    axes[i,0].ticklabel_format(axis=\"x\", style=\"sci\", scilimits=(0,0))\n    \n    data=MDvsSales_NoOut[MDvsSales_NoOut['IsHoliday']==False]\n    sns.regplot(ax=axes[i,1],data=data,x=data.iloc[:,i+2],y='Weekly_Sales',scatter_kws={\"color\":\"gold\"}, line_kws={\"color\":\"black\"})\n    r2, p2 = st.pearsonr(data.iloc[:,i+2], data['Weekly_Sales'])\n    axes[i,1].text(0.45,0.9,'Plot {:n}'.format(i+7),transform=axes[i,1].transAxes)\n    axes[i,1].text(0.83,0.78,'r={:.2f}\\np={:.3f}'.format(r2,p2),transform=axes[i,1].transAxes)\n    axes[i,1].ticklabel_format(axis=\"x\", style=\"sci\", scilimits=(0,0))\n    \n    data=MDvsSales_NoOut[MDvsSales_NoOut['IsHoliday']==True]\n    sns.regplot(ax=axes[i,2],data=data,x=data.iloc[:,i+2],y='Weekly_Sales',scatter_kws={\"color\":\"gold\"}, line_kws={\"color\":\"black\"})\n    r3, p3 = st.pearsonr(data.iloc[:,i+2], data['Weekly_Sales'])\n    axes[i,2].text(0.45,0.9,'Plot {:n}'.format(i+13),transform=axes[i,2].transAxes)\n    axes[i,2].text(0.83,0.78,'r={:.2f}\\np={:.3f}'.format(r3,p3),transform=axes[i,2].transAxes)\n    axes[i,2].ticklabel_format(axis=\"x\", style=\"sci\", scilimits=(0,0))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"07e07f8a-ae6a-4b63-87ac-c6c1364d41dd","_cell_guid":"530c2c20-f3ad-4450-a92a-1d50c5d79f61","trusted":true},"cell_type":"markdown","source":"Os gráficos explicitam a relação entre a soma das vendas semanais de todas as lojas com a quantidade de cada tipo de *Markdown* aplicado na semana, como também para o total de *Markdowns*. Para isso, foi empregado o modelo de regressão linear com o objetivo de extrair a correlação entre as variáveis. Dispomos das seguintes ferramentas de análise e de suas contribuições pertinentes:\n* **Intuição**: É razoável presumir que as vendas aumentam caso o preço diminua;\n* **Visualização**: Ao observar a dispersão dos pontos nos gráficos, parece razoável afirmar que a relação entre vendas e ofertas tende a ser linear e a apresentar uma leve covariância positiva na maioria dos casos, apesar da ampla variância dos dados. Essa amplitude pode ser justificada ao considerar que a ofertas influenciariam mais determinados tipos de produtos que outros. O mesmo pode ser dito com relação as marcas. Como não dispomos destes dados aqui, essa poderia ser uma análise futura.\n* **Coeficiente de Correlação de Pearson, $\\rho$ (\"r\" no gráfico)**: Analisando os valores de $\\rho$ das distribuições que parecem melhor ajustadas, a maior correlação se dá quando se relaciona o total das ofertas em semanas com feriado, o que é exatamente um dos objetivos deste desafio, de forma geral. Neste caso, olhando somente o valor de $\\rho$~0.5 aparenta traduzir uma correlação razoável, dada a quantidade de variáveis que envolvem a efetivação de uma compra. Em segundo lugar, nota-se que o *Markdown5* correlaciona-se positivamente com as vendas mais que os outros *Markdowns*, em todos três casos de classificação de semanas.\n* **Valor-p**: Para os casos mencionados no item anterior (Plots 6, 12, 13 e 18), os valores-p são praticamente zero, indicando que os respectivos tipos de ofertas têm uma associação com as vendas que é altamente significante.\n\nComo ainda é possível classificar as semanas com feriado em quatro datas distintas (que também se traduzem em diferentes tipos de celebração), vamos fazê-lo a seguir, já que as correlações continuam não sendo tão evidentes."},{"metadata":{"_uuid":"791a9d53-f3e5-4a23-ab96-671b0b710ce6","_cell_guid":"614f9ff0-8133-4e07-88e1-9a4e6022956b","trusted":true},"cell_type":"code","source":"sns.set(font_scale=1.3)\nfig, axes = plt.subplots(6, 4, figsize=(20,20), tight_layout=True)\nfig.suptitle('Markdowns Effect on Holidays Sales\\n', fontsize=22)\naxes[0,0].set_title(\"\\n\".join([\"Super Bowl\\n\"]), fontsize=16)\naxes[0,1].set_title(\"\\n\".join([\"Labor Day\\n\"]), fontsize=16)\naxes[0,2].set_title(\"\\n\".join([\"Thanksgiving\\n\"]), fontsize=16)\naxes[0,3].set_title(\"\\n\".join([\"Christmas\\n\"]), fontsize=16)\nMD = ['T', 1, 2, 3, 4, 5]\nfor i in range(6): \n    data=MDvsSales_NoOut[MDvsSales_NoOut['Store_Week_Year'].str.contains('_6_')]\n    sns.regplot(ax=axes[i,0],data=data,x=data.iloc[:,i+2],y='Weekly_Sales',scatter_kws={\"color\":\"gold\"},line_kws={\"color\":\"black\"})\n    r1, p1 = st.pearsonr(data.iloc[:,i+2], data['Weekly_Sales'])\n    axes[i,0].text(0.4,0.9,'Plot {:n}'.format(i+19),transform=axes[i,0].transAxes)\n    axes[i,0].text(0.75,0.02,'r={:.2f}\\np={:.3f}'.format(r1,p1),transform=axes[i,0].transAxes)\n    axes[i,0].ticklabel_format(axis=\"x\", style=\"sci\", scilimits=(0,0))\n    axes[i,0].set_autoscale_on(True)\n    \n    data=MDvsSales_NoOut[MDvsSales_NoOut['Store_Week_Year'].str.contains('_36_')]\n    sns.regplot(ax=axes[i,1],data=data,x=data.iloc[:,i+2],y='Weekly_Sales',scatter_kws={\"color\":\"gold\"}, line_kws={\"color\":\"black\"})\n    r2, p2 = st.pearsonr(data.iloc[:,i+2], data['Weekly_Sales'])\n    axes[i,1].text(0.4,0.9,'Plot {:n}'.format(i+25),transform=axes[i,1].transAxes)\n    axes[i,1].text(0.75,0.02,'r={:.2f}\\np={:.3f}'.format(r2,p2),transform=axes[i,1].transAxes)\n    axes[i,1].ticklabel_format(axis=\"x\", style=\"sci\", scilimits=(0,0))\n    \n    data=MDvsSales_NoOut[MDvsSales_NoOut['Store_Week_Year'].str.contains('47')]\n    sns.regplot(ax=axes[i,2],data=data,x=data.iloc[:,i+2],y='Weekly_Sales',scatter_kws={\"color\":\"gold\"}, line_kws={\"color\":\"black\"})\n    r3, p3 = st.pearsonr(data.iloc[:,i+2], data['Weekly_Sales'])\n    axes[i,2].text(0.4,0.9,'Plot {:n}'.format(i+31),transform=axes[i,2].transAxes)\n    axes[i,2].text(0.75,0.02,'r={:.2f}\\np={:.3f}'.format(r3,p3),transform=axes[i,2].transAxes)\n    axes[i,2].ticklabel_format(axis=\"x\", style=\"sci\", scilimits=(0,0))\n    \n    data=MDvsSales_NoOut[MDvsSales_NoOut['Store_Week_Year'].str.contains('51')]\n    sns.regplot(ax=axes[i,3],data=data,x=data.iloc[:,i+2],y='Weekly_Sales',scatter_kws={\"color\":\"gold\"}, line_kws={\"color\":\"black\"})\n    r4, p4 = st.pearsonr(data.iloc[:,i+2], data['Weekly_Sales'])\n    axes[i,3].text(0.4,0.9,'Plot {:n}'.format(i+37),transform=axes[i,3].transAxes)\n    axes[i,3].text(0.75,0.02,'r={:.2f}\\np={:.3f}'.format(r4,p4),transform=axes[i,3].transAxes)\n    axes[i,3].ticklabel_format(axis=\"x\", style=\"sci\", scilimits=(0,0))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"30dc35b0-6176-46c6-971e-563b037becee","_cell_guid":"35747277-617b-4fe6-b146-7f018f35b4cd","trusted":true},"cell_type":"markdown","source":"Aqui podemos notar que as correlações se tornaram mais expressivas em alguns casos, possibilitando conhecer melhor o impacto de cada tipo de oferta em cada feriado.\nDe modo geral, ou seja, considerando o total de ofertas, temos consideráveis associações em todos os feriados, sendo o período entre o Dia de Ação de Graças o mais afetado, seguido pelo Dia do Trabalho, *Super Bowl* e Natal. \n\nA primeira vista, a sequência pode não fazer sentido já que o Natal está em último. Porém, uma hipótese que poderia ser melhor estudada no futuro seria que o período do Natal, por ser tradicionalmente a época que está mais relacionada a compras, é portanto, naturalmente potencializada, gerando o maior pico de venda e consequentemente uma maior capacidade das lojas em absorver esta demanda. Devido a esse fato, pode não ser interessante alavancar ainda mais esse período já esponteneamente vendável e saturar as lojas ou reduzir o faturamento. \nJá os demais feriados seguem uma lógica de acordo com a tradição de vendas americana.\n\nMais especificamente, para cada semana de feriado abaixo, seguem os tipos de ofertas que denotam evidente correlação (maior que moderada), do mais forte para o mais fraco:\n* ***Super Bowl***: *MarkDown* 3, 5, e 2;\n* ***Labor Day***: *MarkDown* 5, 4 e 1;\n* ***Thanksgiving***: *MarkDown* 3;\n* ***Christmas***: *MarkDown* 3.\n\nPelas informações disponibilizadas, não é possível saber o caráter de cada tipo de *MarkDown*, se são frutos de escolhas (eventuais promoções/descontos) ou de consequências (concorrência, custo de estocagem, produtos danificados etc). Sendo assim, recomendar determinado tipo de *MarkDown* a um período de feriado específico é incerto a princípio. Supondo que sejam somente do tipo promoções/descontos, a priorização destacada logo acima seria indicada."},{"metadata":{"_uuid":"4089a0ae-cb96-4730-aad0-ee5ee30f4fc4","_cell_guid":"3f2c5046-2abe-459c-955f-59f1cea5b2e4","trusted":true},"cell_type":"markdown","source":"Partimos agora para o desenvolvimento do modelo preditivo:"},{"metadata":{"_uuid":"e50e0a2c-7578-4981-99dd-1630e78f7d47","_cell_guid":"05329dda-30b3-4dc5-a108-8599a2945fd7","trusted":true},"cell_type":"markdown","source":"## 3) Desenvolvimento do Modelo Preditivo:"},{"metadata":{"_uuid":"0f7e2051-26bd-427b-83bb-92e4db76113f","_cell_guid":"7993fccf-e197-4e59-9242-0d4eac2bd93c","trusted":true},"cell_type":"markdown","source":"Como vimos anteriormente, por se tratar de um problema supervisionado de regressão (vide train.csv), com variáveis tanto categóricas (Store, Dept, Week, IsHoliday...) quanto contínuas (features, Weekly_Sales...), apresentar um comportamento não linear (vide gráfico \"Average Weekly Sales per Year) e de fina granularidade de dados (45 Stores > 99 Depts > 52 Weeks), aplicar algum modelo de aprendizado de máquina baseado no algoritmo **Árvore de Decisão** parece ser uma boa aposta inicialmente, já que ele costuma a lidar bem em *datasets* com este conjunto de particularidades.\n\nA depender dos parâmetros configurados, esse algoritmo pode tender ao fenômeno de *overfitting*, ou seja, ajustar especificamente ao *dataset* de treino e não de uma forma generalizada, perdendo acurácia a receber quaisquer dados diferentes ao que o modelo foi treinado. Portanto, iremos utilizar o algoritmo denominado ***Random Forest*** (ou Floresta Aleatória), o qual também é baseado em **Árvore de Decisão**, porém minimiza a chance de ocorrer *overfitting* ao incorporar múltiplas árvores de decisão em paralelo e tomar o valor mais frequente dos resultados. Mesmo assim, há algum risco de sobreajustar o modelo, então iremos verificar a acurácia em dados de teste adiante.\n\nFeitas as considerações iniciais, vamos implementá-lo:\n\nO primeiro a se fazer é conhecer e selecionar somente as variáveis realmente relevantes para a predição das vendas semanais. Essa etapa é importante pois faz com que o modelo seja mais fácil de compreender, além de proporcioná-lo um melhor desempenho, tanto com relação à acurácia quanto ao tempo de execução."},{"metadata":{"_uuid":"c2ac1b6c-91b7-4dd3-90a6-88e05d755e0c","_cell_guid":"3c0b2657-3213-4e5d-869c-52426507f7d7","trusted":true},"cell_type":"code","source":"train_ftrs = features\ntrain_ftrs['Date'] = pd.to_datetime(train_ftrs['Date'])\ntrain_ftrs.insert(2,'Year',train_ftrs['Date'].dt.year)\ntrain_ftrs = train_ftrs[train_ftrs['Date'] <= (pd.to_datetime('26-Oct-12', format='%d-%b-%y'))]\ntrain_ftrs['Date'] = train_ftrs['Date'].dt.isocalendar().week\ntrain_ftrs = train_ftrs.rename(columns={'Date': 'WeekNo'})\nprint(len(stores.columns.append(train_ftrs.columns).append(train.columns).drop_duplicates()),'variáveis')\nNAs = pd.DataFrame(round(train_ftrs.isnull().sum()/train_ftrs.shape[0], 3)*100,columns=['%NA'])\nNAs.T","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c680ae1b-6ee6-4c58-b4e1-412f04e7300c","_cell_guid":"e9232efc-d89a-4775-8e4e-192905df54ec","trusted":true},"cell_type":"markdown","source":"**Análise das variáveis**:\n    \nNo total temos 17 variáveis. Como visto anteriormente, boa parte das *features* \"*MarkDown*\" têm valores faltantes. Como para o treino é essencial que não haja dados nulos, penso em três opções: \n1.     Desconsiderar as variáveis *MarkDown* para a predição de *Weekly_Sales* > Perda de poder preditivo, já que foi visto uma correlação entre as variáveis acima, especialmente em semanas de feriado, onde a penalização da métrica WMAE é maior;\n2.  Eliminar as linhas onde há dados nulos > eliminar quase 75% do *dataset*, perdendo informações das outras variáveis para compor a predição;\n3.  Substituir os valores pela média ou mediana do respectivo *MarkDown* > resultados enviesados por essa manipulação, já que mais da metade das variáveis são nulas.\n\nA opção que parece mais razoável no momento é desconsiderar as variáveis *MarkDown* para o treino do modelo. Considerando que os registros de remarcação sejam mais constantes a partir de novembro de 2011, é possível incorporar estas variáveis na realimentação do modelo com novos dados históricos.\n\nOutra modificação que a princípio parece ser interessante é incluir uma variável para cada feriado, já que foi visto que cada um deles impacta de forma diferente as vendas semanais, com o Thanksgiving e o *Christmas* apresentando os maiores picos de venda.\n\nJá a normalização das variáveis não é necessária quando se utiliza algoritmos baseados em Arvore de Decisão."},{"metadata":{"_uuid":"792458f2-f94d-4a8f-98ed-7afb786394d3","_cell_guid":"980edb45-adab-45c7-8642-3375cf6f36bc","trusted":true},"cell_type":"code","source":"# Criando variáveis para cada feriado\ntrain_ftrs = train_ftrs.drop(columns=['MarkDown1','MarkDown2','MarkDown3','MarkDown4','MarkDown5'])\nstr_ftrs = stores.merge(train_ftrs, on=['Store'], how='inner')\ntrain_data = train.merge(str_ftrs,on=['Store','WeekNo','IsHoliday','Year'], how='inner')\n\nHolidays\n\nSuperBowl = np.zeros(len(train_data))\nLaborDay = np.zeros(len(train_data))\nThanksgiving = np.zeros(len(train_data))\nChristmas = np.zeros(len(train_data))\nholidays_df = pd.DataFrame({'SuperBowl':SuperBowl, 'LaborDay':LaborDay, 'Thanksgiving':Thanksgiving,'Christmas':Christmas})\ntrain_data = train_data.join(holidays_df)\n\ntrain_data.loc[train_data['WeekNo']==6, 'SuperBowl'] = 1\ntrain_data.loc[train_data['WeekNo']==36, 'LaborDay'] = 1\ntrain_data.loc[train_data['WeekNo']==47, 'Thanksgiving'] = 1\ntrain_data.loc[train_data['WeekNo']==52, 'Christmas'] = 1\n\ntrain_data.Type = [(ord(x)- 64) for x in train_data.Type] # Transformando os valores categóricos (A,B,C) da variável Type em números(1,2,3)\n\ntrain_data['IsHoliday'] = (train_data['IsHoliday'] == True).astype(int) # Transformando os valores booleanos da variável IsHoliday em números(0,1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d1c181ae-36ab-4cc6-9361-a756a20aec2d","_cell_guid":"8166fa83-49af-45c9-88d1-ee0fb6e99427","trusted":true},"cell_type":"markdown","source":"O método usado para selecionar as variaveis mais relevantes foi *Recursive Feature Elimination* utilizando o próprio algoritmo *Random Forest*. A cada iteração o método elimina uma variável, executa o algoritmo e armazena a acurácia encontrada. Ao final das iterações ele determina quais são as variáveis mais relevantes com base na perda de acurácia que cada uma resultou ao ser eliminada.\n\nVamos saber quais são as variáveis mais relevantes e em seguida fazer um comparativo rodando o *Random Forest* uma vez com todas as variáveis e outra vez somente com as variáveis selecionadas."},{"metadata":{"_uuid":"a8a01d22-52aa-4711-b653-7e0edfbf2919","_cell_guid":"f5e6b77d-a287-4c1e-a0c9-8b8742812114","trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.feature_selection import RFE\n\ntrees = 5\nrfe = RFE(RandomForestRegressor(n_estimators=trees), step=1).fit(train_data.loc[:, train_data.columns != 'Weekly_Sales'], train_data['Weekly_Sales'])\nrfe.ranking_\nrfe.support_\n\nRF_imp = RandomForestRegressor(n_estimators=trees)\nRF_imp.fit(train_data.loc[:, train_data.columns != 'Weekly_Sales'], train_data['Weekly_Sales'])\nimportance = RF_imp.feature_importances_\nxy = pd.DataFrame(\n        {'Variables':[(train_data.loc[:, train_data.columns != 'Weekly_Sales'].columns[x]) for x in range(len(importance))],\n                      'Importance (%)':importance*100})\nxy.insert(2,'RFE',rfe.support_)\nfeature_importance = xy.sort_values(by=['Importance (%)'], ascending=False)\nfeature_importance.style.format({'Importance (%)': '{:,.1f}'.format})","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1e2e0429-10df-4bb1-a839-2cfb5d97df11","_cell_guid":"5ac04219-f529-4a64-a044-564367f5a5b3","trusted":true},"cell_type":"markdown","source":"Agora vamos ao comparativo:"},{"metadata":{"_uuid":"2678ab54-e22e-4376-a677-9d24a73205ce","_cell_guid":"890697c8-6f7c-473a-998f-99b7ad224d38","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n\ndef Model(train_data):\n    Train, Test = train_test_split(train_data, random_state=1)\n    \n    X_Train = Train.sort_index().drop(columns=['Weekly_Sales'])\n    Y_Train = Train['Weekly_Sales'].sort_index()\n    \n    X_Test = Test.sort_index().drop(columns=['Weekly_Sales'])\n    Y_Test = Test['Weekly_Sales'].sort_index()\n    \n    # Random Forest Regressor\n    print('Calculando')\n    trees = 5\n    RF = RandomForestRegressor(n_estimators=trees)\n    RF.fit(X_Train, Y_Train)\n    SalesPrediction = RF.predict(X_Test)\n    RF_accuracy = RF.score(X_Test, Y_Test)\n    RF_accuracy = metrics.r2_score(Y_Test, SalesPrediction)\n    print(\"Model Accuracy:\", round(RF_accuracy*100,1),\"%\")\n\n    # AVALIAÇÃO WMAE\n    soma_SalesPred = 0\n    w = np.zeros(len(X_Test.index))\n    for l in range(len(X_Test.index)):\n        if X_Test['IsHoliday'].iloc[l] == 0:\n            w[l] = 1\n        else: w[l] = 5\n        soma_SalesPred += w[l]*abs(Y_Test.iloc[l] - SalesPrediction[l])\n    WMAE = round(soma_SalesPred/np.sum(w),5)\n    print('WMAE =',round(WMAE,2))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"30114dc6-e2ce-48c2-aaeb-fab4140b49af","_cell_guid":"cf8f5657-739a-4a96-a04d-e9b0327b1c37","trusted":true},"cell_type":"code","source":"print('Com todas as variáveis:')\nModel(train_data)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2c65bfc2-fd76-4506-af68-c62b9c338121","_cell_guid":"1541b2a8-d6c3-46c0-ae60-479dd906f54b","trusted":true},"cell_type":"code","source":"print('Somente as 8 variáveis mais relevantes:')\ntrain_data_filtered8 = train_data.drop(columns=['Unemployment','Fuel_Price','Christmas','LaborDay','SuperBowl'])\nModel(train_data_filtered8)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f09ad39d-fc6e-4daf-b5c7-78f0916720ab","_cell_guid":"9a379e94-4cc9-411b-9870-a0ec7e3318f2","trusted":true},"cell_type":"code","source":"print('Somente as 4 variáveis mais relevantes:')\ntrain_data_filtered4 = train_data.loc[:,['Store','Dept','Size','WeekNo','IsHoliday','Weekly_Sales','Year']]\nModel(train_data_filtered4)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"17767db2-5031-45ab-8b63-06157c86b125","_cell_guid":"43dd7f67-074a-4cfd-947f-175f9d2dcdad","trusted":true},"cell_type":"markdown","source":"Houve uma significativa redução do WMAE ao considerar somente as 4 variáveis principais. Ponto positivo para o RFE."},{"metadata":{"_uuid":"01427a4f-c3f5-4537-b808-617a46ed2449","_cell_guid":"31afe5f8-fdf6-4dbe-8a45-96eaa8c7dfd6","trusted":true},"cell_type":"markdown","source":"Agora vamos deixar o modelo mais robusto, ajustando os parâmetros do *Random Forest* de maneira a buscar o menor WMAE. Foi utilizada a ferramenta *GridSearchCV* que executa o *Random Forest* para cada combinação dos intervalos predefinidos de parâmetros e, através do método de validação cruzada, encontra os parâmetros que entregam os melhores resultados."},{"metadata":{"_uuid":"115eebe3-7eb8-4bc9-825e-fe4b80d99995","_cell_guid":"5da2cabf-7b90-46a8-9604-f6c8bb0ed3c5","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\n# Como comentário pois demora alguns minutos para executar:\n#parameters = {'n_estimators':[50,60,70],'min_samples_split':[4,8,16]}\n#grid = GridSearchCV(estimator = RandomForestRegressor(), param_grid=parameters, cv=4, n_jobs=-1, verbose=1)                    \n#grid.fit(train_data_filtered4.drop(columns=['Weekly_Sales']), train_data_filtered4['Weekly_Sales'])\n#grid.best_params_","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f4729d8d-97b0-41de-82d1-6422d804064e","_cell_guid":"93a71749-4421-42f4-aceb-34e6fbd0f4d4","trusted":true},"cell_type":"markdown","source":"Obs: como o *dataset* é relativamente grande, a obtenção dos melhores parâmetros é demorada. Então, tentou-se reduzir o número de combinações para agilizar a execução. Com melhor poder computacional e mais tempo, poderíamos encontrar melhores parâmetros."},{"metadata":{"_uuid":"d559347a-db5b-47f5-8fc6-fea568fd2b08","_cell_guid":"ff457c94-ccd3-4144-8e68-4d9591631a43","trusted":true},"cell_type":"code","source":"def Optmized_Model(train_data):\n    Train, Test = train_test_split(train_data, random_state=1)\n    \n    X_Train = Train.sort_index().drop(columns=['Weekly_Sales'])\n    Y_Train = Train['Weekly_Sales'].sort_index()\n    \n    X_Test = Test.sort_index().drop(columns=['Weekly_Sales'])\n    Y_Test = Test['Weekly_Sales'].sort_index()\n    \n    # Random Forest Regressor\n    print('Calculando')\n    RF = RandomForestRegressor(n_estimators=70, min_samples_split=4)\n    RF.fit(X_Train, Y_Train)\n    SalesPrediction = RF.predict(X_Test)\n    RF_accuracy = RF.score(X_Test, Y_Test)\n    RF_accuracy = metrics.r2_score(Y_Test, SalesPrediction)\n    print(\"Model Accuracy:\", round(RF_accuracy*100,1),\"%\")\n\n    # AVALIAÇÃO WMAE\n    soma_SalesPred = 0\n    w = np.zeros(len(X_Test.index))\n    for l in range(len(X_Test.index)):\n        if X_Test['IsHoliday'].iloc[l] == 0:\n            w[l] = 1\n        else: w[l] = 5\n        soma_SalesPred += w[l]*abs(Y_Test.iloc[l] - SalesPrediction[l])\n    WMAE = round(soma_SalesPred/np.sum(w),5)\n    print('WMAE =',round(WMAE,2))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7272f829-832e-4e67-95b5-900a1c46b011","_cell_guid":"72e69e6a-9640-4d2c-8f84-e184367eb546","trusted":true},"cell_type":"markdown","source":"Rodando com os parâmetros encontrados: 'min_samples_split' = 4 e 'n_estimators' = 70"},{"metadata":{"_uuid":"8b90abea-cb78-4e35-a34c-5714039c5366","_cell_guid":"dfc89d9b-14cf-444d-8624-4865492e67d4","trusted":true},"cell_type":"code","source":"Optmized_Model(train_data_filtered4)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3f750497-4d38-4851-adba-16bfce96147b","_cell_guid":"033dffe1-2df2-4be2-9523-9b1ca565e0ed","trusted":true},"cell_type":"markdown","source":"Novamente, nota-se que o valor de WMAE diminuiu, indicando bons resultados ao refinar o modelo."},{"metadata":{"_uuid":"bac2b335-8dc5-4ce9-b591-3cef744c828a","_cell_guid":"b600a668-3c04-43bb-9e9e-6d6560252308","trusted":true},"cell_type":"markdown","source":"Finalmente, vamos implementá-lo aos dados de teste para submissão do arquivo:"},{"metadata":{"_uuid":"d9d79040-b69c-44c1-bd28-781f3d635f94","_cell_guid":"40bdc7b9-1bc5-428c-9ca9-4455eabade75","trusted":true},"cell_type":"code","source":"Train = train.merge(stores, on=['Store']).drop(columns=['Type'])\nTrain['IsHoliday'] = (Train['IsHoliday'] == True).astype(int)\n\nX_Train = Train.drop(columns=['Weekly_Sales'])\nY_Train = Train['Weekly_Sales']\n\nTest = test.merge(stores, on=['Store']).drop(columns=['Type'])\nTest['IsHoliday'] = (Test['IsHoliday'] == True).astype(int)\n\nprint('Calculando')\nRF_Test = RandomForestRegressor(n_estimators=70, min_samples_split=4)\nRF_Test.fit(X_Train, Y_Train)\nSalesPrediction_Test = RF_Test.predict(Test)\nWeekly_Sales_Test = pd.Series(SalesPrediction_Test, name='Weekly_Sales')\nWeekly_Sales_Test.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"54faed51-956f-40cc-af0b-c6d9246c8c75","_cell_guid":"3af5ab16-e1bf-4b2c-b286-fe7594122e6c","trusted":true},"cell_type":"markdown","source":"Plotando os resultados no mesmo gráfico de vendas médias por semana, vemos que o modelo adere bem ao comportamento dos dados históricos:"},{"metadata":{"_uuid":"1a20e2d4-a2dc-4244-be31-b132b9dc8759","_cell_guid":"e33746e2-90cc-48c1-84b8-fa70462a1c6b","trusted":true},"cell_type":"code","source":"# Configurando o gráfico:\nWeekly_Sales_2010 = train[train.Year==2010]['Weekly_Sales'].groupby(train['WeekNo']).mean()\nWeekly_Sales_2011 = train[train.Year==2011]['Weekly_Sales'].groupby(train['WeekNo']).mean()\nWeekly_Sales_2012 = train[train.Year==2012]['Weekly_Sales'].groupby(train['WeekNo']).mean()\nSales = Test.join(Weekly_Sales_Test)\nWeekly_Sales_Prediction_2012 = Sales[Sales.Year==2012]['Weekly_Sales'].groupby(Test['WeekNo']).mean()\nWeekly_Sales_Prediction_2013 = Sales[Sales.Year==2013]['Weekly_Sales'].groupby(Test['WeekNo']).mean()\nplt.figure(figsize=(20,7))\nsns.lineplot(x=Weekly_Sales_2010.index, y=Weekly_Sales_2010.values)\nsns.lineplot(x=Weekly_Sales_2011.index, y=Weekly_Sales_2011.values)\nsns.lineplot(x=Weekly_Sales_2012.index, y=Weekly_Sales_2012.values)\nsns.lineplot(x=Weekly_Sales_Prediction_2012.index, y=Weekly_Sales_Prediction_2012.values, color='red',linewidth=3.0)\nsns.lineplot(x=Weekly_Sales_Prediction_2013.index, y=Weekly_Sales_Prediction_2013.values, color='red',linewidth=3.0)\nplt.xticks(range(min(train['WeekNo']), max(train['WeekNo'])+1,1))\nplt.legend(['2010', '2011', '2012','Sales Prediction'], loc='best', fontsize=16)\nplt.gca().add_artist(plt.legend(['2010', '2011', '2012','Sales Prediction'], loc='upper left', fontsize=16))\nplt.title('Sales Prediction\\n', fontsize=18)\nplt.ylabel('Average Sales', fontsize=16)\nplt.xlabel('Week', fontsize=16)\ni=0\ncolors = ['blue', 'lime', 'magenta', 'cyan']\nfor x in Holidays.iloc[:3,:].drop_duplicates().stack():\n    plt.axvline(x, color = colors[i], label = Holidays.columns[i])\n    plt.legend(loc='upper center',fontsize=16)\n    i = i + 1\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8e4e3d95-ef8f-4270-b3c0-3fe0d2dd830a","_cell_guid":"58a034ee-4b6d-4c2c-aad3-b4d06c9e3cda","trusted":true},"cell_type":"markdown","source":"Criando o arquivo de submissão:"},{"metadata":{"_uuid":"e933acfb-25a8-4790-82dc-c0f6bb22f435","_cell_guid":"6d884927-9689-4852-8fc7-d5bd6a17e6c1","trusted":true},"cell_type":"code","source":"Submission_File = pd.read_csv('../input/walmart-recruiting-store-sales-forecasting/sampleSubmission.csv.zip', sep=',')\nSubmission_File['Weekly_Sales'] = Weekly_Sales_Test\nSubmission_File.to_csv('Submission_File.csv',index=False)\nSubmission_File","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dcc1a27b-f4bc-427f-8d95-a44018fb58d4","_cell_guid":"4d9d3c5c-a9ff-4b0f-a104-97ecd17f4b2c","trusted":true},"cell_type":"markdown","source":"**Submission score (11Mar2021):** 2802.54183"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}