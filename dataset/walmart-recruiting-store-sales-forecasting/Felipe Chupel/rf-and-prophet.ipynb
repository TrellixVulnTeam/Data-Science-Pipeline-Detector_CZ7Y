{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Abstract\n\nThis notebook seeks to find a good solution to the problem of predicting sales at Walmart stores and departments based on data provided by Kaggle. In the document, extraction, cleaning and exploratory data analysis were performed, as well as testing different machine learning algorithms, choosing the one with the least absolute error in the cross validation test."},{"metadata":{},"cell_type":"markdown","source":"## Notes\n\n1. Some features are more important to the constructed model than others. It's nice implement models sometimes deleting/adding features to verify if the model's performance improves.\n2. It's interesting adding columns based on other columns. Sometimes infos related to some feature may be better represented in another way.\n3. In this notebook, I have called train data as the union of train and validation data. So, where I referece train data, you can read train + validation.\n4. I have chosen random Forest Regressor and Python as base alghoritms for model predictions. There arte a lot of other models that could be tested (Decision Tree, Bossting Alghoritms, Neural Networks, Logistic Regression, etc). If you have the time, it is better test some of them.\n5. I have not used cross validation to measure Prophet performance, but I have used to mesaure Random Forest Regressor. Although it is almost always recommend to use cross validation to reduce overfitting, the technique is different to Time Series because dates from train set have to be lower than dates from validation set, it is not possible to randomly select a folder from data. Due to the fact that it was provided only two years of data and I have identified yearly seasonality, it become impratctible apply cross validation for the data. To compare WMAE from Prophet and Random Forest, I have used mean WMAE from cross validation in Random Forest."},{"metadata":{},"cell_type":"markdown","source":"## Importing Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Importing general libraries\nimport numpy as np\nimport pylab as plt\nimport pandas as pd\nfrom scipy.stats.stats import pearsonr\nfrom matplotlib import pyplot as plt\nfrom matplotlib.gridspec import GridSpec\nimport seaborn as sns\nfrom scipy import stats\nfrom string import ascii_letters\nimport math\nimport random\nfrom sklearn.ensemble import RandomForestRegressor\n\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loading Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Loading data from local csv files\n\n\ndf_stores = pd.read_csv('../input/data-files/stores.csv')\ndf_features = pd.read_csv('../input/data-files/features.csv')\ndf_train = pd.read_csv('../input/data-files/train.csv')\ndf_test = pd.read_csv('../input/data-files/test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Merging Data Into a Single Dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Function to remove columns not desired and rename desired columns - It was created to modify dataframes resulted from left join operation between dataframes\ndef standadize_columns(df):\n    df_aux = df;\n    df_aux.columns = [x.replace('_left', '').replace('_x', '') if x.find('_left') != -1 or x.find('_x') != -1 else x for x in df_aux.columns]\n    for x in df_aux.columns:\n        if x.find('_right') != -1 or x.find('_y') != -1:\n            del df[x]\n    df_aux = df_aux.drop_duplicates()\n    return df_aux;\n\ndef deduplicate_array(arr_elements):\n    elements = []\n    for x in arr_elements:\n        if x not in elements:\n            elements.append(x)\n    return elements\n\n#Comcatenate Train and test Data to obtain a more complete dataframe\ndf_train['Data_Type'] = 'Train'\ndf_test['Data_Type'] = 'Test'\ndf_train_test = pd.concat([df_train, df_test])\n\n#Converting 'Date' column, for each dataframe, to datetime object\ndf_features['Date'] = pd.to_datetime(df_features['Date'])\ndf_train_test['Date'] = pd.to_datetime(df_train_test['Date'])\ndf_train['Date'] = pd.to_datetime(df_train['Date'])\ndf_test['Date'] = pd.to_datetime(df_test['Date'])\ndf_features['Date'] = pd.to_datetime(df_features['Date'])\n\n#Merging dataframe with features data\ndf_blended_data = df_train_test.merge(df_features, how='left', left_on=[\"Date\", \"Store\"], right_on=[\"Date\",\"Store\"])\ndf_blended_data = standadize_columns(df_blended_data)\n\n#Merging dataframe with stores data\ndf_blended_data = df_blended_data.join(df_stores.set_index('Store'), on = 'Store', how='left', \n                                          lsuffix='_left', rsuffix='_right')\n\ndf_blended_data = standadize_columns(df_blended_data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Adding New Columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Adding columns that shows week, month and year of row date\ndf_blended_data['Year_Month'] = df_blended_data['Date'].dt.to_period('M')\ndf_blended_data['Year'] = pd.DatetimeIndex(df_blended_data['Date']).year\ndf_blended_data['Month'] = pd.DatetimeIndex(df_blended_data['Date']).month\ndf_blended_data['Week'] = pd.DatetimeIndex(df_blended_data['Date']).week\ndf_blended_data['Day'] = pd.DatetimeIndex(df_blended_data['Date']).day\ndf_blended_data['Year_Week'] = df_blended_data['Year'] + df_blended_data['Week']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Classify columns according their type"},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_features = ['Data_Type', 'Type', 'Year_Month', 'Year_Week', 'Year', 'Month', 'Week', 'Store', 'Dept']\nnumerical_features = ['Weekly_Sales', 'Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3', \n                      'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment', 'Size'\n                     ]\n\n#Forcing Categorical Columns to be String\nfor feature in categorical_features:\n    df_blended_data[feature] = df_blended_data[feature].astype('str')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Inspecting Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Printing first lines of dataframe\ndisplay(df_blended_data.head())\n\n#Printing Features general info\ndf_description = df_blended_data.groupby('Data_Type').describe()\ndf_T = df_description.T\n\nwith pd.option_context('display.max_rows', None, 'display.max_columns', None):\n    display(df_T)\n\n#Printing column variables type and null count\nprint(df_blended_data.info())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As it is shown by data above, markdown columns have a lot of null values. For column 'MarkDown2', more than 60% of values are null. If these columns are not highly correlated with the amount of sales, it is recommended to eliminate them."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Obtain holidays\ndf_blended_data[df_blended_data['IsHoliday'] == True]['Date'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Verify Days With More Weekly Sales: This is good because we can inspect these days closely\nweekly_sales = df_blended_data.groupby(['Date'])['Weekly_Sales'].agg('sum').reset_index().copy()\nweekly_sales = weekly_sales.sort_values(by=['Weekly_Sales'], ascending = False)\nweekly_sales.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, the dates with more sales are weeks with holidays, in Christmas case, weeks that preced holidays. This is something interesting to observe because we can model as christmas holiday the weeks right before Christmas and not Christmas week itself. This algo give us an insight that people may get their supermarket products, for Christmas specifically, in advance."},{"metadata":{},"cell_type":"markdown","source":"### Adding More Columns"},{"metadata":{},"cell_type":"markdown","source":"Now it is time to break holidays by type and consider weeks right before Christmas as holiday"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Inserting Holidat Type Column\ndf_blended_data['Holiday_Type'] = ''\ndf_blended_data.loc[(df_blended_data['IsHoliday'] == True) & (df_blended_data['Month'] == 2), 'Holiday_Type'] = 'Super Bowl'\ndf_blended_data.loc[(df_blended_data['IsHoliday'] == True) & (df_blended_data['Month'] == 9), 'Holiday_Type'] = 'Labor Day'\ndf_blended_data.loc[(df_blended_data['IsHoliday'] == True) & (df_blended_data['Month'] == 11), 'Holiday_Type'] = 'Thanksgiving'\ndf_blended_data.loc[(df_blended_data['Date'] == pd.to_datetime('2010-12-24')), 'Holiday_Type'] = 'Christmas W-1'\ndf_blended_data.loc[(df_blended_data['Date'] == pd.to_datetime('2011-12-23')), 'Holiday_Type'] = 'Christmas W-1'\ndf_blended_data.loc[(df_blended_data['Date'] == pd.to_datetime('2010-12-17')), 'Holiday_Type'] = 'Christmas W-2'\ndf_blended_data.loc[(df_blended_data['Date'] == pd.to_datetime('2011-12-16')), 'Holiday_Type'] = 'Christmas W-2'\ndf_blended_data.loc[(df_blended_data['Date'] == pd.to_datetime('2010-12-17')), 'Holiday_Type'] = 'Christmas W-3'\ndf_blended_data.loc[(df_blended_data['Date'] == pd.to_datetime('2011-12-16')), 'Holiday_Type'] = 'Christmas W-3'   \ndf_holidays = df_blended_data[df_blended_data['IsHoliday'] == True][['Date', 'IsHoliday', 'Holiday_Type']].drop_duplicates()\n\n#Including Holiday Type in Array Of Categorical Features\ncategorical_features.append('Holiday_Type')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Creating new columns that give numerical values to categorical features"},{"metadata":{},"cell_type":"markdown","source":"It is important to give numerical values to categorical columns because we can use them in regression models. In the code below, numerical values, for each categorical feature, were marked according to the order of average weekly sales for each possible value in the features. For example, if Type = 'C' returns bigger average sales then Type = 'A', then std_Type (numerical value to column Type) for Type = 'A' < std_type for Type = 'C'. This order was constructed due to the fact that improves the probability of best performance by regresison models."},{"metadata":{"trusted":true},"cell_type":"code","source":"# For each categorical variable, create a column that lists a number for each value of the categorical variable. \n# The related values are in increasing order of impact on the amount of sales \ndf_aux = df_blended_data\nfor feature in categorical_features:\n    df_avg_sales = df_blended_data.groupby(feature)['Weekly_Sales'].agg('mean').reset_index().drop_duplicates().dropna()\n    # Order categorical values according to weekly sales\n    df_avg_sales = df_avg_sales.sort_values(by=['Weekly_Sales']).reset_index()\n    new_col_name = 'std_' + feature\n    numerical_features.append(new_col_name)\n    df_avg_sales[new_col_name] = df_avg_sales.index\n    df_aux = df_aux.join(df_avg_sales.set_index(feature), on = feature, how='left', \n                                              lsuffix='_left', rsuffix='_right')\n    \n    df_aux = standadize_columns(df_aux)\n\ndf_blended_data = df_aux.copy()\nnumerical_features = deduplicate_array(numerical_features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Correlation Between Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Code imported from https://seaborn.pydata.org/examples/many_pairwise_correlations.html\n\nsns.set(style=\"white\")\ncorr = df_blended_data[numerical_features].corr()\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\nf, ax = plt.subplots(figsize=(20, 16))\ncmap = sns.diverging_palette(220, 15, as_cmap=True)\nplt.title('Correlation Matrix', fontsize=18)\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, \n            center=0, square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True)\n\ncorr_weekly_sales = corr['Weekly_Sales'].loc[~corr['Weekly_Sales'].index.isin(['Weekly_Sales'])]\ncorr_weekly_sales.sort_values().tail(15)\n\nfig = plt.figure(figsize=(16,8))\nplt.bar(corr_weekly_sales.index, corr_weekly_sales.values)\nplt.xticks(rotation=90)\nplt.grid()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are features correlated (|correlation| > 0.7) and therefore it is not interesting using them together in the models. For each pair of correlated features, it is recommended to use those that have biggest correlation with weekly sales and have bigger importances in model prediction. Something that is valid to observe is that information associated with Department seems to have high importance in model prediction."},{"metadata":{},"cell_type":"markdown","source":"## Categorical Variables Distribution"},{"metadata":{},"cell_type":"markdown","source":"Observing categorical features distribution is nice to understand data distribution. For example, if, for a specific categorical feature, a certain value occurs way more than other, it is good to know because it can bias the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(32,16))\nfor feat in ['Data_Type', 'Type', 'Store', 'Dept']:\n    sns.catplot(x= feat, kind = \"count\", palette=\"ch:.25\", data=df_blended_data)\n    plt.xticks(rotation=90)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Weekly Sales Distribution For Each Numerical feature"},{"metadata":{},"cell_type":"markdown","source":"Below, i have plotted boxplot, breaking in 'Type', 'Store', 'Dept' and 'IsHoliday' for each numerical value. The graphs give us important infomation not only about data assimetry as give us too information about how each of the for categorical features are related to the numerical features. "},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"def order_cat_variables(cat_var, num_var):\n    cat_sorted_average = df_blended_data.groupby([cat_var])[num_var].agg('mean').reset_index().sort_values(by=num_var)[cat_var]\n    return cat_sorted_average.unique()\n\ndef print_pretty_subplot(df):\n    qty_variables = len(numerical_features);\n    fig, axes = plt.subplots(qty_variables, 2, figsize=(60, 300));\n    i = 1;\n    for feature in numerical_features:\n        try:\n            plt.subplot(2*qty_variables, 2, i)\n            order_type = order_cat_variables('Type', feature)\n            sns.boxplot(x='Type', y=feature, data=df_blended_data, showfliers=False, order = order_type)\n            plt.subplot(2*qty_variables, 2, i + 1)\n            order_store = order_cat_variables('Store', feature)\n            sns.boxplot(x='Store', y=feature, data=df_blended_data, showfliers=False, order = order_store)\n            plt.subplot(2*qty_variables, 2, i + 2)\n            order_dept = order_cat_variables('Dept', feature)\n            sns.boxplot(x='Dept', y=feature, data=df_blended_data, showfliers=False, order = order_dept)\n            plt.subplot(2*qty_variables, 2, i + 3)\n            order_holiday = order_cat_variables('IsHoliday', feature)\n            sns.boxplot(x='IsHoliday', y=feature, data=df_blended_data, showfliers=False, order = order_holiday)\n        except Exception as e:\n            print(\"Error: \", str(e), \" i = \", i)\n        i = i + 4\n\nnumerical_features = deduplicate_array(numerical_features)\nprint_pretty_subplot(df_blended_data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Graphs on the left side:\n\n1. As shown in Boxplot 1, on average, Type C has presented more weekly sales. When comparing Types A and B, it is possible to say that they both have similar interquantile range, which means that 75% of their weekly sales are up to 16000.\n\n2. Boxplot 2 shows that the departments with the largest weekly sales are 38, 95 and 92. It is worth to mention that department 65 has more consistent weekly sales, as its quantiles are close to its average.\n\n3. Boxplot 3 shows that Type C has higher average and quantiles for Temperature. When comparing departments, it is noticeable on Boxplot 4 that only departments 77, 43 and 39 behave differently than other departments, with lower average and quantiles.\n\n4. There isn't much difference between Types when compared with fuel price, as shown on Boxplot 5. When comparing departments, however, Boxplot 6 shows that some departments present lower averages: 51, 78, 45 and 65.\n\n5. Boxplot 7 shows that Type A presents highest average and quantiles, while Type C presents significantly lower values, when compared with Markdown 1. Boxplot 8 compares departments with Markdown 1, and shows that departments with higher average are 39 and 78. Most departments have the similar average values, however, departments 65 and 43 have more consistent values, as their quantiles are closer to the average than other departments.\n\n6. Boxplot 9 shows that all types have similar average but Type A presents more dispersed values. As for departments (Boxplot 10), 43 has the highest average and quantiles, while 51 has lower values.\n\n7. Boxplot 11 shows that Type A has higher values, while Boxplot 12 shows that the only department that stood out was 77.\n\n8. Boxplot 13 presents similar information as Boxplot 11. When comparing departments (Boxplot 14), the ones that stand out are 65, 51, with more consistent values, and 43, with higher average and quantiles.\n\n9. Boxplot 15 presents similar information as Boxplot 13, where Type A has higher average and quantiles. When comparing the departments on Boxplot 16, the ones that stand out are 77, with the longest range, and 43, with lower values.\n\n10. Boxplot 18 shows that Type A has the highest average, while Boxplot 19 shows thar departments 65, 43 and 39 have the lowest values.\n\n11. Boxplot 20 doesn't show much difference between types. However, Boxplot 21 shows that department 65 has the highest average and quantiles, and 39 has the most consistent values.\n\n12. Boxplot 22 shows that Type A has the highest values, while Type C has the lowest. When comparing departments (Boxplot 23), the ones that stand out are 65, 50, 99, 37 and 39.\n\n#### Graphs on the right side\n\n1. Boxplot 1 shows that Stores 5, 33, 44, 3 and 38 have the lowest weekly sales, while Stores 2, 13, 14, 4 and 20 have the highest weekly sales.\n\n2. Boxplot 2 does not show difference in weekly sales, rather it is a holiday or not.\n\n3. Boxplot 3 shows that Stores 7, 26, 16 have the lowest values, while 10, 42 and 33 have the highest values.\n\n4. Boxplot 4 shows that values are high when it is a holiday.\n\n5. Boxplots 5 compares stores weekly sales and fuel price, and shows that store 19 has the highest values.\n\n6. Boxplot 7 shows that when compraring with Markdown 1, stores 37, 43, 36, 33, 42, 38, 44 and 30 have the lowest weekly sales, while 27 and 19 have the largest weekly sales on average. Boxplot 8 does not show much difference rather it is a holiday or not.\n\n7. Boxplot 9 shows that when compraring with Markdown 2, stores 33, 37, 44, 38, 30 and 36 have the lowest weekly sales, while 27 and 13 have the longest range of weekly sales. Boxplot 10 shows that when it is a holiday, the range is longer.\n\n8. Boxplot 11 shows that when compraring with Markdown 3, stores 36, 33, 30, 44, 43, 42 and 36 have the lowest weekly sales, while 27 and 13 have the longest range of weekly sales. Boxplot 12 shows that when it is a holiday, the range is longer.\n\n9. Boxplot 13 shows that when compraring with Markdown 4, stores 36, 33, 30, 44, 43, 42 and 36 have the lowest weekly sales, while 12 and 4 have the longest range of weekly sales, and highest average weekly sales. Boxplot 14 shows that when it is a holiday, the range is slightly longer, although lower average.\n\n10. Boxplot 15 shows that when compraring with Markdown 5, store39 stands out with  the longest range of weekly sales, and highest average weekly sales. Boxplot 16 does not show difference rather it is a holiday or not.\n\n11. Boxplot 17 shows that CPI impacts directly on weekly sales of every store, however, Boxplot 18 shows that there is no difference rather it is a holiday or not.\n\n12. Boxplot 18 shows that unemployment also impacts directly on weekly sales of every store, however, Boxplot 19 shows that there is no difference rather it is a holiday or not.\n"},{"metadata":{},"cell_type":"markdown","source":"## Normalize data"},{"metadata":{},"cell_type":"markdown","source":"Normalize data is important because improves model performance. Data not normally distributed tend to generate biggest errors in predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import preprocessing\n\nnum_x = []\nfor feat in numerical_features:\n    if feat not in num_x and feat != 'Weekly_Sales':\n        num_x.append(feat)\n        \nscaler = preprocessing.StandardScaler().fit(df_blended_data[num_x])\n\ndf_blended_data = df_blended_data.sort_values(by=['Store', 'Dept', 'Date', 'Weekly_Sales', 'IsHoliday', 'Data_Type',\n       'Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3',\n       'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment', 'Type', 'Size',\n       'Year_Month', 'Year', 'Month', 'Week', 'Day', 'Year_Week',\n       'Holiday_Type', 'index', 'std_Data_Type', 'std_Type', 'std_Year_Month',\n       'std_Year_Week', 'std_Year', 'std_Month', 'std_Week', 'std_Store',\n       'std_Dept', 'std_Holiday_Type'])\n\n#creating dataframe that will be used to recover original data at the end of notebook\ndf_original_data = df_blended_data.copy().sort_values(by=['Store', 'Dept', 'Date', 'Weekly_Sales', 'IsHoliday', 'Data_Type',\n       'Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3',\n       'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment', 'Type', 'Size',\n       'Year_Month', 'Year', 'Month', 'Week', 'Day', 'Year_Week',\n       'Holiday_Type', 'index', 'std_Data_Type', 'std_Type', 'std_Year_Month',\n       'std_Year_Week', 'std_Year', 'std_Month', 'std_Week', 'std_Store',\n       'std_Dept', 'std_Holiday_Type'])\n\ndf_changed_data = df_blended_data.copy().sort_values(by=['Store', 'Dept', 'Date', 'Weekly_Sales', 'IsHoliday', 'Data_Type',\n       'Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3',\n       'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment', 'Type', 'Size',\n       'Year_Month', 'Year', 'Month', 'Week', 'Day', 'Year_Week',\n       'Holiday_Type', 'index', 'std_Data_Type', 'std_Type', 'std_Year_Month',\n       'std_Year_Week', 'std_Year', 'std_Month', 'std_Week', 'std_Store',\n       'std_Dept', 'std_Holiday_Type'])\n\ndf_blended_data[num_x] = scaler.transform(df_blended_data[num_x])\ndf_changed_data[num_x] = scaler.transform(df_changed_data[num_x])\n\ndf_changed_data.columns = ['new_' + str(col) for col in df_changed_data.columns]\n\ndf_mix_data = pd.concat([df_original_data, df_changed_data], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Spliting Data and Removing MarkDown Columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"# The code below split data between train (train + validation) and test\n\nmarkdown_columns = ['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']\n\n#Drop MarkDown Columns\ntrain_data = df_blended_data[df_blended_data['Data_Type'] == 'Train'][numerical_features].drop(markdown_columns, axis = 1).to_numpy()\ntrain_columns = df_blended_data[df_blended_data['Data_Type'] == 'Train'][numerical_features].drop(markdown_columns, axis = 1).columns\ntrain_columns = list(train_columns)\n\ntrain_columns.remove('Weekly_Sales')\n\nnrow, ncol = train_data.shape\ny = train_data[:,-0]\nX = train_data[:,1:ncol]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Features Importances"},{"metadata":{},"cell_type":"markdown","source":"Calculating features importance is recommended because give us an idea of which variables are important to the model and which are not."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Codes imported from https://machinelearningmastery.com/calculate-feature-importance-with-python/ and https://jakevdp.github.io/PythonDataScienceHandbook/02.08-sorting.html\n\nimport math\nfrom statsmodels.tools.eval_measures import rmse\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import RandomizedSearchCV\n\np = 0.25 # fracao de elementos no conjunto de teste\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size = p, random_state = 42)\n\nrf = RandomForestRegressor()\n\nnp.random.seed(500)\n\n#Commented code below was used to obtain best hyperparaeters of random forest\n'''\n# Number of trees in random forest\nn_estimators = [20, 100, 500]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [10, 30, 50]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [2, 5]\n# Method of selecting samples for training each tree\nbootstrap = [False]\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\n'''\n\nrf = RandomForestRegressor(n_estimators=100, n_jobs=-1, min_samples_split=5,min_samples_leaf= 2,\n                           max_features=0.5,max_depth=50,bootstrap=False)\n\n\nrf.fit(x_train, y_train)\npredictions = rf.predict(x_test)\n\ndef selection_sort(x):\n    for i in range(len(x)):\n        swap = i + np.argmax(x[i:])\n        (x[i], x[swap]) = (x[swap], x[i])\n    return x\n\nfeats = {}\nfeats_cum = {}\ncum_importance = 0\nfor feature, importance in zip(train_columns, rf.feature_importances_):\n    feats[feature] = importance\n\nfeats = dict(sorted(feats.items(), key=lambda item: item[1], reverse = True))\nfor k, v in feats.items():\n    cum_importance = cum_importance + v\n    feats_cum[k] = cum_importance\n\nplt.bar([x[0] for x in feats.items()], [x[1] for x in feats.items()], label = 'Feature Importance')\nplt.plot([x[0] for x in feats_cum.items()], [x[1] for x in feats_cum.items()], label = 'Cum. Feature Importance', color = 'red')\nplt.xticks(rotation=90)\nplt.legend(loc='lower right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Time Series Behavior"},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.graphics.tsaplots import plot_acf,plot_pacf \n%matplotlib inline\n\nweekly_sales = df_blended_data[df_blended_data['Data_Type'] == 'Train'].groupby('Date')['Weekly_Sales'].agg('sum').reset_index().drop_duplicates().sort_values(by=['Date'])\n\ntserie = df_blended_data[df_blended_data['Data_Type'] == 'Train'].groupby('Date')['Weekly_Sales'].agg('sum').reset_index().drop_duplicates()\ntserie.columns = ['ds', 'y'];\ntserie['ds'] = pd.to_datetime(tserie['ds'])\ntserie = tserie[['ds', 'y']]\ntserie = tserie[tserie['ds'] < pd.to_datetime('2020-09-01')]\n\nweekly_sales.set_index('Date', inplace = True)\nweekly_sales.plot(label = 'Sales', legend = True)\n\nplot_acf(tserie.set_index('ds'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Autocorrelation graph above show us that lags 1 and 2 are the ones that indeed influence values in a given week. This means, therefore, that, for a given week, weekly sales from W-1 and W-2 probably have impact in current week sales.\nTime Series plot suggest us that we have periods with high seasonal effect (ThanksGiving and Christmas Holiday) and it is important to consider this in the model."},{"metadata":{},"cell_type":"markdown","source":"## Time Serie Components"},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tsa.seasonal import seasonal_decompose\nfrom matplotlib import pyplot\n\nweekly_sales_2 = df_blended_data[(df_blended_data['Data_Type'] == 'Train')].groupby('Date')['Weekly_Sales'].agg('sum').reset_index().drop_duplicates().sort_values(by=['Date'])[['Weekly_Sales']]\n\nresult = seasonal_decompose(weekly_sales_2, model='additive', freq = 52)\nfig = plt.figure(figsize=(20,20))\nresult.plot()\nplt.xticks(rotation=90)\n\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Time Series components validates our hypothesis from before: we have indeed have periods with high seasonal effect (ThanksGiving and Christmas Holiday). Weekly Sales trend is ascending and show us that we can expect more weekly sales in future"},{"metadata":{},"cell_type":"markdown","source":"## Prophet"},{"metadata":{"trusted":true},"cell_type":"code","source":"from fbprophet import Prophet\nm = Prophet();\nm.add_seasonality(name='yearly', period=365, fourier_order=5);\n\nregressors = ['std_Dept', 'std_Store', 'std_Type', 'CPI', 'Unemployment', 'std_Week', 'std_Holiday_Type', 'Size']\nfor regressor in regressors:\n    m.add_regressor(name = regressor, mode = 'multiplicative', standardize = False)\n\ninterest_columns = regressors\ninterest_columns.append('Date')\nfeatures_group = interest_columns\nfeatures_group.append('Data_Type')\n\ntserie = df_blended_data.sort_values(by=['Date'])\ntserie = tserie.rename(columns = {'Date': 'ds', 'Weekly_Sales': 'y'}, inplace = False)\n\ntserie = df_blended_data.groupby(features_group)['Weekly_Sales'].agg('sum').reset_index().drop_duplicates()\ntserie = tserie.rename(columns = {'Date': 'ds', 'Weekly_Sales': 'y'}, inplace = False)\ntserie['ds'] = pd.to_datetime(tserie['ds'])\n\ntreino = tserie[tserie['Data_Type'] == 'Train']\n\nteste = tserie[tserie['Data_Type'] == 'Test']\n\nm.fit(treino)\n\nperiods_test = len(teste.ds.unique())\n\nfuture = m.make_future_dataframe(periods=periods_test, freq = '7d')\n\nfuture = future.merge(teste, how='inner', on = ['ds'])\nfuture = future.sort_values(by=['ds'])\nfcst = m.predict(future);\n\npast_data = treino.groupby('ds')['y'].agg('sum')\nfcst_day = fcst.groupby('ds')['yhat'].agg('sum')\npast_data = past_data.reset_index()\nfcst_day = fcst_day.reset_index()\n\nplt.plot(past_data.ds, past_data.y, label = \"Real\")\nplt.plot(fcst_day.ds, fcst_day.yhat, label = \"Fitted Values\")\nplt.xticks(rotation=90)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Prophet Algo is not predicting well as we can we by grapgh above. I have to investigate why this happened once it were expected satisfatory results from this alghoritm."},{"metadata":{},"cell_type":"markdown","source":"## Random Forest Regressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = rf.predict(x_test)\nprint(y_test, ' ', type(y_test), ' ', len(y_test))\nprint(predictions, ' ', type(predictions), ' ', len(predictions))\nprint(fcst, ' ', type(fcst), ' ', len(fcst))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Calculating Error and Selecting Best Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Due to the fact that this Kaggle problem seems to use WMAE as metric to evaluate model performance, this notebook will show how to calculate WMAE, but I am going to use use RMSE as performance measure."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Function that calculates WMAE between predicted values and test values\ndef WMAE(y, y_pred, is_holiday):\n    #Give weight 5 for holidays\n    hol_multiply = [1 if x == 0 else 5 for x in is_holiday]\n    wae = np_absolute(np_subtract(y_pred, y))\n    wae = np.multiply(wae, hol_multiply)\n    return wae","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n\nrf = RandomForestRegressor(n_estimators=100, n_jobs=-1, min_samples_split=5,min_samples_leaf= 2,\n                           max_features=0.5,max_depth=50,bootstrap=False)\n\nused_features = ['Weekly_Sales', 'std_Dept', 'std_Store', 'std_Type', 'CPI', 'Unemployment', \n                 'Year', 'std_Week', 'std_Holiday_Type', 'Size']\n\n#Droping rows with null values. This is not the right way to do. It is better fill the null values with statisticlal mesaures (mean, median, etc)\n#of other rows with similars characteristics or take external variables\n\ndf_blended_data['CPI'] = df_blended_data['CPI'].fillna(0)\ndf_blended_data['Unemployment'] = df_blended_data['Unemployment'].fillna(0)\ntrain_data = df_blended_data[df_blended_data['Data_Type'] == 'Train'][used_features].sort_values(by=used_features).dropna().to_numpy()\ntrain_columns = df_blended_data[df_blended_data['Data_Type'] == 'Train'][used_features].columns\ntrain_columns = list(train_columns)\ntrain_columns.remove('Weekly_Sales')\n\nnrow, ncol = train_data.shape\ny = train_data[:,-0]\nX = train_data[:,1:ncol]\n\np = 0.25 # elemets fraction to be used on test\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size = p, random_state = 42)\n    \ndef evaluate_models(dict_model):\n    best_rmse = 999999999\n    best_model = 'Prophet'\n    for k, v in dict_model.items():\n        if k == 'Prophet':\n            pass\n        else:\n            v.fit(x_train, y_train)\n            rmse = cross_val_score(v, x_train, y_train, scoring=\"neg_root_mean_squared_error\", cv=5)\n            mean_rmse = np.mean(rmse)\n            if mean_rmse < best_rmse:\n                best_rmse = mean_rmse\n                best_model = {k: v}\n    \n    return ('Best Model: ', best_model, '\\nRMSE: ', best_rmse)\n\ndict_model = {'RF': rf, 'Prophet': m}\nprint(evaluate_models(dict_model))\n\nused_features = ['std_Dept', 'std_Store', 'std_Type', 'CPI', 'Unemployment', \n                 'Year', 'std_Week', 'std_Holiday_Type', 'Size']\n\ndf_test_data = df_blended_data[df_blended_data['Data_Type'] == 'Test'][used_features].sort_values(by=used_features).dropna()\ntest_data = df_test_data.to_numpy()\ntest_columns = df_blended_data[df_blended_data['Data_Type'] == 'Test'][used_features].columns\ntest_columns = list(test_columns)\n\nnrow, ncol = test_data.shape\ntest = test_data[:,0:ncol]\nx_test = test[:,0:ncol]\ny_pred = rf.predict(x_test)\n\ndf_test_data['y'] = y_pred.tolist()\n\n#Columns to be used on merge\ninterest_columns = ['std_Store', 'std_Dept', 'Date', 'new_std_Store', 'new_std_Dept',\n                    'std_Week', 'new_std_Week',\n                    'Store', 'Dept', 'Year', 'Week']\n\ndf_mix = df_mix_data[df_mix_data['Data_Type'] == 'Test'].groupby(interest_columns)['Weekly_Sales'].agg('count').reset_index()\ndf_test_data = df_test_data.rename(columns = {'std_Store': 'new_std_Store', \n                                              'std_Dept': 'new_std_Dept',\n                                              'std_Week': 'new_std_Week'\n                                             }, \n                                   inplace = False)\n\ndf_test_data['Year'] = df_test_data['Year'].astype(str).astype(int)\ndf_mix['Year'] = df_mix['Year'].astype(str).astype(int)\n\ndf_test_data.new_std_Store = df_test_data.new_std_Store.round(2)\ndf_test_data.new_std_Dept = df_test_data.new_std_Dept.round(2)\ndf_test_data.new_std_Week = df_test_data.new_std_Week.round(2)\n\ndf_mix.new_std_Store = df_mix.new_std_Store.round(2)\ndf_mix.new_std_Dept = df_mix.new_std_Dept.round(2)\ndf_mix.new_std_Week = df_mix.new_std_Week.round(2)\n\ndf_test_final = df_mix.merge(df_test_data, how='left', \n                             left_on=[\"new_std_Store\", \"new_std_Dept\", \"Year\", \"new_std_Week\"], \n                             right_on=[\"new_std_Store\", \"new_std_Dept\", \"Year\", \"new_std_Week\"])\n\n#Replace NaN values by zero\ndf_test_final['y'] = df_test_final['y'].fillna(0)\ndf_test_final = df_test_final[['Store', 'Dept', 'Date', 'y']]\n\ndf_test_final['Date'] = pd.to_datetime(df_test_final['Date'])\ndf_test_final['Store'] = df_test_final['Store'].astype(str).astype(int)\ndf_test_final['Dept'] = df_test_final['Dept'].astype(str).astype(int)\n\ndf_test['Date'] = pd.to_datetime(df_test['Date'])\n\ndf_test_final = df_test.merge(df_test_final, how='left', \n                             left_on=[\"Store\", \"Dept\", \"Date\"], \n                             right_on=[\"Store\", \"Dept\", \"Date\"])\n\n\ndf_test_final = df_test_final.rename(columns = {'y': 'Weekly_Sales'}, inplace = False)\ndf_test_final['Id'] = df_test_final['Store'].astype(str) + '_' + df_test_final['Dept'].astype(str) + '_' + df_test_final['Date'].astype(str)\ndf_test_final = df_test_final[['Id', 'Weekly_Sales']]\ndf_test_final = df_test_final.sort_values(by=['Id'])\n\ndf_test_final.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Recommended Improvements\n- Hypertunning of parameters (there are mecanisms that help to do this: GridSearchCV, RandomSearchCV, etc)\n- Test new alghoritms (XGBoosting, Neural Networks, SARIMAX, etc)\n- Model holidays separately: holidays, especially Christmas and ThanksGiving, seems to have a big importance and that is not recognized by the models implemented in this notebook\n- Delete rows in train set where Weekly_Sales < 0: this was not done because the objective is present good results in the output of kaggle exercise. In real life, we know that we can't have a number of sales lower than zero\n- Delete outliers\n- Investigate why Prophet has returned future predictions like this\n- Fill NaN values from features with high importance (CPI, etc) with statisticlal mesaures (mean, median, etc) of other rows with similars characteristics. It is not recommended drop these rows because it its predicted zero sales for rows in this situation"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}