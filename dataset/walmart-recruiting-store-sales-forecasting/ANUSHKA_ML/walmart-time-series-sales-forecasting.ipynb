{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n\n## <p style=\"background-color:white;font-family:newtimeroman;color:coral;font-size:230%;text-align:center;border-radius:20px 60px;\">Sales Prediction for Chain of Stores - WALMART</p>\n\n![](https://appinventiv.com/wp-content/uploads/sites/1/2019/09/Walmart%E2%80%99s-Journey-in-the-Blockchain-Arena-1.png)\n\n\n#### Predicting future sales for a company is one of the most important aspects of strategic planning. \n\nIn this kernel, we will wanted analyze in depth how internal and external factors of one of the biggest companies in the US can affect their Weekly Sales in the future. \n\n*This module contains complete analysis of data , includes time series analysis , identifies the best performing stores , performs sales prediction with the help of multiple linear regression.*\n\nThe data collected ranges from 2010 to 2012, where 45 Walmart stores across the country were included in this analysis. It is important to note that we also have external data available like CPI, Unemployment Rate and Fuel Prices in the region of each store which, hopefully, help us to make a more detailed analysis.","metadata":{}},{"cell_type":"markdown","source":"## <p style=\"background-color:white;font-family:newtimeroman;color:coral;font-size:170%;text-align:center;border-radius:20px 60px;\">Let's dive in . . .</p>\n","metadata":{}},{"cell_type":"markdown","source":"\n### <p style=\"background-color:white;font-family:timesnewroman;color:lightseagreen;font-size:200%;border-radius:20px 60px;\">Introduction</p> \n \n #### In Retail Industry and chain of stores one of the biggest issue they face are supply chain management. The component of supply chain management (SCM) involved with determining how best to fulfill the requirements created from the Demand Plan. \n \n *It's objective is to balance supply and demand in a manner that achieves the financial and service objectives of the enterprise.*\n \n  If we look into the case of a retail chain stores one of the basic case is to know the demand of products that are sold in the store. If the decision making authority know whats the demand of each products for a week or month, they would be able to plan the supply chain accordingly. If that is possible this would save a lot of money for them because they don't have to overstock or can plan their Logistics accordingly.\n   \n![](https://miro.medium.com/max/600/1*ixQ_TdPP3RmPQcPUzSVCrA.jpeg)\n\n \n### <p style=\"background-color:white;font-family:calibri;color:lightseagreen;font-size:200%;border-radius:20px 60px;\">Data</p> \n\n### There are 3 Datasets :\n\n#### Stores:\n- Store: The store number. Range from 1–45.\n- Type: Three types of stores ‘A’, ‘B’ or ‘C’.\n- Size: Sets the size of a Store would be calculated by the no. of products available in the particular store ranging from 34,000 to 210,000.\n\n***primary key is Store***\n\n#### Sales:\n    -Date: The date of the week where this observation was taken.\n    -Weekly_Sales: The sales recorded during that Week.\n    -Store: The store which observation in recorded 1–45\n    -Dept: One of 1–99 that shows the department.\n    -IsHoliday: Boolean value representing a holiday week or not.\n\n***primary key is a combination of (Store,Dept,Date).***\n\n#### Features:\n\n    -Temperature: Temperature of the region during that week.\n    -Fuel_Price: Fuel Price in that region during that week.\n    -MarkDown1:5 : Represents the Type of markdown and what quantity was available during that week.\n    -CPI: Consumer Price Index during that week.\n    -Unemployment: The unemployment rate during that week in the region of the store.\n\n***primary key here is a combination of (Store,Date)***\n\n![](https://i.imgur.com/XuDXqGU.png)\n\n\n### <p style=\"background-color:white;font-family:calibri;color:lightseagreen;font-size:200%;border-radius:20px 60px;\">Plan of Action</p> \n\n    1. We will perform dertailed EDA & Time series analysis and gather useful insights\n    2. Next we will build the following Regression models to predict future sales.\n\n\n **List of Models -**\n \n     1. Lasso Regressor\n     2. Random Forest Regressor\n     3. Gradient Boosting Regressor\n     4. Support Vector Regressor\n     5. Time Series Analysis\n \n","metadata":{}},{"cell_type":"markdown","source":"# <p style=\"background-color:coral;font-family:newtimeroman;color:white;font-size:150%;text-align:center;border-radius:20px 60px;\">Dataset Importing and Querying</p>\n\n    We will load all 3 datasets and merge them  into one big dataset that gives whole data. \n\n### Note :\n\n*Since here we are only predicting Store level sales we will group the dataframes such that Department level data gets eliminated and take the sum of department level sales to give the store level sales.*","metadata":{}},{"cell_type":"code","source":"# Importing all necessary libraries to proceed with this project.\n\nimport warnings\nimport itertools\nimport numpy as np\nimport scipy.stats as stats\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport calendar\nfrom sklearn.model_selection import cross_val_score, cross_val_predict\nfrom sklearn import metrics\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import Lasso, LinearRegression, LassoCV\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.neighbors import KNeighborsRegressor\nimport random\nimport sqlite3\nfrom itertools import cycle, islice\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nimport xgboost as xgb\nimport catboost as cb\nimport lightgbm as lgb\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import HistGradientBoostingRegressor\nfrom sklearn.svm import SVR\n# Import timedelta from datetime library\nfrom datetime import timedelta\n\n\nss = StandardScaler()\n\n","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:22:53.187369Z","iopub.execute_input":"2022-05-05T12:22:53.187906Z","iopub.status.idle":"2022-05-05T12:22:56.287116Z","shell.execute_reply.started":"2022-05-05T12:22:53.187816Z","shell.execute_reply":"2022-05-05T12:22:56.286434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pwd","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:23:11.382412Z","iopub.execute_input":"2022-05-05T12:23:11.382914Z","iopub.status.idle":"2022-05-05T12:23:11.387774Z","shell.execute_reply.started":"2022-05-05T12:23:11.382878Z","shell.execute_reply":"2022-05-05T12:23:11.387101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cd ../input/walmart-recruiting-store-sales-forecasting","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:23:12.021594Z","iopub.execute_input":"2022-05-05T12:23:12.022259Z","iopub.status.idle":"2022-05-05T12:23:12.027944Z","shell.execute_reply.started":"2022-05-05T12:23:12.022226Z","shell.execute_reply":"2022-05-05T12:23:12.027092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pwd","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:23:12.497308Z","iopub.execute_input":"2022-05-05T12:23:12.497629Z","iopub.status.idle":"2022-05-05T12:23:12.503678Z","shell.execute_reply.started":"2022-05-05T12:23:12.497595Z","shell.execute_reply":"2022-05-05T12:23:12.502813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Load and read data","metadata":{}},{"cell_type":"code","source":"walmart = pd.read_csv('train.csv.zip')\n\nwalmart_feature = pd.read_csv('features.csv.zip')\n\nwalmart_store = pd.read_csv('stores.csv')\n","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:23:12.803688Z","iopub.execute_input":"2022-05-05T12:23:12.804241Z","iopub.status.idle":"2022-05-05T12:23:13.26316Z","shell.execute_reply.started":"2022-05-05T12:23:12.804185Z","shell.execute_reply":"2022-05-05T12:23:13.262186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"walmart.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:23:13.293748Z","iopub.execute_input":"2022-05-05T12:23:13.294061Z","iopub.status.idle":"2022-05-05T12:23:13.313778Z","shell.execute_reply.started":"2022-05-05T12:23:13.294026Z","shell.execute_reply":"2022-05-05T12:23:13.313169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"walmart.shape","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:23:13.565547Z","iopub.execute_input":"2022-05-05T12:23:13.565987Z","iopub.status.idle":"2022-05-05T12:23:13.70622Z","shell.execute_reply.started":"2022-05-05T12:23:13.565954Z","shell.execute_reply":"2022-05-05T12:23:13.705372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n\n### <p style=\"background-color:white;font-family:calibri;color:lightseagreen;font-size:200%;border-radius:20px 60px;\">Group by Store</p> \n\n#### Since we are am predicting the weekly sales for Store level, we will grouping the data in walmart dataset to avoid the department and take the sum of department sales to store level.","metadata":{}},{"cell_type":"code","source":"walmart_store_group=walmart.groupby([\"Store\",\"Date\"])[[\"Weekly_Sales\"]].sum()\nwalmart_store_group.reset_index(inplace=True)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:23:13.845568Z","iopub.execute_input":"2022-05-05T12:23:13.845993Z","iopub.status.idle":"2022-05-05T12:23:13.952593Z","shell.execute_reply.started":"2022-05-05T12:23:13.845963Z","shell.execute_reply":"2022-05-05T12:23:13.951623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Merging all the datasets into one place for easier test and analysis.**","metadata":{}},{"cell_type":"code","source":"result = pd.merge(walmart_store_group, walmart_store, how='inner', on='Store', left_on=None, right_on=None,\n        left_index=False, right_index=False, sort=False,\n        suffixes=('_x', '_y'), copy=True, indicator=False)\n\ndata = pd.merge(result, walmart_feature, how='inner', on=['Store','Date'], left_on=None, right_on=None,\n        left_index=False, right_index=False, sort=False,\n        suffixes=('_x', '_y'), copy=True, indicator=False)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:23:14.146637Z","iopub.execute_input":"2022-05-05T12:23:14.14693Z","iopub.status.idle":"2022-05-05T12:23:14.169821Z","shell.execute_reply.started":"2022-05-05T12:23:14.146896Z","shell.execute_reply":"2022-05-05T12:23:14.16906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(data.shape)","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:23:14.418228Z","iopub.execute_input":"2022-05-05T12:23:14.418674Z","iopub.status.idle":"2022-05-05T12:23:14.422782Z","shell.execute_reply.started":"2022-05-05T12:23:14.418641Z","shell.execute_reply":"2022-05-05T12:23:14.42221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Dataframe Walmart with 421570 rows has come down to 6435 rows by doing a group by and merge** \n\n\n","metadata":{}},{"cell_type":"markdown","source":"# <p style=\"background-color:coral;font-family:newtimeroman;color:white;font-size:120%;text-align:center;border-radius:20px 60px;\">Data Cleaning</p>\n\n<p style=\"background-color:white;font-family:calibri;color:lightseagreen;font-size:200%;border-radius:20px 60px;\">Now let's look through the data and do some basic data cleaning</p> \n","metadata":{}},{"cell_type":"code","source":"\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:23:14.699408Z","iopub.execute_input":"2022-05-05T12:23:14.699712Z","iopub.status.idle":"2022-05-05T12:23:14.720984Z","shell.execute_reply.started":"2022-05-05T12:23:14.699679Z","shell.execute_reply":"2022-05-05T12:23:14.720402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#let's encode the categorical column : IsHoliday\n\ndata['IsHoliday'] = data['IsHoliday'].apply(lambda x: 1 if x == True else 0)\n# Will convert the bool to 1 and 0 for easier use later.\n#data.IsHoliday=data.IsHoliday.map(lambda x: 1 if x==True else 0)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:23:14.946381Z","iopub.execute_input":"2022-05-05T12:23:14.946808Z","iopub.status.idle":"2022-05-05T12:23:14.957937Z","shell.execute_reply.started":"2022-05-05T12:23:14.946776Z","shell.execute_reply":"2022-05-05T12:23:14.956599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Want to check the date column is in object format or datetime\ndata.dtypes","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:23:15.197121Z","iopub.execute_input":"2022-05-05T12:23:15.197872Z","iopub.status.idle":"2022-05-05T12:23:15.205486Z","shell.execute_reply.started":"2022-05-05T12:23:15.197829Z","shell.execute_reply":"2022-05-05T12:23:15.204573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# yes my assumption was correct python have something to do with date format. \n# Now converting \"Date\"  to date time\ndata[\"Date\"]=pd.to_datetime(data.Date)\n\n# Extracting details from date given. so that can be used for seasonal checks or grouping\n\ndata[\"Day\"]=data.Date.dt.day\ndata[\"Month\"]=data.Date.dt.month\ndata[\"Year\"]=data.Date.dt.year\n\n# Changing the Months value from numbers to real values like Jan, Feb to Dec\ndata['Month'] = data['Month'].apply(lambda x: calendar.month_abbr[x])","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:23:15.508212Z","iopub.execute_input":"2022-05-05T12:23:15.508955Z","iopub.status.idle":"2022-05-05T12:23:15.554774Z","shell.execute_reply.started":"2022-05-05T12:23:15.508918Z","shell.execute_reply":"2022-05-05T12:23:15.553994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lets look into the null values\ndata.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:23:16.005722Z","iopub.execute_input":"2022-05-05T12:23:16.006163Z","iopub.status.idle":"2022-05-05T12:23:16.018204Z","shell.execute_reply.started":"2022-05-05T12:23:16.006132Z","shell.execute_reply":"2022-05-05T12:23:16.017355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Wow!! Now that's huge. More Than 65% of value are missing in MarkDown values**\n\n*We can impute sum values as of now for the missing  and will later decide  whether to use Markdown for modeling or should take some other approach for imputing or whether to discard MarkDowns completely*\n","metadata":{}},{"cell_type":"code","source":"#will create this column for later use\n#data['MarkdownsSum'] = data['MarkDown1'] + data['MarkDown2'] + data['MarkDown3'] + data['MarkDown4'] + data['MarkDown5'] ","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:23:16.25194Z","iopub.execute_input":"2022-05-05T12:23:16.252933Z","iopub.status.idle":"2022-05-05T12:23:16.255963Z","shell.execute_reply.started":"2022-05-05T12:23:16.252866Z","shell.execute_reply":"2022-05-05T12:23:16.255382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#data.fillna(0, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:23:16.435534Z","iopub.execute_input":"2022-05-05T12:23:16.436126Z","iopub.status.idle":"2022-05-05T12:23:16.442286Z","shell.execute_reply.started":"2022-05-05T12:23:16.436087Z","shell.execute_reply":"2022-05-05T12:23:16.441611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.describe().T","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-05-05T12:23:16.705182Z","iopub.execute_input":"2022-05-05T12:23:16.705604Z","iopub.status.idle":"2022-05-05T12:23:16.767205Z","shell.execute_reply.started":"2022-05-05T12:23:16.705572Z","shell.execute_reply":"2022-05-05T12:23:16.766484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#add a 'week' column to the dataset for further analysis\ndata['Week'] = data.Date.dt.isocalendar().week ","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:23:17.025158Z","iopub.execute_input":"2022-05-05T12:23:17.025948Z","iopub.status.idle":"2022-05-05T12:23:17.036385Z","shell.execute_reply.started":"2022-05-05T12:23:17.025899Z","shell.execute_reply":"2022-05-05T12:23:17.035571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <p style=\"background-color:coral;font-family:newtimeroman;color:white;font-size:150%;text-align:center;border-radius:20px 60px;\">EDA</p>\n![](https://visme.co/blog/wp-content/uploads/2016/04/Header-5.gif)","metadata":{}},{"cell_type":"code","source":"data.describe().T.style.bar(subset=['mean'], color='#205ff2')\\\n                            .background_gradient(subset=['std'], cmap='Reds')\\\n                            .background_gradient(subset=['50%'], cmap='coolwarm')","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:23:17.341828Z","iopub.execute_input":"2022-05-05T12:23:17.342109Z","iopub.status.idle":"2022-05-05T12:23:17.475601Z","shell.execute_reply.started":"2022-05-05T12:23:17.342079Z","shell.execute_reply":"2022-05-05T12:23:17.474743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_weeks = data.groupby('Week').sum()\n\nimport plotly.express as px\nfig = px.line( data_frame = df_weeks, x = df_weeks.index, y = 'Weekly_Sales', labels = {'Weekly_Sales' : 'Weekly Sales', 'x' : 'Weeks' }, title = 'Sales over weeks')\nfig.update_traces(line_color='deeppink', line_width=3)","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:23:17.661981Z","iopub.execute_input":"2022-05-05T12:23:17.662971Z","iopub.status.idle":"2022-05-05T12:23:19.514512Z","shell.execute_reply.started":"2022-05-05T12:23:17.66293Z","shell.execute_reply":"2022-05-05T12:23:19.513531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_weeks.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:23:19.516645Z","iopub.execute_input":"2022-05-05T12:23:19.517074Z","iopub.status.idle":"2022-05-05T12:23:19.539077Z","shell.execute_reply.started":"2022-05-05T12:23:19.517027Z","shell.execute_reply":"2022-05-05T12:23:19.538336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import plotly.graph_objs as go\nfrom plotly import tools\n\nfig = go.Figure()\n\n#fig.add_trace(go.Scatter( x = df_weeks.index, y = df_weeks['Weekly_Sales'], name = 'Weekly Sales', mode = 'lines') )\nfig.add_trace(go.Scatter( x = df_weeks.index, y = df_weeks['MarkDown1'], name = 'MarkDown1', mode = 'lines') )\nfig.add_trace(go.Scatter( x = df_weeks.index, y = df_weeks['MarkDown2'], name = 'MarkDown2', mode = 'lines') )\nfig.add_trace(go.Scatter( x = df_weeks.index, y = df_weeks['MarkDown3'], name = 'MarkDown3', mode = 'lines') )\nfig.add_trace(go.Scatter( x = df_weeks.index, y = df_weeks['MarkDown4'], name = 'MarkDown4', mode = 'lines') )\nfig.add_trace(go.Scatter( x = df_weeks.index, y = df_weeks['MarkDown5'], name = 'MarkDown5', mode = 'lines') )\n\nfig.update_layout(title = 'Sales vs Markdown', xaxis_title = 'Weeks')","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:23:19.540405Z","iopub.execute_input":"2022-05-05T12:23:19.540616Z","iopub.status.idle":"2022-05-05T12:23:19.562853Z","shell.execute_reply.started":"2022-05-05T12:23:19.540591Z","shell.execute_reply":"2022-05-05T12:23:19.561995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## setting all missing values in markdown columns to -500 for now. We will treat them later while performing Feature scaling\ndata['MarkDown1'].fillna(-500, inplace=True)\ndata['MarkDown2'].fillna(-500, inplace=True)\ndata['MarkDown3'].fillna(-500, inplace=True)\ndata['MarkDown4'].fillna(-500, inplace=True)\ndata['MarkDown5'].fillna(-500, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:23:19.565043Z","iopub.execute_input":"2022-05-05T12:23:19.565843Z","iopub.status.idle":"2022-05-05T12:23:19.580974Z","shell.execute_reply.started":"2022-05-05T12:23:19.565794Z","shell.execute_reply":"2022-05-05T12:23:19.58013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# From the Describe function we see that weekly sales for each store are very high. \n# we will scale down the value for ease of use and revert back when we look residuals or where necessary\n\nplt.figure(figsize=(10, 6))\ndata[\"Weekly_Sales\"]=data.Weekly_Sales/1000\n\nsns.distplot(data.Weekly_Sales, kde=False, bins=30, color = 'tomato')\nplt.title('Weekly Sales Function Distribution')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:23:19.582026Z","iopub.execute_input":"2022-05-05T12:23:19.583073Z","iopub.status.idle":"2022-05-05T12:23:19.882549Z","shell.execute_reply.started":"2022-05-05T12:23:19.583029Z","shell.execute_reply":"2022-05-05T12:23:19.880897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\nIn the Distribution, natural Log of Sales and the square root of Sales look better distributed. We can use Natural Log for predictions later\n","metadata":{}},{"cell_type":"code","source":"def scatter(dataset, column):\n    plt.figure()\n    plt.scatter(data[column] , data['Weekly_Sales'], color = 'turquoise')\n    plt.ylabel('Weekly Sales')\n    plt.xlabel(column)","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:23:19.883651Z","iopub.execute_input":"2022-05-05T12:23:19.883919Z","iopub.status.idle":"2022-05-05T12:23:19.888897Z","shell.execute_reply.started":"2022-05-05T12:23:19.883852Z","shell.execute_reply":"2022-05-05T12:23:19.888209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scatter(data, 'Fuel_Price')\nscatter(data, 'Size')\nscatter(data, 'CPI')\nscatter(data, 'Type')\nscatter(data, 'IsHoliday')\nscatter(data, 'Unemployment')\nscatter(data, 'Temperature')\nscatter(data, 'Store')\n","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:23:19.890418Z","iopub.execute_input":"2022-05-05T12:23:19.891072Z","iopub.status.idle":"2022-05-05T12:23:21.715178Z","shell.execute_reply.started":"2022-05-05T12:23:19.891034Z","shell.execute_reply":"2022-05-05T12:23:21.7143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['Week'] = data.Date.dt.isocalendar().week ","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:23:21.716974Z","iopub.execute_input":"2022-05-05T12:23:21.717931Z","iopub.status.idle":"2022-05-05T12:23:21.727461Z","shell.execute_reply.started":"2022-05-05T12:23:21.717881Z","shell.execute_reply":"2022-05-05T12:23:21.726646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:23:21.728492Z","iopub.execute_input":"2022-05-05T12:23:21.728706Z","iopub.status.idle":"2022-05-05T12:23:21.760099Z","shell.execute_reply.started":"2022-05-05T12:23:21.728679Z","shell.execute_reply":"2022-05-05T12:23:21.759082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"weekly_sales_2010 = data[data.Year==2010]['Weekly_Sales'].groupby(data['Week']).mean()\nweekly_sales_2011 = data[data.Year==2011]['Weekly_Sales'].groupby(data['Week']).mean()\nweekly_sales_2012 = data[data.Year==2012]['Weekly_Sales'].groupby(data['Week']).mean()\nplt.figure(figsize=(20,8))\nsns.lineplot(weekly_sales_2010.index, weekly_sales_2010.values)\nsns.lineplot(weekly_sales_2011.index, weekly_sales_2011.values)\nsns.lineplot(weekly_sales_2012.index, weekly_sales_2012.values)\nplt.grid()\nplt.xticks(np.arange(1, 53, step=1))\nplt.legend(['2010', '2011', '2012'], loc='best', fontsize=16)\nplt.title('Average Weekly Sales - Per Year', fontsize=18)\nplt.ylabel('Sales', fontsize=16)\nplt.xlabel('Week', fontsize=16)\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:23:21.762397Z","iopub.execute_input":"2022-05-05T12:23:21.762819Z","iopub.status.idle":"2022-05-05T12:23:22.645248Z","shell.execute_reply.started":"2022-05-05T12:23:21.762769Z","shell.execute_reply":"2022-05-05T12:23:22.644278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Note :\n\n**As we can see, there is one important Holiday not included in 'IsHoliday'. It's the Easter Day. It is always in a Sunday, but can fall on different weeks.**\n\nIn 2010 is in Week 13\n\nIn 2011, Week 16\n\nWeek 14 in 2012\n\nWeek 13 in 2013 for **Test set**\n\n**So, we can change to 'True' these Weeks in each Year.**","metadata":{}},{"cell_type":"code","source":"weekly_sales_mean = data['Weekly_Sales'].groupby(data['Date']).mean()\nweekly_sales_median = data['Weekly_Sales'].groupby(data['Date']).median()\nplt.figure(figsize=(20,8))\nsns.lineplot(weekly_sales_mean.index, weekly_sales_mean.values, color = 'indigo')\nsns.lineplot(weekly_sales_median.index, weekly_sales_median.values, color = 'tomato')\nplt.grid()\nplt.legend(['Mean', 'Median'], loc='best', fontsize=16)\nplt.title('Weekly Sales - Mean and Median', fontsize=18)\nplt.ylabel('Sales', fontsize=16)\nplt.xlabel('Date', fontsize=16)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:23:22.646536Z","iopub.execute_input":"2022-05-05T12:23:22.646779Z","iopub.status.idle":"2022-05-05T12:23:23.040787Z","shell.execute_reply.started":"2022-05-05T12:23:22.646749Z","shell.execute_reply":"2022-05-05T12:23:23.040184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n<p style=\"background-color:white;font-family:calibri;color:lightseagreen;font-size:200%;border-radius:20px 60px;\"> Checking the relationship of the other features with weekly sales</p> ","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(18,8))\nsns.lineplot ( data = data, x = 'Size', y =  'Weekly_Sales', hue = 'IsHoliday');","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:23:23.041736Z","iopub.execute_input":"2022-05-05T12:23:23.042229Z","iopub.status.idle":"2022-05-05T12:23:25.654676Z","shell.execute_reply.started":"2022-05-05T12:23:23.042195Z","shell.execute_reply":"2022-05-05T12:23:25.653646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"px.histogram(data, x='Temperature', y ='Weekly_Sales', color='IsHoliday', marginal='box')","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:23:25.655845Z","iopub.execute_input":"2022-05-05T12:23:25.656068Z","iopub.status.idle":"2022-05-05T12:23:25.86525Z","shell.execute_reply.started":"2022-05-05T12:23:25.65604Z","shell.execute_reply":"2022-05-05T12:23:25.864237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"px.histogram(data, x='Fuel_Price', y ='Weekly_Sales', color='IsHoliday', marginal='box')","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:23:25.867663Z","iopub.execute_input":"2022-05-05T12:23:25.868112Z","iopub.status.idle":"2022-05-05T12:23:25.996056Z","shell.execute_reply.started":"2022-05-05T12:23:25.868057Z","shell.execute_reply":"2022-05-05T12:23:25.995179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"px.histogram(data, x='CPI', y ='Weekly_Sales', color='IsHoliday')","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:23:25.997475Z","iopub.execute_input":"2022-05-05T12:23:25.99781Z","iopub.status.idle":"2022-05-05T12:23:26.101128Z","shell.execute_reply.started":"2022-05-05T12:23:25.997768Z","shell.execute_reply":"2022-05-05T12:23:26.100249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"weekly_sales = data['Weekly_Sales'].groupby(data['Store']).mean()\nplt.figure(figsize=(20,8))\nplt.style.use('default')\nsns.barplot(weekly_sales.index, weekly_sales.values)\nplt.grid()\nplt.title('Average Sales - per Store', fontsize=18)\nplt.ylabel('Sales', fontsize=16)\nplt.xlabel('Store', fontsize=16)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:23:26.10225Z","iopub.execute_input":"2022-05-05T12:23:26.1025Z","iopub.status.idle":"2022-05-05T12:23:26.718705Z","shell.execute_reply.started":"2022-05-05T12:23:26.102472Z","shell.execute_reply":"2022-05-05T12:23:26.717873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set(style=\"white\")\n\ncorr = data.corr()\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\nf, ax = plt.subplots(figsize=(20, 15))\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\nplt.title('Correlation Matrix', fontsize=18)\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=1, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:23:26.719802Z","iopub.execute_input":"2022-05-05T12:23:26.720028Z","iopub.status.idle":"2022-05-05T12:23:27.752195Z","shell.execute_reply.started":"2022-05-05T12:23:26.719999Z","shell.execute_reply":"2022-05-05T12:23:27.751244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n# <p style=\"background-color:coral;font-family:newtimeroman;color:white;font-size:150%;text-align:center;border-radius:20px 60px;\">Detailed Time-Series Analysis</p>\n\n![](https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/69aa52fb-b692-463f-84c1-cde2470445ae/css-animation-craziness.gif)\n![](https://windenergyscience.com/wp-content/uploads/2020/02/Data.gif)\n\n\n\n<p style=\"background-color:white;font-family:calibri;color:lightseagreen;font-size:200%;border-radius:20px 60px;\"> What is Time Series</p> \n\nAny data recorded with some fixed interval of time is called as time series data. This fixed interval can be hourly, daily, monthly or yearly. e.g. hourly temp reading, daily changing fuel prices, monthly electricity bill, annul company profit report etc. In time series data, time will always be independent variable and there can be one or many dependent variable. \n\nSales forecasting time series with shampoo sales for every month will look like this, \n\n![Shampoo_Sales](https://raw.githubusercontent.com/satishgunjal/images/master/Shampoo_Sales.png)\n\nIn above example since there is only one variable dependent on time so its called as univariate time series. If there are multiple dependent variables, then its called as multivariate time series.\n\nObjective of time series analysis is to understand how change in time affect the dependent variables and accordingly predict values for future time intervals.\n\n<p style=\"background-color:white;font-family:timesnewroman;color:lightseagreen;font-size:200%;border-radius:20px 60px;\"> Stationarity Data</p> \n\nFor accurate analysis and forecasting trend and seasonality is removed from the time series and converted it into stationary series.\nTime series data is said to be stationary when statistical properties like mean, standard deviation are constant and there is no seasonality. In other words statistical properties of the time series data should not be a function of time.\n\n![Stationarity](https://raw.githubusercontent.com/satishgunjal/images/master/Stationarity.png)\n\n<p style=\"background-color:white;font-family:timesnewroman;color:lightseagreen;font-size:200%;border-radius:20px 60px;\"> Test for Stationarity</p>\n\nEasy way is to look at the plot and look for any obvious trend or seasonality. While working on real world data we can also use more sophisticated methods like rolling statistic and Augmented Dickey Fuller test to check stationarity of the data. \n\n### Rolling Statistics <a id =\"11\"></a>\nIn rolling statistics technique we define a size of window to calculate the mean and standard deviation throughout the series. For stationary series mean and standard deviation shouldn't change with time.\n\n<p style=\"background-color:white;font-family:timesnewroman;color:lightseagreen;font-size:200%;border-radius:20px 60px;\"> Augmented Dickey Fuller (ADF) Test</p>\n\nI won't go into the details of how this test works. I will concentrate more on how to interpret the result of this test to determine the stationarity of the series. ADF test will return 'p-value' and 'Test Statistics' output values.\n* **p-value > 0.05**: non-stationary.\n* **p-value <= 0.05**: stationary.\n* **Test statistics**: More negative this value more likely we have stationary series. Also, this value should be smaller than critical values(1%, 5%, 10%). For e.g. If test statistic is smaller than the 5% critical values, then we can say with 95% confidence that this is a stationary series\n\n\n![](https://i.pinimg.com/originals/cc/fe/1f/ccfe1f7fee5fe18a65127d4ed1f4c036.gif)\n","metadata":{}},{"cell_type":"markdown","source":"### We will consider one of the stores and perform a detailed time-series analysis on it","metadata":{}},{"cell_type":"markdown","source":"### Store 4","metadata":{}},{"cell_type":"code","source":"data1 = pd.read_csv('train.csv.zip')\ndata1.set_index('Date', inplace=True)\n\nstore4 = data1[data1.Store == 4]\n# there are about 45 different stores in this dataset.\n\nsales4 = pd.DataFrame(store4.Weekly_Sales.groupby(store4.index).sum())\nsales4.dtypes\nsales4.head(20)\n# Grouped weekly sales by store 4\n\n#remove date from index to change its dtype because it clearly isnt acceptable.\nsales4.reset_index(inplace = True)\n\n#converting 'date' column to a datetime type\nsales4['Date'] = pd.to_datetime(sales4['Date'])\n# resetting date back to the index\nsales4.set_index('Date',inplace = True)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:23:27.753412Z","iopub.execute_input":"2022-05-05T12:23:27.753644Z","iopub.status.idle":"2022-05-05T12:23:28.056813Z","shell.execute_reply.started":"2022-05-05T12:23:27.753616Z","shell.execute_reply":"2022-05-05T12:23:28.055827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Store 6","metadata":{}},{"cell_type":"code","source":"# Lets take store 6 data for analysis\nstore6 = data1[data1.Store == 6]\n# there are about 45 different stores in this dataset.\n\nsales6 = pd.DataFrame(store6.Weekly_Sales.groupby(store6.index).sum())\nsales6.dtypes\n# Grouped weekly sales by store 6\n\n#remove date from index to change its dtype because it clearly isnt acceptable.\nsales6.reset_index(inplace = True)\n\n#converting 'date' column to a datetime type\nsales6['Date'] = pd.to_datetime(sales6['Date'])\n# resetting date back to the index\nsales6.set_index('Date',inplace = True)","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:23:28.058267Z","iopub.execute_input":"2022-05-05T12:23:28.058528Z","iopub.status.idle":"2022-05-05T12:23:28.072097Z","shell.execute_reply.started":"2022-05-05T12:23:28.058499Z","shell.execute_reply":"2022-05-05T12:23:28.071133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sales4.Weekly_Sales.plot(figsize=(10,6), title= 'Weekly Sales of Store1', fontsize=14, color = 'salmon')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:23:28.073529Z","iopub.execute_input":"2022-05-05T12:23:28.073772Z","iopub.status.idle":"2022-05-05T12:23:28.522123Z","shell.execute_reply.started":"2022-05-05T12:23:28.073745Z","shell.execute_reply":"2022-05-05T12:23:28.521489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <p style=\"background-color:white;font-family:newtimeroman;color:coral;font-size:150%;text-align:center;border-radius:20px 60px;\">Time Series Components</p>","metadata":{}},{"cell_type":"markdown","source":"\n<p style=\"background-color:white;font-family:timesnewroman;color:lightseagreen;font-size:220%;border-radius:20px 60px;\"> Time Series Characteristics</p>\n\nMean, standard deviation and seasonality defines different characteristics of the time series. \n\n![Time_Series_Characteristics](https://raw.githubusercontent.com/satishgunjal/images/master/Time_Series_Characteristics.png)\n\nImportant characteristics of the time series are as below\n\n### Trend <a id =\"3\"></a>\nTrend represent the change in dependent variables with respect to time from start to end. In case of increasing trend dependent variable will increase with time and vice versa. It's not necessary to have definite trend in time series, we can have a single time series with increasing and decreasing trend. In short trend represent the varying mean of time series data.\n\n![Trend](https://raw.githubusercontent.com/satishgunjal/images/master/Trend.png)\n\n### Seasonality <a id =\"4\"></a>\nIf observations repeats after fixed time interval then they are referred as seasonal observations. These seasonal changes in data can occur because of natural events or man-made events. For example every year warm cloths sales increases just before winter season. So seasonality represent the data variations at fixed intervals.\n\n![Seasonality](https://raw.githubusercontent.com/satishgunjal/images/master/Seasonality.png)\n\n### Irregularities <a id =\"5\"></a>\nThis is also called as noise. Strange dips and jump in the data are called as irregularities. These fluctuations are caused by uncontrollable events like earthquakes, wars, flood, pandemic etc. For example because of COVID-19 pandemic there is huge demand for hand sanitizers and masks.\n\n![Irregularities](https://raw.githubusercontent.com/satishgunjal/images/master/Irregularities.png)\n\n### Cyclicity <a id =\"6\"></a>\nCyclicity occurs when observations in the series repeats in random pattern. Note that if there is any fixed pattern then it becomes seasonality, in case of cyclicity observations may repeat after a week, months or may be after a year. These kinds of patterns are much harder to predict.\n\n![Cyclicity](https://raw.githubusercontent.com/satishgunjal/images/master/Cyclicity.png)\n\nTime series data which has above characteristics is called as 'Non-Stationary Data'. For any analysis on time series data we must convert it to 'Stationary Data'\n\nThe general guideline is to estimate the trend and seasonality in the time series, and then make the time series stationary for data modeling. In data modeling step statistical techniques are used for time series analysis and forecasting. Once we have the predictions, in the final step forecasted values converted into the original scale by applying trend and seasonality constraints back.\n\n\n<p style=\"background-color:white;font-family:timesnewroman;color:lightseagreen;font-size:200%;border-radius:20px 60px;\"> Time Series Analysis</p>\nAs name suggest its analysis of the time series data to identify the patterns in it. I will briefly explain the different techniques and test for time series data analysis.\n\n<p style=\"background-color:white;font-family:timesnewroman;color:lightseagreen;font-size:200%;border-radius:20px 60px;\"> Time Series Decomposition</p>\nTime series decomposition helps to deconstruct the time series into several component like trend and seasonality for better visualization of its characteristics. Using time-series decomposition makes it easier to quickly identify a changing mean or variation in the data\n\n![Decomposition_of_Time_Series](https://raw.githubusercontent.com/satishgunjal/images/master/Decomposition_of_Time_Series.png)\n\n### Additive Decomposition\n\n* An additive model suggests that the components are added together.\n* An additive model is linear where changes over time are consistently made by the same amount.\n* A linear seasonality has the same frequency (width of the cycles) and amplitude (height of the cycles).\n\nThe statsmodels library provides an implementation of the naive, or classical, decomposition method in a function called seasonal_decompose(). You need to specify whether the model is additive or multiplicative.\n\nThe seasonal_decompose() function returns a result object which contains arrays to access four pieces of data from the decomposition.\n\nhttps://machinelearningmastery.com/decompose-time-series-data-trend-seasonality/\n","metadata":{}},{"cell_type":"code","source":"from statsmodels.tsa.seasonal import seasonal_decompose\n\ndecomposition = seasonal_decompose(sales4.Weekly_Sales, period=12)  \nfig = plt.figure()  \nfig = decomposition.plot()  \nfig.set_size_inches(12, 10)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:23:28.523642Z","iopub.execute_input":"2022-05-05T12:23:28.523869Z","iopub.status.idle":"2022-05-05T12:23:30.290604Z","shell.execute_reply.started":"2022-05-05T12:23:28.52384Z","shell.execute_reply":"2022-05-05T12:23:30.289658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n\n### Multiplicative Decomposition\n\n* An additive model suggests that the components are multipled together.\n* An additive model is non-linear such as quadratic or exponential. \n* Changes increase or decrease over time.\n* A non-linear seasonality has an increasing or decreasing frequency (width of the cycles) and / or amplitude (height of the cycles) over time.","metadata":{}},{"cell_type":"code","source":"decomposition = seasonal_decompose(sales4.Weekly_Sales, model= 'multiplicative', period=12)  \nfig = plt.figure()  \nfig = decomposition.plot()  \nfig.set_size_inches(12, 10)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:23:30.292048Z","iopub.execute_input":"2022-05-05T12:23:30.292368Z","iopub.status.idle":"2022-05-05T12:23:31.96986Z","shell.execute_reply.started":"2022-05-05T12:23:30.292327Z","shell.execute_reply":"2022-05-05T12:23:31.96899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y1=sales4.Weekly_Sales\ny2=sales6.Weekly_Sales","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:23:31.971183Z","iopub.execute_input":"2022-05-05T12:23:31.971979Z","iopub.status.idle":"2022-05-05T12:23:31.977403Z","shell.execute_reply.started":"2022-05-05T12:23:31.971926Z","shell.execute_reply":"2022-05-05T12:23:31.976529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y1.plot(figsize=(15, 6), legend=True, color = 'turquoise')\ny2.plot(figsize=(15, 6), legend=True, color = 'salmon')\nplt.ylabel('Weekly Sales')\nplt.title('Store4 vs Store6 Weekly Sales', fontsize = '16')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:23:31.978994Z","iopub.execute_input":"2022-05-05T12:23:31.979297Z","iopub.status.idle":"2022-05-05T12:23:32.466639Z","shell.execute_reply.started":"2022-05-05T12:23:31.979256Z","shell.execute_reply":"2022-05-05T12:23:32.465669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### This shows an interesting trend during year ends (during both 2011 & 2012). The best thing is both the stores have almost the same trends and spike just the magnitude is different. \n    This clearly tells its a timeseries problem and it will be interesting to look more into it","metadata":{}},{"cell_type":"code","source":"# Lets Look into 2012 data for a better view\ny1['2012'].plot(figsize=(15, 6),legend=True, color = 'chocolate')\ny2['2012'].plot(figsize=(15, 6), legend=True, color = 'turquoise')\nplt.ylabel('Weekly Sales')\nplt.title('Store4 vs Store6 on 2012', fontsize = '16')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:23:32.47145Z","iopub.execute_input":"2022-05-05T12:23:32.471789Z","iopub.status.idle":"2022-05-05T12:23:33.228599Z","shell.execute_reply.started":"2022-05-05T12:23:32.471743Z","shell.execute_reply":"2022-05-05T12:23:33.227693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Not quite a trend. Here comes yet another component of timeseries - Irregular component\n\n    which are other non random sources of variations of series and are mainly the impact of random events such as strikes, earthquakes, and sudden changes in the weather. By their nature, these effects are completely unpredictable.","metadata":{}},{"cell_type":"code","source":"# Define the p, d and q parameters to take any value between 0 and 2\np = d = q = range(0, 5)\n\n# Generate all different combinations of p, d and q triplets\npdq = list(itertools.product(p, d, q))\n\n# Generate all different combinations of seasonal p, d and q triplets\nseasonal_pdq = [(x[0], x[1], x[2], 52) for x in list(itertools.product(p, d, q))]\n","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:23:33.229992Z","iopub.execute_input":"2022-05-05T12:23:33.231542Z","iopub.status.idle":"2022-05-05T12:23:33.23879Z","shell.execute_reply.started":"2022-05-05T12:23:33.23149Z","shell.execute_reply":"2022-05-05T12:23:33.237828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import statsmodels.api as sm\n\nmod = sm.tsa.statespace.SARIMAX(y1,\n                                order=(4, 4, 3),\n                                seasonal_order=(1, 1, 0, 52),   #enforce_stationarity=False,\n                                enforce_invertibility=False)\n\nresults = mod.fit()\n\nprint(results.summary().tables[1])","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:23:33.240385Z","iopub.execute_input":"2022-05-05T12:23:33.24061Z","iopub.status.idle":"2022-05-05T12:24:08.782217Z","shell.execute_reply.started":"2022-05-05T12:23:33.240583Z","shell.execute_reply":"2022-05-05T12:24:08.781163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.style.use('seaborn-pastel')\nresults.plot_diagnostics(figsize=(15, 12))\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:24:08.7883Z","iopub.execute_input":"2022-05-05T12:24:08.789306Z","iopub.status.idle":"2022-05-05T12:24:09.757404Z","shell.execute_reply.started":"2022-05-05T12:24:08.789242Z","shell.execute_reply":"2022-05-05T12:24:09.756511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Will predict for last 90 days. So setting the date according to that\npred = results.get_prediction(start=pd.to_datetime('2012-07-27'), dynamic=False)\npred_ci = pred.conf_int()","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:24:09.759146Z","iopub.execute_input":"2022-05-05T12:24:09.759924Z","iopub.status.idle":"2022-05-05T12:24:09.768767Z","shell.execute_reply.started":"2022-05-05T12:24:09.759878Z","shell.execute_reply":"2022-05-05T12:24:09.76768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ax = y1['2010':].plot(label='observed')\npred.predicted_mean.plot(ax=ax, label='One-step ahead Forecast', alpha=.7)\n\nax.fill_between(pred_ci.index,\n                pred_ci.iloc[:, 0],\n                pred_ci.iloc[:, 1], color='k', alpha=.2)\n\nax.set_xlabel('Time Period')\nax.set_ylabel('Sales')\nplt.legend()\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:24:09.769983Z","iopub.execute_input":"2022-05-05T12:24:09.770205Z","iopub.status.idle":"2022-05-05T12:24:10.261156Z","shell.execute_reply.started":"2022-05-05T12:24:09.770179Z","shell.execute_reply":"2022-05-05T12:24:10.260224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_forecasted = pred.predicted_mean\ny_truth = y1['2012-7-27':]\n\n# Compute the mean square error\nmse = ((y_forecasted - y_truth) ** 2).mean()\nprint('The Mean Squared Error of our forecasts is {}'.format(round(mse, 2)))","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:24:10.2623Z","iopub.execute_input":"2022-05-05T12:24:10.262534Z","iopub.status.idle":"2022-05-05T12:24:10.272713Z","shell.execute_reply.started":"2022-05-05T12:24:10.262505Z","shell.execute_reply":"2022-05-05T12:24:10.271762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_dynamic = results.get_prediction(start=pd.to_datetime('2012-7-27'), dynamic=True, full_results=True)\npred_dynamic_ci = pred_dynamic.conf_int()","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:24:10.274133Z","iopub.execute_input":"2022-05-05T12:24:10.274472Z","iopub.status.idle":"2022-05-05T12:24:10.319464Z","shell.execute_reply.started":"2022-05-05T12:24:10.274428Z","shell.execute_reply":"2022-05-05T12:24:10.317526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ax = y1['2010':].plot(label='observed', figsize=(12, 8))\npred_dynamic.predicted_mean.plot(label='Dynamic Forecast', ax=ax)\n\nax.fill_between(pred_dynamic_ci.index,\n                pred_dynamic_ci.iloc[:, 0],\n                pred_dynamic_ci.iloc[:, 1], color='k', alpha=.25)\n\nax.fill_betweenx(ax.get_ylim(), pd.to_datetime('2012-7-26'), y1.index[-1],\n                 alpha=.1, zorder=-1)\n\nax.set_xlabel('Time Period')\nax.set_ylabel('Sales')\n\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:24:10.321646Z","iopub.execute_input":"2022-05-05T12:24:10.321989Z","iopub.status.idle":"2022-05-05T12:24:10.873119Z","shell.execute_reply.started":"2022-05-05T12:24:10.321947Z","shell.execute_reply":"2022-05-05T12:24:10.872246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### That looks good. Both the observed and predicted lines go together indicating nearly accurate prediction","metadata":{}},{"cell_type":"code","source":"# Extract the predicted and true values of our time series\ny_forecasted = pred_dynamic.predicted_mean\n\ny_truth = y1['2012-7-27':]\n\n# Compute the Root mean square error\nrmse = np.sqrt(((y_forecasted - y_truth) ** 2).mean())\nprint('The Root Mean Squared Error of our forecasts is {}'.format(round(rmse, 2)))","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:24:10.87452Z","iopub.execute_input":"2022-05-05T12:24:10.875013Z","iopub.status.idle":"2022-05-05T12:24:10.886558Z","shell.execute_reply.started":"2022-05-05T12:24:10.874964Z","shell.execute_reply":"2022-05-05T12:24:10.885174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Residual= y_forecasted - y_truth\nprint(\"Residual for Store1\",np.abs(Residual).sum())","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:24:10.887835Z","iopub.execute_input":"2022-05-05T12:24:10.888212Z","iopub.status.idle":"2022-05-05T12:24:10.900859Z","shell.execute_reply.started":"2022-05-05T12:24:10.888176Z","shell.execute_reply":"2022-05-05T12:24:10.899929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get forecast 12 weeks ahead in future\npred_uc = results.get_forecast(steps=12)\n\n# Get confidence intervals of forecasts\npred_ci = pred_uc.conf_int()","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:24:10.902343Z","iopub.execute_input":"2022-05-05T12:24:10.902608Z","iopub.status.idle":"2022-05-05T12:24:10.947481Z","shell.execute_reply.started":"2022-05-05T12:24:10.902577Z","shell.execute_reply":"2022-05-05T12:24:10.945756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ax = y1.plot(label='observed', figsize=(12, 8))\npred_uc.predicted_mean.plot(ax=ax, label='Forecast')\nax.fill_between(pred_ci.index,\n                pred_ci.iloc[:, 0],\n                pred_ci.iloc[:, 1], color='k', alpha=.25)\nax.set_xlabel('Time Period')\nax.set_ylabel('Sales')\n\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:24:10.950634Z","iopub.execute_input":"2022-05-05T12:24:10.953118Z","iopub.status.idle":"2022-05-05T12:24:11.553065Z","shell.execute_reply.started":"2022-05-05T12:24:10.953069Z","shell.execute_reply":"2022-05-05T12:24:11.552217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### For future prediction the model is not that great because the error interval is way big. But if we just check the green line prediction this is almost like earlier years. If we look for may be first 2 weeks the prediction is way better and error is also low.","metadata":{}},{"cell_type":"markdown","source":"\n# <p style=\"background-color:coral;font-family:newtimeroman;color:white;font-size:150%;text-align:center;border-radius:20px 60px;\">Modelling & Machine learning</p>","metadata":{}},{"cell_type":"code","source":"# create dummy variables for 'Type' and keeping all columns to see heatmap then will drop 1 column\nType_dummies = pd.get_dummies(data.Type, prefix='Type')\n\n# concatenate two DataFrames (axis=0 for rows, axis=1 for columns)\ndata = pd.concat([data, Type_dummies], axis=1)\n\n# Not dropping the orginal Type column now so that I can use the field in some data analysis ","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:24:11.554753Z","iopub.execute_input":"2022-05-05T12:24:11.555288Z","iopub.status.idle":"2022-05-05T12:24:11.566097Z","shell.execute_reply.started":"2022-05-05T12:24:11.555244Z","shell.execute_reply":"2022-05-05T12:24:11.565276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Create a dataframe for heatmap\ndata_heatmap_df=data.copy()\n\n# Eliminating all the columns that are not continuous/binary  variables from the heatmap section.\ndata_heatmap_df.drop(['Store','Day','Month','Year','Date','Store','Type','Type_A','Type_B','Type_C'], axis=1,inplace=True)\n\n\n# Lets look the correlation matrix and heat map of the \n\n## Correlation Heat map\ndef correlation_heat_map(df):\n    corrs = df.corr()\n\n    # Set the default matplotlib figure size:\n    fig, ax = plt.subplots(figsize=(12,8))\n\n    # Generate a mask for the upper triangle (taken from seaborn example gallery)\n    mask = np.zeros_like(corrs, dtype=np.bool)\n    mask[np.triu_indices_from(mask)] = True\n\n    # Plot the heatmap with seaborn.\n    # Assign the matplotlib axis the function returns. This will let us resize the labels.\n    ax = sns.heatmap(corrs, mask=mask, annot=True, cmap='Pastel1_r')\n\n    # Resize the labels.\n    ax.set_xticklabels(ax.xaxis.get_ticklabels(), fontsize=14, rotation=90)\n    ax.set_yticklabels(ax.yaxis.get_ticklabels(), fontsize=14, rotation=0)\n\n    # If you put plt.show() at the bottom, it prevents those useless printouts from matplotlib.\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:24:11.567272Z","iopub.execute_input":"2022-05-05T12:24:11.567584Z","iopub.status.idle":"2022-05-05T12:24:11.583946Z","shell.execute_reply.started":"2022-05-05T12:24:11.56755Z","shell.execute_reply":"2022-05-05T12:24:11.58299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n<p style=\"background-color:white;font-family:timesnewroman;color:lightseagreen;font-size:200%;border-radius:20px 60px;\"> Statistical analysis and correlations</p>","metadata":{}},{"cell_type":"code","source":"correlation_heat_map(data_heatmap_df)\n\n#inference: By checking the direct correlation of features there is no much promising correlations. \n#           There are no much correlation within the features as well. In a way this is good because \n#           there won't be multicollinearity that we have to take care while running models.\n","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:24:11.590948Z","iopub.execute_input":"2022-05-05T12:24:11.591378Z","iopub.status.idle":"2022-05-05T12:24:12.350005Z","shell.execute_reply.started":"2022-05-05T12:24:11.591336Z","shell.execute_reply":"2022-05-05T12:24:12.349069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n<p style=\"background-color:white;font-family:timesnewroman;color:lightseagreen;font-size:200%;border-radius:20px 60px;\">  Plotting data</p>","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(8,6))\nsns.pointplot(x=\"Date\", y=\"Temperature\", data=data, color = 'salmon')\nplt.xlabel('Time Period')\nplt.ylabel('Temperature')\nplt.title('Temperature over Time')\nplt.show()\n# inference: Graph clearly shows Temperature is more of a seasonal and repeated in cycles and this would \n# be an interesting data point that we can use for studies further","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:24:12.35147Z","iopub.execute_input":"2022-05-05T12:24:12.351798Z","iopub.status.idle":"2022-05-05T12:24:22.855944Z","shell.execute_reply.started":"2022-05-05T12:24:12.351753Z","shell.execute_reply":"2022-05-05T12:24:22.854831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(8,6))\nsns.pointplot(x=\"Date\", y=\"Fuel_Price\", data=data, color = 'sandybrown')\nplt.xlabel('Time Period')\nplt.ylabel('Fuel Price')\nplt.title('Fuel Price over Time')\nplt.show()\n# inference: Fuel price varies over time and there are high and lows","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:24:22.857461Z","iopub.execute_input":"2022-05-05T12:24:22.857787Z","iopub.status.idle":"2022-05-05T12:24:33.662298Z","shell.execute_reply.started":"2022-05-05T12:24:22.857745Z","shell.execute_reply":"2022-05-05T12:24:33.661106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(8,6))\nsns.pointplot(x=\"Date\", y=\"CPI\", data=data, color = 'turquoise')\nplt.xlabel('Time Period')\nplt.ylabel('Consumer Price Index')\nplt.title('Consumer Price Index over Time')\nplt.show()\n# inference: over time CPI have increased. but the change is not much","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:24:33.66527Z","iopub.execute_input":"2022-05-05T12:24:33.665576Z","iopub.status.idle":"2022-05-05T12:24:44.201399Z","shell.execute_reply.started":"2022-05-05T12:24:33.665529Z","shell.execute_reply":"2022-05-05T12:24:44.200725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(8,6))\nsns.pointplot(x=\"Date\", y=\"Unemployment\", data=data, color='khaki')\nplt.xlabel('Time Period')\nplt.ylabel('Unemployment')\nplt.title('Unemployment over Time')\nplt.show()\n# inference:  Over time unemployment have came down we can see this factor also whether it have affected the Sales\n","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:24:44.202564Z","iopub.execute_input":"2022-05-05T12:24:44.203307Z","iopub.status.idle":"2022-05-05T12:24:54.621632Z","shell.execute_reply.started":"2022-05-05T12:24:44.203272Z","shell.execute_reply":"2022-05-05T12:24:54.620693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**This is interesting. Features over time changes quite a bit. We will see whether these have any effects on Sales while we model**","metadata":{}},{"cell_type":"markdown","source":"We know that **Markdown columns have quiet few outliers** it can be because of our imputation as well. So we will treate them later and try implement a better way of imputation. Eg: KNN imputation, Random value between min and max imputation..\n\n**Weekly Sales also seem to have some Outliers but we will keep them as it is for now as they may indicate crucial information.**\n","metadata":{}},{"cell_type":"markdown","source":"\n<p style=\"background-color:white;font-family:timesnewroman;color:lightseagreen;font-size:200%;border-radius:20px 60px;\"> Visual analysis</p>","metadata":{}},{"cell_type":"code","source":"# Checking how the Type of the store have effect on the sales.\ncol=['coral', 'greenyellow', 'turquoise']\nsns.barplot(x=\"Type\", y=\"Weekly_Sales\", data=data,orient='v', palette ='YlOrBr')\nplt.xlabel('Type of Store')\nplt.ylabel(' Mean Weekly Sales')\nplt.title('Mean Sales vs Type of Store')\n#plt.savefig('./images/Type_vs_Sales.png')\nplt.show()\n\n# inference: From the graph its clear that Type A > Type B > Type C in mean weekly sales. ","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:24:54.623104Z","iopub.execute_input":"2022-05-05T12:24:54.623451Z","iopub.status.idle":"2022-05-05T12:24:54.99415Z","shell.execute_reply.started":"2022-05-05T12:24:54.623406Z","shell.execute_reply":"2022-05-05T12:24:54.993035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.subplots(figsize=(14,10))\nsns.barplot(x=\"Store\", y=\"Weekly_Sales\", data=data,orient='v')\nplt.xlabel('Store Number')\nplt.ylabel(' Mean Weekly Sales')\nplt.title('Mean weekly Sales of each Store ')\n#plt.savefig('./images/Mean_Weekly_Sales_vs_Stores.png')\nplt.show()\n\n# inference : From the chart we can see that there are stores that have a weekly sales from $250,000  \n#             to $2,200,000","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:24:54.995765Z","iopub.execute_input":"2022-05-05T12:24:54.996119Z","iopub.status.idle":"2022-05-05T12:24:57.253271Z","shell.execute_reply.started":"2022-05-05T12:24:54.996073Z","shell.execute_reply":"2022-05-05T12:24:57.252539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.subplots(figsize=(10,7))\nsns.barplot(x=\"Month\", y=\"Weekly_Sales\", data=data,orient='v')\nplt.xlabel('Month')\nplt.ylabel(' Mean Weekly Sales')\nplt.title('Mean weekly Sales in each Month')\n#plt.savefig('./images/Mean_Weekly_Sales_vs_Months.png')\nplt.show()\n# inference: Graph shows sales in each month and from this we can see December seems to have a very high sales \n#            compared to every other month and January have the least sales. ","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:24:57.254616Z","iopub.execute_input":"2022-05-05T12:24:57.254837Z","iopub.status.idle":"2022-05-05T12:24:58.011488Z","shell.execute_reply.started":"2022-05-05T12:24:57.254809Z","shell.execute_reply":"2022-05-05T12:24:58.010348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### With this we come to an end of EDA & Time series analysis. We will now move forward with Machine Learning & Modelling","metadata":{}},{"cell_type":"markdown","source":"\n<p style=\"background-color:white;font-family:timesnewroman;color:lightseagreen;font-size:230%;border-radius:20px 60px;\"> Feature Engineering </p>\n\n#### Before creating and running model we will look through the columns and make meaningful columns and create dummies where needed. \n\n**We need to take special care (KNN Imputation) of Markdown5 which has many outlier and as we know Markdown1-5 which have a lot of missing values.** \n\nWe will create dummies for **\"Store, Month, Year\"**\n\n","metadata":{}},{"cell_type":"code","source":"# Create Week column which says which week of the month it is. \ndata[\"Week\"]= round(np.floor(((data.Day-1)/7)+1))\n\n# Create dummies for the columns that are required for later studies\nStore_dummies = pd.get_dummies(data.Store, prefix='Store')\nMonth_dummies = pd.get_dummies(data.Month, prefix='Month')\nYear_dummies = pd.get_dummies(data.Year, prefix='Year')\nWeek_dummies = pd.get_dummies(data.Week, prefix='Week')\n\n# concatenate DataFrames (axis=0 for rows, axis=1 for columns)\ndata = pd.concat([data, Store_dummies,Month_dummies,Year_dummies,Week_dummies], axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:24:58.012969Z","iopub.execute_input":"2022-05-05T12:24:58.013298Z","iopub.status.idle":"2022-05-05T12:24:58.031619Z","shell.execute_reply.started":"2022-05-05T12:24:58.013255Z","shell.execute_reply":"2022-05-05T12:24:58.03043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_decision=data.iloc[:,:18]\n\ndata_decision[\"Week\"]= round(np.floor(((data_decision.Day-1)/7)+1))","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:24:58.033454Z","iopub.execute_input":"2022-05-05T12:24:58.033802Z","iopub.status.idle":"2022-05-05T12:24:58.043842Z","shell.execute_reply.started":"2022-05-05T12:24:58.033749Z","shell.execute_reply":"2022-05-05T12:24:58.042725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Drop the columns that we have created dummies\ndata.drop(['Type', 'Store','Month','Year','Day','Week'], axis=1, inplace=True) ","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:24:58.044969Z","iopub.execute_input":"2022-05-05T12:24:58.045788Z","iopub.status.idle":"2022-05-05T12:24:58.057695Z","shell.execute_reply.started":"2022-05-05T12:24:58.045748Z","shell.execute_reply":"2022-05-05T12:24:58.057035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# drop each column from the list of dummies to make it perfect to use in models\ndata.drop(['Type_C', 'Store_1','Month_Jan','Year_2010','Week_5.0'], axis=1, inplace=True) ","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:24:58.058984Z","iopub.execute_input":"2022-05-05T12:24:58.059509Z","iopub.status.idle":"2022-05-05T12:24:58.069298Z","shell.execute_reply.started":"2022-05-05T12:24:58.059457Z","shell.execute_reply":"2022-05-05T12:24:58.068393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n\nNext, we will perform **KNN (K Nearest Neighbours) imputation for Markdown columns**. \n\n    This might not be a big success but according to current situation that might be the best way so that I can use those variables for modeling and try how this comes out. Other ways are imputing with 0, median value, random value but those will put same value in all missing data and that can be very miss leading or bad way to do it.\n\n**Lets see whether there would be any effect in sales according to the MarkDowns after this**\n\n\n<p style=\"background-color:white;font-family:timesnewroman;color:lightseagreen;font-size:200%;border-radius:20px 60px;\">  KNN model imputation </p>\n","metadata":{}},{"cell_type":"markdown","source":"#### Finally, it's time for treating the Markdown columns","metadata":{}},{"cell_type":"code","source":"data.iloc[:,5:10].describe().T\n\n# Inference: more than 50% is missing values with (-500) so imputing with KNN might not be a good idea. \n# But what are the other methods? imputing with random values in the range of that particular columns?\n# Lets try that first.\n","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:24:58.070717Z","iopub.execute_input":"2022-05-05T12:24:58.071137Z","iopub.status.idle":"2022-05-05T12:24:58.112774Z","shell.execute_reply.started":"2022-05-05T12:24:58.071101Z","shell.execute_reply":"2022-05-05T12:24:58.111943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.MarkDown1=data.MarkDown1.map(lambda x: np.nan if x==-500 else x)\ndata.MarkDown2=data.MarkDown2.map(lambda x: np.nan if x==-500 else x)\ndata.MarkDown3=data.MarkDown3.map(lambda x: np.nan if x==-500 else x)\ndata.MarkDown4=data.MarkDown4.map(lambda x: np.nan if x==-500 else x)\ndata.MarkDown5=data.MarkDown5.map(lambda x: np.nan if x==-500 else x)","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:24:58.114194Z","iopub.execute_input":"2022-05-05T12:24:58.114457Z","iopub.status.idle":"2022-05-05T12:24:58.141569Z","shell.execute_reply.started":"2022-05-05T12:24:58.114423Z","shell.execute_reply":"2022-05-05T12:24:58.140496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"missing_cols = ['MarkDown1','MarkDown2','MarkDown3','MarkDown4','MarkDown5']\n\n# Not including our actual y(Weekly Sales) and Size of store for Markdown since by including weekly sales\n# It can be a bad method to use those MarkDown again for predicting weekly sales. \n\nimpute_cols = [c for c in data.columns if not c in ['Weekly_Sales','Date','Sqrt_Sales','lnSales']+missing_cols]\n\ndata_imputed=data.copy()\n","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:24:58.142979Z","iopub.execute_input":"2022-05-05T12:24:58.143226Z","iopub.status.idle":"2022-05-05T12:24:58.148735Z","shell.execute_reply.started":"2022-05-05T12:24:58.143194Z","shell.execute_reply":"2022-05-05T12:24:58.148039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def find_best_k_reg(X, y, k_min=1, k_max=51, step=2, cv=10):\n    k_range = range(k_min, k_max+1, step)\n    r2s = []\n    for k in k_range:\n        knn = KNeighborsRegressor(n_neighbors=k)\n        scores = cross_val_score(knn, X, y, cv=cv)\n        r2s.append(np.mean(scores))\n    print (\"Best R2 value:\",np.max(r2s),\"\\nBest k: \",np.argmax(k_range))\n    return np.argmax(k_range)","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:24:58.149953Z","iopub.execute_input":"2022-05-05T12:24:58.150269Z","iopub.status.idle":"2022-05-05T12:24:58.164252Z","shell.execute_reply.started":"2022-05-05T12:24:58.150241Z","shell.execute_reply":"2022-05-05T12:24:58.163512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"impute_missing = data.loc[data.MarkDown1.isnull(), :]\nimpute_valid = data.loc[~data.MarkDown1.isnull(), :]\n\ny = impute_valid.MarkDown1.values\nX = impute_valid[impute_cols]\n\nXs = ss.fit_transform(X)","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:24:58.165613Z","iopub.execute_input":"2022-05-05T12:24:58.166043Z","iopub.status.idle":"2022-05-05T12:24:58.19061Z","shell.execute_reply.started":"2022-05-05T12:24:58.16601Z","shell.execute_reply":"2022-05-05T12:24:58.189617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_k = find_best_k_reg(Xs, y)\nknn = KNeighborsRegressor(n_neighbors=best_k)\nknn.fit(Xs, y)\n\nX_miss = impute_missing[impute_cols]\nX_miss_s = ss.transform(X_miss)\n\nMarkDown1_impute = knn.predict(X_miss_s)\n\ndata_imputed.loc[data.MarkDown1.isnull(), 'MarkDown1'] = MarkDown1_impute\n\n#Lets look how the MarkDown1 vs Weekly_Sales appear\nsns.jointplot(data_imputed.MarkDown1, data_imputed.Weekly_Sales, joint_kws=dict(s=25, alpha=0.6), color='deeppink')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:24:58.192099Z","iopub.execute_input":"2022-05-05T12:24:58.19278Z","iopub.status.idle":"2022-05-05T12:25:03.735337Z","shell.execute_reply.started":"2022-05-05T12:24:58.192724Z","shell.execute_reply":"2022-05-05T12:25:03.734377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"impute_missing = data.loc[data.MarkDown2.isnull(), :]\nimpute_valid = data.loc[~data.MarkDown2.isnull(), :]\n\ny = impute_valid.MarkDown2.values\nX = impute_valid[impute_cols]\n\nss = StandardScaler()\nXs = ss.fit_transform(X)\nbest_k = find_best_k_reg(Xs, y)\nknn = KNeighborsRegressor(n_neighbors=best_k)\nknn.fit(Xs, y)\n\nX_miss = impute_missing[impute_cols]\nX_miss_s = ss.transform(X_miss)\n\nMarkDown2_impute = knn.predict(X_miss_s)\n\ndata_imputed.loc[data.MarkDown2.isnull(), 'MarkDown2'] = MarkDown2_impute\n\n#Lets look how the MarkDown1 vs Weekly_Sales appear\nsns.jointplot(data_imputed.MarkDown2, data_imputed.Weekly_Sales, joint_kws=dict(s=25, alpha=0.6), color = 'crimson')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:25:03.73692Z","iopub.execute_input":"2022-05-05T12:25:03.737232Z","iopub.status.idle":"2022-05-05T12:25:07.836789Z","shell.execute_reply.started":"2022-05-05T12:25:03.737192Z","shell.execute_reply":"2022-05-05T12:25:07.835853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"impute_missing = data.loc[data.MarkDown3.isnull(), :]\nimpute_valid = data.loc[~data.MarkDown3.isnull(), :]\n\ny = impute_valid.MarkDown3.values\nX = impute_valid[impute_cols]\n\nss = StandardScaler()\nXs = ss.fit_transform(X)\nbest_k = find_best_k_reg(Xs, y)\nknn = KNeighborsRegressor(n_neighbors=best_k)\nknn.fit(Xs, y)\n\nX_miss = impute_missing[impute_cols]\nX_miss_s = ss.transform(X_miss)\n\nMarkDown3_impute = knn.predict(X_miss_s)\n\ndata_imputed.loc[data.MarkDown3.isnull(), 'MarkDown3'] = MarkDown3_impute\n\nsns.jointplot(data_imputed.MarkDown3, data_imputed.Weekly_Sales, joint_kws=dict(s=25, alpha=0.6), color= 'maroon')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:25:07.838114Z","iopub.execute_input":"2022-05-05T12:25:07.838475Z","iopub.status.idle":"2022-05-05T12:25:17.918523Z","shell.execute_reply.started":"2022-05-05T12:25:07.838437Z","shell.execute_reply":"2022-05-05T12:25:17.915707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"impute_missing = data.loc[data.MarkDown4.isnull(), :]\nimpute_valid = data.loc[~data.MarkDown4.isnull(), :]\n\ny = impute_valid.MarkDown4.values\nX = impute_valid[impute_cols]\n\nss = StandardScaler()\nXs = ss.fit_transform(X)\nbest_k = find_best_k_reg(Xs, y)\nknn = KNeighborsRegressor(n_neighbors=best_k)\nknn.fit(Xs, y)\n\nX_miss = impute_missing[impute_cols]\nX_miss_s = ss.transform(X_miss)\n\nMarkDown4_impute = knn.predict(X_miss_s)\n\ndata_imputed.loc[data.MarkDown4.isnull(), 'MarkDown4'] = MarkDown4_impute\n\nsns.jointplot(data_imputed.MarkDown4, data_imputed.Weekly_Sales, joint_kws=dict(s=25, alpha=0.6), color = 'salmon')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:25:17.919898Z","iopub.execute_input":"2022-05-05T12:25:17.920191Z","iopub.status.idle":"2022-05-05T12:25:22.623702Z","shell.execute_reply.started":"2022-05-05T12:25:17.920155Z","shell.execute_reply":"2022-05-05T12:25:22.620559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"impute_missing = data.loc[data.MarkDown5.isnull(), :]\nimpute_valid = data.loc[~data.MarkDown5.isnull(), :]\n\ny = impute_valid.MarkDown5.values\nX = impute_valid[impute_cols]\n\nss = StandardScaler()\nXs = ss.fit_transform(X)\nbest_k = find_best_k_reg(Xs, y)\nknn = KNeighborsRegressor(n_neighbors=best_k)\nknn.fit(Xs, y)\n\nX_miss = impute_missing[impute_cols]\nX_miss_s = ss.transform(X_miss)\n\nMarkDown5_impute = knn.predict(X_miss_s)\n\ndata_imputed.loc[data.MarkDown5.isnull(), 'MarkDown5'] = MarkDown5_impute\n\nsns.jointplot(data_imputed.MarkDown4, data_imputed.Weekly_Sales, joint_kws=dict(s=25, alpha=0.6), color = 'blueviolet')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:25:22.625175Z","iopub.execute_input":"2022-05-05T12:25:22.625509Z","iopub.status.idle":"2022-05-05T12:25:28.066787Z","shell.execute_reply.started":"2022-05-05T12:25:22.625466Z","shell.execute_reply":"2022-05-05T12:25:28.065919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"walmart_data=data_imputed.copy()\n\n# The accuracy and R2 are very bad. This means that we likely imputing crap with these models.\n# This doesn't necessarily mean that imputation is a bad idea, but we may want to consider\n# using a different method.","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:25:28.068346Z","iopub.execute_input":"2022-05-05T12:25:28.069213Z","iopub.status.idle":"2022-05-05T12:25:28.074355Z","shell.execute_reply.started":"2022-05-05T12:25:28.069166Z","shell.execute_reply":"2022-05-05T12:25:28.073371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So finally we have replaced all missing values in **MarkDown1-5** with **KNN imputation** from the given value range itself. We will see the performance of the model and see through whether we have any better methods. As of now my assumption is KNN imputation values would work because thats the best way we have.\n\n**Lets Try Linear Regression for predicting Weekly_Sales with and without the imputed values and see whether its better to keep the imputed MarkDown Values**\n","metadata":{}},{"cell_type":"markdown","source":"<p style=\"background-color:white;font-family:timesnewroman;color:lightseagreen;font-size:220%;border-radius:20px 60px;\">  Modelling </p>\n","metadata":{}},{"cell_type":"code","source":"predictors=[col for col in data_imputed.columns if col not in ['Date','Weekly_Sales']]\nX=data_imputed[predictors]\ny=data_imputed.Weekly_Sales.values \n\nXs = ss.fit_transform(X)\n\nX_train, X_test, y_train, y_test = train_test_split(Xs, y, test_size=0.33)\n\nmlr = LinearRegression()\nmlr.fit(X_train, y_train)\nr2=mlr.score(X_test, y_test)\nprint(mlr.score(X_test, y_test))\nprint(mlr.score(X_train, y_train))\nadj_r2 = 1 - (len(y)-1)/(len(y)-X.shape[1]-1)*(1-r2)\nprint(\"Adjusted R^2\",adj_r2)\n\n\n# Perform 10-fold cross validation\nscores = cross_val_score(mlr, X_train, y_train, cv=10)\nprint (\"Cross-validated scores:\", scores)\nprint (\"Mean Cross validation\",scores.mean())\n\n\n\n# Make cross validated predictions on the test sets\npredictions = cross_val_predict(mlr, X_test, y_test, cv=10)\n\nplt.scatter(predictions, y_test, s=30, c='crimson', zorder=10)\nplt.xlabel('Predicted Weekly Sales')\nplt.ylabel(' Actual Weekly Sales')\nplt.title('Predicted vs Actual Sales')","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:25:28.076201Z","iopub.execute_input":"2022-05-05T12:25:28.076557Z","iopub.status.idle":"2022-05-05T12:25:28.979234Z","shell.execute_reply.started":"2022-05-05T12:25:28.07652Z","shell.execute_reply":"2022-05-05T12:25:28.978355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Now let us look the same model without MarkDowns to check whether data with MarkDown or without MarkDown is good.** ","metadata":{}},{"cell_type":"code","source":"predictors=[col for col in data.columns if col not in ['Date','Weekly_Sales']]\npredictors=[col for col in predictors if 'MarkDown' not in col]\nX=data[predictors]\ny=data.Weekly_Sales.values \nXs = ss.fit_transform(X)\n\nX_train, X_test, y_train, y_test = train_test_split(Xs, y, test_size=0.2)\n\nlr = LinearRegression()\nlr.fit(X_train, y_train)\n\nprint(lr.score(X_test, y_test))\nprint(lr.score(X_train, y_train))\n\n\n# Perform 10-fold cross validation\nscores = cross_val_score(lr, X, y, cv=10)\nprint (\"Cross-validated scores:\", scores)\nprint (\"Mean Cross validation\",scores.mean())\n\n\n\n# Make cross validated predictions on the test sets\npredictions = cross_val_predict(lr, X_test, y_test, cv=10)\n\nplt.scatter(predictions, y_test, s=30, color = 'coral', zorder=10)\nplt.xlabel('Predicted Weekly Sales')\nplt.ylabel(' Actual Weekly Sales')\nplt.title('Predicted vs Actual Sales')","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:25:28.980715Z","iopub.execute_input":"2022-05-05T12:25:28.981646Z","iopub.status.idle":"2022-05-05T12:25:29.949972Z","shell.execute_reply.started":"2022-05-05T12:25:28.981597Z","shell.execute_reply":"2022-05-05T12:25:29.948859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### From the Cross validation its clear that the one with Markdown values are better than they are dropped. \n\nSo we will keep MarkDowns as of now and **assume** the KNN model have imputed the correct values. \nNow lets move on to real **Modeling** for the prediction of Weekly_sales\n\nWe have seen that normal running of Linear regression is a very bad idea. Even though it gives a high R2 value the cross validated score doesn't make any sense. Now its time to go on with **feature selection**\n","metadata":{}},{"cell_type":"code","source":"data=data_imputed.copy()","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:25:29.951615Z","iopub.execute_input":"2022-05-05T12:25:29.952212Z","iopub.status.idle":"2022-05-05T12:25:29.957256Z","shell.execute_reply.started":"2022-05-05T12:25:29.952161Z","shell.execute_reply":"2022-05-05T12:25:29.956277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**We will divide our train and test datasets first and then deal with that seperately**","metadata":{}},{"cell_type":"code","source":"# Setting the offset to finalize the test data.\noffset = timedelta(days=90)\nsplit_date=data.Date.max()-offset","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:25:29.959004Z","iopub.execute_input":"2022-05-05T12:25:29.959419Z","iopub.status.idle":"2022-05-05T12:25:29.97552Z","shell.execute_reply.started":"2022-05-05T12:25:29.959373Z","shell.execute_reply":"2022-05-05T12:25:29.974849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_train=data[data.Date < split_date]\ndata_test=data[data.Date > split_date]","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:25:29.976973Z","iopub.execute_input":"2022-05-05T12:25:29.977934Z","iopub.status.idle":"2022-05-05T12:25:29.99325Z","shell.execute_reply.started":"2022-05-05T12:25:29.977885Z","shell.execute_reply":"2022-05-05T12:25:29.992289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Before we start lets shuffle the dataframe a bit because while we use crossvalidation for regressors it won't take a random sample as test and train, instead it takes section by section. Here my Dataframe have data for each store in order. So if we take section by section model might not have enough data to learn about certain stores and which intern will give terrible answers**","metadata":{}},{"cell_type":"code","source":"data_train = data_train.reindex(np.random.permutation(data_imputed.index))## Identify outliers","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:25:29.994775Z","iopub.execute_input":"2022-05-05T12:25:29.995855Z","iopub.status.idle":"2022-05-05T12:25:30.010772Z","shell.execute_reply.started":"2022-05-05T12:25:29.995817Z","shell.execute_reply":"2022-05-05T12:25:30.009998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_train.columns","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:25:30.011924Z","iopub.execute_input":"2022-05-05T12:25:30.01253Z","iopub.status.idle":"2022-05-05T12:25:30.02237Z","shell.execute_reply.started":"2022-05-05T12:25:30.012497Z","shell.execute_reply":"2022-05-05T12:25:30.021748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_box=data_train.iloc[:, 1:12]\ndata_norm = (data_box - data_box.mean()) / data_box.std()\n\nfig = plt.figure(figsize=(15, 12))\nax = fig.gca()\n\nax = sns.boxplot(data=data_norm, orient='h', fliersize=5, \n                 linewidth=3, notch=True, saturation=0.5, ax=ax)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:25:30.023529Z","iopub.execute_input":"2022-05-05T12:25:30.02393Z","iopub.status.idle":"2022-05-05T12:25:30.434198Z","shell.execute_reply.started":"2022-05-05T12:25:30.023901Z","shell.execute_reply":"2022-05-05T12:25:30.433361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**There are quite a lot of outliers in MarkDown, But Lets first deal the outliers in weekly sales data because we might just drop MarkDowns Later because the percentage of missing values are really high in MarkDowns**","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(figsize=(10, 5))\nax = fig.gca()\n\nax = sns.boxplot(data_train.Weekly_Sales, orient='h', fliersize=5, \n                 linewidth=3, notch=True, saturation=0.5, ax=ax, color = 'yellow')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:25:30.435382Z","iopub.execute_input":"2022-05-05T12:25:30.435598Z","iopub.status.idle":"2022-05-05T12:25:30.664196Z","shell.execute_reply.started":"2022-05-05T12:25:30.435573Z","shell.execute_reply":"2022-05-05T12:25:30.663547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lets consider 3,000,000 as upper limit \ndata_train[data_train.Weekly_Sales>3000].shape","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:25:30.66514Z","iopub.execute_input":"2022-05-05T12:25:30.665732Z","iopub.status.idle":"2022-05-05T12:25:30.676807Z","shell.execute_reply.started":"2022-05-05T12:25:30.665697Z","shell.execute_reply":"2022-05-05T12:25:30.676061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# there is only 14 outliers. Lets drop it and proceed.\ndata_train=data_train[data_train.Weekly_Sales<3000]","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:25:30.678098Z","iopub.execute_input":"2022-05-05T12:25:30.67913Z","iopub.status.idle":"2022-05-05T12:25:30.68693Z","shell.execute_reply.started":"2022-05-05T12:25:30.679089Z","shell.execute_reply":"2022-05-05T12:25:30.685862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <p style=\"background-color:coral;font-family:newtimeroman;color:white;font-size:150%;text-align:center;border-radius:20px 60px;\">Improve and Evaluate Model</p>\n\n\n<p style=\"background-color:white;font-family:timesnewroman;color:lightseagreen;font-size:220%;border-radius:20px 60px;\"> Feature Selection </p>\n\n\nWe have seen the models are giving a terrible crossvalidation score. So lets tune our model. for that first we should figure out what all should we use as our features. For this a feature selection would be the best thing. So lets use Lasso Feature Selection and see how our model works with that.","metadata":{}},{"cell_type":"code","source":"predictors=[col for col in data.columns if col not in ['Weekly_Sales','Sqrt_Sales','lnSales','Date']] # Date\n\npredictors=[col for col in predictors if 'Month' not in col]\npredictors=[col for col in predictors if 'Week' not in col]\npredictors=[col for col in predictors if 'Year' not in col]","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:25:30.688737Z","iopub.execute_input":"2022-05-05T12:25:30.68967Z","iopub.status.idle":"2022-05-05T12:25:30.698904Z","shell.execute_reply.started":"2022-05-05T12:25:30.689612Z","shell.execute_reply":"2022-05-05T12:25:30.69791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n<p style=\"background-color:white;font-family:timesnewroman;color:lightseagreen;font-size:220%;border-radius:20px 60px;\"> Test-Train Split such that Test data is last 90 days data (future 3 months prediction) </p>","metadata":{}},{"cell_type":"code","source":"X_train = data_train[predictors]\ny_train = data_train.Weekly_Sales.values\n\nX_test = data_test[predictors]\ny_test = data_test.Weekly_Sales.values","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:25:30.700421Z","iopub.execute_input":"2022-05-05T12:25:30.700872Z","iopub.status.idle":"2022-05-05T12:25:30.715793Z","shell.execute_reply.started":"2022-05-05T12:25:30.700836Z","shell.execute_reply":"2022-05-05T12:25:30.714752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_s=ss.fit_transform(X_train)\nX_test_s=ss.fit_transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:25:30.717112Z","iopub.execute_input":"2022-05-05T12:25:30.717399Z","iopub.status.idle":"2022-05-05T12:25:30.742671Z","shell.execute_reply.started":"2022-05-05T12:25:30.717362Z","shell.execute_reply":"2022-05-05T12:25:30.741709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n<p style=\"background-color:white;font-family:timesnewroman;color:lightseagreen;font-size:220%;border-radius:20px 60px;\"> LGBM Regressor </p>\n\n**LightGBM is a relatively new algorithm. It is a gradient boosting framework that uses tree based learning algorithm.**\n\n*Light GBM grows tree vertically while other algorithm grows trees horizontally meaning that Light GBM grows tree leaf-wise while other algorithm grows level-wise.*\n\nIt chooses the leaf with max delta loss to grow. When growing the same leaf, Leaf-wise algorithm can reduce more loss than a level-wise algorithm. Light GBM is prefixed as ‘Light’ because of its high speed. \n\n**Light GBM can handle the large size of data and takes lower memory to run. Another reason of why Light GBM is popular is because it focuses on accuracy of results. LGBM also supports GPU learning and thus data scientists are widely using LGBM for data science application development.**","metadata":{}},{"cell_type":"code","source":"lgbm_features = lgb.LGBMRegressor() ","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:25:30.743991Z","iopub.execute_input":"2022-05-05T12:25:30.744281Z","iopub.status.idle":"2022-05-05T12:25:30.7489Z","shell.execute_reply.started":"2022-05-05T12:25:30.744243Z","shell.execute_reply":"2022-05-05T12:25:30.747849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgbm_features.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:25:30.750226Z","iopub.execute_input":"2022-05-05T12:25:30.750586Z","iopub.status.idle":"2022-05-05T12:25:31.036231Z","shell.execute_reply.started":"2022-05-05T12:25:30.750549Z","shell.execute_reply":"2022-05-05T12:25:31.035478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"importance_df = pd.DataFrame({\n    'feature': X_train.columns,\n    'importance': lgbm_features.feature_importances_\n}).sort_values('importance', ascending=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:25:31.039978Z","iopub.execute_input":"2022-05-05T12:25:31.04048Z","iopub.status.idle":"2022-05-05T12:25:31.051126Z","shell.execute_reply.started":"2022-05-05T12:25:31.040435Z","shell.execute_reply":"2022-05-05T12:25:31.050472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(8,6))\nplt.title('Feature Importance')\nsns.barplot(data=importance_df.head(15), x='importance', y='feature');","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:25:31.052506Z","iopub.execute_input":"2022-05-05T12:25:31.052916Z","iopub.status.idle":"2022-05-05T12:25:31.438495Z","shell.execute_reply.started":"2022-05-05T12:25:31.052867Z","shell.execute_reply":"2022-05-05T12:25:31.437548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Clearly we have the top features listed above.\n\n**Top five :**\n    \n    Fuel_Price\n    CPI\n    Markdown3\n    Temperature\n    Size\n    ","metadata":{}},{"cell_type":"markdown","source":"## Note :\n    \n 1. sklearn’s algorithm cheat sheet suggests to try Lasso, ElasticNet, or Ridge when our data-set is smaller than 100k rows. Else, try GDRegressor.\n\n 2. Lasso and ElasticNet tend to give sparse weights (most zeros), because the l1 regularization cares equally about driving down big weights to small weights, or driving small weights to zeros. If you have a lot of predictors (features), and you suspect that not all of them are that important, Lasso and ElasticNet may be really good idea to start with.\n\n***As both the above conditions satisfy, we should go with Lasso regressor and see the performance.***\n\n*If it's not good enough we can always switch back to LGBM/XGB etc.*","metadata":{}},{"cell_type":"markdown","source":"#### LASSO is a regression analysis method that performs both variable selection and regularization in order to enhance the prediction accuracy and interpretability of the statistical model it produces. \n\n*The basic idea is to penalize the model coefficients such that they don’t grow too big and overfit the data. Using LASSO regression, we are essentially eliminating the higher-order terms in the more complex models.*\n\n**So, LASSO regression is similar to Linear Regression, but with a penalization coefficient at the end of the formula, eliminating the least important terms.**\n\nHere, we want to evaluate the best model complexity (order of polynomial degree) for our LASSO regression model. Do we need linear regression with 7th degree order terms to reach the best accuracy, or is 2nd degree enough? Let's see.","metadata":{}},{"cell_type":"markdown","source":"\n<p style=\"background-color:white;font-family:timesnewroman;color:lightseagreen;font-size:220%;border-radius:20px 60px;\"> Lasso Feature Selection </p>\n\nTest- Train Split such that Test data is last 90 days data (Like a furture 3 months prediction)","metadata":{}},{"cell_type":"markdown","source":"#### LASSO is a regression analysis method that performs both variable selection and regularization in order to enhance the prediction accuracy and interpretability of the statistical model it produces. \n\n*The basic idea is to penalize the model coefficients such that they don’t grow too big and overfit the data. Using LASSO regression, we are essentially eliminating the higher-order terms in the more complex models.*\n\n**So, LASSO regression is similar to Linear Regression, but with a penalization coefficient at the end of the formula, eliminating the least important terms.**\n\nHere, we want to evaluate the best model complexity (order of polynomial degree) for our LASSO regression model. Do we need linear regression with 7th degree order terms to reach the best accuracy, or is 2nd degree enough? Let's see.","metadata":{}},{"cell_type":"code","source":"lasso_cv = LassoCV(n_alphas=1000,max_iter=2000, cv=10, verbose=1)\nlasso_cv.fit(X_train_s, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:25:31.439981Z","iopub.execute_input":"2022-05-05T12:25:31.440286Z","iopub.status.idle":"2022-05-05T12:25:36.039504Z","shell.execute_reply.started":"2022-05-05T12:25:31.440244Z","shell.execute_reply":"2022-05-05T12:25:36.0384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Put the features and coefs into a dataframe\n# sort by magnitude\nlasso_feat = pd.DataFrame(dict(feature=X_train.columns, coef=lasso_cv.coef_, abscoef=np.abs(lasso_cv.coef_)))\nlasso_feat.sort_values('abscoef', inplace=True, ascending=False)\n# main_features\nlasso_feat[lasso_feat.coef != 0.]","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:25:36.041231Z","iopub.execute_input":"2022-05-05T12:25:36.041614Z","iopub.status.idle":"2022-05-05T12:25:36.078245Z","shell.execute_reply.started":"2022-05-05T12:25:36.041565Z","shell.execute_reply":"2022-05-05T12:25:36.077304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"importance_df = pd.DataFrame({\n    'feature': X_train.columns,\n    'importance': lgbm_features.feature_importances_\n}).sort_values('importance', ascending=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:25:36.08006Z","iopub.execute_input":"2022-05-05T12:25:36.080643Z","iopub.status.idle":"2022-05-05T12:25:36.089238Z","shell.execute_reply.started":"2022-05-05T12:25:36.080593Z","shell.execute_reply":"2022-05-05T12:25:36.08834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10,6))\nplt.title('Feature Importance')\nsns.barplot(data=lasso_feat.head(20), x='abscoef', y='feature');","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:25:36.103695Z","iopub.execute_input":"2022-05-05T12:25:36.104394Z","iopub.status.idle":"2022-05-05T12:25:36.497168Z","shell.execute_reply.started":"2022-05-05T12:25:36.10434Z","shell.execute_reply":"2022-05-05T12:25:36.496368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The list of features that are seleted and their magnitude of effect on weekly sales can be seen above (remember the target is scaled down)**","metadata":{}},{"cell_type":"markdown","source":"*We will set the predictors that we got from Lasso as our actual predictors and use in further models*","metadata":{}},{"cell_type":"code","source":"actual_predictors=lasso_feat[lasso_feat.coef != 0.].feature.values","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:25:36.498474Z","iopub.execute_input":"2022-05-05T12:25:36.498905Z","iopub.status.idle":"2022-05-05T12:25:36.505677Z","shell.execute_reply.started":"2022-05-05T12:25:36.498862Z","shell.execute_reply":"2022-05-05T12:25:36.504687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lets see the best alpha score\nlasso_cv.alpha_\n\n#best alpha value is 0.45384197291954748 which could be used later to run model","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:25:36.50698Z","iopub.execute_input":"2022-05-05T12:25:36.507826Z","iopub.status.idle":"2022-05-05T12:25:36.52115Z","shell.execute_reply.started":"2022-05-05T12:25:36.507786Z","shell.execute_reply":"2022-05-05T12:25:36.520447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We will assign the best alpha score and according to that we will train and test our model\nbest_lasso = Lasso(alpha=lasso_cv.alpha_)\nbest_lasso.fit(X_train_s, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:25:36.52246Z","iopub.execute_input":"2022-05-05T12:25:36.522819Z","iopub.status.idle":"2022-05-05T12:25:36.88141Z","shell.execute_reply.started":"2022-05-05T12:25:36.522786Z","shell.execute_reply":"2022-05-05T12:25:36.880294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lasso_scores = cross_val_score(best_lasso, X_train_s, y_train, cv=10)\n\nprint (lasso_scores)\nprint (np.mean(lasso_scores))","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:25:36.883108Z","iopub.execute_input":"2022-05-05T12:25:36.883851Z","iopub.status.idle":"2022-05-05T12:25:40.146738Z","shell.execute_reply.started":"2022-05-05T12:25:36.883792Z","shell.execute_reply":"2022-05-05T12:25:40.145801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Thats great. getting a cross validated score of .933 is good. Now lets use this to predict our last 90 days data which the model don't know about. So if this works well in this test data give a good score and residual is small or comparable to train data we can assume its not overfitting","metadata":{}},{"cell_type":"code","source":"lasso_yhat=best_lasso.predict(X_test_s)\nlasso_score=best_lasso.score(X_test_s, y_test)\nprint(\"R2: \",lasso_score)\nlasso_adj_r2 = 1 - (len(y_test)-1)/(len(y_test)-X_test.shape[1]-1)*(1-lasso_score)\nprint(\"Adjusted R2: \",lasso_adj_r2)","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:25:40.148551Z","iopub.execute_input":"2022-05-05T12:25:40.149121Z","iopub.status.idle":"2022-05-05T12:25:40.166465Z","shell.execute_reply.started":"2022-05-05T12:25:40.149073Z","shell.execute_reply":"2022-05-05T12:25:40.165346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# converting the residuals into the actual dimenssion\n\ntrain_resids = y_train*1000 - best_lasso.predict(X_train_s)*1000\ntest_resids = y_test*1000 - lasso_yhat*1000\nlasso_residue=np.abs(test_resids).sum()\n# Let me look at the actual Residuals.\nprint(\"Train Residual\",np.abs(train_resids).sum())\nprint(\"Test Residual\",lasso_residue)\nprint(\"Residual ratio of Test to Train\",np.abs(test_resids).sum()/np.abs(train_resids).sum())\n# The Residual looks quite big. But this can be because our base values ( Weekly Sales) are quite big \n# and in terms of millions","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:25:40.169355Z","iopub.execute_input":"2022-05-05T12:25:40.171255Z","iopub.status.idle":"2022-05-05T12:25:40.193259Z","shell.execute_reply.started":"2022-05-05T12:25:40.171195Z","shell.execute_reply":"2022-05-05T12:25:40.191919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The residuals seems to be in same ratio,Train dataset have a higher ratio because its compartively bigger in size. \n","metadata":{}},{"cell_type":"code","source":"sns.distplot(test_resids, kde=False, bins=40, color = 'turquoise')\nplt.show()\n\n# The residuals looks ok and almost like a normal distribution","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:25:40.196187Z","iopub.execute_input":"2022-05-05T12:25:40.197916Z","iopub.status.idle":"2022-05-05T12:25:40.48376Z","shell.execute_reply.started":"2022-05-05T12:25:40.197861Z","shell.execute_reply":"2022-05-05T12:25:40.4828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.subplots(figsize=(8,6))\nplt.scatter(lasso_yhat,y_test, c='yellowgreen')\nplt.xlabel('Sales Predicted')\nplt.ylabel('Sales- Actual')\nplt.title('Predicted versus Actual Weekly Sales of Stores')\n#plt.savefig('./images/Actual_vs_Predicted_Sales.png')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:25:40.4853Z","iopub.execute_input":"2022-05-05T12:25:40.485578Z","iopub.status.idle":"2022-05-05T12:25:40.704396Z","shell.execute_reply.started":"2022-05-05T12:25:40.485543Z","shell.execute_reply":"2022-05-05T12:25:40.703552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lasso Model works well giving me a good adjusted R2 and low residual. The residual is also form a normal distribution whcih shows the model is doing not much wierd things. **The actual vs Predicted Sales looks promising** ","metadata":{}},{"cell_type":"markdown","source":"\n<p style=\"background-color:white;font-family:timesnewroman;color:lightseagreen;font-size:220%;border-radius:20px 60px;\"> Setting the predictors from Lasso and Train Test split for further use </p>","metadata":{}},{"cell_type":"markdown","source":"We will set my predictors to the ones that are selected from Lasso and will use this predictors and Train Test split for further use in all my models.","metadata":{}},{"cell_type":"code","source":"X_train = X_train[actual_predictors]\nX_test = X_test[actual_predictors]\n\nX_train_s=ss.fit_transform(X_train)\nX_test_s=ss.fit_transform(X_test)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:25:40.705923Z","iopub.execute_input":"2022-05-05T12:25:40.706402Z","iopub.status.idle":"2022-05-05T12:25:40.728198Z","shell.execute_reply.started":"2022-05-05T12:25:40.706358Z","shell.execute_reply":"2022-05-05T12:25:40.727438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n\n# <p style=\"background-color:coral;font-family:newtimeroman;color:white;font-size:150%;text-align:center;border-radius:20px 60px;\">Final Model selection & Implementation</p>\n","metadata":{}},{"cell_type":"markdown","source":"We have already implemented Lasso Regressor and would be looking through and implementing different model of regressors in this part. This is more or a trail and error method with the know models that could do better in Regressions","metadata":{}},{"cell_type":"markdown","source":"\n\n#### Decision Trees are likely to overfit and result in over learning. \nSo we will go for Random Forest Regressor which is a ensemble method of decision tree and check how it works.","metadata":{}},{"cell_type":"markdown","source":"\n<p style=\"background-color:white;font-family:timesnewroman;color:lightseagreen;font-size:220%;border-radius:20px 60px;\"> Random Forest Regressor </p>","metadata":{}},{"cell_type":"code","source":"rfr=RandomForestRegressor(n_estimators=100, max_depth=None, max_features='auto')","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:25:40.729366Z","iopub.execute_input":"2022-05-05T12:25:40.729705Z","iopub.status.idle":"2022-05-05T12:25:40.733408Z","shell.execute_reply.started":"2022-05-05T12:25:40.729676Z","shell.execute_reply":"2022-05-05T12:25:40.732745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fit and crossvalidate on train data\nrfr.fit(X_train_s, y_train)\nrfr_scores = cross_val_score(rfr, X_train_s, y_train, cv=10)\nnp.mean(rfr_scores)","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:25:40.734694Z","iopub.execute_input":"2022-05-05T12:25:40.735131Z","iopub.status.idle":"2022-05-05T12:26:26.577597Z","shell.execute_reply.started":"2022-05-05T12:25:40.735094Z","shell.execute_reply":"2022-05-05T12:26:26.576985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rfr_yhat = rfr.predict(X_test_s)\nrfr_score=rfr.score(X_test_s, y_test)\n\nprint(\"R2: \",rfr_score)\nrfr_adj_r2 = 1 - (len(y_test)-1)/(len(y_test)-X_test.shape[1]-1)*(1-rfr_score)\nprint(\"Adjusted R2: \",rfr_adj_r2)","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:26:26.578822Z","iopub.execute_input":"2022-05-05T12:26:26.579212Z","iopub.status.idle":"2022-05-05T12:26:26.641492Z","shell.execute_reply.started":"2022-05-05T12:26:26.579182Z","shell.execute_reply":"2022-05-05T12:26:26.64053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.scatter(rfr_yhat, y_test, color='slateblue')\nplt.xlabel('Sales Predicted')\nplt.ylabel('Sales- Actual')\nplt.title('Predicted versus Actual Weekly Sales of Stores')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:26:26.642735Z","iopub.execute_input":"2022-05-05T12:26:26.642994Z","iopub.status.idle":"2022-05-05T12:26:26.831231Z","shell.execute_reply.started":"2022-05-05T12:26:26.642963Z","shell.execute_reply":"2022-05-05T12:26:26.830258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_resids = y_train*1000 - rfr.predict(X_train_s)*1000\ntest_resids = y_test*1000 - rfr_yhat*1000\nrfr_residue=np.abs(test_resids).sum()\n# Let me look at the actual Residuals.\nprint(\"Train Residual\",np.abs(train_resids).sum())\nprint(\"Test Residual\",rfr_residue)\nprint(\"Residual ratio of Test to Train\",np.abs(test_resids).sum()/np.abs(train_resids).sum())","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:26:26.832616Z","iopub.execute_input":"2022-05-05T12:26:26.832864Z","iopub.status.idle":"2022-05-05T12:26:27.007382Z","shell.execute_reply.started":"2022-05-05T12:26:26.832835Z","shell.execute_reply":"2022-05-05T12:26:27.006392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The cross validated score on train data seems to be very good but test score is not that upto mark which says this might not be the best model. Even when we look into residuals its big compared to the size of train data. So we will move on to another model and see how it would be.","metadata":{}},{"cell_type":"markdown","source":"\n<p style=\"background-color:white;font-family:timesnewroman;color:lightseagreen;font-size:220%;border-radius:20px 60px;\"> GradientBoostingRegressor</p>","metadata":{}},{"cell_type":"code","source":"gb = GradientBoostingRegressor(n_estimators=100,max_depth=10,learning_rate=0.1)","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:26:27.008729Z","iopub.execute_input":"2022-05-05T12:26:27.008955Z","iopub.status.idle":"2022-05-05T12:26:27.013599Z","shell.execute_reply.started":"2022-05-05T12:26:27.008928Z","shell.execute_reply":"2022-05-05T12:26:27.012661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gb.fit(X_train_s, y_train)\ngb_scores = cross_val_score(gb, X_train_s, y_train, cv=6)\nnp.mean(gb_scores)","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:26:27.014997Z","iopub.execute_input":"2022-05-05T12:26:27.015231Z","iopub.status.idle":"2022-05-05T12:26:56.78133Z","shell.execute_reply.started":"2022-05-05T12:26:27.015204Z","shell.execute_reply":"2022-05-05T12:26:56.780371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gb_yhat=gb.predict(X_test_s)\ngb_score=gb.score(X_test_s,y_test)\n\nprint(\"R2: \",gb_score)\ngb_adj_r2 = 1 - (len(y_test)-1)/(len(y_test)-X_test.shape[1]-1)*(1-gb_score)\nprint(\"Adjusted R2: \",gb_adj_r2)","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:26:56.782702Z","iopub.execute_input":"2022-05-05T12:26:56.782925Z","iopub.status.idle":"2022-05-05T12:26:56.800171Z","shell.execute_reply.started":"2022-05-05T12:26:56.782899Z","shell.execute_reply":"2022-05-05T12:26:56.799244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.scatter(gb_yhat, y_test, c='orange')\nplt.xlabel('Sales Predicted')\nplt.ylabel('Sales- Actual')\nplt.title('Predicted versus Actual Weekly Sales of Stores')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:26:56.802475Z","iopub.execute_input":"2022-05-05T12:26:56.80284Z","iopub.status.idle":"2022-05-05T12:26:56.991859Z","shell.execute_reply.started":"2022-05-05T12:26:56.802794Z","shell.execute_reply":"2022-05-05T12:26:56.99113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_resids = y_train*1000 - gb.predict(X_train_s)*1000\ntest_resids = y_test*1000 - gb_yhat*1000\ngb_residue=np.abs(test_resids).sum()\n# Let me look at the actual Residuals.\nprint(\"Train Residual\",np.abs(train_resids).sum())\nprint(\"Test Residual\",gb_residue)\nprint(\"Residual ratio of Test to Train\",np.abs(test_resids).sum()/np.abs(train_resids).sum())","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:26:56.992989Z","iopub.execute_input":"2022-05-05T12:26:56.99345Z","iopub.status.idle":"2022-05-05T12:26:57.041765Z","shell.execute_reply.started":"2022-05-05T12:26:56.993418Z","shell.execute_reply":"2022-05-05T12:26:57.041037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we can see the model is learning a lot from train data and gives a cross validated score of .95 where as in the test data the score is .79 which is too low compared to train data. Its not a good sign.\nI will check the residuals and here the test residual is higher than train even though the train dataset is much bigger.\n\n**This again is a clear case of over fitting and we wont use this model further.**","metadata":{}},{"cell_type":"markdown","source":"\n<p style=\"background-color:white;font-family:timesnewroman;color:lightseagreen;font-size:220%;border-radius:20px 60px;\"> Support Vector Regression</p>","metadata":{}},{"cell_type":"code","source":"svr=SVR(C=50000.0, max_iter=500)\n\nsvr.fit(X_train_s, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:26:57.043011Z","iopub.execute_input":"2022-05-05T12:26:57.043268Z","iopub.status.idle":"2022-05-05T12:26:57.324049Z","shell.execute_reply.started":"2022-05-05T12:26:57.043238Z","shell.execute_reply":"2022-05-05T12:26:57.32323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"svr_scores = cross_val_score(svr, X_train_s, y_train, cv=10)\nnp.mean(svr_scores)","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:26:57.325591Z","iopub.execute_input":"2022-05-05T12:26:57.326084Z","iopub.status.idle":"2022-05-05T12:27:00.219559Z","shell.execute_reply.started":"2022-05-05T12:26:57.326039Z","shell.execute_reply":"2022-05-05T12:27:00.218676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"svr_yhat=svr.predict(X_test_s)\nsvr_score=svr.score(X_test_s,y_test)\nprint(\"R2: \",svr_score)\nsvr_adj_r2 = 1 - (len(y_test)-1)/(len(y_test)-X_test.shape[1]-1)*(1-svr_score)\nprint(\"Adjusted R2: \",svr_adj_r2)","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:27:00.22111Z","iopub.execute_input":"2022-05-05T12:27:00.221639Z","iopub.status.idle":"2022-05-05T12:27:00.313609Z","shell.execute_reply.started":"2022-05-05T12:27:00.221592Z","shell.execute_reply":"2022-05-05T12:27:00.312664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.scatter(svr_yhat, y_test, c='turquoise')\nplt.xlabel('Sales Predicted')\nplt.ylabel('Sales- Actual')\nplt.title('Predicted versus Actual Weekly Sales of Stores')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:27:00.31498Z","iopub.execute_input":"2022-05-05T12:27:00.31522Z","iopub.status.idle":"2022-05-05T12:27:00.527804Z","shell.execute_reply.started":"2022-05-05T12:27:00.315189Z","shell.execute_reply":"2022-05-05T12:27:00.52695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_resids = y_train*1000 - rfr.predict(X_train_s)*1000\ntest_resids = y_test*1000 - svr_yhat*1000\nsvr_residue=np.abs(test_resids).sum()\n# Let me look at the actual Residuals.\nprint(\"Train Residual\", np.abs(train_resids).sum())\nprint(\"Test Residual\",svr_residue)\nprint(\"Residual ratio of Test to Train\",np.abs(test_resids).sum()/np.abs(train_resids).sum())","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:27:00.528951Z","iopub.execute_input":"2022-05-05T12:27:00.529199Z","iopub.status.idle":"2022-05-05T12:27:00.702687Z","shell.execute_reply.started":"2022-05-05T12:27:00.529169Z","shell.execute_reply":"2022-05-05T12:27:00.701706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**This looks great!** The cross validation score and score on test data are almost same even though its not a very high percentage. Even the residuals are small. This tells atleast there is no much overfittig.","metadata":{}},{"cell_type":"markdown","source":"#### So out of all models the best comes with Lasso regression","metadata":{}},{"cell_type":"code","source":"Residual_graph=pd.DataFrame()\nResidual_graph[\"Store\"]=range(1,46)\nResidual_graph['actual_y']=0\nResidual_graph['predicted_lasso_y']=0\n\ncount=0\nfor x in y_test:\n    count+=1\n    Residual_graph['actual_y'][count%45]+=x\n\ncount=0\nfor x in lasso_yhat:\n    count+=1\n    Residual_graph['predicted_lasso_y'][count%45]+=x  \n\nResidual_graph[\"actual_y\"]=Residual_graph[\"actual_y\"]/13\nResidual_graph[\"predicted_lasso_y\"]=Residual_graph[\"predicted_lasso_y\"]/13\n\nResidual_graph[\"Residual_lasso\"]=np.abs(Residual_graph[\"actual_y\"] - Residual_graph[\"predicted_lasso_y\"])\nResidual_graph[\"Residual_lasso_percentage\"]=(Residual_graph[\"Residual_lasso\"]/Residual_graph[\"actual_y\"])*100","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:27:00.704146Z","iopub.execute_input":"2022-05-05T12:27:00.704407Z","iopub.status.idle":"2022-05-05T12:27:01.034726Z","shell.execute_reply.started":"2022-05-05T12:27:00.704373Z","shell.execute_reply":"2022-05-05T12:27:01.033736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Setting the size of bubble according to the percentage change in prediction\ns=Residual_graph.Residual_lasso_percentage.values\ns=s*100","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:27:01.036138Z","iopub.execute_input":"2022-05-05T12:27:01.036557Z","iopub.status.idle":"2022-05-05T12:27:01.041509Z","shell.execute_reply.started":"2022-05-05T12:27:01.036521Z","shell.execute_reply":"2022-05-05T12:27:01.040574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.subplots(figsize=(12,5))\nplt.scatter(Residual_graph.Store, Residual_graph.Residual_lasso_percentage, s=s, color = 'yellowgreen')\nplt.xlabel('Store Number')\nplt.ylabel('Percentage of variation')\nplt.title('Percentage variation of prediction in each store with Lasso Model')\n#plt.savefig('./images/percentage_prediction_variation.png')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:27:01.042859Z","iopub.execute_input":"2022-05-05T12:27:01.0432Z","iopub.status.idle":"2022-05-05T12:27:01.258895Z","shell.execute_reply.started":"2022-05-05T12:27:01.043156Z","shell.execute_reply":"2022-05-05T12:27:01.25797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### This plot shows the variation of prediction or the percentage residual change for each store.\n\nHere the bigger the bubble is the higher the variation in prediction. So we can see that there are **around 4 stores which have more than 5% variation in prediction.**\n\n**The smaller the bubble the better it is** since that shows how predictions are close to actual values. This can be used to futher tune our model because some other model might be doing good with the store which is doing bad in this model.\n","metadata":{}},{"cell_type":"code","source":"# Create a dataframe to compare different models\nScore=pd.DataFrame()\nScore[\"Model_Name\"]=('Lasso','RandomForest','GradientBoosting','SupportVector')\nScore[\"Test_Score\"]=(lasso_score,rfr_score,gb_score,svr_score)\nScore[\"Adj_R2\"]=(lasso_adj_r2,rfr_adj_r2,gb_adj_r2,svr_adj_r2)\nScore[\"Test_Residual\"]=(lasso_residue,rfr_residue,gb_residue,svr_residue)","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:27:01.260198Z","iopub.execute_input":"2022-05-05T12:27:01.260463Z","iopub.status.idle":"2022-05-05T12:27:01.271283Z","shell.execute_reply.started":"2022-05-05T12:27:01.260431Z","shell.execute_reply":"2022-05-05T12:27:01.270279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n# <p style=\"background-color:coral;font-family:newtimeroman;color:white;font-size:150%;text-align:center;border-radius:20px 60px;\">Insights & Conclusion</p>\n\n<p style=\"background-color:white;font-family:timesnewroman;color:lightseagreen;font-size:220%;border-radius:20px 60px;\"> Interpret findings and relate to goals/criteria</p>\n","metadata":{}},{"cell_type":"markdown","source":" - From the modeling we have found **Lasso method works the best by itself with an accuracy of .93.**\n\n - The **rain forest regressor and SVR also works good**. We have plotted a percentage residual change with bubbles to check how good is the prediction for each store. \n \n - This could be a good method to look into each store and see which stores are doing good with this model and which are not. \n \n - Similarly we can find model that works better for that particular store and make an ensemble model with prediction power for each store assigned to each model.","metadata":{}},{"cell_type":"code","source":"# Checking how the Type of the store have effect on the sales.\nsns.barplot(x=\"Model_Name\", y=\"Adj_R2\", data=Score,orient='v', palette='rainbow')\nplt.xlabel('Model used')\nplt.ylabel(' R2 value for test data')\nplt.title('R2 for different models')\n#plt.savefig('./images/R2_for_different_models.png')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:27:01.272812Z","iopub.execute_input":"2022-05-05T12:27:01.273062Z","iopub.status.idle":"2022-05-05T12:27:01.464238Z","shell.execute_reply.started":"2022-05-05T12:27:01.27303Z","shell.execute_reply":"2022-05-05T12:27:01.463256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Lasso Regressor gives the best prediction and outperforms all other models as indicated by the R2 values.**","metadata":{}},{"cell_type":"code","source":"sns.barplot(x=\"Model_Name\", y=\"Test_Residual\", data=Score,orient='v', palette='rainbow')\nplt.xlabel('Model used')\nplt.ylabel(' Sum of Residual from all stores for 90 days')\nplt.title('Sum of Residual for each model')\n#plt.savefig('./images/Residual_for_different_models.png')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:27:01.465482Z","iopub.execute_input":"2022-05-05T12:27:01.465712Z","iopub.status.idle":"2022-05-05T12:27:01.694586Z","shell.execute_reply.started":"2022-05-05T12:27:01.465684Z","shell.execute_reply":"2022-05-05T12:27:01.693697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Even the residual for Lasso model is way smaller compared to other models**","metadata":{}},{"cell_type":"markdown","source":"### Conclusion\n\n*Finally, after building multiple models to predict the weekly sales of 45 stores, we got the best accuracy of around 94% of accuracy for 3 months(90 Days).*\n\nOut of all the models, we select the **Lasso Regressor as our predictive Model since due to it's highest prediction accuracy  with the lowest residual(error).**\n","metadata":{}},{"cell_type":"markdown","source":"\n<p style=\"background-color:white;font-family:timesnewroman;color:lightseagreen;font-size:220%;border-radius:20px 60px;\"> Recommendation and Benefits</p>\n\n1. **Size of the store is the highest contributing predictor in the model out of all.**\n2. **Each store has a unique prediction power**. They can be separately analyzed to get prediction for each individual store\n3. The **Sales are very high during November and December and go down in January**. So its better to employee more staff as casual employee in November and December and encourage permanent staff to take leaves during January.\n4. The predicted sales data can be used to analyse the sales pattern and accordingly **adjust the staff in the store.**\n5. When we implement the project to department level it helps to plan the inventory and staff from a centralised station to every store, which will further help in **better planning and cost cutting** for **inventory management, supply chain management and human resource.**\n6. The **low selling stores should look forward to increasing their size and capacity to store more items and consumer products.**\n7. **Special discount coupons can be distributed during low selling periods to attract more customers**\n\n8. Sales are likely to fluctuate during holidays. **Special offers can be given during festive season** accompanied with suitable marketing to keep the sales high during holidays as well","metadata":{}},{"cell_type":"markdown","source":"\n<p style=\"background-color:white;font-family:timesnewroman;color:lightseagreen;font-size:220%;border-radius:20px 60px;\"> Next steps</p>","metadata":{}},{"cell_type":"markdown","source":"**The Next steps would be :-**\n\n1. To check into the store that have poor prediction and check deep what makes those bad. \n2. To further improve the predictive model using the ensembling method to combine models and come with better model.\n3. Take the data to Department level and to predict the Department level sales which would help to solve the inventory management issues and supply chain management.","metadata":{}},{"cell_type":"markdown","source":"![](https://businessfirstfamily.com/wp-content/uploads/2016/01/sales-forecasting-methods-graphs.jpg)","metadata":{}},{"cell_type":"markdown","source":"# <p style=\"background-color:white;font-family:newtimeroman;color:turquoise;font-size:150%;text-align:center;border-radius:20px 60px;\">REFERENCES</p>\n\nhttps://medium.datadriveninvestor.com/walmart-sales-data-analysis-sales-prediction-using-multiple-linear-regression-in-r-programming-adb14afd56fb\n\nhttps://prezi.com/vpvakz73fjjp/walmart-sales-analysis/\n\nhttps://www.kaggle.com/code/maxdiazbattan/wallmart-sales-eda-feat-eng-future-update\n\nhttps://www.kaggle.com/code/avelinocaio/walmart-store-sales-forecasting\n\nhttps://www.kaggle.com/code/yepp2411/walmart-prediction-1-eda-with-time-and-space","metadata":{}},{"cell_type":"markdown","source":"# <p style=\"background-color:coral;font-family:newtimeroman;color:white;font-size:150%;text-align:center;border-radius:20px 60px;\">HAPPY FORECASTING!!!</p>","metadata":{"execution":{"iopub.status.busy":"2022-05-05T06:37:00.487565Z","iopub.execute_input":"2022-05-05T06:37:00.487874Z","iopub.status.idle":"2022-05-05T06:37:00.492617Z","shell.execute_reply.started":"2022-05-05T06:37:00.487839Z","shell.execute_reply":"2022-05-05T06:37:00.491564Z"}}}]}