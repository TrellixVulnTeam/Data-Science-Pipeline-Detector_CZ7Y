{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### ASSIGNMENT\n\n<b>Problem Statement:</b>\nYou are provided with historical sales data for 45 stores of a Retail chain located in different\nregions. Each store contains a number of departments, and you are tasked with predicting the\ndepartment-wide sales for each store.\n\nThe data is provided in 4 different CSVs.\n\n","metadata":{"slideshow":{"slide_type":"slide"}}},{"cell_type":"code","source":"# importing basic packages\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O\nimport datetime # manipulating date formats\n# Viz\nimport matplotlib.pyplot as plt # basic plotting\nimport seaborn as sns # for prettier plots\n%matplotlib inline\n\n# settings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"slideshow":{"slide_type":"subslide"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# reading data\nfeatures=pd.read_csv(\"../input/walmart-recruiting-store-sales-forecasting/features.csv.zip\")\nstores=pd.read_csv(\"../input/walmart-recruiting-store-sales-forecasting/stores.csv\")\ntrain=pd.read_csv(\"../input/walmart-recruiting-store-sales-forecasting/train.csv.zip\")\ntest=pd.read_csv(\"../input/walmart-recruiting-store-sales-forecasting/test.csv.zip\")","metadata":{"slideshow":{"slide_type":"fragment"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_names=['features','stores','train','test']\ndf_list=[features,stores,train,test]\nfor i in range(4):\n    print('--'*15)\n    print(f'Dataframe {df_names[i]} has {df_list[i].shape[0]} rows and {df_list[i].shape[1]} columns.')\n    print('--'*15)\n    display(df_list[i].head(5))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checking time duration of records\nfor i in [0,2,3]:\n    print(f'Dataframe {df_names[i]} contains data from {df_list[i].Date.min()} to {df_list[i].Date.max()}.\\n')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checking missing values\nfor i in range(4):\n    print(f'Dataframe {df_names[i]} has missing values.\\n') if (df_list[i].isna().sum().any()==True) else print(f'Dataframe {df_names[i]} does not have missing values.')","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import missingno as msno\nmsno.bar(features,figsize=(15, 5),fontsize=15,color='orange');","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Percentages of missing values in features dataframe.')\n(100*features.isna().sum()/features.shape[0]).sort_values()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All the markdown<sub>i</sub> columns have more than 50% missing values. These are anonymized data related to promotional markdowns that the retail chain is running. MarkDown data is only available after Nov 2011. So, it's quite difficult to choose best imputation technique. If correlation of these columns is not strong with target variable, I will drop them. Let's do EDA first.\n\n### EDA","metadata":{}},{"cell_type":"code","source":"# pie chart\nlabels = stores.Type.value_counts().index.tolist()\nsizes = stores.Type.value_counts().values.tolist()\nexplode = (0.05, 0.02, 0)\nplt.figure(figsize=(5,5))\nplt.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%', startangle=60,\n        textprops={'fontsize': 18},colors=['#f538cc','#fa5282','#facc69'])\nplt.title('Different types of stores');","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ax = sns.countplot(stores.Type ,facecolor=(0,0,0,0),linewidth=10,\n                   edgecolor=sns.color_palette(\"spring\", 3))\nfor p in ax.patches:\n    ax.annotate(f'Number of\\n stores:\\n {p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()-4),\n               ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points',fontsize=12);","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Almost half of the stores are of type A. Type C stores are least in number.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(15,5))\nsns.barplot(x='Store',y='Size',data=stores,order=stores.sort_values('Size')['Store'].tolist())\nplt.title('Sizes of all the stores.',fontsize=15)\nplt.tight_layout();","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are broadly 3 types of stores: small-sized, medium-sized and large-sized. This numerical variable can be converted into categorical variable using pd.cut function but first let's check relation between size and type of the stores.","metadata":{}},{"cell_type":"code","source":"sns.set_style('whitegrid')\nsns.boxplot(x='Type',y='Size',data=stores,palette='spring')\nplt.title('Type vs Size',fontsize=15);","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Though we don't have any knowledge about how stores were divided into these types, we can see from the graph that it covers the effect of size column.","metadata":{}},{"cell_type":"code","source":"# combining train/test and stores and features dataframes for further analysis\n\ntrain_expanded = train.merge(features, how='inner',on=['Store','Date','IsHoliday']).sort_values(by=\n                            ['Store','Dept','Date']).reset_index(drop=True)\ntrain_expanded = train_expanded.merge(stores, how='inner', on=['Store'])\n\ntest_expanded = test.merge(features, how='inner',on=['Store','Date','IsHoliday']).sort_values(by=\n                            ['Store','Dept','Date']).reset_index(drop=True)\ntest_expanded = test_expanded.merge(stores, how='inner', on=['Store'])\n\n# converting dtype of date column\ntrain_expanded['Date'] = pd.to_datetime(train_expanded['Date'])\ntest_expanded['Date'] = pd.to_datetime(test_expanded['Date'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_expanded.head(3)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15,3))\ntrain_expanded.groupby('Date')['Weekly_Sales'].mean().plot()\nplt.title('Average weekly Sales of the company across all stores in given timeframe', fontsize=18)\nplt.ylabel('Sales', fontsize=16)\nplt.xlabel('Date', fontsize=16);","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# creating 3 new features from date column\n\nfor df in [train_expanded,test_expanded]:\n    df['Week'] = df['Date'].dt.week\n    df['Month'] = df['Date'].dt.month\n    df['Year'] = df['Date'].dt.year\n\nplt.figure(figsize=(15,3))\ntrain_expanded[train_expanded['Year']==2010].groupby('Month').mean()['Weekly_Sales'].plot()\ntrain_expanded[train_expanded['Year']==2011].groupby('Month').mean()['Weekly_Sales'].plot()\ntrain_expanded[train_expanded['Year']==2012].groupby('Month').mean()['Weekly_Sales'].plot()\nplt.title('Average weekly Sales of the company in each year', fontsize=18)\nplt.legend(['2010', '2011', '2012'], loc='best', fontsize=16)\nplt.ylabel('Sales', fontsize=16)\nplt.xlabel('Months', fontsize=16);","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15,3))\ntrain_expanded[train_expanded['Type']=='A'].groupby('Month').mean()['Weekly_Sales'].plot()\ntrain_expanded[train_expanded['Type']=='B'].groupby('Month').mean()['Weekly_Sales'].plot()\ntrain_expanded[train_expanded['Type']=='C'].groupby('Month').mean()['Weekly_Sales'].plot()\nplt.title('Average weekly Sales of the company by type of the store', fontsize=18)\nplt.legend(['Type A', 'Type B', 'Type C'], loc='best', fontsize=16)\nplt.ylabel('Sales', fontsize=16)\nplt.xlabel('Months', fontsize=16);","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Sales of the company see rise during the end of the year. Possible reasons could be any tradition or festival in that company.\n\nType A stores seem to have comparitively high sales. But this can be due to difference in number of stores of different type. Also, we can notice that sales of type C are constant over the year.","metadata":{}},{"cell_type":"code","source":"def av_sales_plotter(str):\n    plt.figure(figsize=(20,5))\n    train_expanded.groupby(str).mean()['Weekly_Sales'].sort_values().plot(kind='bar',color='#b7f28a')\n    plt.title(f'Average Sales of each {str} in given timeframe.', fontsize=18)\n    plt.ylabel('Sales', fontsize=16)\n    plt.xlabel(str, fontsize=16)\n    plt.tick_params(axis='x', labelsize=14)\n    \nav_sales_plotter('Store')","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"av_sales_plotter('Dept')","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Around 10 departments have lowest sales. The company can further analyse as to what are the possible reasons and how it can be improved.","metadata":{}},{"cell_type":"code","source":"print('IsHoliday vs Weekly_Sales')\nplt.figure(figsize=(15,5))\nplt.subplot(1,2,1)\nsns.stripplot(y=train_expanded['Weekly_Sales'],x=train_expanded['IsHoliday'])\nplt.subplot(1,2,2)\nsns.violinplot(y=train_expanded['Weekly_Sales'],x=train_expanded['IsHoliday']);","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Highest sales events were recorded in the special holiday week.","metadata":{}},{"cell_type":"code","source":"print('Type vs Weekly_Sales')\nplt.figure(figsize=(15,5))\nplt.subplot(1,2,1)\nsns.stripplot(y=train_expanded['Weekly_Sales'],x=train_expanded['Type'])\nplt.subplot(1,2,2)\nsns.boxenplot(y=train_expanded['Weekly_Sales'],x=train_expanded['Type']);","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Highest sales events were recorded in the type B stores.","metadata":{}},{"cell_type":"code","source":"print('Weekly sales vs size')\nsns.jointplot(train_expanded['Weekly_Sales'],train_expanded['Size']);","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"No specific pattern.","metadata":{}},{"cell_type":"code","source":"train_expanded[['Date', 'Temperature', 'Fuel_Price', 'CPI', 'Unemployment']].plot(x='Date', subplots=True, figsize=(20,15));","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As expected, temperature has high seasonality. Week and month column can effectively cover its effect.\n\nFuel price and CPI show an upward trend and unemployment shows downward trend.\n\nLet's explore effect of these feature with weekly sales.","metadata":{}},{"cell_type":"code","source":"sns.set_palette(\"summer\")\nsns.pairplot(train_expanded[['Weekly_Sales', 'Temperature', 'Fuel_Price', 'CPI', 'Unemployment']],\n             corner=True,diag_kind=\"kde\");","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is no particular relationship between these features and target variable.\n\nAlso, distribution of target variable is highly skewed. That's why, I will not go with linear regression.","metadata":{}},{"cell_type":"code","source":"train_expanded.dropna().corr()['Weekly_Sales'].abs().sort_values()[:-1].plot(kind='bar');","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Dize and Dept are most correlated with the target variable.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(15,10))\nsns.heatmap(train_expanded.corr(),annot=True,cmap='summer');","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In above heatmap, correlation between different columns can be checked.","metadata":{}},{"cell_type":"markdown","source":"## Pre-processing and modelling","metadata":{}},{"cell_type":"code","source":"# importing relevant libraries\n\nfrom sklearn.model_selection import train_test_split,KFold,cross_val_score\nfrom sklearn.preprocessing import StandardScaler\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_absolute_error,r2_score,mean_squared_error\nfrom sklearn.ensemble import RandomForestRegressor\n# from sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.model_selection import GridSearchCV,RandomizedSearchCV","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I will label encode the IsHoliday and Type feature. From EDA, I concluded to drop Year, CPI, unemployment, temperature and fuel price. I am also dropping all the 'markdown' columns.","metadata":{}},{"cell_type":"code","source":"# preprocessing\n\ndef preprocessor(df):\n    # label-encoding\n    df['IsHoliday'] = df['IsHoliday'].astype('str').map({'True':0,'False':1})\n    df.Type = df.Type.map({'A':2,'B':1,'C':0})\n    # deleting less important features\n    return df.drop(['Date','Year','MarkDown1','MarkDown2','MarkDown4','MarkDown3','MarkDown5','CPI',\n             'Unemployment','Temperature','Fuel_Price'],axis=1)\n\ntrain_preprocessed = preprocessor(train_expanded)\ntest_preprocessed = preprocessor(test_expanded)","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# splitting data into 2 parts\n\ny = train_preprocessed[\"Weekly_Sales\"]\nX = train_preprocessed.drop(['Weekly_Sales'],axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 101)\n\n# function for displaying scores\n\ndef score_calc(predictions):\n    scores = pd.DataFrame([mean_absolute_error(y_test, predictions)\n#                           ,mean_squared_error(y_test, predictions)\n                          ,r2_score(y_test, predictions)],columns=['score'],index=['MAE','R2 score'])\n    return scores\n\n# function for building model\n\ndef run_model(model,name):\n    \n    name = model()\n    name.fit(X_train, y_train)\n    preds = name.predict(X_test)\n    try:\n        feat_imp = name.feature_importances_\n        plt.bar(X_train.columns,feat_imp,color='green')\n        plt.title('Feature Importance graph')\n        plt.xticks(rotation=45)\n    except:\n        pass\n    return score_calc(preds)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_preprocessed.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b> Approach 1: Taking all columns</b>","metadata":{}},{"cell_type":"code","source":"run_model(DecisionTreeRegressor,'dtree')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"run_model(RandomForestRegressor,'rfc')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"run_model(XGBRegressor,'xgb')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"run_model(KNeighborsRegressor,'knn')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b> Approach 2: Standardizing size column</b>\n\nHere, we can see tree based models do not need feature scaling.","metadata":{}},{"cell_type":"code","source":"# scaling size column\nscaler=StandardScaler()\nscaler.fit(train_preprocessed['Size'].values.reshape(-1,1))\nX_train['Size'] = scaler.transform(X_train['Size'].values.reshape(-1, 1))\nX_test['Size'] = scaler.transform(X_test['Size'].values.reshape(-1, 1))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"run_model(DecisionTreeRegressor,'dtree')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# run_model(XGBRegressor,'xgb')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# run_model(RandomForestRegressor,'rfc')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b> Approach 3: Using only Store, Dept and Size columns</b>","metadata":{}},{"cell_type":"code","source":"X_train = X_train[['Store','Dept','Size']]\nX_test = X_test[['Store','Dept','Size']]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"run_model(DecisionTreeRegressor,'dtree')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# I tried one hot encoding week feature but score did not improve so I dropped the idea.\n\n# dummies=pd.get_dummies(train_preprocessed.Week.astype(str),drop_first=True,prefix='week')\n# dum_week = pd.concat([train_preprocessed,dummies],axis=1)\n# dum_week.drop('Week',axis=1,inplace=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Cross-validation","metadata":{}},{"cell_type":"code","source":"# option A\nfrom sklearn.model_selection import cross_val_score\nscore = cross_val_score(RandomForestRegressor(), X, y, cv=4)\nprint(f\"Average 4-Fold CV Score: {score.mean().round(4)}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Hyper parameter tuning","metadata":{}},{"cell_type":"code","source":"random_grid = {'n_estimators': [50,60,70],\n               'max_features': [3,4],\n               'max_depth': [25,30,35],\n               'min_samples_split': [3,4],\n              'min_samples_leaf':[1,2]}\n\nrf_grid = RandomizedSearchCV(RandomForestRegressor(),\n                        random_grid,\n                        cv = 4,\n                        n_jobs = 5,\n                        verbose=True)\n\nrf_grid.fit(X,y)\n\nprint(rf_grid.best_score_)\nprint(rf_grid.best_params_)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So, cv scored increased after hyper-parameter tuning.","metadata":{}},{"cell_type":"code","source":"# parameters = {'learning_rate': [.03, 0.05, .07],\n#               'max_depth': [6,7,8,9],\n#               'n_estimators': [500,700]}\n\n# xgb_grid = GridSearchCV(XGBRegressor(),\n#                         parameters,\n#                         cv = 3,\n#                         n_jobs = 5,\n#                         verbose=True)\n\n# xgb_grid.fit(X,y)\n\n# print(xgb_grid.best_score_)\n# print(xgb_grid.best_params_)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# option B - cross-validation using kfold\n\npredictor_train_scale = train_preprocessed.drop('Weekly_Sales',axis=1)\npredictor_test_scale = test_preprocessed\ntarget_train = train_preprocessed.Weekly_Sales\n\nkf=KFold(n_splits=4,shuffle=True)\n\npreds_3   = list()\ny_pred_3  = []\nr2_score_ = []\nmae=[]\n\n# Applying model on each fold and calculating mean of score\nfor i,(train_idx,val_idx) in enumerate(kf.split(predictor_train_scale)):    \n    \n    X_train, y_train = predictor_train_scale.iloc[train_idx,:], target_train.iloc[train_idx]    \n    X_val, y_val = predictor_train_scale.iloc[val_idx, :], target_train.iloc[val_idx]\n   \n    print('\\nFold: {}\\n'.format(i+1))\n    rf = RandomForestRegressor()\n    rf.fit(X_train, y_train)\n\n    r2 = r2_score(y_val,rf.predict(X_val))\n    mae_ = mean_absolute_error(y_val,rf.predict(X_val))\n    r2_score_.append(r2)\n    mae.append(mae_)\n    preds_3.append(rf.predict(predictor_test_scale[predictor_test_scale.columns]))\n    \ny_pred_final_3 = np.mean(preds_3,axis=0)    \n\nprint('R2 - CV Score: {}'.format((sum(r2_score_)/4)),'\\n')\nprint('MAE Score: {}'.format((sum(mae)/4)),'\\n')\nprint(\"Score : \",r2_score_)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here cv score is much higher. I was not able to figure out why. If you know please tell in comments section.","metadata":{}},{"cell_type":"code","source":"sns.set_style('darkgrid')\nplt.figure(figsize=(15,5))\ntest_expanded['Weekly_Sales'] = y_pred_final_3\ntrain_expanded.groupby('Date')['Weekly_Sales'].mean().plot()\ntest_expanded.groupby('Date')['Weekly_Sales'].mean().plot(color='orange')\nplt.legend(['Actual', 'Predicted'], loc='best', fontsize=16)\nplt.ylabel('Sales', fontsize=16);","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Making final predictions","metadata":{}},{"cell_type":"code","source":"submission = pd.read_csv('../input/walmart-recruiting-store-sales-forecasting/sampleSubmission.csv.zip')\nsubmission['Weekly_Sales'] = y_pred_final_3\nsubmission.to_csv('results_rf_cv_tuned.csv',index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"pLEASE PROVIDE FEEDBACK SO THAT I CAN IMPROVE!!","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}