{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Libs","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport datetime\n\nfrom scipy import stats\nfrom scipy.stats import norm \nimport warnings \n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold, GroupKFold, GridSearchCV, train_test_split, TimeSeriesSplit, RepeatedStratifiedKFold\nfrom sklearn import metrics\nimport gc\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom bayes_opt import BayesianOptimization\n\nwarnings.filterwarnings('ignore')\n\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, mean_squared_log_error\nfrom sklearn.metrics.scorer import make_scorer\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.model_selection import KFold, cross_val_score, GridSearchCV\n\nimport xgboost as xgb\nfrom lightgbm import LGBMRegressor\nimport gc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load datasets","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/input/walmart-recruiting-store-sales-forecasting/train.csv.zip', parse_dates=['Date'])\ndf_test = pd.read_csv('/kaggle/input/walmart-recruiting-store-sales-forecasting/test.csv.zip', parse_dates=['Date'])\n\ndf_stores = pd.read_csv('/kaggle/input/walmart-recruiting-store-sales-forecasting/stores.csv')\ndf_features = pd.read_csv('/kaggle/input/walmart-recruiting-store-sales-forecasting/features.csv.zip', parse_dates=['Date'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Looking at data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_stores.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_features.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Joining the datasets","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#train\ndf_train_join = df_train.merge(df_stores, on='Store', how='left')\ndf_train_join = df_train_join.merge(df_features, on=['Store', 'Date', 'IsHoliday'], how='left')\n\n#test\ndf_test_join = df_test.merge(df_stores, on='Store', how='left') \ndf_test_join = df_test_join.merge(df_features, on=['Store', 'Date', 'IsHoliday'], how='left')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Check","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_join.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test_join.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## First look on our dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_join.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_join.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's transform some of out features into categorical values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_join.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Negative values on weekly sales needs to be corrected or neglected. Let's see if this erros occurs on many rows.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"len(df_train_join[df_train_join['Weekly_Sales'] < 0])/len(df_train_join)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This erros occurs on less than 1% of the dataset. Phew.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Checking missing values:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_join.isnull().sum(axis = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test_join.isnull().sum(axis = 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Train set:\n- We only have null values on MarkDown columns. Those columns represents the Type of markdown and what quantity was available during that week.\n\nTest set:\n- Null values on CPI and Unemployment rate","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"set(df_test_join[~df_test_join.Unemployment.notnull()]['Date'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"set(df_test_join[~df_test_join.CPI.notnull()]['Date'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All missing values on test set are from 2013.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_join[df_train_join.isnull().any(axis=1)]['Date']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Missing values on years 2010, 2011 and 2012 on out train set.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_join.skew()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Markdown columns are skewed features.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Data Exploration","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's see our sales distribution per day:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.kdeplot(df_train_join['Weekly_Sales'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Skewness: {} \\nKurtosis: {}\".format(df_train_join['Weekly_Sales'].skew().round(), df_train_join['Weekly_Sales'].kurt().round()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hightly skewed, big tail (high kurtosis). Lets create more datetime columns:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#train\ndf_train_join['Year'] = df_train_join['Date'].dt.year\ndf_train_join['Month'] = df_train_join['Date'].dt.month\ndf_train_join['Week'] = df_train_join['Date'].dt.week\ndf_train_join['Day'] = df_train_join['Date'].dt.day\n\n#test\ndf_test_join['Year'] = df_test_join['Date'].dt.year\ndf_test_join['Month'] = df_test_join['Date'].dt.month\ndf_test_join['Week'] = df_test_join['Date'].dt.week\ndf_test_join['Day'] = df_test_join['Date'].dt.day","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pylab import rcParams\nrcParams['figure.figsize'] = 8, 8","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For now, let's plot just one of them to see if there's an impact of sales.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x=\"Month\", y=\"Weekly_Sales\", data=df_train_join, showfliers=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see some seasonality here.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## What about hollidays?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x=\"IsHoliday\", y=\"Weekly_Sales\", data=df_train_join, showfliers=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_join[df_train_join['IsHoliday']==True]['Week'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- We have some indications of more sales on holidays, but not much.\n- We see hollidays on weeks 6 (superbowl), 36 (labor day), 47 (thanksgiving) and 52 (christmas)\n- It is missing Easter!","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Sales per year","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.lineplot(x=\"Week\", y=\"Weekly_Sales\", data=df_train_join, hue='Year')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Peak of yearly sales on the last weeks/holliday related\n- Sales on year before seems like a strong predictor (may become a feature)\n- Easter looks like an important holliday on 1st semester, we should include it. Our first weak model could be simply the weekly sales iin the year before +- some threshold, quick win :)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Sales per store","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(y=\"Weekly_Sales\", x=\"Store\", data=df_train_join, showfliers=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Each store has its own sales pattern.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Sales per department type","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x=\"Dept\", y=\"Weekly_Sales\", data=df_train_join, showfliers=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Big difference between department types. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Sales per store type","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(y=\"Type\", x=\"Weekly_Sales\", data=df_train_join, showfliers=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Visually, we get a hint that there are some stores selling more than others, and store type is algo important: Store type C tends to sell less than store type B or A.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## What if we combine store type and department?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10, 50))\nsns.boxplot(y='Dept', x='Weekly_Sales', data=df_train_join, showfliers=False, hue=\"Type\",orient=\"h\") ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can wee clear distinction of weekly sales just by oversing those two variables combined.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Does store size matter?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x='Size', y='Weekly_Sales', data=df_train_join, showfliers=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Yes it does. The smaller the store, the smaller the sales.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Store type distribution","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_join[['Store','Type']].groupby(['Type']).nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The distribution of store types is not equal. We have more store types that tends to have more weekly sales.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Sales per unenployment rate","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.scatterplot(x=\"Weekly_Sales\", y=\"Unemployment\", data=df_train_join)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can't see much here. One could argue that higher unemployment rates would affect weekly sales, but we cannot see that clearly on the plot. Most of weekly sales comes from average unemployment rate locations, but that only means that most of the locations have average unenployment rates.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Correlations","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We already looked into some features, but let's see how they correlate to each other:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = df_train_join.corr()\nplt.figure(figsize=(15, 10))\nsns.heatmap(corr, annot=True, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can have some new takeaways and confirm what we already saw on the previous plots:\n- Temperature no correlation with weekly sales, but strong correlation with variable Month (as expected, of course)\n- The bigger the store, more weekly sales\n- Fuel price does not seem to have correlation with weekly sales\n- CPI and unemployment rate (with missing values on test set) have low correlation rate with weekly sales","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Data preprocessing","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's use all out previous conclusions to move forward.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Handling missing values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#MarkDown1 to Markdown5: null values = 0\ndf_train_join[['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']] = df_train_join[['MarkDown1', 'MarkDown2', 'MarkDown3', \n                                                                                                  'MarkDown4', 'MarkDown5']].fillna(value=0)\n\ndf_test_join[['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']] = df_test_join[['MarkDown1', 'MarkDown2', 'MarkDown3', \n                                                                                                  'MarkDown4', 'MarkDown5']].fillna(value=0)\n#MarkDown1 to Markdown5: if less than 0 = 0\ndf_train_join['MarkDown1'] = df_train_join['MarkDown1'].apply(lambda x: 0 if x < 0 else x)\ndf_train_join['MarkDown2'] = df_train_join['MarkDown2'].apply(lambda x: 0 if x < 0 else x)\ndf_train_join['MarkDown3'] = df_train_join['MarkDown3'].apply(lambda x: 0 if x < 0 else x)\ndf_train_join['MarkDown4'] = df_train_join['MarkDown4'].apply(lambda x: 0 if x < 0 else x)\ndf_train_join['MarkDown5'] = df_train_join['MarkDown5'].apply(lambda x: 0 if x < 0 else x)\n\ndf_test_join['MarkDown1'] = df_test_join['MarkDown1'].apply(lambda x: 0 if x < 0 else x)\ndf_test_join['MarkDown2'] = df_test_join['MarkDown2'].apply(lambda x: 0 if x < 0 else x)\ndf_test_join['MarkDown3'] = df_test_join['MarkDown3'].apply(lambda x: 0 if x < 0 else x)\ndf_test_join['MarkDown4'] = df_test_join['MarkDown4'].apply(lambda x: 0 if x < 0 else x)\ndf_test_join['MarkDown5'] = df_test_join['MarkDown5'].apply(lambda x: 0 if x < 0 else x)\n\n# Negative weekly sales on train data\ndf_train_join['Weekly_Sales'] = df_train_join['Weekly_Sales'].apply(lambda x: 0 if x < 0 else x)\n\n# CPI and Unemployment rate - drop (low correlation + missing values on test set)\n# We can - always - change our mind later if model performance is poor)\ndf_train_join.drop(['CPI', 'Unemployment'], axis=1, inplace=True)\ndf_test_join.drop(['CPI', 'Unemployment'], axis=1, inplace=True)\n\n# Fuel price and Temperature - Drop (low value on correlation plot)\ndf_train_join.drop(['Fuel_Price', 'Temperature'], axis=1, inplace=True)\ndf_test_join.drop(['Fuel_Price', 'Temperature'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop date - we transformed it into year/month/day/week variables\ndf_train_join.drop(['Date'], axis=1, inplace=True)\ndf_test_join.drop(['Date'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature engineering","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#log for skewed variables\nskewed = ['MarkDown1','MarkDown2','MarkDown3','MarkDown4','MarkDown5']\ndf_train_join[skewed] = df_train_join[skewed].apply(lambda x: np.log(x + 1))\ndf_test_join[skewed] = df_test_join[skewed].apply(lambda x: np.log(x + 1))\n\n#adding easter holiday - we saw on the EDA it was important!\ndf_train_join.loc[(df_train_join.Year==2010) & (df_train_join.Week==13), 'IsHoliday'] = True\ndf_train_join.loc[(df_train_join.Year==2011) & (df_train_join.Week==16), 'IsHoliday'] = True\ndf_train_join.loc[(df_train_join.Year==2012) & (df_train_join.Week==14), 'IsHoliday'] = True\n\ndf_test_join.loc[(df_test_join.Year==2013) & (df_test_join.Week==13), 'IsHoliday'] = True\n\n#create dummy variables - store type and holliday\ndf_train_join = pd.get_dummies(df_train_join, columns=['Type'])\ndf_test_join = pd.get_dummies(df_test_join, columns=['Type'])\n\n#trasform IsHoliday\ndf_train_join['IsHoliday'] = df_train_join['IsHoliday'].apply(lambda x: 1 if x==True else 0)\ndf_test_join['IsHoliday'] = df_test_join['IsHoliday'].apply(lambda x: 1 if x==True else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Also apply log to weekly sales\ndf_train_join['Weekly_Sales'] = df_train_join['Weekly_Sales'].apply(lambda x: np.log(x + 1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data split (train-test-validation)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X, val_X, train_y, val_y = train_test_split(df_train_join.drop('Weekly_Sales', axis = 1), \n                                                  df_train_join['Weekly_Sales'], \n                                                  test_size = 0.2, \n                                                  random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Chosen model: Ensemble with tree based models","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Why?\n- Robust to outliers\n- Proven performance on several competitions\n- Easy interpretability for tree based models (for non-technical stakeholders)\n- Can handle both categorical and numerical data :)\n\nWatch out for:\n- Overfitting!!!!","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Chosen metric: Weighted Mean Absolute Error (WMAE)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"It was the chosen metric for this competition.\nIt punishes mistakes based on some weight criteria, specially on hollidays.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def wmae(pred_y, test_y, weights):\n    return 1/sum(weights) * sum(weights * abs(test_y - pred_y))\n\ndef calculate_weights(holidays):\n    return holidays.apply(lambda x: 1 if x==0 else 5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train and predict base model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_predict(model, train_X, train_y, test_X, test_y, verbose=0): \n    \n    results = {}  \n    \n    model = model.fit(train_X, train_y)\n    predictions = model.predict(test_X)\n            \n    # WMAE on Test Set\n    results['WMAE'] = wmae(np.exp(test_y), \n                           np.exp(predictions), \n                           calculate_weights(test_X['IsHoliday']))\n    \n\n    importances = model.feature_importances_\n    std = np.std([model.feature_importances_ for model in model.estimators_],\n             axis=0)\n    indices = np.argsort(importances)[::-1]\n    #Print importances\n    for f in range(train_X.shape[1]):\n        print(\"%d. feature %s (%f)\" % (f + 1, train_X.columns[f], importances[indices[f]]))\n\n    # Success\n    print(\"Model Name:\", model.__class__.__name__)\n    print(\"WMAE:\", round(results['WMAE'],2))\n    \n    # Return the model & predictions\n    return (model, predictions, importances)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_rf = RandomForestRegressor(random_state=42, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m, pred_y, feature_importances = train_predict(model_rf, train_X, train_y, val_X, val_y, verbose=3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we can see which variables affects our model the most :)\nFirst feature seems highly correlated to the outcome, which is often **undesirable**.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Tuning using Grid Search CV","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"params = { \n    'n_estimators': [50, 80],\n    'max_features': [None, 'auto'],\n    'bootstrap': [True, False],\n    'max_depth':[None],\n    'random_state': [42], \n    'verbose': [1]\n}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If you have more time/computing power, you can test lots other parameters/values.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"CV = GridSearchCV(estimator=model_rf, param_grid=params, cv=3, verbose=1)\nCV.fit(train_X, train_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Best parameters: {}'.format(CV.best_params_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_model = model_rf.set_params(**CV.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m_final, pred_y_val_final, feature_importances_final = train_predict(final_model, train_X, train_y, val_X, val_y, verbose=3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Plotting results","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test_pred = m_final.predict(df_test_join)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test_join['Weekly_Sales'] = y_test_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.lineplot(x=\"Week\", y=\"Weekly_Sales\", data=df_test_join)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Important remarks","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Tunned model feature importances didn't change its magnitude.\nIf WMAE is good enough, the model that goes into production uses this parameters!\n<br>\n<br>\nWhat dictates if the model is good enough overall? **Business needs** (output speed + computing cost + some WMAE threshold)\n<br>\n<br>\nWhat if the error is still too big?\n- Retrain w/o higly correlated feature\n- Use more parameters on grid search or use Bayesian Optimization (+ computing cost)\n- New features e.g countdown for important hollidays (xmas, thanksgiving...)\n- Boosting models such as lightgbm, catboost, xboost (it decreases the explainability and needs more computing power, but a  lot more powerful)","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}