{"cells":[{"metadata":{},"cell_type":"markdown","source":"![](https://cdn.corporate.walmart.com/dims4/WMT/c2bbbe9/2147483647/strip/true/crop/2389x930+0+0/resize/1446x563!/quality/90/?url=https%3A%2F%2Fcdn.corporate.walmart.com%2Fd6%2Fe7%2F48e91bac4a8ca8f22985b3682370%2Fwalmart-logos-lockupwtag-horiz-blu-rgb.png)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"![](https://dox4euoyzny9u.cloudfront.net/images/blog/uploads/dataprocessinggdpr.jpg)"},{"metadata":{},"cell_type":"markdown","source":"**Organização do documento**\n\n1. Carregando os dados de entrada e carregando bibliotecas<br>\n   1.1 Manipulação com os dados Iniciais<br>\n2. Elaboração de 4 questões que ajudem a compreender os dados<br>\n   2.1 Questão 1: Os feriados impactam nas vendas?<br>\n   2.2 Questão 2: As vendas semanais são consistentes ao longo dos anos?<br>\n   2.3 Questão 3: Existe relevância na correlação das variáveis?<br>\n   2.4 Questão 4: <br>\n3. Listar 1 limpeza que precisa ser feita nos dados e a executar <br>\n4. Planejamento de aplicação de Machine Learning para resolver o problema <br>\n   4.1 Escolher um algoritmo de Machine Learning <br>\n   4.2 Comentar sobre a escolha do algoritmo <br>\n5. Autocrítica e próximos passos <br>\n   5.1 Enumerar eventuais problemas e limitações da estratégia <br>\n   5.2 Comentar sobre o que você teria feito se tivesse mais tempo para tratar o problema <br>\n   \n# Carregando os dados de entrada e bibliotecas"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport pandas as pd\nimport seaborn as sns\n\n\ninput_path  = '/kaggle/input/walmart-recruiting-store-sales-forecasting/'\ntrain_data  = pd.read_csv(os.path.join(input_path, 'train.csv.zip'))\nstores_data = pd.read_csv(os.path.join(input_path, 'stores.csv'))\nfeatures    = pd.read_csv(os.path.join(input_path, 'features.csv.zip'))\ntest_data   = pd.read_csv(os.path.join(input_path, 'test.csv.zip'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stores_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Mesclando os dados de entrada\n\nObserve que `train_data` e `features` contém variáveis que se complementam. Logo, faz sentido unificar esses dados em uma só variável que permita melhor manipulação e análise dos dados."},{"metadata":{"trusted":true},"cell_type":"code","source":"intersection_columns = ['Store', 'Date', 'IsHoliday']\ndata = pd.merge(train_data,\n                features,\n                how='left',\n                left_on=intersection_columns,\n                right_on=intersection_columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"intersection_columns = ['Store']\ndata = pd.merge(data,\n                stores_data,\n                how='left',\n                left_on=intersection_columns,\n                right_on=intersection_columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Quando fazemos o merge de dois dataframes, é possível que o dataframe resultante tenha um tamanho menor que o dataframe de referência, caso não haja correspondência entre as colunas chaves usadas no merge. Inclusive, caso não seja possível identificar interseção entre nenhuma das linhas considerando as chaves de ambos os dataframes, é possível que o dataframe de saída tenha tamanho zero. Ou seja, é importante verificar o tamanho do dataframe resultante para saber se alguma linha foi perdida no merge por falta de matching entre as chaves."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(train_data))\nprint(len(features))\nprint(len(data))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Como podemos observar acima, o dataframe resultante tem o mesmo número de instâncias que o dataframe de referência (`train_data`). Pois, nesse caso, as features de uma mesma observação são compartilhadas por mais de uma instância e por isso, o dataframe `features` tem um tamanho menor. O importante a observar aqui é que não houve desperdício de dados.\n\n# Elaborar 4 questões que ajudem a compreender os dados\n\n![](https://codemyviews-blog-post-images.s3.amazonaws.com/uploads/machine-learning.png)\n\n## Questão 1: Os feriados impactam nas vendas?\n\nAo manipular os dados de vendas de uma grande varejista pela primeira vez, a principal dúvida que surge é: feriados realmente impactam nas vendas?\nUma informação importante é tentar observar se o nível de oscilação nas vendas semanais podem indicar que uma grande número de vendar irá acontecer na semana do feriado.\nPortanto, usei um gráfico que mostra a distribuição de vendas para cada semana do ano nos três anos de vendas fornecidos. \nAlém disso, usei uma linha em destaque para enfatizar a semana que acontece o feriado, o que gerou algumas conclusões discutidas a seguir.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nPrimeiro passo:\n- Preciso converter os dados temporais (timestamp)\nem identificadores que permita maior facilidade na \nmanipulação dos dados.\n'''\ndata['week'] = pd.to_datetime(data.Date).dt.week\ndata['year'] = pd.to_datetime(data.Date).dt.year","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nSegundo passo:\n- Plotar o volume de vendas semanais indicando a semana\nque ocorre o feriado com uma linha azul\n'''\nplt.figure(figsize=(20,8))\nfor year in data.year.unique():\n    var = data[data.year==year]['Weekly_Sales'].groupby(data['week']).sum()\n    sns.lineplot(var.index, var.values)\nplt.grid()\nplt.xticks(np.arange(1, 53, step=1))\nplt.legend(data.year.unique(), loc='best', fontsize=18)\nplt.title('Vendas semanais por ano', fontsize=24)\nplt.ylabel('Vendas', fontsize=21)\nplt.xlabel('Semana', fontsize=21)\nfor value in data[data.IsHoliday].week.unique():\n    plt.axvline(value, color='blue')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Com base no gráfico acima podemos fazer as seguintes observações:\n* Super bowl: é um feriado com pouco impacto nas vendas. Se você observar os meses circunvizinhos, a semana que ocorre o Super Bowl pouco se destoa das demais semanas e talvez seja um feriado que ser melhor explorado pela gestão do varejo para expandir as vendas nesse feriado.\n* Labor day: também é um feriado com pouco destaque. Ao longo dos três anos poucas vendas a mais são feitas nessa data. Provavelmente, esse feriado é usado para renovação de estoque e reabastecimento dos estoques dos novos produtos que serão vendidos no Natal.\n* Thanksgiving: Primeiro feriado do ano com grande impacto nas vendas. Percebe-se um grande volume de vendas nessa semana, embora que, esse volume de vendas tenha pouco variado ao longo dos anos.\n* Christmas: Poucas semanas após o dia de ação de graças ocorre o Natal. Ao observar as vendas realizadas no Natal, percebe-se um baixo volume nas vendas. Entretanto, a semana que antecede o Natal apresenta um pico de vendas que supera todas as demais semanas do ano, para todos os anos fornecidos. Assim, faz-se necessário que o varejo se prepare antecipadamente para uma semana de vendas aquecida do Natal.\n\n\nCom base nessas análises, podemos perceber que os dois primeiros feriados (Super bowl e Labor day) tem um nível de vendas constante quando comparado as semanas de seu entorno. Por outro lado, os feriados de Thanksgiving e Christmas fazem as pessoas consumir muito mais que as demais semanas. Respondendo a questão: Os feriados realmente impactam as vendas? Depende. Sim, ao considerar os feriados de Thanksgiving e Christmas. Não, ao considerar os feriados de Super bowl e Labor day."},{"metadata":{},"cell_type":"markdown","source":"## Questão 2: As vendas semanais são consistentes ao longo dos anos?\n\nÉ comum que grandes varejistas tenham semanas com números excepcionais de vendas. Por exemplo, a Alibaba informou que no 'Dia dos solteiros', a movimentação de vendas na China foi de R$ 300 milhões [(fonte)](https://g1.globo.com/economia/noticia/2020/11/11/dia-dos-solteiros-movimenta-cerca-de-r-300-bilhoes-em-vendas-na-china.ghtml). Em relação ao problema de negócio, a varejista precisa se preparar antecipadamente para semanas com pico de vendas, buscando evitar atrasos nas entregas, minimizar a falta de itens no estoque, expandir a disponibilidade dos servidores para evitar congestionamento no site, entre outras logísticas que permitam melhorar a experiência do cliente e maximizar o lucro da empresa. Diante desse cenário, surge um importante problema técnico, que seria prever as vendas semanais com base em dados de vendas de anos anteriores. Para responder essa pergunta, usei uma matriz de correlação que analisa as vendas semanais em todos os anos registrados."},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nPrimeiro passo:\nEstruturando o dado ao formato necessário para visualização.\n'''\ncorr = pd.DataFrame([])\nfor year in data.year.unique():\n    corr = pd.concat([corr, data[data.year==year]['Weekly_Sales'].groupby(data['week']).sum()], axis=1)\ncolumns_name = list()\nfor year in data.year.unique():\n    columns_name += [f'data_{year}']\ncorr.columns = columns_name","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nCalculando o coeficiente de correlação das vendas semanais agrupando-as por ano.\n'''\ncorr = corr.corr()\n\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\n\nf, ax = plt.subplots(figsize=(10, 5))\n\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\nplt.title('Matriz de Correlação', fontsize=18)\n\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observando a matriz de correlação acima, percebemos que há uma forte correlação entre as vendas semanais para os três anos fornecidos.\n\nO que isso quer dizer? Com base no volume de vendas realizados nos anos anteriores, existem fortes indícios que as oscilações no volume de vendas se mantenham. Isso quer dizer que o volume de vendas crescem (ou diminuem) de maneira consistente ao longo dos anos. E que essas oscilações podem variar pouco de um ano para outro e muito no longo prazo.\n\nObserve a correlação de 0.95% entre 2010 e 2011, valor extretamente alto. Outra correlação muito forte de 0.76 entre os anos de 2011 e 2012. Entretanto, anos mais distantes como 2010 e 2012 apresentam um nível de correlação embora alto mas um pouco inferior que os outros dois casos (0.58), isso pode indicar que o nível de vendas semanais pode se parecer muito com o ano anterior, mas que essa sútil mudança ao longo dos anos pode fazer com que as vendas desse ano sejam muito diferentes das vendas de 5 anos atrás.\n\nAssim, o cientista de dados que analisa os dados dessa empresa, deve ter cuidado ao modelar o problema usando modelos com algoritmos como LSTM que usam muitos dados históricos, pois os mesmos, podem usar como referência um volume de vendas que não reflete a realidade do ano atual."},{"metadata":{},"cell_type":"markdown","source":"## Questão 3: Existe relevância na correlação das variáveis?\n\nAo lapidar os dados em busca de extrair insights relevantes para a tomada de decisão, um aspecto importante a se observar é se existem aspectos ou fenômenos que expliquem a correlação das variáveis coletadas. Com isso, novas ideias podem ser implementadas ao modelo e aspectos não observados podem ser notados. Assim, busquei fazer uma matriz de correlação para todas as variáveis que julguei fazer sentido observar a correlação.\n\n*Disclaimer*: Correlação é uma medida que indica relação entre duas variáveis, que não necessariamente indicam relação de causalidade entre elas. Por exemplo, suponhamos que existe uma alta correlação entre o número de banheiros de uma casa e a renda bruta da família que habita nessa casa. Isso quer dizer que pessoas que ganham mais, tendem a morar em casas com mais banheiros. Ou seja, há correlação positiva entre essas duas variáveis. No entanto, se você construir mais banheiros em sua casa, você não irá aumentar a sua renda bruta. Pois essa não há relação de causalidade entre essas duas variáveis."},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nCalculando o coeficiente de correlação entre as variáveis.\n\nPara responder a questão acima, foi desconsiderado algumas variáveis:\n\n- Store e Dept: o identificador da loja e do departamento são variáveis categóricas armazenadas como números,\ne para analisar o coeficiente de correlação entre duas variáveis você precisa analisar variáveis númericas.\n- MarkDown: Existem alguns aspectos que a análise dessa variável nesse quesito podem levar a conclusões imprecisas.\nAs minhas duas maiores preocupações são:\nI) Os dados estão anonimizados e isso prejudica na interpretabilidade da análise. Como explicar a correlação\nse você não entende porque ela acontece já que os dados são anônimos?\nII) Os dados estão disponíveis para parte da base, ou seja, você pode tecer análises que só são válidas para\numa parte dos dados. Extender essas conclusões para o resto da base pode levar a conclusões erradas.\n\nPor tal razão, resolvi desconsiderar essas variáveis na análise.\n'''\n\ncorr = data.drop(['Store', 'Dept', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5'], axis=1).corr()\n\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\n\nf, ax = plt.subplots(figsize=(20, 15))\n\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\nplt.title('Matriz de Correlação', fontsize=18)\n\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ao analisar uma matriz de correlação, procuramos entender quais aspectos podem explicar correlações fortes (sejam elas positivas ou negativas). Destaquei algumas correlações e busquei tecer a minha análise sobre elas. A variável mais importante a se observar são as vendas semanais. Entretanto, essa variável só possui correlação relevante com uma outra variável que é o tamanho da loja. Seria muito interessante ver outras variáveis correlacionadas ao volume de vendas semanal. Mas, busquei aprofundar em cada detalhe da análise.\n\n**CORRELAÇÕES POSITIVAS**\n* 0.78 Preço do combustível vs Ano: Esse alto índice de correlação pode ser explicado pelo aumento no preço dos combustíveis ao longo dos anos. Entretanto, observando o preço do combustível e as vendas semanais, percebemos que mesmo o preço do combustível tenha subido ao longo dos anos, as vendas semanais não foi impactada por esse aspecto (correlação muito próxima de zero).\n* 0.24 Tamanho da loja vs Vendas Semanais: Um aspecto bastante relevante no volume de vendas semanais deveria ser o tamanho da loja. Quanto maior a loja, maior o volume de vendas. Entretanto, esse indicador nos mostra que essa regra não necessariamente é válido. Isto é, se quanto maior a loja, maior fosse o volume de vendas, e essa regra fosse sempre obedecida, nós observaríamos um coeficiente de correlação entre essas duas variáveis igual a 1.0. No entanto, esse valor (0.24) indica uma correlação moderada, e que maiores lojas tendem a vender mais em boa parte dos casos.\n* 0.24 Temperatura vs Semana: Ao passar das semanas a temperatura também aumenta pois o verão americano acontece aproximadamente no meio do ano e temperatura volta a cair no segundo semestre. Sendo assim, essa correlação apresenta pouca relevância para a análise.\n* 0.18 CPI e Temperatura + 0.14 Preço do Combustível e Temperatura: Considerando o aquecimento global um fenômeno verdadeiro, ou seja, que ao longo dos anos a temperatura irá sempre subir, e que, índices de preços como o CPI também tendem a ser sempre positivos pois a inflação tendem a aumentar esse valor, e consequentemente também elevar o preço do combustível. Vemos uma leve correlação entre esses aspectos. O índice de preços ao consumir sobe com uma correlação de 0.18 em relação a temperatura, como também, a temperatura sobe com uma correlação de 0.14 com o preço do combustível.\n\n**CORRELAÇÕES NEGATIVAS**\n* -0.3 CPI e desemprego: Essa medida indica que o índice de preços ao consumidor é menor quando o desemprego está mais alto. *Disclaimer*: Embora ambas as variáveis não apresentem correlação significativa entre si e o volume de vendas semanais, esse tipo de observação pode parecer fazer pouco sentido para nós, mas variáveis descorrelacionadas ajudam a máquina a fazer melhores predições. Imagine uma base de dados com redundância nas variáveis, por exemplo, a área de uma casa medida em centímetros quadrados e metros quadrados. A correlação entre essas duas variáveis seria de 100%, por outro lado, fornecer ambas as variáveis para o modelo traria pouco ganho na capacidade preditiva. Ou seja, por mais que essas correlações façam pouco sentido para nós, podem ser bastante interessantes para o modelo.\n* -0.24 Desemprego e Ano: Essa correlação indica que o desemprego teve leve queda ao longo dos anos. Por outro lado, essa relação não impactou no volume de vendas. Entretanto, foi disponibilizado o histórico de apenas 3 anos. É possível que as pessoas que conseguiram emprego busquem quitar as suas dívidas e posteriormente fazer mais compras. Com um histórico maior, tendo acesso aos anos seguintes, eu buscaria observar se houve aumento no volume de vendas que pudesse estar associado a taxa de desemprego. É difícil de isolar apenas esse aspecto na análise, mas um modelo que usa dados históricos poderá se beneficiar com essa relação.\n* -0.16 CPI e Preço de combustível: Percebe-se que o preço do combustível subiu ao longo dos anos (correlação de 0.78 entre o preço de combustível e o ano), e que o índice de preços ao consumidor \"andou de lado\" ao longo do período (correlação de 0.075 do CPI e ano). No entanto, o preço do combustível têm correlação ligeiramente negativa em relação ao CPI. Ou seja, o índice de preços ao consumidor deve estar associado a outras variáveis que andam em sentidos levemente opostos ao preço do combustível.\n\n"},{"metadata":{},"cell_type":"markdown","source":"## Questão 4: A categoria do estabelecimento influência nas vendas?\n\nNos dados observamos três categorias de estabelecimentos (A, B e C). Essas categorias de estabelecimentos podem identificar diferentes aspectos, por exemplo, o público-alvo do estabelecimento, ou a qualidade da estrutura física da loja, ou a lucratividade do empreedimento. De toda maneira, o uso apropriado dessas informações podem trazer ganhos, tanto para o negócio quanto para o cientista de dados. Por exemplo, ao abrir uma nova loja, a empresa financia diversos planos de negócios que buscam maximizar os ganhos da companhia, entre elas, essa métrica pode ajudar a definir esse melhor local. Por outro lado, o cientista de dados ao prever o nível de vendas, poderia usar essa informação ao seu favor. Será que a categoria do estabelecimento tem alguma relação com as vendas?"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,8))\nfor type_ in data.Type.unique():\n    var = data[data.Type==type_]['Weekly_Sales'].groupby(data['week']).median()\n    sns.lineplot(var.index, var.values)\nplt.grid()\nplt.xticks(np.arange(1, 53, step=1))\nplt.legend(data.Type.unique(), loc='best', fontsize=18)\nplt.title('Vendas semanais por tipo de estabelecimento', fontsize=24)\nplt.ylabel('Vendas', fontsize=21)\nplt.xlabel('Semana', fontsize=21)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observando o gráfico acima, vemos claramente que o volume de vendas (usando a mediana) por estabelecimento de cada categoria têm sim grande impacto nas vendas. E respondendo as questões levantadas no primeiro trecho desse tópico, essas categorias podem estar associadas ao volume de vendas de cada estabelecimento. Sendo os estabelecimentos mais lucrativos aqueles que pertecem a categoria A, e os menos lucrativos aqueles que pertecem a categoria C."},{"metadata":{},"cell_type":"markdown","source":"# Listar 1 limpeza que precisa ser feita nos dados e a executar\n\nUma boa limpeza de dados pode começar a ser feita tratando os dados faltantes. Diversas abordagens diferentes podem ser aplicadas, como por exemplo, imputar novos dados usando a média, ou a predição de algum modelo como uma regressão linear ou logística. No entando, precisamos usar alguma ferramenta adequada para essa análise. Vou começar o processo usando uma biblioteca chamada `missingno` [link](https://github.com/ResidentMario/missingno) que pode ser bastante útil nesses casos."},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install quilt\nimport missingno as msno\nmsno.matrix(data.sort_values(by='year'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Com o auxílio dessa ferramenta, podemos visualizar os dados faltantes por células brancas nas respectivas colunas de cada variável. Com isso, percebemos que as variávels `MarkDown` começaram a ser registradas em 2011 ainda foi de maneira muito pequena (perceba o excesso de células em branco indicando a esparsidade dos dados). Mesmo após isso, em meados de 2012, ainda existe muitas células em branco para as variáveis `MarkDown2`, `MarkDown3` e `MarkDown4`. Nesse caso, pode-se usar algum modelo de predição para preencher os dados faltantes, como por exemplo, a fatoração de matrizes para os dados faltantes. No entanto, não há representantes para 2010 e poucos casos para 2011, assim, o preenchimento automático desses valores poderá enviesar o modelo que treinará com esses dados para fazer predições erradas. Sendo assim, a minha recomendação seria descartar esses atributos.\n"},{"metadata":{},"cell_type":"markdown","source":"![](https://www.kaggle.com/static/images/host-home/host-home-research.png)\n\n# Planejamento de aplicação de Machine Learning para resolver o problema\n#### Escolher um algoritmo de Machine Learning \n#### Comentar sobre a escolha do algoritmo \n","attachments":{}},{"metadata":{},"cell_type":"markdown","source":"Esse problema se caracteriza pela predição no volume de vendas considerando uma linha temporal.\nSendo assim, pensamos que usar Redes Neurais Recorrentes (RNN/LSTM) possa ser o mais indicado. \nEntretanto convido para uma análise/reflexão mais profunda sobre essa abordagem.\n\nA solução proposta pode envolver duas maneiras de modelar os dados: \n    I) Considerar o volume de vendas realizadas nas últimas semanas, ou\n    II) Considerar o volume de vendas semanais realizadas nas mesmas semanas que a predita nos últimos anos.\nDetalhando um pouco mais:\n\nA solução (I) indica que o volume de vendas nas últimas `n` semanas pode não ser útil para a modelo pois as semanas de picos são antecedidas por semanas de vendas comuns, e maior parte das semanas tem um volume de vendas praticamente constante. Nesse caso, o modelo poderia ter dificuldade em prever o volume de vendas nas semanas finais do ano pois poderiam ser consideradas *outliers*. Um problema clássico entre especialização ou generalização de modelos, um *trade-off* onde você melhora a acurácia ao custo de diminuir a capacidade de generalização e vice-versa. \n\nA solução (II) pode parecer mais promissora por alguns aspectos, isto é, olhando o volume de vendas realizada em uma mesma semana nos diferentes anos, você percebe que esse volume praticamente não muda. Ou seja, caso você queira prever o volume de vendas na semana 23, em vez de você olhar para as `n` semanas anteriores a semana 23. Você deveria observar o volume de vendas da semana 23 nos anos anteriores. No entanto, ao usar essa abordagem você esbarra com um problema, pois os dados são limitados e você não terá muitos exemplos para treinar o modelo, consequentemente, ele poderá não apresentar um bom desempenho.\n\nNesse sentido, penso que tratar problemas envolvendo séries temporais, o uso de redes neurais recorrentes (do inglês, RNN) pode fazer sentido. No entanto, o pequeno volume de dados me faz pensar que alguns problemas poderão acontecer. Por exemplo, o modelo ter dificuldades para convergir, e isso impactar em seu desempenho. Nesse caso, eu acredito que diante o cenário encontrado, a solução mais promissora possa ser usando árvores de decisão. Diante a minha experiência, usaria alguma implementação como [XGBoost](https://xgboost.readthedocs.io/en/latest/) ou o [CatBoost](https://catboost.ai/) para solucionar esse problema. Sendo este último algoritmo, bastante promissor por apresentar uma funcionalidade específica para interpretação de modelos que é o uso do [SHAP](https://github.com/slundberg/shap), o que permite identificar quais variáveis estão constribuindo para a previsão do modelo. Esse tipo de técnica (SHAP) permite trazer interpretabilidade para qualquer tipo de modelo *machine-learning*, até mesmo os que são considerados *black-box*. E o CatBoost é um modelo que traz isso implementado nativamente. \n\n\n"},{"metadata":{},"cell_type":"markdown","source":"![](https://www.kaggle.com/static/images/about/inclass/howitworks@2x.png)\n\n# Autocrítica e próximos passos \n#### Enumerar eventuais problemas e limitações da estratégia \n#### Comentar sobre o que você teria feito se tivesse mais tempo para tratar o problema "},{"metadata":{},"cell_type":"markdown","source":"Eu acredito que algumas soluções mais criativas e rebuscadas podem apresentar desempenho superior ao que eu propus.\n\nEntretanto, acredito no princípio de Pareto, em que você realiza 80% de uma tarefa em 20% do tempo.\n\nSendo assim, eu testaria a abordagem proposta e veria o resultado do modelo, após isso, estudaria o custo-benefício em aplicar mais esforços na abordagem proposta para melhorar o desempenho, por exemplo, usando alguma técnica como *oversampling* (aumento no volume de dados de maneira sintética), *feature generation* (geração de novas variáveis) ou *fine-tunning* (modificação nos hiper-parâmetros do modelo para aumentar o desempenho do modelo). Certamente, se eu tivesse mais tempo, buscaria investir esforços nas técnicas citadas aqui, pois não consigo pensar em algoritmos de machine-learning que pudessem apresentar soluções melhores. \n\n\n\n\n"},{"metadata":{},"cell_type":"markdown","source":"![](https://www.vogue.pt/media/content/balanco-me-too-times-up-assedio-sexual.jpg)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}