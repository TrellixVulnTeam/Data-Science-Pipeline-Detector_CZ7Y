{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Description given by Kaggle\n\nYou are provided with historical sales data for 45 Walmart stores located in different regions. Each store contains a number of departments, and you are tasked with predicting the department-wide sales for each store.\n\nIn addition, Walmart runs several promotional markdown events throughout the year. These markdowns precede prominent holidays, the four largest of which are the Super Bowl, Labor Day, Thanksgiving, and Christmas. The weeks including these holidays are weighted five times higher in the evaluation than non-holiday weeks. Part of the challenge presented by this competition is modeling the effects of markdowns on these holiday weeks in the absence of complete/ideal historical data.\n\nstores.csv\n\nThis file contains anonymized information about the 45 stores, indicating the type and size of store.\n\ntrain.csv\n\nThis is the historical training data, which covers to 2010-02-05 to 2012-11-01. Within this file you will find the following fields:\n\nStore - the store number\nDept - the department number\nDate - the week\nWeekly_Sales -  sales for the given department in the given store\nIsHoliday - whether the week is a special holiday week\ntest.csv\n\nThis file is identical to train.csv, except we have withheld the weekly sales. You must predict the sales for each triplet of store, department, and date in this file.\n\nfeatures.csv\n\nThis file contains additional data related to the store, department, and regional activity for the given dates. It contains the following fields:\n\nStore - the store number\nDate - the week\nTemperature - average temperature in the region\nFuel_Price - cost of fuel in the region\nMarkDown1-5 - anonymized data related to promotional markdowns that Walmart is running. MarkDown data is only available after Nov 2011, and is not available for all stores all the time. Any missing value is marked with an NA.\nCPI - the consumer price index\nUnemployment - the unemployment rate\nIsHoliday - whether the week is a special holiday week\nFor convenience, the four holidays fall within the following weeks in the dataset (not all holidays are in the data):\n\nSuper Bowl: 12-Feb-10, 11-Feb-11, 10-Feb-12, 8-Feb-13\n\nLabor Day: 10-Sep-10, 9-Sep-11, 7-Sep-12, 6-Sep-13\n\nThanksgiving: 26-Nov-10, 25-Nov-11, 23-Nov-12, 29-Nov-13\n\nChristmas: 31-Dec-10, 30-Dec-11, 28-Dec-12, 27-Dec-13","metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f525dfcc-bbd0-4c70-84e9-59e8ddb6ce24"}}},{"cell_type":"markdown","source":"# First notes on strategy\n\nI will take this task as I would do for first steps of a POC. Most of the work done here concerns first data exploration and insights for first modelling. Of course, to be able to handle this appropriately for a first version model, I might need more domain knowledge, in order to understand the features given in features.csv (mainly the so called  markdowns) and to study further intrinsic behaviour of specific Store-Depts. \n\nFor these Weekly aggregated data (like explained in the description), it seems to me reasonable to use Pandas. I will not try to build my own Python package, but use available packages on PyPI, such as dataprep (for basic edas: missing, correlations) and pycaret to load different models and data transformations. I will work with popular models, like XGBoost, Catboost, LightGBM and Linear models from Sklearn. \n\nSince it seems like useful to build several features for the model, I will probably not try Arima, statsmodels, Facebook Prophet at this time, since I would need to take into account many assumptions to work with several different Stores and Depts (total of 3331 store-depts). \n\nLSTM, from Keras for instance, will not be studied here either.  I think it would be interesting to give it a try with a multivariable approach, though. Specially if the time series follow seasonal trends.","metadata":{}},{"cell_type":"code","source":"!pip install pycaret==2.3.1 dataprep","metadata":{"execution":{"iopub.status.busy":"2021-06-15T14:37:10.139097Z","iopub.execute_input":"2021-06-15T14:37:10.139462Z","iopub.status.idle":"2021-06-15T14:37:18.731764Z","shell.execute_reply.started":"2021-06-15T14:37:10.139433Z","shell.execute_reply":"2021-06-15T14:37:18.730623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loading main libraries","metadata":{}},{"cell_type":"code","source":"from pycaret import regression as pyreg\nimport numpy as np\nfrom sklearn.metrics import make_scorer\nimport logging\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport time\nimport random\nfrom dataprep import eda","metadata":{"execution":{"iopub.status.busy":"2021-06-15T13:59:15.404478Z","iopub.execute_input":"2021-06-15T13:59:15.404888Z","iopub.status.idle":"2021-06-15T13:59:20.033117Z","shell.execute_reply.started":"2021-06-15T13:59:15.40485Z","shell.execute_reply":"2021-06-15T13:59:20.032058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Checking DATA","metadata":{}},{"cell_type":"code","source":"\nimport gc\n\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"65c0222d-e4dd-4670-94c3-c850b9fbc2cc"},"execution":{"iopub.status.busy":"2021-06-15T13:59:20.034969Z","iopub.execute_input":"2021-06-15T13:59:20.035295Z","iopub.status.idle":"2021-06-15T13:59:20.064551Z","shell.execute_reply.started":"2021-06-15T13:59:20.035262Z","shell.execute_reply":"2021-06-15T13:59:20.063438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def del_df(df):\n    try:\n        del(df)\n    except:\n        pass\n    gc.collect()\n\ndf_features = pd.read_csv(\"/kaggle/input/walmart-recruiting-store-sales-forecasting/features.csv.zip\")\ndf_train = pd.read_csv(\"/kaggle/input/walmart-recruiting-store-sales-forecasting/train.csv.zip\")\ndf_test = pd.read_csv(\"/kaggle/input/walmart-recruiting-store-sales-forecasting/test.csv.zip\")\ndf_sample = pd.read_csv(\"/kaggle/input/walmart-recruiting-store-sales-forecasting/sampleSubmission.csv.zip\")\ndf_stores = pd.read_csv(\"/kaggle/input/walmart-recruiting-store-sales-forecasting/stores.csv\")","metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9951ded4-8f93-429c-8c2d-2ac53bdf724c"},"execution":{"iopub.status.busy":"2021-06-15T13:59:20.066455Z","iopub.execute_input":"2021-06-15T13:59:20.06677Z","iopub.status.idle":"2021-06-15T13:59:20.674526Z","shell.execute_reply.started":"2021-06-15T13:59:20.066739Z","shell.execute_reply":"2021-06-15T13:59:20.673468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"622dc83c-97d5-44b4-8c72-d9e95cb1da3b"},"execution":{"iopub.status.busy":"2021-06-15T13:59:20.675881Z","iopub.execute_input":"2021-06-15T13:59:20.676159Z","iopub.status.idle":"2021-06-15T13:59:20.857436Z","shell.execute_reply.started":"2021-06-15T13:59:20.676133Z","shell.execute_reply":"2021-06-15T13:59:20.856378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Basic info and sample of dataframes\n\nTaking into account the size(size up to tens of MB) and number of lines (up to ~400k lines), it seems reasonable to use Pandas transformations for this test. ","metadata":{}},{"cell_type":"code","source":"df_stores.info()\ndf_features.info()\ndf_train.info()\ndf_test.info()\ndf_sample.info()","metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c8e6d500-1540-499e-9654-c34b6f594aee"},"execution":{"iopub.status.busy":"2021-06-15T13:59:20.858723Z","iopub.execute_input":"2021-06-15T13:59:20.859031Z","iopub.status.idle":"2021-06-15T13:59:20.998051Z","shell.execute_reply.started":"2021-06-15T13:59:20.858999Z","shell.execute_reply":"2021-06-15T13:59:20.996888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-15T14:00:27.070655Z","iopub.execute_input":"2021-06-15T14:00:27.071098Z","iopub.status.idle":"2021-06-15T14:00:27.094331Z","shell.execute_reply.started":"2021-06-15T14:00:27.071056Z","shell.execute_reply":"2021-06-15T14:00:27.092924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.nunique()","metadata":{"execution":{"iopub.status.busy":"2021-06-15T14:01:27.396638Z","iopub.execute_input":"2021-06-15T14:01:27.397025Z","iopub.status.idle":"2021-06-15T14:01:27.544291Z","shell.execute_reply.started":"2021-06-15T14:01:27.396993Z","shell.execute_reply":"2021-06-15T14:01:27.543441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.groupby(['Store', 'Dept']).size()","metadata":{"execution":{"iopub.status.busy":"2021-06-15T14:04:53.598304Z","iopub.execute_input":"2021-06-15T14:04:53.598769Z","iopub.status.idle":"2021-06-15T14:04:53.651341Z","shell.execute_reply.started":"2021-06-15T14:04:53.59873Z","shell.execute_reply":"2021-06-15T14:04:53.650187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_stores.head()","metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"79553aa3-3ba6-4c5e-8ba8-4a4a906a6d04"},"execution":{"iopub.status.busy":"2021-06-15T14:00:52.622893Z","iopub.execute_input":"2021-06-15T14:00:52.623448Z","iopub.status.idle":"2021-06-15T14:00:52.654742Z","shell.execute_reply.started":"2021-06-15T14:00:52.623387Z","shell.execute_reply":"2021-06-15T14:00:52.653748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_stores.nunique()","metadata":{"execution":{"iopub.status.busy":"2021-06-15T14:01:05.849765Z","iopub.execute_input":"2021-06-15T14:01:05.850175Z","iopub.status.idle":"2021-06-15T14:01:05.859864Z","shell.execute_reply.started":"2021-06-15T14:01:05.850139Z","shell.execute_reply":"2021-06-15T14:01:05.858679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_features.head()","metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"328e1ecc-6116-444d-ac00-a781f507796e"},"execution":{"iopub.status.busy":"2021-06-15T14:00:29.566126Z","iopub.execute_input":"2021-06-15T14:00:29.566531Z","iopub.status.idle":"2021-06-15T14:00:29.589655Z","shell.execute_reply.started":"2021-06-15T14:00:29.566494Z","shell.execute_reply":"2021-06-15T14:00:29.588498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_features.sort_values(\"Date\").tail(4)","metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fb1c9ba1-4820-48db-aa24-581167a13562"},"execution":{"iopub.status.busy":"2021-06-15T14:00:30.779345Z","iopub.execute_input":"2021-06-15T14:00:30.779757Z","iopub.status.idle":"2021-06-15T14:00:30.818128Z","shell.execute_reply.started":"2021-06-15T14:00:30.779723Z","shell.execute_reply":"2021-06-15T14:00:30.817036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Min, max dates\n\ndf_test starts in a week just after df_train. df_sample corresponds to the name range as df_test as indicated in the competition description. ","metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c5375d27-96c4-40c4-aed7-cb2af8904e75"}}},{"cell_type":"code","source":"df_train[\"Date\"].min(),df_train[\"Date\"].max(),df_train[\"Date\"].count()","metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"81001156-43d8-46f1-9649-540ca9d1c9f8"},"execution":{"iopub.status.busy":"2021-06-15T05:21:48.582848Z","iopub.execute_input":"2021-06-15T05:21:48.584929Z","iopub.status.idle":"2021-06-15T05:21:48.777684Z","shell.execute_reply.started":"2021-06-15T05:21:48.584889Z","shell.execute_reply":"2021-06-15T05:21:48.776758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test[\"Date\"].min(),df_test[\"Date\"].max(),df_test[\"Date\"].count()","metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0ba7ee16-7237-437c-bfe4-2f558a6aaee3"},"execution":{"iopub.status.busy":"2021-06-15T05:21:48.781832Z","iopub.execute_input":"2021-06-15T05:21:48.783944Z","iopub.status.idle":"2021-06-15T05:21:48.853074Z","shell.execute_reply.started":"2021-06-15T05:21:48.783906Z","shell.execute_reply":"2021-06-15T05:21:48.852263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_sample.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-15T05:21:48.857072Z","iopub.execute_input":"2021-06-15T05:21:48.859217Z","iopub.status.idle":"2021-06-15T05:21:48.874901Z","shell.execute_reply.started":"2021-06-15T05:21:48.859179Z","shell.execute_reply":"2021-06-15T05:21:48.874052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_sample[\"Id\"].min(),df_sample[\"Id\"].max(),df_sample[\"Id\"].count()","metadata":{"execution":{"iopub.status.busy":"2021-06-15T05:21:48.878789Z","iopub.execute_input":"2021-06-15T05:21:48.880928Z","iopub.status.idle":"2021-06-15T05:21:48.945432Z","shell.execute_reply.started":"2021-06-15T05:21:48.88089Z","shell.execute_reply":"2021-06-15T05:21:48.944516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Some remarks about range and granularity of ids of time series\n\nThis competition asks for a forecast up to 8 months in advance, which is very challenging given little information provided. We really need to rely upon seasonality in order to provided good forecasts. Considering 3331 pairs Store-Dept are quite tough as well, since the problem is too much granular. We cannot generalize the model with a variable taking into account Store-Dept ID due to the complexity it would generate, reducing a lot the degrees of freedom for model. So it is important to find features that could explain store-depts general information.\n\nBesides using the features provided, there are some approaches we could think of:\n1. propagating one-year information like lags sales, avgs, etc, improving information with available features\n2. propagating the least information available retaining long time information for the last months but short information for the first weeks of prediction. \n\nThe option 2 is probably better in terms of Kaggle metrics, but it makes the model less general, if we decide to transform it in an application. Besides, there will be more technical details to take care of. I would postpone this for a later version of this model. ","metadata":{}},{"cell_type":"markdown","source":"# Check Stores","metadata":{}},{"cell_type":"markdown","source":"We lack interesting information from Stores and Departments:\n* What is the meaning of Weekly_Sales? Total Amount? Gross Profit? Should Weekly_Sales be strictly positive?\n* What are the start and end date(store which was closed)? \n* What are the start and end date of given Depts? Many of them can be seasonal, which means we do not expect sales at some specific weeks in the year. Does the sample provided here for Kaggle only contain available Departments? Can you assume for some cases the first entry in weekly sales as a guess for the Dept starting point? Store-Depts not present means zero Weekly_Sales?\n* A better description of categories in Departments and Stores would be interesting, specially to undestand whether we should expect increasing in sales in given seasons. \n* A plus would be to provide insights from stocks.\n\nI think we cannot answer these questions with the description given. We will need to think about strategies to go on.\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-06-14T04:11:16.29768Z","iopub.execute_input":"2021-06-14T04:11:16.298011Z","iopub.status.idle":"2021-06-14T04:11:16.303826Z","shell.execute_reply.started":"2021-06-14T04:11:16.297979Z","shell.execute_reply":"2021-06-14T04:11:16.302416Z"}}},{"cell_type":"markdown","source":"## Type A: Bigger size and number of stores\n\nThe definition of A, B and C follow the common sense order:\n* There are more stores Type A available, and they are bigger in size\n* There are only a few store for Type C and they are smaller than both A and B.","metadata":{}},{"cell_type":"code","source":"df_stores.head(2)","metadata":{"execution":{"iopub.status.busy":"2021-06-15T05:21:48.949265Z","iopub.execute_input":"2021-06-15T05:21:48.951378Z","iopub.status.idle":"2021-06-15T05:21:48.966702Z","shell.execute_reply.started":"2021-06-15T05:21:48.95134Z","shell.execute_reply":"2021-06-15T05:21:48.965854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(x=\"Type\",data=df_stores)\nplt.grid(True)\n\nplt.show()\nsns.boxplot(x=\"Type\", y=\"Size\", data=df_stores)\n\nplt.grid(True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-15T05:21:48.970346Z","iopub.execute_input":"2021-06-15T05:21:48.972467Z","iopub.status.idle":"2021-06-15T05:21:49.34675Z","shell.execute_reply.started":"2021-06-15T05:21:48.972428Z","shell.execute_reply":"2021-06-15T05:21:49.345874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n# Join df_stores and date tranformations","metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d84c08dd-ff2d-4bb9-9da5-d665814b642c"}}},{"cell_type":"markdown","source":"## Functions to be used later on","metadata":{}},{"cell_type":"code","source":"FIRST_DATE_AVAIABLE='2010-02-05' # This is to take into account the WeekIndex\ndef apply_ID(df):\n    \"\"\"\n    Here, the transformation needed for the sample data\n    \"\"\"\n    df[\"Store_Dept\"] = df[\"Store\"].astype(str)+\"_\"+df[\"Dept\"].astype(str)\n    df[\"Id\"] = df[\"Store_Dept\"]+\"_\"+df[\"Date\"].astype(str)\n    return df        \n\ndef date_transforms(df):\n    \"\"\" Let's create some features related to date specific information, like: Day of Month(and Day_ranges), \n    week_index(from first date available),\n    \"\"\"\n    df['dt'] = pd.to_datetime(df['Date'])\n    df['Year'] = df['dt'].dt.year\n    df['Month'] = df['dt'].dt.month\n    df['Week'] = df['dt'].dt.week\n    df['Day'] = df['dt'].dt.day\n    df['Day_Range'] = pd.cut(df[\"Day\"], bins=[0, 10, 20, 31], right=True)\n    df[\"YearMonth\"] = df['dt'].dt.strftime('%Y-%b')\n    df[\"Week_Index\"] = (df['dt']-pd.to_datetime(FIRST_DATE_AVAIABLE)).dt.days/7\n\n    return df","metadata":{"execution":{"iopub.status.busy":"2021-06-15T05:21:49.350644Z","iopub.execute_input":"2021-06-15T05:21:49.352778Z","iopub.status.idle":"2021-06-15T05:21:49.364542Z","shell.execute_reply.started":"2021-06-15T05:21:49.352739Z","shell.execute_reply":"2021-06-15T05:21:49.363448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def correct_negative_sales(sales):\n    \"\"\" Since we find negative weekly sales, let's propose a fix for that \"\"\"\n    return 0 if sales < 0 else sales\n\ndef join_stores(df,df_stores):\n  return df.merge(df_stores, on=[\"Store\"], how=\"left\")\n\ndef join_features(df,df_feat):\n  return df.merge(df_feat, on=[\"Store\",\"Date\",\"IsHoliday\"], how=\"left\")","metadata":{"execution":{"iopub.status.busy":"2021-06-15T05:21:49.369229Z","iopub.execute_input":"2021-06-15T05:21:49.372174Z","iopub.status.idle":"2021-06-15T05:21:49.380625Z","shell.execute_reply.started":"2021-06-15T05:21:49.372042Z","shell.execute_reply":"2021-06-15T05:21:49.379824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Apply ID-like for samples, joining stores and features + applying date tranformations\n\nSince we are going to work with one year lags, we will use data from one year first after first date to train the model. \n\nOne interesting feature we might take is the min, max weekly sales for this first year, in order to propagate some information on scales.\nAnother approach that would be interesting is to normalize each store_dept values, which could be interesting to understand the seasonal shapes of the time series. \n\nBesides improving the model (without taking into account scales of different stores) maybe it also reduced the number of progated avg, lag information, making the model more general.\n\nFor this version of the model, we decided to use first year min, max values as features, letting the use of norms for later on.","metadata":{}},{"cell_type":"code","source":"del_df(df_train)\ndel_df(df_test)\ndel_df(df_stores)\ndel_df(df_features)\ngc.collect()\n\ndf_train = pd.read_csv(\"/kaggle/input/walmart-recruiting-store-sales-forecasting/train.csv.zip\")\ndf_test = pd.read_csv(\"/kaggle/input/walmart-recruiting-store-sales-forecasting/test.csv.zip\")\ndf_stores = pd.read_csv(\"/kaggle/input/walmart-recruiting-store-sales-forecasting/stores.csv\")\ndf_features = pd.read_csv(\"/kaggle/input/walmart-recruiting-store-sales-forecasting/features.csv.zip\")\n\ndf_test[\"Weekly_Sales\"] = None\n\ndf_train = apply_ID(df = df_train)\ndf_train = date_transforms(df = df_train)\ndf_train = join_stores(df = df_train, df_stores=df_stores)\ndf_train = join_features(df = df_train, df_feat=df_features)\n\ndf_test = apply_ID(df = df_test)\ndf_test = date_transforms(df = df_test)\ndf_test = join_stores(df = df_test, df_stores=df_stores)\ndf_test = join_features(df = df_test, df_feat=df_features)\n\n#Min max span one year\ndf_max_mins = df_train.groupby([\"Store\",\"Dept\"]).agg(min_sales_history=(\"Weekly_Sales\",\"min\"),\n                                                     max_sales_history=(\"Weekly_Sales\",\"max\"),\n                                                     avg_sales_history=(\"Weekly_Sales\",\"mean\")).reset_index()\n        \ndf_max_mins[\"min_sales_history_nonneg\"] = df_max_mins.apply(lambda x:correct_negative_sales(x[\"min_sales_history\"]),axis=1)\ndf_max_mins[\"min_sales_history_norm\"] = df_max_mins[\"min_sales_history_nonneg\"]/df_max_mins[\"max_sales_history\"]\ndf_max_mins[\"avg_sales_history_norm\"] = df_max_mins[\"avg_sales_history\"]/df_max_mins[\"max_sales_history\"]\n\n\ndf_train = df_train.merge(df_max_mins,on=[\"Store\",\"Dept\"],how=\"left\")\ndf_test = df_test.merge(df_max_mins,on=[\"Store\",\"Dept\"],how=\"left\")\n\ndf_train[\"Weekly_Sales_Norm\"] = df_train[\"Weekly_Sales\"]/df_train[\"max_sales_history\"]\ndf_test[\"Weekly_Sales_Norm\"] = None","metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"aed8f462-fa3a-40a8-8d4f-e356235a933c"},"execution":{"iopub.status.busy":"2021-06-15T05:21:49.385312Z","iopub.execute_input":"2021-06-15T05:21:49.388311Z","iopub.status.idle":"2021-06-15T05:22:00.263654Z","shell.execute_reply.started":"2021-06-15T05:21:49.388275Z","shell.execute_reply":"2021-06-15T05:22:00.26271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Negative Weekly Sales\n\nYes, there are negative entries!!!\nIn a real problem, I would ask business and data teams to check what are the meaning of such entries. For this test, I will remove those entries from training/testing and assume zeros for predictions in test data when labels are zero. The fraction is small, though, around 1/400.","metadata":{}},{"cell_type":"code","source":"df_train[df_train.Weekly_Sales<0].shape","metadata":{"execution":{"iopub.status.busy":"2021-06-15T05:22:00.268073Z","iopub.execute_input":"2021-06-15T05:22:00.270147Z","iopub.status.idle":"2021-06-15T05:22:00.409482Z","shell.execute_reply.started":"2021-06-15T05:22:00.270107Z","shell.execute_reply":"2021-06-15T05:22:00.408634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Checking Missing data","metadata":{}},{"cell_type":"markdown","source":"From the features, we miss a lot of Markdown informations. In the description it is written that information is recorded since 2011-11-11. Even with this time cut, only Markdown1 is very populated.\nOther variables are thoroughly populated.","metadata":{}},{"cell_type":"code","source":"eda.plot_missing(df_train,display=[\"Bar Chart\"])","metadata":{"execution":{"iopub.status.busy":"2021-06-15T05:22:00.413259Z","iopub.execute_input":"2021-06-15T05:22:00.415422Z","iopub.status.idle":"2021-06-15T05:22:07.858454Z","shell.execute_reply.started":"2021-06-15T05:22:00.415367Z","shell.execute_reply":"2021-06-15T05:22:07.857337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eda.plot_missing(df_train[df_train.Date>=\"2011-11-11\"],display=[\"Bar Chart\"])","metadata":{"execution":{"iopub.status.busy":"2021-06-15T05:22:07.859819Z","iopub.execute_input":"2021-06-15T05:22:07.860176Z","iopub.status.idle":"2021-06-15T05:22:11.61328Z","shell.execute_reply.started":"2021-06-15T05:22:07.860137Z","shell.execute_reply":"2021-06-15T05:22:11.612491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eda.plot_missing(df_train[df_train.Date>=\"2011-02-05\"],display=[\"Bar Chart\"])","metadata":{"execution":{"iopub.status.busy":"2021-06-15T05:22:11.614585Z","iopub.execute_input":"2021-06-15T05:22:11.615087Z","iopub.status.idle":"2021-06-15T05:22:17.138422Z","shell.execute_reply.started":"2021-06-15T05:22:11.615048Z","shell.execute_reply":"2021-06-15T05:22:17.137337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Time Series for Store and/or Type aggregated data\n\nLet's have a first look into Time Series plot in order to understand import general aspects of our Weekly_Sales","metadata":{"execution":{"iopub.status.busy":"2021-06-14T02:33:30.804924Z","iopub.execute_input":"2021-06-14T02:33:30.805379Z","iopub.status.idle":"2021-06-14T02:33:30.838503Z","shell.execute_reply.started":"2021-06-14T02:33:30.805337Z","shell.execute_reply":"2021-06-14T02:33:30.837538Z"}}},{"cell_type":"code","source":"def stores_month(df,col=\"Weekly_Sales\"):\n    \"\"\" Monthly behaviour by store and type\"\"\"\n    return df.groupby([\"Store\",\"Type\",\"Year\",\"Month\",\"YearMonth\"]).agg(avg_sales=(col,\"mean\"),\n                                                                             sum_sales=(col,\"sum\"),\n                                                                             n_dist_Dept=(\"Dept\",\"nunique\")).reset_index().sort_values([\"Year\",\"Month\"])\n\ndef stores_week(df,col=\"Weekly_Sales\"):\n    \"\"\" Type of Store weekly aggregated behaviour\"\"\"\n    return df.groupby([\"Type\",\"Week_Index\",\"Date\"]).agg(avg_sales=(col,\"mean\"),\n                                           sum_sales=(col,\"sum\"),\n                                           n_dist_Store_Dept=(\"Store_Dept\",\"nunique\")).reset_index().sort_values([\"Date\"])","metadata":{"execution":{"iopub.status.busy":"2021-06-15T05:22:17.139828Z","iopub.execute_input":"2021-06-15T05:22:17.140162Z","iopub.status.idle":"2021-06-15T05:22:17.152878Z","shell.execute_reply.started":"2021-06-15T05:22:17.140128Z","shell.execute_reply":"2021-06-15T05:22:17.151877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_agg_stores_month = stores_month(df_train,\"Weekly_Sales\")\ndf_agg_stores_week= stores_week(df_train,\"Weekly_Sales\")","metadata":{"execution":{"iopub.status.busy":"2021-06-15T05:22:17.154428Z","iopub.execute_input":"2021-06-15T05:22:17.154788Z","iopub.status.idle":"2021-06-15T05:22:17.536972Z","shell.execute_reply.started":"2021-06-15T05:22:17.154754Z","shell.execute_reply":"2021-06-15T05:22:17.536088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Functions to help plot boxplot and time series","metadata":{}},{"cell_type":"code","source":"def plot_boxes_ts(df,xcol=\"YearMonth\",hue=\"Type\"):\n    \"\"\" Boxplot plots for 3 diff aggregated-variables: sum, avg of sales and n distinct Department, Hue is type of store\"\"\"\n    width = 25\n    height = 10\n\n    plt.figure(figsize=(width,height))\n    sns.boxplot(x=xcol,y=\"sum_sales\",data=df_agg_stores_month,hue=\"Type\").set_title(\"Sum sales by Month\")\n    plt.xticks(rotation=90)\n    plt.grid(True)\n    plt.show()\n\n    plt.figure(figsize=(width,height))\n    sns.boxplot(x=xcol,y=\"avg_sales\",data=df_agg_stores_month,hue=\"Type\").set_title(\"Avg sales by Month\")\n    plt.xticks(rotation=90)\n    plt.grid(True)\n    plt.show()\n\n    plt.figure(figsize=(width,height))\n    sns.boxplot(x=xcol,y=\"n_dist_Dept\",data=df_agg_stores_month,hue=\"Type\").set_title(\"n_dist_Dept sales by Month\")\n    plt.xticks(rotation=90)\n    plt.grid(True)\n    plt.show()\n\ndef plot_ts(df, var,  title='', xlabel='', ylabel='', all_labels=False, savefig=False, savepath=\"\",label=None,color=None):\n    \"\"\" Time series for a given variable \"\"\"\n    df[var].plot(title=title, marker=\"o\",label=label,color=color)\n    plt.xlabel(xlabel)\n    plt.ylabel(ylabel)\n    plt.grid(True)\n    if all_labels:\n        labels = df.index.to_list()\n        arrticks = np.arange(len(labels))\n        plt.xticks(arrticks, labels, rotation=90)\n    else:\n        plt.xticks(rotation=90)\n    if savefig and savepath != \"\":\n        plt.savefig(savepath)    \n        \ndef plot_ts_multi(df, var1=None,var2=None,title='', xlabel='', ylabel='', all_labels=False, savefig=False, savepath=\"\"):\n    \"\"\" Time series comparting to variables with two distinc y-axis\"\"\"\n    if(var1 is None):\n      print(\"var1 cannot be none, returning\")\n      return False\n    \n    ax1 = df[var1].plot(figsize=(12, 7), title=title, marker=\"o\",color=\"blue\",label=var1)\n    ax1.set_ylabel(var1,color=\"blue\") \n    if(var2 is not None):\n      ax2 = ax1.twinx()\n      ax2.set_ylabel(var2,color=\"red\") \n      df[var2].plot(figsize=(12, 7), title=title, marker=\"o\",color=\"red\",label=var2)\n   \n    plt.xlabel(xlabel)\n    plt.grid(True)\n    \n    title = f\"Time Series, {var1}\" \n    if(var2 is not None):\n        title+=f\" and {var2}\"\n    plt.title(title)\n    \n    if all_labels:\n        labels = df.index.to_list()\n        arrticks = np.arange(len(labels))\n        plt.xticks(arrticks, labels, rotation=90)\n    else:\n        plt.xticks(rotation=90)\n    if savefig and savepath != \"\":\n        plt.savefig(savepath)\n \n\ndef plot_ts_multix(df, var1=None,var2=None, title='', xlabel='', ylabel='', all_labels=False, savefig=False, savepath=\"\"):\n    \"\"\" Just a different size. I need time to improve this\"\"\"\n    if(var1 is None):\n      print(\"var1 cannot be none, returning\")\n      return False\n    \n    ax1 = df[var1].plot(title=title, marker=\"o\",color=\"blue\",label=var1)\n    ax1.set_ylabel(var1,color=\"blue\") \n    if(var2 is not None):\n      ax2 = ax1.twinx()\n      ax2.set_ylabel(var2,color=\"red\") \n      df[var2].plot(title=title, marker=\"o\",color=\"red\",label=var2)\n   \n    plt.xlabel(xlabel)\n    plt.grid(True)\n    \n    title = f\"Time Series, {var1}\" \n    if(var2 is not None):\n        title+=f\" and {var2}\"\n    plt.title(title)\n    \n    if all_labels:\n        labels = df.index.to_list()\n        arrticks = np.arange(len(labels))\n        plt.xticks(arrticks, labels, rotation=90)\n    else:\n        plt.xticks(rotation=90)\n    if savefig and savepath != \"\":\n        plt.savefig(savepath)\n            ","metadata":{"execution":{"iopub.status.busy":"2021-06-15T05:22:17.538294Z","iopub.execute_input":"2021-06-15T05:22:17.538888Z","iopub.status.idle":"2021-06-15T05:22:17.56767Z","shell.execute_reply.started":"2021-06-15T05:22:17.53885Z","shell.execute_reply":"2021-06-15T05:22:17.566839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Stores monthly","metadata":{}},{"cell_type":"markdown","source":"Some things are quite as expected:\n* Size A has consistently more departments and sales by month, followed B and C.\n* There are peak trends on December for type A and B (as expected by major holidays). For Type C is not clear in the general basis. Type C stores might be out of trend and maybe need specific care.","metadata":{}},{"cell_type":"code","source":"plot_boxes_ts(df_agg_stores_month,xcol=\"YearMonth\",hue=\"Type\")","metadata":{"execution":{"iopub.status.busy":"2021-06-15T05:22:17.569202Z","iopub.execute_input":"2021-06-15T05:22:17.569871Z","iopub.status.idle":"2021-06-15T05:22:27.485037Z","shell.execute_reply.started":"2021-06-15T05:22:17.569832Z","shell.execute_reply":"2021-06-15T05:22:27.483993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Stores weekly","metadata":{}},{"cell_type":"markdown","source":"### Stores: Type A","metadata":{}},{"cell_type":"markdown","source":"We see clearly peaks around black friday and Christmas both for the sum and average of weekly sales. Although there is a small drop from 2010-2011 in the christmas peak, it seems like a seasonal trend. There is a drop seen in January, which may be due to 2 aspects:\n* one: side-effect of overwhelming sales during Christmas.\n* two: Weather effects.\n\nCase one seems to be the main reason, but we may think about when dealing with Temperature feature.","metadata":{}},{"cell_type":"code","source":"var1=\"sum_sales\"\nvar2=\"avg_sales\"\nvar3=\"n_dist_Store_Dept\"\ntypestore=\"A\"\nwidth = 25\nheight = 10\nplt.figure(figsize=(width,height))\nplot_ts_multi(df=df_agg_stores_week[df_agg_stores_week.Type==typestore].set_index(\"Date\"),var1=var1,var2=var2)\n\nplt.figure(figsize=(width,height))\nplot_ts_multi(df=df_agg_stores_week[df_agg_stores_week.Type==typestore].set_index(\"Date\"),var1=var1,var2=var3)","metadata":{"execution":{"iopub.status.busy":"2021-06-15T05:22:27.489114Z","iopub.execute_input":"2021-06-15T05:22:27.489514Z","iopub.status.idle":"2021-06-15T05:22:28.234583Z","shell.execute_reply.started":"2021-06-15T05:22:27.489465Z","shell.execute_reply":"2021-06-15T05:22:28.2336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here, the peaks around Black Friday and Christmas are zoomed. It seems interesting to give special attention to them.","metadata":{"execution":{"iopub.status.busy":"2021-06-14T03:59:31.79688Z","iopub.execute_input":"2021-06-14T03:59:31.797229Z","iopub.status.idle":"2021-06-14T03:59:31.802628Z","shell.execute_reply.started":"2021-06-14T03:59:31.797198Z","shell.execute_reply":"2021-06-14T03:59:31.801441Z"}}},{"cell_type":"code","source":"var1=\"sum_sales\"\nvar2=\"avg_sales\"\ntypestore=\"A\"\nwidth = 25\nheight = 10\nplt.figure(figsize=(width,height))\nplot_ts_multi(df=df_agg_stores_week[(df_agg_stores_week.Type==typestore)&(df_agg_stores_week.Date>\"2010-10-20\")&(df_agg_stores_week.Date<\"2011-01-10\")].set_index(\"Date\"),\n              var1=var1,var2=var2)\n","metadata":{"execution":{"iopub.status.busy":"2021-06-15T05:22:28.238722Z","iopub.execute_input":"2021-06-15T05:22:28.239073Z","iopub.status.idle":"2021-06-15T05:22:28.592579Z","shell.execute_reply.started":"2021-06-15T05:22:28.239037Z","shell.execute_reply":"2021-06-15T05:22:28.591726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Stores: Type B","metadata":{"execution":{"iopub.status.busy":"2021-06-14T02:33:37.84181Z","iopub.execute_input":"2021-06-14T02:33:37.842205Z","iopub.status.idle":"2021-06-14T02:33:38.341864Z","shell.execute_reply.started":"2021-06-14T02:33:37.842167Z","shell.execute_reply":"2021-06-14T02:33:38.340883Z"}}},{"cell_type":"markdown","source":"With a expected lower absolute value than Type A, Type B trends follow a seasonality like stores type A.","metadata":{}},{"cell_type":"code","source":"var1=\"sum_sales\"\nvar2=\"avg_sales\"\nvar3=\"n_dist_Store_Dept\"\ntypestore=\"B\"\nwidth = 25\nheight = 10\nplt.figure(figsize=(width,height))\nplot_ts_multi(df=df_agg_stores_week[df_agg_stores_week.Type==typestore].set_index(\"Date\"),var1=var1,var2=var2)\n\nplt.figure(figsize=(width,height))\nplot_ts_multi(df=df_agg_stores_week[df_agg_stores_week.Type==typestore].set_index(\"Date\"),var1=var1,var2=var3)","metadata":{"execution":{"iopub.status.busy":"2021-06-15T05:22:28.596443Z","iopub.execute_input":"2021-06-15T05:22:28.598496Z","iopub.status.idle":"2021-06-15T05:22:29.364663Z","shell.execute_reply.started":"2021-06-15T05:22:28.598455Z","shell.execute_reply":"2021-06-15T05:22:29.363772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Stores: Type C","metadata":{}},{"cell_type":"markdown","source":"Different from Types A and B, C does not seem to follow a well-behavioured trend. Although type C Stores are smaller, it is interesting to have a close look to understand how\nto improve the model.","metadata":{}},{"cell_type":"code","source":"var1=\"sum_sales\"\nvar2=\"avg_sales\"\nvar3=\"n_dist_Store_Dept\"\ntypestore=\"C\"\nwidth = 25\nheight = 10\nplt.figure(figsize=(width,height))\nplot_ts_multi(df=df_agg_stores_week[df_agg_stores_week.Type==typestore].set_index(\"Date\"),var1=var1,var2=var2)\n\nplt.figure(figsize=(width,height))\nplot_ts_multi(df=df_agg_stores_week[df_agg_stores_week.Type==typestore].set_index(\"Date\"),var1=var1,var2=var3)","metadata":{"execution":{"iopub.status.busy":"2021-06-15T05:22:29.368676Z","iopub.execute_input":"2021-06-15T05:22:29.370825Z","iopub.status.idle":"2021-06-15T05:22:30.149916Z","shell.execute_reply.started":"2021-06-15T05:22:29.370786Z","shell.execute_reply":"2021-06-15T05:22:30.14907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Aggregate by store and IsHoliday\n\nMotivation: check missing dates","metadata":{}},{"cell_type":"code","source":"try:\n  del(df_agg_stores_hol)\nexcept:\n  pass\ngc.collect()\n\ndf_agg_stores_hol = df_train.groupby([\"Store\",\"IsHoliday\"]).agg(\n    ndist_date_store=(\"Date\",\"nunique\"),\n    min_sales_store=(\"Weekly_Sales\",\"min\"),\n    max_sales_store=(\"Weekly_Sales\",\"max\"),\n    avg_sales_store=(\"Weekly_Sales\",\"mean\"),\n    sum_sales_store=(\"Weekly_Sales\",\"sum\")).reset_index()","metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0c295234-a116-425f-88e3-4d58a33f650f"},"execution":{"iopub.status.busy":"2021-06-15T05:22:30.154018Z","iopub.execute_input":"2021-06-15T05:22:30.156128Z","iopub.status.idle":"2021-06-15T05:22:30.642036Z","shell.execute_reply.started":"2021-06-15T05:22:30.156087Z","shell.execute_reply":"2021-06-15T05:22:30.641087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Checking missing dates\n\nIn the time span of trainning sample, let's check missing daily entries for stores. In order to do so, the ndist_date_store will be used. \nAs seen below, there entries for all the stores in the 10 holidays and on 133 non-holidays.","metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3b5f602f-d0ea-4a9a-bc56-46c9005321fe"}}},{"cell_type":"code","source":"try:\n  del(df_agg_stores_dept_hol)\nexcept:\n  pass\ngc.collect()\n  \ndf_agg_stores_dept_hol = df_train.groupby([\"Store\",\"Dept\",\"IsHoliday\"]).agg(\n    first_date=(\"Date\",\"first\"),\n    last_date=(\"Date\",\"last\"),\n    ndist_date=(\"Date\",\"nunique\"),\n    max_sales=(\"Weekly_Sales\",\"max\"),\n    min_sales=(\"Weekly_Sales\",\"min\"),\n    avg_sales=(\"Weekly_Sales\",\"mean\"),\n    sum_sales=(\"Weekly_Sales\",\"sum\")).reset_index()\n\n#Let's join total store levels and check share\ndf_agg_stores_dept_hol[\"Store_Dep\"] = df_agg_stores_dept_hol[\"Store\"].astype(str)+\"_\"+df_agg_stores_dept_hol[\"Dept\"].astype(str)\ndf_agg_stores_dept_hol = df_agg_stores_dept_hol.merge(df_agg_stores_hol,on=[\"Store\",\"IsHoliday\"],how=\"inner\")\ndf_agg_stores_dept_hol[\"share_Dep\"] = df_agg_stores_dept_hol[\"sum_sales\"]/df_agg_stores_dept_hol[\"sum_sales_store\"]\ndf_agg_stores_dept_hol[\"diff_Dep_avg\"] = (df_agg_stores_dept_hol[\"avg_sales\"]-df_agg_stores_dept_hol[\"avg_sales_store\"])/df_agg_stores_dept_hol[\"avg_sales_store\"]\n","metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"068173b9-677d-4ea6-9d48-34ab23767f84"},"execution":{"iopub.status.busy":"2021-06-15T05:22:30.646526Z","iopub.execute_input":"2021-06-15T05:22:30.648656Z","iopub.status.idle":"2021-06-15T05:22:31.138123Z","shell.execute_reply.started":"2021-06-15T05:22:30.648615Z","shell.execute_reply":"2021-06-15T05:22:31.137188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Checking missing dates dept\n\nIn the time span of trainning sample, let's check missing daily entries for depts. In order to do so, the ndist_date will be used. \n\nOut of 3331 (See below cells), for non-holidays:\n* 2663 store_depts have entries in all dates.\n* 196  store_depts have entries in between(100,132) dates. \n* 128  store_depts have entries in between(50,99) dates.\n* 340  store_depts have entries in < 50 dates.\n* 49 store_depts have entries in only one single date.\n\nOut of 3331 (See below cells), for holidays:\n* 2785 store_depts have entries in all dates.\n* 60 store_depts have entries 9/10 dates.\n* 109  store_depts have entries in only one single date.\n\n\nIt is important to recognize what is the source of these missing dates:\n* New Departments?\n* Lack of transactions? which means sales = 0 for given departments?\n\n### What to do with them? \nIn a real business, we would probably contact business, software engineering or/and data engineering teams in order to understand carefully the source of this behaviour. With these already made csvs, we need to figure out strategies to deal with missing dates.","metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"258f50c9-9198-46c3-81d9-23dc77c3e6e6"}}},{"cell_type":"code","source":"df_agg_stores_dept_hol[\"Store_Dep\"].nunique()","metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"de250719-5d28-4eb6-8918-5f74b59a40ce"},"execution":{"iopub.status.busy":"2021-06-15T05:22:31.142649Z","iopub.execute_input":"2021-06-15T05:22:31.144801Z","iopub.status.idle":"2021-06-15T05:22:31.156452Z","shell.execute_reply.started":"2021-06-15T05:22:31.144761Z","shell.execute_reply":"2021-06-15T05:22:31.155424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_count_non_holidays = df_agg_stores_dept_hol[df_agg_stores_dept_hol.IsHoliday==False].groupby([\"ndist_date\",\"IsHoliday\"]).agg(n_Store_dep=(\"Store_Dep\",\"nunique\")).reset_index()","metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"467c4b00-b44c-424d-92a3-ae4cdd86aff5"},"execution":{"iopub.status.busy":"2021-06-15T05:22:31.160707Z","iopub.execute_input":"2021-06-15T05:22:31.162826Z","iopub.status.idle":"2021-06-15T05:22:31.202294Z","shell.execute_reply.started":"2021-06-15T05:22:31.162788Z","shell.execute_reply":"2021-06-15T05:22:31.201491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_count_holidays = df_agg_stores_dept_hol[df_agg_stores_dept_hol.IsHoliday==True].groupby([\"ndist_date\",\"IsHoliday\"]).agg(n_Store_dep=(\"Store_Dep\",\"nunique\")).reset_index()","metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b22aaf94-9b9b-4e12-9e4d-28a9f2e47305"},"execution":{"iopub.status.busy":"2021-06-15T05:22:31.206251Z","iopub.execute_input":"2021-06-15T05:22:31.216498Z","iopub.status.idle":"2021-06-15T05:22:31.249064Z","shell.execute_reply.started":"2021-06-15T05:22:31.21646Z","shell.execute_reply":"2021-06-15T05:22:31.248261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\n df_count_non_holidays[df_count_non_holidays.ndist_date==133][\"n_Store_dep\"].sum(),    df_count_non_holidays[df_count_non_holidays.ndist_date.between(100,132)][\"n_Store_dep\"].sum(),  df_count_non_holidays[df_count_non_holidays.ndist_date.between(50,99)][\"n_Store_dep\"].sum(),\n df_count_non_holidays[df_count_non_holidays.ndist_date<50][\"n_Store_dep\"].sum(),\n df_count_non_holidays[df_count_non_holidays.ndist_date==1][\"n_Store_dep\"].sum()) ","metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a8a28dcb-67d9-404a-8dd6-89f9f607383f"},"execution":{"iopub.status.busy":"2021-06-15T05:22:31.258708Z","iopub.execute_input":"2021-06-15T05:22:31.268128Z","iopub.status.idle":"2021-06-15T05:22:31.297437Z","shell.execute_reply.started":"2021-06-15T05:22:31.268082Z","shell.execute_reply":"2021-06-15T05:22:31.296582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_count_holidays","metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2dca864b-f2b4-4817-b47c-f5607f082a49"},"execution":{"iopub.status.busy":"2021-06-15T05:22:31.301362Z","iopub.execute_input":"2021-06-15T05:22:31.30353Z","iopub.status.idle":"2021-06-15T05:22:31.322999Z","shell.execute_reply.started":"2021-06-15T05:22:31.303491Z","shell.execute_reply":"2021-06-15T05:22:31.322178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Checking dates between","metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3b294f27-4375-492e-883d-24632ce32453"}}},{"cell_type":"markdown","source":"Here, we test whether there are empty spaces between first/last weekly sales, meaning it is not just the case the Dept started at some pointing. \nThe answer is yes, there are distinct spots in time series. Probably, due to seasonal Departments.","metadata":{"execution":{"iopub.status.busy":"2021-06-14T04:25:14.983285Z","iopub.execute_input":"2021-06-14T04:25:14.983637Z","iopub.status.idle":"2021-06-14T04:25:14.994098Z","shell.execute_reply.started":"2021-06-14T04:25:14.983605Z","shell.execute_reply":"2021-06-14T04:25:14.992737Z"}}},{"cell_type":"code","source":"df_agg_dates = df_train.groupby([\"Store\",\"Dept\"]).agg(\n    first_date=(\"Date\",\"first\"),\n    last_date=(\"Date\",\"last\"),\n    ndist_date=(\"Date\",\"nunique\"))\ndf_agg_dates[\"diff_first_last_weeks\"] = 1 + ((pd.to_datetime(df_agg_dates[\"last_date\"]) - pd.to_datetime(df_agg_dates[\"first_date\"])).dt.days)/7\ndf_agg_dates = df_agg_dates[[\"first_date\",\"last_date\",\"diff_first_last_weeks\",\"ndist_date\"]].reset_index()\ndf_agg_dates[\"ndist_equals_diff\"] = df_agg_dates[\"diff_first_last_weeks\"]==df_agg_dates[\"ndist_date\"]\ndf_agg_dates[\"Store_Dep\"] = df_agg_dates[\"Store\"].astype(str) + \"_\" + df_agg_dates[\"Dept\"].astype(str)","metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"74cd34dd-044c-4f32-8602-35b747901c7c"},"execution":{"iopub.status.busy":"2021-06-15T05:22:31.335493Z","iopub.execute_input":"2021-06-15T05:22:31.335841Z","iopub.status.idle":"2021-06-15T05:22:31.659411Z","shell.execute_reply.started":"2021-06-15T05:22:31.335806Z","shell.execute_reply":"2021-06-15T05:22:31.658288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_agg_dates.groupby(\"ndist_equals_diff\")[\"Store_Dep\"].count()","metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e4ecd00d-37d5-4ebc-b888-91cc8ea23bc4"},"execution":{"iopub.status.busy":"2021-06-15T05:22:31.660799Z","iopub.execute_input":"2021-06-15T05:22:31.66112Z","iopub.status.idle":"2021-06-15T05:22:31.681357Z","shell.execute_reply.started":"2021-06-15T05:22:31.661086Z","shell.execute_reply":"2021-06-15T05:22:31.680278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Aggregate for store type","metadata":{}},{"cell_type":"code","source":"try:\n  del(df_agg_type)\nexcept:\n  pass\ngc.collect()\n\n\ndf_agg_type = df_train.groupby([\"Type\"]).agg(\n    min_size=(\"Size\",\"min\"),\n    max_size=(\"Size\",\"max\"),\n    avg_size=(\"Size\",\"mean\"),\n    max_sales_store=(\"Weekly_Sales\",\"max\"),\n    min_sales_store=(\"Weekly_Sales\",\"min\"),\n    avg_sales_store=(\"Weekly_Sales\",\"mean\")).reset_index()\n\n\n\ntry:\n  del(df_agg_type_date)\nexcept:\n  pass\ngc.collect()\n\n\ndf_agg_type_date = df_train.groupby([\"Type\",\"Date\"]).agg(\n    ndist_Store=(\"Store\",\"nunique\"),\n    ndist_StoreDept=(\"Store_Dept\",\"nunique\"),\n    \n    min_size=(\"Size\",\"min\"),\n    max_size=(\"Size\",\"max\"),\n    avg_size=(\"Size\",\"mean\"),\n    max_sales_store=(\"Weekly_Sales\",\"max\"),\n    min_sales_store=(\"Weekly_Sales\",\"min\"),\n    avg_sales_store=(\"Weekly_Sales\",\"mean\")).reset_index()\n","metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f40d4edd-02ac-40aa-910b-fb20590f5f20"},"execution":{"iopub.status.busy":"2021-06-15T05:22:31.695787Z","iopub.execute_input":"2021-06-15T05:22:31.696115Z","iopub.status.idle":"2021-06-15T05:22:32.558133Z","shell.execute_reply.started":"2021-06-15T05:22:31.696081Z","shell.execute_reply":"2021-06-15T05:22:32.557198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Join Features\n\nThe strategy here will be get together train and test (test requires some propagation of information from train period like lags), preparing the features and then separating into train and test.","metadata":{}},{"cell_type":"markdown","source":"## Concat Train + Test for lag propagation","metadata":{}},{"cell_type":"code","source":"df_all = pd.concat([df_train,df_test])","metadata":{"execution":{"iopub.status.busy":"2021-06-15T05:22:32.56269Z","iopub.execute_input":"2021-06-15T05:22:32.564809Z","iopub.status.idle":"2021-06-15T05:22:32.842897Z","shell.execute_reply.started":"2021-06-15T05:22:32.564768Z","shell.execute_reply":"2021-06-15T05:22:32.841922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_all.Date.min(),df_all.Date.max()","metadata":{"execution":{"iopub.status.busy":"2021-06-15T05:22:32.847418Z","iopub.execute_input":"2021-06-15T05:22:32.849527Z","iopub.status.idle":"2021-06-15T05:22:33.033221Z","shell.execute_reply.started":"2021-06-15T05:22:32.849478Z","shell.execute_reply":"2021-06-15T05:22:33.03242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_date_table(df):\n    \"\"\" For cross dats \"\"\"\n    return df[[\"dt\",\"IsHoliday\"]].drop_duplicates()\n\ndef apply_selection(week1,week2,lag1y,lag):\n  if(week1!=week2):\n    return lag\n  else:\n    return lag1y\n\n\ndef cross_dates(df):\n    \"\"\" This to try to propage zeros for all missing dates. Basically, cross join with all the available date.\n    We end up with ~ 560k row which are not so much more.\n    Seems promissing and sounded good in first results.\n    Aborted at this version since we need more time to investigate them \"\"\"\n    df_list = df.groupby([\"Store\",\"Dept\",\"Store_Dept\",\"Type\",\"Size\"]).agg({\"Weekly_Sales\":\"count\"}).reset_index()\n    df_dates = create_date_table(df)\n    df_list['key'] = 1\n    df_dates['key'] = 1\n    df_list = df_list.merge(df_dates,on=\"key\").drop(columns=[\"Weekly_Sales\",\"key\"])\n    df_list = df_list.merge(df,on=[\"Store\",\"Dept\",\"Store_Dept\",\"Type\",\"Size\",\"dt\",\"IsHoliday\"],how=\"left\")\n    df_list[\"was_missing\"] = df_list[\"Date\"].isnull()\n    df_list[\"Date\"] = df_list[\"dt\"].astype(str)\n    df_list = apply_ID(df = df_list)\n    df_list = date_transforms(df = df_list)\n    return df_list.fillna({\"Weekly_Sales\":0,\"Weekly_Sales_Norm\":0})\n\n\ndef rolling_means(df):\n  \"\"\" Rolling mean up to 25 weeks. Counts are just for cross check of missing dates \"\"\"  \n  df = df.sort_values(['Store','Dept', 'Date'])\n  df = df.set_index(['Store','Dept','Date'])\n  df[\"avg_sales_w\"] = df[\"Weekly_Sales\"].rolling(25,min_periods=1).mean()\n  df[\"ncounts_w\"] = df[\"Weekly_Sales\"].rolling(25,min_periods=1).count()\n  df[\"avg_sales_w_norm\"] = df[\"Weekly_Sales_Norm\"].rolling(25,min_periods=1).mean()\n  df[\"ncount_w_norm\"] = df[\"Weekly_Sales_Norm\"].rolling(25,min_periods=1).count()\n  return df.reset_index()\n\n\ndef apply_month_zero(df):\n    \"\"\" applying zeros for negative min sales month\"\"\"\n    if(df[\"min_sales_month\"]<0):\n        return 0\n    else:\n        return df[\"min_sales_month\"]\n    \ndef month_avg(df):\n    \"\"\" some agregations in months to be propated to following year\"\"\"\n    df['Weekly_Sales']=df['Weekly_Sales'].astype(float)\n    df['avg_month'] = df.groupby(['Store','Dept',\"Year\",\"Month\"])['Weekly_Sales'].transform('mean')  \n    df['min_month'] = df.groupby(['Store','Dept',\"Year\",\"Month\"])['Weekly_Sales'].transform('min')    \n    df['max_month'] = df.groupby(['Store','Dept',\"Year\",\"Month\"])['Weekly_Sales'].transform('max')\n    df['avg_month_norm'] = df.groupby(['Store','Dept',\"Year\",\"Month\"])['Weekly_Sales_Norm'].transform('mean')  \n    df['min_month_norm'] = df.groupby(['Store','Dept',\"Year\",\"Month\"])['Weekly_Sales_Norm'].transform('min')    \n    df['max_month_norm'] = df.groupby(['Store','Dept',\"Year\",\"Month\"])['Weekly_Sales_Norm'].transform('max')\n    return df\n\ndef shift_dates(df):\n    \"\"\" Since there are missing dates we cannot rely upon shift 52. \n    We need to first try to join  the exactly week number with its correpondent one year past, then fill with other strategy when last year is missing.\n    For lag sales we will try:\n    * firstly, lag weekly sales same week past year.\n    * secondly, lag moving average at same week past year.\n    * thirdly, fill zero.\n    \"\"\"\n    df_lag1 = df[['Store','Dept', 'Week','Year',\"avg_sales_w\",\"avg_sales_w_norm\",\"Weekly_Sales\",\"Weekly_Sales_Norm\",\"Date\",\"avg_month\",\"avg_month_norm\"]]\n    df_lag1[\"lag_avg_month\"] = df_lag1[\"avg_month\"]\n    df_lag1[\"lag_avg_month_norm\"] = df_lag1[\"avg_month_norm\"]\n    df_lag1[\"lag_avg_sales_w\"] = df_lag1[\"avg_sales_w\"]\n    df_lag1[\"lag_avg_sales_w_norm\"] = df_lag1[\"avg_sales_w_norm\"]\n    df_lag1[\"lag_Weekly_Sales\"] = df_lag1[\"Weekly_Sales\"]\n    df_lag1[\"lag_Weekly_Sales_Norm\"] = df_lag1[\"Weekly_Sales_Norm\"]  \n    df_lag1[\"lag_Date\"] = df_lag1[\"Date\"]\n    df_lag1[\"Year\"] = df_lag1[\"Year\"]+1\n    df_lag1 = df_lag1.drop(columns=[\"avg_sales_w\",\"avg_sales_w_norm\",\"avg_month\",\"avg_month_norm\",\"Weekly_Sales\",\"Weekly_Sales_Norm\",\"Date\"])\n    df = df.sort_values(['Store','Dept', 'Date'])\n    df = df.set_index(['Store','Dept'])\n\n    df[\"lag1y\"] = df[\"Weekly_Sales\"].shift(52)\n    df[\"lag1y_norm\"] = df[\"Weekly_Sales_Norm\"].shift(52)\n    df[\"lag1y_date\"] = df[\"Date\"].shift(52)\n    df[\"lag1y_Week\"] = df[\"Week\"].shift(52)\n\n    df[\"lag1y_avg_sales_w\"] = df[\"avg_sales_w\"].shift(52)\n    df[\"lag1y_ncounts_w\"] = df[\"ncounts_w\"].shift(52)\n\n    df = df.merge(df_lag1,on=['Store','Dept', 'Week','Year'],how=\"left\").copy()\n\n    df[\"lag_sales\"] = df[\"lag_Weekly_Sales\"].fillna(df[\"lag_avg_sales_w\"]).fillna(0)\n    df[\"lag_sales_Norm\"] = df[\"lag_Weekly_Sales_Norm\"].fillna(df[\"lag_avg_sales_w_norm\"]).fillna(0)\n\n    return df.reset_index()\n\ndef apply_holiday(df):\n    \"\"\" Trying special category for holidays, from Black Friday to Christmas \"\"\"\n    if(df[\"Week\"] in([47,52])):\n        return df[\"IsHoliday\"]*10\n    if(df[\"Week\"] in([48, 49, 50, 51])):\n        return df[\"IsHoliday\"]*5\n    else:\n        return  df[\"IsHoliday\"]*1\n\ndef new_features(df):\n    \"\"\" features for holiday, Store Size and temparature ranges\"\"\"\n    df[\"IsHolidayw\"] = df[\"IsHoliday\"]*5\n    df[\"IsHolidayfix\"] = df.apply(apply_holiday,axis=1)\n    df[\"Month\"] = df[\"Month\"].astype(str)\n    df[\"Temperature_Range\"] = pd.qcut(df[\"Temperature\"], 4)\n    df[\"Size_Range\"] = pd.qcut(df[\"Temperature\"], 10)\n    return df","metadata":{"execution":{"iopub.status.busy":"2021-06-15T05:22:33.037354Z","iopub.execute_input":"2021-06-15T05:22:33.039471Z","iopub.status.idle":"2021-06-15T05:22:33.077004Z","shell.execute_reply.started":"2021-06-15T05:22:33.039428Z","shell.execute_reply":"2021-06-15T05:22:33.07604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Running the final tranformations on features","metadata":{}},{"cell_type":"code","source":"try:\n  del(df_all_features)\nexcept:\n  pass\ngc.collect()\n\ndf_all_features = df_all.copy()\ndf_all_features = df_all_features.fillna({\"Weekly_Sales\":0,\"Weekly_Sales_Norm\":0})\n# df_all_features = cross_dates(df=df_all_features) aborted\ndf_all_features = rolling_means(df=df_all_features)\ndf_all_features = month_avg(df=df_all_features)\ndf_all_features = shift_dates(df=df_all_features)\ndf_all_features = new_features(df=df_all_features)","metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d3c52b45-f30e-4b86-b08a-0ad5bb7c2dcd"},"execution":{"iopub.status.busy":"2021-06-15T05:22:33.0821Z","iopub.execute_input":"2021-06-15T05:22:33.084289Z","iopub.status.idle":"2021-06-15T05:23:04.838054Z","shell.execute_reply.started":"2021-06-15T05:22:33.084252Z","shell.execute_reply":"2021-06-15T05:23:04.837146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_all_features.head(2)","metadata":{"execution":{"iopub.status.busy":"2021-06-15T05:23:04.842464Z","iopub.execute_input":"2021-06-15T05:23:04.844524Z","iopub.status.idle":"2021-06-15T05:23:04.914746Z","shell.execute_reply.started":"2021-06-15T05:23:04.844475Z","shell.execute_reply":"2021-06-15T05:23:04.91398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Separating train and test dataframes ","metadata":{}},{"cell_type":"code","source":"df_train_features = df_all_features[df_all_features.Date<=\"2012-10-26\"]\ndf_test_features = df_all_features[df_all_features.Date>\"2012-10-26\"]","metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2aabd976-eb11-4c9d-b130-6385ad9b3d70"},"execution":{"iopub.status.busy":"2021-06-15T05:23:04.918545Z","iopub.execute_input":"2021-06-15T05:23:04.920638Z","iopub.status.idle":"2021-06-15T05:23:05.458849Z","shell.execute_reply.started":"2021-06-15T05:23:04.9206Z","shell.execute_reply":"2021-06-15T05:23:05.457919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# min max dates train features\n\nChecking information provided in the description","metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b21f809e-fc61-4375-ba73-4827f1c47c8d"}}},{"cell_type":"code","source":"df_train_features[\"Date\"].min(),df_train_features[\"Date\"].max()","metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d12b8343-a523-48fb-81ee-ae90cea9e9ac"},"execution":{"iopub.status.busy":"2021-06-15T05:23:05.48983Z","iopub.execute_input":"2021-06-15T05:23:05.491842Z","iopub.status.idle":"2021-06-15T05:23:05.636159Z","shell.execute_reply.started":"2021-06-15T05:23:05.491804Z","shell.execute_reply":"2021-06-15T05:23:05.635435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train_features[df_train_features.MarkDown1.isnull()==False][\"Date\"].min(),df_train_features[df_train_features.MarkDown1.isnull()==False][\"Date\"].max()","metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"be02775c-85b7-468f-a40b-6730e8513dfd"},"execution":{"iopub.status.busy":"2021-06-15T05:23:05.642041Z","iopub.execute_input":"2021-06-15T05:23:05.644079Z","iopub.status.idle":"2021-06-15T05:23:05.84796Z","shell.execute_reply.started":"2021-06-15T05:23:05.64404Z","shell.execute_reply":"2021-06-15T05:23:05.846967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TimeSeries: Features\n\nBefore studying correlation plots, let's start with some Time Series plots, aggregating sales by store. The main point here is to signal seasonal expected events.\n","metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"51c91ac7-3a94-4515-9b4b-779d0ae65b5b"}}},{"cell_type":"markdown","source":"Filling out 'na' with zeros to be able to plot comparison plots and aggregating by Store.","metadata":{}},{"cell_type":"code","source":"# Filling out 'na' with zeros to be able to plot comparison plots\ndf_agg_train_features = df_train_features.fillna({\"MarkDown1\":0,\"MarkDown2\":0,\"MarkDown3\":0,\"MarkDown4\":0,\"MarkDown5\":0}).groupby([\"Store\",\"Type\",\"Temperature\",\"CPI\",\n                                                                           \"Unemployment\",\"Fuel_Price\",\"MarkDown1\",\"MarkDown2\",\n                                                                           \"MarkDown3\",\"MarkDown4\",\"MarkDown5\",\"Date\",\"Week_Index\"]).agg(sum_sales=(\"Weekly_Sales\",\"sum\"),\n                                                                                                               avg_sales=(\"Weekly_Sales\",\"mean\")).reset_index()","metadata":{"execution":{"iopub.status.busy":"2021-06-15T05:23:05.851841Z","iopub.execute_input":"2021-06-15T05:23:05.853896Z","iopub.status.idle":"2021-06-15T05:23:06.227242Z","shell.execute_reply.started":"2021-06-15T05:23:05.853856Z","shell.execute_reply":"2021-06-15T05:23:06.226372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##  Having a look into random stores","metadata":{}},{"cell_type":"markdown","source":"### Temperature\n\nIt seems to have some  dependency in January-February, due to lowest temperatures, but it is also after Christmas. Part of this may signal a consequence of large sales on December.  ","metadata":{}},{"cell_type":"code","source":"var1=\"sum_sales\"\nvar2=\"Temperature\"\nlist_stores = random.choices(df_agg_train_features.Store.unique(),k=8)\n\n\nwidth=30\nheight=30\nfig = plt.figure(figsize=(width,height), dpi=80)\nx=[421,422,423,424,425,426,427,428]\nk=-1\n\n\nfor store in list_stores:    \n    k+=1\n    ax = fig.add_subplot(x[k])    \n    plot_ts_multix(df=df_agg_train_features[df_agg_train_features.Store==store].sort_values(\"Date\").set_index(\"Date\"),var1=var1,var2=var2)\n    \nplt.show()    \n","metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"db133069-6301-47c6-846d-8864b9a774a7"},"execution":{"iopub.status.busy":"2021-06-15T05:23:06.231653Z","iopub.execute_input":"2021-06-15T05:23:06.23371Z","iopub.status.idle":"2021-06-15T05:23:09.304369Z","shell.execute_reply.started":"2021-06-15T05:23:06.23367Z","shell.execute_reply":"2021-06-15T05:23:09.303495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Unemployment\n\nThe reduction of unemployment in many cases are not clearly reflected in more sales. Unless some really correlated effects are seen, no idea how this feature could help","metadata":{}},{"cell_type":"code","source":"var1=\"sum_sales\"\nvar2=\"Unemployment\"\nlist_stores = random.choices(df_agg_train_features.Store.unique(),k=8)\n\n\nwidth=30\nheight=30\nfig = plt.figure(figsize=(width,height), dpi=80)\nx=[421,422,423,424,425,426,427,428]\nk=-1\n\n\nfor store in list_stores:    \n    k+=1\n    ax = fig.add_subplot(x[k])    \n    plot_ts_multix(df=df_agg_train_features[df_agg_train_features.Store==store].sort_values(\"Date\").set_index(\"Date\"),var1=var1,var2=var2)\n    \nplt.show()    ","metadata":{"execution":{"iopub.status.busy":"2021-06-15T05:23:09.305839Z","iopub.execute_input":"2021-06-15T05:23:09.306389Z","iopub.status.idle":"2021-06-15T05:23:12.514967Z","shell.execute_reply.started":"2021-06-15T05:23:09.306351Z","shell.execute_reply":"2021-06-15T05:23:12.514105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## CPI: Consumer Price Index\n\nFor this variable, unless there was an abrupt peak, it does not seem to be promissing. There is a marginal reduction of Christmas peak from one year to another which may be related to some effect of purchasing power, whereas it seems to be the very close to past year in the other months. It really needs futher studies to understand impacts.","metadata":{}},{"cell_type":"code","source":"var1=\"sum_sales\"\nvar2=\"CPI\"\nlist_stores = random.choices(df_agg_train_features.Store.unique(),k=8)\n\n\nwidth=30\nheight=30\nfig = plt.figure(figsize=(width,height), dpi=80)\nx=[421,422,423,424,425,426,427,428]\nk=-1\n\n\nfor store in list_stores:    \n    k+=1\n    ax = fig.add_subplot(x[k])    \n    plot_ts_multix(df=df_agg_train_features[df_agg_train_features.Store==store].sort_values(\"Date\").set_index(\"Date\"),var1=var1,var2=var2)\n    \nplt.show()    ","metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cd3bb929-70e3-468e-961a-7af6d375ca66"},"execution":{"iopub.status.busy":"2021-06-15T05:23:12.516435Z","iopub.execute_input":"2021-06-15T05:23:12.516992Z","iopub.status.idle":"2021-06-15T05:23:16.042621Z","shell.execute_reply.started":"2021-06-15T05:23:12.516951Z","shell.execute_reply":"2021-06-15T05:23:16.041841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Fuel price\n\nSames as CPI there was an increasing from one year to another, which maybe affected prices and sales on Christmas. It also needs carefully studies, since only the Christmas period seemed to be clearly affected","metadata":{}},{"cell_type":"code","source":"var1=\"sum_sales\"\nvar2=\"Fuel_Price\"\nlist_stores = random.choices(df_agg_train_features.Store.unique(),k=8)\n\n\nwidth=30\nheight=30\nfig = plt.figure(figsize=(width,height), dpi=80)\nx=[421,422,423,424,425,426,427,428]\nk=-1\n\n\nfor store in list_stores:    \n    k+=1\n    ax = fig.add_subplot(x[k])    \n    plot_ts_multix(df=df_agg_train_features[df_agg_train_features.Store==store].sort_values(\"Date\").set_index(\"Date\"),var1=var1,var2=var2)\n    \nplt.show()  ","metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2069f4c5-f744-485a-b9c0-0286fe947a6a"},"execution":{"iopub.status.busy":"2021-06-15T05:23:16.043808Z","iopub.execute_input":"2021-06-15T05:23:16.047652Z","iopub.status.idle":"2021-06-15T05:23:19.179253Z","shell.execute_reply.started":"2021-06-15T05:23:16.047614Z","shell.execute_reply":"2021-06-15T05:23:19.176514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Markdowns\n\nThere were no clear insights from them. Some of them seems to be after Christmas, which will not help so much. Markdown3 is related to Black Friday, which gives some insights of peak of sales, but it is also common sense. We could simply mark them as we did for the feature IsHolydayfix. Markdown5 has also some dependency of the main holiday, but also in other intervals where it does to seem to be really correlated with increase in sales.\nMaybe if there is any dependency with departments that appear only in a few weeks of year, they could be interesting for this competition. For real life, not sure, we really need to ask business people about the meaning of these features!","metadata":{}},{"cell_type":"code","source":"var1=\"sum_sales\"\nvar2=\"MarkDown1\"\nlist_stores = random.choices(df_agg_train_features.Store.unique(),k=8)\n\n\nwidth=30\nheight=30\nfig = plt.figure(figsize=(width,height), dpi=80)\nx=[421,422,423,424,425,426,427,428]\nk=-1\n\n\nfor store in list_stores:    \n    k+=1\n    ax = fig.add_subplot(x[k])    \n    plot_ts_multix(df=df_agg_train_features[df_agg_train_features.Store==store].sort_values(\"Date\").set_index(\"Date\"),var1=var1,var2=var2)\n    \nplt.show()  ","metadata":{"execution":{"iopub.status.busy":"2021-06-15T05:23:19.180502Z","iopub.execute_input":"2021-06-15T05:23:19.180948Z","iopub.status.idle":"2021-06-15T05:23:22.328461Z","shell.execute_reply.started":"2021-06-15T05:23:19.18091Z","shell.execute_reply":"2021-06-15T05:23:22.327401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"var1=\"sum_sales\"\nvar2=\"MarkDown2\"\nlist_stores = random.choices(df_agg_train_features.Store.unique(),k=8)\n\n\nwidth=30\nheight=30\nfig = plt.figure(figsize=(width,height), dpi=80)\nx=[421,422,423,424,425,426,427,428]\nk=-1\n\n\nfor store in list_stores:    \n    k+=1\n    ax = fig.add_subplot(x[k])    \n    plot_ts_multix(df=df_agg_train_features[df_agg_train_features.Store==store].sort_values(\"Date\").set_index(\"Date\"),var1=var1,var2=var2)\n    \nplt.show()  ","metadata":{"execution":{"iopub.status.busy":"2021-06-15T05:23:22.329979Z","iopub.execute_input":"2021-06-15T05:23:22.330315Z","iopub.status.idle":"2021-06-15T05:23:25.421993Z","shell.execute_reply.started":"2021-06-15T05:23:22.330279Z","shell.execute_reply":"2021-06-15T05:23:25.421001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"var1=\"sum_sales\"\nvar2=\"MarkDown3\"\nlist_stores = random.choices(df_agg_train_features.Store.unique(),k=8)\n\n\nwidth=30\nheight=30\nfig = plt.figure(figsize=(width,height), dpi=80)\nx=[421,422,423,424,425,426,427,428]\nk=-1\n\n\nfor store in list_stores:    \n    k+=1\n    ax = fig.add_subplot(x[k])    \n    plot_ts_multix(df=df_agg_train_features[df_agg_train_features.Store==store].sort_values(\"Date\").set_index(\"Date\"),var1=var1,var2=var2)\n    \nplt.show() ","metadata":{"execution":{"iopub.status.busy":"2021-06-15T05:23:25.423559Z","iopub.execute_input":"2021-06-15T05:23:25.423948Z","iopub.status.idle":"2021-06-15T05:23:29.41837Z","shell.execute_reply.started":"2021-06-15T05:23:25.423912Z","shell.execute_reply":"2021-06-15T05:23:29.417506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"var1=\"sum_sales\"\nvar2=\"MarkDown4\"\nlist_stores = random.choices(df_agg_train_features.Store.unique(),k=8)\n\n\nwidth=30\nheight=30\nfig = plt.figure(figsize=(width,height), dpi=80)\nx=[421,422,423,424,425,426,427,428]\nk=-1\n\n\nfor store in list_stores:    \n    k+=1\n    ax = fig.add_subplot(x[k])    \n    plot_ts_multix(df=df_agg_train_features[df_agg_train_features.Store==store].sort_values(\"Date\").set_index(\"Date\"),var1=var1,var2=var2)\n    \nplt.show() ","metadata":{"execution":{"iopub.status.busy":"2021-06-15T05:23:29.419723Z","iopub.execute_input":"2021-06-15T05:23:29.426544Z","iopub.status.idle":"2021-06-15T05:23:33.226291Z","shell.execute_reply.started":"2021-06-15T05:23:29.426496Z","shell.execute_reply":"2021-06-15T05:23:33.225512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"var1=\"sum_sales\"\nvar2=\"MarkDown5\"\nlist_stores = random.choices(df_agg_train_features.Store.unique(),k=8)\n\n\nwidth=30\nheight=30\nfig = plt.figure(figsize=(width,height), dpi=80)\nx=[421,422,423,424,425,426,427,428]\nk=-1\n\n\nfor store in list_stores:    \n    k+=1\n    ax = fig.add_subplot(x[k])    \n    plot_ts_multix(df=df_agg_train_features[df_agg_train_features.Store==store].sort_values(\"Date\").set_index(\"Date\"),var1=var1,var2=var2)\n    \nplt.show() ","metadata":{"execution":{"iopub.status.busy":"2021-06-15T05:23:33.227509Z","iopub.execute_input":"2021-06-15T05:23:33.227957Z","iopub.status.idle":"2021-06-15T05:23:36.493318Z","shell.execute_reply.started":"2021-06-15T05:23:33.227908Z","shell.execute_reply":"2021-06-15T05:23:36.492309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Correlations\n\nNow some correlation plots, using different date intervals, based on availability and strategy for training process.","metadata":{}},{"cell_type":"markdown","source":"### Date >= 2011-11-11 (Where markdowns are available)\n\nConsideting the targe:\n* as expected lags, avg, min max sales are highly correlated with target.\n* Markdowns 1 and 5 are slightly correlated. I would keep temperature due to it seasonal effent on January, although not very correlated.","metadata":{}},{"cell_type":"code","source":"cols_corr=[]\nfor col in df_train_features.select_dtypes(include=['float64']).columns:\n    if((\"_norm\" not in col) and (\"_Norm\" not in col) and (\"counts\" not in col) and (\"ndist\" not in col)):\n        cols_corr.append(col)\neda.plot_correlation(df_train_features[df_train_features.Date>=\"2011-11-11\"][cols_corr],col1=\"Weekly_Sales\")","metadata":{"execution":{"iopub.status.busy":"2021-06-15T05:23:36.508525Z","iopub.execute_input":"2021-06-15T05:23:36.508908Z","iopub.status.idle":"2021-06-15T05:23:41.561569Z","shell.execute_reply.started":"2021-06-15T05:23:36.50887Z","shell.execute_reply":"2021-06-15T05:23:41.560535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Pair correlations:\n\n* Lags, avgs and so on are very correlated with each other. That's why will of course not use most of them.\n* Markdown 1 and 4 are also correlated with each other.\n* CPI and Price have non neglectable negative correlation with each other, which indicates the dependency of price with Fuel. ","metadata":{}},{"cell_type":"code","source":"cols_corr=[]\nfor col in df_train_features.select_dtypes(include=['float64']).columns:\n    if((\"_norm\" not in col) and (\"_Norm\" not in col) and (\"counts\" not in col) and (\"ndist\" not in col)):\n        cols_corr.append(col)\neda.plot_correlation(df_train_features[df_train_features.Date>=\"2011-11-11\"][cols_corr])","metadata":{"execution":{"iopub.status.busy":"2021-06-15T05:23:41.563293Z","iopub.execute_input":"2021-06-15T05:23:41.563979Z","iopub.status.idle":"2021-06-15T05:24:02.211337Z","shell.execute_reply.started":"2021-06-15T05:23:41.563933Z","shell.execute_reply":"2021-06-15T05:24:02.210319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Final Missing checks with features","metadata":{}},{"cell_type":"code","source":"eda.plot_missing(df_train_features[df_train_features.Date<\"2011-11-11\"],display=[\"Bar Chart\"])","metadata":{"execution":{"iopub.status.busy":"2021-06-15T05:24:02.212949Z","iopub.execute_input":"2021-06-15T05:24:02.213288Z","iopub.status.idle":"2021-06-15T05:24:13.057169Z","shell.execute_reply.started":"2021-06-15T05:24:02.213252Z","shell.execute_reply":"2021-06-15T05:24:13.056085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eda.plot_missing(df_train_features[df_train_features.Date>=\"2011-11-11\"],display=[\"Bar Chart\"])","metadata":{"execution":{"iopub.status.busy":"2021-06-15T05:24:13.05883Z","iopub.execute_input":"2021-06-15T05:24:13.059181Z","iopub.status.idle":"2021-06-15T05:24:20.325339Z","shell.execute_reply.started":"2021-06-15T05:24:13.059144Z","shell.execute_reply":"2021-06-15T05:24:20.324496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Models\n\nSome points for the model.\n1. Since it is not very promissing to use Markdowns in a short time study, I will keep them aside at this POC. Doing so, the available time range for the model will also be extended (from 2011-11 to 2011-02). Using Markdowns with this time range would require hypothesis to deal with missing data.\n2. Starting the train range in 2011-02-05, so that it is possible to work with our strategy of one year lag.\n3. Filter out negative lag_sales and Weekly_Sales for the training process. For the test labels, we assume zero for negative labels.  ","metadata":{}},{"cell_type":"code","source":"filter_dates=None\nfilter_dates=\"2011-02-05\"\n\n\n#min2010-02-05\nfilter_positive=True\nif(filter_dates is not None):\n  print(f\"filter dates: {filter_dates}\")\n  df_train_features_final = df_train_features[df_train_features.Date>=filter_dates]\nelse:\n  df_train_features_final = df_train_features\n\nif(filter_positive):\n  print(f\"filter positive: {filter_positive}\")\n  df_train_features_final = df_train_features_final[df_train_features_final.Weekly_Sales>=0]\n  df_train_features_final = df_train_features_final[df_train_features_final.lag_sales>=0]\nelse:\n  df_train_features_final = df_train_features_final\n\n    ","metadata":{"execution":{"iopub.status.busy":"2021-06-15T05:24:20.326735Z","iopub.execute_input":"2021-06-15T05:24:20.327272Z","iopub.status.idle":"2021-06-15T05:24:20.71473Z","shell.execute_reply.started":"2021-06-15T05:24:20.327234Z","shell.execute_reply":"2021-06-15T05:24:20.713782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train_features_final.IsHolidayfix.unique()","metadata":{"execution":{"iopub.status.busy":"2021-06-15T05:24:20.716148Z","iopub.execute_input":"2021-06-15T05:24:20.716699Z","iopub.status.idle":"2021-06-15T05:24:20.725626Z","shell.execute_reply.started":"2021-06-15T05:24:20.716662Z","shell.execute_reply":"2021-06-15T05:24:20.724921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Transformations:\n* Removing outliers 5% threshold (Pycaret uses SVD for this purpose)\n* Normalize (using zscore) for linear models\n* Using following features:\n 1. Weekly_Sales as target\n 2. Day_Range (beggining and end of month dependency)\n 3. Temperature special for some trends seen in January-February\n 4. lag_sales --> One year lag or moving average as explained before. Specially for seasonal dependency.\n 5. lag_avg_month --> To bring month dependy. We decided to remove Month variable at this version, reducing complexity\n 6. min_sales_history_nonneg --> for lower boundary scale\n 7. max_sales_history --> for higher boundary scale\n    \nWe thought for improvements it would be interesting to try the normalized Weekly_Sales, reducing also the need of min,max sales as variable for the model, capturing more intrinsic shapes of store-dept.    ","metadata":{}},{"cell_type":"code","source":"experiment_name = \"test\" # for Mlflow use\nlog_experiment = False # for Mlflow use\nlog_plots = False# for Mlflow use\nlog_data = False# for Mlflow use\nuse_gpu = True  #yes we use GPUs for Xgboost\nhigh_cardinality_features = None\n\ndata_split_stratify = [] \nbin_numeric_features = []  \n\n\ntrain_size = 0.8\ntransform_target = False\ntransform_target_method = \"box-cox\"\nremove_multicollinearity = False\nmulticollinearity_threshold = 0.9\n\ncombine_rare_levels = False  #\nrare_level_threshold = 0.1\n\nfeature_interaction = False\ninteraction_threshold = 0.01\n\nfeature_selection = False\nfeature_selection_threshold = 0.9\n\nnormalize = True\nnormalize_method = \"zscore\"\n\npca = False\npca_components = 10\n\nremove_outliers = True\noutliers_threshold = 0.05\n\nall_cols=df_train_features.columns\n\nuse_cols=[\"Weekly_Sales\",\"Type\",\"Day_Range\",'Temperature', 'Size',\"Weekly_Sales\",'lag_sales',\"lag_avg_month\",'min_sales_history_nonneg', 'max_sales_history']#\"MarkDown1\",\nnumeric_features = []  #\"IsHolidayw\"\ncategorical_features = [\"Month\"]\nordinal_features = {\"Type\":[\"C\",\"B\",\"A\"]}\n\nignore_features = set(all_cols) - set(use_cols)\ntarget_col=\"Weekly_Sales\"\nfix_imbalance=False ","metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e82a31b4-3c05-41e2-91e2-e6027426aee3"},"execution":{"iopub.status.busy":"2021-06-15T05:24:20.729181Z","iopub.execute_input":"2021-06-15T05:24:20.730537Z","iopub.status.idle":"2021-06-15T05:24:20.742953Z","shell.execute_reply.started":"2021-06-15T05:24:20.730491Z","shell.execute_reply":"2021-06-15T05:24:20.74192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train_features_final.Date.max(),df_train_features_final.Date.min()","metadata":{"execution":{"iopub.status.busy":"2021-06-15T05:24:20.746827Z","iopub.execute_input":"2021-06-15T05:24:20.748949Z","iopub.status.idle":"2021-06-15T05:24:20.848193Z","shell.execute_reply.started":"2021-06-15T05:24:20.748906Z","shell.execute_reply":"2021-06-15T05:24:20.847472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train/Test and validation sample\nStrategies:\n* First separate the data into train_data = 80% and validation(hold) 20%. The validation data will be selected based on date, being the last N months, checking feasibility to forecast months in advance and also having a sample held from the model intrinsic tests.\n\n* From the train_data, randomly select 80% for train and 20% for test. For this task, since we do not use serial patterns, we will split the data randomly, in order to guarantee we have plenty of seasonality.\n\n* Afterwards, the whole data will be used for trainning from 2011-02-05 on.\n\n* For the propose of model selection we will use MAE as score. Implementations of WMAE (like explained in the description, weighting 5 for holidays) with sklearn make_score might be used in next versions of this work in order to improve selections a bit. At this version WMAE will be used only in validation process comparing with MAE results.\n","metadata":{}},{"cell_type":"code","source":"train_fraction=0.8\ntotal = df_train_features_final.count()[0]\ntrain_sample = int(total*train_fraction)","metadata":{"execution":{"iopub.status.busy":"2021-06-15T05:24:20.851727Z","iopub.execute_input":"2021-06-15T05:24:20.853696Z","iopub.status.idle":"2021-06-15T05:24:21.117607Z","shell.execute_reply.started":"2021-06-15T05:24:20.853654Z","shell.execute_reply":"2021-06-15T05:24:21.116693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(total,train_sample)","metadata":{"execution":{"iopub.status.busy":"2021-06-15T05:24:21.121685Z","iopub.execute_input":"2021-06-15T05:24:21.123701Z","iopub.status.idle":"2021-06-15T05:24:21.131683Z","shell.execute_reply.started":"2021-06-15T05:24:21.12365Z","shell.execute_reply":"2021-06-15T05:24:21.130881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndf_train_features_final=df_train_features_final.sort_values(\"Date\")\ndate_val = df_train_features_final[0:train_sample].Date.max()\ntrain_data = df_train_features_final[df_train_features_final.Date<date_val]\nval_data  = df_train_features_final[df_train_features_final.Date>=date_val]","metadata":{"execution":{"iopub.status.busy":"2021-06-15T05:24:21.134659Z","iopub.execute_input":"2021-06-15T05:24:21.13754Z","iopub.status.idle":"2021-06-15T05:24:22.26255Z","shell.execute_reply.started":"2021-06-15T05:24:21.137502Z","shell.execute_reply":"2021-06-15T05:24:22.261641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Checking dates","metadata":{}},{"cell_type":"code","source":"train_data.Date.min(),train_data.Date.max(),train_data.shape","metadata":{"execution":{"iopub.status.busy":"2021-06-15T05:24:22.266715Z","iopub.execute_input":"2021-06-15T05:24:22.268789Z","iopub.status.idle":"2021-06-15T05:24:22.34164Z","shell.execute_reply.started":"2021-06-15T05:24:22.268748Z","shell.execute_reply":"2021-06-15T05:24:22.340891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_data.Date.min(),val_data.Date.max(),val_data.shape","metadata":{"execution":{"iopub.status.busy":"2021-06-15T05:24:22.345441Z","iopub.execute_input":"2021-06-15T05:24:22.347463Z","iopub.status.idle":"2021-06-15T05:24:22.372843Z","shell.execute_reply.started":"2021-06-15T05:24:22.347425Z","shell.execute_reply":"2021-06-15T05:24:22.371994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.shape,val_data.shape","metadata":{"execution":{"iopub.status.busy":"2021-06-15T05:24:22.376545Z","iopub.execute_input":"2021-06-15T05:24:22.378553Z","iopub.status.idle":"2021-06-15T05:24:22.38665Z","shell.execute_reply.started":"2021-06-15T05:24:22.378515Z","shell.execute_reply":"2021-06-15T05:24:22.38587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Pycaret setup for train used for first selections and validations","metadata":{}},{"cell_type":"code","source":"reg_val = pyreg.setup(data=train_data,target=target_col,\n            high_cardinality_features=high_cardinality_features,\n     ignore_features=ignore_features,categorical_features=categorical_features,\n     silent=True, experiment_name=experiment_name,\n     normalize=normalize, normalize_method=normalize_method,\n     rare_level_threshold=rare_level_threshold, combine_rare_levels=combine_rare_levels,\n     html=False, log_experiment=log_experiment, log_plots=log_plots, log_data=log_data,\n     numeric_features=numeric_features,\n     remove_multicollinearity=remove_multicollinearity,\n     multicollinearity_threshold=multicollinearity_threshold,\n     feature_interaction=feature_interaction, interaction_threshold=interaction_threshold,\n     pca=pca, pca_components=pca_components, feature_selection=feature_selection,\n     feature_selection_threshold=feature_selection_threshold, train_size=train_size,\n     use_gpu=use_gpu,\n     remove_outliers=remove_outliers,\n     outliers_threshold=0.05,\n     bin_numeric_features=bin_numeric_features,\n     ordinal_features=ordinal_features,transform_target=transform_target\n     )\n","metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"82e948a9-85a8-474f-a73f-2d3353ad0b96"},"execution":{"iopub.status.busy":"2021-06-15T05:24:22.389553Z","iopub.execute_input":"2021-06-15T05:24:22.392516Z","iopub.status.idle":"2021-06-15T05:24:35.835192Z","shell.execute_reply.started":"2021-06-15T05:24:22.392477Z","shell.execute_reply":"2021-06-15T05:24:35.834114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Sample of tranformed data","metadata":{}},{"cell_type":"code","source":"pyreg.get_config(\"X_train\").head(30).T\n","metadata":{"execution":{"iopub.status.busy":"2021-06-15T05:24:35.836797Z","iopub.execute_input":"2021-06-15T05:24:35.837362Z","iopub.status.idle":"2021-06-15T05:24:35.887189Z","shell.execute_reply.started":"2021-06-15T05:24:35.837324Z","shell.execute_reply":"2021-06-15T05:24:35.886259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Comparing Defaults: \"lightgbm\",\"ridge\",lr\",\"xgboost\"","metadata":{}},{"cell_type":"markdown","source":"Using the default configs, which might be a bit misleading of course, Xgboost seems to be the best choice for this POC. We will use the linear model from sklearn(lr) for benchmarking, along with the use of lag_sales.","metadata":{}},{"cell_type":"code","source":"start_time = time.time()\nbest = pyreg.compare_models(include=[\"lightgbm\",\"ridge\",\"lr\",\"xgboost\"],fold=2,sort=\"MAE\")\nprint(\"--- %s seconds ---\" % (time.time() - start_time))","metadata":{"execution":{"iopub.status.busy":"2021-06-15T05:24:35.890841Z","iopub.execute_input":"2021-06-15T05:24:35.891243Z","iopub.status.idle":"2021-06-15T05:24:48.1903Z","shell.execute_reply.started":"2021-06-15T05:24:35.891205Z","shell.execute_reply":"2021-06-15T05:24:48.189352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best","metadata":{"execution":{"iopub.status.busy":"2021-06-15T05:24:48.194053Z","iopub.execute_input":"2021-06-15T05:24:48.194411Z","iopub.status.idle":"2021-06-15T05:24:48.209386Z","shell.execute_reply.started":"2021-06-15T05:24:48.194359Z","shell.execute_reply":"2021-06-15T05:24:48.208313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## A few plots\n\nThe main features are lags and avgs lags, since they are intrinsic related to scale and seasonal behaviour. The other ones will make a small effect in some cases. An improvement for this model of course is to reduce the use of this feature and find other features related to intrinsic behaviour of the store sales. ","metadata":{}},{"cell_type":"code","source":"pyreg.plot_model(best,plot=\"feature\")","metadata":{"execution":{"iopub.status.busy":"2021-06-15T05:24:48.213093Z","iopub.execute_input":"2021-06-15T05:24:48.215203Z","iopub.status.idle":"2021-06-15T05:24:49.504986Z","shell.execute_reply.started":"2021-06-15T05:24:48.21516Z","shell.execute_reply":"2021-06-15T05:24:49.504116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pyreg.plot_model(best,plot=\"feature_all\")","metadata":{"execution":{"iopub.status.busy":"2021-06-15T05:24:49.50913Z","iopub.execute_input":"2021-06-15T05:24:49.511193Z","iopub.status.idle":"2021-06-15T05:24:50.807823Z","shell.execute_reply.started":"2021-06-15T05:24:49.511145Z","shell.execute_reply":"2021-06-15T05:24:50.806563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"linear_model = pyreg.create_model(\"lr\",fold=3)","metadata":{"execution":{"iopub.status.busy":"2021-06-15T05:24:50.809772Z","iopub.execute_input":"2021-06-15T05:24:50.81045Z","iopub.status.idle":"2021-06-15T05:24:51.19973Z","shell.execute_reply.started":"2021-06-15T05:24:50.810394Z","shell.execute_reply":"2021-06-15T05:24:51.198802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Finalizing Linear model for later use","metadata":{}},{"cell_type":"code","source":"# Finalizing (test+train sample together) for further studies with validation samples\nlinear_model_final = pyreg.finalize_model(linear_model)","metadata":{"execution":{"iopub.status.busy":"2021-06-15T05:24:51.201301Z","iopub.execute_input":"2021-06-15T05:24:51.201895Z","iopub.status.idle":"2021-06-15T05:24:52.044023Z","shell.execute_reply.started":"2021-06-15T05:24:51.201856Z","shell.execute_reply":"2021-06-15T05:24:52.043135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Crating XGboost model","metadata":{}},{"cell_type":"code","source":"#Crating Xgboost model\nstart_time = time.time()\nmodel_train = pyreg.create_model(\"xgboost\",fold=3,max_depth=6,n_estimators=100,learning_rate=0.3)\nprint(\"--- %s seconds ---\" % (time.time() - start_time))","metadata":{"execution":{"iopub.status.busy":"2021-06-15T05:24:52.0455Z","iopub.execute_input":"2021-06-15T05:24:52.046037Z","iopub.status.idle":"2021-06-15T05:25:00.858168Z","shell.execute_reply.started":"2021-06-15T05:24:52.045999Z","shell.execute_reply":"2021-06-15T05:25:00.857276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# First Grid to check Depth\n\nSince depth is deeply related to the complexity of our model.","metadata":{}},{"cell_type":"code","source":"fold=3\n# tune hyperparameters with custom_grid\nparams = {\"max_depth\": [2,4,6,8,10,12,14]\n          }\ntuned_model = pyreg.tune_model(model_train, custom_grid = params,return_tuner=True,return_train_score=True,fold=fold,optimize=\"MAE\")","metadata":{"execution":{"iopub.status.busy":"2021-06-15T05:25:00.859645Z","iopub.execute_input":"2021-06-15T05:25:00.860212Z","iopub.status.idle":"2021-06-15T05:34:51.725209Z","shell.execute_reply.started":"2021-06-15T05:25:00.860174Z","shell.execute_reply":"2021-06-15T05:34:51.724137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tuned_model[0]","metadata":{"execution":{"iopub.status.busy":"2021-06-15T05:34:51.729731Z","iopub.execute_input":"2021-06-15T05:34:51.730086Z","iopub.status.idle":"2021-06-15T05:34:51.744634Z","shell.execute_reply.started":"2021-06-15T05:34:51.730049Z","shell.execute_reply":"2021-06-15T05:34:51.743807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_metrics(tuned_model,save_file=None):\n    \"\"\" This is to get the metrics from train and test\"\"\"\n    rand = tuned_model[1]\n    params = rand.cv_results_[\"params\"][0]\n    data = dict()\n    for name in params.keys():\n        print(name, name.split(\"__\")[1])\n        data[name.split(\"__\")[1]] = [rand.cv_results_[\"params\"][i][name] for i in\n                                     range(len(rand.cv_results_[\"params\"]))]\n\n    data[\"mean_train\"] = rand.cv_results_[\"mean_train_score\"]\n    data[\"mean_test\"] = rand.cv_results_[\"mean_test_score\"]\n    data[\"std_train\"] = rand.cv_results_[\"std_train_score\"]\n    data[\"std_test\"] = rand.cv_results_[\"std_test_score\"]\n    data[\"mean_fit_time\"] = rand.cv_results_[\"mean_fit_time\"]\n\n    df_metrics = pd.DataFrame(data=data)\n    if(save_file is not None):\n        print(f\"Saving file: {save_file}\")\n        df_metrics.to_csv(save_file)\n    return df_metrics    \n\ndef plot_metrics(df_metrics):\n    \"\"\" Plotting Metrics x Depth\"\"\"\n    sns.lineplot(x=\"max_depth\",y=\"mean_train\",data=df_metrics,marker=\"o\",label=\"train\")\n    sns.lineplot(x=\"max_depth\",y=\"mean_test\",data=df_metrics,marker=\"o\",label=\"test\")","metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"470bf412-583b-44cc-ad0e-35a8fcb09f5d"},"execution":{"iopub.status.busy":"2021-06-15T05:34:51.748466Z","iopub.execute_input":"2021-06-15T05:34:51.750589Z","iopub.status.idle":"2021-06-15T05:34:51.76352Z","shell.execute_reply.started":"2021-06-15T05:34:51.750548Z","shell.execute_reply":"2021-06-15T05:34:51.76271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_metrics_depth = get_metrics(tuned_model=tuned_model,save_file=\"metrics_study_xgboost_mae_depth_with_val_sample.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-06-15T05:34:51.769026Z","iopub.execute_input":"2021-06-15T05:34:51.771688Z","iopub.status.idle":"2021-06-15T05:34:51.78866Z","shell.execute_reply.started":"2021-06-15T05:34:51.77165Z","shell.execute_reply":"2021-06-15T05:34:51.787816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Checking Depth dependency\nSince it seems like, the model is not improving information after 8-10 depths, I will keep the chosen depth=10 for validations tests, then I will try to reduce complexity by adding some things like col_samples for hypertune can be done. For future improvements, maybe keeping safe around 6-8 tuning better other parameters. ","metadata":{}},{"cell_type":"code","source":"plot_metrics(df_metrics_depth)","metadata":{"execution":{"iopub.status.busy":"2021-06-15T05:34:51.792345Z","iopub.execute_input":"2021-06-15T05:34:51.794431Z","iopub.status.idle":"2021-06-15T05:34:52.05077Z","shell.execute_reply.started":"2021-06-15T05:34:51.794376Z","shell.execute_reply":"2021-06-15T05:34:52.050006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tuned_model[0]","metadata":{"execution":{"iopub.status.busy":"2021-06-15T05:34:52.054448Z","iopub.execute_input":"2021-06-15T05:34:52.056606Z","iopub.status.idle":"2021-06-15T05:34:52.06917Z","shell.execute_reply.started":"2021-06-15T05:34:52.056568Z","shell.execute_reply":"2021-06-15T05:34:52.068239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pyreg.plot_model(tuned_model[0],plot=\"feature\")","metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"32cede44-d073-4815-91e0-c95324250c16"},"execution":{"iopub.status.busy":"2021-06-15T05:34:52.139743Z","iopub.execute_input":"2021-06-15T05:34:52.14033Z","iopub.status.idle":"2021-06-15T05:35:00.960215Z","shell.execute_reply.started":"2021-06-15T05:34:52.140293Z","shell.execute_reply":"2021-06-15T05:35:00.959351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_model_val = pyreg.finalize_model(tuned_model[0])","metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"45806cb4-6e91-4c6e-b1fa-b704ecaaf519"},"execution":{"iopub.status.busy":"2021-06-15T05:35:00.964563Z","iopub.execute_input":"2021-06-15T05:35:00.96659Z","iopub.status.idle":"2021-06-15T05:38:14.931212Z","shell.execute_reply.started":"2021-06-15T05:35:00.966552Z","shell.execute_reply":"2021-06-15T05:38:14.930336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Plotting on validation \n\nFor the validation, we will compare the model curve with data and a baseline assumption(using only lag 1y information). We expected that our model would provide better performance than a lag expectation.","metadata":{}},{"cell_type":"code","source":"predictions=pyreg.predict_model(final_model_val,data=df_train_features_final)\npredictions_lr=pyreg.predict_model(linear_model_final,data=df_train_features_final)","metadata":{"execution":{"iopub.status.busy":"2021-06-15T05:38:14.93273Z","iopub.execute_input":"2021-06-15T05:38:14.933312Z","iopub.status.idle":"2021-06-15T05:38:23.707927Z","shell.execute_reply.started":"2021-06-15T05:38:14.933273Z","shell.execute_reply":"2021-06-15T05:38:23.707045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def apply_positives(df):\n    if(df[\"Label\"]<0):\n        return 0\n    else:\n        return df[\"Label\"]\npredictions[\"Label\"] =  predictions.apply(apply_positives,axis=1) \npredictions_lr[\"Label\"] =  predictions_lr.apply(apply_positives,axis=1) ","metadata":{"execution":{"iopub.status.busy":"2021-06-15T05:38:23.709415Z","iopub.execute_input":"2021-06-15T05:38:23.709948Z","iopub.status.idle":"2021-06-15T05:38:53.84005Z","shell.execute_reply.started":"2021-06-15T05:38:23.709911Z","shell.execute_reply":"2021-06-15T05:38:53.839007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_stores_week_data =  stores_week(predictions,col=\"Weekly_Sales\").drop(columns=['n_dist_Store_Dept']).sort_values(['Type', 'Date'])\npred_stores_week_label =  stores_week(predictions,col=\"Label\").drop(columns=['n_dist_Store_Dept']).sort_values(['Type', 'Date']).rename(columns={\"avg_sales\": \"avg_sales_label\", \"sum_sales\": \"sum_sales_label\"})\n\npred_stores_week_data_lr =  stores_week(predictions_lr,col=\"Weekly_Sales\").drop(columns=['n_dist_Store_Dept']).sort_values(['Type', 'Date'])\npred_stores_week_label_lr =  stores_week(predictions_lr,col=\"Label\").drop(columns=['n_dist_Store_Dept']).sort_values(['Type', 'Date']).rename(columns={\"avg_sales\": \"avg_sales_label\", \"sum_sales\": \"sum_sales_label\"})\n\npred_stores_week_lag =  stores_week(predictions,col=\"lag_sales\").drop(columns=['n_dist_Store_Dept']).sort_values(['Type', 'Date']).rename(columns={\"avg_sales\": \"avg_sales_lag\", \"sum_sales\": \"sum_sales_lag\"})","metadata":{"execution":{"iopub.status.busy":"2021-06-15T05:38:53.844242Z","iopub.execute_input":"2021-06-15T05:38:53.8446Z","iopub.status.idle":"2021-06-15T05:38:54.827885Z","shell.execute_reply.started":"2021-06-15T05:38:53.844564Z","shell.execute_reply":"2021-06-15T05:38:54.827034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_stores_week_label.columns,pred_stores_week_data.columns","metadata":{"execution":{"iopub.status.busy":"2021-06-15T05:38:54.831952Z","iopub.execute_input":"2021-06-15T05:38:54.833961Z","iopub.status.idle":"2021-06-15T05:38:54.842615Z","shell.execute_reply.started":"2021-06-15T05:38:54.833914Z","shell.execute_reply":"2021-06-15T05:38:54.841846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_stores_week =  pred_stores_week_data.merge(pred_stores_week_label,on=['Type', 'Date',\"Week_Index\"], how=\"left\")\npred_stores_week_lr =  pred_stores_week_data_lr.merge(pred_stores_week_label_lr,on=['Type', 'Date',\"Week_Index\"], how=\"left\")","metadata":{"execution":{"iopub.status.busy":"2021-06-15T05:38:54.846767Z","iopub.execute_input":"2021-06-15T05:38:54.84886Z","iopub.status.idle":"2021-06-15T05:38:54.866546Z","shell.execute_reply.started":"2021-06-15T05:38:54.848819Z","shell.execute_reply":"2021-06-15T05:38:54.865792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_stores_week.columns","metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f68fb55e-d57c-4241-93b9-37f60a2026b6"},"execution":{"iopub.status.busy":"2021-06-15T05:38:54.870221Z","iopub.execute_input":"2021-06-15T05:38:54.872227Z","iopub.status.idle":"2021-06-15T05:38:54.880982Z","shell.execute_reply.started":"2021-06-15T05:38:54.872186Z","shell.execute_reply":"2021-06-15T05:38:54.880097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error #(y_true, y_pred, *, sample_weight=None, multioutput='uniform_average')[source]\n","metadata":{"execution":{"iopub.status.busy":"2021-06-15T05:38:54.885218Z","iopub.execute_input":"2021-06-15T05:38:54.887428Z","iopub.status.idle":"2021-06-15T05:38:54.893207Z","shell.execute_reply.started":"2021-06-15T05:38:54.887368Z","shell.execute_reply":"2021-06-15T05:38:54.892331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"try:\n    del(predictions_val)\nexcept:\n    pass\ntry:\n    del(predictions_val_lr)\nexcept:\n    pass\n\npredictions_val = predictions[predictions.Date>=date_val].copy()\npredictions_val_lr = predictions_lr[predictions_lr.Date>=date_val].copy()","metadata":{"execution":{"iopub.status.busy":"2021-06-15T05:38:54.897364Z","iopub.execute_input":"2021-06-15T05:38:54.89785Z","iopub.status.idle":"2021-06-15T05:38:55.133454Z","shell.execute_reply.started":"2021-06-15T05:38:54.897812Z","shell.execute_reply":"2021-06-15T05:38:55.132536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##  Checking WMAE x MAE\n\nFor the validation sample, xgboost is also better followed by linear model. We also compute lag_sales as predictions.\nWMAE is not too different from MAE, as expected. It seems to be relevant only for selections with small margin gains.","metadata":{}},{"cell_type":"code","source":"def WMAE(df, y, ypred):\n    weights = df.IsHoliday.apply(lambda x: 5 if x else 1)\n    return np.round(np.sum(weights*abs(y-ypred))/(np.sum(weights)), 2)","metadata":{"execution":{"iopub.status.busy":"2021-06-15T05:38:55.137829Z","iopub.execute_input":"2021-06-15T05:38:55.139848Z","iopub.status.idle":"2021-06-15T05:38:55.147059Z","shell.execute_reply.started":"2021-06-15T05:38:55.139797Z","shell.execute_reply":"2021-06-15T05:38:55.146308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Xgboost:\",mean_absolute_error(predictions_val[\"Weekly_Sales\"],predictions_val[\"Label\"]) )\nprint(\"lag_sales:\",mean_absolute_error(predictions_val[\"Weekly_Sales\"],predictions_val[\"lag_sales\"]) )\nprint(\"lr:\",mean_absolute_error(predictions_val_lr[\"Weekly_Sales\"],predictions_val_lr[\"Label\"]) )","metadata":{"execution":{"iopub.status.busy":"2021-06-15T05:38:55.754109Z","iopub.execute_input":"2021-06-15T05:38:55.756153Z","iopub.status.idle":"2021-06-15T05:38:55.770674Z","shell.execute_reply.started":"2021-06-15T05:38:55.756108Z","shell.execute_reply":"2021-06-15T05:38:55.769639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Xgboost:\",WMAE(predictions_val,predictions_val[\"Weekly_Sales\"],predictions_val[\"Label\"]) )\nprint(\"lag_sales:\",WMAE(predictions_val,predictions_val[\"Weekly_Sales\"],predictions_val[\"lag_sales\"]) )\nprint(\"lr:\",WMAE(predictions_val_lr,predictions_val_lr[\"Weekly_Sales\"],predictions_val_lr[\"Label\"]) )","metadata":{"execution":{"iopub.status.busy":"2021-06-15T05:38:55.774347Z","iopub.execute_input":"2021-06-15T05:38:55.776331Z","iopub.status.idle":"2021-06-15T05:38:55.900827Z","shell.execute_reply.started":"2021-06-15T05:38:55.776289Z","shell.execute_reply":"2021-06-15T05:38:55.900035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Checking the sum of sales for grouping by Store Type\n\nThe motivation here is to study the overall distributions segmented by store type, which we have seen has an important effect on time series, especially considering the type C, which does not appear to have a well distributed time series. \nConsidering the plots, we see that the trained distribution seems to interpret quite well the curve for types A and B, having a distribution fluctuating up and down around the data, as expected (also considering the hold-out validation data). For type C, it seems to under predict the growing behaviour seen after WeekIndex 100.\nThe lag distributon seems to over predict the peak and under predict other region and describes badly the Type C stores.","metadata":{"execution":{"iopub.status.busy":"2021-06-14T22:49:24.711895Z","iopub.execute_input":"2021-06-14T22:49:24.712303Z","iopub.status.idle":"2021-06-14T22:49:24.718319Z","shell.execute_reply.started":"2021-06-14T22:49:24.712233Z","shell.execute_reply":"2021-06-14T22:49:24.717454Z"}}},{"cell_type":"code","source":"for typestore in[\"A\",\"B\",\"C\"]:\n    var1=\"sum_sales\"\n    var2=\"sum_sales_label\"\n    var3=\"sum_sales_lag\"\n    pred_stores_week_val = pred_stores_week[pred_stores_week.Date>=date_val]\n    pred_stores_week_train = pred_stores_week[pred_stores_week.Date<date_val]\n    \n    pred_stores_week_val_lr = pred_stores_week_lr[pred_stores_week_lr.Date>=date_val]\n    pred_stores_week_train_lr = pred_stores_week_lr[pred_stores_week_lr.Date<date_val]\n    \n    width = 25\n    height = 10\n    plt.figure(figsize=(width,height))\n    plot_ts(df=pred_stores_week[pred_stores_week.Type==typestore].set_index(\"Week_Index\"),var=var1,label=\"data\")\n    plot_ts(df=pred_stores_week_lag[pred_stores_week_lag.Type==typestore].set_index(\"Week_Index\"),var=var3,label=\"lag\")\n    plot_ts(df=pred_stores_week_train[pred_stores_week_train.Type==typestore].set_index(\"Week_Index\"),var=var2,label=\"train labels\")\n    plot_ts(df=pred_stores_week_val[pred_stores_week_val.Type==typestore].set_index(\"Week_Index\"),var=var2,label=\"validation labels\",color=\"black\")\n\n\n    plt.legend()","metadata":{"execution":{"iopub.status.busy":"2021-06-15T05:38:55.904507Z","iopub.execute_input":"2021-06-15T05:38:55.906478Z","iopub.status.idle":"2021-06-15T05:38:57.085295Z","shell.execute_reply.started":"2021-06-15T05:38:55.906435Z","shell.execute_reply":"2021-06-15T05:38:57.084484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for typestore in[\"A\",\"B\",\"C\"]:\n    var1=\"avg_sales\"\n    var2=\"avg_sales_label\"\n    var3=\"avg_sales_lag\"\n    pred_stores_week_val = pred_stores_week[pred_stores_week.Date>date_val]\n    pred_stores_week_train = pred_stores_week[pred_stores_week.Date<=date_val]\n    width = 25\n    height = 10\n    plt.figure(figsize=(width,height))\n    plot_ts(df=pred_stores_week[pred_stores_week.Type==typestore].set_index(\"Week_Index\"),var=var1,label=\"data\")\n    plot_ts(df=pred_stores_week_lag[pred_stores_week_lag.Type==typestore].set_index(\"Week_Index\"),var=var3,label=\"lag\")\n    plot_ts(df=pred_stores_week_train[pred_stores_week_train.Type==typestore].set_index(\"Week_Index\"),var=var2,label=\"train labels\")\n    plot_ts(df=pred_stores_week_val[pred_stores_week_val.Type==typestore].set_index(\"Week_Index\"),var=var2,label=\"validation labels\",color=\"black\")\n    plt.legend()","metadata":{"execution":{"iopub.status.busy":"2021-06-15T05:38:57.08658Z","iopub.execute_input":"2021-06-15T05:38:57.090916Z","iopub.status.idle":"2021-06-15T05:38:58.192607Z","shell.execute_reply.started":"2021-06-15T05:38:57.090876Z","shell.execute_reply":"2021-06-15T05:38:58.191769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Brief Discussion on Linear Models\n\nLooking into the times series from Linear model in comparison with Xgboost, it seems to be slightly worse, specially in the case of Type Store C. Since it does not follow the lag trend well, other nonlinear relations may play an important role.","metadata":{}},{"cell_type":"code","source":"for typestore in[\"A\",\"B\",\"C\"]:\n    var1=\"sum_sales\"\n    var2=\"sum_sales_label\"\n    var3=\"sum_sales_lag\"\n    pred_stores_week_val = pred_stores_week[pred_stores_week.Date>=date_val]\n    pred_stores_week_train = pred_stores_week[pred_stores_week.Date<date_val]\n    \n    pred_stores_week_val_lr = pred_stores_week_lr[pred_stores_week_lr.Date>=date_val]\n    pred_stores_week_train_lr = pred_stores_week_lr[pred_stores_week_lr.Date<date_val]\n    \n    width = 25\n    height = 10\n    plt.figure(figsize=(width,height))\n    plot_ts(df=pred_stores_week[pred_stores_week.Type==typestore].set_index(\"Week_Index\"),var=var1,label=\"data\")\n\n    \n    \n    plot_ts(df=pred_stores_week_train[pred_stores_week_train.Type==typestore].set_index(\"Week_Index\"),var=var2,label=\"train labels\",color=\"red\")\n    plot_ts(df=pred_stores_week_val[pred_stores_week_val.Type==typestore].set_index(\"Week_Index\"),var=var2,label=\"validation labels\",color=\"black\")\n\n    \n    plot_ts(df=pred_stores_week_train_lr[pred_stores_week_train_lr.Type==typestore].set_index(\"Week_Index\"),var=var2,label=\"train labels lr\",color=\"green\")\n    plot_ts(df=pred_stores_week_val_lr[pred_stores_week_val_lr.Type==typestore].set_index(\"Week_Index\"),var=var2,label=\"validation labels lr\",color=\"magenta\")\n\n    plt.legend()","metadata":{"execution":{"iopub.status.busy":"2021-06-15T05:38:58.195493Z","iopub.execute_input":"2021-06-15T05:38:58.198292Z","iopub.status.idle":"2021-06-15T05:38:59.590884Z","shell.execute_reply.started":"2021-06-15T05:38:58.198252Z","shell.execute_reply":"2021-06-15T05:38:59.590008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for typestore in[\"A\",\"B\",\"C\"]:\n    var1=\"avg_sales\"\n    var2=\"avg_sales_label\"\n    var3=\"avg_sales_lag\"\n    pred_stores_week_val = pred_stores_week[pred_stores_week.Date>=date_val]\n    pred_stores_week_train = pred_stores_week[pred_stores_week.Date<date_val]\n    \n    pred_stores_week_val_lr = pred_stores_week_lr[pred_stores_week_lr.Date>=date_val]\n    pred_stores_week_train_lr = pred_stores_week_lr[pred_stores_week_lr.Date<date_val]\n    \n    width = 25\n    height = 10\n    plt.figure(figsize=(width,height))\n    plot_ts(df=pred_stores_week[pred_stores_week.Type==typestore].set_index(\"Week_Index\"),var=var1,label=\"data\")\n\n    \n    \n    plot_ts(df=pred_stores_week_train[pred_stores_week_train.Type==typestore].set_index(\"Week_Index\"),var=var2,label=\"train labels\",color=\"red\")\n    plot_ts(df=pred_stores_week_val[pred_stores_week_val.Type==typestore].set_index(\"Week_Index\"),var=var2,label=\"validation labels\",color=\"black\")\n\n    \n    plot_ts(df=pred_stores_week_train_lr[pred_stores_week_train_lr.Type==typestore].set_index(\"Week_Index\"),var=var2,label=\"train labels lr\",color=\"green\")\n    plot_ts(df=pred_stores_week_val_lr[pred_stores_week_val_lr.Type==typestore].set_index(\"Week_Index\"),var=var2,label=\"validation labels lr\",color=\"magenta\")\n\n    plt.legend()","metadata":{"execution":{"iopub.status.busy":"2021-06-15T05:38:59.594851Z","iopub.execute_input":"2021-06-15T05:38:59.596819Z","iopub.status.idle":"2021-06-15T05:39:00.79073Z","shell.execute_reply.started":"2021-06-15T05:38:59.596781Z","shell.execute_reply":"2021-06-15T05:39:00.789848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Finalize Model\n\nLet's finalize the model using the whole data from 2011-02-05 on.","metadata":{}},{"cell_type":"code","source":"reg_final = pyreg.setup(data=df_train_features_final,target=target_col,\n            high_cardinality_features=high_cardinality_features,\n     ignore_features=ignore_features,categorical_features=categorical_features,\n     silent=True, experiment_name=experiment_name,\n     normalize=normalize, normalize_method=normalize_method,\n     rare_level_threshold=rare_level_threshold, combine_rare_levels=combine_rare_levels,\n     html=False, log_experiment=log_experiment, log_plots=log_plots, log_data=log_data,\n     numeric_features=numeric_features,\n     remove_multicollinearity=remove_multicollinearity,\n     multicollinearity_threshold=multicollinearity_threshold,\n     feature_interaction=feature_interaction, interaction_threshold=interaction_threshold,\n     pca=pca, pca_components=pca_components, feature_selection=feature_selection,\n     feature_selection_threshold=feature_selection_threshold, train_size=train_size,\n     use_gpu=use_gpu,\n     remove_outliers=remove_outliers,\n     outliers_threshold=0.05,\n     bin_numeric_features=bin_numeric_features,\n     ordinal_features=ordinal_features,transform_target=transform_target\n     )","metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a4c668bc-3fdc-4b7f-86b1-ff0fe050e3eb"},"execution":{"iopub.status.busy":"2021-06-15T05:39:00.794696Z","iopub.execute_input":"2021-06-15T05:39:00.796793Z","iopub.status.idle":"2021-06-15T05:39:17.293548Z","shell.execute_reply.started":"2021-06-15T05:39:00.796753Z","shell.execute_reply":"2021-06-15T05:39:17.292624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Creating generic model for tuning","metadata":{}},{"cell_type":"code","source":"start_time = time.time()\nmodel_train_xgb = pyreg.create_model(\"xgboost\",fold=3,max_depth=6,n_estimators=100,learning_rate=0.3)\nprint(\"--- %s seconds ---\" % (time.time() - start_time))","metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9ebd37a1-5bc2-4424-ba72-80fcbc0b8bfc"},"execution":{"iopub.status.busy":"2021-06-15T05:39:17.297983Z","iopub.execute_input":"2021-06-15T05:39:17.300043Z","iopub.status.idle":"2021-06-15T05:39:26.476243Z","shell.execute_reply.started":"2021-06-15T05:39:17.299994Z","shell.execute_reply":"2021-06-15T05:39:26.475194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Hypertuning \n\nWe will try up to 30 samples of randomized grid. We also worked with Bayes approach using scikit-optimize, which was also worked well. The main purpose here was just to find a first group of parameters. Since we saw more than depth 10 is pointless, we will work with some combination of colsample_bytree(reducing complexity), learning rate and nestimators for depths from 6 to 10.","metadata":{}},{"cell_type":"code","source":"start_time = time.time()\nfold=3\n# tune hyperparameters with custom_grid\nparams = {\"max_depth\": [6,7,8,9,10],\n          \"learning_rate\":[0.1,0.3],\n          \"n_estimators\":[200,100],\n          \"colsample_bytree\":[0.6,0.8,1.0]\n          }\ntuned_model_final = pyreg.tune_model(model_train_xgb, \n                                     custom_grid = params,\n                                     return_tuner=True,\n                                     return_train_score=True,\n                                     fold=fold,\n                                     optimize=\"MAE\",\n                                     n_iter=30)\nprint(\"--- %s seconds ---\" % (time.time() - start_time))","metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f95b6fa3-cf22-4421-84e1-bf9fa18b67cb"},"execution":{"iopub.status.busy":"2021-06-15T05:39:26.480111Z","iopub.execute_input":"2021-06-15T05:39:26.48049Z","iopub.status.idle":"2021-06-15T06:01:58.528158Z","shell.execute_reply.started":"2021-06-15T05:39:26.48045Z","shell.execute_reply":"2021-06-15T06:01:58.527184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Getting final model","metadata":{"execution":{"iopub.status.busy":"2021-06-15T01:35:15.373978Z","iopub.execute_input":"2021-06-15T01:35:15.374301Z","iopub.status.idle":"2021-06-15T01:35:15.379744Z","shell.execute_reply.started":"2021-06-15T01:35:15.374269Z","shell.execute_reply":"2021-06-15T01:35:15.378435Z"}}},{"cell_type":"code","source":"tuned_model_final[0]","metadata":{"execution":{"iopub.status.busy":"2021-06-15T06:01:58.529465Z","iopub.execute_input":"2021-06-15T06:01:58.529947Z","iopub.status.idle":"2021-06-15T06:01:58.538965Z","shell.execute_reply.started":"2021-06-15T06:01:58.52991Z","shell.execute_reply":"2021-06-15T06:01:58.53781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tuned_model_final[1] ","metadata":{"execution":{"iopub.status.busy":"2021-06-15T06:01:58.540259Z","iopub.execute_input":"2021-06-15T06:01:58.54064Z","iopub.status.idle":"2021-06-15T06:01:58.558513Z","shell.execute_reply.started":"2021-06-15T06:01:58.540602Z","shell.execute_reply":"2021-06-15T06:01:58.557571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_metrics_final = get_metrics(tuned_model=tuned_model_final,save_file=\"metrics_study_xgboost_mae_final.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-06-15T06:01:58.560453Z","iopub.execute_input":"2021-06-15T06:01:58.562144Z","iopub.status.idle":"2021-06-15T06:01:58.575793Z","shell.execute_reply.started":"2021-06-15T06:01:58.562104Z","shell.execute_reply":"2021-06-15T06:01:58.574692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_metrics_final","metadata":{"execution":{"iopub.status.busy":"2021-06-15T06:01:58.577454Z","iopub.execute_input":"2021-06-15T06:01:58.577789Z","iopub.status.idle":"2021-06-15T06:01:58.60228Z","shell.execute_reply.started":"2021-06-15T06:01:58.577755Z","shell.execute_reply":"2021-06-15T06:01:58.601367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_metrics_final.sort_values(\"mean_test\")","metadata":{"execution":{"iopub.status.busy":"2021-06-15T06:01:58.603647Z","iopub.execute_input":"2021-06-15T06:01:58.60397Z","iopub.status.idle":"2021-06-15T06:01:58.629314Z","shell.execute_reply.started":"2021-06-15T06:01:58.603937Z","shell.execute_reply":"2021-06-15T06:01:58.628444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_model_xgb = pyreg.finalize_model(tuned_model_final[0])\n","metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"93b5c6dd-2a91-4639-a201-51987099abee"},"execution":{"iopub.status.busy":"2021-06-15T06:01:58.630838Z","iopub.execute_input":"2021-06-15T06:01:58.631267Z","iopub.status.idle":"2021-06-15T06:05:52.847107Z","shell.execute_reply.started":"2021-06-15T06:01:58.63123Z","shell.execute_reply":"2021-06-15T06:05:52.84625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions_xgb=pyreg.predict_model(final_model_xgb,df_test_features[df_test_features.Date>='2012-11-02'])\n# predictions_lr=pyreg.predict_model(final_model_lr,df_test_features[df_test_features.Date>='2012-11-02']) # for linear","metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"54ab06c6-4f6d-4ccb-8291-aae78bbd383a"},"execution":{"iopub.status.busy":"2021-06-15T06:05:52.848177Z","iopub.execute_input":"2021-06-15T06:05:52.848517Z","iopub.status.idle":"2021-06-15T06:05:54.700331Z","shell.execute_reply.started":"2021-06-15T06:05:52.848483Z","shell.execute_reply":"2021-06-15T06:05:54.699494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_sample_pyreg_xgb = df_sample.drop(columns=[\"Weekly_Sales\"]).merge(predictions_xgb,on=[\"Id\"],how=\"inner\")\n# df_sample_pyreg_lr = df_sample.drop(columns=[\"Weekly_Sales\"]).merge(predictions_lr,on=[\"Id\"],how=\"inner\") #for linear ","metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8ffb803c-54e2-4cce-9f7f-3bd86a0efcb5"},"execution":{"iopub.status.busy":"2021-06-15T06:05:54.701619Z","iopub.execute_input":"2021-06-15T06:05:54.701944Z","iopub.status.idle":"2021-06-15T06:05:54.97737Z","shell.execute_reply.started":"2021-06-15T06:05:54.70191Z","shell.execute_reply":"2021-06-15T06:05:54.976514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_sample_pyreg_xgb[\"Weekly_Sales\"] = df_sample_pyreg_xgb[\"Label\"]\n# df_sample_pyreg_lr[\"Weekly_Sales\"] = df_sample_pyreg_lr[\"Label\"]","metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"32241f6e-b0b7-4b0b-a021-7492846539f0"},"execution":{"iopub.status.busy":"2021-06-15T06:05:54.978626Z","iopub.execute_input":"2021-06-15T06:05:54.979133Z","iopub.status.idle":"2021-06-15T06:05:54.983953Z","shell.execute_reply.started":"2021-06-15T06:05:54.979095Z","shell.execute_reply":"2021-06-15T06:05:54.98295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Fixing any negative weekly sales","metadata":{}},{"cell_type":"code","source":"def apply_non_zeros(sales):\n  if(sales<0):\n    return 0\n  else:\n    return sales","metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a7a028af-d0ac-4513-9ba0-94f7f788a2b2"},"execution":{"iopub.status.busy":"2021-06-15T06:05:54.985251Z","iopub.execute_input":"2021-06-15T06:05:54.985772Z","iopub.status.idle":"2021-06-15T06:05:54.996195Z","shell.execute_reply.started":"2021-06-15T06:05:54.985737Z","shell.execute_reply":"2021-06-15T06:05:54.995261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_sample_pyreg_zeros_xgb=df_sample_pyreg_xgb.copy()\ndf_sample_pyreg_zeros_xgb[\"Weekly_Sales\"] = df_sample_pyreg_zeros_xgb.apply(lambda x:apply_non_zeros(x[\"Weekly_Sales\"]),axis=1)","metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"210fadb6-53ca-466f-b430-1dfc8d57c3da"},"execution":{"iopub.status.busy":"2021-06-15T06:05:54.997513Z","iopub.execute_input":"2021-06-15T06:05:54.998018Z","iopub.status.idle":"2021-06-15T06:05:58.866907Z","shell.execute_reply.started":"2021-06-15T06:05:54.997956Z","shell.execute_reply":"2021-06-15T06:05:58.866057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_sample_pyreg_zeros_xgb[[\"Id\",\"Weekly_Sales\"]].to_csv('/kaggle/working/final_hyper_xgb.csv',index=False)\n","metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"640891ef-3556-441a-96b4-3343167b758d"},"execution":{"iopub.status.busy":"2021-06-15T06:05:58.868096Z","iopub.execute_input":"2021-06-15T06:05:58.868478Z","iopub.status.idle":"2021-06-15T06:05:59.289953Z","shell.execute_reply.started":"2021-06-15T06:05:58.868443Z","shell.execute_reply":"2021-06-15T06:05:59.289091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_sample_pyreg_zeros_xgb.head(2)","metadata":{"execution":{"iopub.status.busy":"2021-06-15T06:05:59.29121Z","iopub.execute_input":"2021-06-15T06:05:59.291594Z","iopub.status.idle":"2021-06-15T06:05:59.32161Z","shell.execute_reply.started":"2021-06-15T06:05:59.291557Z","shell.execute_reply":"2021-06-15T06:05:59.320671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Output with lag_sales","metadata":{}},{"cell_type":"code","source":"# df_sample_pyreg_lag=df_sample_pyreg_xgb\n# df_sample_pyreg_lag[\"Weekly_Sales\"]=df_sample_pyreg_lag[\"lag_sales\"]\n# df_sample_pyreg_lag_zeros=df_sample_pyreg_lag.copy()\n# df_sample_pyreg_lag_zeros[\"Weekly_Sales\"] = df_sample_pyreg_lag_zeros.apply(lambda x:apply_non_zeros(x[\"Weekly_Sales\"]),axis=1)\n# df_sample_pyreg_lag_zeros[[\"Id\",\"Weekly_Sales\"]].to_csv('/kaggle/working/final_lag_zeros.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-15T06:05:59.323029Z","iopub.execute_input":"2021-06-15T06:05:59.323367Z","iopub.status.idle":"2021-06-15T06:05:59.327257Z","shell.execute_reply.started":"2021-06-15T06:05:59.323331Z","shell.execute_reply":"2021-06-15T06:05:59.326296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Final Scores:\n\n* Xgboost --> 2998.46 (Private) Public(2848.31)\n* Lag Sales --> 3025.84 (Private) Public(2943.89) \n* Linear Model --> 3010.27 (Private) Public(3088.29) \n","metadata":{}},{"cell_type":"markdown","source":"# Discussion\n\nIn general the model with Xgboost retained better scores in all cases. In the Public data the difference was higher, maybe because with the depth=10 we overfitted a bit this sample. \nThe Lag sales were even better than linear models in the public data. In the private one it was the opposite, which means that the last part of data had less dependency with previous year result. Maybe they hid more Store Type C data, which were more difficult to find a good trend based on previous years.\n\nI think much work still need to be done on feature engineering, specially generating better features and reducing a bit the use of averages and lags. Important things to be considered as evolution:\n\n* Maybe Use norms instead of absolute weekly sales\n* Understand better missing Dates for Store-Depts.\n* Understand specific behviour of type C stores.\n* Understand better how to use markdowns.\n* Maybe clusterize store-depts (Type A, B and C maybe are not enough) or find dependencies of specific stores or Depts.\n* With parallel computing, maybe prepare prefits for single Store-Depts.\n* Use of Sequential analysis as LSTM may be interesting .\n* improve use of wmae or check carefully its importance.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}