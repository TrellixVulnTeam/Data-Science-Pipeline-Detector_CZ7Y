{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Objetivo: "},{"metadata":{},"cell_type":"markdown","source":"Temos como principal objetivo da competição, fazer a predição de vendas das lojas e departamentos do Walmart, utilizando variáveis como tamanho da loja, se existe feriado ou não naquela semana entre outras que estão disponíveis nas bases fornecidas pela própria empresa e listadas abaixo\n\n\n*Abaixo, segue descrição dos dados.*\n"},{"metadata":{},"cell_type":"markdown","source":"stores.csv: This file contains anonymized information about the 45 stores, indicating the type and size of store.\n\ntrain.csv: This is the historical training data, which covers to 2010-02-05 to 2012-11-01. Within this file you will find the following fields:\n\n    Store - the store number\n    Dept - the department number\n    Date - the week\n    Weekly_Sales -  sales for the given department in the given store\n    IsHoliday - whether the week is a special holiday week\n\ntest.csv: This file is identical to train.csv, except we have withheld the weekly sales. You must predict the sales for each triplet of store, department, and date in this file.\n\nfeatures.csv: This file contains additional data related to the store, department, and regional activity for the given dates. It contains the following fields:\n\n    Store - the store number\n    Date - the week\n    Temperature - average temperature in the region\n    Fuel_Price - cost of fuel in the region\n    MarkDown1-5 - anonymized data related to promotional markdowns that Walmart is running. MarkDown data is only available after Nov 2011, and is not available for all stores all the time. Any missing value is marked with an NA.\n    CPI - the consumer price index\n    Unemployment - the unemployment rate\n    IsHoliday - whether the week is a special holiday week\n\nFor convenience, the four holidays fall within the following weeks in the dataset (not all holidays are in the data):\n\n- Super Bowl: 12-Feb-10, 11-Feb-11, 10-Feb-12, 8-Feb-13\n- Labor Day: 10-Sep-10, 9-Sep-11, 7-Sep-12, 6-Sep-13\n- Thanksgiving: 26-Nov-10, 25-Nov-11, 23-Nov-12, 29-Nov-13\n- Christmas: 31-Dec-10, 30-Dec-11, 28-Dec-12, 27-Dec-13 "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt \nimport datetime as dt\nfrom matplotlib import pylab\nplt.rcParams[\"figure.figsize\"] = [16,9]\nfrom sklearn.preprocessing import StandardScaler\nimport seaborn as sns \nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn import base\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import make_scorer\nfrom h2o.automl import H2OAutoML\n\n## Importando biblioteca h2o para utilizar o Target Encoder:\nimport h2o\nh2o.init()\nfrom h2o.estimators import H2OTargetEncoderEstimator\nfrom h2o.estimators.gbm import H2OGradientBoostingEstimator\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"## Importando dados "},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(\"../input/walmart-recruiting-store-sales-forecasting/train.csv.zip\")\nfeatures_df = pd.read_csv(\"../input/walmart-recruiting-store-sales-forecasting/features.csv.zip\")\nstores_df = pd.read_csv(\"../input/walmart-recruiting-store-sales-forecasting/stores.csv\")\ntest_df = pd.read_csv(\"../input/walmart-recruiting-store-sales-forecasting/test.csv.zip\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Convertendo nome das colunas para letra minuscula"},{"metadata":{"trusted":true},"cell_type":"code","source":"def columns_to_lower(data):\n    return data.columns.str.lower()\n\n\ntrain_df.columns = columns_to_lower(train_df)\nfeatures_df.columns = columns_to_lower(features_df)\nstores_df.columns = columns_to_lower(stores_df)\ntest_df.columns = columns_to_lower(test_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Join entre os dataframes disponíveis"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = pd.DataFrame()\n\nprint(train_df.shape)\ndataset = train_df.merge(features_df.drop(\"isholiday\",axis = 1) , how = \"left\" , on = [\"date\",\"store\"])\nprint(dataset.shape)\ndataset = dataset.merge(stores_df,how='left', on = [\"store\"])\nprint(dataset.shape)\n\ndataset['store'] = dataset['store'].astype('object')\ndataset['dept']  = dataset['dept'].astype('object')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Convertendo: \n\n* Coluna date para formato de data \n\n* Colunas store and dept para categorical"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset['year'] = dataset['date'].str[:4]\ndataset['date']  = pd.to_datetime(dataset['date'])\ndataset['month'] = dataset['date'].dt.month\n\nfeatures_df['date'] = pd.to_datetime(features_df['date'])\nfeatures_df['week'] = features_df['date'].dt.isocalendar()['week']\nfeatures_df['year'] = features_df['date'].dt.isocalendar()['year']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Verificando valores faltantes "},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame({'missing_quantity' : dataset.isnull().sum().sort_values(ascending = False),\n              'missing_pct' : dataset.isnull().mean().sort_values(ascending = False)})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Todas as colunas Markdown, apresentam pelo menos 60% de valores faltantes, o que pode o que pode ser um problema (ou não). Nesse caso, onde as colunas indicam a presença ou não de uma campanha de desconto, podemos criar uma coluna indicando se houve desconto ou não naquela seamana"},{"metadata":{},"cell_type":"markdown","source":"## Analisando a variável resposta "},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset['weekly_sales'].describe().T.to_frame()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Analisando as medidas resumo da variável resposta, é possível notar a presençã de valores negativos, o que é estranho por se tratar de vendas. Para não \"alterar\" os dados vou seguir com os valores nesse formato mesmo. Porém o mais indicado, nesse caso, é contactar o responsável pelo fornecimento dos dados para ter um melhor entendimento do porque desses valores.\n\n*Ps: Uma outra abordagem possível, seria aplicar o modulo e utilizar os valores absolutos, porém estariamos a alterar os dados sem um conhecimento para justificar tal ação*"},{"metadata":{"trusted":true},"cell_type":"code","source":"def target_description_plot(df):\n\n    plt.figure(figsize = (25,8))\n\n    plt.subplot(131)\n    sns.lineplot(x = 'date' , y = 'weekly_sales' , data = df )\n    plt.title(\"Time Series from Weekly Sales\")\n\n    plt.subplot(132)\n    sns.distplot(df['weekly_sales'],hist = True )\n    plt.title(\"Density from Weekly Sales\")\n\n    plt.subplot(133)\n    sns.boxplot(y = df['weekly_sales'])\n    plt.title(\"Boxplot from Weekly Sales\") ; ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Análise univariada da variável resposta "},{"metadata":{"trusted":true},"cell_type":"code","source":"target_description_plot(dataset)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Com base nos gráficos acima, podemos notar: \n\n* As semanas que antecedem o natal e o feriado de thanksaving, já apresentam um aumento nas vendas, podemos pensar em alguma variável que indique um número n de semanas anteriores a semana desses feriados (ou até mesmo dos demais feriados)\n\n* A distribuição apresenta assimetria positiva\n\n* Com base no boxplot, e considerando que um possível valor discrepante pode existir quando o valor da observação é maior que 3Quartil + 1.5 * IQD (1Quartil - 1.5 * IQD). O que pode impactar, inicialmente, no valor da média de vendas (vamos gerar o gráfico do valor médio e mediano de vendas para verficiar isso)."},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.groupby(['date'])['weekly_sales'].mean().plot()\ndataset.groupby(['date'])['weekly_sales'].median().plot()\nplt.legend(['Media','Mediana']) \nplt.title(\"Media e Mediana de vendas ao longo do tempo\")\nplt.ylabel(\"Vendas\"); \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Como esperado, os possíveis outliers acabam por elevar a média. Vale a pena estar considerando a mediana nas nossas análises para variável resposta,por ser uma medida mais robusta em comparação a média"},{"metadata":{},"cell_type":"markdown","source":"## Analise univariada:"},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in ['temperature','fuel_price', 'cpi', 'unemployment']:\n\n    plt.figure(figsize = (30,6))\n\n    plt.subplot(131)\n    sns.lineplot(x = 'date', y =  i , data = features_df,ci=None)\n    plt.title( i + \" ao longo do tempo\"  )\n    plt.xticks(rotation = 90)\n    \n    plt.subplot(132)\n    sns.boxplot(y =  i , data = features_df)\n    plt.title(\"Boxplot \" + i)\n   \n\n    plt.subplot(133)\n    sns.distplot(features_df[i].dropna())\n\n    plt.show() ; ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Alguns pontos: \n\n- A variável temperatura apresenta um comportamento sazonal ao longo dos anos (como esperado). Uma vez que o inverno americano é bem rigoroso, chegando a dificultar a locomoçã, as vendas podem ser impactadas por esse motivo, além disso vale a pena criar variáveis dummy indicando a estação do ano ?\n\n- A variável fuel_price, apresenta um compartamento sazonal também, apresentando maiores valores próximo ao mês de maio. Além disso, ela sofreu uma um aumento relevante do ano de 2010 para 2011/2012. O que pode impactar as vendas, uma vez que o custo de locomoção/logística aumenta.\n\n- CPI mede a variação de preços a partir da perspectiva do consumidor.\n  É uma maneira fundamental para medir as variações de tendências de compra e a inflação nos Estados Unidos. Valores superiores aos esperados devem ser considerados como positivo/alta para o USD   (sendo o caminho comum para lutar contra a inflação o aumento das taxas, o que pode atrair o investimento estrangeiro), enquanto valores inferiores aos esperados devem ser considerados como    negativos/baixos para o USD. (fonte:[https://br.investing.com/economic-calendar/cpi-733] ). \n  Uma vez que os valores do CPI estão aumentando, seria esse um bom sinal para as vendas  e teria ele influência com as vendas semanais do Walmart ?\n  \n- A variável Unemployement (taxa de desemprego) apresenta queda ao longo dos anos, ou seja, maior parte da população está empregada e pode estar apta a realizar compras. Vale verificar se existe alguma correlação com as vendas! "},{"metadata":{},"cell_type":"markdown","source":"## Variáveis categóricas: "},{"metadata":{"trusted":true},"cell_type":"code","source":"stores_df['type'].value_counts().plot(kind='bar') \nplt.title(\"Quantidade de cada tipo de loja\")  \nplt.ylabel(\"Quantidade\") ; ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (30,6))\n\nplt.subplot(121)\nsns.boxplot(y = stores_df['size'])\nplt.title(\"Boxplot - Tamanho da Loja\")\n\nplt.subplot(122)\nsns.distplot(stores_df['size'])\nplt.title(\"Densidade - Tamanho da loja\"); ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Nenuma loja aparenta ter um valor discrepante em relação as demais (sem presençã de possíveis outliers)"},{"metadata":{},"cell_type":"markdown","source":"## Análise Bivariada:"},{"metadata":{},"cell_type":"markdown","source":"## Matriz de correlação"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_columns = dataset.select_dtypes(['int','float64']).columns\n# multicolinearidade\nplt.figure(figsize = (20,5))\nsns.heatmap(data = dataset[num_columns].corr() , annot=True) ;","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* A variável weekly sales não apresenta nenhum indício de correlação linear forte com as demais variávei numéricas, a variável size é a que apresenta o maior indice (0.24). Mas além de correlação linear, será que alguma outra correlação pode existir nos dados ?\n\n* As variáveis Markdown1 e Markdown4 apresentam uma correlação forte (-0.82), possivelmente vamos utilizar apenas uma delas no processo de modelagem\n\n"},{"metadata":{},"cell_type":"markdown","source":"## SccaterPlot das variáveis continuas"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_scatter( var ):\n    if var != 'weekly_sales':\n        sns.scatterplot(x = var , y = 'weekly_sales' , data = dataset)\n        plt.title(var + \" weekly_sales\")\n        plt.show() ; ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in num_columns:\n    plot_scatter(i) ; ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Com base nos gráficos acima não foi possível identificar nenhuma relação explicita entre as variáveis. O unico ponto que pode ser relevante aqui, é em relação a temperatura, onde podemos observar um certo aumento nas vendas a partir dos 20 Graus mas nada muito relevante que possa gerar algum insight."},{"metadata":{},"cell_type":"markdown","source":"## Variáveis categóricas"},{"metadata":{"trusted":true},"cell_type":"code","source":"def cat_plot(var_cat):\n    sns.boxplot(x = var_cat , y = 'weekly_sales' , data = dataset) \n    plt.title(\"Boxplot weekly sales por \" + var_cat) \n    plt.show(); ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_features = dataset.select_dtypes(['object']).columns\nfor i in categorical_features:\n    cat_plot(i)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* A maioria das lojas apresenta uma mediana de vendas proximas, algumas com maior presença de possíveis outlier's.\n\n* Já para os departamentos, é possível notar que alguns desses apresentam uma mediana maior em relação a outros. Além disso, a elevada quantidade de departamentos pode levar ao overfitting, podemos pensar em agrupa-los de alguma maneira. \n\n* Loja do tipo A, apresentam uma média e uma mediana de vendas maior em comparação as demais. Vamos verificar o tamanho da loja, por tipo de loja "},{"metadata":{},"cell_type":"markdown","source":"## Tamnho da loja por tipo de loja e vendas por tipo de loja \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (25,8))\nplt.subplot(121)\nsns.lineplot(x = 'date',y = 'weekly_sales',hue='type',data=dataset.groupby(['date','store','type'])['weekly_sales'].median().reset_index(),ci = None)\n\n\nplt.subplot(122)\nsns.boxplot(x = 'type',\n            y = 'size' ,\n            data = dataset) \nplt.title(\"Boxplot: Tamanho da loja por tipo de loja \"); ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As lojas do tipo A em gerale apresentam a venda mediana maior ao longo do tempo, são maiores comparadas com as demais, já a lojas do tipo C são as menores e apresentam a menor mediana de vendas ao longo do tempo. Seria algum tipo de loja Premium ou Express (caso exista esse tipo de loja na rede) "},{"metadata":{},"cell_type":"markdown","source":"## Criando estação do ano:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def getSeason(x):\n    \n    if (x > 11 or x <= 3):\n        return \"winter\"\n    elif ( x == 4 or x == 5):\n        return \"spring\"\n    elif (x >=6 and x <= 9):\n        return \"summer\"\n    else:\n        return \"fall\"\n    \ndataset['season'] = dataset['month'].apply(getSeason)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Variável: Departamento"},{"metadata":{"trusted":true},"cell_type":"code","source":"\"Como visto, a variável departamento apresenta {} categorias distintas para tentar melhorar isso, vamos tentar agrupa-las de alguma maneira\".format(dataset['dept'].nunique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modelagem "},{"metadata":{},"cell_type":"markdown","source":"Usualmente utilizamos a função train_test_split para verificar a perfomance do modelo na base de teste, nesse caso vamos separar com base em datas\n\nA distribuição não ficou como é feito geralmente (70% treino/30%teste), pois é interessante avaliar como o modelo irá performar em feriados, principalmente no natal e thanksaving"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = dataset.query(\"date < '2011-10-01'\")\ntest  = dataset.query(\"date >= '2011-10-01'\") \ntrain.fillna(0,inplace=True)\ntest.fillna(0,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Criando variável grupo departamento:"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.groupby(['dept'])['weekly_sales'].median().sort_values().plot(kind='bar') ; ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def group_dept(x):\n    if x < 10000:\n        return 'C'\n    elif x <20000:\n        return 'B'\n    else:\n        return 'A'\n\n    \n    \ntrain_dept = train.groupby(['dept'])['weekly_sales'].median().sort_values().reset_index()\ntrain_dept['grupo_dept'] = train_dept['weekly_sales'].apply(group_dept)\ntrain = train.merge(train_dept[['dept','grupo_dept']].drop_duplicates(),on = 'dept')\ntest  = test.merge(train_dept[['dept','grupo_dept']].drop_duplicates(),on='dept')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Para agrupar a variável departamento, vamos nos basear na mediana de vendas de cada departamento agrupando-os com base e valores próximos. Ainda poderia ser pensado em clusterizarmos esses departmentos utilizando algum método de agrupamento (K-Means,DbScan, entre outros...). Mas por hora vamos seguir dessa maneira"},{"metadata":{},"cell_type":"markdown","source":"## Função para calculo do erro personalizado"},{"metadata":{"trusted":true},"cell_type":"code","source":"def WMAE(data, real, predicted):\n    weights = data['isholiday_True'].apply(lambda x: 5 if x == 1 else 1)\n\n    return np.round(np.sum(weights*abs(real-predicted))/(np.sum(weights)), 2)\n\n\n\n# Erro personalizado para utilizar no grid search:\nerro = make_scorer(WMAE,greater_is_better = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preparando o dataset para testar alguns modelos "},{"metadata":{"trusted":true},"cell_type":"code","source":"def prepdata(data ,categorical_var,continuous_var):\n    \n    \n    onehot = OneHotEncoder(handle_unknown='error',sparse = False,drop='first')\n    onehot.fit(data[categorical_var])\n    df_cat = pd.DataFrame( onehot.transform( data[categorical_var] ) , columns = onehot.get_feature_names(categorical_var) )\n    \n    Standard = StandardScaler()\n    Standard.fit(train[numerical_features]) \n    df_standard = pd.DataFrame( Standard.transform(data[numerical_features]) , columns = list(numerical_features) )\n    \n    data_train = pd.concat( [df_standard,df_cat] , axis = 1 )\n    data_train['target'] = data[target]\n    \n    X_train = data_train.drop('target',axis=1)\n    y_train = data_train['target']\n    \n    \n    return X_train,y_train\n\ndef fit_model(X,y):\n    \n    fit_LR = LinearRegression().fit( X , np.ravel(y) )\n    fit_knn = KNeighborsRegressor( n_neighbors = 10 ).fit( X, np.ravel(y) )\n    fit_Rf = RandomForestRegressor(min_samples_split=50,max_depth=10).fit( X , np.ravel(y) )\n    fit_GB = GradientBoostingRegressor(min_samples_split=50,max_depth=10).fit( X , np.ravel(y) )\n    \n    \n    return fit_LR,fit_knn,fit_Rf,fit_GB\n\n\ndef test_model(data ,categorical_var,continuous_var):\n    \n    onehot = OneHotEncoder(handle_unknown='error',sparse = False,drop='first')\n    onehot.fit(train[categorical_var])\n    df_cat = pd.DataFrame( onehot.transform( data[categorical_var] ) , columns = onehot.get_feature_names(categorical_var) )\n    \n    Standard = StandardScaler()\n    Standard.fit(data[numerical_features]) \n    df_standard = pd.DataFrame( Standard.transform(data[numerical_features]) , columns = list(numerical_features) )\n    \n    data_test = pd.concat( [df_standard,df_cat] , axis = 1 )\n    data_test['target'] = data[target]\n    \n       \n    return data_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Scorando primeiros modelos:"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ncategorical_features = ['store','isholiday','grupo_dept','type']\nnumerical_features = ['temperature','fuel_price','markdown1','markdown2','markdown3','markdown5','cpi','unemployment','size']\n\ntarget = 'weekly_sales'\nX_train,y_train = prepdata(train,categorical_features,numerical_features)\n\n\nfitted_models = []\nfitted_models = fit_model(X_train,y_train)\n\n##################################################################################################################################\n\ntest_df = test_model(test,categorical_features,numerical_features)\nX_test = test_df.drop(\"target\",axis=1)\ny_test = test_df.drop(\"target\",axis=1)\n\n##################################################################################################################################\n\nlist_erro = []\nlist_model = []\nfor i in fitted_models:\n        list_erro.append( WMAE(test_df,test_df['target'],i.predict(X_test) ))\n        list_model.append( str(i) )\n\n\n        \ndisplay( pd.DataFrame(list_erro,list_model) )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Utilizando todas as variáveis fornecidas, podemos observar que o modelo random forest apresenta melhor desempenho comparado aos demais. Vamos veriricar a importância das variáveis do Random Forest e em seguida ajustar os mesmos modelos com a adição de algumas features como: \n\n* Tempo até o feriado mais recente ( a ser criada )\n\n* Estação do ano"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(fitted_models[2].feature_importances_,X_train.columns).sort_values(by = 0 , ascending = False).plot(kind = 'bar') ; ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A variável grupo_dept (referente ao agrupamaneto dos departamentos), ter maior importancia no nosso modelo. Já a variável Markdown apresenta menor importancia, podemos incluisive tentar o ajuste de um modelo sem a mesma!"},{"metadata":{},"cell_type":"markdown","source":"## Scorando modelo reduzido adicionado da feature estação do ano"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ncategorical_features = ['store','isholiday','season','grupo_dept','type']\nnumerical_features = ['temperature','fuel_price','cpi','unemployment','size']\ntarget = 'weekly_sales'\n\n##################################################################################################################################\n\nX_train,y_train = prepdata(train,categorical_features,numerical_features)\nfitted_models = []\nfitted_models = fit_model(X_train,y_train)\n\n##################################################################################################################################\n\ntest_df = test_model(test,categorical_features,numerical_features)\nX_test = test_df.drop(\"target\",axis=1)\ny_test = test_df.drop(\"target\",axis=1)\n\nlist_erro = []\nlist_model = []\nfor i in fitted_models:\n        list_erro.append( WMAE(test_df,test_df['target'],i.predict(X_test) ))\n        list_model.append( str(i) )\n\n\n        \ndisplay(pd.DataFrame(list_erro,list_model) )\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Reduzindo o número de variáveis e utilizando a variável estação do ano, temos uma redução no erro, principalmente no modelo de regressão linear, indicando que a estação do ano (de forma mais geral, existe uma sazonalidade) que existe nos dados. Indicando que um trabalho de feature engineering pode agregar muito ao nosso modelo!"},{"metadata":{},"cell_type":"markdown","source":"## Abaixo também temos um exemplo de uso do framework H2O"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_columns = ['temperature',\n             'fuel_price',\n             'markdown1',\n             'markdown2',\n             'markdown3',\n             'markdown5',\n             'cpi',\n             'unemployment',\n             'size',\n             'store',\n             'isholiday',\n             'season',\n             'grupo_dept',\n             'type']\n\ny_column = 'weekly_sales'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Identify predictors and response\n# #train = h2o.H2OFrame(train)\n# x = x_columns\n# y = y_column\n\n\n# # Run AutoML for 20 base models (limited to 1 hour max runtime by default)\n# aml = H2OAutoML(max_models= 8 , seed=1)\n# aml.train(x=x, y=y, btraining_frame=train)\n\n# # View the AutoML Leaderboard\n# lb = aml.leaderboard\n# lb.head(rows=lb.nrows)  # Print all rows instead of default (10 rows)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Possiveis próximos passos"},{"metadata":{},"cell_type":"markdown","source":"* Utilizar alguma método de otimização de parâmetros, GridSearchCV, RandomizerSearchCv entre outros. \n\n* Utilizar algoritmos mais complexos como XgBoosting,CatBoosting,Adaptative Boosting entre outros. Que dos testados acima, do ponto de vista de \"implementação\", é apenas organizar os dados da maneira aceita e seguir o mesmo processo de modelagem.\n\n* Criação de novas features que podem melhorar a qualidade do nosso modelo.\n"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}