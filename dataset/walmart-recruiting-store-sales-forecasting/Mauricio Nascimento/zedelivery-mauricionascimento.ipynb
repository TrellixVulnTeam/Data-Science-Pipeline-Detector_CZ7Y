{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Solução Case Data Science - Zé Delivery\n\n**Autor**: Maurício Reis do Nascimento\n\nA solução abaixo refere-se à resolução do problema proposto na competição Walmart Recruiting - Store Sales Forecasting. O problema consiste em realizar previsão de demanda para um intervalo de 38 semanas, para vários departamentos de diferentes lojas do Walmart.\n\nÍndice das seções:\n* [Leitura dos Dataframes](#leitura-dfs)\n* [Análise e correção dos tipos de variáveis](#var-types)\n* [EDA](#EDA)\n    - [Análise dos nulos](#anl-nulls)\n    - [Imput de nulos](#imput-nulls)\n* [Análise de Correlações e PPS](#anl-corr)\n    - [Correlações](#corr)\n    - [PPS](#PPS)\n* [Modelo Baseline](#baseline)\n    - [Data Prep](#dataprep)\n    - [Métrica de Avaliação](#eval)\n    - [Função de validação cruzada](#cross-val)\n* [Feature Engineering](#feat-eng)\n    - [Variáveis de distância para o Natal e para o próximo feriado](#natal)\n    - [Classificação dos feriados](#holidays)\n    - [Variável de lag 52 semanas](#lag52)\n    - [Classificação das semanas (clustering)](#cluster-weeks)\n    - [Classificação dos depts (clustering)](#cluster-depts)\n* [Ajuste de modelos](#models)\n    - [Construção do Pipeline](#pipeline)\n* [Ajuste de Hiperparâmetros](#hyperparam)\n* [Submissão](#sub)\n* [Resultado](#result)\n* [Próximos passos/ Oportunidades de melhoria](#next-steps)\n\n\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"!pip install ppscore\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport ppscore as pps\nimport optuna\n\nimport random\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.model_selection import KFold, train_test_split, cross_val_score\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.metrics import make_scorer\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.cluster import KMeans","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='leitura-dfs'></a>\n# Leitura e união dos datasets"},{"metadata":{"trusted":true},"cell_type":"code","source":"path = r'../input/walmart-recruiting-store-sales-forecasting/'\n\ntrain = pd.read_csv(os.path.join(path, 'train.csv.zip'))\ntest = pd.read_csv(os.path.join(path, 'test.csv.zip'))\nstores = pd.read_csv(os.path.join(path, 'stores.csv'))\nfeatures = pd.read_csv(os.path.join(path, 'features.csv.zip'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Juntando os Dataframes de treino e teste para facilitar a manipulação\ntrain['df_label'] = 'train'\ntest['df_label'] = 'test'\n\nfull_df = pd.concat([train, test], axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Agregando ao Dataframe principal variáveis de store\nfull_df = pd.merge(\n    full_df,\n    stores,\n    how='left',\n    on='Store'\n)\n\n# Agregando ao Dataframe principal variáveis de features\nfull_df = pd.merge(\n    full_df,\n    features,\n    how='left',\n    on=[\n        'Store',\n        'Date',\n        'IsHoliday'\n    ]\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"var-types\"></a>\n# Análise e correção dos tipos de variáveis"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Análise dos tipos de dados\nfull_df.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Coluna date está como object\n# Necessário transformá-la para datetime\nfull_df['Date'] = pd.to_datetime(full_df['Date'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Ao olharmos as datas, podemos perceber que não temos todos os dias representados no dataset e, além disso, elas parecem ser igualmente espaçadas. Vamos explorar melhor este tema:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualizando, de forma ordenada, as datas presentes\n# no dataframe\nfull_df['Date'].drop_duplicates().sort_values()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* De fato, parece que os dias estão igualmente espaçados por uma janela de 7 dias. Para comprovar isso, podemos olhar para os dias da semana presentes no Dataframe****"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualizando dias da semana presentes no dataset\nfull_df['Date'].dt.weekday.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Ao verificarmos os dias da semana presentes no dataset, percebemos que todas as datas correspondem à sexta-feira. Além disso, as demais variáveis são agrupadas semanalmente, então vamos transformar a variável de data para semana, criando uma variável para semana do ano, uma variável para semana corrida, uma variável para dia e uma variável para ano****"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cria variável de semana do ano\nfull_df['WeekOfYear'] = full_df.Date.dt.isocalendar().week\n\n# Cria variável referente ao ano\nfull_df['Year'] = full_df.Date.dt.year\n\n# Cria variável referente ao dia do mês\nfull_df['Day'] = full_df.Date.dt.day\n\n# Cria variável referente à semana corrida\nfull_df['WeekSeq'] = full_df['WeekOfYear'] + 52*(full_df['Year'] - full_df['Year'].min())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualizando o dataset\nfull_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='EDA'></a>\n# Análise Exploratória (EDA)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualizando o dataset\ndisplay(full_df.head())\ndisplay(full_df.tail())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Verificando estatísticas das variáveis do Dataframe completo\nfull_df.describe().T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observações:\n\n* Store e Dept: 45 lojas e 99 departamentos diferentes. Isto aponta que o Dataframe tem uma granularidade bem alta, de forma que considerar um modelo para representar cada combinação loja-departamento seria impratícável\n\n* Weekly_Sales: Vemos que a variável pode assumir valores negativos. Tais valores, possivelmente, indicam algum tipo de devolução."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Separando o dataframe em treino e teste para que possamos\n# entender analisar o comportamento da variável target\ntrain = full_df.loc[\n    full_df['df_label'] == 'train'\n].drop(['df_label'], axis=1)\n\ntest = full_df.loc[\n    full_df['df_label'] == 'test'\n].drop(['df_label'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def EDA(df, col):\n    \n    '''\n    Função para auxiliar na Análise Exploratória através\n    de plots.\n    \n    Para variável target:\n        Plota distribuição e BoxPlot da variável além de\n        plotar a média e mediana de vendas ao longo das\n        semanas do ano\n        \n    Para variáveis categóricas:\n        Plota distribuição da variável target para cada uma das categorias,\n        além de BoxPlot da variável target em função das categorias\n        \n    Para variáveis numéricas:\n        Plota distribuição e BoxPlot da variável\n    \n    '''\n    if col != 'Weekly_Sales':\n        fig,ax = plt.subplots(1, 2, figsize=(15, 6));\n    \n    if col in ['IsHoliday', 'Type']:\n        \n        if col == 'IsHoliday':\n            sflier=True\n        else:\n            sflier=False\n        \n        for val in df[col].unique():\n            ax[0].hist(\n                df['Weekly_Sales'].loc[\n                    df[col] == val\n                ],\n                bins=50,\n                alpha=0.6\n            )\n            ax[0].legend(df[col].unique())\n        ax[0].set_title(f'Distribuição de Weekly Sales por {col}')\n        ax[0].set_xlim([0, 200000])\n\n        sns.boxplot(\n            x=col,\n            y='Weekly_Sales',\n            data=df,\n            orient='v',\n            showfliers=sflier\n        )\n        ax[1].set_title(f'Boxplot de {col}')\n        \n    elif col == 'Weekly_Sales':\n    \n        ax = {}\n        fig = plt.figure(figsize=(15, 12))\n        grid = plt.GridSpec(3, 2, hspace=0.4, wspace=0.1)\n        ax[0] = plt.subplot(grid[0, 0])\n        ax[1] = plt.subplot(grid[0, 1])\n        ax[2] = plt.subplot(grid[1, :])\n\n\n        ax[0].hist(df[col], bins=50);\n        ax[0].set_title(f'Histograma de {col}')\n        ax[0].set_xlim([0, 200000])\n\n        sns.boxplot(df[col], ax=ax[1], orient='h');\n        ax[1].set_title(f'Boxplot de {col}');\n\n        mean_sales = df.groupby('WeekOfYear', as_index=False)[col].mean()\n        median_sales = df.groupby('WeekOfYear', as_index=False)[col].median()\n\n        ax[2].plot(mean_sales['WeekOfYear'], mean_sales[col], 'o-')\n        ax[2].plot(median_sales['WeekOfYear'], median_sales[col], 'o-')\n        ax[2].set_xticks(range(1, 53))\n        ax[2].legend(['mean', 'median'])\n        ax[2].set_title(f'Distribuição de {col} ao longo das semanas')\n        plt.show()\n        \n    else:\n        \n        ax[0].hist(df[col], bins=50);\n        ax[0].set_title(f'Histograma de {col}')\n\n        sns.boxplot(df[col], ax=ax[1], orient='v');\n        ax[1].set_title(f'Boxplot de {col}');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in train.columns[3: -4]:\n    EDA(train, col)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observações:\n\n* Weekly_Sales:\n    * Semanas próximas ao Natal apresentam maiores vendas no ano todo.\n    * Grade discrepância entre média e mediana de vendas (grande presença de outliers)\n    * Distribuição positivamente assimétrica\n\n\n* Markdowns:\n    * Alta concentração de Outliers\n    * Distribuições positivamente assimétricas\n\n\n* Unemployment:\n    * Alta concentração de Outliers"},{"metadata":{},"cell_type":"markdown","source":"<a id='anl-nulls'></a>\n## Análise dos nulos"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dataset de treino\nnulls = pd.DataFrame(\n    round(train.isnull().sum()/train.shape[0], 3),\n    columns=[\n        'Null Percent'\n    ]\n)\nnulls","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observações:\n\n* Chama a atenção a alta concentração de nulos nas colunas MarkDowns (60 a 70%). Caso façamos preenchimento dos nulos, mais da metade das observações terão valores inseridos, o que comprometerá a explicabilidade destas variáveis. Deste modo, aqui devemos considerar apenas se o fato de a variável ser nula contribui com algum poder preditivo."},{"metadata":{"trusted":true},"cell_type":"code","source":"nulls_test = pd.DataFrame(\n    round(test.isnull().sum()/test.shape[0], 3),\n    columns=[\n        'Null Percent'\n    ]\n)\nnulls_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observações:\n\n* Aqui percebemos que as variáveis MarkDowns apresentam menor proporção de nulos. Contudo, como o modelo terá dificuldade para capturar a explicabilidade destas variáveis devido à grande quantidade de nulos no dataset de treino, elas, provavelmente, não serão de grande valia.\n\n* Além disso, percebemos que, no Dataset de teste, temos nulos em CPI e Unemployment. Consultando a EDA, vemos que a distribuição de CPI é bem comportada, sem presença de outliers, de forma que uma possível estratégia para preencher nulos é utilizar a média. Como tais variáveis variam com o tempo, utilizaremos a média móvel do último ano. Para Unemployment, no entanto, como há presença de outliers, é mais recomendável utilizar a mediana móvel dos últimos 12 meses."},{"metadata":{},"cell_type":"markdown","source":"<a id='imput-nulls'></a>\n## Imput de nulos (CPI e Unemployment)\n\n* Entendendo como as variáveis CPI e Unemployment variam de loja para loja"},{"metadata":{"trusted":true},"cell_type":"code","source":"# CPI\n\n# Fixando semana de análise\nweek = 5\n\n# Agrupando CPI por loja e contando valores \n# unicos de CPI\nanl_CPI = full_df.loc[\n    full_df['WeekSeq'] == week\n].groupby(\n    'Store',\n    as_index=False\n).agg(unique_values_CPI=('CPI', 'nunique'))\n\n# Checando se algum dos valores é diferente de 1\n(anl_CPI['unique_values_CPI'] != 1).any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Unemployment\n\n# Fixando semana de análise\nweek = 5\n\n# Agrupando Unemployment por loja e contando valores \n# unicos de Unemployment\nanl_Unemployment = full_df.loc[\n    full_df['WeekSeq'] == week\n].groupby(\n    'Store',\n    as_index=False\n).agg(unique_values_Unemployment=('Unemployment', 'nunique'))\n\n# Checando se algum dos valores é diferente de 1\n(anl_Unemployment['unique_values_Unemployment'] != 1).any()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Para a semana 5, podemos observar que tanto CPI quanto Unemployment são únicos para cada loja. Vamos validar essa hipótese iterando por todas as semanas nas quais estes indicadores não são nulos."},{"metadata":{"trusted":true},"cell_type":"code","source":"def check_for_different_values(df, col):\n    '''\n    Percorre as semanas de df verificando se, em alguma,\n    o indicador col assume diferentes valores para uma mesma loja,\n    em alguma delas.\n    '''\n    \n    # Selecionando semanas em que o indicador não é nulo\n    weeks_not_null = df.loc[\n        ~df[col].isnull(), 'WeekSeq'\n    ].unique()\n    \n    # Para cada semana, checamos se a quantidade de valores\n    # unicos do indicador em alguma das lojas é diferente de 1\n    for week in weeks_not_null:\n\n        store_week = df.loc[\n        (df['WeekSeq'] == week)\n        ]\n\n        store_week_grpd = store_week.groupby('Store').agg(\n        nunique=(col, 'nunique')\n        )\n\n        if (store_week_grpd['nunique'] != 1).any():\n            print(\n                f'Valores diferentes para {col} na semana {week}'\n            )\n            break\n        \n    print(f'{col} não varia de loja para loja!')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = ['CPI', 'Unemployment']\n\n# Testa se para CPI e Unemployment os valores variam\n# dentro de alguma loja em alguma semana.\nfor col in cols:\n    check_for_different_values(full_df, col)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Ok, vimos que nem CPI nem Unemployment variam entre os depts de uma mesma loja em uma dada semana. Agora vamos ver, para algumas lojas, como CPI e Unemployment variam ao longo das semanas."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Selecionando 5 lojas aleatoriamente\ndef plot_indicator_over_weeks(df, indicators):\n    '''\n    Plota gráficos de linha ,para um conjunto de 5 lojas\n    escolhidas aleatoriamente, de indicadores selecionados.\n    '''\n    \n    random.seed(95)\n    stores = random.sample(\n        range(full_df['Store'].min(), full_df['Store'].max()+1),\n        5,\n    )\n\n    fig, ax = plt.subplots(len(cols), 1, figsize=(15, 7))\n\n    for i in range(len(cols)):\n\n        for store in stores:\n            store_df = full_df.loc[\n                full_df['Store'] == store\n            ]\n            store_df = store_df.groupby(['Store', 'WeekSeq'], as_index=False)[cols[i]].mean()\n\n            ax[i].plot(store_df['WeekSeq'], store_df[cols[i]], '.-')\n            ax[i].set_title(f'Variação de {cols[i]} ao longo das semanas')\n\n    fig.legend(stores, loc='center right', ncol=1)\n\n    plt.show()\n    \n    \nplot_indicator_over_weeks(full_df, cols)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Conforme vimos, tanto CPI quanto Unemployment são constantes para uma mesma loja em uma mesma semana, mas variam de loja para loja e ao longo das semanas. Desta forma, a estratégia que adotaremos para preenchimento de nulos será utilizar uma medida de centralidade por loja, considerando uma janela móvel de 52 semanas (1 ano).\n\n* Utilizaremos medidas de centralidade diferentes para CPI e Unemployment pois a distribuição de CPI é bem comportada (sem outliers), de forma que a média funciona bem, mas a distribuição de Unemployment apresenta uma quantidade significativa de outliers, fazendo com que a média seja distorcida. Assim sendo, utilizaremos a mediana."},{"metadata":{"trusted":true},"cell_type":"code","source":"def input_nulls_rolling_window(df, window_size, cols, centrality_measures):\n    '''\n    Para os indicadores selecionados, preenche os nulos de cada loja\n    com a respectiva medida de centralidade indicada, calculada sobre\n    janela móvel de tamanho escolhido.\n    '''\n    \n    for col in cols:\n        for store in df['Store'].unique():\n            aux = df.loc[\n                df['Store'] == store\n            ]\n\n            last_week = aux.loc[\n                ~aux[col].isnull(),'WeekSeq'\n            ].max()\n            end_week = aux.loc[\n                aux[col].isnull(),'WeekSeq'\n            ].max()\n\n            for week in range(last_week+1, end_week+1):\n\n                if centrality_measures[col] == 'mean':\n                    RolMean = aux.loc[\n                        aux['WeekSeq'].between(week-window_size, week-1)\n                    ].groupby('Store')[col].mean()\n\n                    df.loc[\n                        (df['Store'] == store)\n                        &(df['WeekSeq'] == week),\n                        col\n                    ] = df.loc[\n                        (df['Store'] == store)\n                        &(df['WeekSeq'] == week),\n                        col\n                    ].fillna(RolMean.values[0])\n\n\n                elif centrality_measures[col] == 'median':\n\n                    RolMedian = aux.loc[\n                        aux['WeekSeq'].between(week-window_size, week-1)\n                    ].groupby('Store')[col].median()\n\n                    df.loc[\n                        (df['Store'] == store)\n                        &(df['WeekSeq'] == week),\n                        col\n                    ] = df.loc[\n                        (df['Store'] == store)\n                        &(df['WeekSeq'] == week),\n                        col\n                    ].fillna(RolMedian.values[0])\n\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Faz imput de nulos para CPI e Unemployment\nfull_df = input_nulls_rolling_window(\n    df=full_df,\n    window_size=52,\n    cols=['CPI', 'Unemployment'],\n    centrality_measures={\n        'CPI' : 'mean',\n        'Unemployment' : 'median'\n    }\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='anl-corr'></a>\n# Análise de correlações e PPS"},{"metadata":{},"cell_type":"markdown","source":"<a id='corr'></a>\n## Correlações"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Heatmap de correlações\nplt.figure(figsize=(15, 8))\nsns.heatmap(\n    train.corr(),\n    annot=True,\n    fmt='.2f',\n    cmap='Greens'\n);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observações\n\n- Nenhuma variável apresenta correlação muito expressiva com a variável target &rarr; modelo linear provavelmente não vai performar bem;\n- Variáveis MarkDown possuem muitos nulos e são pouco correlacionadas com a variável target &rarr; potencial para serem descartadas;\n- Variáveis Fuel_Price e Year, MarkDown1 e MarkDown4 fortemente correlacionadas entre si. Possibilidade de descarte para uma variável de cada par."},{"metadata":{},"cell_type":"markdown","source":"<a id='PPS'></a>\n## PPS\n\nO PPS (Predictive Power Score) é uma métrica que mede o poder preditivo univariado. Ou seja, é uma expressão do poder preditivo de uma variável em relação à outra. Diferente da correlação, esta métrica não é simétrica, o que faz todo sentido, já que explicabilidade não é uma via de duas mãos. (não é porque uma variável consegue explicar a outra que a outra conseguirá explicar essa uma variável).\n\nTomando duas variáveis, A e B, em que B é a variável target, o PPS ajusta um modelo univariado (árvore de decisão) utilizando A para prever B e mede quão perto do score perfeito o modelo chegou, quando comparado a um modelo baseline simples. Por fim, normaliza-se tal medida  com base em quão perto o modelo baseline chegou do score perfeito.\n\nO score perfeito para regressão é considerado MAE = 0 e para classificação F1 = 1. Já os modelos baseline consistem em modelos ingênuos como prever sempre a mediana para problemas de regressão e sempre a classe mais comum para problemas de classificação.\n\nAssim sendo, o PPS varia de 0 a 1. PPS de 0 indica que a variável A sozinha não contribui para a predição de B, enquanto que PPS de 1 indica que a variável A consegue explicar B perfeitamente."},{"metadata":{"trusted":true},"cell_type":"code","source":"## PPS\npps.predictors(train, 'Weekly_Sales')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Observações:\n\n* A única variável que, sozinha, agrega em explicabilidade é Dept. De fato, assumir vendas semanais diferentes para cada departamento faz sentido, mas esta é uma variável que deve ser tratada como categórica e criar 99 dummies pode aumentar demais a complexidade do dataset. Vamos trabalhar isso mais pra frente;\n\n* O PPS confirmou que as variáveis de MarkDown agregam pouco para a previsão. Assim sendo, podemos descartá-las. Além disso, Fuel_Price também será descartada por ter alta correlação com ano;"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Descartando variáveis mencionadas acima\nfull_df.drop([x for x in full_df.columns if 'MarkDown' in x], axis=1, inplace=True)\nfull_df.drop(['Fuel_Price'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualizando o dataset\nfull_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='baseline'></a>\n# Modelo Baseline\n\nTreinar modelo baseline, apenas com transformações mínimas necessárias, para servir de comparação."},{"metadata":{},"cell_type":"markdown","source":"<a id='dataprep'></a>\n## Data Prep"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Separando dataframe nos dataset de treino e teste\ntrain = full_df.loc[\n    full_df['df_label'] == 'train'\n].drop(['df_label'], axis=1)\n\ntest = full_df.loc[\n    full_df['df_label'] == 'test'\n].drop(['df_label'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Definindo X e y, descartando variável de data\n# já representada por outras variáveis.\nX = train.drop(\n    [\n        'Date',\n        'Weekly_Sales'\n    ], axis=1\n)\ny = train['Weekly_Sales']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Transformando variáveis categóricas em Dummy\n\ncat_features = ['IsHoliday', 'Type']\n\nfor col in cat_features:\n\n    enc = OneHotEncoder(sparse=False)\n    transf_df = pd.DataFrame(\n        enc.fit_transform(X[[col]]),\n        columns=[col + '_' + str(x) for x in enc.categories_[0]]\n    )\n    X.drop(col, axis=1, inplace=True)\n    X = pd.concat(\n        [\n            X,\n            transf_df\n        ],\n        axis=1\n    )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='eval'></a>\n## Métrica de avaliação\n* O problema pede uma métrica de erro customizada, que penaliza mais os erros em feriados. Vamos defini-la abaixo, de forma compatível com os métodos do sklearn."},{"metadata":{"trusted":true},"cell_type":"code","source":"def WMAE_Func(y_true, y_pred, **kwargs):\n    '''\n    Função de erro customizada, que dá um peso maior para\n    datas que são feriados.\n    '''\n    \n    df = kwargs['df']\n    df = df.loc[\n        y_true.index\n    ]\n    df['Weights'] = 1\n    df.loc[\n        df['IsHoliday_True'] == 1,\n        'Weights'\n    ] == 5\n    \n    weights = df['Weights'].to_numpy()\n\n    wmae = 1/np.sum(weights)*np.sum(weights*np.abs(y_true-y_pred))\n    \n    return -wmae","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Definindo scorer do scikit learn a partir de função\n# de erro customizada\nwmae_scorer = make_scorer(WMAE_Func, greater_is_better=False, df=X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='cross-val'></a>\n## Função de validação cruzada\n\n* Como trata-se de um problema em que as variáveis mudam com o tempo, para avaliação do modelo, a ordem das observações (semanas) deve ser preservada. Assim sendo, vamos definir uma função para validação cruzada que consiste em janelas móveis de treino e validação. Por exemplo, na primeira folha de validação cruzada, ajustamos o modelo com dados da semana 5 de 2010 até a semana 5 de 2011 e avaliamos o modelo nas 38 semanas seguintes (quantidade de semanas presentes no dataset de teste proposto pela competição). Para a segunda folha, andamos a janela, ajustando o modelo da semana 25 de 2010 até a semana 25 de 2011, por exemplo, e avaliando o modelo nas 38 semanas seguintes. O processo se repete para o número de folhas escolhido e as janelas se ajustam de acordo com tal escolha."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Definindo objeto de validação cruzada\n\nclass TimeSeriesCV:\n    '''\n    Função para validação cruzada utilizando janelas móveis\n    '''\n    \n    def __init__(self, nsplits, window_size, test_size, start_week):\n        self.nsplits = nsplits\n        self.start_week = start_week\n        self.window_size = window_size\n        self.test_size = test_size\n        \n    def split(self, X, y, groups=None):\n        \n        start_week = self.start_week\n        end_week = X['WeekSeq'].max()\n        test_size = self.test_size\n        window_size = self.window_size\n        nsplits = self.nsplits\n        max_train_week = end_week - test_size\n        step = (max_train_week - start_week - window_size)//(nsplits-1)\n\n        for i in range(nsplits):\n\n            week_ini_train = start_week + i*step\n            week_final_train = week_ini_train + window_size\n            week_final_test = week_final_train + test_size\n            train_index = X.loc[\n                (X['WeekSeq'] >= week_ini_train)\n                & (X['WeekSeq'] < week_final_train)\n            ].index.values\n\n            test_index = X.loc[\n                (X['WeekSeq'] >= week_final_train)\n                & (X['WeekSeq'] < week_final_test)\n            ].index.values\n            \n            yield train_index, test_index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Criando objeto de validação cruzada\ntscv = TimeSeriesCV(\n    nsplits=6,\n    window_size=52,\n    test_size=38,\n    start_week=X['WeekSeq'].min()\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Definindo modelo baseline\nbaseline = LinearRegression()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Avaliando modelo baseline com base na metrica WMAE\nscores = cross_val_score(\n    baseline,\n    X,\n    y,\n    scoring=wmae_scorer,\n    cv=tscv\n)\n\nprint(f'WMAE ao longo das folhas: {scores.round(2)}')\nprint('\\n')\nprint(f'WMAE médio: {scores.mean().round(2)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='feat-eng'></a>\n# Feature Engineering\n\n* Ok, agora que temos nosso score de comparação, podemos tentar agregar à explicabilidade criando novas variáveis a partir das existentes."},{"metadata":{},"cell_type":"markdown","source":"<a id='natal'></a>\n## Variáveis de distância até o Natal e até o próximo feriado.\n\n* Como pudemos observar durante a EDA, as semanas próximas ao Natal são as que apresentam maiores vendas no ano. Assim sendo, uma variável que explicita a distância para o Natal possivelmente agregará em explicabilidade.\nAlém disso, os feriados também são bem importantes, uma vez que têm peso 5 na métrica de erro. Logo, passar pro modelo a distância até o próximo feriado também tem potencial para agregar em poder preditivo."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Criando variável com a data do Natal do ano em questão\nfull_df['Christmas'] = full_df['Year'].astype(str) + '-12-25'\nfull_df['Christmas'] = pd.to_datetime(full_df['Christmas'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Contando os dias até o Natal do ano vigente\nfull_df['Days_to_Christmas'] = (full_df['Christmas'] - full_df['Date']).dt.days","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Mapeando as semanas que contém feriado\nweeks_holiday = full_df.loc[\n    full_df['IsHoliday'] == True, 'WeekOfYear'\n].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Criando variável com a distância (em semanas) até o próximo feriado\nfull_df['Weeks_To_Holiday'] = full_df['WeekOfYear'].apply(\n    lambda x: min([week - x for week in weeks_holiday if week - x >= -1])\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Descartando data do natal\nfull_df.drop(['Christmas'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualizando o dataset\nfull_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='holidays'></a>\n## Classificação dos feriados\n\n* Conforme informado na competição, são considerados apenas 4 feriados. Contudo, tais feriados impactam de formas diferentes as vendas da semana. Assim, uma variável que explicite se é feriado ou não na semana em questão e, em caso afirmativo, qual é o feriado, tem potencial para agregar explicabilidade."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Mapeando as semanas que contém feriado\nweeks_holiday = full_df.loc[\n    full_df['IsHoliday'] == True, 'WeekOfYear'\n].unique()\n\n# Criando dict que mapeia numero da semana à classe de feriado\nholiday_dict = {}\ni=1\nfor week in weeks_holiday:\n    holiday_dict[week] = i\n    i += 1\n\n# Atribuindo aos nulos (semanas que não são feriado) a classe 0\nfull_df['Holiday_Clf'] = full_df['WeekOfYear'].map(holiday_dict).fillna(0).astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualizando o dataset\nfull_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='lag52'></a>\n## Variável de lag de 52 semanas nas vendas semanais\n\n* Aqui, a ideia é criar uma variável que passe para o modelo o valor vendido na mesma semana do ano anterior. Isso permite que o modelo tenha uma \"base\" de vendas para cada combinção loja-departamento-semana, além de adicionar uma componente de sazonalidade ao modelo. Além disso, como o intervalo de predição é menor do que 52 semanas, conseguimos suprir tais informações para o modelo sem precisar se apoiar em previsões anteriores, o que causaria propagação do erro (mais sobre isso na seção Próximos passos).\n\n* O ponto negativo é que, ao fazer isso, perderemos o primeiro ano de dados, uma vez que para este ano não teremos valores dos anos anteriores."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Criando variavel com vendas semanais da mesma semana do ano anterior\nfull_df['y_lag52'] = full_df.groupby(['Store', 'Dept'])[['Weekly_Sales']].shift(52).values\n\n# Para lojas/departamentos que ainda não existiam na referida semana\n# do ano anterior (nulos), preenchemos com 0\nfull_df['y_lag52'] = full_df['y_lag52'].fillna(0)\n\n# Desprezando o primeiro ano, para o qual o valor da variavel será nulo\nfull_df = full_df.loc[full_df['WeekSeq'] >= full_df['WeekSeq'].min() + 52].reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualizando o dataset\nfull_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='cluster-weeks'></a>\n## Análise e classificação das semanas (clustering)\n\n* Uma variável categórica com a semana do ano pode agregar em explicabilidade, uma vez que diferentes semanas do ano apresentam diferentes padrões de venda. Contudo, criar 52 dummies pode aumentar demais a complexidade do modelo.\n\n* Desta forma, a ideia aqui é utilizar-se de uma clusterização para classificar as semanas do ano de acordo com suas vendas. Assim, semanas com valores médios de vendas próximos serão incluídas na mesma classe.\n\n* O número de classes (clusters) é um parâmetro a ser otimizado de forma a minimizar o erro. Para tanto, criaremos uma classe, compatível com Pipeline do sklearn, que faça a referida classificação e que permita otimização do parâmetro referente ao número de clusters."},{"metadata":{"trusted":true},"cell_type":"code","source":"class KMeansTransformerWeek(BaseEstimator, TransformerMixin):\n    '''\n    Agrupa e classifica semanas do ano com base na média de vendas.\n    '''\n    \n    def __init__(self, num_clusters):\n        self.num_clusters = num_clusters\n        self.grped_weeks = pd.DataFrame()\n        \n    def fit(self, X, y):\n        df = X.copy()\n        df['Weekly_Sales'] = y.values\n        grped_weeks = df.groupby(['WeekOfYear'], as_index=False)['Weekly_Sales'].mean()\n        \n        kmeans = KMeans(n_clusters=self.num_clusters)\n        \n        labels = kmeans.fit_predict(grped_weeks[['Weekly_Sales']])\n        grped_weeks['Week_Clf'] = labels\n        self.grped_weeks = grped_weeks\n        \n        return self\n    \n    def transform(self, X):\n        \n        X_ = X.copy()\n        \n        grped_weeks = self.grped_weeks\n        \n        X_ = pd.merge(\n            X_,\n            grped_weeks[\n                [\n                    'WeekOfYear',\n                    'Week_Clf'\n                ]\n            ],\n            how='left',\n            on='WeekOfYear'  \n        )\n        \n        X_['Week_Clf'] = X_['Week_Clf'].fillna(99)\n\n        return X_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='cluster-depts'></a>\n## Análise e classificação dos departamentos (clustering)\n\n* O raciocínio aqui é o mesmo da classificação de semanas, mas considerando cada combinação loja-departamento.\n\n* Também definimos uma classe, compatível com Pipeline do sklearn, que faz a transformação e que permite que o número de clusters seja otimizado."},{"metadata":{"trusted":true},"cell_type":"code","source":"class KMeansTransformerDepts(BaseEstimator, TransformerMixin):\n    \n    def __init__(self, num_clusters):\n        self.num_clusters = num_clusters\n        self.grped_depts = pd.DataFrame()\n        \n    def fit(self, X, y):\n        df = X.copy()\n        df['Weekly_Sales'] = y.values\n        \n        grped_depts = df.groupby(\n            [\n                'Store',\n                'Dept'\n            ], as_index=False\n        )['Weekly_Sales'].mean()\n    \n        kmeans = KMeans(n_clusters=self.num_clusters)\n        labels = kmeans.fit_predict(grped_depts[['Weekly_Sales']])\n        grped_depts['Dept_Clf'] = labels\n        self.grped_depts = grped_depts\n\n        return self\n    \n    def transform(self, X):\n\n        X_ = X.copy()\n        \n        grped_depts = self.grped_depts\n        \n        X_ = pd.merge(\n            X_,\n            grped_depts[\n                [\n                    'Store',\n                    'Dept',\n                    'Dept_Clf'\n                ]\n            ],\n            how='left',\n            on=[\n                'Store',\n                'Dept'\n            ]  \n        )\n        \n        X_['Dept_Clf'] = X_['Dept_Clf'].fillna(99)\n        return X_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='models'></a>\n# Ajuste de modelos\n\n* Agora que criamos algumas variáveis, vamos testar diferentes modelos e ver qual deles performa melhor para o problema em questão"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Definindo X e y\nX = full_df.loc[\n    full_df['df_label'] == 'train'\n]\n\ny = full_df.loc[\n    full_df['df_label'] == 'train',\n    'Weekly_Sales'\n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Aqui definimos quais features usaremos e qual o tipo de cada uma delas\n# As variáveis que não estão inclusas em num_cols nem em cat_cols serão descartadas\ncols = X.columns.tolist()\n\ncat_cols = [\n    'Type',\n    'Holiday_Clf']\n\nnum_cols = [\n    'Size',\n    'Temperature', \n    'CPI', \n    'Unemployment',\n    'Year', \n    'Days_to_Christmas', \n    'Weeks_To_Holiday',\n    'y_lag52'\n]\n\n# Pegando o identificador posicional de cada uma das colunas selecionadas\nnum_idx = [cols.index(x) for x in num_cols]\ncat_idx = [cols.index(x) for x in cat_cols]\n\n# Adicionamos mais duas posições referentes às colunas que serão\n# criadas pelas clusterizações\ncat_idx += [len(cols), len(cols)+1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='pipeline'></a>\n## Construção do Pipeline\n\n* Agora construiremos um Pipeline que elenca as transformações a serem realizadas além de passar o dataset transformado para o modelo escolhido"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Construindo Pipeline para os transformadores baseados em clusterização\npipe_kmeans = Pipeline(\n    [\n        (\n            'week_label', KMeansTransformerWeek(5)\n        ),\n        (\n            'dept_label', KMeansTransformerDepts(12)\n        )\n    ]\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Construindo ColumnTransformer que cria dummies para variáveis categóricas\n# e padroniza variáveis numéricas, dropando as demais variáveis.\ndataprep = ColumnTransformer(\n    [\n        (\n            'ohe',\n            OneHotEncoder(handle_unknown='ignore'),\n            cat_idx\n        ),\n        (\n            'sc',\n            StandardScaler(),\n            num_idx\n            \n        ),\n    ],\n    remainder='drop'\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Agregando todos os passos em um só pipeline\ndef create_pipe(model, transformers):\n    '''\n    Cria Pipeline a partir de um determinado modelo e um\n    conjuto de transformadores.\n    '''\n    \n    pipe = Pipeline(\n    [\n        ('KMeansTransform', transformers['clustering']),\n        ('dataprep', transformers['dataprep']),\n        ('model', model)\n    ]\n    )\n    \n    return pipe","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Como criamos nova variável para classificação dos feriados, precisamos redefinir o cálculo da métrica de avaliação do modelo levando esta variável em consideração."},{"metadata":{"trusted":true},"cell_type":"code","source":"def WMAE_Func2(y_true, y_pred, **kwargs):\n    '''\n    Função de erro customizada, que dá um peso maior para\n    datas que são feriados, considerando variável de classificação\n    dos feriados.\n    '''\n    \n    df = kwargs['df']\n    df = df.loc[\n        y_true.index\n    ]\n    df['Weights'] = 1\n    df.loc[\n        df['Holiday_Clf'] > 0,\n        'Weights'\n    ] == 5\n    \n    weights = df['Weights'].to_numpy()\n\n    wmae = 1/np.sum(weights)*np.sum(weights*np.abs(y_true-y_pred))\n    \n    return -wmae\n\n\n# Recriando objeto referente à métrica de avaliação\nwmae_scorer = make_scorer(WMAE_Func2, greater_is_better=False, df=X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Criando objeto de validação cruzada\ntscv = TimeSeriesCV(\n    nsplits=6,\n    window_size=52,\n    test_size=38,\n    start_week=X['WeekSeq'].min()\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Definindo dicionário com transformers\ntransformers = {\n    'clustering' : pipe_kmeans,\n    'dataprep' : dataprep\n}\n\n# Definindo modelos a serem testados\nmodels = [\n    LinearRegression(),\n    DecisionTreeRegressor(),\n    RandomForestRegressor(),\n    GradientBoostingRegressor(),\n    XGBRegressor()\n]\n\n# Testando modelos\nfor model in models:\n    \n    print('***********************************')\n    print('Teste do modelo:', type(model).__name__)\n    # Cria pipeline com o modelo em avaliação\n    pipe = create_pipe(model, transformers)\n    \n    # Calcula métrica de avaliação ao longo das folhas de\n    # validação cruzada\n    scores = cross_val_score(\n    pipe,\n    X,\n    y,\n    cv=tscv,\n    scoring=wmae_scorer,\n    n_jobs=-1\n    )\n    \n    print('Scores:', scores.round(2))\n    print('WMAE médio:', scores.mean().round(2))\n    print('\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Observações:\n\n* O modelo RandomForest foi o que performou melhor (por pouco!), portanto é com ele que seguiremos para otimização dos hiperparâmetros.\n\n* Aqui vale notar que o resultado da regressão linear piorou consideravelmente. Isto, provavelmente, se deve ao fato de que aumentamos consideravelmente a complexidade do dataset ao criar mais features/categorias. Além disso, trata-se de um problema de alta variância, uma vez que ao mudarmos do departamento 1 para o departamento 2, na mesma loja, as vendas podem variar significativamente. A regressão linear, por ser um modelo de baixa variância, têm dificuldade de capturar esse tipo de comportamento, prejudicando seu resultado."},{"metadata":{},"cell_type":"markdown","source":"<a id='hyperparam'></a>\n# Ajuste de Hiperparâmetros\n\n* Agora que já definimos nosso modelo, vamos utilizar uma ferramenta de otimização para buscar os valores ideais para alguns dos hiperparâmetros do modelo, como o numero de clusters para classificar semanas ou departamentos, o número de estimadores do modelo ou a profundidade máxima das árvores."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Criando pipeline com modelo RandomForest\npipe = create_pipe(RandomForestRegressor(), transformers)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def objective(trial):\n    \n    '''\n    Função objetivo para otimização, que define o intervalo de busca\n    para os hiperparâmetros selecionados.\n    '''\n    \n    # Definindo intervalos de busca\n    nclusters_week = trial.suggest_int('KMeansTransform__week_label__num_clusters', 3, 10)\n    nclusters_dept = trial.suggest_int('KMeansTransform__dept_label__num_clusters', 3, 15)\n    n_estimators = trial.suggest_int('model__n_estimators', 50, 150)\n    max_depth = trial.suggest_int('model__max_depth', 10, 100)\n    bootstrap = trial.suggest_categorical('model__bootstrap', [False, True])\n    max_features = trial.suggest_categorical('model__max_features', ['auto', 'log2'])\n    min_samples_leaf = trial.suggest_int('model__min_samples_leaf', 1, 10)\n    min_samples_split = trial.suggest_int('model__min_samples_split', 2, 15)\n    \n    params = {\n        'KMeansTransform__week_label__num_clusters': nclusters_week,\n        'KMeansTransform__dept_label__num_clusters': nclusters_dept,\n        'model__bootstrap': bootstrap,\n        'model__max_depth' : max_depth,\n        'model__max_features': max_features,\n        'model__min_samples_leaf': min_samples_leaf,\n        'model__min_samples_split': min_samples_split\n    }\n    \n    # Aplicando tais intervalos ao pipeline\n    pipe.set_params(**params)\n    \n    # Avaliando o modelo com base no WMAE e utilizando a\n    # validação cruzada com janela móvel\n    scores = cross_val_score(\n    pipe,\n    X,\n    y,\n    cv=tscv,\n    scoring=wmae_scorer,\n    n_jobs=-1\n    )\n    \n    return scores.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Realizando a otimização\nstudy = optuna.create_study()\nstudy.optimize(objective, n_trials=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Atribuindo os melhores parâmetros encontrados pela otimização ao modelo\npipe.set_params(**study.best_params);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='sub'></a>\n# Submissão\n\n* Finalmente temos nosso modelo! Agora vamos treiná-lo nas 52 semanas anteriores ao intervalo de predição para, em seguida, fazer a predição e submeter o resultado."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Definindo conjunto de treino como sendo as 52 semanas anteriores\n# à primeira semana do intervalo de predição.\n\nstart_week = full_df.loc[full_df['df_label'] == 'test', 'WeekSeq'].min() - 52\n\nX_train = full_df.loc[\n    (full_df['df_label'] == 'train')\n    & (full_df['WeekSeq'] >= start_week)\n]\n\ny_train = full_df.loc[\n    (full_df['df_label'] == 'train')\n    & (full_df['WeekSeq'] >= start_week),\n    'Weekly_Sales'\n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Ajustando modelo\npipe.fit(X_train, y_train);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Definindo X de teste\nX_test = full_df.loc[\n    full_df['df_label'] == 'test'\n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fazendo predição\nyhat = pipe.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Montando arquivo de submissão\n\nsub = full_df.loc[\n    full_df['df_label'] == 'test'\n]\n\nsub['Id'] = sub['Store'].astype(str).str.cat(\n    [\n        sub['Dept'].astype(str),\n        sub['Date'].astype(str)\n    ],\n    sep='_'\n)\n\nsub['Weekly_Sales'] = yhat\n\nsub = sub[\n    [\n        'Id',\n        'Weekly_Sales'\n    ]\n]\n\nsub.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='result'></a>\n# Resultado\n\n![image.png](attachment:image.png)","attachments":{"image.png":{"image/png":"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"}}},{"metadata":{},"cell_type":"markdown","source":"<a id='next-steps'></a>\n# Próximos Passos / Oportunidades de melhoria\n\n\n- **Explorar variáveis com lag de 1 e 2 semanas para vendas semanais:**\n    \n    Variáveis com a venda semanal da semana anterior ou de 2 semanas atrás para a respectiva combinação loja-departamento têm bastante potencial para agregar em poder preditivo, especialmente para um intervalo curto de predições. Para ilustrar o problema deste método, imagine que hoje queremos fazer uma previsão das vendas semanais para daqui a três semanas. Focando exclusivamente na semana 3, hoje não temos os valores vendidos na semana 1 e na semana 2. O que precisaria ser feito neste caso é fazer uma predição para a semana 1 e usar o resultado da predição da semana 1 para prever a semana 3. Este movimento de usar uma predição anterior para fazer uma nova pode causar uma propagação de erro que venha a prejudicar a performance do modelo para predições de longo prazo.\n    \n\n- **\"Simplificar\" o modelo, entendendo quais variáveis estão sendo mais relevantes para a predição e cortando as irrelevantes:**\n    \n    Ao analisarmos o PPS vimos que quase nenhuma variável contribuía, individualmente, para a predição. Uma vez que temos o modelo ajustado, podemos fazer um estudo, a partir de técnicas como Permutation Feature Importance ou SHAP, para entender quais variáveis ou combinações de variáveis estão efetivamente contribuindo para o modelo e descartar as variáveis que não agregam poder preditivo.\n    \n    \n- **Ampliar o espaço de busca para os hiperparâmetros e aumentar número de iterações:**\n\n    Devido à restrição de tempo, optou-se por executar apenas 10 ensaios para buscar os hiperparâmetros ótimos para o modelo. Com mais tempo disponível, uma alternativa que poderia trazer melhoras sensíveis à performance do modelo, seria aumentar o número de iterações e ampliar o espaço de busca. É importante ressaltar que, provavelmente, a melhoria de performance seria marginal.\n\n\n - **Explorar diferentes abordagens/modelos:**\n     \n     Nesta solução, optou-se pela estratégia de abordar o problema como uma regressão e utilizar modelos baseados em árvore. Outras possibilidades seriam utilizar modelos estatísticos de séries temporais (SARIMAX, por exemplo) ou mesmo modelos baseados em redes neurais (como LSTM)."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}