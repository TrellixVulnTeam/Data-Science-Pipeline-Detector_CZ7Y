{"cells":[{"metadata":{},"cell_type":"markdown","source":"# 1 - Importing used Libraries","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Importing all the libraries used in this work","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns; sns.set()\nimport missingno as msno\nfrom sklearn.model_selection import train_test_split, cross_val_predict, RandomizedSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.pipeline import Pipeline\nfrom sklearn import linear_model\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, make_scorer\nimport shap\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install xgboost==1.0.0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2 - Used Functions","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"This cell contains all the functions built for use in this work","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class CreateFeatures(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        pass\n    def fit(self, X, y=None):\n        return self\n        \n    def transform(self, X, y=None):\n        X['week_of_year'] = pd.to_datetime(X.Date).dt.weekofyear\n        X['year'] = pd.to_datetime(X.Date).dt.year\n\n        X = X.merge(df_4holidays[['year', 'current_week', 'week_holiday']]\n                                       , how='left', left_on=['year', 'week_of_year'],right_on=['year', 'current_week'])\n\n        X = X.merge(df_4holidays[['year', 'last_week', 'last_week_holiday']]\n                                       , how='left', left_on=['year', 'week_of_year'],right_on=['year', 'last_week'])\n\n        X = X.merge(df_4holidays[['year', 'next_week', 'next_week_holiday']]\n                                       , how='left', left_on=['year', 'week_of_year'],right_on=['year', 'next_week'])\n        \n        X['prop_to_buy'] =  ((X.Temperature * (100 - X.Unemployment) ) / (X.CPI * X.Fuel_Price ))\n        X['move_cost'] = X.CPI / X.Fuel_Price\n        X['revenue_potential'] = (100 * X.Unemployment) * X.Size\n        return X\n\nclass FeatureSelector(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        pass\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X, y=None):\n        X.Store = X.Store.astype(str)\n        X.Dept = X.Dept.astype(str)\n        X.Type = X.Type.astype(str)\n        X.week_holiday = X.week_holiday.astype(str)\n        X.last_week_holiday = X.last_week_holiday.astype(str)\n        X.next_week_holiday = X.next_week_holiday.astype(str)\n        X = pd.get_dummies(X)\n        return X[FEATURES_TO_MODEL]\n    \nclass FeatureSelector1(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        pass\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X, y=None):\n        X.Store = X.Store.astype(str)\n        X.Dept = X.Dept.astype(str)\n        X.Type = X.Type.astype(str)\n        X.week_holiday = X.week_holiday.astype(str)\n        X.last_week_holiday = X.last_week_holiday.astype(str)\n        X.next_week_holiday = X.next_week_holiday.astype(str)\n        X = pd.get_dummies(X)\n        return X\n\n\nclass FillNaValues(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        pass\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X, y=None):\n        X.MarkDown1 = X.MarkDown1.fillna(X.MarkDown1.dropna().median())\n        X.next_week_holiday.fillna('None', inplace=True)\n        X.last_week_holiday.fillna('None', inplace=True)\n        X.week_holiday.fillna('None', inplace=True)\n        X.Unemployment.fillna(X.Unemployment.dropna().median(), inplace=True)\n        X.IsHoliday.fillna(0, inplace=True)\n        X.prop_to_buy.fillna(X.prop_to_buy.dropna().median(), inplace=True)\n        X.move_cost.fillna(X.move_cost.dropna().median(), inplace=True)\n        X.revenue_potential.fillna(X.revenue_potential.dropna().median(), inplace=True)\n        return X\n\n    \ndef train_linear_regression(X_train, y_train, X_val, y_val):\n    lr = linear_model.ElasticNet(random_state=42)\n    lr.fit(X_train, y_train)\n    print('R^2 = {}'.format(r2_score(y_val, lr.predict(X_val))))\n    print('MAE = {}'.format(mean_absolute_error(y_val, lr.predict(X_val)) ))\n    print('RMSE = {}'.format(mean_squared_error(y_val, lr.predict(X_val), squared=False) ))\n    # cross_val_predict returns an array of the same size as `y` where each entry\n    # is a prediction obtained by cross validation:\n    predicted = cross_val_predict(lr, X_train, y_train, cv=5)\n    fig, ax = plt.subplots()\n    ax.scatter(y_train, predicted, edgecolors=(0, 0, 0))\n    ax.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], 'k--', lw=4)\n    ax.set_xlabel('Measured')\n    ax.set_ylabel('Predicted')\n    plt.show()\n    return lr  \n\ndef train_xgboost_regressor(X_train, y_train, X_val, y_val):\n    xgb_model = xgboost.XGBRegressor(random_state=42, n_jobs=-1)\n    xgb_model.fit(X_train, y_train)\n    print('R^2 = {}'.format(r2_score(y_val, xgb_model.predict(X_val))))\n    print('MAE = {}'.format(mean_absolute_error(y_val, xgb_model.predict(X_val)) ))\n    print('RMSE = {}'.format(mean_squared_error(y_val, xgb_model.predict(X_val), squared=False) ))\n    predicted = cross_val_predict(xgb_model, X_train, y_train, cv=5)\n    fig, ax = plt.subplots()\n    ax.scatter(y_train, predicted, edgecolors=(0, 0, 0))\n    ax.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], 'k--', lw=4)\n    ax.set_xlabel('Measured')\n    ax.set_ylabel('Predicted')\n    plt.show()\n    return xgb_model\n\ndef ensemble_xgb_elastic_net(model_1, model_2, X_train_1, X_train_2, y_train, X_val_1, X_val_2, y_val):\n    train_pred1 = model_1.predict(X_train_1)\n    train_pred2 = model_2.predict(X_train_2)\n    val_pred1 = model_1.predict(X_val_1)\n    val_pred2 = model_2.predict(X_val_2)\n    df_train = pd.DataFrame({'feat_model_1': train_pred1, 'feat_model_2': train_pred2})\n    df_val = pd.DataFrame({'feat_model_1': val_pred1, 'feat_model_2': val_pred2})\n    model_lr = linear_model.LinearRegression()\n    model_lr.fit(df_train, y_train)\n    print('R^2 = {}'.format(r2_score(y_val, model_lr.predict(df_val))))\n    print('MAE = {}'.format(mean_absolute_error(y_val, model_lr.predict(df_val)) ))\n    print('RMSE = {}'.format(mean_squared_error(y_val, model_lr.predict(df_val), squared=False) ))\n    return model_lr\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3 - Loading the database","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"In this step I am loading all data source given for this problem","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_stores = pd.read_csv('/kaggle/input/walmart-recruiting-store-sales-forecasting/stores.csv', sep=',')\ndf_features = pd.read_csv('/kaggle/input/walmart-recruiting-store-sales-forecasting/features.csv.zip', sep=',')\ndf_train = pd.read_csv('/kaggle/input/walmart-recruiting-store-sales-forecasting/train.csv.zip', sep=',')\ndf_teste = pd.read_csv('/kaggle/input/walmart-recruiting-store-sales-forecasting/test.csv.zip', sep=',')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4 - Exploratory Analysis","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"In this step we will try to explore and understand the data provided ... it is a step to be comfortable with the dataset that we will use in the modeling, and to obtain \"insights\" from the data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 4.1 - Stores dataset","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"First step is to understand the data of each dataset source. I am starting with \"stores.csv\"","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_stores.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_stores.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_stores.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_stores.Store.nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4.1.1 - Type of Store","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's go deeper in store types:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_stores['Type'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.barplot(x=sorted(df_stores.Type.unique()),y=df_stores['Type'].value_counts(),\n                 palette=\"Blues_d\")\nplt.xlabel('Store types')\nplt.ylabel(\"Quantity\")\nplt.title('Quantity analysis of store types')\nsns.despine()\nplt.show();\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Although we don't know each type of store is, we can realize that the types \"A\" and \"B\" is at least 80% overall.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 4.1.2 - Store Sizes","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now, let's go deeper in the store sizes","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(df_stores['Size']);\nsns.despine();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_stores['Size'].plot.hist(density=True);\nplt.xlabel('Store Size');\nplt.title('Analysing the Size distribution ');\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for t in df_stores['Type'].unique():\n    df_stores.loc[df_stores.Type==t, 'Size'].plot.hist(density=False, label=t, alpha=0.8);\n    plt.xlabel('Store Size');\nplt.legend(title='Type of store');\nplt.title('Analysing the Size distribution by Type of Store');\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for t in df_stores['Type'].unique():\n    print('Analysing the Size distribution for Store Type {}'.format(t));\n    display(df_stores.loc[df_stores.Type==t, 'Size'].describe())\n    df_stores.loc[df_stores.Type==t, 'Size'].plot.hist(density=True, label=t, alpha=0.8);\n    plt.xlabel('Store Size');\n    plt.legend(title='Type of store');\n    plt.title('');\n    plt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Interesting! Type \"A\" stores are the largest, with an average size of ~ 180kunits of measurement, followed by type \"B\", with average size of ~ 102k measurement units, and finally type C stores with average size of ~ 40k measurement units .\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 4.2 - Features dataset","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now let's explore the feature dataset!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_features.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_features.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_features.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4.2.1 - Date","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We will analyze if all stores have the same number of lines (sales dates) and if the minimum and maximum dates coincide.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_features.groupby(['Store']).agg({'Date':['count', 'min','max']})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Nice! We have 182 records for each store, and the records range from 2010-02-05 to 2013-07-26\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 4.2.2 - Temperature","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's understand how the temperature distribution of the base is based on the dates\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_features.Temperature.plot.hist(density=True, alpha=0.85);\nplt.xlabel('Temperature');\nplt.title('Temperature Distribution for the whole dataset');\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, grouping by Stores, let's analyze how the temperature variable behaves","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_features.groupby(['Store']).agg({'Temperature': ['min','mean','max', 'std']})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Most stores have an average temperature between 50-70. What draws attention is the coldest store of all (Store 7) ... with an average temperature of ~ 37, and a minimum of -7! Is there a relationship between the temperature of the place and the amount of sales? In warmer places do people buy more? We will study this soon.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The table above also shows that the hottest store is 33! Let's compare the temperature distribution of the warmest store with the coldest store.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_features.loc[df_features.Store==7, 'Temperature'].plot.hist(label='Store 7', density=True, alpha=.6);\ndf_features.loc[df_features.Store==33, 'Temperature'].plot.hist(label='Store 33', density=True, alpha=.6);\nplt.xlabel('Temperature');\nplt.legend();\nplt.title('Comparing the temperature of the hottest store to the coldest store');\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4.2.3 - Fuel Price","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's look at the fuel price distribution\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_features.Fuel_Price.plot.hist();\nplt.xlabel('Fuel Price');\nplt.title('Fuel Price Distribution for the whole dataset');\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Below we will analyze the evolution of the fuel price over time for each Store\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in df_features.Store.unique():\n    df_features.loc[df_features.Store==i, 'Fuel_Price'].plot();\n    plt.title('Fuel Price through time for Store {}'.format(i));\n    plt.ylabel('Fuel Price');\n    plt.xticks([]);\n    plt.xlabel('Time {} to {}'.format(min(df_features.Date),max(df_features.Date) ));\n    plt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On average, the price of fuel for all stores followed the same upward and downward trend during the period. It is worth noting that the regions of Stores 44, 41, 38, 33, 32, 28, 17, 16, 13 and 7. Perhaps these stores are in nearby regions and some other external factor may have influenced the further decline in those regions.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 4.2.4 - MarkDown","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's analyze and understand the \"MarkDown\" variable. anonymized data related to promotional markdowns that Walmart is running.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_features.sample()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for mark in [i for i in df_features.columns if 'Mark' in i]:\n    df_features[mark].plot.hist(density=False);\n    plt.title('Distribuition of {}'.format(mark));\n    plt.show();  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The \"Markdown\" column with more information (greater dispersion in the distribution) is Markdown1. Let's look at it in more detail\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_features.groupby(['Store']).agg({'MarkDown1':['min','mean', 'max']})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for st in df_features.Store.unique():\n    df_features.loc[df_features.Store==st, 'MarkDown1'].plot.hist(density=True, alpha=0.8, label=st);\n    plt.xlabel('MarkDown1');\n    plt.title('MarkDown1 distribution for Store {}'.format(st));\n    plt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4.2.5 - CPI (The Consumer Price Index)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_features.CPI.plot.hist();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for st in df_features.Store.unique():\n    df_features.loc[df_features.Store==st, 'CPI'].plot.hist(density=True, alpha=0.8, label=st);\n    plt.xlabel('CPI value');\n    plt.title('CPI distribution for Store {}'.format(st));\n    plt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_features.groupby('Store').agg({'CPI': ['min', 'mean', 'max']})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4.2.5 - Unemployment","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_features.sample()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_features.Unemployment.plot.hist();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_features.groupby('Store').agg({'Unemployment':['min', 'mean', 'max']})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Stores in regions with higher unemployment rates are expected to have less sales\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 4.2.6 - Is Holiday","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_features.IsHoliday.dtype","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_features.IsHoliday = df_features.IsHoliday.astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_features.groupby(['Store']).agg({'IsHoliday':sum})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4.2.7 - CPI vs Fuel Price","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_features.sample()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.scatterplot(x=\"CPI\", y=\"Fuel_Price\",hue='Store', data=df_features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4.2.8 - CPI vs Unemployment","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.scatterplot(x=\"CPI\", y=\"Unemployment\",hue='Store', data=df_features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4.3 - Merging datasets","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_teste.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_stores.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_features.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(df_train.Store.dtype == df_stores.Store.dtype, df_teste.Store.dtype == df_stores.Store.dtype )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_temp_train = df_train.merge(df_stores, how='left', on='Store')\ndf_temp_test = df_teste.merge(df_stores, how='left', on='Store')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_temp_test.sample()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_temp_train.sample()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(df_temp_train.Store.dtype == df_features.Store.dtype, df_temp_train.Date.dtype == df_features.Date.dtype)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(df_temp_test.Store.dtype == df_features.Store.dtype, df_temp_test.Date.dtype == df_features.Date.dtype)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_full = df_temp_train.merge(df_features, how='left', on=['Store', 'Date'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test_full = df_temp_test.merge(df_features, how='left', on=['Store', 'Date'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_full.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test_full.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_full.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_full.IsHoliday_x.astype(int).sum() == df_train_full.IsHoliday_y.sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_full.drop('IsHoliday_x', axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_full.rename(columns={'IsHoliday_y':'IsHoliday'}, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test_full.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test_full.IsHoliday_x.astype(int).sum() == df_test_full.IsHoliday_y.sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test_full.drop('IsHoliday_x', axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test_full.rename(columns={'IsHoliday_y':'IsHoliday'}, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4.4 - Plug-in the Four Largest Holidays in the dataset","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The weeks including these holidays are weighted five times higher in the evaluation than non-holiday weeks:\n\n- **Super Bowl**: 12-Feb-10, 11-Feb-11, 10-Feb-12, 8-Feb-13\n- **Labor Day**: 10-Sep-10, 9-Sep-11, 7-Sep-12, 6-Sep-13\n- **Thanksgiving**: 26-Nov-10, 25-Nov-11, 23-Nov-12, 29-Nov-13\n- **Christmas**: 31-Dec-10, 30-Dec-11, 28-Dec-12, 27-Dec-13","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dict_lgt_hlds ={'Super_Bowl': ['12-Feb-10', '11-Feb-11', '10-Feb-12', '8-Feb-13']\n               ,'Labor_Day': ['10-Sep-10', '9-Sep-11', '7-Sep-12', '6-Sep-13']\n                ,'Thanksgiving': ['26-Nov-10', '25-Nov-11', '23-Nov-12', '29-Nov-13']\n                ,'Christmas': ['31-Dec-10', '30-Dec-11', '28-Dec-12', '27-Dec-13']\n               }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lista = []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for hol in dict_lgt_hlds.keys():\n    for dt in dict_lgt_hlds[hol]:\n        lista.append([hol, pd.to_datetime(dt).year, pd.to_datetime(dt).week])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_4holidays = pd.DataFrame(lista, columns=['week_holiday','year', 'current_week'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_4holidays['last_week'] =df_4holidays['current_week'] +1\ndf_4holidays['next_week'] =df_4holidays['current_week'] - 1\ndf_4holidays['last_week_holiday'] = df_4holidays['week_holiday']\ndf_4holidays['next_week_holiday'] = df_4holidays['week_holiday']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_4holidays","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Transforming the dataset for week view of the year:\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_full['week_of_year'] = pd.to_datetime(df_train_full.Date).dt.weekofyear\ndf_train_full['year'] = pd.to_datetime(df_train_full.Date).dt.year","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(data=df_train_full, x='week_of_year');\nplt.xticks(rotation=45);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(data=df_train_full, x='year');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nMarking the weeks of big holidays","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_full_4h = df_train_full.merge(df_4holidays[['year', 'current_week', 'week_holiday']]\n                                       , how='left', left_on=['year', 'week_of_year'],right_on=['year', 'current_week'])\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our hypothesis here is that the sales weeks before and after the big holidays are also affected! So we are going to mark these weeks on the bases.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"marking the weeks before the big holidays\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_full_4h = df_train_full_4h.merge(df_4holidays[['year', 'last_week', 'last_week_holiday']]\n                                       , how='left', left_on=['year', 'week_of_year'],right_on=['year', 'last_week'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Marking the weeks after the holiday\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_full_4h = df_train_full_4h.merge(df_4holidays[['year', 'next_week', 'next_week_holiday']]\n                                       , how='left', left_on=['year', 'week_of_year'],right_on=['year', 'next_week'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.options.display.max_columns=None","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_full_4h.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5 - Bivariate Analysis","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"\nAt this stage, the idea is to analyze the main variables of the base with the variable response of the problem, in this case the amount of weekly sales.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 5.1 - Store x Weekly Sales","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data = df_train_full_4h.groupby('Store').agg({'Weekly_Sales':'mean', 'Type':'max', 'Size':'mean'}).reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.groupby('Type').agg({'Weekly_Sales':'mean'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.scatterplot(x=\"Store\", y=\"Weekly_Sales\",hue='Type', data=data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is a bias that bigger stores will sell more, so let's weigh sales by store size and see what happens\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data['sales_per_size'] = data['Weekly_Sales'] /data['Size']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.groupby('Type').agg({'sales_per_size':'mean'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.scatterplot(x=\"Store\", y=\"sales_per_size\",hue='Type', data=data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Wow! Type C stores are twice as efficient in their sales considering their size ... given that a larger store can generate a higher fixed cost.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 5.2 - Dept vs Sales","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Are there departments that sell more than others?\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data = df_train_full_4h.groupby(['Type','Dept']).agg({'Weekly_Sales':'mean'}).reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.scatterplot(x=\"Dept\", y=\"Weekly_Sales\",hue='Type' ,data=data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We conclude that the store and department type variables discriminate well as to the gross value sold\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 5.3 - Temperature vs Sales","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Does temperature influence people's propensity to spend more money?\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data = df_train_full_4h.groupby(['Temperature']).agg({'Weekly_Sales':'mean'}).reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.scatterplot(x=\"Temperature\", y=\"Weekly_Sales\" ,data=data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['temp_bins'] = pd.cut(data.Temperature, bins=10).astype(str)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.lineplot(x=\"temp_bins\", y=\"Weekly_Sales\" ,data=data.groupby('temp_bins').agg({'Weekly_Sales':'mean'}).reset_index())\nplt.xticks(rotation=45);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.scatterplot(x=\"Temperature\", y=\"Weekly_Sales\",hue='Type' ,data=df_train_full_4h)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Analyzing the temperature alone does not tell us much, the biggest purchases happen at the average temperature observed in the dataset, which is the average temperature for the regions analyzed\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 5.4 - Fuel Price vs Sales","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Does cheaper gasoline influence people's propensity to go out more to buy?\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.scatterplot(x=\"Fuel_Price\", y=\"Weekly_Sales\",hue='Type' ,data=df_train_full_4h)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Another variable that alone doesn't tell us much, later we will try to create new variables using the fuel price \n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 5.5 - MarkDown1 vs Sales","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"How does this promotion related variable behave?\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.scatterplot(x=\"MarkDown1\", y=\"Weekly_Sales\",hue='Type' ,data=df_train_full_4h)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Apparently MarkDown1 has a slightly negative relationship to sales ... lower MarkDown, higher sales?\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 5.6 - CPI vs Sales","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Do regions with a lower CPI have a propensity to spend more?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.scatterplot(x=\"CPI\", y=\"Weekly_Sales\",hue='Type' ,data=df_train_full_4h)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Nice! As expected, regions with a lower CPI have a slight tendency to spend more than regions with a higher CPI, customers who experience less price increases spend more\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 5.7 - Unemployment vs Sales","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Do places with a lower unemployment rate spend more money?\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.scatterplot(x=\"Unemployment\", y=\"Weekly_Sales\",hue='Type' ,data=df_train_full_4h)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Another hypothesis confirmed here, stores in regions with a high unemployment rate, spend less !!\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 5.8 - Holiday\tvs Sales","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Certainly these special holidays drive people to spend more, shall we check?\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_full_4h.groupby('IsHoliday').agg({'Weekly_Sales':'mean'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.lineplot(x=\"IsHoliday\", y=\"Weekly_Sales\", markers=True ,data=df_train_full_4h.groupby('IsHoliday').agg({'Weekly_Sales':'mean'}).reset_index())\nplt.xticks(rotation=45);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_full_4h.groupby('week_holiday').agg({'Weekly_Sales':'mean'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.lineplot(x=\"week_holiday\", y=\"Weekly_Sales\", markers=True ,data=df_train_full_4h.groupby('week_holiday').agg({'Weekly_Sales':'mean'}).reset_index())\nplt.xticks(rotation=45);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_full_4h.groupby('last_week_holiday').agg({'Weekly_Sales':'mean'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.lineplot(x=\"last_week_holiday\", y=\"Weekly_Sales\", markers=True ,data=df_train_full_4h.groupby('last_week_holiday').agg({'Weekly_Sales':'mean'}).reset_index())\nplt.xticks(rotation=45);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_full_4h.groupby('next_week_holiday').agg({'Weekly_Sales':'mean'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.lineplot(x=\"next_week_holiday\", y=\"Weekly_Sales\", markers=True ,data=df_train_full_4h.groupby('next_week_holiday').agg({'Weekly_Sales':'mean'}).reset_index())\nplt.xticks(rotation=45);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Weeks that discriminate most: Thanksgiving week, and one week before Christmas\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 6 - Feature Engineering","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_full_4h.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 6.1 - Creating new features","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 6.1.1 - The region's propensity to buy","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We know that a low CPI rate, a low unemployment rate, and cheap fuel price are factors that encourage people to spend money. So we are going to create a variable with these 3 factors, which we will call propensity to buy the region\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_full_4h['prop_to_buy'] =  ((df_train_full_4h.Temperature * (100 - df_train_full_4h.Unemployment) ) / (df_train_full_4h.CPI * df_train_full_4h.Fuel_Price ))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.scatterplot(x=\"prop_to_buy\", y=\"Weekly_Sales\", hue='Type' ,data=df_train_full_4h)\n#plt.xticks(rotation=45);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.jointplot(x=\"prop_to_buy\", y=\"Weekly_Sales\" ,data=df_train_full_4h,\n                  kind=\"reg\", truncate=False,\n                  #xlim=(0, 60), ylim=(0, 12),\n                  color=\"m\"\n                  #, height=7\n                 )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 6.1.2 - Locomotion cost\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Here we will try to create a variable that I will call \"transportation cost\", which relates the CPI and the fuel price\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_full_4h['move_cost'] = df_train_full_4h.CPI / df_train_full_4h.Fuel_Price","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.scatterplot(x=\"move_cost\", y=\"Weekly_Sales\", hue='Type' ,data=df_train_full_4h)\n#plt.xticks(rotation=45);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.jointplot(x=\"move_cost\", y=\"Weekly_Sales\" ,data=df_train_full_4h,\n                  kind=\"reg\", truncate=False,\n                  #xlim=(0, 60), ylim=(0, 12),\n                  color=\"m\"\n                  #, height=7\n                 )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 6.1.3 - Revenue potential","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Here I will create a variable that I call revenue potential, which relates the size of the store to the local unemployment rate\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_full_4h['revenue_potential'] = (100 * df_train_full_4h.Unemployment) * df_train_full_4h.Size","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.scatterplot(x=\"revenue_potential\", y=\"Weekly_Sales\", hue='Type' ,data=df_train_full_4h)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.jointplot(x=\"revenue_potential\", y=\"Weekly_Sales\" ,data=df_train_full_4h,\n                  kind=\"reg\", truncate=False,\n                  #xlim=(0, 60), ylim=(0, 12),\n                  color=\"m\"\n                  #, height=7\n                 )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 6.2 - Feature Selection","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"In this step we will prepare the dataset for the training, as we intend to use a linear regression, we have to analyze the correction of the variables, fill in missings, normalize numerical variables, among other treatments\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"msno.matrix(df_train_full_4h);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let’s drop some variables that we found in the exploratory analysis that didn’t make sense in modeling\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_sel = df_train_full_4h.drop(['MarkDown2'\n                                     ,'MarkDown3'\n                                     ,'MarkDown4'\n                                     ,'MarkDown5'\n                                     ,'year'\n                                     ,'Date'\n                                     ,'current_week'\n                                      ,'last_week'\n                                      ,'next_week'\n                                      ,'week_of_year'\n                                     ], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_sel.MarkDown1 = df_train_sel.MarkDown1.fillna(df_train_sel.MarkDown1.dropna().median())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_sel.next_week_holiday.fillna('None', inplace=True)\ndf_train_sel.last_week_holiday.fillna('None', inplace=True)\ndf_train_sel.week_holiday.fillna('None', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Variables to normalize\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"FET_TO_SCALER = [\n    'Size'\n    ,'Temperature'\n    ,'Fuel_Price'\n    ,'MarkDown1'\n    ,'CPI'\n    ,'Unemployment'\n    ,'prop_to_buy'\n    ,'move_cost'\n    ,'revenue_potential'\n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_sel[FET_TO_SCALER] = scaler.fit_transform(df_train_sel[FET_TO_SCALER])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_sel.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"msno.matrix(df_train_sel);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_sel.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_sel.Store = df_train_sel.Store.astype(str)\ndf_train_sel.Dept = df_train_sel.Dept.astype(str)\ndf_train_sel.Type = df_train_sel.Type.astype(str)\ndf_train_sel.week_holiday = df_train_sel.week_holiday.astype(str)\ndf_train_sel.last_week_holiday = df_train_sel.last_week_holiday.astype(str)\ndf_train_sel.next_week_holiday = df_train_sel.next_week_holiday.astype(str)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Transforming categorical variables into binary variables\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_dummies = pd.get_dummies(df_train_sel)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_dummies.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_dummies.sample(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list(df_train_dummies.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"GP1 = ['Size'\n       ,'Temperature'\n       ,'Fuel_Price'\n       ,'MarkDown1'\n       ,'CPI'\n       ,'Unemployment'\n       ,'IsHoliday'\n       ,'prop_to_buy'\n       ,'move_cost'\n       ,'revenue_potential'\n    \n]\nGP2 = [\n    'week_holiday_Christmas'\n    ,'week_holiday_Labor_Day'\n    ,'week_holiday_None'\n    ,'week_holiday_Super_Bowl'\n    ,'week_holiday_Thanksgiving'\n    ,'last_week_holiday_Labor_Day'\n    ,'last_week_holiday_None'\n    ,'last_week_holiday_Super_Bowl'\n    ,'last_week_holiday_Thanksgiving'\n    ,'next_week_holiday_Christmas'\n    ,'next_week_holiday_Labor_Day'\n    ,'next_week_holiday_None'\n    ,'next_week_holiday_Super_Bowl'\n    ,'next_week_holiday_Thanksgiving'\n]\n\nGP3 = [\n    'Type_A'\n    ,'Type_B'\n    ,'Type_C'\n]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's look at the correlation of the variables. The idea here is to eliminate variables that are very correlated with each other, either positively or negatively. In my criteria I will remove variables that have a correlation module> 0.6","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(11, 6))\nsns.heatmap(df_train_dummies[GP1].corr(), annot=True, linewidths=.5, ax=ax);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(15, 8))\nsns.heatmap(df_train_dummies[GP2].corr(), annot=True, linewidths=.5, ax=ax);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(9, 6))\nsns.heatmap(df_train_dummies[GP3].corr(), annot=True, linewidths=.5, ax=ax);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Variables selected for modeling\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"FEATURES_TO_MODEL = [\n\n 'MarkDown1',\n 'Unemployment',\n 'IsHoliday',\n 'prop_to_buy',\n 'move_cost',\n 'revenue_potential',\n \n 'Store_1',\n 'Store_10',\n 'Store_11',\n 'Store_12',\n 'Store_13',\n 'Store_14',\n 'Store_15',\n 'Store_16',\n 'Store_17',\n 'Store_18',\n 'Store_19',\n 'Store_2',\n 'Store_20',\n 'Store_21',\n 'Store_22',\n 'Store_23',\n 'Store_24',\n 'Store_25',\n 'Store_26',\n 'Store_27',\n 'Store_28',\n 'Store_29',\n 'Store_3',\n 'Store_30',\n 'Store_31',\n 'Store_32',\n 'Store_33',\n 'Store_34',\n 'Store_35',\n 'Store_36',\n 'Store_37',\n 'Store_38',\n 'Store_39',\n 'Store_4',\n 'Store_40',\n 'Store_41',\n 'Store_42',\n 'Store_43',\n 'Store_44',\n 'Store_45',\n 'Store_5',\n 'Store_6',\n 'Store_7',\n 'Store_8',\n 'Store_9',\n \n 'Dept_1',\n 'Dept_10',\n 'Dept_11',\n 'Dept_12',\n 'Dept_13',\n 'Dept_14',\n 'Dept_16',\n 'Dept_17',\n 'Dept_18',\n 'Dept_19',\n 'Dept_2',\n 'Dept_20',\n 'Dept_21',\n 'Dept_22',\n 'Dept_23',\n 'Dept_24',\n 'Dept_25',\n 'Dept_26',\n 'Dept_27',\n 'Dept_28',\n 'Dept_29',\n 'Dept_3',\n 'Dept_30',\n 'Dept_31',\n 'Dept_32',\n 'Dept_33',\n 'Dept_34',\n 'Dept_35',\n 'Dept_36',\n 'Dept_37',\n 'Dept_38',\n 'Dept_39',\n 'Dept_4',\n 'Dept_40',\n 'Dept_41',\n 'Dept_42',\n 'Dept_43',\n 'Dept_44',\n 'Dept_45',\n 'Dept_46',\n 'Dept_47',\n 'Dept_48',\n 'Dept_49',\n 'Dept_5',\n 'Dept_50',\n 'Dept_51',\n 'Dept_52',\n 'Dept_54',\n 'Dept_55',\n 'Dept_56',\n 'Dept_58',\n 'Dept_59',\n 'Dept_6',\n 'Dept_60',\n 'Dept_65',\n 'Dept_67',\n 'Dept_7',\n 'Dept_71',\n 'Dept_72',\n 'Dept_74',\n 'Dept_77',\n 'Dept_78',\n 'Dept_79',\n 'Dept_8',\n 'Dept_80',\n 'Dept_81',\n 'Dept_82',\n 'Dept_83',\n 'Dept_85',\n 'Dept_87',\n 'Dept_9',\n 'Dept_90',\n 'Dept_91',\n 'Dept_92',\n 'Dept_93',\n 'Dept_94',\n 'Dept_95',\n 'Dept_96',\n 'Dept_97',\n 'Dept_98',\n 'Dept_99',\n \n 'Type_A',\n \n 'Type_C',\n \n 'week_holiday_Christmas',\n 'week_holiday_Labor_Day',\n 'week_holiday_None',\n 'week_holiday_Super_Bowl',\n 'week_holiday_Thanksgiving',\n 'last_week_holiday_Labor_Day',\n 'last_week_holiday_None',\n 'last_week_holiday_Super_Bowl',\n 'last_week_holiday_Thanksgiving',\n 'next_week_holiday_Christmas',\n 'next_week_holiday_Labor_Day',\n 'next_week_holiday_None',\n 'next_week_holiday_Super_Bowl',\n 'next_week_holiday_Thanksgiving'\n]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 7 - Pipeline to models","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"In this step we will create pipelines for handling the modeling dataset for both models we intend to use\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline_regression = Pipeline([\n                                ('createFeatures', CreateFeatures())                         \n                                ,('fillNaValues', FillNaValues()) \n                                ,('featureSelector', FeatureSelector())\n                                ,('scaler', StandardScaler())\n                                \n                               ])\n\npipeline_xgboost = Pipeline([\n                                ('createFeatures', CreateFeatures())\n                                ,('featureSelector', FeatureSelector1())\n                                \n                               ])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 8 - Modeling","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 8.1 - Linear Regression (Elastic Net)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"In this step, we will train a Linear Regression Elastic Net that has the regularization parameters l1 and l2 combined (Lasso + Ridge). This is a simpler model computationally speaking and has low variance.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df_train_full.drop('Weekly_Sales', axis=1)\ny = df_train_full['Weekly_Sales']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Separating the dataset between training and validation\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_val.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Transforming the training and validation dataset with the Pipeline made for regression\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_LR = pipeline_regression.fit_transform(X_train) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_val_LR = pipeline_regression.transform(X_val)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Training the model:\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_model = train_linear_regression(X_train_LR, y_train, X_val_LR, y_val)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 8.1.1 - Randomized Search for LR Elastic Net","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We will try to better calibrate the parameters of Elastic Net and try to obtain better performance. In this step we will use a Randomized Search.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"hyperparameters = {\"max_iter\": [1, 5, 10],\n                      \"alpha\": [0.0001, 0.001, 0.01, 0.1, 1, 10, 100],\n                      \"l1_ratio\": np.arange(0.0, 1.0, 0.1)}\n    \n    \nscoring = {'MAE': make_scorer(mean_absolute_error), 'r2': make_scorer(r2_score)}\n\n# Create randomized search 5-fold cross validation \nrand_ser = RandomizedSearchCV(lr_model\n                         , hyperparameters\n                         , random_state=42\n                        , n_iter=100\n                         , cv=5\n                         , verbose=0\n                         , n_jobs=-1\n                         , scoring=scoring\n                         ,refit='r2'\n                         )\n\n# Fit randomized search\nbest_model_lr = rand_ser.fit(X_train_LR, y_train)\n\nprint('R^2 = {}'.format(r2_score(y_val, best_model_lr.predict(X_val_LR))))\nprint('MAE = {}'.format(mean_absolute_error(y_val, best_model_lr.predict(X_val_LR)) ))\nprint('RMSE = {}'.format(mean_squared_error(y_val, best_model_lr.predict(X_val_LR), squared=False) ))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 8.2 - XGBOOST Regressor","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"In this step we will try to train a more complex model with a greater variance, a Gradient Boosting (decision tree ensemble), in this case the XGBOOST. This model is very useful because it requires little treatment in the input dataset, accepts nulls, can work well with correlated variables, and does not need to normalize variables, as in the case of Linear Regression\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_XG = pipeline_xgboost.fit_transform(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_val_XG = pipeline_xgboost.transform(X_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb = train_xgboost_regressor(X_train_XG, y_train, X_val_XG, y_val)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Incredible performance!\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 8.2.1 - Model Interpretability - Shapley Values","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Another nice thing about XGBOOST is that it has great interpretability of the most important variables, which is a very good thing to validate if the model is making sense, rationally speaking at the level of variables, and not just looking at performance metrics.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"In this step, we will use a tool called Shapley Values. It has 3 dimensions to analyze. On the y-axis, it is the importance of the variable, from top to bottom (most important to least important). Axis x, it is the strength of the variable, if it points to the left, it contributes to a lower output value of the model, and to the right, a higher output value of the model. The other dimension is the intensity of the variable, represented by colors, a lower intensity represented by blue, and a higher intensity represented by red.\n","execution_count":null},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"explainer = shap.TreeExplainer(xgb)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shap.initjs()\nshap_values = explainer.shap_values(X_train_XG, check_additivity=False)\nshap.summary_plot(shap_values, X_train_XG)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we see that the store size and departments are variable with high predictive power\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 8.3 - Ensemble XGBOOST + ELastic Net","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"This step, we go further! How about joining XGBOOST with Elastic Net? Although XGBOOST performed much better than regression, there must be cases where the regression shows predictions with a smaller error than XGBOOST, so we will use the prediction of the two models and train a third regression in order to obtain a smaller error . Does it work?\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model_xgb_elatcnet = ensemble_xgb_elastic_net(model_1=lr_model\n                         , model_2=xgb\n                         , X_train_1=X_train_LR\n                         , X_train_2=X_train_XG\n                         , y_train = y_train\n                         , X_val_1 = X_val_LR\n                         , X_val_2 = X_val_XG\n                         , y_val = y_val\n                        )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It worked! We achieved a minor error by joining the two models!\n","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}