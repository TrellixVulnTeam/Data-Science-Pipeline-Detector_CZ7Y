{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import zipfile\nfrom subprocess import check_output\ntrain_data = \"train.csv\"\nfeature_data=\"features.csv\"\ntest_data=\"test.csv\"\n\ndateparse = lambda x: pd.datetime.strptime(x, '%Y-%m-%d')\n\nwith zipfile.ZipFile(\"../input/walmart-recruiting-store-sales-forecasting/\"+train_data+\".zip\",\"r\") as z:\n    z.extractall(\".\")\nprint(check_output([\"ls\", \"train.csv\"]).decode(\"utf8\"))\ntrain_df=pd.read_csv(\"train.csv\")\n#train_df = pd.read_csv(\"train.csv\", parse_dates=['Date'], date_parser=dateparse,index_col=['Date'])\n\nwith zipfile.ZipFile(\"../input/walmart-recruiting-store-sales-forecasting/\"+feature_data+\".zip\",\"r\") as z:\n    z.extractall(\".\")\n\nprint(check_output([\"ls\", \"features.csv\"]).decode(\"utf8\"))\n#feature_df=pd.read_csv(\"features.csv\",parse_dates=['Date'], date_parser=dateparse,index_col=['Date'])\nfeature_df=pd.read_csv(\"features.csv\")\nstore_df=pd.read_csv(\"../input/walmart-recruiting-store-sales-forecasting/stores.csv\")\n\n# with zipfile.ZipFile(\"../input/walmart-recruiting-store-sales-forecasting/\"+test_data+\".zip\",\"r\") as z:\n#     z.extractall(\".\")\n\n# print(check_output([\"ls\", \"test.csv\"]).decode(\"utf8\"))\n# #test_df=pd.read_csv(\"test.csv\",parse_dates=['Date'], date_parser=dateparse,index_col=['Date'])\n# test_df=pd.read_csv(\"test.csv\")\n\n\n\ntrain_df=pd.merge(train_df, store_df, on='Store')\ntrain_df=pd.merge(train_df, feature_df, on=['Store','Date'])\ntrain_df['Date'] = pd.to_datetime(train_df['Date'],format='%Y/%m/%d')\n\n\ntrain_df['Month'] = train_df['Date'].apply(lambda x: x.month)\n# category_dummies=pd.get_dutmmies(train_df[['Month']],prefix=['Month'])\n# train_df=pd.concat([train_df,category_dummies],axis=1).drop(['Month'],axis=1)\n\n\ntrain_df.to_csv(\"final_df.csv\",index=False)\ntrain_df = pd.read_csv(\"final_df.csv\",index_col=['Date'])\n#setting date as index for ordered data\ntrain_df['IsHoliday_x'] = train_df['IsHoliday_x'].apply(lambda x: 1 if x==True else 0)\ncategory_dummies=pd.get_dummies(train_df[['Type']],prefix=['Type'])\ntrain_df=pd.concat([train_df,category_dummies],axis=1).drop(['Type'],axis=1)\ntrain_df=train_df.drop([\"IsHoliday_y\"],axis=1)\n\n\ncategory_dummies=pd.get_dummies(train_df['Month'],prefix='month')\ntrain_df=pd.concat([train_df,category_dummies],axis=1).drop(['Month'],axis=1)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.figure()\nplt.scatter(train_df['Fuel_Price'] , train_df['Weekly_Sales'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure()\nplt.scatter(train_df['CPI'] , train_df['Weekly_Sales'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure()\nplt.scatter(train_df['Unemployment'] , train_df['Weekly_Sales'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure()\nplt.scatter(train_df['MarkDown3'] , train_df['Weekly_Sales'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure()\nplt.scatter(train_df['MarkDown5'] , train_df['Weekly_Sales'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Removing Irevelant Coloumns which have little or no impact on target variable\n\ntrain_df=train_df.drop(['CPI','Fuel_Price','Unemployment'],axis=1)\n\n#Coloumns missing from the test data, repeated modelling with various combinations also showed Markdown3 adds very little varince to our data hence removing it\ntrain_df=train_df.drop(['month_8','month_9','month_10','MarkDown3'],axis=1)\n\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nimport numpy as np\nx_train, x_val, y_train, y_val = train_test_split(train_df.drop(columns = ['Weekly_Sales']), \n                                                    train_df['Weekly_Sales'], \n                                                    test_size=0.2, \n                                                    random_state=1)\nw_train = x_train['IsHoliday_x'].replace(1, 5).replace(0, 1)\nw_val=x_val['IsHoliday_x'].replace(1, 5).replace(0, 1)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"                                                                            ****Random Forrest****"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nrfm = RandomForestRegressor(n_estimators = 100,criterion='mse')\nrf_xtrain=x_train.fillna(0)\nrf_ytrain=y_train.fillna(0)\nrf_xval=x_val.fillna(0)\nrf_yval=y_val.fillna(0)\nrfm.fit(rf_xtrain,rf_ytrain)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n\nwith zipfile.ZipFile(\"../input/walmart-recruiting-store-sales-forecasting/\"+test_data+\".zip\",\"r\") as z:\n    z.extractall(\".\")\n\nprint(check_output([\"ls\", \"test.csv\"]).decode(\"utf8\"))\n# test_df=pd.read_csv(\"test.csv\",parse_dates=['Date'], date_parser=dateparse,index_col=['Date'])\n\ntest_df=pd.read_csv(\"test.csv\")\n\ntest_df=pd.merge(test_df, store_df, on='Store')\ntest_df=pd.merge(test_df, feature_df, on=['Store','Date'])\ntest_df['Date'] = pd.to_datetime(test_df['Date'],format='%Y/%m/%d')\n\ntest_df['Month'] = test_df['Date'].apply(lambda x: x.month)\n\ntest_df.to_csv(\"final_test_df.csv\",index=False)\ntest_df = pd.read_csv(\"final_test_df.csv\",index_col=['Date'])\ntest_df['IsHoliday_x'] = test_df['IsHoliday_x'].apply(lambda x: 1 if x==True else 0)\ncategory_dummies=pd.get_dummies(test_df[['Type']],prefix=['Type'])\ntest_df=pd.concat([test_df,category_dummies],axis=1).drop(['Type'],axis=1)\n# test_df.head(5)\ntest_df=test_df.drop([\"IsHoliday_y\"],axis=1)\ncategory_dummies=pd.get_dummies(test_df['Month'],prefix='month')\ntest_df=pd.concat([test_df,category_dummies],axis=1).drop(['Month'],axis=1)\n\ntest_df=test_df.drop(['CPI','Fuel_Price','Unemployment','MarkDown3'],axis=1)\n\nw_test=test_df['IsHoliday_x'].replace(1, 5).replace(0, 1)\n\n#dtest = xgb.DMatrix(data = test_df,weight=w_test)\n#lgb_test = lgb.Dataset(test_df)\n\n# test_data = test_df\n# #val_labels = y_val\n# test_weight = w_test\n# test_dataset = Pool(test_data,weight=test_weight,cat_features=categorical_var)\n\n# test_df['Weekly_Sales']=cat_boost_model.predict(test_dataset)\ntest_df['Weekly_Sales']=rfm.predict(test_df.fillna(0))\nsolution_df=test_df\nsolution_df=solution_df.drop([\"IsHoliday_x\",\"Size\",\"Temperature\",\"MarkDown1\",'MarkDown2','MarkDown4','MarkDown5','Type_A','Type_B','Type_C'],axis=1)\n#solution_df=solution_df.drop(['CPI','Fuel_Price','Unemployment'],axis=1)\nsolution_df=solution_df[['Store','Dept','Weekly_Sales']]\nsolution_df['Id']=(solution_df['Store']).astype(str)+\"_\"+(solution_df['Dept']).astype(str)+'_'+(solution_df.index).astype(str)\nsolution_df=solution_df.reset_index(drop=True)\nsolution_df=solution_df.drop([\"Store\",\"Dept\"],axis=1)\ncols = solution_df.columns.tolist()\ncols\ncols=['Id','Weekly_Sales']\nsolution_df = solution_df[cols]\nsolution_df.to_csv(\"rf_solution3.csv\",index=False)\nsolution_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error\npred = rfm.predict(rf_xval)\nmean_absolute_error(rf_yval, pred,sample_weight=w_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import r2_score\nprint(r2_score(rf_yval, pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"         ************************************************************************************XGBoost**********************************************************************************"},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\ndtrain = xgb.DMatrix(data = x_train, label = y_train,weight=w_train)\ndval = xgb.DMatrix(data = x_val, label = y_val,weight=w_val)\nwatchlist = [(dtrain, 'train'), (dval, 'eval')]\nnum_round = 10000\nparam = {'max_depth':10,\n         'eta': 0.3,\n         'silent':1,\n         'objective':'reg:squarederror',\n         #'disable_default_eval_metric':1,\n         'eval_metric': 'mae',\n         #'feval':'custom_eval',\n          #'gamma': 0.10983,\n          'lambda': 200,\n          'alpha': 10,\n          #'subsample' : 0.78,\n          #'min_child_weight': 10,\n          #'colsample_bytree' :0.8 \n          #,'colsample_bynode' : 0.6\n          #,'colsample_bylevel':0.7\n          #,'scale_pos_weight' : 0.276,\n          'maximize' : 'FALSE',\n          'n_jobs' : -1\n         #,'base_score' : ???\n         #,'max_delta_step' : 5\n        }\n\nxgb_model = xgb.train(param, dtrain, num_round, watchlist, early_stopping_rounds = 20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = xgb_model.predict(dval)\nmean_absolute_error(y_val, pred,sample_weight=w_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(r2_score(y_val, pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df=pd.read_csv(\"test.csv\")\n\ntest_df=pd.merge(test_df, store_df, on='Store')\ntest_df=pd.merge(test_df, feature_df, on=['Store','Date'])\ntest_df['Date'] = pd.to_datetime(test_df['Date'],format='%Y/%m/%d')\n\ntest_df['Month'] = test_df['Date'].apply(lambda x: x.month)\n\n\ntest_df.to_csv(\"final_test_df.csv\",index=False)\ntest_df = pd.read_csv(\"final_test_df.csv\",index_col=['Date'])\ntest_df['IsHoliday_x'] = test_df['IsHoliday_x'].apply(lambda x: 1 if x==True else 0)\ncategory_dummies=pd.get_dummies(test_df[['Type']],prefix=['Type'])\ntest_df=pd.concat([test_df,category_dummies],axis=1).drop(['Type'],axis=1)\n# test_df.head(5)\ntest_df=test_df.drop([\"IsHoliday_y\"],axis=1)\n\ncategory_dummies=pd.get_dummies(test_df['Month'],prefix='month')\ntest_df=pd.concat([test_df,category_dummies],axis=1).drop(['Month'],axis=1)\n\n\ntest_df=test_df.drop(['CPI','Fuel_Price','Unemployment','MarkDown3'],axis=1)\nw_test=test_df['IsHoliday_x'].replace(1, 5).replace(0, 1)\ndtest = xgb.DMatrix(data = test_df,weight=w_test)\ntest_df['Weekly_Sales']=xgb_model.predict(dtest)\nsolution_df=test_df\nsolution_df=solution_df.drop([\"IsHoliday_x\",\"Size\",\"Temperature\",\"MarkDown1\",'MarkDown2','MarkDown4','MarkDown5','Type_A','Type_B','Type_C'],axis=1)\nsolution_df=solution_df[['Store','Dept','Weekly_Sales']]\nsolution_df['Id']=(solution_df['Store']).astype(str)+\"_\"+(solution_df['Dept']).astype(str)+'_'+(solution_df.index).astype(str)\nsolution_df=solution_df.reset_index(drop=True)\nsolution_df=solution_df.drop([\"Store\",\"Dept\"],axis=1)\ncols = solution_df.columns.tolist()\ncols\ncols=['Id','Weekly_Sales']\nsolution_df = solution_df[cols]\nsolution_df.to_csv(\"solution_xgboost_final.csv\",index=False)\nsolution_df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"                                                    ******************************* Light GBM  ********************************************"},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgb\nlgb_train = lgb.Dataset(x_train, y_train)\nlgb_val = lgb.Dataset(x_val, y_val)\nlgb_params = {\n    'boosting_type': 'dart',\n    'objective': 'regression_l1',\n    'metric': 'mean_absolute_error',\n    'max_depth' : 10,\n    'num_leaves' : 250,\n    \n    #'feature_fraction':0.9,\n    'learning_rate': 0.5,\n    #'num_threads' : -1,\n    #'scale_pos_weight' : ???\n    'early_stopping_round' : 20,\n    # min_data_in_leaf = ???,\n     #'pos_bagging_fraction' : 0.15,\n     #'neg_bagging_fraction' : 0.45,\n     #'bagging_freq' : 200,\n    # max_delta_step = ???,\n    #'top_rate' : 0.8\n    #'other_rate' : ???\n    #'lambda_l1' : 35\n    #'lambda_l1' :15,\n    #'lambda_l2' : 5\n}\nlgb_gbm_model = lgb.train(params = lgb_params, train_set = lgb_train,\n                num_boost_round = 4000, valid_sets = [lgb_val, lgb_train],\n               valid_names = ['Evaluation', 'Train'])\n# lgb_gbm_model = lgb.cv(params = lgb_params, train_set = lgb_train,\n#                 num_boost_round = 20000, valid_sets = [lgb_val, lgb_train],\n#                valid_names = ['Evaluation', 'Train'],nfold=5)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = lgb_gbm_model.predict(x_val)\nmean_absolute_error(y_val, pred,sample_weight=w_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(r2_score(y_val, pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df=pd.read_csv(\"test.csv\")\n\ntest_df=pd.merge(test_df, store_df, on='Store')\ntest_df=pd.merge(test_df, feature_df, on=['Store','Date'])\ntest_df['Date'] = pd.to_datetime(test_df['Date'],format='%Y/%m/%d')\ntest_df['Month'] = test_df['Date'].apply(lambda x: x.month)\n\ntest_df.to_csv(\"final_test_df.csv\",index=False)\ntest_df = pd.read_csv(\"final_test_df.csv\",index_col=['Date'])\ntest_df['IsHoliday_x'] = test_df['IsHoliday_x'].apply(lambda x: 1 if x==True else 0)\ncategory_dummies=pd.get_dummies(test_df[['Type']],prefix=['Type'])\ntest_df=pd.concat([test_df,category_dummies],axis=1).drop(['Type'],axis=1)\n# test_df.head(5)\ntest_df=test_df.drop([\"IsHoliday_y\"],axis=1)\n\ncategory_dummies=pd.get_dummies(test_df['Month'],prefix='month')\ntest_df=pd.concat([test_df,category_dummies],axis=1).drop(['Month'],axis=1)\n\ntest_df=test_df.drop(['CPI','Fuel_Price','Unemployment','MarkDown3'],axis=1)\nw_test=test_df['IsHoliday_x'].replace(1, 5).replace(0, 1)\n\n#dtest = xgb.DMatrix(data = test_df,weight=w_test)\nlgb_test = lgb.Dataset(test_df)\n\ntest_df['Weekly_Sales']=lgb_gbm_model.predict(test_df)\nsolution_df=test_df\nsolution_df=solution_df.drop([\"IsHoliday_x\",\"Size\",\"Temperature\",\"MarkDown1\",'MarkDown2','MarkDown4','MarkDown5','Type_A','Type_B','Type_C'],axis=1)\nsolution_df=solution_df[['Store','Dept','Weekly_Sales']]\nsolution_df['Id']=(solution_df['Store']).astype(str)+\"_\"+(solution_df['Dept']).astype(str)+'_'+(solution_df.index).astype(str)\nsolution_df=solution_df.reset_index(drop=True)\nsolution_df=solution_df.drop([\"Store\",\"Dept\"],axis=1)\ncols = solution_df.columns.tolist()\ncols\ncols=['Id','Weekly_Sales']\nsolution_df = solution_df[cols]\nsolution_df.to_csv(\"solution_lightgbm.csv\",index=False)\nsolution_df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"            **************************************************** CatBoost *********************************************************"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train_cat = x_train\nx_val_cat = x_val\npredictors = x_train_cat\ncategorical_var = np.where((predictors.dtypes != np.float64) & (predictors.dtypes != np.int64))[0]\nprint('Categorical Variables indices :',categorical_var)\nfrom catboost import Pool\ntrain_data = x_train\ntrain_labels = y_train\ntrain_weight = w_train\ntrain_dataset = Pool(train_data,\n                     train_labels,\n                     weight=train_weight,cat_features=categorical_var)\n\n\n\nval_data = x_val\nval_labels = y_val\nval_weight = w_val\nval_dataset = Pool(val_data,\n                     val_labels,\n                     weight=val_weight,cat_features=categorical_var)\n\nfrom catboost import CatBoostRegressor, Pool, cv\ncat_boost_model = CatBoostRegressor(\n    loss_function = 'MAE',\n    eval_metric='MAE',\n    #hints=skip_train~false,\n    #random_seed=42,\n    iterations = 10000,\n    has_time= True,\n    learning_rate = 0.4,\n    early_stopping_rounds = 20,\n    l2_leaf_reg = 200,\n    per_float_feature_quantization='3:border_count=1024',\n    depth = 10,\n    bagging_temperature=0,\n    #task_type='GPU',\n    #subsample=0.7,\n    #colsample_bylevel=0.7\n    grow_policy='SymmetricTree',\n    #min_data_in_leaf=10\n    random_strength=2\n    \n)\n\n# cat_boost_model.fit(\n#     x_train_cat, y_train\n#     ,cat_features=categorical_var,\n#     eval_set=(x_val_cat, y_val)\n#     , plot = True,use_best_model=True\n# )\n\n\ncat_boost_model.fit(\n    train_dataset,\n    eval_set=val_dataset\n    ,plot = True,use_best_model=True\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = cat_boost_model.predict(val_dataset)\nmean_absolute_error(y_val, pred,sample_weight=w_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(r2_score(y_val, pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df=pd.read_csv(\"test.csv\")\n\ntest_df=pd.merge(test_df, store_df, on='Store')\ntest_df=pd.merge(test_df, feature_df, on=['Store','Date'])\ntest_df['Date'] = pd.to_datetime(test_df['Date'],format='%Y/%m/%d')\n\ntest_df['Month'] = test_df['Date'].apply(lambda x: x.month)\n\ntest_df.to_csv(\"final_test_df.csv\",index=False)\ntest_df = pd.read_csv(\"final_test_df.csv\",index_col=['Date'])\ntest_df['IsHoliday_x'] = test_df['IsHoliday_x'].apply(lambda x: 1 if x==True else 0)\ncategory_dummies=pd.get_dummies(test_df[['Type']],prefix=['Type'])\ntest_df=pd.concat([test_df,category_dummies],axis=1).drop(['Type'],axis=1)\n# test_df.head(5)\ntest_df=test_df.drop([\"IsHoliday_y\"],axis=1)\ncategory_dummies=pd.get_dummies(test_df['Month'],prefix='month')\ntest_df=pd.concat([test_df,category_dummies],axis=1).drop(['Month'],axis=1)\n\n\n\ntest_df=test_df.drop(['CPI','Fuel_Price','Unemployment','MarkDown3'],axis=1)\n\n\n\nw_test=test_df['IsHoliday_x'].replace(1, 5).replace(0, 1)\n\n#dtest = xgb.DMatrix(data = test_df,weight=w_test)\n#lgb_test = lgb.Dataset(test_df)\n\ntest_data = test_df\n#val_labels = y_val\ntest_weight = w_test\ntest_dataset = Pool(test_data,weight=test_weight,cat_features=categorical_var)\n\n# test_df['Weekly_Sales']=cat_boost_model.predict(test_dataset)\ntest_df['Weekly_Sales']=cat_boost_model.predict(test_dataset)\n\n\n\nsolution_df=test_df\nsolution_df=solution_df.drop([\"IsHoliday_x\",\"Size\",\"Temperature\",\"MarkDown1\",'MarkDown2','MarkDown4','MarkDown5','Type_A','Type_B','Type_C'],axis=1)\n#solution_df=solution_df.drop(['CPI','Fuel_Price','Unemployment'],axis=1)\nsolution_df=solution_df[['Store','Dept','Weekly_Sales']]\nsolution_df['Id']=(solution_df['Store']).astype(str)+\"_\"+(solution_df['Dept']).astype(str)+'_'+(solution_df.index).astype(str)\nsolution_df=solution_df.reset_index(drop=True)\nsolution_df=solution_df.drop([\"Store\",\"Dept\"],axis=1)\ncols = solution_df.columns.tolist()\ncols\ncols=['Id','Weekly_Sales']\nsolution_df = solution_df[cols]\nsolution_df.to_csv(\"solution_catboost.csv\",index=False)\nsolution_df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"********************************************************************HyperOPT Light GBM******************************************************************"},{"metadata":{"trusted":true},"cell_type":"code","source":"from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\nimport numpy as np\ndef objective(space):\n        print(space)\n        lgb_params = {\n            'boosting_type': 'dart',\n            'objective': 'regression_l1',\n            'metric': 'mean_absolute_error',\n            'max_depth' : space['max_depth'],\n            'num_leaves' : space['num_leaves'],\n\n            #'feature_fraction':0.9,\n            'learning_rate': 0.4,\n            #'num_threads' : -1,\n            #'scale_pos_weight' : ???\n            'early_stopping_round' : 20,\n            # min_data_in_leaf = ???,\n             #'pos_bagging_fraction' : 0.15,\n             #'neg_bagging_fraction' : 0.45,\n             #'bagging_freq' : 200,\n            # max_delta_step = ???,\n            #'top_rate' : 0.8\n            #'other_rate' : ???\n            #'lambda_l1' : 35\n            #'lambda_l1' :15,\n            'lambda_l2' : space['lambda_l2']\n        }\n        lgbm_model = lgb.train(params = lgb_params, train_set = lgb_train,\n                        num_boost_round = 2000, valid_sets = [lgb_val, lgb_train],\n                       valid_names = ['Evaluation', 'Train'])\n        pred = lgbm_model.predict(x_val)\n        mae=mean_absolute_error(y_val, pred,sample_weight=w_val)\n\n        return {'loss':mae, 'status': STATUS_OK }\n\nspace ={\n'max_depth':  hp.choice(\"x_depth\", np.arange(8,9,10, dtype=int)),\n#'min_child_weight': hp.quniform ('x_min_child', 1, 10, 1),\n#'subsample': hp.uniform ('x_subsample', 0.5, 1),\n'num_leaves':  hp.choice(\"x_num_leaves\", np.arange(250,300,400, dtype=int)),    \n#'gamma' : hp.uniform ('x_gamma', 10,30),\n'lambda_l2':  hp.choice(\"x_lambda_l2\", np.arange(200,300,500, dtype=int)),  \n# 'alpha' : hp.uniform ('x_alpha', 23,40),\n# 'colsample_bynode' : hp.uniform ('colsample_bynode', 0.6,1),\n#'reg_lambda' : hp.uniform ('x_reg_lambda', 0,1),\n#'eta' : hp.uniform ('x_eta', 0.3,1),\n#'max_delta_step': hp.quniform ('x_max_delta_step', 1, 10,1)\n}\n\n\n\ntrials = Trials()\nbest = fmin(fn=objective,\nspace=space,\nalgo=tpe.suggest,\nmax_evals=10,\n#trials=trials\n)\nprint (best)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\nfilename = 'randomforrest_model1.sav'\npickle.dump(rfm, open(filename, 'wb'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"filename = 'xgb_model.sav'\npickle.dump(xgb_model, open(filename, 'wb'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"filename = 'lgbm_model.sav'\npickle.dump(lgb_gbm_model, open(filename, 'wb'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"filename = 'catboost_model.sav'\npickle.dump(cat_boost_model, open(filename, 'wb'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}