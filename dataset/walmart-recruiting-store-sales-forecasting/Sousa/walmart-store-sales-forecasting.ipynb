{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **Walmart - Store Sales Forecasting**\n\nA competição Walmart - Store Sales Forecasting tem como principal objetivo utilizar dados históricos de vendas de 45 lojas do Walmart localizadas em diferentes regiões para fazer predições de vendas por departamento, loja e semana observada.\n\nUm ponto relevante neste estudo, são os eventos promocionais que a Walmart realiza antes dos feriados. Os principais feriados destacados na competição são: \n\n- Super Bowl: 12-Fev-10, 11-Fev-11, 10-Fev-12, 8-Fev-13\n- Dia do Trabalho: 10-Set-10, 9-Set-11, 7-Set-12, 6-Set -13\n- Ação de Graças: 26-Nov-10, 25-Nov-11, 23-Nov-12, 29-Nov-13\n- Natal: 31-Dez-10, 30-Dez-11, 28-Dez-12, 27-Dez -13\n\n\n### Solução\n\nA formulação do problema se caracteriza como um problema de série temporal. Como estamos tentando fazer predições de valores numéricos, como por exemplo: valor das vendas, preços, um volume, e assim por diante, temos uma modelagem preditiva do tipo regressão. Neste desafio nossa variável resposta é representada pela coluna Weekly_Sales.\n\n\nEste notebook está dividido nas seguintes seções:\n\n1.  **Leitura dos dados**\n2.  **Análise Exploratória dos dados**\n3.  **Engenharia de Features**\n4.  **Buscar o melhor algoritmo**\n5.  **Buscar os melhores parâmetros**\n6.  **Testes**\n7.  **Make Submission**\n8.  **Próximos passos**\n\nCada seção inicialmente contem as funções que seram utilizadas, e em seguida uma breve explicação.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from pandas.tseries.holiday import *\n\nimport math\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n!pip install plotly\n!pip install shap\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport seaborn as sns\nimport shap\nshap.initjs()\n\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.pipeline import make_pipeline, Pipeline\nfrom sklearn.preprocessing import OrdinalEncoder, StandardScaler\n\nfrom sklearn.model_selection import cross_val_predict, train_test_split, KFold\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, BaggingRegressor, GradientBoostingRegressor\n!pip install xgboost\n!pip install lightgbm\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1.  Leitura dos Dados"},{"metadata":{"trusted":true},"cell_type":"code","source":"def merge_data(df):\n\n    df = df.merge(df_stores, 'left', 'Store')\n    \n    df = df.merge(df_features, 'left', ['Store', 'Date', 'IsHoliday'])\n    \n    df['Date'] = pd.to_datetime(df['Date'], format='%Y-%m-%d')\n\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Paths\n\npath_project = '../input/walmart-recruiting-store-sales-forecasting/'\n\npath_train = path_project + 'train.csv.zip'\n\npath_test = path_project + 'test.csv.zip'\n\npath_stores = path_project + 'stores.csv'\n\npath_features = path_project + 'features.csv.zip'\n\npath_sample_submission = path_project + 'sampleSubmission.csv.zip'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.read_csv(path_train)\n\ndf_test = pd.read_csv(path_test)\n\ndf_stores = pd.read_csv(path_stores)\n\ndf_features = pd.read_csv(path_features)\n\ndf_submission = pd.read_csv(path_sample_submission)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = merge_data(df_train)\n\ndf_test = merge_data(df_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head(1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Análise Exploratória dos dados"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_percent_null(df):\n    '''\n    Esta função calcula a porcentagem de valores Null em dataframe\n    '''\n    return (\n        \n        (df.isnull().sum()/len(df) * 100).sort_values(ascending=False)\n        \n    ).to_frame(name='percent_null')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_heatmap(df, title=''):\n    '''\n    Esta função faz o plot heatmap do seanborn\n    '''\n    corr = df.corr()\n    mask = np.zeros_like(corr)\n    mask[np.triu_indices_from(mask)] = True\n    with sns.axes_style(\"white\"):\n        f, ax = plt.subplots(figsize=(15, 15))\n        ax.set_title(title)\n        ax = sns.heatmap(corr, mask=mask, annot=True, vmax=.3, center=0, cmap=sns.color_palette(\"vlag\", as_cmap=True))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_two_col(df, col_x='Type', col_y = 'count', type_plot='Bar', mode='markers'):\n\n    '''\n    Esta função faz o plot de duas variáveis, onde: \n    - col_x: Representa variável que assumiram o eixo x em cada um dos subplots\n    - col_y: Representa a variável que assumira o eixo y nos dois subplots\n    - type_plot: Tipo do plot, apenas são consideradas duas opções 'Bar' ou 'Scatter'\n    - mode: Apenas utilizado quando type_plot é 'Scatter'\n    '''\n    \n    data = None\n    \n    if type_plot == 'Bar': \n            \n        if col_y is None: \n            col_y = 'count'\n\n        df_ = df.groupby(by=[col_x]).size().reset_index(name='count').copy()  \n        data = [go.Bar(x=df_[col_x], \n                       y=df_[col_y], \n                       marker=dict(color='#197278')\n                )]\n\n    elif type_plot == 'Scatter':\n\n        if col_y is None: \n            col_y = 'Weekly_Sales'\n\n        df = df_train.groupby(by=[col_x], as_index=False)[col_y].mean()\n        data = [go.Scatter(x=df[col_x], \n                           y=df[col_y], \n                           marker=dict(color='#197278'), \n                           mode=mode)]\n    else: \n\n        raise ValueError('Apenas duas opções são aceitas no parâmetro type_plot: Bar ou Sacetter.')\n    \n    fig = go.Figure(data=data)\n\n    fig.update_layout(plot_bgcolor=\"white\",showlegend= False, \n                      xaxis_title=col_x, yaxis_title=col_y, title=f'{col_x} vs {col_y}')\n    fig.update_yaxes(showline=True, gridcolor='#dadae8')\n    \n    fig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_scatter_per_year(df, col_x='week', col_y='Weekly_Sales', filter_month=None):\n\n    '''\n    Esta função faz o plot Scatter sobre duas variáveis, em cada ano, onde:\n    - df: Dataframe\n    - col_x: Coluna que ficará no eixo x\n    - col_y: Coluna que ficará no eixo y\n    - filter_month: Lista com o número dos meses que se deseja filtrar\n    '''\n\n    df = df[['Date', 'Weekly_Sales']].copy()\n    df['week'] = df.Date.dt.isocalendar().week.astype('int64')\n    df['year'] = df.Date.dt.year\n    df['month'] = df.Date.dt.month\n\n    fig = go.Figure()\n\n    colors = ['#C17767', '#197278', '#512D38']\n\n    title = f'{col_y} per mean {col_x}'\n\n    if not filter_month is None:\n        df = df[df['month'].isin(filter_month)]\n        title += f' and only month: {filter_month}' \n    \n    for year, color in zip(df['year'].unique().tolist(), colors):\n\n        \n\n        weekly_sales_year = df[df['year'] == year].groupby(col_x)[col_y].mean()\n\n        fig.add_trace(\n            \n            go.Scatter(x=weekly_sales_year.index, y=weekly_sales_year.values,\n                          mode='lines',\n                          name= f'year {year}',\n                          line=dict(color=color))\n        )\n\n    fig.update_layout(title=title, plot_bgcolor=\"white\", yaxis_title=col_y, xaxis_title=col_x)\n\n    fig.update_xaxes(showline=True, gridcolor='#dadae8')\n    fig.update_yaxes(showline=True, gridcolor='#dadae8')\n\n    fig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Investigando valores Nan\n> df_train: Apenas as colunas de Markdown possuem valores Nan no dataframe de train\n\n> df_test: Além das colunas que possuem prefixo markdown, as colunas Unemployment e CPI também possuem valores Nan\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"get_percent_null(df_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_percent_null(df_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Logo, vamos precisar desenvolver alguma estratégia para lidar com os valores Nan, temos algumas opções:\n\n1. Remover as colunas que possuem algum, ou todos os valores Nan\n2. Utilizar alguma estratégia de imputação\n\nPara as colunas que possuem o prefixo markdown, no dataframe de treino observamos que elas possuem mais da das linhas com valores Nan, utilizar estratégia de imputação de valores em colunas com baixíssimo preenchimento é muito arriscado, portanto, vamos excluir elas. Para as colunas Unemployment e CPI, vamos observar a correlação delas, para decidirmos se vale apena manter elas."},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_heatmap(df_train, 'Dataframe Train')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Analisando a matriz de correlação acima, observamos que as colunas Unemployment e CPI\npossuem baixa correlação com nossa variável resposta Weekly_Sales. Observamos também que as colunas, Fuel_Price, Temperature apresentam baixa correlação com a variável resposta. As variáveis que possuem alguma correlação positiva com a variável resposta são: Dept, Size e IsHoliday. \n\n> Logo, por enquanto manteremos apenas as colunas: \n> 1. Store\n> 2. Dept\n> 3. Size\n> 4. Type (categorical)\n> 5. IsHoliday"},{"metadata":{},"cell_type":"markdown","source":"Como descrito na Seção de métricas desse desafio¹, utilizaremos a métrica customizada WMAE, onde penalizamos mais os erros em feriados, visto que semanas com feriado possuem peso 5 vezes maior.\n\nCom isso percebemos que as informações relacionadas aos feriados são muito importantes para o nosso modelo, vamos observar a representatividade dessa variável na nossa base de treino. Um ponto importante é que nem todos os feriados foram adicionados na base de treino, apenas os principais:\n\n- Super Bowl: 12-Feb-10, 11-Feb-11, 10-Feb-12, 8-Feb-13\n- Labor Day: 10-Sep-10, 9-Sep-11, 7-Sep-12, 6-Sep-13\n- Thanksgiving: 26-Nov-10, 25-Nov-11, 23-Nov-12, 29-Nov-13\n- Christmas: 31-Dec-10, 30-Dec-11, 28-Dec-12, 27-Dec-13\n\nTambém vamos observar a representatividade e categorias que existem na coluna Type.\n\n¹https://www.kaggle.com/c/walmart-recruiting-store-sales-forecasting/overview/evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"for col_x in ['Type', 'IsHoliday']:\n    \n    plot_two_col(df_train, col_x=col_x, col_y='count', type_plot='Bar')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Vamos observar também a relação entre as variáveis Dept e Size com nossa variável resposta."},{"metadata":{"trusted":true},"cell_type":"code","source":"for col_x in ['Dept', 'Size']:\n    \n    plot_two_col(df_train, col_x=col_x, col_y='Weekly_Sales', type_plot='Scatter', mode='markers')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Com isso, concluímos que alguns depatamentos possuem faturamento muito superior aos outros, e que quanto maior a variável size também alcançamos um maior valor de vendas semanais."},{"metadata":{},"cell_type":"markdown","source":"Agora vamos observar como nossa variável resposta se comporta ao longo das semanas em cada ano."},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_scatter_per_year(df_train, col_x='week', col_y='Weekly_Sales')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Adicionando feriados\n\n> Como observado pelo participante [Avelino Caio](https://www.kaggle.com/avelinocaio/walmart-store-sales-forecasting), que notou que o pico das semanas 14 e 13 poderiam estar associadas ao feriado de páscoa (feriado não mapeado no nosso dataframe de treino), nos traz a reflexão de que outros feriado podem influenciar no aumentar das vendas também e que não estão mapeados no nosso dataframe de treino. \n\nA patir de uma busca rápida na internet [Public holidays in  the United States](https://en.wikipedia.org/wiki/Public_holidays_in_the_United_States), podemos encontrar outros feriados que poderiam influênciar também nas vendas dos departamentos. Um ponto de referência para decidirmos se um feriado pode ou não ser relevante para adicionar no nosso problema, seria analisando o impacto financeiro desse feriados no país. Em [https://nrf.com/insights/retail-holiday-and-seasonal-trends](https://nrf.com/insights/retail-holiday-and-seasonal-trends) encontramos a informação de vendas em bilhões de dolares americanos nos feriados. \n\n> É importante mencionar também que não foi fornecida as informações da localização das lojas e departamentos do nosso dataframe de treino, apenas sabemos que são lojas da Walmart, uma empresa american multinacional. Por sabermos que a sede da empresa se localiza nos EUA, estou tomando como base o calendário nacional de feriados de lá. \n\nUm outro ponto importante que nos ajuda a escolher esses feriados adicionais, é **observar os picos do gráfico anterior e observar se as semanas de pico estão relacionadas a algum feriado**. Seguindo essa linha encontramos um pico entre as semanas 26 e 27, mesma semana que ocorre o dia da independência americana (4 de julho). \n\n> Bom, com essas observações vamos incluir os feriados\n> - **Páscoa** \n> - **Dia da independência Americana**"},{"metadata":{},"cell_type":"markdown","source":"### Picos de vendas que não são feriados\n\n> Um comportamento de senso comum observado aqui no Brasil, é o aumento do fluxo de pessoas em supermercados, lojas de conveniência, shoppings, entre outros, no **começo do mês devido ao pagamento do salário mensal das pessoas**. \n\nIsso me levou a pensar se esse comportamento poderia acontecer em outros paísese também. Utilizando como base os EUA, e uma rápida busca na internet [https://www.patriotsoftware.com/blog/payroll/pay-frequency-requirements-state-federal/](https://www.patriotsoftware.com/blog/payroll/pay-frequency-requirements-state-federal/), observamos que a freqência de pagamento dos funcionários é bem variada, mensal, quizenal, entre outros. \n\nAbaixo observamos o nosso gráfico anterior com um zoom nos meses Maio, Junho, Julho e Agosto, e vamos tentar observar algum padrão."},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_scatter_per_year(df_train, col_x='week', col_y='Weekly_Sales', filter_month=[5, 6, 7, 8])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As semans 22, 26, 27 e 31, que representam pico, são semanas do começo do mês. As semanas 30 e 20, representam o final do mês."},{"metadata":{},"cell_type":"markdown","source":"# 2. Engenharia de Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_df_holidays(df):\n    '''\n    Função que retorna um dataframe com as semanas de feriados do dataframe de treino\n    + os novos feriados\n    '''\n    holiday_list = [\n          '2010-02-12', '2011-02-11', '2012-02-10', '2013-02-08',\n          '2010-09-10','2011-09-09', '2012-09-07', '2013-09-06',\n          '2010-11-26', '2011-11-25', '2012-11-23', '2013-11-29',\n          '2010-12-31', '2011-12-30', '2012-12-28', '2013-12-27'\n    ]\n\n    df_holiday = pd.DataFrame(zip(holiday_list, [True] * len(holiday_list)), \n                              columns=['day', 'is_holiday'])\n\n    df_holiday['day'] = pd.to_datetime(df_holiday['day'], format='%Y-%m-%d')\n\n    df['year'] = df.Date.dt.year\n    df['week'] = df.Date.dt.isocalendar().week.astype('int64')\n\n    list_new_holidays = [\n                      \n                    # Páscoa \n                    (2010, 13), (2011, 16), (2012, 14), (2013, 13),\n\n                    # Dia da Independência Americana (04/07)\n                    (2010, 26), (2011, 26), (2012, 27), (2013, 27),\n    ] \n\n    for year, week in list_new_holidays:\n        df.loc[((df.year == year) & (df.week == week)), 'is_holiday'] = True\n\n    df = df[df['is_holiday'] == True][['Date', 'is_holiday']]\n\n    df.columns = ['day', 'is_holiday']\n\n    return pd.concat([df_holiday, df], ignore_index=True).drop_duplicates().reset_index(drop=True) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def count_holidays_per_key(df, keys, col_name):\n    '''\n    Função que calcula a quantidade de feriados em relação a alguma outra feature.\n    Ex: Quantidade de feriados por mês, ou por semana\n    '''\n    df_grouped =  df.groupby(keys, as_index=False)['is_holiday'].count()\n\n    df_grouped.columns = keys + [col_name]\n    \n    return (\n    \n        df.merge(\n\n                df_grouped, how='left', on=keys\n\n            )\n    \n    ).fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_features_holidays(df, df_holidays, start, end):\n\n    '''\n    Função que cria features booleanas que informam se a semana é do \n    - início do mês \n    - quizena do mês\n    - final do mês\n    '''\n        \n    all_days = pd.date_range(start, end, freq='D').to_series()\n    \n    df_all_days = pd.DataFrame(all_days, columns=['day'])\n\n    df_all_days.loc[\n        \n            df_all_days.day.dt.strftime('%d') == '15', 'is_fortnight'\n\n    ] = True\n\n    df_all_days['is_fortnight'] = df_all_days['is_fortnight'].fillna(value=False)\n    \n    df_all_days['year'] = df_all_days.day.dt.year\n    \n    df_all_days['week'] = df_all_days.day.dt.isocalendar().week.astype('int64')\n    \n    df_all_days['week_day'] = df_all_days.day.dt.dayofweek\n    \n    df_all_days['is_month_start'] = df_all_days.day.dt.is_month_start\n    \n    df_all_days['is_month_end'] = df_all_days.day.dt.is_month_end\n\n    df_all_days = df_all_days.merge(df_holidays, 'left', 'day')\n    \n    df_all_days['is_holiday'] = df_all_days['is_holiday'].fillna(value=False)\n    \n    df_week_year = df_all_days.groupby(['week', 'year'], as_index=False)\n\n    df_week_year = df_week_year.any()\n    \n    df_week_year = df_week_year[[\n    \n        'week', 'year', 'is_month_start', 'is_fortnight', \n        'is_month_end', 'is_holiday'\n    ]]\n    \n    df_week_year.columns = [\n    \n        'week', 'year', 'is_week_start_month', 'is_week_fortnight', \n        'is_week_end_month', 'is_holiday'\n    ]\n\n    return df.merge(\n        \n        df_week_year, 'left', ['week', 'year']\n\n    ).fillna(value=False)    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_features(df):\n\n    '''\n    Função que criar as features adicionadas ao dataframe de treinamento:\n    - year\n    - month\n    - week\n    - 'is_week_start_month'\n    - 'is_week_fortnight' \n    - 'is_week_end_month'\n    - 'is_holiday'\n    - 'qt_holiday_week'\n    '''\n       \n    start = pd.Timestamp(df.Date.dt.date.min()).to_pydatetime()\n    \n    end = pd.Timestamp(df.Date.dt.date.max()).to_pydatetime()\n    \n    df_holidays = get_df_holidays(df.copy())\n\n    df['year'] = df.Date.dt.year\n    \n    df['week'] = df.Date.dt.isocalendar().week.astype('int64')\n    \n    df['month'] = df.Date.dt.month\n    \n    df = create_features_holidays(df.copy(), df_holidays, start, end)\n\n    df = count_holidays_per_key(df.copy(), ['year', 'week'], 'qt_holiday_week')\n\n    return df.drop_duplicates().reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def convert_to_numeric(df):\n    '''\n    Função que converte todas as colunas pra float e preenche valores nan.\n    '''\n\n    list_col_numeric = [col for col in df if df[col].dtype != 'O']\n\n    list_col_cat = [col for col in df if df[col].dtype == 'O']\n\n    for col in df.columns:\n\n        if col in list_col_numeric:\n            \n            df[col] = df[col].fillna(-9999).astype('float64')\n            \n        elif col in list_col_cat:\n            \n            df[col] = df[col].fillna('ND')\n\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def prepare_dataframe(df, is_test = False):\n\n    '''\n    Função que chama a criação de feature, seleciona as features usadas no treinamento\n    '''\n    \n    df = create_features(df)\n    \n    col_selected = [\n    \n           'Store', 'Dept', 'Type', 'Size', 'is_holiday',\n           'year', 'week', 'is_week_start_month', 'month',\n           'is_week_fortnight', 'is_week_end_month',\n           'qt_holiday_week', 'Weekly_Sales'\n    ]\n    type_df = 'df_train'\n\n    if is_test:\n        \n        col_selected.remove('Weekly_Sales')\n        type_df = 'df_test'\n\n    plot_heatmap(df, f'{type_df} After add all the features')\n\n    df = df[col_selected]\n    \n    df = convert_to_numeric(df) \n\n    plot_heatmap(df, f'{type_df} After removing some features')\n\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_make_col_transformer(df):\n\n    '''\n    Função que cria um pipeline, onde definimos a estratégia de transformação das colunas \n    categóricas e a forma de normalização que será utilizada\n    '''\n\n    list_col_numeric = [col for col in df if df[col].dtype != 'O']\n\n    list_col_cat = [col for col in df if df[col].dtype == 'O']\n\n    if 'Weekly_Sales' in list_col_numeric: list_col_numeric.remove('Weekly_Sales')\n\n    list_categories = [df[column].unique() for column in df[list_col_cat]]\n\n    encoder_col_cat = make_pipeline(\n        OrdinalEncoder(categories=list_categories)\n    )\n\n    normalize_col_numerics = make_pipeline(\n        StandardScaler()\n    )\n    \n    return make_column_transformer(\n        \n        (encoder_col_cat, list_col_cat),\n        (normalize_col_numerics, list_col_numeric)\n        \n    )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Features Criadas\n\n- year\n- month\n- week\n- is_week_start_month\n- is_week_fortnight\n- is_week_end_month\n- is_holiday\n- qt_holiday_week"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train  = prepare_dataframe(df_train)\n\ndf_test = prepare_dataframe(df_test, is_test=True)\n\ntransform_columns = get_make_col_transformer(df_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Buscando o melhor algoritmo\n\nAgora que definimos nossas features, precisamos encontrar o melhor algoritmo para fazermos a regressão."},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df_train.drop(['Weekly_Sales'], axis=1)\ny = df_train['Weekly_Sales']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def WMAE(dataset, real, predicted, col_target='is_holiday'):\n    \"\"\"\n    Métrica da competição\n    \"\"\"\n    weights = dataset[col_target].apply(lambda x: 5 if x else 1)\n    return np.round(np.sum(weights*abs(real-predicted))/(np.sum(weights)), 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models = {\n\n    'LinearRegression': LinearRegression(),\n    \n    'XGBRegressor': XGBRegressor(),\n    \n    'RandomForestRegressor': RandomForestRegressor(verbose = False, n_jobs = -1),\n       \n    'LGBMRegressor': LGBMRegressor(),\n    \n    'ExtraTreesRegressor' : ExtraTreesRegressor(verbose = False, n_jobs = -1),\n    \n    'BaggingRegressor' : BaggingRegressor(verbose = False, n_jobs = -1),\n    \n    'GradientBoostingRegressor': GradientBoostingRegressor()\n}\n\nbest_nome_model = None\nbest_model = None\nbest_error = math.inf\nlist_metrics = []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for name in models:\n  \n    print(f'\\n\\n\\n{name}')\n    regressor = models[name]\n                       \n    training_pipeline = make_pipeline(\n        transform_columns,\n        regressor\n    )\n\n    kfold = KFold(n_splits=3)\n    \n    y_pred = cross_val_predict(\n        training_pipeline,\n        X_train,\n        y_train,\n        cv = kfold,\n    )\n    \n    wmae_metric = WMAE(X_train, y_train, y_pred)\n    \n    print(f'\\nModel {name} Metrica WMAE', wmae_metric)\n    \n    list_metrics.append((name, wmae_metric))\n    \n    if wmae_metric < best_error:\n        best_error = wmae_metric\n        best_model = regressor\n        best_nome_model = name","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list_metrics","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_error","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Buscando os melhores parâmetros do melhor modelo "},{"metadata":{"trusted":true},"cell_type":"code","source":"def find_best_paramns(best_model, df_train, sample_train=0.5, param_grid=None, \n                      type_search='randomized', n_iter=None):\n    \n    '''\n    Função que encontra os melhores parâmetros para o melhor modelo\n    \n    best_model: Instância do melhor modelo encontrado\n    df_train: Dataframe de treinamento\n    sample_train: Sample utilizada para fazer a otimização de parâmetros, se 1 considera todo o dataframe\n    param_grid: Dict com os parâmetros e valores a serem testados\n    type_search: Tipo da busca otimizada, pode ser: 'grid' ou 'randomized' \n    '''\n    \n    df_sample = df_train.sample(frac=sample_train)\n    \n    X, y = df_sample.drop(['Weekly_Sales'], axis=1), df_sample['Weekly_Sales']\n\n    X_train, _, y_train, _ = train_test_split(X, y, test_size=0.3)\n\n    sample_train_pipeline = Pipeline([\n        (\"transform_columns\", transform_columns),\n        (\"model\", best_model)\n    ])\n\n    kfold = KFold(n_splits=3)\n    \n    if param_grid is None:\n      \n      param_grid = {\n          \n          #'model__bootstrap': [True],\n          #'model__max_features': ['auto', 'log2'],\n          #'model__min_samples_split': [2, 3],\n          'model__max_depth': [None, 30],\n          'model__n_estimators': [60, 150, 200]\n\n      }\n\n    if type_search == 'randomized':\n        \n        model_search = RandomizedSearchCV(\n            sample_train_pipeline,\n            param_distributions = param_grid,\n            cv = kfold,\n            n_iter=n_iter,\n            n_jobs = -1\n        )\n        \n    elif type_search == 'grid':\n        \n        model_search = GridSearchCV(\n            sample_train_pipeline,\n            param_grid = param_grid,\n            cv = kfold,\n            n_jobs = -1\n        )\n    \n    else:\n        \n        raise ValueError('Apenas duas opções são aceitas no parâmetro type_search: randomized ou grid.')\n\n    return model_search.fit(X_train, y_train).best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_params = find_best_paramns(best_model, df_train, sample_train=1, type_search='grid')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_params","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"paramns_name = []\n\nfor key, value in best_params.items():\n    paramns_name.append(key.replace('model__', ''))\n\nbest_params_ = dict(zip(paramns_name, best_params.values()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_params_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Treinando com os melhores parâmetros"},{"metadata":{"trusted":true},"cell_type":"code","source":"def fit_best_model(best_model, X_train, y_train, X_test, y_test):\n\n    model = best_model.set_params(**best_params_)\n\n    training_pipeline = Pipeline([\n              (\"transform_columns\", transform_columns),\n              (\"model\", model)\n    ])\n\n    training_pipeline.fit(X_train, y_train)\n\n    y_pred = training_pipeline.predict(X_test)\n\n    wmae_metric = WMAE(X_test, y_test, y_pred)\n\n    print(f'\\nBest Model WMAE: ', wmae_metric)\n\n    return training_pipeline, wmae_metric","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_model_, wmae_metric = fit_best_model(best_model, X_train, y_train, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_feature_importance(feature_importance, columns):\n    '''\n    Função que plota o feature importance do melhor modelo\n    '''\n    df_importance = pd.DataFrame(\n    \n        zip(columns, feature_importance), \n        columns=['Feature', 'Importance']\n\n    ).sort_values(by=['Importance'])\n\n    fig = go.Figure(data=[\n        go.Bar(y=df_importance['Feature'], \n               x=df_importance['Importance'], \n               marker=dict(color='#197278'),\n               orientation='h'\n              )\n    ])\n\n    fig.update_layout(plot_bgcolor=\"white\",showlegend= False, title='Feature Importance', \n                      xaxis_title='Importance', yaxis_title='Feature')\n    fig.update_yaxes(showline=True, gridcolor='#dadae8')\n\n    fig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_feature_importance(best_model_.steps[1][1].feature_importances_, X_train.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Das variáveis que foram criadas e tiveram algum impacto significativo, foram apenas:\n\n- week\n- qt_holiday_week\n- month\n- is_week_end_month\n- year\n"},{"metadata":{},"cell_type":"markdown","source":"# 6. Testes"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = best_model_.predict(df_test)\n\ndf_test['Weekly_Sales'] = y_pred","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 7. Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_submission['Weekly_Sales'] = y_pred\n\ndf_submission.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_submission","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 8. Próximos passos"},{"metadata":{},"cell_type":"markdown","source":"1. Aumentar o número de parâmetros na otimização de parâmetros\n2. Refinar o processo de engenharia de features, tentar encontrar mais features que expliquem as variações de vendas entre as semanas\n3. Tentar utilizar outros algoritmos\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}