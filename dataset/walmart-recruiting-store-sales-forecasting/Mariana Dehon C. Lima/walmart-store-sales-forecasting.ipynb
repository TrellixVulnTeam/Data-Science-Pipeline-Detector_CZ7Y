{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Walmart - Store Sales Forecasting Problem\n\n**Author: Mariana Dehon C. Lima**<br>\n[Linkedin](https://www.linkedin.com/in/mariana-dehon/) | [Gitlab](https://gitlab.com/users/mariana.dehon/projects)\n\n___\n## Table of Contents\n- [Imports and common functions](#imports-and-common-functions)\n- [Descriptive Analysis](#descriptive-analysis)\n- [Feature Engineering](#feature-engineering)\n- [Preprocessing](#preprocessing)\n- [Training](#training)\n- [Testing our Model](#testing)\n- [Make submission](#submission)\n- [Next Steps](#next-steps)\n___\n## Problem Description\n**Goal:** predict the Weekly Sales for each triple of (`Store`, `Dept`, `Date`) from *Walmart Store Sales* database.\n<div id=\"evaluation-metric\"/>\n\nThe Evaluation metric is:\n\n![\\Large%20WMAE=\\frac{1}{\\sum{w_i}}\\sum_{i=1}^n%20w_i|y_i-\\hat{y}_{i}|](https://latex.codecogs.com/svg.latex?\\Large%20WMAE=\\frac{1}{\\sum{w_i}}\\sum_{i=1}^n%20w_i|y_i-\\hat{y}_{i}|) \n\nwhere\n\n* $n$ is the number of rows\n* $\\hat{y}_{i}$ is the predicted sales \n* $y^i$ is the actual sales\n* $w_i$ are weights. $w = 5$ if the week is a holiday week, $1$ otherwise\n\nConsidering the **Evaluation metric**, one of the main goals of this project is to be able to predict the Holiday impact on weekly sales.\n\n**Reference:** https://www.kaggle.com/c/walmart-recruiting-store-sales-forecasting/overview/evaluation"},{"metadata":{},"cell_type":"markdown","source":"<div id=\"imports-and-common-functions\"/>\n\n## Imports and common functions\nThis section contains all imports and common functions used in this notebook.<br>\nYou can skip this section and go to the [Descriptive Analysis](#descriptive-analysis)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Standart libraries\nimport glob\nimport math\nimport re\nimport warnings\n\n# Data / Graph manipulation libraries\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\n# Preprocessing / Pipeline / Decomposition libraries\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import make_pipeline, Pipeline\nfrom sklearn.preprocessing import OrdinalEncoder, StandardScaler\nfrom sklearn import set_config\n\n# Model Selection / Grid Search libraries\nfrom sklearn.model_selection import cross_val_predict, train_test_split, KFold\nfrom sklearn.model_selection import GridSearchCV\n\n# ML models \nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.linear_model import LassoCV\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom xgboost import XGBRegressor\n\n# Default settings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\npd.set_option('display.max_columns', 200)\npd.set_option('display.max_rows', 20)\npd.set_option('display.min_rows', 20)\nsns.set_style(style='whitegrid')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def WMAE(dataset, real, predicted):\n    \"\"\"\n    Implementation of competition evaluation metric\n    \"\"\"\n    weights = dataset.IsHoliday.apply(lambda x: 5 if x else 1)\n    return np.round(np.sum(weights*abs(real-predicted))/(np.sum(weights)), 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def print_correlation_matrix(df):\n    \"\"\"\n    Default seaborn code to plot diagonal correlation matrix\n    \n    References:\n    https://seaborn.pydata.org/examples/many_pairwise_correlations.html\n    https://seaborn.pydata.org/generated/seaborn.diverging_palette.html\n    \"\"\"\n    corr = df.corr()\n    f, ax = plt.subplots(figsize=(15, 15))\n    cmap = sns.diverging_palette(220, 20, as_cmap=True)\n    sns.heatmap(corr, cmap=cmap, vmax=.3, center=0, annot=True,\n                square=True, linewidths=.5, cbar_kws={'shrink': .5})\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def graph_relation_to_weekly_sale(col_relation, df, x='Week', palette=None):\n    \"\"\"\n    This function plots the graph relation between \n    a categorized feature and the Weekly_Sales\n    \"\"\"\n    df.Date = pd.to_datetime(df.Date)\n    df['Week'] = df.Date.dt.week\n    df['Month'] = df.Date.dt.month\n    df['Year'] = df.Date.dt.year\n    \n    cmap = sns.diverging_palette(220, 20, as_cmap=True)\n\n    sns.relplot(\n        x=x,\n        y='Weekly_Sales',\n        hue=col_relation,\n        data=df,\n        kind='line',\n        height=5,\n        aspect=2,\n        palette=palette\n    )\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_groupped_analysis(col, df):\n    \"\"\"\n    This function creates a  groupBy relation between the Weekly_Sales\n    and the input column\n    \"\"\"\n    unique_values = df[[col, 'Weekly_Sales']].groupby(by=col).mean().size\n    print(f'The number of unique values on the category \\'{col}\\' is {unique_values}\\n')\n    print('-' * 40)\n    print(f'The top 5 \\'{col}\\' with higher average \\'Weekly_Sales\\':')\n    print('-' * 40)\n    return (\n        df[[col, 'Weekly_Sales']]\n        .groupby(by=col)\n        .mean()\n        .sort_values('Weekly_Sales', ascending=False)\n        .head()\n    )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_groupped_graph(col1, col2, df):\n    \"\"\"\n    This function plots a relplot between one or two columns and\n    the Weekly_Sales\n    \"\"\"\n    if col2:\n        df = (\n            df[[col1, col2, 'Weekly_Sales']]\n            .groupby([col1, col2])\n            .mean()\n            .sort_values('Weekly_Sales', ascending=False)\n        )\n    else:\n        df = (\n            df[[col1, 'Weekly_Sales']]\n            .groupby([col1])\n            .mean()\n            .sort_values('Weekly_Sales', ascending=False)\n        )\n\n    df.reset_index(inplace=True)\n    sns.relplot(\n        x=col1,\n        y='Weekly_Sales',\n        hue=col2,\n        data=df.sort_values(col1),\n        height=5,\n        aspect=2\n    )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_regression_results(y_true, y_pred, title, scores):\n    \"\"\"\n    Scatter plot of the predicted vs true targets\n    \"\"\"\n    ax = plt.gca()\n    ax.plot([y_true.min(), y_true.max()],\n            [y_true.min(), y_true.max()],\n            '--r', linewidth=2)\n    ax.scatter(y_true, y_pred, alpha=0.2)\n\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    ax.get_xaxis().tick_bottom()\n    ax.get_yaxis().tick_left()\n    ax.spines['left'].set_position(('outward', 10))\n    ax.spines['bottom'].set_position(('outward', 10))\n    ax.set_xlim([y_true.min(), y_true.max()])\n    ax.set_ylim([y_true.min(), y_true.max()])\n    ax.set_xlabel('Measured')\n    ax.set_ylabel('Predicted')\n    extra = plt.Rectangle((0, 0), 0, 0, fc='w', fill=False,\n                          edgecolor='none', linewidth=0)\n    ax.legend([extra], [scores], loc='upper left')\n    title = title\n    ax.set_title(title)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_features_importance(importance, names, model_type):\n    \"\"\"\n    This function plots the model features importance \n    \"\"\"\n    feature_importance = np.array(importance)\n    feature_names = np.array(names)\n\n\n    data={'feature_names': feature_names, 'feature_importance': feature_importance}\n    fi_df = pd.DataFrame(data)\n\n    fi_df.sort_values(by=['feature_importance'], ascending=False, inplace=True)\n\n    #Define size of bar plot\n    plt.figure(figsize=(10, 7))\n    #Plot Searborn bar chart\n    sns.barplot(x=fi_df['feature_importance'], y=fi_df['feature_names'])\n    #Add chart labels\n    plt.title(f'{model_type} - Feature Importance')\n    plt.xlabel('FEATURE IMPORTANCE')\n    plt.ylabel('FEATURE NAMES')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def model_factory(model_type, **kwargs):\n    \"\"\"\n    This function creates a model considering the model_type\n    and specific arguments.\n    \n    Reference: https://medium.com/xp-inc/desing-patterns-factory-method-a7496ae071aa\n    \"\"\"\n    # Specific for RandomForestRegressor or ExtraTreesRegressor\n    if model_type in ['RandomForestRegressor', 'ExtraTreesRegressor'] :\n        n_estimators = kwargs.get('n_estimators')\n        verbose = kwargs.get('verbose')\n        n_jobs = kwargs.get('n_jobs')\n        \n        if n_estimators is None:   \n            n_estimators = 50\n        \n        verbose = 0 if verbose is None else verbose\n\n    if model_type == 'LinearRegression':\n        return LinearRegression()\n    elif model_type == 'KNN':\n        return KNeighborsRegressor()\n    elif model_type == 'Ridge':\n        return RidgeCV()\n    elif model_type == 'Lasso':\n        return LassoCV()\n    elif model_type == 'XGBRegressor':\n        return XGBRegressor()\n    elif model_type == 'RandomForestRegressor':\n        return RandomForestRegressor(\n            n_estimators = n_estimators,\n            verbose = verbose,\n            n_jobs = n_jobs\n        )\n    elif model_type == 'ExtraTreesRegressor':\n        return ExtraTreesRegressor(\n            n_estimators = n_estimators,\n            verbose = verbose,\n            n_jobs = n_jobs\n        )\n    else:\n        raise ValueError('Model not defined on factory.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div id=\"descriptive-analysis\"/>\n\n# Descriptive Analysis\n\nTo better understand **Walmart - Store Sales Forecasting** data, I will analyze its distribution and behavior. Since this is a **product design problem**, I will conduct all my analysis considering the product and the final result.\n\nHere, the main goal is to correctly predict **Weekly_Sales** values. To do so, we need to consider the [evaluation metric](#evaluation-metric) for this problem.\n\nWalmart defined the weights on **Holidays** are 5 times bigger than the other days, it makes all Holiday's related features important and I will conduct this analysis considering that we need to minimize the error prediction in those cases."},{"metadata":{},"cell_type":"markdown","source":"### Reading the provided data and checking data format and structure"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Reading the provided data\nbase_path = '/kaggle/input/walmart-recruiting-store-sales-forecasting/'\ninput_paths = glob.glob(f'{base_path}*.csv*')\n\ninput_dfs = {\n    re.search(r'forecasting\\/(.*)\\.csv', path, re.IGNORECASE)[1] : pd.read_csv(path)\n    for path in input_paths\n}\n\n# In this data structure, we have a dict and we can access each dataframe by its key\ninput_dfs.keys()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking data format and structure\ndf_train = input_dfs['features'].merge(input_dfs['stores'], how='left', on='Store')\ndf_train = input_dfs['train'].merge(df_train, how='left', on=['Store', 'Date', 'IsHoliday'])\ndf_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Checking some basic descriptive statistics and data distribution\n\n* Considering this first analysis, we can see outliers and `NaN` values on a few columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.describe(include = 'all')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Checking the `NaN` percentage in each column\n\nWe can drop some features here since most of the **MarkDown** columns are not available and. Nn this case, Walmart specified that those values are not consistent since we don't have them for every store all the time.\n\n> MarkDown1-5 - anonymized data related to promotional markdowns that Walmart is running. MarkDown data is only available after Nov 2011, and is not available for all stores all the time. Any missing value is marked with an NA.<br>\n**Reference:** https://www.kaggle.com/c/walmart-recruiting-store-sales-forecasting/data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the NaN percentage\ndf_train.isnull().mean() * 100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dropping columns with none values: `MarkDown1`, `MarkDown2`, `MarkDown3`, `MarkDown4` and `MarkDown5`\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.dropna(axis='columns', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Checking features correlation with the output feature **Weekly_Sales**\n\nWhen we look at the correlation matrix, we can see that **Weekly_Sales** have a higher correlation ($\\small abs(corr)> 0.05$) with:\n* `Store`\n* `Dept`\n* `Size`\n\nThis is a really important insight because, as we know, being able to correctly predict weekly sales on Holidays is extremely important. However, the correlation between `IsHoliday` and `Weekly_Sales` has a lower value (0.013).\n\nDespite having a low correlation, we need to remember that this feature only happens 4 times a year besides being a really important one considering our problem. Considering it, we will also need to check for other ways to explain time-related features to the model. `IsHoliday` feature may not be enough to explain every sales behavior."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting the correlation matrix. Here I will check for relations with the Weekly Sales feature\nprint_correlation_matrix(df_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Dropping features with lower correlation"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.drop(['Temperature', 'Fuel_Price', 'Unemployment', 'CPI'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Checking **Weekly_Sales** behavior\n\nThis problem is, by concept, a **Time Series** problem. The sales are directly related to time and our dataset is made in a way that each row corresponds to a different week, store, and dept.\n\nSince this is a time series problem, we can have 3 components that affect the data, besides our original features:\n\n1. **Trend:** are the values growing in time?\n1. **Periodic Fluctuations:** do we have some behaviors that happen every month, week, period?\n1. **Random or Irregular movements:** some things can not be predicted and they could affect our response. Maybe we had hearthquakes or a flood that could affect our results."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Here we can see a sazonal behavior on the sales\ngraph_relation_to_weekly_sale('Year', df_train, x='Date', palette='Set2')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting the average sales on our dataset, we can see:\n\n* **Trend:** no strong evidence of major trend on data\n* **Periodic Fluctuations:** strong seasonal behavior. They seem to happen in the same period\n* **Random or Irregular movements:** no strong evidence of major random events on data\n\nA strong seasonal behavior makes time extremely important for this problem. So, we will need to create some time-related features to be able to correctly learning those characteristics. Looking at the graph, we can see that the highest sales happens at the end of the year, on **Thanksgiving** and **Christmas** weeks."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Week of the year can explain some Weekly_Sales variation\ngraph_relation_to_weekly_sale(None, df_train, x='Week')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Month of the year can also explain some Weekly_Sales variation, but not as detailed as the week\ngraph_relation_to_weekly_sale(None, df_train, x='Month')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The current year can also explain some `Weekly_Sales` variation since the weekly sales seem to  be decreasing with time. However, we need to be careful before jumping to conclusions.\nThe last year (2012) does not have **Thanksgiving** and **Christmas** days on the training set. The average sales can also be low because of that."},{"metadata":{"trusted":true},"cell_type":"code","source":"graph_relation_to_weekly_sale(None, df_train, x='Year')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The year behavior is easier to see in the table below\ncreate_groupped_analysis('Year', df_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Store Dept, Type, Size vs Weekly_Sales/Holidays\n\n**Type C** seems to have the lowest weekly sales, **Type B** the medium and **Type A** the highest. They are also directly related to the store size.\nHolidays have an impact at on Type sales, especially on **Type A** and **Type B**."},{"metadata":{"trusted":true},"cell_type":"code","source":"# We can check the correlation between the store and type on the graph below.\ngraph_relation_to_weekly_sale('Type', df_train, 'Size')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We can also check this behavior for a year. Store type/store seems to be a multiplier\ngraph_relation_to_weekly_sale('Type', df_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# On the graph below we can see the relation between 'IsHoliday' and 'Type'.\nplot_groupped_graph('Type', 'IsHoliday', df_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The weekly sales vary depending on the`Dept`. Some `Dept` have higher weekly sales than others and they are affected by the Holidays"},{"metadata":{"trusted":true},"cell_type":"code","source":"# On the graph below we can see the average weekly sales considering each `Dept`\nplot_groupped_graph('Dept', None, df_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# On the graph below we can see the relation between 'IsHoliday' and 'Dept'\nplot_groupped_graph('Dept', 'IsHoliday', df_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div id=\"feature-engineering\"/>\n    \n# Feature Engineering\n\nAfter the initial analysis, we found out the necessity of creating some features that could express time and also holiday-related characteristics.\n\nWalmart provided the dates of the four largest holidays:\n\n>For convenience, the four holidays fall within the following weeks in the dataset (not all holidays are in the data): <br>\n>**Super Bowl:** 12-Feb-10, 11-Feb-11, 10-Feb-12, 8-Feb-13<br>\n>**Labor Day:** 10-Sep-10, 9-Sep-11, 7-Sep-12, 6-Sep-13<br>\n>**Thanksgiving:** 26-Nov-10, 25-Nov-11, 23-Nov-12, 29-Nov-13<br>\n>**Christmas: 31-Dec-10, 30-Dec-11, 28-Dec-12, 27-Dec-13<br>\n><br>\n>**Reference:** https://www.kaggle.com/c/walmart-recruiting-store-sales-forecasting/data\n\nConsidering our initial analysis and the nature of the problem, I will create four features:\n\n1. **Week:** week of the year\n1. **Month:** month of the year\n1. **Year:** observed year\n1. **HolidayType:** `[-1 = No Holiday, 0 = Super Bowl, 1 = Labor Day, 2 = Thanksgiving, 3 = Christmas]`"},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_features(df):\n    \"\"\"\n    This function creates the following features: Week, Month, Year and\n    HolidayType\n    \"\"\"\n    def create_holiday_type_column(df, dates, holiday_type, name):\n        df.loc[\n            df['Date'].isin(dates),\n            'HolidayType'\n        ] = holiday_type\n\n    df['HolidayType'] = -1\n    \n    holiday_list = [\n        (['2010-02-12', '2011-02-11', '2012-02-10', '2013-02-08'], 'Super_Bowl'),\n        (['2010-09-10','2011-09-09', '2012-09-07', '2013-09-06'], 'Labor_Day'),\n        (['2010-11-26', '2011-11-25', '2012-11-23', '2013-11-29'], 'Thanksgiving'),\n        (['2010-12-31', '2011-12-30', '2012-12-28', '2013-12-27'], 'Christmas')\n    ]\n    \n    for index in range(0, len(holiday_list)):\n        holiday = holiday_list[index]\n        create_holiday_type_column(df, holiday[0], index, holiday[1])\n\n    df.Date = pd.to_datetime(df.Date)\n    df['Week'] = df.Date.dt.week\n    df['Year'] =  df.Date.dt.year\n    df['Month'] =  df.Date.dt.month\n    \n    # Convert all columns to float\n    for x in df:\n        if df[x].dtypes == \"int64\":\n            df[x] = df[x].astype(float)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"create_features(df_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div id=\"preprocessing\"/>\n\n# Preprocessing\n\nOne important step in our pipeline is to treat data before our training. On this training dataset, we removed all NaN values but that may not be true considering some predictor unknown data (test dataset). We also make some data transformation to make sure that our model can correctly understand data.\n\nAs a programming strategy, we will be using sklearn [Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) at all steps during our training. This approach makes it easier to have the same transformation at different datasets and to have better control of the entire model pipeline.\n\n**Categorical Features**\n1. Fill all missing values with the string `missing` using [SimpleImputer](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html)\n1. Transform all categorical features into ordinal categories using [OrdinalEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html). Here, we use this transformation for the column `Type`.\n\n**Numerical Features**\n1. All numerical data were transformed into `float64` on function **create_features**\n1. Fill all missing values to -9999 using [SimpleImputer](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html). We don't have any on the training set, but that can not be true on the test set.  -9999 were chosen because this value is lower than any other value at [Descriptive Analysis](#descriptive-analysis)\n1. Standardize features by removing the mean and scaling to unit variance using [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Divide dataset for training\nX = df_train.drop(['Weekly_Sales', 'Date'], axis=1)\ny = df_train['Weekly_Sales']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_features = X.columns[X.dtypes == 'O']\nnumeric_features = X.columns[X.dtypes == 'float64']\n\ncategories = [X[column].unique() for column in X[categorical_features]]\nfor cat in categories:\n    cat[cat == None] = 'missing'\n\ntransform_categorical_features = make_pipeline(\n    # Fill all NaN values to missing\n    SimpleImputer(missing_values=None,\n                  strategy='constant',\n                  fill_value='missing'),\n    # Transform categorical features into ordinal categories\n    OrdinalEncoder(categories=categories)\n)\n\ntransform_numeric_features = make_pipeline(\n    # Fill all NaN values to -1\n    SimpleImputer(strategy='constant', fill_value=-9999),\n    # Standardize features by removing the mean and scaling to unit variance\n    StandardScaler()\n)\n\n# Pre-processing transformation\npreprocessing_step = make_column_transformer(\n    (transform_categorical_features, categorical_features),\n    (transform_numeric_features, numeric_features),\n    remainder='passthrough'\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div id=\"training\"/>\n\n# Training\n\nTraining step can be divided into 3 major steps:\n\n1. **[Step 1](#best-ml-model):** Find the best ML model type\n1. **[Step 2](#best-hyperparameter):** Find the best hyperparameter combination\n1. **[Step 3](#training-full-dataset):** Training the full dataset considering the best model type and hyperparameter combination\n---\n\nWe avoid overfitting in every step of the training pipeline, I used [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function with a `test_size` of 0.2 since this is a common value used in literature."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Divide dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div id=\"best-ml-model\"/>\n\n## Step 1: Find the best ML model type\n\nIn this section, we used some regression models to find the better one considering **WMAE** evaluation on hyperparameter combination.\n\nThe following regression models were tested:\n[Linear Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html), \n[KNN](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html), \n[Ridge](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html), \n[Lasso](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html), \n[XGBRegressor](https://xgboost.readthedocs.io/en/latest/python/python_api.html), \n[RandomForestRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html), and \n[ExtraTreesRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesRegressor.html)\n\nTo avoid *overfitting* and have a better view of the models' behavior on different datasets, we used a [cross validation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_predict.html) strategy, considering 3 folds. The better model will be the one with the lowest **WMA**."},{"metadata":{"trusted":true},"cell_type":"code","source":"regression_models = [\n    'LinearRegression',\n    'KNN',\n    'Ridge',\n    'Lasso',\n    'XGBRegressor',\n    'RandomForestRegressor',\n    'ExtraTreesRegressor'\n]\n\nbest_model = None\nbest_error = math.inf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('-' * 50)\nprint('Training evaluation')\nprint('-' * 50)\n\nfor name in regression_models:\n    # Create a basic pipeline\n    print(f':: {name} - Training Pipeline')\n    regressor = model_factory(name)\n    training_pipeline = make_pipeline(\n        preprocessing_step,\n        regressor\n    )\n\n    kfold = KFold(n_splits=3)\n    \n    y_pred = cross_val_predict(\n        training_pipeline,\n        X_train,\n        y_train,\n        cv = kfold,\n        n_jobs = -1,\n        verbose = 1\n    )\n    \n    wmae_metric = WMAE(X_train, y_train, y_pred)\n    \n    if wmae_metric < best_error:\n        best_error = wmae_metric\n        best_model = name\n    \n    plot_regression_results(\n        y_train,\n        y_pred,\n        name,\n        (r'$WMAE ={:.2f}$').format(wmae_metric)\n    )\n\n    print('-' * 50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Best model is {best_model}')\nprint(f'Best score is {best_error}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div id=\"best-hyperparameter\"/>\n\n## Step 2: Find the best hyperparameter combination\n\nAfter finding the best model, we need to find the best combination of hyperparameters. For simplicity's sake, this step was only implemented for the two models with the lowest on [Step 1](#best-ml-model): **ExtraTreesRegressor** and **RandomForestRegressor**.\n\nI'm also using all default model options except for **n_estimators**, since this feature has a huge impact on the final result.\n\t\n> **n_estimators:** The number of trees in the forest.<br>\n> **References:**\n> [Sklearn - RandomForestRegressor](https://scikit-learn.org/0.16/modules/generated/sklearn.ensemble.RandomForestRegressor.html), \n> [Sklearn - ExtraTreesRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesRegressor.html)\n\nThe strategy used here is finding the best hyperparameter combination using [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) approach."},{"metadata":{"trusted":true},"cell_type":"code","source":"# The number of trees in the forest\nn_estimators = [x for x in range(50, 250, 50)]\n\n# Create the random grid\nrandom_grid = {\n    'model__n_estimators': n_estimators\n}\n\nrandom_grid","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_best_model_parameters(random_grid, X, y):\n    if best_model in ['ExtraTreesRegressor', 'RandomForestRegressor']:\n        model = model_factory(best_model)\n    else:\n        # grid search not implemented for other models\n        return {'model__n_estimators': None}\n\n    grid_pipeline =  Pipeline(\n        [(\"preprocessing\", preprocessing_step),\n         (\"model\", model)]\n    )\n\n    kfold = KFold(n_splits=3)\n\n    model_random = GridSearchCV(\n        grid_pipeline,\n        param_grid = random_grid,\n        cv = kfold,\n        verbose = 2,\n        n_jobs = -1\n    )\n\n    # Create a sample to speed up parameter search\n    X_sample, _, y_sample, _ = train_test_split(X, y, test_size=0.9)\n    return model_random.fit(X_sample, y_sample).best_params_\n\n# Best Parameter combination\nbest_params = get_best_model_parameters(random_grid, X, y)\nbest_params","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div id=\"training-full-dataset\"/>\n\n## Step 3: Training the full dataset\n\nThis step trains a model considering the best model type and hyperparameter combination found on the steps before. Here, I also plotted some graphs and the whole pipeline schema to have a better view of our results and training metrics."},{"metadata":{"trusted":true},"cell_type":"code","source":"print('-' * 70)\nprint(f'Best Model: {best_model}: {best_params}\\n')\nprint('Tip: click on the Pipeline diagram below to have a better view of each step.')\nprint('-' * 70)\n\nmodel = model_factory(\n    best_model,\n    n_estimators = best_params['model__n_estimators'],\n    verbose = 1,\n    n_jobs = -1\n)\n\nbest_model_pipeline = Pipeline(\n    [\n        (\"preprocessing\", preprocessing_step),\n        (\"model\", model)\n    ]\n)\n\nset_config(display='diagram')\nbest_model_pipeline.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = best_model_pipeline.predict(X_test)\nwmae_metric = WMAE(X_test, y_test, y_pred)\n\nplot_regression_results(\n    y_test,\n    y_pred,\n    f'{best_model} - Test dataset',\n    (r'$WMAE ={:.2f}$').format(wmae_metric)\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features_importance = best_model_pipeline.steps[1][1].feature_importances_\n    \nplot_features_importance(\n    features_importance,\n    X_test.columns,\n    best_model\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div id=\"testing\"/>\n\n# Testing our Model\n\nAfter model training, we can apply the resulting model to the test dataset provided by Walmart.\n\n> **test.csv**<br>\n> *This file is identical to train.csv, except we have withheld the weekly sales. You must predict the sales for each triplet of store, department, and date in this file.*<br>\n><br>\n> **Reference:** https://www.kaggle.com/c/walmart-recruiting-store-sales-forecasting/data\n\nOn this set, we apply our whole model pipeline on this data (**Preprocessing** + **Model**) using `best_model_pipeline` and plot a graph to see our estimation on the test dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creates df_test dataset\ndf_test = input_dfs['features'].merge(input_dfs['stores'], how='left', on='Store')\ndf_test = input_dfs['test'].merge(df_test, how='left', on=['Store', 'Date', 'IsHoliday'])\ncreate_features(df_test)\ndf_test = df_test[list(X_train.columns)]\ndf_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = best_model_pipeline.predict(df_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* We can see, on the graph below, that our prediction was able to get data sazonal component of this problem"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test['Weekly_Sales'] = y_pred\ndf_test['DatasetType'] = 'Predicted'\ndf_test['Date'] = input_dfs['test']['Date'].values # we dropped this column earlier\n\ndf_train['DatasetType'] = 'Actual'\ndf_train['Date'] = input_dfs['train']['Date'].values # we dropped this column earlier\n\ndf_result = df_train.append(df_test)\n\ngraph_relation_to_weekly_sale('DatasetType', df_result, x='Date')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div id=\"submission\"/>\n\n# Make submission\n\nAfter predicting the test dataset, we can submit our results to Kaggle evaluation and check our final score."},{"metadata":{"trusted":true},"cell_type":"code","source":"input_dfs['sampleSubmission']['Weekly_Sales'] = y_pred\ninput_dfs['sampleSubmission'].to_csv('submission.csv',index=False)\ninput_dfs['sampleSubmission']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div id=\"next-steps\"/>\n\n# Next steps\n\n1. Work with time series decomposition methods, to get a better view of seasonal components and get other important data markdowns besides the four holidays that Walmart provided;\n1. The created custom features, besides time-related ones, don't seem to contribute to this model. When we look at the Feature Importance, `HolidayType` didn't have a great contribution. Maybe this is happening because this feature and the feature `Week` kind of explain the same thing. A further investigation in this matter would be a nice next step;\n1. Try new models and hyperparameters combinations."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}