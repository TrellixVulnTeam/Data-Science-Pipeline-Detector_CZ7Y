{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Portfolio: Commercial Aviation Fatalities\n**Mallory Cotton-Whitehorn**","metadata":{}},{"cell_type":"markdown","source":"This notebook is a portfolio for my Capstone in Data Science that synthesizes my analysis and findings from a competition on Kaggle called Reducing Commercial Aviation Fatalities. This portfolio contains several other Kaggle notebooks that show my analysis of this Kaggle dataset.","metadata":{}},{"cell_type":"markdown","source":"# Project Description","metadata":{}},{"cell_type":"markdown","source":"The project that I analyzed was called Reducing Commercial Aviation Fatalities from a Kaggle data source.\n\nMost flight-related incidents come from pilots' loss of \"airplane state awareness\" during their flights. Loss of airplane state awareness is poor attention management by pilots due to distractions, drowsiness, and many other different states of cognitive awareness that are dangerous. \n\nThis dataset uses real physiological data. In order to get this data, pilots were subjected to various distracting events.\n\nThere are three different cognitive states of awareness that each pilot undergoes while collecting the physiological data. These states include Channelized Attention (CA), Diverted Attention (DA), and Startle/Surprise (SS). Pilots can be only one of these said states at one time or in baseline, which is no event.\n\nThree Cognitive States Defined:\n1. Channelized Attention (CA) = State of being focused on one task with the exclusion of all others.\n\n   *Benchmarked by playing a puzzle-based video game.*\n\n2. Diverted Attention (DA) = State of being diverted by actions or thought processes associated with a decision\n\n   *Benchmarked by solving a period math problem before returning to task*\n\n3. Startle/Surprise (SS) = An unexpected event that causes a mild shock\n\n  *Benchmarked by watching movie clips with jump scares*\n\n\n**Goal** = My goal in this project is to build a model to detect these pilots' different states of cognitive awareness. \n","metadata":{}},{"cell_type":"markdown","source":"# Exploratory Analysis\n\nWhen I started this project, I initially had to look at the data to visualize its main characteristics and how I would approach this dataset. I browsed through the data to see how big the data set was and to get an actual look at the labels within the dataset. Below is a brief look into some of my initial findings while exploring the training dataset.","metadata":{}},{"cell_type":"code","source":"# Import numpy, pandas, and matplotlib using the standard aliases.\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Import the following tools from sklearn: \n#     Pipeline, SimpleImputer, ColumnTransformer, OneHotEncoder, StandardScaler\n#     LogisticRegression, DecisionTreeClassifier, RandomForestClassifier, GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n\n\nfrom sklearn.model_selection import GridSearchCV\n# Import joblib\nimport joblib\nimport os\nimport gc\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\n\nimport seaborn as sns\n\nimport warnings\nimport itertools\nfrom tqdm import tqdm_notebook as tqdm\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, log_loss\nwarnings.simplefilter(action='ignore')\nsns.set_style('whitegrid')","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2022-05-03T18:09:36.322857Z","iopub.execute_input":"2022-05-03T18:09:36.323531Z","iopub.status.idle":"2022-05-03T18:09:38.527298Z","shell.execute_reply.started":"2022-05-03T18:09:36.3234Z","shell.execute_reply":"2022-05-03T18:09:38.526429Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This dataset has 28 columns and 4,867,421 rows.","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('../input/reducing-commercial-aviation-fatalities/train.csv')\ntrain = train.sample(frac=1, random_state=1)\nprint(train.shape)","metadata":{"execution":{"iopub.status.busy":"2022-05-03T18:09:38.528949Z","iopub.execute_input":"2022-05-03T18:09:38.529266Z","iopub.status.idle":"2022-05-03T18:10:11.981242Z","shell.execute_reply.started":"2022-05-03T18:09:38.529224Z","shell.execute_reply":"2022-05-03T18:10:11.980503Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Below is the list of column names in the original dataset provided in the Kaggle competition.","metadata":{}},{"cell_type":"code","source":"for col in train.columns:\n    print(col)","metadata":{"execution":{"iopub.status.busy":"2022-05-03T18:10:11.982353Z","iopub.execute_input":"2022-05-03T18:10:11.983051Z","iopub.status.idle":"2022-05-03T18:10:11.992424Z","shell.execute_reply.started":"2022-05-03T18:10:11.983017Z","shell.execute_reply":"2022-05-03T18:10:11.991798Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# States of Awareness Labeled","metadata":{}},{"cell_type":"markdown","source":"The different states of awarness are label in the dataset from letters A-D.\n* A = Baseline\n* B = SS\n* C = CA\n* D = DA","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(15,10))\nsns.countplot(train['event'])\nplt.xlabel(\"State of the pilot\", fontsize=15)\nplt.ylabel(\"Count\", fontsize=12)\nplt.title(\"Target Labels\", fontsize=20)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-03T18:10:11.994149Z","iopub.execute_input":"2022-05-03T18:10:11.994821Z","iopub.status.idle":"2022-05-03T18:10:18.223846Z","shell.execute_reply.started":"2022-05-03T18:10:11.994788Z","shell.execute_reply":"2022-05-03T18:10:18.222888Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The graph below shows the different states of awareness and the baseline state between each state.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(15,10))\nsns.countplot('experiment', hue='event', data=train)\nplt.xlabel(\"State of Awarnesss and Experiment of the Pilot\", fontsize=12)\nplt.ylabel(\"Count (log)\", fontsize=12)\nplt.yscale('log')\nplt.title(\"Target Labels for Different Experiments\", fontsize=15)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-03T18:10:18.225386Z","iopub.execute_input":"2022-05-03T18:10:18.225929Z","iopub.status.idle":"2022-05-03T18:10:29.428322Z","shell.execute_reply.started":"2022-05-03T18:10:18.225897Z","shell.execute_reply":"2022-05-03T18:10:29.427708Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**To take a more in-depth look into the exploratory analysis stage of this notebook. Please view the notebook linked below.**\n\n\n***EDA Notebook: https://www.kaggle.com/code/mallorycw/eda-reducing-commercial-aviation-fatalities?scriptVersionId=94677397***","metadata":{}},{"cell_type":"markdown","source":"# Model Training\n\n\nAfter exploring my data, I proceeded with this project by training different models. I trained two different models. Below is a clip of the models I trained.","metadata":{}},{"cell_type":"markdown","source":"To set up my model to be trained, I started by inserting the \"event\" column into the variable x in my training data. After this, I dropped the following labels from the training dataset:  crew,  experiment, time, seat, event. The event column is what I am predicting, and the other columns that I dropped are unneeded. Then I inserted all other columns except the event column into my y variable. I did an 80/20 split, meaning I trained on only 20% of the data. \n\nAfter processing and splitting my data, I began training on two different models. The two models that I chose were a decision tree model and a random forest model. For both of these models, I choose the following parameters.\n\n                      'max_depth': [8,16, 24, 32, 40],\n                      'min_samples_leaf': [8, 16, 24, 32, 40]\n                      \n**Note: Instead of scoring on neg_log_loss, my initial models were scored based on accuracy. Both of these scoring techniques did not influence my models as much; however, for the future of this project, I scored on neg_log_loss.**\n\nBelow is a look at the model performances.\n\n**Decision Tree Model:**\n\n                Best Parameters: {'max_depth': 40, 'min_samples_leaf': 8}\n                Best CV Score:   0.9589346206679769\n                Training Acc:    0.988669153541475\n                Wall time: 19min 25s\n\n**Random Forest Model:**\n\n                  Best Parameters: {'max_depth': 40, 'min_samples_leaf': 8}\n                  Best CV Score:   0.9283920104511193\n                  Training Acc:    0.9646901305640092\n                  Wall time: 2h 13min 46s\n                \nAs you can see, both of these models had a good score output after training; however, the decision tree model had a slightly better score. I decided to test this decision tree model. As you can see, based on the training accuracy of both models, the Random Forest Model is not far off. The random forest model took a long time to run; however, I did not rule out the random forest model for future training.\n","metadata":{}},{"cell_type":"markdown","source":"image1 = Image.open(\"../input/images-aviation/Screenshot (214).png\")\nimage1.show()","metadata":{}},{"cell_type":"markdown","source":"## Decision Tree Graph","metadata":{}},{"cell_type":"code","source":"from IPython.display import Image\nimg = '../input/images-aviation/Screenshot (214).png'\nImage(filename=img)","metadata":{"execution":{"iopub.status.busy":"2022-05-03T22:46:23.443279Z","iopub.execute_input":"2022-05-03T22:46:23.444022Z","iopub.status.idle":"2022-05-03T22:46:23.464457Z","shell.execute_reply.started":"2022-05-03T22:46:23.443975Z","shell.execute_reply":"2022-05-03T22:46:23.463799Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Random Forest Graph","metadata":{}},{"cell_type":"code","source":"from IPython.display import Image\nimg2 = '../input/images-aviation/Screenshot (215).png'\nImage(filename=img2)","metadata":{"execution":{"iopub.status.busy":"2022-05-03T22:46:27.742086Z","iopub.execute_input":"2022-05-03T22:46:27.742582Z","iopub.status.idle":"2022-05-03T22:46:27.759867Z","shell.execute_reply.started":"2022-05-03T22:46:27.74252Z","shell.execute_reply":"2022-05-03T22:46:27.758905Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Based on the graphs, these models can be trained for a long time since the CV score steadily increases instead of being stagnant.","metadata":{}},{"cell_type":"markdown","source":"**Take a closer look into my initial model training by viewing the notebook attached within this cell.**\n\n***Initial Training Notebook: https://www.kaggle.com/code/mallorycw/mcw-aviation-modeling-process***","metadata":{}},{"cell_type":"markdown","source":"### Decision Tree Testing\n\nFrom my training notebook, I used the best parameters that you see above to test the model on the test data set and submitted it to the competition.\n\n\n\nDecision Tree Model Scores:\n        \n        Private Score: 12.76630\n        Public Score:  12.88538\n        \nThis model did not give me a good public or private score; therefore, I knew I needed to train and test more models.\n        \n        \nBelow are two links to my model testing for the decision tree model. Both models are the same; however, the second link has a different graph.\n\n\n***Decision Tree Model Testing 1: \nhttps://www.kaggle.com/code/mallorycw/mcw-aviation-submission-3-9-22***\n\n***Decision Tree Model Testing 2: https://www.kaggle.com/code/mallorycw/mcw-decision-tree-model-reducing-aviation***","metadata":{}},{"cell_type":"markdown","source":"**Comments:** When I first decided to perform the same models on the test set, I encountered issues with uploading the test data. Overall, the test data was too large to import. I used my laptop, and Kaggle has limited storage, so I had to upload and test the data in chunks. I ended up using this method for all future testing of my models.","metadata":{}},{"cell_type":"markdown","source":"**Conclusion:** Based on my decision tree model, I knew that I needed to go in a different direction during my modeling process. My public and private score were too high. These scores were not where I wanted them to be when scored by Kaggle.","metadata":{}},{"cell_type":"markdown","source":"# Model Training With Optuna","metadata":{}},{"cell_type":"markdown","source":"Modeling with Optuna was the next step I decided to take throughout my modeling process.\n\nWhat is Optuna? \n    \n    Optuna is a hyperparameter optimization software. Optuna allows users to construct and manipulate hyperparameters that make overall increases in efficiency and provides optimal functionality. Optuna is very versatile.\n    \nHow is Optuna beneficial for modeling within this dataset?\n\n    As you can see in the last section, I had to choose the max depth and min samples leaf for both my models. Instead of choosing the parameters, Optuna allows me to submit a range to each parameter, and Optuna chooses for me. After each run, Optuna chooses a better parameter than the last.","metadata":{}},{"cell_type":"markdown","source":"I chose to train my dataset on a variety of models using Optuna. During all of these modeling processes, I used neg_log_loss scoring.\n\nI used the same beginning processes as the regular models without Optuna by setting up the x and y training variables with an 80/20 split.\n\nFor the decision tree model, I chose a max_depth range of 2 to 500 and a min_samples_leaf range from 1 to 500.\n\nFor my Decision Tree Model with Optuna, my best parameters and scoring are as follows:\n\n            Max Depth: 192\n            Min_Samples_Leaf: 155\n            Score: -0.1807557283078063\n            Wall Time: 1h 49min 13s\n   \n***Decision Tree Model With Optuna: https://www.kaggle.com/code/mallorycw/mcw-aviation-dt-model-w-optuna***\n            \nNext, I chose to train a Light Gradient Boosting Machine (LightGBM) model. I chose a range of 20 to 150 for the number of estimators. I chose a max_depth range of 2 to 40. The learning rate was from 0 to 1 with log=False, and the subsample was from 0.6 to 1 with log=False. Conclusion: there are several different gradient boosting trees so that Optuna can decide between gbdt, dart, and goss.\n\nFor my LGBM Model with Optuna, my best parameters and scoring are as follows:\n\n            n_estimators: 150\n            Max Depth: 9\n            learning_rate = 0.17833752251\n            subsample: 0.6819212428783524\n            boosting_type: gbdt\n            Score: -0.15114824241046781\n            Wall Time: 2h 16min 57s\n\n            \n            \n***LGBM Model wtih Optuna: https://www.kaggle.com/code/mallorycw/mcw-aviation-lightgbm***\n\nNext, I made a random forest model. I chose a max_depth range of 2 to 556 and a min_inst range from 1 to 64, and the number of estimators between 20 and 300.\n\nSince my random forest model took a long time, I decided to test on three different models from the Optuna outputs. I used two of the best score given to test the model before the training Optuna model ultimately finished; therefore, I have three different random forest models. Here you can see the parameters I chose for each of the three models.\n\nModel 1:\n\n            n_estimators: 124\n            max_depth: 156\n            min_samples_leaf: 1\n            Score: -0.2396593729280911\n\nModel 2:\n\n            n_estimators: 145\n            max_depth: 138\n            min_sampes_leaf = 6\nModel 3:\n    \n            n_estimators: 248\n            max_depth: 112\n            min_sampes_leaf = 11           \n            \n***Random Forest Model With Optuna: https://www.kaggle.com/code/mallorycw/mcw-aviation-rf-model***\n\nLastly, I made an extra random tree model. I chose a max_depth range of 2 to 300 and a min_inst range from 1 to 64, and the number of estimators between 20 and 300. I choose features between 4 and 12. The extra-random tree criteria are gini or entropy.\n\nFor my Extra Random Tree Model with Optuna, my best parameters and scoring are as follows:\n\n            n_estimators: 135\n            max_depth: 525\n            min_inst: 1\n            features: 10\n            criterion: entropy\n            Score: --0.10811535177329061\n            Wall Time: 2h 22min 17s\n\n***Extra-Random Trees: https://www.kaggle.com/code/mallorycw/mcw-extra-random-trees-aviation*** ","metadata":{}},{"cell_type":"markdown","source":"\n","metadata":{}},{"cell_type":"markdown","source":"# Testing Optuna Model\n\n**Decision Tree Score:**\n\n                Private Score: 10.92707\n                Public Score:  10.50096\n                \n***Decision Tree Optuna Submission: https://www.kaggle.com/code/mallorycw/mcw-dt-submission-w-optuna?scriptVersionId=92201062***\n \n\n\n\n**LGBM Score:**\n\n                Private Score: 1.25951\n                Public Score:  0.81812\n\n\n***LGBM Model with Optuna Submission: https://www.kaggle.com/code/mallorycw/mcw-lgbm-submission-w-optuna?scriptVersionId=92202554***\n\n\n                            ","metadata":{}},{"cell_type":"markdown","source":"**Random Forest Score:**\n\n\n**Model 1:**\n \n                 Private Score: 0.82668\n                 Public Score:  0.62620\n\n***Random Forest Model 1 Submission: https://www.kaggle.com/code/mallorycw/mcw-rf-submission-version3-w-optuna?scriptVersionId=92772167***                 \n \n\n**Model 2:**\n \n                 Private Score: 0.72443\n                 Public Score:  0.57131\n\n***Random Forest Model 2 Submission: https://www.kaggle.com/code/mallorycw/mcw-rf-submission-w-optuna?scriptVersionId=92364284***   \n\n\n**Model 3:**\n \n                 Private Score: 0.70192\n                 Public Score:  0.55555\n \n\n***Random Forest Model 3 Submission: https://www.kaggle.com/code/mallorycw/mcw-rf-submission-version2-w-optuna?scriptVersionId=92364768***  \n \n \n***After submitting all of the models to the Kaggle competition, the best model overall was the Random Forest: Model 3.***\n ","metadata":{}},{"cell_type":"markdown","source":"# Feature Engineering\n","metadata":{}},{"cell_type":"markdown","source":"**What is feature engineering?**\n\nFeature engineering is beneficial in developing models. Depending on the dataset, you can use various techniques to transform the dataset already given.\n\n**What is the best way to perform feature engineering for my model?**\n\nThe \"Reducing Commercial Aviation Fatalities\" dataset uses physiological data from different pilots. EEG is shorthand for electroencephalography, a method to record electrical activity on the scalp. The column labels within my model are as follows; eeg_fp1, eeg_f7, eeg_f8, eeg_t4, etc. Each of these columns symbolizes a different electrode from the EEG data collected.\n\nDoctors do not use just one electrode to detect patterns; however, they use electrode pairs to detect activities. There are numerous ways to pair these electrodes; however, I found a picture of different montages that show how I can combine these different EEG columns.\n\n**Here is a depiction of the Montages:**","metadata":{}},{"cell_type":"code","source":"from IPython.display import Image\nimg3 = '../input/images-aviation/Screenshot (216).png'\nImage(filename=img3)","metadata":{"execution":{"iopub.status.busy":"2022-05-03T22:47:40.010827Z","iopub.execute_input":"2022-05-03T22:47:40.01199Z","iopub.status.idle":"2022-05-03T22:47:40.066179Z","shell.execute_reply.started":"2022-05-03T22:47:40.011912Z","shell.execute_reply":"2022-05-03T22:47:40.064992Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I attached a link below with some visualizations and Montages 1 through 3. I did not use all of the montages; however, I did construct all of these montages within this Kaggle Notebook to start the feature engineering process.\n\nInitial Feature Engineering Notebook:\nhttps://www.kaggle.com/code/mallorycw/mcw-aviation-feature-engineering/edit/run/93027900","metadata":{}},{"cell_type":"markdown","source":"# Training Notebooks","metadata":{}},{"cell_type":"markdown","source":"Instead of using all of the montages depicted in the photo for my models. I decided to use the top three photos. Here is the training notebooks below.\n\n**Random Forest Model Optuna with Feature Engineering:**\n\n            Best Score: -0.20848149619455\n            n_estimators: 240\n            max_depth: 398\n            min_inst: 1\n\n***Random Forest Optuna Model with Montage 2: \nhttps://www.kaggle.com/code/mallorycw/mcw-aviation-rf-model-w-feature-engineering***\n\n**Extra Random Trees with Feature Engineering (Original Features + Montage 2):**\n\n            Best Score: -0.09885626885827817\n            n_estimators: 256\n            max_depth: 125\n            min_inst: 1\n            features: 11\n            criterion: 'entropy'\n\n***Extra Random Trees Optuna Model with Original Features and Montage 2): https://www.kaggle.com/code/mallorycw/mcw-extra-random-trees-w-feature-engineering***\n\n**Extra Random Trees with Feature Engineering (Montage 1 + 3):**\n\n            Best Score: -0.07354581038472197\n            n_estimators: 115\n            max_depth: 96\n            min_inst: 1\n            features: 12\n            criterion: 'entropy'\n\n***Extra Random Trees Optuna Model with Montage 1 and 3: https://www.kaggle.com/code/mallorycw/mcw-ert-w-features-montage-1-and-3***\n","metadata":{}},{"cell_type":"markdown","source":"# All Original Features and Montages\n\nI submitted the notebook using all of the features and the parameters from extra random trees parameters from the extra random trees notebook with the following parameters.\n            \n            n_estimators: 115\n            max_depth: 96\n            min_inst: 1\n            features: 12\n            criterion: 'entropy'\n            \n**Submission Score:**\n            \n            Private Score: 0.77650\n            Public Score:  0.57646\n\n***Testing on All Features and Montages: https://www.kaggle.com/code/mallorycw/mcw-2-submission-ert-w-features-montage-1***\n","metadata":{}},{"cell_type":"markdown","source":"# Final Conclusion\n\nBased on the score of all of these models. My Random Forest Model with Optuna is the best overall model. Here is the score below.\n\n**Model 3:**\n \n                 Private Score: 0.70192\n                 Public Score:  0.55555\n                 \n\nSince this model was my best model, I made a notebook to get a full view of the model and how it was performing. I used 70% of the training data and the parameters from this model.\n    The model was a Random Forest Model with a max depth of 112, min_samples_leaf of 11, n_estimators of 248, and a random_state of 1. I used cross_val_predict to estimate each of our three states and the possibility of no event.\n    After that, I did a classification report of the data. Overall, I had a 94% accuracy. My weighted average is 93% which is good since it calculates the number of true positives in all classes.\n    \n***Analysis of Best Model: https://www.kaggle.com/code/mallorycw/best-model-accuracy-2?scriptVersionId=94761300***\n\n\n***Reference to the Original Random Forest Model 3 Submission suing Optuna: https://www.kaggle.com/code/mallorycw/mcw-rf-submission-version2-w-optuna?scriptVersionId=92364768***  ","metadata":{}},{"cell_type":"markdown","source":"# Future Input\n\nIf someone were to continue this project, I would suggest looking more into feature engineering. I would suggest feature engineering on a random forest model using optuna, although it would require a longer run time. I would also suggest approaching ensembling.\n","metadata":{}}]}