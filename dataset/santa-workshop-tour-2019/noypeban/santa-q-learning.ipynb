{"cells":[{"metadata":{},"cell_type":"markdown","source":"In this competision, our task is to assign a day of each 5000 families. If we change an assigned day of one family, we'll get a different score. Assuming this as `Markov decision process (MDP)`, and we can imagine problem like\n\n* S is state, assined day of each families\n* A is action, how to re assign day of a family\n* R is reward, how much score decrease\n\nBoth state after transition $S'$ and Reward $R$ only depends on current state $S$ and action $A$ . \nNow we'd like to know how action of current state $A_S$ can get max $R$ ?\n\nIn this notebook, I'll try to use Q-learning approach. Because I just wanted to do new things at the end of the year. Thank you for google, wikipedia, kaggle and our greate community! And welcome to any comment (I realy not understand method..). Have a good new year!"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom tqdm.notebook import tqdm as tqdm\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/santa-workshop-tour-2019/family_data.csv')\nprint(df.shape)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Score function\n\nhttps://www.kaggle.com/inversion/santa-s-2019-starter-notebook"},{"metadata":{"trusted":true},"cell_type":"code","source":"family_size_dict = df[['n_people']].to_dict()['n_people']\n\ncols = [f'choice_{i}' for i in range(10)]\nchoice_dict = df[cols].to_dict()\n\nN_DAYS = 100\nMAX_OCCUPANCY = 300\nMIN_OCCUPANCY = 125\n\n# from 100 to 1\ndays = list(range(N_DAYS,0,-1))\n\ndef cost_function(prediction):\n\n    penalty = 0\n\n    # We'll use this to count the number of people scheduled each day\n    daily_occupancy = {k:0 for k in days}\n    \n    # Looping over each family; d is the day for each family f\n    for f, d in enumerate(prediction):\n\n        # Using our lookup dictionaries to make simpler variable names\n        n = family_size_dict[f]\n        choice_0 = choice_dict['choice_0'][f]\n        choice_1 = choice_dict['choice_1'][f]\n        choice_2 = choice_dict['choice_2'][f]\n        choice_3 = choice_dict['choice_3'][f]\n        choice_4 = choice_dict['choice_4'][f]\n        choice_5 = choice_dict['choice_5'][f]\n        choice_6 = choice_dict['choice_6'][f]\n        choice_7 = choice_dict['choice_7'][f]\n        choice_8 = choice_dict['choice_8'][f]\n        choice_9 = choice_dict['choice_9'][f]\n\n        # add the family member count to the daily occupancy\n        daily_occupancy[d] += n\n\n        # Calculate the penalty for not getting top preference\n        if d == choice_0:\n            penalty += 0\n        elif d == choice_1:\n            penalty += 50\n        elif d == choice_2:\n            penalty += 50 + 9 * n\n        elif d == choice_3:\n            penalty += 100 + 9 * n\n        elif d == choice_4:\n            penalty += 200 + 9 * n\n        elif d == choice_5:\n            penalty += 200 + 18 * n\n        elif d == choice_6:\n            penalty += 300 + 18 * n\n        elif d == choice_7:\n            penalty += 300 + 36 * n\n        elif d == choice_8:\n            penalty += 400 + 36 * n\n        elif d == choice_9:\n            penalty += 500 + 36 * n + 199 * n\n        else:\n            penalty += 500 + 36 * n + 398 * n\n\n    # for each date, check total occupancy\n    #  (using soft constraints instead of hard constraints)\n    for _, v in daily_occupancy.items():\n        if (v > MAX_OCCUPANCY) or (v < MIN_OCCUPANCY):\n            penalty += 100000000\n\n    # Calculate the accounting cost\n    # The first day (day 100) is treated special\n    accounting_cost = (daily_occupancy[days[0]]-125.0) / 400.0 * daily_occupancy[days[0]]**(0.5)\n    # using the max function because the soft constraints might allow occupancy to dip below 125\n    accounting_cost = max(0, accounting_cost)\n    \n    # Loop over the rest of the days, keeping track of previous count\n    yesterday_count = daily_occupancy[days[0]]\n    for day in days[1:]:\n        today_count = daily_occupancy[day]\n        diff = abs(today_count - yesterday_count)\n        accounting_cost += max(0, (daily_occupancy[day]-125.0) / 400.0 * daily_occupancy[day]**(0.5 + diff / 50.0))\n        yesterday_count = today_count\n\n    penalty += accounting_cost\n\n    return penalty\n\nsubmission = pd.read_csv('../input/santa-workshop-tour-2019/sample_submission.csv')\ncost_function(submission['assigned_day'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ary_n_people = df.n_people.values\nary_n_people","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ary_choices = df.iloc[:, 1:-1].values-1\nary_choices","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## data\n\nFor simplicity, I'll make a binary matrix. `data` has row = number of all families, column = number of all workshop day and 1 if assined.\n\n内部状態を表すために、家族数 x 日付の表を作り、参加する日に1、参加しない日に０を入れる。 \nまずはsample_submissionの日付を入れた。"},{"metadata":{"trusted":true},"cell_type":"code","source":"N_FAMILY = len(df)\nN_DAYS = 100\ndata = np.zeros((N_FAMILY, N_DAYS))\nfor i, d in enumerate(submission['assigned_day']):\n    data[i, d-1]=1\n# for i, d in enumerate(ary_choices[:,0]):\n#     data[i, d-1]=1\ndata","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### State\n\nIt's feel like a feature engineearing. You can put any other state features like family numbers, N<125, difference of prev day $N_{d-1}$\n\n$S_{family\\_id,day}$\n\n|key|mean|min|max|\n|--|--|--|--|\n|choiced_order|whether if given day is choiced order of this family|0 (choice_0)|10 (other)|\n|occupancy_flag0|total people of choice_0 is over 300? $N_{choice\\_0}>300$|0|1|\n|occupancy_flag1|total people of choice_1 is over 300? $N_{choice\\_1}>300$|0|1|\n|occupancy_flag2|total people of choice_2 is over 300? $N_{choice\\_2}>300$|0|1|\n|occupancy_flag3|total people of choice_3 is over 300? $N_{choice\\_3}>300$|0|1|\n|occupancy_flag4|total people of choice_4 is over 300? $N_{choice\\_4}>300$|0|1|\n|occupancy_flag5|total people of choice_5 is over 300? $N_{choice\\_5}>300$|0|1|\n|occupancy_flag6|total people of choice_6 is over 300? $N_{choice\\_6}>300$|0|1|\n|occupancy_flag7|total people of choice_7 is over 300? $N_{choice\\_7}>300$|0|1|\n|occupancy_flag8|total people of choice_8 is over 300? $N_{choice\\_8}>300$|0|1|\n|occupancy_flag9|total people of choice_9 is over 300? $N_{choice\\_9}>300$|0|1|"},{"metadata":{"trusted":true},"cell_type":"code","source":"N_STATE = 11 * 2**10\nN_STATE","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_state(data, family_id, day):\n    choice_order = np.where(ary_choices[family_id] == day)[0]\n    if choice_order.shape[0] > 0:\n        choice_order = choice_order[0]\n    else:\n        choice_order = 10\n    occupancy_flag = np.zeros(10)\n    for i, choiced_day in enumerate(ary_choices[family_id]):\n        if (data[:,choiced_day] * ary_n_people).sum() > 300:\n            occupancy_flag[i] = 1\n    return choice_order, occupancy_flag\n\nget_state(data, 0, np.argmax(data[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_state_row(data, family_id):\n    day = np.argmax(data[family_id])\n    choice_order, occupancy_flag = get_state(data, family_id, day)\n    res = choice_order + 11 * np.sum(occupancy_flag * np.array([512,256,128,64,32,16,8,4,2,1]))\n    return res.astype(int)\n\nget_state_row(data, 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Action\n$A_{family\\_id,day}$\n\n|value|mean|\n|--|--|\n|0-9|move to choice_0～choice_9|\n|10|not move|\n|11|move most less people|"},{"metadata":{"trusted":true},"cell_type":"code","source":"N_ACTION = 12","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_action(next_state_row, episode, q_table):\n    epsilon = 0.5 * (0.99** episode)\n    if epsilon <= np.random.uniform(0, 1): \n        next_action = np.argmax(q_table[next_state_row])\n    else:\n        next_action = np.random.choice(range(N_ACTION))\n    return next_action","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Reward"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_score(data):\n    pred = 1+np.argmax(data, axis=1)\n    return round(cost_function(pred))\n\nget_score(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def update_data(data, family_id, action):\n    if action<10:\n        # move to choice_0 - choice_9\n        new_day = ary_choices[family_id, action]\n    elif action == 10:\n        # no action\n        return data\n    elif action == 11:\n        # move most less people day\n        new_day = np.argmin([np.sum(data[:,i]*ary_n_people) for i in range(100)])\n    new_data = data.copy()\n    new_row = np.zeros(N_DAYS)\n    new_row[new_day] = 1\n    new_data[family_id] = new_row\n    return new_data\n\nget_score(update_data(data, 0, 11))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Q table\n\n`q_table` is matrix of expected reward. row=state, column=actions.\n\nIn the Q learning approach, q_table is model. Updates q_table with reward feedback; and select next action from argmax(q_table[state])"},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_qtable(*size):\n    return np.random.uniform(low = -1, high = 1, size = size)\n\nmake_qtable(N_STATE, N_ACTION).shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def update_qtable(q_table, state, action, reward, next_state):\n    gamma = 0.99\n    alpha = 0.50\n    next_max = max(q_table[next_state])\n    q_table[state, action] = (1-alpha)*q_table[state, action] +\\\n    alpha * (reward + gamma * next_max)\n    return q_table","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# learn\n\n* reward clipping `np.clip(reward, -1, 1)`"},{"metadata":{"trusted":true},"cell_type":"code","source":"def learn(data, q_table=None, n_loop=10_000, step=10):\n    if not hasattr(q_table, 'shape'):\n        q_table = make_qtable(N_STATE, N_ACTION)\n    history = []\n    best_data = data.copy()\n    best_score = get_score(data)\n    score = best_score\n    reward = 0   \n    with tqdm(total=n_loop) as pbar:\n        for episode in range(n_loop):\n            new_data = best_data.copy()\n            state = get_state_row(new_data, 0)\n            action = np.argmax(q_table[state])\n\n            for family_id in np.random.choice(range(N_FAMILY), step):\n                next_state = get_state_row(new_data, family_id)\n                q_table = update_qtable(q_table, state, action, reward, next_state)\n                action = get_action(next_state, episode, q_table)\n                state = next_state\n                new_data = update_data(new_data, family_id, action)\n\n            new_score = get_score(new_data)\n            reward = np.clip(score - new_score, -1, 1)\n            score = new_score\n\n            if best_score > new_score:\n                best_data = new_data.copy()\n                best_score = new_score\n            history.append(best_score)\n            pbar.set_description(f'best_score={best_score:,}')\n            pbar.update()\n    plt.plot(history)\n    plt.show()\n    return best_data, q_table","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_data, q_table = learn(data, n_loop=30_000, step=5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It's looks like plateau, try to search best step parameter."},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in [3,5,10,15,20,30,50,100]:\n    print(f'step={i}')\n    _, _ = learn(best_data, n_loop=100, step=i)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2nd learn\n\n* step=3\n* n_loop=10_000\n* 100 times learning with reset q_table"},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(100):\n    best_data, _ = learn(best_data, step=3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = np.argmax(best_data, axis=1)+1\npred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cost_function(pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission['assigned_day'] = pred\nsubmission.to_csv('submission.csv', index=False)\nsubmission.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":1}