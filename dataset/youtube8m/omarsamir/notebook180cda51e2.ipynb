{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9b4c9441-4da0-c93e-50aa-619a51552b79"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b2f800a8-210e-c4dc-4af0-a3d5f9171e10"},"outputs":[],"source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf\nimport seaborn as sns\nfrom IPython.display import YouTubeVideo\nimport matplotlib.pyplot as plt\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\nprint(check_output([\"ls\", \"../input/video_level\"]).decode(\"utf8\"))\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"795bc03e-25e1-0d4b-cb21-53833de26a3f"},"outputs":[],"source":"frame_lvl_record = \"../input/frame_level/train-1.tfrecord\"\n\ndef rgb_and_audio_from(tf_seq_example):\n    feat_rgb = []\n    feat_audio = []\n    n_frames = len(tf_seq_example.feature_lists.feature_list['audio'].feature)\n    sess = tf.InteractiveSession()\n    rgb_frame = []\n    audio_frame = []\n    # iterate through frames for that example\n    for i in range(n_frames):\n        rgb_frame.append(tf.cast(tf.decode_raw(\n                tf_seq_example.feature_lists.feature_list['rgb'].feature[i].bytes_list.value[0],tf.uint8)\n                       ,tf.float32).eval())\n        audio_frame.append(tf.cast(tf.decode_raw(\n                tf_seq_example.feature_lists.feature_list['audio'].feature[i].bytes_list.value[0],tf.uint8)\n                       ,tf.float32).eval())\n        \n        \n    sess.close()\n    feat_rgb.append(rgb_frame)\n    feat_audio.append(audio_frame)\n    return feat_rgb, feat_audio\n\n# now, let's read the frame-level data\n# due to execution time, we're only going to read the first video\n\ni = 0\nfor example in tf.python_io.tf_record_iterator(frame_lvl_record):     \n#    i+=1\n#    if i < 3:\n#        continue\n    tf_seq_example = tf.train.SequenceExample.FromString(example)\n    video_id = tf_seq_example.context.feature['video_id'].bytes_list.value[0]\n    print('https://www.youtube.com/watch?v={}'.format(str(video_id)))\n    rgb, audio = rgb_and_audio_from(tf_seq_example)\n    break"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8b474530-6bdb-66d2-7422-d11531a8c60f"},"outputs":[],"source":"filenames = [\"../input/video_level/train-{}.tfrecord\".format(i) for i in range(10)]\nlabels_df = pd.read_csv('../input/label_names.csv')\n\nvideo_lables = []\nvideo_rgb_mean = []\nvideo_audio_mean = []\nlabels = []\n\nfor filename in filenames:\n    print(filename)\n    i = 0;\n    for example in tf.python_io.tf_record_iterator(filename):\n        tf_example = tf.train.Example.FromString(example)\n        label_example = list(tf_example.features.feature['labels'].int64_list.value)\n        label_example_textual = list(labels_df[labels_df['label_id'].isin(label_example)]['label_name'])\n        int_labels = []\n        for k in range(0, len(label_example_textual)):\n            v = labels_df[labels_df['label_name'] == label_example_textual[k]].index\n            int_labels.append(v[0])\n        rgb_mean = tf_example.features.feature['mean_rgb'].float_list.value\n        audio_mean = tf_example.features.feature['mean_rgb'].float_list.value\n        video_rgb_mean.append(rgb_mean)\n        video_audio_mean.append(audio_mean)\n        labels.append(int_labels)\n        if((i % 100000) == 0):\n            print(i)\n        i = i + 1\nX_list = video_audio_mean\nY = labels"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9e0075b8-893a-1ab1-14eb-a6fe1c199847"},"outputs":[],"source":"X = np.array(X_list)\nprint(len(labels))\nnew_label = []\nfor i in range(0, len(Y)):\n    #if(i % 10 == 0):\n    #    print(i)\n    if(len(Y[i]) == 0):\n        new_label.append(0)\n    else:\n        new_label.append(Y[i][0])\ny = np.array(new_label)\ny.shape\nlabels[1]\nX.shape"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"33729fda-92da-d561-eab7-94f3cf552a1d"},"outputs":[],"source":"len(y[3390])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0b487e7a-7842-df73-85ac-53134d9e36d8"},"outputs":[],"source":"## importing the required packages\nfrom time import time\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import offsetbox\nfrom sklearn import (manifold, datasets, decomposition, ensemble,\n             discriminant_analysis, random_projection)\n## Loading and curating the data\n#digits = datasets.load_digits(n_class=10)\n#X = digits.data\n#y = digits.target\nn_samples, n_features = X.shape\nn_neighbors = 30\n## Function to Scale and visualize the embedding vectors\ndef plot_embedding(X, title=None):\n    x_min, x_max = np.min(X, 0), np.max(X, 0)\n    X = (X - x_min) / (x_max - x_min)     \n    plt.figure()\n    ax = plt.subplot(111)\n    for i in range(X.shape[0]):\n        plt.text(X[i, 0], X[i, 1], str(y[i]),\n                 color=plt.cm.Set1(y[i] / 4716.),\n                 fontdict={'weight': 'bold', 'size': 4716})\n    if hasattr(offsetbox, 'AnnotationBbox'):\n        ## only print thumbnails with matplotlib > 1.0\n        shown_images = np.array([[1., 1.]])  # just something big\n        for i in range(y.shape[0]):\n            dist = np.sum((X[i] - shown_images) ** 2, 1)\n            if np.min(dist) < 4e-3:\n                ## don't show points that are too close\n                continue\n            shown_images = np.r_[shown_images, [X[i]]]\n            imagebox = offsetbox.AnnotationBbox(\n                offsetbox.OffsetImage(y[i], cmap=plt.cm.gray_r),\n                X[i])\n            ax.add_artist(imagebox)\n    plt.xticks([]), plt.yticks([])\n    if title is not None:\n        plt.title(title)\n\n#----------------------------------------------------------------------\nplt.title('A selection from the 64-dimensional digits dataset')\n## Computing PCA\nprint(\"Computing PCA projection\")\nt0 = time()\nX_pca = decomposition.TruncatedSVD(n_components=2).fit_transform(X)\nplot_embedding(X_pca,\n               \"Principal Components projection of the digits (time %.2fs)\" %\n               (time() - t0))\n## Computing t-SNE\nprint(\"Computing t-SNE embedding\")\ntsne = manifold.TSNE(n_components=2, init='pca', random_state=0)\nt0 = time()\nX_tsne = tsne.fit_transform(X)\nplot_embedding(X_tsne,\n               \"t-SNE embedding of the digits (time %.2fs)\" %\n               (time() - t0))\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"db690dde-22d4-360a-013f-035cf43e1e39"},"outputs":[],"source":"v = labels_df[labels_df['label_name'] == 'Car'].index\nv[0]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"aa7f54f2-4508-ac7f-88d2-1ef23e4fec73"},"outputs":[],"source":"v.at[0, 'label_id']"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e1f02668-ad11-1e53-4cce-d8e96ec9a612"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}