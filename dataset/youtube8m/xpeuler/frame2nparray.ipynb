{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"d4388e19-0670-90c6-a2a8-88e9f79c1b49"},"source":"# process frame_level records, convert data into numpy array\n\nTested in **python 3.5** local. \n\nMostly copy-and-paste of starter code clips. Scratches only. Comments and refinement welcome."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"16a36c4f-3a01-a9aa-4ff4-618e03aa5a38"},"outputs":[],"source":"import json\nimport os\nimport time\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom IPython.display import YouTubeVideo\nfrom subprocess import check_output\n\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nfrom tensorflow import app\nfrom tensorflow import flags\nfrom tensorflow import gfile\nfrom tensorflow import logging\n\n#%% source starter code. Place this notebook in the same directory of starter code.\n#import readers\n#import utils\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"733b3a84-0445-c83d-e21d-a22aa6d4e23c"},"outputs":[],"source":"# use interactive session\nsess = tf.InteractiveSession()\nLABELs = pd.read_csv('../input/label_names.csv') #4716 labels"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"dd7f3d4d-0a9a-a6e9-906f-fe37cf2dd185"},"outputs":[],"source":"# helper functions.\n# modified from sarter code. using readers.py and utils.py.\n\n# from utils.py\ndef Dequantize(feat_vector, max_quantized_value=2, min_quantized_value=-2):\n  \"\"\"Dequantize the feature from the byte format to the float format.\n\n  Args:\n    feat_vector: the input 1-d vector.\n    max_quantized_value: the maximum of the quantized value.\n    min_quantized_value: the minimum of the quantized value.\n\n  Returns:\n    A float vector which has the same shape as feat_vector.\n  \"\"\"\n  assert max_quantized_value > min_quantized_value\n  quantized_range = max_quantized_value - min_quantized_value\n  scalar = quantized_range / 255.0\n  bias = (quantized_range / 512.0) + min_quantized_value\n  return feat_vector * scalar + bias\n\n# from readers.py\ndef resize_axis(tensor, axis, new_size, fill_value=0):\n  \"\"\"Truncates or pads a tensor to new_size on on a given axis.\n\n  Truncate or extend tensor such that tensor.shape[axis] == new_size. If the\n  size increases, the padding will be performed at the end, using fill_value.\n\n  Args:\n    tensor: The tensor to be resized.\n    axis: An integer representing the dimension to be sliced.\n    new_size: An integer or 0d tensor representing the new value for\n      tensor.shape[axis].\n    fill_value: Value to use to fill any new entries in the tensor. Will be\n      cast to the type of tensor.\n\n  Returns:\n    The resized tensor.\n  \"\"\"\n  tensor = tf.convert_to_tensor(tensor)\n  shape = tf.unstack(tf.shape(tensor))\n\n  pad_shape = shape[:]\n  pad_shape[axis] = tf.maximum(0, new_size - shape[axis])\n\n  shape[axis] = tf.minimum(shape[axis], new_size)\n  shape = tf.stack(shape)\n\n  resized = tf.concat([\n      tf.slice(tensor, tf.zeros_like(shape), shape),\n      tf.fill(tf.stack(pad_shape), tf.cast(fill_value, tensor.dtype))\n  ], axis)\n\n  # Update shape.\n  new_shape = tensor.get_shape().as_list()  # A copy is being made.\n  new_shape[axis] = new_size\n  resized.set_shape(new_shape)\n  return resized\n\ndef get_video_matrix(features,\n                     feature_size,\n                     max_frames,\n                     max_quantized_value,\n                     min_quantized_value):\n    \"\"\"Decodes features from an input string and quantizes it.\n\n    Args:\n      features: raw feature values\n      feature_size: length of each frame feature vector\n      max_frames: number of frames (rows) in the output feature_matrix\n      max_quantized_value: the maximum of the quantized value.\n      min_quantized_value: the minimum of the quantized value.\n\n    Returns:\n      feature_matrix: matrix of all frame-features\n      num_frames: number of frames in the sequence\n    \"\"\"\n    decoded_features = tf.reshape(\n        tf.cast(tf.decode_raw(features, tf.uint8), tf.float32),\n        [-1, feature_size])\n\n    num_frames = tf.minimum(tf.shape(decoded_features)[0], max_frames)\n    feature_matrix = Dequantize(decoded_features,\n                                      max_quantized_value,\n                                      min_quantized_value)\n    feature_matrix = resize_axis(feature_matrix, 0, max_frames)\n    return feature_matrix, num_frames"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8f7a79bd-9cda-8250-67cc-95a77885e095"},"outputs":[],"source":"# consts\nnum_features=2\nfeature_names, feature_sizes = (['rgb','audio'], [1024,128])\nmax_frames=300\nmax_quantized_value=2\nmin_quantized_value=-2"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6d62ee52-5048-7704-5405-0aa4c0d24940"},"outputs":[],"source":"# working with local files. change path to your own.\nvidfilenames = \"../input/frame_level/train-2.tfrecord\"\nexamples = tf.python_io.tf_record_iterator(vidfilenames)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2cd4307f-2dbc-0ac4-4237-dc01b05570a2"},"outputs":[],"source":"aa = next(examples)\nprint(len(aa))\nprint(type(aa))\nbb = tf.decode_raw(aa,tf.uint8)\ncc = sess.run([bb])\ncc[0].shape"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5f33f567-e29e-702f-c5ca-a389b8c90a68"},"outputs":[],"source":"# from reader. Important to set context_features and sequance_features correctly\ncontexts, features = tf.parse_single_sequence_example(\n        aa,\n        context_features={\"video_id\": tf.FixedLenFeature(\n            [], tf.string),\n                          \"labels\": tf.VarLenFeature(tf.int64)},\n        sequence_features={\n            feature_name : tf.FixedLenSequenceFeature([], dtype=tf.string)\n            for feature_name in ['rgb','audio']\n        })\n\nlabels = (tf.cast(\n        tf.sparse_to_dense(contexts[\"labels\"].values, (4716,), 1,\n            validate_indices=False),\n        tf.bool))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"db873b9a-5835-f387-d8e0-6d3e34b652dd"},"outputs":[],"source":"num_frames = -1  # the number of frames in the video\nfeature_matrices = [None] * num_features  # an array of different features\nfor feature_index in range(num_features):\n    feature_matrix, num_frames_in_this_feature = get_video_matrix(\n        features[feature_names[feature_index]],\n        feature_sizes[feature_index],\n        max_frames,\n        max_quantized_value,\n        min_quantized_value)\n    if num_frames == -1:\n        num_frames = num_frames_in_this_feature\n    else:\n        tf.assert_equal(num_frames, num_frames_in_this_feature)\n\n    feature_matrices[feature_index] = feature_matrix\n\n# cap the number of frames at self.max_frames\nnum_frames = tf.minimum(num_frames, max_frames)\n\n# concatenate different features\nvideo_matrix = tf.concat(feature_matrices, 1)\n\n# convert to batch format.\n# TODO: Do proper batch reads to remove the IO bottleneck.\nbatch_video_ids = tf.expand_dims(contexts[\"video_id\"], 0)\nbatch_video_matrix = tf.expand_dims(video_matrix, 0)\nbatch_labels = tf.expand_dims(labels, 0)\nbatch_frames = tf.expand_dims(num_frames, 0)"},{"cell_type":"markdown","metadata":{"_cell_guid":"a289889c-ff33-31ba-acaa-9782b8979469"},"source":"### Pick some frames to display"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cc8848cf-d4bf-680a-0af5-81422cbbbec8"},"outputs":[],"source":"# sess.run to get data in numpy array.\n[x1, x2, x3, x4] = sess.run([num_frames, video_matrix, batch_labels, batch_frames])\n[z1, z2] = sess.run([labels, num_frames])\nvid_byte = sess.run(batch_video_ids)\nvid=vid_byte[0].decode()\nprint('vid = %s'%vid)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6fbe9a8b-7f1c-75d0-c7a0-06472d25b418"},"outputs":[],"source":"print(vid, x1)\nprint(LABELs[x3.T])\nprint(x2.shape)\nprint(contexts[\"labels\"].values)\nplt.figure(figsize=[12,8])\nfor i in range(4):\n    k = int( (i+1)*2/10 * x1 )\n    plt.subplot(2,4, i+1)\n    plt.imshow(np.array(x2[k,:1024]).reshape([32,32]));\n    plt.subplot(2,4, i+5)\n    plt.imshow(np.array(x2[k,1024:]).reshape([16,8]));"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8cbd77cc-c8b4-628d-cec4-795db1f6f929"},"outputs":[],"source":"# will run on local machine, not on kagglegym.\nYouTubeVideo(vid)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"172946c0-6be1-b61d-0204-c185552227d4"},"outputs":[],"source":"sess.close()"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}