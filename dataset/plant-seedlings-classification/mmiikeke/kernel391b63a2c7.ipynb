{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.utils.data import Dataset\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\nimport torch.nn as nn\nimport torch\n\nfrom torchvision import transforms\nfrom PIL import Image\nimport math\nimport copy\nfrom os.path import join\nfrom os import listdir\nimport os.path\nimport random\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nclass PlantSeedlingDataset(Dataset):\n    def __init__(self, root_dir, transform=None):\n        self.root_dir = root_dir\n        self.x = []\n        self.y = []\n        self.transform = transform\n        self.num_classes = 0\n        \n        i = 0\n        \n        for f in listdir(root_dir):\n            path2 = join(root_dir, f)\n            print(f)\n            for f2 in listdir(path2):\n                self.x.append(join(path2, f2))\n                self.y.append(i)\n            self.num_classes += 1\n            i += 1\n\n    def __len__(self):\n        return len(self.x)\n\n    def __getitem__(self, index):\n        image = Image.open(self.x[index]).convert('RGB')\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, self.y[index]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class VGG16(nn.Module):\n    def __init__(self, num_classes=1000):\n        super(VGG16, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n\n            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n\n            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n        )\n        self.classifier = nn.Sequential(\n            # input shape: (batch_size, 3, 224, 224) and\n            # downsampled by a factor of 2^5 = 32 (5 times maxpooling)\n            # So features' shape is (batch_size, 7, 7, 512)\n            nn.Linear(in_features=28 * 28 * 256, out_features=2048),\n            nn.ReLU(),\n            nn.Dropout(),\n            nn.Linear(in_features=2048, out_features=2048),\n            nn.ReLU(),\n            nn.Dropout(),\n            nn.Linear(in_features=2048, out_features=num_classes)\n        )\n\n        # initialize parameters\n        for module in self.modules():\n            if isinstance(module, nn.Conv2d):\n                n = module.kernel_size[0] * module.kernel_size[1] * module.out_channels\n                module.weight.data.normal_(0, math.sqrt(2. / n))\n                module.bias.data.zero_()\n            elif isinstance(module, nn.Linear):\n                module.weight.data.normal_(0, 0.01)\n                module.bias.data.zero_()\n\n    def forward(self, x):\n        x = self.features(x)\n        # flatten\n        x = x.view(x.size(0), -1)\n        x = self.classifier(x)\n        return x\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"CUDA_DEVICES = 0\ntrain_loss_list = []\nvalidation_loss_list = []\ntrain_acc_list = []\nvalidation_acc_list = []\n\ndef train():\n    data_transform = transforms.Compose([\n        transforms.RandomResizedCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n    data_set = PlantSeedlingDataset('/kaggle/input/plant-seedlings-classification/train', data_transform)\n    train_set = PlantSeedlingDataset('/kaggle/input/plant-seedlings-classification/train', data_transform)\n    validation_set = PlantSeedlingDataset('/kaggle/input/plant-seedlings-classification/train', data_transform)\n    \n    #data_loader = DataLoader(dataset=data_set, batch_size=32, shuffle=True, num_workers=1)\n    \n    \"\"\"\n    #train test split\n    batch_size = 16\n    validation_split = .2\n    shuffle_data_loader = True\n    random_seed= 42\n\n    # Creating data indices for training and validation splits:\n    dataset_size = len(data_loader)\n    indices = list(range(dataset_size))\n    split = int(np.floor(validation_split * dataset_size))\n    if shuffle_data_loader :\n        np.random.seed(random_seed)\n        np.random.shuffle(indices)\n    train_indices, val_indices = indices[split:], indices[:split]\n\n    # Creating PT data samplers and loaders:\n    train_sampler = SubsetRandomSampler(train_indices)\n    valid_sampler = SubsetRandomSampler(val_indices)\n\n    train_loader = torch.utils.data.DataLoader(data_loader, batch_size=batch_size, \n                                               sampler=train_sampler)\n    validation_loader = torch.utils.data.DataLoader(data_loader, batch_size=batch_size,\n                                                    sampler=valid_sampler)\n    \"\"\"\n    \n    #train test split\n    dataset_size = len(data_set)\n    validation_split = .2\n    split = int(np.floor(validation_split * dataset_size))\n    array_all = np.arange(dataset_size)\n    random.shuffle(array_all)\n    array_validation = array_all[:split]\n    array_train = array_all[split:]\n    \n    train_set.x = [data_set.x[idx] for idx in array_train]\n    train_set.y = [data_set.y[idx] for idx in array_train]\n    validation_set.x = [data_set.x[idx] for idx in array_validation]\n    validation_set.y = [data_set.y[idx] for idx in array_validation]\n    \n    train_loader = DataLoader(dataset=train_set, batch_size=32, shuffle=True, num_workers=1)\n    validation_loader = DataLoader(dataset=validation_set, batch_size=32, shuffle=True, num_workers=1)\n\n\n    model = VGG16(num_classes=data_set.num_classes)\n    model = model.cuda(CUDA_DEVICES)\n    model.train()\n\n    best_model_params = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n    num_epochs = 80\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.SGD(params=model.parameters(), lr=0.001, momentum=0.9)\n\n    for epoch in range(num_epochs):\n        print(f'Epoch: {epoch + 1}/{num_epochs}')\n        print('-' * len(f'Epoch: {epoch + 1}/{num_epochs}'))\n\n        training_loss = 0.0\n        training_corrects = 0.0\n        validation_loss = 0.0\n        validation_corrects = 0.0\n        \n        for i, (inputs, labels) in enumerate(train_loader):\n            inputs = Variable(inputs.cuda(CUDA_DEVICES))\n            labels = Variable(labels.cuda(CUDA_DEVICES))\n\n            optimizer.zero_grad()\n\n            outputs = model(inputs)\n            _, preds = torch.max(outputs.data, 1)\n            loss = criterion(outputs, labels)\n\n            loss.backward()\n            optimizer.step()\n            \n            training_loss += loss.data * inputs.size(0)\n            training_corrects += torch.sum(preds == labels.data)\n\n        training_loss = float(training_loss) / (dataset_size - split)\n        training_acc = float(training_corrects) / (dataset_size - split)\n\n        print(f'Train loss: {training_loss:.4f}\\taccuracy: {training_acc:.4f}')\n        \n        for i, (inputs, labels) in enumerate(validation_loader):\n            inputs = Variable(inputs.cuda(CUDA_DEVICES))\n            labels = Variable(labels.cuda(CUDA_DEVICES))\n\n            outputs = model(inputs)\n            _, preds = torch.max(outputs.data, 1)\n            loss = criterion(outputs, labels)\n            \n            validation_loss += loss.data * inputs.size(0)\n            validation_corrects += torch.sum(preds == labels.data)\n        \n        validation_loss = float(validation_loss) / split\n        validation_acc = float(validation_corrects) / split\n\n        print(f'Validation loss: {validation_loss:.4f}\\taccuracy: {validation_acc:.4f}')\n        \n        train_acc_list.append(training_acc)\n        validation_acc_list.append(validation_acc)\n        train_loss_list.append(training_loss)\n        validation_loss_list.append(validation_loss)\n        \n        if training_acc > best_acc:\n            best_acc = training_acc\n            best_model_params = copy.deepcopy(model.state_dict())\n\n    model.load_state_dict(best_model_params)\n    torch.save(model, f'model-weight_and_bias.pth')\n\n\nif __name__ == '__main__':\n    train()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = np.arange(len(train_acc_list))\n\nplt.plot(x, train_acc_list, marker='o', label='train', markevery=2)\nplt.plot(x, validation_acc_list, marker='s', label='validation', markevery=2)\nplt.xlabel(\"epochs\")\nplt.ylabel(\"accuracy\")\nplt.ylim(0, 1.0)\nplt.legend(loc='lower right')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = np.arange(len(train_acc_list))\n\nplt.plot(x, train_loss_list, marker='o', label='train', markevery=2)\nplt.plot(x, validation_loss_list, marker='s', label='validation', markevery=2)\nplt.xlabel(\"epochs\")\nplt.ylabel(\"loss\")\nplt.legend(loc='upper right')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"CUDA_DEVICES = 0\n\none_hot_key = {}\n\ndef test():\n    data_transform = transforms.Compose([\n        transforms.Scale(224),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n    \n    root_dir = \"/kaggle/input/plant-seedlings-classification/train\"\n    i = 0\n    for f in listdir(root_dir):\n        one_hot_key[i] = f\n        i += 1\n    \n    model = torch.load('model-weight_and_bias.pth')\n    model = model.cuda(CUDA_DEVICES)\n    model.eval()\n\n    sample_submission = pd.read_csv('/kaggle/input/plant-seedlings-classification/sample_submission.csv')\n    submission = sample_submission.copy()\n    \n    for i, filename in enumerate(sample_submission['file']):\n        image = Image.open(join('/kaggle/input/plant-seedlings-classification/test', filename)).convert('RGB')\n        image = data_transform(image).unsqueeze(0)\n        inputs = Variable(image.cuda(CUDA_DEVICES))\n        outputs = model(inputs)\n        _, preds = torch.max(outputs.data, 1)\n        \n        print(preds[0].item())\n        submission['species'][i] = one_hot_key[preds[0].item()]\n\n    submission.to_csv('submission.csv', index=False)\n\n\nif __name__ == '__main__':\n    test()\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}