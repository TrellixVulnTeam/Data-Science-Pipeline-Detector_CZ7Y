{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Optimize Model Performance Using TF-TRT and TensorFlow Model Optimization Toolkit**\n\n\n\n\n\n","metadata":{"id":"JCJeXYHH32z9"}},{"cell_type":"markdown","source":"## **Part 1: Loading,Transforming and One Hot Encoding**\n","metadata":{"id":"zedj0tKj38Rn"}},{"cell_type":"code","source":"import os\nimport fnmatch\nimport os\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom keras.applications.vgg16 import preprocess_input\nfrom keras.applications.inception_v3 import preprocess_input\nfrom keras.applications.resnet50 import preprocess_input\nfrom keras.preprocessing import image\nnp.random.seed(21)\n\n\n\nimport tensorflow as tf\nfrom keras.layers import Conv2D,MaxPooling2D,Flatten,Dense,BatchNormalization,Activation,Dropout,GlobalAveragePooling2D\nfrom keras.models import Sequential,Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.optimizers import Adam\n\n\na = '/kaggle/input/plant-seedlings-classification'\n\nprint(os.listdir(os.path.join(a,'train')))","metadata":{"id":"3nX4FkHzur5W","outputId":"6f52d6a1-0f6d-4659-a24a-97ef646a9a41","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"    from sklearn.metrics import accuracy_score\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_train_data(root):\n    \n    \"\"\"Performs required pre processing on the input images and fetches data\n\n    Args:\n      root: Directory in which we are working\n\n    Returns: \n      train_img: A numpy array consisting of train images\n      train_y : A OHE numpy array of train labels\n    \"\"\"\n    train_dir = (os.path.join(root,'train'))\n    train_label = []\n    train_img = []\n    label2num = {'Loose Silky-bent':0, 'Charlock':1, 'Sugar beet':2, 'Small-flowered Cranesbill':3,\n                 'Common Chickweed':4, 'Common wheat':5, 'Maize':6, 'Cleavers':7, 'Scentless Mayweed':8,\n                 'Fat Hen':9, 'Black-grass':10, 'Shepherds Purse':11}\n    for i in os.listdir(train_dir):\n        label_number = label2num[i]\n        new_path = os.path.join(train_dir,i)\n        for j in fnmatch.filter(os.listdir(new_path), '*.png'):\n            temp_img = image.load_img(os.path.join(new_path,j), target_size=(128,128))\n            train_label.append(label_number)\n            temp_img = image.img_to_array(temp_img)\n            train_img.append(temp_img)\n        print(i)\n    train_img = np.array(train_img)\n\n    train_y=pd.get_dummies(train_label)\n    train_y = np.array(train_y)\n    train_img=preprocess_input(train_img)\n    \n    return train_img,train_y\n\n","metadata":{"id":"Ec4CE8p7ur5e","outputId":"093d9058-bcf1-4143-d596-92424819f3e8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_img,train_y = get_train_data(a)\nprint('Training data shape: ', train_img.shape)\nprint('Training labels shape: ', train_y.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_valid, Y_train, Y_valid=train_test_split(train_img,train_y,test_size=0.1, random_state=42)","metadata":{"id":"ES-0H8qO6Gez","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Part 2: Data Augmentation**\n","metadata":{"id":"WoGyR5vZ4WPY"}},{"cell_type":"code","source":"from keras.preprocessing.image import ImageDataGenerator\n\ndatagen = ImageDataGenerator(\n        rotation_range=30,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.1, # Randomly zoom image \n        width_shift_range=0.2,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.2,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=True,  # randomly flip images\n        vertical_flip=True)  # randomly flip images\n\ndatagen.fit(X_train)\n","metadata":{"id":"qOEtFGPGur5n","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Part 3- Transfer Learing**","metadata":{"id":"mdbs0dS-5UdZ"}},{"cell_type":"markdown","source":"### **1) VGG-16**","metadata":{"id":"yzqgD_qR5Z1O"}},{"cell_type":"markdown","source":"#### **Customizing VGG-16 for our problem statement**","metadata":{"id":"HZYZHXsKxWMZ"}},{"cell_type":"code","source":"def vgg16_model(num_classes=None):\n    \n    \"\"\" Adding custom model to the VGG-16\n\n    Args:\n      num_classes: Number of layers in the final layer(Number of classes)\n\n    Returns:\n      model: Returns the custom model added to VGG\n    \"\"\"\n\n    model = VGG16(weights='imagenet', include_top=False,input_shape=(128,128,3))\n    model.layers.pop()\n    model.layers.pop()\n    model.layers.pop()\n\n    model.outputs = [model.layers[-1].output]\n\n    #model.layers[-2].outbound_node= []\n    x=Conv2D(256, kernel_size=(2,2),strides=2)(model.output)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)    \n    x=Conv2D(128, kernel_size=(2,2),strides=1)(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    x=Flatten()(x)\n    x=Dense(num_classes, activation='softmax')(x)\n\n    model=Model(model.input,x)\n\n    for layer in model.layers[:15]:\n\n        layer.trainable = False\n\n\n    return model","metadata":{"id":"Mlm_XoCaMPad","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.applications.vgg16 import VGG16\nfrom keras import backend as K\nnum_classes=12\nmodel = vgg16_model(num_classes)\nmodel.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=['accuracy'])\nmodel.summary()","metadata":{"id":"02gjBdA9i9H7","outputId":"eb6835f8-b807-4080-9635-d4adf9d5b400","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **Training VGG-16**","metadata":{"id":"azFmM8N0xOZ3"}},{"cell_type":"code","source":"from keras.callbacks import ModelCheckpoint\nepochs = 10\nbatch_size = 32\nmodel_checkpoint = ModelCheckpoint('vgg_weights.h5', monitor='val_accuracy', save_best_only=True,mode='max',verbose=1)\nreduce_lr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.5, patience=7, min_lr=0.000001)\n#early_stop = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=7, verbose=0, mode='min', restore_best_weights=True)\n\nmodel.fit(datagen.flow(X_train,Y_train),\n          batch_size=128,\n          epochs=20,\n          verbose=1, shuffle=True, validation_data=(X_valid,Y_valid), callbacks=[model_checkpoint,reduce_lr])","metadata":{"id":"gnqrJ7vejYq4","outputId":"716a5d98-3ca1-4842-c48b-22319c19c2a1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Model Optimization Using TF-Lite(Post Training Dynamic range quantization) ","metadata":{}},{"cell_type":"code","source":"import pathlib\nimport time","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tflite_models_dir = pathlib.Path(os.path.join(os.getcwd(),'tflite_models'))\ntflite_models_dir.mkdir(exist_ok=True, parents=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"converter_vgg = tf.lite.TFLiteConverter.from_keras_model(model)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert to TF Lite without quantization\nvgg16_tflite_file = tflite_models_dir/\"vgg16.tflite\"\nvgg16_tflite_file.write_bytes(converter_vgg.convert())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"interpreter = tf.lite.Interpreter(model_path='./tflite_models/vgg16.tflite')\ninput_type = interpreter.get_input_details()[0]['dtype']\nprint('input: ', input_type)\noutput_type = interpreter.get_output_details()[0]['dtype']\nprint('output: ', output_type)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def run_tflite_model(tflite_file, test_image_indices):\n    global X_valid\n\n  # Initialize the interpreter\n    interpreter = tf.lite.Interpreter(model_path=str(tflite_file))\n    interpreter.allocate_tensors()\n\n    input_details = interpreter.get_input_details()[0]\n    output_details = interpreter.get_output_details()[0]\n\n    predictions = np.zeros((len(test_image_indices),), dtype=int)\n    for i, test_image_index in enumerate(test_image_indices):\n        test_image = X_valid[test_image_index]\n        test_label = Y_valid[test_image_index]\n\n        test_image = np.expand_dims(test_image, axis=0).astype(input_details[\"dtype\"])\n        interpreter.set_tensor(input_details[\"index\"], test_image)\n        interpreter.invoke()\n        output = interpreter.get_tensor(output_details[\"index\"])[0]\n\n        predictions[i] = output.argmax()\n\n    return predictions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate_model(tflite_file, model_type):\n    global X_valid\n    global Y_valid\n\n    test_image_indices = range(X_valid.shape[0])\n    predictions = run_tflite_model(tflite_file, test_image_indices)\n    \n    predictions=pd.get_dummies(predictions)\n    predictions = np.array(predictions)\n    #print(predictions)\n    \n    accuracy = accuracy_score(y_true=Y_valid, y_pred=predictions) \n\n    #accuracy = (np.sum(Y_valid== predictions) * 100) / len(X_valid)\n\n    return accuracy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"op = evaluate_model(vgg16_tflite_file, model_type=\"Float\")\nprint(\"Accuracy of TFLite - VGG : {}\".format(op))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Optimization of VGG Model with TensorRT","metadata":{}},{"cell_type":"code","source":"from tensorflow.python.compiler.tensorrt import trt_convert as trt\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save('vgg_saved_model')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### TF-TRT FP32 model","metadata":{}},{"cell_type":"code","source":"print('Converting to TF-TRT FP32...')\nconversion_params = trt.DEFAULT_TRT_CONVERSION_PARAMS._replace(precision_mode=trt.TrtPrecisionMode.FP32,\n                                                               max_workspace_size_bytes=8000000000)\n\nconverter = trt.TrtGraphConverterV2(input_saved_model_dir='vgg_saved_model',\n                                    conversion_params=conversion_params)\nconverter.convert()\nconverter.save(output_saved_model_dir='vgg_saved_model_TFTRT_FP32')\nprint('Done Converting to TF-TRT FP32')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 8\nbatched_input = np.zeros((batch_size, 128, 128, 3), dtype=np.float32)\n\nfor i in range(batch_size):\n    img_path = '../input/plant-seedlings-classification/train/Common Chickweed'\n    for j in os.listdir(img_path):\n        img = image.load_img(os.path.join(img_path,j), target_size=(128, 128))\n        x = image.img_to_array(img)\n        x = np.expand_dims(x, axis=0)\n        x = preprocess_input(x)\n        batched_input[i, :] = x\nbatched_input = tf.constant(batched_input)\nprint('batched_input shape: ', batched_input.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.python.saved_model import tag_constants\n\ndef benchmark_tftrt(input_saved_model):\n    saved_model_loaded = tf.saved_model.load(input_saved_model, tags=[tag_constants.SERVING])\n    infer = saved_model_loaded.signatures['serving_default']\n\n    N_warmup_run = 50\n    N_run = 500\n    elapsed_time = []\n\n    for i in range(N_warmup_run):\n        labeling = infer(batched_input)\n\n    for i in range(N_run):\n        start_time = time.time()\n        labeling = infer(batched_input)\n        #prob = labeling['probs'].numpy()\n        end_time = time.time()\n        elapsed_time = np.append(elapsed_time, end_time - start_time)\n        if i % 50 == 0:\n            print('Step {}: {:4.1f}ms'.format(i, (elapsed_time[-50:].mean()) * 1000))\n\n    print('Throughput: {:.0f} images/s'.format(N_run * batch_size / elapsed_time.sum()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"benchmark_tftrt('vgg_saved_model_TFTRT_FP32')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### TF-TRT FP16 model","metadata":{}},{"cell_type":"code","source":"print('Converting to TF-TRT FP16...')\nconversion_params = trt.DEFAULT_TRT_CONVERSION_PARAMS._replace(\n    precision_mode=trt.TrtPrecisionMode.FP16,\n    max_workspace_size_bytes=8000000000)\nconverter = trt.TrtGraphConverterV2(\n   input_saved_model_dir='vgg_saved_model', conversion_params=conversion_params)\nconverter.convert()\nconverter.save(output_saved_model_dir='vgg_saved_model_TFTRT_FP16')\nprint('Done Converting to TF-TRT FP16')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"benchmark_tftrt('vgg_saved_model_TFTRT_FP16')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### TF-TRT INT8 model (with calibration)","metadata":{}},{"cell_type":"code","source":"print('Converting to TF-TRT INT8...')\nconversion_params = trt.DEFAULT_TRT_CONVERSION_PARAMS._replace(\n    precision_mode=trt.TrtPrecisionMode.INT8, \n    max_workspace_size_bytes=8000000000, \n    use_calibration=True)\nconverter = trt.TrtGraphConverterV2(\n    input_saved_model_dir='vgg_saved_model', \n    conversion_params=conversion_params)\n\ndef calibration_input_fn():\n    yield (batched_input, )\nconverter.convert(calibration_input_fn=calibration_input_fn)\n\nconverter.save(output_saved_model_dir='vgg_saved_model_TFTRT_INT8')\nprint('Done Converting to TF-TRT INT8')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"benchmark_tftrt('vgg_saved_model_TFTRT_INT8')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **2) ResNet50**","metadata":{"id":"gj-z1rfE04OS"}},{"cell_type":"markdown","source":"#### **Customizing ResNet50 for our problem statement**","metadata":{"id":"eRu8qanN0-uG"}},{"cell_type":"code","source":"def resnet_model(num_classes=None):\n    \n    \"\"\" Adding custom model to the ResNet50\n\n    Args:\n      num_classes: Number of layers in the final layer(Number of classes)\n\n    Returns:\n      model: Returns the custom model added to ResNet\n    \"\"\"\n\n    model = ResNet50(weights='imagenet', include_top=False,input_shape=(128,128,3))\n    model.layers.pop()\n    #model.layers.pop()\n    #model.layers.pop()\n\n    model.outputs = [model.layers[-1].output]\n\n    #model.layers[-2].outbound_node= []\n    x=Conv2D(256, kernel_size=(2,2),strides=2)(model.output)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)    \n    x=Conv2D(128, kernel_size=(2,2),strides=1)(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    x=Flatten()(x)\n    x=Dense(num_classes, activation='softmax')(x)\n\n    model=Model(model.input,x)\n\n    for layer in model.layers[:15]:\n\n        layer.trainable = False\n\n\n    return model","metadata":{"id":"ZCCs5p1g1hQk","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.applications.resnet50 import ResNet50\nfrom keras import backend as K\nnum_classes=12\nmodel_res = resnet_model(num_classes)\nmodel_res.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=['accuracy'])\nmodel_res.summary()","metadata":{"id":"tHLHJob04XwU","outputId":"753789b0-d4ec-4766-ef84-d585d939a4a1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **Training ResNet50**","metadata":{"id":"XDRnWf1w4tJn"}},{"cell_type":"code","source":"epochs = 10\nbatch_size = 32\nmodel_checkpoint = ModelCheckpoint('resnet_weights.h5', monitor='val_accuracy', save_best_only=True,mode='max',verbose=1)\nreduce_lr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.5, patience=7, min_lr=0.000001)\n#early_stop = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=7, verbose=0, mode='min', restore_best_weights=True)\n\nmodel_res.fit(datagen.flow(X_train,Y_train),\n          batch_size=128,\n          epochs=20,\n          verbose=1, shuffle=True, validation_data=(X_valid,Y_valid), callbacks=[model_checkpoint,reduce_lr])","metadata":{"id":"ZR4N1zH63wpd","outputId":"672d1bcd-19a6-4877-e7b9-15960d18c181","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Model Optimization Using TF-Lite(Post Training Dynamic range quantization) ","metadata":{}},{"cell_type":"code","source":"converter_resnet = tf.lite.TFLiteConverter.from_keras_model(model_res)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert to TF Lite without quantization\nresnet_tflite_file = tflite_models_dir/\"resnet.tflite\"\nresnet_tflite_file.write_bytes(converter_resnet.convert())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"interpreter = tf.lite.Interpreter(model_path='./tflite_models/resnet.tflite')\ninput_type = interpreter.get_input_details()[0]['dtype']\nprint('input: ', input_type)\noutput_type = interpreter.get_output_details()[0]['dtype']\nprint('output: ', output_type)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"op = evaluate_model(resnet_tflite_file, model_type=\"Float\")\nprint(\"Accuracy of TFLite - Resnet50 : {}\".format(op))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Optimization of Resnet Model with TensorRT","metadata":{}},{"cell_type":"markdown","source":"#### TF-TRT FP32 model","metadata":{}},{"cell_type":"code","source":"model_res.save('res_saved_model')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Converting to TF-TRT FP32...')\nconversion_params = trt.DEFAULT_TRT_CONVERSION_PARAMS._replace(precision_mode=trt.TrtPrecisionMode.FP32,\n                                                               max_workspace_size_bytes=8000000000)\n\nconverter = trt.TrtGraphConverterV2(input_saved_model_dir='res_saved_model',\n                                    conversion_params=conversion_params)\nconverter.convert()\nconverter.save(output_saved_model_dir='resnet_saved_model_TFTRT_FP32')\nprint('Done Converting to TF-TRT FP32')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"benchmark_tftrt('resnet_saved_model_TFTRT_FP32')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### TF-TRT FP16 model","metadata":{}},{"cell_type":"code","source":"print('Converting to TF-TRT FP16...')\nconversion_params = trt.DEFAULT_TRT_CONVERSION_PARAMS._replace(\n    precision_mode=trt.TrtPrecisionMode.FP16,\n    max_workspace_size_bytes=8000000000)\nconverter = trt.TrtGraphConverterV2(\n   input_saved_model_dir='res_saved_model', conversion_params=conversion_params)\nconverter.convert()\nconverter.save(output_saved_model_dir='res_saved_model_TFTRT_FP16')\nprint('Done Converting to TF-TRT FP16')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"benchmark_tftrt('res_saved_model_TFTRT_FP16')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### TF-TRT INT8 model (with calibration)","metadata":{}},{"cell_type":"code","source":"print('Converting to TF-TRT INT8...')\nconversion_params = trt.DEFAULT_TRT_CONVERSION_PARAMS._replace(\n    precision_mode=trt.TrtPrecisionMode.INT8, \n    max_workspace_size_bytes=8000000000, \n    use_calibration=True)\nconverter = trt.TrtGraphConverterV2(\n    input_saved_model_dir='res_saved_model', \n    conversion_params=conversion_params)\n\ndef calibration_input_fn():\n    yield (batched_input, )\nconverter.convert(calibration_input_fn=calibration_input_fn)\n\nconverter.save(output_saved_model_dir='resnet_saved_model_TFTRT_INT8')\nprint('Done Converting to TF-TRT INT8')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"benchmark_tftrt('resnet_saved_model_TFTRT_INT8')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **3) InceptionV3**","metadata":{"id":"WIlv7ZEt7ukH"}},{"cell_type":"markdown","source":"#### **Customizing InceptionV3 for our problem statement**","metadata":{"id":"HZieesu370fd"}},{"cell_type":"code","source":"def incep_model(num_classes=None):\n    \n    \"\"\" Adding custom model to the InceptionV3\n\n    Args:\n      num_classes: Number of layers in the final layer(Number of classes)\n\n    Returns:\n      model: Returns the custom model added to Inception\n    \"\"\"\n\n    model = InceptionV3(weights='imagenet', include_top=False,input_shape=(128,128,3))\n    \n    x = model.output\n    x = GlobalAveragePooling2D()(x)\n    x = Dense(1024)(x)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n    x = Dropout(0.3)(x)\n    x = Dense(512)(x)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n    x = Dropout(0.3)(x)\n\n    predictions = Dense(12, activation='softmax')(x)\n\n    model = Model(model.input, predictions)\n\n\n\n\n    return model","metadata":{"id":"wPpu8lxL7nRl","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.applications.inception_v3 import InceptionV3\nfrom keras import backend as K\nnum_classes=12\nmodel_incep = incep_model(num_classes)\nmodel_incep.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=['accuracy'])\nmodel_incep.summary()","metadata":{"id":"aRTxpla28LQa","outputId":"b1bf866d-ff48-4b70-fdf9-ce962e809178","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **Training InceptionV3**","metadata":{"id":"tI03DIzED5MJ"}},{"cell_type":"code","source":"epochs = 10\nbatch_size = 32\nmodel_checkpoint = ModelCheckpoint('incep_weights.h5', monitor='val_accuracy', save_best_only=True,mode='max',verbose=1)\nreduce_lr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.5, patience=7, min_lr=0.000001)\n#early_stop = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=7, verbose=0, mode='min', restore_best_weights=True)\n\nmodel_incep.fit(datagen.flow(X_train,Y_train),\n          batch_size=128,\n          epochs=20,\n          verbose=1, shuffle=True, validation_data=(X_valid,Y_valid), callbacks=[model_checkpoint])","metadata":{"id":"e0Bgv-KH9JUA","outputId":"fd7b2ab2-ea6d-4aac-af5f-a7fe2d5328d4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Model Optimization Using TF-Lite(Post Training Dynamic range quantization) ","metadata":{}},{"cell_type":"code","source":"converter_incep = tf.lite.TFLiteConverter.from_keras_model(model_incep)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert to TF Lite without quantization\nincep_tflite_file = tflite_models_dir/\"incep.tflite\"\nincep_tflite_file.write_bytes(converter_incep.convert())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"interpreter = tf.lite.Interpreter(model_path='./tflite_models/incep.tflite')\ninput_type = interpreter.get_input_details()[0]['dtype']\nprint('input: ', input_type)\noutput_type = interpreter.get_output_details()[0]['dtype']\nprint('output: ', output_type)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"op = evaluate_model(incep_tflite_file, model_type=\"Float\")\nprint(\"Accuracy of TFLite - InceptionV3 : {}\".format(op))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Optimization of InceptionV3 Model with TensorRT","metadata":{}},{"cell_type":"markdown","source":"#### TF-TRT FP32 model","metadata":{}},{"cell_type":"code","source":"model_incep.save('incep_saved_model')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Converting to TF-TRT FP32...')\nconversion_params = trt.DEFAULT_TRT_CONVERSION_PARAMS._replace(precision_mode=trt.TrtPrecisionMode.FP32,\n                                                               max_workspace_size_bytes=8000000000)\n\nconverter = trt.TrtGraphConverterV2(input_saved_model_dir='incep_saved_model',\n                                    conversion_params=conversion_params)\nconverter.convert()\nconverter.save(output_saved_model_dir='incep_saved_model_TFTRT_FP32')\nprint('Done Converting to TF-TRT FP32')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"benchmark_tftrt('incep_saved_model_TFTRT_FP32')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### TF-TRT FP16 model","metadata":{}},{"cell_type":"code","source":"print('Converting to TF-TRT FP16...')\nconversion_params = trt.DEFAULT_TRT_CONVERSION_PARAMS._replace(\n    precision_mode=trt.TrtPrecisionMode.FP16,\n    max_workspace_size_bytes=8000000000)\nconverter = trt.TrtGraphConverterV2(\n   input_saved_model_dir='incep_saved_model', conversion_params=conversion_params)\nconverter.convert()\nconverter.save(output_saved_model_dir='incep_saved_model_TFTRT_FP16')\nprint('Done Converting to TF-TRT FP16')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"benchmark_tftrt('incep_saved_model_TFTRT_FP16')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### TF-TRT INT8 model (with calibration)","metadata":{}},{"cell_type":"code","source":"print('Converting to TF-TRT INT8...')\nconversion_params = trt.DEFAULT_TRT_CONVERSION_PARAMS._replace(\n    precision_mode=trt.TrtPrecisionMode.INT8, \n    max_workspace_size_bytes=8000000000, \n    use_calibration=True)\nconverter = trt.TrtGraphConverterV2(\n    input_saved_model_dir='incep_saved_model', \n    conversion_params=conversion_params)\n\ndef calibration_input_fn():\n    yield (batched_input, )\nconverter.convert(calibration_input_fn=calibration_input_fn)\n\nconverter.save(output_saved_model_dir='incep_saved_model_TFTRT_INT8')\nprint('Done Converting to TF-TRT INT8')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"benchmark_tftrt('incep_saved_model_TFTRT_INT8')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}