{"cells":[{"metadata":{"_uuid":"e66768ecbc9edd1ae666c88f4e5de442d4228b8e","_cell_guid":"9bfce78a-de6d-453c-91bd-da15b3f0e527"},"cell_type":"markdown","source":""},{"metadata":{"_uuid":"56d3ea1a8bb1ee742ea817ee82b64d744a84cb57","_cell_guid":"e83a8430-f7d6-49e6-b55a-f605805c6fde"},"cell_type":"markdown","source":"# Plant seedling recognition experimentation (Edited by Jordan Micah Bennett)\n\nClassification process will consist of next steps:\n1. [Get data](#section1) — reading of images and labels\n2. [Cleaning data](#section2) — removing of image background, input normalization and label preparatin\n3. [Model](#section3) — creating training and validation sets, creating and fitting model\n4. [Evaluate model](#section4) — evaluation of model, make prediction.\n\n**(Note by [JordanMicahBennett](http://folioverse.appspot.com): Model achieves 96.210526% accuracy on evaluation of test data. Code for test is \"model_loss, model_accuracy = model.evaluate ( testX, testY, verbose=0))**\n"},{"metadata":{"_uuid":"6dbfc7e2b3ed403bcab1d16248bb7dea71d61c9d"},"cell_type":"markdown","source":""},{"metadata":{"_cell_guid":"ba57c7b2-6a8f-46c3-b88a-39c8badb6e61","_uuid":"07e1cb3d84872e645e865fe1d38dcbe5bec6bca4","colab_type":"text","id":"h3QHZ_qeYzSP"},"cell_type":"markdown","source":"<a id='section1'></a>\n## 1. Get data\n\nObtaining images and resizing to 70 x 70 px. We use a 70 x 70 px size for a quicker training of model. Also here we get image labels from folder name"},{"metadata":{"_uuid":"3d8330361f41b90be1556a5333ae562fe3103ee9","colab":{"output_extras":[{"item_id":1},{"item_id":46}],"autoexec":{"wait_interval":0,"startup":false},"height":799,"base_uri":"https://localhost:8080/"},"id":"dkuI17C4YzSP","colab_type":"code","outputId":"7e0bf30d-e796-4743-dacd-d6100bb4b932","_cell_guid":"cf48a187-a9b9-4767-9995-530f4042f2ef","trusted":true},"cell_type":"code","source":"import cv2\nfrom glob import glob\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport math\nimport pandas as pd\n\nScaleTo = 70  # px to scale\nseed = 7  # fixing random\n\npath = '../input/plant-seedlings-classification/train/*/*.png' \nfiles = glob(path)\n\ntrainImg = []\ntrainLabel = []\nj = 1\nnum = len(files)\n\n# Obtain images and resizing, obtain labels\nfor img in files:\n    print(str(j) + \"/\" + str(num), end=\"\\r\")\n    trainImg.append(cv2.resize(cv2.imread(img), (ScaleTo, ScaleTo)))  # Get image (with resizing)\n    trainLabel.append(img.split('/')[-2])  # Get image label (folder name)\n    j += 1\n\ntrainImg = np.asarray(trainImg)  # Train images set\ntrainLabel = pd.DataFrame(trainLabel)  # Train labels set","execution_count":9,"outputs":[]},{"metadata":{"_uuid":"e5a77340c42fa54e4ab7bc2174d6fccae8777a9d","_cell_guid":"da1b2625-138a-4ca7-97fc-7ac74d883b4d"},"cell_type":"markdown","source":"Let's look at some examples of plant photos"},{"metadata":{"_uuid":"41f4551cf7c83b58d1606c80e9fe1fc787eb7757","_cell_guid":"ac427863-563e-47c1-9741-462e4da7a774","trusted":false,"collapsed":true},"cell_type":"code","source":"# Show some example images\nfor i in range(8):\n    plt.subplot(2, 4, i + 1)\n    plt.imshow(trainImg[i])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"84bc37d6c27d3efaf93383d800b4272b7f71e18e","_cell_guid":"f36467af-e389-4064-b353-66aecb06d5ec"},"cell_type":"markdown","source":"As we can see, every photo of plant seedling has an background, so let's try to remove it. It'll help us to goal better model accuracy"},{"metadata":{"_cell_guid":"3fc90485-e2b4-456c-9958-ed1253db5a79","_uuid":"0d7d83e4b71d5e4ef8a0b565afb369f4fa772120","colab_type":"text","id":"gnLUeAThYzSU"},"cell_type":"markdown","source":"<a id='section2'></a>\n## 2. Cleaning data\n\nFor removing the background, we'll use the fact, that all plants on our photos are green. We can create a mask, which will leave some range of green color and remove other part of image.\n\n### 2.1. Masking green plant \nFor creating mask, which will remove background, we need to convert RGB image to HSV. HSV is alternative of the RGB color model. In HSV, it is easier to represent a color range than in RGB color space.\n\n![HSV color model](https://codewords.recurse.com/images/six/image-processing-101/hsv.png)\n\nBesides of this, we'll blur image firstly for removing noise. After creating HSV image, we'll create mask based on empirically selected range of green color, convert it to boolean mask and apply it to the origin image.\n\n* Use gaussian blur for remove noise\n* Convert color to HSV\n* Create mask\n* Create boolean mask\n* Apply boolean mask and getting image whithout background"},{"metadata":{"_uuid":"c861144e23c2102f47885c83e4b4fde7a99cd043","colab":{"output_extras":[{"item_id":1}],"autoexec":{"wait_interval":0,"startup":false},"height":345,"base_uri":"https://localhost:8080/"},"colab_type":"code","id":"UlXr1HtLYzSV","outputId":"7a79a1d0-bf3c-4f61-cd17-48b53412d78c","executionInfo":{"status":"ok","elapsed":5900,"user_tz":-180,"timestamp":1517995337230,"user":{"displayName":"Никита Константиновский","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"118348132992166048978"}},"_cell_guid":"8e21f534-f0f7-4bb6-85c3-9a9c8be3cd5e","trusted":true},"cell_type":"code","source":"clearTrainImg = []\nexamples = []; getEx = True\nfor img in trainImg:\n    # Use gaussian blur\n    blurImg = cv2.GaussianBlur(img, (5, 5), 0)   \n    \n    # Convert to HSV image\n    hsvImg = cv2.cvtColor(blurImg, cv2.COLOR_BGR2HSV)  \n    \n    # Create mask (parameters - green color range)\n    lower_green = (25, 40, 50)\n    upper_green = (75, 255, 255)\n    mask = cv2.inRange(hsvImg, lower_green, upper_green)  \n    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (11, 11))\n    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n    \n    # Create bool mask\n    bMask = mask > 0  \n    \n    # Apply the mask\n    clear = np.zeros_like(img, np.uint8)  # Create empty image\n    clear[bMask] = img[bMask]  # Apply boolean mask to the origin image\n    \n    clearTrainImg.append(clear)  # Append image without backgroung\n    \n    # Show examples\n    if getEx:\n        plt.subplot(2, 3, 1); plt.imshow(img)  # Show the original image\n        plt.subplot(2, 3, 2); plt.imshow(blurImg)  # Blur image\n        plt.subplot(2, 3, 3); plt.imshow(hsvImg)  # HSV image\n        plt.subplot(2, 3, 4); plt.imshow(mask)  # Mask\n        plt.subplot(2, 3, 5); plt.imshow(bMask)  # Boolean mask\n        plt.subplot(2, 3, 6); plt.imshow(clear)  # Image without background\n        getEx = False\n\nclearTrainImg = np.asarray(clearTrainImg)","execution_count":12,"outputs":[]},{"metadata":{"_uuid":"c7a2b717ac1ece011e03a9da2f07c00635c07987","_cell_guid":"fd15895c-0609-490d-8646-48d17818bc84"},"cell_type":"markdown","source":"Good result! Let's look at other examples of masked images "},{"metadata":{"_uuid":"18ab5c64f074fb5ee8ce4dceeea467fae116ba0d","colab":{"output_extras":[{"item_id":1}],"autoexec":{"wait_interval":0,"startup":false},"height":350,"base_uri":"https://localhost:8080/"},"colab_type":"code","id":"W5gQorjqYzSe","collapsed":true,"outputId":"67b03f4b-c00a-4ca1-dfe3-07a853b16f99","executionInfo":{"status":"ok","elapsed":1703,"user_tz":-180,"timestamp":1517995342379,"user":{"displayName":"Никита Константиновский","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"118348132992166048978"}},"_cell_guid":"a3e52696-bac1-40ff-b8d7-edf8a0bf40ce","trusted":false},"cell_type":"code","source":"# Show sample result\nfor i in range(8):\n    plt.subplot(2, 4, i + 1)\n    plt.imshow(clearTrainImg[i])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"834fbde96ade12ac339d30cf4b2e7bb1ed98089b","_cell_guid":"56758b6c-ad98-4aba-8a20-1f320b457c86"},"cell_type":"markdown","source":"As we can see, we removed most of the background."},{"metadata":{"_cell_guid":"7cc27f81-9c2c-459c-8b35-ef5b9be2d181","_uuid":"6489aa4c23ae6c6e07c07e7555d09242cf7413db","colab_type":"text","id":"qyoKl0p_YzSi"},"cell_type":"markdown","source":"### 2.2. Normalize input\n\nNow set the values of input from [0...255] to [0...1] (RGB color-space encode colors with numbers [0...255]). CNN will be faster train if we use [0...1] input"},{"metadata":{"_uuid":"e831049e1526c3732ce81a4a8b9521e48ad2ac2b","colab":{"autoexec":{"wait_interval":0,"startup":false}},"id":"9P7UIelKYzSi","colab_type":"code","collapsed":true,"_cell_guid":"b8010cb8-e9c8-4f88-a660-071c8b91b8ba","trusted":true},"cell_type":"code","source":"clearTrainImg = clearTrainImg / 255\n\n#Note By JordanMicahBennett - The use of normalization here is questionable, because:\n#1.) Model evaluation time without norm (0,1) = TrainData 156 seconds, TestData 19 seconds\n#2.) Model evaluation time with norm(0,1) = TrainData 206 seconds, TestData 27 seconds\n","execution_count":11,"outputs":[]},{"metadata":{"_cell_guid":"a184fb96-476c-42e2-9012-b0552922efa7","_uuid":"00334d8001013329d0ea54ec23206fbcda0c7276","colab_type":"text","id":"PjtJmDJ6YzSl"},"cell_type":"markdown","source":"### 2.3. Categories labels\n\nNow we encode image labels. Labels are 12 string names, so we could create classes array with this names, for example ['Black-grass' 'Charlock' 'Cleavers' 'Common Chickweed' 'Common wheat' 'Fat Hen' 'Loose Silky-bent' 'Maize' 'Scentless Mayweed' 'Shepherds Purse' 'Small-flowered Cranesbill' 'Sugar beet'] and encode every label by position in this array. For example 'Charlock' -> [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0].\n\nWe need to encode all labels in this way."},{"metadata":{"_uuid":"61f289328cdf2ffd1bd1c66f91e3b8e8514d069d","colab":{"output_extras":[{"item_id":1},{"item_id":2},{"item_id":3}],"autoexec":{"wait_interval":0,"startup":false},"height":535,"base_uri":"https://localhost:8080/"},"colab_type":"code","id":"jHz3WJhrYzSm","outputId":"f5f5d244-fa92-4571-d4d4-0c8e202329d6","executionInfo":{"status":"ok","elapsed":1199,"user_tz":-180,"timestamp":1517995349525,"user":{"displayName":"Никита Константиновский","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"118348132992166048978"}},"_cell_guid":"1e17fd46-a7b4-4470-b28b-215a39a206e0","trusted":true},"cell_type":"code","source":"from keras.utils import np_utils\nfrom sklearn import preprocessing\nimport matplotlib.pyplot as plt\n\n# Encode labels and create classes\nle = preprocessing.LabelEncoder()\nle.fit(trainLabel[0])\nprint(\"Classes: \" + str(le.classes_))\nencodeTrainLabels = le.transform(trainLabel[0])\n\n# Make labels categorical\nclearTrainLabel = np_utils.to_categorical(encodeTrainLabels)\nnum_clases = clearTrainLabel.shape[1]\nprint(\"Number of classes: \" + str(num_clases))\n\n# Plot of label types numbers\ntrainLabel[0].value_counts().plot(kind='bar')","execution_count":13,"outputs":[]},{"metadata":{"_uuid":"49e51b6b50ea58102bfa26effb2f733638b9b3e4","_cell_guid":"f01b8fd7-cd51-4754-bb74-6101c0920208"},"cell_type":"markdown","source":"As we can see, we have different counts of images of different species. So, data is unbalanced"},{"metadata":{"_cell_guid":"df1f4d6c-726b-4c3e-8f8c-d344f942ff9d","_uuid":"01a4d08cb1fede2162d58589bd4d2ce181bea275","colab_type":"text","id":"PBsNYbfNYzSq"},"cell_type":"markdown","source":"<a id='section3'></a>\n## 3. Model\n\n### 3.1. Split dataset\nSplit data on training and validation set. 10% of data became the validation set\n\nOur data is unbalanced, so for avoide inaccurate evaluation of model set stratify=clearTrainLabel"},{"metadata":{"_uuid":"a92bd39afd9769d7ea9e793ee85a505f3c19fdd5","colab":{"autoexec":{"wait_interval":0,"startup":false}},"id":"EJy83dSEYzSr","colab_type":"code","_cell_guid":"6b14bc73-6e8d-4ae9-98fb-37b692b75103","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrainX, testX, trainY, testY = train_test_split(clearTrainImg, clearTrainLabel, \n                                                test_size=0.1, random_state=seed, \n                                                stratify = clearTrainLabel)","execution_count":14,"outputs":[]},{"metadata":{"_cell_guid":"09d0ad71-0bb2-4b6b-b015-8be9600ddabd","_uuid":"8bb49cf39cc87436d34d6d831d55c53ac10d682e","colab_type":"text","id":"nvjJmQ9nYzSu"},"cell_type":"markdown","source":"### 3.2. Data generator\nTo avoide overfitting we need to create image generator which will randomly rotate, zoom, shift and flip image during the fitting of the model.\n\n* Set random rotation from 0 to 180 degrees\n* Set random zoom at 0.1\n* Set random shifting at 0.1\n* Set horisontal and vertical flips"},{"metadata":{"_uuid":"53d859526c2edc7b0063f07592b0c45b05cf1695","colab":{"autoexec":{"wait_interval":0,"startup":false}},"id":"ftwKbrJ5YzSv","colab_type":"code","collapsed":true,"_cell_guid":"6ed3e088-f882-4536-b889-1007979830c9","trusted":false},"cell_type":"code","source":"from keras.preprocessing.image import ImageDataGenerator\ndatagen = ImageDataGenerator(\n        rotation_range=180,  # randomly rotate images in the range\n        zoom_range = 0.1, # Randomly zoom image \n        width_shift_range=0.1,  # randomly shift images horizontally\n        height_shift_range=0.1,  # randomly shift images vertically \n        horizontal_flip=True,  # randomly flip images horizontally\n        vertical_flip=True  # randomly flip images vertically\n    )  \ndatagen.fit(trainX)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"bc368c34-d8a7-4578-b1a9-561534fe4a04","_uuid":"9e98b9806e9d0ec27afaa768eb40406ec7acdd8f","colab_type":"text","id":"qq1DW3AQYzS0"},"cell_type":"markdown","source":"### 3.3. Create model\n\nFor creation model i used Keras Sequential.\n\nI created model with six convolutional layers and three fully-connected layers in the end. First two convolutional layers have 64 filters, next 128 filters and the last two layers have 256 filters. After each pair of convolution layers model have max pooling layer. Also, to reduce overfitting after each pair of convolution layers we use dropout layer (10% between convolutional layers and 50% between fully connect layers) and between each layer we use batch normalization layer. \n\nIn the end i used three fully-connected layers for classifing. In the last layer the neural net outputs destribution of probability for each of 12 classes. "},{"metadata":{"_uuid":"f078274c7eeb7935aaed1fad4d45b9f1a93a0a64","colab":{"output_extras":[{"item_id":1}],"autoexec":{"wait_interval":0,"startup":false},"height":782,"base_uri":"https://localhost:8080/"},"colab_type":"code","id":"iIM5G4UrYzS3","outputId":"69acde5e-2805-4368-dc45-0f4155eebee2","executionInfo":{"status":"ok","elapsed":1279,"user_tz":-180,"timestamp":1517995360543,"user":{"displayName":"Никита Константиновский","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"118348132992166048978"}},"_cell_guid":"d3fdb265-6af1-420f-a5af-6d507d7814d9","trusted":true},"cell_type":"code","source":"import numpy\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom keras.layers import Flatten\nfrom keras.layers.convolutional import Conv2D\nfrom keras.layers.convolutional import MaxPooling2D\nfrom keras.layers import BatchNormalization\n\nnumpy.random.seed(seed)  # Fix seed\n\nmodel = Sequential()\n\nmodel.add(Conv2D(filters=64, kernel_size=(5, 5), input_shape=(ScaleTo, ScaleTo, 3), activation='relu'))\nmodel.add(BatchNormalization(axis=3))\nmodel.add(Conv2D(filters=64, kernel_size=(5, 5), activation='relu'))\nmodel.add(MaxPooling2D((2, 2)))\nmodel.add(BatchNormalization(axis=3))\nmodel.add(Dropout(0.1))\n\nmodel.add(Conv2D(filters=128, kernel_size=(5, 5), activation='relu'))\nmodel.add(BatchNormalization(axis=3))\nmodel.add(Conv2D(filters=128, kernel_size=(5, 5), activation='relu'))\nmodel.add(MaxPooling2D((2, 2)))\nmodel.add(BatchNormalization(axis=3))\nmodel.add(Dropout(0.1))\n\nmodel.add(Conv2D(filters=256, kernel_size=(5, 5), activation='relu'))\nmodel.add(BatchNormalization(axis=3))\nmodel.add(Conv2D(filters=256, kernel_size=(5, 5), activation='relu'))\nmodel.add(MaxPooling2D((2, 2)))\nmodel.add(BatchNormalization(axis=3))\nmodel.add(Dropout(0.1))\n\nmodel.add(Flatten())\n\nmodel.add(Dense(256, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(256, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(num_clases, activation='softmax'))\n\nmodel.summary()\n\n# compile model\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])","execution_count":15,"outputs":[]},{"metadata":{"_cell_guid":"f7936cad-c644-4cb5-8e15-045ec161fe61","_uuid":"1afe9e3f61fa7dc5f354bd16c73c73e0c6d6875e","colab_type":"text","id":"PXjrtevDYzS8"},"cell_type":"markdown","source":"### 3.4. Fit model\nHere we'll train our model. Firstly, we set several callbacks. First callback reduce model learning rate. With high learning rate model quiker is the convergance, however with high learning rate the model could fall into a local minimum. So, we will decreace the learning rate at the process of fitting. We will reduce learning rate if the accuracy is not improved after for three epoch. Other two callbacks save best and last weights of model.\n\nWe won't train model on kaggle kernel, becauce it is too long process, so i comment the lines of code with fitting."},{"metadata":{"_uuid":"eb1fffe45e463ed1e8951a7293f2fea14c08811a","colab":{"output_extras":[{"item_id":1},{"item_id":2}],"autoexec":{"wait_interval":0,"startup":false},"height":3125,"base_uri":"https://localhost:8080/"},"id":"cy4S_8g7YzS9","colab_type":"code","_kg_hide-input":false,"_cell_guid":"0d46cea4-c01c-42f6-a615-500ccaa6abbb","outputId":"151aaa64-9ea3-4ec8-cb60-69cdf54511e4","executionInfo":{"status":"error","elapsed":11199,"user_tz":-180,"timestamp":1517994941027,"user":{"displayName":"Никита Константиновский","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"118348132992166048978"}},"collapsed":true,"trusted":false},"cell_type":"code","source":"from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, CSVLogger\n\n# learning rate reduction\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n                                            patience=3, \n                                            verbose=1, \n                                            factor=0.4, \n                                            min_lr=0.00001)\n\n# checkpoints\nfilepath=\"drive/DataScience/PlantReco/weights.best_{epoch:02d}-{val_acc:.2f}.hdf5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='val_acc', \n                             verbose=1, save_best_only=True, mode='max')\nfilepath=\"drive/DataScience/PlantReco/weights.last_auto4.hdf5\"\ncheckpoint_all = ModelCheckpoint(filepath, monitor='val_acc', \n                                 verbose=1, save_best_only=False, mode='max')\n\n# all callbacks\ncallbacks_list = [checkpoint, learning_rate_reduction, checkpoint_all]\n\n# fit model\n# hist = model.fit_generator(datagen.flow(trainX, trainY, batch_size=75), \n#                            epochs=2, validation_data=(testX, testY), \n#                            steps_per_epoch=trainX.shape[0], callbacks=callbacks_list)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"62cc51ad9ca7f7fc43b77b62dfe1238b9f100e6e","_cell_guid":"d10e22b5-a811-4cbf-97b8-f85968614605"},"cell_type":"markdown","source":"<a id='section4'></a>\n## 4. Evaluate model\n### 4.1. Load model from file\n\nHere we load the weights of best-fitting model from file (i used kaggle dataset with weights of model, which i trained earlier). Also i load from Data.npz training and validation data sets, on which model was fitting for evaluating of model accuracy."},{"metadata":{"_uuid":"d642cde459cddcbc8c57a21289fab8f423a34f5b","colab":{"output_extras":[{"item_id":1}],"autoexec":{"wait_interval":0,"startup":false},"height":2751,"base_uri":"https://localhost:8080/"},"colab_type":"code","id":"TKkO1FHuYzTK","outputId":"b03d6105-928e-47a2-9926-5b661c179d94","executionInfo":{"status":"error","elapsed":24784,"user_tz":-180,"timestamp":1517995421257,"user":{"displayName":"Никита Константиновский","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"118348132992166048978"}},"_cell_guid":"fafb1f53-33a1-4c61-be15-61d0091a5a18","trusted":true},"cell_type":"code","source":"model.load_weights(\"../input/plantrecomodels/weights.best_17-0.96.hdf5\")\n\ndata = np.load(\"../input/plantrecomodels/Data.npz\")\nd = dict(zip((\"trainX\",\"testX\",\"trainY\", \"testY\"), (data[k] for k in data)))\ntrainX = d['trainX']\ntestX = d['testX']\ntrainY = d['trainY']\ntestY = d['testY']\n\nprint(model.evaluate(trainX, trainY))  # Evaluate on train set\nprint(model.evaluate(testX, testY))  # Evaluate on test set","execution_count":23,"outputs":[]},{"metadata":{"_uuid":"8c99fb9cc81bf27437b9734daad19150c2390ab7","_cell_guid":"f1aacb36-4f2a-4f00-b820-575fe2285039"},"cell_type":"markdown","source":"### 4.2. Confusion matrix\n\nA good way to look at model errors."},{"metadata":{"_uuid":"af292cea90fb8cd98e6d6b0eff8cc6b8eeeacaba","_cell_guid":"494dd1ba-8329-4667-924a-ef7ba83c16ae","collapsed":true,"trusted":false},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport itertools\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \n    fig = plt.figure(figsize=(10,10))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=90)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\n# Predict the values from the validation dataset\npredY = model.predict(testX)\npredYClasses = np.argmax(predY, axis = 1) \ntrueY = np.argmax(testY, axis = 1) \n\n# confusion matrix\nconfusionMTX = confusion_matrix(trueY, predYClasses) \n\n# plot the confusion matrix\nplot_confusion_matrix(confusionMTX, classes = le.classes_) ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0826f50f-db77-4063-884e-84b1105af8cb","_uuid":"39f7431849e766e2ad26b293f510f52f5196269b","colab_type":"text","id":"MylCVy4_YzTD"},"cell_type":"markdown","source":"### 4.3. Get results\n\nAnd finnaly we get the result of prediction on test data."},{"metadata":{"_uuid":"fbcc8ed936331ad2ed68e61169dc17f0f8400392","colab":{"autoexec":{"wait_interval":0,"startup":false}},"id":"Xfg3GA69YzTE","colab_type":"code","collapsed":true,"_cell_guid":"86687bf4-6e50-463e-a80c-54a67cae4211","trusted":false},"cell_type":"code","source":"path = '../input/plant-seedlings-classification/test/*.png'\nfiles = glob(path)\n\ntestImg = []\ntestId = []\nj = 1\nnum = len(files)\n\n# Obtain images and resizing, obtain labels\nfor img in files:\n    print(\"Obtain images: \" + str(j) + \"/\" + str(num), end='\\r')\n    testId.append(img.split('/')[-1])  # Images id's\n    testImg.append(cv2.resize(cv2.imread(img), (ScaleTo, ScaleTo)))\n    j += 1\n\ntestImg = np.asarray(testImg)  # Train images set\n\nfor i in range(8):\n    plt.subplot(2, 4, i + 1)\n    plt.imshow(testImg[i])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"533e370a65ec08291b1b476b9de9d063adb442e5","colab":{"autoexec":{"wait_interval":0,"startup":false}},"id":"p_gDyP28YzTP","colab_type":"code","collapsed":true,"_cell_guid":"0ab0df7d-3775-4ea5-945a-f6a6b2c488b3","trusted":false},"cell_type":"code","source":"clearTestImg = []\nexamples = []; getEx = True\nfor img in testImg:\n    # Use gaussian blur\n    blurImg = cv2.GaussianBlur(img, (5, 5), 0)   \n    \n    # Convert to HSV image\n    hsvImg = cv2.cvtColor(blurImg, cv2.COLOR_BGR2HSV)  \n    \n    # Create mask (parameters - green color range)\n    lower_green = (25, 40, 50)\n    upper_green = (75, 255, 255)\n    mask = cv2.inRange(hsvImg, lower_green, upper_green)  \n    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (11, 11))\n    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n    \n    # Create bool mask\n    bMask = mask > 0  \n    \n    # Apply the mask\n    clear = np.zeros_like(img, np.uint8)  # Create empty image\n    clear[bMask] = img[bMask]  # Apply boolean mask to the origin image\n    \n    clearTestImg.append(clear)  # Append image without backgroung\n    \n    # Show examples\n    if getEx:\n        plt.subplot(2, 3, 1); plt.imshow(img)  # Show the original image\n        plt.subplot(2, 3, 2); plt.imshow(blurImg)  # Blur image\n        plt.subplot(2, 3, 3); plt.imshow(hsvImg)  # HSV image\n        plt.subplot(2, 3, 4); plt.imshow(mask)  # Mask\n        plt.subplot(2, 3, 5); plt.imshow(bMask)  # Boolean mask\n        plt.subplot(2, 3, 6); plt.imshow(clear)  # Image without background\n        getEx = False\n\nclearTestImg = np.asarray(clearTestImg)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1696af0fda008ff58ecffc5aacedd01ec63fa8c4","_cell_guid":"57a4f87d-c01a-4361-97b1-e304d12011fa","collapsed":true,"trusted":false},"cell_type":"code","source":"# Show sample result\nfor i in range(8):\n    plt.subplot(2, 4, i + 1)\n    plt.imshow(clearTestImg[i])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2abe8c490cdbae62889499b515141127f191499c","_cell_guid":"e70bc2aa-ed88-4979-8b25-eb10d11da478","collapsed":true,"trusted":false},"cell_type":"code","source":"clearTestImg = clearTestImg / 255","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"059698296ec62ee33519102206f6c19ddad48b6c","_cell_guid":"afc1253d-fca1-42da-ac34-d416ed7d416b","collapsed":true,"trusted":false},"cell_type":"code","source":"pred = model.predict(clearTestImg)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c352558464a0f6aade5852d5e6fe3479b14425fc","_cell_guid":"72ba3a09-643e-4eb5-ac1a-8b388c53e10e","collapsed":true,"trusted":false},"cell_type":"code","source":"# Write result to file\npredNum = np.argmax(pred, axis=1)\npredStr = le.classes_[predNum]\n\nres = {'file': testId, 'species': predStr}\nres = pd.DataFrame(res)\nres.to_csv(\"res.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"79f3af84692c91f65cfb27c389b6ed013e21a5c5","_cell_guid":"e82ceabe-6706-48cf-bbe5-3e57e9b4c2d9","collapsed":true,"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"colab":{"provenance":[],"views":{},"name":"pred.ipynb","default_view":{},"version":"0.3.2","collapsed_sections":[]},"accelerator":"GPU","language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}