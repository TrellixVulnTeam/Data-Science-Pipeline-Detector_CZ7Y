{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\n\nimport os\nfrom os import listdir\nimport cv2\n\n\nfrom sklearn.model_selection import train_test_split","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-08T12:01:58.76788Z","iopub.execute_input":"2021-07-08T12:01:58.768341Z","iopub.status.idle":"2021-07-08T12:02:04.380726Z","shell.execute_reply.started":"2021-07-08T12:01:58.768235Z","shell.execute_reply":"2021-07-08T12:02:04.379908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install adabelief-tf==0.2.0\n!pip install --extra-index-url https://developer.download.nvidia.com/compute/redist --upgrade nvidia-dali-cuda110","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-07-08T12:17:23.583277Z","iopub.execute_input":"2021-07-08T12:17:23.583604Z","iopub.status.idle":"2021-07-08T12:18:00.991985Z","shell.execute_reply.started":"2021-07-08T12:17:23.583572Z","shell.execute_reply":"2021-07-08T12:18:00.991027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\nimport shutil\n\nshutil.copytree(\"/kaggle/input/plant-seedlings-classification/train\", \"/dev/shm/train\")","metadata":{"execution":{"iopub.status.busy":"2021-07-08T12:02:12.061357Z","iopub.execute_input":"2021-07-08T12:02:12.061718Z","iopub.status.idle":"2021-07-08T12:02:46.232909Z","shell.execute_reply.started":"2021-07-08T12:02:12.061681Z","shell.execute_reply":"2021-07-08T12:02:46.232078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\ntrain_path = \"/dev/shm/train/\"\nlabel_dict = {k: i for i, k in enumerate(os.listdir(train_path))}\ntrain_file_paths = []\ntrain_labels = []\nfor label in os.listdir(train_path):\n    for file in os.listdir(train_path + label):\n        train_file_path = train_path + label + \"/{}\".format(file)\n        train_file_paths.append(train_file_path)\n        train_labels.append(label_dict[label])","metadata":{"execution":{"iopub.status.busy":"2021-07-08T13:49:02.92232Z","iopub.execute_input":"2021-07-08T13:49:02.922644Z","iopub.status.idle":"2021-07-08T13:49:02.935875Z","shell.execute_reply.started":"2021-07-08T13:49:02.922616Z","shell.execute_reply":"2021-07-08T13:49:02.935082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dev_paths, val_paths, dev_labels, val_labels = train_test_split(train_file_paths, train_labels, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2021-07-08T13:49:03.307704Z","iopub.execute_input":"2021-07-08T13:49:03.30802Z","iopub.status.idle":"2021-07-08T13:49:03.316238Z","shell.execute_reply.started":"2021-07-08T13:49:03.307977Z","shell.execute_reply":"2021-07-08T13:49:03.31511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rng = tf.random.Generator.from_seed(2434, alg='philox')\n\ndef read_image(path, label):\n    image = tf.io.read_file(path)\n    image = tf.image.decode_png(image, channels=3)\n    image = tf.image.resize(image, [21*10, 21*10])\n    label = tf.one_hot(label, 12)\n    return image, label\n\ndef read_and_augment_image(path, label):\n    image = tf.io.read_file(path)\n    image = tf.image.decode_png(image, channels=3)\n    image = tf.image.resize(image, [21*10, 21*10])\n    label = tf.one_hot(label, 12)\n\n    seed = rng.make_seeds(2)[0]\n    \n    image = tf.image.stateless_random_crop(value=image, size=(21*8, 21*8, 3), seed=seed)\n    image = tf.image.stateless_random_flip_left_right(image, seed)\n    image = tf.image.stateless_random_flip_up_down(image, seed)\n    image = tf.image.stateless_random_saturation(image, 0.5, 1., seed)\n    image = tf.image.stateless_random_hue(image, 0.05, seed)\n    image = tf.image.stateless_random_brightness(image, 0.2, seed)\n    return image, label","metadata":{"execution":{"iopub.status.busy":"2021-07-08T13:50:10.005453Z","iopub.execute_input":"2021-07-08T13:50:10.005772Z","iopub.status.idle":"2021-07-08T13:50:10.015977Z","shell.execute_reply.started":"2021-07-08T13:50:10.005743Z","shell.execute_reply":"2021-07-08T13:50:10.015054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 32\nSHUFFLE_BUFFER_SIZE = len(dev_labels)\n\ndev_dataset = tf.data.Dataset.from_tensor_slices((dev_paths, dev_labels)).shuffle(SHUFFLE_BUFFER_SIZE)\nval_dataset = tf.data.Dataset.from_tensor_slices((val_paths, val_labels))","metadata":{"execution":{"iopub.status.busy":"2021-07-08T13:50:11.494348Z","iopub.execute_input":"2021-07-08T13:50:11.494676Z","iopub.status.idle":"2021-07-08T13:50:11.533153Z","shell.execute_reply.started":"2021-07-08T13:50:11.494646Z","shell.execute_reply":"2021-07-08T13:50:11.532364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for image, label in dev_dataset.map(read_and_augment_image, num_parallel_calls=tf.data.AUTOTUNE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE):\n    break","metadata":{"execution":{"iopub.status.busy":"2021-07-08T13:49:04.630763Z","iopub.execute_input":"2021-07-08T13:49:04.631116Z","iopub.status.idle":"2021-07-08T13:49:04.878581Z","shell.execute_reply.started":"2021-07-08T13:49:04.631085Z","shell.execute_reply":"2021-07-08T13:49:04.877677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2021-07-08T13:49:41.159726Z","iopub.execute_input":"2021-07-08T13:49:41.160057Z","iopub.status.idle":"2021-07-08T13:49:41.173908Z","shell.execute_reply.started":"2021-07-08T13:49:41.160021Z","shell.execute_reply":"2021-07-08T13:49:41.172971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.imshow(image.numpy()[0].astype(np.uint8))","metadata":{"execution":{"iopub.status.busy":"2021-07-08T13:46:11.542201Z","iopub.execute_input":"2021-07-08T13:46:11.542462Z","iopub.status.idle":"2021-07-08T13:46:11.712145Z","shell.execute_reply.started":"2021-07-08T13:46:11.542437Z","shell.execute_reply":"2021-07-08T13:46:11.711392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image[0]","metadata":{"execution":{"iopub.status.busy":"2021-07-08T13:43:23.737292Z","iopub.execute_input":"2021-07-08T13:43:23.737652Z","iopub.status.idle":"2021-07-08T13:43:23.752764Z","shell.execute_reply.started":"2021-07-08T13:43:23.737621Z","shell.execute_reply":"2021-07-08T13:43:23.751659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class GeneralizedMeanPooling2D(tf.keras.layers.Layer):\n    def __init__(self, p, data_format=\"channels_last\"):\n        super(GeneralizedMeanPooling2D, self).__init__()\n        if p <= 0:\n            raise NotImplementedError\n        self.p = p\n        self.data_format = data_format\n \n\n    def safe_power(self, x, p):\n        return tf.sign(x) * tf.pow(tf.maximum(tf.abs(x), tf.keras.backend.epsilon()), p)\n\n\n    def call(self, inputs):\n        if self.data_format == \"channels_last\":\n            inputs_mean = tf.keras.backend.mean(self.safe_power(inputs, self.p), axis=[1, 2])\n        else:\n            inputs_mean = tf.keras.backend.mean(self.safe_power(inputs, self.p), axis=[2, 3])\n        inputs_res = self.safe_power(inputs_mean, 1/self.p)\n        return inputs_res\n    ","metadata":{"execution":{"iopub.status.busy":"2021-07-08T13:50:41.018415Z","iopub.execute_input":"2021-07-08T13:50:41.018729Z","iopub.status.idle":"2021-07-08T13:50:41.026885Z","shell.execute_reply.started":"2021-07-08T13:50:41.018702Z","shell.execute_reply":"2021-07-08T13:50:41.02608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow_probability as tfp\nclass CurricularAdaCos(tf.keras.layers.Layer):\n    \n    def __init__(self,\n                 out_dim=None,\n                 margin=0.4,\n                 init_scale=None,\n                 init_t=0.,\n                 momentum=0.9,\n                 adaptive_scale=True,\n                 adaptive_cface=True,\n                 adaptive_t=True,\n                 builtin_weight=True,\n                 different_weight_per_sample=False,\n                 weight_kernel_initializer=\"glorot_uniform\",\n                 ):\n        super(CurricularAdaCos, self).__init__()\n        self.out_dim = out_dim\n        self.margin = margin\n        self.cos_m = tf.cos(self.margin)\n        self.sin_m = tf.sin(self.margin) \n        \n        if init_scale is None:\n            if out_dim is None:\n                self.init_scale = tf.sqrt(2.) * tf.math.log(2**6 - 1)\n            else:\n                self.init_scale = tf.sqrt(2.) * tf.math.log(tf.cast(out_dim, \"float32\") - 1)\n        self.scale = tf.Variable(self.init_scale, trainable=False)\n        self.t = tf.Variable(init_t, trainable=False)\n        self.momentum = momentum\n        \n        self.adaptive_scale = adaptive_scale\n        self.adaptive_cface = adaptive_cface\n        self.adaptive_t = adaptive_t\n        self.builtin_weight = builtin_weight\n        self.different_weight_per_sample = different_weight_per_sample\n        self.weight_kernel_initializer = weight_kernel_initializer\n        \n    def build(self, input_shape):\n        if self.builtin_weight and self.out_dim:\n            last_dim = input_shape[-1]\n            self.kernel = self.add_weight(\"kernel\",\n                                          shape=[self.out_dim, last_dim],\n                                          initializer=self.weight_kernel_initializer,\n                                          trainable=True)\n        self.built = True\n    \n    def call(self, X, y, W=None, training=False):\n        if not self.built:\n            self.build(X.shape)\n        \n        if self.builtin_weight:\n            W = self.kernel\n        \n        X = tf.math.l2_normalize(X, axis=-1)\n        W = tf.math.l2_normalize(W, axis=-1)\n        \n        if self.different_weight_per_sample:\n            cos = tf.reduce_sum(X*W, axis=-1)\n        else:\n            cos = tf.matmul(X, W, transpose_b=True)\n        \n        if training:\n            if self.adaptive_t:\n                new_t = tf.reduce_mean(tf.reduce_sum(cos*y, axis=-1))\n            sin = tf.math.sqrt(1. - tf.math.square(cos))\n            \n            # add margin\n            cos = tf.where(y==1., cos*self.cos_m - sin*self.sin_m, cos)\n            \n            # curricularFace part\n            if self.adaptive_cface:\n                positive_cos = tf.reduce_mean(cos*y, axis=-1, keepdims=True)\n                new_negative_cos = tf.where(cos > positive_cos, cos * (self.t + cos), cos)\n                cos = tf.where(y == 1., cos, new_negative_cos)\n            \n            if self.adaptive_t:\n                self.t.assign(self.momentum * self.t + (1 - self.momentum) * new_t)\n            \n            # AdaCos part\n            if self.adaptive_scale:\n                B = (1 - y) * tf.exp(self.scale*cos)\n                B_avg = tf.reduce_mean(tf.reduce_sum(B, axis=-1), axis=0)\n\n                theta = tf.acos(tf.clip_by_value(cos, -(1. - tf.keras.backend.epsilon()), 1. - tf.keras.backend.epsilon()))\n                theta_true = tf.reduce_sum(y*theta, axis=-1)\n                theta_med = tfp.stats.percentile(theta_true, q=50)\n\n                # ここでmarginを引くかは要検討\n                scale = tf.math.log(tf.maximum(B_avg, tf.keras.backend.epsilon())) / tf.maximum(tf.cos(tf.minimum(np.pi/4 , theta_med - self.margin)), tf.keras.backend.epsilon())\n                scale = tf.maximum(tf.keras.backend.epsilon(), scale)\n                if tf.math.is_finite(scale):\n                    new_scale = self.scale * self.momentum + (1 - self.momentum) * tf.stop_gradient(scale)\n                else:\n                    new_scale = self.scale * self.momentum + (1 - self.momentum) * self.init_scale\n                self.scale.assign(tf.stop_gradient(new_scale))\n            \n        logit = self.scale * cos\n        return logit","metadata":{"execution":{"iopub.status.busy":"2021-07-08T13:50:41.266718Z","iopub.execute_input":"2021-07-08T13:50:41.266998Z","iopub.status.idle":"2021-07-08T13:50:41.859164Z","shell.execute_reply.started":"2021-07-08T13:50:41.266972Z","shell.execute_reply":"2021-07-08T13:50:41.858292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.applications import EfficientNetB3\n\nclass BaseModel(tf.keras.Model):\n\n    def __init__(self, projection_dim, out_dim):\n        super(BaseModel, self).__init__()\n        self.cnn = EfficientNetB3(weights = 'imagenet',\n                                  include_top=False,\n                                  pooling=\"avg\",\n                                  input_shape=None)\n        self.dense = tf.keras.layers.Dense(projection_dim, activation=\"relu\")\n        self.top = tf.keras.layers.Dense(out_dim)\n        \n    def call(self, X, y, training=False):\n        X = self.cnn(X)\n        X = self.dense(X)\n        pred = self.top(X)\n        return pred\n\nclass MyModel(tf.keras.Model):\n\n    def __init__(self, projection_dim, out_dim):\n        super(MyModel, self).__init__()\n        self.cnn = EfficientNetB3(weights = 'imagenet',\n                                  include_top=False,\n                                  pooling=None,\n                                  input_shape=None)\n        self.pool = GeneralizedMeanPooling2D(p=3.)\n        self.dense = tf.keras.layers.Dense(projection_dim, use_bias=False)\n        self.top = CurricularAdaCos(out_dim=12,\n                                    margin=0.4)\n\n    def call(self, X, y, training=False):\n        X = self.cnn(X)\n        X = self.pool(X)\n        X = self.dense(X)\n        pred = self.top(X, y, training=training)\n        return pred\n","metadata":{"execution":{"iopub.status.busy":"2021-07-08T13:50:42.054461Z","iopub.execute_input":"2021-07-08T13:50:42.054765Z","iopub.status.idle":"2021-07-08T13:50:42.065937Z","shell.execute_reply.started":"2021-07-08T13:50:42.054737Z","shell.execute_reply":"2021-07-08T13:50:42.064976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2021-07-07T14:25:22.7438Z","iopub.execute_input":"2021-07-07T14:25:22.744167Z","iopub.status.idle":"2021-07-07T14:25:24.740028Z","shell.execute_reply.started":"2021-07-07T14:25:22.744134Z","shell.execute_reply":"2021-07-07T14:25:24.739119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from adabelief_tf import AdaBeliefOptimizer\nfrom tensorflow_addons.metrics import F1Score\nfrom tqdm import tqdm\n\nNUM_EPOCH = 10\n\nmodel = BaseModel(projection_dim=128, out_dim=12)\noptimizer = AdaBeliefOptimizer(learning_rate=1e-3, weight_decay=1e-4, epsilon=1e-14, print_change_log = False) \nloss_object = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\ndev_loss = tf.keras.metrics.Mean(name='dev_loss')\ndev_f1 = F1Score(num_classes=12, average=\"micro\")\n\nval_loss = tf.keras.metrics.Mean(name='val_loss')\nval_f1 = F1Score(num_classes=12, average=\"micro\")\n\n@tf.function\ndef train_step(X, y):\n    with tf.GradientTape() as tape:\n        logit = model.call(X, y, training=True)\n        loss = loss_object(y, logit)\n    gradients = tape.gradient(loss, model.trainable_variables)\n    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n    dev_loss.update_state(loss)\n    dev_f1.update_state(y, tf.exp(logit))\n    \n@tf.function\ndef test_step(X, y):\n    logit = model.call(X, y)\n    loss = loss_object(y, logit)\n    val_loss.update_state(loss)\n    val_f1.update_state(y, tf.exp(logit))\n\nlast_dev_loss = np.inf\nlast_val_loss = np.inf\nlast_dev_f1 = 0.\nlast_val_f1 = 0.\n    \nwith tqdm(total=NUM_EPOCH) as pbar:\n    for epoch in range(NUM_EPOCH):\n        step = 0\n        for batch_X, batch_y in dev_dataset.map(read_and_augment_image, num_parallel_calls=tf.data.AUTOTUNE)\\\n                                           .batch(BATCH_SIZE)\\\n                                           .prefetch(tf.data.AUTOTUNE):\n            batch_X = tf.cast(batch_X, \"float32\")\n            batch_y = tf.cast(batch_y, \"float32\")\n            train_step(batch_X, batch_y)\n            param_text = \"\"#\"scale: {:.3f} t: {:.3f} \".format(model.top.scale.numpy(), model.top.t.numpy())\n            learning_text = \"[{}/{}] \".format(str(step).zfill(4), len(dev_dataset)//BATCH_SIZE)\n            progress_text = \"dev | Loss: {:.5f} f1: {:.5f} val| Loss: {:.5f} f1 {:.5f}\".format(dev_loss.result().numpy(),\n                                                                                               dev_f1.result().numpy(),\n                                                                                               last_val_loss,\n                                                                                               last_val_f1)\n            pbar.set_postfix_str(learning_text + param_text + progress_text)\n        last_dev_loss = dev_loss.result().numpy()\n        last_dev_f1 = dev_f1.result().numpy()\n        dev_loss.reset_states()\n        dev_f1.reset_states()\n        \n        for batch_X, batch_y in val_dataset.map(read_image, num_parallel_calls=tf.data.AUTOTUNE)\\\n                                           .batch(BATCH_SIZE)\\\n                                           .prefetch(tf.data.AUTOTUNE):\n            batch_X = tf.cast(batch_X, \"float32\")\n            batch_y = tf.cast(batch_y, \"float32\")\n            test_step(batch_X, batch_y)\n            param_text = \"\"#\"scale: {:.3f} t: {:.3f} \".format(model.top.scale.numpy(), model.top.t.numpy())\n            learning_text = \"[{}/{}] \".format(str(step).zfill(4), len(dev_dataset))\n            progress_text = \"dev | Loss: {:.5f} f1: {:.5f} val| Loss: {:.5f} f1 {:.5f}\".format(last_dev_loss,\n                                                                                               last_dev_f1,\n                                                                                               val_loss.result().numpy(),\n                                                                                               val_f1.result().numpy())\n            pbar.set_postfix_str(learning_text + param_text + progress_text) \n        print(progress_text)\n        last_val_loss = val_loss.result().numpy()\n        last_val_f1 = val_f1.result().numpy()\n        val_loss.reset_states()\n        val_f1.reset_states()\n        pbar.update(1)","metadata":{"execution":{"iopub.status.busy":"2021-07-07T11:08:56.594381Z","iopub.execute_input":"2021-07-07T11:08:56.594701Z","iopub.status.idle":"2021-07-07T11:17:28.148507Z","shell.execute_reply.started":"2021-07-07T11:08:56.594666Z","shell.execute_reply":"2021-07-07T11:17:28.147621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from adabelief_tf import AdaBeliefOptimizer\nfrom tensorflow_addons.metrics import F1Score\nfrom tqdm import tqdm\n\nNUM_EPOCH = 10\n\nmodel = MyModel(projection_dim=64, out_dim=12)\noptimizer = AdaBeliefOptimizer(learning_rate=1e-3, weight_decay=1e-4, epsilon=1e-14, print_change_log = False) \nloss_object = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\ndev_loss = tf.keras.metrics.Mean(name='dev_loss')\ndev_f1 = F1Score(num_classes=12, average=\"micro\")\n\nval_loss = tf.keras.metrics.Mean(name='val_loss')\nval_f1 = F1Score(num_classes=12, average=\"micro\")\n\n@tf.function\ndef train_step(X, y):\n    with tf.GradientTape() as tape:\n        logit = model.call(X, y, training=True)\n        loss = loss_object(y, logit)\n    gradients = [tf.clip_by_norm(g, 10) for g in tape.gradient(loss, model.trainable_variables)]\n    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n    dev_loss.update_state(loss)\n    dev_f1.update_state(y, tf.exp(logit))\n    \n@tf.function\ndef test_step(X, y):\n    logit = model.call(X, y)\n    loss = loss_object(y, logit)\n    val_loss.update_state(loss)\n    val_f1.update_state(y, tf.exp(logit))\n\nlast_dev_loss = np.inf\nlast_val_loss = np.inf\nlast_dev_f1 = 0.\nlast_val_f1 = 0.\n\nfin = 0\nwith tqdm(total=NUM_EPOCH) as pbar:\n    for epoch in range(NUM_EPOCH):\n        step = 0\n        for batch_X, batch_y in dev_dataset.map(read_and_augment_image, num_parallel_calls=tf.data.AUTOTUNE)\\\n                                           .batch(BATCH_SIZE)\\\n                                           .prefetch(tf.data.AUTOTUNE):\n            batch_X = tf.cast(batch_X, \"float32\")\n            batch_y = tf.cast(batch_y, \"float32\")\n            train_step(batch_X, batch_y)\n\n            param_text = \"scale: {:.3f} t: {:.3f} \".format(model.top.scale.numpy(), model.top.t.numpy())\n            learning_text = \"[{}/{}] \".format(str(step).zfill(4), len(dev_dataset)//BATCH_SIZE)\n            progress_text = \"dev | Loss: {:.5f} f1: {:.5f} val| Loss: {:.5f} f1 {:.5f}\".format(dev_loss.result().numpy(),\n                                                                                               dev_f1.result().numpy(),\n                                                                                               last_val_loss,\n                                                                                               last_val_f1)\n            pbar.set_postfix_str(learning_text + param_text + progress_text)\n            step += 1\n\n        last_dev_loss = dev_loss.result().numpy()\n        last_dev_f1 = dev_f1.result().numpy()\n        dev_loss.reset_states()\n        dev_f1.reset_states()\n        \n        for batch_X, batch_y in val_dataset.map(read_image, num_parallel_calls=tf.data.AUTOTUNE)\\\n                                           .batch(BATCH_SIZE)\\\n                                           .prefetch(tf.data.AUTOTUNE):\n            batch_X = tf.cast(batch_X, \"float32\")\n            batch_y = tf.cast(batch_y, \"float32\")\n            test_step(batch_X, batch_y)\n            param_text = \"scale: {:.3f} t: {:.3f} \".format(model.top.scale.numpy(), model.top.t.numpy())\n            learning_text = \"[{}/{}] \".format(str(step).zfill(4), len(dev_dataset))\n            progress_text = \"dev | Loss: {:.5f} f1: {:.5f} val| Loss: {:.5f} f1 {:.5f}\".format(last_dev_loss,\n                                                                                               last_dev_f1,\n                                                                                               val_loss.result().numpy(),\n                                                                                               val_f1.result().numpy())\n            pbar.set_postfix_str(learning_text + param_text + progress_text)\n        print(progress_text)\n        last_val_loss = val_loss.result().numpy()\n        last_val_f1 = val_f1.result().numpy()\n        val_loss.reset_states()\n        val_f1.reset_states()\n        pbar.update(1)","metadata":{"execution":{"iopub.status.busy":"2021-07-08T14:12:30.702941Z","iopub.execute_input":"2021-07-08T14:12:30.703309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}