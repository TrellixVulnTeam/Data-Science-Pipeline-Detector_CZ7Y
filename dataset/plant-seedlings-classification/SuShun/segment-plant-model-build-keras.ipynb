{"cells":[{"metadata":{"trusted":true,"collapsed":true,"_uuid":"1fd73f804b782225e8c0af90c4f39a3dd22c3e11"},"cell_type":"code","source":"import cv2\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"b6665e5b0ae59a621917c750054c48f16aff09ea"},"cell_type":"code","source":"def create_mask_for_plant(image):\n    image_hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n\n    sensitivity = 10\n    # lower_hsv = np.array([60 - sensitivity, 100, 50])\n    # upper_hsv = np.array([60 + sensitivity, 255, 255])\n    lower_hsv = np.array([35 - sensitivity, 43, 46])\n    upper_hsv = np.array([77 + sensitivity, 255, 255])\n\n    mask = cv2.inRange(image_hsv, lower_hsv, upper_hsv)\n    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (11, 11))  # 椭圆结构\n    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)  # 进行闭运算（将物体合并）\n\n    #  通过腐蚀，膨胀将一些小的噪音去除\n    kernel = np.ones((3, 3), np.int8)\n    mask = cv2.erode(mask, kernel=kernel, iterations=2)\n    mask = cv2.dilate(mask, kernel=kernel, iterations=2)\n\n    return mask\n\n\ndef segment_plant(image):\n    mask = create_mask_for_plant(image)\n    segment_plant = cv2.bitwise_and(image, image, mask=mask)\n    return segment_plant\n\n\ndef sharpen_image(image):\n    image_blurred = cv2.GaussianBlur(image, (0, 0), 3)\n    image_sharp = cv2.addWeighted(image, 1.5, image_blurred, -0.5, 0)\n    return image_sharp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"54b100746e31b3decc435331dee70669cd80f384","collapsed":true},"cell_type":"code","source":"classes = 12\nscale = 70\nseed = 7\n\ndef creat_model_kaggle():\n    from keras.models import Sequential\n    from keras.layers import Conv2D, BatchNormalization, MaxPooling2D, Dropout, Flatten, Dense\n\n    model = Sequential()\n\n    model.add(\n        Conv2D(filters=64, kernel_size=(5, 5), input_shape=(3, scale, scale), activation='relu', dim_ordering='th'))\n    model.add(BatchNormalization(axis=3))\n    model.add(Conv2D(filters=64, kernel_size=(5, 5), activation='relu', dim_ordering='th'))\n    model.add(MaxPooling2D((2, 2), dim_ordering='th'))\n    model.add(BatchNormalization(axis=3))\n    model.add(Dropout(0.1))\n\n    model.add(Conv2D(filters=128, kernel_size=(5, 5), activation='relu', dim_ordering='th'))\n    model.add(BatchNormalization(axis=3))\n    model.add(Conv2D(filters=128, kernel_size=(5, 5), activation='relu', dim_ordering='th'))\n    model.add(MaxPooling2D((2, 2), dim_ordering='th'))\n    model.add(BatchNormalization(axis=3))\n    model.add(Dropout(0.1))\n\n    model.add(Conv2D(filters=256, kernel_size=(5, 5), activation='relu', dim_ordering='th'))\n    model.add(BatchNormalization(axis=3))\n    model.add(Conv2D(filters=256, kernel_size=(5, 5), activation='relu', dim_ordering='th'))\n    model.add(MaxPooling2D((2, 2), dim_ordering='th'))\n    model.add(BatchNormalization(axis=3))\n    model.add(Dropout(0.1))\n\n    model.add(Flatten())\n\n    model.add(Dense(256, activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.5))\n\n    model.add(Dense(256, activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.5))\n\n    model.add(Dense(classes, activation='softmax'))\n\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n    # model.summary()\n    return model\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"04b47fd7fbaa399f0e97f79acdc9471edfe5833d"},"cell_type":"code","source":"def load_train(train_path):\n    # 该函数读取数据集，并且对数据集进行处理，以便于之后放入keras的网络中进行训练\n    import os\n    import glob\n    import cv2\n    import numpy as np\n\n    x_train = []\n    x_train_class = []\n    x_train_index = []\n    print('Reading train images ... ')\n\n    cate = []\n    for x in os.listdir(train_path):\n        if os.path.isdir(train_path + x):\n            cate.append(train_path + x)\n\n    for index, floder in enumerate(cate):\n        for im in glob.glob(floder + '/*.png'):\n            image = cv2.resize(cv2.imread(im), (scale, scale))\n            segment_plants = segment_plant(image=image)\n            x_train.append(segment_plants)\n            x_train_class.append(os.path.basename(im))\n            x_train_index.append(index)\n\n    return np.asarray(x_train), x_train_class, np.asarray(x_train_index)\n\n\ndef read_normalize_train_data(train_path):\n    from keras.utils import np_utils\n    import numpy as np\n    x_train, x_train_class, x_train_index = load_train(train_path=train_path)\n\n    print('Reshape:np.transpose ... ')\n    x_train = x_train.transpose((0, 3, 1, 2))  # transpose之后的(train个数，channels,weight,height)\n    x_train = x_train.astype('float32')\n    x_train = x_train / 255\n    x_train_index = np_utils.to_categorical(x_train_index, len(np.unique(x_train_index)))\n\n    print('Train shape:', x_train.shape)\n    print(x_train.shape[0], 'train samples')\n\n    return x_train, x_train_class, x_train_index\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"4da539c56b14a838ef2950929601c61e9d90536f"},"cell_type":"code","source":"\ndef read_normalize_test_data(img_path):\n    import cv2\n    import numpy as np\n    from keras.preprocessing.image import img_to_array\n\n    plant = segment_plant(image=cv2.resize(cv2.imread(img_path), (scale, scale)))\n    # cv2.imshow('',plant)\n    # cv2.waitKey(0)\n    plant = img_to_array(plant)\n    plant = [plant]\n    plant = np.array(plant, dtype=np.uint8)\n    plant = plant.transpose((0, 3, 1, 2))\n    plant = plant.astype('float32')\n    plant = plant / 255\n    return plant","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1839613afec555ac09a3f1c5f802c187319efef8","collapsed":true},"cell_type":"code","source":"def run_cross_validation_create_models(train_path, nfolds=10):\n    from sklearn.cross_validation import KFold\n    from keras.callbacks import EarlyStopping\n    from sklearn.metrics import log_loss\n    import matplotlib.pyplot as plt\n\n    batch_size = 32\n    nb_epoch = 50  # 训练次数\n    random_state = seed  # 随机数种子的数量\n    x_train, x_train_class, x_train_index = read_normalize_train_data(train_path)\n\n    kf = KFold(len(x_train), n_folds=nfolds, shuffle=True, random_state=random_state)\n    num_fold = 0\n    sum_loss = 0\n\n    for train_index, test_index in kf:\n        X_train = x_train[train_index]\n        X_val = x_train[test_index]\n        Y_train = x_train_index[train_index]\n        Y_val = x_train_index[test_index]\n\n        num_fold += 1\n        print('*' * 50)\n\n        print('Start KFold number {} from {}'.format(num_fold, nfolds))\n        print('Split train: ', len(X_train))\n        print('Split valid: ', len(X_val))\n\n        model = creat_model_kaggle()\n        # model = multi_gpu_model(model=model, gpus=2)\n\n        # 提前停止训练:patience:能够容忍多少个epoch内都没有improvement,verbose:是否打印出提前停止训练的相关信息\n        callbacks = [EarlyStopping(monitor='val_loss', patience=10, verbose=0)]\n\n        result = model.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=0, callbacks=callbacks,\n                           validation_data=(X_val, Y_val), shuffle=True)\n\n        # model.save('models/plant_segment_' + str(num_fold) + '.h5')\n\n        predictions_val = model.predict(X_val.astype('float32'), batch_size=batch_size, verbose=2)\n        logloss = log_loss(Y_val, predictions_val)  # 对数损失函数\n        loss, acc = model.evaluate(X_train, Y_train, verbose=0)\n\n        sum_loss += logloss * len(test_index)\n        print('Score log_loss:{}'.format(logloss))\n        print('Score loss:{}'.format(acc))\n        print('Score accuracy:{}'.format(acc))\n\n        plt.figure()\n        plt.plot(result.epoch, result.history['acc'], label=\"acc\")\n        plt.plot(result.epoch, result.history['val_acc'], label=\"val_acc\")\n        plt.scatter(result.epoch, result.history['acc'], marker='*')\n        plt.scatter(result.epoch, result.history['val_acc'])\n        plt.legend(loc='under right')\n        # plt.savefig('evaluation/plant_segment_accuracy_' + str(num_fold) + '.jpg')\n\n        plt.figure()\n        plt.plot(result.epoch, result.history['loss'], label=\"loss\")\n        plt.plot(result.epoch, result.history['val_loss'], label=\"val_loss\")\n        plt.scatter(result.epoch, result.history['loss'], marker='*')\n        plt.scatter(result.epoch, result.history['val_loss'])\n        plt.legend(loc='upper right')\n        # plt.savefig('evaluation/plant_segment_loss_' + str(num_fold) + '.jpg')\n\n    avg_loss = sum_loss / len(x_train)\n    print(\"Log_Loss_Avg train independent: \", avg_loss)\n\n\nif __name__ == '__main__':\n    import time\n    import tensorflow as tf\n\n    # import keras.backend.tensorflow_backend as KTF\n    #\n    # KTF.set_session(tf.Session(config=tf.ConfigProto(device_count={'gpu': 0})))\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    session = tf.Session(config=config)\n    \n    run_cross_validation_create_models(train_path='../input/train/')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"b0bbb3c6e128a34404108033e6a3bc88e78869dd"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}