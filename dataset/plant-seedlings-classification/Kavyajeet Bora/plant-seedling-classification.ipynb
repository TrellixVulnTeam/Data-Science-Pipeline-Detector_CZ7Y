{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport cv2\nimport time\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\n\nfrom torch.utils.data import Dataset, DataLoader\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import transforms, datasets, models\n\nfrom tqdm import tqdm_notebook\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_folder = '/kaggle/input/plant-seedlings-classification/train'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def return_classes(parent_folder):\n    classes = {}\n    for i,plant_type in enumerate(os.listdir(parent_folder)):\n        classes.setdefault(i,plant_type)\n    return classes\n\nclasses = return_classes(training_folder)\nclasses","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def create_train_dataframe(parent_folder, classes, verbose=True):\n    data = []\n    for i,plant_class in classes.items():\n        folder = os.path.join(parent_folder,plant_class)\n        images = os.listdir(folder)\n        for image in images:\n            image_path = os.path.join(folder,image)\n            data.append([image_path,plant_class,i])\n            \n    df = pd.DataFrame(data,columns=['image','type','class'], index=np.arange(1,len(data)+1))\n    return df\n\ntraining_folder = '/kaggle/input/plant-seedlings-classification/train'\nplant_df = create_train_dataframe(training_folder,classes)\nprint(len(plant_df))\nplant_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def images_per_class(dataframe):\n    img_per_class = []\n    for i,plant_type in classes.items():\n        total_images = len(dataframe[dataframe['class'] == i])\n        img_per_class.append(total_images)\n        print(plant_type, total_images)\n    return img_per_class\n\nimages_per_class(plant_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1. Displaying the ground truth labels"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_img_per_class(dataframe):\n    plt.figure(figsize=(15,10))\n    for i in range(12):\n        index = np.random.choice\n        images = plant_df[plant_df['class']==i]['image'].values\n        index = np.random.choice(len(images))\n        image = Image.open(images[index])\n        plt.subplot(4,3,i+1)\n        plt.imshow(image)\n        plt.title(classes[i])\n        plt.xticks([])\n        plt.yticks([])\n        \n        \nplot_img_per_class(plant_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. Preparing the data for training"},{"metadata":{"trusted":true},"cell_type":"code","source":"## test train split\ntrain_data = plant_df.sample(frac=0.8)\nvalid_data = plant_df[~plant_df['image'].isin(train_data['image'])]\n\nprint(train_data.shape, valid_data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Plant_Dataset(Dataset):\n    \n    def __init__(self, dataframe, transform=None):\n        self.dataframe = dataframe\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.dataframe)\n    \n    def __getitem__(self,index):\n        image_file = self.dataframe.iloc[index,0]\n        image = Image.open(image_file).convert('RGB')\n        if self.transform is not None:\n            image = self.transform(image)\n        label = self.dataframe.iloc[index,2]\n        \n        return image, label","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plant_dataset = Plant_Dataset(plant_df)\ntrain_transform = transforms.Compose([\n    transforms.Resize((224,224)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomVerticalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # check the theory based on the normalizing the image\n])\n\nvalidation_transform = transforms.Compose([\n    transforms.Resize((224,224)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # check the theory based on the normalizing the image\n])\n\nimage, label = plant_dataset.__getitem__(1)\ntrain_transform(image)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## make dataset and dataloaders for train and test set respectively\ndatasets =  {}\n\ndatasets['train'] = Plant_Dataset(train_data, train_transform)\ndatasets['validation'] = Plant_Dataset(valid_data, validation_transform)\n\nbatch_size=32\n\ndataloaders = {}\ndataloaders['train'] = DataLoader(datasets['train'], shuffle=True, batch_size=batch_size)\ndataloaders['validation'] = DataLoader(datasets['validation'], shuffle=False, batch_size=batch_size)\n\ndataset_sizes = {}\ndataset_sizes['train'] = len(datasets)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3. Model\n\n**Things to do further to improve the model**\n\nData preprocessing improvements - \n1. Store the file_path and category into dictionary or dataframe **= Done**\n2. Split the data into train and test set  **= Done**\n3. Make a dataset class for loading data from given dataframe **= Done**\n4. Make dataloaders for train and validation **=Done**\n5. Since the dataset is skewed, feed the images with equal distribution while training\n6. Data Augmentation - Horizontal/vertical flip, randomresized crop etc\n \n\n**Before train incoporate the training as well validation in the function**\n\nModel Improvements - \n1. Start training on pretrained model\n2. Is softmax layer required for training ?\n3. Activate only the last layer of the model to requires_grad=True, since it is a pretrained model\n4. Use a deeper model like resnet50\n5. To improve the model, implement data augmentation\n6. Run it for more epochs\n7. **How to design the CNN architecture**\n\noptimizer - \n1. Use Lr_scheduler -> plateau lr scheduler\n2. Use weight decay in optimizer\n \nEvaluation - \n1. Calculate the overall accuracy on training dataset\n2. Calculate the F_score on training dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nprint(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = models.resnet50(pretrained=True)\n#I recommend training with these layers unfrozen for a couple of epochs after the initial frozen training\nfor param in model.parameters():  ## freezing the initial layers\n    param.requires_grad = False\n    \nmodel.fc = nn.Sequential(\n    nn.Linear(model.fc.in_features,12)\n)\n\nloss_fn = nn.CrossEntropyLoss()\nopt = optim.Adam(model.parameters())\n# lr scheduler - reduce on loss plateau decay\n# lr = lr * factor \n# mode (str) – One of min, max. In min mode, lr will be reduced when the quantity monitored has stopped decreasing; \n# in max mode it will be reduced when the quantity monitored has stopped increasing. Default: ‘min\n# patience: number of epochs - 1 where loss plateaus before decreasing LR\n        # patience = 0, after 1 bad epoch, reduce LR\n# factor = decaying factor\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(opt, patience=5, factor=0.1, verbose=True, mode='max')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"images,labels = next(iter(dataloaders['train']))\nimages,labels = images.to(device),labels.to(device)\nmodel = model.to(device)\ntorch.argmax(model(images),dim=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(model, optimizer, loss_fn, epochs=10, device=device):\n    start_time = time.time()\n    best_acc = 0\n    best_model_wts = model.state_dict()\n    \n    train_loss = []\n    val_loss = []\n    \n    train_acc = []\n    val_acc = []\n    \n    model = model.to(device)\n    for epoch in tqdm_notebook(range(epochs)):\n        print('Epoch {}/{}'.format(epoch,epochs))\n        print('-'*20)\n        \n        for phase in ['train', 'validation']:\n            \n            ## 1. setting up the training mode \n            if phase == 'train':\n                model.train(True)\n            else:\n                model.train(False)\n                \n            running_loss = 0.0\n            running_corrects = 0.0\n            running_batch = 0.0\n            \n            for data in dataloaders[phase]:\n                images,labels = data\n                images,labels = images.to(device), labels.to(device)\n                \n                ## zero the parameter gradients\n                optimizer.zero_grad()\n\n                # forward pass\n                output = model(images)\n                loss = loss_fn(output,labels)\n                preds = torch.argmax(output,dim=1) ## for calculating the running corrects\n\n                if phase == 'train':\n                    loss.backward()\n                    optimizer.step()\n\n                # statistics\n                running_loss += loss.item()\n                running_corrects += torch.sum(preds==labels)\n                running_batch += 1\n            \n            epoch_loss = running_loss/running_batch\n            epoch_accuracy = running_corrects/len(datasets[phase])\n            \n            # store the statistics for plotting and step lr scheduler\n            if phase == 'train':\n                train_loss.append(epoch_loss)\n                train_acc.append(epoch_accuracy)\n            else:\n                val_loss.append(epoch_loss)\n                val_acc.append(epoch_accuracy)\n                scheduler.step(epoch_accuracy) ## step the learning rate in validation phase to avoid overfitting of the dataset\n                \n            print('{} Loss {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_accuracy))\n            \n            ## save the best model\n            if phase == 'validation' and epoch_accuracy > best_acc:\n                best_acc = epoch_accuracy\n                best_model_wts = model.state_dict()\n                \n        \n                \n    time_elapsed = time.time() - start_time    \n    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed//60, time_elapsed % 60))\n    print('Best accuracy {:.4f}'.format(best_acc))\n    model.load_state_dict(best_model_wts)\n    \n    \n    ## plot the statistics\n    fig, (ax1,ax2) = plt.subplots(nrows=1,ncols=2, sharex=True)\n    ax1.plot(train_loss)\n    ax1.plot(val_loss)\n    ax1.set_title('Cross Entropy loss')\n    ax1.set_xlabel('Epochs')\n    \n    ax2.plot(train_acc)\n    ax2.plot(val_acc)\n    ax2.set_title('Accuracy (%)')\n    ax2.set_xlabel('Epochs')\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntrained_model = train(model,opt, loss_fn, device=device, epochs=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## save the model\nstate_dict = trained_model.state_dict() \ntorch.save(state_dict, 'model.pth')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 4. Evaluation on validation dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"images_per_class(valid_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_confusion_matrix(model, dataloader, device='cpu'):\n    y_true = []\n    y_preds = []\n    model = model.to(device)\n    for images,labels in tqdm_notebook(dataloader):\n        images = images.to(device)\n        y_true.extend(labels.numpy())\n        with torch.no_grad():\n            output = model(images)\n            pred = torch.argmax(output,dim=1)\n            y_preds.extend(pred.cpu().numpy())\n\n    cm = confusion_matrix(y_true,y_preds)\n    df_cm = pd.DataFrame(cm, index=list(classes.values()), columns = list(classes.values()))\n    plt.figure(figsize=(10,5))\n    ax = sns.heatmap(df_cm, annot=True, fmt='d')\n    ax.set_xlabel('Predicted')\n    ax.set_ylabel('True Labels')\n    plt.show()\n    \n    return cm\n\ndef evaluation_statistics(cm):\n    for j,plant in tqdm_notebook(classes.items()):\n        pred_positive_labels = np.sum(cm[:,j])\n        true_labels = np.sum(cm[j,:])\n        true_positives = cm[j,j]\n\n        precision = true_positives/pred_positive_labels\n        recall = true_positives/true_labels\n        f_score = 2*precision*recall/(precision+recall)\n        accuracy = true_positives/(true_labels+pred_positive_labels-true_positives)\n\n        print('{} | TP = {} | Predicted Yes = {} | True Labels = {}'.format(plant,true_positives, pred_positive_labels, true_labels))\n        print('Precision {:.2f}'.format(precision)) # Out of all predicted positive how many are actually correct\n        print('Recall {:.2f}'.format(recall)) # True Positive Rate\n        print('F_score {:.2f}'.format(f_score))\n        print('Accuracy {:.2f}'.format(accuracy)) # True Positive Rate\n        print('-'*20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# evaluate_data\ndatasets['train'] = Plant_Dataset(train_data, validation_transform)\ntrain_dataloader= DataLoader(datasets['train'], shuffle=False, batch_size=32)\n\ntrain_cm = plot_confusion_matrix(trained_model,train_dataloader,device=device)\nevaluation_statistics(train_cm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time \ndatasets['validation'] = Plant_Dataset(valid_data, validation_transform)\nval_dataloader= DataLoader(datasets['validation'], shuffle=False, batch_size=32)\n\nval_cm = plot_confusion_matrix(trained_model,val_dataloader,device=device)\nevaluation_statistics(val_cm)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Calculate the precision, recall and F-Score from the Confusion matrix**\n- Find the number of true positives which is all the diagonal elements\n- Find the number of true labes which is the row wise summation for each class\n- Find the number of predicted true which is the column wise summation for each class"},{"metadata":{},"cell_type":"markdown","source":"### 5. Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time \ntest_folder = '/kaggle/input/plant-seedlings-classification/test'\n\ndef predict_test_images(folder, model, device='cpu'):\n    classification = []\n    model = model.to(device)\n    for image_file_name in tqdm_notebook(os.listdir(folder)):\n        image_path = os.path.join(folder,image_file_name)\n        image = Image.open(image_path)\n        image_input = validation_transform(image).unsqueeze(0).to(device)\n        \n        ## prediction\n        with torch.no_grad():\n            output = model(image_input)\n            pred = torch.argmax(output,dim=1).item()\n            classification.append([image_file_name,classes[pred]])\n            \n    return classification\n\ntest_classification = predict_test_images(test_folder, trained_model, device=device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame(np.array(test_classification), columns= ['file','species'], index=np.arange(1,len(test_classification)+1))\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission.csv',index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}