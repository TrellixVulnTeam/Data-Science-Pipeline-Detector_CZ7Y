{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers.experimental import preprocessing\nfrom tensorflow.keras.preprocessing import image\n\nimport pathlib\nimport datetime as dt\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls ../input/plant-seedlings-classification","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dir = '../input/plant-seedlings-classification/'\ntrain_dir = os.path.join(data_dir, 'train')\ntest_dir = os.path.join(data_dir, 'test')\nsample_submission = pd.read_csv(os.path.join(data_dir, 'sample_submission.csv'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dir = pathlib.Path(train_dir)\nimage_count = len(list(train_dir.glob('*/*.png')))\nprint(image_count)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 32\nimg_height = 260\nimg_width = 260","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_ds = tf.keras.preprocessing.image_dataset_from_directory(train_dir,\n                                                              seed = 7,\n                                                              image_size = (img_height, img_width),\n                                                              batch_size = batch_size)\n\n'''val_ds = tf.keras.preprocessing.image_dataset_from_directory(train_dir,\n                                                            validation_split = 0.2,\n                                                            subset = \"validation\",\n                                                            seed = 7,\n                                                            image_size = (img_height, img_width),\n                                                            batch_size = batch_size)'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class_names = train_ds.class_names\nnum_classes = len(class_names)\nprint(class_names)\nprint(num_classes)\n#print(type(class_names))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10, 10))\nfor images, labels in train_ds.take(1):\n    for i in range(9):\n        ax = plt.subplot(3, 3, i + 1)\n        plt.imshow(images[i].numpy().astype(\"uint8\"))\n        plt.title(class_names[labels[i]])\n        plt.axis(\"off\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img_augmentation = Sequential(\n    [\n        preprocessing.RandomRotation(factor=0.15),\n        preprocessing.RandomFlip(),\n        preprocessing.RandomContrast(factor=0.1),\n    ],\n    name=\"img_augmentation\",\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10, 10))\nfor image, label in train_ds.take(1):\n    for i in range(9):\n        ax = plt.subplot(3, 3, i + 1)\n        aug_img = img_augmentation(image)\n        plt.imshow(aug_img[i].numpy().astype(\"uint8\"))\n        plt.title(\"{}\".format(class_names[label[i]]))\n        plt.axis(\"off\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def input_preprocess(image, label):\n    label = tf.one_hot(label, 12)\n    return image, label","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_ds = train_ds.map(\n    input_preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE\n)\n\n#val_ds = val_ds.map(input_preprocess)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"strategy = tf.distribute.MirroredStrategy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.applications import EfficientNetB2\n\n\ndef build_model(num_classes):\n    inputs = layers.Input(shape=(img_height, img_width, 3))\n    x = img_augmentation(inputs)\n    model = EfficientNetB2(include_top=False, input_tensor=x, weights=\"imagenet\")\n\n    # Freeze the pretrained weights\n    model.trainable = False\n\n    # Rebuild top\n    x = layers.GlobalAveragePooling2D(name=\"avg_pool\")(model.output)\n    x = layers.BatchNormalization()(x)\n    \n    x = layers.Dropout(0.3, name = 'drop2')(x)\n    x = layers.Dense(100, activation = \"relu\")(x)\n    x = layers.BatchNormalization()(x)\n    \n    x = layers.Dropout(0.2, name = 'drop1')(x)\n    x = layers.Dense(50, activation = \"relu\")(x)\n    x = layers.BatchNormalization()(x)\n\n    top_dropout_rate = 0.2\n    x = layers.Dropout(top_dropout_rate, name=\"top_dropout\")(x)\n    outputs = layers.Dense(num_classes, activation=\"softmax\", name=\"pred\")(x)\n\n    # Compile\n    model = tf.keras.Model(inputs, outputs, name=\"EfficientNet\")\n    optimizer = optimizer=tf.keras.optimizers.Nadam(learning_rate=1e-2)\n    model.compile(\n        optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n    )\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with strategy.scope():\n    model = build_model(num_classes=num_classes)\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = 30  # @param {type: \"slider\", min:8, max:80}\nhist = model.fit(train_ds, epochs=epochs, verbose=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_hist(hist):\n    plt.plot(hist.history[\"accuracy\"])\n    plt.title(\"model accuracy\")\n    plt.ylabel(\"accuracy\")\n    plt.xlabel(\"epoch\")\n    plt.legend([\"train\", \"validation\"], loc=\"upper left\")\n    plt.show()\n    \n    plt.plot(hist.history['loss'], label='train')\n    # plt.plot(result.history['val_loss'], label='test')\n    plt.legend(loc='upper right')\n    plt.title('Model Cost')\n    plt.ylabel('Cost')\n    plt.xlabel('Epoch')\n    plt.show()\n\nplot_hist(hist)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def unfreeze_model(model):\n    # We unfreeze the top 35 layers while leaving BatchNorm layers frozen\n    for layer in model.layers[-35:]:\n        if not isinstance(layer, layers.BatchNormalization):\n            layer.trainable = True\n\n    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n    model.compile(\n        optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n    )\n\n\nunfreeze_model(model)\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = 30  # @param {type: \"slider\", min:8, max:50}\nhist = model.fit(train_ds, epochs=epochs, verbose=2)\nplot_hist(hist)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\nmodel.compile(\n    optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n)\n\nepochs = 20  # @param {type: \"slider\", min:8, max:50}\nhist = model.fit(train_ds, epochs=epochs, verbose=2)\nplot_hist(hist)\n\noptimizer = tf.keras.optimizers.Adam(learning_rate=1e-6)\nmodel.compile(\n    optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n)\n\nepochs = 10  # @param {type: \"slider\", min:8, max:50}\nhist = model.fit(train_ds, epochs=epochs, verbose=2)\nplot_hist(hist)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = []\nfor file in os.listdir(test_dir):\n    test.append(['test/{}'.format(file), file])\ntest = pd.DataFrame(test, columns=['filepath', 'file'])\nprint(test.head(2))\ntest.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm\n\nx_test = np.zeros((len(test), img_height, img_width, 3), dtype='float32')\nfor i, filepath in tqdm(enumerate(test['filepath'])):\n    img = keras.preprocessing.image.load_img(os.path.join(data_dir, filepath), target_size = (img_height, img_width))\n    x = keras.preprocessing.image.img_to_array(img)\n    x_test[i] = x\nprint('test Images shape: {} size: {:,}'.format(x_test.shape, x_test.size))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"one_hot_prediction = model.predict(x_test)\none_hot_prediction[0:2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = np.argmax(one_hot_prediction, axis=1)\nprint(preds.shape)\nprint(preds)\nprediction = [class_names[i] for i in preds]\nprint(len(prediction))\nprint(prediction)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_submission = pd.DataFrame({'file': test['file'], 'species': prediction})\nprint(my_submission.head(5))\nprint(my_submission.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_submission.to_csv('output.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}