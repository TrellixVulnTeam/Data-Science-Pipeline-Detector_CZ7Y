{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Importing Required Libraries","metadata":{}},{"cell_type":"code","source":"from skimage.io import imread, imshow\nfrom skimage import transform\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nimport pandas as pd\nimport os\nimport random as rnd\n# importing matplotib to plot images and graphs\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\n# PyTorch libraries and modules\nimport torch\nfrom torch.autograd import Variable\nfrom torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout\nfrom torch.optim import Adam, SGD\nimport torch.nn.functional as F\nfrom skimage.color import rgb2gray\nfrom sklearn.decomposition import PCA\nimport random\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2021-08-23T00:05:20.214107Z","iopub.execute_input":"2021-08-23T00:05:20.214924Z","iopub.status.idle":"2021-08-23T00:05:23.368825Z","shell.execute_reply.started":"2021-08-23T00:05:20.214876Z","shell.execute_reply":"2021-08-23T00:05:23.367998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Plotting images of each class","metadata":{}},{"cell_type":"code","source":"%matplotlib inline\ndataset_dir = '../input/plant-seedlings-classification/train'\nsubplot_size = len([dir for dir in os.listdir(dataset_dir)])\nfig, axs = plt.subplots(subplot_size, sharey = True, figsize=(50, 50))\n\nfor ax, dir in zip(axs ,os.listdir(dataset_dir)):\n    \n    file_name = rnd.choice([file for file in os.listdir(os.path.join(dataset_dir, dir))])\n#     print(file_name)\n    img = mpimg.imread(os.path.join(dataset_dir, dir, file_name))\n    img = transform.resize(img, (100, 100))\n    ax.imshow(img)\n    # Add title and axis names\n    ax.set_title(dir)\n    ","metadata":{"execution":{"iopub.status.busy":"2021-08-23T00:05:26.783051Z","iopub.execute_input":"2021-08-23T00:05:26.783537Z","iopub.status.idle":"2021-08-23T00:05:30.145528Z","shell.execute_reply.started":"2021-08-23T00:05:26.783487Z","shell.execute_reply":"2021-08-23T00:05:30.144752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_map = {}\nid = 0\nfor dir_name in os.listdir('../input/plant-seedlings-classification/train'):\n    class_map[dir_name] = id\n    id += 1\nclass_reverse_map ={}\nid = 0\nfor dir_name in os.listdir('../input/plant-seedlings-classification/train'):\n    class_reverse_map[id] = dir_name\n    id += 1\n","metadata":{"execution":{"iopub.status.busy":"2021-08-23T00:05:30.146603Z","iopub.execute_input":"2021-08-23T00:05:30.147091Z","iopub.status.idle":"2021-08-23T00:05:30.152524Z","shell.execute_reply.started":"2021-08-23T00:05:30.147058Z","shell.execute_reply":"2021-08-23T00:05:30.151663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Applying PCA to a subset of images in training directory","metadata":{}},{"cell_type":"markdown","source":"Counting num. of image files in each directory","metadata":{}},{"cell_type":"code","source":"train_dir = '../input/plant-seedlings-classification/train'\ntrain_size = 0.85\nsub_dir = os.listdir(train_dir)\n# print(len(sub_dir))\nsub_dir_lengths ={}\ntrain_set_files = []\ntest_set_files = []\nfor sub in sub_dir:\n    full_path = os.path.join(train_dir, sub)\n    files = [os.path.join(train_dir, sub,name) for name in os.listdir(full_path)]\n    sub_dir_lengths[sub]=len(files)\n    random.shuffle(files)\n    train_set_files.extend(files[:int(train_size*sub_dir_lengths[sub])])\n    test_set_files.extend(files[int(train_size*sub_dir_lengths[sub]):])","metadata":{"execution":{"iopub.status.busy":"2021-08-23T00:05:31.735454Z","iopub.execute_input":"2021-08-23T00:05:31.735962Z","iopub.status.idle":"2021-08-23T00:05:31.761494Z","shell.execute_reply.started":"2021-08-23T00:05:31.735913Z","shell.execute_reply":"2021-08-23T00:05:31.76059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(test_set_files)","metadata":{"execution":{"iopub.status.busy":"2021-08-23T00:05:34.11856Z","iopub.execute_input":"2021-08-23T00:05:34.119049Z","iopub.status.idle":"2021-08-23T00:05:34.127829Z","shell.execute_reply.started":"2021-08-23T00:05:34.119012Z","shell.execute_reply":"2021-08-23T00:05:34.126514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for sub in os.listdir(train_dir):\n#     full_path = os.path.join(train_dir, sub)\n#     print(f'{sub} - ({len(train_set_files[sub])}, {len(test_set_files[sub])})')","metadata":{"execution":{"iopub.status.busy":"2021-08-22T18:37:01.439343Z","iopub.execute_input":"2021-08-22T18:37:01.439631Z","iopub.status.idle":"2021-08-22T18:37:01.450763Z","shell.execute_reply.started":"2021-08-22T18:37:01.439605Z","shell.execute_reply":"2021-08-22T18:37:01.449933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pca = PCA(n_components = 100)\n# train_df = pd.DataFrame(['file_path','image'])\ntrain_images = []\n# for sub_name in train_set_files.keys():\nfor file_name in train_set_files:\n    img = imread(file_name)\n    img = transform.resize(img, (100, 100, 3))\n    img = img.reshape(-1)\n    train_images.append(img)\n# train_df=pd.DataFrame(train_dict, columns = ['file_path','image'])\n# test_dict = []\n# for sub_name in test_set_files.keys():\n#     for file_name in test_set_files[sub_name]:\n#         img = imread(file_name)\n#         img = transform.resize(img, (100, 100, 3))\n#         img = img.reshape(-1)\n#         test_dict.append((file_name, img))\n# test_df = pd.DataFrame(test_dict, columns = ['file_path','image'])","metadata":{"execution":{"iopub.status.busy":"2021-08-23T00:05:37.680717Z","iopub.execute_input":"2021-08-23T00:05:37.681365Z","iopub.status.idle":"2021-08-23T00:11:22.078926Z","shell.execute_reply.started":"2021-08-23T00:05:37.681325Z","shell.execute_reply":"2021-08-23T00:11:22.077803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_images)","metadata":{"execution":{"iopub.status.busy":"2021-08-23T00:16:29.866551Z","iopub.execute_input":"2021-08-23T00:16:29.866904Z","iopub.status.idle":"2021-08-23T00:16:30.054413Z","shell.execute_reply.started":"2021-08-23T00:16:29.86687Z","shell.execute_reply":"2021-08-23T00:16:30.052975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"fit PCA instance on training dataset","metadata":{}},{"cell_type":"code","source":"images_fit = pd.DataFrame(train_images)\nimages_fit = images_fit.values.tolist()\npca.fit(images_fit)","metadata":{"execution":{"iopub.status.busy":"2021-08-23T00:16:38.253568Z","iopub.execute_input":"2021-08-23T00:16:38.253923Z","iopub.status.idle":"2021-08-23T00:18:06.03414Z","shell.execute_reply.started":"2021-08-23T00:16:38.253891Z","shell.execute_reply":"2021-08-23T00:18:06.032419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pca.explained_variance_ratio_","metadata":{"execution":{"iopub.status.busy":"2021-08-23T00:18:06.036131Z","iopub.execute_input":"2021-08-23T00:18:06.036455Z","iopub.status.idle":"2021-08-23T00:18:06.188324Z","shell.execute_reply.started":"2021-08-23T00:18:06.036423Z","shell.execute_reply":"2021-08-23T00:18:06.187006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SeedDataSet(Dataset):\n    def __init__(self, dir, transform = None):\n        \"\"\"\n        dir has all the file names from training set which will be used for training (as opposed to few for validation)\n        \"\"\"\n        self.df = pd.DataFrame(dir, columns = ['Name'])\n       #         self.df = pd.DataFrame(columns = ['Name','Labels'])\n#         for i, dir_name in enumerate(os.listdir(dir)):\n#             for name in os.listdir(os.path.join(dir, dir_name)):\n#                 labels.append(( name, i ))\n#         self.df = pd.DataFrame(labels, columns = ['Name','Labels'])\n#         self.transforms = transform\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        image_path_name = self.df.iloc[idx, 0]\n        \n        head_tail = os.path.split(image_path_name)\n        sub_dir_name = os.path.split(head_tail[0])[1]\n        \n#         label = self.df.iloc[idx, 1]\n#         path = os.path.join(self.dir, class_reverse_map[label] ,image_name)\n        img = imread(image_path_name)\n        \n        img = transform.resize(img, (100, 100, 3))\n        img = img.reshape(1, -1)\n        img = pca.transform(img)\n#         img = torch.from_numpy(img).unsqueeze(0)\n        return img, class_map[sub_dir_name]","metadata":{"execution":{"iopub.status.busy":"2021-08-23T00:18:06.191097Z","iopub.execute_input":"2021-08-23T00:18:06.191621Z","iopub.status.idle":"2021-08-23T00:18:06.201227Z","shell.execute_reply.started":"2021-08-23T00:18:06.191574Z","shell.execute_reply":"2021-08-23T00:18:06.200042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = SeedDataSet(train_set_files,transform=transforms.Resize(100))\nvalidation_dataset = SeedDataSet(test_set_files,transform=transforms.Resize(100))","metadata":{"execution":{"iopub.status.busy":"2021-08-23T00:18:06.203742Z","iopub.execute_input":"2021-08-23T00:18:06.204195Z","iopub.status.idle":"2021-08-23T00:18:06.224854Z","shell.execute_reply.started":"2021-08-23T00:18:06.20415Z","shell.execute_reply":"2021-08-23T00:18:06.223381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(validation_dataset)","metadata":{"execution":{"iopub.status.busy":"2021-08-23T00:18:06.226787Z","iopub.execute_input":"2021-08-23T00:18:06.227239Z","iopub.status.idle":"2021-08-23T00:18:06.242092Z","shell.execute_reply.started":"2021-08-23T00:18:06.227192Z","shell.execute_reply":"2021-08-23T00:18:06.2407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image, label = train_dataset[0]","metadata":{"execution":{"iopub.status.busy":"2021-08-23T00:18:06.243973Z","iopub.execute_input":"2021-08-23T00:18:06.244397Z","iopub.status.idle":"2021-08-23T00:18:06.288329Z","shell.execute_reply.started":"2021-08-23T00:18:06.244353Z","shell.execute_reply":"2021-08-23T00:18:06.286976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dir = '../input/plant-seedlings-classification/train'\n# shapes = []\n# df = pd.DataFrame(columns = ['Name','Labels'])\n# for i, dir_name in enumerate(os.listdir(dir)):\n#     for name in os.listdir(os.path.join(dir, dir_name)):\n#         img = imread(os.path.join(dir, dir_name,name))\n#         if img.ndim > 3:\n#             print(name)\n# df = pd.DataFrame(labels, columns = ['Name','Labels'])","metadata":{"execution":{"iopub.status.busy":"2021-08-22T18:44:25.664758Z","iopub.execute_input":"2021-08-22T18:44:25.665214Z","iopub.status.idle":"2021-08-22T18:44:25.67052Z","shell.execute_reply.started":"2021-08-22T18:44:25.665171Z","shell.execute_reply":"2021-08-22T18:44:25.66908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_size = int(0.8 * len(dataset))\n# test_size = len(dataset) - train_size\n# train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])","metadata":{"execution":{"iopub.status.busy":"2021-08-22T18:44:25.672443Z","iopub.execute_input":"2021-08-22T18:44:25.673901Z","iopub.status.idle":"2021-08-22T18:44:25.680972Z","shell.execute_reply.started":"2021-08-22T18:44:25.673846Z","shell.execute_reply":"2021-08-22T18:44:25.679397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Image dimensions","metadata":{}},{"cell_type":"code","source":"image.shape","metadata":{"execution":{"iopub.status.busy":"2021-08-23T00:18:06.28999Z","iopub.execute_input":"2021-08-23T00:18:06.290391Z","iopub.status.idle":"2021-08-23T00:18:06.297395Z","shell.execute_reply.started":"2021-08-23T00:18:06.290339Z","shell.execute_reply":"2021-08-23T00:18:06.296506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loader = DataLoader(train_dataset, batch_size = 100, shuffle = True)\nval_loader = DataLoader(validation_dataset, batch_size = 100)","metadata":{"execution":{"iopub.status.busy":"2021-08-23T00:18:12.29115Z","iopub.execute_input":"2021-08-23T00:18:12.291463Z","iopub.status.idle":"2021-08-23T00:18:12.298355Z","shell.execute_reply.started":"2021-08-23T00:18:12.291436Z","shell.execute_reply":"2021-08-23T00:18:12.297071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Defining a NN Model","metadata":{}},{"cell_type":"code","source":"class CNN(torch.nn.Module):\n    def __init__(self):\n        super(CNN, self).__init__()\n#         self.conv_layer1 = torch.nn.Sequential(\n#             torch.nn.Conv2d(in_channels = 1, out_channels= 10, kernel_size = 11), \n#             torch.nn.ReLU(),\n#             torch.nn.MaxPool2d(kernel_size =2, stride = 2)\n#         )\n#         torch.nn.init.xavier_uniform_(self.conv_layer1.weight)\n#         torch.nn.init.xavier_uniform_(self.conv_layer1.bias)\n        \n#         self.conv_layer2 = torch.nn.Sequential(\n#         torch.nn.Conv2d(in_channels = 10, out_channels = 20, kernel_size = 10), \n#         torch.nn.ReLU(),\n#         torch.nn.MaxPool2d(kernel_size = 2, stride = 2)\n#         )\n#         torch.nn.init.xavier_uniform_(self.conv_layer2.weight)\n#         torch.nn.init.xavier_uniform_(self.conv_layer2.bias)\n        \n        self.fc1 = torch.nn.Linear(100, 25)\n        self.fc2 = torch.nn.Linear(25, 12)\n        \n    def forward(self, x):\n#         out = self.conv_layer1(x)\n#         out = self.conv_layer2(out)\n#         out = out.contiguous().view(out.size(0), -1)\n        out = F.relu(self.fc1(x))\n        out = self.fc2(out)\n        return out","metadata":{"execution":{"iopub.status.busy":"2021-08-23T00:18:23.943471Z","iopub.execute_input":"2021-08-23T00:18:23.943792Z","iopub.status.idle":"2021-08-23T00:18:23.95178Z","shell.execute_reply.started":"2021-08-23T00:18:23.943761Z","shell.execute_reply":"2021-08-23T00:18:23.951035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# defining the model\nmodel = CNN()\n# defining the optimizer\noptimizer = Adam(model.parameters(), lr=0.07)\n# defining the loss function\ncriterion = CrossEntropyLoss()\n# checking if GPU is available\nif torch.cuda.is_available():\n    model = model.cuda()\n    criterion = criterion.cuda()\n    \nprint(model)","metadata":{"execution":{"iopub.status.busy":"2021-08-23T00:18:27.316023Z","iopub.execute_input":"2021-08-23T00:18:27.316488Z","iopub.status.idle":"2021-08-23T00:18:27.35467Z","shell.execute_reply.started":"2021-08-23T00:18:27.316456Z","shell.execute_reply":"2021-08-23T00:18:27.353458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learning_rate = 0.01\nloss_fn = torch.nn.CrossEntropyLoss()    # Softmax is internally computed.\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)","metadata":{"execution":{"iopub.status.busy":"2021-08-23T00:18:30.354451Z","iopub.execute_input":"2021-08-23T00:18:30.354775Z","iopub.status.idle":"2021-08-23T00:18:30.359595Z","shell.execute_reply.started":"2021-08-23T00:18:30.354743Z","shell.execute_reply":"2021-08-23T00:18:30.358363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_loop(dataloader, model, loss_fn, optimizer, epoch):\n    size = len(dataloader.dataset)\n    epoch_loss = 0\n    correct = 0\n    num_batches = len(dataloader)\n    for batch, (X, y) in enumerate(dataloader):\n        # Compute prediction and loss\n#         X = X.permute(0,1,2)\n        X = X.float()\n        if torch.cuda.is_available():\n            X = X.cuda()\n            y = y.cuda()\n        pred = model(X)\n        pred = pred.squeeze()\n        y = y.long()\n        loss = loss_fn(pred, y)\n\n        # Backpropagation\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        \n        \n        \n        loss, current = loss.item(), batch * len(X)\n        \n        print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n        epoch_loss += loss/num_batches\n        correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n    train_loss[epoch] = epoch_loss\n    train_acc[epoch] = correct\n    \n\ndef test_loop(dataloader, model, loss_fn, epoch):\n    size = len(dataloader.dataset)\n    num_batches = len(dataloader)\n    test_loss, correct = 0, 0\n\n    with torch.no_grad():\n        for X, y in dataloader:\n#             X = X.permute(0,1,2)\n            X = X.float()\n            if torch.cuda.is_available():\n                X = X.cuda()\n                y = y.cuda()\n            pred = model(X)\n            pred = pred.squeeze()\n            y = y.long()\n#             pred = F.one_hot(pred)\n            test_loss += loss_fn(pred, y).item()\n            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n\n    test_loss /= num_batches\n    val_loss[epoch] = test_loss\n    correct /= size\n    val_acc[epoch] = correct\n    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")","metadata":{"execution":{"iopub.status.busy":"2021-08-23T00:20:15.804592Z","iopub.execute_input":"2021-08-23T00:20:15.804947Z","iopub.status.idle":"2021-08-23T00:20:15.815804Z","shell.execute_reply.started":"2021-08-23T00:20:15.804912Z","shell.execute_reply":"2021-08-23T00:20:15.814783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if torch.cuda.is_available():\n    loss_fn = loss_fn.cuda()\n\nepochs = 10\ntrain_loss, val_loss, train_acc, val_acc = [0]*epochs, [0]*epochs, [0]*epochs, [0]*epochs\nfor t in range(epochs):\n    print(f\"Epoch {t+1}\\n-------------------------------\")\n    train_loop(train_loader, model, loss_fn, optimizer,  t)\n    test_loop(val_loader, model, loss_fn,t)\nprint(\"Done!\")","metadata":{"execution":{"iopub.status.busy":"2021-08-23T00:20:18.865036Z","iopub.execute_input":"2021-08-23T00:20:18.865528Z","iopub.status.idle":"2021-08-23T01:25:29.626975Z","shell.execute_reply.started":"2021-08-23T00:20:18.865489Z","shell.execute_reply":"2021-08-23T01:25:29.625746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Plotting loss graphs","metadata":{}},{"cell_type":"code","source":"x1 = list(range(epochs))\ny1 = train_loss\ny2 = val_loss\nfig, (ax0, ax1) = plt.subplots(2, sharex= True)\nax0.plot(x1, y1)\nax0.plot(x1, y2)\nax0.legend(['training loss','validation loss'])\ny1 = train_acc\ny2 = val_acc\nax1.plot(x1, y1)\nax1.plot(x1, y2)\nax1.legend(['training accuracy','validation accuracy'])\n\n","metadata":{"execution":{"iopub.status.busy":"2021-08-23T01:31:15.924405Z","iopub.execute_input":"2021-08-23T01:31:15.924905Z","iopub.status.idle":"2021-08-23T01:31:16.205182Z","shell.execute_reply.started":"2021-08-23T01:31:15.924856Z","shell.execute_reply":"2021-08-23T01:31:16.204138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TestDataset(Dataset):\n    def __init__(self, dir):\n        self.dir = dir\n        \n        names = [name for name in os.listdir(dir)]\n        self.df = pd.DataFrame(names, columns = ['Name'])\n    def __len__(self):\n        return len([name for name in os.listdir(self.dir)])\n    def __getitem__(self,idx):\n        \n        img_name = self.df.iloc[idx,0]\n        image = imread(os.path.join(self.dir, img_name))\n        \n#         img = imread(image_path_name)\n        \n        img = transform.resize(image, (100, 100, 3))\n        img = img.reshape(1, -1)\n        img = pca.transform(img)\n        \n        \n        return img, img_name","metadata":{"execution":{"iopub.status.busy":"2021-08-23T01:31:33.325911Z","iopub.execute_input":"2021-08-23T01:31:33.326293Z","iopub.status.idle":"2021-08-23T01:31:33.333893Z","shell.execute_reply.started":"2021-08-23T01:31:33.326258Z","shell.execute_reply":"2021-08-23T01:31:33.332606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_Dataset=  TestDataset('../input/plant-seedlings-classification/test')","metadata":{"execution":{"iopub.status.busy":"2021-08-23T01:31:36.155884Z","iopub.execute_input":"2021-08-23T01:31:36.156238Z","iopub.status.idle":"2021-08-23T01:31:36.216099Z","shell.execute_reply.started":"2021-08-23T01:31:36.156203Z","shell.execute_reply":"2021-08-23T01:31:36.215076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_loader = DataLoader(test_Dataset, batch_size = 1)","metadata":{"execution":{"iopub.status.busy":"2021-08-23T01:31:37.472489Z","iopub.execute_input":"2021-08-23T01:31:37.472825Z","iopub.status.idle":"2021-08-23T01:31:37.476852Z","shell.execute_reply.started":"2021-08-23T01:31:37.47278Z","shell.execute_reply":"2021-08-23T01:31:37.47598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_dict = []\nfor i, (img, img_name) in enumerate(test_loader):\n    img_name = img_name[0]\n    \n    with torch.no_grad():\n        img = img.float()\n#         img = img.cuda()\n        output = model(img)\n\n        output = torch.argmax(output).cpu().numpy().item()\n        pred_dict.append((img_name, class_reverse_map[output]))\n        \ndf = pd.DataFrame(pred_dict, columns = ['file','species'])","metadata":{"execution":{"iopub.status.busy":"2021-08-23T01:31:39.251462Z","iopub.execute_input":"2021-08-23T01:31:39.252057Z","iopub.status.idle":"2021-08-23T01:32:09.197341Z","shell.execute_reply.started":"2021-08-23T01:31:39.252009Z","shell.execute_reply":"2021-08-23T01:32:09.196165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.to_csv('sample_submission.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2021-08-23T01:32:09.199161Z","iopub.execute_input":"2021-08-23T01:32:09.199753Z","iopub.status.idle":"2021-08-23T01:32:09.216329Z","shell.execute_reply.started":"2021-08-23T01:32:09.199703Z","shell.execute_reply":"2021-08-23T01:32:09.215229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}