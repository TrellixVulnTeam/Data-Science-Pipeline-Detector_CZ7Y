{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n%matplotlib inline \nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport datetime as dt\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom keras.applications import xception\nfrom keras.preprocessing import image\nfrom mpl_toolkits.axes_grid1 import ImageGrid\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom tqdm import tqdm\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\" \nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"start = dt.datetime.now()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls ../input/keras-pretrained-models/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#이미 학습된 케라스 모델(Keras Pretrained Model) 중에서 선택하여 적용할 것.\n# copy the pretrained models to the cache directroy\ncache_dir = os.path.expanduser(os.path.join('~', '.keras')) # expanduser: ~를 사용자의 홈 디렉토리 절대경로로 바꿈\n\nif not os.path.exists(cache_dir):\n    os.makedirs(cache_dir)\nmodels_dir = os.path.join(cache_dir, 'models')\nif not os.path.exists(models_dir):\n    os.makedirs(models_dir)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!cp ../input/keras-pretrained-models/xception* ~/.keras/models/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls ~/.keras/models","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"CATEGORIES = ['Black-grass', 'Charlock', 'Cleavers', 'Common Chickweed', 'Common wheat', 'Fat Hen', 'Loose Silky-bent',\n              'Maize', 'Scentless Mayweed', 'Shepherds Purse', 'Small-flowered Cranesbill', 'Sugar beet']\nNUM_CATEGORIES = len(CATEGORIES)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SAMPLE_PER_CATEGORY = 200\nSEED = 1987\ndata_dir = '../input/plant-seedlings-classification/'\ntrain_dir = os.path.join(data_dir, 'train')\ntest_dir = os.path.join(data_dir, 'test')\nsample_submission = pd.read_csv(os.path.join(data_dir, 'sample_submission.csv'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission.head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"train data에 각 식물이 몇개씩 있는지 확인."},{"metadata":{"trusted":true},"cell_type":"code","source":"for category in CATEGORIES:\n    print('{} {} images'.format(category, len(os.listdir(os.path.join(train_dir, category)))))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = []\nfor category_id, category in enumerate(CATEGORIES):\n    for file in os.listdir(os.path.join(train_dir, category)):\n        train.append(['train/{}/{}'.format(category, file), category_id, category])\ntrain = pd.DataFrame(train, columns=['file', 'category_id', 'category'])\ntrain.head(2)\ntrain.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"pd.concat() : 동일한 형태의 Dataframe을 합침.\nSAMPLE_PER_CATEGORY = 200 이므로, 카테고리가 c인 element에서 각각 200개씩 빼서 샘플을 만든다.\n\ntrain.sample(frac=1): shuffle할때쓰이는 함수. frac = fraction of rows to return in the random sample\n\nnp.arange(): range()함수의 nummpy버전. 0~(n-1)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.concat([train[train['category'] == c][:SAMPLE_PER_CATEGORY] for c in CATEGORIES])\ntrain = train.sample(frac=1)\ntrain.index = np.arange(len(train))\ntrain.head(2)\ntrain.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = []\nfor file in os.listdir(test_dir):\n    test.append(['test/{}'.format(file), file])\n\ntest = pd.DataFrame(test, columns = ['filepath', 'file'])\ntest.head(2)\ntest.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_img(filepath, size):\n    img = image.load_img(os.path.join(data_dir, filepath), target_size = size)\n    img = image.img_to_array(img)\n    return img","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"plt.figure : new figure(그림이나 도형)을 생성.\nImageGrid에서 111은 축의 위치를 나타내는 숫자."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(1, figsize = (NUM_CATEGORIES, NUM_CATEGORIES))\ngrid = ImageGrid(fig, 111, nrows_ncols=(NUM_CATEGORIES, NUM_CATEGORIES), axes_pad=0.05)\ni = 0\nfor category_id, category in enumerate(CATEGORIES):\n    for filepath in train[train['category'] == category]['file'].values[:NUM_CATEGORIES]:\n        ax = grid[i]\n        img = read_img(filepath, (224,224))\n        ax.imshow(img / 255.)\n        ax.axis('off')\n        if i % NUM_CATEGORIES == NUM_CATEGORIES - 1:\n            ax.text(250, 112,filepath.split('/')[1], verticalalignment='center')\n        i += 1\n\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"validation set 만들기\n\nnp.random.random(n)은 0이상 1미만의 실수를 n의 사이즈만큼 리턴함.\n\ntrain_idx는 각 원소들이 0.8보다 작은지 보고 boolean을 리턴함.\n\ntrain.loc[train_idx,'category_id'].values : train_idx에서 참인 frame만 뽑아오는데 이때, 열이 'category_id'인 것들의 values만 뽑아오게 됨.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.seed(seed=SEED)\nrnd = np.random.random(len(train))\ntrain_idx = rnd < 0.8\nvalid_idx = rnd >= 0.8\nytr = train.loc[train_idx, 'category_id'].values\nyv = train.loc[valid_idx, 'category_id'].values\nlen(ytr), len(yv)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"tqdm : for문에서의 진행상태를 보여줌."},{"metadata":{},"cell_type":"markdown","source":"XCeption model\nhttps://datascienceschool.net/view-notebook/0faaf59e0fcd455f92c1b9a1107958c4/\n\nDepth-wise-separable convolution: 각 채널별로 conv하고, 그 결과에 1x1 conv 연산.\n각 채널별로 feature map을 하나씩만들고(conv), feature map 의 수를 조정(1x1).\n\n비선형 activation 함수가 없음.\n\nResNet의 skip connection도 적용됨. maxpool 다음에 위치해 있어서 이전입력에 대해 크기를 줄이는 연산을 함. 1x1 convolution에서는 stride = 2로 함.\n"},{"metadata":{},"cell_type":"markdown","source":"expand_dims():axis=0이면 제일 앞에있는 차원이 추가되어 확장된다."},{"metadata":{"trusted":true},"cell_type":"code","source":"INPUT_SIZE = 299\nPOOLING = 'avg'\nx_train = np.zeros((len(train), INPUT_SIZE, INPUT_SIZE, 3), dtype='float32')\nfor i, file in tqdm(enumerate(train['file'])):\n    img = read_img(file, (INPUT_SIZE, INPUT_SIZE))\n    x = xception.preprocess_input(np.expand_dims(img.copy(),axis=0))\n    x_train[i] = x\nprint('Train Images Shape : {} size: {:,}'.format(x_train.shape, x_train.size))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"bottleneck feature\nCNN의 앞쪽 레이어에서 특징추출하고, 뒤쪽레이어에서 그 특징을 가지고 이미지를 분류.\n\n앞쪽 레이어를 통과시켜 추출된 특징 = bottleneck feature\n\n{:,} : 서식지정자. 숫자 사이에 ,가 들어갈것.\n\nverbose는 학습 중 출력되는 문구 설정하는 것.\ninclude_top 은 마지막 layer를 선택할것인지 말지를 결정."},{"metadata":{"trusted":true},"cell_type":"code","source":"Xtr = x_train[train_idx]\nXv = x_train[valid_idx]\nprint((Xtr.shape, Xv.shape, ytr.shape, yv.shape))\nxception_bottleneck = xception.Xception(weights='imagenet', include_top=False, pooling=POOLING)\ntrain_x_bf = xception_bottleneck.predict(Xtr, batch_size=32, verbose=1)\nvalid_x_bf = xception_bottleneck.predict(Xv, batch_size=32, verbose=1)\nprint('Xception train bottleneck features shape: {} size: {:,}'.format(train_x_bf.shape, train_x_bf.size))\nprint('Xception valid bottleneck features shape: {} size: {:,}'.format(valid_x_bf.shape, valid_x_bf.size))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"multinomial : 범주가 3개이상이고 명목형(nomial)일때.\n\nmulti_class = {'ovr', 'multinomial', 'auto'} 디폴트는 ovr(이진형}. auto는 자동\nsolver : optimization알고리즘 선택. lbfgs는 L2"},{"metadata":{},"cell_type":"markdown","source":"특징추출로 bottleneck을 쓰고, 분류로는 logistic regression을 씀.\n\nlogreg.fit() -> 회귀분석 적용\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"logreg = LogisticRegression(multi_class='multinomial', solver='lbfgs', random_state=SEED)\nlogreg.fit(train_x_bf, ytr)\nvalid_probs = logreg.predict_proba(valid_x_bf)\nvalid_preds = logreg.predict(valid_x_bf)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Validation Xception Accuracy {}'.format(accuracy_score(yv, valid_preds)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Confusion Matrix: 성능지표를 나타낸 행렬(생략)"},{"metadata":{"trusted":true},"cell_type":"code","source":"cnf_matrix = confusion_matrix(yv, valid_preds)\nabbreviation = ['BG', 'Ch', 'Cl', 'CC', 'CW', 'FH', 'LSB', 'M', 'SM', 'SP', 'SFC', 'SB']\npd.DataFrame({'class': CATEGORIES, 'abbreviation': abbreviation})\nfig, ax = plt.subplots(1)\nax = sns.heatmap(cnf_matrix, ax=ax, cmap=plt.cm.Greens, annot=True)\nax.set_xticklabels(abbreviation)\nax.set_yticklabels(abbreviation)\nplt.title('Confusion Matrix')\nplt.ylabel('True class')\nplt.xlabel('Predicted class')\nfig.savefig('Confusion matrix.png', dpi=300)\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"TEST"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test = np.zeros((len(test), INPUT_SIZE, INPUT_SIZE, 3), dtype='float32')\nfor i, filepath in tqdm(enumerate(test['filepath'])):\n    img = read_img(filepath, (INPUT_SIZE, INPUT_SIZE))\n    x = xception.preprocess_input(np.expand_dims(img.copy(), axis=0))\n    x_test[i] = x\nprint('test Images shape: {} size: {:,}'.format(x_test.shape, x_test.size))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_x_bf = xception_bottleneck.predict(x_test, batch_size=32, verbose=1)\nprint('Xception test bottleneck features shape: {} size: {:,}'.format(test_x_bf.shape, test_x_bf.size))\ntest_preds = logreg.predict(test_x_bf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['category_id'] = test_preds\ntest['species'] = [CATEGORIES[c] for c in test_preds]\ntest[['file', 'species']].to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"end = dt.datetime.now()\nprint('Total time {} s.'.format((end - start).seconds))\nprint('We almost used the one hour time limit.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}