{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#importing necessary libraries\n%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.axes_grid1 import ImageGrid\nplt.rcParams['figure.figsize'] = [16, 10]\nplt.rcParams['font.size'] = 16\n\nimport os\nfrom tqdm import tqdm \n\nimport seaborn as sns\nfrom keras.preprocessing import image\nfrom keras.applications import xception\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\n# Our data files are available in the \"../input/\" directory.\nprint(os.listdir(\"../input\"))\n# For Kaggle kernel purposes any results we write to the current directory is saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = 'all'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Display the pretrained models that we have prepared in our file directory\n!ls ../input/keras-pretrained-models/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create keras cache directories in Kaggle Kernels to load the pretrained models into\ncache_dir = os.path.expanduser(os.path.join('~', '.keras')) # Cache directory\nif not os.path.exists(cache_dir):\n    os.makedirs(cache_dir)\nmodels_dir = os.path.join(cache_dir, 'models') # Models directory\nif not os.path.exists(models_dir):\n    os.makedirs(models_dir)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Copy a selection of our pretrained models files onto the keras cache directory so Keras can access them\n!cp ../input/keras-pretrained-models/xception* ~/.keras/models","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Display our pretrained models that are located in the keras cache directory\n!ls ~/.keras/models","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls ../input/plant-seedlings-classification","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define Y-labels and NUM_CLASSES\nCATEGORIES = ['Black-grass', 'Charlock', 'Cleavers', 'Common Chickweed', 'Common wheat', 'Fat Hen', 'Loose Silky-bent',\n             'Maize', 'Scentless Mayweed', 'Shepherds Purse', 'Small-flowered Cranesbill', 'Sugar beet']\nNUM_CATEGORIES = len(CATEGORIES)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SAMPLE_PER_CATEGORY = 200\nSEED = 7\ndata_dir = '../input/plant-seedlings-classification/'\ntrain_dir = os.path.join(data_dir, 'train')\ntest_dir = os.path.join(data_dir, 'test')\nsample_submission = pd.read_csv(os.path.join(data_dir, 'sample_submission.csv'))\n\nprint(train_dir)\nprint(test_dir)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Displaying the training data\n#The training images are organized into sub-folders within the main folder,\n# organized by plant species. Hence, we are simply calling each directory name and printing out their lengths\n\nfor category in CATEGORIES:\n    print('{} {} images'.format(category, len(os.listdir(os.path.join(train_dir, category)))))\n    # \"Print length of this directory -- an integer output\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We are going to do a traversal over the directories and folders containing the training set data in order \n# to collate all the image-files and their corresponding class index and class_names into an aggregate training-set\n# collection, and convert it into a pandas DataFrame.\n\ntrain = []\nfor category_id, category in enumerate(CATEGORIES): \n    for file in os.listdir(os.path.join(train_dir, category)): \n        train.append(['train/{}/{}'.format(category, file), category_id, category]) \ntrain = pd.DataFrame(train, columns = ['file', 'category_id', 'category']) # Define a pandas DataFrame over training data\ntrain.head(5) # Print preview of the training DataFrame\ntrain.shape # Check shape: should be of dims (m, 3), where 3 represents file_name, category_id (int index), and class_name\n# for each example","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remember: as this point, \"train\" is a pandas DataFrame object\n\ntrain = pd.concat([train[train['category'] == c][:SAMPLE_PER_CATEGORY] for c in CATEGORIES])\ntrain = train.sample(frac=1) # pandas function for returning a random sample of items from an axis\n\ntrain.index = np.arange(len(train)) # This specifies the DataFrame's index (the leftmost \"column\" counter for m)\ntrain.head(5)\ntrain.shape ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Similar procedure to creating the training set\n# Remember, the test examples are NOT labeled; the labels are not provided for competition purposes\n# So, the purpose of this is primarily to collate all the test examples into a neatly organized pandas DataFrame with\n# appropriate headers\n\n\n\ntest = []\nfor file in os.listdir(test_dir):\n    test.append(['test/{}'.format(file), file])\ntest = pd.DataFrame(test, columns=['filepath', 'file'])\ntest.head(5)\ntest.shape # We would expect (m, 2) with m being the number of test examples, and 2 being the filepath and file columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Image is a keras.preprocessing object containing function for preprocessing images for use in keras / tf models\n# Essentially, converting images into their corresponding 3-D numpy arrays\n\n# The data_dir is the root-project directory, and the filepaths we have set up nicely when preparing our datasets-in-name\n# Thus here, all we have to do is concat the filepaths, and the function will spit out the image file's array format\ndef read_img(filepath, size):\n    img = image.load_img(os.path.join(data_dir, filepath), target_size = size)\n    img = image.img_to_array(img)\n    return img","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using matplotlib for this\n\nfig = plt.figure(1, figsize=(NUM_CATEGORIES, NUM_CATEGORIES)) # Displaying a square matrix with num_categories number of\n# images for each category, across all categories\ngrid = ImageGrid(fig, 111, nrows_ncols=(NUM_CATEGORIES, NUM_CATEGORIES), axes_pad=0.05) # Set-up grid using 'fig'\ni = 0 # Initialize counter\n\n# Iterate through the files in the categories\nfor category_id, category in enumerate(CATEGORIES):\n    for filepath in train[train['category'] == category]['file'].values[:NUM_CATEGORIES]:\n        ax = grid[i]\n        img = read_img(filepath, (224,224)) # read_img function call; filepath specified, img_size hard-coded\n        ax.imshow(img/255.)\n        ax.axis('off')\n        if i % NUM_CATEGORIES == NUM_CATEGORIES - 1: # Labeling the row-categories (I believe)\n            ax.text(250, 112, filepath.split('/')[1], verticalalignment='center')\n        i += 1\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# A bit more sophisticated / randomized method of splitting train-dev than simply picking the split index to be\n# len(trainset) * split_percentage\n\nnp.random.seed(seed=SEED)\nrnd = np.random.random(len(train)) # Returns a list of len(train) of 'continuous uniform'\n# random distribution floats b/t [0.0, 1.0)\ntrain_idx = rnd < 0.8 # Indices in which rnd is <0.8 (which should come out to roughly 80% of the dataset)\nvalid_idx = rnd >= 0.8\nytr = train.loc[train_idx, 'category_id'].values # pandas function calls\nyv = train.loc[valid_idx, 'category_id'].values\nlen(ytr)\nlen(yv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Specify parameters:\nINPUT_SIZE = 299\nPOOLING = 'avg'\nx_train = np.zeros((len(train), INPUT_SIZE, INPUT_SIZE, 3), dtype='float32')\n# Initialize aggregate trainset object of shape (m_total, height, width, channels)\n\n# Fill the numpy array with image files converted into their image-3D arrays\nfor i, file in tqdm(enumerate(train['file'])): # tqdm is a progress bar\n    img = read_img(file, (INPUT_SIZE, INPUT_SIZE)) # Read the filepath into an array via our function call\n    x = xception.preprocess_input(np.expand_dims(img.copy(), axis=0)) # Pre-process that into a format for Xception model\n    x_train[i] = x # Set the i-th example in our initialized zero-4D-array to the particular example\nprint('Train Images shape: {} size: {:,}'.format(x_train.shape, x_train.size))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split X into training and validation, now that we have loaded the actual image arrays into x_train\n\nXtr = x_train[train_idx]\nXv = x_train[valid_idx]\nprint((Xtr.shape, Xv.shape, ytr.shape, yv.shape)) # Print shapes to confirm dims are correct","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Forward propagation through pre-trained Xception model for feature-extraction\n# Note: No classification yet in this step\n\nxception_bottleneck = xception.Xception(weights='imagenet', include_top=False, pooling=POOLING) # Define Xception object\n    # based on \"off-the-shelf\" pre-trained Xception model\ntrain_x_bf = xception_bottleneck.predict(Xtr, batch_size=32, verbose=1) # Fwdprop through Xception for feature-extraction\nvalid_x_bf = xception_bottleneck.predict(Xv, batch_size=32, verbose=1)\n\n# Check output dims:\nprint(\"Xception train bottleneck-features shape: {} size: {:,}\".format(train_x_bf.shape, train_x_bf.size))\nprint(\"Xception valid bottleneck-features shape: {} size: {:,}\".format(valid_x_bf.shape, valid_x_bf.size))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define logistic regression object (that we have imported via sk-learn)\n\nlogreg = LogisticRegression(multi_class='multinomial', solver='lbfgs', random_state=SEED)\nlogreg.fit(train_x_bf, ytr) # We need to fit the classifier to our (X,Y pairs)\nvalid_probs = logreg.predict_proba(valid_x_bf) # Classification on our dev set -- probabilities of various classes\nvalid_preds = logreg.predict(valid_x_bf) # Classification on our dev set -- predicted classes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# accuracy_score is an object we've imported from sk-learn\n\nprint(\"Validation Xception Accuracy: {}\".format(accuracy_score(yv, valid_preds)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cnf_matrix = confusion_matrix(yv, valid_preds) # Confusion matrix imported from sk-learn\nabbreviation = ['BG', 'Ch', 'Cl', 'CC', 'CW', 'FH', 'LSB', 'M', 'SM', 'SP', 'SFC', 'SB']\npd.DataFrame({'class': CATEGORIES, 'abbreviation': abbreviation})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting the confusion matrix to illustrate correct and incorrect predictions\n\nfig, ax = plt.subplots(1)\nax = sns.heatmap(cnf_matrix, ax=ax, cmap=plt.cm.Greens, annot=True)\nax.set_xticklabels(abbreviation)\nax.set_yticklabels(abbreviation)\nplt.title('Confusion Matrix')\nplt.ylabel('True Class')\nplt.xlabel('Predicted Class')\nfig.savefig('Confusion matrix.png', dpi=300)\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating the X input objects for the test data\n# (similar to how we constructed the 4D input x_train from the filepath object)\n\nx_test = np.zeros((len(test), INPUT_SIZE, INPUT_SIZE, 3), dtype='float32')\nfor i, filepath in tqdm(enumerate(test['filepath'])):\n    img = read_img(filepath, (INPUT_SIZE, INPUT_SIZE))\n    x = xception.preprocess_input(np.expand_dims(img.copy(), axis=0))\n    x_test[i] = x\nprint(\"Test images dataset shape: {} size: {:,}\".format(x_test.shape, x_test.size))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Run forwardprop on the test set input through Xception to get encoded-feature-representation\n\ntest_x_bf = xception_bottleneck.predict(x_test, batch_size=32, verbose=1)\nprint('Xception test bottleneck features shape: {} size: {:,}'.format(test_x_bf.shape, test_x_bf.size))\n\n# Run encoded-feature-representations through the Logistic-Regression classifier (by sk-learn)\n\ntest_preds = logreg.predict(test_x_bf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Note: test_preds have shape of (794, 1) that corresponds to the class for each of the test examples\n\n# Creating the submission file\n\ntest['category_id'] = test_preds\ntest['species'] = [CATEGORIES[c] for c in  test_preds]\ntest[['file', 'species']].to_csv('submission_3027.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}