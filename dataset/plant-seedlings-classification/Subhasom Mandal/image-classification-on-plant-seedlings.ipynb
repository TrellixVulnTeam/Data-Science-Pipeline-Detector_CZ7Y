{"cells":[{"metadata":{"trusted":true,"_uuid":"355ee592eddf093c31e616ac3750a3f2c5440853"},"cell_type":"code","source":"%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.axes_grid1 import ImageGrid\nplt.rcParams['figure.figsize'] = [16, 10]\nplt.rcParams['font.size'] = 16\n\nimport os\nfrom tqdm import tqdm # Fancy progress bars\n\nimport seaborn as sns\nfrom keras.preprocessing import image\nfrom keras.applications import xception\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b28708aa5e6dfa5a56a545764e8701761a26d28e"},"cell_type":"code","source":"from IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = 'all'","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ada160760c408d29828923b70ba1c8950cdbd0f1"},"cell_type":"markdown","source":"Loading up Keras Models"},{"metadata":{"trusted":true,"_uuid":"240b52e516c8d3a79f34840ef2eb2d9826e1f2fa"},"cell_type":"code","source":"!ls ../input/keras-pretrained-models/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"11fdc16299215acc12f5712286ae854caa977e96"},"cell_type":"code","source":"cache_dir = os.path.expanduser(os.path.join('~', '.keras')) # Cache\nif not os.path.exists(cache_dir):\n    os.makedirs(cache_dir)\nmodels_dir = os.path.join(cache_dir, 'models')\nif not os.path.exists(models_dir):\n    os.makedirs(models_dir)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Copy a selection of our pretrained models files onto the keras cache directory so Keras can access them"},{"metadata":{"trusted":true,"_uuid":"adfa2dd3cd50fe6da40e4f3c5340c9864404a7d7"},"cell_type":"code","source":"!cp ../input/keras-pretrained-models/xception* ~/.keras/models","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a0b80367d9f80f64849d2d3714651ad089291324"},"cell_type":"code","source":"!ls ~/.keras/models","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5780c320f3d536e518d07813ce1449c5c9463fc6"},"cell_type":"code","source":"!ls ../input/plant-seedlings-classification","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9ef643494e498a1ba99261b11d7237ba711d5be2"},"cell_type":"markdown","source":"Preparing the Dataset for the Model"},{"metadata":{"trusted":true,"_uuid":"0f0e19f7daaab0b7cafb380ab8b999073349e131"},"cell_type":"code","source":"# Define Y-labels and NUM_CLASSES\nCATEGORIES = ['Black-grass', 'Charlock', 'Cleavers', 'Common Chickweed', 'Common wheat', 'Fat Hen', 'Loose Silky-bent',\n             'Maize', 'Scentless Mayweed', 'Shepherds Purse', 'Small-flowered Cranesbill', 'Sugar beet']\nNUM_CATEGORIES = len(CATEGORIES)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3f7a7ab68159fc89b5544a44e197d732f6c3445b"},"cell_type":"code","source":"SAMPLE_PER_CATEGORY = 200\nSEED = 7\ndata_dir = '../input/plant-seedlings-classification/'\ntrain_dir = os.path.join(data_dir, 'train')\ntest_dir = os.path.join(data_dir, 'test')\nsample_submission = pd.read_csv(os.path.join(data_dir, 'sample_submission.csv'))\n\nprint(train_dir)\nprint(test_dir)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dfc4215861ec1a10ed3fcef72436efaf219f51e8"},"cell_type":"code","source":"sample_submission.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cc58c47cb57fa6d4b56e9a6dd6647160efc5ff03"},"cell_type":"code","source":"# Displaying the training data: Note that the training images are organized into sub-folders within the main folder,\n# organized by plant species. Hence, we are simply calling each directory name and printing out their lengths\n\nfor category in CATEGORIES:\n    print('{} {} images'.format(category, len(os.listdir(os.path.join(train_dir, category)))))\n    # \"Print length of this directory -- an integer output\"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f219ad27bfb6474f84c1a9d8d5677efaa4e7c095"},"cell_type":"markdown","source":"**Creating Our Aggregate Training Sample for the CNN:**"},{"metadata":{"trusted":true,"_uuid":"5f8b156302cfc22566a30b95b0adad2b39afbd4a"},"cell_type":"code","source":"# We are going to do a traversal over the directories and folders containing the training set data in order \n# to collate all the image-files and their corresponding class index and class_names into an aggregate training-set\n# collection, and convert it into a pandas DataFrame.\n\ntrain = []\nfor category_id, category in enumerate(CATEGORIES): # category_id is the integer index corresponding to each class_name\n    for file in os.listdir(os.path.join(train_dir, category)): # Means: \"for every \"file\" in this directory,:\"\n        train.append(['train/{}/{}'.format(category, file), category_id, category]) # Renaming the file names and\n        # adding to the train list\ntrain = pd.DataFrame(train, columns = ['file', 'category_id', 'category']) # Define a pandas DataFrame over training data\ntrain.head(5) # Print preview of the training DataFrame\ntrain.shape # Check shape: should be of dims (m, 3), where 3 represents file_name, category_id (int index), and class_name\n# for each example","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a84e4fe8c877f19e25dc75037279b2d3b9ec20b2"},"cell_type":"markdown","source":"**Creating Our Training Set**"},{"metadata":{"trusted":true,"_uuid":"1702cb4acf40a6d7537096c4030b02a0d8b5d3ab"},"cell_type":"code","source":"train = pd.concat([train[train['category'] == c][:SAMPLE_PER_CATEGORY] for c in CATEGORIES])\ntrain = train.sample(frac=1)\ntrain.index = np.arange(len(train))\ntrain.head(5)\ntrain.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"16b3ac90eba7fde4ba348a6013ba82bbca1f5931"},"cell_type":"markdown","source":"**Creating Our Test Set**"},{"metadata":{"trusted":true,"_uuid":"426eedb34eb3476eb2a1427bdd1ec8bb2e48fcb1"},"cell_type":"code","source":"test = []\nfor file in os.listdir(test_dir):\n    test.append(['test/{}'.format(file), file])\ntest = pd.DataFrame(test, columns=['filepath', 'file'])\ntest.head(5)\ntest.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b6ec98ce3bfac037faac56fb29f423a11ef7603d"},"cell_type":"markdown","source":"**Reading an Image to an Array**"},{"metadata":{"trusted":true,"_uuid":"81d6397c79691af2c4e3f249041ecae85df0e876"},"cell_type":"code","source":"def read_img(filepath, size):\n    img = image.load_img(os.path.join(data_dir, filepath), target_size = size)\n    img = image.img_to_array(img)\n    return img","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"23ae7eb731784eb3d783f9974699d407855cc8e4"},"cell_type":"markdown","source":"**Loading and Visualizing Sample Images (Training Examples)**"},{"metadata":{"trusted":true,"_uuid":"67c85bdcb0c128d8fcbcbf774a6ee30af45714a9"},"cell_type":"code","source":"# Matplotlib\n\nfig = plt.figure(1, figsize=(NUM_CATEGORIES, NUM_CATEGORIES))\ngrid = ImageGrid(fig, 111, nrows_ncols=(NUM_CATEGORIES, NUM_CATEGORIES), axes_pad=0.05)\ni = 0 # Initialize counter\n\n# Iterate through the files in the categories\nfor category_id, category in enumerate(CATEGORIES):\n    for filepath in train[train['category'] == category]['file'].values[:NUM_CATEGORIES]:\n        ax = grid[i]\n        img = read_img(filepath, (224,224))\n        ax.imshow(img/255.)\n        ax.axis('off')\n        if i % NUM_CATEGORIES == NUM_CATEGORIES - 1:\n            ax.text(250, 112, filepath.split('/')[1], verticalalignment='center')\n        i += 1\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eef71cb97f3b7b10181ca6e4121b27dae37aed5d"},"cell_type":"markdown","source":"**Train-Validation Split**"},{"metadata":{"trusted":true,"_uuid":"3c0084b08dc2ce7bd8567df20db34a6bc4f5da62"},"cell_type":"code","source":"np.random.seed(seed=SEED)\nrnd = np.random.random(len(train))\ntrain_idx = rnd < 0.8 # Indices in which rnd is <0.8 (which should come out to roughly 80% of the dataset)\nvalid_idx = rnd >= 0.8\nytr = train.loc[train_idx, 'category_id'].values # pandas function calls\nyv = train.loc[valid_idx, 'category_id'].values\nlen(ytr)\nlen(yv)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e0b77124d94f1eaeb9934a8bdb07dfa1873440a3"},"cell_type":"markdown","source":"**Run Examples through the Pre-trained Xception Model to Extract Xception Features / Representations:  \n(pre-classification step)**"},{"metadata":{"trusted":true,"_uuid":"b7cbae41b2cdb229cc219842aac5759208c4495c"},"cell_type":"code","source":"# Specify parameters:\nINPUT_SIZE = 299\nPOOLING = 'avg'\nx_train = np.zeros((len(train), INPUT_SIZE, INPUT_SIZE, 3), dtype='float32')\nfor i, file in tqdm(enumerate(train['file'])): # tqdm is a progress bar\n    img = read_img(file, (INPUT_SIZE, INPUT_SIZE)) # Read the filepath into an array via our function call\n    x = xception.preprocess_input(np.expand_dims(img.copy(), axis=0)) # Pre-process that into a format for Xception model\n    x_train[i] = x # Set the i-th example in our initialized zero-4D-array to the particular example\nprint('Train Images shape: {} size: {:,}'.format(x_train.shape, x_train.size))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"013edcbaec0b504bfb3f8bfd27233a23770c5883"},"cell_type":"code","source":"Xtr = x_train[train_idx]\nXv = x_train[valid_idx]\nprint((Xtr.shape, Xv.shape, ytr.shape, yv.shape)) # Print shapes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"31de9276c4112e77f2bb4f180512ecbcc149eb5d"},"cell_type":"code","source":"xception_bottleneck = xception.Xception(weights='imagenet', include_top=False, pooling=POOLING) # Define Xception object\n    # based on \"off-the-shelf\" pre-trained Xception model\ntrain_x_bf = xception_bottleneck.predict(Xtr, batch_size=32, verbose=1)\nvalid_x_bf = xception_bottleneck.predict(Xv, batch_size=32, verbose=1)\n\n# Check output dims:\nprint(\"Xception train bottleneck-features shape: {} size: {:,}\".format(train_x_bf.shape, train_x_bf.size))\nprint(\"Xception valid bottleneck-features shape: {} size: {:,}\".format(valid_x_bf.shape, valid_x_bf.size))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cf10a0fcdc51d37a4908edf499a1938d30dc87b3"},"cell_type":"markdown","source":"**LogReg Classification on (\"using\") Resulting Xception-bottleneck Features:**"},{"metadata":{"trusted":true,"_uuid":"6e81a5913f1c315d218bf03ad90ed89886be9546"},"cell_type":"code","source":"logreg = LogisticRegression(multi_class='multinomial', solver='lbfgs', random_state=SEED)\nlogreg.fit(train_x_bf, ytr) # We need to fit the classifier to our (X,Y pairs)\nvalid_probs = logreg.predict_proba(valid_x_bf) # Classification on our dev set -- probabilities of various classes\nvalid_preds = logreg.predict(valid_x_bf) # Classification on our dev set -- predicted classes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f5fca069db08d3b090585a65b7814749e678d0ee"},"cell_type":"code","source":"print(\"Validation Xception Accuracy: {}\".format(accuracy_score(yv, valid_preds)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0ebbf0d86fd584842f7376008b13010897bf0152"},"cell_type":"markdown","source":"Illustrating the Results: Confusion Matrix"},{"metadata":{"trusted":true,"_uuid":"eb89daaa127b12c213d6312d531740a5e0d2ecfe"},"cell_type":"code","source":"cnf_matrix = confusion_matrix(yv, valid_preds)\nabbreviation = ['BG', 'Ch', 'Cl', 'CC', 'CW', 'FH', 'LSB', 'M', 'SM', 'SP', 'SFC', 'SB']\npd.DataFrame({'class': CATEGORIES, 'abbreviation': abbreviation})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6331751e94f64006fbd1e24d6a84440727febd25"},"cell_type":"code","source":"# Plotting the confusion matrix\nfig, ax = plt.subplots(1)\nax = sns.heatmap(cnf_matrix, ax=ax, cmap=plt.cm.Greens, annot=True)\nax.set_xticklabels(abbreviation)\nax.set_yticklabels(abbreviation)\nplt.title('Confusion Matrix')\nplt.ylabel('True Class')\nplt.xlabel('Predicted Class')\nfig.savefig('Confusion matrix.png', dpi=300)\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dd956bf2a42387d6eb70a721a47f0d9b5187e58b"},"cell_type":"markdown","source":"Finalization and Creating the Submission"},{"metadata":{"trusted":true,"_uuid":"e8ac72d6e1fa9d049c1759c7b6720af8765d4a6d"},"cell_type":"code","source":"x_test = np.zeros((len(test), INPUT_SIZE, INPUT_SIZE, 3), dtype='float32')\nfor i, filepath in tqdm(enumerate(test['filepath'])):\n    img = read_img(filepath, (INPUT_SIZE, INPUT_SIZE))\n    x = xception.preprocess_input(np.expand_dims(img.copy(), axis=0))\n    x_test[i] = x\nprint(\"Test images dataset shape: {} size: {:,}\".format(x_test.shape, x_test.size))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ec883ec31266c29335b247212d92bd6d0b2de7cb"},"cell_type":"code","source":"test_x_bf = xception_bottleneck.predict(x_test, batch_size=32, verbose=1)\nprint('Xception test bottleneck features shape: {} size: {:,}'.format(test_x_bf.shape, test_x_bf.size))\n\ntest_preds = logreg.predict(test_x_bf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7bf6524f1e5e044609c3e05cd201c1576a70cce4"},"cell_type":"code","source":"test['category_id'] = test_preds\ntest['species'] = [CATEGORIES[c] for c in  test_preds]\ntest[['file', 'species']].to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}