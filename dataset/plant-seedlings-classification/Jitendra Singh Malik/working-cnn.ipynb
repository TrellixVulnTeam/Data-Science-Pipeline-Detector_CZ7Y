{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"scrolled":false},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n#    for filename in filenames:\n        #print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"scrolled":false},"cell_type":"code","source":"\nimport matplotlib.pyplot as plt \nimport math\nfrom glob import glob \nimport itertools\nfrom keras.utils import np_utils\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Dropout,Flatten\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.layers.convolutional import (Conv2D,MaxPooling2D)\nfrom keras.layers import BatchNormalization\n#from keras.callbacks import (ModelCheckpoint,ReduceLROnPlateau,CSVLogger)\nfrom sklearn import preprocessing\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nimport cv2\nimport seaborn as sns\nimport numpy as np \nimport pandas as pd\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom keras import layers\nfrom keras import models\nfrom keras import optimizers\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.preprocessing.image import img_to_array, load_img\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.layers. normalization import BatchNormalization\nfrom keras.optimizers import RMSprop\nfrom keras.optimizers import Adam","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"\n#-------------------------------------Reading Data-------------------------------------------------------#\n\n#this is the path of training data to read the data\n#star is used here becuase folder path are dynamic and data is to be read from each folder after going back\ntrain_data_path = '../input/plant-seedlings-classification/train/*/*.png'\ndata = glob(train_data_path)\n\ntraining_data=[]\ntraining_label=[]\n\ndata_count= len(data)\n\nprint(\"Reading Training Data\")\n\nfor d in data:\n    training_data.append(cv2.resize(cv2.imread(d),(70,70)))\n    training_label.append(d.split('/')[-2])\n    \ntraining_data = np.asarray(training_data)\ntraining_label = pd.DataFrame(training_label)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#-------------------------------------Opertions on training data-------------------------------------------------------#\n\n#new_train = []\n\n\ndef ImageOperation(data):\n    #create a new list to store the modified images\n    new_data = []\n    # this step is done so as the proper operations can be performed on the dataset\n    image_Display=True\n    for t in data:\n        \n        #applying hsv\n        lower_hsv = np.array([25, 100, 50])\n        upper_hsv = np.array([95, 255, 255])\n        hsv = cv2.cvtColor(t,cv2.COLOR_BGR2HSV)\n        \n        #masking image\n        masking = cv2.inRange(hsv,lower_hsv,upper_hsv)\n        structuring = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(11,11))\n        masking = cv2.morphologyEx(masking,cv2.MORPH_CLOSE,structuring)\n        \n        #boolean masking image\n        boolean = masking>0\n        \n        #removing backgraound from the image\n        new = np.zeros_like(t,np.uint8)\n        new[boolean] = t[boolean]\n        new_data.append(new)\n    \n        #the image will be dsiplayed through this code for each operation\n        if image_Display == True:\n            #showing original image\n            plt.subplot(2, 3, 1); plt.imshow(t)  \n            #showing hsv image\n            plt.subplot(2, 3, 2); plt.imshow(hsv) \n            # showing mask image\n            plt.subplot(2, 3, 3); plt.imshow(masking)  \n            # showing boolean mask image\n            plt.subplot(2, 3, 4); plt.imshow(boolean) \n            #showing image without the background\n            plt.subplot(2, 3, 5); plt.imshow(new)\n        \n        #once it is turned false, now when the next time loop runs, it will not show the image again\n        image_Display = False\n        \n    return new_data\n\nnew_training_data= ImageOperation(training_data)\nnew_training_data = np.asarray(new_training_data) \nnew_training_data=new_training_data/255\n#print(new_training_data.shape)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#-----------------------Data Visulaisation ----------------------------------------------------#\n\ncategory = ['Black-grass', 'Charlock', 'Cleavers', 'Common Chickweed', 'Common wheat', 'Fat Hen',\n              'Loose Silky-bent', 'Maize', 'Scentless Mayweed', 'Shepherds Purse',\n              'Small-flowered Cranesbill', 'Sugar beet']\nprint_data = {}\n\nprint('------ Data Contains----------')\n\nfor s in category:\n    count= len(os.listdir(os.path.join('../input/plant-seedlings-classification/train',s )))\n    print('{} data for - {} category'.format(count,s))\n    print_data[s] = count\n    \nplt.figure(figsize=(23, 8))  \nsns.barplot(list(print_data.keys()), list(print_data.values()))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"LabelEncode = preprocessing.LabelEncoder()\nLabelEncode.fit(training_label[0])\nnew_label = LabelEncode.transform(training_label[0])\nclearalllabels = np_utils.to_categorical(new_label)\n\nprint(new_training_data.shape, clearalllabels.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"#-----------------------------------Splitting the test and training data----------------------------------\n\n\nx_train,x_test,y_train,y_test = train_test_split(new_training_data,clearalllabels,test_size=0.1,random_state=1,stratify=clearalllabels)\n\nprint(x_train.shape,x_test.shape,y_train.shape,y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"\n#--------------------------------------BUILDING MODEL-------------------------------------------------------#\n\n#initialising sequential model\nmodel = Sequential()\n\n\n#----------------- 1st convolution layer --------------------------\n\n#adding convolution layer with 64 filter and imput shape 70 x70 with relu function\n\nmodel.add(Conv2D(filters=64, kernel_size=(5, 5), input_shape=(70, 70, 3), activation='relu'))\n#normalising batch\nmodel.add(BatchNormalization(axis=3))\n\n\n#----------------- 1st convolution layer --------------------------\n\n#adding convolution layer with 64 filters\n\nmodel.add(Conv2D(filters=128, kernel_size=(5, 5), activation='relu'))\n#maxpooling\nmodel.add(MaxPooling2D((2, 2)))\n#normalising batch\nmodel.add(BatchNormalization(axis=3))\nmodel.add(Dropout(0.1))\n\n\n#----------------- 1st convolution layer --------------------------\n\n\n#adding convolution layer with 128 filters\n\nmodel.add(Conv2D(filters=128, kernel_size=(5, 5), activation='relu'))\n#normalising batch\nmodel.add(BatchNormalization(axis=3))\n\n\n#----------------- 1st convolution layer --------------------------\n\n\n#adding convolution layer with 128 filters\n\nmodel.add(Conv2D(filters=256, kernel_size=(5, 5), activation='relu'))\n#maxpooling\nmodel.add(MaxPooling2D((2, 2)))\n#normalising batch\nmodel.add(BatchNormalization(axis=3))\nmodel.add(Dropout(0.1))\n\n\n#----------------- 1st convolution layer --------------------------\n\n\n#adding convolution layer with 128 filters with 256 filters\nmodel.add(Conv2D(filters=256, kernel_size=(5, 5), activation='relu'))\n#normalising batch\nmodel.add(BatchNormalization(axis=3))\n\n'''\n\n#----------------- 1st convolution layer --------------------------\n\n#adding convolution layer with 256 filters\nmodel.add(Conv2D(filters=256, kernel_size=(5, 5), activation='relu'))\n#maxpooling\nmodel.add(MaxPooling2D((2, 2)))\n#normalising batch\nmodel.add(BatchNormalization(axis=3))\nmodel.add(Dropout(0.1))\n\n'''\n\n#------------------- flattening layer-------------------------\nmodel.add(Flatten())\n\n#------------------- Dense layer-------------------------\n\n#adding dense layer\nmodel.add(Dense(128, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.5))\n\n#------------------- Dense layer-------------------------\n\n#adding dense layer\nmodel.add(Dense(256, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.5))\n\n#------------------- Dense layer-------------------------\n\n\n#adding dense layer with same output as no of cateogries, in our case 12 category with softmax function\nmodel.add(Dense(12, activation='softmax'))\n\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"from keras.optimizers import Adam\n\noptimizer = Adam()\nmodel.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n    \nmodel.summary()\n\n#Designing imagedategenrator so as to obtain as much accuracy, and to make sure same image is not fed into the newtork\n# for this we will rotate image, randomdy zoom the imahe, shoft the image vertically, horizontally and flipping it\ngenerate_data = ImageDataGenerator(\n        rotation_range=180,\n        horizontal_flip=True,\n        vertical_flip=True,\n        zoom_range = 0.1, \n        width_shift_range=0.1,\n        height_shift_range=0.1,\n        \n    )  \n\ngenerate_data.fit(x_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Running the model with different batch size of 4 ,8, 16 ,32, 64\nbatch_size = [32]\nfor i in batch_size:\n    history = model.fit_generator(generate_data.flow(x_train, y_train,batch_size=i),\n                            steps_per_epoch= (x_train.shape[0] // i),\n                            epochs = 200,\n                            workers=4,\n                            validation_data=(x_test,y_test),\n                            verbose =2\n                            )\n    score, acc = model.evaluate(x_test,y_test)\n    score2, acc2 = model.evaluate(x_train,y_train)\n    print('---------------')\n    print('Test score:', score,'   Test accuracy:', acc)\n    print('Train score:', score2,'   Train accuracy:',acc2)\n    print('---------------')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"#Final score and accuracy of the model\n\nscore, acc = model.evaluate(x_test,y_test)\nscore2, acc2 = model.evaluate(x_train,y_train)\nprint('--------------------------------------------------------------')\nprint('Test score:', score,'   Test accuracy:', acc)\nprint('Train score:', score2,'   Train accuracy:',acc2)\nprint('--------------------------------------------------------------')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# evaluating model on test data\npath_to_test = '../input/plant-seedlings-classification/test/*.png'\npics = glob(path_to_test)\n\n#creating two list for isnseritng testing and testing labels\ntestimages = []\ntests = []\nnum = len(pics)\n\n# performing same operations on the testing data as on training data\nfor i in pics:\n    tests.append(i.split('/')[-1])\n    testimages.append(cv2.resize(cv2.imread(i),(70,70)))\n\nnewtestimages = []\nnewtestimages = ImageOperation(testimages)    \ntestimages = np.asarray(testimages)\nnewtestimages = np.asarray(newtestimages)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#using the model to predict the values on the testing data\nprediction = model.predict(newtestimages)\n\npred = np.argmax(prediction,axis=1)\npredStr = LabelEncode.classes_[pred]\nsubmission = {'file':tests,'species':predStr}\nsubmission = pd.DataFrame(submission)\nsubmission.to_csv(\"Prediction.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"#------------------------------- Creating confusion matrix ---------------------------------\n\nimport seaborn as sns\nypred = model.predict(x_test)\n\ny_correct = np.argmax(y_test, axis=1)\ny_pred = np.argmax(ypred, axis=1)\n\ncm = confusion_matrix(y_correct, y_pred)\n\nplt.figure(figsize=(12, 12))\nax = sns.heatmap(cm, cmap=plt.cm.Greens_r, annot=True, square=True)\n\nax.set_ylabel('Correct', fontsize=40)\nax.set_xlabel('Predicted', fontsize=40)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#------------------------ Creating the accuracy and losss curve -----------------------------\n\nfrom matplotlib import pyplot\n\n#plotting the accuracy and the loss curve using seaborn\n\npyplot.plot(history.history['accuracy'], label='train')\npyplot.plot(history.history['val_accuracy'], label='test')\npyplot.legend()\npyplot.show()\n\npyplot.plot(history.history['loss'], label='train')\npyplot.plot(history.history['val_loss'], label='test')\npyplot.legend()\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV,cross_val_score\nfrom sklearn.ensemble import (GradientBoostingClassifier,RandomForestClassifier,AdaBoostClassifier,VotingClassifier)\nfrom xgboost import XGBClassifier\nfrom glob import glob\nimport os\n\ncategory = ['Black-grass', 'Charlock', 'Cleavers', 'Common Chickweed', 'Common wheat', 'Fat Hen',\n              'Loose Silky-bent', 'Maize', 'Scentless Mayweed', 'Shepherds Purse',\n              'Small-flowered Cranesbill', 'Sugar beet']\n\nx = []\ny = []\n\ndef ImageOperation2(image):\n    image_hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n    lower_hsv = np.array([25, 100, 50])\n    upper_hsv = np.array([95, 255, 255])\n    masking = cv2.inRange(image_hsv, lower_hsv, upper_hsv)\n    structuring = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (11,11))\n    mask = cv2.morphologyEx(masking, cv2.MORPH_CLOSE, structuring)\n    output = cv2.bitwise_and(image, image, mask = mask)\n    return output\n\ntrain=[] \n\nprint('--------- Reading training data -----------')\n\nfor c in category:\n    num_samples= os.path.join('../input/plant-seedlings-classification/train', c)\n    for i in glob(os.path.join(num_samples, \"*.png\")):\n        \n        image = cv2.imread(i, cv2.IMREAD_COLOR)\n        image = cv2.resize(image, (150, 150))\n        \n        image = ImageOperation2(image)\n        \n        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n        image = cv2.resize(image, (45,45))\n        \n        image = image.flatten()\n        \n        x.append(image)\n        y.append(c)\n        \nx = np.array(x)\ny = np.array(y)\n\nprint(x.shape)\nprint()\nprint(y.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\nlabel_to_id_dict = {a:b for b,a in enumerate(np.unique(y))}\nid_to_label_dict = {a: b for b, a in label_to_id_dict.items()}\nlabel_ids = np.array([label_to_id_dict[a] for a in y])\n\n\nprint('------------------ Creating Visualisation --------------------')\n\nimages_scaled = StandardScaler().fit_transform(x)\n\npca = PCA(n_components=2)\npca_200= PCA(n_components=200)\n\n# -------------------------- for 2 components ---------------------\n\npca_result = pca.fit_transform(images_scaled)\npca_result_scaled = StandardScaler().fit_transform(pca_result)\n\n# -------------------------- for 200 components ---------------------\n\npca_result_200 = pca_200.fit_transform(images_scaled)\npca_result_scaled_200= StandardScaler().fit_transform(pca_result_200)\n\n\ndef Plotting(data, label,figsize=(20,20)):\n    plt.figure(figsize=figsize)\n    plt.grid()\n    noOfClass = len(np.unique(label))\n    for l in np.unique(label):\n        plt.scatter(data[np.where(label == l), 0],\n                    data[np.where(label == l), 1],\n                    marker='*',\n                    color= plt.cm.Set1(l / float(noOfClass)),\n                    linewidth='1',\n                    alpha=0.8,\n                    label=id_to_label_dict[l])\n    plt.legend(loc='best')\n    \n\ntsne = TSNE(n_components=2, perplexity=40.0)\n\ntsne_result = tsne.fit_transform(pca_result)\ntsne_result_200 = tsne.fit_transform(pca_result_200)\n\n#print(tsne_result)\n#print()\n#print(tsne_result.shape)\n\ntsne_result_scaled = StandardScaler().fit_transform(tsne_result)\ntsne_result_scaled_200 = StandardScaler().fit_transform(tsne_result_200)\n\nPlotting(pca_result_scaled_200, label_ids)\nPlotting(tsne_result_scaled, label_ids)\nPlotting(tsne_result_scaled_200, label_ids)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"xgb= XGBClassifier()\nada= AdaBoostClassifier()\ngb= GradientBoostingClassifier()\nrf= RandomForestClassifier()\nfrom sklearn import svm\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.decomposition import PCA\n\npca = PCA(n_components=100)\npca_result = pca.fit_transform(images_scaled)\n\nprint(pca_result.shape)\nkfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n\nsvm= svm.SVC()\n\neclf = VotingClassifier(estimators=[('rf',rf),('xgb',xgb),('svm',svm),('adaboost', ada)], voting='hard')\n\nfor clf, label in zip([ rf,xgb,svm,ada, eclf],['Random Forest', 'XGB', 'SVM', 'adaboost','Combine']):\n    score=cross_val_score(clf,pca_result,y,scoring='accuracy',cv=kfold)\n    #print(score)\n    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (score.mean(), score.std(), label))\n\n#eclf.fit(x_train,y_train)\n#prediction = eclf.predict(x_test)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}