{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"%matplotlib inline\nimport datetime as dt\nimport matplotlib.pyplot as plt\nplt.rcParams['figure.figsize'] = [16, 10]\nplt.rcParams['font.size'] = 16\nimport numpy as np\nimport os\nimport pandas as pd\nimport seaborn as sns\nfrom keras.applications import xception\nfrom keras.preprocessing import image\nfrom mpl_toolkits.axes_grid1 import ImageGrid\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom tqdm import tqdm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7c4fd41545745d2017e46c8b0497effac77b7281"},"cell_type":"code","source":"!ls ../input/train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e7fe667150a5aaf3b3cf0990b559a074cd525df7"},"cell_type":"code","source":"CATEGORIES = ['Black-grass', 'Charlock', 'Cleavers', 'Common Chickweed', 'Common wheat', 'Fat Hen', 'Loose Silky-bent',\n              'Maize', 'Scentless Mayweed', 'Shepherds Purse', 'Small-flowered Cranesbill', 'Sugar beet']\nNUM_CATEGORIES = len(CATEGORIES)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"daf5396cf24fd9dd7d4d93d39d9254fc350d5724"},"cell_type":"code","source":"SAMPLE_PER_CATEGORY = 200\nSEED = 1987\ndata_dir = '../input/'\ntrain_dir = os.path.join(data_dir, 'train')\ntest_dir = os.path.join(data_dir, 'test')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"63ee14e87bac5c36fbf4e546ffb00dc3f3ea3995"},"cell_type":"code","source":"for category in CATEGORIES:\n    print('{} {} images'.format(category, len(os.listdir(os.path.join(train_dir, category)))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4db419592890a6757183b8e233164d5170f3dda3"},"cell_type":"code","source":"train = []\nfor category_id, category in enumerate(CATEGORIES):\n    for file in os.listdir(os.path.join(train_dir, category)):\n        train.append(['train/{}/{}'.format(category, file), category_id, category])\ntrain = pd.DataFrame(train, columns=['file', 'category_id', 'category'])\ntrain.head(2)\ntrain.shape\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"73dd9ae1c36d98f8fd9a308f3f3496261950df0b"},"cell_type":"code","source":"train = pd.concat([train[train['category'] == c][:SAMPLE_PER_CATEGORY] for c in CATEGORIES])\ntrain = train.sample(frac=1)\ntrain.index = np.arange(len(train))\ntrain.head(2)\ntrain.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f23855b6b3bd17fd22826d62140ebabb1a8b9a24"},"cell_type":"code","source":"test = []\nfor file in os.listdir(test_dir):\n    test.append(['test/{}'.format(file), file])\ntest = pd.DataFrame(test, columns=['filepath', 'file'])\ntest.head(2)\ntest.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3c4d2aa0b83bab3f51bd02d948ec8ac413db7d41"},"cell_type":"code","source":"def read_img(filepath, size):\n    img = image.load_img(os.path.join(data_dir, filepath), target_size=size)\n    img = image.img_to_array(img)\n    return img","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4a99ce0f510e6c2aa87011b75035f05254f8ad29"},"cell_type":"code","source":"fig = plt.figure(1, figsize=(NUM_CATEGORIES, NUM_CATEGORIES))\ngrid = ImageGrid(fig, 111, nrows_ncols=(NUM_CATEGORIES, NUM_CATEGORIES), axes_pad=0.05)\ni = 0\nfor category_id, category in enumerate(CATEGORIES):\n    for filepath in train[train['category'] == category]['file'].values[:NUM_CATEGORIES]:\n        ax = grid[i]\n        img = read_img(filepath, (224, 224))\n        ax.imshow(img / 255.)\n        ax.axis('off')\n        if i % NUM_CATEGORIES == NUM_CATEGORIES - 1:\n            ax.text(250, 112, filepath.split('/')[1], verticalalignment='center')\n        i += 1\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5e46a72f4188ded32ba4e73f19ec74161caa878f"},"cell_type":"code","source":"np.random.seed(seed=SEED)\nrnd = np.random.random(len(train))\ntrain_idx = rnd < 0.8\nvalid_idx = rnd >= 0.8\nytr = train.loc[train_idx, 'category_id'].values\nyv = train.loc[valid_idx, 'category_id'].values\nlen(ytr), len(yv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d1452ec3dab49814ff9b9d3a303f4dc6c93079a0"},"cell_type":"code","source":"from keras.applications.inception_v3 import InceptionV3,preprocess_input, decode_predictions\nINPUT_SIZE=224\nPOOLING='avg'\nx_train=np.zeros((len(train),INPUT_SIZE,INPUT_SIZE,3),dtype=np.float32)\nfor i,file in tqdm(enumerate(train['file'])):\n    img = read_img(os.path.join(data_dir,file),(INPUT_SIZE,INPUT_SIZE))\n    x=preprocess_input(np.expand_dims(img.copy(),axis=0)) #need to be changed for every model\n    x_train[i]=x\nprint('Train image shape: {} size: {:,}'.format(x_train.shape,x_train.size))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8850a1b16bb519d10f7eb9f16828e682f7a70a37"},"cell_type":"code","source":"xtrain=x_train[train_idx]\nxvalid=x_train[valid_idx]\nprint((xtrain.shape,xvalid.shape))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b847da4e7c3a989b64de6754f27c98e16f085e03"},"cell_type":"markdown","source":"**Model building starts from here**"},{"metadata":{"trusted":true,"_uuid":"55734fc00a1f1545adcc1e7148b5b5f9019c78cd"},"cell_type":"code","source":"from keras.applications.inception_v3 import InceptionV3\nfrom keras.preprocessing import image\nvgg_bottleneck = InceptionV3(weights='imagenet', include_top=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3d37ecd160641988951cce11232d5697d600bd16"},"cell_type":"code","source":"train_vgg_bf = vgg_bottleneck.predict(xtrain, batch_size=32, verbose=1)\nvalid_vgg_bf = vgg_bottleneck.predict(xvalid, batch_size=32, verbose=1)\nprint('VGG train bottleneck features shape: {} size: {:,}'.format(train_vgg_bf.shape, train_vgg_bf.size))\nprint('VGG valid bottleneck features shape: {} size: {:,}'.format(valid_vgg_bf.shape, valid_vgg_bf.size))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9dbf639b4eb453de4603f11d5b60e96c38fbec41"},"cell_type":"code","source":"train_vgg_bf=train_vgg_bf.reshape(1899,51200)\nvalid_vgg_bf=valid_vgg_bf.reshape(501,51200)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e503209a8fee701ddd942631b48e9e4e38f27892"},"cell_type":"code","source":"import keras\none_hot_labels = keras.utils.to_categorical(ytr, num_classes=12)\nvalid_labels = keras.utils.to_categorical(yv, num_classes=12)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dfa4b0db13f7efece3ba2f88f8ebcd79f7ce845f"},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense, Activation\nmodel=Sequential()\n\nmodel.add(Dense(1000, input_dim=51200, activation='relu',kernel_initializer='uniform'))\nkeras.layers.core.Dropout(0.9, noise_shape=None, seed=None)\n\nmodel.add(Dense(500,input_dim=1000,activation='sigmoid'))\nkeras.layers.core.Dropout(0.9, noise_shape=None, seed=None)\n\nmodel.add(Dense(150,input_dim=500,activation='sigmoid'))\nkeras.layers.core.Dropout(0.9, noise_shape=None, seed=None)\n\nmodel.add(Dense(units=12))\nmodel.add(Activation('softmax'))\n\nmodel.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c92d902ca84e4bd913d558b24dd9d0397a882ab0"},"cell_type":"code","source":"history= model.fit(train_vgg_bf,one_hot_labels, epochs=20, batch_size=128,validation_data=(valid_vgg_bf, valid_labels))\nprint(history.history.keys())\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('batch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\nmodel.save_weights('inception_model.h5')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3cc54c0c8e057622656a275e2b43565fea82e0fe"},"cell_type":"markdown","source":"**Extra Work**"},{"metadata":{"trusted":true,"_uuid":"1fbb2a45c977cecce7804a31d6167c30c9965b3e"},"cell_type":"code","source":"test = []\nfor file in os.listdir(test_dir):\n    test.append(['test/{}'.format(file),file])\ntest=pd.DataFrame(test,columns=['file_path','file'])\ntest.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"38607f4ede544a2af1f55b3710a8387ef89361c6"},"cell_type":"code","source":"x_test = np.zeros((len(test), INPUT_SIZE, INPUT_SIZE, 3), dtype='float32')\nfor i, file in tqdm(enumerate(test['file_path'])):\n    img = read_img(os.path.join(data_dir,file), (INPUT_SIZE, INPUT_SIZE))\n    x = preprocess_input(np.expand_dims(img.copy(), axis=0))\n    x_test[i] = x\nprint('test Images shape: {} size: {:,}'.format(x_test.shape, x_test.size))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"54c55891b0651af61c7c21c9f2954e8edb76d531"},"cell_type":"code","source":"test_x_bf = vgg_bottleneck.predict(x_test, batch_size=32, verbose=1)\nprint('Test bottleneck features shape: {} size: {:,}'.format(test_x_bf.shape, test_x_bf.size))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"27a32096857046560d1ecacfc5c357e063033b66"},"cell_type":"code","source":"test_x_bf=test_x_bf.reshape(794,51200)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d6a0811e4ccc9dd38233acff15eea0a528fd9511"},"cell_type":"code","source":"test_preds = model.predict(test_x_bf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2d892dea0a96de2f66382ace40858f5925a8bfbb"},"cell_type":"code","source":"np.argmax(test_preds,axis=1).shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"986a8b2340209dd0ece158d0e59c316c3f42f315"},"cell_type":"code","source":"test_pred_one = np.argmax(test_preds,axis=1)\ntest['category_id'] = test_pred_one\ntest['species'] = [CATEGORIES[c] for c in test_pred_one]\ntest[['file', 'species']].to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a2bcdfdcdddb3455fac240d5300ba4cc3ea8f48e"},"cell_type":"code","source":"!ls","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}