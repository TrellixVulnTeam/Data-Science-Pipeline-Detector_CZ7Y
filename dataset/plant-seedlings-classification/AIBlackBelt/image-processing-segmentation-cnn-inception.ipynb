{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Hello Community,**\n\n**This notebook is destined to show viewers differents techniques to approach Plant Seeedling classifcation problem. We will implement image processing techniques and see its effect on the predictive modeling part, below are the techniques used in this work. This notebook is exported from Google colab, a few adjustement will be needed to run it on kaggle (images paths)**\n\n**Image processing :**\n\n**Data augmentation**\n\n**Image morphological operations (erosion and dilation)**\n\n**Image segmentation**\n\n**Modeling :**\n\n**Convolutional neural network based model**\n\n**InceptionV3 based model**\n\n**Support vector machine based model**","metadata":{"id":"67yOg0x12y8a"}},{"cell_type":"markdown","source":"We will need this module later on for the preprocessing part","metadata":{"id":"9yTRjDKl8HvE"}},{"cell_type":"code","source":"!pip install -U tensorflow-addons\n","metadata":{"id":"uuh8UjgxM6x6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n#from google.colab.patches import cv2_imshow\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom keras import layers\nimport keras\nimport random\nimport numpy as np\n%matplotlib inline\nimport tqdm\nimport cv2\nimport os \nimport tensorflow_addons as tfa\nfrom keras import layers\nimport gc\nfrom tensorflow.keras.regularizers import l2\n","metadata":{"id":"-HX_Zz0cMpSw","execution":{"iopub.status.busy":"2022-06-28T12:39:31.060239Z","iopub.execute_input":"2022-06-28T12:39:31.061117Z","iopub.status.idle":"2022-06-28T12:39:39.679429Z","shell.execute_reply.started":"2022-06-28T12:39:31.060978Z","shell.execute_reply":"2022-06-28T12:39:39.677952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we get a grasp of the data distribution, we notice a clear imbalance in the dataset. Some plant species are more frequent then other, thus this could lead to an imminent bias in the predictive. There will be a good probability to label an image as \"Loose Silky\" or \"Common Chickweed\" regardless of how the plant features. We don't have enough examples of how other species look in order to be able to infere confidently. In fact we need a larger dataset to enable the model to understand the right spatial characteristics and patterns of each plant specie. Especially in the cases where the problem at hand is multilabeled, the larger the number of labels is, the bigger the dataset should be so as to distinct between different species with ease. We could solve this problem by augmenting and balancing the dataset.","metadata":{"id":"CQplh7wRSCQQ"}},{"cell_type":"code","source":"training_images_files_names_black_grass = len(os.listdir(\"../input/plant-seedlings-classification/train/Black-grass\"))\ntraining_images_files_names_Charlock = len(os.listdir(\"../input/plant-seedlings-classification/train/Charlock\"))\ntraining_images_files_names_Cleavers = len(os.listdir(\"../input/plant-seedlings-classification/train/Cleavers\"))\ntraining_images_files_names_Common_Chickweed = len(os.listdir(\"../input/plant-seedlings-classification/train/Common Chickweed\"))\ntraining_images_files_names_Common_wheat = len(os.listdir(\"../input/plant-seedlings-classification/train/Common wheat\"))\ntraining_images_files_names_Scentless_Mayweed = len(os.listdir(\"../input/plant-seedlings-classification/train/Scentless Mayweed\"))\ntraining_images_files_names_Fat_Hen = len(os.listdir(\"../input/plant-seedlings-classification/train/Fat Hen\"))\ntraining_images_files_names_Loose_Silky_bent = len(os.listdir(\"../input/plant-seedlings-classification/train/Loose Silky-bent\"))\ntraining_images_files_names_Maize = len(os.listdir(\"../input/plant-seedlings-classification/train/Maize\"))\ntraining_images_files_names_Shepherds_Purse = len(os.listdir(\"../input/plant-seedlings-classification/train/Shepherds Purse\"))\ntraining_images_files_names_Small_flowered_Cranesbill = len(os.listdir(\"../input/plant-seedlings-classification/train/Small-flowered Cranesbill\"))\ntraining_images_files_names_Sugar_beet = len(os.listdir(\"../input/plant-seedlings-classification/train/Sugar beet\"))\nprint(\"black grass sample size : \",training_images_files_names_black_grass,\"\\n\")\nprint(\"Charlock sample size : \",training_images_files_names_Charlock,\"\\n\")\nprint(\"Cleavers sample size : \",training_images_files_names_Cleavers,\"\\n\")\nprint(\"Common Chickweed sample size : \",training_images_files_names_Common_Chickweed,\"\\n\")\nprint(\"Common wheat sample size : \",training_images_files_names_Common_wheat,\"\\n\")\nprint(\"Scentless Mayweed sample size : \",training_images_files_names_Scentless_Mayweed,\"\\n\")\nprint(\"Fat Hen sample size : \",training_images_files_names_Fat_Hen,\"\\n\")\nprint(\"Loose Silky sample size : \",training_images_files_names_Loose_Silky_bent,\"\\n\")\nprint(\"Maize sample size : \",training_images_files_names_Maize,\"\\n\")\nprint(\"Shepherds Purse sample size : \",training_images_files_names_Shepherds_Purse,\"\\n\")\nprint(\"Small flowered Cranesbill sample size : \",training_images_files_names_Small_flowered_Cranesbill,\"\\n\")\nprint(\"Sugar_beet sample size : \",training_images_files_names_Sugar_beet,\"\\n\")\noccurences = [training_images_files_names_black_grass/4750,training_images_files_names_Charlock/4750,training_images_files_names_Cleavers/4750,training_images_files_names_Common_Chickweed/4750,training_images_files_names_Common_wheat/4750,training_images_files_names_Fat_Hen/4750,training_images_files_names_Loose_Silky_bent/4750,training_images_files_names_Maize/4750,training_images_files_names_Scentless_Mayweed/4750,training_images_files_names_Shepherds_Purse/4750,training_images_files_names_Small_flowered_Cranesbill/4750,training_images_files_names_Sugar_beet/4750]\nfig = plt.figure(figsize=[20,10])\nax = fig.add_axes([0,0,1,1])\nax.bar([\"Black-grass\",\"Charlock\",\"Cleavers\",\"Common Chickweed\",\"Common wheat\",\"Fat Hen\",\"Loose Silky-bent\",\"Maize\",\"Scentless Mayweed\",\"Shepherds Purse\",\"Small-flowered Cranesbill\",\"Sugar beet\"],occurences)\nplt.show()\n\n","metadata":{"id":"8kar_XqwNtqR","execution":{"iopub.status.busy":"2022-06-28T12:39:58.485539Z","iopub.execute_input":"2022-06-28T12:39:58.486183Z","iopub.status.idle":"2022-06-28T12:40:00.867106Z","shell.execute_reply.started":"2022-06-28T12:39:58.486154Z","shell.execute_reply":"2022-06-28T12:40:00.865656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"the script below allows you to visualize a sample of 3 images of each plant species","metadata":{"id":"QWxmzfilXUwp"}},{"cell_type":"code","source":"plant_species = os.listdir(\"../input/plant-seedlings-classification/train\")\nfor plant_specie in plant_species:\n   images_file_name = os.listdir(\"../input/plant-seedlings-classification/train/\"+plant_specie)\n   sample_size_to_display = random.sample(range(0,100), 3)\n   if plant_specie == \".ipynb_checkpoints\":\n     continue\n   print(\"plant species :\",plant_specie)\n   for i in range(0,3): \n     img = cv2.imread(\"../input/plant-seedlings-classification/train/\"+plant_specie+\"/\"+images_file_name[sample_size_to_display[i]], cv2.IMREAD_COLOR)\n     img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n     plt.imshow(img)\n     plt.show()\n     #cv2_imshow(img)","metadata":{"id":"hPT5ArOjNxYd","execution":{"iopub.status.busy":"2022-06-28T12:40:05.495279Z","iopub.execute_input":"2022-06-28T12:40:05.495681Z","iopub.status.idle":"2022-06-28T12:40:16.775812Z","shell.execute_reply.started":"2022-06-28T12:40:05.49565Z","shell.execute_reply":"2022-06-28T12:40:16.774656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will be calling this function later on to visualize brut images and their corresponding processed versions","metadata":{"id":"hDbPtz7UXozy"}},{"cell_type":"code","source":"def vizualize_random_images(original_tensor,transformed_tensor,sample_size_to_display):\n     sample_size_to_display = random.sample(range(0,original_tensor.shape[0]), sample_size_to_display)\n     for i in range(0,len(sample_size_to_display)): \n         img = original_tensor[sample_size_to_display[i]]\n         mask = transformed_tensor[sample_size_to_display[i]]\n         figure_size = 10\n         print('Original image :')\n         plt.figure(figsize=(figure_size,figure_size))\n         plt.subplot(1,2,1)\n         plt.imshow(img)\n         plt.show()\n         print('Transformed image :') \n         plt.figure(figsize=(figure_size,figure_size))\n         plt.subplot(1,2,1)\n         plt.imshow(mask)\n         plt.show()","metadata":{"id":"GIsbDgNIj1uZ","execution":{"iopub.status.busy":"2022-06-28T12:40:19.906837Z","iopub.execute_input":"2022-06-28T12:40:19.907224Z","iopub.status.idle":"2022-06-28T12:40:19.918208Z","shell.execute_reply.started":"2022-06-28T12:40:19.907194Z","shell.execute_reply":"2022-06-28T12:40:19.91674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We proceed to read image data from their respective folders and store them in 4D array of size (number of image,100,100,3). We choose to resize image to the same dimension (100,100). Encode the species name following a one-hot encoding format, a matrix of size (number of images,12)","metadata":{"id":"0igxV5KtX5VB"}},{"cell_type":"code","source":"# Labels encoding :\n# black-grass : 0\n# Charlock : 1\n# Cleavers : 2\n# Common_Chickweed : 3\n# Common_wheat : 4\n# Fat_Hen : 5\n# Loose_Silky-bent : 6\n# Maize : 7\n# Scentless Mayweed : 8\n# Shepherds Purse : 9\n# Small flowered Cranesbill : 10\n# Sugar beet : 11\nlabel_encoding_mapper = {\"Black-grass\" : 0,\"Charlock\" : 1,\"Cleavers\": 2,\"Common Chickweed\" : 3,\"Common wheat\" : 4,\"Fat Hen\" : 5,\"Loose Silky-bent\" : 6,\"Maize\" : 7,\"Scentless Mayweed\" : 8,\"Shepherds Purse\" : 9,\"Small-flowered Cranesbill\" : 10,\"Sugar beet\" : 11}\ntraining_size = training_images_files_names_black_grass + training_images_files_names_Charlock + training_images_files_names_Cleavers + training_images_files_names_Common_Chickweed + training_images_files_names_Common_wheat + training_images_files_names_Fat_Hen + training_images_files_names_Loose_Silky_bent + training_images_files_names_Maize + training_images_files_names_Scentless_Mayweed + training_images_files_names_Shepherds_Purse + training_images_files_names_Small_flowered_Cranesbill + training_images_files_names_Sugar_beet\ny_train =  np.array([[0 for i in range(0,12)] for k in range(0,training_size)])\nplant_species = os.listdir(\"../input/plant-seedlings-classification/train\")\nx_train = []\nindex = 0\nfor plant_specie in plant_species:\n   images_file_name = os.listdir(\"../input/plant-seedlings-classification/train/\"+plant_specie)\n   for i in range(0,len(images_file_name)):\n     img = cv2.imread(\"../input/plant-seedlings-classification/train/\"+plant_specie+\"/\"+images_file_name[i], cv2.IMREAD_COLOR)\n     img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n     if img.shape != (100,100,3):\n         resized = cv2.resize(img,(100,100), interpolation = cv2.INTER_AREA)\n     x_train = x_train + [resized]\n     y_train[index][label_encoding_mapper[plant_specie]] = 1 \n     index = index + 1\nx_train = np.array(x_train)\ny_train = np.array(y_train)\n","metadata":{"id":"nQjLtiInN0Va","execution":{"iopub.status.busy":"2022-06-28T12:40:27.706773Z","iopub.execute_input":"2022-06-28T12:40:27.707177Z","iopub.status.idle":"2022-06-28T12:42:17.947554Z","shell.execute_reply.started":"2022-06-28T12:40:27.707147Z","shell.execute_reply":"2022-06-28T12:42:17.946291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"url = \"../input/plant-seedlings-classification/sample_submission.csv\"\nsubmission_file = pd.read_csv(url)\ntest_images_file_names = list(submission_file[\"file\"])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_test = []\n\nfor i in range(0,len(test_images_file_names)):\n    img = cv2.imread(\"../input/plant-seedlings-classification/test/\"+test_images_file_names[i], cv2.IMREAD_COLOR)\n    img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n    if img.shape != (100,100,3):\n        resized = cv2.resize(img,(100,100), interpolation = cv2.INTER_AREA)\n    x_test = x_test + [resized]\nx_test = np.array(x_test)","metadata":{"id":"AwZwPxRvZfcm","execution":{"iopub.status.busy":"2022-06-28T12:42:29.151368Z","iopub.execute_input":"2022-06-28T12:42:29.151776Z","iopub.status.idle":"2022-06-28T12:42:40.960934Z","shell.execute_reply.started":"2022-06-28T12:42:29.151743Z","shell.execute_reply":"2022-06-28T12:42:40.959305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"training sample : \",x_train.shape,\"  \",y_train.shape,\"\\n\")\nprint(\"test sample : \",x_test.shape,\"\\n\")\n","metadata":{"id":"jB7WgSMSZnIz","execution":{"iopub.status.busy":"2022-06-28T12:42:40.97552Z","iopub.execute_input":"2022-06-28T12:42:40.976236Z","iopub.status.idle":"2022-06-28T12:42:40.989486Z","shell.execute_reply.started":"2022-06-28T12:42:40.97619Z","shell.execute_reply":"2022-06-28T12:42:40.987904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we extract the images indices grouped by species from the array.","metadata":{"id":"ZKGcMuzwaq2G"}},{"cell_type":"code","source":"plant_species= [\"Black-grass\",\"Charlock\",\"Cleavers\",\"Common Chickweed\",\"Common wheat\",\"Fat Hen\",\"Loose Silky-bent\",\"Maize\",\"Scentless Mayweed\",\"Shepherds Purse\",\"Small-flowered Cranesbill\",\"Sugar beet\"]\nindex_storage = {\"Black-grass\" : [],\"Charlock\" : [],\"Cleavers\": [],\"Common Chickweed\" : [],\"Common wheat\" : [],\"Fat Hen\" : [],\"Loose Silky-bent\" : [],\"Maize\" : [],\"Scentless Mayweed\" : [],\"Shepherds Purse\" : [],\"Small-flowered Cranesbill\" : [],\"Sugar beet\" : []}\nfor i in range(0,len(x_train)):\n    index = y_train[i].tolist().index(1)\n    index_storage[plant_species[index]] = index_storage[plant_species[index]] + [i]","metadata":{"id":"N4TnKKxf0Sba","execution":{"iopub.status.busy":"2022-06-28T12:46:41.745848Z","iopub.execute_input":"2022-06-28T12:46:41.746288Z","iopub.status.idle":"2022-06-28T12:46:41.767767Z","shell.execute_reply.started":"2022-06-28T12:46:41.746255Z","shell.execute_reply":"2022-06-28T12:46:41.766471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Then we apply random operations on images, so as to get a wider augmented dataset","metadata":{"id":"mNGaJQdMau6K"}},{"cell_type":"code","source":"data_augmentation1 = tf.keras.Sequential([layers.RandomFlip(\"horizontal_and_vertical\"),layers.RandomRotation(0.2)])\naugmented_dataset1 = data_augmentation1(x_train)\ngc.collect()\ndata_augmentation2 = tf.keras.Sequential([layers.RandomTranslation(height_factor=(-0.1,0.1),width_factor=(-0.1,0.1),interpolation=\"nearest\")])\naugmented_dataset2 = data_augmentation2(x_train)\ngc.collect()\ndata_augmentation3 = tf.keras.Sequential([layers.RandomFlip(\"horizontal_and_vertical\"),layers.RandomRotation(0.3)])\naugmented_dataset3 = data_augmentation3(x_train)\ngc.collect()","metadata":{"id":"NqCZ19we0biV","execution":{"iopub.status.busy":"2022-06-28T12:46:54.605053Z","iopub.execute_input":"2022-06-28T12:46:54.605464Z","iopub.status.idle":"2022-06-28T12:46:59.505013Z","shell.execute_reply.started":"2022-06-28T12:46:54.605425Z","shell.execute_reply":"2022-06-28T12:46:59.503669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we apply a processing procedure to each plant specie. We augment the specie sample up to a certain size specified by the parameter \"Specie_total_size\", the parameter \"training_size\" refers to the specie sample size. The augmented batches are concatenated to the 4D Tensor that embedds the original dataset.","metadata":{"id":"xGVLpXCKd0CS"}},{"cell_type":"code","source":"\ndef augment_data(x_train,y_train,Specie,training_size,Specie_total_size):\n\n  j = training_size\n  one_hot_encoded_array_for_species = [0 for i in range(0,12)]\n  for i in range(0,12):\n      if label_encoding_mapper[Specie] == i:\n           one_hot_encoded_array_for_species[i] = 1\n\n  while j <= Specie_total_size:\n     if j + training_size > Specie_total_size:\n        b = training_size - (j + training_size - Specie_total_size)\n        dummy_variable = random.randint(1,3)\n        if dummy_variable == 1:\n           augmented_dataset = data_augmentation1(x_train[index_storage[Specie]])\n           gc.collect()\n        if dummy_variable == 2:\n           augmented_dataset = data_augmentation2(x_train[index_storage[Specie]])\n           gc.collect()\n        if dummy_variable == 3:\n           augmented_dataset = data_augmentation3(x_train[index_storage[Specie]])\n           gc.collect()\n        x_train = np.concatenate([x_train,augmented_dataset[:b]])\n        for i in range(0,b):\n          y_train = y_train + [one_hot_encoded_array_for_species] \n     else:\n        dummy_variable = random.randint(1,3)\n        if dummy_variable == 1:\n           augmented_dataset = data_augmentation1(x_train[index_storage[Specie]])\n           gc.collect()\n        if dummy_variable == 2:\n           augmented_dataset = data_augmentation2(x_train[index_storage[Specie]])\n           gc.collect()\n        if dummy_variable == 3:\n           augmented_dataset = data_augmentation3(x_train[index_storage[Specie]])\n           gc.collect()\n        x_train = np.concatenate([x_train,augmented_dataset])\n        for i in range(0,training_size):\n          y_train = y_train + [one_hot_encoded_array_for_species]\n     j = j + training_size\n     gc.collect()\n     \n  print(x_train.shape)\n  \n","metadata":{"id":"K_IzSUD300Uc","execution":{"iopub.status.busy":"2022-06-28T12:47:02.140316Z","iopub.execute_input":"2022-06-28T12:47:02.140704Z","iopub.status.idle":"2022-06-28T12:47:02.155285Z","shell.execute_reply.started":"2022-06-28T12:47:02.140675Z","shell.execute_reply":"2022-06-28T12:47:02.153535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If we apply this procedure for every plant specie with the same parameter \"Specie_total_size\", we get a fully balanced dataset with augmented images. However \"Specie_total_size\" parameter shouldn't be too large due to the limited RAM resources available. the higher \"Specie_total_size\" is, the more computations should be made on the specie image batch, thus the more augmented images should be loaded into RAM. Each augmented batch consumes considerable memory space, therefore an overflow could be easily attained.","metadata":{"id":"LUUOcvXpgdrr"}},{"cell_type":"code","source":"augment_data(x_train,y_train,\"Black-grass\",len(index_storage[\"Black-grass\"]),1000)","metadata":{"id":"yMyMTMop1GZl","execution":{"iopub.status.busy":"2022-06-28T12:47:06.332582Z","iopub.execute_input":"2022-06-28T12:47:06.333938Z","iopub.status.idle":"2022-06-28T12:47:07.889866Z","shell.execute_reply.started":"2022-06-28T12:47:06.333889Z","shell.execute_reply":"2022-06-28T12:47:07.888496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"after augmenting black grass specie sample up to 1000 image we get a training set of size equal 5478 = 4750 (initial sample) + 728 (additional black grass augmented images)","metadata":{"id":"qBnb4Kt4VGHe"}},{"cell_type":"markdown","source":"# 1) Image processing\n   # 1.1) Image blurring ","metadata":{"id":"t-iGyq44cJH6"}},{"cell_type":"markdown","source":"We start by applying gaussian filter to smoothen the image. As we can see the plant images contains many useless edges in the background, that are formed by the rocks. These edges would just create background noise, therefore we transform the discontinous pixels values into continous ones.  ","metadata":{"id":"j6kptIhLmOhD"}},{"cell_type":"code","source":"x_train_blurred = tfa.image.gaussian_filter2d(x_train,filter_shape=(4,4),sigma=100)\nx_test_blurred = tfa.image.gaussian_filter2d(x_test,filter_shape=(4,4),sigma=100)\nvizualize_random_images(x_train,x_train_blurred,2)\n","metadata":{"id":"4__P3sxNcIkd","execution":{"iopub.status.busy":"2022-06-28T12:47:18.981046Z","iopub.execute_input":"2022-06-28T12:47:18.981482Z","iopub.status.idle":"2022-06-28T12:47:21.497098Z","shell.execute_reply.started":"2022-06-28T12:47:18.98145Z","shell.execute_reply":"2022-06-28T12:47:21.495665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our next step will consist of detecting the usefull information in the image, and eliminating the background. We need to construct a binary image (black and white) that indicate the object, that we wish to classify. We will be able to observe the shape of the usefull object alone, and the classification process will be much easier.\nIn order to achieve this, we convert RGB image to HSV format, a much helpful format to distinguish between colors. Luckily, the object detection problem at hand is simple one, because the object is clearly distinguishable from the background, no overlap so far (meaning no green background that could overlap with plants green color). Unfortunately, we would still have some noise ,imperfections in the image, thus as follow up, we erode the image a little bit ( to eliminate the isolated portions of the image that doesn't correspond to parts of the plant, noise in other words ) and then right after apply dilation to put more emphasis on the object shape.","metadata":{"id":"H_eydBvanFD4"}},{"cell_type":"code","source":"def segment_image(img): \n  img2 = cv2.cvtColor(np.float32(img),cv2.COLOR_RGB2HSV)\n  green_segment_values = 50\n  hsv_lower_bound = np.array([100-green_segment_values,0,0])\n  hsv_upper_bound = np.array([100+green_segment_values,255,240])\n  mask = cv2.inRange(img2,hsv_lower_bound,hsv_upper_bound)\n  kernel = np.ones((3,3),np.uint8)\n  mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n  return mask\n\n  ","metadata":{"id":"-8OQifymsX4J","execution":{"iopub.status.busy":"2022-06-28T12:47:29.483838Z","iopub.execute_input":"2022-06-28T12:47:29.484211Z","iopub.status.idle":"2022-06-28T12:47:29.49367Z","shell.execute_reply.started":"2022-06-28T12:47:29.484182Z","shell.execute_reply":"2022-06-28T12:47:29.491524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We define a function that applies the processing pipeling to each image in the dataset and returns the processed dataset.","metadata":{"id":"JZlg8-z53GWe"}},{"cell_type":"code","source":"def preprocess_dataset(x_train):\n  preprocessed_data = []\n  for i in range(0,x_train.shape[0]):\n    preprocessed_data = preprocessed_data + [segment_image(x_train[i])]\n  return preprocessed_data ","metadata":{"id":"_r_Sq1xV2brB","execution":{"iopub.status.busy":"2022-06-28T12:47:31.700975Z","iopub.execute_input":"2022-06-28T12:47:31.701595Z","iopub.status.idle":"2022-06-28T12:47:31.709345Z","shell.execute_reply.started":"2022-06-28T12:47:31.701539Z","shell.execute_reply":"2022-06-28T12:47:31.707636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preprocessed_data = np.array(preprocess_dataset(x_train_blurred))\npreprocessed_test_data = np.array(preprocess_dataset(x_test_blurred))\nvizualize_random_images(x_train,preprocessed_data,2)","metadata":{"id":"98Ch_KpC2hrN","execution":{"iopub.status.busy":"2022-06-28T12:47:33.923203Z","iopub.execute_input":"2022-06-28T12:47:33.924636Z","iopub.status.idle":"2022-06-28T12:47:38.561398Z","shell.execute_reply.started":"2022-06-28T12:47:33.924549Z","shell.execute_reply":"2022-06-28T12:47:38.5597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We split the original dataset into training and validation","metadata":{"id":"Djg5QE9c0hMZ"}},{"cell_type":"code","source":"(x_training, valX, y_training, valY) = train_test_split(x_train,y_train,test_size=0.15, random_state=10)\n","metadata":{"id":"kRNDecgzpmKi","execution":{"iopub.status.busy":"2022-06-28T12:49:45.339896Z","iopub.execute_input":"2022-06-28T12:49:45.340297Z","iopub.status.idle":"2022-06-28T12:49:45.405789Z","shell.execute_reply.started":"2022-06-28T12:49:45.340267Z","shell.execute_reply":"2022-06-28T12:49:45.404508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Then we embedd the training and validation set into an ImageDataGenerator object to generate batches of augmented images.","metadata":{"id":"_TzEBWBI1Z0o"}},{"cell_type":"code","source":"datagen = tf.keras.preprocessing.image.ImageDataGenerator(rotation_range=20,horizontal_flip=True,vertical_flip=True,rescale=1/255)\ndatagen.fit(x_training)\nit = datagen.flow(x_training,y_training,batch_size=32)\ndatagenval = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1/255)\ndatagenval.fit(valX)\nitval = datagenval.flow(valX, valY)\ndatagentest = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1/255)\ndatagentest.fit(x_test)\nittest = datagentest.flow(x_test)\n\n\n\n","metadata":{"id":"GC0ND5R5usVp","execution":{"iopub.status.busy":"2022-06-28T12:49:47.18973Z","iopub.execute_input":"2022-06-28T12:49:47.190106Z","iopub.status.idle":"2022-06-28T12:49:47.952905Z","shell.execute_reply.started":"2022-06-28T12:49:47.190078Z","shell.execute_reply":"2022-06-28T12:49:47.951659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We split the newly processed dataset and embedds into an ImageDataGenerator.","metadata":{"id":"WoXueThh3ujo"}},{"cell_type":"code","source":"(x_training_processed, valX_processed, y_training_processed, valY_processed) = train_test_split(preprocessed_data,y_train,test_size=0.15, random_state=10)\n\n","metadata":{"id":"BS--MbjRfgn1","execution":{"iopub.status.busy":"2022-06-28T12:49:51.030546Z","iopub.execute_input":"2022-06-28T12:49:51.031112Z","iopub.status.idle":"2022-06-28T12:49:51.073599Z","shell.execute_reply.started":"2022-06-28T12:49:51.031063Z","shell.execute_reply":"2022-06-28T12:49:51.071667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"datagen = tf.keras.preprocessing.image.ImageDataGenerator(rotation_range=20,horizontal_flip=True,vertical_flip=True,rescale=1/255)\nx_training_processed = tf.reshape(x_training_processed,shape=(4037, 100, 100,1))\ndatagen.fit(x_training_processed)\ntraining_processed_data_flow = datagen.flow(x_training_processed,y_training_processed)\ndatagenval = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1/255)\nvalX_processed = tf.reshape(valX_processed,shape=(713, 100, 100,1))\ndatagenval.fit(valX_processed)\nvalidation_processed_data_flow = datagenval.flow(valX_processed,valY_processed)\ndatagentest = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1/255)\npreprocessed_test_data = tf.reshape(preprocessed_test_data,shape=(794, 100, 100,1))\ndatagentest.fit(preprocessed_test_data)\ntest_processed_data_flow = datagentest.flow(preprocessed_test_data)","metadata":{"id":"IXceKE6qlvji","execution":{"iopub.status.busy":"2022-06-28T13:02:42.129197Z","iopub.execute_input":"2022-06-28T13:02:42.129818Z","iopub.status.idle":"2022-06-28T13:02:42.470363Z","shell.execute_reply.started":"2022-06-28T13:02:42.12976Z","shell.execute_reply":"2022-06-28T13:02:42.468919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We do this in order to have both instances of data available and be able to compare. Next we set the parameters, that will be used during training.","metadata":{"id":"7N7gSqNk59IJ"}},{"cell_type":"code","source":"INIT_LR = 1e-2 # initial learning rate\nBATCH_SIZE = 32\nEPOCHS = 150\n\ns = tf.keras.backend.clear_session()  # clear default graph\n# don't call K.set_learning_phase() !!! (otherwise will enable dropout in train/test simultaneously)\n\nopt = tf.keras.optimizers.Adam(learning_rate=INIT_LR)\n# prepare model for fitting (loss, optimizer, etc)\ntqdm_callback = tfa.callbacks.TQDMProgressBar()\n\n\n# scheduler of learning rate (decay with epochs)\ndef lr_scheduler(epoch):\n     #return (INIT_LR*0.9)**epoch\n   return (INIT_LR * 1.1)  \n    #return INIT_LR\n# callback for printing of actual learning rate used by optimizer\nclass LrHistory(keras.callbacks.Callback):\n    def on_epoch_begin(self, epoch, logs={}):\n        print(\"Learning rate:\", tf.keras.backend.get_value(model.optimizer.lr))\nclass GarbageCollectorCallback(keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs=None):\n        gc.collect()\n","metadata":{"id":"tqtUn7Czv0RA","execution":{"iopub.status.busy":"2022-06-28T13:02:44.297202Z","iopub.execute_input":"2022-06-28T13:02:44.29765Z","iopub.status.idle":"2022-06-28T13:02:44.312444Z","shell.execute_reply.started":"2022-06-28T13:02:44.297619Z","shell.execute_reply":"2022-06-28T13:02:44.311047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2) Modeling","metadata":{"id":"mNn3tWxlu2f5"}},{"cell_type":"markdown","source":"Next we train multiple models on both versions of dataset.","metadata":{"id":"kVdJgz8nOT_E"}},{"cell_type":"code","source":"def make_svm_model():\n    \n    model = tf.keras.Sequential()\n    model.add(tf.keras.layers.Flatten())    \n    model.add(tf.keras.layers.Dense(12,kernel_regularizer=l2(100),activation= 'linear'))\n  \n    return model","metadata":{"id":"aiKXaLV_Hbvr","execution":{"iopub.status.busy":"2022-06-28T13:02:48.588443Z","iopub.execute_input":"2022-06-28T13:02:48.589118Z","iopub.status.idle":"2022-06-28T13:02:48.595816Z","shell.execute_reply.started":"2022-06-28T13:02:48.589083Z","shell.execute_reply":"2022-06-28T13:02:48.594348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"s = tf.keras.backend.clear_session()\nmodel = make_svm_model()\nmodel.build(input_shape=(None,100,100,1))\nmodel.summary()\n\nmodel.compile(\n    loss='categorical_hinge',\n    optimizer=opt,  # for SGD\n    metrics=[tfa.metrics.F1Score(num_classes=12,average=\"micro\")]  # report accuracy during training\n)\nhistory = model.fit(\n    training_processed_data_flow,  # prepared data\n    epochs=150,\n    validation_data=validation_processed_data_flow,\n    #steps_per_epoch=len(x_training) // BATCH_SIZE,\n    callbacks=[keras.callbacks.LearningRateScheduler(lr_scheduler), \n               LrHistory(), \n               tqdm_callback,\n               GarbageCollectorCallback(),\n               ],\n    shuffle=True\n    ,verbose=0)\n\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.show() ","metadata":{"id":"V_iRFR_7xjfY","execution":{"iopub.status.busy":"2022-06-28T13:02:50.236162Z","iopub.execute_input":"2022-06-28T13:02:50.236563Z","iopub.status.idle":"2022-06-28T13:13:05.268105Z","shell.execute_reply.started":"2022-06-28T13:02:50.236533Z","shell.execute_reply":"2022-06-28T13:13:05.266668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_customized_cnn_model(image_number_channel):\n    \n\n    model = tf.keras.Sequential()\n    \n    model.add(tf.keras.layers.Conv2D(64, (3, 3), strides = (1, 1), padding=\"same\", name = 'conv1', input_shape=(100,100,image_number_channel)))   \n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.Activation(\"relu\"))\n    \n\n    model.add(tf.keras.layers.Conv2D(64, (3, 3), strides = (1, 1), padding=\"same\", name = 'conv2'))   \n    model.add(tf.keras.layers.MaxPooling2D((2, 2),padding=\"same\",name='max_pool_1'))\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.Activation(\"relu\"))\n    \n    \n   \n    \n\n    model.add(tf.keras.layers.Dropout(0.10, noise_shape=None, seed=0))\n\n\n    model.add(tf.keras.layers.Conv2D(128, (3, 3), strides = (1, 1), padding=\"same\", name = 'conv3'))   \n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.Activation(\"relu\"))\n   \n    model.add(tf.keras.layers.Conv2D(128, (3, 3), strides = (1, 1), padding=\"same\", name = 'conv4'))\n    model.add(tf.keras.layers.MaxPooling2D((2, 2),padding=\"same\",name='max_pool_2'))   \n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.Activation(\"relu\"))\n    \n   \n    \n    model.add(tf.keras.layers.Dropout(0.10, noise_shape=None, seed=0))\n    \n    \n    model.add(tf.keras.layers.Conv2D(256, (3, 3), strides = (1, 1), padding=\"same\", name = 'conv5', input_shape=(100,100,1)))   \n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.Activation(\"relu\"))\n   \n    model.add(tf.keras.layers.Conv2D(256, (3, 3), strides = (1, 1), padding=\"same\", name = 'conv6', input_shape=(100,100,1))) \n    model.add(tf.keras.layers.MaxPooling2D((2, 2),padding=\"same\",name='max_pool_3'))\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.Activation(\"relu\"))\n    \n    \n\n    model.add(tf.keras.layers.Dropout(0.10, noise_shape=None, seed=0))\n\n    model.add(tf.keras.layers.Flatten())    \n    \n    \n    model.add(tf.keras.layers.Dense(256, name='fc1'))\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.Dropout(0.10, noise_shape=None, seed=0))\n    model.add(tf.keras.layers.Dense(256, name='fc2'))\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.Dropout(0.10, noise_shape=None, seed=0))\n\n    model.add(tf.keras.layers.Dense(12))    \n    model.add(tf.keras.layers.Activation(\"softmax\"))  \n    return model","metadata":{"id":"PzGajYlrw73R","execution":{"iopub.status.busy":"2022-06-28T13:16:09.465685Z","iopub.execute_input":"2022-06-28T13:16:09.466151Z","iopub.status.idle":"2022-06-28T13:16:09.491879Z","shell.execute_reply.started":"2022-06-28T13:16:09.466104Z","shell.execute_reply":"2022-06-28T13:16:09.490316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"s = tf.keras.backend.clear_session()\nmodel = make_customized_cnn_model(3)\nmodel.summary()\ntqdm_callback = tfa.callbacks.TQDMProgressBar()\n\nmodel.compile(\n    loss='categorical_crossentropy',  \n    optimizer=opt,  \n    metrics=[tfa.metrics.F1Score(num_classes=12,average=\"micro\")] \n)\nhistory = model.fit(\n    it,  \n    epochs=200,\n    validation_data=itval,\n    steps_per_epoch=len(x_training) // BATCH_SIZE,\n    callbacks=[keras.callbacks.LearningRateScheduler(lr_scheduler), \n               LrHistory(), \n               tqdm_callback,\n               GarbageCollectorCallback(),\n               ],\n    shuffle=True\n    ,verbose=0)\n\nplt.plot(history.history['loss'],label=\"training loss\")\nplt.plot(history.history['val_loss'],label=\"validation loss\")\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.show() ","metadata":{"id":"QsLm317qwMbS","execution":{"iopub.status.busy":"2022-06-28T13:16:16.99943Z","iopub.execute_input":"2022-06-28T13:16:16.999833Z","iopub.status.idle":"2022-06-28T14:11:42.372919Z","shell.execute_reply.started":"2022-06-28T13:16:16.999801Z","shell.execute_reply":"2022-06-28T14:11:42.371646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"s = tf.keras.backend.clear_session()\nmodel = make_customized_cnn_model(1)\nmodel.summary()\n\nmodel.compile(\n    loss='categorical_crossentropy',  \n    optimizer=opt,  \n    metrics=[tfa.metrics.F1Score(num_classes=12,average=\"micro\")]  \n)\nhistory = model.fit(\n    training_processed_data_flow, \n    epochs=250,\n    validation_data=validation_processed_data_flow,\n    callbacks=[keras.callbacks.LearningRateScheduler(lr_scheduler), \n               LrHistory(), \n               tqdm_callback,\n               GarbageCollectorCallback(),\n               ],\n    shuffle=True\n    ,verbose=0)\n\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.show() ","metadata":{"id":"qSsMtlAvyrM-","execution":{"iopub.status.busy":"2022-06-27T22:23:57.192875Z","iopub.execute_input":"2022-06-27T22:23:57.193269Z","iopub.status.idle":"2022-06-27T22:50:12.533347Z","shell.execute_reply.started":"2022-06-27T22:23:57.193226Z","shell.execute_reply":"2022-06-27T22:50:12.532442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Inception model doesn't accept binary images as input. It always work with 3 channels","metadata":{"id":"OyVH1J6fAn4g"}},{"cell_type":"code","source":"def make_Inception_based_model():\n\n    model = tf.keras.Sequential()\n    \n    model.add(tf.keras.applications.InceptionV3(weights='imagenet',include_top=False,input_shape=(100,100,3)))\n    model.add(tf.keras.layers.Flatten())    \n\n    \n    model.add(tf.keras.layers.Dense(256, name='fc1'))\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.Dropout(0.10, noise_shape=None, seed=0))\n    model.add(tf.keras.layers.Dense(256, name='fc2'))\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.Dropout(0.10, noise_shape=None, seed=0))\n    # FC\n    model.add(tf.keras.layers.Dense(12))  # the last layer with neuron for each class    \n    model.add(tf.keras.layers.Activation(\"softmax\"))  # output probabilities\n    return model\n    \n    \n    \n   \n    \n    \n    ","metadata":{"id":"MboxNMrsPz6v","execution":{"iopub.status.busy":"2022-06-27T23:00:41.716722Z","iopub.execute_input":"2022-06-27T23:00:41.717081Z","iopub.status.idle":"2022-06-27T23:00:41.727343Z","shell.execute_reply.started":"2022-06-27T23:00:41.717051Z","shell.execute_reply":"2022-06-27T23:00:41.726083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"s = tf.keras.backend.clear_session()\nmodel = make_Inception_based_model()\nmodel.summary()\ntqdm_callback = tfa.callbacks.TQDMProgressBar()\n\nmodel.compile(\n    loss='categorical_crossentropy',  \n    optimizer=opt,  \n    metrics=[tfa.metrics.F1Score(num_classes=12,average=\"micro\")]  \n)\nhistory = model.fit(\n    it,  # prepared data\n    epochs=250,\n    validation_data=itval,\n    steps_per_epoch=len(x_training) // BATCH_SIZE,\n    callbacks=[keras.callbacks.LearningRateScheduler(lr_scheduler), \n               LrHistory(), \n               tqdm_callback,\n               GarbageCollectorCallback(),\n               ],\n    shuffle=True\n    ,verbose=0)\n\nplt.plot(history.history['loss'],label=\"training loss\")\nplt.plot(history.history['val_loss'],label=\"validation loss\")\nplt.title('model loss')\nplt.ylabel('loss')","metadata":{"id":"Pe5iYwXGZquJ","execution":{"iopub.status.busy":"2022-06-27T23:06:13.115791Z","iopub.execute_input":"2022-06-27T23:06:13.116133Z","iopub.status.idle":"2022-06-28T00:15:31.120333Z","shell.execute_reply.started":"2022-06-27T23:06:13.116104Z","shell.execute_reply":"2022-06-28T00:15:31.119445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In general the fluctuations observed in the loss function are explained by the use of Stochastic gradient descent optimizer, that makes use of sampling during optimization. the sample are generated from the training data at every iteration therefore the gradient formulas changes significantly from one iteration to another which generate sudden fluctuations.","metadata":{"id":"dgLwqIAzuPNl"}},{"cell_type":"markdown","source":"save the model into google drive (or another location) via the specified path location, so you can load it back later and use it for future use. I saved three models : inception model, CNN model that trains on raw images, CNN models that trains on processed images, in google drive, then I used a simple python script that loads these models and use them for classification. The script file is named load_model.py : https://github.com/KarimAILab/Plant-seedling-classification.git","metadata":{"id":"KXs67kzUB__6"}},{"cell_type":"markdown","source":"We can then infere on unseen images","metadata":{"id":"b9F7AidVbkBG"}},{"cell_type":"code","source":"y_pred_test = model.predict(ittest)\ny_pred_test_classes = np.argmax(y_pred_test, axis=1)","metadata":{"id":"OYFqRnRzjBaY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In brief, the models inceptions and cnn were the much more promosing.","metadata":{"id":"iQJi6-8wHaA9"}},{"cell_type":"markdown","source":"That's it, thanks for giving time to this notebook, I hope it got you ready and inspired to create more advanced model!, feel free to ask anything in the comment section, or provide me with some more guidance or tips on how I could better present my work.\n\n","metadata":{"id":"DOnWIVQ4Gxgt"}}]}