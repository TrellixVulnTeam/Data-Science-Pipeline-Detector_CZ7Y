{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#Importing required Dependencies\n%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.axes_grid1 import ImageGrid\nplt.rcParams['figure.figsize'] = [16, 10]\nplt.rcParams['font.size'] = 16\n\nimport os\nfrom tqdm import tqdm # Fancy progress bars\n\nimport seaborn as sns\nfrom keras.preprocessing import image\nfrom keras.applications import xception\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\n# Our data files are available in the \"../input/\" directory.\nprint(os.listdir(\"../input\"))\n# For Kaggle kernel purposes any results we write to the current directory is saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = 'all'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Loading the Keras Pretrained Model into Kaggle Kernels\n#Copying the Keras pretrained models into the cache directories and displaying the pretrained models that we have prepared in our file directory\n\n!ls ../input/keras-pretrained-models/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating the keras cache directories in Kaggle Kernels to load the pretrained models\ncache_dir = os.path.expanduser(os.path.join('~', '.keras')) #The Cache directory\nif not os.path.exists(cache_dir):\n    os.makedirs(cache_dir)\nmodels_dir = os.path.join(cache_dir, 'models') #The Models directory\nif not os.path.exists(models_dir):\n    os.makedirs(models_dir)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Copying a selection of our pretrained models files onto the keras cache directory for Keras to access\n!cp ../input/keras-pretrained-models/xception* ~/.keras/models","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Displaying the pretrained models\n!ls ~/.keras/models","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls ../input/plant-seedlings-classification","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Preparing the dataset for the model\n#Defining Y-labels of the classes of the dataset\n#Defining the NUM_CLASSES, i.e. total classes in the dataset\nCATEGORIES = ['Black-grass', 'Charlock', 'Cleavers', 'Common Chickweed', 'Common wheat', 'Fat Hen', 'Loose Silky-bent',\n             'Maize', 'Scentless Mayweed', 'Shepherds Purse', 'Small-flowered Cranesbill', 'Sugar beet']\nNUM_CATEGORIES = len(CATEGORIES)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SAMPLE_PER_CATEGORY = 200\nSEED = 7\ndata_dir = '../input/plant-seedlings-classification/'\ntrain_dir = os.path.join(data_dir, 'train')\ntest_dir = os.path.join(data_dir, 'test')\nsample_submission = pd.read_csv(os.path.join(data_dir, 'sample_submission.csv'))\n\nprint(train_dir)\nprint(test_dir)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Displaying the training data: Note that the training images are organized into sub-folders within the main folder,organized by plant species.\n\nfor category in CATEGORIES:\n    print('{} {} images'.format(category, len(os.listdir(os.path.join(train_dir, category)))))\n    # \"Print length of this directory -- an integer output\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating Aggregate Training Sample for CNN\n#Traversing over the directories and folders containing the training set data to collate all the image-files\n#and their corresponding class index and class_names into training-set collection, and converting into a pandas DataFrame.\n\n#Aggregating the data (i.e.: filenames) and their labels.\n\n\ntrain = []\nfor category_id, category in enumerate(CATEGORIES): #category_id is the integer index corresponding to each class_name\n    for file in os.listdir(os.path.join(train_dir, category)): \n        train.append(['train/{}/{}'.format(category, file), category_id, category]) #Renaming the file names and adding to the train list\n        \ntrain = pd.DataFrame(train, columns = ['file', 'category_id', 'category']) #Defining a pandas DataFrame over training data\ntrain.head(5) #Print preview of the training DataFrame\ntrain.shape ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating our training set\ntrain = pd.concat([train[train['category'] == c][:SAMPLE_PER_CATEGORY] for c in CATEGORIES])\ntrain = train.sample(frac=1) #Returning a random sample of items from an axis using pandas function with axis defaults to =0\n\ntrain.index = np.arange(len(train)) #Specifying the DataFrame's index\ntrain.head(5)\ntrain.shape #m decreased because we selected a random sample from the aggregate training-set","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating our test set\n#Collating all the test examples into a neatly organized pandas DataFrame with appropriate headers\n\ntest = []\nfor file in os.listdir(test_dir):\n    test.append(['test/{}'.format(file), file])\ntest = pd.DataFrame(test, columns=['filepath', 'file'])\ntest.head(5)\ntest.shape #We would expect (m, 2) with m being the number of test examples, and 2 being the filepath and file columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Reading an Image to an Array\n# Image is a keras.preprocessing object containing function for preprocessing images for use in keras / tf models\n# Essentially, converting images into their corresponding 3-D numpy arrays\n#concating the filepaths, and the function will spit out the image file's array format\n\ndef read_img(filepath, size):\n    img = image.load_img(os.path.join(data_dir, filepath), target_size = size)\n    img = image.img_to_array(img)\n    return img","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Loading and Visualizing Sample Images (Training Examples)\n# Using matplotlib\n\nfig = plt.figure(1, figsize=(NUM_CATEGORIES, NUM_CATEGORIES)) # Displaying a square matrix with num_categories number of\n#images for each category, across all categories\ngrid = ImageGrid(fig, 111, nrows_ncols=(NUM_CATEGORIES, NUM_CATEGORIES), axes_pad=0.05) #Set-up grid using 'fig'\ni = 0 # Initialize counter\n\n#Iterate through the files in the categories\nfor category_id, category in enumerate(CATEGORIES):\n    for filepath in train[train['category'] == category]['file'].values[:NUM_CATEGORIES]:\n        ax = grid[i]\n        img = read_img(filepath, (224,224)) \n        ax.imshow(img/255.)\n        ax.axis('off')\n        if i % NUM_CATEGORIES == NUM_CATEGORIES - 1: #Labeling the row-categories\n            ax.text(250, 112, filepath.split('/')[1], verticalalignment='center')\n        i += 1\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Train-Validation Split\n#A bit more sophisticated / randomized method of splitting train-dev than simply picking the split index to be\n#len(trainset) * split_percentage\n\nnp.random.seed(seed=SEED)\nrnd = np.random.random(len(train)) \ntrain_idx = rnd < 0.8 #Indices in which rnd is <0.8 (which should come out to roughly 80% of the dataset)\nvalid_idx = rnd >= 0.8\nytr = train.loc[train_idx, 'category_id'].values #pandas function calls\nyv = train.loc[valid_idx, 'category_id'].values\nlen(ytr)\nlen(yv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Run Examples through the Pre-trained Xception Model to Extract Xception Features / Representations:\n#Specify parameters:\nINPUT_SIZE = 299\nPOOLING = 'avg'\nx_train = np.zeros((len(train), INPUT_SIZE, INPUT_SIZE, 3), dtype='float32')\n# Initialize aggregate trainset object of shape (m_total, height, width, channels)\n\n# Filling the numpy array with image files converted into their image-3D arrays\nfor i, file in tqdm(enumerate(train['file'])): # tqdm is a progress bar\n    img = read_img(file, (INPUT_SIZE, INPUT_SIZE))\n    x = xception.preprocess_input(np.expand_dims(img.copy(), axis=0)) #Pre-process that into a format for Xception model\n    x_train[i] = x #Set the i-th example in our initialized zero-4D-array to the particular example\nprint('Train Images shape: {} size: {:,}'.format(x_train.shape, x_train.size))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Spliting X into training and validation\n\nXtr = x_train[train_idx]\nXv = x_train[valid_idx]\nprint((Xtr.shape, Xv.shape, ytr.shape, yv.shape)) # Print shapes to confirm dims are correct","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Forward propagation through pre-trained Xception model for feature-extraction\n#Defining Xception object based on \"off-the-shelf\" pre-trained Xception model\n\nxception_bottleneck = xception.Xception(weights='imagenet', include_top=False, pooling=POOLING) \ntrain_x_bf = xception_bottleneck.predict(Xtr, batch_size=32, verbose=1) #Fwdprop through Xception for feature-extraction\nvalid_x_bf = xception_bottleneck.predict(Xv, batch_size=32, verbose=1)\n\n#Checking output dims:\nprint(\"Xception train bottleneck-features shape: {} size: {:,}\".format(train_x_bf.shape, train_x_bf.size))\nprint(\"Xception valid bottleneck-features shape: {} size: {:,}\".format(valid_x_bf.shape, valid_x_bf.size))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#LogReg Classification on (\"using\") Resulting Xception-bottleneck Features:\n#Defining logistic regression object\n\nlogreg = LogisticRegression(multi_class='multinomial', solver='lbfgs', random_state=SEED)\nlogreg.fit(train_x_bf, ytr) # We need to fit the classifier to our (X,Y pairs)\nvalid_probs = logreg.predict_proba(valid_x_bf) # Classification on our dev set -- probabilities of various classes\nvalid_preds = logreg.predict(valid_x_bf) # Classification on our dev set -- predicted classes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Finding out the accuracy using accuracy_score is an object we've imported from sk-learn\n\nprint(\"Validation Xception Accuracy: {}\".format(accuracy_score(yv, valid_preds)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Illustrating the Results: Confusion Matrix\ncnf_matrix = confusion_matrix(yv, valid_preds) # Confusion matrix imported from sk-learn\nabbreviation = ['BG', 'Ch', 'Cl', 'CC', 'CW', 'FH', 'LSB', 'M', 'SM', 'SP', 'SFC', 'SB']\npd.DataFrame({'class': CATEGORIES, 'abbreviation': abbreviation})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plotting the confusion matrix to illustrate correct and incorrect predictions\n\nfig, ax = plt.subplots(1)\nax = sns.heatmap(cnf_matrix, ax=ax, cmap=plt.cm.Greens, annot=True)\nax.set_xticklabels(abbreviation)\nax.set_yticklabels(abbreviation)\nplt.title('Confusion Matrix')\nplt.ylabel('True Class')\nplt.xlabel('Predicted Class')\nfig.savefig('Confusion matrix.png', dpi=300)\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Finalization and Creating the Submission\n#Creating the X input objects for the test data\n\nx_test = np.zeros((len(test), INPUT_SIZE, INPUT_SIZE, 3), dtype='float32')\nfor i, filepath in tqdm(enumerate(test['filepath'])):\n    img = read_img(filepath, (INPUT_SIZE, INPUT_SIZE))\n    x = xception.preprocess_input(np.expand_dims(img.copy(), axis=0))\n    x_test[i] = x\nprint(\"Test images dataset shape: {} size: {:,}\".format(x_test.shape, x_test.size))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Running forwardprop on the test set input through Xception to get encoded-feature-representation\n\ntest_x_bf = xception_bottleneck.predict(x_test, batch_size=32, verbose=1)\nprint('Xception test bottleneck features shape: {} size: {:,}'.format(test_x_bf.shape, test_x_bf.size))\n\n#Running encoded-feature-representations through the Logistic-Regression classifier (by sk-learn)\n\ntest_preds = logreg.predict(test_x_bf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating the submission file\n\ntest['category_id'] = test_preds\ntest['species'] = [CATEGORIES[c] for c in  test_preds]\ntest[['file', 'species']].to_csv('submission_101703017.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}