{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#importing the nesce\nimport numpy as np\nimport itertools\nimport pandas as pd\nimport os\nimport math\nimport random\nimport cv2\nimport sys\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport tensorflow as tf\nimport keras_preprocessing\nfrom keras_preprocessing import image\nfrom keras_preprocessing.image import ImageDataGenerator","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dir = \"../input/plant-seedlings-classification/train/\"\ntest_dir = \"../input/plant-seedlings-classification/test/\"\nsave_dir = \"/kaggle/working/plant-seedlings-classification/train\"\ntarget_size = (224, 224)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get names of all the categories \ncategories = [category for category in sorted(os.listdir(train_dir))]\n\n# Get the number of images in each cateogry\nimages_per_category = [len(os.listdir(os.path.join(train_dir, category))) for category in categories]\n\n# Plot to see the distribution\nplt.figure(figsize=(24,12))\nsns.barplot(categories, images_per_category)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocessing_pipeline(path, target_size):\n    \"\"\"Accepts a path and returns a processed image involving reading and resizing\"\"\"\n    image = cv2.resize(cv2.imread(path), target_size, interpolation = cv2.INTER_NEAREST)\n    return image\n\ndef show_sample_images(train_dir,data_og):\n    categories = [category for category in sorted(os.listdir(train_dir))]\n    random_indices = random.sample(range(0, len(data_og)), 4)\n    \n    # Plot some sample images from the dataset\n    _, axs = plt.subplots(1, 4, figsize=(20, 20))\n    for i in range(4):\n        axs[i].imshow(data_og[random_indices[i]])\n\ndata_og = [preprocessing_pipeline(os.path.join(train_dir, category, img_path),target_size) for category in categories for img_path in os.listdir(os.path.join(train_dir, category))]\nshow_sample_images(train_dir,data_og)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom imgaug import augmenters as iaa\nfrom tqdm.notebook import tqdm\n# Augment passed images\ndef augment_images(class_images):\n    seq = iaa.Sequential([\n        iaa.Fliplr(0.5),\n        iaa.Flipud(0.5),\n        iaa.Affine(rotate=(-45, 45)),\n        iaa.TranslateX(percent=(-0.1, 0.1)),\n        iaa.TranslateY(percent=(-0.1, 0.1))\n    ], random_order=True)\n\n    images_aug = seq(images = class_images)\n    return images_aug\n\n# Helper Function 5\n# Randomly sample images from a set of passed images\ndef random_unique_sampling(class_images, remainder):\n    random_unique_indices = random.sample(range(0, len(class_images)), remainder)\n    random_unique_images = [class_images[idx] for idx in random_unique_indices]\n    return random_unique_images\n    \n\ndef augmentation_pipeline(class_images, number_of_images):\n    \"\"\"Accepts a batch of images (of a single class) and returns a required number of augmented images\"\"\"\n\n    if number_of_images == 0:\n            return []\n\n    elif number_of_images >= len(class_images):\n        batches = math.floor(number_of_images / len(class_images))\n        remainder = number_of_images % len(class_images)\n        remainder_images = random_unique_sampling(class_images, remainder)\n        class_images = class_images * batches\n        class_images.extend(remainder_images)\n        images_aug = augment_images(class_images)\n        return images_aug\n\n    else:\n        assert number_of_images < len(class_images)\n        class_images = random_unique_sampling(class_images, number_of_images)\n        images_aug = augment_images(class_images)\n        return images_aug\n\n\ndef balance_dataset(save_dir,train_dir):\n    \"\"\"Create augmented data to balance classes from the passed training data path\"\"\"\n\n    # Make a directory for augmented dataset\n    os.makedirs(save_dir, exist_ok=True)\n\n    # Get categories\n    categories = [category for category in sorted(os.listdir(train_dir))]\n\n    # Get the maximum amount of images that exists in a class\n    max_in_class = max([len(os.listdir(os.path.join(train_dir, category))) for category in categories])\n\n    # Find out the number of images that exist in each class\n    images_per_category = {category : len(os.listdir(os.path.join(train_dir, category))) for category in categories}\n\n    # Find out the augmentations required for each class\n    required_augmentations = dict(zip(categories,  [max_in_class - num_in_class for num_in_class in list(images_per_category.values())]))\n\n    # Augment each unbalanced class and save the new dataset to disk\n    # We preferring saving the data to disk\n    # Because we prefer to not hold large numpy arrays in the RAM\n    # This allows for large models to be loaded and trained on\n    # We use for loops here instead of list comprehensions for readiblity\n    for category in tqdm(categories):\n        try:\n            os.mkdir(os.path.join(save_dir, category))\n        except FileExistsError:\n            pass\n        class_images = list()\n\n        # Preprocessing and Augmentation\n        for img_path in sorted(os.listdir(os.path.join(train_dir, category))):\n            image = preprocessing_pipeline(os.path.join(train_dir, category, img_path),target_size)\n            class_images.append(image)\n        augmented_images = augmentation_pipeline(class_images, required_augmentations[category])\n        class_images.extend(augmented_images)\n\n        # Writing the augmented data to disk\n        for image_number, class_image in enumerate(class_images):\n            cv2.imwrite(os.path.join(save_dir, category, \"{}.png\".format(image_number + 1)), class_image)\n\nbalance_dataset(save_dir,train_dir)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"raw","source":"#InceptionV3 \n\nfrom tensorflow.keras.applications import InceptionV3\nfrom tensorflow.keras.applications import inception_v3\n\ndatagen = ImageDataGenerator(preprocessing_function = inception_v3.preprocess_input, validation_split=0.15)\ntarget_size = (299, 299)\n\ntrain_generator = datagen.flow_from_directory(\n        directory= os.path.join(save_dir),\n        target_size= target_size,\n        class_mode = \"categorical\",\n        batch_size=32,\n        shuffle=True,\n        subset='training'\n    )\n\nval_generator = datagen.flow_from_directory(\n        directory= os.path.join(save_dir),\n        target_size= target_size,\n        class_mode = 'categorical',\n        batch_size=32,\n        shuffle=False,\n        subset='validation'\n    )\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#InceptionV3 \n\nfrom tensorflow.keras.applications import InceptionV3\nfrom tensorflow.keras.applications import inception_v3\n\ndatagen = ImageDataGenerator(preprocessing_function = inception_v3.preprocess_input, validation_split=0.15)\ntarget_size = (299, 299)\n\ntrain_generator = datagen.flow_from_directory(\n        directory= os.path.join(save_dir),\n        target_size= target_size,\n        class_mode = \"categorical\",\n        batch_size=32,\n        shuffle=True,\n        subset='training'\n    )\n\nval_generator = datagen.flow_from_directory(\n        directory= os.path.join(save_dir),\n        target_size= target_size,\n        class_mode = 'categorical',\n        batch_size=32,\n        shuffle=False,\n        subset='validation'\n    )\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Callbacks\n\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n\nmodel_save_path = '/kaggle/working/model_inceptionv3.h5'\ncheckpoint = ModelCheckpoint(filepath='/kaggle/working/model_inceptionv3.h5', monitor='val_loss', mode='min', save_best_only=True)\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1, min_lr=0.00000001)\nearly_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='min', restore_best_weights=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, ReLU, Flatten, Activation\nfrom tensorflow.keras.activations import swish\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.optimizers import Adam\n\nInception_base = InceptionV3(weights='imagenet', include_top=False, pooling='avg', input_shape=(299, 299, 3))\nfor layer in Inception_base.layers[:-22]:\n    layer.trainable = False\nx = Flatten() (Inception_base.output)\nx = Dense(1024)(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\nx = Dropout(0.2)(x)\nx = Dense(256)(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\nx = Dropout(0.2)(x)\npredictions = Dense(12, activation='softmax')(x)\nmodel = Model(inputs = Inception_base.input, outputs = predictions)\n\n# Freeze the earlier layers\n#for layer in model.layers[:-22]:\n    #layer.trainable = False\n    \n    \n# Compile the model    \nmodel.compile(Adam(lr=0.00001), loss='categorical_crossentropy', metrics=['accuracy'])\n\n\n\n# Train the model\nhistory_inception_v3 = model.fit(train_generator,\n                      steps_per_epoch = 196,\n                      validation_data = val_generator,\n                      #validation_steps = 48,\n                      epochs = 5,\n                      verbose = 1,\n                      callbacks = [reduce_lr, checkpoint, early_stop])\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Save the best model\nmodel.save('inception_best_model.h5')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_file_size(file_path):\n    size = os.path.getsize(file_path)\n    return size","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def convert_bytes(size, unit=None):\n    if unit == \"KB\":\n        return print(\"File size: \" + str(round(size/1024,3)) + \" Kilobytes\")\n    elif unit ==\"MB\":\n        return print(\"File size: \" + str(round(size/(1024*1024),3)) + \" Megabytes\")\n    else:\n        return print(\"File size: \" + str(size)+ \" bytes\")            ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"convert_bytes(get_file_size('inception_best_model.h5'), \"MB\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_loss,test_acc = model.evaluate(val_generator,verbose =2)\n\nprint('test accuracy:', test_acc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pathlib\nimport time","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tflite_models_dir = pathlib.Path(os.path.join(os.getcwd(),'tflite_models'))\ntflite_models_dir.mkdir(exist_ok=True, parents=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"converter_vgg = tf.lite.TFLiteConverter.from_keras_model(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert to TF Lite without quantization\nvgg16_tflite_file = tflite_models_dir/\"vgg16.tflite\"\nvgg16_tflite_file.write_bytes(converter_vgg.convert())\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"interpreter = tf.lite.Interpreter(model_path='./tflite_models/vgg16.tflite')\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"== Input details ==\")\nprint(\"shape:\", input_details[0]['shape'])\nprint(\"type:\", input_details[0]['dtype'])\nprint(\"\\n== Output details ==\")\nprint(\"shape:\", output_details[0]['shape'])\nprint(\"type:\", output_details[0]['dtype'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"interpreter.resize_tensor_input(input_details[0]['index'], (32, 299, 299, 3))\ninterpreter.resize_tensor_input(output_details[0]['index'], (32,12 ))\ninterpreter.allocate_tensors()\n\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\nprint(\"== Input details ==\")\nprint(\"shape:\", input_details[0]['shape'])\nprint(\"\\n== Output details ==\")\nprint(\"shape:\", output_details[0]['shape'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_valid, Y_valid = next(iter(val_generator))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set batch of images into input tensor\ninterpreter.set_tensor(input_details[0]['index'], X_valid)\n# Run inference\ninterpreter.invoke()\n# Get prediction results\ntflite_model_predictions = interpreter.get_tensor(output_details[0]['index'])\npredictions = tflite_model_predictions.argmax()\nprint(\"Prediction results shape:\", predictions.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions=pd.get_dummies(tflite_model_predictions)\npredictions = np.array(predictions)\nprint(predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_tflite_model(tflite_file, test_image_indices):\n    global X_valid\n\n  # Initialize the interpreter\n    interpreter = tf.lite.Interpreter(model_path=str(tflite_file))\n    interpreter.allocate_tensors()\n\n    input_details = interpreter.get_input_details()[0]\n    output_details = interpreter.get_output_details()[0]\n\n    predictions = np.zeros((len(test_image_indices)), dtype=int)\n    for i, test_image_index in enumerate(test_image_indices):\n        test_image = X_valid[test_image_index]\n        test_label = Y_valid[test_image_index]\n\n        test_image = np.expand_dims(test_image, axis=0).astype(input_details[\"dtype\"])\n        interpreter.set_tensor(input_details[\"index\"], test_image)\n        interpreter.invoke()\n        output = interpreter.get_tensor(output_details[\"index\"])[0]\n\n        predictions[i] = output.argmax()\n\n    return predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluate_model(tflite_file, model_type):\n    global X_valid\n    global Y_valid\n\n    test_image_indices = range(X_valid.shape[0])\n    predictions = run_tflite_model(tflite_file, test_image_indices)\n    \n    predictions=pd.get_dummies(predictions)\n    predictions = np.array(predictions)\n    print(predictions)\n    \n    accuracy = accuracy_score(y_true=Y_valid, y_pred=predictions) \n\n    #accuracy = (np.sum(Y_valid== predictions) * 100) / len(X_valid)\n\n    return accuracy\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"op = evaluate_model(vgg16_tflite_file, model_type=\"Float\")\nprint(\"Accuracy of TFLite - VGG : {}\".format(op))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nval_label_batch = np.argmax(val_label_batch,axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"interpreter.set_tensor(input_details[0]['index'], val_image_batch)\n# Run inference\ninterpreter.invoke()\n# Get prediction results\ntflite_model_predictions = interpreter.get_tensor(output_details[0]['index'])\nprint(\"Prediction results shape:\", tflite_model_predictions.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rounded_labels=np.argmax(tflite_model_predictions, axis=0)\n\nrounded_labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nacc = accuracy_score(rounded_labels,val_label_batch)\n\nacc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_curves(train_dir, save_dir, history):\n    plt.style.use('seaborn')\n    '''\n    Args:\n    history(History callback): which has a history attribute containing the lists of successive losses and other metrics\n    '''\n    plt.style.use('seaborn')\n    NUM_EPOCHS = len(history.history['loss'])\n    plt.style.use(\"ggplot\")\n    plt.figure(figsize=(16,10))\n    plt.plot(np.arange(0, NUM_EPOCHS), history.history['loss'], label='train_loss')\n    plt.plot(np.arange(0, NUM_EPOCHS), history.history['val_loss'], label='val_loss')\n    plt.plot(np.arange(0, NUM_EPOCHS), history.history['accuracy'], label='train_acc')\n    plt.plot(np.arange(0, NUM_EPOCHS), history.history['val_accuracy'], label='val_acc')\n    plt.title(\"Training Loss and Accuracy on Dataset\")\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Loss/Accuracy\")\n    plt.legend(loc = 'upper left')\n    plt.tight_layout()\n    plt.show()\nplot_curves(train_dir,save_dir,history_inception_v3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_classification_metrics(categories, model, val_generator):\n\n    predictiions = model.predict_generator(val_generator, 48)\n    y_pred = np.argmax(predictiions, axis=1)\n    cf_matrix = confusion_matrix(val_generator.classes, y_pred)\n    print('Classification Report')\n    print(classification_report(val_generator.classes, y_pred, target_names=categories))\n    plt.figure(figsize=(20,20))\n    sns.heatmap(cf_matrix, annot=True, xticklabels=categories, yticklabels=categories, cmap='Blues')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_classification_metrics(categories, model, val_generator)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#ResNet50\nfrom tensorflow.keras.applications import ResNet50\nfrom tensorflow.keras.applications import resnet50\nfrom keras.applications.resnet50 import preprocess_input as resnet_preprocess_input\n\ndatagen = ImageDataGenerator(preprocessing_function = resnet50.preprocess_input, samplewise_center=False, \n                              samplewise_std_normalization=False, \n                              horizontal_flip = True, \n                              vertical_flip = False, \n                              height_shift_range = 0.15, \n                              width_shift_range = 0.15, \n                              rotation_range = 5, \n                              shear_range = 0.01,\n                              fill_mode = 'nearest',\n                              zoom_range=0.2, validation_split=0.15)\ntarget_size = (224, 224)\n\ntrain_generator = datagen.flow_from_directory(\n                directory= os.path.join(save_dir),\n                target_size= target_size,\n                class_mode = \"categorical\",\n                batch_size=32,\n                shuffle=True,\n                subset='training'\n            )\n\nval_generator = datagen.flow_from_directory(\n                directory= os.path.join(save_dir),\n                target_size= target_size,\n                class_mode = 'categorical',\n                batch_size=32,\n                shuffle=False,\n                subset='validation'\n            )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#CallBacks\nmodel_save_path = '/kaggle/working/model_resent50.h5'\ncheckpoint = ModelCheckpoint(filepath=model_save_path, monitor='val_loss', mode='min', save_best_only=True)\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1, min_lr=0.00000001)\nearly_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='min', restore_best_weights=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"resnet50_base = ResNet50(include_top=False, weights='imagenet', input_shape=(224,224,3))\nfor layer in resnet50_base.layers[:-5]:\n    layer.trainable = False\n    \nx = Flatten()(resnet50_base.output)\nx = Dense(1024)(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\nx = Dropout(0.2)(x)\nx = Dense(512)(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\nx = Dropout(0.2)(x)\npredictions = Dense(12, activation='softmax')(x)\n\nmodel_resnet50 = Model(resnet50_base.input, outputs = predictions)\n\n\n\nmodel_resnet50.compile(Adam(lr=0.00001), loss='categorical_crossentropy', metrics=['accuracy'])\n\n\n\n# Train the model\nhistory_resnet50 = model_resnet50.fit(train_generator,\n                      steps_per_epoch = 196,\n                      validation_data = val_generator,\n                      #validation_steps = 48,\n                      epochs = 25,\n                      verbose = 1,\n                      callbacks = [reduce_lr, checkpoint, early_stop])\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Save the best model\nmodel_resnet50.save('resnet_best_model.h5')      \n       ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_curves(train_dir,save_dir,history_resnet50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_classification_metrics(categories, model_resnet50, val_generator)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#VGG16\n\nfrom tensorflow.keras.applications import VGG16\nfrom tensorflow.keras.applications import vgg16\n\ndatagen = ImageDataGenerator(preprocessing_function = vgg16.preprocess_input,samplewise_center=False, \n                              samplewise_std_normalization=False, \n                              horizontal_flip = True, \n                              vertical_flip = False, \n                              height_shift_range = 0.15, \n                              width_shift_range = 0.15, \n                              rotation_range = 5, \n                              shear_range = 0.01,\n                              fill_mode = 'nearest',\n                              zoom_range=0.2, validation_split=0.15)\ntarget_size = (224, 224)\n\ntrain_generator = datagen.flow_from_directory(\n                directory= os.path.join(save_dir),\n                target_size= target_size,\n                class_mode = \"categorical\",\n                batch_size=32,\n                shuffle=True,\n                subset='training'\n            )\n\nval_generator = datagen.flow_from_directory(\n                directory= os.path.join(save_dir),\n                target_size= target_size,\n                class_mode = 'categorical',\n                batch_size=32,\n                shuffle=False,\n                subset='validation'\n            )\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#callbacks\n\nmodel_save_path = '/kaggle/working/model_vgg16.h5'\ncheckpoint = ModelCheckpoint(filepath=model_save_path, monitor='val_loss', mode='min', save_best_only=True)\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1, min_lr=0.00000001)\nearly_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='min', restore_best_weights=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vgg16_base = VGG16(weights='imagenet', include_top=False, pooling='avg', input_shape=(224, 224, 3))\nfor layer in vgg16_base.layers[:-11]:\n    layer.trainable = False\nx = vgg16_base.output\nx = Dropout(0.5)(x)\n    \npredictions = Dense(12, activation='softmax')(x)\nvgg16_model = Model(inputs = vgg16_base.input, outputs = predictions)\n\n\n    \n    \n# Compile the model    \nvgg16_model.compile(Adam(lr=0.00001), loss='categorical_crossentropy', metrics=['accuracy'])\n\n\n\n# Train the model\nhistory_vgg16 = vgg16_model.fit(train_generator,\n                      steps_per_epoch = 196,\n                      validation_data = val_generator,\n                      #validation_steps = 48,\n                      epochs = 25,\n                      verbose = 1,\n                      callbacks = [reduce_lr, checkpoint, early_stop])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Save the best model\nvgg16_model.save('vgg_best_model.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_curves(train_dir,save_dir,history_vgg16)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_classification_metrics(categories, vgg16_model, val_generator)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loaded_vgg16 = load_model(\"vgg_best_model.h5\")\nloaded_resnet50 = load_model(\"resnet_best_model.h5\")\nloaded_inception = load_model(\"inception_best_model.h5\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}