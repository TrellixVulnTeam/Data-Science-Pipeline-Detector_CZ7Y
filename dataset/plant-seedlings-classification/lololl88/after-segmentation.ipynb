{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#importing other required libraries\nimport numpy as np\nimport pandas as pd\nfrom sklearn.utils.multiclass import unique_labels\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport seaborn as sns\nimport itertools\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nimport tensorflow as tf\n#from keras import Sequential laura com\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.python.keras.applications.resnet import ResNet50\n# from tensorflow.python.keras.applications.resnet_v2 import \nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.optimizers import SGD,Adam\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nfrom tensorflow.keras.layers import Flatten, Dense, BatchNormalization, Activation,Dropout\nfrom tensorflow.keras.utils import to_categorical\n\nimport random\nimport cv2\n\n#Import the libraries\nimport zipfile\nimport os\n\nimport pandas as pd\nimport matplotlib.pyplot  as plt\n# from PIL import Image\nfrom pathlib import Path\n# import imagesize\nimport numpy as np\n\nimport cv2\n\nimport os\n\nfrom tensorflow.keras import layers\nfrom tensorflow import keras\nimport tensorflow as tf\nimport random","metadata":{"papermill":{"duration":5.819402,"end_time":"2021-11-05T03:45:48.777265","exception":false,"start_time":"2021-11-05T03:45:42.957863","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-21T09:43:02.692453Z","iopub.execute_input":"2021-11-21T09:43:02.692869Z","iopub.status.idle":"2021-11-21T09:43:08.32862Z","shell.execute_reply.started":"2021-11-21T09:43:02.692764Z","shell.execute_reply":"2021-11-21T09:43:08.327876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def save_model(itr, model_num):\n  checkpoint_path = \"exp_{}/cp_{}.ckp\".format(itr, model_num)\n  print(checkpoint_path)\n  model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n      filepath=checkpoint_path,\n      monitor='val_loss',\n      mode='min', ## ? shld be min???\n      save_best_only=True)\n  return model_checkpoint_callback\n\n\ndef color_segment_function(img_array):\n  '''\n  Generate a mask image for the images that is compatible to ImageDataGenerator\n  :param image: an image read by cv2.imread\n  :return result: masked image\n  '''\n  img_array= np.rint(img_array)\n  img_array= img_array.astype('uint8')\n  hsv_img= cv2.cvtColor(img_array, cv2.COLOR_RGB2HSV)\n  mask = cv2.inRange(hsv_img, (24, 40, 0), (60, 255, 255))\n  result = cv2.bitwise_and(img_array, img_array, mask=mask)\n  result= result.astype('float64')\n  return result","metadata":{"execution":{"iopub.status.busy":"2021-11-21T09:43:08.330362Z","iopub.execute_input":"2021-11-21T09:43:08.330673Z","iopub.status.idle":"2021-11-21T09:43:08.340044Z","shell.execute_reply.started":"2021-11-21T09:43:08.330618Z","shell.execute_reply":"2021-11-21T09:43:08.339336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n\ndef call_backs(file_name):\n  cb = []\n  best_cb= tf.keras.callbacks.ModelCheckpoint(file_name, \n                                    monitor='val_loss', \n                                    verbose=1, \n                                    save_best_only=True, \n                                    save_weights_only=False, \n                                    mode='auto', \n                                    save_freq='epoch')\n  cb.append(best_cb)\n  \n  early = EarlyStopping(monitor=\"val_loss\", \n                        mode=\"min\", \n                        patience=15) # probably needs to be more patience\n  \n  cb.append(early)\n\n  # Reduce LR On Plateau\n  lr_reduced = ReduceLROnPlateau(monitor='val_accuracy', factor=0.1, min_delta=1e-5, patience=8, verbose=1)\n  \n  cb.append(lr_reduced)\n  return cb","metadata":{"execution":{"iopub.status.busy":"2021-11-21T09:43:08.34121Z","iopub.execute_input":"2021-11-21T09:43:08.34176Z","iopub.status.idle":"2021-11-21T09:43:08.363916Z","shell.execute_reply.started":"2021-11-21T09:43:08.34172Z","shell.execute_reply":"2021-11-21T09:43:08.363096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocessing_func(im):\n    INP_SIZE = (224, 224) \n    kernel = np.array([[0, -1, 0],\n                   [-1, 5,-1],\n                   [0, -1, 0]])\n    \n#     im = cv2.imread(filename)\n    im = cv2.resize(im, (INP_SIZE[0] , INP_SIZE[1]))\n    im = cv2.filter2D(src=im, ddepth=-1, kernel=kernel)\n    return im\n\nos.mkdir('/kaggle/working/data')\nos.mkdir('/kaggle/working/data/test')\n!cp /kaggle/input/plant-seedlings-classification/test/*.png /kaggle/working/data/test","metadata":{"papermill":{"duration":5.778411,"end_time":"2021-11-05T03:45:54.562762","exception":false,"start_time":"2021-11-05T03:45:48.784351","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-21T09:43:08.36758Z","iopub.execute_input":"2021-11-21T09:43:08.367868Z","iopub.status.idle":"2021-11-21T09:43:12.239409Z","shell.execute_reply.started":"2021-11-21T09:43:08.36784Z","shell.execute_reply":"2021-11-21T09:43:12.238519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test_preprocessing_func(im):\n    im = color_segment_function(im)\n    im = preprocessing_func(im)\n    return im","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def class_weight_coeff(data_frame):\n  class_weight = {}\n  index_weight ={}\n  for class_name in list(data_frame.species.unique()):\n    count = len(train_df[data_frame.species == class_name])\n    class_weight[class_name] = 1/count\n\n  norm_factor= np.mean(list(class_weight.values()))\n\n  i = 0\n  for class_name in list(data_frame.species.unique()):\n    class_weight[class_name]= class_weight[class_name]/norm_factor\n    index_weight[i] = class_weight[class_name]\n    i = i+1\n  return class_weight, index_weight","metadata":{"execution":{"iopub.status.busy":"2021-11-21T09:43:12.242612Z","iopub.execute_input":"2021-11-21T09:43:12.242854Z","iopub.status.idle":"2021-11-21T09:43:12.251751Z","shell.execute_reply.started":"2021-11-21T09:43:12.242827Z","shell.execute_reply":"2021-11-21T09:43:12.250953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"change this part of code, for the weights","metadata":{}},{"cell_type":"code","source":"def files_to_CSV(src_dir, filename):\n  '''\n  Generate a csv file for train dataset using the directory stucture of\n  of images by mapping each image to the labelled ground truth. The csv \n  is then saved in the current working directory.\n  :param src_dir: the directory of the target csv\n  :param dest_dir: the directory of the CSV being stored\n  :param filename: name of the generated csv\n  :return: void\n  '''\n  #listing filenames from source directory\n  fnames = os.listdir(src_dir)\n  #sorting list in alphabetical order\n  fnames.sort()\n  #creating a dictionary assign images to ground truths\n  img_dict = {}\n  for i in range(len(fnames)):\n    img_path = os.path.join(src_dir, fnames[i])\n    for img_name in os.listdir(img_path):\n      #assigning to key value pair of dictionary\n      img_dict[img_name] = fnames[i]\n      img_df =  pd.DataFrame(img_dict.items(), columns=['file', 'species'])\n      img_df.to_csv(str(filename)+\".csv\", index = False)\n  return ","metadata":{"execution":{"iopub.status.busy":"2021-11-21T09:47:34.413704Z","iopub.execute_input":"2021-11-21T09:47:34.414437Z","iopub.status.idle":"2021-11-21T09:47:34.421974Z","shell.execute_reply.started":"2021-11-21T09:47:34.4144Z","shell.execute_reply":"2021-11-21T09:47:34.420347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Directory of dataset \ncwd_dir = r'/content'\ntrain_dir = r'../input/segmented-individually/segmented_images'\ntest_dir = r'/content/test'","metadata":{"execution":{"iopub.status.busy":"2021-11-21T09:46:24.859255Z","iopub.execute_input":"2021-11-21T09:46:24.859512Z","iopub.status.idle":"2021-11-21T09:46:24.863109Z","shell.execute_reply.started":"2021-11-21T09:46:24.859483Z","shell.execute_reply":"2021-11-21T09:46:24.862433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Generate train.csv\nfiles_to_CSV(src_dir= train_dir, filename='Train')","metadata":{"execution":{"iopub.status.busy":"2021-11-21T09:47:36.648936Z","iopub.execute_input":"2021-11-21T09:47:36.649441Z","iopub.status.idle":"2021-11-21T09:48:11.699258Z","shell.execute_reply.started":"2021-11-21T09:47:36.649402Z","shell.execute_reply":"2021-11-21T09:48:11.698477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv(\"./Train.csv\")\nclass_weight , index_weight = class_weight_coeff(train_df)\nclass_weight_df =pd.DataFrame(list(class_weight.items()),columns = ['class_label','class_weight'])\nclass_weight_df","metadata":{"execution":{"iopub.status.busy":"2021-11-21T09:48:42.650646Z","iopub.execute_input":"2021-11-21T09:48:42.651305Z","iopub.status.idle":"2021-11-21T09:48:42.694623Z","shell.execute_reply.started":"2021-11-21T09:48:42.651267Z","shell.execute_reply":"2021-11-21T09:48:42.69397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_model(train_ds):\n    # define model\n    #Initializing ResNet50\n    base_model_resnet = ResNet50(include_top = False, weights = 'imagenet', input_shape = (224,224,3), classes = len(train_ds.class_indices))\n    #Adding layers to the ResNet50\n    model_resnet=Sequential()\n    #Add the Dense layers along with activation and batch normalization\n    model_resnet.add(base_model_resnet)\n    model_resnet.add(Flatten())\n    #Add the Dense layers along with activation and batch normalization\n    # model_resnet.add(Dense(1024,activation=('relu'),input_dim=512))\n    model_resnet.add(Dense(512,activation=('relu'))) \n    model_resnet.add(Dropout(0.3))\n    model_resnet.add(Dense(512,activation=('relu'))) \n    model_resnet.add(Dropout(0.3))\n    model_resnet.add(Dense(256,activation=('relu')))\n    model_resnet.add(Dropout(.3))\n    model_resnet.add(Dense(256,activation=('relu')))\n    model_resnet.add(Dropout(.3))\n    model_resnet.add(Dense(128,activation=('relu')))\n    model_resnet.add(Dropout(.3))\n    model_resnet.add(Dense(12,activation=('softmax')))\n    return model_resnet","metadata":{"execution":{"iopub.status.busy":"2021-11-21T09:48:47.185648Z","iopub.execute_input":"2021-11-21T09:48:47.186215Z","iopub.status.idle":"2021-11-21T09:48:47.192548Z","shell.execute_reply.started":"2021-11-21T09:48:47.186165Z","shell.execute_reply":"2021-11-21T09:48:47.191854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ImageDataGenerator\nimage_generator = ImageDataGenerator(\n    validation_split = 0.20,\n    rescale = 1.0/255.0,\n    horizontal_flip = True,\n    vertical_flip=True,\n    width_shift_range=0.3,\n    height_shift_range=0.3,\n    preprocessing_function = preprocessing_func,\n    zoom_range=0.1, \n    shear_range = 0.2,\n)\n\nimage_generator_test = ImageDataGenerator(rescale = 1.0/255.0, preprocessing_function = test_preprocessing_func)\n\n# paramaters\nbatch_size= 32\nepochs=50\n\n# train_dir = '../input/segmented-individually/segmented_images'\ntrain_dir = '../input/segmented-images/segmented_images'\nimg_size = [224, 224]\n\ntrain_ds = image_generator.flow_from_directory(\n  train_dir,\n  shuffle = True,\n  target_size=(img_size[0], img_size[1]),\n  class_mode = 'categorical',\n  batch_size=batch_size,\n  subset=\"training\")\n\nval_ds = image_generator.flow_from_directory(\n  train_dir,\n  shuffle = True,\n  target_size=(img_size[0], img_size[1]),\n  class_mode = 'categorical',\n  batch_size=batch_size,\n  subset=\"validation\")\n\nprint(len(train_ds.class_indices))\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-11-21T09:48:48.624555Z","iopub.execute_input":"2021-11-21T09:48:48.625097Z","iopub.status.idle":"2021-11-21T09:48:49.057429Z","shell.execute_reply.started":"2021-11-21T09:48:48.625062Z","shell.execute_reply":"2021-11-21T09:48:49.056237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_resnet = get_model(train_ds)\n#Compiling ResNet50\n# learn_rate=.001\n# sgd=SGD(lr=learn_rate)\n#Compiling ResNet50\nlearning_rate = 0.00001 # initial learning rate\ndecay_rate = 0.1\nmomentum = 0.8\n\n# define the optimizer function\n# sgd = SGD(lr=learning_rate, momentum=momentum, decay=decay_rate, nesterov=False)\n\n#Setting optimzer learning rate & decay\namsgrad= keras.optimizers.Adam(lr= learning_rate, amsgrad=True) \n\nmodel_resnet.compile(optimizer =amsgrad, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n\n# train data size / batch size\ntrain_steps = train_ds.samples//batch_size\nval_steps = val_ds.samples//batch_size\n\ncb = save_model(1,1)\nhistory = model_resnet.fit_generator(\n                train_ds,\n                steps_per_epoch = train_steps,\n                validation_data = val_ds, \n                validation_steps = val_steps,\n                epochs = epochs, \n                callbacks=[cb],\n                class_weight = index_weight)","metadata":{"execution":{"iopub.status.busy":"2021-11-21T09:48:55.335149Z","iopub.execute_input":"2021-11-21T09:48:55.335401Z","iopub.status.idle":"2021-11-21T09:52:33.562369Z","shell.execute_reply.started":"2021-11-21T09:48:55.335373Z","shell.execute_reply":"2021-11-21T09:52:33.561562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_resnet.summary()","metadata":{"execution":{"iopub.status.busy":"2021-11-21T09:52:33.578707Z","iopub.execute_input":"2021-11-21T09:52:33.578958Z","iopub.status.idle":"2021-11-21T09:52:33.602096Z","shell.execute_reply.started":"2021-11-21T09:52:33.578925Z","shell.execute_reply":"2021-11-21T09:52:33.601098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## plotting of graph","metadata":{}},{"cell_type":"code","source":"f,ax=plt.subplots(2,1) #Creates 2 subplots under 1 column\n#Training loss and validation loss\nax[0].plot(history.history['loss'],color='b',label='Training Loss')\nax[0].plot(history.history['val_loss'],color='r',label='Validation Loss')\n#Training accuracy and validation accuracy\nax[1].plot(history.history['accuracy'],color='b',label='Training  Accuracy')\nax[1].plot(history.history['val_accuracy'],color='r',label='Validation Accuracy')","metadata":{"execution":{"iopub.status.busy":"2021-11-21T09:52:33.603529Z","iopub.execute_input":"2021-11-21T09:52:33.604377Z","iopub.status.idle":"2021-11-21T09:52:33.890155Z","shell.execute_reply.started":"2021-11-21T09:52:33.604328Z","shell.execute_reply":"2021-11-21T09:52:33.889399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_size = (224, 224)\ntest_dir = '/kaggle/working/data'\n\ntest_generator = image_generator_test.flow_from_directory(\n    test_dir,\n    target_size=(img_size[0], img_size[1]),\n    class_mode = None,\n    batch_size=1,\n    shuffle=False)\n\nSTEP_SIZE_TEST=test_generator.n//test_generator.batch_size\ntest_generator.reset()\npred=model_resnet.predict_generator(test_generator,\nsteps=STEP_SIZE_TEST,\nverbose=1)\n\npredicted_class_indices=np.argmax(pred,axis=1)\nlabels = (train_ds.class_indices)\nlabels = dict((v,k) for k,v in labels.items())\npredictions = [labels[k] for k in predicted_class_indices]\n\nfilenames=test_generator.filenames\nfilenames = [i.split('/')[1] for i in filenames]\nresults=pd.DataFrame({\"file\":filenames,\n                      \"species\":predictions})\nresults.to_csv(\"results.csv\",index=False)","metadata":{"papermill":{"duration":12.05158,"end_time":"2021-11-05T05:18:00.660932","exception":false,"start_time":"2021-11-05T05:17:48.609352","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-21T09:52:33.891865Z","iopub.execute_input":"2021-11-21T09:52:33.892066Z","iopub.status.idle":"2021-11-21T09:52:45.704042Z","shell.execute_reply.started":"2021-11-21T09:52:33.892042Z","shell.execute_reply":"2021-11-21T09:52:45.703333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## load best model","metadata":{}},{"cell_type":"code","source":"best_model = get_model(train_ds)\ncheckpoint_path = 'exp_1/cp_1.ckp'\ncheckpoint_dir = os.path.dirname(checkpoint_path)\nbest_model.load_weights(checkpoint_path)","metadata":{"execution":{"iopub.status.busy":"2021-11-21T09:52:45.706481Z","iopub.execute_input":"2021-11-21T09:52:45.707116Z","iopub.status.idle":"2021-11-21T09:52:47.958316Z","shell.execute_reply.started":"2021-11-21T09:52:45.707085Z","shell.execute_reply":"2021-11-21T09:52:47.957528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_size = (224, 224)\ntest_dir = '/kaggle/working/data'\n\ntest_generator = image_generator_test.flow_from_directory(\n    test_dir,\n    target_size=(img_size[0], img_size[1]),\n    class_mode = None,\n    batch_size=1,\n    shuffle=False)\n\nSTEP_SIZE_TEST=test_generator.n//test_generator.batch_size\ntest_generator.reset()\npred=best_model.predict_generator(test_generator,\nsteps=STEP_SIZE_TEST,\nverbose=1)\n\npredicted_class_indices=np.argmax(pred,axis=1)\nlabels = (train_ds.class_indices)\nlabels = dict((v,k) for k,v in labels.items())\npredictions = [labels[k] for k in predicted_class_indices]\n\nfilenames=test_generator.filenames\nfilenames = [i.split('/')[1] for i in filenames]\nresults=pd.DataFrame({\"file\":filenames,\n                      \"species\":predictions})\nresults.to_csv(\"results_best.csv\",index=False)","metadata":{"execution":{"iopub.status.busy":"2021-11-21T09:52:47.960292Z","iopub.execute_input":"2021-11-21T09:52:47.961952Z","iopub.status.idle":"2021-11-21T09:52:59.896039Z","shell.execute_reply.started":"2021-11-21T09:52:47.961905Z","shell.execute_reply":"2021-11-21T09:52:59.895361Z"},"trusted":true},"execution_count":null,"outputs":[]}]}