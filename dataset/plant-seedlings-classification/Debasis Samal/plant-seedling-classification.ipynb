{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Importing Library"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nfrom tqdm.notebook import tqdm_notebook as tqdm\nimport cv2\nimport numpy as np\nimport pandas as pd\nfrom glob import glob\nfrom keras.models import Sequential\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers.convolutional import Conv2D\nfrom keras.layers.convolutional import MaxPooling2D\nfrom keras.layers.core import Activation\nfrom keras.layers.core import Flatten\nfrom keras.layers.core import Dropout\nfrom keras.layers.core import Dense\nfrom keras.utils import np_utils\nimport tensorflow as tf\nimport keras\nfrom keras.applications.mobilenet import MobileNet, preprocess_input\n\nfrom keras.preprocessing.image import ImageDataGenerator\n\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport seaborn as sns\n\nfrom keras.optimizers import Adam\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Defining Class"},{"metadata":{"trusted":true},"cell_type":"code","source":"CLASS = {\n    'Black-grass': 0,\n    'Charlock': 1,\n    'Cleavers': 2,\n    'Common Chickweed': 3,\n    'Common wheat': 4,\n    'Fat Hen': 5,\n    'Loose Silky-bent': 6,\n    'Maize': 7,\n    'Scentless Mayweed': 8,\n    'Shepherds Purse': 9,\n    'Small-flowered Cranesbill': 10,\n    'Sugar beet': 11\n}\n\nINV_CLASS = {CLASS[j]:j for j in CLASS}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprop_img(image_path, verbose=0):\n    if verbose:\n        print(image_path)\n    img=cv2.imread(image_path)\n    img=cv2.resize(img, (128,128))\n    return img","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## EDA, Visualization and Data preparation"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Reading the image file and converting them to array\ntrain_image=[]\ntrain_label=[]\nBASE='../input/plant-seedlings-classification/train'\nfor i in tqdm(os.listdir(BASE), total=len(CLASS)):\n    for j in os.listdir(os.path.join(BASE,i)):\n        train_image.append(preprop_img(os.path.join(BASE,i,j)))\n        train_label.append(CLASS[i])\ntrain_image=np.array(train_image)\ntrain_label=np.array(train_label)\n\nprint(\"Shape of train_image:\",train_image.shape,\"Shape of train_label:\",train_label.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_label_cat = keras.utils.to_categorical(train_label,len(CLASS))\nprint(train_label_cat.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,12))\n\nfor i in range(12):  \n    \n    plt.subplot(3,4,i+1)\n    \n    index = np.where(train_label==i)[0][1]\n    plt.imshow(train_image[index])\n    plt.title(INV_CLASS[np.argmax(train_label_cat[index])])\n    plt.xticks([]), plt.yticks([])\n\nplt.suptitle(\"Visualization of Plant Seedlings\", fontsize=20)    \nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Cleaning Image data\nWe will clean the image by removing the image background. This help us in following ways;\n* Since we have less training data, it reduces our need for more training data.\n* Reduces false positives and hence improve accuracy.\n* Here in our given image dataset, it is important to do background subtraction because in most images the plants are small and for training the data with smaller objects in it is harder and it will be difficult for the model to pick them up."},{"metadata":{"trusted":true},"cell_type":"code","source":"clearTrainImg = []\nexamples = []; getEx = True\nplt.figure(figsize=(10,9))\n\nfor img in train_image:\n    \n    # Use gaussian blur\n    blurImg = cv2.GaussianBlur(img, (5, 5), 0)   \n    \n    # Convert to HSV image\n    hsvImg = cv2.cvtColor(blurImg, cv2.COLOR_BGR2HSV)  \n    \n    # Create mask (parameters - green color range)\n    lower_green = (25, 40, 50)\n    upper_green = (75, 255, 255)\n    mask = cv2.inRange(hsvImg, lower_green, upper_green)  \n    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (11, 11))\n    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n    \n    # Create bool mask\n    bMask = mask > 0  \n    \n    # Apply the mask\n    clear = np.zeros_like(img, np.uint8)  # Create empty image\n    clear[bMask] = img[bMask]  # Apply boolean mask to the origin image\n    \n    clearTrainImg.append(clear)  # Append image without backgroung\n    \n    # Show examples\n    if getEx:\n        plt.subplot(2, 3, 1); plt.imshow(img)  # Show the original image\n        plt.xticks([]), plt.yticks([]), plt.title(\"Original Image\")\n        plt.subplot(2, 3, 2); plt.imshow(blurImg)  # Blur image\n        plt.xticks([]), plt.yticks([]), plt.title(\"Blur Image\")\n        plt.subplot(2, 3, 3); plt.imshow(hsvImg)  # HSV image\n        plt.xticks([]), plt.yticks([]), plt.title(\"HSV Image\")\n        plt.subplot(2, 3, 4); plt.imshow(mask)  # Mask\n        plt.xticks([]), plt.yticks([]), plt.title(\"Mask\")\n        plt.subplot(2, 3, 5); plt.imshow(bMask)  # Boolean mask\n        plt.xticks([]), plt.yticks([]), plt.title(\"Boolean mask Image\")\n        plt.subplot(2, 3, 6); plt.imshow(clear)  # Image without background\n        plt.xticks([]), plt.yticks([]), plt.title(\"Image without background\")\n        getEx = False\n\nplt.suptitle(\"Masking the Plant\", fontsize=20)\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we have created the HSV image. And after that we'll create mask based on empirically selected range of green color, convert it to boolean mask and apply it to the origin image.\n\n* Use gaussian blur for remove noise\n* Convert color to HSV\n* Create mask\n* Create boolean mask\n* Apply boolean mask and getting image whithout background"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visulaising the sample result\nclearTrainImg = np.asarray(clearTrainImg)\nplt.figure(figsize=(12,8))\n\nfor i in range(8):\n    plt.subplot(2, 4, i + 1)\n    plt.imshow(clearTrainImg[i])\n    plt.xticks([]), plt.yticks([])\n    \nplt.suptitle(\"Sample result\", fontsize=20)  \nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot of label types numbers\nclasses = list(INV_CLASS.values())\n\nsns.set_style('darkgrid')  \nax = sns.countplot(x=0, data=pd.DataFrame(train_label))\nax.set_xticklabels(classes)\n\nplt.xticks(rotation=90)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Normalising Input\n* Now set the values of input from [0...255] to [0...1] (RGB color-space encode colors with numbers [0...255]). CNN will be faster train if we use [0...1] input"},{"metadata":{"trusted":true},"cell_type":"code","source":"clearTrainImg = clearTrainImg / 255","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Splitting the data\n* Splitting the image data into train and test class"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(clearTrainImg,train_label_cat, shuffle=True, test_size=0.2)\nprint(X_train.shape, Y_train.shape)\nprint(X_test.shape, Y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train=X_train.astype('float32') \nX_test=X_test.astype('float32')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Augmentation\n* In order to avoid overfitting problem, we need to expand artificially our handwritten digit dataset. This will help us in increasing the number of data in our dataset.\n* This will change the training data with small transformations to reproduce the variations occuring when someone is writing a digit."},{"metadata":{"trusted":true},"cell_type":"code","source":"datagen = ImageDataGenerator(\n        rotation_range=180,  # randomly rotate images in the range\n        zoom_range = 0.1, # Randomly zoom image \n        width_shift_range=0.1,  # randomly shift images horizontally\n        height_shift_range=0.1,  # randomly shift images vertically \n        horizontal_flip=True,  # randomly flip images horizontally\n        vertical_flip=True  # randomly flip images vertically\n    )  \ndatagen.fit(X_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For the data augmentation, I choosed to :\n\n* Randomly rotate some training images by 180 degrees.\n* Randomly Zoom by 10% on training images.\n* Randomly shift images horizontally by 10% of the width.\n* Randomly shift images vertically by 10% of the height.\n* Randomly flipped the image vertically and horizontally.\n\nOnce our model is ready, we will use this to train  the training dataset"},{"metadata":{},"cell_type":"markdown","source":"## Model Building\n* 1st trained data on model built from screatch.\n* 2nd trained data on model built from transfer learning. (Used mobilenet)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Got 90 % test accuracy on model built from scratch\n\ntf.keras.backend.clear_session() #clear the weights\n\nnp.random.seed(2)  # Fix seed\n\nmodel = Sequential([Conv2D(filters=64, kernel_size=(5, 5), input_shape=(128, 128, 3), activation='relu'),\n                    BatchNormalization(axis=3),\n                    Conv2D(filters=64, kernel_size=(5, 5), activation='relu'),\n                    MaxPooling2D((2, 2)),\n                    BatchNormalization(axis=3),\n                    Dropout(0.1),\n                    \n                    Conv2D(filters=128, kernel_size=(5, 5), activation='relu'),\n                    BatchNormalization(axis=3),\n                    Conv2D(filters=128, kernel_size=(5, 5), activation='relu'),\n                    MaxPooling2D((2, 2)),\n                    BatchNormalization(axis=3),\n                    Dropout(0.1),\n                   \n                    Conv2D(filters=256, kernel_size=(5, 5), activation='relu'),\n                    BatchNormalization(axis=3),\n                    Conv2D(filters=128, kernel_size=(5, 5), activation='relu'),\n                    MaxPooling2D((2, 2)),\n                    BatchNormalization(axis=3),\n                    Dropout(0.1),\n                   \n                    Flatten(),\n                    \n                    Dense(256, activation='relu'),\n                    BatchNormalization(),\n                    Dropout(0.5),\n                   \n                    Dense(256, activation='relu'),\n                    BatchNormalization(),\n                    Dropout(0.5),\n                   \n                    Dense(12, activation='softmax')])\n\n\n\nmodel.summary()\n\n# compile model\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ALPHA = 1.0\n\n# # mnet = MobileNet(input_shape=(128,128,3), include_top=False, alpha=ALPHA)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Got 91.26% accuracy on validation dataset on 10 epochs and with batch size as 32.\n# tf.keras.backend.clear_session()\n\n# for layers in mnet.layers:\n#       layers.trainable = False\n\n# model = Sequential([mnet,\n#                     Flatten(),\n                    \n#                     Dense(256, activation='relu'),\n#                     BatchNormalization(),\n#                     Dropout(0.5),\n                   \n#                     Dense(256, activation='relu'),\n#                     BatchNormalization(),\n#                     Dropout(0.5),\n                    \n#                     Dense(12,activation='softmax')])\n\n# model.summary()\n\n# # compile model\n# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])            ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Callback"},{"metadata":{"trusted":true},"cell_type":"code","source":"learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', \n                                            patience=3, \n                                            verbose=1, \n                                            factor=0.4, \n                                            min_lr=0.00001)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit_generator(datagen.flow(X_train, Y_train, batch_size=32),epochs=5,\n                    validation_data=(X_test, Y_test),\n                    steps_per_epoch=(X_train.shape[0]),\n                    verbose=1,\n                    callbacks=[learning_rate_reduction])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Visualizing the training"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,10))\nfig, ax = plt.subplots(2,1)\nax[0].plot(history.history['loss'], color='b', label=\"Training loss\")\nax[0].plot(history.history['val_loss'], color='r', label=\"validation loss\",axes =ax[0])\nlegend = ax[0].legend(loc='best', shadow=True)\n\nax[1].plot(history.history['accuracy'], color='b', label=\"Training accuracy\")\nax[1].plot(history.history['val_accuracy'], color='r',label=\"Validation accuracy\")\nlegend = ax[1].legend(loc='best', shadow=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preparing Test Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"path = '../input/plant-seedlings-classification/test/*.png'\nfiles = glob(path)\n\ntestImg = []\n\nfor img in files:\n    testImg.append(cv2.resize(cv2.imread(img), (128, 128)))\n\ntestImg = np.asarray(testImg)  # Train images set\n\nplt.figure(figsize=(10,6))\nfor i in range(8):\n    plt.subplot(2, 4, i + 1)\n    plt.imshow(testImg[i])\n    \nplt.suptitle(\"Visualising the test dataset\", fontsize=20)    \nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clearTestImg = []\nexamples = []; getEx = True\nplt.figure(figsize=(10,9))\n\nfor img in testImg:\n    # Use gaussian blur\n    blurImg = cv2.GaussianBlur(img, (5, 5), 0)   \n    \n    # Convert to HSV image\n    hsvImg = cv2.cvtColor(blurImg, cv2.COLOR_BGR2HSV)  \n    \n    # Create mask (parameters - green color range)\n    lower_green = (25, 40, 50)\n    upper_green = (75, 255, 255)\n    mask = cv2.inRange(hsvImg, lower_green, upper_green)  \n    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (11, 11))\n    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n    \n    # Create bool mask\n    bMask = mask > 0  \n    \n    # Apply the mask\n    clear = np.zeros_like(img, np.uint8)  # Create empty image\n    clear[bMask] = img[bMask]  # Apply boolean mask to the origin image\n    \n    clearTestImg.append(clear)  # Append image without backgroung\n    \n    # Show examples\n    if getEx:\n        plt.subplot(2, 3, 1); plt.imshow(img)  # Show the original image\n        plt.xticks([]), plt.yticks([]), plt.title(\"Original Image\")\n        plt.subplot(2, 3, 2); plt.imshow(blurImg)  # Blur image\n        plt.xticks([]), plt.yticks([]), plt.title(\"Blur Image\")\n        plt.subplot(2, 3, 3); plt.imshow(hsvImg)  # HSV image\n        plt.xticks([]), plt.yticks([]), plt.title(\"HSV Image\")\n        plt.subplot(2, 3, 4); plt.imshow(mask)  # Mask\n        plt.xticks([]), plt.yticks([]), plt.title(\"Mask\")\n        plt.subplot(2, 3, 5); plt.imshow(bMask)  # Boolean mask\n        plt.xticks([]), plt.yticks([]), plt.title(\"Boolean mask Image\")\n        plt.subplot(2, 3, 6); plt.imshow(clear)  # Image without background\n        plt.xticks([]), plt.yticks([]), plt.title(\"Image without background\")\n        getEx = False\n\nplt.suptitle(\"Masked Test Image\", fontsize=20)\nplt.tight_layout()\nclearTestImg = np.asarray(clearTestImg)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Normalizing the test data\nclearTestImg = clearTestImg / 255","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = model.predict(clearTestImg)\npredNum = np.argmax(pred, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testId = []\nfor i in files:\n    testId.append(i.split('/')[-1]) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predStr=[]\nfor i in predNum:\n    predStr.append(INV_CLASS[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Write result to file\n# PS = {'file': testId, 'species': predStr}\n# PS = pd.DataFrame(res)\n# PS.to_csv(\"PS.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Visualising Prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,12))\n\nfor i,j in enumerate(files[:12]):  \n    \n    plt.subplot(3,4,i+1)\n    \n    img = np.array(cv2.imread(j))\n    plt.imshow(img)\n    plt.title(predStr[i])\n    plt.xticks([]), plt.yticks([])\n\nplt.suptitle(\"Visualization of Predicted Plant Seedlings\", fontsize=20)    \nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}