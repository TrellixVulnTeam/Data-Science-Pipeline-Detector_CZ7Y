{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nprint(os.listdir(\"../input\"))\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\n\nimport cv2\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.axes_grid1 import ImageGrid\nimport numpy as np\nfrom keras.utils import np_utils\nfrom keras import applications\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras import optimizers\nfrom keras.models import Sequential, Model \nfrom keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D\nfrom keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.applications.vgg16 import preprocess_input as vgg_preprocess\nfrom keras.applications.resnet50 import preprocess_input as resnet_preprocess\nfrom keras.applications.inception_v3 import preprocess_input as inception_preprocess","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3326f1388ce01fa6d1989afc682202b726247609"},"cell_type":"code","source":"CATEGORIES = ['Black-grass', 'Charlock', 'Cleavers', 'Common Chickweed', 'Common wheat', 'Fat Hen', 'Loose Silky-bent',\n              'Maize', 'Scentless Mayweed', 'Shepherds Purse', 'Small-flowered Cranesbill', 'Sugar beet']\nNUM_CATEGORIES = len(CATEGORIES)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8660ec6c99788ab52f55ca8dd2a5a8724c085fc0"},"cell_type":"code","source":"SEED = 1\ndata_dir = '../input/'\ntrain_dir = os.path.join(data_dir, 'train')\ntest_dir = os.path.join(data_dir, 'test')\nsample_submission = pd.read_csv(os.path.join(data_dir, 'sample_submission.csv'))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7099ad30d67386be7980bc2e414bb6a60ff4f0ef"},"cell_type":"markdown","source":"### Number of training images for each Category","execution_count":null},{"metadata":{"trusted":true,"_uuid":"812183d40b80919fb5d2b0a09a2cb8cba15bc1d6","scrolled":true},"cell_type":"code","source":"for category in CATEGORIES:\n    print('{} {} images'.format(category, len(os.listdir(os.path.join(train_dir, category)))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b6763d312e10f497b0a6c434508cca9a1dfd3621"},"cell_type":"code","source":"train = []\nfor category_id, category in enumerate(CATEGORIES):\n    for file in os.listdir(os.path.join(train_dir, category)):\n        train.append(['train/{}/{}'.format(category, file), category_id, category])\ntrain = pd.DataFrame(train, columns=['file', 'category_id', 'category'])\ntrain.head(2)\ntrain.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2e51cc8c2972498dc59770ad38789648d911e3ee"},"cell_type":"code","source":"test = []\nfor file in os.listdir(test_dir):\n    test.append(['test/{}'.format(file), file])\ntest = pd.DataFrame(test, columns=['filepath', 'file'])\ntest.head(2)\ntest.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2f4631dde3a93b40fe3e58fdcbdc38f9e5a6bd70"},"cell_type":"markdown","source":"### See some of the Images","execution_count":null},{"metadata":{"trusted":true,"_uuid":"dff15c1e9c995eac5ff5ec7fb4e2d9e82c551b8b"},"cell_type":"code","source":"fig = plt.figure(1, figsize=(NUM_CATEGORIES, NUM_CATEGORIES))\ngrid = ImageGrid(fig, 111, nrows_ncols=(NUM_CATEGORIES, NUM_CATEGORIES), axes_pad=0.05)\ni = 0\nfor category_id, category in enumerate(CATEGORIES):\n    for filepath in train[train['category'] == category]['file'].values[:NUM_CATEGORIES]:\n        ax = grid[i]\n        img = Image.open(\"../input/\"+filepath)\n        img = img.resize((240,240))\n        ax.imshow(img)\n        ax.axis('off')\n        if i % NUM_CATEGORIES == NUM_CATEGORIES - 1:\n            ax.text(250, 112, filepath.split('/')[1], verticalalignment='center')\n        i += 1\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"w=[]\nv=[]\nfor category in CATEGORIES:\n    print('{} {} images'.format(category, len(os.listdir(os.path.join(train_dir, category)))))\n    w.append(category)\n    v.append(len(os.listdir(os.path.join(train_dir, category))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(25, 7))\nplt.xlabel('Classes')  \nplt.ylabel('Count')  \nplt.title(\"Data Distribution for each class\")\nplt.bar(w,v)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_segmented_image(img):\n    blurr = cv2.GaussianBlur(img,(5,5),0)\n    hsv = cv2.cvtColor(blurr, cv2.COLOR_BGR2HSV)\n    \n    sensitivity = 30\n    lower_hsv = np.array([60 - sensitivity, 100, 50])\n    upper_hsv = np.array([60 + sensitivity, 255, 255])\n    \n    mask = cv2.inRange(hsv, lower_hsv, upper_hsv)\n    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (11,11))\n    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n    masked = mask > 0\n    preprocessed = np.zeros_like(img,np.uint8)\n    preprocessed[masked] = img[masked]\n\n    return np.asarray(preprocessed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"152a64f1027292a5fe224f7640322b2eee730015"},"cell_type":"markdown","source":"## Model Preparation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import BatchNormalization","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"86eb75b30a81fb16a022fb79f0f4bdfff3ba7e09","scrolled":true},"cell_type":"code","source":"model1 = applications.ResNet50(weights = \"imagenet\", include_top=False, input_shape = (224, 224,3))\nmodel2 = applications.VGG16(weights = \"imagenet\", include_top=False, input_shape = (224, 224, 3))\nmodel3 = applications.InceptionV3(weights = \"imagenet\", include_top=False, input_shape = (299, 299, 3))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ae2c3181315d6c87ae7988e32fcb8fafd4e264f4"},"cell_type":"markdown","source":"### freeze all layers except last 4 layers","execution_count":null},{"metadata":{"_uuid":"bb32f72977f25a4b96b1b247089e1068064b1bf7"},"cell_type":"markdown","source":"### Adding output Layer","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model1.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"877d5e590c492e0aa39a6d701d513314ae0f33cf"},"cell_type":"code","source":"x = model1.output\nx = Flatten()(x)\nx = Dense(512, activation=\"relu\")(x)\nx = Dropout(0.3)(x)\nx=BatchNormalization()(x)\nx = Dense(256, activation=\"relu\")(x)\nx = Dropout(0.3)(x)\nx=BatchNormalization()(x)\npredictions = Dense(12, activation=\"softmax\")(x) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = model1.output\nx = Flatten()(x)\nx = Dense(512, activation=\"relu\")(x)\nx = Dropout(0.3)(x)\nx=BatchNormalization()(x)\nx = Dense(256, activation=\"relu\")(x)\nx = Dropout(0.3)(x)\nx=BatchNormalization()(x)\npredictions = Dense(12, activation=\"softmax\")(x) \nmodel_final = Model(input = model1.input, output = predictions)\n#compling our model\nmodel_final.compile(loss = \"categorical_crossentropy\", optimizer = optimizers.SGD(lr=0.01), metrics=[\"accuracy\"])\nfor layer in model_final.layers[:-9]:\n    layer.trainable = False\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"InceptionV3","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x = model3.output\nx = Flatten()(x)\nx = Dense(512, activation=\"relu\")(x)\nx = Dropout(0.3)(x)\nx=BatchNormalization()(x)\npredictions = Dense(12, activation=\"softmax\")(x) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nmodel_final.fit_generator(\n                    train_generator,\n                    validation_data=val_generator,\n                    epochs = 10,\n                    shuffle= True,\n                    callbacks = [early])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_final = Model(input = model3.input, output = predictions)\n#compling our model\nmodel_final.compile(loss = \"categorical_crossentropy\", optimizer = optimizers.SGD(lr=0.01), metrics=[\"accuracy\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d3595451acbc2ada1a9498f5d2a0822707d654eb"},"cell_type":"code","source":"model_final = Model(input = model1.input, output = predictions)\n#compling our model\nmodel_final.compile(loss = \"categorical_crossentropy\", optimizer = optimizers.SGD(lr=0.01), metrics=[\"accuracy\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c69f442273fa79483146c1152396614c18b35dee","scrolled":true},"cell_type":"code","source":"model_final.summary() #Model summary","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3d9be455e5ae87f7ca50d4914e8f83f2aba72de4"},"cell_type":"markdown","source":"## Data Augmentation","execution_count":null},{"metadata":{"trusted":true,"_uuid":"fe1f5492d35f66ea15bb4afbdb47201f8dc6be13"},"cell_type":"code","source":"gen = ImageDataGenerator(preprocessing_function = resnet_preprocess,\n           # rescale=1./255, #only change\n            width_shift_range=0.1,\n            rotation_range=30.,\n            validation_split=0.25,\n            horizontal_flip=True,\n            vertical_flip=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"00940fef5caf50923526641e52d10a8a2dd21bb4"},"cell_type":"code","source":"train_data_dir = \"../input/train\"\ntrain_generator = gen.flow_from_directory(\n                        train_data_dir,\n                        target_size = (299, 299),\n                        batch_size = 64, \n                        subset='training',\n                        class_mode = \"categorical\")\nval_generator = gen.flow_from_directory(train_data_dir,target_size = (299, 299),\n                        batch_size = 64, \n                        subset='validation',\n                        class_mode = \"categorical\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7c88ef71b87222e42507af04a109677613a08529"},"cell_type":"code","source":"checkpoint = ModelCheckpoint(\"inceptionv3.h5\", monitor='loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\nearly = EarlyStopping(monitor='loss', min_delta=0, patience=4, verbose=1, mode='auto')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8292b1ad43391d637d46fd10eaeb040715b32d3d"},"cell_type":"markdown","source":"### Train our Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for layer in model_final.layers[:-9]:\n    layer.trainable = False\n    #v3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for layer in model_final.layers[:-11]:\n    layer.trainable = False\n    #resnet","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for l in model_final.layers:\n    print(l.name, l.trainable)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"23257aaa09dbbc2d21c2bac200a9aa6a40daec7a","scrolled":false},"cell_type":"code","source":"model_final.fit_generator(\n                    train_generator,\n                    validation_data=val_generator,\n                    epochs = 10,\n                    shuffle= True,\n                    callbacks = [early])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"attempt 2 with rescale","execution_count":null},{"metadata":{"_uuid":"5c9f509ca511efffffa08620a98c1c3f86bb966e"},"cell_type":"markdown","source":"## Predicting the test images from trained model","execution_count":null},{"metadata":{"trusted":true,"_uuid":"7e5c1aab36b8f5385e23ed109fd5297bcdf5a06f"},"cell_type":"code","source":"classes = train_generator.class_indices  \nprint(classes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"334057468e9fc151e9b06758efe40189a37d7797"},"cell_type":"code","source":"#Invert Mapping\nclasses = {v: k for k, v in classes.items()}\nprint(classes)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3d6fda46ac6fe15d5744f4373ae328b2cafbcfc8"},"cell_type":"markdown","source":"### Prediction on each image","execution_count":null},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"42ec1709f17c7702add9357fbae6e9a396349829"},"cell_type":"code","source":"prediction = []\nfor filepath in test['filepath']:\n    img = cv2.imread(os.path.join(data_dir,filepath))\n    img = cv2.resize(img,(240,240))\n    img = np.asarray(img)\n    img = img.reshape(1,240,240,3)\n    pred = model_final.predict(img)\n    prediction.append(classes.get(pred.argmax(axis=-1)[0])) #Invert Mapping helps to map Label","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"552f422733c54922988dd88776ba640589c4a06a","collapsed":true},"cell_type":"code","source":"test = test.drop(columns =['filepath']) #Remove file path from test DF","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b562c5a66684e4914ca759e0f021f22890090517","collapsed":true},"cell_type":"code","source":"sample_submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1cfa63d5328b6b10171738a77391bc0446be8048","scrolled":false,"collapsed":true},"cell_type":"code","source":"pred = pd.DataFrame({'species': prediction})\ntest =test.join(pred)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"87a5d1e61b28c36e1f5ff659799bc8d6d2c0de48"},"cell_type":"markdown","source":"### Final submission File","execution_count":null},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"8ac1053202f8ffa448d967df57477193942a8dcb"},"cell_type":"code","source":"test.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b1f47feb769dd7a905215ca818efb6d1b4e3dc97","collapsed":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}