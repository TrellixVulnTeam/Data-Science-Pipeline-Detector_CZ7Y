{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os, cv2\nfrom tqdm import tqdm\nfrom sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold\n\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization, GlobalAveragePooling2D\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.applications import *\nimport tensorflow_addons as tfa","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-26T14:44:14.241754Z","iopub.execute_input":"2022-03-26T14:44:14.242368Z","iopub.status.idle":"2022-03-26T14:44:14.250597Z","shell.execute_reply.started":"2022-03-26T14:44:14.242317Z","shell.execute_reply":"2022-03-26T14:44:14.249715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train = []\nx_test = []\ny_train = []\n\ndf_test = pd.read_csv('../input/plant-seedlings-classification/sample_submission.csv')\n\n\nlabel_map = {   \"Black-grass\"               :0,\n                \"Charlock\"                  :1,\n                \"Cleavers\"                  :2,\n                \"Common Chickweed\"          :3,\n                \"Common wheat\"              :4,\n                \"Fat Hen\"                   :5,\n                \"Loose Silky-bent\"          :6,\n                \"Maize\"                     :7,\n                \"Scentless Mayweed\"         :8,\n                \"Shepherds Purse\"           :9,\n                \"Small-flowered Cranesbill\" :10,\n                \"Sugar beet\"                :11}\n\ndim = 300","metadata":{"execution":{"iopub.status.busy":"2022-03-26T14:43:18.675325Z","iopub.execute_input":"2022-03-26T14:43:18.67565Z","iopub.status.idle":"2022-03-26T14:43:18.696529Z","shell.execute_reply.started":"2022-03-26T14:43:18.675607Z","shell.execute_reply":"2022-03-26T14:43:18.69581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Preparing training data\ndirs = os.listdir(\"../input/plant-seedlings-classification/train\")\nfor k in tqdm(range(len(dirs))):    # Directory\n    files = os.listdir(\"../input/plant-seedlings-classification/train/{}\".format(dirs[k]))\n    for f in range(len(files)):     # Files\n        img = cv2.imread('../input/plant-seedlings-classification/train/{}/{}'.format(dirs[k], files[f]))\n        targets = np.zeros(12)\n        targets[label_map[dirs[k]]] = 1 \n        x_train.append(cv2.resize(img, (dim, dim)))\n        y_train.append(targets)\n    \ny_train = np.array(y_train, np.uint8)\nx_train = np.array(x_train, np.float32)\n\nprint(x_train.shape)\nprint(y_train.shape)","metadata":{"execution":{"iopub.status.busy":"2022-03-26T14:46:08.497801Z","iopub.execute_input":"2022-03-26T14:46:08.498092Z","iopub.status.idle":"2022-03-26T14:47:39.714637Z","shell.execute_reply.started":"2022-03-26T14:46:08.498062Z","shell.execute_reply":"2022-03-26T14:47:39.713784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.01, random_state=42)\n# sss = StratifiedShuffleSplit(n_splits=1, test_size=0.16, random_state=42) # Want a balanced split for all the classes\n# for train_index, test_index in sss.split(x_train, y_train):\n#     print(\"Using {} for training and {} for validation\".format(len(train_index), len(test_index)))\n#     x_train, x_valid = x_train[train_index], x_train[test_index]\n#     y_train, y_valid = y_train[train_index], y_train[test_index]","metadata":{"execution":{"iopub.status.busy":"2022-03-26T14:43:18.708643Z","iopub.execute_input":"2022-03-26T14:43:18.709425Z","iopub.status.idle":"2022-03-26T14:43:18.720682Z","shell.execute_reply.started":"2022-03-26T14:43:18.709383Z","shell.execute_reply":"2022-03-26T14:43:18.720067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow import keras\nfrom tensorflow.keras import backend as K\n\ndef cosine_decay_with_warmup(global_step,\n                             learning_rate_base,\n                             total_steps,\n                             warmup_learning_rate=0.0,\n                             warmup_steps=0,\n                             hold_base_rate_steps=0):\n    \"\"\"Cosine decay schedule with warm up period.\n    Cosine annealing learning rate as described in:\n      Loshchilov and Hutter, SGDR: Stochastic Gradient Descent with Warm Restarts.\n      ICLR 2017. https://arxiv.org/abs/1608.03983\n    In this schedule, the learning rate grows linearly from warmup_learning_rate\n    to learning_rate_base for warmup_steps, then transitions to a cosine decay\n    schedule.\n    Arguments:\n        global_step {int} -- global step.\n        learning_rate_base {float} -- base learning rate.\n        total_steps {int} -- total number of training steps.\n    Keyword Arguments:\n        warmup_learning_rate {float} -- initial learning rate for warm up. (default: {0.0})\n        warmup_steps {int} -- number of warmup steps. (default: {0})\n        hold_base_rate_steps {int} -- Optional number of steps to hold base learning rate\n                                    before decaying. (default: {0})\n    Returns:\n      a float representing learning rate.\n    Raises:\n      ValueError: if warmup_learning_rate is larger than learning_rate_base,\n        or if warmup_steps is larger than total_steps.\n    \"\"\"\n\n    if total_steps < warmup_steps:\n        raise ValueError('total_steps must be larger or equal to '\n                         'warmup_steps.')\n    learning_rate = 0.5 * learning_rate_base * (1 + np.cos(\n        np.pi *\n        (global_step - warmup_steps - hold_base_rate_steps\n         ) / float(total_steps - warmup_steps - hold_base_rate_steps)))\n    if hold_base_rate_steps > 0:\n        learning_rate = np.where(global_step > warmup_steps + hold_base_rate_steps,\n                                 learning_rate, learning_rate_base)\n    if warmup_steps > 0:\n        if learning_rate_base < warmup_learning_rate:\n            raise ValueError('learning_rate_base must be larger or equal to '\n                             'warmup_learning_rate.')\n        slope = (learning_rate_base - warmup_learning_rate) / warmup_steps\n        warmup_rate = slope * global_step + warmup_learning_rate\n        learning_rate = np.where(global_step < warmup_steps, warmup_rate,\n                                 learning_rate)\n    return np.where(global_step > total_steps, 0.0, learning_rate)\n\n\nclass WarmUpCosineDecayScheduler(keras.callbacks.Callback):\n    \"\"\"Cosine decay with warmup learning rate scheduler\n    \"\"\"\n\n    def __init__(self,\n                 learning_rate_base,\n                 total_steps,\n                 global_step_init=0,\n                 warmup_learning_rate=0.0,\n                 warmup_steps=0,\n                 hold_base_rate_steps=0,\n                 verbose=0):\n        \"\"\"Constructor for cosine decay with warmup learning rate scheduler.\n    Arguments:\n        learning_rate_base {float} -- base learning rate.\n        total_steps {int} -- total number of training steps.\n    Keyword Arguments:\n        global_step_init {int} -- initial global step, e.g. from previous checkpoint.\n        warmup_learning_rate {float} -- initial learning rate for warm up. (default: {0.0})\n        warmup_steps {int} -- number of warmup steps. (default: {0})\n        hold_base_rate_steps {int} -- Optional number of steps to hold base learning rate\n                                    before decaying. (default: {0})\n        verbose {int} -- 0: quiet, 1: update messages. (default: {0})\n        \"\"\"\n\n        super(WarmUpCosineDecayScheduler, self).__init__()\n        self.learning_rate_base = learning_rate_base\n        self.total_steps = total_steps\n        self.global_step = global_step_init\n        self.warmup_learning_rate = warmup_learning_rate\n        self.warmup_steps = warmup_steps\n        self.hold_base_rate_steps = hold_base_rate_steps\n        self.verbose = verbose\n        self.learning_rates = []\n\n    def on_batch_end(self, batch, logs=None):\n        self.global_step = self.global_step + 1\n        lr = K.get_value(self.model.optimizer.lr)\n        self.learning_rates.append(lr)\n\n    def on_batch_begin(self, batch, logs=None):\n        lr = cosine_decay_with_warmup(global_step=self.global_step,\n                                      learning_rate_base=self.learning_rate_base,\n                                      total_steps=self.total_steps,\n                                      warmup_learning_rate=self.warmup_learning_rate,\n                                      warmup_steps=self.warmup_steps,\n                                      hold_base_rate_steps=self.hold_base_rate_steps)\n        K.set_value(self.model.optimizer.lr, lr)\n        if self.verbose > 0:\n            print('\\nBatch %05d: setting learning '\n                  'rate to %s.' % (self.global_step + 1, lr))","metadata":{"execution":{"iopub.status.busy":"2022-03-26T14:43:18.722046Z","iopub.execute_input":"2022-03-26T14:43:18.722291Z","iopub.status.idle":"2022-03-26T14:43:18.742416Z","shell.execute_reply.started":"2022-03-26T14:43:18.722252Z","shell.execute_reply":"2022-03-26T14:43:18.741583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"datagen = ImageDataGenerator(horizontal_flip=True, \n                             vertical_flip=True)   \n\nepochs = 100\nlearning_rate = 0.0001\nbatch_size = 16\n\n# total_steps = x_train.shape[0] // batch_size * epochs\n# warmup_steps = 0.1 * total_steps\n# warm_up_lr = WarmUpCosineDecayScheduler(learning_rate_base=learning_rate,\n#                                         total_steps=total_steps,\n#                                         warmup_learning_rate=0.0,\n#                                         warmup_steps=warmup_steps,\n#                                         hold_base_rate_steps=0)","metadata":{"execution":{"iopub.status.busy":"2022-03-26T14:43:18.743963Z","iopub.execute_input":"2022-03-26T14:43:18.744332Z","iopub.status.idle":"2022-03-26T14:43:18.756794Z","shell.execute_reply.started":"2022-03-26T14:43:18.744285Z","shell.execute_reply":"2022-03-26T14:43:18.756168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ------ TRAINING ------\n\nskf = StratifiedKFold(n_splits=5)\nfor fold, (train_index, val_index) in enumerate(skf.split(x_train, np.where(y_train == 1)[1])):\n    base_model = efficientnet.EfficientNetB3(input_shape=(dim, dim, 3), include_top=False, weights='imagenet') # Average pooling rswishes output dimensions\n    x = base_model.output\n    x = GlobalAveragePooling2D(name=\"avg_pool\")(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.2, name=\"top_dropout\")(x)\n    outputs = Dense(12, activation=\"softmax\", name=\"pred\")(x)\n    model = Model(inputs=base_model.input, outputs=outputs)\n    \n    callbacks = [EarlyStopping(monitor='val_loss', patience=5, verbose=0), \n                  ModelCheckpoint(f\"fold_{fold}.h5\", monitor='val_loss', save_best_only=True, verbose=0),\n                  ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, verbose=0, mode='auto', epsilon=0.0001, cooldown=0, min_lr=0)]\n    \n    model.compile(loss='categorical_crossentropy', optimizer=optimizers.Adam(learning_rate=learning_rate), metrics=['accuracy'])\n    model.fit_generator(datagen.flow(x_train[train_index], y_train[train_index], batch_size=batch_size),\n                        steps_per_epoch=len(train_index)//batch_size, \n                        validation_data=datagen.flow(x_train[val_index], y_train[val_index], batch_size=batch_size), \n                        validation_steps=len(val_index)/batch_size,\n                        callbacks=callbacks,\n                        epochs=epochs, \n                        verbose=1)","metadata":{"execution":{"iopub.status.busy":"2022-03-26T14:50:27.962164Z","iopub.execute_input":"2022-03-26T14:50:27.962418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ------ TESTING ------\nfor f, species in tqdm(df_test.values, miniters=100):\n    img = cv2.imread('../input/plant-seedlings-classification/test/{}'.format(f))\n    x_test.append(cv2.resize(img, (dim, dim)))\n\nx_test1 = np.array(x_test, np.float32)\nx_test2 = np.array([np.rot90(i, k=1) for i in x_test], np.float32)\nx_test3 = np.array([np.rot90(i, k=2) for i in x_test], np.float32)\nx_test4 = np.array([np.rot90(i, k=3) for i in x_test], np.float32)\n\n# print(x_test.shape)\n\np_test1 = np.zeros((len(x_test), 12), dttype=float)\np_test2 = np.zeros((len(x_test), 12), dttype=float)\np_test3 = np.zeros((len(x_test), 12), dttype=float)\np_test4 = np.zeros((len(x_test), 12), dttype=float)\nfor fold in range(5):\n    model.load_weights(f'fold_{fold}.h5')\n    p_test1 += model.predict(x_test1, verbose=1)\n    p_test2 += model.predict(x_test2, verbose=1)\n    p_test3 += model.predict(x_test3, verbose=1)\n    p_test4 += model.predict(x_test4, verbose=1)\n\npreds = []\nfor i in range(len(p_test1)):\n    pos = np.argmax(p_test1[i] + p_test2[i] + p_test3[i] + p_test4[i])\n    preds.append(list(label_map.keys())[list(label_map.values()).index(pos)])\n    \ndf_test['species'] = preds\ndf_test.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-03-26T14:43:19.103103Z","iopub.status.idle":"2022-03-26T14:43:19.10355Z","shell.execute_reply.started":"2022-03-26T14:43:19.103371Z","shell.execute_reply":"2022-03-26T14:43:19.103389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_test.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-26T14:43:19.10456Z","iopub.status.idle":"2022-03-26T14:43:19.104919Z","shell.execute_reply.started":"2022-03-26T14:43:19.104718Z","shell.execute_reply":"2022-03-26T14:43:19.104743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}