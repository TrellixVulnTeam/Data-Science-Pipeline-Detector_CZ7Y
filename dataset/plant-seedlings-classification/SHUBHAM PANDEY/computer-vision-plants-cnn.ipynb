{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Task 1:"},{"metadata":{},"cell_type":"markdown","source":"# Plant Seedling Classification"},{"metadata":{},"cell_type":"markdown","source":"# 0. Intro\n\n**Task type:** Classification\n\n**ML algorithms used:** Convolutional neural network\n\n**Other features:** Visualizing the filters used by the CNN"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport pickle\n\n\nimport cv2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Image importing"},{"metadata":{"trusted":true},"cell_type":"code","source":"image_size = 256\nbatch_size = 32","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**I will load the images using ImageDataGeneretor class and explicitly indicate the needed parameters (rescaling, flipping, validation splitting.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"idg = tf.keras.preprocessing.image.ImageDataGenerator(\n    rescale=1./255,\n    #rotation_range=20, # You can uncomment these parameters to make you generator rotate & flip the images to put the train model in stricter conditions.\n    #width_shift_range=0.2,\n    #height_shift_range=0.2,\n    horizontal_flip=True,\n    vertical_flip=True,\n    validation_split=0.2\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Train set**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_gen = idg.flow_from_directory('../input/plant-seedlings-classification/train/',\n                                                    target_size=(image_size, image_size),\n                                                    subset='training',\n                                                    class_mode='categorical',\n                                                    batch_size=batch_size,\n                                                    shuffle=True,\n                                                    seed=1\n                                                )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Validation set**"},{"metadata":{"trusted":true},"cell_type":"code","source":"val_gen = idg.flow_from_directory('../input/plant-seedlings-classification/train/',\n                                                   target_size=(image_size, image_size),                                                   \n                                                   subset='validation',\n                                                   class_mode='categorical',\n                                                   batch_size=batch_size,\n                                                   shuffle=True,\n                                                   seed=1\n                                                )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"unique, counts = np.unique(train_gen.classes, return_counts=True)\ndict1 = dict(zip(train_gen.class_indices, counts))\n\nkeys = dict1.keys()\nvalues = dict1.values()\n\nplt.xticks(rotation='vertical')\nbar = plt.bar(keys, values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The train dataset is quite balanced.**"},{"metadata":{},"cell_type":"markdown","source":"**Let's visualize a portion of images to make sure they're correctly loaded.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"x,y = next(train_gen)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from mpl_toolkits.axes_grid1 import ImageGrid\n\ndef show_grid(image_list, nrows, ncols, label_list=None, show_labels=False, figsize=(10,10)):\n\n    fig = plt.figure(None, figsize,frameon=False)\n    grid = ImageGrid(fig, 111, \n                     nrows_ncols=(nrows, ncols),  \n                     axes_pad=0.2, \n                     share_all=True,\n                     )\n    for i in range(nrows*ncols):\n        ax = grid[i]\n        ax.imshow(image_list[i],cmap='Greys_r')\n        ax.axis('off')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_grid(x,2,4,show_labels=True,figsize=(10,10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The images are nicely loaded and do not have any rotation and distortion.**"},{"metadata":{},"cell_type":"markdown","source":"# 2. Model building"},{"metadata":{},"cell_type":"markdown","source":"**Quick description, before heading on:**\n\n**Model type:** Sequential\n\n**Layers used:**\n\n    0. InputLayer\n    1. Conv2D (64, 128 filters)\n    2. MaxPool2D\n    3. GlobalMaxPool2D\n    4. Batch Normalization\n    5. Flatten\n    6. Dropout\n    6. Dense\n    \n**Input size:** 256 x 256 x 3 (size x colors)\n\n**Pool size:** 2 x 2 (for MaxPool2D)\n\n**Kernel size:** 3 x 3 (for Conv2D)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#NN\nmodelNN1 = tf.keras.models.Sequential([\n    tf.keras.layers.InputLayer(input_shape=(image_size,image_size,3,)),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dense(128, activation='relu'),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dense(12, activation='softmax')\n    \n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"modelNN1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nmodelNN1.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpoint = tf.keras.callbacks.ModelCheckpoint('plant_classifierNN.h5', #where to save the model\n                                                    save_best_only=True, \n                                                    monitor='val_accuracy', \n                                                    mode='max', \n                                                    verbose = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = modelNN1.fit(train_gen,\n          epochs=20, # Increase number of epochs if you have sufficient hardware\n          steps_per_epoch= 3803//batch_size,  # Number of train images // batch_size\n          validation_data=val_gen,\n          validation_steps = 947//batch_size, # Number of val images // batch_size\n          callbacks = [checkpoint],\n          verbose = 1\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#CNN\nmodelCNN1 = tf.keras.models.Sequential()\n\n# Input layer\n# Can be omitted, you can specify the input_shape in other layers\nmodelCNN1.add(tf.keras.layers.InputLayer(input_shape=(image_size,image_size,3,)))\n\n# Here we add a 2D Convolution layer\n# Check https://keras.io/api/layers/convolution_layers/convolution2d/ for more info\nmodelCNN1.add(tf.keras.layers.Conv2D(64, kernel_size=(3,3), activation='relu'))\n\n# Max Pool layer \n# It downsmaples the input representetion within the pool_size size\nmodelCNN1.add(tf.keras.layers.MaxPool2D(pool_size = (2,2)))\n\n# Normalization layer\n# The layer normalizes its output using the mean and standard deviation of the current batch of inputs.\nmodelCNN1.add(tf.keras.layers.BatchNormalization())\n\n# 2D Convolution layer\nmodelCNN1.add(tf.keras.layers.Conv2D(64, kernel_size=(3,3), strides = (1,1), activation='relu'))\n\n# Max Pool layer \nmodelCNN1.add(tf.keras.layers.MaxPool2D(pool_size = (2,2)))\n\n# Normalization layer\nmodelCNN1.add(tf.keras.layers.BatchNormalization())\n\n# 2D Convolution layer\nmodelCNN1.add(tf.keras.layers.Conv2D(128, kernel_size=(3,3), strides = (1,1), activation='relu'))\n\n# Max Pool layer \nmodelCNN1.add(tf.keras.layers.MaxPool2D(pool_size = (2,2)))\n\n# Normalization layer\nmodelCNN1.add(tf.keras.layers.BatchNormalization())\n\n# 2D Convolution layer\nmodelCNN1.add(tf.keras.layers.Conv2D(128, kernel_size=(3,3), strides = (1,1), activation='relu'))\n\n# Max Pool layer \nmodelCNN1.add(tf.keras.layers.MaxPool2D(pool_size = (2,2)))\n\n# Global Max Pool layer\nmodelCNN1.add(tf.keras.layers.GlobalMaxPool2D())\n\n# Dense Layers after flattening the data\nmodelCNN1.add(tf.keras.layers.Flatten())\n\nmodelCNN1.add(tf.keras.layers.Dense(128, activation='relu'))\n\n# Dropout\n# is used to nullify the outputs that are very close to zero and thus can cause overfitting.\nmodelCNN1.add(tf.keras.layers.Dropout(0.2))\nmodelCNN1.add(tf.keras.layers.Dense(64, activation='relu'))\n\n# Normalization layer\nmodelCNN1.add(tf.keras.layers.BatchNormalization())\n\n#Add Output Layer\nmodelCNN1.add(tf.keras.layers.Dense(12, activation='softmax')) # = 12 predicted classes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"modelCNN1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"modelCNN1.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# You can save the best model to the checkpoint\ncheckpoint = tf.keras.callbacks.ModelCheckpoint('plant_classifier.h5', #where to save the model\n                                                    save_best_only=True, \n                                                    monitor='val_accuracy', \n                                                    mode='max', \n                                                    verbose = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = modelCNN1.fit(train_gen,\n          epochs=20, # Increase number of epochs if you have sufficient hardware\n          steps_per_epoch= 3803//batch_size,  # Number of train images // batch_size\n          validation_data=val_gen,\n          validation_steps = 947//batch_size, # Number of val images // batch_size\n          callbacks = [checkpoint],\n          verbose = 1\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"**Learning curves vs epoch graph**"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history.history['accuracy'], label='accuracy')\nplt.plot(history.history['val_accuracy'], label = 'val_accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.xticks(list(range(1,21)))\nplt.ylim([0, 1])\nplt.legend(loc='lower right')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weigh= modelCNN1.get_weights()\npklfile= \"modelweights.pkl\"\ntry:\n    fpkl= open(pklfile, 'wb')    #Python 3     \n    pickle.dump(weigh, fpkl, protocol= pickle.HIGHEST_PROTOCOL)\n    fpkl.close()\nexcept:\n    fpkl= open(pklfile, 'w')    #Python 2      \n    pickle.dump(weigh, fpkl, protocol= pickle.HIGHEST_PROTOCOL)\n    fpkl.close()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualizing"},{"metadata":{},"cell_type":"markdown","source":"**Here we will check the predictions made by our model as well as visualize how the model filter one of the images taken from the dataset.**\n\n**Let's first check the prediction power.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"maize = cv2.imread('../input/prediction/Predict.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(maize)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We need to preprocess the image before passing it on to the model: resize + expand_dims.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"maize = cv2.resize(maize, (256,256))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"maize_batch = np.expand_dims(maize, axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"conv_maize = modelCNN1.predict(maize_batch)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# conv_maize.shape\nind=np.argmax(conv_maize,axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(list(train_gen.class_indices.keys())[ind[0]])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Conclusion\n\nWe have built a **CNN-model** to predict the class of a plant, which works quite well. (Increasing **number of epochs** and/or **adding layers** to a model can even increase the performance.\n\n**CNN + Maxpooling + Global pooling + Dense** is a good combination for image classification.\n"},{"metadata":{},"cell_type":"markdown","source":"# Task 2:\n"},{"metadata":{},"cell_type":"markdown","source":"Neural Networks (NN), or more precisely Artificial Neural Networks (ANN), is a class of Machine Learning algorithms that recently received a lot of attention (again!) due to the availability of Big Data and fast computing facilities (most of Deep Learning algorithms are essentially different variations of ANN).\n\nThe class of ANN covers several architectures including Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN) eg LSTM and GRU, Autoencoders, and Deep Belief Networks. Therefore, CNN is just one kind of ANN.\n\nGenerally speaking, an ANN is a collection of connected and tunable units (a.k.a. nodes, neurons, and artificial neurons) which can pass a signal (usually a real-valued number) from a unit to another. The number of (layers of) units, their types, and the way they are connected to each other is called the network architecture.\n\nA CNN, in specific, has one or more layers of convolution units. A convolution unit receives its input from multiple units from the previous layer which together create a proximity. Therefore, the input units (that form a small neighborhood) share their weights.\n\nThe convolution units (as well as pooling units) are especially beneficial as:\n\nThey reduce the number of units in the network (since they are many-to-one mappings). This means, there are fewer parameters to learn which reduces the chance of overfitting as the model would be less complex than a fully connected network.\nThey consider the context/shared information in the small neighborhoods. This future is very important in many applications such as image, video, text, and speech processing/mining as the neighboring inputs (eg pixels, frames, words, etc) usually carry related information.\n\nAlso from our outputs we can see the that CNN outperfromed NN with a very wide gap of accuracy."},{"metadata":{},"cell_type":"markdown","source":"# Task 3:"},{"metadata":{"trusted":true},"cell_type":"code","source":"datagen = tf.keras.preprocessing.image.ImageDataGenerator(rotation_range=10, width_shift_range=0.1, \nheight_shift_range=0.1,shear_range=0.15, \nzoom_range=0.1,channel_shift_range = 10, horizontal_flip=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"images=np.array([1,2,3])\nlen(images)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image_path = '../input/carsdata'\nimages=np.array([])\n\nfiles=os.listdir(image_path)\nfor file in files:\n    path=image_path+'/'+file\n    img=plt.imread(path)\n    img=cv2.resize(img,(224,224))\n    if len(images)==0:\n        images = np.expand_dims(img, 0)\n    images=np.append(images,[img],axis=0)\nimages.shape\n# image = np.expand_dims(ndimage.imread(image_path), 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.mkdir('newcardata')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"save_here = './newcardata'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"datagen.fit(images)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for x, val in zip(datagen.flow(images,                    #image we chose\n        save_to_dir=save_here,     #this is where we figure out where to save\n         save_prefix='aug',        # it will save the images as 'aug_0912' some number for every new augmented image\n        save_format='png'),range(10)) :     # here we define a range because we want 10 augmented images otherwise it will keep looping forever I think\n    pass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(1, (14, 14))\n\nimage_path = './newcardata'\nfiles=os.listdir(image_path)\n\n\nk = 0\nfor file in files:\n    path=image_path+'/'+file\n    img=plt.imread(path)\n    img=cv2.resize(img,(224,224))\n\n    \n    k += 1\n    if k==50:\n        break\n    ax = plt.subplot(7, 7, k)\n    ax.imshow(img, cmap='gray')\n    ax.set_xticks([])\n    ax.set_yticks([])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **So we succesfully created the augmented images from the cars images.**"},{"metadata":{},"cell_type":"markdown","source":"# Task 4:"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install tflearn\nimport tflearn.datasets.oxflower17 as oxflower17\nX, Y = oxflower17.load_data(one_hot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#NN\nmodelNN2 = tf.keras.models.Sequential([\n    tf.keras.layers.InputLayer(input_shape=(224,224,3)),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dense(128, activation='relu'),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dense(17, activation='softmax')\n    \n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_grid(X,2,4,show_labels=True,figsize=(10,10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"modelNN2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nmodelNN2.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpoint = tf.keras.callbacks.ModelCheckpoint('flower_classifierNN.h5', #where to save the model\n                                                    save_best_only=True, \n                                                    monitor='val_acc', \n                                                    mode='max', \n                                                    verbose = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history=modelNN2.fit(X, Y, batch_size=64, epochs=60, verbose=1, validation_split=0.2, shuffle=True,callbacks = [checkpoint])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#CNN\nmodelCNN2 = tf.keras.models.Sequential()\n\n# Input layer\n# Can be omitted, you can specify the input_shape in other layers\nmodelCNN2.add(tf.keras.layers.InputLayer(input_shape=(224,224,3,)))\n\n# Here we add a 2D Convolution layer\n# Check https://keras.io/api/layers/convolution_layers/convolution2d/ for more info\nmodelCNN2.add(tf.keras.layers.Conv2D(64, kernel_size=(3,3), activation='relu'))\n\n# Max Pool layer \n# It downsmaples the input representetion within the pool_size size\nmodelCNN2.add(tf.keras.layers.MaxPool2D(pool_size = (2,2)))\n\n# Normalization layer\n# The layer normalizes its output using the mean and standard deviation of the current batch of inputs.\nmodelCNN2.add(tf.keras.layers.BatchNormalization())\n\n# 2D Convolution layer\nmodelCNN2.add(tf.keras.layers.Conv2D(64, kernel_size=(3,3), strides = (1,1), activation='relu'))\n\n# Max Pool layer \nmodelCNN2.add(tf.keras.layers.MaxPool2D(pool_size = (2,2)))\n\n# Normalization layer\nmodelCNN2.add(tf.keras.layers.BatchNormalization())\n\n# 2D Convolution layer\nmodelCNN2.add(tf.keras.layers.Conv2D(128, kernel_size=(3,3), strides = (1,1), activation='relu'))\n\n# Max Pool layer \nmodelCNN2.add(tf.keras.layers.MaxPool2D(pool_size = (2,2)))\n\n# Normalization layer\nmodelCNN2.add(tf.keras.layers.BatchNormalization())\n\n# 2D Convolution layer\nmodelCNN2.add(tf.keras.layers.Conv2D(128, kernel_size=(3,3), strides = (1,1), activation='relu'))\n\n# Max Pool layer \nmodelCNN2.add(tf.keras.layers.MaxPool2D(pool_size = (2,2)))\n\n# Global Max Pool layer\nmodelCNN2.add(tf.keras.layers.GlobalMaxPool2D())\n\n# Dense Layers after flattening the data\nmodelCNN2.add(tf.keras.layers.Flatten())\n\nmodelCNN2.add(tf.keras.layers.Dense(128, activation='relu'))\n\n# Dropout\n# is used to nullify the outputs that are very close to zero and thus can cause overfitting.\nmodelCNN2.add(tf.keras.layers.Dropout(0.2))\nmodelCNN2.add(tf.keras.layers.Dense(64, activation='relu'))\n\n# Normalization layer\nmodelCNN2.add(tf.keras.layers.BatchNormalization())\n\n#Add Output Layer\nmodelCNN2.add(tf.keras.layers.Dense(17, activation='softmax')) # = 12 predicted classes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"modelCNN2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nmodelCNN2.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpoint = tf.keras.callbacks.ModelCheckpoint('flower_classifierNN.h5', #where to save the model\n                                                    save_best_only=True, \n                                                    monitor='val_acc', \n                                                    mode='max', \n                                                    verbose = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history=modelCNN2.fit(X, Y, batch_size=64, epochs=70, verbose=1, validation_split=0.2, shuffle=True,callbacks = [checkpoint])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#CNN with Transfer Learning\nbase = tf.keras.applications.VGG16(include_top=False, weights='imagenet', input_shape=(224,224,3))\n# base = tf.keras.applications.ResNet50(include_top=False, weights='imagenet', input_shape=(224,224,3))\n\nbase.trainable = False\n\n# for i in range(len(resnet.layers)-8):\n#     resnet.layers[i].trainable = False\n#     print(resnet.layers[i])\n    \nmodel = tf.keras.Sequential([\n    base,\n    tf.keras.layers.GlobalAveragePooling2D(),\n#     tf.keras.layers.Flatten(),\n    \n    tf.keras.layers.Dense(128,name=\"dense100\",activation=\"relu\"),\n    tf.keras.layers.Dropout(0.3),\n\n\n    \n    tf.keras.layers.Dense(17,activation=\"softmax\")\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"base.trainable = True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(16):\n    base.layers[i].trainable = False\n#     print(resnet.layers[i])\nbase.summary()\n# base.layers[15].trainable = True\n# base.layers[16].trainable = True\n# base.layers[17].trainable = True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpoint = tf.keras.callbacks.ModelCheckpoint('flower_classifierNN.h5', #where to save the model\n                                                    save_best_only=True, \n                                                    monitor='val_acc', \n                                                    mode='max', \n                                                    verbose = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history=model.fit(X, Y, batch_size=64, epochs=30, verbose=1, validation_split=0.2, shuffle=True,callbacks = [checkpoint])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"So as we can see CNN with transfer learning gave a val_acc of 93.75%\nwhile CNN had 86.09% and NN had 48.53% and hence we can say that\nCNN with transfer learning performs a lot better than any of the rest."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}