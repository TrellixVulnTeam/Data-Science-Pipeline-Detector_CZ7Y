{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport cv2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import keras\nfrom keras.layers import Dense,Conv2D,Dropout,BatchNormalization,Activation,Flatten,MaxPool2D,Input,LeakyReLU\nfrom keras.models import Sequential\nfrom keras.activations import relu\nfrom keras.optimizers import Adam,RMSprop,SGD\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.utils import to_categorical\nfrom keras.applications.vgg16 import VGG16\nfrom sklearn import svm\nfrom keras.preprocessing import image\nimport xgboost\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nimport sklearn.preprocessing as pr","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualizing the image","metadata":{}},{"cell_type":"code","source":"img = cv2.imread('../input/plant-seedlings-classification/train/Maize/6e9ff31e7.png')\nplt.imshow(img)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Loading the data 128x128 for ANN model**","metadata":{}},{"cell_type":"markdown","source":"# Importing Images for Folder for supervise learning model","metadata":{}},{"cell_type":"code","source":"train_path='../input/plant-seedlings-classification/train'\ntest_path='../input/plant-seedlings-classification/test'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train_ann =[];\ny_train_ann =[]\n\nfor subfolder in os.listdir(train_path):\n    for file in os.listdir(train_path+'/' +subfolder):\n        path = train_path+'/' +subfolder+ \"/\" + file\n        im = cv2.imread(path)\n        ims = cv2.resize(im,(128,128))\n        ims = ims/255.0\n        x_train_ann.append(ims)\n        y_train_ann.append(subfolder)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"le_ann = pr.LabelEncoder()\ny_train_le_ann = le_ann.fit_transform(y_train_ann)\n\nx_ann,x_test_ann,y_ann,y_test_ann =  train_test_split(x_train_ann,y_train_le_ann ,test_size=0.2,random_state=10)\n#y_train_1_le = le.fit_transform(y_train_1)\nx_array_ann = np.array(x_ann)\nx_test_arr_ann = np.array(x_test_ann)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train_ann_cat = to_categorical(y_ann)\ny_test_ann_cat = to_categorical(y_test_ann)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Loding Data for Support Vector Machine model 64x64 because of computation limitation**","metadata":{}},{"cell_type":"code","source":"x_train = []\ny_train = []","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for subfolder in os.listdir(train_path):\n    for file in os.listdir(train_path+'/' +subfolder):\n        path = train_path+'/' +subfolder+ \"/\" + file\n        im = cv2.imread(path)\n        ims = cv2.resize(im,(64,64))\n        ims = ims/127.0\n        x_train.append(ims)\n        y_train.append(subfolder)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"le = pr.LabelEncoder()\ny_train_le = le.fit_transform(y_train)\n\n\nx_svm,x_test_svm,y_svm,y_test_svm =  train_test_split(x_train,y_train_le ,test_size=0.2,random_state=10)\n#y_train_1_le = le.fit_transform(y_train_1)\nx_array_svm = np.array(x_svm)\nx_test_arr_svm = np.array(x_test_svm)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train_cat = to_categorical(y_train_le)\n#y_train_1_cat = to_categorical(y_train_1_le)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"flat_arr_size = len(x_array_svm[0])*len(x_array_svm[0][0])*len(x_array_svm[0][0][0])\nx_arr_flat_svm = x_array_svm.reshape(len(x_array_svm),flat_arr_size)\nx_test_arr_flat_svm = x_test_arr_svm.reshape(len(x_test_arr_svm),flat_arr_size)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Model Building - SVM**","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import roc_curve, auc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define support vector classifier\nsvm = SVC(kernel='linear', probability=True, random_state=10)\n\n# fit model\nsvm.fit(x_arr_flat_svm, y_svm)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# generate predictions\ny_pred = svm.predict(x_test_arr_flat_svm)\n\n# calculate accuracy\naccuracy = accuracy_score(y_test_svm, y_pred)\nprint('Model accuracy is: ', accuracy)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\n\nfilename = 'finalized_svm_model.sav'\npickle.dump(svm, open(filename, 'wb'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Image importing- for CNN - 256x256","metadata":{}},{"cell_type":"code","source":"image_size = 256\nbatch_size = 32","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**To load the images using ImageDataGeneretor class and explicitly indicate the needed parameters (rescaling, flipping, validation splitting.**","metadata":{}},{"cell_type":"code","source":"idg = tf.keras.preprocessing.image.ImageDataGenerator(\n    rescale=1./255,\n    horizontal_flip=True,\n    vertical_flip=True,\n    validation_split=0.2\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Train set**","metadata":{}},{"cell_type":"code","source":"train_gen = idg.flow_from_directory('../input/plant-seedlings-classification/train/',\n                                                    target_size=(image_size, image_size),\n                                                    subset='training',\n                                                    class_mode='categorical',\n                                                    batch_size=batch_size,\n                                                    shuffle=True,\n                                                    seed=1\n                                                )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Validation set**","metadata":{}},{"cell_type":"code","source":"val_gen = idg.flow_from_directory('../input/plant-seedlings-classification/train/',\n                                                   target_size=(image_size, image_size),                                                   \n                                                   subset='validation',\n                                                   class_mode='categorical',\n                                                   batch_size=batch_size,\n                                                   shuffle=True,\n                                                   seed=1\n                                                )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"256*256*3","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **2. Model Building - Deep Neural Network**","metadata":{}},{"cell_type":"code","source":"model_nn = Sequential()\n\ninputs = Input(shape=(128,128,3))\nmodel_nn.add(tf.keras.layers.Flatten())\n\nmodel_nn.add(Dense(512,activation = \"relu\"))\nmodel_nn.add(Dropout(0.2))\n# Normalization layer\nmodel_nn.add(tf.keras.layers.BatchNormalization())\n\nmodel_nn.add(Dense(256,activation = \"relu\"))\nmodel_nn.add(Dropout(0.2))\n# Normalization layer\nmodel_nn.add(tf.keras.layers.BatchNormalization())\n\nmodel_nn.add(Dense(128,activation = \"relu\"))\nmodel_nn.add(Dropout(0.2))\n# Normalization layer\n\nmodel_nn.add(tf.keras.layers.BatchNormalization())\nmodel_nn.add(Dense(32,activation = \"relu\"))\nmodel_nn.add(Dropout(0.2))\n\n\nmodel_nn.add(Dense(12,activation='softmax'))\n#model_nn.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_nn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# You can save the best model to the checkpoint\ncheckpoint_ann = tf.keras.callbacks.ModelCheckpoint('plant_ANN_classifier.h5', #where to save the model\n                                                    save_best_only=True, \n                                                    monitor='val_accuracy', \n                                                    mode='max', \n                                                    verbose = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_gen)\n3803//32","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(x_ann[0][0][0]))\nprint(len(y_train_ann_cat))\ny_train_ann_cat[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(x_test_ann))\nprint(len(y_test_ann_cat))\ny_test_ann_cat[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Without batch-generator its taking time in training the model- Because of computation limitation just running for 50 Epochs \nwith 32 batch sizes","metadata":{}},{"cell_type":"code","source":"\n\n#model_nn.fit(x_train_ann,y,epochs=150,validation_data=(x_train_ann,y_train_ann_cat),batch_size=128,callbacks = [checkpoint_ann],\n #         verbose = 1)\nhistory = model_nn.fit_generator(train_gen,\n          epochs=50, # Increase number of epochs if you have sufficient hardware\n          steps_per_epoch= 3803//batch_size,  # Number of train images // batch_size\n          validation_data=val_gen,\n          validation_steps = 947//batch_size, # Number of val images // batch_size\n          callbacks = [checkpoint_ann],\n          verbose = 1\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Visualize a portion of images to make sure they're correctly loaded.**","metadata":{}},{"cell_type":"code","source":"x,y = next(train_gen)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from mpl_toolkits.axes_grid1 import ImageGrid\n\ndef show_grid(image_list, nrows, ncols, label_list=None, show_labels=False, figsize=(10,10)):\n\n    fig = plt.figure(None, figsize,frameon=False)\n    grid = ImageGrid(fig, 111, \n                     nrows_ncols=(nrows, ncols),  \n                     axes_pad=0.2, \n                     share_all=True,\n                     )\n    for i in range(nrows*ncols):\n        ax = grid[i]\n        ax.imshow(image_list[i],cmap='Greys_r')\n        ax.axis('off')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_grid(x,2,4,show_labels=True,figsize=(10,10))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The images are nicely loaded and do not have any rotation and distortion.**","metadata":{}},{"cell_type":"markdown","source":"# 3. Model building - CNN ","metadata":{}},{"cell_type":"code","source":"n_classes = 12;\nmodel = tf.keras.models.Sequential()\n\n# Input layer\n# Can be omitted, you can specify the input_shape in other layers\nmodel.add(tf.keras.layers.InputLayer(input_shape=(image_size,image_size,3,)))\n\n# 1. 2D Convolution layer\nmodel.add(tf.keras.layers.Conv2D(64, kernel_size=(3,3), activation='relu'))\nmodel.add(tf.keras.layers.MaxPool2D(pool_size = (2,2)))\nmodel.add(tf.keras.layers.BatchNormalization())\n\n# 2. 2D Convolution layer\nmodel.add(tf.keras.layers.Conv2D(64, kernel_size=(3,3), strides = (1,1), activation='relu')) \nmodel.add(tf.keras.layers.MaxPool2D(pool_size = (2,2)))\nmodel.add(tf.keras.layers.BatchNormalization())\n\n# 3. 2D Convolution layer\nmodel.add(tf.keras.layers.Conv2D(128, kernel_size=(3,3), strides = (1,1), activation='relu'))\nmodel.add(tf.keras.layers.MaxPool2D(pool_size = (2,2)))\nmodel.add(tf.keras.layers.BatchNormalization())\n\n# 4. 2D Convolution layer\nmodel.add(tf.keras.layers.Conv2D(128, kernel_size=(3,3), strides = (1,1), activation='relu'))\nmodel.add(tf.keras.layers.MaxPool2D(pool_size = (2,2)))\nmodel.add(tf.keras.layers.GlobalMaxPool2D())\n\n# Dense Layers after flattening the data\nmodel.add(tf.keras.layers.Flatten())\n\nmodel.add(tf.keras.layers.Dense(128, activation='relu'))\n\n# Dropout\n# is used to nullify the outputs that are very close to zero and thus can cause overfitting.\nmodel.add(tf.keras.layers.Dropout(0.2))\nmodel.add(tf.keras.layers.Dense(64, activation='relu'))\nmodel.add(tf.keras.layers.BatchNormalization())\n\n#Add Output Layer- total 12 classes we have to predict\nmodel.add(tf.keras.layers.Dense(n_classes, activation='softmax')) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# You can save the best model to the checkpoint\ncheckpoint = tf.keras.callbacks.ModelCheckpoint('plant_cnn_classifier.h5', #where to save the model\n                                                    save_best_only=True, \n                                                    monitor='val_accuracy', \n                                                    mode='max', \n                                                    verbose = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(train_gen,\n          epochs=30, # Increase number of epochs if you have sufficient hardware\n          steps_per_epoch= 3803//batch_size,  # Number of train images // batch_size\n          validation_data=val_gen,\n          validation_steps = 947//batch_size, # Number of val images // batch_size\n          callbacks = [checkpoint],\n          verbose = 1\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Learning curves vs epoch graph**","metadata":{}},{"cell_type":"code","source":"plt.plot(history.history['accuracy'], label='accuracy')\nplt.plot(history.history['val_accuracy'], label = 'val_accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.xticks(list(range(1,50)))\nplt.ylim([0, 1])\nplt.legend(loc='lower right')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Conclusion\n\nTo classify the image, we have build total 3 models:- \n1. SVM Classifier - Only obtained approx 0.44 accuracy. It can be more but due to computation limitation, i used 64x64 data with default param. No have done any hyperparams tunning, bcz its taking time to get trained\n\n2. Deep Neural Network - Accuracy i got only aprrox .40, we can get more if we will run for more epochs. due to high computation time and resources are required not have tunned the model.\n\n3. CNN Model - Obtained accuracy is approx .93, we can get it more as well but here resouces and time even to achiving 93 its took less time than both DNN and SVN model. Even CNN is also without tunning we are able to reach 93-94 accuracy\n\nFinal conclusion is, If image size is smaller then may SVM and DNN can give bit better result but not better than CNN.\nHere best model is cnn which save with \"plant_cnn_classifier.h5\" name","metadata":{}},{"cell_type":"code","source":"predict_img = '../input/prediction/Predict.png'\nimg = cv2.imread(predict_img)\nplt.imshow(img)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_img = cv2.imread(predict_img)\ndata_img = cv2.resize(data_img,(256,256))\ndata_img = data_img/255.0\nlen(data_img[0])\n#model.predict(data_img)\n\npredict_gen = idg.flow_from_directory('../input/prediction',\n                                                    target_size=(image_size, image_size),\n                                                    subset='training',\n                                                    batch_size=batch_size,\n                                                    shuffle=True,\n                                                    seed=1\n                                                )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*** Convolution Neural Networks or CNN uses filtration to analyze image inputs by casting multiple layers on images. These rectified linear unit layer understand patterns and provide an n-dimensional vector output. To learning param is less so it will easy to train CNN model with higher pixel images. Mainly CNN is to extract the features and it will pass to ANN only to predict but CNN is more capable to create feature due to localization of pixel that its passed to network ***","metadata":{}},{"cell_type":"markdown","source":"Task to prepare the data for AI model to predict car manufacturers are popular in a certain area of the city or locality.\n\nThere are Mainly Approx 60 Automobile brand. So there are 60 classes we can consider to classify.\n\nIf its just image classification then we have to do manual effort to separate car images to there respective classes-\n \nAs per given image, i can face below difficulties :-\n\n1. We need to collect more data to maintaining the veraities of each car brands and there different product\n2. There should be difference combination of images get captured as we can do.\n3. Seperate it out all the images to there respestive class lable folders\n4. As in given iamges, 2 images have poor pixel quality and in training data we are not able to find it manually that its belong to which brand. So, there should be such example present but correctly placed.\n5. Size of images are varying large range. Its should be in smaller range so, after resizing the images it will maintain their features.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}