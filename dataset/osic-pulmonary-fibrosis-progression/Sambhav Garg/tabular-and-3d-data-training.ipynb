{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nfrom PIL import Image\nimport pydicom\nimport matplotlib.pyplot as plt\nimport pylab\nimport cv2\nfrom tensorflow.keras.utils import Sequence\n#import gdcm\nfrom tqdm import tqdm\n\nimport tensorflow as tf\nimport tensorflow.keras.layers as L\nimport tensorflow.keras.models as M\nimport tensorflow.keras.backend as K\nimport tensorflow.keras.regularizers as R\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import KFold","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EPOCHS = 5\nNUM_IMAGES = 150\nBATCH_SIZE = 5\nFOLDS = 5\nIMAGE_DIM = (NUM_IMAGES,55,55)\nCOMP_DIR = '../input/osic-pulmonary-fibrosis-progression/'\nTRAIN_PATH = '../input/osic-pulmonary-fibrosis-progression/train'\nTEST_PATH = '../input/osic-pulmonary-fibrosis-progression/test'\nSUB_PATH = '../input/osic-pulmonary-fibrosis-progression/sample_submission.csv'","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"comp_dir = '../input/osic-pulmonary-fibrosis-progression'\n\ntrain_data = pd.read_csv(os.path.join(comp_dir,'train.csv'))\nsub = pd.read_csv(os.path.join(comp_dir,'sample_submission.csv'))\ntest_data = pd.read_csv(os.path.join(comp_dir,'test.csv'))\ntrain_data.drop_duplicates(keep=False, inplace=True, subset=['Patient','Weeks'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data_u = train_data\ntrain_data_u = train_data_u.drop_duplicates(subset=['Patient'])\ntrain_data_u = train_data_u.rename(columns={'Weeks':'Base_Week','FVC':'Base_FVC','Percent':'Base_Percent'})\ntrain_data_u['Typical_FVC'] = (train_data_u.Base_FVC.values/train_data_u.Base_Percent.values)*100\ntrain_data = train_data.merge(train_data_u.drop(['Age','Sex','SmokingStatus'],axis=1),on='Patient',how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub['Patient'] = sub['Patient_Week'].apply(lambda x:x.split('_')[0])\nsub['Weeks'] = sub['Patient_Week'].apply(lambda x: int(x.split('_')[-1]))\nsub = sub.drop(['Confidence'],axis=1)\nsub =  sub[['Patient','Weeks','Patient_Week']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data = test_data.rename(columns={'Weeks':'Base_Week','FVC':'Base_FVC','Percent':'Base_Percent'})\ntest_data['Typical_FVC'] = (test_data.Base_FVC.values/test_data.Base_Percent.values)*100\nsub = sub.merge(test_data, how='left', on='Patient')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['Type'] = 'train'\nsub['Type'] = 'test'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = train_data.append(sub)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction_col = [\"FVC\"]\nContinuos_cols = [\"Weeks\",\"Base_Week\",\"Base_FVC\",\"Typical_FVC\",\"Age\",\"Percent\",\"Base_Percent\"]\nCategorical_cols = ['Sex','Smoking_status']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = MinMaxScaler()\nconti = scaler.fit_transform(data[Continuos_cols])\ndata[Continuos_cols] = conti","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(np.mean(train_data_u.query('SmokingStatus == \\'Never smoked\\'').Base_Percent.values))\nprint(np.mean(train_data_u.query('SmokingStatus == \\'Currently smokes\\'').Base_Percent.values))\nprint(np.mean(train_data_u.query('SmokingStatus == \\'Ex-smoker\\'').Base_Percent.values))\nprint(np.mean(train_data_u.query('Sex == \\'Male\\'').Base_Percent.values))\nprint(np.mean(train_data_u.query('Sex == \\'Female\\'').Base_Percent.values))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sex_m = np.zeros((len(data['Sex'].values),1))\nsex_f = np.zeros((len(data['Sex'].values),1))\nsm_es = np.zeros((len(data['Sex'].values),1))\nsm_ns = np.zeros((len(data['Sex'].values),1))\nsm_cs = np.zeros((len(data['Sex'].values),1))\nfor i in range(len(data['Sex'].values)):\n    if data['Sex'].values[i] == 'Male':\n        sex_m[i] = 1\n    elif data['Sex'].values[i] == 'Female':\n        sex_f[i] = 1\nfor i in range(len(data['SmokingStatus'].values)):\n    if data['SmokingStatus'].values[i] =='Ex-smoker':\n        sm_es[i] = 1\n    elif data['SmokingStatus'].values[i] =='Never smoked':\n        sm_ns[i] = 1\n    else:\n        sm_cs[i] = 1\n\ndata['sex_m'] = sex_m\ndata['sex_f'] = sex_f\ndata['sm_es'] = sm_es\ndata['sm_ns'] = sm_ns\ndata['sm_cs'] = sm_cs\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"for i in range(len(data['Sex'].values)):\n    if data['Sex'].values[i] =='Male':\n        data['Sex'].values[i] = 0\n    else:\n        data['Sex'].values[i] = 1\n\nfor i in range(len(data['SmokingStatus'].values)):\n    if data['SmokingStatus'].values[i] =='Ex-smoker':\n        data['SmokingStatus'].values[i] = 0\n    elif data['SmokingStatus'].values[i] =='Never smoked':\n        data['SmokingStatus'].values[i] = 1     \n    else:\n        data['SmokingStatus'].values[i] = 2 "},{"metadata":{"trusted":true},"cell_type":"code","source":"data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_cols = ['Weeks','Base_Week','Base_FVC','Age','sex_m','sex_f','sm_es','sm_ns','sm_cs']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = data[x_cols].loc[data['Type'] == \"train\"].values.astype(np.float)\ny_train = data[prediction_col].loc[data['Type'] == \"train\"].values.astype(np.float)\nx_test = data[x_cols].loc[data['Type'] == \"test\"].values.astype(np.float)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = x_train.astype(np.float32)\ny_train = y_train.astype(np.float32)\nx_test = x_test.astype(np.float32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train.shape,y_train.shape,x_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(x_train[0][3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(x_train),type(y_train),type(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Data_Generator(tf.keras.utils.Sequence):\n    \n    def __init__(self,batch_size,patient_ids,tab_data,dim,target=None,train=True,augment=False):\n        self.batch_size = batch_size\n        self.image_ids = patient_ids\n        self.augment = augment\n        self.dim = dim\n        self.target = target\n        self.indices = range(len(self.image_ids))\n        self.train = train\n        self.tab_data = tab_data\n        #self.on_epoch_end()\n    \n    def getimage(self,image_id):\n        X1 = np.zeros((NUM_IMAGES,self.dim[1],self.dim[2], 1))\n        if self.train:\n            path = TRAIN_PATH\n        else:\n            path = TEST_PATH\n        im_num = len(os.listdir(os.path.join(path,image_id)))\n        if im_num < NUM_IMAGES+1:\n            for i,dcm_i in enumerate(os.listdir(os.path.join(path,image_id))):\n                try:\n                    im = pydicom.dcmread(os.path.join(TRAIN_PATH,f'{image_id}/{dcm_i}'))\n                    img = im.pixel_array/255\n                    img = cv2.resize(img, (self.dim[1],self.dim[2]))\n                    img = np.reshape(img,(IMAGE_DIM[1],IMAGE_DIM[2],1))\n                    X1[i,] = img\n                    if i>=NUM_IMAGES-1:\n                        break\n                except:\n                    continue\n        else:\n            val = (im_num - NUM_IMAGES)//2\n            dir_list = os.listdir(os.path.join(path,image_id))\n            dir_list.sort()\n            for i,dcm_i in enumerate(dir_list[val:]):\n                try:\n                    im = pydicom.dcmread(os.path.join(TRAIN_PATH,f'{image_id}/{dcm_i}'))\n                    img = im.pixel_array/255\n                    img = cv2.resize(img, (self.dim[1],self.dim[2]))\n                    img = np.reshape(img,(IMAGE_DIM[1],IMAGE_DIM[2],1))\n                    X1[i,] = img\n                    if i>=NUM_IMAGES-1:\n                        break\n                except:\n                    continue\n               \n        if self.augment == True:\n            img = self.ImageAugment(img)\n            return img\n        return X1\n    \n    def on_epoch_end(self):\n        return self.indices\n    \n    def getdata(self, image_id_list):\n        X = np.empty((self.batch_size,*self.dim, 1))\n        for i, im_id in enumerate(image_id_list):\n            #print(i)\n            X[i,] = self.getimage(im_id)\n        \n        return X\n    '''\n    def ImageAugment(self,image):\n        augmentor = ImageAugmentor(image,axis_point=[self.dim/2,self.dim/2])\n        augmentor.cutmix()\n        #augmentor.zoom()\n        augmentor.flip()\n        augmentor.rotate()\n        return augmentor.get_image()\n    ''' \n    \n    def __getitem__(self,index):\n        indices = self.indices[index*self.batch_size:(index+1)*self.batch_size]\n        \n        image_id_list = [self.image_ids[k] for k in indices]\n        tab_X = np.array([self.tab_data[k] for k in indices]).astype(np.float32)\n        X = self.getdata(image_id_list)\n        if self.train == True:\n            target_list = [self.target[k] for k in indices]\n            y = np.array(target_list).astype(np.float32)\n            return X,y\n        return X\n    \n    def __len__(self):\n        return int(np.floor(len(self.indices)/self.batch_size))\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"C1, C2 = tf.constant(70, dtype='float32'), tf.constant(1000, dtype=\"float32\")\n#=============================#\ndef score(y_true, y_pred):\n    tf.dtypes.cast(y_true, tf.float32)\n    tf.dtypes.cast(y_pred, tf.float32)\n    sigma = y_pred[:,2]-y_pred[:,0]\n    fvc_pred = y_pred[:,1]\n    #sigma_clip = sigma + C1\n    sigma_clip = tf.maximum(sigma, C1)\n    delta = tf.abs(y_true[:,0] - fvc_pred)\n    delta = tf.minimum(delta, C2)\n    sq2 = tf.sqrt( tf.dtypes.cast(2, dtype=tf.float32) )\n    metric = (delta / sigma_clip)*sq2 + tf.math.log(sigma_clip* sq2)\n    return K.mean(metric)\n#============================#\ndef qloss(y_true, y_pred):\n    # Pinball loss for multiple quantiles\n    qs = [0.2,0.50,0.8]\n    q = tf.constant(np.array([qs]), dtype=tf.float32)\n    e = y_true - y_pred\n    v = tf.maximum(q*e,(q-1)*e)\n    return K.mean(v)\n#=============================#\ndef mloss(_lambda):\n    def loss(y_true, y_pred):\n        return _lambda * qloss(y_true, y_pred) + (1 - _lambda)*score(y_true, y_pred)\n    return loss\n#=================","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model_dense():\n    \n    inp = L.Input((9,))\n    x = L.Dense(128,activation ='relu',kernel_regularizer=R.l2(5e-4),name='dense')(inp)\n    x = L.Dense(128,activation ='relu',kernel_regularizer=R.l2(5e-4),name='dense_1')(x)\n    x = L.Dense(64,activation ='relu',kernel_regularizer=R.l2(5e-5),name='dense_2')(x)\n    o1 = L.Dense(3,activation = 'linear',name='dense_3')(x)\n    o2 = L.Dense(3,activation = 'relu',name='dense_4')(x)\n    pred1 = L.Lambda(lambda x: (x[0]+(tf.cumsum(x[1],axis=1))))([o1,o2])    \n    \n    \n    model = M.Model(inputs=inp,outputs=pred1)\n    \n    model.compile(loss=mloss(0.85), optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3), metrics=[score])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dense_model = build_model_dense()\ndense_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold\nfolds = 5\nKF = KFold(n_splits=folds,shuffle=True,random_state=24)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ncounter=0\nstopper = tf.keras.callbacks.EarlyStopping(monitor='loss',mode='min',patience=100,restore_best_weights=True)\nreduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.1,patience=50, min_lr=1e-3)\nfor tr_idx, val_idx in KF.split(x_train):\n    counter+=1\n    print(f'############## FOLD {counter} ###############')\n    history = dense_model.fit(x_train[tr_idx],y_train[tr_idx],epochs=500,batch_size = 256,verbose=0,validation_data=(x_train[val_idx],y_train[val_idx]))\n    print(\"train\", dense_model.evaluate(x_train[tr_idx], y_train[tr_idx], verbose=0, batch_size=256))\n    print(\"val\", dense_model.evaluate(x_train[val_idx], y_train[val_idx], verbose=0, batch_size=256))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dense_model.save('dense_model.h5')\ndense_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model():\n        \n    inp2 = L.Input(shape=(*IMAGE_DIM,1,),name='conv_input')\n    \n    x = L.Conv3D(64,5,activation='relu')(inp2)\n    \n    x = L.Conv3D(64,5,activation='relu')(x)\n    \n    x = L.BatchNormalization()(x)\n    \n    x = L.MaxPooling3D()(x)\n    \n    x1 = L.Dropout(0.3)(x)\n    \n    x = L.Conv3D(64,7,activation='relu',padding='same')(x1)\n    \n    x = L.Dropout(0.3)(x)\n    \n    x = L.add([x,x1])\n    \n    x = L.Conv3D(256,7,activation='relu')(x)\n    \n    x = L.Conv3D(128,9,activation='relu')(x)\n    \n    x = L.BatchNormalization()(x)\n    \n    x = L.MaxPooling3D()(x)\n    \n    x2 = L.Dropout(0.43)(x)\n    \n    x = L.Conv3D(128,7,activation='relu',padding='same',data_format='channels_last')(x2)\n    \n    x = L.Dropout(0.43)(x)\n    \n    x = L.add([x,x2])\n    \n    x = L.BatchNormalization()(x)\n    \n    x = L.MaxPooling3D()(x)\n    \n    x = L.Dropout(0.43)(x)\n    \n    op1 = L.GlobalAveragePooling3D()(x)\n    '''\n    tab_model = M.load_model('dense_model.h5',custom_objects={'loss':mloss,'score':score})\n    \n    tab_model.trainable = False\n    \n    inp = L.Input((9,),name='input_d')\n\n    d = tab_model.get_layer(name='dense')(inp)\n    \n    d1 = tab_model.get_layer(name='dense_1')(d)\n    \n    op2 = tab_model.get_layer(name='dense_2')(d1)    \n    \n    inp = L.Input(shape=(4,),name='tab_input')\n    \n    x = L.Dense(128,activation ='relu',kernel_regularizer=R.l2(5e-4))(inp)\n    \n    x = L.Dense(128,activation ='relu',kernel_regularizer=R.l2(4e-4))(x)\n    \n    op2 = L.Dense(64,activation ='relu',kernel_regularizer=R.l2(2e-5))(x)\n \n    x = L.add([op1,op2])\n    '''\n    o1 = L.Dense(3,activation = 'linear',name = 'dense_f1')(op1)\n    o2 = L.Dense(3,activation = 'relu', name='dense_f2')(op1)\n    \n    pred1 = L.Lambda(lambda x: (x[0]+(tf.cumsum(x[1],axis=1))),name='output')([o1,o2])    \n    \n    \n    model = M.Model(inputs=inp2,outputs=pred1)\n    \n    model.compile(loss=mloss(0.9), optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), metrics=[score])\n    \n    return model\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = build_model()\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"folds = 5\nKF = KFold(n_splits=folds,shuffle=True,random_state=24)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = data[x_cols].loc[data['Type'] == \"train\"].values\ny_train = data[prediction_col].loc[data['Type'] == \"train\"].values\nx_test = data[x_cols].loc[data['Type'] == \"test\"].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_lr_callback():\n    lr_start   = 0.001\n    lr_max     = 0.001\n    lr_min     = 0.00001\n    lr_ramp_ep = EPOCHS\n    lr_sus_ep  = 0\n    lr_decay   = 0.9\n   \n    def lrfn(epoch):\n        if epoch < lr_ramp_ep:\n            lr = lr_start - ( lr_start - lr_min ) / lr_ramp_ep * epoch \n            \n        elif epoch < lr_ramp_ep + lr_sus_ep:\n            lr = lr_min\n            '''\n        else:\n            lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n            '''\n        return lr\n    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=False)\n    return lr_callback","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#tf.keras.utils.plot_model(model, \"multi_input_and_output_model.png\", show_shapes=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nx_train = data[x_cols].loc[data['Type'] == \"train\"].values\npatient_ids = data['Patient'].loc[data['Type']=='train'].values\ntarget_val = data[prediction_col].loc[data['Type'] == \"train\"].values\nfor fold,(t_idx,v_idx) in enumerate(KF.split(patient_ids)):\n    print(f\"################ FOLD {fold+1} #####################\")\n    train_gen = Data_Generator(BATCH_SIZE,patient_ids[t_idx],x_train[t_idx],IMAGE_DIM,target=target_val[t_idx],train=True)\n    val_gen = Data_Generator(BATCH_SIZE,patient_ids[v_idx],x_train[v_idx],IMAGE_DIM,target=target_val[v_idx],train=True)\n    total_val_train = len(patient_ids[v_idx])\n    total_train = len(patient_ids[t_idx])\n    history = model.fit(\n        train_gen,\n        steps_per_epoch=total_train//BATCH_SIZE,\n        epochs=EPOCHS,\n        validation_data = val_gen,\n        validation_steps = total_val_train//BATCH_SIZE,\n        verbose=1,\n        callbacks=[get_lr_callback()]\n    )\n    \n    plt.figure()\n    plt.plot(list(range(EPOCHS)),history.history['val_loss'])\n    plt.plot(list(range(EPOCHS)),history.history['loss'])\n    plt.xlabel(\"Epochs\")\n    plt.ylabel('loss')\n    plt.legend(['val_loss','loss'])\n    plt.show()\n    \n    plt.figure()\n    plt.plot(list(range(EPOCHS)),history.history['val_score'])\n    plt.plot(list(range(EPOCHS)),history.history['score'])\n    plt.xlabel(\"Epochs\")\n    plt.ylabel('score')\n    plt.legend(['val_score','score'])\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save('model.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#model = M.load_model('../input/tab-data-osic/model.h5',custom_objects={'loss':mloss,'score':score})\n#model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#test_patient_ids = data['Patient'].loc[data['Type'] == \"test\"].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#test_gen = Data_Generator(2,test_patient_ids,x_test,IMAGE_DIM,train=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print(\"Inferencing\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#pred = model.predict(test_gen,verbose=1,batch_size=BATCH_SIZE)3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#pred","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"#conf = pred[:,2] - pred[:,0]\n#for i in range(len(conf)):\n#    conf[i] = max(conf[i],70)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#pred[:,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#pred_dict = {'FVC':pred[:,1],'Confidence':conf}\n#pred_df = pd.DataFrame(pred_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sub['Confidence'] = pred_df['Confidence']\n#sub['FVC'] = pred_df['FVC']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#subm = sub[['Patient_Week','FVC','Confidence']].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#subm.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"import matplotlib.animation as animation\nimport matplotlib.pyplot as plt\nfor j in pat:\n    fig = plt.figure()\n    Writer = animation.writers['ffmpeg']\n    writer = Writer(fps=15, metadata=dict(artist='Me'), bitrate=1800)\n    im=[]\n    length = len([name for name in os.listdir(f'../input/osic-pulmonary-fibrosis-progression/train/{j}')])\n    try:\n        for i in range(1,length+1):\n            img_path = f'{comp_dir}/train/{j}/{i}.dcm'\n            img = pydicom.dcmread(img_path)\n            img = plt.imshow(img.pixel_array,cmap='gray')\n            im.append([img])\n        #im_ani = animation.FuncAnimation(fig, im, 25,interval=50, blit=True)\n        im_ani = animation.ArtistAnimation(fig, im, interval=50,blit=True)\n        im_ani.save(f'{j}.mp4', writer=writer)\n    except:\n        pass"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}