{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import copy\nfrom datetime import datetime, timedelta\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom sklearn.model_selection import GroupKFold,GroupShuffleSplit\nfrom torch.utils.data import Dataset\nfrom tqdm import tqdm\nimport torch\nfrom torch.utils.data import DataLoader, Subset\n\nfrom torch.optim import Adam\nfrom torch.optim.lr_scheduler import StepLR\nfrom torch.utils.tensorboard import SummaryWriter\nfrom tqdm.notebook import trange\nfrom time import time\nroot_dir = Path('/kaggle/input/osic-pulmonary-fibrosis-progression')\nmodel_dir = Path('/kaggle/working')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 要写自定义的数据集（__init__,__len__,__getitem__）","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# pd.read_csv(Path(root_dir)/\"train.csv\").head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# #先做些basic check\n# tr = pd.read_csv(Path(root_dir)/\"train.csv\")# print(len(tr))=1549\n# tr[tr.duplicated(keep=False,subset=['Patient', 'Weeks'])==True]#把重复的打印了一下康康\n# # 好神奇啊，这是一周内看了两次病？","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sub = pd.read_csv(Path(root_dir)/\"sample_submission.csv\")\n# sub #预测的时候是对样本做全段预测   5个病人*146周","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# tst = pd.read_csv(Path(root_dir)/\"test.csv\")#test_data\n# tst#已知 week0 CT info + tabular below","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#这部分对sub的结构小小处理了一下，到时候做test的时候再说\n# sub = pd.read_csv(Path(root_dir)/\"sample_submission.csv\")\n# sub['Patient'] = sub['Patient_Week'].apply(lambda x: x.split('_')[0])\n# sub['Weeks'] = sub['Patient_Week'].apply(lambda x: int(x.split('_')[-1]))\n# sub = sub[['Patient', 'Weeks', 'Confidence', 'Patient_Week']]\n# sub.merge(tst.drop('Weeks', axis=1), on=\"Patient\",how='inner')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# tr = pd.read_csv(Path(root_dir)/\"train.csv\")#train_data\n# tr.drop_duplicates(keep=\"first\", inplace=True, subset=['Patient', 'Weeks'])\n# tr['minweek']=tr.groupby('Patient')['Weeks'].transform(min)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# tr.iloc[tr['Percent'].argmax(),:]#这个percent还是可以用一下的，不过我看的两个本子好像都没用","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# base = tr.loc[tr.Weeks == tr.minweek]\n# base = base[['Patient', 'FVC']].copy()\n# base.columns = ['Patient', 'minweek_FVC']\n# tr=tr.merge(base, on='Patient', how='inner')#train部分数据不脏，left和inner都没问题\n# tr['base_week']=tr['Weeks']-tr['minweek']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# base#恰好对应176个人","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# data=tr.copy()\n# COLS = ['Sex', 'SmokingStatus']\n# FE = []\n# for col in COLS:\n#     for mod in data[col].unique():\n#         FE.append(mod)\n#         data[mod] = (data[col] == mod).astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import numpy as np\n# x = np.array([0, 1, 2, 3])\n# y = np.array([-1, 0.2, 0.9, 2.1])\n# A = np.vstack([x, np.ones(len(x))]).T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# np.vstack([x, np.ones(len(x))])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# np.linalg.lstsq(A,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import pydicom\n# import cv2\n# def get_img(path):\n#     d = pydicom.dcmread(path)\n#     return cv2.resize((d.pixel_array - d.RescaleIntercept) / (d.RescaleSlope), (512, 512))\n\n# d = pydicom.dcmread(root_dir/'train/ID00007637202177411956430'/'6.dcm')\n# print(d.pixel_array)\n# print(d.RescaleIntercept)\n# print(d.RescaleSlope)\n# im=((d.pixel_array - d.RescaleIntercept) / (d.RescaleSlope * 1000))\n\n# # %matplotlib inline\n# import os\n# from matplotlib import pyplot as plt\n# k=0\n# for i in os.listdir(root_dir/'train/ID00007637202177411956430'):\n#     if k>=0:\n#         m=get_img(root_dir/'train/ID00007637202177411956430'/i)\n# #         print(m)\n#         plt.imshow(m,cmap='g')\n#         plt.show()\n#         k=k+1\n# #         plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#819 new！！\n# tst = pd.read_csv(Path(root_dir)/\"test.csv\")#test_data\n# tst['minweek']=tst['Weeks']\n# tst['minweek_FVC']=tst['FVC']\n# sub = pd.read_csv(Path(root_dir)/\"sample_submission.csv\")\n# sub['Patient'] = sub['Patient_Week'].apply(lambda x: x.split('_')[0])\n# sub['Weeks'] = sub['Patient_Week'].apply(lambda x: int(x.split('_')[-1]))\n# sub = sub[['Patient', 'Weeks', 'Confidence', 'Patient_Week']]\n# sub = sub.merge(tst.drop('Weeks', axis=1), on=\"Patient\",how=\"inner\")#样子就是之前单元格中的样子\n# sub['base_week']=sub['Weeks']-sub['minweek']\n\n# tr = pd.read_csv(Path(root_dir)/\"train.csv\")#train_data\n# tr.drop_duplicates(keep=\"first\", inplace=True, subset=['Patient', 'Weeks']) #暂且留其中一个把 keep=“first”\n# tst = pd.read_csv(Path(root_dir)/\"test.csv\")#test_data\n# tr['minweek']=tr.groupby('Patient')['Weeks'].transform(min)\n# base = tr.loc[tr.Weeks == tr.minweek] #找基准\n# base = base[['Patient', 'FVC']]\n# base.columns = ['Patient', 'minweek_FVC']#改名\n# tr=tr.merge(base, on='Patient', how='inner')#train部分数据不脏，left和inner都没问题\n# tr['base_week']=tr['Weeks']-tr['minweek']\n# data=tr.append(sub)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# data[~data['Patient_Week'].isna()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 从这里开始","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class ClinicalDataset(Dataset):\n    def __init__(self, root_dir, mode, transform=None):\n        self.transform = transform   #tabular的东西transform先挂着，感觉一般不用\n        self.mode = mode             #train or test\n        tst = pd.read_csv(Path(root_dir)/\"test.csv\")#test_data\n        tst['minweek']=tst['Weeks']\n        tst['minweek_FVC']=tst['FVC']\n        sub = pd.read_csv(Path(root_dir)/\"sample_submission.csv\")\n        sub['Patient'] = sub['Patient_Week'].apply(lambda x: x.split('_')[0])\n        sub['Weeks'] = sub['Patient_Week'].apply(lambda x: int(x.split('_')[-1]))\n        sub = sub[['Patient', 'Weeks', 'Confidence', 'Patient_Week']]\n        sub = sub.merge(tst.drop('Weeks', axis=1), on=\"Patient\",how=\"inner\")#样子就是之前单元格中的样子\n        sub['base_week']=sub['Weeks']-sub['minweek']\n\n        tr = pd.read_csv(Path(root_dir)/\"train.csv\")#train_data\n        tr.drop_duplicates(keep=\"first\", inplace=True, subset=['Patient', 'Weeks']) #暂且留其中一个把 keep=“first”\n        tst = pd.read_csv(Path(root_dir)/\"test.csv\")#test_data\n        tr['minweek']=tr.groupby('Patient')['Weeks'].transform(min)\n        base = tr.loc[tr.Weeks == tr.minweek] #找基准\n        base = base[['Patient', 'FVC']]\n        base.columns = ['Patient', 'minweek_FVC']#改名\n        tr=tr.merge(base, on='Patient', how='inner')#train部分数据不脏，left和inner都没问题\n        tr['base_week']=tr['Weeks']-tr['minweek']\n        data=tr.append(sub)\n        #归一标准化，数据分位\n        COLS = ['Sex', 'SmokingStatus']\n        self.FE = []\n        for col in COLS:\n            for mod in data[col].unique():\n                self.FE.append(mod)\n                data[mod] = (data[col] == mod).astype(int)\n    \n        data['N_age'] = (data['Age'] - data['Age'].min()) / \\\n                      (data['Age'].max() - data['Age'].min())\n        data['N_base_minweek_FVC'] = (data['minweek_FVC'] - data['minweek_FVC'].min()) / \\\n                       (data['minweek_FVC'].max() - data['minweek_FVC'].min())\n        \n        data['N_week'] = (data['base_week'] - data['base_week'].min()) / \\\n                       (data['base_week'].max() - data['base_week'].min())\n        \n        data['N_percent'] = (data['Percent'] - data['Percent'].min()) / \\\n                          (data['Percent'].max() - data['Percent'].min())\n        \n        self.FE += ['N_age', 'N_percent', 'N_week', 'N_base_minweek_FVC']\n        \n        #目前这个pytorchDatasets 只针对train\n        if self.mode=='train':\n            self.raw = data[data['Patient_Week'].isna()].reset_index()\n        elif self.mode=='test':\n            self.raw =data[~data['Patient_Week'].isna()].reset_index()\n        \n        del base\n        del data\n\n    def __len__(self):\n        return len(self.raw)\n\n    def __getitem__(self, idx):\n        #为了dataframe的切片，转换一下\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n\n        sample = {\n            'patient_id': self.raw['Patient'].iloc[idx],\n            'features': self.raw[self.FE].iloc[idx].values,\n            'target': self.raw['FVC'].iloc[idx]\n        }\n        if self.transform:\n            sample = self.transform(sample)\n\n        return sample\n    \n    def group_kfold(self, n_splits):\n        gkf = GroupKFold(n_splits=n_splits)\n        groups = self.raw['Patient']\n        for train_idx, val_idx in gkf.split(self.raw, self.raw, groups):\n            train = Subset(self, train_idx)\n            val = Subset(self, val_idx)\n            yield train, val\n    def group_split(self, test_size=0.1):\n        gss = GroupShuffleSplit(n_splits=1, test_size=test_size)\n        groups = self.raw['Patient']\n        idx = list(gss.split(self.raw, self.raw, groups))\n        train = Subset(self, idx[0][0])\n        val = Subset(self, idx[0][1])\n        return train, val","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# myDataSet=ClinicalDataset(root_dir, \"test\")\n# i=0\n# for k in enumerate(myDataSet):\n#     if i<10:\n#         print(k)\n#         i=i+1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import torch\n# torch.max(torch.FloatTensor([1,2,3,4,2,8]),torch.FloatTensor([1,2,3,4,2,9])).unsqueeze(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nclass QuantModel(nn.Module):\n    def __init__(self, in_tabular_features=9, out_quantiles=3):\n        super(QuantModel, self).__init__()\n        self.fc1 = nn.Linear(in_tabular_features, 100)\n        self.fc2 = nn.Linear(100, 100)\n        self.fc3 = nn.Linear(100, 50)\n        self.fc4 = nn.Linear(50, 3)\n        self.fc5 = nn.Linear(3,out_quantiles)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = F.relu(self.fc3(x))\n        x = self.fc4(x)\n        x = self.fc5(x)\n        return x\n\n#分位数回归\nquantiles = (0.2, 0.5, 0.8)\ndef quantile_loss(preds, target, quantiles):\n    assert not target.requires_grad\n    assert preds.size(0) == target.size(0)\n    losses = []\n    for i, q in enumerate(quantiles):\n        errors = target - preds[:, i]\n        losses.append(torch.max((q - 1) * errors, q * errors).unsqueeze(1))#添加维度   （x,1）\n    loss = torch.mean(torch.sum(torch.cat(losses, dim=1), dim=1))\n    return loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ten=torch.FloatTensor([[1.        , 0.        , 1.        , 0.        , 0.        ,\n#        0.76923077, 0.23639327, 0.        , 0.24145617],[1.        , 0.        , 1.        , 0.        , 0.        ,\n#        0.76923077, 0.23639327, 0.        , 0.24145617]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# net=QuantModel()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# losses=[]\n# losses.append(net(ten))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# losses","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# net(ten).size()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# torch.mean((torch.sum(torch.cat(losses,dim=1),dim=1)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Path(model_dir).mkdir(parents=True, exist_ok=True)\nclass Monitor:\n    def __init__(self, model, es_patience, experiment_name, tensorboard_dir,\n                 num_epochs, dataset_sizes, model_file):\n\n        self.model = model\n        self.model_file = model_file\n        self.es_patience = es_patience\n        self.tensorboard_dir = tensorboard_dir\n        self.dataset_sizes = dataset_sizes\n        date_time = datetime.now().strftime(\"%Y%m%d-%H%M\")\n        log_dir = tensorboard_dir / f'{experiment_name}-{date_time}'\n        self.w = SummaryWriter(log_dir)\n\n        self.bar = trange(num_epochs, desc=experiment_name)\n\n        self.epoch_loss = {'train': np.inf, 'val': np.inf}\n        self.epoch_metric = {'train': -np.inf, 'val': -np.inf}\n        self.best_loss = np.inf\n        self.best_model_wts = None\n\n        self.e = {'train': 0, 'val': 0}  # epoch counter\n        self.t = {'train': 0, 'val': 0}  # global time-step (never resets)\n        self.running_loss = 0.0\n        self.running_metric = 0.0\n        self.es_counter = 0\n\n    def reset_epoch(self):\n        self.running_loss = 0.0\n        self.running_metric = 0.0\n\n    def step(self, loss, inputs, preds, targets, phase):\n        self.running_loss += loss.item() * inputs.size(0)\n        self.running_metric += self.metric(preds, targets).sum()\n        self.t[phase] += 1\n\n    def log_epoch(self, phase):\n        self.epoch_loss[phase] = self.running_loss / self.dataset_sizes[phase]\n        self.epoch_metric[phase] = self.running_metric / self.dataset_sizes[phase]\n        self.bar.set_postfix(\n            a_train_loss=f'{self.epoch_loss[\"train\"]:0.1f}',\n            b_val_loss=f'{self.epoch_loss[\"val\"]:0.1f}',\n            c_train_metric=f'{self.epoch_metric[\"train\"]:0.4f}',\n            d_val_metric=f'{self.epoch_metric[\"val\"]:0.4f}',\n            es_counter=self.es_counter\n        )\n        self.w.add_scalar(\n            f'Loss/{phase}', self.epoch_loss[phase], self.e[phase])\n        self.w.add_scalar(\n            f'Accuracy/{phase}', self.epoch_metric[phase], self.e[phase])\n\n        self.e[phase] += 1\n\n        # Early stop and model backup\n        early_stop = False\n        if phase == 'val':\n            if self.epoch_loss['val'] < self.best_loss:\n                self.best_loss = self.epoch_loss['val']\n                self.best_model_wts = copy.deepcopy(self.model.state_dict())\n                torch.save(self.best_model_wts, self.model_file)\n                self.es_counter = 0\n            else:\n                self.es_counter += 1\n                if self.es_counter >= self.es_patience:\n                    early_stop = True\n                    self.bar.close()\n\n        return early_stop\n\n    @staticmethod\n    def metric(preds, targets):\n        sigma = preds[:, 2] - preds[:, 0]\n        sigma[sigma < 70] = 70\n        delta = (preds[:, 1] - targets).abs()\n        delta[delta > 1000] = 1000\n        return -np.sqrt(2) * delta / sigma - torch.log(np.sqrt(2) * sigma)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models = []\nnum_kfolds=10\n# Load the data\nmydata = ClinicalDataset(root_dir=root_dir, mode='train')\nfolds = mydata.group_kfold(num_kfolds)\nt0 = time()\nbatch_size = 16\nlearning_rate = 3e-3\nnum_epochs = 2000\nes_patience = 20\nquantiles = (0.25, 0.5, 0.75)\nmodel_name ='Pacific'\ntensorboard_dir = Path('/kaggle/working/runs')\ntrainset=mydata.group_split(0.15)[0]\nvalset=mydata.group_split(0.15)[1]\n\n# for fold, (trainset, valset) in enumerate(folds):\n    # Prepare to save model weights\nfold=0\nPath(model_dir).mkdir(parents=True, exist_ok=True)\nnow = datetime.now()\nfname = f'{model_name}-{now.year}{now.month:02d}{now.day:02d}_{fold}.pth'\nmodel_file = Path(model_dir) / fname\n\ndataset_sizes = {'train': len(trainset), 'val': len(valset)}\ndataloaders = {\n    'train': DataLoader(trainset, batch_size=batch_size,\n                        shuffle=True, num_workers=2),\n    'val': DataLoader(valset, batch_size=batch_size,\n                      shuffle=True, num_workers=2)\n}\n\n# Create the model and optimizer\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel = QuantModel().to(device)\noptimizer = Adam(model.parameters(), lr=learning_rate)\nscheduler = StepLR(optimizer, step_size=20, gamma=0.5)\nmonitor = Monitor(\n    model=model,\n    es_patience=es_patience,\n    experiment_name=f'{model_name}_fold_{fold}',\n    tensorboard_dir=tensorboard_dir,\n    num_epochs=num_epochs,\n    dataset_sizes=dataset_sizes,\n    model_file=model_file\n)\n\n# Training loop\nfor epoch in monitor.bar:\n    for phase in ['train', 'val']:\n        if phase == 'train':\n            model.train()  # Set model to training mode\n        else:\n            model.eval()   # Set model to evaluate mode\n\n        monitor.reset_epoch()\n\n        # Iterate over data\n        for batch in dataloaders[phase]:\n            inputs = batch['features'].float().to(device)\n            targets = batch['target'].to(device)\n\n            # zero the parameter gradients\n            optimizer.zero_grad()\n            # forward\n            # track gradients if only in train\n            with torch.set_grad_enabled(phase == 'train'):\n                preds = model(inputs)\n                loss = quantile_loss(preds, targets, quantiles)\n                # backward + optimize only if in training phase\n                if phase == 'train':\n                    loss.backward()\n                    optimizer.step()\n\n            monitor.step(loss, inputs, preds, targets, phase)\n\n#             epoch statistics\n        early_stop = monitor.log_epoch(phase)\n\n    if early_stop:\n        break\n\n    # Updates the learning rate\n    scheduler.step()\n\n# load best model weights\nmodel.load_state_dict(monitor.best_model_wts)\nmodels.append(model)\n\nprint(f'Training complete! Time: {timedelta(seconds=time() - t0)}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# next(iter(dataloaders['train']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# len([4562, 3499, 2793, 2582, 3327, 1529, 2774, 4169, 3370, 4635, 2694, 2914,\n#          1584, 1450, 2541, 2561, 1841, 1401, 2727, 3237, 1613, 2783, 2015, 2182,\n#          4130, 2415, 3829, 3603, 3252, 2600, 3363, 2889])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = ClinicalDataset(root_dir, mode='test')\navg_preds = np.zeros((len(data), len(quantiles)))\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nfor model in models:\n    dataloader = DataLoader(data, batch_size=batch_size,\n                            shuffle=False, num_workers=2)\n    preds = []\n    for batch in dataloader:\n        inputs = batch['features'].float().to(device)\n        with torch.no_grad():\n            x = model(inputs)\n            preds.append(x)\n\n    preds = torch.cat(preds, dim=0).cpu().numpy()\n    avg_preds += preds\n\navg_preds /= len(models)\ndf = pd.DataFrame(data=avg_preds, columns=list(quantiles))\ndf['Patient_Week'] = data.raw['Patient_Week']\ndf['FVC'] = df[quantiles[1]]\ndf['Confidence'] = df[quantiles[2]] - df[quantiles[0]]\ndf = df.drop(columns=list(quantiles))\ndf.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# tr = pd.read_csv(Path(root_dir)/\"train.csv\")#train_data\n# tr.drop_duplicates(keep=\"first\", inplace=True, subset=['Patient', 'Weeks']) #暂且留其中一个把 keep=“first”\n# tst = pd.read_csv(Path(root_dir)/\"test.csv\")#test_data\n\n# #         sub = pd.read_csv(Path(root_dir)/\"sample_submission.csv\")\n# #         sub['Patient'] = sub['Patient_Week'].apply(lambda x: x.split('_')[0])\n# #         sub['Weeks'] = sub['Patient_Week'].apply(lambda x: int(x.split('_')[-1]))\n# #         sub = sub[['Patient', 'Weeks', 'Confidence', 'Patient_Week']]\n# #         sub = sub.merge(tst.drop('Weeks', axis=1), on=\"Patient\",how=\"inner\")#样子就是之前单元格中的样子\n# #         sub['minweek']=sub.groupby('Patient')['Weeks'].transform(min)#待定\n\n# tr['minweek']=tr.groupby('Patient')['Weeks'].transform(min)\n# base = tr.loc[tr.Weeks == tr.minweek] #找基准\n# base = base[['Patient', 'FVC']]\n# base.columns = ['Patient', 'minweek_FVC']#改名\n# tr=tr.merge(base, on='Patient', how='inner')#train部分数据不脏，left和inner都没问题\n# del base\n# tr['base_week']=tr['Weeks']-tr['minweek']\n# #归一标准化，数据分位\n# data=tr.copy()\n# COLS = ['Sex', 'SmokingStatus']\n# FE = []\n# for col in COLS:\n#     for mod in data[col].unique():\n#         FE.append(mod)\n#         data[mod] = (data[col] == mod).astype(int)\n\n# data['N_age'] = (data['Age'] - data['Age'].min()) / \\\n#               (data['Age'].max() - data['Age'].min())\n# data['N_base_minweek_FVC'] = (data['minweek_FVC'] - data['minweek_FVC'].min()) / \\\n#                (data['minweek_FVC'].max() - data['minweek_FVC'].min())\n\n# data['N_week'] = (data['base_week'] - data['base_week'].min()) / \\\n#                (data['base_week'].max() - data['base_week'].min())\n\n# data['N_percent'] = (data['Percent'] - data['Percent'].min()) / \\\n#                   (data['Percent'].max() - data['Percent'].min())\n\n# FE += ['N_age', 'N_percent', 'N_week', 'N_base_minweek_FVC']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# raw=data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# raw","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# gkf=GroupKFold(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for i in gkf.split(raw,raw,raw['Patient']):\n#     print(len(i[0]),len(i[1]))\n#     train=i[0]\n#     val=i[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from torch.utils.data import Subset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Subset(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}