{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# !pip install ../input/kerasapplications/keras-team-keras-applications-3b180cb -f ./ --no-index\n# !pip install ../input/efficientnet/efficientnet-1.1.0/ -f ./ --no-index","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data proc\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pydicom\n# import gdcm\nimport os\nimport matplotlib.gridspec as gridspec\nfrom tqdm import tqdm\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential, load_model, Model\nfrom tensorflow.keras.layers import ZeroPadding2D,UpSampling2D,ThresholdedReLU,Conv2DTranspose, Cropping2D, DepthwiseConv2D,add, Activation, LeakyReLU, Dense, Conv2D, GlobalMaxPooling2D , MaxPooling2D, Flatten,Concatenate, Input, Dropout, BatchNormalization, GlobalAveragePooling2D, SeparableConv2D, AveragePooling2D\nimport cv2\nfrom tensorflow.keras.optimizers import RMSprop, Adam, SGD \nfrom tensorflow.keras.metrics import TruePositives, FalsePositives, TrueNegatives, FalseNegatives, AUC, BinaryAccuracy\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.activations import softsign\nfrom keras.initializers import Constant\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import KFold,StratifiedKFold,StratifiedShuffleSplit\nimport random\nimport keras.backend as K\nimport gc\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler,StandardScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.neighbors import KNeighborsRegressor,NearestNeighbors\nfrom sklearn.linear_model import LinearRegression,Ridge, Lasso, LogisticRegression\nfrom sklearn.ensemble import AdaBoostRegressor,GradientBoostingRegressor\nfrom sklearn.linear_model import ElasticNet, BayesianRidge\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.preprocessing import OneHotEncoder\n# import efficientnet.tfkeras as efn","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"TRAIN = pd.read_csv(\"../input/osic-pulmonary-fibrosis-progression/train.csv\")\nTRAIN22 = pd.read_csv(\"../input/osic-pulmonary-fibrosis-progression/train.csv\")\nTEST =  pd.read_csv(\"../input/osic-pulmonary-fibrosis-progression/test.csv\")\nSUB =  pd.read_csv(\"../input/osic-pulmonary-fibrosis-progression/sample_submission.csv\")\nTRAIN_DIR = \"../input/osic-pulmonary-fibrosis-progression/train/\"\nTEST_DIR = \"../input/osic-pulmonary-fibrosis-progression/test/\"\n# TRAIN = TRAIN.loc[~TRAIN['Patient'].isin([\"ID00011637202177653955184\",\"ID00052637202186188008618\"])]\nTRAIN.reset_index(drop=True, inplace=True)\nTEST.reset_index(drop=True, inplace=True)\n\nBATCH = 15\nSHAPE_RESIZE = 256\nCUT = 10\nCOUNT_MODEL = 4\nTRAIN[\"dir\"] = TRAIN_DIR\nTEST[\"dir\"] = TEST_DIR#Todo\n# TRAIN22[\"dir\"] = TRAIN_DIR\n# PACIENT_TEST = TRAIN[\"Patient\"].unique().tolist()\n# count = int(len(PACIENT_TEST)*80/100)\n# PACIENT_train = random.sample(set(PACIENT_TEST), count)\n# PACIENT_val = np.setdiff1d(PACIENT_TEST,PACIENT_train).tolist()\n# TRAIN = TRAIN22.loc[~TRAIN22['Patient'].isin(PACIENT_val)]\n# TEST = TRAIN22.loc[TRAIN22['Patient'].isin(PACIENT_val)]\n# TRAIN.drop_duplicates(subset=[\"Patient\",\"Weeks\"],keep=False,inplace=True)\n# TEST.drop_duplicates(subset=[\"Patient\",\"Weeks\"],keep=False,inplace=True)\n# def calculate_2(row):\n#     week = TEST[TEST.Patient == row[\"Patient\"]][\"Weeks\"].unique().tolist()\n    \n#     w0 = week[0]\n    \n#     if row['Weeks'] == w0:\n       \n#         return row\n# TEST23 = TEST.apply(calculate_2, axis=1)\n# TEST23.dropna(inplace=True)\n# print(TRAIN.shape,TEST.shape)\n# TRAIN = TRAIN.append(TEST23, ignore_index = True).copy()\n# print(TRAIN.shape,TEST.shape)\n# print(TRAIN.shape)\n# TRAIN.drop_duplicates(subset=[\"Patient\",\"Weeks\"],keep=False,inplace=True)\n# TRAIN.reset_index(drop=True, inplace=True)   \n# TEST.reset_index(drop=True, inplace=True)   \n# print(TRAIN.shape)\n# TEST = TRAIN22.loc[TRAIN22['Patient'].isin(PACIENT_val)]\n# TEST.drop_duplicates(subset=[\"Patient\",\"Weeks\"],keep=False,inplace=True)\n# print(TEST.shape)\nTRAIN = TRAIN.append(TEST, ignore_index = True).copy()\nTRAIN.drop_duplicates(subset=[\"Patient\",\"Weeks\"],keep=False,inplace=True)\n# print(TRAIN22.shape)\n# print(TEST.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"PACIENT = TEST[\"Patient\"].unique()\nTRAIN_NEW = pd.DataFrame()\nfor ID in tqdm(PACIENT):\n    data = TEST[TEST.Patient == ID].copy()\n    data.reset_index(inplace=True,drop=True)\n    data = data[:1]\n    r = range(-12,134)\n    count_week = len(r)\n    data = data.loc[data.index.repeat(count_week)].reset_index(drop=True)\n    week_predict = [i for i in r]\n    data[\"week_predict\"] = week_predict\n    TRAIN_NEW = pd.concat([TRAIN_NEW, data], ignore_index=True)\nTEST = TRAIN_NEW","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def counsruct(dataframe,test = False):\n    PACIENT = dataframe[\"Patient\"].unique()\n    TRAIN_NEW = pd.DataFrame()\n    for ID in tqdm(PACIENT):\n        data = dataframe[dataframe.Patient == ID].copy()\n        data.reset_index(inplace=True,drop=True)\n        week_start, FVC_start, Percent_kt,d = data.loc[0,[\"Weeks\",\"FVC\",\"Percent\",\"dir\"]].values\n        DIR=d\n        DIR_DCM = DIR+ID+\"/\"\n        d = os.listdir(DIR_DCM)\n        d = sorted(os.listdir(DIR_DCM), key=lambda v:int(v.split('.')[0]))\n        count_slice = len(d)\n        d = {i+1:dcm for i,dcm in enumerate(d)}\n        center = len(d)//2\n#         arr_slice = [center-5,center-4,center-3,center-2,center-1,center,center+1,center+2,center+3,center+4,center+5]\n#         arr_slice = [center-3,center-2,center-1,center,center+1,center+2,center+3]\n        c=center-(center*40//100)\n        arr_slice = [c]\n        count_repeat = len(arr_slice)\n        \n        d = np.array([[d[i],int(i)] for j in range(data.shape[0]) for i in arr_slice])\n        d = pd.DataFrame(d,columns=[\"dcm\",\"num_slice\"])\n        d[\"num_slice\"] = d[\"num_slice\"].astype(\"int\")\n        data = data.loc[data.index.repeat(count_repeat)].reset_index(drop=True)\n        data[[\"dcm\",\"num_slice\"]] = d[[\"dcm\",\"num_slice\"]]\n        data[\"count_slice\"] = count_slice\n        data[\"week_kt\"] = week_start\n        data[\"FVC_kt\"] = FVC_start\n        data[\"Percent_kt\"] = Percent_kt\n        TRAIN_NEW = pd.concat([TRAIN_NEW, data], ignore_index=True)\n    return TRAIN_NEW\nTRAIN_C = counsruct(TRAIN)\nTEST_C = counsruct(TEST,True)\n# TEST_C[\"Weeks\"] = TEST_C[\"week_predict\"]\nTEST_C[\"Count_weks\"] = TEST_C[\"week_predict\"]-TEST_C[\"week_kt\"]\n# TEST_C[\"Count_weks\"] = TEST_C[\"Weeks\"]-TEST_C[\"week_kt\"]\nTRAIN_C[\"Count_weks\"] = TRAIN_C[\"Weeks\"]-TRAIN_C[\"week_kt\"]\n# TRAIN_C['week_min'] = TRAIN_C['Patient'].map(TRAIN_C.groupby(['Patient']).Weeks.min())\n# TRAIN_C['week_max'] = TRAIN_C['Patient'].map(TRAIN_C.groupby(['Patient']).Weeks.max())\n# TEST_C['week_min'] = TEST_C['Patient'].map(TEST_C.groupby(['Patient']).week_predict.min())\n# TEST_C['week_max'] = TEST_C['Patient'].map(TEST_C.groupby(['Patient']).week_predict.max())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TRAIN_C.loc[TRAIN_C.Percent_kt>94,[\"Percent_kt\"]] = 15.2567\n# TEST_C.loc[TEST_C.Percent_kt>94,[\"Percent_kt\"]] = 15.2567","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"r = 1\ne=70\ndef custom_data(dataframe):\n    \n    dataframe[\"FVC_n\"] = (dataframe[\"FVC_kt\"]*100/dataframe[\"Percent_kt\"])\n    for i in range(r,e):\n        name =\"FVC_mean\"+str(i)\n        name2 = \"FVC_custom\"+str(i)\n        dataframe[name] = (dataframe[\"FVC_n\"]-dataframe[\"FVC_kt\"])/(30*i)\n        dataframe[name2] = (dataframe[\"FVC_kt\"]-(dataframe[\"Count_weks\"])*dataframe[name])-((dataframe[\"Count_weks\"]+90))#-80\n    return dataframe\nTRAIN_C=custom_data(TRAIN_C)\nTEST_C=custom_data(TEST_C)\nname = [\"FVC_custom\"+str(i) for i in range(r,e)]\n# TEST_C[\"FVC_PRE\"] = TEST_C[name[5:-60]].mean(axis=1)\n# TRAIN_C[\"FVC_PRE\"] = TRAIN_C[name[5:-60]].mean(axis=1)\nTEST_C[\"FVC_PRE\"] = TEST_C[name[10:]].mean(axis=1)\nTRAIN_C[\"FVC_PRE\"] = TRAIN_C[name[10:]].mean(axis=1)\nTEST_C[\"FVC_PRE2\"] = TEST_C[\"FVC_PRE\"]**2\nTRAIN_C[\"FVC_PRE2\"] = TRAIN_C[\"FVC_PRE\"]**2\nTEST_C[\"FVC_n2\"] = TEST_C[\"FVC_n\"]**2\nTRAIN_C[\"FVC_n2\"] = TRAIN_C[\"FVC_n\"]**2\nTEST_C[\"r1\"] = TEST_C[\"FVC_n\"] - TEST_C[\"FVC_PRE\"]\nTEST_C[\"r1mean\"] = TEST_C[[\"FVC_n\",\"FVC_PRE\"]].mean(axis=1)\nTEST_C[\"r2\"] = TEST_C[[\"FVC_n\",\"FVC_PRE\"]].std(axis=1)\nTRAIN_C[\"r1\"] = TRAIN_C[\"FVC_n\"] - TRAIN_C[\"FVC_PRE\"]\nTRAIN_C[\"r1mean\"] = TRAIN_C[[\"FVC_n\",\"FVC_PRE\"]].mean(axis=1)\nTRAIN_C[\"r2\"] = TRAIN_C[[\"FVC_n\",\"FVC_PRE\"]].std(axis=1)\nTRAIN_C[[\"Female\",\"Male\",\"Currently smokes\",\"Ex-smoker\",\"Never smoked\"]]=0\nTEST_C[[\"Female\",\"Male\",\"Currently smokes\",\"Ex-smoker\",\"Never smoked\"]]=0\ndef calculate_height(row):\n    if row['Sex'] == 'Male':\n        return row['FVC_kt'] / (27.63 - 0.112 * row['Age'])\n    else:\n        return row['FVC_kt'] / (21.78 - 0.101 * row['Age'])\ndef calculate_all(row):\n    if row['Sex'] == 'Male':\n        row['Male'] = 1\n       \n    else:\n        row['Female']=1\n        \n    if row['SmokingStatus'] == \"Currently smokes\":\n        row['Currently smokes']= 1\n    if row['SmokingStatus'] == \"Ex-smoker\":\n        row['Ex-smoker']= 1\n    if row['SmokingStatus'] == \"Never smoked\":\n        row['Never smoked']= 1\n    return row\n        \nTRAIN_C['Height'] = TRAIN_C.apply(calculate_height, axis=1)\nTEST_C['Height'] = TEST_C.apply(calculate_height, axis=1)\nTRAIN_C = TRAIN_C.apply(calculate_all, axis=1)\nTEST_C = TEST_C.apply(calculate_all, axis=1)\nTRAIN_C = TRAIN_C.drop(columns=['Sex', 'SmokingStatus'])\nTEST_C = TEST_C.drop(columns=['Sex', 'SmokingStatus'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def construct_binned(dataframe_train,dataframe_test,column,count,combi=True):\n    col = dataframe_train[column].unique().tolist()\n    col.extend(dataframe_test[column].unique().tolist())\n    all_col = np.array(col)\n    bins = np.linspace(all_col.min(),all_col.max(),count)\n    col_data_train =  dataframe_train[column].values.reshape(-1,1)\n    col_data_test =  dataframe_test[column].values.reshape(-1,1)\n    witch_bin = np.digitize(col_data_train,bins)\n    witch_bin2 = np.digitize(col_data_test,bins)\n    encoder = OneHotEncoder(sparse=False)\n    encoder.fit(witch_bin)\n    col_binned = encoder.transform(witch_bin)\n    col_binned2 = encoder.transform(witch_bin2)\n    col_binned_combi = np.hstack([col_data_train,col_binned])\n    col_binned2_combi = np.hstack([col_data_test,col_binned2])\n    set1 = col_binned_combi\n    set2 = col_binned2_combi\n    if not combi:\n        set1 = col_binned\n        set2 = col_binned2\n    name_col_binned = [column+\"_binned\"+str(i) for i in range(set1.shape[1])]\n    dataframe_train[name_col_binned] = set1\n    dataframe_test[name_col_binned] = set2\n    \n    return dataframe_train, dataframe_test,name_col_binned\nTRAIN_C,TEST_C,binned_age = construct_binned(TRAIN_C,TEST_C,\"Age\",60,True)#50#70\nTRAIN_C,TEST_C,binned_height = construct_binned(TRAIN_C,TEST_C,\"Height\",5,True)\nTRAIN_C,TEST_C,binned_Percent_kt = construct_binned(TRAIN_C,TEST_C,\"Percent_kt\",60,True)\nTRAIN_C,TEST_C,binned_FVC_kt = construct_binned(TRAIN_C,TEST_C,\"FVC_kt\",60,True)#50#100\nTRAIN_C,TEST_C,binned_Count_week_kt = construct_binned(TRAIN_C,TEST_C,\"week_kt\",15,True)\nTRAIN_C,TEST_C,binned_FVC_n = construct_binned(TRAIN_C,TEST_C,\"FVC_n\",60,True)\nTRAIN_C,TEST_C,binned_count_slice = construct_binned(TRAIN_C,TEST_C,\"count_slice\",15,True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Height = TRAIN_C[\"Height\"].unique().tolist()\nHeight.extend(TEST_C[\"Height\"].unique().tolist())\nall_Height = np.unique(Height)\nbins = np.linspace(all_Height.min(),all_Height.max(),5)\nwitch_bin = np.digitize(TRAIN_C.Height,bins)\nwitch_bin2 = np.digitize(TEST_C.Height,bins)\nencoder = OneHotEncoder(sparse=False)\nencoder.fit(witch_bin.reshape(-1, 1))\nHeight_binned = encoder.transform(witch_bin.reshape(-1, 1))\nHeight_binned2 = encoder.transform(witch_bin2.reshape(-1, 1))\nname_height_binned = [\"Height_binned\"+str(i) for i in range(Height_binned.shape[1])]\nTRAIN_C[name_height_binned] = Height_binned\nTEST_C[name_height_binned] = Height_binned2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"FVC_kt_train = TRAIN_C[\"FVC_kt\"].unique().tolist()\nFVC_kt_train.extend(TEST_C[\"FVC_kt\"].unique().tolist())\nall_FVC_kt = np.unique(FVC_kt_train)\nbins = np.linspace(all_FVC_kt.min(),all_FVC_kt.max(),11)\nwitch_bin = np.digitize(TRAIN_C.FVC_kt,bins)\nwitch_bin2 = np.digitize(TEST_C.FVC_kt,bins)\nencoder = OneHotEncoder(sparse=False)\nencoder.fit(witch_bin.reshape(-1, 1))\nFVC_binned = encoder.transform(witch_bin.reshape(-1, 1))\nFVC_binned2 = encoder.transform(witch_bin2.reshape(-1, 1))\n# print(FVC_binned2.shape)\nname_bin_fvckt = [\"FVC_KT_bin\"+str(i) for i in range(FVC_binned.shape[1])]\nTRAIN_C[name_bin_fvckt] = FVC_binned\nTEST_C[name_bin_fvckt] = FVC_binned2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"FVC_PRE_train = TRAIN_C[\"FVC_PRE\"].unique().tolist()\nFVC_PRE_train.extend(TEST_C[\"FVC_PRE\"].unique().tolist())\nall_FVC_PRE_kt = np.unique(FVC_PRE_train)\nbins2 = np.linspace(all_FVC_PRE_kt.min(),all_FVC_PRE_kt.max(),5)\nwitch_bin2 = np.digitize(TRAIN_C.FVC_PRE,bins2)\nwitch_bin22 = np.digitize(TEST_C.FVC_PRE,bins2)\n# print(witch_bin2)\nencoder = OneHotEncoder(sparse=False)\nencoder.fit(witch_bin.reshape(-1, 1))\nFVC_PRE_binned = encoder.transform(witch_bin2.reshape(-1, 1))\nFVC_PRE_binned2 = encoder.transform(witch_bin22.reshape(-1, 1))\nname_bin_pre = [\"FVC_PRE_bin\"+str(i) for i in range(FVC_binned.shape[1])]\nTRAIN_C[name_bin_pre] = FVC_PRE_binned\nTEST_C[name_bin_pre] = FVC_PRE_binned2\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def calculate_FVC(row):\n    if row['Weeks'] == row['week_kt']:\n        row['FVC_PRE'] = row['FVC']\n    return row\nTRAIN_C = TRAIN_C.apply(calculate_FVC, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Patient = LabelEncoder()\n# Sex = LabelEncoder()\n# SmokingStatus = LabelEncoder()\ntrain_pac = TRAIN_C[\"Patient\"].unique().tolist()\ntrain_pac.extend(TEST_C[\"Patient\"].unique().tolist())\nall_pacient = np.unique(train_pac)\nPatient.fit(all_pacient)\ndef LE(dataframe, val=False,dense=False):\n    dataframe[\"patiet_id\"] = Patient.transform(dataframe[\"Patient\"])\n    col = [\"Patient\",\"dcm\"]\n    if not val:\n        col.extend([\"FVC\",\"Weeks\"])\n    if val:\n        col.extend([\"week_predict\"])\n#         col.extend([\"FVC\",\"Weeks\"])\n    col.extend(name_bin_pre)\n#     col.extend(name_bin_fvckt)\n    col.extend(name_height_binned)\n    col.extend(binned_age)\n    col.extend(binned_FVC_kt)\n    col.extend(binned_Percent_kt)\n    col.extend(binned_FVC_n)\n#     col.extend(binned_age)\n    col.extend([\"FVC_PRE\",\"FVC_PRE2\",\"count_slice\",\"week_kt\",\"Count_weks\",'Height',\"Currently smokes\",\"Ex-smoker\",\"Never smoked\",\"Female\",\"Male\",\"r1\",\"r2\",\"r1mean\"])\n#     print(col)\n    dataframe = dataframe[col]\n    dataframe['dcm'] = dataframe.agg('{0[Patient]}/{0[dcm]}'.format, axis=1)\n    return dataframe\nTRAIN2 = LE(TRAIN_C.copy())\nTEST2 = LE(TEST_C.copy(),True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = 0\nvalidation = 0\ndef Fold(dataframe):\n    train = []\n    val = []\n    PACIENT = dataframe[\"Patient\"].unique()\n \n    for ID in PACIENT:\n        \n        \n        d = dataframe[dataframe.Patient==ID]\n        d.reset_index(inplace=True,drop=True)\n        week = d[\"Weeks\"].unique().tolist()\n        if len(week)==1:\n            week_train = [week[0]]\n            week_val = [week[0]]\n            \n        else:\n            week_val = week[:]\n            week_train =week[:]\n#         print(week_val)\n        train_index = dataframe[(dataframe.Patient==ID) & (dataframe.Weeks.isin(week_train))].index.tolist()\n        val_index = dataframe[(dataframe.Patient==ID) & (dataframe.Weeks.isin(week_val))].index.tolist()\n        train.extend(train_index)\n        val.extend(val_index)\n    return train,val\ntrain_index, val_index = Fold(TRAIN2)\n# train_index2, val_index2 = Fold(TEST2)\ntrain = TRAIN2.loc[train_index]\n# validation = TEST2.loc[val_index2]\n# train.reset_index(drop=True, inplace=True)    \n# validation.reset_index(drop=True, inplace=True)\n# train = TRAIN2.loc[train_index]\nvalidation = TRAIN2.loc[val_index]\ntrain.reset_index(drop=True, inplace=True)    \nvalidation.reset_index(drop=True, inplace=True)\n# train = train.sample(frac=1, random_state=123).reset_index(drop=True)\n# validation = validation.sample(frac=1, random_state=123).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" def laplace_log_likelihood(actual_fvc, predicted_fvc, confidence, return_values = False):\n    \"\"\"\n    Calculates the modified Laplace Log Likelihood score for this competition.\n    \"\"\"\n    sd_clipped = np.maximum(confidence, 70)\n    delta = np.minimum(np.abs(actual_fvc - predicted_fvc), 1000)\n    metric = - np.sqrt(2) * delta / sd_clipped - np.log(np.sqrt(2) * sd_clipped)\n\n    if return_values:\n        return metric\n    else:\n        return np.mean(metric)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# validation = True\ntrain_split = train.copy()\ntest_split = validation.copy()\nX_train = train_split[train_split.columns.tolist()[3:]].copy()\nX_val= test_split[test_split.columns.tolist()[3:]].copy()\nX_val2= test_split[test_split.columns.tolist()[3:]].copy()\nX_end = TRAIN2[train_split.columns.tolist()[3:]].copy()\nX_end2 = TRAIN2[train_split.columns.tolist()[3:]].copy()\nY_end = TRAIN2[\"FVC\"].copy()\nY_train = train_split[\"FVC\"].copy()\nY_train2 = Y_train- train_split[\"FVC_PRE\"]\nY_train=Y_train2.abs()\nY_val = test_split[\"FVC\"].copy()\nY_val2 = Y_val- test_split[\"FVC_PRE\"]\nY_val2 = Y_val2.abs()\n# X_val2[\"Y_val\"] = Y_val2\n\n\nX_test = TEST2[TEST2.columns.tolist()[2:]]\nX_train_kneigboards = train_split[train_split.columns.tolist()[3:]].copy()\nX_val_kneigboards= test_split[test_split.columns.tolist()[3:]].copy()\nX_test_kneigboards= TEST2[TEST2.columns.tolist()[2:]]\n\n# std_scaller = StandardScaler()\n# name = [\"count_slice\",\"FVC_n\",\"Percent_kt\",\"Count_weks\",\"r1\",\"r2\"]\n# std_scaller.fit(X_train[name])\n# X_train[name] = std_scaller.transform(X_train[name])\n# X_val[name] =  std_scaller.transform(X_val[name])\n# val_x_std_scalled = std_scaller.transform(val_x)\n\nalpha = 0.9\ntree1 = GradientBoostingRegressor(loss='quantile', alpha=alpha,\n                                n_estimators=250, max_depth=5,\n                                learning_rate=.1, min_samples_leaf=49,\n                                min_samples_split=49)\ntree1.fit(X_train_kneigboards,Y_train)\ny_upper = tree1.predict(X_val_kneigboards)\n\n\ntree1.set_params(alpha=0.1)\ntree1.fit(X_train_kneigboards, Y_train)\ny_lower = tree1.predict(X_val_kneigboards)\n\ntree1.set_params(loss='ls')\ntree1.fit(X_train_kneigboards, Y_train)\ny_pred = tree1.predict(X_val_kneigboards)\ntree3 = KNeighborsRegressor(n_neighbors=252)\ntree3.fit(X_train_kneigboards,Y_train)\npred_k = tree3.predict(X_val_kneigboards)\n\ntree2 =  RandomForestRegressor(n_estimators=30,min_samples_leaf=2, min_samples_split=2)\ntree2.fit(X_train,Y_train)\npred_r = tree2.predict(X_val)\n\ntree4 = LinearRegression()\ntree4.fit(X_train_kneigboards[X_train_kneigboards.columns.tolist()[:-11]],Y_train)\npred_lr = tree4.predict(X_val_kneigboards[X_val_kneigboards.columns.tolist()[:-11]])\ntree5 = Ridge(alpha=0.03)\ntree5.fit(X_train_kneigboards[X_train_kneigboards.columns.tolist()[:-11]],Y_train)\npred_ridge = tree5.predict(X_val_kneigboards[X_val_kneigboards.columns.tolist()[:-11]])\n\nlf = BayesianRidge()\nlf.fit(X_train_kneigboards[X_train_kneigboards.columns.tolist()[:]],Y_train)\npred_baes = lf.predict(X_val_kneigboards[X_val_kneigboards.columns.tolist()[:]])\n\nprint(X_val_kneigboards.columns.tolist()[8:-10])\nprint(mean_squared_error(Y_val2,y_upper, squared=False))\nprint(mean_squared_error(Y_val2,y_lower, squared=False))\nprint(mean_squared_error(Y_val2,y_pred, squared=False))\nprint(\"custom\",mean_squared_error(Y_val,X_val2[\"FVC_PRE\"], squared=False))\n\n\nprint(\"kneugboard\",mean_squared_error(Y_val2,pred_k, squared=False))\nprint(\"randonf\",mean_squared_error(Y_val2,pred_r, squared=False))\nprint(\"linear\",mean_squared_error(Y_val2,pred_lr, squared=False))\nprint(\"ridge\",mean_squared_error(Y_val2,pred_ridge, squared=False))\nprint(\"baes\",mean_squared_error(Y_val2,pred_baes, squared=False))\nX_val2[\"y_upper\"] = y_upper\nX_val2[\"y_lower\"] = y_lower\nX_val2[\"y_pred\"] = y_pred\nX_val2[\"pred_k\"] = pred_k\nX_val2[\"pred_r\"] = pred_r\nX_val2[\"pred_lr\"] = pred_lr\nX_val2[\"pred_ridge\"] = pred_ridge\nX_val2[\"baes\"] = pred_baes\nname = [\"baes\",\"pred_r\"]\nX_val2[\"Confidence\"] = Y_val- X_val2[\"FVC_PRE\"]\nX_val2[\"Confidence\"] = X_val2[\"Confidence\"].abs()\n\n\n\n# X_val2[\"Confidence\"] = 200\n# display(X_val2[\"Confidence\"])\n# X_val2[\"Confidence\"] = (X_val2[[\"pred_k\",\"FVC_PRE\"]].max(axis=1) - X_val2[[\"pred_k\",\"FVC_PRE\"]].min(axis=1))\ndelta = 60\nX_val2[\"end\"] = X_val2[name].mean(axis=1)\nprint(\"----\",X_val2[\"end\"].std())\nX_val2[\"end\"]+=delta\nprint(\"mean\",mean_squared_error(Y_val,X_val2[\"end\"], squared=False))\ntest_split[\"end\"]= X_val2[\"FVC_PRE\"]\nprint(laplace_log_likelihood(Y_val,X_val2.pred_lr,X_val2[\"Confidence\"]))#140\nprint(laplace_log_likelihood(Y_val,X_val2.FVC_PRE,X_val2.end))#140\nprint(laplace_log_likelihood(Y_val,X_val2.end,X_val2[\"Confidence\"]))#140\n\ntree1.set_params(loss='ls')\ny_pred2 = tree1.predict(X_test_kneigboards)\npred_k2 = tree3.predict(X_test_kneigboards)\npred_r2 = tree2.predict(X_test)\npred_lr2 = tree4.predict(X_test_kneigboards[X_test_kneigboards.columns.tolist()[:-11]])\npred_ridge2 = tree5.predict(X_test_kneigboards[X_test_kneigboards.columns.tolist()[:-11]])\npred_baes2 = lf.predict(X_test_kneigboards[X_test_kneigboards.columns.tolist()[:]])\nTEST2[\"y_pred\"] = y_pred2\nTEST2[\"pred_k\"] = pred_k2\nTEST2[\"pred_r\"] = pred_r2\nTEST2[\"pred_lr\"] = pred_lr2\nTEST2[\"pred_ridge\"] = pred_ridge2\nTEST2[\"baes\"] = pred_baes2\nTEST2[\"end\"] = TEST2[name].mean(axis=1)\nTEST2[\"end\"]+=delta\n# TEST2[\"Confidence\"] =  170+(TEST2[\"Count_weks\"])\nTEST2['Patient_Week'] = TEST2.agg('{0[Patient]}_{0[week_predict]}'.format, axis=1)\nSUBMISSINO1_pred2 = TEST2[['Patient_Week','FVC_PRE','end',]]\nSUBMISSINO1_pred2.columns = [\"Patient_Week\",\"FVC\",\"Confidence\"]\nSUBMISSINO1_pred2.to_csv(\"submission.csv\", index=False)\nSUBMISSINO1_pred2\n# # display(SUBMISSINO1_pred2)\n# # # plt.figure(figsize=(15,8))\n# # # sns.barplot(y=X_val_kneigboards.columns, x=tree4.feature_importances_, )\n# # # plt.figure(figsize=(15,8))\n# # # sns.barplot(y=X_val.columns, x=tree2.feature_importances_, )\n# train_patients = test_split.loc[test_split.Percent_kt>90]['Patient'].unique()\n# fig, ax = plt.subplots(40, 1, figsize=(20, 70))\n\n# for i in range(0,40):\n#     patient_log = test_split[test_split['Patient'] == train_patients[i]]\n#     ax[i].set_title(patient_log[[\"Percent_kt\",\"FVC_n\"]].values[0])\n# #     print(patient_log[\"Percent_kt\"])\n#     ax[i].plot(patient_log['Weeks'], patient_log['FVC'], label='truth')\n#     ax[i].plot(patient_log['Weeks'], patient_log['end'], label='prediction')\n#     ax[i].legend()\n# #     -7.060406176501061","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ['FVC_KT_bin7', 'FVC_KT_bin8', 'Height_binned0', 'Height_binned1', 'Height_binned2', 'Height_binned3', 'Age', 'count_slice', 'week_kt', 'Count_weks', 'Height', 'FVC_kt', 'Currently smokes', 'Ex-smoker']\n# 112.12877222068204\n# 158.5906741062012\n# 60.857885911243045\n# custom 218.45109162754719\n# kneugboard 137.54239529961768\n# randonf 135.94007416439462\n# linear 151.7000903643887\n# ridge 151.70009960419222\n# baes 146.3852677292098\n# mean 2674.0087667436064\n# -18.391432439384857\n# -6.5973666610214705\n# -18.390146002170916","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}