{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n    ##for filename in filenames:\n        #print(os.path.join(dirname, filename))\n#from matplotlib.image import imread  \n\nfrom cv2 import imread\nimport matplotlib.pyplot as plt\n\ntrain_path = '../input/state-farm-distracted-driver-detection/imgs/train/'\ntest_path = '..//input/state-farm-distracted-driver-detection//imgs//test//'\n#test_path_2 = '..//input/state-farm-distracted-driver-detection//imgs//'\n#print(os.listdir(train_path))\n\ntrain_data=[]\ntest_data=[]\nclasses = [folder for folder in os.listdir(train_path)]\n\n# explanation for each of the classes\nclass_dict = {\n    'c0': 'hands on the wheel',\n    'c1': 'mobile in right hand',\n    'c2': 'talking on the phone with right hand',\n    'c3': \"mobile in left hand\",\n    'c4': 'talking on the phone with left hand',\n    'c5': 'touching at the dash',\n    'c6': 'drinking',\n    'c7': 'reaching behind',\n    'c8': 'touching the head',\n    'c9': 'looking to the side'\n}\n\n\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## checking the dimensions of images in test data set so that we can resize the images accordingly.\nimport random\ndim1 =[]\ndim2=[]\nimage_list = os.listdir(test_path)\nimage_list = random.sample(image_list,500)\nfor image in image_list:\n    path = os.path.join(test_path,image)\n    #print(path)\n    d1,d2,channels = imread(path).shape\n    dim1.append(d1)\n    dim2.append(d2)\n    \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(np.mean(dim1))\nprint(np.mean(dim2))\n\n# we can see that the dimension of each image is (480,640,3) from a sample of 500 randomly selected test images.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2\nfor item in classes:\n    path = os.path.join(train_path,item)\n    #count=0\n    for image in os.listdir(path):\n        image_array = imread(os.path.join(path,image))#,cv2.IMREAD_COLOR)\n        count+=1\n        #print(image_array.shape)\n        print(item)\n        print(class_dict[item])\n        plt.imshow(image_array,cmap='gray')\n        plt.show()\n        print('\\n')\n        break\n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow .keras.preprocessing.image import ImageDataGenerator","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#         train_datagen = ImageDataGenerator(\n#         rescale=1./255,\n#         #rotation_range=30,\n#         shear_range=0.1,\n#         width_shift_range = 0.2,\n#         height_shift_range = 0.2,\n#         fill_mode='nearest',\n#         zoom_range=0.2,\n#         #brightness_range= [0.1,1]\n#         #validation_split=0.2\n#         )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#         train_generator = train_datagen.flow_from_directory(\n#         train_path,\n#         target_size=(224, 224),\n#         batch_size=32,\n#         class_mode='categorical',\n#         #subset='training'\n#         )\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_datagen = ImageDataGenerator(\n        rescale=1./255,\n        rotation_range=20,\n        #featurewise_std_normalization=True,\n        #featurewise_center = True,\n        shear_range=0.1,\n        width_shift_range = 0.1,\n        height_shift_range = 0.1,\n        fill_mode='nearest',\n        #zoom_range=0.2,\n        #horizontal_flip=True,\n        #vertical_flip=True,\n        #brightness_range= [0.1,1],\n        validation_split=0.2)\n\ntrain_generator = train_datagen.flow_from_directory(\n        train_path,\n        target_size=(224, 224),\n        batch_size=32,\n        class_mode='categorical',\n        subset='training')\n\ntest_generator = train_datagen.flow_from_directory(\n        train_path,\n        target_size=(224, 224),\n        batch_size=32,\n        class_mode='categorical',\n        subset='validation',\n        shuffle=True)\n\n","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## VERIFYING THE IMAGE AND THE LABELS\n\n# batch =  train_generator.next()\n# #print(batch.shape)\n# for i ,j in zip(batch[0],batch[1]):\n#     plt.imshow(i)\n#     plt.show()\n#     print(class_dict['c'+str(np.argmax(j))])\n#     print('\\n')\n    \n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#next(train_generator)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train_generator.next()[0].shape # x is scaled we can see\n#train_generator.next()[1]# y is one hot encoded as well","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Lets Create Model Now","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Flatten, Dropout\nimport tensorflow as tf\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Sequential()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## convolution layer 1 and max pool 1\n\n\nmodel.add(Conv2D(64,(2,2),input_shape = (224,224,3), activation='relu',padding='valid'))\nmodel.add(MaxPool2D(pool_size=(3,3)))\n\n## convolution layer 2 and max pool 2\n\nmodel.add(Conv2D(128,(2,2), activation='relu',padding='valid'))\nmodel.add(MaxPool2D(pool_size=(2,2)))\n\nmodel.add(Conv2D(256,(2,2), activation='relu',padding='valid'))\nmodel.add(MaxPool2D(pool_size=(2,2)))\n## flattening the image before going to hidden layer\nmodel.add(Flatten())\n\n\n\n## Dropout layer 1 and Hidden layer 1\n\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dropout(0.2))\n\n## Hidden layer 2\n\nmodel.add(Dense(256,activation ='relu'))\nmodel.add(Dropout(0.2))\n\n\n## output layer\n\nmodel.add(Dense(10,activation='softmax'))# softmax because of multiclass classification\n\n\n#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Compiling the model now\n\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Early_stop = EarlyStopping(monitor='val_loss',patience=2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#model.fit_generator(train_generator, epochs=10,   callbacks=[Early_stop],use_multiprocessing=True)\n   \n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit_generator(train_generator, epochs=15,  validation_data=test_generator, callbacks=[Early_stop],use_multiprocessing=True)\n   \n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.metrics_names","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = model.predict_generator(test_generator, use_multiprocessing=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## checking the predictions on validation generator and matching with the image .\nimport cv2\n# for i in range(50):\nbatch = test_generator.next()\nfor i,j in zip(batch[0],batch[1]):\n    plt.imshow(i)\n    \n    plt.show()\n    #plt.show()\n    print('Actual: ', class_dict['c'+str(np.argmax(j))])\n    #print('\\n')\n    #print('\\n')\n    #break\n\n    #image =cv2.resize(image,(224,224))\n    image=i.reshape(1,224,224,3)\n    print('predicted: ', class_dict['c'+ str(np.argmax(model.predict(image)))])\n    print('\\n')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.evaluate_generator(test_generator,use_multiprocessing=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## we can see that we are almost getting 97 percent accuracy on the validation data as well.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import cv2\n# for image in os.listdir(test_path)[4:]:\n#     path = os.path.join(test_path,image)\n#     print(path)\n#     image = cv2.imread(path)\n#     plt.imshow(image)\n#     break\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ## checking the output on test folder manually though no label is there in the images in test folder.\n# import cv2\n# for image in os.listdir(test_path)[10:40]:\n#     path = os.path.join(test_path,image)\n#     #print(path)\n#     image = cv2.imread(path)\n#     plt.imshow(image)\n#     plt.show()\n#     #print(image.shape)\n#     #print('\\n')\n#     #break\n\n#     image =cv2.resize(image,(224,224))\n#     image=image.reshape(1,224,224,3)\n#     print('predicted : ' , class_dict['c'+ str(np.argmax(model.predict(image)))])\n#     print('\\n')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}