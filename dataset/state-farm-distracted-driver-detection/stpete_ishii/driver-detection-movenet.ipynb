{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Driver Detection MoveNet","metadata":{"papermill":{"duration":0.025209,"end_time":"2021-11-01T17:27:00.189956","exception":false,"start_time":"2021-11-01T17:27:00.164747","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"https://tfhub.dev/s?q=movenet","metadata":{"id":"KqtQzBCpIJ7Y","papermill":{"duration":0.020501,"end_time":"2021-11-01T17:27:00.232554","exception":false,"start_time":"2021-11-01T17:27:00.212053","status":"completed"},"tags":[]}},{"cell_type":"code","source":"!pip install -q imageio\n!pip install -q opencv-python\n!pip install -q git+https://github.com/tensorflow/docs","metadata":{"id":"TtcwSIcgbIVN","papermill":{"duration":38.253106,"end_time":"2021-11-01T17:27:38.506529","exception":false,"start_time":"2021-11-01T17:27:00.253423","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-02T02:25:34.825052Z","iopub.execute_input":"2021-11-02T02:25:34.825894Z","iopub.status.idle":"2021-11-02T02:26:12.282109Z","shell.execute_reply.started":"2021-11-02T02:25:34.825798Z","shell.execute_reply":"2021-11-02T02:26:12.28127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport cv2\nimport os\nfrom tqdm import tqdm\nimport random\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.collections import LineCollection\nimport matplotlib.patches as patches\nimport imageio\nfrom IPython.display import HTML, display\n\nimport tensorflow as tf\nfrom tensorflow.keras.utils import to_categorical\nimport tensorflow_hub as hub\nfrom tensorflow_docs.vis import embed\nimport keras\nfrom keras.models import Sequential\n\nfrom sklearn.metrics import classification_report, log_loss, accuracy_score\nfrom sklearn.model_selection import train_test_split","metadata":{"id":"9BLeJv-pCCld","papermill":{"duration":7.464056,"end_time":"2021-11-01T17:27:45.992851","exception":false,"start_time":"2021-11-01T17:27:38.528795","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-02T02:26:12.284266Z","iopub.execute_input":"2021-11-02T02:26:12.284647Z","iopub.status.idle":"2021-11-02T02:26:19.602572Z","shell.execute_reply.started":"2021-11-02T02:26:12.284601Z","shell.execute_reply":"2021-11-02T02:26:19.60194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"KEYPOINT_DICT = {\n    'nose': 0,'left_eye': 1,'right_eye': 2,'left_ear': 3,'right_ear': 4,'left_shoulder': 5,\n    'right_shoulder': 6,'left_elbow': 7,'right_elbow': 8,'left_wrist': 9,'right_wrist': 10,\n    'left_hip': 11,'right_hip': 12,'left_knee': 13,'right_knee': 14,'left_ankle': 15,\n    'right_ankle': 16\n}\n\nKEYPOINT_EDGE_INDS_TO_COLOR = {\n    (0, 1): 'm',(0, 2): 'c',(1, 3): 'm',(2, 4): 'c',(0, 5): 'm',(0, 6): 'c',\n    (5, 7): 'm',(7, 9): 'm',(6, 8): 'c',(8, 10): 'c',(5, 6): 'y',(5, 11): 'm',\n    (6, 12): 'c',(11, 12): 'y',(11, 13): 'm',(13, 15): 'm',(12, 14): 'c',(14, 16): 'c'\n}","metadata":{"papermill":{"duration":0.034516,"end_time":"2021-11-01T17:27:46.049576","exception":false,"start_time":"2021-11-01T17:27:46.01506","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-02T02:26:19.604032Z","iopub.execute_input":"2021-11-02T02:26:19.604522Z","iopub.status.idle":"2021-11-02T02:26:19.615445Z","shell.execute_reply.started":"2021-11-02T02:26:19.604478Z","shell.execute_reply":"2021-11-02T02:26:19.614651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def _keypoints_and_edges_for_display(keypoints_with_scores,\n         height,width,keypoint_threshold=0.11):\n\n  keypoints_all = []\n  keypoint_edges_all = []\n  edge_colors = []\n  num_instances, _, _, _ = keypoints_with_scores.shape\n  for idx in range(num_instances):\n    kpts_x = keypoints_with_scores[0, idx, :, 1]\n    kpts_y = keypoints_with_scores[0, idx, :, 0]\n    kpts_scores = keypoints_with_scores[0, idx, :, 2]\n    kpts_absolute_xy = np.stack(\n        [width * np.array(kpts_x), height * np.array(kpts_y)], axis=-1)\n    kpts_above_thresh_absolute = kpts_absolute_xy[\n        kpts_scores > keypoint_threshold, :]\n    keypoints_all.append(kpts_above_thresh_absolute)\n\n    for edge_pair, color in KEYPOINT_EDGE_INDS_TO_COLOR.items():\n      if (kpts_scores[edge_pair[0]] > keypoint_threshold and\n          kpts_scores[edge_pair[1]] > keypoint_threshold):\n        x_start = kpts_absolute_xy[edge_pair[0], 0]\n        y_start = kpts_absolute_xy[edge_pair[0], 1]\n        x_end = kpts_absolute_xy[edge_pair[1], 0]\n        y_end = kpts_absolute_xy[edge_pair[1], 1]\n        line_seg = np.array([[x_start, y_start], [x_end, y_end]])\n        keypoint_edges_all.append(line_seg)\n        edge_colors.append(color)\n  if keypoints_all:\n    keypoints_xy = np.concatenate(keypoints_all, axis=0)\n  else:\n    keypoints_xy = np.zeros((0, 17, 2))\n\n  if keypoint_edges_all:\n    edges_xy = np.stack(keypoint_edges_all, axis=0)\n  else:\n    edges_xy = np.zeros((0, 2, 2))\n  return keypoints_xy, edges_xy, edge_colors\n\n\ndef draw_prediction_on_image(\n    image, keypoints_with_scores, crop_region=None, close_figure=False,\n    output_image_height=None):\n\n  height, width, channel = image.shape\n  aspect_ratio = float(width) / height\n  fig, ax = plt.subplots(figsize=(12 * aspect_ratio, 12))\n  # To remove the huge white borders\n  fig.tight_layout(pad=0)\n  ax.margins(0)\n  ax.set_yticklabels([])\n  ax.set_xticklabels([])\n  plt.axis('off')\n\n  im = ax.imshow(image)\n  line_segments = LineCollection([], linewidths=(4), linestyle='solid')\n  ax.add_collection(line_segments)\n  # Turn off tick labels\n  scat = ax.scatter([], [], s=60, color='#FF1493', zorder=3)\n\n  (keypoint_locs, keypoint_edges,\n   edge_colors) = _keypoints_and_edges_for_display(\n       keypoints_with_scores, height, width)\n\n  line_segments.set_segments(keypoint_edges)\n  line_segments.set_color(edge_colors)\n  if keypoint_edges.shape[0]:\n    line_segments.set_segments(keypoint_edges)\n    line_segments.set_color(edge_colors)\n  if keypoint_locs.shape[0]:\n    scat.set_offsets(keypoint_locs)\n\n  if crop_region is not None:\n    xmin = max(crop_region['x_min'] * width, 0.0)\n    ymin = max(crop_region['y_min'] * height, 0.0)\n    rec_width = min(crop_region['x_max'], 0.99) * width - xmin\n    rec_height = min(crop_region['y_max'], 0.99) * height - ymin\n    rect = patches.Rectangle(\n        (xmin,ymin),rec_width,rec_height,\n        linewidth=1,edgecolor='b',facecolor='none')\n    ax.add_patch(rect)\n\n  fig.canvas.draw()\n  image_from_plot = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n  image_from_plot = image_from_plot.reshape(\n      fig.canvas.get_width_height()[::-1] + (3,))\n  plt.close(fig)\n  if output_image_height is not None:\n    output_image_width = int(output_image_height / height * width)\n    image_from_plot = cv2.resize(\n        image_from_plot, dsize=(output_image_width, output_image_height),\n         interpolation=cv2.INTER_CUBIC)\n  return image_from_plot\n\n\ndef to_gif(images, fps):\n  \"\"\"Converts image sequence (4D numpy array) to gif.\"\"\"\n  imageio.mimsave('./animation.gif', images, fps=fps)\n  return embed.embed_file('./animation.gif')\n\n\ndef progress(value, max=100):\n  return HTML(\"\"\"\n      <progress\n          value='{value}'\n          max='{max}',\n          style='width: 100%'\n      >\n          {value}\n      </progress>\n  \"\"\".format(value=value, max=max))","metadata":{"cellView":"form","id":"bEJBMeRb3YUy","papermill":{"duration":0.05187,"end_time":"2021-11-01T17:27:46.123894","exception":false,"start_time":"2021-11-01T17:27:46.072024","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-02T02:26:19.620632Z","iopub.execute_input":"2021-11-02T02:26:19.623131Z","iopub.status.idle":"2021-11-02T02:26:19.653621Z","shell.execute_reply.started":"2021-11-02T02:26:19.623091Z","shell.execute_reply":"2021-11-02T02:26:19.652725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_name = \"movenet_lightning\" #@param [\"movenet_lightning\", \"movenet_thunder\", \"movenet_lightning_f16.tflite\", \"movenet_thunder_f16.tflite\", \"movenet_lightning_int8.tflite\", \"movenet_thunder_int8.tflite\"]\n\nif \"tflite\" in model_name:\n  if \"movenet_lightning_f16\" in model_name:\n    !wget -q -O model.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/lightning/tflite/float16/4?lite-format=tflite\n    input_size = 192\n  elif \"movenet_thunder_f16\" in model_name:\n    !wget -q -O model.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/thunder/tflite/float16/4?lite-format=tflite\n    input_size = 256\n  elif \"movenet_lightning_int8\" in model_name:\n    !wget -q -O model.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/lightning/tflite/int8/4?lite-format=tflite\n    input_size = 192\n  elif \"movenet_thunder_int8\" in model_name:\n    !wget -q -O model.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/thunder/tflite/int8/4?lite-format=tflite\n    input_size = 256\n  else:\n    raise ValueError(\"Unsupported model name: %s\" % model_name)\n\n  # Initialize the TFLite interpreter\n  interpreter = tf.lite.Interpreter(model_path=\"model.tflite\")\n  interpreter.allocate_tensors()\n\n  def movenet(input_image):\n\n    # TF Lite format expects tensor type of uint8.\n    input_image = tf.cast(input_image, dtype=tf.uint8)\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n    interpreter.set_tensor(input_details[0]['index'], input_image.numpy())\n    # Invoke inference.\n    interpreter.invoke()\n    # Get the model prediction.\n    keypoints_with_scores = interpreter.get_tensor(output_details[0]['index'])\n    return keypoints_with_scores\n\nelse:\n  if \"movenet_lightning\" in model_name:\n    module = hub.load(\"https://tfhub.dev/google/movenet/singlepose/lightning/4\")\n    input_size = 192\n  elif \"movenet_thunder\" in model_name:\n    module = hub.load(\"https://tfhub.dev/google/movenet/singlepose/thunder/4\")\n    input_size = 256\n  else:\n    raise ValueError(\"Unsupported model name: %s\" % model_name)\n\n  def movenet(input_image):\n\n    model = module.signatures['serving_default']\n\n    # SavedModel format expects tensor type of int32.\n    input_image = tf.cast(input_image, dtype=tf.int32)\n    # Run model inference.\n    outputs = model(input_image)\n    # Output is a [1, 1, 17, 3] tensor.\n    keypoint_with_scores = outputs['output_0'].numpy()\n    return keypoint_with_scores","metadata":{"id":"zeGHgANcT7a1","papermill":{"duration":17.8581,"end_time":"2021-11-01T17:28:04.004134","exception":false,"start_time":"2021-11-01T17:27:46.146034","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-02T02:26:19.655612Z","iopub.execute_input":"2021-11-02T02:26:19.656008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_dir='../input/state-farm-distracted-driver-detection/imgs/train/c1'\nfiles=os.listdir(data_dir)\nprint(files[0:5])","metadata":{"papermill":{"duration":0.038667,"end_time":"2021-11-01T17:28:04.065369","exception":false,"start_time":"2021-11-01T17:28:04.026702","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-02T02:35:40.661084Z","iopub.execute_input":"2021-11-02T02:35:40.661397Z","iopub.status.idle":"2021-11-02T02:35:40.740568Z","shell.execute_reply.started":"2021-11-02T02:35:40.661318Z","shell.execute_reply":"2021-11-02T02:35:40.739595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"paths=[]\nfor item in files:\n    paths+=[os.path.join(data_dir,item)]","metadata":{"papermill":{"duration":0.029142,"end_time":"2021-11-01T17:28:04.117234","exception":false,"start_time":"2021-11-01T17:28:04.088092","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(5,5))\nimageT0 = tf.io.read_file(paths[1])\nimageT = tf.image.decode_jpeg(imageT0)\nprint(type(imageT))\nplt.imshow(imageT)\n_ = plt.axis('off')","metadata":{"papermill":{"duration":0.284813,"end_time":"2021-11-01T17:28:04.42453","exception":false,"start_time":"2021-11-01T17:28:04.139717","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def overlay(image):\n    input_image = tf.expand_dims(image, axis=0)\n    input_image = tf.image.resize_with_pad(input_image, input_size, input_size)\n\n    # Run model inference.\n    keypoint_with_scores = movenet(input_image)\n\n    # Visualize the predictions with image.\n    display_image = tf.expand_dims(image, axis=0)\n    display_image = tf.cast(tf.image.resize_with_pad(\n        display_image,128,128), dtype=tf.int32)\n    output_overlay = draw_prediction_on_image(\n        np.squeeze(display_image.numpy(), axis=0), keypoint_with_scores)\n\n    return output_overlay","metadata":{"papermill":{"duration":0.039144,"end_time":"2021-11-01T17:28:04.49126","exception":false,"start_time":"2021-11-01T17:28:04.452116","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(5,5))\n#imageT0 = tf.io.read_file(image_path)\n#imageT = tf.image.decode_jpeg(imageT0)\noutput_overlay=overlay(imageT)\nplt.imshow(output_overlay)\n_ = plt.axis('off')","metadata":{"papermill":{"duration":1.67548,"end_time":"2021-11-01T17:28:06.194779","exception":false,"start_time":"2021-11-01T17:28:04.519299","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def overlay2(image):\n    input_image = tf.expand_dims(image, axis=0)\n    input_image = tf.image.resize_with_pad(input_image, input_size, input_size)\n\n    # Run model inference.\n    keypoint_with_scores = movenet(input_image)\n\n    # Visualize the predictions with image.\n    display_image = tf.expand_dims(image, axis=0)\n    display_image = tf.cast(tf.image.resize_with_pad(\n        display_image, 128, 128), dtype=tf.int32)\n    output_overlay = draw_prediction_on_image(\n        np.squeeze(display_image.numpy()*0, axis=0), keypoint_with_scores)\n\n    return output_overlay","metadata":{"papermill":{"duration":0.041184,"end_time":"2021-11-01T17:28:06.268284","exception":false,"start_time":"2021-11-01T17:28:06.2271","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(5,5))\n#imageT0 = tf.io.read_file(image_path)\n#imageT = tf.image.decode_jpeg(imageT0)\noutput_overlay=overlay2(imageT)\nplt.imshow(output_overlay)\n_ = plt.axis('off')","metadata":{"papermill":{"duration":0.414462,"end_time":"2021-11-01T17:28:06.714012","exception":false,"start_time":"2021-11-01T17:28:06.29955","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dir = '../input/state-farm-distracted-driver-detection/imgs/train'","metadata":{"papermill":{"duration":0.039585,"end_time":"2021-11-01T17:28:06.785686","exception":false,"start_time":"2021-11-01T17:28:06.746101","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Name0=[]\nfor file in os.listdir(train_dir):\n    Name0+=[file]\nName=sorted(Name0)\nprint(Name)\nprint(len(Name))","metadata":{"papermill":{"duration":0.051427,"end_time":"2021-11-01T17:28:06.870437","exception":false,"start_time":"2021-11-01T17:28:06.81901","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"N=list(range(len(Name)))\nnormal_mapping=dict(zip(Name,N)) \nreverse_mapping=dict(zip(N,Name)) ","metadata":{"papermill":{"duration":0.040986,"end_time":"2021-11-01T17:28:06.945011","exception":false,"start_time":"2021-11-01T17:28:06.904025","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainset=list()\ntrainset2=list()\ntrainname=list()\nfor name in tqdm(Name):\n    path=os.path.join(train_dir,name)\n    for i in range(10):\n        im=os.listdir(path)[i]\n        img1=cv2.imread(os.path.join(path,im))\n        img2=cv2.resize(img1,dsize=(100,100),interpolation=cv2.INTER_CUBIC)\n        img3=overlay2(img2)\n        trainset.append(img3)\n        trainset2.append(img2)\n        trainname.append(name)","metadata":{"papermill":{"duration":157.916019,"end_time":"2021-11-01T17:30:44.894891","exception":false,"start_time":"2021-11-01T17:28:06.978872","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainX=np.array(trainset)\ntrainX2=np.array(trainset2)","metadata":{"papermill":{"duration":0.825396,"end_time":"2021-11-01T17:30:45.787151","exception":false,"start_time":"2021-11-01T17:30:44.961755","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainY0=pd.Series(trainname).map(normal_mapping)","metadata":{"papermill":{"duration":0.078267,"end_time":"2021-11-01T17:30:45.921079","exception":false,"start_time":"2021-11-01T17:30:45.842812","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"m=len(trainX)\nM=list(range(m))\nrandom.seed(2022)\nrandom.shuffle(M)","metadata":{"papermill":{"duration":0.064359,"end_time":"2021-11-01T17:30:46.041241","exception":false,"start_time":"2021-11-01T17:30:45.976882","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axs = plt.subplots(4,2,figsize=(10,20))\nfor i in range(4):\n    ax=axs[i][0].axis(\"off\")\n    ax=axs[i][0].set_title(trainname[M[i]])\n    ax=axs[i][0].imshow(cv2.cvtColor(trainX2[M[i]],cv2.COLOR_BGR2RGB))\n    ax=axs[i][1].axis(\"off\")\n    ax=axs[i][1].set_title(trainname[M[i]])\n    ax=axs[i][1].imshow(trainX[M[i]])\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.092413,"end_time":"2021-11-01T17:30:50.183129","exception":false,"start_time":"2021-11-01T17:30:50.090716","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-11-02T02:52:36.199208Z","iopub.execute_input":"2021-11-02T02:52:36.200068Z","iopub.status.idle":"2021-11-02T02:52:36.221282Z","shell.execute_reply.started":"2021-11-02T02:52:36.200026Z","shell.execute_reply":"2021-11-02T02:52:36.220093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.092364,"end_time":"2021-11-01T17:30:50.368485","exception":false,"start_time":"2021-11-01T17:30:50.276121","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.089896,"end_time":"2021-11-01T17:30:50.550213","exception":false,"start_time":"2021-11-01T17:30:50.460317","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]}]}