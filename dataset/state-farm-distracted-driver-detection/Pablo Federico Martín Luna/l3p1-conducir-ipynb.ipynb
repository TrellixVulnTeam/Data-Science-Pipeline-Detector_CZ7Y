{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# IMPORTS\n\n# Librerías principales, para tratar imágenes, desarrollar la red neuronal, tratar datos, etcétera\nimport os\nimport tensorflow\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport cv2\nimport keras\n\n# Librería para pintar histogramas\nimport plotly.express as px\n\n# FROMS\n\n# Hace algo importante\nfrom glob import glob\n\n# Para hacer sortcuts a los xrange() de los bucles\nfrom tqdm import tqdm\n\n# Todo lo que necesitamos de la librería sklearn\nfrom sklearn.model_selection import train_test_split      \nfrom keras.utils import np_utils\n\n# Todo lo que necesitamos de la librería keras-tensorflow\nfrom keras.models import Sequential, Model\nfrom keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization","metadata":{"execution":{"iopub.status.busy":"2022-04-04T10:33:32.535746Z","iopub.execute_input":"2022-04-04T10:33:32.536011Z","iopub.status.idle":"2022-04-04T10:33:32.54242Z","shell.execute_reply.started":"2022-04-04T10:33:32.535979Z","shell.execute_reply":"2022-04-04T10:33:32.541757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Declaración de funciones","metadata":{}},{"cell_type":"markdown","source":"### Sacamos la imagen","metadata":{}},{"cell_type":"code","source":"def sacar_imagen(path, img_filas, img_columnas):\n    # Cargamos la imagen que se encuentra en el path con los colores RGB\n    img = cv2.imread(path, cv2.IMREAD_COLOR)\n    # Redimensionamos la imagen al tamaño que queremos\n    img = cv2.resize(img, (img_filas, img_columnas)) \n    return img","metadata":{"execution":{"iopub.status.busy":"2022-04-04T10:33:32.556077Z","iopub.execute_input":"2022-04-04T10:33:32.556515Z","iopub.status.idle":"2022-04-04T10:33:32.560605Z","shell.execute_reply.started":"2022-04-04T10:33:32.556484Z","shell.execute_reply":"2022-04-04T10:33:32.559895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Cargamos el entrenamiento","metadata":{}},{"cell_type":"code","source":"def cargar_entrenamiento(img_filas, img_columnas):\n    # Inicializamos las variables\n    imagenes_entrenamiento, id_entrenamiento = [], []\n    # Hacemos un bucle que se va ha ejecutar num_clases veces\n    for clases in tqdm(range(num_clases)):\n        # Leemos la carpeta de donde sacaremos las imagenes que vamos a utilizar en el entrenamiento\n        fichero = glob(os.path.join('../input/state-farm-distracted-driver-detection/imgs/train/c' + str(clases), '*.jpg'))\n        # Hacemos un bucle que recorre toda la carpeta de imágenes\n        for img in fichero:\n            # Sacamos una imagen, utilizando la función que hemos creado\n            ig = sacar_imagen(img, img_filas, img_columnas)\n            # Almacenamos la imagen\n            imagenes_entrenamiento.append(ig)\n            # Almacenamos la clase\n            id_entrenamiento.append(clases)\n    return imagenes_entrenamiento, id_entrenamiento","metadata":{"execution":{"iopub.status.busy":"2022-04-04T10:33:32.577494Z","iopub.execute_input":"2022-04-04T10:33:32.577778Z","iopub.status.idle":"2022-04-04T10:33:32.58488Z","shell.execute_reply.started":"2022-04-04T10:33:32.577751Z","shell.execute_reply":"2022-04-04T10:33:32.584146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Leer y normalizar los datos de entrenamiento","metadata":{}},{"cell_type":"code","source":"def LyN_datos_entrenamiento(img_filas, img_columnas):\n    # Cargamos el entrenamiento (entrada y salida)\n    X, etiquetas = cargar_entrenamiento(img_filas, img_columnas)\n    # Convertimos nuestra salida de un vector de número enteros a una matriz de clase binaria\n    y = np_utils.to_categorical(etiquetas, num_clases)\n    # Dividimos los datos de E/S en: set de entrenamiento y set de test\n    entrada_entrenamiento, entrada_validacion, salida_entrenamiento, salida_validacion = train_test_split(X, y, test_size=0.2, random_state=42)\n    # Hacemos que la entrada del entrenamiento y del test sean una numpy array y las reescalamos\n    entrada_entrenamiento = np.array(entrada_entrenamiento, dtype=np.uint8).reshape(-1, img_filas, img_columnas, 3)\n    entrada_validacion = np.array(entrada_validacion, dtype=np.uint8).reshape(-1, img_filas, img_columnas, 3)\n    return entrada_entrenamiento, entrada_validacion, salida_entrenamiento, salida_validacion","metadata":{"execution":{"iopub.status.busy":"2022-04-04T10:33:32.599506Z","iopub.execute_input":"2022-04-04T10:33:32.599706Z","iopub.status.idle":"2022-04-04T10:33:32.605446Z","shell.execute_reply.started":"2022-04-04T10:33:32.599683Z","shell.execute_reply":"2022-04-04T10:33:32.604596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Cargamos el test","metadata":{}},{"cell_type":"code","source":"def cargar_test(size, img_filas, img_columnas):\n    # Cargamos el path del test\n    path = os.path.join('../input/state-farm-distracted-driver-detection/imgs/test', '*.jpg')\n    ficheros = sorted(glob(path))\n    # Declaramos las variables\n    entrada_evaluacion, id_entrada_evaluacion = [], []\n    total = 0\n    # Vemos cuantas imágenes hay en la dirección del test\n    size_fichero = len(ficheros)\n    # Hacemos un bucle, que recorre todas las imágenes una a una\n    for fichero in tqdm(ficheros):\n        # Si total es mayor o igual a size o a la cantidad de imágenes que hay\n        if total >= size or total >= size_fichero:\n            # En caso afirmativo: \n            break\n        base_fichero = os.path.basename(fichero)\n        # Sacamos la imagen\n        img = sacar_imagen(fichero, img_filas, img_columnas)\n        # Almacenamos la imagen\n        entrada_evaluacion.append(img)\n        id_entrada_evaluacion.append(base_fichero)\n        # Incrementamos el total\n        total+=1\n    return entrada_evaluacion, id_entrada_evaluacion","metadata":{"execution":{"iopub.status.busy":"2022-04-04T10:33:32.620476Z","iopub.execute_input":"2022-04-04T10:33:32.621Z","iopub.status.idle":"2022-04-04T10:33:32.629192Z","shell.execute_reply.started":"2022-04-04T10:33:32.620969Z","shell.execute_reply":"2022-04-04T10:33:32.628562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Leer y normalizar los datos de test","metadata":{}},{"cell_type":"code","source":"def LyN_datos_test(size, img_filas, img_columnas):\n    # Cargamos las imagenes del test y sus id's\n    datos_test, ids_test = cargar_test(size, img_filas, img_columnas)\n    # Hacemos que los datos para el test, sean una numpy array y la reescalamos\n    datos_test = np.array(datos_test, dtype=np.uint8).reshape(-1, img_filas, img_columnas, 3)\n    return datos_test, ids_test","metadata":{"execution":{"iopub.status.busy":"2022-04-04T10:33:32.638778Z","iopub.execute_input":"2022-04-04T10:33:32.638958Z","iopub.status.idle":"2022-04-04T10:33:32.644852Z","shell.execute_reply.started":"2022-04-04T10:33:32.638936Z","shell.execute_reply":"2022-04-04T10:33:32.644064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Declaración de variables globales","metadata":{}},{"cell_type":"code","source":"# 0: practica_1 | 1: practica_2\ndato = 1\nimg_filas = 128\nimg_columnas = 98\nnum_test_examples = 200","metadata":{"execution":{"iopub.status.busy":"2022-04-04T10:33:32.665959Z","iopub.execute_input":"2022-04-04T10:33:32.666154Z","iopub.status.idle":"2022-04-04T10:33:32.670238Z","shell.execute_reply.started":"2022-04-04T10:33:32.666131Z","shell.execute_reply":"2022-04-04T10:33:32.669292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Declaración del dataset","metadata":{}},{"cell_type":"code","source":"dataset = pd.read_csv('../input/state-farm-distracted-driver-detection/driver_imgs_list.csv')\n\nif dato == 0:\n    num_clases = len(dataset['classname'].unique())\nelse:\n    num_clases = len(dataset['subject'].unique())","metadata":{"execution":{"iopub.status.busy":"2022-04-04T10:33:32.682188Z","iopub.execute_input":"2022-04-04T10:33:32.68246Z","iopub.status.idle":"2022-04-04T10:33:32.707296Z","shell.execute_reply.started":"2022-04-04T10:33:32.682433Z","shell.execute_reply":"2022-04-04T10:33:32.706652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Cargamos las imágenes de entrenamiento","metadata":{}},{"cell_type":"code","source":"# Sacamos la E/S del entrenamiento y el test, ya normalizados\nentrada_entrenamiento, entrada_validacion, salida_entrenamiento, salida_validacion = LyN_datos_entrenamiento(img_filas, img_columnas)","metadata":{"execution":{"iopub.status.busy":"2022-04-04T10:33:32.708733Z","iopub.execute_input":"2022-04-04T10:33:32.70897Z","iopub.status.idle":"2022-04-04T10:35:11.583965Z","shell.execute_reply.started":"2022-04-04T10:33:32.708938Z","shell.execute_reply":"2022-04-04T10:35:11.583231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Cargamos las imágenes de validación","metadata":{}},{"cell_type":"code","source":"# Sacamos la E/S de la validación, ya normalizado\nentrada_evaluacion, salida_evaluacion = LyN_datos_test(num_test_examples, img_filas, img_columnas)","metadata":{"execution":{"iopub.status.busy":"2022-04-04T10:35:11.585958Z","iopub.execute_input":"2022-04-04T10:35:11.586228Z","iopub.status.idle":"2022-04-04T10:35:12.751236Z","shell.execute_reply.started":"2022-04-04T10:35:11.586193Z","shell.execute_reply":"2022-04-04T10:35:12.749966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Datos (Visualización)","metadata":{}},{"cell_type":"markdown","source":"### Histograma de Categorias ","metadata":{}},{"cell_type":"code","source":"# Pintamos el histograma\npx.histogram(dataset, x='classname', color='classname', title='Categorias')","metadata":{"execution":{"iopub.status.busy":"2022-04-04T10:35:12.752558Z","iopub.execute_input":"2022-04-04T10:35:12.752885Z","iopub.status.idle":"2022-04-04T10:35:12.984184Z","shell.execute_reply.started":"2022-04-04T10:35:12.752846Z","shell.execute_reply":"2022-04-04T10:35:12.983517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Histograma de Conductores","metadata":{}},{"cell_type":"code","source":"# Generamos un dataframe propio para los conductores \nconductores_id = pd.DataFrame((dataset['subject'].value_counts()).reset_index())\nconductores_id.columns = ['id_conductor', 'cantidad']\n# Pintamos el histograma\npx.histogram(conductores_id, x='id_conductor', y='cantidad', color='id_conductor', title='Conductores')","metadata":{"execution":{"iopub.status.busy":"2022-04-04T10:35:12.986258Z","iopub.execute_input":"2022-04-04T10:35:12.986997Z","iopub.status.idle":"2022-04-04T10:35:13.146899Z","shell.execute_reply.started":"2022-04-04T10:35:12.986956Z","shell.execute_reply":"2022-04-04T10:35:13.146127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## CNN (Convolution Neural Network)","metadata":{}},{"cell_type":"code","source":"# Inicializamos el modelo\nmodelo = Sequential()\n\n# CNN 1 - Input\nmodelo.add(Conv2D(64,(3,3),activation='relu',input_shape=(img_filas, img_columnas, 3)))\nmodelo.add(BatchNormalization())\nmodelo.add(Conv2D(64,(3,3),activation='relu'))\nmodelo.add(BatchNormalization(axis = 3))\nmodelo.add(MaxPooling2D(pool_size=(2,2)))\nmodelo.add(Dropout(0.5))\n\n# CNN 2\nmodelo.add(Conv2D(64,(3,3),activation='relu'))\nmodelo.add(BatchNormalization())\nmodelo.add(Conv2D(64,(3,3),activation='relu'))\nmodelo.add(BatchNormalization(axis = 3))\nmodelo.add(MaxPooling2D(pool_size=(2,2)))\nmodelo.add(Dropout(0.5))\n\n# CNN 3\nmodelo.add(Conv2D(128,(3,3),activation='relu'))\nmodelo.add(BatchNormalization())\nmodelo.add(Conv2D(128,(3,3),activation='relu'))\nmodelo.add(BatchNormalization(axis = 3))\nmodelo.add(MaxPooling2D(pool_size=(2,2)))\nmodelo.add(Dropout(0.5))\n\n# Output\nmodelo.add(Flatten())\nmodelo.add(Dense(512,activation='relu'))\nmodelo.add(BatchNormalization())\nmodelo.add(Dropout(0.5))\nmodelo.add(Dense(512,activation='relu'))\nmodelo.add(Dropout(0.25))\nmodelo.add(Dense(num_clases,activation='softmax'))","metadata":{"execution":{"iopub.status.busy":"2022-04-04T10:35:13.148293Z","iopub.execute_input":"2022-04-04T10:35:13.148629Z","iopub.status.idle":"2022-04-04T10:35:13.229737Z","shell.execute_reply.started":"2022-04-04T10:35:13.148593Z","shell.execute_reply":"2022-04-04T10:35:13.229076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"modelo.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2022-04-04T10:35:13.230893Z","iopub.execute_input":"2022-04-04T10:35:13.231231Z","iopub.status.idle":"2022-04-04T10:35:13.242166Z","shell.execute_reply.started":"2022-04-04T10:35:13.231192Z","shell.execute_reply":"2022-04-04T10:35:13.24149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = modelo.fit(entrada_entrenamiento, salida_entrenamiento, validation_data=(entrada_validacion, salida_validacion), epochs=10, batch_size=40, verbose=1)","metadata":{"execution":{"iopub.status.busy":"2022-04-04T10:35:13.243434Z","iopub.execute_input":"2022-04-04T10:35:13.243898Z","iopub.status.idle":"2022-04-04T10:36:20.973617Z","shell.execute_reply.started":"2022-04-04T10:35:13.243861Z","shell.execute_reply":"2022-04-04T10:36:20.972849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluacion = modelo.evaluate(entrada_validacion, salida_validacion, verbose=1)\nprint('Error: {:.2f}'.format(evaluacion[0]))\nprint('Precisión: {:.2f}%'.format(evaluacion[1]*100))","metadata":{"execution":{"iopub.status.busy":"2022-04-04T10:36:20.975177Z","iopub.execute_input":"2022-04-04T10:36:20.975511Z","iopub.status.idle":"2022-04-04T10:36:22.607781Z","shell.execute_reply.started":"2022-04-04T10:36:20.975471Z","shell.execute_reply":"2022-04-04T10:36:22.607074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = modelo.predict(entrada_evaluacion)\nfor i in result:\n    print('----------------------------------')\n    if dato == 0:\n        accion = dataset['classname'].unique()\n        porcentajes = [i[k]*100 for k in range(len(i))]\n        plt.pie(porcentajes, labels=accion, autopct=\"%0.2f %%\")\n        plt.axis(\"equal\")\n        plt.show()\n    else:\n        conductores = dataset['subject'].unique()\n        porcentajes = [i[k]*100 for k in range(len(i))]\n        plt.pie(porcentajes, labels=conductores, autopct=\"%0.2f %%\")\n        plt.axis(\"equal\")\n        plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-04T10:36:22.609982Z","iopub.execute_input":"2022-04-04T10:36:22.610804Z","iopub.status.idle":"2022-04-04T10:36:22.617018Z","shell.execute_reply.started":"2022-04-04T10:36:22.610761Z","shell.execute_reply":"2022-04-04T10:36:22.616146Z"},"trusted":true},"execution_count":null,"outputs":[]}]}