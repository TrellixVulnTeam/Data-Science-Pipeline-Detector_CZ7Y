{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Nowadays, safe driving has become a serious problem all over the country and even the world. Most of the consequences of traffic accidents are human factors, such as tired driving, talking on the phone while driving, talking back and so on. In order to reduce traffic accidents caused by human factors, a driver behavior detection method based on convolutional neural networks (CNN) and data enhancement is proposed. Based on the original convolutional neural network, this method optimized the original convolutional neural network and designed a model suitable for the task. At the same time, the data enhancement transformation method is introduced to expand the original data to improve the over-fitting problem caused by the lack of data. Experimental results show that the accuracy of the proposed model is improved by about 0.67% compared with the original model, and the results of the enhanced data are improved by 0.8% compared with the original data.","metadata":{}},{"cell_type":"markdown","source":"1.Import packages ,read file Information and display labels","metadata":{}},{"cell_type":"code","source":"# -*- coding: utf-8 -*-\nimport os\nfrom glob import glob\nimport random\nimport time\nimport tensorflow\nimport datetime\nos.environ['KERAS_BACKEND'] = 'tensorflow'\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # 3 = INFO, WARNING, and ERROR messages are not printed\n\nfrom tqdm import tqdm\n\nimport numpy as np\nimport pandas as pd\nfrom IPython.display import FileLink\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\nimport seaborn as sns \nfrom IPython.display import display, Image\nimport matplotlib.image as mpimg\nimport cv2\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_files       \nfrom keras.utils import np_utils\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import log_loss\n\nfrom keras.models import Sequential, Model\nfrom keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, GlobalAveragePooling2D\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.preprocessing import image\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom keras.applications.vgg16 import VGG16\n\n\ndataset = pd.read_csv('../input/state-farm-distracted-driver-detection/driver_imgs_list.csv')\nby_drivers = dataset.groupby('subject')\nunique_drivers = by_drivers.groups.keys()\nprint(unique_drivers)\n# Load the dataset previously downloaded from Kaggle\nNUMBER_CLASSES = 10\n# Color type: 1 - grey, 3 - rgb","metadata":{"execution":{"iopub.status.busy":"2022-03-17T03:14:45.673696Z","iopub.execute_input":"2022-03-17T03:14:45.673962Z","iopub.status.idle":"2022-03-17T03:14:45.709457Z","shell.execute_reply.started":"2022-03-17T03:14:45.673933Z","shell.execute_reply":"2022-03-17T03:14:45.708712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2.Load data\n\nLoads training and test datasets and displays dataset information\n\nDataset:State Farm, an insurance company, published a collection of 40,000 images of real-life drivers' behaviour.","metadata":{}},{"cell_type":"code","source":"def get_cv2_image(path, img_rows, img_cols, color_type=3):\n    # Loading as Grayscale image\n    if color_type == 1:\n        img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n    elif color_type == 3:\n        img = cv2.imread(path, cv2.IMREAD_COLOR)\n    # Reduce size\n    img = cv2.resize(img, (img_rows, img_cols)) \n    return img\n\n# Load training data\ndef load_train(img_rows, img_cols, color_type=3):\n    start_time = time.time()\n    train_images = [] \n    train_labels = []\n    # Loop over the training folder \n    for classed in tqdm(range(NUMBER_CLASSES)):\n        print('Loading directory c{}'.format(classed))\n        file_path = os.path.join('..', 'input', 'state-farm-distracted-driver-detection', 'imgs','train', 'c' + str(classed), '*.jpg')\n        #print('file_path:',file_path)\n        files = glob(file_path)\n        print('files len:',len(files))\n        #files = glob(os.path.join('.', 'imgs','train', 'c' + str(classed), '*.jpg'))\n        for file in files:\n            img = get_cv2_image(file, img_rows, img_cols, color_type)\n            #print(\"img:\",img)\n            #print(\"classed:\",classed)\n            train_images.append(img)\n            train_labels.append(classed)\n    print(\"Data Loaded in {} second\".format(time.time() - start_time))\n    return train_images, train_labels \ndef read_and_normalize_train_data(img_rows, img_cols, color_type):\n    X, labels = load_train(img_rows, img_cols, color_type)\n    #print(\"labels:\",labels);\n    y = np_utils.to_categorical(labels, 10)\n    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    \n    x_train = np.array(x_train, dtype=np.uint8).reshape(-1,img_rows,img_cols,color_type)\n    x_test = np.array(x_test, dtype=np.uint8).reshape(-1,img_rows,img_cols,color_type)\n    \n    return x_train, x_test, y_train, y_test\n\n# Validation\ndef load_test(size=200000, img_rows=64, img_cols=64, color_type=3):\n    path = os.path.join('..', 'input', 'state-farm-distracted-driver-detection', 'imgs', 'test', '*.jpg')\n    files = sorted(glob(path))\n    X_test, X_test_id = [], []\n    total = 0\n    files_size = len(files)\n    for file in tqdm(files):\n        if total >= size or total >= files_size:\n            break\n        file_base = os.path.basename(file)\n        img = get_cv2_image(file, img_rows, img_cols, color_type)\n        X_test.append(img)\n        X_test_id.append(file_base)\n        total += 1\n    return X_test, X_test_id\n\ndef read_and_normalize_sampled_test_data(size, img_rows, img_cols, color_type=3):\n    test_data, test_ids = load_test(size, img_rows, img_cols, color_type)\n    \n    test_data = np.array(test_data, dtype=np.uint8)\n    test_data = test_data.reshape(-1,img_rows,img_cols,color_type)\n    \n    return test_data, test_ids\n\n\nimg_rows = 64\nimg_cols = 64\ncolor_type = 1\n\nx_train, x_test, y_train, y_test = read_and_normalize_train_data(img_rows, img_cols, color_type)\nprint('Train shape:', x_train.shape)\nprint(x_train.shape[0], 'train samples')\n\nnb_test_samples = 200\ntest_files, test_targets = read_and_normalize_sampled_test_data(nb_test_samples, img_rows, img_cols, color_type)\nprint('Test shape:', test_files.shape)\nprint(test_files.shape[0], 'Test samples')\n\n# Statistics\n# Load the list of names\nnames = [item[17:19] for item in sorted(glob(\"../input/state-farm-distracted-driver-detection/imgs/train/*/\"))]\ntest_files_size = len(np.array(glob(os.path.join('..','input','state-farm-distracted-driver-detection','imgs', 'test', '*.jpg'))))\nx_train_size = len(x_train)\ncategories_size = len(names)\nx_test_size = len(x_test)\nprint('There are %s total images.\\n' % (test_files_size + x_train_size + x_test_size))\nprint('There are %d training images.' % x_train_size)\nprint('There are %d total training categories.' % categories_size)\nprint('There are %d validation images.' % x_test_size)\nprint('There are %d test images.'% test_files_size)\n","metadata":{"execution":{"iopub.status.busy":"2022-03-17T03:14:45.860615Z","iopub.execute_input":"2022-03-17T03:14:45.860869Z","iopub.status.idle":"2022-03-17T03:15:41.609691Z","shell.execute_reply.started":"2022-03-17T03:14:45.860838Z","shell.execute_reply":"2022-03-17T03:15:41.608242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"3.Data visualization\n\nThe data set is divided into 10 categories:\n\nc0: safe driving,c1: texting - right,c2: talking on the phone - right,c3: texting - left,c4: talking on the phone - left,c5: operating the radio,c6: drinking,c7: reaching behind,c8: hair and makeup,c9: talking to passenger\n\n\nOne of these 10 classes is randomly selected to show the data set.\n","metadata":{}},{"cell_type":"code","source":"# Plot figure size\nplt.figure(figsize = (10,10))\n# Count the number of images per category\nsns.countplot(x = 'classname', data = dataset)\n# Change the Axis names\nplt.ylabel('Count')\nplt.title('Categories Distribution')\n# Show plot\nplt.show()\n\n# Find the frequency of images per driver找出每个驱动程序的图像频率\ndrivers_id = pd.DataFrame((dataset['subject'].value_counts()).reset_index())\ndrivers_id.columns = ['driver_id', 'Counts']\n\n\n# Plotting class distribution画出每一类的分布\ndataset['class_type'] = dataset['classname'].str.extract('(\\d)',expand=False).astype(np.float)\nplt.figure(figsize = (20,20))\ndataset.hist('class_type', alpha=0.5, layout=(1,1), bins=10)\nplt.title('Class distribution')\nplt.show()\n\n\nactivity_map = {'c0': 'Safe driving', \n                'c1': 'Texting - right', \n                'c2': 'Talking on the phone - right', \n                'c3': 'Texting - left', \n                'c4': 'Talking on the phone - left', \n                'c5': 'Operating the radio', \n                'c6': 'Drinking', \n                'c7': 'Reaching behind', \n                'c8': 'Hair and makeup', \n                'c9': 'Talking to passenger'}\n\n\n\nplt.figure(figsize = (12, 20))\nimage_count = 1\nBASE_URL = '../input/state-farm-distracted-driver-detection/imgs/train/'\nfor directory in os.listdir(BASE_URL):\n    if directory[0] != '.':\n        for i, file in enumerate(os.listdir(BASE_URL + directory)):\n            if i == 1:\n                break\n            else:\n                fig = plt.subplot(5, 2, image_count)\n                image_count += 1\n                image = mpimg.imread(BASE_URL + directory + '/' + file)\n                plt.imshow(image)\n                plt.title(activity_map[directory])\n                \n                \n                \n\ndef create_submission(predictions, test_id, info):\n    result = pd.DataFrame(predictions, columns=['c0', 'c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7', 'c8', 'c9'])\n    result.loc[:, 'img'] = pd.Series(test_id, index=result.index)\n    \n    now = datetime.datetime.now()\n    \n    if not os.path.isdir('kaggle_submissions'):\n        os.mkdir('kaggle_submissions')\n\n    suffix = \"{}_{}\".format(info,str(now.strftime(\"%Y-%m-%d-%H-%M\")))\n    sub_file = os.path.join('kaggle_submissions', 'submission_' + suffix + '.csv')\n    \n    result.to_csv(sub_file, index=False)\n    \n    return sub_file\n","metadata":{"execution":{"iopub.status.busy":"2022-03-17T03:15:41.611935Z","iopub.execute_input":"2022-03-17T03:15:41.612511Z","iopub.status.idle":"2022-03-17T03:15:43.989354Z","shell.execute_reply.started":"2022-03-17T03:15:41.612468Z","shell.execute_reply":"2022-03-17T03:15:43.987426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"4.Model building\n\nThe original neural network model, denoted as Model1. BN layer is introduced into the optimized CNN network, denoted as Model2. On the basis of the optimized CNN model, the data input to the model is denoted as Model3 by using the data enhancement method to avoid over-fitting and merging to increase the generalization ability of the model.\n\nModel3 is compared with the original convolutional neural network model (Model1) and the optimized convolutional neural network model (Model2) for experimental analysis. All models use the method of 80% training data set and 20% validation data set to segment the training set. Experiments verify the accuracy and loss value of the training set to analyze the experimental results.","metadata":{}},{"cell_type":"code","source":"batch_size = 40\nnb_epoch = 10\n\nif os.path.exists('./saved_models/weights_best_vanilla.hdf5'):\n    os.remove('./saved_models/weights_best_vanilla.hdf5')\n    \n    \n\nmodels_dir = \"saved_models\"\nif not os.path.exists(models_dir):\n    os.makedirs(models_dir)\n    \ncheckpointer = ModelCheckpoint(filepath='./saved_models/weights_best_vanilla.hdf5', \n                               monitor='val_loss', mode='min',\n                               verbose=1, save_best_only=True)\nes = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2)\ncallbacks = [checkpointer, es]\n\n\ndef plot_train_history(history):\n    # Summarize history for accuracy\n    plt.plot(history.history['accuracy'])\n    plt.plot(history.history['val_accuracy'])\n    plt.title('Model accuracy')\n    plt.ylabel('accuracy')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    plt.show()\n\n    # Summarize history for loss\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('Model loss')\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    plt.show()\n    \n    \ndef create_model_v1():\n    # Vanilla CNN model\n    #构建一个最简单的CNN 模型\n    model = Sequential()\n\n    model.add(Conv2D(filters = 64, kernel_size = 3, padding='same', activation = 'relu', input_shape=(img_rows, img_cols, color_type)))\n    model.add(MaxPooling2D(pool_size = 2))\n\n    model.add(Conv2D(filters = 128, padding='same', kernel_size = 3, activation = 'relu'))\n    model.add(MaxPooling2D(pool_size = 2))\n\n    model.add(Conv2D(filters = 256, padding='same', kernel_size = 3, activation = 'relu'))\n    model.add(MaxPooling2D(pool_size = 2))\n\n    model.add(Conv2D(filters = 512, padding='same', kernel_size = 3, activation = 'relu'))\n    model.add(MaxPooling2D(pool_size = 2))\n\n    model.add(Dropout(0.5))#50%的神经元不工作,拿掉部分神经元防止过拟合\n\n    model.add(Flatten())#卷积层到全连接层的过渡\n\n    model.add(Dense(500, activation = 'relu'))#全连接层，500个节点，使用relu激活函数\n    model.add(Dropout(0.5))\n    model.add(Dense(10, activation = 'softmax'))\n    \n    return model\n\n\nmodel_v1 = create_model_v1()\n\n# More details about the layers\nmodel_v1.summary()\n\n# Compiling the model\n#定义优化器，损失函数，训练效果中计算准确率\n#使用'rmsprop'优化器，loss = 损失用交叉熵，速度会更快，metrics计算准确率\nmodel_v1.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n\n\n# Training the Vanilla Model version 1\n#训练\nhistory_v1 = model_v1.fit(x_train, y_train, \n          validation_data=(x_test, y_test),\n          callbacks=[checkpointer],\n          epochs=10, batch_size=batch_size, verbose=1)\n\n\nmodel_v1.load_weights('./saved_models/weights_best_vanilla.hdf5')\n\n\n\ndef plot_test_class(model, test_files, image_number, color_type=1):\n    print(\"test_files len:\",len(test_files))\n    img_brute = test_files[image_number]\n    img_brute = cv2.resize(img_brute,(img_rows,img_cols))\n    plt.imshow(img_brute, cmap='gray')\n\n    new_img = img_brute.reshape(-1,img_rows,img_cols,color_type)\n    #predict预测所提供数据的推断中最后一层的输出\n    y_prediction = model.predict(new_img, batch_size=batch_size, verbose=1)\n    print('Y prediction: {}'.format(y_prediction))\n    print('Predicted: {}'.format(activity_map.get('c{}'.format(np.argmax(y_prediction)))))\n    \n    plt.show()\n    \nscore = model_v1.evaluate(x_test, y_test, verbose=1)\nprint('Score: ', score)\n\nplot_train_history(history_v1)\n\nplot_test_class(model_v1, test_files, 20)\n\npredictions_v1 = model_v1.predict(test_files, batch_size=batch_size)\nFileLink(create_submission(predictions_v1, test_targets, score[0]))\n\n\nif os.path.exists('./saved_models/weights_best_vanilla.hdf5'):\n    os.remove('./saved_models/weights_best_vanilla.hdf5')\n    \n    \n    \ndef create_model_v2():\n    # Optimised Vanilla CNN model 优化模型\n    model = Sequential()\n\n    ## CNN 1\n    model.add(Conv2D(32,(3,3),activation='relu',input_shape=(img_rows, img_cols, color_type)))\n    model.add(BatchNormalization())#标准化层\n    model.add(Conv2D(32,(3,3),activation='relu',padding='same'))\n    model.add(BatchNormalization(axis = 3))\n    model.add(MaxPooling2D(pool_size=(2,2),padding='same'))\n    model.add(Dropout(0.3))\n\n    ## CNN 2\n    model.add(Conv2D(64,(3,3),activation='relu',padding='same'))\n    model.add(BatchNormalization())\n    model.add(Conv2D(64,(3,3),activation='relu',padding='same'))\n    model.add(BatchNormalization(axis = 3))\n    model.add(MaxPooling2D(pool_size=(2,2),padding='same'))\n    model.add(Dropout(0.3))\n\n    ## CNN 3\n    model.add(Conv2D(128,(3,3),activation='relu',padding='same'))\n    model.add(BatchNormalization())\n    model.add(Conv2D(128,(3,3),activation='relu',padding='same'))\n    model.add(BatchNormalization(axis = 3))\n    model.add(MaxPooling2D(pool_size=(2,2),padding='same'))\n    model.add(Dropout(0.5))\n\n    ## Output\n    model.add(Flatten())\n    model.add(Dense(512,activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.5))\n    model.add(Dense(128,activation='relu'))\n    model.add(Dropout(0.25))\n    model.add(Dense(10,activation='softmax'))\n\n    return model\n\n\nmodel_v2 = create_model_v2()\n\n# More details about the layers\nmodel_v2.summary()\n\n# Compiling the model\nmodel_v2.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n\n\n# Training the Vanilla Model\nhistory_v2 = model_v2.fit(x_train, y_train, \n          validation_data=(x_test, y_test),\n          callbacks=[checkpointer],\n          epochs=nb_epoch, batch_size=batch_size, verbose=1)\n\n\nplot_train_history(history_v2)\n\n#model_v2.load_weights('saved_models/weights_best_vanilla.hdf5')#读取权重文件\nmodel_v2.load_weights('./saved_models/weights_best_vanilla.hdf5')#读取权重文件\n\nscore = model_v2.evaluate(x_test, y_test, verbose=1)#模型评估\nprint('Score: ', score)\n\n\n\n#生成目标文件\npredictions_v2 = model_v2.predict(test_files, batch_size=batch_size)\nFileLink(create_submission(predictions_v2, test_targets, score[0]))\n\n\n#预测\ny_pred = model_v2.predict(x_test, batch_size=batch_size, verbose=1)\nscore = log_loss(y_test, y_pred)\nprint('Score log loss:', score)\n\n\nplot_test_class(model_v2, test_files, 101) # The model really performs badly这个型号的性能真的很差\nplot_test_class(model_v2, test_files, 1) # The model really performs badly这个模型的性能真的很差\nplot_test_class(model_v2, test_files, 143) \n\n\n# if os.path.exists('./saved_models/weights_best_vanilla.hdf5'):\n#     os.remove('./saved_models/weights_best_vanilla.hdf5')\n#     print('文件已删除')\n    \n    \n# Prepare data augmentation configuration\ntrain_datagen = ImageDataGenerator(rescale = 1.0/255, \n                                   #zoom_range = 0.25, \n                                   validation_split = 0.2)\n\ntest_datagen = ImageDataGenerator(rescale=1.0/ 255,\n                                  validation_split = 0.2)\n\nnb_train_samples = x_train.shape[0]\nnb_validation_samples = x_test.shape[0]\n\n#print(nb_train_samples)\n#print(nb_validation_samples)\n\ntraining_generator = train_datagen.flow(x_train, y_train, batch_size=batch_size)\nvalidation_generator = test_datagen.flow(x_test, y_test, batch_size=batch_size)\n\n\ncheckpoint = ModelCheckpoint('./saved_models/weights_best_vanilla.hdf5', monitor='val_acc', verbose=1, save_best_only=True, mode='max')\nhistory_v3 = model_v2.fit_generator(training_generator,\n                         steps_per_epoch = nb_train_samples // batch_size,\n                         epochs = 10, \n                         callbacks= [checkpoint],\n                         verbose = 1,\n                         validation_data = validation_generator,\n                         validation_steps = nb_validation_samples // batch_size)\n\n\n#model_v2.load_weights('saved_models/weights_best_vanilla.hdf5')\nmodel_v2.load_weights('./saved_models/weights_best_vanilla.hdf5')\n\nplot_train_history(history_v3)\n# Evaluate the performance of the new model评估新模型的性能\nscore = model_v2.evaluate_generator(validation_generator, nb_validation_samples // batch_size)\nprint(\"Test Score:\", score[0])\nprint(\"Test Accuracy:\", score[1])\n\nplot_test_class(model_v2, test_files, 101)\nplot_test_class(model_v2, test_files, 1)\nplot_test_class(model_v2, test_files, 145)\nplot_test_class(model_v2, test_files, 143) \n\npredictions_v3 = model_v2.predict(test_files, batch_size=batch_size)\nFileLink(create_submission(predictions_v3, test_targets, score[0]))\n\n","metadata":{"execution":{"iopub.status.busy":"2022-03-17T03:15:43.990745Z","iopub.execute_input":"2022-03-17T03:15:43.991087Z","iopub.status.idle":"2022-03-17T03:19:23.471579Z","shell.execute_reply.started":"2022-03-17T03:15:43.991054Z","shell.execute_reply":"2022-03-17T03:19:23.470913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"5. Experimental comparison of Model1, Model2 and Model3.","metadata":{}},{"cell_type":"code","source":"def plot_train_history(history_v1,history_v2,history_v3):\n    # Summarize history for accuracy\n    plt.plot(history_v1.history['val_accuracy'])\n    plt.plot(history_v2.history['val_accuracy'])\n    plt.plot(history_v3.history['val_accuracy'])\n    #plt.title('Model accuracy')\n    plt.ylabel('val_accuracy')\n    plt.xlabel('epoch')\n    plt.legend(['Model1', 'Model2','Model3'], loc='upper left')\n    plt.show()\n\n    # Summarize history for loss\n    plt.plot(history_v1.history['val_loss'])\n    plt.plot(history_v2.history['val_loss'])\n    plt.plot(history_v3.history['val_loss'])\n    #plt.title('Model loss')\n    plt.ylabel('val_loss')\n    plt.xlabel('epoch')\n    plt.legend(['Model1', 'Model2','Model3'], loc='upper left')\n    plt.show()\n    \n    \n    \nplot_train_history(history_v1,history_v2,history_v3)\n","metadata":{"execution":{"iopub.status.busy":"2022-03-17T03:19:23.47341Z","iopub.execute_input":"2022-03-17T03:19:23.473653Z","iopub.status.idle":"2022-03-17T03:19:23.82599Z","shell.execute_reply.started":"2022-03-17T03:19:23.473618Z","shell.execute_reply":"2022-03-17T03:19:23.825297Z"},"trusted":true},"execution_count":null,"outputs":[]}]}