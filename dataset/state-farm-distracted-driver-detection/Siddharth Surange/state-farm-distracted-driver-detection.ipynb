{"cells":[{"metadata":{},"cell_type":"markdown","source":"<H1>State Farm Distracted Driver Detection</H1>\n<h2>Importing Libraries</h2>"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport cv2 #opencv library\nimport glob\nimport matplotlib.pyplot as plt  #plotting library\nimport matplotlib.image as mpimg\nfrom sklearn.model_selection import train_test_split\nimport tensorflow\nimport random\nfrom keras.callbacks import EarlyStopping\nfrom PIL import Image\nimport h5py\nimport os\nprint(os.listdir(\"../input\"))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"directory = '../input/state-farm-distracted-driver-detection/train'\ntest_directory = '../input/state-farm-distracted-driver-detection/test/'\nrandom_test = '../input/driver/'\nclasses = ['c0','c1','c2','c3','c4','c5','c6','c7','c8','c9']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>Loading the Training dataset</h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"training_data = []\ntesting_data = []\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# creating a training dataset.\n#training_data = []\ndef create_training_data():\n    for category in classes:\n        path = os.path.join(directory,category)\n        class_num = classes.index(category)\n        \n        for img in os.listdir(path):\n            img_array = cv2.imread(os.path.join(path,img),cv2.IMREAD_GRAYSCALE)\n            new_img = cv2.resize(img_array,(240,240))\n            training_data.append([\n                new_img,class_num])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2> Loading Test dataset </h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a test dataset.\n#testing_data = []\ndef create_testing_data():        \n    for img in os.listdir(test_directory):\n        img_array = cv2.imread(os.path.join(test_directory,img),cv2.IMREAD_GRAYSCALE)\n        new_img = cv2.resize(img_array,(240,240))\n        testing_data.append([img,\n            new_img])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"create_training_data()\ncreate_testing_data()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2> Count the number of images in each subdirectory </h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Count the number of files in each subdirectory\ndef listDirectoryCounts(path):\n    d = []\n    for subdir, dirs, files in os.walk(path,topdown=False):\n        filecount = len(files)\n        dirname = subdir\n        d.append((dirname,filecount))\n    return d \n\ndef SplitCat(df):\n    for index, row in df.iterrows():\n        directory=row['Category'].split('/')\n        if directory[4]!='':\n            directory=directory[4]\n            df.at[index,'Category']=directory\n        else:\n            df.drop(index, inplace=True)\n    return\n\n\n#Get image count per category\ndirCount=listDirectoryCounts(\"../input/state-farm-distracted-driver-detection/train/\")\ncategoryInfo = pd.DataFrame(dirCount, columns=['Category','Count'])\nSplitCat(categoryInfo)\ncategoryInfo=categoryInfo.sort_values(by=['Category'])\nprint(categoryInfo.to_string(index=False))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plotting class distribution\nimg_list = pd.read_csv('../input/state-farm-distracted-driver-detection/driver_imgs_list.csv')\nimg_list['class_type'] = img_list['classname'].str.extract('(\\d)',expand=False).astype(np.float)\nplt.figure()\nimg_list.hist('class_type',alpha=0.5,layout=(1,1),bins=9)\nplt.title('class distribution')\nplt.draw()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2> Creating features and labels</h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"random.shuffle(training_data)\nx = []\ny = []\nfor features, label in training_data:\n    x.append(features)\n    y.append(label)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x[0].shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3> Display training image </h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in classes:\n    path = os.path.join(directory,i)\n    for img in os.listdir(path):\n        img_array = cv2.imread(os.path.join(path,img),cv2.IMREAD_COLOR)\n        plt.imshow(img_array, cmap='gray')\n        plt.show()\n        break\n    break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# load the image and show it\n#image = cv2.imread('../input/train/c0/img_2380.jpg',cv2.IMREAD_COLOR)\nimage = mpimg.imread('../input/state-farm-distracted-driver-detection/train/c6/img_212.jpg',cv2.IMREAD_COLOR)\nimgplot = plt.imshow(image)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3> Label Encoding </h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"y[0:20]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.utils import np_utils\ny_cat = np_utils.to_categorical(y,num_classes=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_cat[0:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = np.array(x).reshape(-1,240,240,1)\nX[0].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3> Train/Test Split </h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train,X_test,y_train,y_test = train_test_split(X,y_cat,test_size=0.3,random_state=50)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Shape of train images is:\", X_train.shape)\nprint(\"Shape of validation images is:\", X_test.shape)\nprint(\"Shape of labels is:\", y_train.shape)\nprint(\"Shape of labels is:\", y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2> Creating Model </h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 32\nfrom keras import layers\nfrom keras import models\nfrom keras import optimizers\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.preprocessing.image import img_to_array, load_img\nfrom keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout,BatchNormalization","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"model = models.Sequential()\nmodel.add(Conv2D(32, kernel_size=(3, 3),activation='relu',input_shape=(240,240,1)))\nmodel.add(BatchNormalization())\n\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.5))\n\nmodel.add(Conv2D(128, (5, 5), activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.5))\n\n#Dense\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.5))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(10, activation='softmax'))"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = models.Sequential()\n\n## CNN 1\nmodel.add(Conv2D(32,(3,3),activation='relu',input_shape=(240,240,1)))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(32,(3,3),activation='relu',padding='same'))\nmodel.add(BatchNormalization(axis = 3))\nmodel.add(MaxPooling2D(pool_size=(2,2),padding='same'))\nmodel.add(Dropout(0.2))\n\n## CNN 2\nmodel.add(Conv2D(64,(3,3),activation='relu',padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(64,(3,3),activation='relu',padding='same'))\nmodel.add(BatchNormalization(axis = 3))\nmodel.add(MaxPooling2D(pool_size=(2,2),padding='same'))\nmodel.add(Dropout(0.3))\n\n## CNN 3\nmodel.add(Conv2D(128,(3,3),activation='relu',padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(128,(3,3),activation='relu',padding='same'))\nmodel.add(BatchNormalization(axis = 3))\nmodel.add(MaxPooling2D(pool_size=(2,2),padding='same'))\nmodel.add(Dropout(0.5))\n\n## CNN 3\nmodel.add(Conv2D(256,(5,5),activation='relu',padding='same'))\nmodel.add(BatchNormalization(axis = 3))\nmodel.add(MaxPooling2D(pool_size=(2,2),padding='same'))\nmodel.add(Dropout(0.5))\n\n## Dense & Output\nmodel.add(Flatten())\nmodel.add(Dense(units = 512,activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.5))\nmodel.add(Dense(units = 128,activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(10,activation='softmax'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"callbacks = [EarlyStopping(monitor='val_acc',patience=5)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3> Fit Model</h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"results = model.fit(X_train,y_train,batch_size=batch_size,epochs=12,verbose=1,validation_data=(X_test,y_test),callbacks=callbacks)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save('C:\\\\Users\\\\sidsu\\\\Desktop\\\\Algorithms\\\\Project1.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save('Project13.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_json = model.to_json()\nwith open(\"C:\\\\Users\\\\sidsu\\\\Desktop\\\\Algorithms\\\\model.json\", \"w\") as json_file:\n    json_file.write(model_json)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b><h2>Transfer Learning</h2></b>"},{"metadata":{"trusted":true},"cell_type":"code","source":"#from keras.applications.resnet50 import ResNet50, preprocess_input\n#from keras.preprocessing.image import ImageDataGenerator\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Loading the ResNet50 model with pre-trained ImageNet weights\n#model = ResNet50(weights='imagenet', include_top=False, input_shape=(200, 200, 3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<H2> Data Augumentation </h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"#First Augument\n#train_datagen = ImageDataGenerator(rescale=1./255,   #Scale the image between 0 and 1\n#                                    rotation_range=40,\n#                                    width_shift_range=0.2,\n#                                    height_shift_range=0.2,\n#                                    shear_range=0.2,\n#                                    zoom_range=0.2,\n#                                    horizontal_flip=True,)\n#\n#val_datagen = ImageDataGenerator(rescale=1./255)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_generator = train_datagen.flow(X_train, y_train, batch_size=batch_size)\n#val_generator = val_datagen.flow(X_val, y_val, batch_size=batch_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"#get the length of the train and validation data\nntrain = len(X_train)\nprint(ntrain)\nnval = len(X_test)\nnval"},{"metadata":{"trusted":true},"cell_type":"code","source":"#FIRST MODEL\n#history = model.fit_generator(train_generator,\n#                              steps_per_epoch=ntrain // batch_size,\n#                              epochs=4,\n#                              validation_data=val_generator,\n#                              validation_steps=nval // batch_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.evaluate(X_test,y_test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot training & validation accuracy values\nplt.plot(results.history['acc'])\nplt.plot(results.history['val_acc'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\n# Plot training & validation loss values\nplt.plot(results.history['loss'])\nplt.plot(results.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = model.predict(np.array(testing_data[0][1]).reshape(-1,240,240,1))\npreds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class_idx = np.argmax(preds)\nclass_idx","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#class_output = model.output[:, class_idx]\n#class_output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Predicted: {}'.format(np.argmax(preds)))\nnew_img = cv2.resize(testing_data[0][1],(240,240))\nplt.imshow(new_img,cmap='gray')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<H2>Model Interpretability </h2>"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"image = mpimg.imread('../input/state-farm-distracted-driver-detection/test/img_8009.jpg',cv2.IMREAD_COLOR)\nimgplot = plt.imshow(image)\nplt.show()"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"from keras.preprocessing import image\nimport numpy as np\n\nimg_path = '../input/state-farm-distracted-driver-detection/test/img_8009.jpg'\nimg_tensor = cv2.imread(img_path,cv2.IMREAD_GRAYSCALE)\nnew_img1 = cv2.resize(img_tensor,(240,240))\nx1 = np.array(new_img1).reshape(-1,240,240,1)\nprint(x1.shape)"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"img_tensor.shape"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"from keras import models\n\n# Extracts the outputs of the top 8 layers:\nlayer_outputs = [layer.output for layer in model.layers[:7]]\n# Creates a model that will return these outputs, given the model input:\nactivation_model = models.Model(inputs=model.input, outputs=layer_outputs)"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# This will return a list of 5 Numpy arrays:\n# one array per layer activation\nactivations = activation_model.predict(x1)"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"first_layer_activation = activations[0]\nprint(first_layer_activation.shape)"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"\nimport matplotlib.pyplot as plt\n\nplt.matshow(first_layer_activation[0, :, :, 2], cmap='viridis')\nplt.show()"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.matshow(first_layer_activation[0, :, :, 30], cmap='viridis')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"plt.matshow(first_layer_activation[0, :, :, 30], cmap='viridis')\nplt.show()"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"plt.matshow(first_layer_activation[0, :, :, 30], cmap='viridis')\nplt.show()"},{"metadata":{"trusted":true},"cell_type":"code","source":"#!pip install keras-vis","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import specific functions from keras-vis package\n#from vis.utils import utils\n#from vis.visualization import visualize_cam, overlay","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"conv_layer"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"import keras\n\n# These are the names of the layers, so can have them as part of our plot\nlayer_names = []\nfor layer in model.layers[:15]:\n    layer_names.append(layer.name)\n\nimages_per_row = 16\n\n# Now let's display our feature maps\nfor layer_name, layer_activation in zip(layer_names, activations):\n    # This is the number of features in the feature map\n    n_features = layer_activation.shape[-1]\n\n    # The feature map has shape (1, size, size, n_features)\n    size = layer_activation.shape[1]\n\n    # We will tile the activation channels in this matrix\n    n_cols = n_features // images_per_row\n    display_grid = np.zeros((size * n_cols, images_per_row * size))\n\n    # We'll tile each filter into this big horizontal grid\n    for col in range(n_cols):\n        for row in range(images_per_row):\n            channel_image = layer_activation[0,\n                                             :, :,\n                                             col * images_per_row + row]\n            # Post-process the feature to make it visually palatable\n            channel_image -= channel_image.mean()\n            channel_image /= channel_image.std()\n            channel_image *= 64\n            channel_image += 128\n            channel_image = np.clip(channel_image, 0, 255).astype('uint8')\n            display_grid[col * size : (col + 1) * size,\n                         row * size : (row + 1) * size] = channel_image\n\n    # Display the grid\n    scale = 1. / size\n    plt.figure(figsize=(scale * display_grid.shape[1],\n                        scale * display_grid.shape[0]))\n    plt.title(layer_name)\n    plt.grid(False)\n    plt.imshow(display_grid, aspect='auto', cmap='viridis')\n    plt.savefig(layer_name)\n    \nplt.show()"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"!pip install https://github.com/OlafenwaMoses/ImageAI/releases/download/2.0.2/imageai-2.0.2-py3-none-any.whl"},{"metadata":{},"cell_type":"markdown","source":"<H1>Object detection</H1>"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"from imageai.Detection import ObjectDetection\nimport os\n\nexecution_path = os.getcwd()\nprint(execution_path)\ndetector = ObjectDetection()\ndetector.setModelTypeAsRetinaNet()\ndetector.setModelPath( os.path.join('../input/resnet/', \"resnet50_coco_best_v2.0.1.h5\"))\ndetector.loadModel()\n#detections = detector.detectObjectsFromImage(input_image=os.path.join('../input/state-farm-distracted-driver-detection/train/c0/' , \"img_195.jpg\"), output_image_path='D:/Springboard/state-farm-distracted-driver-detection/imgs/imagenew.jpg')\nreturned_image,detections = detector.detectObjectsFromImage(input_image=os.path.join('../input/state-farm-distracted-driver-detection/train/c6/' , \"img_212.jpg\"), output_type = 'array')\n#print(returned_image)\n\nfor eachObject in detections:\n   print(eachObject[\"name\"] , \" : \" , eachObject[\"percentage_probability\"] )"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"from imageai.Detection import ObjectDetection\nimport os\n\nexecution_path = os.getcwd()\nprint(execution_path)\ndetector = ObjectDetection()\ndetector.setModelTypeAsRetinaNet()\ndetector.setModelPath( os.path.join('../input/resnet/', \"resnet50_coco_best_v2.0.1.h5\"))\ndetector.loadModel()\n#detections = detector.detectObjectsFromImage(input_image=os.path.join('../input/state-farm-distracted-driver-detection/train/c0/' , \"img_195.jpg\"), output_image_path='D:/Springboard/state-farm-distracted-driver-detection/imgs/imagenew.jpg')\nreturned_image,detections = detector.detectObjectsFromImage(input_image=os.path.join('../input/state-farm-distracted-driver-detection/train/c6/' , \"img_212.jpg\"), output_type = 'array')\n#print(returned_image)\n\nfor eachObject in detections:\n   print(eachObject[\"name\"] , \" : \" , eachObject[\"percentage_probability\"] )"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}