{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# initiating gpu using tensorflow.\nimport tensorflow as tf\n#from keras.backend.tensorflow_backend import set_session\n#config = tf.ConfigProto()\n#config.gpu_options.allow_growth = True\n#config.log_device_placement = True\n\n\n#sess = tf.Session(config=config)\n#set_session(sess)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#!pip install albumentations > /dev/null","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#import albumentations","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#importing libraries for the data processing and model.\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport cv2\nimport random\nimport tensorflow as tf\nimport datetime\nfrom sklearn.model_selection import train_test_split\nfrom keras.utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Dropout, Flatten, Activation\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.datasets import cifar10\nfrom keras.utils import np_utils\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.callbacks import EarlyStopping\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import misc\nfrom keras.models import load_model\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# defining the path and classes.\ndirectory = '../input/state-farm-distracted-driver-detection/imgs/train'\ntest_directory = '../input/state-farm-distracted-driver-detection/imgs/test/'\nrandom_test = '../input/driver/'\nclasses = ['c0','c1','c2','c3','c4','c5','c6','c7','c8','c9']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# defining a shape to be used for our models.\nimg_size1 = 64\nimg_size2 = 64","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train class image for display.\nfor i in classes:\n    path = os.path.join(directory,i)\n    for img in os.listdir(path):\n        print(img)\n        img_array = cv2.imread(os.path.join(path,img),cv2.IMREAD_GRAYSCALE)\n        plt.imshow(img_array, cmap='gray')\n        plt.show()\n        break\n    break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.listdir(path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test class image for display.\ntest_array = []\nfor img in os.listdir(test_directory):\n    img_array = cv2.imread(os.path.join(test_directory,img),cv2.IMREAD_GRAYSCALE)\n    test_array = img_array\n    plt.imshow(img_array, cmap='gray')\n    plt.show()\n    break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# checkking image size using shape.\nprint(img_array.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Check preprocessing techniques","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# trying out the resize image functionality\nnew_img = cv2.resize(test_array,(img_size2,img_size1))\nplt.imshow(new_img,cmap='gray')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#flipping_horizontal (8/30add)\nhflip_img = cv2.flip(new_img, 1)\nplt.imshow(hflip_img,cmap='gray')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#flipping_vertical (8/30add)\nvflip_img = cv2.flip(new_img, 0)\nplt.imshow(vflip_img,cmap='gray')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#flipping_horizontal and vertical (8/30add)\nhvflip_img = cv2.flip(new_img, -1)\nplt.imshow(hvflip_img,cmap='gray')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#blur (8/30add)\nblur_img = cv2.blur(new_img, (5,5))\n\nplt.imshow(blur_img,cmap='gray')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Gaussian blur (8/30add)\ngau_img = cv2.GaussianBlur(new_img, (5,5), 0)\n\nplt.imshow(gau_img,cmap='gray')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#median blur (8/30add)\nmed_img = cv2.medianBlur(new_img, 5)\n\nplt.imshow(med_img,cmap='gray')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Binarization (8/30add)\nret, bin_img = cv2.threshold(new_img, 128, 255, cv2.THRESH_BINARY)\n\nplt.imshow(bin_img,cmap='gray')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Erosion (8/30add)\nkernel = np.ones((10,10), np.uint8)\nimg_el = cv2.erode(new_img, kernel, iterations=1)\n\nplt.imshow(img_el,cmap='gray')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dilation (8/30add)\nkernel = np.ones((5,5), np.uint8)\nimg_dl = cv2.dilate(new_img, kernel, iterations=1)\n\nplt.imshow(img_dl,cmap='gray')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Opening (8/30add)\nkernel = np.ones((5,5), np.uint8)\nimg_op = cv2.morphologyEx(new_img, cv2.MORPH_OPEN, kernel)\n\nplt.imshow(img_op,cmap='gray')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Closing (8/30add)\nkernel = np.ones((5,5), np.uint8)\nimg_cl = cv2.morphologyEx(new_img, cv2.MORPH_CLOSE, kernel)\n\nplt.imshow(img_cl,cmap='gray')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#GaussianNoise (8/30add)\ndef addGaussianNoise(new_img):\n    row,col= new_img.shape\n    mean = 0\n    var = 0.1\n    sigma = 100\n    gauss = np.random.normal(mean,sigma,(row,col))\n    gauss = gauss.reshape(row,col)\n    noisy = new_img + gauss\n\n    return noisy\n\n\ngau_noi_img = addGaussianNoise(new_img)\nplt.imshow(gau_noi_img,cmap='gray')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Salt_Pepper_Noise (8/30add)\ndef add_Salt_Pepper_Noise(new_img, s_vs_p = 0.5, amount = 0.05):\n    row,col = new_img.shape\n    s_and_p = np.copy(new_img)\n    # Salt mode\n    num_salt = np.ceil(amount * new_img.size * s_vs_p)\n    coords = [np.random.randint(0, i - 1, int(num_salt))\n              for i in new_img.shape]\n    s_and_p[coords] = 1\n\n    # Pepper mode\n    num_pepper = np.ceil(amount* new_img.size * (1. - s_vs_p))\n    coords = [np.random.randint(0, i - 1, int(num_pepper))\n              for i in new_img.shape]\n    s_and_p[coords] = 0\n    return s_and_p\n\nsap_noi_img = add_Salt_Pepper_Noise(new_img, s_vs_p = 0.5, amount = 0.05)\nplt.imshow(sap_noi_img,cmap='gray')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#NegaPosiDiverse (8/30add)\nngp_img = cv2.bitwise_not(new_img)\n\nplt.imshow(ngp_img,cmap='gray')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Canny (8/30add)\ncanny_img = cv2.Canny(new_img, 200, 200)\n\nplt.imshow(canny_img,cmap='gray')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Rotation\nheight,width = new_img.shape[:2]\ncenter = (int(width/2), int(height/2)) # 中心点\nangle = 45 # 左回転\nM = cv2.getRotationMatrix2D(center, angle, 1)\nrotated_img = cv2.warpAffine(new_img, M, (width, height))\nplt.imshow(rotated_img,cmap='gray')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Shifted\nmoving_x = -10\nmoving_y = -10\nM = np.float32([[1, 0, moving_x], [0, 1, moving_y]])\nshifted_img = cv2.warpAffine(new_img, M, (width, height))\nplt.imshow(shifted_img,cmap='gray')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Investigation of Data preprocessing","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Investigate the best practice for this task\n### hlflip is not necessary because the direction of driver is the same\n### In my opinion, Canny , negaposi, binaryzation are better than others.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"os.listdir(test_directory)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_directory","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"try_img_size1 = 128\ntry_img_size2 = 128","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img = 'img_58997.jpg'\n\ntest_array = []\nimg_array = cv2.imread(os.path.join(test_directory,img),cv2.IMREAD_GRAYSCALE)\ntest_array = img_array\nplt.imshow(img_array, cmap='gray')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# trying out the resize image functionality\nnew_img = cv2.resize(test_array,(try_img_size2,try_img_size1))\nplt.imshow(new_img,cmap='gray')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Canny (8/30add)\ncanny_img = cv2.Canny(new_img, 150, 150)\n\nplt.imshow(canny_img,cmap='gray')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#NegaPosiDiverse (8/30add)\nngp_img = cv2.bitwise_not(new_img)\n\nplt.imshow(ngp_img,cmap='gray')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Binarization (8/30add)\nret, bin_img = cv2.threshold(new_img, 128, 255, cv2.THRESH_BINARY)\n\nplt.imshow(bin_img,cmap='gray')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#NegaPosi -> GaussianNoise (8/30add)\ngau_noi_img = addGaussianNoise(ngp_img)\nplt.imshow(gau_noi_img,cmap='gray')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#NegaPosi -> Salt and Pepper (8/30add)\nsap_noi_img = add_Salt_Pepper_Noise(ngp_img, s_vs_p = 0.5, amount = 0.15)\nplt.imshow(sap_noi_img,cmap='gray')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Binarization (8/30add)\nret, bin_img = cv2.threshold(new_img, 128, 255, cv2.THRESH_BINARY)\n\nsap_noi_img = add_Salt_Pepper_Noise(bin_img, s_vs_p = 0.5, amount = 0.10)\nplt.imshow(sap_noi_img,cmap='gray')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Albamentation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Pre-Processing\n\n## Training data -> x4\n## a. Original, b. Negaposi, c. Original + SandP, d Negaposi + SandP\n## Test data -> no preprocessing","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# creating a training dataset.\ntraining_data = []\ni = 0\ndef create_training_data():\n    for category in classes:\n        path = os.path.join(directory,category)\n        class_num = classes.index(category)\n        \n        for img in os.listdir(path):\n            img_array = cv2.imread(os.path.join(path,img),cv2.IMREAD_GRAYSCALE)\n            new_img = cv2.resize(img_array,(img_size2,img_size1))\n            \n            #Additional preprocessing(8/30)\n            #Binarization (8/30add)\n            ret, bin_img = cv2.threshold(new_img, 128, 255, cv2.THRESH_BINARY)           \n            training_data.append([\n                bin_img,class_num])\n            #Binarization　-> Salt and Pepper\n            sap_img = add_Salt_Pepper_Noise(bin_img, s_vs_p = 0.5, amount = 0.01)\n            training_data.append([\n                sap_img,class_num])\n            #Binarization　-> Salt and Pepper\n            sap_img = add_Salt_Pepper_Noise(bin_img, s_vs_p = 0.5, amount = 0.03)\n            training_data.append([\n                sap_img,class_num])\n            #Binarization　-> Salt and Pepper\n            sap_img = add_Salt_Pepper_Noise(bin_img, s_vs_p = 0.5, amount = 0.05)\n            training_data.append([\n                sap_img,class_num])\n            #Rotation\n            height,width = new_img.shape[:2]\n            center = (int(width/2), int(height/2)) # 中心点\n            angle = 20 # 左回転\n            M = cv2.getRotationMatrix2D(center, angle, 1)\n            rotated_img = cv2.warpAffine(bin_img, M, (width, height))\n            training_data.append([\n                rotated_img,class_num])\n            #Shifted\n            moving_x = -10\n            moving_y = -10\n            M = np.float32([[1, 0, moving_x], [0, 1, moving_y]])\n            shifted_img = cv2.warpAffine(bin_img, M, (width, height))\n            training_data.append([\n                shifted_img,class_num])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for category in classes:\n    path = os.path.join(directory,category)\n    class_num = classes.index(category)\n        \n    for img in os.listdir(path):\n        img_array = cv2.imread(os.path.join(path,img),cv2.IMREAD_GRAYSCALE)\n        new_img = cv2.resize(img_array,(img_size2,img_size1))\n        training_data.append([\n            new_img,class_num])\n        \n        print('path     :', path)\n        print('img      :', img)\n        print('img_array:', img_array)\n        print('img_array_shape:', img_array.shape)\n        print('new_img  :', new_img)\n        print('new_img_shape:', new_img.shape)\n        print('class_num:', class_num)\n        \n        break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_data = []\ni = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a test dataset.\ntesting_data = []\ni = 0\ndef create_testing_data():        \n    for img in os.listdir(test_directory):\n        img_array = cv2.imread(os.path.join(test_directory,img),cv2.IMREAD_GRAYSCALE)\n        new_img = cv2.resize(img_array,(img_size2,img_size1))\n        #Binarization (8/30add)\n        ret, bin_img = cv2.threshold(new_img, 128, 255, cv2.THRESH_BINARY)          \n        testing_data.append([img,\n            bin_img])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for img in os.listdir(test_directory):\n    img_array = cv2.imread(os.path.join(test_directory,img),cv2.IMREAD_GRAYSCALE)\n    new_img = cv2.resize(img_array,(img_size2,img_size1))\n    testing_data.append([img,\n                         new_img])\n    \n    print('test_directory     :', test_directory)\n    print('img      :', img)\n    print('img_array:', img_array)\n    print('img_array_shape:', img_array.shape)\n    print('new_img  :', new_img)\n    print('new_img_shape:', new_img.shape)\n        \n    break    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testing_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testing_data = []\ni = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\nstart = time.time()\ncreate_training_data()\nprint('Elapsed_time: ', time.time()-start, '[sec]')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start = time.time()\ncreate_testing_data()\nprint('Elapsed_time: ', time.time()-start, '[sec]')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('training_data.size:', len(training_data))\nprint('testing_data.size :', len(testing_data))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random.shuffle(training_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = []\ny = []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for features, label in training_data:\n    x.append(features)\n    y.append(label)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('features: ', x[0])\nprint('label   : ', y[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x[0].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = np.array(x).reshape(-1,img_size2,img_size1,1)\nX.shape,X[0].shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"np.array.reshape(-1)とは\n\nhttps://qiita.com/yosshi4486/items/deb49d5a433a2c8a8ed4","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"np.array(x).shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train,x_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_train = np_utils.to_categorical(y_train,num_classes=10)\nY_test = np_utils.to_categorical(y_test,num_classes=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.add(Conv2D(32,kernel_size=(3,3),activation='relu',input_shape=(img_size1,img_size2,1)))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(32,kernel_size=(3,3),activation='relu',padding='same'))\nmodel.add(BatchNormalization(axis = 3))\nmodel.add(MaxPooling2D(pool_size=(2,2),padding='same'))\nmodel.add(Dropout(0.3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.add(Conv2D(64,kernel_size=(3,3),activation='relu',padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(64,kernel_size=(3,3),activation='relu',padding='same'))\nmodel.add(BatchNormalization(axis = 3))\nmodel.add(MaxPooling2D(pool_size=(2,2),padding='same'))\nmodel.add(Dropout(0.3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.add(Conv2D(128,kernel_size=(3,3),activation='relu',padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(128,kernel_size=(3,3),activation='relu',padding='same'))\nmodel.add(BatchNormalization(axis = 3))\nmodel.add(MaxPooling2D(pool_size=(2,2),padding='same'))\nmodel.add(Dropout(0.5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.add(Flatten())\nmodel.add(Dense(units = 512,activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.5))\nmodel.add(Dense(units = 128,activation='relu'))\nmodel.add(Dropout(0.25))\nmodel.add(Dense(10,activation='softmax'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(loss='categorical_crossentropy',metrics=['accuracy'],optimizer='adam')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"callbacks = [EarlyStopping(monitor='val_acc',patience=5)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 50\nn_epochs = 20","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nresults = model.fit(x_train,Y_train,batch_size=batch_size,epochs=n_epochs,verbose=1,validation_data=(x_test,Y_test),callbacks=callbacks)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot training & validation accuracy values\nplt.plot(results.history['accuracy'])\nplt.plot(results.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\n# Plot training & validation loss values\nplt.plot(results.history['loss'])\nplt.plot(results.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = model.predict(np.array(testing_data[0][1]).reshape(-1,img_size2,img_size1,1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save_weights('./driverdistraction_lr_weights.h5', overwrite=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save('./driverdistraction_lr_weights.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loaded_model = load_model('./driverdistraction_lr_weights.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data = np.array(testing_data[1001][1]).reshape(-1,img_size2,img_size1,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = model.predict(test_data)\n#preds= np.argmax(preds)\npreds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds= np.argmax(preds)\npreds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classes = {0: \"safe driving\",\n1: \"texting - right\",\n2: \"talking on the phone - right\",\n3: \"texting - left\",\n4: \"talking on the phone - left\",\n5: \"operating the radio\",\n6: \"drinking\",\n7: \"reaching behind\",\n8: \"hair and makeup\",\n9: \"talking to passenger\",\n}\n\n\nfor key,value in classes.items():\n    if preds==key:\n        predicted = value\n\npredicted     ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(predicted)\nnew_img = cv2.resize(testing_data[1000][1],(img_size2,img_size1))\nplt.imshow(new_img,cmap='gray')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n    c0: safe driving\n    c1: texting - right\n    c2: talking on the phone - right\n    c3: texting - left\n    c4: talking on the phone - left\n    c5: operating the radio\n    c6: drinking\n    c7: reaching behind\n    c8: hair and makeup\n    c9: talking to passenger\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"testing_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test=[]\ny_test=[]\n\nfor test_id, feature in testing_data:\n    x_test.append(feature)\n    y_test.append(test_id)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('features: ', x_test[0])\nprint('test_id : ', y_test[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = np.array(x_test).reshape(-1,img_size2,img_size1,1)\nX_test.shape,X_test[0].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.array(x).shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_submission(predictions, test_id, info):\n    result1 = pd.DataFrame(predictions, columns=['c0', 'c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7', 'c8', 'c9'])\n    result1.loc[:, 'img'] = pd.Series(test_id, index=result1.index)\n    result1 = result1.sort_values(['img'])\n\n    result1.to_csv(index=False)\n    return result1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"info = '200824'\nsubmission = create_submission(preds, y_test, info)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"now = datetime.datetime.now()\n\nif not os.path.isdir('subm'):\n    os.mkdir('subm')\nsuffix = info + '_' + str(now.strftime(\"%Y-%m-%d-%H-%M\"))\nsub_file = os.path.join('subm', 'submission_' + suffix + '.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}