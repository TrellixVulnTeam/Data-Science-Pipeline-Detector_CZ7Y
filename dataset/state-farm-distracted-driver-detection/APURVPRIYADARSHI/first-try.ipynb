{"cells":[{"cell_type":"markdown","metadata":{},"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.\n\nimport numpy as np\nnp.random.seed(2016)\nimport theano\n  \nimport os\nos.environ['THEANO_FLAGS'] = \"device=gpu1\"  \nimport glob\nimport cv2\nimport math\nimport pickle\nimport datetime\nimport pandas as pd\nimport statistics\n\n\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.cross_validation import KFold\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Dropout, Activation, Flatten\nfrom keras.layers.convolutional import Convolution2D, MaxPooling2D\nfrom keras.optimizers import SGD,Adam,Adamax\nfrom keras.utils import np_utils\nfrom keras.models import model_from_json\nfrom sklearn.metrics import log_loss\nfrom scipy.misc import imread, imresize\n\nuse_cache = 1\n# color type: 1 - grey, 3 - rgb\ncolor_type_global = 1\n\n\n# color_type = 1 - gray\n# color_type = 3 - RGB\ndef get_im_skipy(path, img_rows, img_cols, color_type=1):\n    # Load as grayscale\n    if color_type == 1:\n        img = imread(path, True)\n    elif color_type == 3:\n        img = imread(path)\n    # Reduce size\n    resized = imresize(img, (img_cols, img_rows))\n    return resized\n\n\ndef get_im_cv2(path, img_rows, img_cols, color_type=1):\n    # Load as grayscale\n    if color_type == 1:\n        img = cv2.imread(path, 0)\n    elif color_type == 3:\n        img = cv2.imread(path)\n    # Reduce size\n    resized = cv2.resize(img, (img_cols, img_rows))\n    return resized\n\n\ndef get_driver_data():\n    dr = dict()\n    path = os.path.join('..', 'input', 'driver_imgs_list.csv')\n    print('Read drivers data')\n    f = open(path, 'r')\n    line = f.readline()\n    while (1):\n        line = f.readline()\n        if line == '':\n            break\n        arr = line.strip().split(',')\n        dr[arr[2]] = arr[0]\n    f.close()\n    return dr\n\n\ndef load_train(img_rows, img_cols, color_type=1):\n    X_train = []\n    y_train = []\n    driver_id = []\n\n    driver_data = get_driver_data()\n\n    print('Read train images')\n    for j in range(10):\n        print('Load folder c{}'.format(j))\n        path = os.path.join('..', 'input', 'train', 'c' + str(j), '*.jpg')\n        files = glob.glob(path)\n        for fl in files:\n            flbase = os.path.basename(fl)\n            img = get_im_cv2(fl, img_rows, img_cols, color_type)\n            X_train.append(img)\n            y_train.append(j)\n            driver_id.append(driver_data[flbase])\n\n    unique_drivers = sorted(list(set(driver_id)))\n    print('Unique drivers: {}'.format(len(unique_drivers)))\n    print(unique_drivers)\n    return X_train, y_train, driver_id, unique_drivers\n\n\ndef load_test(img_rows, img_cols, color_type=1):\n    print('Read test images')\n    path = os.path.join('..', 'input', 'test', '*.jpg')\n    files = glob.glob(path)\n    X_test = []\n    X_test_id = []\n    total = 0\n    thr = math.floor(len(files)/10)\n    for fl in files:\n        flbase = os.path.basename(fl)\n        img = get_im_cv2(fl, img_rows, img_cols, color_type)\n        X_test.append(img)\n        X_test_id.append(flbase)\n        total += 1\n        if total%thr == 0:\n            print('Read {} images from {}'.format(total, len(files)))\n\n    return X_test, X_test_id\n\n\ndef cache_data(data, path):\n    if os.path.isdir(os.path.dirname(path)):\n        file = open(path, 'wb')\n        pickle.dump(data, file)\n        file.close()\n    else:\n        print('Directory doesnt exists')\n\n\ndef restore_data(path):\n    data = dict()\n    if os.path.isfile(path):\n        file = open(path, 'rb')\n        data = pickle.load(file)\n    return data\n\n\ndef save_model(model):\n    json_string = model.to_json()\n    if not os.path.isdir('cache'):\n        os.mkdir('cache')\n    open(os.path.join('cache', 'architecture.json'), 'w').write(json_string)\n    model.save_weights(os.path.join('cache', 'model_weights.h5'), overwrite=True)\n\n\ndef read_model():\n    model = model_from_json(open(os.path.join('cache', 'architecture.json')).read())\n    model.load_weights(os.path.join('cache', 'model_weights.h5'))\n    return model\n\n\ndef split_validation_set(train, target, test_size):\n    random_state = 51\n    X_train, X_test, y_train, y_test = train_test_split(train, target, test_size=test_size, random_state=random_state)\n    return X_train, X_test, y_train, y_test\n\n\ndef create_submission(predictions, test_id, info):\n    result1 = pd.DataFrame(predictions, columns=['c0', 'c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7', 'c8', 'c9'])\n    result1.loc[:, 'img'] = pd.Series(test_id, index=result1.index)\n    now = datetime.datetime.now()\n    if not os.path.isdir('subm'):\n        os.mkdir('subm')\n    suffix = info + '_' + str(now.strftime(\"%Y-%m-%d-%H-%M\"))\n    sub_file = os.path.join('subm', 'submission_' + suffix + '.csv')\n    result1.to_csv(sub_file, index=False)\n\n\ndef read_and_normalize_train_data(img_rows, img_cols, color_type=1):\n    cache_path = os.path.join('cache', 'train_r_' + str(img_rows) + '_c_' + str(img_cols) + '_t_' + str(color_type) + '.dat')\n    if not os.path.isfile(cache_path) or use_cache == 0:\n        train_data, train_target, driver_id, unique_drivers = load_train(img_rows, img_cols, color_type)\n        cache_data((train_data, train_target, driver_id, unique_drivers), cache_path)\n    else:\n        print('Restore train from cache!')\n        (train_data, train_target, driver_id, unique_drivers) = restore_data(cache_path)\n\n    train_data = np.array(train_data, dtype=np.uint8)\n    train_target = np.array(train_target, dtype=np.uint8)\n    train_data = train_data.reshape(train_data.shape[0], color_type, img_rows, img_cols)\n    train_target = np_utils.to_categorical(train_target, 10)\n    train_data = train_data.astype('float32')\n    train_data /= 255\n    print('Train shape:', train_data.shape)\n    print(train_data.shape[0], 'train samples')\n    return train_data, train_target, driver_id, unique_drivers\n\n\ndef read_and_normalize_test_data(img_rows, img_cols, color_type=1):\n    cache_path = os.path.join('cache', 'test_r_' + str(img_rows) + '_c_' + str(img_cols) + '_t_' + str(color_type) + '.dat')\n    if not os.path.isfile(cache_path) or use_cache == 0:\n        test_data, test_id = load_test(img_rows, img_cols, color_type)\n        cache_data((test_data, test_id), cache_path)\n    else:\n        print('Restore test from cache!')\n        (test_data, test_id) = restore_data(cache_path)\n\n    test_data = np.array(test_data, dtype=np.uint8)\n    test_data = test_data.reshape(test_data.shape[0], color_type, img_rows, img_cols)\n    test_data = test_data.astype('float32')\n    test_data /= 255\n    print('Test shape:', test_data.shape)\n    print(test_data.shape[0], 'test samples')\n    return test_data, test_id\n\n\ndef dict_to_list(d):\n    ret = []\n    for i in d.items():\n        ret.append(i[1])\n    return ret\n\n\ndef merge_several_folds_mean(data, nfolds):\n    a = np.array(data[0])\n    for i in range(1, nfolds):\n        a += np.array(data[i])\n    a /= nfolds\n    return a.tolist()\n\n\ndef merge_several_folds_geom(data, nfolds):\n    a = np.array(data[0])\n    for i in range(1, nfolds):\n        a *= np.array(data[i])\n    a = np.power(a, 1/nfolds)\n    return a.tolist()\n\n\ndef copy_selected_drivers(train_data, train_target, driver_id, driver_list):\n    data = []\n    target = []\n    index = []\n    for i in range(len(driver_id)):\n        if driver_id[i] in driver_list:\n            data.append(train_data[i])\n            target.append(train_target[i])\n            index.append(i)\n    data = np.array(data, dtype=np.float32)\n    target = np.array(target, dtype=np.float32)\n    index = np.array(index, dtype=np.uint32)\n    return data, target, index\n\n\ndef create_model_v1(img_rows, img_cols, color_type=1):\n    nb_classes = 10\n    # number of convolutional filters to use\n    nb_filters = 6\n    # size of pooling area for max pooling\n    nb_pool = 2\n    # convolution kernel size\n    nb_conv = 4\n    model = Sequential()\n    model.add(Convolution2D(nb_filters, nb_conv, nb_conv,\n                            border_mode='valid',\n                            input_shape=(color_type, img_rows, img_cols)))\n    model.add(Activation('relu'))\n    model.add(Convolution2D(nb_filters, nb_conv, nb_conv))\n    model.add(Activation('relu'))\n    model.add(MaxPooling2D(pool_size=(nb_pool, nb_pool)))\n    model.add(Dropout(0.7))\n\n    model.add(Flatten())\n    model.add(Dense(128))\n    model.add(Activation('relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(nb_classes))\n    model.add(Activation('softmax'))\n\n    #sgd=Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n    sgd=Adamax(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n    #sgd = SGD(lr=0.1, decay=0, momentum=0, nesterov=False)\n    model.compile(loss='categorical_crossentropy', optimizer=sgd)\n    return model\n\n\ndef run_single():\n    # input image dimensions\n    img_rows, img_cols = 40, 50\n    batch_size = 128\n    nb_epoch = 2\n    random_state = 51\n\n    train_data, train_target, driver_id, unique_drivers = read_and_normalize_train_data(img_rows, img_cols, color_type_global)\n    test_data, test_id = read_and_normalize_test_data(img_rows, img_cols, color_type_global)\n\n    yfull_train = dict()\n    yfull_test = []\n    unique_list_train = ['p002', 'p012', 'p014', 'p015', 'p016', 'p021', 'p022', 'p024',\n                     'p026', 'p035', 'p039', 'p041', 'p042', 'p045', 'p047', 'p049',\n                     'p050', 'p051', 'p052', 'p056', 'p061', 'p064', 'p066', 'p072',\n                     'p075']\n    X_train, Y_train, train_index = copy_selected_drivers(train_data, train_target, driver_id, unique_list_train)\n    unique_list_valid = ['p081']\n    X_valid, Y_valid, test_index = copy_selected_drivers(train_data, train_target, driver_id, unique_list_valid)\n\n    print('Start Single Run')\n    print('Split train: ', len(X_train), len(Y_train))\n    print('Split valid: ', len(X_valid), len(Y_valid))\n    print('Train drivers: ', unique_list_train)\n    print('Test drivers: ', unique_list_valid)\n\n    model = create_model_v1(img_rows, img_cols, color_type_global)\n    model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch,\n              show_accuracy=True, verbose=1, validation_data=(X_valid, Y_valid))\n\n    # score = model.evaluate(X_valid, Y_valid, show_accuracy=True, verbose=0)\n    # print('Score log_loss: ', score[0])\n\n    predictions_valid = model.predict(X_valid, batch_size=128, verbose=1)\n    score = log_loss(Y_valid, predictions_valid)\n    print('Score log_loss: ', score)\n\n    # Store valid predictions\n    for i in range(len(test_index)):\n        yfull_train[test_index[i]] = predictions_valid[i]\n\n    # Store test predictions\n    test_prediction = model.predict(test_data, batch_size=128, verbose=1)\n    yfull_test.append(test_prediction)\n\n    print('Final log_loss: {}, rows: {} cols: {} epoch: {}'.format(score, img_rows, img_cols, nb_epoch))\n    info_string = 'loss_' + str(score) \\\n                    + '_r_' + str(img_rows) \\\n                    + '_c_' + str(img_cols) \\\n                    + '_ep_' + str(nb_epoch)\n\n    test_res = merge_several_folds_mean(yfull_test, 1)\n    print(\"Creating submission file\")\n    create_submission(test_res, test_id, info_string)\n\n\nrun_single()"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":0}