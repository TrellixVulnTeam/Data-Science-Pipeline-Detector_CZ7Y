{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install geffnet\n!pip install git+https://github.com/pabloppp/pytorch-tools -U\n!pip install thop ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom geffnet import efficientnet_b1\nimport torch.nn as nn","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nclass CrossEntropyLabelSmoothLoss(nn.Module):\n    \"\"\"Cross entropy loss with label smoothing regularizer.\n    Reference:\n    Szegedy et al. Rethinking the Inception Architecture for Computer Vision. CVPR 2016.\n    Equation: y = (1 - epsilon) * y + epsilon / K.\n    Args:\n        num_classes (int): number of classes.\n        epsilon (float): weight.\n    \"\"\"\n\n    def __init__(self, num_classes, epsilon=0.1, use_gpu=True):\n        super(CrossEntropyLabelSmoothLoss, self).__init__()\n        self.num_classes = num_classes\n        self.epsilon = epsilon\n        self.use_gpu = use_gpu\n        self.logsoftmax = nn.LogSoftmax(dim=1)\n\n    def forward(self, inputs, targets):\n        \"\"\"\n        Args:\n            inputs: prediction matrix (before softmax) with shape (batch_size, num_classes)\n            targets: ground truth labels with shape (num_classes)\n        \"\"\"\n\n        log_probs = self.logsoftmax(inputs)\n        targets = torch.zeros(log_probs.size()).scatter_(1, targets.unsqueeze(1).cpu(), 1)\n        if self.use_gpu: targets = targets.cuda()\n        targets = (1 - self.epsilon) * targets + self.epsilon / self.num_classes\n        loss = (- targets * log_probs).mean(0).sum()\n        return loss\n    \n\nclass Flat(torch.optim.lr_scheduler._LRScheduler):\n    def __init__(self, optimizer, epochs, anneal_start=0.65, last_epoch=-1):\n        self.epochs = epochs\n        self.start = anneal_start\n        super(Flat, self).__init__(optimizer, last_epoch)\n\n    def get_lr(self):\n        if self.last_epoch < self.epochs * self.start:\n            return [base_lr for base_lr in self.base_lrs]\n        else:\n            return [\n                base_lr * (1 + math.cos(math.pi * self.last_epoch / 5)) / 2\n                for base_lr in self.base_lrs\n            ]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from fastai.vision.all import get_image_files\nimport random\nfrom sklearn.model_selection import KFold,StratifiedKFold\ndata=pd.read_csv('/kaggle/input/state-farm-distracted-driver-detection/driver_imgs_list.csv')\nimage_label_dict={}\nfor i in range(data.shape[0]):\n    info=data.iloc[i]\n    image_name=info['img']\n    label=info['classname']\n    image_label_dict[image_name]=label\nfrom fastai.vision.all import get_image_files\nimport random\nfrom sklearn.model_selection import KFold,StratifiedKFold\nnames=get_image_files('/kaggle/input/state-farm-distracted-driver-detection/imgs/train')\nrandom.shuffle(names)\nX=[]\nY=[]\nfor i in range(len(names)):\n    name=str(names[i])\n    name=name.split('/')[-1]\n    label=image_label_dict[name]\n    X.append([i])\n    Y.append (label)\n    \nsfolder = StratifiedKFold(n_splits=5,random_state=0,shuffle=True)\n\ntrain,test=next(sfolder.split(X,Y))\ntrain_name=[]\ntest_name=[]\ntrain_label=[]\ntest_label=[]\nfor i in train:\n    index=X[i][0]\n    train_name.append(names[index])\n    train_label.append(Y[i])\nfor i in test:\n    test_name.append(names[index])\n    test_label.append(Y[i])\nimport torch.nn as nn\nimport torch.utils.data as Data\nfrom PIL import Image\nimport torchvision.transforms as T\nimport torch.nn.functional as F\ntrain_transform=T.Compose([\n       T.Resize([384,384]),\n       T.RandomHorizontalFlip(),\n       T.Pad(10),\n       T.RandomCrop([384,384]),\n       T.ColorJitter(),\n       T.RandomAffine(degrees=10),\n       T.ToTensor(),\n       T.Normalize(mean=[0.485,0.456,0.406],std=[0.229,0.224,0.225])\n]\n)\ntest_transform=T.Compose([\n    T.Resize([384,384]),\n    T.ToTensor(),\n    T.Normalize(mean=[0.485,0.456,0.406],std=[0.229,0.224,0.225])\n])\n\n\nclass Dataset(Data.Dataset):\n    def __init__(self,name,label,transform):\n        super(Dataset, self).__init__()\n        self.name=name\n        self.label=label\n        self.transform=transform\n\n    def __getitem__(self, index):\n        path=str(self.name[index])\n        label=self.label[index]\n        label=int(label.strip()[-1])\n        image=Image.open(path)\n        image=self.transform(image)\n        return image,label\n\n    def __len__(self):\n        return len(self.name)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from fastai.metrics import accuracy\n\nclass Checkpoint:\n    def __init__(self, ckpt):\n        self.ckpt = ckpt\n        self.init = -1000\n\n    def __call__(self, metric, model, optimizer):\n        if metric > self.init:\n            self.init = metric\n            torch.save({'model': model.state_dict(), 'optimizer': optimizer.state_dict()}, self.ckpt)\n\ndef do_eval(model,valid_dl):\n    device=torch.device('cuda:0')\n    accs=0\n    with torch.no_grad():\n        for images,labels in valid_dl:\n            images=images.to(device)\n            labels=labels.to(device)\n            logit=model(images)\n            acc=accuracy(logit,labels)\n            accs+=acc\n    return accs/len(valid_dl)\nfrom torchtools.optim import RangerLars\nfrom torch.cuda.amp import autocast,GradScaler\nscaler=GradScaler()\nepochs=50\ntrain_dataset=Dataset(train_name,train_label,train_transform)\nval_dataset=Dataset(test_name,test_label,test_transform)\ntrain_dl=Data.DataLoader(train_dataset,batch_size=64,shuffle=True,num_workers=8)\nval_dl=Data.DataLoader(val_dataset,batch_size=64,shuffle=False,num_workers=8)\nmodel=efficientnet_b1(True)\nmodel.classifier=nn.Linear(1280,10)\noptimizer=RangerLars(model.parameters(),lr=0.001)\nscheduler=Flat(optimizer,50)\ncls_criterion=CrossEntropyLabelSmoothLoss(10)\n\n\nckpt='best.pt'\ncheckpoint=Checkpoint(ckpt)\nscaler=torch.cuda.amp.GradScaler()\ndevice=torch.device('cuda:0')\nmodel=model.to(device)\n\nfrom tqdm import tqdm\nwith tqdm(total=epochs) as pbar:\n    for epoch in range(epochs):\n        model.train()\n        for images,labels in train_dl:\n            \n            images=images.to(device)\n            labels=labels.to(device)\n            optimizer.zero_grad()\n            loss=0\n            with autocast():\n                logit=model(images)\n                loss=loss+cls_criterion(logit,labels)\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n        scheduler.step()    \n            \n        if epoch%5==0:\n            model.eval()\n            acc=do_eval(model,val_dl)\n            print('epoch:{}--acc:{}'.format(epoch,acc))\n            checkpoint(acc,model,optimizer)\n        pbar.update(1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}