{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Table of Contest**\n\n* [Can Computer Vision spot Distracted Drivers?](#chapter1)\n* [Importing Libraries](#chapter2)\n* [Loading Data](#chapter3)\n* [Defining Model Architecture](#chapter4)\n* [Training Model](#chapter5)\n* [Modifying Pretrained Resnet34 Model](#chapter6)\n* [Comparison of Models](#chapter7)","metadata":{}},{"cell_type":"markdown","source":"<h1 style=\"color:Red;\">Can Computer Vision Spot Distracted Drivers?</h1> <a class=\"anchor\"  id=\"chapter1\"></a>","metadata":{}},{"cell_type":"markdown","source":"<h3 style=\"color:Red;\">Identify the Problem:</h3>\n      What is the goal?  ->\n          The main goal is to detect the distracted drivers.<br>\n      Why do we need that?  -> \n          A scenario would be the safe driving.<br>\n          Six percent of all drivers involved in fatal crashes in 2019 were reported as distracted at the time of the crashes. \n          Nine percent of drivers 15 to 20 years old  involved in fatal crashes were reported as distracted.\n          \n   [Full Report here](https://crashstats.nhtsa.dot.gov/Api/Public/ViewPublication/813111).\n <br>\n\n<h3 style=\"color:Red;\">Success Criteria:</h3>\n    Measure the Effectiveness of the solution.<br>\n    We will use accuracy as a success criteria.<br><br>\n\n<h3 style=\"color:Red;\">Technique Used:</h3><br>\n    More precisely we want to clasify the causes of distraction.<br>\n    We have 10 classes to predict the causes of distraction.<br>\n        c0: normal driving<br>\n        c1: texting - right<br>\n        c2: talking on the phone - right<br>\n        c3: texting - left<br>\n        c4: talking on the phone - left<br>\n        c5: operating the radio<br>\n        c6: drinking<br>\n        c7: reaching behind<br>\n        c8: hair and makeup<br>\n        c9: talking to passenger<br>","metadata":{}},{"cell_type":"markdown","source":"<h1 style=\"color:Red;\">Importing Libraries</h1><a class=\"anchor\"  id=\"chapter2\"></a>","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport os \nimport torchvision\nimport torch.optim as optim\nfrom torchvision.datasets import ImageFolder\nimport torchvision.transforms as T\nfrom torch.utils.data import random_split\nfrom torch.utils.data import Dataset,DataLoader\nfrom torchvision.utils import make_grid\nimport torch.nn.functional as F\nfrom torchvision import models\nfrom tqdm.notebook import tqdm\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2022-03-27T11:34:21.376143Z","iopub.execute_input":"2022-03-27T11:34:21.376444Z","iopub.status.idle":"2022-03-27T11:34:21.386301Z","shell.execute_reply.started":"2022-03-27T11:34:21.376411Z","shell.execute_reply":"2022-03-27T11:34:21.385553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train path\ntrain_path = \"../input/state-farm-distracted-driver-detection/imgs/train\"\n# Test path\ntest_path = \"../input/state-farm-distracted-driver-detection/imgs/test\"\n\ntrain_length = 0\nfor clss in os.listdir(train_path):\n    print(\"%s size: %d\" % (clss, len(os.listdir(os.path.join(train_path, clss)))))\n    train_length += len(os.listdir(os.path.join(train_path, clss)))\nprint(\"Train size: %d\" % train_length)\nprint(\"Test Size: %d\" % len(os.listdir(test_path)))","metadata":{"execution":{"iopub.status.busy":"2022-03-27T08:44:31.5152Z","iopub.execute_input":"2022-03-27T08:44:31.515727Z","iopub.status.idle":"2022-03-27T08:44:31.582435Z","shell.execute_reply.started":"2022-03-27T08:44:31.515672Z","shell.execute_reply":"2022-03-27T08:44:31.581716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The training data is equally distributed.","metadata":{}},{"cell_type":"markdown","source":"<h1 style=\"color:Red;\">Loading Data</h1><a class=\"anchor\"  id=\"chapter3\"></a>\n\n    The Train Dataset has over 22.5k images.\n    The Test Dataset has over 80k images without labels.\n    We will use ImageFolder to load train dataset.","metadata":{}},{"cell_type":"code","source":"# Data Transforms and Augmentation\ntrain_transforms = T.Compose([ T.Resize((64, 64)),\n                               T.RandomAdjustSharpness(2),\n                               T.RandomRotation((-15, 15)),\n                               T.ColorJitter(brightness=.5, hue=.3),\n                               T.ToTensor(),\n                             ])\n\n# Loading Data using ImageFolder\ntrain_ds = ImageFolder(train_path, train_transforms)\nclasses = train_ds.classes\nprint(classes)\n\n# Splitting into train-val set\nval_pct = .1\nval_size = int(val_pct * len(train_ds))\ntrain_ds ,valid_ds = random_split(train_ds, [len(train_ds)-val_size, val_size])\n\n# Data Loader\nbatch_size = 64\ntrain_dl = DataLoader(train_ds, batch_size, shuffle = True, num_workers=2, pin_memory=True)\nvalid_dl = DataLoader(valid_ds, batch_size, num_workers=2, pin_memory=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-27T12:15:03.459445Z","iopub.execute_input":"2022-03-27T12:15:03.459786Z","iopub.status.idle":"2022-03-27T12:15:06.12094Z","shell.execute_reply.started":"2022-03-27T12:15:03.459749Z","shell.execute_reply":"2022-03-27T12:15:06.120139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Examples from train dataset**","metadata":{}},{"cell_type":"code","source":"def show_batch(dl):\n  for images,labels in dl:\n    fig, ax = plt.subplots(figsize=(8, 16))\n    ax.set_xticks([]);ax.set_yticks([])\n    ax.imshow(make_grid(images, nrow=8).permute(1, 2, 0).clamp(0, 1))\n    break","metadata":{"execution":{"iopub.status.busy":"2022-03-27T08:44:34.385457Z","iopub.execute_input":"2022-03-27T08:44:34.385701Z","iopub.status.idle":"2022-03-27T08:44:34.390836Z","shell.execute_reply.started":"2022-03-27T08:44:34.385665Z","shell.execute_reply":"2022-03-27T08:44:34.39005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_batch(train_dl)","metadata":{"execution":{"iopub.status.busy":"2022-03-27T08:44:34.392309Z","iopub.execute_input":"2022-03-27T08:44:34.392852Z","iopub.status.idle":"2022-03-27T08:44:38.499108Z","shell.execute_reply.started":"2022-03-27T08:44:34.392804Z","shell.execute_reply":"2022-03-27T08:44:38.496731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style=\"color:Red;\">Defining Model Architecture</h1><a class=\"anchor\"  id=\"chapter4\"></a>\n    \n    We will define a ConvBlock and use that ConvBlock 4 times.\n    The ConvBlocks are;\n    >Input(image) size is [64, 3, 64, 64].\n    > ConvBlock(in_channels=3, out_channels=64)(image) >>> output size is [64, 64, 32, 32].\n    > ConvBlock(64, 128)(out) >>> [64, 128, 16, 16].\n    > ConvBlock(128, 256)(out) >>> [64, 256, 8, 8].\n    > ConvBlock(256, 512)(out) >>> [64, 512, 4, 4].\n    \n    \n    After ConvBlocks we have fully connected layer.\n    > Flatten                     >>> [64, 512*4*4] = [64, 8192]\n    > Linear(512*4*4, 500)        >>> [64, 500]\n    > Linear(500, 10)             >>> [64, 10]","metadata":{}},{"cell_type":"code","source":"def ConvBlock(in_channels, out_channels):\n    \"\"\"\n        >>>conv_block = ConvBlock(3, 64)\n        >>>noise = torch.rand((32, 3, 64, 64))\n        >>>out = conv_block(noise)\n        >>>out.size()\n        \n        >>>torch.Size([1, 64, 32, 32])\n    \"\"\"\n    layers = [nn.Conv2d(in_channels, out_channels, kernel_size=2, padding=\"same\"),\n              nn.ReLU(),\n              nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n              nn.BatchNorm2d(out_channels),\n              nn.ReLU(inplace=True),\n              nn.MaxPool2d(2),\n    ]\n    return nn.Sequential(*layers)\n\nclass DistractedDriverDetectionModel(nn.Module):\n    def __init__(self, input_size=3, num_classes=10):\n        super(DistractedDriverDetectionModel, self).__init__()\n        self.model = nn.Sequential(\n            ConvBlock(in_channels=input_size, out_channels=64),\n            ConvBlock(in_channels=64, out_channels=128),\n            ConvBlock(in_channels=128, out_channels=256),\n            ConvBlock(in_channels=256, out_channels=512),\n        )\n\n        self.fully_connected_layer = nn.Sequential(\n            nn.Dropout(0.5),\n            nn.Flatten(),\n            nn.Linear(512*4*4, 500),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(500, num_classes),\n            nn.Softmax(1),\n        )\n    \n    def forward(self, x):\n        output = self.model(x)\n        output = self.fully_connected_layer(output)\n        return output","metadata":{"execution":{"iopub.status.busy":"2022-03-27T08:44:38.5005Z","iopub.execute_input":"2022-03-27T08:44:38.500807Z","iopub.status.idle":"2022-03-27T08:44:38.515228Z","shell.execute_reply.started":"2022-03-27T08:44:38.500761Z","shell.execute_reply":"2022-03-27T08:44:38.514572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style=\"color:Red;\">Training the Model</h1><a class=\"anchor\"  id=\"chapter5\"></a>\n\n    First we will set our DistractionDetection Model and Parameters.\n    After that we will train the model about 30 epochs and we'll see the results.","metadata":{}},{"cell_type":"code","source":"# Device configuration\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n\nprint(device)\n\n# Model configuration\nmodel = DistractedDriverDetectionModel(input_size=3, num_classes=10)\n\nmodel = model.to(device)","metadata":{"execution":{"iopub.status.busy":"2022-03-27T08:44:38.516972Z","iopub.execute_input":"2022-03-27T08:44:38.51752Z","iopub.status.idle":"2022-03-27T08:44:38.604219Z","shell.execute_reply.started":"2022-03-27T08:44:38.517477Z","shell.execute_reply":"2022-03-27T08:44:38.603295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set Parameters\nepochs = 30\nlr = 1e-4\nopt_func = torch.optim.RMSprop","metadata":{"execution":{"iopub.status.busy":"2022-03-27T08:44:38.6059Z","iopub.execute_input":"2022-03-27T08:44:38.606187Z","iopub.status.idle":"2022-03-27T08:44:38.610752Z","shell.execute_reply.started":"2022-03-27T08:44:38.606137Z","shell.execute_reply":"2022-03-27T08:44:38.609794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def accuracy(outputs,labels):\n  _,preds = torch.max(outputs,dim=1)\n  return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n        \n@torch.no_grad()\ndef evaluate(model, val_loader):\n    model.eval()\n    outputs = []\n    for batch in val_loader:\n        images,labels = batch\n        images = images.to(device)\n        labels = labels.to(device)\n        out = model(images)                     \n        loss = F.cross_entropy(out, labels)     \n        acc = accuracy(out, labels)             \n        outputs.append({\"val_loss\":loss.detach(), \"val_acc\":acc})\n        \n    batch_losses = [x[\"val_loss\"] for x in outputs]\n    epoch_loss = torch.stack(batch_losses).mean()     # Combine Losses\n    batch_accs = [x[\"val_acc\"] for x in outputs]\n    epoch_acc = torch.stack(batch_accs).mean()        # Combine Accuracies\n    return {\"val_loss\":epoch_loss.item(),\"val_acc\":epoch_acc.item()}\n\ntorch.cuda.empty_cache()\nhistory = []\n\n#Set up custom optimizer with weight decay\noptimizer = opt_func(model.parameters(), lr)\n\nfor epoch in range(epochs):\n    # Training Phase\n    model.train()\n    train_losses = []\n    for batch in tqdm(train_dl):\n        images, labels = batch\n        images = images.to(device)\n        labels = labels.to(device)\n        out = model(images)\n        loss_fn = nn.CrossEntropyLoss()\n        loss = loss_fn(out,labels)\n        train_losses.append(loss)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        \n    \n    # Validation Phase\n    result = evaluate(model, valid_dl)\n    result[\"train_loss\"] = torch.stack(train_losses).mean().item()\n    print(\"Epoch [{}/{}], train_loss : {:.4f}, val_loss : {:.4f}, val_acc : {:.4f}\".format(epoch, epochs,\n                                                                                         result[\"train_loss\"],\n                                                                                         result[\"val_loss\"],\n                                                                                         result[\"val_acc\"]))\n    history.append(result)","metadata":{"execution":{"iopub.status.busy":"2022-03-27T08:44:38.612614Z","iopub.execute_input":"2022-03-27T08:44:38.612919Z","iopub.status.idle":"2022-03-27T09:47:43.568347Z","shell.execute_reply.started":"2022-03-27T08:44:38.612882Z","shell.execute_reply":"2022-03-27T09:47:43.567351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 style=\"color:Red;\">Modifying Pretrained Resnet34 Model</h2><a class=\"anchor\"  id=\"chapter6\"></a>\n\n    We will modify the pretrained ResNet34 model for our case.\n    Then we will train the network about 8 epochs.","metadata":{}},{"cell_type":"code","source":"class ResNet34(nn.Module):\n    def __init__(self,num_classes,pretrained=True):\n        super().__init__()\n        # Use a pretrained model\n        self.network = models.resnet34(pretrained=pretrained)\n        # Replace last layer\n        self.network.fc = nn.Linear(self.network.fc.in_features, num_classes)\n\n    def forward(self,x):\n        return self.network(x)","metadata":{"execution":{"iopub.status.busy":"2022-03-27T09:47:43.572695Z","iopub.execute_input":"2022-03-27T09:47:43.572916Z","iopub.status.idle":"2022-03-27T09:47:43.578822Z","shell.execute_reply.started":"2022-03-27T09:47:43.572887Z","shell.execute_reply":"2022-03-27T09:47:43.578057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Device configuration\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n\n# Model configuration\npretrained_model = ResNet34(num_classes=10)\n\npretrained_model.to(device)","metadata":{"execution":{"iopub.status.busy":"2022-03-27T09:47:43.579977Z","iopub.execute_input":"2022-03-27T09:47:43.58056Z","iopub.status.idle":"2022-03-27T09:47:51.345273Z","shell.execute_reply.started":"2022-03-27T09:47:43.58052Z","shell.execute_reply":"2022-03-27T09:47:51.344294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Setting parameters\nepochs = 8\nlr = 1e-4\nopt_func = torch.optim.Adam","metadata":{"execution":{"iopub.status.busy":"2022-03-27T09:47:51.346841Z","iopub.execute_input":"2022-03-27T09:47:51.347178Z","iopub.status.idle":"2022-03-27T09:47:51.352046Z","shell.execute_reply.started":"2022-03-27T09:47:51.347137Z","shell.execute_reply":"2022-03-27T09:47:51.351332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\n    Training step\n\"\"\"\n#Set up custom optimizer with weight decay\noptimizer = opt_func(pretrained_model.parameters(), lr)\n\n# Saving results for the evaluation of the model and comperison with other model.\nhistory2 = []\n\nfor epoch in range(epochs):\n    # Training Phase\n    pretrained_model.train()\n    train_losses = []\n    lrs = []\n    for batch in tqdm(train_dl):\n        images, labels = batch\n        images = images.to(device)\n        labels = labels.to(device)\n        out = pretrained_model(images)                    \n        loss = F.cross_entropy(out, labels)\n        train_losses.append(loss)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    \n    # Validation Phase\n    result2 = evaluate(pretrained_model,valid_dl)\n    result2[\"train_loss\"] = torch.stack(train_losses).mean().item()\n    # result2[\"lrs\"] = lrs  # I won't add learning rate changes to the history\n    print(\"Epoch [{}/{}], train_loss : {:.4f}, val_loss : {:.4f}], val_acc : {:.4f}\".format(epoch, epochs,\n                                                                                         result2[\"train_loss\"],\n                                                                                         result2[\"val_loss\"],\n                                                                                         result2[\"val_acc\"]))\n    history2.append(result2)","metadata":{"execution":{"iopub.status.busy":"2022-03-27T09:47:51.353127Z","iopub.execute_input":"2022-03-27T09:47:51.353891Z","iopub.status.idle":"2022-03-27T10:05:06.390585Z","shell.execute_reply.started":"2022-03-27T09:47:51.353849Z","shell.execute_reply":"2022-03-27T10:05:06.389736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style=\"color:Red;\">Comparison of Models</h1><a class=\"anchor\"  id=\"chapter7\"></a>\n    \n    The Models got  almost same accuracy.\n    Simple Architecture got ~%98.40. The best accuracy was ~%98.69 at 26th epoch.\n    Pretrained ResNet34 Architecture got ~%98.78. The best accuracy was ~%99.09 at 7th epoch.\n    We saved these as val_acc and train_loss in history(simple architecture) and history2(pretrained ResNet34 architecture).\n    There are subtle differences between loss changes in simple architecture. If we plot these losses the line would look like straight line. ","metadata":{}},{"cell_type":"code","source":"plt.plot([*range(1, 9)], [x[\"val_acc\"]*100 for x in history2], c=\"green\")\nplt.plot(7, history2[6][\"val_acc\"]*100, marker=\"o\", color=\"red\")\nplt.title(\"Accuracy of the pretrained ResNet34 model \\n The Best acc at 7th epoch\")\nplt.text(7.1, 99.1, \"{}\".format(history2[6][\"val_acc\"]*100), c=\"red\");","metadata":{"execution":{"iopub.status.busy":"2022-03-27T10:41:30.067521Z","iopub.execute_input":"2022-03-27T10:41:30.068046Z","iopub.status.idle":"2022-03-27T10:41:30.298225Z","shell.execute_reply.started":"2022-03-27T10:41:30.068007Z","shell.execute_reply":"2022-03-27T10:41:30.297521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot([*range(1, 31)], [x[\"val_acc\"]*100 for x in history], c=\"green\")\nplt.plot(26, history[25][\"val_acc\"]*100, marker=\"o\", color=\"red\")\nplt.title(\"Accuracy of the simple model \\n The Best acc at 26th epoch\")\nplt.text(26.1, history[25][\"val_acc\"]*100 + 0.2, \"{:.2f}\".format(history[25][\"val_acc\"]*100), c=\"red\");","metadata":{"execution":{"iopub.status.busy":"2022-03-27T10:47:21.820192Z","iopub.execute_input":"2022-03-27T10:47:21.820525Z","iopub.status.idle":"2022-03-27T10:47:22.028086Z","shell.execute_reply.started":"2022-03-27T10:47:21.820489Z","shell.execute_reply":"2022-03-27T10:47:22.027375Z"},"trusted":true},"execution_count":null,"outputs":[]}]}