{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install tf-nightly\n\n!pip install imutils","metadata":{"id":"IjLExYgAbWp0","outputId":"6de61322-1344-41f0-c46e-13c4551b35b7","execution":{"iopub.status.busy":"2021-09-15T09:22:16.073568Z","iopub.execute_input":"2021-09-15T09:22:16.073892Z","iopub.status.idle":"2021-09-15T09:22:22.649448Z","shell.execute_reply.started":"2021-09-15T09:22:16.073861Z","shell.execute_reply":"2021-09-15T09:22:22.648514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Summary:\n \n - class CustomCallback(keras.callbacks.Callback) to plot training while its training so can see for every epoch how the val_score, train_score all change -- unnecessary but nice to watch\n - KFold data preprocessing by splitting according to subjects for each fold -- preprocessing only done here.. no training done \n\n\nMain functions used for training: \n- generate_revamped_set(training_data = revamped_splits[\"train\"],\n                          validation_data = revamped_splits[\"validation\"],\n                          testing_data = revamped_splits[\"test\"]) \n                          ---> returns train_data_generator, valid_data_generator, test_data_generator\n                          \n- train_revamped(model, train_data_generator, valid_data_generator, current_model_dir, trainlength, vallength, epoch) ---> returns History of training , model after training","metadata":{}},{"cell_type":"code","source":"# import shutil\n\n# for i in os.listdir(\"./\"):\n#     print(i)\n#     if (\"h5\" not in i) and (\"zip\" not in i) and (\"ipynb\" not in i):\n#         shutil.rmtree(\"./\" + i)","metadata":{"execution":{"iopub.status.busy":"2021-09-15T09:22:22.652987Z","iopub.execute_input":"2021-09-15T09:22:22.653265Z","iopub.status.idle":"2021-09-15T09:22:22.658502Z","shell.execute_reply.started":"2021-09-15T09:22:22.653232Z","shell.execute_reply":"2021-09-15T09:22:22.657656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Experimenting with feature for storing dataframes","metadata":{}},{"cell_type":"code","source":"# Pip\n!pip install feather-format\n\n# # Anaconda\n# conda install -c conda-forge feather-format","metadata":{"execution":{"iopub.status.busy":"2021-09-15T09:22:22.662365Z","iopub.execute_input":"2021-09-15T09:22:22.662615Z","iopub.status.idle":"2021-09-15T09:22:28.618793Z","shell.execute_reply.started":"2021-09-15T09:22:22.662591Z","shell.execute_reply":"2021-09-15T09:22:28.617841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import feather\n# import numpy as np\n# import pandas as pd\n\n# np.random.seed = 42\n# df_size = 10_000_000\n\n# df = pd.DataFrame({\n#     'a': np.random.rand(df_size),\n#     'b': np.random.rand(df_size),\n#     'c': np.random.rand(df_size),\n#     'd': np.random.rand(df_size),\n#     'e': np.random.rand(df_size)\n# })\n# df.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-15T09:22:28.620424Z","iopub.execute_input":"2021-09-15T09:22:28.620686Z","iopub.status.idle":"2021-09-15T09:22:28.62867Z","shell.execute_reply.started":"2021-09-15T09:22:28.620655Z","shell.execute_reply":"2021-09-15T09:22:28.627747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df.to_feather('1M.feather')","metadata":{"execution":{"iopub.status.busy":"2021-09-15T09:22:28.629959Z","iopub.execute_input":"2021-09-15T09:22:28.630321Z","iopub.status.idle":"2021-09-15T09:22:28.635021Z","shell.execute_reply.started":"2021-09-15T09:22:28.630266Z","shell.execute_reply":"2021-09-15T09:22:28.6339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df = pd.read_feather('1M.feather')\n# df","metadata":{"execution":{"iopub.status.busy":"2021-09-15T09:22:28.636549Z","iopub.execute_input":"2021-09-15T09:22:28.636926Z","iopub.status.idle":"2021-09-15T09:22:28.644243Z","shell.execute_reply.started":"2021-09-15T09:22:28.63689Z","shell.execute_reply":"2021-09-15T09:22:28.643477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df = feather.read_dataframe('1M.feather')\n# df","metadata":{"execution":{"iopub.status.busy":"2021-09-15T09:22:28.64666Z","iopub.execute_input":"2021-09-15T09:22:28.646916Z","iopub.status.idle":"2021-09-15T09:22:28.655023Z","shell.execute_reply.started":"2021-09-15T09:22:28.646892Z","shell.execute_reply":"2021-09-15T09:22:28.654184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nos.listdir(\"./\")","metadata":{"id":"tPjD0-8vdE3u","outputId":"ba84643e-598b-486b-ca99-56e20a74fc51","execution":{"iopub.status.busy":"2021-09-15T09:22:28.657928Z","iopub.execute_input":"2021-09-15T09:22:28.6586Z","iopub.status.idle":"2021-09-15T09:22:28.755687Z","shell.execute_reply.started":"2021-09-15T09:22:28.65856Z","shell.execute_reply":"2021-09-15T09:22:28.754715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Imports","metadata":{"id":"BTI8lXE1ex-x"}},{"cell_type":"code","source":"# import the necessary packages\nimport tensorflow\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras import Input\nfrom tensorflow.keras.applications import EfficientNetB7, EfficientNetB0\nfrom tensorflow.keras.applications import ResNet50, ResNet101, ResNet152, ResNet152V2\nfrom tensorflow.keras.applications import DenseNet121, DenseNet169, DenseNet201\n\n# preprocess_input_resnet, preprocess_input_resnetv2, preprocess_input_efficientnet, preprocess_input_densenet\nfrom tensorflow.keras.applications.resnet import preprocess_input as preprocess_input_resnet\nfrom tensorflow.keras.applications.resnet_v2 import preprocess_input as preprocess_input_resnetv2\nfrom tensorflow.keras.applications.efficientnet import preprocess_input as preprocess_input_efficientnet\nfrom tensorflow.keras.applications.densenet import preprocess_input as preprocess_input_densenet \n# import tensorflow.keras.applications.efficientnet.preprocess_input\n\n\nfrom tensorflow.keras.preprocessing.image import img_to_array\nfrom tensorflow.keras.preprocessing.image import load_img\n# from pyimagesearch import config\nfrom imutils import paths\nimport numpy as np\nimport pickle\nimport random\nimport os\n# # load the VGG16 network and initialize the label encoder\n# print(\"[INFO] loading network...\")\n# # model = EfficientNetB7(weights=\"imagenet\", include_top=False, input_tensor=Input(shape=(224, 224, 3)))\n# le = None","metadata":{"id":"wizQqYNvbWnk","execution":{"iopub.status.busy":"2021-09-15T09:22:28.75779Z","iopub.execute_input":"2021-09-15T09:22:28.75819Z","iopub.status.idle":"2021-09-15T09:22:30.476214Z","shell.execute_reply.started":"2021-09-15T09:22:28.758149Z","shell.execute_reply":"2021-09-15T09:22:30.475368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# https://www.pyimagesearch.com/2019/06/03/fine-tuning-with-keras-and-deep-learning/\n# import the necessary packages\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n# from tensorflow.keras.applications import VGG16\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.layers import Flatten\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import SGD\nfrom sklearn.metrics import classification_report\n# from pyimagesearch import config\nfrom imutils import paths\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os","metadata":{"id":"k_QBAJGabWle","execution":{"iopub.status.busy":"2021-09-15T09:22:30.477449Z","iopub.execute_input":"2021-09-15T09:22:30.477775Z","iopub.status.idle":"2021-09-15T09:22:30.495045Z","shell.execute_reply.started":"2021-09-15T09:22:30.477741Z","shell.execute_reply":"2021-09-15T09:22:30.494322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_training(H, N, plotPath):\n\t# construct a plot that plots and saves the training history\n\tplt.style.use(\"ggplot\")\n\tplt.figure()\n\tplt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\n\tplt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\n\tplt.plot(np.arange(0, N), H.history[\"accuracy\"], label=\"train_acc\")\n\tplt.plot(np.arange(0, N), H.history[\"val_accuracy\"], label=\"val_acc\")\n\tplt.title(\"Training Loss and Accuracy\")\n\tplt.xlabel(\"Epoch #\")\n\tplt.ylabel(\"Loss/Accuracy\")\n\tplt.legend(loc=\"lower left\")\n\tplt.savefig(plotPath)\n    \nCLASSES = ['c0', 'c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7', 'c8', 'c9']","metadata":{"id":"ZasoNQdGbWjo","execution":{"iopub.status.busy":"2021-09-15T09:22:30.49626Z","iopub.execute_input":"2021-09-15T09:22:30.496629Z","iopub.status.idle":"2021-09-15T09:22:30.504352Z","shell.execute_reply.started":"2021-09-15T09:22:30.496592Z","shell.execute_reply":"2021-09-15T09:22:30.5033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n# record = pd.read_csv(\"/content/drive/MyDrive/DAP (Actual Idiots)/driverdata/driver_imgs_list.csv\")\nrecord = pd.read_csv(\"../input/state-farm-distracted-driver-detection/driver_imgs_list.csv\")\nrecord.head()","metadata":{"id":"GMXFdsc8dE3x","outputId":"70f0c0c6-0a71-4270-d2a1-b4b217147dc2","execution":{"iopub.status.busy":"2021-09-15T09:22:30.505723Z","iopub.execute_input":"2021-09-15T09:22:30.506091Z","iopub.status.idle":"2021-09-15T09:22:30.540783Z","shell.execute_reply.started":"2021-09-15T09:22:30.506053Z","shell.execute_reply":"2021-09-15T09:22:30.539877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_subjects = list(record.groupby(\"subject\").groups.keys())","metadata":{"id":"K_euIO0xdE3y","execution":{"iopub.status.busy":"2021-09-15T09:22:30.542092Z","iopub.execute_input":"2021-09-15T09:22:30.542445Z","iopub.status.idle":"2021-09-15T09:22:30.550384Z","shell.execute_reply.started":"2021-09-15T09:22:30.54241Z","shell.execute_reply":"2021-09-15T09:22:30.549406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#creating new dataset","metadata":{"id":"bpp3sNVzb7Mg"}},{"cell_type":"code","source":"from imutils import paths\nimport shutil\nimport os\nimport time","metadata":{"id":"SVNlCrOMdE31","execution":{"iopub.status.busy":"2021-09-15T09:22:30.551679Z","iopub.execute_input":"2021-09-15T09:22:30.552131Z","iopub.status.idle":"2021-09-15T09:22:30.556843Z","shell.execute_reply.started":"2021-09-15T09:22:30.552013Z","shell.execute_reply":"2021-09-15T09:22:30.5557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from imutils import paths\nimport shutil\nimport os\nimport time\n\ntotalTrain = 13103 \ntotalVal = 5223\ntotalTest = 4098\n\n\nprint(totalTrain)\nprint(totalTest)","metadata":{"id":"oIWqsvmvbWhY","outputId":"8fabf006-28a3-4114-9877-14be092ac775","execution":{"iopub.status.busy":"2021-09-15T09:22:30.55825Z","iopub.execute_input":"2021-09-15T09:22:30.558622Z","iopub.status.idle":"2021-09-15T09:22:30.567429Z","shell.execute_reply.started":"2021-09-15T09:22:30.558585Z","shell.execute_reply":"2021-09-15T09:22:30.566404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Plotting custom callback","metadata":{"id":"uayH-gn2EyAU"}},{"cell_type":"code","source":"#### CALLBACKS\n\n#callbacks.py\nimport tensorflow as tf\n# tf.config.experimental_run_functions_eagerly(True)\n# tf.config.run_functions_eagerly\n\nfrom keras.callbacks import Callback, LearningRateScheduler\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport plotly.express as px\n\nimport tensorflow as tf\nfrom keras.layers import Conv2D, LeakyReLU, Flatten, Reshape, Conv2DTranspose, Activation\nimport keras.backend as K\nfrom keras import layers\nimport keras\nclass CustomCallback(keras.callbacks.Callback):\n    \n    def __init__(self, run_folder, initial_epoch):\n        self.epoch = initial_epoch\n        self.run_folder = run_folder\n        # self.print_every_n_batches = print_every_n_batches\n        # self.encoder = encoder\n        # self.batch = batch\n\n    def on_train_begin(self, logs={}):\n        self.logs = []\n        self.losses = []\n        self.acc = []\n        self.val_losses = []\n        self.val_acc = []\n        self.logs = []\n\n#     def on_train_batch_end(self, batch, logs=None):\n#         keys = list(logs.keys())\n#         print(\"...Training: end of batch {}; got log keys: {}\".format(batch, keys))\n#         for i in keys:\n#             print(f\"{i}: {logs.get(i)}\")\n        \n        \n    def on_epoch_end(self, epoch, logs={}):  \n        keys = list(logs.keys())\n        for i in keys:\n            print(f\"{i}: {logs.get(i)}\")\n#         print(\"keys available :{}\", keys)\n        # if batch % self.batch_range == 0:\n        self.logs.append(logs)\n        self.losses.append(logs.get('loss'))\n        self.acc.append(logs.get('accuracy'))\n        self.val_losses.append(logs.get('val_loss'))\n        self.val_acc.append(logs.get('val_accuracy'))\n\n      # Before plotting ensure at least 2 epochs have passed\n        if len(self.losses) > 0:\n\n            N = np.arange(0, len(self.losses))\n\n            # You can chose the style of your preference\n            # print(plt.style.available) to see the available options\n            #plt.style.use(\"seaborn\")\n\n            # Plot train loss, train acc, val loss and val acc against epochs passed\n            plt.figure()\n            plt.plot(N, self.losses, label = f\"train_loss = {logs.get('loss')}\")\n            plt.plot(N, self.acc, label = f\"train_acc = {logs.get('accuracy')}\")\n            plt.plot(N, self.val_losses, label = f\"val_loss = {logs.get('val_loss')}\")\n            plt.plot(N, self.val_acc, label = f\"val_acc = {logs.get('val_accuracy')}\")\n            plt.title(\"Training Loss and Accuracy [Epoch {}]\".format(epoch))\n            plt.xlabel(\"Epoch #\")\n            plt.ylabel(\"Loss/Accuracy\")\n            plt.legend()\n            # Make sure there exists a folder called output in the current directory\n            # or replace 'output' with whatever direcory you want to put in the plots\n            plt.savefig(self.run_folder + '/Epoch-{}.png'.format(epoch))\n            plt.show()\n            plt.close()\n\n#         if self.epoch % 1 == 0:\n#             valid_data_generator.reset()\n#             predIdxs = model.predict(x=valid_data_generator,\n#                 steps=(vallength // BATCH_SIZE) + 1)\n#             predIdxs = np.argmax(predIdxs, axis=1)\n#             print(classification_report(valid_data_generator.classes, predIdxs,\n#                 target_names=valid_data_generator.class_indices.keys()))\n#             from sklearn.metrics import confusion_matrix\n#             import seaborn as sns\n#             cm = confusion_matrix(test_data_generator.labels, predIdxs) \n\n#             #plotting confusion matrix\n#             plt.figure(figsize = (14,14))\n#             sns.heatmap(cm, annot = True, fmt = 'g', vmin = 0 , cmap = 'Blues')\n#             plt.xticks(ticks = np.arange(10) + 0.5, labels  = test_data_generator.class_indices, rotation = 90) #label names spaced in the middle \n#             plt.yticks(ticks = np.arange(10) + 0.5, labels  = test_data_generator.class_indices, rotation = 0)\n#             plt.xlabel(\"Predicted classes\")\n#             plt.ylabel(\"Actual classes\")\n#             plt.title('Confusion Matrix for MobileNet v2')\n\n    def on_epoch_begin(self, epoch, logs={}):\n        self.epoch += 1","metadata":{"id":"xAyEE4tdbWfa","execution":{"iopub.status.busy":"2021-09-15T09:22:30.568751Z","iopub.execute_input":"2021-09-15T09:22:30.569201Z","iopub.status.idle":"2021-09-15T09:22:31.801523Z","shell.execute_reply.started":"2021-09-15T09:22:30.569163Z","shell.execute_reply":"2021-09-15T09:22:31.800621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Createmodel!","metadata":{"id":"TDBh5GpnKllT"}},{"cell_type":"code","source":"# load the VGG16 network, ensuring the head FC layer sets are left\n# off\n\nfrom tensorflow.keras.applications import EfficientNetB7\nfrom tensorflow.keras.applications import EfficientNetB0\n\n\ndef create_model():\n#     inputs = Input(shape=(224, 224, 3))\n    # baseModel = EfficientNetB7(weights=\"imagenet\", include_top=False, input_tensor=Input(shape=(224, 224, 3)))\n#     baseModel = ResNet50(weights=\"imagenet\", include_top=False, input_tensor=Input(shape=(224, 224, 3)))\n    # baseModel = ResNet152V2(weights=\"imagenet\", include_top=False, input_tensor=Input(shape=(224, 224, 3)))\n    baseModel = DenseNet201(weights=\"imagenet\", include_top=False, input_tensor=Input(shape=(224, 224, 3)), pooling = 'avg')\n\n#     baseModel = EfficientNetB0(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3))\n    \n    # construct the head of the model that will be placed on top of the\n    # the base model\n    # Freeze the base_model\n    baseModel.trainable = False\n#     headModel = baseModel(inputs, training = False) \n    base_output = baseModel.output\n\n    \n#     headModel = layers.Conv2D(filters = 32, activation = 'relu', kernel_size = (3,3))(headModel)\n    x = layers.BatchNormalization(axis=1)\n#     x = layers.MaxPooling2D((2,2))(headModel)\n       #Dense layer \n#     x = layers.Flatten(name = \"flatten\")(x)\n\n# densenet output has 1920 units:\n    x = layers.Dense(192, activation = 'relu')(base_output)\n    x = layers.Dense(192, activation = 'relu')(base_output)\n#     x = layers.BatchNormalization(axis=1)(x)\n    x = layers.Dense(100, activation = 'relu')(x)\n    x = layers.Dense(50, activation = 'relu')(x)\n#     x = layers.Dense(25, activation = 'softmax')(x)\n    x = layers.Dense(10, activation = 'relu')(x)\n    x = layers.Dense(2, activation = 'softmax')(x)\n    \n    \n    \n#     headModel = Flatten(name=\"flatten\")(headModel)\n#     headModel = Dense(512, activation=\"relu\")(headModel)\n#     headModel = Dropout(0.5)(headModel)\n#     headModel = Dense(len(CLASSES), activation=\"softmax\")(headModel)\n    \n    \n    \n    # place the head FC model on top of the base model (this will become\n    # the actual model we will train)\n    model = Model(inputs=baseModel.input, outputs=x)\n    return model, baseModel, x\n\nimport tensorflow","metadata":{"id":"bAAD3EDGbWZa","execution":{"iopub.status.busy":"2021-09-15T09:22:31.802924Z","iopub.execute_input":"2021-09-15T09:22:31.803295Z","iopub.status.idle":"2021-09-15T09:22:31.811995Z","shell.execute_reply.started":"2021-09-15T09:22:31.803256Z","shell.execute_reply":"2021-09-15T09:22:31.811183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model, baseModel, headModel = create_model()\nmodel.layers[-10:]","metadata":{"execution":{"iopub.status.busy":"2021-09-15T09:22:31.813279Z","iopub.execute_input":"2021-09-15T09:22:31.813854Z","iopub.status.idle":"2021-09-15T09:22:37.222452Z","shell.execute_reply.started":"2021-09-15T09:22:31.813818Z","shell.execute_reply":"2021-09-15T09:22:37.221664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# baseModel = DenseNet201(weights=\"imagenet\", include_top=False, input_tensor=Input(shape=(224, 224, 3)), pooling = 'avg')\n\n# #     baseModel = EfficientNetB0(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3))\n\n# # construct the head of the model that will be placed on top of the\n# # the base model\n# # Freeze the base_model\n# baseModel.trainable = False\n# #     headModel = baseModel(inputs, training = False) \n# headModel = baseModel.output\n    \n# baseModel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-09-15T09:22:37.223786Z","iopub.execute_input":"2021-09-15T09:22:37.224164Z","iopub.status.idle":"2021-09-15T09:22:37.227983Z","shell.execute_reply.started":"2021-09-15T09:22:37.224124Z","shell.execute_reply":"2021-09-15T09:22:37.226839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #Pre-trained model used to extract features from the images \n# #Removing top lets us use it for our own classification purposes\n# pretrained_model = tf.keras.applications.MobileNetV2(\n#     input_shape = (224,224,3), \n#     include_top = False, #include top gives a classification layer for 1000 classes :O\n#     weights = 'imagenet', \n#     pooling = 'avg' #Ensures that output of pre-trained model is 1-dimensional, output is singlet vector\n    \n# )\n# pretrained_model.trainable = False #So weights of imagenet will not be changed ","metadata":{"execution":{"iopub.status.busy":"2021-09-15T09:22:37.229368Z","iopub.execute_input":"2021-09-15T09:22:37.229754Z","iopub.status.idle":"2021-09-15T09:22:37.240993Z","shell.execute_reply.started":"2021-09-15T09:22:37.229714Z","shell.execute_reply":"2021-09-15T09:22:37.240154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pretrained_model.summary()","metadata":{"execution":{"iopub.status.busy":"2021-09-15T09:22:37.242157Z","iopub.execute_input":"2021-09-15T09:22:37.242571Z","iopub.status.idle":"2021-09-15T09:22:37.249444Z","shell.execute_reply.started":"2021-09-15T09:22:37.242532Z","shell.execute_reply":"2021-09-15T09:22:37.248579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # tf.keras.Sequential\n\n# cnn2 = tf.keras.Sequential([\n#     #cnn - feature extraction \n#     pretrained_model, \n    \n#     layers.Conv2D(filters = 32, activation = 'relu', kernel_size = (3,3)),\n#     layers.BatchNormalization(axis=1),\n#     layers.MaxPooling2D((2,2)),\n#        #Dense layer \n#     layers.Flatten(), \n#     layers.Dense(64, activation = 'relu'), \n#     layers.BatchNormalization(axis=1),\n#     layers.Dense(10, activation = 'softmax')\n# ])","metadata":{"execution":{"iopub.status.busy":"2021-09-15T09:22:37.250756Z","iopub.execute_input":"2021-09-15T09:22:37.251229Z","iopub.status.idle":"2021-09-15T09:22:37.257992Z","shell.execute_reply.started":"2021-09-15T09:22:37.251194Z","shell.execute_reply":"2021-09-15T09:22:37.257137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Visualising what transformations are made:","metadata":{"id":"tDEQpQnydE37"}},{"cell_type":"markdown","source":"#revamped splits","metadata":{"id":"DAFgCuVSDCHB"}},{"cell_type":"code","source":"import pandas as pd","metadata":{"execution":{"iopub.status.busy":"2021-09-15T09:22:37.262759Z","iopub.execute_input":"2021-09-15T09:22:37.263027Z","iopub.status.idle":"2021-09-15T09:22:37.268905Z","shell.execute_reply.started":"2021-09-15T09:22:37.263003Z","shell.execute_reply":"2021-09-15T09:22:37.268031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_case_1 = \"[1,2,3]\"\ndef string_to_list(str_):\n    output = str_.strip(\"]\")\n    output = output.strip(\"[\")\n  # joined\n    sep = output.split(\", \")\n    for i in range(len(sep)):\n        sep[i] = sep[i].strip(\"'\")\n#     sep[:5]\n  # print(type(sep[:5]))\n  # print(sep[:5])\n    return sep\n# string_to_list(test_case_1)\n# string_to_list(revamped_df[\"c0\"].loc[\"train\"])\n\ndef create_split_dataframe_from_csv(driver_csv, splits_list_csv):\n    record = pd.read_csv(driver_csv)\n    record['img_path'] =  \"../input/state-farm-distracted-driver-detection/imgs/train\" + \"/\" +  record[\"classname\"] +  \"/\" + record[\"img\"]\n#     record.head()\n    # revamped_df = pd.read_csv(\"/content/drive/MyDrive/DAP (Actual Idiots)/driverdata/imgs/revamped/train_test_split_classes_4.csv\", index_col = \"index\")\n    revamped_df = pd.read_csv(splits_list_csv, index_col = \"index\")\n#     revamped_df\n    revamped_splits = {}\n    for split in (\"train\", \"test\", \"validation\"):\n        # path_current = \"/content/drive/MyDrive/DAP (Actual Idiots)/driverdata/imgs/revamped/\" + split +  \n        combined_split = []\n        for class_ in CLASSES:\n            combined_split += string_to_list(revamped_df.loc[split][class_])\n      # record[record[\"img\"].isin(common_path_img)][\"revamped_split\"] = split\n        revamped_splits[split] = record[record[\"img\"].isin(combined_split)].copy()\n\n    return revamped_splits\n    ","metadata":{"execution":{"iopub.status.busy":"2021-09-15T09:22:37.27048Z","iopub.execute_input":"2021-09-15T09:22:37.270828Z","iopub.status.idle":"2021-09-15T09:22:37.27977Z","shell.execute_reply.started":"2021-09-15T09:22:37.270788Z","shell.execute_reply":"2021-09-15T09:22:37.278689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Continuation -- flow from dataframe -- revamped set_2\n","metadata":{"id":"pOBYbtnj-8wx"}},{"cell_type":"markdown","source":"#Kfolds\n\n- https://medium.com/the-owl/k-fold-cross-validation-in-keras-3ec4a3a00538","metadata":{"id":"9wN2491I5G_o"}},{"cell_type":"code","source":"revamped_splits = create_split_dataframe_from_csv(\"../input/state-farm-distracted-driver-detection/driver_imgs_list.csv\", \n                                \"../input/splits/train_test_split_classes_4.csv\")\nrevamped_splits","metadata":{"execution":{"iopub.status.busy":"2021-09-15T09:22:37.281132Z","iopub.execute_input":"2021-09-15T09:22:37.281514Z","iopub.status.idle":"2021-09-15T09:22:37.363141Z","shell.execute_reply.started":"2021-09-15T09:22:37.28148Z","shell.execute_reply":"2021-09-15T09:22:37.36237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# revamped_splits","metadata":{"execution":{"iopub.status.busy":"2021-09-15T09:22:37.364356Z","iopub.execute_input":"2021-09-15T09:22:37.364683Z","iopub.status.idle":"2021-09-15T09:22:37.368405Z","shell.execute_reply.started":"2021-09-15T09:22:37.364646Z","shell.execute_reply":"2021-09-15T09:22:37.367338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"revamped_splits[\"train\"]","metadata":{"id":"m4ftAd5W7c-m","outputId":"94183aa6-943d-42b6-ef49-ccdb86af8e69","execution":{"iopub.status.busy":"2021-09-15T09:22:37.369661Z","iopub.execute_input":"2021-09-15T09:22:37.370204Z","iopub.status.idle":"2021-09-15T09:22:37.387586Z","shell.execute_reply.started":"2021-09-15T09:22:37.370164Z","shell.execute_reply":"2021-09-15T09:22:37.386603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(list(revamped_splits[\"train\"].columns) == list(revamped_splits[\"validation\"].columns))\nprint(list(revamped_splits[\"train\"].columns))\nprint(list(revamped_splits[\"validation\"].columns))\nrevamped_splits[\"validation\"]\n","metadata":{"id":"IgFP94477dWl","outputId":"95924448-7a66-401f-85e7-c59f6d028ecf","execution":{"iopub.status.busy":"2021-09-15T09:22:37.388722Z","iopub.execute_input":"2021-09-15T09:22:37.389061Z","iopub.status.idle":"2021-09-15T09:22:37.410022Z","shell.execute_reply.started":"2021-09-15T09:22:37.389029Z","shell.execute_reply":"2021-09-15T09:22:37.409248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##Combining train and validation sets for kfold set","metadata":{"id":"1wNcjyn6UN5N"}},{"cell_type":"code","source":"train_set_revamped = pd.concat([revamped_splits[\"train\"], revamped_splits[\"validation\"]], copy = False, ignore_index = True).reset_index().fillna(0).drop(columns = [\"index\"])\n# train_set_revamped.columns = ['subject', 'classname', 'img', 'subject_2', 'classname_2', 'img_2']\nprint(train_set_revamped.columns)\ntrain_set_revamped","metadata":{"id":"ges9p8ZP62F1","outputId":"49e5c7d3-6237-4064-a175-f10d33d5e01b","execution":{"iopub.status.busy":"2021-09-15T09:22:37.41115Z","iopub.execute_input":"2021-09-15T09:22:37.411486Z","iopub.status.idle":"2021-09-15T09:22:37.440225Z","shell.execute_reply.started":"2021-09-15T09:22:37.411452Z","shell.execute_reply":"2021-09-15T09:22:37.43934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(train_set_revamped[['subject_2', 'classname_2', 'img_2']].isin([0]).sum())\n# .value_counts)\n# print(train_set_revamped[['subject', 'classname', 'img']].isnull().sum())\nprint(train_set_revamped[['subject', 'classname', 'img']].isin([0]).sum())","metadata":{"id":"anoMi7QY8hKy","outputId":"6d2a1d94-efdd-4db6-d1ee-15a9331be9b3","execution":{"iopub.status.busy":"2021-09-15T09:22:37.441493Z","iopub.execute_input":"2021-09-15T09:22:37.44184Z","iopub.status.idle":"2021-09-15T09:22:37.451976Z","shell.execute_reply.started":"2021-09-15T09:22:37.441795Z","shell.execute_reply":"2021-09-15T09:22:37.450906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" import numpy as np\nfrom sklearn.model_selection import KFold, StratifiedKFold\nX = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\ny = np.array([1, 2, 3, 4])\nkf = KFold(n_splits=5, random_state=1, shuffle = True)\nprint(kf)\nY = train_set_revamped[['classname']]\nn = len(Y)\nprint(kf.split(np.zeros(n),Y))","metadata":{"id":"ukcpxPl15t4o","outputId":"e5cb4004-49d0-4290-f53f-1f6fedf29a1c","execution":{"iopub.status.busy":"2021-09-15T09:22:37.453473Z","iopub.execute_input":"2021-09-15T09:22:37.453963Z","iopub.status.idle":"2021-09-15T09:22:37.465345Z","shell.execute_reply.started":"2021-09-15T09:22:37.453925Z","shell.execute_reply":"2021-09-15T09:22:37.46446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###checking for repeats in kfolds -- none","metadata":{"id":"R1hkB0W3UY7U"}},{"cell_type":"code","source":"repeats_y = []\nrepeats_x = []\nskf = StratifiedKFold(n_splits = 5, random_state = 7, shuffle = True) \nfor train_index, val_index in skf.split(np.zeros(n),Y):\n  print(train_index, \"-->\", val_index)\n  repeats_y.append(set(val_index))\n  repeats_x.append(set(train_index))\n  print(len(train_index), \"-->\", len(val_index))","metadata":{"id":"SOylHZstF1jl","outputId":"9a7534b4-2bd5-45d2-8501-51f05df49d8e","execution":{"iopub.status.busy":"2021-09-15T09:22:37.466385Z","iopub.execute_input":"2021-09-15T09:22:37.466781Z","iopub.status.idle":"2021-09-15T09:22:37.518162Z","shell.execute_reply.started":"2021-09-15T09:22:37.466743Z","shell.execute_reply":"2021-09-15T09:22:37.517142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{"id":"clY5pnG2UX8F"}},{"cell_type":"code","source":"for i in range(len(repeats_y)):\n  current = repeats_y[i]\n  for j in range(i+1,len(repeats_y)):\n    print(f\"{i} ----> {repeats_y[i].difference(current)}\")","metadata":{"id":"r-pmTCvkHkz8","outputId":"7a2e70e2-d8e4-4d2c-b395-6797c151eb05","execution":{"iopub.status.busy":"2021-09-15T09:22:37.519245Z","iopub.execute_input":"2021-09-15T09:22:37.51978Z","iopub.status.idle":"2021-09-15T09:22:37.527044Z","shell.execute_reply.started":"2021-09-15T09:22:37.519734Z","shell.execute_reply":"2021-09-15T09:22:37.526206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(len(repeats_x)):\n  current = repeats_x[i]\n  for j in range(i+1,len(repeats_x)):\n    print(f\"{i} ----> {repeats_x[i].difference(current)}\")","metadata":{"id":"hTeCw5IKIkwX","outputId":"8bc83d9e-3b9a-4f2f-88b5-bb6911c3c892","execution":{"iopub.status.busy":"2021-09-15T09:22:37.52816Z","iopub.execute_input":"2021-09-15T09:22:37.528685Z","iopub.status.idle":"2021-09-15T09:22:37.53915Z","shell.execute_reply.started":"2021-09-15T09:22:37.528647Z","shell.execute_reply":"2021-09-15T09:22:37.537961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"66KjflAN5ebG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##adding paths","metadata":{"id":"drsQ_qDDdn-j"}},{"cell_type":"code","source":"# train_set_revamped['img_path'] = \"/content/drive/MyDrive/DAP (Actual Idiots)/driverdata/imgs/train/\" + train_set_revamped[\"classname\"] +  \"/\" + train_set_revamped[\"img\"]","metadata":{"id":"uuR4OWPIdqkX","execution":{"iopub.status.busy":"2021-09-15T09:22:37.542003Z","iopub.execute_input":"2021-09-15T09:22:37.542612Z","iopub.status.idle":"2021-09-15T09:22:37.546348Z","shell.execute_reply.started":"2021-09-15T09:22:37.542572Z","shell.execute_reply":"2021-09-15T09:22:37.545388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_set_revamped['img_path'] =  \"../input/state-farm-distracted-driver-detection/imgs/train\" + \"/\" +  train_set_revamped[\"classname\"] +  \"/\" + train_set_revamped[\"img\"]","metadata":{"execution":{"iopub.status.busy":"2021-09-15T09:22:37.547922Z","iopub.execute_input":"2021-09-15T09:22:37.548453Z","iopub.status.idle":"2021-09-15T09:22:37.565634Z","shell.execute_reply.started":"2021-09-15T09:22:37.548415Z","shell.execute_reply":"2021-09-15T09:22:37.564772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y = train_set_revamped[['classname']]\nprint(train_set_revamped.iloc[0][\"img_path\"])\ntrain_set_revamped","metadata":{"id":"icPEdJ6EKvWO","outputId":"f4f89477-b4d4-4fd8-f02a-1898cbbdd6dd","execution":{"iopub.status.busy":"2021-09-15T09:22:37.568605Z","iopub.execute_input":"2021-09-15T09:22:37.56889Z","iopub.status.idle":"2021-09-15T09:22:37.587329Z","shell.execute_reply.started":"2021-09-15T09:22:37.568856Z","shell.execute_reply":"2021-09-15T09:22:37.586355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time","metadata":{"id":"CV2MP6ebFP6e","execution":{"iopub.status.busy":"2021-09-15T09:22:37.588539Z","iopub.execute_input":"2021-09-15T09:22:37.588949Z","iopub.status.idle":"2021-09-15T09:22:37.593198Z","shell.execute_reply.started":"2021-09-15T09:22:37.588912Z","shell.execute_reply":"2021-09-15T09:22:37.592238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##fixing folds and saving in csv file","metadata":{"id":"v5B6J9v1Uphl"}},{"cell_type":"markdown","source":"#KFold training! - Densenet201 -- will take 5*5 folds --> ","metadata":{"id":"PzMgnKY6uJoc"}},{"cell_type":"code","source":"\n# train_data = pd.read_csv('training_labels.csv')\n# Y = train_data[['label']]\nfrom sklearn.model_selection import StratifiedKFold, KFold\n# if i want ratio 7:3 --> num of folds: int((7/3)+1)\n# if i want ratio 8:2 --> num of folds: int((8/2)+1) \n# random state 4 seems to give better proportioning of folds\nkf = KFold(n_splits = 5, random_state = 1, shuffle = True)\n                         \nskf = StratifiedKFold(n_splits = 2, random_state = 7, shuffle = True) ","metadata":{"id":"qklGMdOwu9iT","execution":{"iopub.status.busy":"2021-09-15T09:22:37.594666Z","iopub.execute_input":"2021-09-15T09:22:37.595336Z","iopub.status.idle":"2021-09-15T09:22:37.601619Z","shell.execute_reply.started":"2021-09-15T09:22:37.595146Z","shell.execute_reply":"2021-09-15T09:22:37.600633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###observing class imbalances:","metadata":{"id":"WiO7AUPI1jNc"}},{"cell_type":"code","source":"# for class_ in CLASSES:\n#     img_subject_class = train_set_revamped[train_set_revamped[\"classname\"].isin([class_])]\n#     print(f\"{class_} ----->{len(img_subject_class)} --> {(len(img_subject_class)/len(train_set_revamped))*100}\")","metadata":{"execution":{"iopub.status.busy":"2021-09-15T09:22:37.603027Z","iopub.execute_input":"2021-09-15T09:22:37.60355Z","iopub.status.idle":"2021-09-15T09:22:37.609346Z","shell.execute_reply.started":"2021-09-15T09:22:37.603387Z","shell.execute_reply":"2021-09-15T09:22:37.6084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # print(int((7/3)+1))\n# # print((7/3)+1)\n# all_subjects = list(record.groupby(\"subject\").groups.keys())\n\n\n\n# train_subjects_only = list(train_set_revamped.groupby(\"subject\").groups.keys())\n\n\n# severely_imb = []\n# moderately_imb = []\n# for sub in train_subjects_only:\n#     subject_imgs = train_set_revamped[train_set_revamped[\"subject\"].isin([sub])]\n#     print(f\"{sub}\")\n#     for class_ in CLASSES:\n#         img_subject_class = subject_imgs[subject_imgs[\"classname\"].isin([class_])]\n#         ratio = len(img_subject_class)/len(subject_imgs) *100\n#         # allow 1.5% range diff in classes:\n#         if abs(ratio-10) < 3:\n#             print(f\"     -- {class_}: {len(img_subject_class)} : ratio of all classes: {round(ratio, 2)}\")\n#         else:\n#             print(f\"RED -- {class_}: {len(img_subject_class)} : ratio of all classes: {round(ratio, 2)}\")\n\n#     # if abs(ratio-10) < 3:\n\n\n\n# print(\"old subjects len: \", len(all_subjects))\n# print(\"old subjects: \", all_subjects)\n\n# print(\"new subjects len: \", len(train_subjects_only))\n# print(\"new subjects: \", train_subjects_only)\n","metadata":{"id":"SP47fPSCvqbs","outputId":"9427a859-1ee4-4d32-fa09-7fca9ef39afd","execution":{"iopub.status.busy":"2021-09-15T09:22:37.6106Z","iopub.execute_input":"2021-09-15T09:22:37.610967Z","iopub.status.idle":"2021-09-15T09:22:37.617386Z","shell.execute_reply.started":"2021-09-15T09:22:37.61093Z","shell.execute_reply":"2021-09-15T09:22:37.616528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"current kfolds that splits according to subjects:\n- all the subjects have equal distribution of classes!\n\n","metadata":{"id":"DFqXZTm5xkKO"}},{"cell_type":"markdown","source":"","metadata":{"id":"-m5p_qbtEHAh"}},{"cell_type":"code","source":"\n# VALIDATION_ACCURACY = []\n# VALIDAITON_LOSS = []\n\n# save_dir = '/saved_models/'\n# fold_var = 0\n\n# train_data = train_set_revamped\n# # for train_index, val_index in skf.split(np.zeros(n),Y):\n\n\n# KFOLDS_TRAIN_DATA = []\n# KFOLDS_VAL_DATA = []\n# for train_index, val_index in kf.split(np.zeros(len(train_subjects_only)),train_subjects_only):\n#     fold_var += 1\n#     start = time.perf_counter()\n#     print(f\"fold_var: {fold_var}\")\n#     # print(\"subjects index: \", list(train_index))\n#     train_subjects = [train_subjects_only[i] for i in train_index]\n#     test_subjects = [train_subjects_only[i] for i in val_index]\n#     # print(\"train subjects: \",len(train_subjects))\n#     # print(\"test subjects: \",len(test_subjects))\n#     training_data = train_data[train_data[\"subject\"].isin(train_subjects)]\n#     validation_data = train_data[train_data[\"subject\"].isin(test_subjects)]\n#     KFOLDS_TRAIN_DATA.append(training_data)\n#     KFOLDS_VAL_DATA.append(validation_data)\n#     trainlength = len(training_data)\n#     vallength = len(validation_data)\n#     # print(f\"{fold_var} train data len: {len(training_data)}\")\n#     # print(f\"{fold_var} validation data len: {len(validation_data)}\")\n#     print(f\"ratio of splits : {len(training_data)/len(validation_data)}\")\n#     # print(training_data.head())\n#     # print(validation_data.head())\n\n\n","metadata":{"id":"AcuuluVn-7-P","outputId":"2018a04f-cd17-41c0-c3f6-23ea86313947","execution":{"iopub.status.busy":"2021-09-15T09:22:37.620333Z","iopub.execute_input":"2021-09-15T09:22:37.620606Z","iopub.status.idle":"2021-09-15T09:22:37.627507Z","shell.execute_reply.started":"2021-09-15T09:22:37.62058Z","shell.execute_reply":"2021-09-15T09:22:37.626691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(len(KFOLDS_TRAIN_DATA))\n# print(len(KFOLDS_VAL_DATA))\n# KFOLDS_VAL_DATA[0]","metadata":{"id":"Pk0BocSFF8Ph","outputId":"72247b51-068c-4d28-d47c-294a0c3cc1aa","execution":{"iopub.status.busy":"2021-09-15T09:22:37.628844Z","iopub.execute_input":"2021-09-15T09:22:37.62947Z","iopub.status.idle":"2021-09-15T09:22:37.63806Z","shell.execute_reply.started":"2021-09-15T09:22:37.62943Z","shell.execute_reply":"2021-09-15T09:22:37.637235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##KFOLD one by one","metadata":{"id":"-RWiVBxEFhPJ"}},{"cell_type":"code","source":"# def generated_data(training_data, validation_data):\n#     trainlength = len(training_data)\n#     vallength = len(validation_data)\n#     #     rescale=1./255, - results in less traiing and validataion accruacy of less than .1!!!\n#     trainAug = ImageDataGenerator(\n# #         rotation_range=30,\n# #       zoom_range=0.15,\n# #       width_shift_range=0.2,\n# #       height_shift_range=0.2,\n# #       shear_range=0.15,\n# #       horizontal_flip=True,\n# #       fill_mode=\"nearest\",\n#       # preprocess_input_resnet, preprocess_input_resnetv2, preprocess_input_efficientnet, preprocess_input_densenet\n#         preprocessing_function = preprocess_input_densenet)\n#     # no params for valAug since only standardisation of colors by subtracting mean:\n#     #     [123.675,116.28,103.53] -- why is the same mean order for all nets?\n#     valAug = ImageDataGenerator(preprocessing_function = preprocess_input_densenet)\n\n#     train_data_generator = trainAug.flow_from_dataframe(training_data, \n#                                                   # directory =,\n#                                                   x_col = \"img_path\", \n#                                                   y_col = \"classname\",\n#                                                   class_mode = \"categorical\",\n#                                                   target_size=(224, 224), \n#                                                   shuffle = True)\n#     valid_data_generator  = valAug.flow_from_dataframe(validation_data, \n#                                                   #  directory = image_dir,\n#                                                   x_col = \"img_path\", \n#                                                   y_col = \"classname\",\n#                                                   class_mode = \"categorical\",\n#                                                   target_size=(224, 224), \n#                                                   shuffle = True)\n#     return train_data_generator, valid_data_generator\n\n\n","metadata":{"id":"yPeeZpAPI4BG","outputId":"01930a41-9bd0-4054-bc84-d8d3ef2c0f9d","execution":{"iopub.status.busy":"2021-09-15T09:22:37.639355Z","iopub.execute_input":"2021-09-15T09:22:37.639903Z","iopub.status.idle":"2021-09-15T09:22:37.646426Z","shell.execute_reply.started":"2021-09-15T09:22:37.639867Z","shell.execute_reply":"2021-09-15T09:22:37.645566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_revamped_set(training_data = revamped_splits[\"train\"],\n                          validation_data = revamped_splits[\"validation\"],\n                          testing_data = revamped_splits[\"test\"]):\n    trainlength = len(training_data)\n    vallength = len(validation_data)\n    #     rescale=1./255, - results in less traiing and validataion accruacy of less than .1!!!\n    trainAug = ImageDataGenerator(\n#         rotation_range=30,\n#       zoom_range=0.15,\n#       width_shift_range=0.2,\n#       height_shift_range=0.2,\n#       shear_range=0.15,\n#       horizontal_flip=True,\n#       fill_mode=\"nearest\",\n      # preprocess_input_resnet, preprocess_input_resnetv2, preprocess_input_efficientnet, preprocess_input_densenet\n        preprocessing_function = preprocess_input_densenet)\n    # no params for valAug since only standardisation of colors by subtracting mean:\n    #     [123.675,116.28,103.53] -- why is the same mean order for all nets?\n    valAug = ImageDataGenerator(preprocessing_function = preprocess_input_densenet)\n\n    train_data_generator = trainAug.flow_from_dataframe(training_data, \n                                                  # directory =,\n                                                  x_col = \"img_path\", \n                                                  y_col = \"classname\",\n                                                  class_mode = \"categorical\",\n                                                  target_size=(224, 224), \n                                                  shuffle = True,\n                                                        batch_size = 32, \n                                                        seed = 1)\n    valid_data_generator  = valAug.flow_from_dataframe(validation_data, \n                                                  #  directory = image_dir,\n                                                  x_col = \"img_path\", \n                                                  y_col = \"classname\",\n                                                  class_mode = \"categorical\",\n                                                  target_size=(224, 224), \n                                                  shuffle = True,\n                                                       batch_size = 32, \n                                                          seed = 1)\n    test_data_generator  = valAug.flow_from_dataframe(testing_data, \n                                                  #  directory = image_dir,\n                                                  x_col = \"img_path\", \n                                                  y_col = \"classname\",\n                                                  class_mode = \"categorical\",\n                                                  target_size=(224, 224), \n                                                  shuffle = False,\n                                                      batch_size = 32, \n                                                         seed = 1)\n    \n    return train_data_generator, valid_data_generator, test_data_generator\n\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-09-15T09:22:37.647777Z","iopub.execute_input":"2021-09-15T09:22:37.648335Z","iopub.status.idle":"2021-09-15T09:22:37.659995Z","shell.execute_reply.started":"2021-09-15T09:22:37.64827Z","shell.execute_reply":"2021-09-15T09:22:37.659173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###model_summary","metadata":{"id":"tg3n4UTndCW5"}},{"cell_type":"markdown","source":"# Train Function","metadata":{}},{"cell_type":"code","source":"# def train(model,class_weight, fold_var, train_data_generator, valid_data_generator, current_model_dir, trainlength, vallength, epoch):\n#     BATCH_SIZE = 32\n\n#     # compile our model (this needs to be done after our setting our\n#     # layers to being non-trainable\n#     print(\"[INFO] compiling model...\")\n#     opt = SGD(lr=1e-4, momentum=0.9)\n#     model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n#       metrics=[\"accuracy\"])\n#     # train the head of the network for a few epochs (all other layers\n#     # are frozen) -- this will allow the new FC layers to start to become\n#     # initialized with actual \"learned\" values versus pure random\n#     print(\"[INFO] training head...\")\n#     my_callbacks = [\n#         # tf.keras.callbacks.LearningRateScheduler(scheduler),\n#         tf.keras.callbacks.EarlyStopping(monitor = \"val_accuracy\", min_delta = 0.01, patience = 10, restore_best_weights = True),\n#         tensorflow.keras.callbacks.ModelCheckpoint(\n#     #         filepath='./models/model.{epoch:02d}-{val_loss:.2f}.h5',\n#                                                   filepath= current_model_dir + f\"/models/fold_{fold_var}\"+ \"/cp.{epoch:02d}-{val_loss:.2f}.ckpt\",\n#                                                     save_weights_only=True,\n#                                                     verbose=1),\n#         tensorflow.keras.callbacks.TensorBoard(log_dir= current_model_dir + '/logs'),\n#         CustomCallback(run_folder = current_model_dir + \"/plots\", initial_epoch = 0, fold_var = fold_var)\n#     ]\n\n\n#     H = model.fit(\n#       x=train_data_generator,\n#       steps_per_epoch=trainlength // BATCH_SIZE,\n#       validation_data=valid_data_generator,\n#       validation_steps=vallength // BATCH_SIZE,\n#         class_weight = class_weight,\n#       epochs=epoch,\n#         callbacks=my_callbacks)\n\n\n#     # # CREATE CALLBACKS\n#     # checkpoint = tf.keras.callbacks.ModelCheckpoint(save_dir+ current_model_name + \"_\" + fold_var, \n#     # \t\t\t\t\t\tmonitor='val_accuracy', verbose=1, \n#     # \t\t\t\t\t\tsave_best_only=True, mode='max')\n#     # callbacks_list = [checkpoint]\n#     # # There can be other callbacks, but just showing one because it involves the model name\n#     # # This saves the best model\n#     # # FIT THE MODEL\n#     # history = model.fit(train_data_generator,\n#     # \t\t    epochs=num_epochs,\n#     # \t\t    callbacks=callbacks_list,\n#     # \t\t    validation_data=valid_data_generator)\n#     # #PLOT HISTORY\n#     # #\t\t:\n#     # #\t\t:\n\n#     # # LOAD BEST MODEL to evaluate the performance of the model\n#     # model.load_weights(\"/saved_models/model_\"+str(fold_var)+\".h5\")\n\n#     # results = model.evaluate(valid_data_generator)\n#     # results = dict(zip(model.metrics_names,results))\n\n#     # VALIDATION_ACCURACY.append(results['accuracy'])\n#     # VALIDATION_LOSS.append(results['loss'])\n\n#     # tf.keras.backend.clear_session()\n\n#     end = time.perf_counter()\n#     print(f\"fold {fold_var} took {end - start} seconds\")\n#     return H\n\n\n   \n    \n","metadata":{"id":"i6-eD9T7Fcik","outputId":"771650a7-d759-4819-af1d-74b6dd7a9af6","execution":{"iopub.status.busy":"2021-09-15T09:22:37.662361Z","iopub.execute_input":"2021-09-15T09:22:37.663071Z","iopub.status.idle":"2021-09-15T09:22:37.670675Z","shell.execute_reply.started":"2021-09-15T09:22:37.663031Z","shell.execute_reply":"2021-09-15T09:22:37.669735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_revamped(model,class_weight, train_data_generator, valid_data_generator, current_model_dir, trainlength, vallength, epoch, patience = 10):\n    BATCH_SIZE = 32\n\n    # compile our model (this needs to be done after our setting our\n    # layers to being non-trainable\n    print(\"[INFO] compiling model...\")\n    opt = SGD(lr=1e-4, momentum=0.9)\n    model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n      metrics=[\"accuracy\"])\n    # train the head of the network for a few epochs (all other layers\n    # are frozen) -- this will allow the new FC layers to start to become\n    # initialized with actual \"learned\" values versus pure random\n    print(\"[INFO] training head...\")\n    my_callbacks = [\n        # tf.keras.callbacks.LearningRateScheduler(scheduler),\n        tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = patience,\n                                           restore_best_weights = True),\n        tensorflow.keras.callbacks.ModelCheckpoint(\n    #         filepath='./models/model.{epoch:02d}-{val_loss:.2f}.h5',\n                                                  filepath= current_model_dir + f\"/models\"+ \"/cp.{epoch:02d}-{val_loss:.2f}.ckpt\",\n                                                    save_weights_only=True,\n                                                    verbose=1),\n        tensorflow.keras.callbacks.TensorBoard(log_dir= current_model_dir + '/logs'),\n        CustomCallback(run_folder = current_model_dir + \"/plots\", initial_epoch = 0)\n    ]\n\n\n    H = model.fit(\n      x=train_data_generator,\n      steps_per_epoch=trainlength // BATCH_SIZE,\n      validation_data=valid_data_generator,\n      validation_steps=vallength // BATCH_SIZE,\n      epochs=epoch,\n        callbacks=my_callbacks)\n\n\n    # # CREATE CALLBACKS\n    # checkpoint = tf.keras.callbacks.ModelCheckpoint(save_dir+ current_model_name + \"_\" + fold_var, \n    # \t\t\t\t\t\tmonitor='val_accuracy', verbose=1, \n    # \t\t\t\t\t\tsave_best_only=True, mode='max')\n    # callbacks_list = [checkpoint]\n    # # There can be other callbacks, but just showing one because it involves the model name\n    # # This saves the best model\n    # # FIT THE MODEL\n    # history = model.fit(train_data_generator,\n    # \t\t    epochs=num_epochs,\n    # \t\t    callbacks=callbacks_list,\n    # \t\t    validation_data=valid_data_generator)\n    # #PLOT HISTORY\n    # #\t\t:\n    # #\t\t:\n\n    # # LOAD BEST MODEL to evaluate the performance of the model\n    # model.load_weights(\"/saved_models/model_\"+str(fold_var)+\".h5\")\n\n    # results = model.evaluate(valid_data_generator)\n    # results = dict(zip(model.metrics_names,results))\n\n    # VALIDATION_ACCURACY.append(results['accuracy'])\n    # VALIDATION_LOSS.append(results['loss'])\n\n    # tf.keras.backend.clear_session()\n\n    end = time.perf_counter()\n#     print(f\"fold {fold_var} took {end - start} seconds\")\n    return H, model","metadata":{"execution":{"iopub.status.busy":"2021-09-15T09:22:37.672187Z","iopub.execute_input":"2021-09-15T09:22:37.672621Z","iopub.status.idle":"2021-09-15T09:22:37.684344Z","shell.execute_reply.started":"2021-09-15T09:22:37.672582Z","shell.execute_reply":"2021-09-15T09:22:37.683492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(revamped_splits)","metadata":{"execution":{"iopub.status.busy":"2021-09-15T09:22:37.685828Z","iopub.execute_input":"2021-09-15T09:22:37.686244Z","iopub.status.idle":"2021-09-15T09:22:37.693092Z","shell.execute_reply.started":"2021-09-15T09:22:37.686207Z","shell.execute_reply":"2021-09-15T09:22:37.692218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model.summary()\ndef save_modelh5(name, model):\n    saving_file = f'{name}.h5' \n    model.save_weights(saving_file)\n    return saving_file","metadata":{"execution":{"iopub.status.busy":"2021-09-15T09:22:37.694884Z","iopub.execute_input":"2021-09-15T09:22:37.69513Z","iopub.status.idle":"2021-09-15T09:22:37.702294Z","shell.execute_reply.started":"2021-09-15T09:22:37.695102Z","shell.execute_reply":"2021-09-15T09:22:37.701497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Balancing training with class weights\n\n- https://datascience.stackexchange.com/questions/13490/how-to-set-class-weights-for-imbalanced-classes-in-keras recommended \n- https://scikit-learn.org/stable/modules/generated/sklearn.utils.class_weight.compute_class_weight.html uses n_samples / (n_classes * np.bincount(y)) equation for balancing","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_weight_training = \"no\"\nfrom sklearn.utils import class_weight\ny_train = revamped_splits[\"train\"][\"classname\"]\nclass_weights = None\n\nif class_weight_training == \"yes\":\n    class_weights = class_weight.compute_class_weight('balanced',\n                                                 classes = np.unique(y_train),\n                                                 y =y_train)\n\n# _ = class_weights\n# class_weights = dict(zip(CLASSES, _))\nprint(class_weights)\n","metadata":{"execution":{"iopub.status.busy":"2021-09-15T09:22:37.704432Z","iopub.execute_input":"2021-09-15T09:22:37.704765Z","iopub.status.idle":"2021-09-15T09:22:37.713488Z","shell.execute_reply.started":"2021-09-15T09:22:37.704739Z","shell.execute_reply":"2021-09-15T09:22:37.712674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# # class_weights\n# # counter = 0\n# classes_len = []\n# for class_ in CLASSES:\n#     current = revamped_splits[\"train\"]\n#     class_img = current[current[\"classname\"].isin([class_])]\n#     classes_len.append(len(class_img))\n#     print(f\"class {class_} --> {len(class_img)} --percentage {round(len(class_img)/len(current)*100, 2)} -- assigned {round(class_weights[class_], 2)} ---- after multiplying: {round(class_weights[class_]* len(class_img), 2)}\")\n# #     counter += 1\n\n# dict_ = zip(CLASSES, classes_len)","metadata":{"execution":{"iopub.status.busy":"2021-09-15T09:22:37.714711Z","iopub.execute_input":"2021-09-15T09:22:37.715452Z","iopub.status.idle":"2021-09-15T09:22:37.719289Z","shell.execute_reply.started":"2021-09-15T09:22:37.715413Z","shell.execute_reply":"2021-09-15T09:22:37.718392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import plotly.express as px\n# # data_canada = px.data.gapminder().query(\"country == 'Canada'\")\n# fig = px.bar(x=CLASSES, y=classes_len)\n# fig.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-15T09:22:37.720813Z","iopub.execute_input":"2021-09-15T09:22:37.721397Z","iopub.status.idle":"2021-09-15T09:22:37.727193Z","shell.execute_reply.started":"2021-09-15T09:22:37.721196Z","shell.execute_reply":"2021-09-15T09:22:37.726218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# SMOTE SAMPLING: \n\nhttps://medium.com/swlh/how-to-use-smote-for-dealing-with-imbalanced-image-dataset-for-solving-classification-problems-3aba7d2b9cad\n\nhttps://www.kaggle.com/saumandas/smote-with-inceptionresnet/output  \n\nhttps://towardsdatascience.com/upgrade-your-image-classifier-with-balanced-data-ddea93859c0f\n\nhttps://towardsdatascience.com/smote-synthetic-data-augmentation-for-tabular-data-1ce28090debc ","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training from revamped folder","metadata":{}},{"cell_type":"code","source":"import os\nrevamp_training_dir = \"./revamped\"\nsections = ['plots', 'logs', 'models']\n# shutil.rmtree(\"./revamped\")\nif not os.path.exists(revamp_training_dir):\n    os.mkdir(revamp_training_dir)\nfor i in sections:\n    sec_path = revamp_training_dir + \"/\" + i\n    if not os.path.exists(sec_path):\n        os.mkdir(sec_path)\nos.listdir(revamp_training_dir)","metadata":{"execution":{"iopub.status.busy":"2021-09-15T09:22:37.728484Z","iopub.execute_input":"2021-09-15T09:22:37.729071Z","iopub.status.idle":"2021-09-15T09:22:37.740752Z","shell.execute_reply.started":"2021-09-15T09:22:37.729031Z","shell.execute_reply":"2021-09-15T09:22:37.739907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fold_var = 0\ntraining_data = revamped_splits[\"train\"]\ntrainlength = len(training_data)\nvalidation_data = revamped_splits[\"validation\"]\nvallength = len(validation_data)\ntesting_data = revamped_splits[\"test\"]\ntestlength = len(testing_data)\ntrain_data_generator, valid_data_generator, test_data_generator = generate_revamped_set(training_data, \n                                                                                 validation_data,\n                                                                                testing_data)\ncurrent_model_dir = \"./revamped\"\n# H_fold1 = train_revamped(model, fold_var, train_data_generator, valid_data_generator, current_model_dir, trainlength, vallength)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-09-15T09:21:56.544411Z","iopub.execute_input":"2021-09-15T09:21:56.544832Z","iopub.status.idle":"2021-09-15T09:21:56.611509Z","shell.execute_reply.started":"2021-09-15T09:21:56.544728Z","shell.execute_reply":"2021-09-15T09:21:56.609924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model, baseModel, headModel = create_model()\n  # loop over all layers in the base model and freeze them so they will\n  # *not* be updated during the first training process\nfor layer in baseModel.layers:\n    layer.trainable = False\n    \n# def train_revamped(model,class_weight, train_data_generator, valid_data_generator, current_model_dir, trainlength, vallength, epoch):\nH, model = train_revamped(model,class_weights, train_data_generator, valid_data_generator, current_model_dir, trainlength, vallength, epoch = 50, patience = 10)\n\n\nname = f\"densenet201_2D\"\nsave_modelh5(name, model)","metadata":{"execution":{"iopub.status.busy":"2021-09-15T09:21:56.612772Z","iopub.status.idle":"2021-09-15T09:21:56.613366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2021-09-14T10:51:27.994371Z","iopub.status.idle":"2021-09-14T10:51:27.99517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Finetuning it further:","metadata":{}},{"cell_type":"code","source":"# # fold_var = 0\n# training_data = revamped_splits[\"train\"]\n# trainlength = len(training_data)\n# validation_data = revamped_splits[\"validation\"]\n# vallength = len(validation_data)\n# testing_data = revamped_splits[\"test\"]\n# testlength = len(testing_data)\n# train_data_generator, valid_data_generator, test_data_generator = generate_revamped_set(training_data, \n#                                                                                  validation_data,\n#                                                                                 testing_data)\n# current_model_dir = \"./revamped\"\n\n# print(\"Creating model for finetuning....\")\n# model_2, baseModel_2, headModel_2 = create_model()\n# # model.summary()\n\n# # print(\"Creating model....\")\n# # model_2, baseModel_2, headModel_2 = create_model()\n# # checkpoint_dir = \"./revamped/models\"\n# # latest = tf.train.latest_checkpoint(checkpoint_dir)\n# # model_2.load_weights(latest)\n# model_2.load_weights(\"../input/model-2/densenet201_6.h5\")\n\n# model_2.layers\n# print(\"[INFO] compiling model before finetuning training...\")\n# opt = SGD(lr=1e-4, momentum=0.9)\n# model_2.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n#   metrics=[\"accuracy\"])","metadata":{"execution":{"iopub.status.busy":"2021-09-14T10:51:27.996458Z","iopub.status.idle":"2021-09-14T10:51:27.997319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# trainable = []\n# # at first only last two layers trainable while rest of basemodel was trainable:\n# for layer in model_2.layers:\n#     trainable.append(layer.trainable)\n    \n# print(\"initial: \",len(trainable), trainable[-10:])\n# # model_2.trainable = False\n# # unfreezing half of the basemodel:\n# len_basemodel = 708\n# trainable_current = []\n# for trainable_layer in baseModel_2.layers[-354:]:\n#     trainable_layer.trainable = True\n#     trainable_current.append(layer.trainable)\n    \n# print(\"final: \",len(trainable_current), trainable_current)\n","metadata":{"execution":{"iopub.status.busy":"2021-09-14T10:51:27.998549Z","iopub.status.idle":"2021-09-14T10:51:27.999368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# print(\"[INFO] compiling model before finetuning training...\")\n# opt = SGD(lr=1e-4, momentum=0.9)\n# model_2.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n#   metrics=[\"accuracy\"])","metadata":{"execution":{"iopub.status.busy":"2021-09-14T10:51:28.000559Z","iopub.status.idle":"2021-09-14T10:51:28.001367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(\"[INFO]FINETUNING...\")\n# H, model = train_revamped(model,class_weights, train_data_generator, valid_data_generator, current_model_dir, trainlength, vallength, epoch = 30)\n# name = f\"densenet201_6_FINETUNED\"\n# save_modelh5(name, model)","metadata":{"execution":{"iopub.status.busy":"2021-09-14T10:51:28.002667Z","iopub.status.idle":"2021-09-14T10:51:28.003467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # fold_var = 0\n# training_data = revamped_splits[\"train\"]\n# trainlength = len(training_data)\n# validation_data = revamped_splits[\"validation\"]\n# vallength = len(validation_data)\n# testing_data = revamped_splits[\"test\"]\n# testlength = len(testing_data)\n# train_data_generator, valid_data_generator, test_data_generator = generate_revamped_set(training_data, \n#                                                                                  validation_data,\n#                                                                                 testing_data)","metadata":{"execution":{"iopub.status.busy":"2021-09-14T10:51:28.004664Z","iopub.status.idle":"2021-09-14T10:51:28.005462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n# # model.compile(optimizer, loss)\n# print(\"Evaluating....\")\n# scores = model.evaluate(valid_data_generator)\n# #                           ,vallength) #1514 testing images\n# print(\"Accuracy for valid set = \", scores[1])\n# scores = model.evaluate(test_data_generator)\n# #                           ,testlength) #1514 testing images\n# print(\"Accuracy test= \", scores[1])\n\n# BATCH_SIZE = 32\n# test_data_generator.reset()\n# predIdxs = model.predict(x=test_data_generator,\n# \tsteps=(testlength // BATCH_SIZE) + 1)\n# predIdxs_2 = np.argmax(predIdxs, axis=1)\n# print(\"Weighted Densenet201 classification report:\")\n# print(classification_report(test_data_generator.classes, predIdxs_2,\n# \ttarget_names=test_data_generator.class_indices.keys()))","metadata":{"execution":{"iopub.status.busy":"2021-09-14T10:51:28.006671Z","iopub.status.idle":"2021-09-14T10:51:28.007464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n# from sklearn.metrics import confusion_matrix\n# import seaborn as sns\n# from matplotlib.pyplot import figure\n# # sns.color_palette(\"crest\", as_cmap=True)\n\n# class_dict = {\"c0\": \"safe driving\",\n# \"c1\": \"texting - right\",\n# \"c2\": \"talking on the phone - right\",\n# \"c3\": \"texting - left\",\n# \"c4\": \"talking on the phone - left\",\n# \"c5\": \"operating the radio\",\n# \"c6\": \"drinking\",\n# \"c7\": \"reaching behind\",\n# \"c8\": \"hair and makeup\",\n# \"c9\": \"talking to passenger\",}\n# labels_ = []\n# for i in  test_data_generator.class_indices:\n#     labels_.append(class_dict[i])\n# print(labels_)\n\n\n# cm = confusion_matrix(test_data_generator.labels, predIdxs_2) \n\n# #plotting confusion matrix\n# plt.figure(figsize = (6,6))\n# sns.heatmap(cm, annot = True, fmt = 'g', vmin = 0 , cmap = 'Blues'\n# #             sns.cubehelix_palette(start=.5, rot=-.5, as_cmap=True)\n#            )\n# plt.xticks(ticks = np.arange(10) + 0.5, labels  =labels_ , rotation = 90) #label names spaced in the middle \n# plt.yticks(ticks = np.arange(10) + 0.5, labels  = labels_, rotation = 0)\n# plt.xlabel(\"Predicted classes\")\n# plt.ylabel(\"Actual classes\")\n# plt.title('Confusion Matrix Finetuned Weighted Densenet201')\n\n","metadata":{"execution":{"iopub.status.busy":"2021-09-14T10:51:28.008683Z","iopub.status.idle":"2021-09-14T10:51:28.009504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating folders for folds training","metadata":{}},{"cell_type":"code","source":"# import shutil\n# shutil.rmtree(\"./folds_training\")\n\n# os.mkdir(\"./folds_training\")","metadata":{"execution":{"iopub.status.busy":"2021-09-14T10:51:28.0107Z","iopub.status.idle":"2021-09-14T10:51:28.011508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# current_model_dir = \"./folds_training\"\n# print(current_model_dir)\n# if not os.path.exists(current_model_dir):\n#     os.mkdir(current_model_dir)\n# print(os.listdir(current_model_dir))\n# folders_in_model = [\"/models\", \"/dataaug\", \"/logs\", \"/plots\"]\n\n\n# for path in folders_in_model:\n#     curr = current_model_dir + path\n#   # # for deleting previous folders in model dir:\n#   # shutil.rmtree(curr)\n#     if not os.path.exists(curr):\n#         os.mkdir(curr)\n\n\n# folds = [f\"{current_model_dir}/models/fold_{i}\" for i in range(5)]\n# for fold in folds:\n#     if not os.path.exists(fold):\n#         os.mkdir(fold)\n        \n# print(os.listdir(f\"{current_model_dir}/models\"))\n# print(os.listdir(current_model_dir))","metadata":{"execution":{"iopub.status.busy":"2021-09-14T10:51:28.012704Z","iopub.status.idle":"2021-09-14T10:51:28.013496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Folds training","metadata":{}},{"cell_type":"markdown","source":"## Folds class weights calculation","metadata":{}},{"cell_type":"code","source":"# class_weight_training = \"yes\"\n# from sklearn.utils import class_weight\n# # y_train = revamped_splits[\"train\"][\"classname\"]\n# # class_weights = None\n\n# # if class_weight_training == \"yes\":\n# #     class_weights = class_weight.compute_class_weight('balanced',\n# #                                                  np.unique(y_train),\n# #                                                  y_train)\n\n# # _ = class_weights\n# # class_weights = dict(zip(CLASSES, _))\n# # print(class_weights)\n# def calculate_class_weights(y_train):\n#     class_weights = class_weight.compute_class_weight('balanced',\n#                                                  classes = np.unique(y_train),\n#                                                  y = y_train)\n#     _ = class_weights\n#     classes_ = [i for i in range(10)]\n#     class_weights = dict(zip(classes_, _))\n#     print(class_weights)\n#     return class_weights, _\n\n# class_weights, list_weights = calculate_class_weights(revamped_splits[\"train\"][\"classname\"])\n\n\n# counter = 0\n# for class_ in CLASSES:\n#     current = revamped_splits[\"train\"]\n#     class_img = current[current[\"classname\"].isin([class_])]\n#     print(f\"class {class_} --> {len(class_img)} --percentage {round(len(class_img)/len(current)*100, 2)} -- assigned {round(class_weights[counter], 2)}\")\n#     counter += 1\n\n    \n# kfold_class_weights = []\n# for fold_var in range(5):\n# #     print(f\"fold_var: {fold_var}\")\n# #     model, baseModel, headModel = create_model()\n# #       # loop over all layers in the base model and freeze them so they will\n# #       # *not* be updated during the first training process\n# #     for layer in baseModel.layers:\n# #       layer.trainable = False\n# #     if fold_var == 0:\n# #       # CREATE NEW MODEL\n# #       print(\"creating brand new model.....\")\n\n# #     else:\n# #       print(f\"loading saved model {fold_var - 1}.....\")\n# #       # print(os.listdir(current_model_dir + \"/\" + \"models\" ))\n# #       checkpoint_dir = current_model_dir + \"/models/fold_\" + fold_var-1\n# #       latest = tf.train.latest_checkpoint(checkpoint_dir)\n# #       model.load_weights(latest)\n\n#     training_data = KFOLDS_TRAIN_DATA[fold_var]\n#     trainlength = len(training_data)\n#     validation_data = KFOLDS_VAL_DATA[fold_var]\n#     vallength = len(validation_data)\n#     print(f\"{fold_var} --> {trainlength}, {vallength}\")\n#     class_weights, list_weights = calculate_class_weights(training_data[\"classname\"])\n#     kfold_class_weights.append(class_weights)\n#     counter = 0\n#     for class_ in CLASSES:\n#         current = training_data\n#         class_img = current[current[\"classname\"].isin([class_])]\n#         print(f\"    class {class_} --> {len(class_img)} --percentage {round(len(class_img)/len(current)*100, 2)} -- assigned {round(class_weights[counter], 2)}\")\n#         counter += 1\n","metadata":{"execution":{"iopub.status.busy":"2021-09-14T10:51:28.014742Z","iopub.status.idle":"2021-09-14T10:51:28.015545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(kfold_class_weights[0]))","metadata":{"execution":{"iopub.status.busy":"2021-09-14T10:51:28.016727Z","iopub.status.idle":"2021-09-14T10:51:28.017541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fold_training = \"yes\"\n# load_model_input = \"yes\"\n# if fold_training == \"yes\":\n# #     fold_var = 0\n#     for fold_var in range(1,5):\n#         print(f\"fold_var: {fold_var}\")\n#         model, baseModel, headModel = create_model()\n#           # loop over all layers in the base model and freeze them so they will\n#           # *not* be updated during the first training process\n#         for layer in baseModel.layers:\n#           layer.trainable = False\n#         if (fold_var == 0):\n#           # CREATE NEW MODEL\n#           print(\"creating brand new model.....\")\n#         elif (load_model_input == \"yes\"):\n#             model.load_weights(\"../input/fold0model-2/densenet201_fold0_2.h5\")\n# #             model_2.load_weights(\"../input/model-2/densenet201_6.h5\")\n#             print(\"[INFO] compiling model...\")\n#             opt = SGD(lr=1e-4, momentum=0.9)\n#             model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n#               metrics=[\"accuracy\"])\n#             # model.compile(optimizer, loss)\n            \n#             load_model_input = \"no\"\n#             print(\"load_model_input updated to no!\")\n#         else:\n#             print(f\"loading saved model {fold_var - 1}.....\")\n#             # print(os.listdir(current_model_dir + \"/\" + \"models\" ))\n#             checkpoint_dir = current_model_dir + \"/models/fold_\" + f\"{fold_var-1}_2\"\n#             latest = tf.train.latest_checkpoint(checkpoint_dir)\n#             model.load_weights(latest)\n#             print(\"[INFO] compiling model...\")\n#             opt = SGD(lr=1e-4, momentum=0.9)\n#             model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n#               metrics=[\"accuracy\"])\n#             # model.compile(optimizer, loss)\n\n#         training_data = KFOLDS_TRAIN_DATA[fold_var]\n#         trainlength = len(training_data)\n#         validation_data = KFOLDS_VAL_DATA[fold_var]\n#         vallength = len(validation_data)\n        \n        \n#         print(f\"fold_var: {fold_var} --> traininglen {trainlength} vallength {vallength}\")\n#         train_data_generator, valid_data_generator = generated_data(training_data, validation_data)\n#         class_weights = kfold_class_weights[fold_var]\n#         H_fold1 = train(model,class_weights, fold_var, train_data_generator, valid_data_generator, current_model_dir, trainlength, vallength, 30)\n#         name = f\"densenet201_fold{fold_var}_2\"\n#         save_modelh5(name, model)","metadata":{"execution":{"iopub.status.busy":"2021-09-14T10:51:28.018779Z","iopub.status.idle":"2021-09-14T10:51:28.019577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# current_model_dir = \".\"\n# fold_var = 1\n# print(current_model_dir + \"/models/fold_\" + f\"{fold_var-1}\")\n","metadata":{"execution":{"iopub.status.busy":"2021-09-14T10:51:28.020778Z","iopub.status.idle":"2021-09-14T10:51:28.02156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# os.listdir(\"./\")","metadata":{"execution":{"iopub.status.busy":"2021-09-14T10:51:28.022755Z","iopub.status.idle":"2021-09-14T10:51:28.023548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fold_var = 1\n# model, baseModel, headModel = create_model()\n#   # loop over all layers in the base model and freeze them so they will\n#   # *not* be updated during the first training process\n# for layer in baseModel.layers:\n#     layer.trainable = False\n# if fold_var == 0:\n#   # CREATE NEW MODEL\n#   print(\"creating brand new model.....\")\n\n# else:\n#     print(f\"loading saved model {fold_var - 1}.....\")\n#     # print(os.listdir(current_model_dir + \"/\" + \"models\" ))\n#     checkpoint_dir = current_model_dir + f\"/models/fold_{fold_var-1}\"\n#     latest = tf.train.latest_checkpoint(checkpoint_dir)\n#     model.load_weights(latest)\n# fold_var = 1\n# training_data = KFOLDS_TRAIN_DATA[fold_var]\n# trainlength = len(training_data)\n# validation_data = KFOLDS_VAL_DATA[fold_var]\n# vallength = len(validation_data)\n# train_data_generator, valid_data_generator = generated_data(training_data, validation_data)\n# current_model_dir = \".\"\n# # H_fold1 = train(model, fold_var, train_data_generator, valid_data_generator, current_model_dir, trainlength, vallength)","metadata":{"execution":{"iopub.status.busy":"2021-09-14T10:51:28.024735Z","iopub.status.idle":"2021-09-14T10:51:28.025559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# name = f\"densenet201_fold{fold_var}\"\n# save_modelh5(name, model)","metadata":{"execution":{"iopub.status.busy":"2021-09-14T10:51:28.02678Z","iopub.status.idle":"2021-09-14T10:51:28.027574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fold_var = 2\n# model, baseModel, headModel = create_model()\n#   # loop over all layers in the base model and freeze them so they will\n#   # *not* be updated during the first training process\n# for layer in baseModel.layers:\n#     layer.trainable = False\n# if fold_var == 0:\n#   # CREATE NEW MODEL\n#   print(\"creating brand new model.....\")\n\n# else:\n#     print(f\"loading saved model {fold_var - 1}.....\")\n#     # print(os.listdir(current_model_dir + \"/\" + \"models\" ))\n#     checkpoint_dir = current_model_dir + f\"/models/fold_{fold_var-1}\"\n#     latest = tf.train.latest_checkpoint(checkpoint_dir)\n#     model.load_weights(latest)\n# fold_var = 1\n# training_data = KFOLDS_TRAIN_DATA[fold_var]\n# trainlength = len(training_data)\n# validation_data = KFOLDS_VAL_DATA[fold_var]\n# vallength = len(validation_data)\n# train_data_generator, valid_data_generator = generated_data(training_data, validation_data)\n# current_model_dir = \".\"\n# # H_fold1 = train(model, fold_var, train_data_generator, valid_data_generator, current_model_dir, trainlength, vallength)","metadata":{"execution":{"iopub.status.busy":"2021-09-14T10:51:28.028958Z","iopub.status.idle":"2021-09-14T10:51:28.029818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# name = f\"densenet201_fold{fold_var}\"\n# save_modelh5(name, model)","metadata":{"execution":{"iopub.status.busy":"2021-09-14T10:51:28.031213Z","iopub.status.idle":"2021-09-14T10:51:28.032203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fold_var = 3\n# model, baseModel, headModel = create_model()\n#   # loop over all layers in the base model and freeze them so they will\n#   # *not* be updated during the first training process\n# for layer in baseModel.layers:\n#     layer.trainable = False\n# if fold_var == 0:\n#   # CREATE NEW MODEL\n#   print(\"creating brand new model.....\")\n\n# else:\n#     print(f\"loading saved model {fold_var - 1}.....\")\n#     # print(os.listdir(current_model_dir + \"/\" + \"models\" ))\n#     checkpoint_dir = current_model_dir + f\"/models/fold_{fold_var-1}\"\n#     latest = tf.train.latest_checkpoint(checkpoint_dir)\n#     model.load_weights(latest)\n# fold_var = 1\n# training_data = KFOLDS_TRAIN_DATA[fold_var]\n# trainlength = len(training_data)\n# validation_data = KFOLDS_VAL_DATA[fold_var]\n# vallength = len(validation_data)\n# train_data_generator, valid_data_generator = generated_data(training_data, validation_data)\n# current_model_dir = \".\"\n# # H_fold1 = train(model, fold_var, train_data_generator, valid_data_generator, current_model_dir, trainlength, vallength)","metadata":{"id":"oy0SZy4UaQnL","outputId":"84f73347-f1ac-4fd8-f7c4-22c84ac12af1","execution":{"iopub.status.busy":"2021-09-14T10:51:28.033581Z","iopub.status.idle":"2021-09-14T10:51:28.034392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# name = f\"densenet201_fold{fold_var}\"\n# save_modelh5(name, model)","metadata":{"execution":{"iopub.status.busy":"2021-09-14T10:51:28.035594Z","iopub.status.idle":"2021-09-14T10:51:28.036406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n","metadata":{"id":"elFrjc7771E0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluating previous models:","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# fold_var = 0\ntraining_data = revamped_splits[\"train\"]\ntrainlength = len(training_data)\nvalidation_data = revamped_splits[\"validation\"]\nvallength = len(validation_data)\ntesting_data = revamped_splits[\"test\"]\ntestlength = len(testing_data)\ntrain_data_generator, valid_data_generator, test_data_generator = generate_revamped_set(training_data, \n                                                                                 validation_data,\n                                                                                testing_data)\ncurrent_model_dir = \"./revamped\"","metadata":{"execution":{"iopub.status.busy":"2021-09-14T10:51:28.037615Z","iopub.status.idle":"2021-09-14T10:51:28.038458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Creating model....\")\nmodel_2, baseModel_2, headModel_2 = create_model()\n# model.summary()","metadata":{"execution":{"iopub.status.busy":"2021-09-14T10:51:28.039652Z","iopub.status.idle":"2021-09-14T10:51:28.040448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Creating model....\")\n# model_2, baseModel_2, headModel_2 = create_model()\n# checkpoint_dir = \"./revamped/models\"\n# latest = tf.train.latest_checkpoint(checkpoint_dir)\n# model_2.load_weights(latest)\n\n\nmodel_2.load_weights(\"../input/model/densenet201_5.h5\")\nprint(\"[INFO] compiling model...\")\nopt = SGD(lr=1e-4, momentum=0.9)\nmodel_2.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n  metrics=[\"accuracy\"])\n\nprint(\"Evaluating....densenet201_fold4.h5\")\nscores = model_2.evaluate(valid_data_generator)\n#                           ,vallength) #1514 testing images\nprint(\"Accuracy for valid set = \", scores[1])\nscores = model_2.evaluate(test_data_generator)\n#                           ,testlength) #1514 testing images\nprint(\"Accuracy test= \", scores[1])","metadata":{"id":"_BRM06y1xpNi","outputId":"10e1df31-f445-4e3f-d5f7-fec521971bf1","execution":{"iopub.status.busy":"2021-09-14T10:51:28.041664Z","iopub.status.idle":"2021-09-14T10:51:28.042491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from keract import display_activations\n# # The image path\n# img_path = \"../input/distracteddriversrevampeddataset/test-revamped/test/c0/img_100796.jpg\"\n# # '../images/brain-1.jpg'\n# # Preprocessing the image for the model\n# target_size = (224,224)\n# preprocess_input_densenet()\n# x = preprocess_image(img_path=img_path,model=model_2,resize=target_size)\n# # Generate the activations \n# activations = get_activations(model, x)","metadata":{"execution":{"iopub.status.busy":"2021-09-14T10:51:28.043669Z","iopub.status.idle":"2021-09-14T10:51:28.044469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install keract","metadata":{"execution":{"iopub.status.busy":"2021-09-14T10:51:28.045632Z","iopub.status.idle":"2021-09-14T10:51:28.046421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import numpy as np\n# from tensorflow.keras import Input, Model\n# from tensorflow.keras.layers import Dense, concatenate\n# from keract import get_activations\n# import keract\n# # model definition\n# i1 = Input(shape=(10,), name='i1')\n# i2 = Input(shape=(10,), name='i2')\n\n# a = Dense(1, name='fc1')(i1)\n# b = Dense(1, name='fc2')(i2)\n\n# c = concatenate([a, b], name='concat')\n# d = Dense(1, name='out')(c)\n# model = Model(inputs=[i1, i2], outputs=[d])\n\n# # inputs to the model\n# x = [np.random.uniform(size=(32, 10)), np.random.uniform(size=(32, 10))]\n\n# # call to fetch the activations of the model.\n# activations = get_activations(model, x, auto_compile=True)\n\n# # print the activations shapes.\n# [print(k, '->', v.shape, '- Numpy array') for (k, v) in activations.items()]\n\n# # Print output:\n# # i1 -> (32, 10) - Numpy array\n# # i2 -> (32, 10) - Numpy array\n# # fc1 -> (32, 1) - Numpy array\n# # fc2 -> (32, 1) - Numpy array\n# # concat -> (32, 2) - Numpy array\n# out -> (32, 1) - Numpy array","metadata":{"execution":{"iopub.status.busy":"2021-09-14T10:51:28.047618Z","iopub.status.idle":"2021-09-14T10:51:28.048459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# keract.display_activations(activations, cmap=None, save=False, directory='.', data_format='channels_last', fig_size=(24, 24), reshape_1d_layers=False)\n","metadata":{"execution":{"iopub.status.busy":"2021-09-14T10:51:28.049634Z","iopub.status.idle":"2021-09-14T10:51:28.050437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from PIL import Image\n\n# # im = Image.open(\"../input/distracteddriversrevampeddataset/test-revamped/test/c0/img_100796.jpg\")\n\n# # width, height = im.size\n \n# # # Setting the points for cropped image\n# # left = 4\n# # top = height / 5\n# # right = 154\n# # bottom = 3 * height / 5\n \n# # # Cropped image of above dimension\n# # # (It will not change original image)\n# # im1 = im.crop((left, top, right, bottom))\n\n\n\n# # newsize = (224,224)\n# # im1 = im1.resize(newsize)\n# # # Shows the image in image viewer\n# # im1.show()\n\n\n# # image = Image.open('../input/distracteddriversrevampeddataset/test-revamped/test/c0/img_100796.jpg')\n# # new_image = image.resize((400, 400))\n# # new_image.show()\n\n\n# from skimage.io import imread, imshow\n# import matplotlib.pyplot as plt\n# %matplotlib inline\n\n# image_gray = imread('../input/distracteddriversrevampeddataset/test-revamped/test/c0/img_100796.jpg', as_gray=False)\n# imshow(image_gray)","metadata":{"execution":{"iopub.status.busy":"2021-09-14T10:51:28.051641Z","iopub.status.idle":"2021-09-14T10:51:28.052442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from skimage.transform import resize\n# img = imread('../input/distracteddriversrevampeddataset/test-revamped/test/c0/img_100796.jpg')\n# #resize image\n# img_resized = resize(img, (224, 224))\n\n# #plot images\n# plt.subplot(121), imshow(img)\n# plt.title('Original Image')\n# plt.subplot(122), imshow(img_resized)\n# plt.title('Resized Image')\n# plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-14T10:51:28.053639Z","iopub.status.idle":"2021-09-14T10:51:28.054439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # call to fetch the activations of the model.img_resized\n# activations = get_activations(model_2, img_resized, auto_compile=True)","metadata":{"execution":{"iopub.status.busy":"2021-09-14T10:51:28.055691Z","iopub.status.idle":"2021-09-14T10:51:28.056505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# keract.display_activations(activations, cmap=None, save=False, directory='.', data_format='channels_last', fig_size=(24, 24), reshape_1d_layers=False)\n","metadata":{"execution":{"iopub.status.busy":"2021-09-14T10:51:28.05768Z","iopub.status.idle":"2021-09-14T10:51:28.05847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nBATCH_SIZE = 32\ntest_data_generator.reset()\npredIdxs = model_2.predict(x=test_data_generator,\n\tsteps=(testlength // BATCH_SIZE) + 1)\npredIdxs_2 = np.argmax(predIdxs, axis=1)\nprint(\"Non-Weighted Densenet201 classification report:\")\nprint(classification_report(test_data_generator.classes, predIdxs_2,\n\ttarget_names=test_data_generator.class_indices.keys()))","metadata":{"execution":{"iopub.status.busy":"2021-09-14T10:51:28.059673Z","iopub.status.idle":"2021-09-14T10:51:28.060469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nfrom matplotlib.pyplot import figure\n# sns.color_palette(\"crest\", as_cmap=True)\n","metadata":{"execution":{"iopub.status.busy":"2021-09-14T10:51:28.061641Z","iopub.status.idle":"2021-09-14T10:51:28.062456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nclass_dict = {\"c0\": \"safe driving\",\n\"c1\": \"texting - right\",\n\"c2\": \"talking on the phone - right\",\n\"c3\": \"texting - left\",\n\"c4\": \"talking on the phone - left\",\n\"c5\": \"operating the radio\",\n\"c6\": \"drinking\",\n\"c7\": \"reaching behind\",\n\"c8\": \"hair and makeup\",\n\"c9\": \"talking to passenger\",}\nlabels_ = []\nfor i in  test_data_generator.class_indices:\n    labels_.append(class_dict[i])\nprint(labels_)","metadata":{"execution":{"iopub.status.busy":"2021-09-14T10:51:28.063637Z","iopub.status.idle":"2021-09-14T10:51:28.06445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ncm = confusion_matrix(test_data_generator.labels, predIdxs_2) \n\n#plotting confusion matrix\nplt.figure(figsize = (6,6))\nsns.heatmap(cm, annot = True, fmt = 'g', vmin = 0 , cmap = 'Blues'\n#             sns.cubehelix_palette(start=.5, rot=-.5, as_cmap=True)\n           )\nplt.xticks(ticks = np.arange(10) + 0.5, labels  =labels_ , rotation = 90) #label names spaced in the middle \nplt.yticks(ticks = np.arange(10) + 0.5, labels  = labels_, rotation = 0)\nplt.xlabel(\"Predicted classes\")\nplt.ylabel(\"Actual classes\")\nplt.title('Confusion Matrix Weighted Densenet201 fold_training')\n","metadata":{"execution":{"iopub.status.busy":"2021-09-14T10:51:28.065662Z","iopub.status.idle":"2021-09-14T10:51:28.066449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # print(len(test_data_generator.labels))\n# # print(len(predIdxs_2))\n# class_dict = {\"c0\": \"safe driving\",\n# \"c1\": \"texting - right\",\n# \"c2\": \"talking on the phone - right\",\n# \"c3\": \"texting - left\",\n# \"c4\": \"talking on the phone - left\",\n# \"c5\": \"operating the radio\",\n# \"c6\": \"drinking\",\n# \"c7\": \"reaching behind\",\n# \"c8\": \"hair and makeup\",\n# \"c9\": \"talking to passenger\",}\n# # test_data_generator.class_indices.keys()","metadata":{"execution":{"iopub.status.busy":"2021-09-14T10:51:28.067649Z","iopub.status.idle":"2021-09-14T10:51:28.06849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Class Activation maps:\n\n\nhttps://towardsdatascience.com/demystifying-convolutional-neural-networks-using-class-activation-maps-fe94eda4cef1\n\nhttps://www.tensorflow.org/tutorials/keras/classification","metadata":{}},{"cell_type":"code","source":"# for idx in range(10):\n#     features_for_one_img = features[idx, :, :, :]\n#     height_roomout = X_train.shape[1]/features_for_one_img.shape[0]\n#     width_roomout = X_train.shape[2]/features_for_one_img.shape[1]\n    \n#     cam_features = sp.ndimage.zoom(features_for_one_img, (height_roomout, width_roomout, 1), order = 2)\n#     pred = np.argmax(results[idx])\n#     cam_features = features_for_one_img\n    \n#     plt.figure(facecolor = \"white\")\n#     cam_weights = gap_weights[:, pred]\n#     cam_output = np.dot(cam_features, cam_weights)\n    \n    \n#     buf = \"Predicted Class = \"+ str(pred) + \", Probability = \" + str(results[idx][pred])\n    \n#     plt.xlabel(buf)\n    \n#     plt.imshow(np.squeeze(X_test[idx], -1), alpha = 0.5)\n    \n#     plt.imshow(cam_output, cmap = \"jet\", alpha = 0.5)\n    \n    \n#     plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-14T10:51:28.069842Z","iopub.status.idle":"2021-09-14T10:51:28.07066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import tensorflow as tf\n# import matplotlib.pyplot as plt\n# fashion_mnist = tf.keras.datasets.fashion_mnist\n# numbers_mnist = tf.keras.datasets.mnist\n# # (train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n\n# (train_images, train_labels), (test_images, test_labels) = numbers_mnist.load_data(path=\"mnist.npz\")","metadata":{"execution":{"iopub.status.busy":"2021-09-14T10:51:28.071897Z","iopub.status.idle":"2021-09-14T10:51:28.072713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# m = train_images[0].reshape(28, 28, 1)\n# m.shape\n\n# train_images.shape\n# train_images_ = train_images.reshape(60000, 28, 28, 1)\n# test_images_ = test_images.reshape(10000, 28, 28, 1)","metadata":{"execution":{"iopub.status.busy":"2021-09-14T10:51:28.073888Z","iopub.status.idle":"2021-09-14T10:51:28.074705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test_images.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-14T10:51:28.075896Z","iopub.status.idle":"2021-09-14T10:51:28.076689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plt.figure()\n# plt.imshow(train_images[0])\n# plt.colorbar()\n# plt.grid(False)\n# plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-14T10:51:28.077869Z","iopub.status.idle":"2021-09-14T10:51:28.07867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # cnn architecture:\n# import numpy as np\n# from tensorflow.keras import Sequential\n# from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Dense \n# np.random.seed(0)\n# model = Sequential()\n# model.add(Conv2D(16, input_shape = (28, 28, 1), \n#                  kernel_size = (3,3), activation = \"relu\", \n#                  padding = \"same\"))\n# model.add(MaxPooling2D(pool_size = (2,2)))\n\n# model.add(Conv2D(32, kernel_size = (3,3), \n#                  activation = \"relu\", padding = \"same\"))\n# model.add(MaxPooling2D(pool_size = (2,2)))\n\n# model.add(Conv2D(64, kernel_size = (3,3), \n#                  activation = \"relu\", padding = \"same\"))\n# model.add(MaxPooling2D())\n\n# model.add(Conv2D(128, kernel_size = (3,3), \n#                  activation = \"relu\", padding = \"same\"))\n# model.add(GlobalAveragePooling2D())\n# model.add(Dense(10, activation = \"softmax\"))\n\n# model.summary()\n# model.compile(loss = \"sparse_categorical_crossentropy\", \n#               metrics = [\"accuracy\"], optimizer = \"adam\")\n# model.fit(train_images_, train_labels, batch_size = 32, epochs = 5, \n#           validation_split = 0.1, shuffle = True)","metadata":{"execution":{"iopub.status.busy":"2021-09-14T10:51:28.079995Z","iopub.status.idle":"2021-09-14T10:51:28.080811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# np.random.seed(0)\n# model = Sequential()\n# model.add(Conv2D(16, input_shape = (28,28, 1), kernel_size = (3,3), activation = \"relu\", padding = \"same\"))\n\n# model.add(Conv2D(32, kernel_size = (3,3), activation = \"relu\", padding = \"same\"))\n\n# model.add(Conv2D(64, kernel_size = (3,3), activation = \"relu\", padding = \"same\"))\n\n# model.add(Conv2D(128, kernel_size = (3,3), activation = \"relu\", padding = \"same\"))\n# model.add(GlobalAveragePooling2D())\n# model.add(Dense(10, activation = \"softmax\"))\n\n\n# model.summary()\n# model.compile(loss=\"sparse_categorical_crossentropy\", metrics = [\"accuracy\"], optimizer = \"adam\")\n# model.fit(train_images_, train_labels, batch_size = 32, epochs = 5, validation_split = 0.1, shuffle = True)","metadata":{"execution":{"iopub.status.busy":"2021-09-14T10:51:28.082043Z","iopub.status.idle":"2021-09-14T10:51:28.08286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # getting the features from pretrained to visualise what it sees in images to classify it:\n    \n# # might give clue as to why texting is misinterpreted as talking to radio:\n\n# from keras.models import Model\n# import scipy as sp\n# gap_weights = model.layers[-1].get_weights()[0]\n# gap_weights.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-14T10:51:28.084052Z","iopub.status.idle":"2021-09-14T10:51:28.084837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # consisting of one last activation layer:\n\n# cam_model = Model(inputs = model.input, outputs = (model.layers[-3].output,\n#                                                   model.layers[-1].output))","metadata":{"execution":{"iopub.status.busy":"2021-09-14T10:51:28.086006Z","iopub.status.idle":"2021-09-14T10:51:28.08682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# features, results = cam_model.predict(test_images_)\n# features.shape\n\n# # shd be (1000, 28, 28, 128) but havent yet still (10000, 3, 3, 128)","metadata":{"execution":{"iopub.status.busy":"2021-09-14T10:51:28.088007Z","iopub.status.idle":"2021-09-14T10:51:28.088814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# X_train = train_images_\n# X_test = test_images_\n# class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n#                'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n# for idx in range(10):\n#     features_for_one_img = features[idx, :, :, :]\n#     height_roomout = X_train.shape[1]/features_for_one_img.shape[0]\n#     width_roomout = X_train.shape[2]/features_for_one_img.shape[1]\n#     print((height_roomout, width_roomout, 1))\n    \n#     cam_features = sp.ndimage.zoom(features_for_one_img, (height_roomout, width_roomout, 1), order = 2)\n#     pred = np.argmax(results[idx])\n#     cam_features = features_for_one_img\n    \n#     plt.figure(facecolor = \"white\")\n#     cam_weights = gap_weights[:, pred]\n#     cam_output = np.dot(cam_features, cam_weights)\n    \n# #     class_names[pred]\n#     buf = \"Predicted Class = \"+ str(pred) + \", Probability = \" + str(results[idx][pred])\n    \n#     plt.xlabel(buf)\n    \n#     plt.imshow(np.squeeze(X_test[idx], -1), alpha = 0.5)\n    \n#     plt.imshow(cam_output, cmap = \"Pastel2\", alpha = 0.5)\n    \n    \n#     plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-14T10:51:28.09001Z","iopub.status.idle":"2021-09-14T10:51:28.090815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_labels","metadata":{"execution":{"iopub.status.busy":"2021-09-14T10:51:28.091998Z","iopub.status.idle":"2021-09-14T10:51:28.092799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"old one that splits according to stratified classes in Y:\n","metadata":{"id":"hNmJMSBbxrjJ"}},{"cell_type":"code","source":"","metadata":{"id":"N1Rcf5prATnK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Ignore everything below\n","metadata":{}},{"cell_type":"code","source":"\n# VALIDATION_ACCURACY = []\n# VALIDAITON_LOSS = []\n\n# save_dir = '/saved_models/'\n# fold_var = 0\n\n# train_data = train_set_revamped\n# # np.zeros(n),\n# m = len(Y) - 5\n# for train_index, val_index in skf.split(np.zeros(len(Y)),Y):\n# # for subject_index in skf.split(all_subjects):\n#   fold_var += 1\n#   start = time.perf_counter()\n#   print(f\"fold_var: {fold_var}\")\n#   # training_data = train_data.iloc[train_index]\n#   # validation_data = train_data.iloc[val_index]\n#   print(f\"{fold_var} train data len: {len(training_data)}\")\n#   print(f\"{fold_var} validation data len: {len(validation_data)}\")\n#   print(f\"ratio of splits : {len(training_data)/len(validation_data)}\")\n\n  \n\n#   # for class_ in CLASSES:\n#   #   training_data[\"\"]\n#   print(training_data.head())\n#   print(validation_data.head())\n\n\n# #     rescale=1./255, - results in less traiing and validataion accruacy of less than .1!!!\n#   trainAug = ImageDataGenerator(rotation_range=30,\n#     zoom_range=0.15,\n#     width_shift_range=0.2,\n#     height_shift_range=0.2,\n#     shear_range=0.15,\n#     horizontal_flip=True,\n#     fill_mode=\"nearest\",\n#     # preprocess_input_resnet, preprocess_input_resnetv2, preprocess_input_efficientnet, preprocess_input_densenet\n#       preprocessing_function = preprocess_input_densenet)\n# # no params for valAug since only standardisation of colors by subtracting mean:\n# #     [123.675,116.28,103.53] -- why is the same mean order for all nets?\n#   valAug = ImageDataGenerator(preprocessing_function = preprocess_input_densenet)\n\n\n#   train_data_generator = trainAug.flow_from_dataframe(training_data, \n#                                                 # directory =,\n#                                                 x_col = \"img_path\", \n#                                                 y_col = \"classname\",\n#                                                 class_mode = \"categorical\", \n#                                                 shuffle = True)\n#   valid_data_generator  = valAug.flow_from_dataframe(validation_data, \n#                                                 #  directory = image_dir,\n#                                                  x_col = \"img_path\", \n#                                                  y_col = \"classname\",\n#                                                  class_mode = \"categorical\", \n#                                                  shuffle = True)\n\t\n# \t# CREATE NEW MODEL\n# \tmodel = create_model()\n# \t# COMPILE NEW MODEL\n# \tmodel.compile(loss='categorical_crossentropy',\n# \t\t      optimizer=opt,\n# \t\t      metrics=['accuracy'])\n\t\n# \t# CREATE CALLBACKS\n# \tcheckpoint = tf.keras.callbacks.ModelCheckpoint(save_dir+ current_model_name + \"_\" + fold_var, \n# \t\t\t\t\t\t\tmonitor='val_accuracy', verbose=1, \n# \t\t\t\t\t\t\tsave_best_only=True, mode='max')\n# \tcallbacks_list = [checkpoint]\n# \t# There can be other callbacks, but just showing one because it involves the model name\n# \t# This saves the best model\n# \t# FIT THE MODEL\n# \thistory = model.fit(train_data_generator,\n# \t\t\t    epochs=num_epochs,\n# \t\t\t    callbacks=callbacks_list,\n# \t\t\t    validation_data=valid_data_generator)\n# \t#PLOT HISTORY\n# \t#\t\t:\n# \t#\t\t:\n\t\n# \t# LOAD BEST MODEL to evaluate the performance of the model\n# \tmodel.load_weights(\"/saved_models/model_\"+str(fold_var)+\".h5\")\n\t\n# \tresults = model.evaluate(valid_data_generator)\n# \tresults = dict(zip(model.metrics_names,results))\n\t\n# \tVALIDATION_ACCURACY.append(results['accuracy'])\n# \tVALIDATION_LOSS.append(results['loss'])\n\t\n# \ttf.keras.backend.clear_session()\n\n#   end = time.perf_counter()\n#   print(f\"fold {fold_var} took {end - start} seconds\")\n\t","metadata":{"id":"_UzGscR7xpKj","outputId":"d84f4092-0f5f-4c2e-9d1b-8b2220d86e08","execution":{"iopub.status.busy":"2021-09-14T10:51:28.094052Z","iopub.status.idle":"2021-09-14T10:51:28.094865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"VlVQ8H1XxpI6","outputId":"8ed69248-1a24-4902-bae0-157f57197299","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"4doKLaXW5GQI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Only head is trained","metadata":{"id":"oiy8d76bdE3-"}},{"cell_type":"code","source":"# # models directory to save model weights via checkpoints callback:\n\n# # current_model_dir = \"/content/drive/MyDrive/DAP (Actual Idiots)/models/effnet_model_correct_splits\"\n# print(current_model_dir)\n# print(os.listdir(current_model_dir))\n# folders_in_model = [\"/models\", \"/dataaug\", \"/logs\", \"/plots\"]\n\n\n# for path in folders_in_model:\n#   curr = current_model_dir + path\n#   # # for deleting previous folders in model dir:\n#   # shutil.rmtree(curr)\n#   if not os.path.exists(curr):\n#     os.mkdir(curr)\n \n\n# print(os.listdir(current_model_dir))","metadata":{"id":"1XSx6Cd3bWRp","outputId":"bc14dd83-9d44-4d3c-c990-f673e60a63df","execution":{"iopub.status.busy":"2021-09-14T10:51:28.096047Z","iopub.status.idle":"2021-09-14T10:51:28.097164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# # # https://www.tensorflow.org/tensorboard/scalars_and_keras\n# # logdir = \"logs/scalars/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n# # file_writer = tf.summary.create_file_writer(logdir + \"/metrics\")\n# # file_writer.set_as_default()\n\n# # def lr_schedule(epoch):\n# #   \"\"\"\n# #   Returns a custom learning rate that decreases as epochs progress.\n# #   \"\"\"\n# #   learning_rate = 0.2\n# #   if epoch > 10:\n# #     learning_rate = 0.02\n# #   if epoch > 20:\n# #     learning_rate = 0.01\n# #   if epoch > 50:\n# #     learning_rate = 0.005\n\n# #   tf.summary.scalar('learning rate', data=learning_rate, step=epoch)\n# #   return learning_rate\n\n# # lr_callback = keras.callbacks.LearningRateScheduler(lr_schedule)\n# # tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n\n# # This function keeps the initial learning rate for the first ten epochs\n# # and decreases it exponentially after that.\n# def scheduler(epoch, lr):\n#     if epoch < 6:\n#         return lr\n#     else:\n#         return lr * tf.math.exp(-0.1)\n\n\n# # callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n","metadata":{"id":"C_UF2NCgdE3-","execution":{"iopub.status.busy":"2021-09-14T10:51:28.098466Z","iopub.status.idle":"2021-09-14T10:51:28.099265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(os.listdir(current_model_dir + \"/models\"))\n# print(current_model_dir + \"/models/cp.{epoch:02d}-{val_loss:.2f}.ckpt\")","metadata":{"id":"Py5gW7UQijo3","outputId":"bdd1f56f-16ce-4374-8b3c-70fdc589de92","execution":{"iopub.status.busy":"2021-09-14T10:51:28.100456Z","iopub.status.idle":"2021-09-14T10:51:28.101256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # checking if gpu working:\n\n# %tensorflow_version 2.x\n# import tensorflow as tf\n# device_name = tf.test.gpu_device_name()\n# if device_name != '/device:GPU:0':\n#   raise SystemError('GPU device not found')\n# print('Found GPU at: {}'.format(device_name))\n\n\n","metadata":{"id":"l_i3WEY2lLtg","outputId":"ab12beb2-e9fe-4620-8fcd-8aea6f36c387","execution":{"iopub.status.busy":"2021-09-14T10:51:28.102429Z","iopub.status.idle":"2021-09-14T10:51:28.103235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %tensorflow_version 2.x\n# import tensorflow as tf\n# import timeit\n\n# device_name = tf.test.gpu_device_name()\n# if device_name != '/device:GPU:0':\n#   print(\n#       '\\n\\nThis error most likely means that this notebook is not '\n#       'configured to use a GPU.  Change this in Notebook Settings via the '\n#       'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n#   raise SystemError('GPU device not found')\n\n# def cpu():\n#   with tf.device('/cpu:0'):\n#     random_image_cpu = tf.random.normal((100, 100, 100, 3))\n#     net_cpu = tf.keras.layers.Conv2D(32, 7)(random_image_cpu)\n#     return tf.math.reduce_sum(net_cpu)\n\n# def gpu():\n#   with tf.device('/device:GPU:0'):\n#     random_image_gpu = tf.random.normal((100, 100, 100, 3))\n#     net_gpu = tf.keras.layers.Conv2D(32, 7)(random_image_gpu)\n#     return tf.math.reduce_sum(net_gpu)\n  \n# # We run each op once to warm up; see: https://stackoverflow.com/a/45067900\n# cpu()\n# gpu()\n\n# # Run the op several times.\n# print('Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images '\n#       '(batch x height x width x channel). Sum of ten runs.')\n# print('CPU (s):')\n# cpu_time = timeit.timeit('cpu()', number=10, setup=\"from __main__ import cpu\")\n# print(cpu_time)\n# print('GPU (s):')\n# gpu_time = timeit.timeit('gpu()', number=10, setup=\"from __main__ import gpu\")\n# print(gpu_time)\n# print('GPU speedup over CPU: {}x'.format(int(cpu_time/gpu_time)))","metadata":{"id":"WD5MpUW1lOVj","outputId":"9c0fb3c8-c15c-4d08-b0f3-ae2ddc633b34","execution":{"iopub.status.busy":"2021-09-14T10:51:28.104444Z","iopub.status.idle":"2021-09-14T10:51:28.105246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#training","metadata":{"id":"Aj5ky6VQodDe"}},{"cell_type":"code","source":"# CLASSES\n# print(CLASSES)\n\n# # content_dir = [\"train/\" + i for i in CLASSES]\n# for class_ in CLASSES:\n#   new_path = \"/content/train/\" + class_\n#   print(new_path)\n#   old = \"/content/drive/MyDrive/DAP (Actual Idiots)/driverdata/imgs/train/\" + class_\n#   print(f\"{class_} ---> {len(os.listdir(old))}\")\n#   if not os.path.exists(new_path):\n#     os.makedirs(new_path)\n\n\n# print(os.listdir(\"/content/train\"))","metadata":{"id":"_vyvjqAIoapN","execution":{"iopub.status.busy":"2021-09-14T10:51:28.106441Z","iopub.status.idle":"2021-09-14T10:51:28.107234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from tqdm import tqdm\n# for class_ in CLASSES:\n#   old = \"/content/drive/MyDrive/DAP (Actual Idiots)/driverdata/imgs/train/\" + class_\n#   new_class_path = \"/content/train/\" + class_\n#   old_class_img = os.listdir(old)\n#   print(len(old_class_img))\n#   for img in tqdm(old_class_img):\n#     old_dest_img = old + \"/\" + img\n#     new_dest_img = new_class_path + \"/\" + img\n#     shutil.copy2(old_dest_img, new_dest_img)\n  ","metadata":{"id":"DKQXHRK_pKbk","execution":{"iopub.status.busy":"2021-09-14T10:51:28.108453Z","iopub.status.idle":"2021-09-14T10:51:28.109259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !cp -r \"/content/drive/MyDrive/DAP (Actual Idiots)/driverdata/imgs/train\" \"./train\"","metadata":{"id":"jHa9Mohsmrxu","execution":{"iopub.status.busy":"2021-09-14T10:51:28.110423Z","iopub.status.idle":"2021-09-14T10:51:28.111222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# BATCH_SIZE = 32\n\n# # compile our model (this needs to be done after our setting our\n# # layers to being non-trainable\n# print(\"[INFO] compiling model...\")\n# opt = SGD(lr=1e-4, momentum=0.9)\n# model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n# \tmetrics=[\"accuracy\"])\n# # train the head of the network for a few epochs (all other layers\n# # are frozen) -- this will allow the new FC layers to start to become\n# # initialized with actual \"learned\" values versus pure random\n# print(\"[INFO] training head...\")\n# my_callbacks = [\n#     # tf.keras.callbacks.LearningRateScheduler(scheduler),\n#     tf.keras.callbacks.EarlyStopping(monitor = \"val_accuracy\", min_delta = 0.01, patience = 2),\n#     tensorflow.keras.callbacks.ModelCheckpoint(\n# #         filepath='./models/model.{epoch:02d}-{val_loss:.2f}.h5',\n#                                               filepath= current_model_dir + \"/models/cp.{epoch:02d}-{val_loss:.2f}.ckpt\",\n#                                                  save_weights_only=True,\n#                                                  verbose=1),\n#     tensorflow.keras.callbacks.TensorBoard(log_dir= current_model_dir + '/logs'),\n#     CustomCallback(run_folder = current_model_dir + \"/plots\", initial_epoch = 0)\n# ]\n\n# H = model.fit(\n# \tx=training,\n# \tsteps_per_epoch=totalTrain // BATCH_SIZE,\n# \tvalidation_data=validation,\n# \tvalidation_steps=totalVal // BATCH_SIZE,\n# \tepochs=10,\n#     callbacks=my_callbacks)\n# # reset the testing generator and evaluate the network after\n# # fine-tuning just the network head\n# print(\"[INFO] evaluating after fine-tuning network head...\")\n# evaluation.reset()\n# predIdxs = model.predict(x=evaluation,\n# \tsteps=(totalTest // BATCH_SIZE) + 1)\n# predIdxs = np.argmax(predIdxs, axis=1)\n# print(classification_report(evaluation.classes, predIdxs,\n# \ttarget_names=evaluation.class_indices.keys()))\n# # plot_training(H, 50, config.WARMUP_PLOT_PATH)","metadata":{"id":"pcVIIHq8bWQG","outputId":"8e6545a2-1725-405c-e1fb-5954710d50bd","execution":{"iopub.status.busy":"2021-09-14T10:51:28.112465Z","iopub.status.idle":"2021-09-14T10:51:28.113257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# os.listdir(current_model_dir +\"/models\")","metadata":{"id":"Ze57cO7VdE3_","execution":{"iopub.status.busy":"2021-09-14T10:51:28.114426Z","iopub.status.idle":"2021-09-14T10:51:28.115222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model.save(current_model_dir +\"/model_resnetv2_primary.h5\")\n# os.listdir(current_model_dir)","metadata":{"id":"H3SBK9gIdE3_","execution":{"iopub.status.busy":"2021-09-14T10:51:28.116409Z","iopub.status.idle":"2021-09-14T10:51:28.11722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !zip -r \"\"\n\n# !zip -r current_model_dir+\"/model_resnetv2_primary.h5\"\"/model_resnetv2_primary.zip\" current_model_dir+\"/model_resnetv2_primary.h5\"","metadata":{"id":"JrK74AWGdE3_","execution":{"iopub.status.busy":"2021-09-14T10:51:28.118396Z","iopub.status.idle":"2021-09-14T10:51:28.119175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(len(baseModel.layers)//2)\n# halfway_mark = len(baseModel.layers)//2\n# model.layers[-4:]","metadata":{"id":"QKl5YmYcdE3_","execution":{"iopub.status.busy":"2021-09-14T10:51:28.120374Z","iopub.status.idle":"2021-09-14T10:51:28.12114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"jqEmt0IPJMlr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"qevWz24yJMjX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"NZ_rmhrLJMfv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"ErWVZtDZJMde"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"Jdm9c-4qJMaF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Continue training head","metadata":{"id":"O_OlBz8uJhNP"}},{"cell_type":"code","source":"# os.mkdir(current_model_dir + \"/plots_2\")\n# os.mkdir(current_model_dir + \"/logs_2\")\n# # os.mkdir(current_model_dir + \"/plots_2\") \n# os.listdir(current_model_dir)","metadata":{"id":"mJXcUYXBPx65","outputId":"a9d24317-5b58-452c-928c-ebc9d650470e","execution":{"iopub.status.busy":"2021-09-14T10:51:28.122341Z","iopub.status.idle":"2021-09-14T10:51:28.123124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(os.listdir(current_model_dir + \"/\" + \"models\" ))\n# checkpoint_dir = current_model_dir + \"/\" + \"models\" \n# latest = tf.train.latest_checkpoint(checkpoint_dir)\n# latest","metadata":{"id":"pgz7ncMuKUED","outputId":"c51083bc-7581-4968-9f75-9b81833cc7d4","execution":{"iopub.status.busy":"2021-09-14T10:51:28.124338Z","iopub.status.idle":"2021-09-14T10:51:28.125104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model, baseModel, headModel = create_model()","metadata":{"id":"bt2SPE7KLUsN","execution":{"iopub.status.busy":"2021-09-14T10:51:28.126317Z","iopub.status.idle":"2021-09-14T10:51:28.127093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for layer in baseModel.layers:\n#     layer.trainable = False\n# model.load_weights(latest)\n\n# # model = tf.keras.models.load_model(\"./model_resnetv2_primary.h5\")","metadata":{"id":"m-D23x53dE4A","outputId":"76ddf595-e849-4b64-9478-a1df6ed39fc8","execution":{"iopub.status.busy":"2021-09-14T10:51:28.128456Z","iopub.status.idle":"2021-09-14T10:51:28.129266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # reset the testing generator and evaluate the network after\n# # fine-tuning just the network head\n# print(\"[INFO] evaluating after fine-tuning network head...\")\n# evaluation.reset()\n# predIdxs = model.predict(x=evaluation,\n# \tsteps=(totalTest // BATCH_SIZE) + 1)\n# predIdxs = np.argmax(predIdxs, axis=1)\n# print(classification_report(evaluation.classes, predIdxs,\n# \ttarget_names=evaluation.class_indices.keys()))\n# # plot_training(H, 50, config.WARMUP_PLOT_PATH)","metadata":{"id":"JJMyFoVFLfSA","outputId":"b5c14421-8ca8-423b-e7f6-fe31a687ecce","execution":{"iopub.status.busy":"2021-09-14T10:51:28.13043Z","iopub.status.idle":"2021-09-14T10:51:28.131236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # checking if gpu working:\n\n# %tensorflow_version 2.x\n# import tensorflow as tf\n# device_name = tf.test.gpu_device_name()\n# if device_name != '/device:GPU:0':\n#   raise SystemError('GPU device not found')\n# print('Found GPU at: {}'.format(device_name))\n\n","metadata":{"id":"9QpaJ0-SJSaa","execution":{"iopub.status.busy":"2021-09-14T10:51:28.132421Z","iopub.status.idle":"2021-09-14T10:51:28.13323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# %tensorflow_version 2.x\n# import tensorflow as tf\n# import timeit\n\n# device_name = tf.test.gpu_device_name()\n# if device_name != '/device:GPU:0':\n#   print(\n#       '\\n\\nThis error most likely means that this notebook is not '\n#       'configured to use a GPU.  Change this in Notebook Settings via the '\n#       'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n#   raise SystemError('GPU device not found')\n\n# def cpu():\n#   with tf.device('/cpu:0'):\n#     random_image_cpu = tf.random.normal((100, 100, 100, 3))\n#     net_cpu = tf.keras.layers.Conv2D(32, 7)(random_image_cpu)\n#     return tf.math.reduce_sum(net_cpu)\n\n# def gpu():\n#   with tf.device('/device:GPU:0'):\n#     random_image_gpu = tf.random.normal((100, 100, 100, 3))\n#     net_gpu = tf.keras.layers.Conv2D(32, 7)(random_image_gpu)\n#     return tf.math.reduce_sum(net_gpu)\n  \n# # We run each op once to warm up; see: https://stackoverflow.com/a/45067900\n# cpu()\n# gpu()\n\n# # Run the op several times.\n# print('Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images '\n#       '(batch x height x width x channel). Sum of ten runs.')\n# print('CPU (s):')\n# cpu_time = timeit.timeit('cpu()', number=10, setup=\"from __main__ import cpu\")\n# print(cpu_time)\n# print('GPU (s):')\n# gpu_time = timeit.timeit('gpu()', number=10, setup=\"from __main__ import gpu\")\n# print(gpu_time)\n# print('GPU speedup over CPU: {}x'.format(int(cpu_time/gpu_time)))","metadata":{"id":"u5eb0_mvJRVT","execution":{"iopub.status.busy":"2021-09-14T10:51:28.134421Z","iopub.status.idle":"2021-09-14T10:51:28.135221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # reset our data generators\n# training.reset()\n# validation.reset()\n# initial_layers = []\n# # for layer in baseModel.layers:\n# # # \tprint(\"{}: {}\".format(layer, layer.trainable))\n# #     initial_layers.append(layer.trainable)\n# # print(initial_layers)\n\n\n# # now that the head FC layers have been trained/initialized, lets\n# # unfreeze the final set of 20 CONV layers and make them trainable\n# for layer in baseModel.layers[:halfway_mark]:\n# \tlayer.trainable = False\n# for layer in baseModel.layers[halfway_mark:]:\n# \tlayer.trainable = True\n    \n# # loop over the layers in the model and show which ones are trainable\n# # or not\n# final_layers = []\n# for layer in baseModel.layers:\n# # \tprint(\"{}: {}\".format(layer, layer.trainable))\n#     final_layers.append(layer.trainable)\n# print(final_layers)\n\n# # for layer in model.layers[:-4]:\n# #     layer.trainable = False","metadata":{"id":"H9Sku4NFdE3_","execution":{"iopub.status.busy":"2021-09-14T10:51:28.136415Z","iopub.status.idle":"2021-09-14T10:51:28.137353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model.save(\"./model_3a.h5\")","metadata":{"id":"79KqHKnCbWOR","execution":{"iopub.status.busy":"2021-09-14T10:51:28.138566Z","iopub.status.idle":"2021-09-14T10:51:28.139376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !zip -r \"model.zip\" \"./model_3a.h5\"","metadata":{"id":"RthGChzydE4A","execution":{"iopub.status.busy":"2021-09-14T10:51:28.140543Z","iopub.status.idle":"2021-09-14T10:51:28.14134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !zip -r \"logs_2a.zip\" \"./logs\"","metadata":{"id":"522UxFxZbWMT","execution":{"iopub.status.busy":"2021-09-14T10:51:28.142498Z","iopub.status.idle":"2021-09-14T10:51:28.14329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# os.listdir(\"./models\")","metadata":{"id":"03j5p-_kdE4A","execution":{"iopub.status.busy":"2021-09-14T10:51:28.144462Z","iopub.status.idle":"2021-09-14T10:51:28.145264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# os.mkdir(current_model_dir +\"/plot_2\")\n# os.listdir(current_model_dir)","metadata":{"id":"pXeaW2vJdE4A","execution":{"iopub.status.busy":"2021-09-14T10:51:28.146419Z","iopub.status.idle":"2021-09-14T10:51:28.14722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # This function keeps the initial learning rate for the first ten epochs\n# # and decreases it exponentially after that.\n# def scheduler(epoch, lr):\n#     if epoch < 6:\n#         return lr\n#     else:\n#         return lr * tf.math.exp(-0.1)","metadata":{"id":"D0oeYNLtdE4A","execution":{"iopub.status.busy":"2021-09-14T10:51:28.148434Z","iopub.status.idle":"2021-09-14T10:51:28.149224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # for the changes to the model to take affect we need to recompile\n# # the model, this time using SGD with a *very* small learning rate\n# print(\"[INFO] re-compiling model...\")\n# opt = SGD(lr=1e-6, momentum=0.9)\n# model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n# \tmetrics=[\"accuracy\"])\n# # train the model again, this time fine-tuning *both* the final set\n# # of CONV layers along with our set of FC layers\n# my_callbacks = [\n#     tf.keras.callbacks.LearningRateScheduler(scheduler),\n# #     tf.keras.callbacks.EarlyStopping(monitor = \"val_accuracy\", min_delta = 0.01),\n#     tensorflow.keras.callbacks.ModelCheckpoint(\n# #         filepath='./models/model.{epoch:02d}-{val_loss:.2f}.h5',\n#                                               filepath=current_model_dir + \"/models/cp.{epoch:02d}-{val_loss:.2f}.ckpt\",\n#                                                  save_weights_only=True,\n#                                                  verbose=1),\n#     tensorflow.keras.callbacks.TensorBoard(log_dir=current_model_dir + '/logs'),\n#     CustomCallback(run_folder = current_model_dir + \"/plot_2\", initial_epoch = 0)\n# ]\n\n\n# H = model.fit(\n# \tx=training,\n# \tsteps_per_epoch=totalTrain // BATCH_SIZE,\n# \tvalidation_data=validation,\n# \tvalidation_steps=totalVal // BATCH_SIZE,\n# \tepochs=10,\n# callbacks = my_callbacks)\n\n\n# # reset the testing generator and evaluate the network after\n# # fine-tuning just the network head\n# print(\"[INFO] evaluating after fine-tuning network head...\")\n# evaluation.reset()\n# predIdxs = model.predict(x=evaluation,\n# \tsteps=(totalTest // BATCH_SIZE) + 1)\n# predIdxs = np.argmax(predIdxs, axis=1)\n# print(classification_report(evaluation.classes, predIdxs,\n# \ttarget_names=evaluation.class_indices.keys()))\n# # plot_training(H, 50, config.WARMUP_PLOT_PATH)","metadata":{"id":"7krDxAbcbTDl","execution":{"iopub.status.busy":"2021-09-14T10:51:28.150546Z","iopub.status.idle":"2021-09-14T10:51:28.151383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model.save(\"./model_2b.h5\")","metadata":{"id":"8ApyBWNZdE4B","execution":{"iopub.status.busy":"2021-09-14T10:51:28.152552Z","iopub.status.idle":"2021-09-14T10:51:28.15335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !zip -r \"logs_2b.zip\" \"./logs\"","metadata":{"id":"oe6jXhxgdE4B","execution":{"iopub.status.busy":"2021-09-14T10:51:28.154513Z","iopub.status.idle":"2021-09-14T10:51:28.155306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # reset the testing generator and then use our trained model to\n# # make predictions on the data\n# print(\"[INFO] evaluating after fine-tuning network...\")\n# evaluation.reset()\n# predIdxs = model.predict(x=evaluation,\n# \tsteps=(totalTest // BATCH_SIZE) + 1)\n# predIdxs = np.argmax(predIdxs, axis=1)\n# print(classification_report(evaluation.classes, predIdxs,\n# \ttarget_names=evaluation.class_indices.keys()))\n# plot_training(H, 20, \"./plots\")\n# # serialize the model to disk\n# print(\"[INFO] serializing network...\")\n# # model.save(MODEL_PATH, save_format=\"h5\")","metadata":{"id":"yrgfWITgdE4B","execution":{"iopub.status.busy":"2021-09-14T10:51:28.156485Z","iopub.status.idle":"2021-09-14T10:51:28.157281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # !zip -r \"effnet7_weights.zip\" \"./models/cp.01-0.30.ckpt\"\n# !zip -r effnet7_weights_1.zip . -i ./models/cp.01-0.30.ckpt","metadata":{"id":"Ln0604MAdE4B","execution":{"iopub.status.busy":"2021-09-14T10:51:28.158445Z","iopub.status.idle":"2021-09-14T10:51:28.159236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checkpoints = [i for i in os.listdir(\"./models\") if (\"cp.01-0.30.ckpt\" or \"checkpoint\") in i] + [\"checkpoint\"]\n# print(os.listdir(\"./models\"))\n# checkpoints\n# # models/cp.02-0.28.ckpt","metadata":{"id":"IhlIC-a8dE4C","execution":{"iopub.status.busy":"2021-09-14T10:51:28.160411Z","iopub.status.idle":"2021-09-14T10:51:28.161208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # os.mkdir(\"models_weights_2\")\n# # ./models_weights/cp.01-0.30.ckpt.data-00000-of-00001\n# new_path = \"./models_weights_2\"\n# for i in checkpoints:\n#     src = \"./models/\" + i\n#     dest = new_path + \"/\" + i\n#     shutil.copy(src, dest)\n# # shutil.cp2()","metadata":{"id":"onNDSzShdE4C","execution":{"iopub.status.busy":"2021-09-14T10:51:28.162405Z","iopub.status.idle":"2021-09-14T10:51:28.163195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# os.listdir(\"./models_weights_2\")","metadata":{"id":"_mU72DWodE4C","execution":{"iopub.status.busy":"2021-09-14T10:51:28.16446Z","iopub.status.idle":"2021-09-14T10:51:28.165385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !zip -r \"effnet_finetuned_0.81_weights_2.zip\" \"./models_weights_2\"","metadata":{"id":"pUa--XdtdE4C","execution":{"iopub.status.busy":"2021-09-14T10:51:28.166676Z","iopub.status.idle":"2021-09-14T10:51:28.167544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"Mdxx8k5gdE4C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"dhFkO6FfdE4C"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#COMBINING MULTIPLE GENERATORS:","metadata":{"id":"TjFeDj4YdE4C"}},{"cell_type":"code","source":"# - https://stackoverflow.com/questions/49404993/how-to-use-fit-generator-with-multiple-inputs/49405175#49405175\n","metadata":{"id":"AUzRVdWDdE4C","execution":{"iopub.status.busy":"2021-09-14T10:51:28.168852Z","iopub.status.idle":"2021-09-14T10:51:28.169682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{"id":"IeLUM_-EdE4C"}},{"cell_type":"code","source":"# def generator_two_img(X1, X2, y, batch_size):\n#     genX1 = gen.flow(X1, y,  batch_size=batch_size, \n=1)\n#     genX2 = gen.flow(X2, y, batch_size=batch_size, \n\n\n\n\n\n=1)\n#     while True:\n#         X1i = genX1.next()\n#         X2i = genX2.next()\n#         yield [X1i[0], X2i[0]], X1i[1]","metadata":{"id":"cTO_-1TCdE4C","execution":{"iopub.status.busy":"2021-09-14T10:51:28.170921Z","iopub.status.idle":"2021-09-14T10:51:28.171899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def generator_three_img(X1, X2, X3, y, batch_size):\n#     genX1 = gen.flow(X1, y,  batch_size=batch_size, seed=1)\n#     genX2 = gen.flow(X2, y, batch_size=batch_size, seed=1)\n#     genX3 = gen.flow(X3, y, batch_size=batch_size, seed=1)\n#     while True:\n#         X1i = genX1.next()\n#         X2i = genX2.next()\n#         X3i = genX3.next()\n#         yield [X1i[0], X2i[0], X3i[0]], X1i[1]","metadata":{"id":"uYAcu1a_dE4D","execution":{"iopub.status.busy":"2021-09-14T10:51:28.173127Z","iopub.status.idle":"2021-09-14T10:51:28.173943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from keras.preprocessing.image import ImageDataGenerator\n# from tensorflow.keras.utils import Sequence\n\n\n# class MultipleInputGenerator(Sequence):\n#     \"\"\"Wrapper of 2 ImageDataGenerator\"\"\"\n\n#     def __init__(self, X1, X2, Y, batch_size):\n#         # Keras generator\n#         self.generator = ImageDataGenerator(rotation_range=15, \n#                                             width_shift_range=0.2,\n#                                             height_shift_range=0.2,\n#                                             shear_range=0.2,\n#                                             zoom_range=0.2,\n#                                             horizontal_flip=True, \n#                                             fill_mode='nearest')\n\n#         # Real time multiple input data augmentation\n#         self.genX1 = self.generator.flow(X1, Y, batch_size=batch_size)\n#         self.genX2 = self.generator.flow(X2, Y, batch_size=batch_size)\n\n#     def __len__(self):\n#         \"\"\"It is mandatory to implement it on Keras Sequence\"\"\"\n#         return self.genX1.__len__()\n\n#     def __getitem__(self, index):\n#         \"\"\"Getting items from the 2 generators and packing them\"\"\"\n#         X1_batch, Y_batch = self.genX1.__getitem__(index)\n#         X2_batch, Y_batch = self.genX2.__getitem__(index)\n\n#         X_batch = [X1_batch, X2_batch]\n\n#         return X_batch, Y_batch","metadata":{"id":"QbpKOjVTdE4D","execution":{"iopub.status.busy":"2021-09-14T10:51:28.175224Z","iopub.status.idle":"2021-09-14T10:51:28.176034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 2 dir\n\n# train_gen = MultipleInputGenerator([])\n\n# def generator_three_img(X1, X2, \n#                         # X3,\n#                         y, batch_size):\n#   gen = ImageDataGenerator(rotation_range=30,\n# \tzoom_range=0.15,\n# \twidth_shift_range=0.2,\n# \theight_shift_range=0.2,\n# \tshear_range=0.15,\n# \thorizontal_flip=True,\n# \tfill_mode=\"nearest\",\n#     preprocessing_function = preprocess_input_resnet_v2)\n#   # genX1 = gen.flow(X1, y,  batch_size=batch_size, seed=1)\n#   # genX2 = gen.flow(X2, y, batch_size=batch_size, seed=1)\n#   # genX3 = gen.flow(X3, y, batch_size=batch_size, seed=1)\n#   genX1 = gen.flow_from_directory(valPath, class_mode=\"categorical\",\n# \ttarget_size=(224, 224),\n# \tcolor_mode=\"rgb\",\n# \tshuffle=False,\n# \tbatch_size= BATCH_SIZE)\n\n#   genX2 = gen.flow_from_directory(valPath, class_mode=\"categorical\",\n# \ttarget_size=(224, 224),\n# \tcolor_mode=\"rgb\",\n# \tshuffle=False,\n# \tbatch_size= BATCH_SIZE)\n#   while True:\n#     X1i = genX1.next()\n#     X2i = genX2.next()\n#     # X3i = genX3.next()\n#     # yield [X1i[0], X2i[0], X3i[0]], X1i[1]\n#     yield [X1i[0], X2i[0]], X1i[1]","metadata":{"id":"uB3iDLikdE4D","execution":{"iopub.status.busy":"2021-09-14T10:51:28.177324Z","iopub.status.idle":"2021-09-14T10:51:28.178341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.model_selection import train_test_split\n# record = pd.read_csv('/content/drive/MyDrive/DAP (Actual Idiots)/driverdata/driver_imgs_list.csv',na_values='na')\n# all_subjects = list(record.groupby(\"subject\").groups.keys())\n# print(len(all_subjects))\n# record.head()","metadata":{"id":"uZFr3TUvdE4E","execution":{"iopub.status.busy":"2021-09-14T10:51:28.179545Z","iopub.status.idle":"2021-09-14T10:51:28.18039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # # possible new file structure for kfold testing:\n# # (num of people: 26) folders in folder\n# # -- p1 \n# #   -- c0\n# #   --c1\n# #   ...\n# # -- p2 \n# #   -- c0\n# #   --c1\n# #   ...\n\n# # 10 -- 6, 2, 2\n# train_gen = generator([\"revamped_2/p1\", \"revamped_2/p2\", ....])\n# test_gen = ","metadata":{"id":"9rdQurfldE4E","execution":{"iopub.status.busy":"2021-09-14T10:51:28.181719Z","iopub.status.idle":"2021-09-14T10:51:28.182626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # # for i in record.index:\n# # #   print(record.loc[i])\n\n# # person p002 - records:\n# for subject in all_subjects:\n#   subjects_imgs = record[record.subject.isin([subject])]\n#   print(f\"{subject} --> {len(subjects_imgs)}\")\n#   for class_ in CLASSES:\n#     subject_class_imgs = subjects_imgs[subjects_imgs['classname'].isin([class_])]\n#     print(f\"      {class_} --> {len(subject_class_imgs)}\")","metadata":{"id":"4HNbzWqfdE4E","execution":{"iopub.status.busy":"2021-09-14T10:51:28.183852Z","iopub.status.idle":"2021-09-14T10:51:28.184704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # chekcing for p081 class c0:\n# record[(record[\"classname\"] == \"c9\") & (record[\"subject\"] == \"p081\")]","metadata":{"id":"T5Cv0Ma3dE4E","execution":{"iopub.status.busy":"2021-09-14T10:51:28.185903Z","iopub.status.idle":"2021-09-14T10:51:28.186726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # creating directories:\n\n# new_dir = \"/content/drive/MyDrive/DAP (Actual Idiots)/driverdata/imgs/revamped_2\"\n# for subject in all_subjects:\n#   subject_path = new_dir + \"/\" + subject\n#   if not os.path.exists(subject_path):\n#     os.makedirs(subject_path)\n#   for class_ in CLASSES:\n#     subject_class_path = subject_path + \"/\" + class_\n#     if not os.path.exists(subject_class_path):\n#       os.makedirs(subject_class_path)","metadata":{"id":"0-XEGb0KdE4F","execution":{"iopub.status.busy":"2021-09-14T10:51:28.18808Z","iopub.status.idle":"2021-09-14T10:51:28.188943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import numpy as np\n# np.array(all_subjects)\n\n# all_subjects_order = ['p002', 'p012', 'p014', 'p015', 'p016', 'p021', 'p022', 'p024',\n#        'p026', 'p035', 'p039', 'p041', 'p042', 'p045', 'p047', 'p049',\n#        'p050', 'p051', 'p052', 'p056', 'p061', 'p064', 'p066', 'p072',\n#        'p075', 'p081']\n\n# all_subjects_order\n# all_subjects","metadata":{"id":"RO7N9GR0dE4F","execution":{"iopub.status.busy":"2021-09-14T10:51:28.190177Z","iopub.status.idle":"2021-09-14T10:51:28.191026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from imutils import paths\n# import shutil\n# import os\n# import time\n# from tqdm import tqdm\n# original_path = \"/content/drive/MyDrive/DAP (Actual Idiots)/driverdata/imgs/train\"\n# new_dir = \"/content/drive/MyDrive/DAP (Actual Idiots)/driverdata/imgs/revamped_2\"\n# for subject in all_subjects[:2]:\n#   print(subject)\n#   subject_imgs = record[record[\"subject\"].isin([subject])]\n#   for class_ in CLASSES:\n#     print(class_)\n#     subject_classes_imgs = list(subject_imgs[subject_imgs[\"classname\"].isin([class_])][\"img\"])\n#     # print(class_ ,\" --> \", subject_classes_imgs)\n\n#     for img in tqdm(subject_classes_imgs):\n#       src = original_path + \"/\" + class_ + \"/\" + img\n#       dest = new_dir + \"/\" + subject + \"/\" + class_ + \"/\" + img\n#       # shutil.copy2(src, dest)\n#   print(\"----------------------------------\")\n#     # for img in subject_classes_imgs","metadata":{"id":"WlXS2GQ9dE4F","execution":{"iopub.status.busy":"2021-09-14T10:51:28.192245Z","iopub.status.idle":"2021-09-14T10:51:28.19305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# for class_ in CLASSES:\n#   print(class_, \" ----> \", len(record[(record[\"classname\"] == class_) & (record[\"subject\"] == \"p002\")]))","metadata":{"id":"E8FUJWPbdE4F","execution":{"iopub.status.busy":"2021-09-14T10:51:28.194334Z","iopub.status.idle":"2021-09-14T10:51:28.195109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # loading 2 subjects first:\n# # only p002 and p012 shd be filled\n# def print_current_length(subject_, original_path = new_dir, CLASSES = CLASSES):\n#   for class_ in CLASSES:\n#     path = original_path + \"/\" + subject_ + \"/\" + class_\n#     print(f\"len of {class_}: {len(os.listdir(path))}\")\n# print_current_length('p014')","metadata":{"id":"FSVTIqvIdE4F","execution":{"iopub.status.busy":"2021-09-14T10:51:28.196356Z","iopub.status.idle":"2021-09-14T10:51:28.197147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from imutils import paths\n# import shutil\n# import os\n# import time\n# from tqdm import tqdm\n# # loop over the data splits\n# # only things to change:\n# new_dir = \"/content/drive/MyDrive/DAP (Actual Idiots)/driverdata/imgs/revamped_2\"\n# if not os.path.exists(new_dir):\n#     os.makedirs(new_dir)\n# original_path = \"/content/drive/MyDrive/DAP (Actual Idiots)/driverdata/imgs/train\"\n\n# TRAIN = \"train\"\n# TEST = \"test\"\n# VAL = \"validation\"\n# # splits = [\"training\", \"evaluation\", \"validation\"]\n# for split in (TRAIN, VAL, TEST):\n# #     # grab all image paths in the current split\n# #     print(\"[INFO] processing '{} split'...\".format(split))\n# # \tp = os.path.sep.join([config.ORIG_INPUT_DATASET, split])\n# # \timagePaths = list(paths.list_images(p))\n# # \t# loop over the image paths\n    \n#     if split == TRAIN:\n#         position = 0\n#     elif split == VAL:\n#         position = 1\n#     else:\n#         position = 2\n#     split_path = new_dir + \"/\" + split\n#     if not os.path.exists(split_path):\n#         os.makedirs(split_path)\n#     # printing for first class only:\n#     for class_ in CLASSES[6:]:\n#         start = time.perf_counter()\n#         class_dir_path = split_path + \"/\" + class_\n#         if not os.path.exists(class_dir_path):\n#             os.makedirs(class_dir_path)\n#         # imagePaths = all_images[int(class_[-1])][position]\n#         imagePaths = string_to_list(df_reloaded[class_].loc[split])\n#         print(f'now we are going into {class_}')\n#         counter = 0\n#         for imagePath in tqdm(imagePaths):\n#             # print(f'{imagePath}')\n#             counter += 1\n#             # print(f\"{split} --> {counter}\")\n#             # extract class label from the filename\n#             filename = imagePath.split(os.path.sep)[-1]\n#     #             label = CLASSES[int(filename.split(\"_\")[0])]\n#     #             # construct the path to the output directory\n#     #             dirPath = os.path.sep.join([config.BASE_PATH, split, label])\n#     #             # if the output directory does not exist, create it\n#     #             if not os.path.exists(dirPath):\n#     #                 os.makedirs(dirPath)\n#             # construct the path to the output image file and copy it\n#             dest = os.path.sep.join([class_dir_path, filename])\n#             # shutil.copy2(original_path + \"/\" +class_ + \"/\" + imagePath, dest)\n#         end = time.perf_counter()\n#         print(f\"{class_} took {end - start}\")\n#         # listing whats currently in dir after copying finished for current split_class section:\n#         print(f\"{split} {class_}: \", len(os.listdir(class_dir_path)))","metadata":{"id":"0sF6jF9DdE4G","execution":{"iopub.status.busy":"2021-09-14T10:51:28.198481Z","iopub.status.idle":"2021-09-14T10:51:28.199314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # flow_from_dataframe()\n# # https://vijayabhaskar96.medium.com/tutorial-on-keras-imagedatagenerator-with-flow-from-dataframe-8bd5776e45c1\n\n# # df = None, x_col = absolute paths\n# # y_col = classname\n# generator = ImageDataGenerator(rotation_range=15, \n#                                             width_shift_range=0.2,\n#                                             height_shift_range=0.2,\n#                                             shear_range=0.2,\n#                                             zoom_range=0.2,\n#                                             horizontal_flip=True, \n#                                             fill_mode='nearest')\n\n# train_df = record[record[\"subject\"].isin(all_subjects[:2])].copy()\n# train_df[\"img_path\"] = \"/content/drive/MyDrive/DAP (Actual Idiots)/driverdata/imgs/train/\" + train_df[\"classname\"] + \"/\" +train_df[\"img\"]\n# # train_df\n# trainGen = generator.flow_from_dataframe(dataframe = train_df,  x_col = \"img_path\", y_col = \"classname\")\n\n# # dataframe, directory=None, x_col='filename', y_col='class',\n# #     weight_col=None, target_size=(256, 256), color_mode='rgb',\n# #     classes=None, class_mode='categorical', batch_size=32, shuffle=True,\n# #     seed=None, save_to_dir=None, save_prefix='',\n# #     save_format='png', subset=None, interpolation='nearest',\n# #     validate_filenames=True, **kwargs\n\n\n# # all_subjects[:2]","metadata":{"id":"SDI9DkApdE4G","execution":{"iopub.status.busy":"2021-09-14T10:51:28.200574Z","iopub.status.idle":"2021-09-14T10:51:28.201356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from tensorflow.keras import Sequential\n\n# from keras.models import Sequential\n# from keras.layers import Convolution2D, ZeroPadding2D, MaxPooling2D\n# from keras.layers.core import Flatten, Dense, Dropout, Lambda\n# from keras import backend as K\n# import h5py\n# from keras.optimizers import SGD\n\n# def VGG16_convolutions():\n#     model = Sequential()\n#     model.add(ZeroPadding2D((1,1),input_shape=(3,None,None)))\n#     model.add(Convolution2D(64, 3, 3, activation='relu', name='conv1_1'))\n#     model.add(ZeroPadding2D((1, 1)))\n#     model.add(Convolution2D(64, 3, 3, activation='relu', name='conv1_2'))\n#     model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n\n#     model.add(ZeroPadding2D((1, 1)))\n#     model.add(Convolution2D(128, 3, 3, activation='relu', name='conv2_1'))\n#     model.add(ZeroPadding2D((1, 1)))\n#     model.add(Convolution2D(128, 3, 3, activation='relu', name='conv2_2'))\n#     model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n\n#     model.add(ZeroPadding2D((1, 1)))\n#     model.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_1'))\n#     model.add(ZeroPadding2D((1, 1)))\n#     model.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_2'))\n#     model.add(ZeroPadding2D((1, 1)))\n#     model.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_3'))\n#     model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n\n#     model.add(ZeroPadding2D((1, 1)))\n#     model.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_1'))\n#     model.add(ZeroPadding2D((1, 1)))\n#     model.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_2'))\n#     model.add(ZeroPadding2D((1, 1)))\n#     model.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_3'))\n#     model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n\n#     model.add(ZeroPadding2D((1, 1)))\n#     model.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_1'))\n#     model.add(ZeroPadding2D((1, 1)))\n#     model.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_2'))\n#     model.add(ZeroPadding2D((1, 1)))\n#     model.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_3'))\n#     return model","metadata":{"execution":{"iopub.status.busy":"2021-09-14T10:51:28.202576Z","iopub.status.idle":"2021-09-14T10:51:28.203401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def get_model():\n#     model = VGG16_convolutions()\n#     model = load_model_weights(model, \"vgg16_weights.h5\")\n\n#     model.add(Lambda(global_average_pooling, \n#               output_shape=global_average_pooling_shape))\n#     model.add(Dense(2, activation = 'softmax', init='uniform'))\n#     sgd = SGD(lr=0.01, decay=1e-6, momentum=0.5, nesterov=True)\n#     model.compile(loss = 'categorical_crossentropy', \\\n#         optimizer = sgd, metrics=['accuracy'])\n#     return model","metadata":{"execution":{"iopub.status.busy":"2021-09-14T10:51:28.204557Z","iopub.status.idle":"2021-09-14T10:51:28.205407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def visualize_class_activation_map(model, img_path, output_path):\n# #     model = load_model(model_path)\n#     original_img = cv2.imread(img_path, 1)\n#     width, height, _ = original_img.shape\n\n#     #Reshape to the network input shape (3, w, h).\n#     img = np.array([np.transpose(np.float32(original_img), (2, 0, 1))])\n\n#     #Get the 512 input weights to the softmax.\n#     class_weights = model.layers[-1].get_weights()[0]\n#     final_conv_layer = get_output_layer(model, \"conv5_3\")\n#     get_output = K.function([model.layers[0].input], \\\n#                 [final_conv_layer.output, \n#     model.layers[-1].output])\n#     [conv_outputs, predictions] = get_output([img])\n#     conv_outputs = conv_outputs[0, :, :, :]\n\n#     #Create the class activation map.\n#     cam = np.zeros(dtype = np.float32, shape = conv_outputs.shape[1:3])\n#     target_class = 1\n#     for i, w in enumerate(class_weights[:, target_class]):\n#         cam += w * conv_outputs[i, :, :]\n#     return cam","metadata":{"execution":{"iopub.status.busy":"2021-09-14T10:51:28.206658Z","iopub.status.idle":"2021-09-14T10:51:28.20748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # cam is the activation map i think???\n# import matplotlib.pyplot as plt\n# import matplotlib.image as mpimg\n\n# cam = visualize_class_activation_map(get_model(), \n#                                      \"../input/distracteddriversrevampeddataset/test-revamped/test/c0/img_100796.jpg\", \"\")\n# # plt.imshow(cam)","metadata":{"execution":{"iopub.status.busy":"2021-09-14T10:51:28.208655Z","iopub.status.idle":"2021-09-14T10:51:28.209461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install keract","metadata":{"execution":{"iopub.status.busy":"2021-09-14T10:51:28.210678Z","iopub.status.idle":"2021-09-14T10:51:28.211583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from keract import display_activations\n# # The image path\n# img_path = '../images/brain-1.jpg'\n# # Preprocessing the image for the model\n# x = preprocess_image(img_path=img_path,model=model,resize=target_size)\n# # Generate the activations \n# activations = get_activations(model, x)","metadata":{"execution":{"iopub.status.busy":"2021-09-14T10:51:28.212829Z","iopub.status.idle":"2021-09-14T10:51:28.213665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"1lF9e24UdE4H"},"execution_count":null,"outputs":[]}]}