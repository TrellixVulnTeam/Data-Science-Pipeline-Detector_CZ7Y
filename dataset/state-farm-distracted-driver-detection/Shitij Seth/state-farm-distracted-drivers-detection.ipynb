{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport random\nimport shutil\nimport matplotlib.pyplot as plt\n\nfrom keras.applications import MobileNet\nfrom keras.layers.core import Dense, Activation\nfrom keras.optimizers import Adam, SGD\nfrom keras.metrics import categorical_crossentropy\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.applications import imagenet_utils\nfrom keras.layers import Dense,GlobalAveragePooling2D\nfrom keras.applications import MobileNet\nfrom keras.applications.mobilenet import preprocess_input\nfrom IPython.display import Image\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten,Input\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.utils import to_categorical\nfrom keras.preprocessing import image \nfrom keras.layers.normalization import BatchNormalization\nfrom keras.applications.vgg16 import VGG16, preprocess_input\nfrom keras.models import Model\nfrom keras import optimizers\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ModelCheckpoint,EarlyStopping\nfrom keras.models import load_model\n\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nimport itertools\nimport shutil\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport numpy as np\nimport os\nfrom keras.preprocessing import image\nimport cv2\nimport matplotlib.pyplot as plt\nfrom keras.utils import to_categorical","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Load data file\ndata_file = pd.read_csv(\"/kaggle/input/state-farm-distracted-driver-detection/driver_imgs_list.csv\")\ndata_file","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a new directory\nbase_dir = 'base_dir'\nos.mkdir(base_dir)\n\n#[CREATE FOLDERS INSIDE THE BASE DIRECTORY]\n\n# now we create 2 folders inside 'base_dir':\n# train_dir\n    # 10 subfolders\n# val_dir\n    # 10 subfolders\n\n# create a path to 'base_dir' to which we will join the names of the new folders\n# train_dir\ntrain_dir = os.path.join(base_dir, 'train_dir')\nos.mkdir(train_dir)\n\n# val_dir\nval_dir = os.path.join(base_dir, 'val_dir')\nos.mkdir(val_dir)\n\n# [CREATE FOLDERS INSIDE THE TRAIN AND VALIDATION FOLDERS]\n# Inside each folder we create seperate folders for each class\n\n# create new folders inside train_dir\nc0 = os.path.join(train_dir, 'c0')\nos.mkdir(c0)\nc1 = os.path.join(train_dir, 'c1')\nos.mkdir(c1)\nc2 = os.path.join(train_dir, 'c2')\nos.mkdir(c2)\nc3 = os.path.join(train_dir, 'c3')\nos.mkdir(c3)\nc4 = os.path.join(train_dir, 'c4')\nos.mkdir(c4)\nc5 = os.path.join(train_dir, 'c5')\nos.mkdir(c5)\nc6 = os.path.join(train_dir, 'c6')\nos.mkdir(c6)\nc7 = os.path.join(train_dir, 'c7')\nos.mkdir(c7)\nc8 = os.path.join(train_dir, 'c8')\nos.mkdir(c8)\nc9 = os.path.join(train_dir, 'c9')\nos.mkdir(c9)\n\n\n# create new folders inside val_dir\nc0 = os.path.join(val_dir, 'c0')\nos.mkdir(c0)\nc1 = os.path.join(val_dir, 'c1')\nos.mkdir(c1)\nc2 = os.path.join(val_dir, 'c2')\nos.mkdir(c2)\nc3 = os.path.join(val_dir, 'c3')\nos.mkdir(c3)\nc4 = os.path.join(val_dir, 'c4')\nos.mkdir(c4)\nc5 = os.path.join(val_dir, 'c5')\nos.mkdir(c5)\nc6 = os.path.join(val_dir, 'c6')\nos.mkdir(c6)\nc7 = os.path.join(val_dir, 'c7')\nos.mkdir(c7)\nc8 = os.path.join(val_dir, 'c8')\nos.mkdir(c8)\nc9 = os.path.join(val_dir, 'c9')\nos.mkdir(c9)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check that the folders have been created\nos.listdir('base_dir/train_dir')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.listdir('base_dir/val_dir')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_drivers = random.choices(data_file['subject'].unique().tolist(), k=4)\nval_drivers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imgs = list(data_file['img'])\n\ni = 0\n\nfor img in imgs:\n    i += 1\n    label = data_file.loc[data_file['img'] == img, 'classname'].item()\n    if data_file.loc[data_file['img'] == img, 'subject'].item() in val_drivers:\n        # source path to image\n        src = os.path.join('../input/state-farm-distracted-driver-detection/imgs/train', label, img)\n        # destination path to image\n        dst = os.path.join(val_dir, label, img)\n        # copy the image from the source to the destination\n        shutil.copyfile(src, dst)\n    else:\n        # source path to image\n        src = os.path.join('../input/state-farm-distracted-driver-detection/imgs/train', label, img)\n        # destination path to image\n        dst = os.path.join(train_dir, label, img)\n        # copy the image from the source to the destination\n        shutil.copyfile(src, dst)\n        \n    if i%1000 == 0:\n        print(i)\nprint(i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check how many train images we have in each folder\nval = len(os.listdir('base_dir/train_dir/c0')) + len(os.listdir('base_dir/train_dir/c1')) + len(os.listdir('base_dir/train_dir/c2')) + len(os.listdir('base_dir/train_dir/c3')) + len(os.listdir('base_dir/train_dir/c4')) + len(os.listdir('base_dir/train_dir/c5')) + len(os.listdir('base_dir/train_dir/c6')) + len(os.listdir('base_dir/train_dir/c7')) + len(os.listdir('base_dir/train_dir/c8')) + len(os.listdir('base_dir/train_dir/c9'))\n\nprint(len(os.listdir('base_dir/train_dir/c0')) + len(os.listdir('base_dir/train_dir/c1')) + len(os.listdir('base_dir/train_dir/c2')) + len(os.listdir('base_dir/train_dir/c3')) + len(os.listdir('base_dir/train_dir/c4')) + len(os.listdir('base_dir/train_dir/c5')) + len(os.listdir('base_dir/train_dir/c6')) + len(os.listdir('base_dir/train_dir/c7')) + len(os.listdir('base_dir/train_dir/c8')) + len(os.listdir('base_dir/train_dir/c9')))\nprint(len(os.listdir('base_dir/val_dir/c0')) + len(os.listdir('base_dir/val_dir/c1')) + len(os.listdir('base_dir/val_dir/c2')) + len(os.listdir('base_dir/val_dir/c3')) + len(os.listdir('base_dir/val_dir/c4')) + len(os.listdir('base_dir/val_dir/c5')) + len(os.listdir('base_dir/val_dir/c6')) + len(os.listdir('base_dir/val_dir/c7')) + len(os.listdir('base_dir/val_dir/c8')) + len(os.listdir('base_dir/val_dir/c9')))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Load val Images\n        \nval_image = []\nval_label = []\n\nfor i in range(10):\n    print('Loading images from folder C',i)\n    imgs = os.listdir('base_dir/val_dir/c' + str(i))\n    for j in range(len(imgs)):\n\n        img_name = 'base_dir/val_dir/c'+str(i)+\"/\"+imgs[j]\n        img = cv2.imread(img_name)\n        img = cv2.resize(img, (224,224))\n        val_image.append(img)\n        val_label.append(i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# #Load Images\n# import cv2\n        \n# train_image = []\n# image_label = []\n\n# for i in range(10):\n#     print('Loading images from folder C',i)\n#     imgs = os.listdir('../input/state-farm-distracted-driver-detection/imgs/train/c' + str(i))\n#     for j in range(len(imgs)):\n\n#         img_name = '../input/state-farm-distracted-driver-detection/imgs/train/c'+str(i)+\"/\"+imgs[j]\n#         img = cv2.imread(img_name)\n#         img = cv2.resize(img, (224,224))\n#         label = i\n#         driver = data_file[data_file['img'] == imgs[j]]['subject'].values[0]\n#         train_image.append([img,label,driver])\n#         image_label.append(i)\n        \n\n# import random\n# random.shuffle(train_image)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ## getting list of driver names\n\n# D = []\n# for features,labels,drivers in train_image:\n#     D.append(drivers)\n\n# ## Deduplicating drivers\n\n# deduped = []\n\n# for i in D:\n#     if i not in deduped:\n#         deduped.append(i)\n    \n\n# ## selecting random drivers for the validation set\n# driv_selected = []\n# import random\n# driv_nums = random.sample(range(len(deduped)), 6)\n# for i in driv_nums:\n#     driv_selected.append(deduped[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# X_train= []\n# y_train = []\n# X_test = []\n# y_test = []\n# D_train = []\n# D_test = []\n\n# for features,labels,drivers in train_image:\n#     if drivers in driv_selected:\n#         X_test.append(features)\n#         y_test.append(labels)\n#         D_test.append(drivers)\n    \n#     else:\n#         X_train.append(features)\n#         y_train.append(labels)\n#         D_train.append(drivers)\n    \n# print (len(X_train),len(X_test))\n# print (len(y_train),len(y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from keras.utils import to_categorical\n# x_train, y_train = np.asarray(X_train), np.asarray(y_train)\n# x_test, y_test = np.asarray(X_test), np.asarray(y_test)\n\n# x_train = np.array(x_train, dtype=np.uint8).reshape(-1,224,224,3)\n# x_test = np.array(x_test, dtype=np.uint8).reshape(-1,224,224,3)\n\n# # x_train = x_train.astype('float32')\n# # x_test = x_test.astype('float32')\n\n# # # normalize to range 0-1\n# # x_train /= 255.0\n# # x_test /= 255.0\n\n# y_train = to_categorical(y_train)\n# y_test = to_categorical(y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print(x_train[0], y_train.shape)\n# print(x_test.shape, y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploring the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"# print('Unique drivers: ', data_file['subject'].nunique())\n# print('Number of images per driver')\n# print(data_file['subject'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import matplotlib.pyplot as plt\n# plt.imshow(cv2.cvtColor(x_train[100], cv2.COLOR_BGR2RGB))\n# plt.show()\n# print(y_train[100])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# CNN Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# from keras.utils import to_categorical\n# from keras.models import Sequential\n# from keras.layers import *\n# from keras.optimizers import RMSprop\n# import keras\n# from keras.callbacks import EarlyStopping\n\n\n# model = Sequential()\n\n# size = 16\n\n# model.add(Conv2D(size*2, (3, 3), padding='same', input_shape=(224,224,1)))\n# model.add(Activation('elu'))\n# model.add(Conv2D(size*2, (3, 3)))\n# model.add(Activation('elu'))\n# model.add(MaxPooling2D(pool_size=(2, 2)))\n# model.add(Dropout(0.5))\n\n# model.add(Conv2D(size*4, (3, 3), padding='same'))\n# model.add(Activation('elu'))\n# model.add(Conv2D(size*4, (3, 3)))\n# model.add(Activation('elu'))\n# model.add(MaxPooling2D(pool_size=(2, 2)))\n# model.add(Dropout(0.5))\n\n# model.add(Conv2D(size*8, (3, 3), padding='same'))\n# model.add(Activation('elu'))\n# model.add(Conv2D(size*8, (3, 3)))\n# model.add(Activation('elu'))\n# model.add(MaxPooling2D(pool_size=(2, 2)))\n# model.add(Dropout(0.5))\n\n\n# model.add(Flatten())\n# model.add(Dense(size*16))\n# model.add(Activation('elu'))\n# model.add(Dropout(0.5))\n# model.add(Dense(10))\n# model.add(Activation('softmax'))\n\n# # initiate RMSprop optimizer\n# opt = keras.optimizers.RMSprop(lr=0.0001, decay=1e-6)\n\n# # Let's train the model using RMSprop\n# model.compile(loss='categorical_crossentropy',\n#               optimizer=opt,\n#               metrics=['accuracy'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# history = model.fit(x_train, y_train, \n#           validation_data=(x_test, y_test),\n#           epochs=20, batch_size=32, callbacks = EarlyStopping(monitor='val_loss', mode='min', patience=5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plt.figure(figsize=(8,8))\n# plt.subplot(211)\n# plt.title('Cross Entropy Loss')\n# plt.plot(history.history['loss'], color='blue', label='train')\n# plt.plot(history.history['val_loss'], color='orange', label='test')\n# # plot accuracy\n# plt.subplot(212)\n# plt.title('Classification Accuracy')\n# plt.plot(history.history['accuracy'], color='blue', label='train')\n# plt.plot(history.history['val_accuracy'], color='orange', label='test')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from keras.models import load_model\n\n# model.save('CNN_SF_distracted_drivers_2.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# _, acc = model.evaluate(x_test, y_test, verbose=10)\n# print(acc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **MobileNet model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# base_model=MobileNet(weights='imagenet',include_top=False) #imports the mobilenet model and discards the last 1000 neuron layer.\n\n# final_layer = base_model.output\n# final_layer = GlobalAveragePooling2D()(final_layer)\n\n# pred=Dense(10,activation='softmax')(final_layer) #final layer with softmax activation\n\n# model = Model(inputs=base_model.input, outputs=pred)\n\n# model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# opt = optimizers.SGD(lr = 0.005)\n# # opt = optimizers.Adam(lr=0.001)\n\n\n# model.compile(optimizer=opt,loss='categorical_crossentropy',metrics=['accuracy']) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# filepath = \"mobilenet_sgd0.001_4_v2.h5\"\n# checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n# earlystopper = EarlyStopping(monitor='val_loss', patience=10, verbose=1)\n\n# datagen_val = ImageDataGenerator()\n\n# datagen_train = ImageDataGenerator(height_shift_range=0.5,width_shift_range = 0.5,zoom_range = 0.5,rotation_range=30)\n\n# train_path = 'base_dir/train_dir'\n# valid_path = 'base_dir/val_dir'\n\n# IMAGE_SIZE = 224\n\n# train_gen = datagen_train.flow_from_directory(train_path,\n#                                         target_size=(IMAGE_SIZE,IMAGE_SIZE),\n#                                         batch_size=64,\n#                                         class_mode='categorical')\n\n# val_gen = datagen_val.flow_from_directory(valid_path,\n#                                         target_size=(IMAGE_SIZE,IMAGE_SIZE),\n#                                         batch_size=64,\n#                                         class_mode='categorical')\n\n# #datagen.fit(X_train)\n# # data_generator = datagen.flow(x_train, y_train, batch_size = 4)\n\n# # Fits the model on batches with real-time data augmentation:\n# mobilenet_model = model.fit_generator(train_gen,steps_per_epoch = val / 64, callbacks=[checkpoint,earlystopper],\n#                         epochs = 25, verbose = 1, validation_data = val_gen)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plt.figure(figsize=(8,8))\n# plt.subplot(211)\n# plt.title('Cross Entropy Loss')\n# plt.plot(mobilenet_model.history['loss'], color='blue', label='train')\n# plt.plot(mobilenet_model.history['val_loss'], color='orange', label='test')\n# # plt.ylim(0,1)\n# # plot accuracy\n# plt.subplot(212)\n# plt.title('Classification Accuracy')\n# plt.plot(mobilenet_model.history['accuracy'], color='blue', label='train')\n# plt.plot(mobilenet_model.history['val_accuracy'], color='orange', label='test')\n# plt.ylim(0,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# #delete base_dir\n\n# shutil.rmtree('base_dir')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# val_image = np.array(val_image, dtype=np.uint8).reshape(-1,224,224,3)\n\n# val_label = to_categorical(val_label)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# predictions = model.predict(val_image, steps=len(val_image), verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pred = predictions.argmax(axis=1)\n# pred_val = val_label.argmax(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# cm = confusion_matrix(pred_val, pred)\n# print(cm)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# VGG16 Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import Input\nvgg16_input = Input(shape = (224, 224, 3), name = 'Image_input')\n\n\n## The VGG model\n\nfrom keras.applications.vgg16 import VGG16, preprocess_input\n\n#Get back the convolutional part of a VGG network trained on ImageNet\nmodel_vgg16_conv = VGG16(weights='imagenet', include_top=False, input_tensor = vgg16_input)\nmodel_vgg16_conv.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Use the generated model \n\noutput_vgg16_conv = model_vgg16_conv(vgg16_input)\n\n#Add the fully-connected layers \n\nx = Flatten(name='flatten')(output_vgg16_conv)\n\nx = Dense(10, activation='softmax', name='predictions')(x)\n\nvgg16_pretrained = Model(inputs = vgg16_input, outputs = x)\nvgg16_pretrained.summary()\n\n# Compile CNN model\nsgd = optimizers.SGD(lr = 0.005)\nvgg16_pretrained.compile(loss='categorical_crossentropy',optimizer = sgd,metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"datagen_val = ImageDataGenerator(height_shift_range=0.5,width_shift_range = 0.5,zoom_range = 0.5,rotation_range=30,rescale=1./255)\n\ndatagen_train = ImageDataGenerator(height_shift_range=0.5,width_shift_range = 0.5,zoom_range = 0.5,rotation_range=30, rescale=1./255)\n\ntrain_path = 'base_dir/train_dir'\nvalid_path = 'base_dir/val_dir'\n\nIMAGE_SIZE = 224\n\ntrain_gen = datagen_train.flow_from_directory(train_path,\n                                        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n                                        batch_size=64,\n                                        class_mode='categorical')\n\nval_gen = datagen_val.flow_from_directory(valid_path,\n                                        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n                                        batch_size=64,\n                                        class_mode='categorical')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpoint = ModelCheckpoint(\"vgg16_sgd0.001_4_{epoch:02d}_{val_accuracy:.2f}.h5\", monitor='val_accuracy', verbose=1, mode='max', save_freq='epoch')\nearlystopper = EarlyStopping(monitor='val_loss', patience=10, verbose=1)\n\n# Fits the model on batches with real-time data augmentation:\nvgg16_model = vgg16_pretrained.fit(train_gen,steps_per_epoch = val / 64, callbacks=[checkpoint,earlystopper],\n                        epochs = 25, verbose = 1, validation_data = val_gen)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,8))\nplt.subplot(211)\nplt.title('Cross Entropy Loss')\nplt.plot(vgg16_model.history['loss'], color='blue', label='train')\nplt.plot(vgg16_model.history['val_loss'], color='orange', label='test')\n# plt.ylim(0,1)\n# plot accuracy\nplt.subplot(212)\nplt.title('Classification Accuracy')\nplt.plot(vgg16_model.history['accuracy'], color='blue', label='train')\nplt.plot(vgg16_model.history['val_accuracy'], color='orange', label='test')\nplt.ylim(0,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#delete base_dir\n\nshutil.rmtree('base_dir')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_image = np.array(val_image, dtype=np.uint8).reshape(-1,224,224,3)\nval_label = to_categorical(val_label)\nprint(val_label)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_image = val_image.astype('float32')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_valid = val_label.argmax(axis=1)\ncm1 = confusion_matrix(pred_valid, pred_valid)\nprint(cm1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model = load_model('../input/state-farm-distracted-drivers-detection/vgg16_sgd0.001_4_10_0.79.h5')\n\npredictions = vgg16_pretrained.predict(val_image, steps=len(val_image), verbose=1)\n\npred = predictions.argmax(axis=1)\npred_val = val_label.argmax(axis=1)\n\ncm = confusion_matrix(pred_val, pred)\nprint(cm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test_data_dir = r'../input/state-farm-distracted-driver-detection/imgs/test/'\n\n# class_labels = [\n#     \"normal driving\",\n#     \"texting - right\",\n#     \"talking on the phone - right\",\n#     \"texting - left\",\n#     \"talking on the phone - left\",\n#     \"operating the radio\",\n#     \"drinking\",\n#     \"reaching behind\",\n#     \"hair and makeup\",\n#     \"talking to passenger\"]\n\n# file_names = np.random.choice(os.listdir(test_data_dir),50)\n\n# img_arrays = []\n\n# for file_name in file_names:\n#     img = image.load_img(os.path.join(test_data_dir, file_name), target_size=(224, 224))\n#     img_array = image.img_to_array(img)\n#     img_arrays.append(img_array)\n\n# img_arrays = np.array(img_arrays)\n# # img_arrays = img_arrays.astype('float32')\n# predictions = model.predict(np.array(img_arrays).reshape(-1,224,224,3))\n# # predictions = model.predict(img_arrays)\n\n\n# label_indexes = np.argmax(predictions, axis=1)\n# probabilities = np.max(predictions, axis=1)\n\n# for (file_name, label_index, probability) in zip(file_names, label_indexes, probabilities):\n\n#     label_with_probability = \"{}: {:.2f}%\".format(class_labels[label_index], probability * 100)\n\n#     import cv2\n\n#     image = cv2.imread(os.path.join(test_data_dir, file_name))\n\n#     cv2.putText(image, label_with_probability.upper(), (100, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 2)\n    \n#     plt.imshow(image)\n#     plt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}