{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"#!pip install datasets","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true,"scrolled":false},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport tensorflow as tf\nimport pickle\n#from datasets import load_dataset\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"scrolled":false},"cell_type":"code","source":"from transformers import AutoTokenizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"!pip install nlp\nimport nlp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"max_length = 150\nbatch_size = 256\nlabel_size = 3\nentries_per_file = 512","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"#train = pd.read_csv(\"../input/contradictory-my-dear-watson/train.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#prems = list(train.premise.values)\n#hyps = list(train.hypothesis.values)\n#labels = list(train.label.values)\n\nprems = list()\nhyps = list()\nlabels = list()\n\ndup_set = set()\n\ndef add_to_training(prem_list, hyps_list, lab_list):\n    if len(prem_list) > 1:\n        print(f'sizes before: {len(prems)} {len(hyps)} {len(labels)}')\n    duplicate = 0\n    for p, h, l in zip(prem_list, hyps_list, lab_list):\n        k = p + h\n        if k in dup_set:\n            duplicate += 1\n            continue\n        dup_set.add(k)\n        prems.append(p)\n        hyps.append(h)\n        labels.append(l)\n    if len(prem_list) > 1:\n        print(f'sizes after: {len(prems)} {len(hyps)} {len(labels)}')\n    return duplicate","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"mnli = nlp.load_dataset(path='glue', name='mnli')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"add_to_training(mnli['train']['premise'], mnli['train']['hypothesis'], mnli['train']['label'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def add_data_from_xnli(filename):\n#     df = pd.read_csv('../input/xnlni-zip-file-from-nyu/XNLI-1.0/xnli.dev.tsv', sep='\\t')\n#     ls = [{'entailment': 0, 'neutral': 1, 'contradiction': 2}[x] for x in df.gold_label.values]\n#     add_to_training(df.sentence1.values, df.sentence2.values, ls)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# add_data_from_xnli('../input/xnlni-zip-file-from-nyu/XNLI-1.0/xnli.dev.tsv')\n# add_data_from_xnli('../input/xnlni-zip-file-from-nyu/XNLI-1.0/xnli.test.tsv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"snli = nlp.load_dataset(path='snli')\nadd_to_training(snli['train']['premise'], snli['train']['hypothesis'], snli['train']['label'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"args = {'max_length': max_length, 'pad_to_max_length': True, 'truncation': 'longest_first', 'return_attention_mask': True}\n#args = {'return_tensors': \"tf\", 'max_length': max_length, 'pad_to_max_length': True, 'truncation': True, 'return_attention_mask': True, 'return_token_type_ids': True}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"keys = ['input_ids', 'attention_mask']\n\n\ndef get_feature_description(feat_size, label_size=None):\n    feature_description = {\n        keys[0]: tf.io.FixedLenFeature([feat_size], tf.int64),\n        keys[1]: tf.io.FixedLenFeature([feat_size], tf.int64)\n    }\n\n    if label_size is not None:\n        feature_description.update({'labels': tf.io.FixedLenFeature([label_size], tf.float32)})\n\n    return feature_description\n\n\ndef _float_feature(value):\n    \"\"\"Returns a float_list from a float / double.\"\"\"\n    value = np.ravel(value)\n    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n\n\ndef _int64_feature(value):\n    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n    value = np.ravel(value)\n    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n\n\ndef serialize_sentence_and_label(x0, x1, y=None):\n    \"\"\"\n    Creates a tf.train.Example message ready to be written to a file.\n    \"\"\"\n    feature = {k: _int64_feature(v) for k, v in zip(keys, [x0, x1])}\n    if y is not None:\n        feature['labels'] = _float_feature(y)\n    proto_ = tf.train.Example(features=tf.train.Features(feature=feature))\n    return proto_.SerializeToString()\n\n\ndef write_tf_records_with_labels(filename, x0_list, x1_list, y_list=None, max_lines=None):\n    file_index = 0\n    current_index = 0\n    list_of_files = []\n    assert len(x0_list) == len(x1_list)\n    if y_list is not None:\n        assert len(x0_list) == len(y_list)\n    max_len = len(x0_list)\n    if max_lines is None:\n        max_lines = max_len\n    run = True\n    while run:\n        fn = f'{filename}_{file_index}'\n        list_of_files.append(fn)\n        with tf.io.TFRecordWriter(fn) as writer:\n            for i in range(current_index, min(current_index + max_lines, max_len)):\n                label = None if y_list is None else y_list[i]\n                example = serialize_sentence_and_label(x0_list[i], x1_list[i], label)\n                writer.write(example)\n            current_index = i + 1\n            if current_index == max_len:\n                break\n        print(f'max line for file {fn}')\n        file_index += 1\n    return list_of_files","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### turn sparse labels into one hot triplets"},{"metadata":{"trusted":true},"cell_type":"code","source":"arr = np.arange(len(labels))\nnp.random.shuffle(arr)\ndef shuffle(l):\n    assert len(l) == len(arr)\n    return [l[i] for i in arr]\nlabels = shuffle(labels)\nprems = shuffle(prems)\nhyps = shuffle(hyps)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_triplet(labels):\n    triplets = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]\n    one_hot_labels = np.array([triplets[n] for n in labels])\n    return one_hot_labels\n\none_hot_labels = make_triplet(labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def round_to_batch_size(li, batch_size):\n    le = len(li) // batch_size * batch_size\n    return li[:le]\n\ndef encode_all(s1, s2, batch_size=None):\n    assert len(s1) == len(s2)\n    if batch_size is None:\n        batch_size = 1\n    else:\n        s1, s2 = round_to_batch_size(s1, batch_size), round_to_batch_size(s2, batch_size)\n    results = {k: list() for k in keys}\n    nb_of_batches = len(s1) // batch_size \n    for i in range(nb_of_batches):\n        if i % 1000 == 0:\n            print(i)\n        s1_tmp = s1[i*batch_size:(i+1)*batch_size]\n        s2_tmp = s2[i*batch_size:(i+1)*batch_size]\n        tok = tokenizer.batch_encode_plus([(p, h) for p, h in zip(s1_tmp, s2_tmp)], **args)\n        for k in keys:\n            results[k].append(tok[k])\n    return results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = encode_all(prems, hyps, batch_size=batch_size)\none_hot_labels = round_to_batch_size(one_hot_labels, batch_size)\none_hot_labels = one_hot_labels.reshape((-1, batch_size, label_size))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'number of training exemples: {len(labels)}')\nwrite_tf_records_with_labels('./train_data.proto', train_data[keys[0]], train_data[keys[1]], one_hot_labels, max_lines=entries_per_file)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install googletrans\nfrom googletrans import Translator","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv(\"../input/contradictory-my-dear-watson/test.csv\")\ntranslator = Translator()\ndef translate_row(row):\n    if row['lang_abv'] == 'en':\n        return row['premise'], row['hypothesis']\n    return pd.Series([translator.translate(row['premise']).text, translator.translate(row['hypothesis']).text], index=['translated_premise', 'translated_hypothesis'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"test = pd.read_csv(\"../input/contradictory-my-dear-watson/test.csv\")\ntranslated_df = test.apply(translate_row, axis=1)\nprems_test = list(translated_df.translated_premise.values)\nhyps_test = list(translated_df.translated_hypothesis.values)\ntest_data = encode_all(prems_test, hyps_test)\nwrite_tf_records_with_labels('./test_data.proto', test_data[keys[0]], test_data[keys[1]])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Initial training data alone"},{"metadata":{"trusted":true},"cell_type":"code","source":"# train = pd.read_csv(\"../input/contradictory-my-dear-watson/train.csv\")\n# prems_train = list(train.premise.values)\n# hyps_train = list(train.hypothesis.values)\n# one_hot_labels = make_triplet(train.label.values)\n# one_hot_labels = round_to_batch_size(one_hot_labels, batch_size)\n# one_hot_labels = one_hot_labels.reshape((-1, batch_size, label_size))\n# train_data = encode_all(prems_train, hyps_train, batch_size=batch_size)\n# write_tf_records_with_labels('./initial_train_data.proto', train_data[keys[0]], train_data[keys[1]], one_hot_labels)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}