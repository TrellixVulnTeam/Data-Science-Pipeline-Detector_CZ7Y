{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"!pip install transformers==3.0.2\n!pip install nlp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gc\nimport os\nimport pandas as pd \nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom nlp import load_dataset\nfrom transformers import TFAutoModel, AutoTokenizer\n\nimport tensorflow as tf\nfrom tensorflow.keras.utils import get_custom_objects\nfrom tensorflow.keras import Input, Model, Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, LSTM, Embedding, GlobalAveragePooling1D\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras import regularizers\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle\n\nnp.random.seed(123)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Detect hardware, return appropriate distribution strategy\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n    # set: this is always the case on Kaggle.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MODEL = 'jplu/tf-xlm-roberta-large'\n\nEPOCHS = 4\nMAXLEN = 120\n\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL)\nauto = tf.data.experimental.AUTOTUNE","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model(transformer_layer,learning_rate):\n    input_ids = Input(shape = (MAXLEN,), dtype = tf.int32)\n    #input_masks = Input(shape = (MAXLEN,), dtype = tf.int32)\n\n    #insert roberta layer\n    roberta = TFAutoModel.from_pretrained(transformer_layer)\n    roberta = roberta([input_ids])[0]\n\n    out = GlobalAveragePooling1D()(roberta)\n\n    #add our softmax layer\n    out = Dense(3, activation = 'softmax')(out)\n\n    #assemble model and compile\n    model = Model(inputs = input_ids, outputs = out)\n    model.compile(\n                    optimizer = Adam(lr = learning_rate), \n                    loss = 'sparse_categorical_crossentropy', \n                    metrics = ['accuracy'])\n    return model\n\n\ndef tokeniZer(text,tokenizer):\n    # tokenize\n    encoded = tokenizer.batch_encode_plus(text, padding=True, max_length=MAXLEN, truncation=True)\n    #return np.array(encoded['input_ids'], encoded['attention_mask'])\n    return np.array(encoded['input_ids'])\n\ndef tokeniZer_old(dataset,tokenizer):\n    # tokenize\n    text = dataset[['premise', 'hypothesis']].values.tolist()\n    encoded = tokenizer.batch_encode_plus(text, padding=True, max_length=MAXLEN, truncation=True)\n    # features\n    x = encoded['input_ids'], encoded['attention_mask']\n    # labels\n    y = None\n    if 'label' in dataset.columns:\n        y = dataset.label.values\n    return encoded['input_ids'], encoded['attention_mask'], y\n\n\ndef load_mnli(use_validation=True):\n    result = []\n    dataset = load_dataset(path='glue', name='mnli')\n    print(dataset['train'])\n    keys = ['train', 'validation_matched','validation_mismatched'] if use_validation else ['train']\n    for k in keys:\n        for record in dataset[k]:\n            c1, c2, c3 = record['premise'], record['hypothesis'], record['label']\n            if c1 and c2 and c3 in {0,1,2}:\n                result.append((c1,c2,c3,'en'))\n    result = pd.DataFrame(result, columns=['premise','hypothesis','label','lang_abv'])\n    return result\n\ndef load_snli(use_validation=True):\n    result = []\n    dataset = load_dataset(path='snli')\n    keys = ['train', 'validation'] if use_validation else ['train']\n    for k in keys:\n        for record in dataset[k]:\n            c1, c2, c3 = record['premise'], record['hypothesis'], record['label']\n            if c1 and c2 and c3 in {0,1,2}:\n                result.append((c1,c2,c3,'en'))\n    result = pd.DataFrame(result, columns=['premise','hypothesis','label','lang_abv'])\n    return result\n\ndef load_xnli():\n    result = []\n    dataset = load_dataset(path='xnli')\n    for k in dataset.keys():\n        for record in dataset[k]:\n            hp, pr, lb = record['hypothesis'], record['premise'], record['label']\n            if hp and pr and lb in {0,1,2}:\n                for lang, translation in zip(hp['language'], hp['translation']):\n                    pr_lang = pr.get(lang, None)\n                    if pr_lang is None:\n                        continue\n                    result.append((pr_lang, translation, lb,lang))\n    result = pd.DataFrame(result, columns=['premise','hypothesis','label','lang_abv'])\n    return result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load data\ntrain = pd.read_csv('/kaggle/input/contradictory-my-dear-watson/train.csv')\ntest = pd.read_csv('/kaggle/input/contradictory-my-dear-watson/test.csv')\nsubmission = pd.read_csv('/kaggle/input/contradictory-my-dear-watson/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mnli = load_mnli()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mnli.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mnli.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total_train = train.drop(columns=['id'])\ntotal_train = pd.concat([total_train, mnli], axis=0)\n\nshuffled_data = shuffle(total_train).reset_index(drop = True)\n\nX, y = shuffled_data[['premise', 'hypothesis']].values.tolist(), shuffled_data['label']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train, x_valid, y_train, y_valid = train_test_split(X, y, test_size=0.25, random_state=2020)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = tokeniZer(x_train,tokenizer)\nx_valid = tokeniZer(x_valid,tokenizer)\nx_test  = tokeniZer(test[['premise', 'hypothesis']].values.tolist(),tokenizer)\n\ndel mnli\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# datasets\ndef build_dataset(x, y, mode, batch_size):\n    if mode == \"train\":\n        dataset = (\n            tf.data.Dataset\n            .from_tensor_slices((x, y))\n            .repeat()\n            .shuffle(2048)\n            .batch(batch_size)\n            .prefetch(auto)\n        )\n    elif mode == \"valid\":\n        dataset = (\n            tf.data.Dataset\n            .from_tensor_slices((x, y))\n            .batch(BATCH_SIZE)\n            .cache()\n            .prefetch(auto)\n        )\n    elif mode == \"test\":\n        dataset = (\n            tf.data.Dataset\n            .from_tensor_slices(x)\n            .batch(BATCH_SIZE)\n        )\n    else:\n        raise NotImplementedError\n    return dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset = build_dataset(x_train, y_train, \"train\", BATCH_SIZE)\nvalid_dataset = build_dataset(x_valid, y_valid, \"valid\", BATCH_SIZE)\ntest_dataset  = build_dataset(x_test, None, \"test\", BATCH_SIZE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_xlm(transformer_layer,random_seed,learning_rate):\n    tf.keras.backend.clear_session()\n    tf.random.set_seed(random_seed)\n    with strategy.scope():\n        model = create_model(transformer_layer,learning_rate)\n    model.summary()    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Xlm = create_xlm(MODEL,1124234,1e-5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"callbacks = [tf.keras.callbacks.EarlyStopping(patience = 2, \n                                              monitor = 'val_loss', \n                                              restore_best_weights = True, \n                                              mode = 'min')]\n\nsteps_per_epoch = len(x_train) // BATCH_SIZE\nhistory_xlm = Xlm.fit(train_dataset,\n                      validation_data=valid_dataset,\n                      steps_per_epoch=steps_per_epoch,\n                      epochs = EPOCHS, \n                      callbacks = callbacks)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history_xlm.history['accuracy'])\nplt.plot(history_xlm.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\n# Plot training & validation loss values\nplt.plot(history_xlm.history['loss'])\nplt.plot(history_xlm.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#model predictions\npredictions_xlm = Xlm.predict(test_dataset)\npredictions = predictions_xlm\nfinal = np.argmax(predictions, axis = 1)    \n\nsubmission = pd.DataFrame()    \nsubmission['id'] = test['id']\nsubmission['prediction'] = final.astype(np.int32)\n\nsubmission.to_csv('submission.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}