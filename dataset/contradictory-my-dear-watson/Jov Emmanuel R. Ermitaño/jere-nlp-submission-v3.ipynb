{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nos.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices'","metadata":{"_uuid":"d3a20929-8e1d-48d2-869c-cc57f8c63cc9","_cell_guid":"20666a1f-e31b-4134-94f8-fea9a50998d3","execution":{"iopub.status.busy":"2022-05-25T21:16:11.512638Z","iopub.execute_input":"2022-05-25T21:16:11.512948Z","iopub.status.idle":"2022-05-25T21:16:11.52136Z","shell.execute_reply.started":"2022-05-25T21:16:11.512916Z","shell.execute_reply":"2022-05-25T21:16:11.520295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/input/contradictory-my-dear-watson/train.csv')\ndf_test = pd.read_csv('/kaggle/input/contradictory-my-dear-watson/test.csv')","metadata":{"execution":{"iopub.status.busy":"2022-05-25T21:16:12.353829Z","iopub.execute_input":"2022-05-25T21:16:12.354295Z","iopub.status.idle":"2022-05-25T21:16:12.595795Z","shell.execute_reply.started":"2022-05-25T21:16:12.354259Z","shell.execute_reply":"2022-05-25T21:16:12.595136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Preprocessing","metadata":{}},{"cell_type":"code","source":"df_train.shape, df_test.shape\n# The Train Dataset has 12,1120 rows and 6 columns.\n# Meanwhile, the Test Dataset has 5,195 rows and 5 columns.","metadata":{"execution":{"iopub.status.busy":"2022-05-25T21:16:14.432015Z","iopub.execute_input":"2022-05-25T21:16:14.432861Z","iopub.status.idle":"2022-05-25T21:16:14.44041Z","shell.execute_reply.started":"2022-05-25T21:16:14.432794Z","shell.execute_reply":"2022-05-25T21:16:14.439617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-05-25T21:16:15.064918Z","iopub.execute_input":"2022-05-25T21:16:15.065188Z","iopub.status.idle":"2022-05-25T21:16:15.086353Z","shell.execute_reply.started":"2022-05-25T21:16:15.065163Z","shell.execute_reply":"2022-05-25T21:16:15.084932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.isnull().sum()\n# The 'label' column is not present in the Test Dataset.","metadata":{"execution":{"iopub.status.busy":"2022-05-25T21:16:15.574636Z","iopub.execute_input":"2022-05-25T21:16:15.575257Z","iopub.status.idle":"2022-05-25T21:16:15.587Z","shell.execute_reply.started":"2022-05-25T21:16:15.575207Z","shell.execute_reply":"2022-05-25T21:16:15.586077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.duplicated().sum()","metadata":{"execution":{"iopub.status.busy":"2022-05-25T21:16:16.229484Z","iopub.execute_input":"2022-05-25T21:16:16.230325Z","iopub.status.idle":"2022-05-25T21:16:16.268276Z","shell.execute_reply.started":"2022-05-25T21:16:16.23028Z","shell.execute_reply":"2022-05-25T21:16:16.267406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.duplicated().sum()","metadata":{"execution":{"iopub.status.busy":"2022-05-25T21:16:16.885515Z","iopub.execute_input":"2022-05-25T21:16:16.885881Z","iopub.status.idle":"2022-05-25T21:16:16.90149Z","shell.execute_reply.started":"2022-05-25T21:16:16.885836Z","shell.execute_reply":"2022-05-25T21:16:16.900943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head(15)\n# Two sentences could be related in three ways: one could entail the other, one could contradict the other, or they could be unrelated.\n# In this dataset, the hypothesis (2nd sentence) can do one of the three to the premise (1st sentence). \n# The 'label' column contains the following values, corresponding to the relationship of the two sentences: 0 - entailment; 1 - neutral ; and 2 - contradiction.","metadata":{"execution":{"iopub.status.busy":"2022-05-25T21:16:17.483749Z","iopub.execute_input":"2022-05-25T21:16:17.484406Z","iopub.status.idle":"2022-05-25T21:16:17.503141Z","shell.execute_reply.started":"2022-05-25T21:16:17.484373Z","shell.execute_reply":"2022-05-25T21:16:17.502332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.head(15)","metadata":{"execution":{"iopub.status.busy":"2022-05-25T21:16:18.147121Z","iopub.execute_input":"2022-05-25T21:16:18.147901Z","iopub.status.idle":"2022-05-25T21:16:18.161903Z","shell.execute_reply.started":"2022-05-25T21:16:18.147864Z","shell.execute_reply":"2022-05-25T21:16:18.16119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('train dataset lang_abv: ', len(df_train.lang_abv.unique()), ', languages: ', len(df_train.language.unique()))\nprint('test dataset lang_abv: ', len(df_test.lang_abv.unique()), ', languages: ', len(df_test.language.unique()))\nprint('train dataset languages & test dataset languages: ', len(set(df_train.lang_abv.unique()) and set(df_test.lang_abv.unique())))\n# This code block shows that the Datasets contain premise-hypothesis pairs in fifteen different languages.\n# The languages are Arabic, Bulgarian, Chinese, German, Greek, English, Spanish, French, Hindi, Russian, Swahili, Thai, Turkish, Urdu, and Vietnamese.","metadata":{"execution":{"iopub.status.busy":"2022-05-25T21:16:19.065518Z","iopub.execute_input":"2022-05-25T21:16:19.065862Z","iopub.status.idle":"2022-05-25T21:16:19.0802Z","shell.execute_reply.started":"2022-05-25T21:16:19.065814Z","shell.execute_reply":"2022-05-25T21:16:19.079281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Percent Distribution of Languages in the Train and Test Datasets","metadata":{}},{"cell_type":"code","source":"labels, frequencies = np.unique(df_train.language.values, return_counts = True)\n\nplt.figure(figsize = (15,15))\nplt.pie(frequencies,labels = labels, autopct = '%1.1f%%')\nplt.show()\n# This pie chart shows the quantity of each of the 15 languages in the Train Dataset in percentage.","metadata":{"execution":{"iopub.status.busy":"2022-05-25T21:16:20.326684Z","iopub.execute_input":"2022-05-25T21:16:20.326987Z","iopub.status.idle":"2022-05-25T21:16:20.651304Z","shell.execute_reply.started":"2022-05-25T21:16:20.326951Z","shell.execute_reply":"2022-05-25T21:16:20.650431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels, frequencies = np.unique(df_test.language.values, return_counts = True)\n\nplt.figure(figsize = (15,15))\nplt.pie(frequencies,labels = labels, autopct = '%1.1f%%')\nplt.show()\n# This pie chart shows the quantity of each of the 15 languages in the Test Dataset in percentage.","metadata":{"execution":{"iopub.status.busy":"2022-05-25T21:16:20.973255Z","iopub.execute_input":"2022-05-25T21:16:20.974064Z","iopub.status.idle":"2022-05-25T21:16:21.236989Z","shell.execute_reply.started":"2022-05-25T21:16:20.974026Z","shell.execute_reply":"2022-05-25T21:16:21.235965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Setting up the TPU","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf","metadata":{"_uuid":"e3dd507b-c502-488c-9dd0-419c5d73c159","_cell_guid":"863de620-d4b7-4711-b587-e1c75be9e36f","execution":{"iopub.status.busy":"2022-05-25T21:16:22.608773Z","iopub.execute_input":"2022-05-25T21:16:22.609097Z","iopub.status.idle":"2022-05-25T21:16:28.655762Z","shell.execute_reply.started":"2022-05-25T21:16:22.609069Z","shell.execute_reply":"2022-05-25T21:16:28.654774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n    # TPUs are network-connected accelerators and must be first located on the network; this is what TPUClusterResolver.connect() does.\n    # This code is used to initialize the TPU.\n    tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    # The TPUStrategy object contains the necessary distributed training code that will work on TPUs with their 8 compute cores.\nexcept ValueError:\n    tpu_strategy = tf.distribute.get_strategy()\n    print('Number of replicas:', tpu_strategy.num_replicas_in_sync)","metadata":{"_uuid":"386d0823-63ab-4765-9561-32c5f382e71d","_cell_guid":"ca2729e3-9275-4a9c-b150-592320bd3e54","execution":{"iopub.status.busy":"2022-05-25T21:16:28.657669Z","iopub.execute_input":"2022-05-25T21:16:28.658336Z","iopub.status.idle":"2022-05-25T21:16:35.232962Z","shell.execute_reply.started":"2022-05-25T21:16:28.658291Z","shell.execute_reply":"2022-05-25T21:16:35.232094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if tpu:\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu) \n    print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\nelif len(gpus) > 1:\n    tpu_strategy = tf.distribute.MirroredStrategy([gpu.name for gpu in gpus])\n    print('Running on multiple GPUs ', [gpu.name for gpu in gpus])\nelif len(gpus) == 1:\n    tpu_strategy = tf.distribute.get_strategy() \n    print('Running on single GPU ', gpus[0].name)\nelse:\n    tpu_strategy = tf.distribute.get_strategy() \n    print('Running on CPU')\nprint(\"Number of accelerators: \", tpu_strategy.num_replicas_in_sync)\n# To verify that this Notebook is running on TPU and check the number of accelerators.","metadata":{"execution":{"iopub.status.busy":"2022-05-25T21:16:35.23443Z","iopub.execute_input":"2022-05-25T21:16:35.234705Z","iopub.status.idle":"2022-05-25T21:16:43.560106Z","shell.execute_reply.started":"2022-05-25T21:16:35.234676Z","shell.execute_reply":"2022-05-25T21:16:43.559428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Building the Model","metadata":{"_uuid":"0b64130f-530a-4560-8afd-462a55fc14b3","_cell_guid":"44a1d22f-053c-4188-b25f-d714aa745016","trusted":true}},{"cell_type":"code","source":"from transformers import TFAutoModel, AutoTokenizer\n# This code enables me to use the cutting-edge NLP models from Transformers library maintained and democratized by Hugging Face.","metadata":{"execution":{"iopub.status.busy":"2022-05-25T21:16:47.316449Z","iopub.execute_input":"2022-05-25T21:16:47.31676Z","iopub.status.idle":"2022-05-25T21:16:49.438695Z","shell.execute_reply.started":"2022-05-25T21:16:47.316728Z","shell.execute_reply":"2022-05-25T21:16:49.437963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def input_convert(data):\n# Input IDs (Type, Word, Mask) are simply a set of integers that represent a word.\n# They are the tokenized representation of the text.\n# 'Tokenized' means converting raw text into a 'token' to prepare it for input to the Model.\n        inputs = {\n            'input_word_ids': [],\n            'input_mask': []\n        }\n        \n        for i in data:\n            inputs['input_word_ids'].append(i['input_ids'])\n            inputs['input_mask'].append(i['attention_mask'])\n            # A mask is a special token, which is an array of 0s and 1s where each 1 represents a valid word/input ID, and a 0 represents padding.\n            # Some tokens are 'masked' so that the model can consider the context of the sentences.\n            \n        inputs['input_word_ids'] = tf.ragged.constant(inputs['input_word_ids']).to_tensor()\n        inputs['input_mask'] = tf.ragged.constant(inputs['input_mask']).to_tensor()\n        # Since every bit of my data is useful, I don't want to lose any information with slicing nor I want to append some other information\n        # I decided to use Ragged Tensors help avoid the shape problem and losing information problem.   \n        return inputs","metadata":{"execution":{"iopub.status.busy":"2022-05-25T21:16:51.277137Z","iopub.execute_input":"2022-05-25T21:16:51.277426Z","iopub.status.idle":"2022-05-25T21:16:51.284501Z","shell.execute_reply.started":"2022-05-25T21:16:51.277397Z","shell.execute_reply":"2022-05-25T21:16:51.283942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = df_train.pop('label')\ndf = pd.concat([df_train, df_test], ignore_index = True)\ndf_train.shape, df_test.shape, df.shape\n# The .pop() method removes the element at the specified position.","metadata":{"execution":{"iopub.status.busy":"2022-05-25T21:16:57.671024Z","iopub.execute_input":"2022-05-25T21:16:57.671817Z","iopub.status.idle":"2022-05-25T21:16:57.684313Z","shell.execute_reply.started":"2022-05-25T21:16:57.671763Z","shell.execute_reply":"2022-05-25T21:16:57.683572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_name = 'joeddav/xlm-roberta-large-xnli'\n# THe XLM_RoBERTa Model is a derivative of the original BERT model.\n# It is developed by Facebook, intended for zero-shot text classification especially in languages other than English.\n# Zero-shot Text Classification pertains to a classifier learning on one set of labels and then evaluating a different set of labels that the classifier has never seen before. \n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n# The tokenizer library comprises of tokenizers for all BERT models.\n# A tokenizer turns sequences of words/sentences into arrays of numbers to prepare them for further analysis.\n# AutoTokenizer is used to call tokenizers from the transformers library.\n\nmask = []\nfor i in range(len(df)):\n    padded_seq = tokenizer(df['premise'][i], df['hypothesis'][i], padding = True, add_special_tokens = True)\n    mask.append(padded_seq)\n    # Neural Networks require inputs that have the same shape and size for further processing; however, this cannot be expected of sentences.\n    # This is where padding comes in. It 'pads' out the sentences into a matrix, where each row in that matrix has an padded encoded sentence with the same length.\n    # The length of the padding is determined depending on the dataset.\n\ninputs = input_convert(mask)","metadata":{"execution":{"iopub.status.busy":"2022-05-25T21:16:59.478358Z","iopub.execute_input":"2022-05-25T21:16:59.479236Z","iopub.status.idle":"2022-05-25T21:17:16.422641Z","shell.execute_reply.started":"2022-05-25T21:16:59.4792Z","shell.execute_reply":"2022-05-25T21:17:16.421891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train-test Split","metadata":{}},{"cell_type":"code","source":"inputs_train = {}\ninputs_test = {}\n\nfor key in inputs.keys():\n    inputs_train[key] = inputs[key][:len(y), :]\n    inputs_test[key] = inputs[key][len(y):, :]\n# The .keys() method returns a view object that displays a list of all the keys in a Python dictionary, in order of insertion.","metadata":{"execution":{"iopub.status.busy":"2022-05-25T21:17:16.424473Z","iopub.execute_input":"2022-05-25T21:17:16.424787Z","iopub.status.idle":"2022-05-25T21:17:16.434969Z","shell.execute_reply.started":"2022-05-25T21:17:16.424749Z","shell.execute_reply":"2022-05-25T21:17:16.433656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras import Input, Model\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.optimizers import Adam\n# The Dense layer the regular deeply connected neural network layer. It is most common and frequently used layer. \n# The Dropout layer helps prevent overfitting by randomly setting input units to 0 with a frequency of rate at each step during training. And inputs not set to 0 are scaled up by 1/(1-rate), such that the sum over all inputs is unchanged.\n# Adam is the most advanced optimizer in Tensorflow with weight decay, which can further help reduce overfitting and improve generalization\n\n\nwith tpu_strategy.scope():\n# The TPUStrategy is used here to 'instantiate' the model in the scope of the strategy. Why?\n    max_len = inputs['input_word_ids'].shape[1]\n    \n    encoder = TFAutoModel.from_pretrained(model_name)\n    \n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n\n    embedding = encoder([input_word_ids, input_mask])[0]\n    dense1 = Dense(256, activation='relu')(Dropout(0.1)(embedding[:,0,:]))\n    dense2 = Dense(64, activation='relu')(dense1)\n    output = Dense(3, activation='softmax')(dense2)\n    # I used ReLu because it is a nonlinear function that allows complex relationships to be learned is able to allow learning through all the hidden layers in a deep network by having large derivatives.\n    # In addition, ReLu is more useful for the hidden layers rathen the the final output layers\n    # I used Softmax because it transforms a bunch of arbitrarily large or small numbers into valid probability distributions.\n    # The property of softmax to scale numbers/logits into probabilities is useful and intuitive and is often used as the activation function for the final output/layer.\n\n    model = Model(inputs=[input_word_ids, input_mask], outputs = output)\n    model.compile(Adam(lr=1e-6), loss='sparse_categorical_crossentropy', metrics=['accuracy'], steps_per_execution = 100)\n    # I used sparse categorical crossentropy because it saves time in memory as well as computation because it simply uses a single integer for a class, rather than a whole vector.","metadata":{"execution":{"iopub.status.busy":"2022-05-25T21:31:49.92058Z","iopub.execute_input":"2022-05-25T21:31:49.92113Z","iopub.status.idle":"2022-05-25T21:32:35.228264Z","shell.execute_reply.started":"2022-05-25T21:31:49.921066Z","shell.execute_reply":"2022-05-25T21:32:35.226385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Fitting the Model","metadata":{}},{"cell_type":"code","source":"early_stop = tf.keras.callbacks.EarlyStopping(patience = 4, restore_best_weights = True)\nmodel.fit(inputs_train, y.values, epochs = 10, verbose = 1, validation_split = 0.2,\n                    batch_size = 16 * tpu_strategy.num_replicas_in_sync, callbacks = [early_stop])\n# Early stopping is a basic technique to prevent overfitting.\n# Patience is the number of epochs with no improvement, after which, the training will be stopped.\n# An Epoch can be described as one complete cycle through the entire training dataset and indicates the number of passes that the machine learning algorithm has completed during that training.\n# Verbose is generally an option for producing detailed logging information. It is like asking the program to tell me everything about what it is doing all the time.\n# Batch is composed of a divided dataset that I use when I can't pass the entire dataset into the Neural Network at once.\n# Callbacks are functions called when a task is completed during Model Training to check if the validation loss is increasing.","metadata":{"execution":{"iopub.status.busy":"2022-05-25T21:32:35.232763Z","iopub.execute_input":"2022-05-25T21:32:35.233051Z","iopub.status.idle":"2022-05-25T21:43:33.272348Z","shell.execute_reply.started":"2022-05-25T21:32:35.233019Z","shell.execute_reply":"2022-05-25T21:43:33.2708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Generating & Submitting Predictions","metadata":{"_uuid":"fb1ff888-7684-4888-b861-3c31a3f360b7","_cell_guid":"87b18b05-30f3-45f9-9c3e-6f8184934bd0","trusted":true}},{"cell_type":"code","source":"predictions = [np.argmax(i) for i in model.predict(inputs_test)]\n# The numpy.argmax() function returns indices of the max element of the array in a particular axis.","metadata":{"_uuid":"f9db46a1-1f83-4ddb-85bf-da8f40afe623","_cell_guid":"a7ab0c33-0377-4cb2-b4f9-fc489383fdb7","execution":{"iopub.status.busy":"2022-05-25T21:43:33.274844Z","iopub.execute_input":"2022-05-25T21:43:33.275529Z","iopub.status.idle":"2022-05-25T21:44:30.483269Z","shell.execute_reply.started":"2022-05-25T21:43:33.275467Z","shell.execute_reply":"2022-05-25T21:44:30.482199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = df_test.id.copy().to_frame()\nsubmission['prediction'] = predictions\nsubmission.head()","metadata":{"_uuid":"9e0e34fa-1ef7-4207-a5c3-c21863c7be27","_cell_guid":"add7302f-ae26-4e78-b69d-858cacb35991","execution":{"iopub.status.busy":"2022-05-25T21:44:30.485106Z","iopub.execute_input":"2022-05-25T21:44:30.485349Z","iopub.status.idle":"2022-05-25T21:44:30.544016Z","shell.execute_reply.started":"2022-05-25T21:44:30.485321Z","shell.execute_reply":"2022-05-25T21:44:30.543003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv(\"submission.csv\", index = False)\n# sample_submission.csv: This is a sample submission file in the correct format: id: a unique identifier for each sample label: the classification of the relationship between the premise and hypothesis (0 for entailment, 1 for neutral, 2 for contradiction)","metadata":{"_uuid":"b8463f7e-8b2f-4c24-8eb3-a35ec02a2d6e","_cell_guid":"84abe9e3-ef04-4dac-97d2-2308a0f11313","execution":{"iopub.status.busy":"2022-05-25T21:44:30.545054Z","iopub.execute_input":"2022-05-25T21:44:30.545573Z","iopub.status.idle":"2022-05-25T21:44:30.573558Z","shell.execute_reply.started":"2022-05-25T21:44:30.545539Z","shell.execute_reply":"2022-05-25T21:44:30.572489Z"},"trusted":true},"execution_count":null,"outputs":[]}]}