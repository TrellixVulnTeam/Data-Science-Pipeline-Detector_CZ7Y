{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Contradictory, My dear Watson - A relation extraction task"},{"metadata":{},"cell_type":"markdown","source":"For a any given two sentences, there are three ways they could be related: one could entail the other, one could contradict the other, or they could be unrelated. **Natural Language Inferencing** (NLI) is a popular NLP problem that involves determining how pairs of sentences (consisting of a premise and a hypothesis) are related.\n\nOur task is to create an NLI model that assigns labels of 0, 1, or 2 (corresponding to entailment, neutral, and contradiction) to pairs of premises and hypotheses. To make things more interesting, the train and test set include text in fifteen different languages!"},{"metadata":{},"cell_type":"markdown","source":"About this kernel\n\nThis kernel acts as a starter kit. It gives all the essential Key insights on the given text data.\n\nKey Takeaways\n\n- Effective Story Telling using EDA\n- Creative Feature Engineering\n- Modelling"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Install the necessary libraries\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n\n\n## Data Visualisation\n# from plotly.offline import iplot\n# from plotly import tools\n# import plotly.graph_objects as go\n# import plotly.express as px\n# import plotly.offline as py\n# import plotly.figure_factory as ff\n# py.init_notebook_mode(connected=True)\n# import plotly.offline as pyo\n\n## Data Preprocessing\nimport re\nimport nltk\nfrom gensim.models import word2vec\n\n## Visializing similarity of words\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n##Translation\n#from googletrans import Translator\n\n\n## Models\nimport xgboost as xgb\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn import ensemble, metrics, model_selection, naive_bayes\nfrom sklearn.preprocessing import LabelEncoder\n\n## Bert models from HuggingFace library\nfrom transformers import BertTokenizer, TFAutoModel, AutoTokenizer\nimport tensorflow as tf\nimport keras\nfrom tensorflow.math import softplus, tanh\nfrom tensorflow.keras.utils import get_custom_objects\nfrom tensorflow.keras import Input, Model, Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, LSTM, Embedding, GlobalAveragePooling1D\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.optimizers import Adam\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras import regularizers\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/contradictory-my-dear-watson/train.csv')\nprint('Traning Data, the size of the dataset is: {} \\n'.format(train_df.shape))\n\ntest_df = pd.read_csv('../input/contradictory-my-dear-watson/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/contradictory-my-dear-watson/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/contradictory-my-dear-watson/test.csv\")\nprint(\"Number of rows and columns in train data : \",train_df.shape)\nprint(\"Number of rows and columns in test data : \",test_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Target Variable Explanation"},{"metadata":{"trusted":true},"cell_type":"code","source":"Accuracy=pd.DataFrame()\nAccuracy['Type']=train_df.label.value_counts().index\nAccuracy['Count']=train_df.label.value_counts().values\nAccuracy['Type']=Accuracy['Type'].replace(0,'Entailment')\nAccuracy['Type']=Accuracy['Type'].replace(1,'Neutral')\nAccuracy['Type']=Accuracy['Type'].replace(2,'Contradiction')\nAccuracy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize = (15,5))\n\nplt.subplot(1,2,1)\nplt.title('Traning data label distribution')\nsns.countplot(data = train_df, x = 'label', order = train_df['label'].value_counts().index[:10])\n\n# plt.subplot(1,2,2)\n# plt.title('Testing data label distribution')\n# sns.countplot(data = test_df, x = 'label', order = test_df['label'].value_counts().index[:10])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observation:**\n\nThere are total of **12120** records, which contain\n\n1. **4176** records of Entailment\n2. **4064** records of Contradiction\n3. **3880** records of Neutral.\n\nTherefore, there is **No Class Imblanace** in the given data."},{"metadata":{},"cell_type":"markdown","source":"### Language distribution in dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize = (15,5))\n\nplt.subplot(1,2,1)\nplt.title('Traning data language distribution')\nsns.countplot(data = train_df, x = 'lang_abv', order = train_df['lang_abv'].value_counts().index[:10])\n\nplt.subplot(1,2,2)\nplt.title('Test data laguage distribution')\nsns.countplot(data = test_df, x = 'lang_abv', order = test_df['lang_abv'].value_counts().index[:10])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observation**\n\nFrom the above graph, we can see that **English** is dominating language in the given dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"import string\n\nMeta_features = pd.DataFrame()\n\n## Number of words in the text ##\nMeta_features[\"premise_num_words\"] = train_df[\"premise\"].apply(lambda x: len(str(x).split()))\nMeta_features[\"hypothesis_num_words\"] = train_df[\"hypothesis\"].apply(lambda x: len(str(x).split()))\n\n## Number of characters in the text ##\nMeta_features[\"premise_num_chars\"] = train_df[\"premise\"].apply(lambda x: len(str(x)))\nMeta_features[\"hypothesis_num_chars\"] = train_df[\"hypothesis\"].apply(lambda x: len(str(x)))\n\n## Number of punctuations in the text ##\nMeta_features[\"premise_num_punctuations\"] =train_df[\"premise\"].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\nMeta_features[\"hypothesis_num_punctuations\"] =train_df[\"hypothesis\"].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n\n## Average length of the words in the text ##\nMeta_features[\"premise_mean_word_len\"] = train_df[\"premise\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\nMeta_features[\"hypothesis_mean_word_len\"] = train_df[\"hypothesis\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n\nMeta_features['label'] = train_df['label']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize = (15,5))\n\nplt.subplot(1,2,1)\nplt.title('Premise sentence length')\nplt.hist(Meta_features['premise_num_words'])\n\nplt.subplot(1,2,2)\nplt.title('Hypothesis sentence length')\nplt.hist(Meta_features['hypothesis_num_words'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualizing Word Vectors in Hypothesis and Premise"},{"metadata":{"trusted":true},"cell_type":"code","source":"text_df = train_df.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"STOP_WORDS = nltk.corpus.stopwords.words()\n\ndef clean_sentence(val):\n    regex = re.compile('([^\\s\\w]|_)+')\n    sentence = regex.sub('', val).lower()\n    sentence = sentence.split(\" \")\n    \n    for word in list(sentence):\n        if word in STOP_WORDS:\n            sentence.remove(word)  \n            \n    sentence = \" \".join(sentence)\n    return sentence\n\ntext_df['premise'] =  text_df['premise'].apply(clean_sentence)\ntext_df['hypothesis'] =  text_df['hypothesis'].apply(clean_sentence)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_corpus(data):\n    corpus = []\n    for col in ['premise', 'hypothesis']:\n        for sentence in data[col].iteritems():\n            word_list = sentence[1].split(\" \")\n            corpus.append(word_list)\n            \n    return corpus\n\ntext_df_1 = text_df[text_df['lang_abv'] == 'en']\n\ncorpus = build_corpus(text_df_1)        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### t-SNE on word vectors\n\n* t-Distributed Stochastic Neighbor Embedding is a non-linear dimensionality reduction algorithm used for exploring high-dimensional data.  \n* It works by taking a group of high-dimensional vocabulary word feature vectors, then compresses them down to 2-dimensional x,y coordinate pairs. \n* The idea is to keep similar words close together on the plane, while maximizing the distance between dissimilar words. "},{"metadata":{"trusted":true},"cell_type":"code","source":"def tsne_plot(model):\n    \"Creates and TSNE model and plots it\"\n    labels = []\n    tokens = []\n\n    for word in model.wv.vocab:\n        tokens.append(model[word])\n        labels.append(word)\n    \n    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\n    new_values = tsne_model.fit_transform(tokens)\n\n    x = []\n    y = []\n    for value in new_values:\n        x.append(value[0])\n        y.append(value[1])\n        \n    plt.figure(figsize=(16,16)) \n    for i in range(len(x)):\n        plt.scatter(x[i],y[i])\n        plt.annotate(labels[i],\n                     xy=(x[i], y[i]),\n                     xytext=(5, 2),\n                     textcoords='offset points',\n                     ha='right',\n                     va='bottom')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # A more selective model\n# model = word2vec.Word2Vec(corpus, size=100, window=200, min_count=150, workers=4)\n# tsne_plot(model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"### Modeling to predict the relation"},{"metadata":{},"cell_type":"markdown","source":"#### Text Cleaning and Feature Engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords\nimport re\nimport nltk\nimport string\n\n\nstop_words = set(stopwords.words('english')) \ndef text_cleaner(text):\n    newString = text.lower()\n    newString = re.sub(r'\\([^)]*\\)', '', newString)\n    newString = re.sub('\"','', newString)    \n    newString = re.sub(r\"'s\\b\",\"\",newString)\n    newString = re.sub(\"[^a-zA-Z]\", \" \", newString) \n    tokens = [w for w in newString.split() if not w in stop_words]\n    long_words=[]\n    for i in tokens:\n        if len(i)>=3:                  #removing short word\n            long_words.append(i)   \n    return (\" \".join(long_words)).strip()\n\ncleaned_text = []\nfor t in train_df['premise']:\n    cleaned_text.append(text_cleaner(t))\ntrain_df['premise'] = cleaned_text   \n\ncleaned_text = []\nfor t in test_df['premise']:\n    cleaned_text.append(text_cleaner(t))\ntest_df['premise'] = cleaned_text \n\ncleaned_text = []\nfor t in train_df['hypothesis']:\n    cleaned_text.append(text_cleaner(t))\ntrain_df['hypothesis'] = cleaned_text   \n\ncleaned_text = []\nfor t in test_df['hypothesis']:\n    cleaned_text.append(text_cleaner(t))\ntest_df['hypothesis'] = cleaned_text ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Models\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn import ensemble, metrics, model_selection, naive_bayes\nfrom sklearn.preprocessing import LabelEncoder\nimport torch\nimport keras\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom transformers import BertTokenizer, TFBertModel,TFAutoModel\nimport tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf_vec = TfidfVectorizer(analyzer='word',ngram_range = (1,3),max_features=1000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidfconverter = TfidfVectorizer(max_features=1500, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Split into Train and validation sets"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = train_df['label']\nX= train_df.drop(['label','id','lang_abv','language'],axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    print(\"TPU is on!\")\nexcept ValueError:\n    strategy = tf.distribute.get_strategy() # for CPU and single GPU\n    print('Number of replicas:', strategy.num_replicas_in_sync)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab_size = 10000\noov_tok = '<OOV>'\nemb_dim = 32\nmax_len = 250","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train['combined'] = X_train['premise'] + ' [SEP] ' + X_train['hypothesis']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = X_train['combined']\nx.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer.fit_on_texts(x)\ntrain_seq = tokenizer.texts_to_sequences(x)\npadded_train = pad_sequences(train_seq, maxlen = max_len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"padded_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test['combined'] = X_test['premise'] + ' [SEP] ' + X_test['hypothesis']\nx_test = X_test['combined']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_seq = tokenizer.texts_to_sequences(x_test)\npadded_test = pad_sequences(test_seq, maxlen = max_len)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Building a baseline LSTM model"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = tf.keras.Sequential()\nmodel.add(tf.keras.layers.Embedding(vocab_size, output_dim=emb_dim, input_length=max_len))\nmodel.add(tf.keras.layers.LSTM(16, return_sequences=True))\nmodel.add(tf.keras.layers.LSTM(32,))\nmodel.add(tf.keras.layers.Dropout(0.2))\n\nmodel.add(tf.keras.layers.Dense(16, activation='relu'))\nmodel.add(tf.keras.layers.Dropout(0.3))\nmodel.add(tf.keras.layers.Dense(3, activation='softmax'))\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = pd.DataFrame(y_train)\ny_train = pd.get_dummies(y_train['label'])\n# y_train.head()\ny_train.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model.compile(optimizer=tf.keras.optimizers.Adam(), loss = 'categorical_crossentropy', metrics = ['accuracy'])\n\n# history = model.fit(padded_train,y_train, epochs = 10, validation_split = 0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plt.figure(figsize = (16,6))\n\n# # summarize history for accuracy\n\n# plt.subplot(1,2,1)\n# plt.plot(history.history['accuracy'])\n# plt.plot(history.history['val_accuracy'])\n# plt.title('model accuracy')\n# plt.ylabel('accuracy')\n# plt.xlabel('epoch')\n# plt.legend(['train', 'test'], loc='upper left')\n\n\n# plt.subplot(1,2,2)\n# # summarize history for loss\n# plt.plot(history.history['loss'])\n# plt.plot(history.history['val_loss'])\n# plt.title('model loss')\n# plt.ylabel('loss')\n# plt.xlabel('epoch')\n# plt.legend(['train', 'test'], loc='upper left')\n\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction = model.predict_classes(padded_test)\nprediction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score, confusion_matrix, classification_report","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Accuracy\", accuracy_score(y_test, prediction))\nprint(\"Confusion Matrix\", confusion_matrix(y_test, prediction))\nprint(\"Classification Report\", classification_report(y_test, prediction))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Training a Bi-directional LSTM model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# model = tf.keras.Sequential()\n# model.add(tf.keras.layers.Embedding(vocab_size, output_dim=emb_dim, input_length=max_len))\n# model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16, return_sequences=True)))\n# model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32,)))\n# model.add(tf.keras.layers.Dropout(0.2))\n\n# model.add(tf.keras.layers.Dense(16, activation='relu'))\n# model.add(tf.keras.layers.Dropout(0.3))\n# model.add(tf.keras.layers.Dense(3, activation='softmax'))\n\n# model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model.compile(optimizer=tf.keras.optimizers.Adam(), loss = 'categorical_crossentropy', metrics = ['accuracy'])\n\n# history = model.fit(padded_train,y_train, epochs = 10, validation_split = 0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plt.figure(figsize = (16,6))\n\n# # summarize history for accuracy\n\n# plt.subplot(1,2,1)\n# plt.plot(history.history['accuracy'])\n# plt.plot(history.history['val_accuracy'])\n# plt.title('model accuracy')\n# plt.ylabel('accuracy')\n# plt.xlabel('epoch')\n# plt.legend(['train', 'test'], loc='upper left')\n\n\n# plt.subplot(1,2,2)\n# # summarize history for loss\n# plt.plot(history.history['loss'])\n# plt.plot(history.history['val_loss'])\n# plt.title('model loss')\n# plt.ylabel('loss')\n# plt.xlabel('epoch')\n# plt.legend(['train', 'test'], loc='upper left')\n\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction = model.predict_classes(padded_test)\nprediction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Accuracy\", accuracy_score(y_test, prediction))\nprint(\"Confusion Matrix\", confusion_matrix(y_test, prediction))\nprint(\"Classification Report\", classification_report(y_test, prediction))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Training a base BERT model"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_name = 'bert-base-multilingual-cased'\ntokenizer = BertTokenizer.from_pretrained(model_name)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Tokenizers are used to convert the text into numbers"},{"metadata":{"trusted":true},"cell_type":"code","source":"def encode_sentence(s):\n   tokens = list(tokenizer.tokenize(s))\n   tokens.append('[SEP]')\n   return tokenizer.convert_tokens_to_ids(tokens)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encode_sentence(\"This is the NLP project\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Data Preparation"},{"metadata":{},"cell_type":"markdown","source":"BERT uses three kind of input data- input word IDs, input masks, and input type IDs.\n\nThese allow the model to know that the premise and hypothesis are distinct sentences, and also to ignore any padding from the tokenizer.\n\nWe add a [CLS] token to denote the beginning of the inputs, and a [SEP] token to denote the separation between the premise and the hypothesis. We also need to pad all of the inputs to be the same size. \n\nNow, we're going to encode all of our premise/hypothesis pairs for input into BERT."},{"metadata":{"trusted":true},"cell_type":"code","source":"def bert_encode(hypotheses, premises, tokenizer):\n    \n  num_examples = len(hypotheses)\n  \n  sentence1 = tf.ragged.constant([\n      encode_sentence(s)\n      for s in np.array(hypotheses)])\n  sentence2 = tf.ragged.constant([\n      encode_sentence(s)\n       for s in np.array(premises)])\n\n  cls = [tokenizer.convert_tokens_to_ids(['[CLS]'])]*sentence1.shape[0]\n  input_word_ids = tf.concat([cls, sentence1, sentence2], axis=-1)\n\n  input_mask = tf.ones_like(input_word_ids).to_tensor()\n\n  type_cls = tf.zeros_like(cls)\n  type_s1 = tf.zeros_like(sentence1)\n  type_s2 = tf.ones_like(sentence2)\n  input_type_ids = tf.concat(\n      [type_cls, type_s1, type_s2], axis=-1).to_tensor()\n\n  inputs = {\n      'input_word_ids': input_word_ids.to_tensor(),\n      'input_mask': input_mask,\n      'input_type_ids': input_type_ids}\n\n  return inputs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = train_df['label']\nX = train_df.drop(['label'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.15, random_state = 100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_input = bert_encode(X_train.premise.values,X_train.hypothesis,tokenizer)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Building the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef build_model(model_name, max_len):\n    bert_encoder = TFBertModel.from_pretrained(model_name)\n    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    input_type_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_type_ids\")\n    \n    embedding = bert_encoder([input_word_ids, input_mask, input_type_ids])[0]\n    output = tf.keras.layers.Dense(3, activation='softmax')(embedding[:,0,:])\n    \n    model = tf.keras.Model(inputs=[input_word_ids, input_mask, input_type_ids], outputs=output)\n    model.compile(tf.keras.optimizers.Adam(lr=1e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n    \n    return model\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = build_model(model_name, max_len = 50)\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(train_input, y_train, epochs = 5, verbose = 1, validation_split = 0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (16,6))\n\n# summarize history for accuracy\n\nplt.subplot(1,2,1)\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\n\n\nplt.subplot(1,2,2)\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.hypothesis.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.premise.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_input = bert_encode(test_df.premise.values, test_df.hypothesis.values, tokenizer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = [np.argmax(i) for i in model.predict(test_input)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nimport pandas as pd\n\ndf_submission = pd.DataFrame({\"id\": test_df.id.values, \"prediction\": predictions})\ndf_submission.to_csv(\"submission.csv\", index = False)\n\ndf_submission.prediction.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score, confusion_matrix, classification_report","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Accuracy\", accuracy_score(y_test, predictions))\nprint(\"Confusion Matrix\", confusion_matrix(y_test, predictions))\nprint(\"Classification Report\", classification_report(y_test, predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bert_input = bert_encode(['000000000000000000000000000000000000000000000000000000 Killing someone 00000000000000000000'], ['000000000000000000000000000 Never kill anyone 000000000000000000000000'], tokenizer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bert_input","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.argmax(model.predict(bert_input))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict_burt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Training a XLNet Base Cased"},{"metadata":{"trusted":true},"cell_type":"code","source":"#!pip install googletrans","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#from googletrans import Translator","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#In general, the idea is to expand on this configuration as you progress through a competition with more ideas and elements to experiment on.\n\n## defining configuration\nclass Configuration():\n    \"\"\"\n    All configuration for running an experiment\n    \"\"\"\n    def __init__(\n        self,\n        model_name,\n        translation = False,\n        max_length = 64,\n        padding = True,\n        batch_size = 128,\n        epochs = 5,\n        learning_rate = 1e-5,\n        metrics = [\"sparse_categorical_accuracy\"],\n        verbose = 1,\n        train_splits = 5,\n        accelerator = \"TPU\",\n        myluckynumber = 13\n    ):\n        # seed and accelerator\n        self.SEED = 100\n        self.ACCELERATOR = accelerator\n\n        # paths\n        self.PATH_TRAIN = train_df\n        self.PATH_TEST  = test_df\n\n        # splits\n        self.TRAIN_SPLITS = train_splits\n\n        # mapping of language\n        self.LANGUAGE_MAP = {\n            \"English\"   : 0,\n            \"Chinese\"   : 1,\n            \"Arabic\"    : 2,\n            \"French\"    : 3,\n            \"Swahili\"   : 4,\n            \"Urdu\"      : 5,\n            \"Vietnamese\": 6,\n            \"Russian\"   : 7,\n            \"Hindi\"     : 8,\n            \"Greek\"     : 9,\n            \"Thai\"      : 10,\n            \"Spanish\"   : 11,\n            \"German\"    : 12,\n            \"Turkish\"   : 13,\n            \"Bulgarian\" : 14\n        }\n\n        self.INVERSE_LANGUAGE_MAP = {v: k for k, v in self.LANGUAGE_MAP.items()}\n\n        # model configuration\n        self.MODEL_NAME = model_name\n        self.TRANSLATION = translation\n        self.TOKENIZER = AutoTokenizer.from_pretrained(self.MODEL_NAME)\n\n        # model hyperparameters\n        self.MAX_LENGTH = max_length\n        self.PAD_TO_MAX_LENGTH = padding\n        self.BATCH_SIZE = batch_size\n        self.EPOCHS = epochs\n        self.LEARNING_RATE = learning_rate\n        self.METRICS = metrics\n        self.VERBOSE = verbose\n        \n        # initializing accelerator\n        self.initialize_accelerator()\n\n    def initialize_accelerator(self):\n        \"\"\"\n        Initializing accelerator\n        \"\"\"\n        # checking TPU first\n        if self.ACCELERATOR == \"TPU\":\n            print(\"Connecting to TPU\")\n            try:\n                tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n                print(f\"Running on TPU {tpu.master()}\")\n            except ValueError:\n                print(\"Could not connect to TPU\")\n                tpu = None\n\n            if tpu:\n                try:\n                    print(\"Initializing TPU\")\n                    tf.config.experimental_connect_to_cluster(tpu)\n                    tf.tpu.experimental.initialize_tpu_system(tpu)\n                    self.strategy = tf.distribute.experimental.TPUStrategy(tpu)\n                    self.tpu = tpu\n                    print(\"TPU initialized\")\n                except _:\n                    print(\"Failed to initialize TPU\")\n            else:\n                print(\"Unable to initialize TPU\")\n                self.ACCELERATOR = \"GPU\"\n\n        # default for CPU and GPU\n        if self.ACCELERATOR != \"TPU\":\n            print(\"Using default strategy for CPU and single GPU\")\n            self.strategy = tf.distribute.get_strategy()\n\n        # checking GPUs\n        if self.ACCELERATOR == \"GPU\":\n            print(f\"GPUs Available: {len(tf.config.experimental.list_physical_devices('GPU'))}\")\n\n        # defining replicas\n        self.AUTO = tf.data.experimental.AUTOTUNE\n        self.REPLICAS = self.strategy.num_replicas_in_sync\n        print(f\"REPLICAS: {self.REPLICAS}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## data preparation functions\ndef translate_text_to_english(text):\n    \"\"\"\n    Translates text to English.\n    \"\"\"\n    translator = Translator()\n\n    return translator.translate(text, dest = \"en\").text\n\n\ndef encode_text(df, tokenizer, max_len, padding):\n    \"\"\"\n    Preprocessing textual data into encoded tokens.\n    \"\"\"\n    text = df[[\"premise\", \"hypothesis\"]].values.tolist()\n\n    # encoding text using tokenizer of the model\n    text_encoded = tokenizer.batch_encode_plus(\n        text,\n        pad_to_max_length = padding,\n        max_length = max_len\n    )\n\n    return text_encoded\n\n\ndef get_tf_dataset(X, y, auto, labelled = True, repeat = False, shuffle = False, batch_size = 128):\n    \"\"\"\n    Creating tf.data.Dataset for TPU.\n    \"\"\"\n    if labelled:\n        ds = (tf.data.Dataset.from_tensor_slices((X[\"input_ids\"], y)))\n    else:\n        ds = (tf.data.Dataset.from_tensor_slices(X[\"input_ids\"]))\n\n    if repeat:\n        ds = ds.repeat()\n\n    if shuffle:\n        ds = ds.shuffle(2048)\n\n    ds = ds.batch(batch_size)\n    ds = ds.prefetch(auto)\n\n    return ds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## building model\ndef build_model(model_name, max_len, learning_rate, metrics):\n    \"\"\"\n    Building the Deep Learning architecture\n    \"\"\"\n    # defining encoded inputs\n    input_ids = Input(shape = (max_len,), dtype = tf.int32, name = \"input_ids\")\n    \n    # defining transformer model embeddings\n    transformer_model = TFAutoModel.from_pretrained(model_name)\n    transformer_embeddings = transformer_model(input_ids)[0]\n    \n    # defining output layer\n    \n    output_values = Dense(3, activation = \"softmax\")(transformer_embeddings[:, 0, :])\n\n    # defining model\n    model = Model(inputs = input_ids, outputs = output_values)\n    opt = Adam(learning_rate = learning_rate)\n    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True)\n    metrics = metrics\n\n    model.compile(optimizer = opt, loss = loss, metrics = metrics)\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## stratified k-fold over language and label\ndef run_model(config):\n    \"\"\"\n    Running the model\n    \"\"\"\n    ## reading data\n    df_train = train_df\n    df_test = test_df\n    \n    # translating non-English text to English\n    if config.TRANSLATION:\n        df_train.loc[df_train.language != \"English\", \"premise\"] = df_train[df_train.language != \"English\"].premise.apply(lambda x: translate_text_to_english(x))\n        df_test.loc[df_test.language != \"English\", \"premise\"] = df_test[df_test.language != \"English\"].premise.apply(lambda x: translate_text_to_english(x))\n\n        df_train.loc[df_train.language != \"English\", \"hypothesis\"] = df_train[df_train.language != \"English\"].hypothesis.apply(lambda x: translate_text_to_english(x))\n        df_test.loc[df_test.language != \"English\", \"hypothesis\"] = df_test[df_test.language != \"English\"].hypothesis.apply(lambda x: translate_text_to_english(x))\n\n    # adding column for stratified splitting\n    df_train[\"language_label\"] = df_train.language.astype(str) + \"_\" + df_train.label.astype(str)\n\n    # stratified K-fold on language and label\n    skf = StratifiedKFold(n_splits = config.TRAIN_SPLITS, shuffle = True, random_state = config.SEED)\n\n    # initializing predictions\n    preds_oof = np.zeros((df_train.shape[0], 3))\n    preds_test = np.zeros((df_test.shape[0], 3))\n    acc_oof = []\n\n    # iterating over folds\n    for (fold, (train_index, valid_index)) in enumerate(skf.split(df_train, df_train.language_label)):\n        # initializing TPU\n        if config.ACCELERATOR == \"TPU\":\n            if config.tpu:\n                config.initialize_accelerator()\n\n        # building model\n        #K.clear_session()\n        with config.strategy.scope():\n            model = build_model(config.MODEL_NAME, config.MAX_LENGTH, config.LEARNING_RATE, config.METRICS)\n            if fold == 0:\n                print(model.summary())\n\n        print(\"\\n\")\n        print(\"#\" * 19)\n        print(f\"##### Fold: {fold + 1} #####\")\n        print(\"#\" * 19)\n\n        # splitting data into training and validation\n        X_train = df_train.iloc[train_index]\n        X_valid = df_train.iloc[valid_index]\n\n        y_train = X_train.label.values\n        y_valid = X_valid.label.values\n\n        print(\"\\nTokenizing\")\n\n        # encoding text data using tokenizer\n        X_train_encoded = encode_text(df = X_train, tokenizer = config.TOKENIZER, max_len = config.MAX_LENGTH, padding = config.PAD_TO_MAX_LENGTH)\n        X_valid_encoded = encode_text(df = X_valid, tokenizer = config.TOKENIZER, max_len = config.MAX_LENGTH, padding = config.PAD_TO_MAX_LENGTH)\n\n        # creating TF Dataset\n        ds_train = get_tf_dataset(X_train_encoded, y_train, config.AUTO, repeat = True, shuffle = True, batch_size = config.BATCH_SIZE * config.REPLICAS)\n        ds_valid = get_tf_dataset(X_valid_encoded, y_valid, config.AUTO, batch_size = config.BATCH_SIZE * config.REPLICAS * 4)\n\n        n_train = X_train.shape[0]\n\n        if fold == 0:\n            X_test_encoded = encode_text(df = df_test, tokenizer = config.TOKENIZER, max_len = config.MAX_LENGTH, padding = config.PAD_TO_MAX_LENGTH)\n\n        # saving model at best accuracy epoch\n        sv = tf.keras.callbacks.ModelCheckpoint(\n            \"model.h5\",\n            monitor = \"val_sparse_categorical_accuracy\",\n            verbose = 0,\n            save_best_only = True,\n            save_weights_only = True,\n            mode = \"max\",\n            save_freq = \"epoch\"\n        )\n\n        print(\"\\nTraining\")\n\n        # training model\n        model_history = model.fit(\n            ds_train,\n            epochs = config.EPOCHS,\n            callbacks = [sv],\n            steps_per_epoch = n_train / config.BATCH_SIZE // config.REPLICAS,\n            validation_data = ds_valid,\n            verbose = config.VERBOSE\n        )\n        \n        plt.figure(figsize = (16,6))\n\n    # summarize history for accuracy\n\n        plt.subplot(1,2,1)\n        plt.plot(model_history.history['sparse_categorical_accuracy'])\n        plt.plot(model_history.history['val_sparse_categorical_accuracy'])\n        plt.title('model accuracy')\n        plt.ylabel('accuracy')\n        plt.xlabel('epoch')\n        plt.legend(['train', 'test'], loc='upper left')\n\n\n        plt.subplot(1,2,2)\n        # summarize history for loss\n        plt.plot(model_history.history['loss'])\n        plt.plot(model_history.history['val_loss'])\n        plt.title('model loss')\n        plt.ylabel('loss')\n        plt.xlabel('epoch')\n        plt.legend(['train', 'test'], loc='upper left')\n\n        plt.show()\n        \n        \n\n        print(\"\\nValidating\")\n\n        # scoring validation data\n        model.load_weights(\"model.h5\")\n        ds_valid = get_tf_dataset(X_valid_encoded, -1, config.AUTO, labelled = False, batch_size = config.BATCH_SIZE * config.REPLICAS * 4)\n\n        preds_valid = model.predict(ds_valid, verbose = config.VERBOSE)\n        acc = accuracy_score(y_valid, np.argmax(preds_valid, axis = 1))\n\n        preds_oof[valid_index] = preds_valid\n        acc_oof.append(acc)\n\n        print(\"\\nInferencing\")\n\n        # scoring test data\n        ds_test = get_tf_dataset(X_test_encoded, -1, config.AUTO, labelled = False, batch_size = config.BATCH_SIZE * config.REPLICAS * 4)\n        preds_test += model.predict(ds_test, verbose = config.VERBOSE) / config.TRAIN_SPLITS\n\n        print(f\"\\nFold {fold + 1} Accuracy: {round(acc, 4)}\\n\")\n\n        #g = gc.collect()\n\n    # overall CV score and standard deviation\n    print(f\"\\nCV Mean Accuracy: {round(np.mean(acc_oof), 4)}\")\n    print(f\"CV StdDev Accuracy: {round(np.std(acc_oof), 4)}\\n\")\n\n    return preds_oof, preds_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Model: Bert Base Cased\n# config_1 = Configuration(\"bert-base-cased\", max_length = 32, batch_size = 32, epochs = 2, train_splits = 2)\n# preds_train_1, preds_test_1 = run_model(config_1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Model: Distilbert Multilingual Base Cased\n# config_2 = Configuration(\"distilbert-base-multilingual-cased\", translation = False, max_length = 50, batch_size = 32, epochs = 5, train_splits = 2)\n# preds_train_2,preds_test_2 = run_model(config_2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Model: XLNet Base Cased\n# config_3 = Configuration(\"xlnet-base-cased\", max_length = 50, batch_size = 32, epochs = 2, train_splits = 2)\n# preds_train_3 = run_model(config_3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Model: XLM Roberta Base\n# config_4 = Configuration(\"roberta-base\", translation = False, max_length = 84, batch_size = 64, epochs = 5, train_splits = 2)\n# preds_train_4, preds_test_4 = run_model(config_4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Submitting in the competition"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# import pandas as pd\n\n# df_submission = pd.DataFrame({\"id\": test_df.id.values, \"prediction\": np.argmax(preds_test_2, axis = 1)})\n# df_submission.to_csv(\"submission.csv\", index = False)\n\n# df_submission.prediction.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}