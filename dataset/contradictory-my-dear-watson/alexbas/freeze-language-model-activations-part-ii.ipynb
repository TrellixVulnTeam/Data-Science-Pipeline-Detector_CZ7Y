{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\nimport torch\nimport torch.nn as nn\nfrom torch.optim.lr_scheduler import *\nfrom tqdm.notebook import tqdm, tnrange\n\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\nfrom sklearn.model_selection import train_test_split\nfrom threading import Thread, Semaphore\nimport matplotlib.pyplot as plt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\nDEVICE","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train on frozen activations\n\nThe activations are imported from [Part I](https://www.kaggle.com/alexbas/freeze-language-model-activations-part-i)","metadata":{}},{"cell_type":"code","source":"MODEL_NAME = 'joeddav/xlm-roberta-large-xnli'\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n\nxnli_model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n\nlm = xnli_model.roberta\nhead = xnli_model.classifier.to(DEVICE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Loaders\nTest Loader - preserves order, fetches batches of pre-defined size\n\nTrain Loader - randomly picks smaller arrays (of size 16), since retrieving a batch takes a few seconds - does this on a separate thread (to offload IO)","metadata":{}},{"cell_type":"code","source":"ROOT_DIR =  '/kaggle/input/freeze-language-model-activations-part-i/'\n\nclass TestLoader:\n    def __init__(self):\n        self.folder = f'{ROOT_DIR}test'\n        self.len = len(os.listdir(self.folder))\n    def __len__(self):\n        return self.len\n    def __iter__(self):\n        folder = self.folder\n        for i in range(self.len):\n            X = torch.load(f'{folder}/{i}.pt', map_location=DEVICE)\n            yield X","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_folder = f'{ROOT_DIR}train'\ntotal_files = len([fn for fn in os.listdir(train_folder) if 'l_' != fn[:2]])\nbs = 256\nfiles_per_batch = bs // 16\nn_batches = total_files // files_per_batch","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_size = .1\n\ntrain_len = int((1-test_size) * total_files)\nall_idx = np.random.permutation(total_files)\ntrain_idx = all_idx[:train_len]\nval_idx = all_idx[train_len:]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fetch_batch(idx):\n    files_ids = np.random.choice(idx, size=files_per_batch, replace=False)\n    inputs, labels = [], []\n    for file_id in files_ids:\n        fn_x = f'{train_folder}/{file_id}.pt'\n        fn_y = f'{train_folder}/l_{file_id}.pt'\n        inputs.append(torch.load(fn_x))\n        labels.append(torch.load(fn_y))        \n    X, y = [torch.cat(x) for x in [inputs, labels]]\n    return X, y\n        \nclass Producer(Thread):\n    def __init__(self, idx):\n        super().__init__()\n        self.consume = Semaphore(0)\n        self.produce = Semaphore(0)\n        self.setDaemon(True)\n        self.batch = None\n        self.idx = idx\n    def run(self):\n        while True:\n            self.produce.acquire()\n            self.batch = fetch_batch(self.idx)\n            self.consume.release()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TrainDataloader:\n    def __init__(self, producer, n):        \n        self.producer = producer                                      \n        self.n = n\n        producer.start()\n        \n    def __len__(self):\n        return self.n\n                \n    def __iter__(self):\n        from time import sleep, time\n        consume = self.producer.consume\n        produce = self.producer.produce\n        produce.release()\n        for i in range(self.n):\n            consume.acquire()\n            X, y = self.producer.batch\n            produce.release()\n            yield X.to(DEVICE), y.to(DEVICE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dl = TrainDataloader(Producer(train_idx), n_batches)\nval_dl = TrainDataloader(Producer(val_idx), n_batches)\nmodel = head\nloss_func = torch.nn.CrossEntropyLoss()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class EpochStats():\n    def __init__(self):\n        self.y_hat = []\n        self.y = []\n        self.loss = 0\n    def update(self, y, logits, loss):\n        self.y.append(y)\n        self.y_hat.append(logits.argmax(1))\n        self.loss += loss\n    def metrics(self):\n        y, y_hat = [torch.cat(l) for l in [self.y, self.y_hat]]\n        loss = self.loss / y.shape[0]\n        accuracy = (y == y_hat).float().mean().item()\n        return accuracy, loss","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_batch(model, opt, loss_f, X, y):\n    opt.zero_grad()\n    logits = model(X)\n    loss = loss_f(logits, y)\n    loss.backward()\n    opt.step()\n    return logits, loss.item()\n    \ndef train_epoch(model, opt, loss_f, dl, e):\n    model.train()\n    stats = EpochStats()\n    for X, y in tqdm(dl, desc=f\"Epoch {e}\", leave=False):\n        logits, batch_loss = train_batch(model, opt, loss_f, X, y)\n        stats.update(y, logits, batch_loss)\n    return stats.metrics()\n\ndef eval_epoch(model, loss_f, dl):\n    model.eval()\n    stats = EpochStats()\n    with torch.no_grad():        \n        for X, y in dl:\n            logits = model(X)\n            batch_loss = loss_f(logits, y).item()\n            stats.update(y, logits, batch_loss)\n    return stats.metrics()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Trainer:\n    def __init__(self, model, opt, loss_f, train_dl, val_dl, scheduler=None):\n        self.model = model\n        self.opt = opt\n        self.loss_f = loss_f\n        self.train = train_dl\n        self.val = val_dl\n        self.metrics = []\n        self.scheduler = scheduler\n    def fit(self, n_epochs=1, print_metrics = True):\n        for i in tnrange(n_epochs):\n            train_metrics = train_epoch(self.model,\n                                       self.opt,\n                                       self.loss_f,\n                                       self.train,\n                                       i)\n            if self.scheduler:\n                self.scheduler.step()                \n            val_metrics = eval_epoch(self.model,\n                                    self.loss_f,\n                                    self.val)\n            metrics = train_metrics + val_metrics\n            self.metrics.append(metrics)\n            if print_metrics:\n                print(\"Epoch %d. Train acc: %.3f, loss: %.6f | Val acc: %.3f, loss: %.6f\" %\n                      tuple([i]+list(metrics)))\n    \n    def show_lc(self):\n        def show(ax, idx, name):\n            ax.set_title(name)\n            ax.plot(x, metrics[:, idx[0]], \n                    label = \"Train\")        \n            ax.plot(x, metrics[:, idx[1]], \n                    label = \"Val\")        \n            ax.legend()\n        metrics = np.array(self.metrics)\n        x = np.arange(metrics.shape[0])\n        f, ax = plt.subplots(1,2, figsize=(10,4))\n        show(ax[0], [1,3], \"Loss\")\n        show(ax[1], [0,2], \"Accuracy\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr = 3e-4\n\noptimizer = torch.optim.Adam(model.parameters(), lr)\nscheduler = StepLR(optimizer, 1, .6)\ntrainer = Trainer(model, \n                  optimizer, \n                  loss_func, \n                  train_dl, \n                  val_dl,\n                  scheduler)\ntrainer.fit(20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.show_lc()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def switch_labels(y):\n     return ((y-1) * -1) + 1\n\ndef predict(model, dl):\n    y_hat = []\n    for X in tqdm(dl, \"Running inference\"):\n        logits = model(X)\n        y_hat.append(logits.argmax(1))\n    preds = torch.cat(y_hat).cpu().numpy()\n    preds = switch_labels(preds)\n    return preds\n    \npredictions = predict(model, TestLoader())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test = pd.read_csv('/kaggle/input/contradictory-my-dear-watson/test.csv')\ndf_test['prediction'] = predictions\ndf_test[['id', 'prediction']].to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('submission.csv') as f:\n    for i in range(3):\n        print(f.readline().strip())","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}