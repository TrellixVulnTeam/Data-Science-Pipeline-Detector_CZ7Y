{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **TFM Izar Ortin Riquelme**\n## “Máster Big Data y Business Analytics UNED 2019/2020 - UNED”"},{"metadata":{},"cell_type":"markdown","source":"# i. Abstract\n\nEste trabajo analiza la efectividad comparativa de los dos principales modelos utilizados en problemas de tipo NLP, NLI y NLU, que son BERT y XML-RoBERTa, así como una propuesta de actuación a la hora de abordar este tipo de problemas; incorporando una mayor cantidad de datos similares con los que mejorar la capacidad predictiva de los mismos, realizar data augmentation y, finalmente, realizar un fine tuning sobre los hiperparámetros del modelo.\n\nRealizamos todo el entrenamiento del modelo en TPUs desarrollados por Google, y estructuramos todas las partes del modelo a través de funciones de la biblioteca Keras.\n\nFinalmente, al tratarse de una competición de kaggle, realizaremos un submission y trataremos de alcanzar la máxima puntuación posible."},{"metadata":{},"cell_type":"markdown","source":"# **ii.\tIntroducción y objetivos buscados**\n\nEste trabajo es parte de la evaluación del “Máster Big Data y Business Analytics UNED 2019/2020 - UNED”, y en él se desarrolla el análisis, dentro de la disciplina deep learning, de la competición de Kaggle “Contradictory, My Dear Watson. Detecting contradiction and entailment in multilingual text using TPUs”.\n\nEl **objetivo general** de este trabajo es encontrar un modelo NLI (Natural Language Inference) que asigne correctamente etiquetas correspondientes a implicación, neutralidad o contradicción a pares de premisas e hipótesis, sea cual sea el idioma en el que se encuentren y mejorar su capacidad de predicción.\n\nLos **objetivos específicos**, son:\n\n1.\tEncontrar el modelo de lenguaje que mejor clasifique los pares de sentencias del data-set a analizar, entre BERT y XML-Roberta.\n\n\n2.\tEntrenar el modelo escogido con bases de datos distintas a las proporcionadas inicialmente.\n\n\n3.\tRealizar un \"data augmentation\", para evitar problemas de sobreentrenamiento y mantener la estructura del data set original.\n"},{"metadata":{},"cell_type":"markdown","source":"# iii. Data preparation y carga de librerias necesarias\n\nLo primero que haremos es actualizar la librería transformers de Hugging Face, para evitar problemas a la hora de conectar con ciertos tokenizadores, como ocurría en versiones anteriores."},{"metadata":{"trusted":true,"_kg_hide-input":false,"_kg_hide-output":true},"cell_type":"code","source":"!pip install transformers==3.0.2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Establecemos las rutas de carga de archivos a los input del notebook."},{"metadata":{"_uuid":"d3a20929-8e1d-48d2-869c-cc57f8c63cc9","_cell_guid":"20666a1f-e31b-4134-94f8-fea9a50998d3","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.environ[\"WANDB_API_KEY\"] = \"0\" ## Para silenciar el aviso que nos daría Colab al no darle una KEY válida.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Importamos el resto de librerías que utilizaremos:"},{"metadata":{"_uuid":"e3dd507b-c502-488c-9dd0-419c5d73c159","_cell_guid":"863de620-d4b7-4711-b587-e1c75be9e36f","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"#Las relacionadas con transformers de Hugging Face.#Las relacionadas con transformers de Hugging Face.\n\nimport transformers \nfrom transformers import BertTokenizer, TFBertModel, AutoTokenizer, TFAutoModel\n\n#Las relacionadas con TensorFlow\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam, SGD, Adamax\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom kaggle_datasets import KaggleDatasets\n\n#El train-test split de Scikit-learn.\n\nimport sklearn\nfrom sklearn.model_selection import train_test_split\n\n#Las herramientas gráficas necesarias.\n\nimport plotly.express as px\nimport matplotlib.pyplot as plt\n\n#Para acceder a las librerías de datasets de Hugging Face.\n\n!pip install nlp\nimport nlp\n\n#Resto de funciones\n\nimport datetime\nfrom tqdm.notebook import tqdm\nimport random\nfrom PIL import Image\nimport requests\nfrom io import BytesIO\nimport io\nimport PIL","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Comprobamos que tenemos la versión de transformers correcta."},{"metadata":{"trusted":true},"cell_type":"code","source":"transformers.__version__","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"Image.open(\"../input/tf-logo/tf_logo.png\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"58fb9ec5-c099-494c-bf59-6ec9d6e64628","_cell_guid":"6c5122f9-8c39-4892-81a9-1f8830b64484","trusted":true},"cell_type":"markdown","source":"## Configuración de TensorFlow\n\nTensorFlow es una plataforma de código abierto de extremo a extremo para el aprendizaje automático. Cuenta con un ecosistema integral y flexible de herramientas, bibliotecas y recursos de la comunidad que les permite a los investigadores impulsar un aprendizaje automático innovador y, a los desarrolladores, compilar e implementar con facilidad aplicaciones con tecnología de AA.\n\nConfiguraremos nuestro TPU, siendo este uno de tipo V3-8.\n\nEste tipo define tanto la versión de TPU (v3) como la cantidad de núcleos de \"TPU-v3\" de que dispone.\n\nContamos con 8 y la cantidad de memoria de TPU que está disponible para la carga de trabajo de aprendizaje automático; que son 128 GiB, así como las zonas disponibles, que en este caso son us-central1-a,b y f."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"Image.open(\"../input/tpu-v38/tpu--sys-arch4 1.png\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"386d0823-63ab-4765-9561-32c5f382e71d","_cell_guid":"ca2729e3-9275-4a9c-b150-592320bd3e54","trusted":true},"cell_type":"code","source":"#Detectaremos el hardware requerido, y nos devolverá la distribución adecuada.\n\ntry:\n    #No es necesario introducir un valor en el \"resolver\", ya que la enviroment variable TPU_NAME ya está establecida en Kaggle.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    \n    #Le brindamos dicha información al tf.config.experimental_connect_to_cluster, que conecta con el cluster dado    \n    tf.config.experimental_connect_to_cluster(tpu)\n    \n    #Inicializamos el TPU.\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    \n    #Y definimos la estrategia de distribución.\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    \nexcept ValueError:\n    strategy = tf.distribute.get_strategy() \n    print('Number of replicas:', strategy.num_replicas_in_sync)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0b64130f-530a-4560-8afd-462a55fc14b3","_cell_guid":"44a1d22f-053c-4188-b25f-d714aa745016","trusted":true},"cell_type":"markdown","source":"## Descargamos dataset y EDA"},{"metadata":{"_uuid":"b9285071-38f2-421b-9b1c-9c44d41c7365","_cell_guid":"6fb2939c-b14a-450f-85dd-7220439eaf55","trusted":true},"cell_type":"markdown","source":"El set de entrenamiento contiene una premisa, una hipótesis, un nivel dado (0= Implicación, 1=neutral y 2=contradicción) y el lenguaje del texto. \n\nSon 12120 binomios de hipótesis/premisa en varios idiomas (Árabe, búlgaro, chino, alemán, griego, inglés, español, francés, hindi, ruso, swahili, tailandés, turco, urdu y vietnamita). \n\nPuede comprobarse en: https://www.kaggle.com/c/contradictory-my-dear-watson/data"},{"metadata":{"_uuid":"6e60d19f-aeae-417a-a10a-50cc1d5ee685","_cell_guid":"f3ad567b-a156-4ffc-a6f8-1f5e6e989a4e","trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/contradictory-my-dear-watson/train.csv\")\n\ntest = pd.read_csv(\"../input/contradictory-my-dear-watson/test.csv\")\n\nsubmission = pd.read_csv('/kaggle/input/contradictory-my-dear-watson/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Revisamos las primeras filas. Como puede observarse, se respetan tanto las tildes como los distintos grafos utilizados por las diferentes lenguas."},{"metadata":{},"cell_type":"markdown","source":"### Train, Validation y Test Sets en Machine Learning\n\nA continuación, explicaremos las diferencias entre estos tres conceptos.\n\n1. Training Dataset\n\n**Definición:** Una muestra de datos utilizada para ajustar el modelo.\n\nEn caso de una red neuronal, los weights y bias. El modelo observa y aprende de estos datos.\n\n2. Validation Dataset\n\n**Definición:** Es una muestra de datos utilizada para proporcionar una evaluación del entrenamiento del modelo, **mientras** se ajustan los hiperparametros en el entrenamiento. La evaluación se va volviendo más sesgada a medida que el conjunto de datos de validación se va incorporando al modelo en las distintas iteraciones.\n\nSe usan los datos para ajustar los hiperparámetros. Así, el modelo ve los datos, pero no aprende de ellos. El modelo recoge los resultados evaluados en el conjunto de validación y actualiza los hiperparámetros. \n\nSe entiende así que el conjunto de validación afecta al modelo, pero de manera indirecta, es decir: a través de los resultados obtenidos al evaluar el modelo con dichos datos.\n\nTambién se le conoce (aunque con menos frecuencia) como Development Dataset, y tiene sentido ya que ayuda al modelo en su etapa de \"desarrollo\".\n\n3. Test Dataset\n\n**Definición:** Es el conjunto de datos utilizado para evaluar de manera **insesgada** el ajuste final del modelo, que ha sido ajustado con los datos de entrenamiento. Este conjunto de datos solo se utiliza cuando el modelo esta completamente entrenado, y dicho conjunto debe estar cuidadosamente muestreado con las diferentes clases al que se enfrentaría el modelo en el mundo real.\n\nLas diferencias entre el concepto de test en data science y kaggle, las veremos al final de este trabajo."},{"metadata":{},"cell_type":"markdown","source":"Un esquema aproximado de las proporciones de datos que debe incorporar cada conjunto de datos, sería el siguiente:"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"response = requests.get('https://miro.medium.com/max/700/1*Nv2NNALuokZEcV6hYEHdGA.png')\nimage_bytes = io.BytesIO(response.content)\n\nimg = PIL.Image.open(image_bytes)\nimg","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### EDA"},{"metadata":{"_uuid":"82e6d183-b8e1-412b-816f-c9020bac1428","_cell_guid":"d3b9a632-7abb-4bef-acd3-9cba577dc2c0","trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"abfb11e9-865c-4b50-b0cc-9d25851618ff","_cell_guid":"817c2470-9778-46e8-be86-fdd1b9449b93","trusted":true},"cell_type":"markdown","source":"Revisamos un binomio de frases, empezando por la premisa"},{"metadata":{"_uuid":"dea716f0-9698-4a10-a7f1-1a0bb36054db","_cell_guid":"bcca3548-bb15-4868-86ec-95fe084d6e06","trusted":true},"cell_type":"code","source":"train.premise.values[1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ahora la hipótesis."},{"metadata":{"_uuid":"5b639eff-aeea-4dca-b516-7e729cdeb741","_cell_guid":"15dcbe6c-4914-4400-a8fd-65b6e4cbf652","trusted":true},"cell_type":"code","source":"train.hypothesis.values[1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Y su label, que como se puede observar, corresponde a una contradicción."},{"metadata":{"_uuid":"c63a91e0-05f7-4a67-acf8-2131ef50f054","_cell_guid":"e16e8879-c565-4a29-bacc-26648659da29","trusted":true},"cell_type":"code","source":"train.label.values[1]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"216dc9c2-1699-4b89-af27-443d1b4c7289","_cell_guid":"8c9122ea-797d-48f1-b4ba-85bc2e0e6f18","trusted":true},"cell_type":"markdown","source":"Revisamos la distribución de idiomas en el training set."},{"metadata":{"_uuid":"7243f511-d81e-436c-971f-6328b7c0cf43","_cell_guid":"7d9b43ef-4ac1-40d5-aebc-a162f1b9a6c0","trusted":true},"cell_type":"code","source":"labels, frequencies = np.unique(train.language.values, return_counts = True)\n\nplt.figure(figsize = (10,10))\nplt.pie(frequencies,labels = labels, autopct = '%1.1f%%')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Veamos cuantos valores son nulos:"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Como podemos observar, no hay ningún valor null"},{"metadata":{"_uuid":"68cee874-838e-4f0e-9f80-bfebc4a295d0","_cell_guid":"a7f5d429-083e-4d81-883f-e032dfb0e236","trusted":true},"cell_type":"markdown","source":"# iv. Evaluación de modelos"},{"metadata":{"_uuid":"27e66828-76b9-44f4-ba79-66fe0cb8f922","_cell_guid":"f20a33db-a377-43b6-825e-157456ed1092","trusted":true},"cell_type":"markdown","source":"## Definición de las principales variables "},{"metadata":{"trusted":true},"cell_type":"code","source":"#Definimos los dos modelos a estudiar\nmodel_xml_roberta = 'jplu/tf-xlm-roberta-large'\n\nmodel_bert = 'bert-base-multilingual-cased'\n\n#Definimos el número de epochs a iterar y el max_len\nn_epochs = 10\nmax_len = 80\n\n# El tamaño de nuestro batch_size dependerá del número de réplicas en nuestra estrategia\nbatch_size = 16 * strategy.num_replicas_in_sync","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Características de los modelos\n\n### Overview\n\n* BERT fue lanzado en noviembre de 2018 por Google, con modelos para Inglés y Chino, poco después incluyeron un modelo más, el mBERT, que incluía 104 lenguas diferentes.\n\n* XLM-RoBERTa fue lanzando en noviembre de 2019 por Facebook, incluyendo 100 lenguas diferentes.\n\nSi bien es cierto que ambos son modelos multilenguaje, hay un elemento que en un principio parece darle un mayor protagonismo a XLM-R sobre BERT:\n\nAnalistas de la Charles University en Praga, encontraron que mBERT internamente clusteriza lenguas en \"familias\" y representa oraciones similares de manera diferente en diferentes idiomas. Esto ayudaría a explicar que clasifique mejor en unas pocas lenguas (ingles sobre todo) frente a otras.\nEsta diferencia, tienen un efecto en la precisión a la hora de clasificar y pueden observarse en la siguiente tabla (obtenida del  Unsupervised Cross-lingual Representation Learning at Scale by Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer and Veselin Stoyanov). \n"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"Image.open(\"../input/table-accurancy-models/Table accurancy models.png\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Especificaciones y funcionamiento de BERT"},{"metadata":{},"cell_type":"markdown","source":"* **Arquitectura del modelo**\n\nPara empezar, hemos de saber que existen dos tamaños distintos para el modelo BERT:\n\n1. BERT BASE: con 12 capas, 12 attention heads y 110 millones de parámetros.\n\n2. BERT LARGE: con 24 capas, 16 attention heads y 340 millones de parámetros."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"response = requests.get('http://jalammar.github.io/images/bert-base-bert-large-encoders.png')\nimage_bytes = io.BytesIO(response.content)\n\nimg = PIL.Image.open(image_bytes)\nimg","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Bert es básicamente un Transformer Encoder stack entrenado, en el que se le introducen unos inputs compuestos de tokens, los cuales son procesados y arroja unos outputs numericos."},{"metadata":{},"cell_type":"markdown","source":"El esquema de entrada seria así:"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"response = requests.get('http://jalammar.github.io/images/bert-encoders-input.png')\nimage_bytes = io.BytesIO(response.content)\n\nimg = PIL.Image.open(image_bytes)\nimg","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"La estructura de estos tokens, son explicados más adelante en el apartado \"Similitudes\".\n\nEn cuanto al output, sería un vector numérico, siguiendo el siguiente esquema:"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"response = requests.get('http://jalammar.github.io/images/bert-output-vector.png')\nimage_bytes = io.BytesIO(response.content)\n\nimg = PIL.Image.open(image_bytes)\nimg","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **Pre-Procesamiento de texto**\n\nLos desarrolladores de BERT, han establecido una serie de reglas a la hora de introducir los imputs.\n\nCada input embedding es una combinación de 3 embeddings:\n\n1. Position Embeddings: El modelo usa y conoce la posición de los distintos vocablos en el input introducido, lo cual lo diferencia de las Redes Neuronales Recurrentes (RNN) que no son capaces de captar información de \"orden\" o \"secuencia\".\n\n2. Segment Embeddings: Aprende una inserción única para la primera y segunda oración y ayuda al modelo a distinguir entre ellas. Esto también genera problemas a la hora de calcular loss functions si una misma primera oración se repite con distintas segundas oraciones, lo cual ocurre en la base de datos SNLI.\n\n3. Token Embeddings: De esta última es de donde aprenden los distintos token específicos procedentes del WordPiece."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"response = requests.get('https://cdn.analyticsvidhya.com/wp-content/uploads/2019/09/bert_emnedding.png')\nimage_bytes = io.BytesIO(response.content)\n\nimg = PIL.Image.open(image_bytes)\nimg","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **Masked Language Modeling**\n\nBERT es un modelo altamente bidireccional.\n\nA diferencia de modelos contexto de izquierda a derecha (o viceversa), que son entrenados para predecir la siguiente palabra de la oración, y por lo tanto susceptibles a error por perdida de información, como ocurre con el modelo GPT; el modelo ELMo, trató de solventar dicho problema entrenando dos LSTM de izquierda a derecha y de derecha a izquierda y concatenandolos a posteriori, pero no resultó lo suficientemente preciso.\n\nA continuación podemos ver el esquema de flujos de informacion de BERT, OpenAI-GPT y ELMo:"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"response = requests.get('https://cdn.analyticsvidhya.com/wp-content/uploads/2019/09/bert-vs-openai-.jpg')\nimage_bytes = io.BytesIO(response.content)\n\nimg = PIL.Image.open(image_bytes)\nimg","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Las flechas representan la dirección de la información de una capa a otra. Se puede apreciar facilmente que BERT es bidireccional, GPT es unidireccional y ELMo es artificialmente \"bidireccional\".\n\n"},{"metadata":{},"cell_type":"markdown","source":"### Especificaciones y funcionamiento de XML-RoBERTa"},{"metadata":{},"cell_type":"markdown","source":"* **Arquitectura del modelo**\n\nPara empezar, hemos de saber que existen dos tamaños distintos para el modelo BERT:\n\nXML-R BASE: con 12 capas, 12 attention heads y 270 millones de parametros.\n\nXML-R LARGE: con 24 capas, 16 attention heads y 550 millones de parametros."},{"metadata":{},"cell_type":"markdown","source":"* **Training Data**\n\nLa base de datos usada para entrenar a XML-RoBERTa, es significativamente mayor que la usada por BERT, ya que este usa Wiki-100, una base de datos procedente de Wikipedia, mientras que XML-R usa CommonCrawl. Se utilizó CommonCrawl ya que aumenta considerablemente el conjunto de datos, sobre todo para idiomas de bajos recursos, como el Birmano y el Suajili. Comprobamos esto en el gráfico mostrado a continuación:"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"response = requests.get('https://miro.medium.com/max/2272/1*9cRchmIyxP4LUnONXLM82g.png')\nimage_bytes = io.BytesIO(response.content)\n\nimg = PIL.Image.open(image_bytes)\nimg","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **Tokenización multilenguaje**\n\nSe entrena el Sentence Piece Model (SPM) y se aplica directamente en los datos de texto sin procesar. No se observa pérdida de rendimiento con respecto a modelos entrenados con preprocesamiento específico del lenguaje y codificación tipo byte-pair, como puede observarse en la siguiente figura."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"response = requests.get('https://images.deepai.org/converted-papers/1911.02116/x7.png')\nimage_bytes = io.BytesIO(response.content)\n\nimg = PIL.Image.open(image_bytes)\nimg","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n### Similitudes\n\nAmbos modelos tienen unos inputs parecidos.\n\nSe necesita codificar todos nuestros pares de premisas/hipótesis y prepararemos los inputs necesarios para el modelo: input word IDs, input masks y  input type IDs; siendo:\n\n•\t*input word IDs*: Las palabras (o segmentos de palabras) convertidos a IDs.\n\n•\t*Input masks*: Los IDs que ayudan al modelo a distinguir entre los tokens relevantes de los de relleno o padding, que son añadidos para que ambas sentencias tengan una misma longitud.\n\n•\t *input type IDs*: Se le asignan ceros tanto al CLS como a la primera sentencia y unos a la segunda.\n\nLa principal diferencia en este punto es la configuración de los datos de entrada en los *input word IDS*:\n\n•\tBERT: Son del tipo [CLS] A [SEP] B [SEP], siendo A y B un par de sentencias.\n\n•\tXML-Roberta: Son del tipo < s > A < /s > < /s > B < /s >, siendo A y B un par de sentencias."},{"metadata":{},"cell_type":"markdown","source":"## BERT"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cargamos el tokenizador correspondiente\ntokenizer_bert = AutoTokenizer.from_pretrained(model_bert)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Transformaremos el texto en listas, para poder introducirlas en el batch_encode_plus\ntrain_text_bert = train[['premise', 'hypothesis']].values.tolist()\ntest_text_bert = test[['premise', 'hypothesis']].values.tolist()\n\n# Ahora utilizaremos el tokenizador que hemos preparado previamente.\ntrain_encoded_bert = tokenizer_bert.batch_encode_plus(\n    train_text_bert,\n    pad_to_max_length=True,\n    max_length=max_len\n)\n\ntest_encoded_bert = tokenizer_bert.batch_encode_plus(\n    test_text_bert,\n    pad_to_max_length=True,\n    max_length=max_len\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cortamos el conjunto de entrenamiento original en entrenamiento y validación"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train, x_valid, y_train, y_valid = train_test_split(\n    train_encoded_bert['input_ids'], train.label.values, \n    test_size=0.2, random_state=2020\n)\n\nx_test = test_encoded_bert['input_ids']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Convertimos los diferentes conjuntos de datos en **tf.data.Dataset.**\n\nA la hora de operar con TPUs, antes que suministrar diccionarios al modelo, es mucho más interesante mover la información a través de tf.data.Dataset, ya que se canaliza de una manera mucho más eficiente y ágil.\n\nLo que entre otras ventajas, recorta sensiblemente los tiempos que necesita el modelo para realizar las distintas iteraciones de la estimación (epochs).\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"auto = tf.data.experimental.AUTOTUNE\n\ntrain_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((x_train, y_train))\n    .repeat()\n    .shuffle(2048)\n    .batch(batch_size)\n    .prefetch(auto)\n)\n\nvalid_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((x_valid, y_valid))\n    .batch(batch_size)\n    .cache()\n    .prefetch(auto)\n)\n\ntest_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices(x_test)\n    .batch(batch_size)\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ahora construiremos el modelo dentro del TPU, lo haremos a través de la orden strategy.scope() de forma que todo estará dentro del scope de la strategy y no lo correrá el CPU por defecto."},{"metadata":{"trusted":true},"cell_type":"code","source":"with strategy.scope():\n    # Primero cargamos la capa de codificador.\n    transformer_encoder = TFAutoModel.from_pretrained(model_bert)\n\n    # Definimos los inputs tokenizados\n    input_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_ids\")\n\n    # Ahora, codificamos los inputs segun el encoder que hemos definido anteriormente.\n    sequence_output = transformer_encoder(input_ids)[0]\n\n    # Extraemos los tokens utilizados para clasificar, en este caso [CLS]\n    cls_token = sequence_output[:, 0, :]\n\n    # La última capa es la que pasamos a través de softmax para la aplicación del uso de probabilidades. Con 3 niveles, ya que son 3 posibles resultados (0,1 y 2)\n    out = Dense(3, activation='softmax')(cls_token)\n\n    # Construimos y compilamos el modelo.\n    model = Model(inputs=input_ids, outputs=out)\n    model.compile(\n        Adam(lr=1e-5), \n        loss='sparse_categorical_crossentropy', \n        metrics=['accuracy']\n    )\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Y finalmente lo entrenaremos:"},{"metadata":{"trusted":true},"cell_type":"code","source":"n_steps = len(x_train) // batch_size\n\ntrain_history_bert = model.fit(\n    train_dataset,\n    steps_per_epoch=n_steps,\n    validation_data=valid_dataset,\n    epochs=n_epochs\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ahora revisaremos el comportamiento de las funciones de pérdida y de precisión, para poder resolver si el modelo esta sobreentrenado o hay que seguir iterando."},{"metadata":{"_uuid":"3bfd96e9-cf2c-41ad-bdcc-8c31408f05bd","_cell_guid":"e26cba08-7fe9-4e2e-ab2e-062a26a5fcac","trusted":true},"cell_type":"code","source":"# list all data in history\nprint(train_history_bert.history.keys())\n# summarize history for loss\nplt.plot(train_history_bert.history['loss'])\nplt.plot(train_history_bert.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()\n# summarize history for accuracy\nplt.plot(train_history_bert.history['accuracy'])\nplt.plot(train_history_bert.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## XML-Roberta"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cargamos el tokenizador correspondiente\ntokenizer_xml_roberta = AutoTokenizer.from_pretrained(model_xml_roberta)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Transformaremos el texto en listas, para poder introducirlas en el batch_encode_plus\ntrain_text_xml_roberta = train[['premise', 'hypothesis']].values.tolist()\ntest_text_xml_roberta = test[['premise', 'hypothesis']].values.tolist()\n\n# Ahora utilizaremos el tokenizador que hemos preparado previamente.\ntrain_encoded_xml_roberta = tokenizer_xml_roberta.batch_encode_plus(\n    train_text_xml_roberta,\n    pad_to_max_length=True,\n    max_length=max_len\n)\n\ntest_encoded_xml_roberta = tokenizer_xml_roberta.batch_encode_plus(\n    test_text_xml_roberta,\n    pad_to_max_length=True,\n    max_length=max_len\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cortamos el conjunto de entrenamiento original en entrenamiento y validación."},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train, x_valid, y_train, y_valid = train_test_split(\n    train_encoded_xml_roberta['input_ids'], train.label.values, \n    test_size=0.2, random_state=2020\n)\n\nx_test = test_encoded_xml_roberta['input_ids']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Convertimos los diferentes conjuntos de datos en tf.data.Dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"auto = tf.data.experimental.AUTOTUNE\n\ntrain_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((x_train, y_train))\n    .repeat()\n    .shuffle(2048)\n    .batch(batch_size)\n    .prefetch(auto)\n)\n\nvalid_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((x_valid, y_valid))\n    .batch(batch_size)\n    .cache()\n    .prefetch(auto)\n)\n\ntest_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices(x_test)\n    .batch(batch_size)\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ahora construiremos el modelo dentro del TPU, lo haremos a través de la orden strategy.scope() de forma que todo estará dentro del scope de la strategy y no lo correrá el CPU por defecto."},{"metadata":{"trusted":true},"cell_type":"code","source":"with strategy.scope():\n    # Primero cargamos la capa de codificador.\n    transformer_encoder = TFAutoModel.from_pretrained(model_xml_roberta)\n\n    # Definimos los inputs tokenizados\n    input_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_ids\")\n\n    # Ahora, codificamos los inputs segun el encoder que hemos definido anteriormente.\n    sequence_output = transformer_encoder(input_ids)[0]\n\n    # Extraemos los tokens utilizados para clasificar, en este caso <s>\n    cls_token = sequence_output[:, 0, :]\n\n    # La última capa es la que pasamos a través de softmax para la aplicación del uso de probabilidades. Con 3 niveles, ya que son 3 posibles resultados (0,1 y 2)\n    out = Dense(3, activation='softmax')(cls_token)\n\n    # Construimos y compilamos el modelo.\n    model = Model(inputs=input_ids, outputs=out)\n    model.compile(\n        Adam(lr=1e-5), \n        loss='sparse_categorical_crossentropy', \n        metrics=['accuracy']\n    )\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Y finalmente lo entrenaremos:"},{"metadata":{"trusted":true},"cell_type":"code","source":"n_steps = len(x_train) // batch_size\n\ntrain_history_xml_roberta = model.fit(\n    train_dataset,\n    steps_per_epoch=n_steps,\n    validation_data=valid_dataset,\n    epochs=n_epochs\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ahora revisaremos el comportamiento de las funciones de pérdida y de precisión, para poder resolver si el modelo esta sobreentrenado o hay que seguir iterando"},{"metadata":{"trusted":true},"cell_type":"code","source":"# list all data in history\nprint(train_history_xml_roberta.history.keys())\n# summarize history for loss\nplt.plot(train_history_xml_roberta.history['loss'])\nplt.plot(train_history_xml_roberta.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()\n# summarize history for accuracy\nplt.plot(train_history_xml_roberta.history['accuracy'])\nplt.plot(train_history_xml_roberta.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Overfitting y Undercomputing\n\nEl problema central en machine learning es el aprendizaje supervisado. \n\nLos algoritmos de aprendizaje esencialmente buscan un espacio de funciones (llamadas normalmente hypothesis class) para una función dada que encaje en un conjunto de datos dado.\n\nEsto lo diferencia de la programación tradicional a un nivel primario, así, mientras que en la programación tradicional se trata de escoger un programa o función dada, introducirle datos, y nos da un output (o respuesta esperada), en machine learning, se trata de lo opuesto; se le proporciona datos y respuestas y nos tiene que devolver un programa o función que encaje a ambos."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"response = requests.get('https://miro.medium.com/max/398/1*BfvKeP4ykqi4J4C5g4EZzg.png')\nimage_bytes = io.BytesIO(response.content)\n\nimg = PIL.Image.open(image_bytes)\nimg","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Debido a que el número de hypothesis functions es exponencialmente enorme, no se puede examinar individualmente cada una de ellas. En lugar de eso, se suele formalizar definiendo una funcion objetiva (por ejemplo, el número de puntos incorrectamente predichos) y aplicando varios algoritmos que minimicen esa función objetivo (suelen ser algoritmos heurísticos como el *gradient descent*).\n\nLa clave del problema surge de la tarea asignada al proble de machine-learning. Un algoritmo es entrenado con un conjunto de datos de entrenamiento, pero una vez entrenado es aplicado para realizar predicciones de datos nuevos. El **objetivo último** es maximizar su precisión predictiva de estos nuevos datos, no la precisión en los datos de entrenamiento.\n\nSi trabaja excesivamente en estos datos de entrenamiento, absorbe sus peculiaridades (que no tienen por qué repetirse en los nuevos datos que queremos tratar), por lo tanto, genera un ruido no deseado en lugar de encontrar una regla de predicción general. Este fenómeno se denomina **overfitting** o sobre-entrenamiento.\n\nAsí, un algoritmo demasiado codicioso a la hora de encontrar un árbol de decisión o red neuronal que minimice la función objetivo al máximo, en teoría, la óptima, podría no ser válido para nuestro objetivo último, mientras que otro que encuentre un resultado sub-óptimo durante el entrenamiento, podría resultarnos el resultado óptimo a la hora de predecir nuevos datos. A este concepto se le denomina *undercomputing*, y actuaría efectivamente a la hora de evitar el overfitting."},{"metadata":{},"cell_type":"markdown","source":"### Conclusiones analisis de modelos\n\nAunque ambos modelos sobreentrenan a partir de la 3ª iteración, (momento en el que se cruzan las funciones de pérdida del entrenamiento y validación) en dicho punto la precisión del XLM-Roberta es mayor que la de BERT, por lo que utilizaremos el **XML-Roberta** a partir de este punto."},{"metadata":{},"cell_type":"markdown","source":"# v. Dataset Extra"},{"metadata":{},"cell_type":"markdown","source":"A la hora de usar datasets extra hay que tener en cuenta que si el conjunto de datos es diferente, probablemente sus características tengan diferentes significados y tamaños.\n\nEstos datos deberían reunir el significado, la interpretación y el tamaño del conjunto de entrada original a estudiar, ya que de otra forma, la interpretabilidad y la confianza del modelo serían menos útiles que el conjunto original de datos únicamente.\n\nPor lo que en nuestro caso, lo hacemos con un gran cuidado y fijando unos parámetros:\n\n1. Tamaño: Las frases introducidas en todos los conjuntos que añadimos al modelo, son de una longitud similar.\n\n2. Interpretación: En este punto también coinciden, ya que son todas pares de sentencias de premisas e hipótesis junto con un label de relación entre ambas.\n\n3. Estructura de los datos: Si bien XNLI tiene una distribución de idiomas parecida al conjunto original, no es el mismo, y una vez realicemos el data augmentation, esta proporción quedaría más desvalanceada. De esta forma,  solucionamos esta eventualidad introduciendo una muestra del MNLI (que sólo son sentencias en inglés) para mantener la proporción de idiomas del conjunto de datos original."},{"metadata":{},"cell_type":"markdown","source":"Ahora cargaremos dos conjuntos de datos extra;\n\nEl corpus **MNLI**: Es una colección de múltiples fuentes con 433.000 pares de oraciones anotadas con información sobre la vinculación semántica entre ellas. Todas en inglés.\n\nEl corpus **XNLI**: Es una colección de 7500 pares de de oraciones, traducidas a 15 idiomas. Lo que da un total de 112.500 pares de oraciones."},{"metadata":{},"cell_type":"markdown","source":"### MNLI"},{"metadata":{},"cell_type":"markdown","source":"Cargamos el set."},{"metadata":{"trusted":true},"cell_type":"code","source":"mnli = nlp.load_dataset(path='glue', name='mnli')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Convertimos a dataframe los subconjuntos, ya que en este dataset hay 3 subconjuntos de datos \"train\", \"validation_matched\" y \"validation_mismatched\""},{"metadata":{"trusted":true},"cell_type":"code","source":"mnli_train_df = pd.DataFrame(mnli['train'])\n\nmnli_train_df = mnli_train_df[['premise', 'hypothesis', 'label']]\n\nmnli_train_df['lang_abv'] = 'en'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Esta es una base de datos que no nos concuerda con la proporción de idiomas utilizados en nuestro data set original, por lo que sólo utilizaremos las filas necesarias para ajustar el dataset final a la proporción de idiomas hallada en el dataset original."},{"metadata":{"trusted":true},"cell_type":"code","source":"mnli_train_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mnli_sample= mnli_train_df.sample(n = 40000,random_state= 2020)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mnli_sample.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### XNLI"},{"metadata":{},"cell_type":"markdown","source":"Cargamos el dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"xnli = nlp.load_dataset(path='xnli')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Convertimos a dataframe. En éste, en cambio, sólo hay un subconjunto de datos"},{"metadata":{"trusted":true},"cell_type":"code","source":"buffer = {\n    'premise': [],\n    'hypothesis': [],\n    'label': [],\n    'lang_abv': []\n}\n\n\nfor x in xnli['validation']:\n    label = x['label']\n    for idx, lang in enumerate(x['hypothesis']['language']):\n        hypothesis = x['hypothesis']['translation'][idx]\n        premise = x['premise'][lang]\n        buffer['premise'].append(premise)\n        buffer['hypothesis'].append(hypothesis)\n        buffer['label'].append(label)\n        buffer['lang_abv'].append(lang)\n        \n# convert to a dataframe and view\nxnli_valid_df = pd.DataFrame(buffer)\nxnli_valid_df = xnli_valid_df[['premise', 'hypothesis', 'label', 'lang_abv']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Revisamos el subconjunto"},{"metadata":{"trusted":true},"cell_type":"code","source":"xnli_valid_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# vi. Data Augmentation"},{"metadata":{},"cell_type":"markdown","source":"El proceso de Data Augmentation puede actuar como un regularizador en la prevención de overfitting en redes neuronales y mejorar el rendimiento en casos de problemas desbalanceados (como es nuestro caso). \n\nPero esto tiene una serie de limitaciones; al realizar cualquier proceso de Data Augmentation, corremos el riesgo de modificar el significado de las variables de estudio de tal forma que ya no correspondan a los labels asignados, por lo que debe de hacerse con extremo cuidado y siendo conocedores de cómo afecta cada cambio introducido en el modelo a entrenar.\n\nSi bien es posible realizar este proceso en *data-space* (aumentando el numero de datos pero manteniendo las características) o hacerlo en *feature-space* (modificando ciertas caracteristicas creando nuevos datos con estas nuevas características), es más recomendable realizarlo en data-space, como indican diferentes estudios, entre ellos \"Understanding data augmentation for classification: when to warp?\" de Sebastien C. Wong, Adam Gatt,Victor Stamatescu and Mark D. McDonnell, en orden de mantener la adecuada correlación de los labels.\n\nUn ejemplo de esto, sería rotar o deformar una imagen o variar la pose del objeto en un caso de clasificación de imágenes:\n\n* Mientras que rotarlo, deformarlo o transformarlo a blanco y negro sería un cambio en **data-space**:"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"response = requests.get('https://miro.medium.com/max/750/0*cZwrV8EyfJSeunag.png')\nimage_bytes = io.BytesIO(response.content)\n\nimg = PIL.Image.open(image_bytes)\nimg","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Realizar un cambio en la pose del objeto, sería un cambio en **feature-space**:"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"response = requests.get('https://d3i71xaburhd42.cloudfront.net/f3ee8dcaaad5f47f347354fe5d740096097cbed5/1-Figure1-1.png')\nimage_bytes = io.BytesIO(response.content)\n\nimg = PIL.Image.open(image_bytes)\nimg","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"En orden de aumentar el ponderamiento del conjunto de datos original, y que por lo tanto, tenga más peso sobre el análisis, vamos a realizar un data augmentation de tipo data-space:\n\n1. Traduciendo las frases no inglesas a inglés.\n\n2. Traduciendo frases en inglés a lenguas escogidas aleatoriamente."},{"metadata":{},"cell_type":"markdown","source":"### Descargaremos las librerías necesarias"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"!pip -q install googletrans\nimport gc\nfrom googletrans import Translator\nfrom dask import bag, diagnostics\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Estableceremos una semilla, para mantener la elección de lenguas."},{"metadata":{"trusted":true},"cell_type":"code","source":"SEED = 42\nos.environ['PYTHONHASHSEED']=str(SEED)\nrandom.seed(SEED)\nnp.random.seed(SEED)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Definimos las funciones necesarias para traducir el conjunto de datos."},{"metadata":{"trusted":true},"cell_type":"code","source":"def translate(words, dest):\n    dest_choices = ['zh-cn',\n                    'ar',\n                    'fr',\n                    'sw',\n                    'ur',\n                    'vi',\n                    'ru',\n                    'hi',\n                    'el',\n                    'th',\n                    'es',\n                    'de',\n                    'tr',\n                    'bg'\n                    ]\n    if not dest:\n        dest = np.random.choice(dest_choices)\n        \n    translator = Translator()\n    decoded = translator.translate(words, dest=dest).text\n    return decoded\n\ndef trans_parallel(df, dest):\n    premise_bag = bag.from_sequence(df.premise.tolist()).map(translate, dest)\n    hypo_bag =  bag.from_sequence(df.hypothesis.tolist()).map(translate, dest)\n    with diagnostics.ProgressBar():\n        premises = premise_bag.compute()\n        hypos = hypo_bag.compute()\n    df[['premise', 'hypothesis']] = list(zip(premises, hypos))\n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finalmente, realizamos la traducción:"},{"metadata":{"trusted":true},"cell_type":"code","source":"eng = train.loc[train.lang_abv == \"en\"].copy().pipe(trans_parallel, dest=None)\nnon_eng =  train.loc[train.lang_abv != \"en\"].copy().pipe(trans_parallel, dest='en')\ntrain_data_translate = train.append([eng, non_eng])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# vii. Entrenamiento final del modelo"},{"metadata":{},"cell_type":"markdown","source":"### Análisis del conjunto final \nEn esta parte final del trabajo, unificaremos todas los dataset, y entrenaremos al modelo con ellos."},{"metadata":{"trusted":true},"cell_type":"code","source":"frames = [train_data_translate, xnli_valid_df, mnli_sample]\ntrain_data_def = pd.concat(frames)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Y reiniciearemos el TPU, ya que se encuentra saturado por los cálculos anteriores y no sería capaz de mover el modelo final"},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.tpu.experimental.initialize_tpu_system(tpu)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Comprobaremos que mantienen tanto el formato, como la proporción de lenguas inglesas y no inglesas de comienzo."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data_def.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels, frequencies = np.unique(train_data_def.lang_abv.values, return_counts = True)\n\nplt.figure(figsize = (10,10))\nplt.pie(frequencies,labels = labels, autopct = '%1.1f%%')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Podemos observar que el conjunto de datos final, guarda unas proporciones de idiomas muy similar al conjunto original, así como al conjunto de test de esta competición.\n\nLo cual es muy interesante de cara al último paso: la confección del submission."},{"metadata":{},"cell_type":"markdown","source":"### Entrenamiento del modelo definitivo"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Transformaremos el texto en listas, para poder introducirlas en el batch_encode_plus\ntrain_text_xml_roberta = train_data_def[['premise', 'hypothesis']].values.tolist()\ntest_text_xml_roberta = test[['premise', 'hypothesis']].values.tolist()\n\n# Ahora utilizaremos el tokenizador que hemos preparado previamente.\ntrain_encoded_xml_roberta = tokenizer_xml_roberta.batch_encode_plus(\n    train_text_xml_roberta,\n    pad_to_max_length=True,\n    max_length=max_len\n)\n\ntest_encoded_xml_roberta = tokenizer_xml_roberta.batch_encode_plus(\n    test_text_xml_roberta,\n    pad_to_max_length=True,\n    max_length=max_len\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train, x_valid, y_train, y_valid = train_test_split(\n    train_encoded_xml_roberta['input_ids'], train_data_def.label.values, \n    test_size=0.2, random_state=2020\n)\n\nx_test = test_encoded_xml_roberta['input_ids']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"auto = tf.data.experimental.AUTOTUNE\n\ntrain_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((x_train, y_train))\n    .repeat()\n    .shuffle(2048)\n    .batch(batch_size)\n    .prefetch(auto)\n)\n\nvalid_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((x_valid, y_valid))\n    .batch(batch_size)\n    .cache()\n    .prefetch(auto)\n)\n\ntest_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices(x_test)\n    .batch(batch_size)\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### ADAM, ADAMAX y SGD"},{"metadata":{},"cell_type":"markdown","source":"**ADAM**\n\nEs un método de optimización estocástica que solo requiere gradientes de primer orden con relativa poca memoria. \n\nCalcula las tasas de aprendizaje adaptativo individuales para los diferentes parámetros a partir de estimaciones del primer y segundo momento de los gradientes. El nombre proviene de ADAptative Moment estimation. \nEstá pensado para combinar las ventajas de dos métodos recientemente populares: AdaGrad que funciona bien con gradientes dispersos y RMSProp que funciona bien en configuraciones on-line y no estacionarias.\n\nAlgunas de sus ventajas son:\n\n* Las magnitudes de las actualizaciones de los parámetros son invariantes al cambio de escala del gradiente.\n\n* Los tamaños de los steps están aproximadamente delimitados por el hiperparámetro stepsize.\n\n* No requiere un parámetro estacionario objetivo.\n\n* Trabaja con gradientes reducidos.\n\nSu algoritmo sería el siguiente:"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"response = requests.get('https://miro.medium.com/max/1100/1*zfdW5zAyQxge85gA_mFPYg.png')\nimage_bytes = io.BytesIO(response.content)\n\nimg = PIL.Image.open(image_bytes)\nimg","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"El principal objetivo es minimizar el valor de la función estocástica objetivo. La esperanza de dicha función es el parámetro theta. Con los diferentes f sub 0, 1, 2,..., T, denotamos las realizaciones de los subsecuentes steps.\n\nLa estocasticidad puede provenir de la evaluación en submuestras aleatorias (minibatches) de puntos de datos, o surgir del ruido inherente a la propia función.\n\nCon gt = ∇θft(θ) denotamos el gradiente, es decir, el vector de las derivadas parciales de f con respecto a θ evaluado a timestep t.\n\nEl algoritmo actualiza las medias móviles exponenciales del gradiente (mt) y el cuadrado del gradiente (vt) donde los hiperparametros beta1 y beta2 controlan las tasas de caída exponencial de estas medias móviles, tanto gt como vt.\n\nLas medias móviles en si mismas son estimadores del primer momento (la media) y del segundo momento (la varianza) del gradiente. Sin embargo, dichas medias móviles, son inicializadas como vectores 0, lo que lleva a que las estimaciones estén sesgadas hacia 0, especialmente durante los pasos iniciales, y también cuando los ratios de caída son pequeños (es decir, cuando las betas tienden a 1). Aunque esta inicialización es fácilmente contrarrestada en el **bias-corrected** estimando los parámetros indicados en el Compute bias-corrected first moment estimate y Compute bias-corrected second raw moment estimate.\n\nEn Adam, la regla de actualización para pesos individuales es escalar sus gradientes inversamente proporcionales a un (escalado) L^2 normal de sus gradientes individuales actuales y pasados.\n\nPara conocer en más profundidad la **regla de actualizacion de Adam**, el **bias-corrected** o un análisis teórico de la convergencia de Adam in online convex programming, consultar \"ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION\" de Diederik P. Kingma y Jimmy Lei Ba."},{"metadata":{},"cell_type":"markdown","source":"### ADAMAX\n\nEste modelo se basa en Adam, pero generalizando la regla de actualización L^2 a una L^p. Esta variante se vuelve numéricamente inestable para grandes p, es decir cuando p->∞, pero en este caso, surge un algoritmo que estabilizaría la función (cuya demostración matemática no incluiré en este trabajo. Puede consultarse fácilmente en \"ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION\" de Diederik P. Kingma y Jimmy Lei Ba).\n\nA continuación, veremos el algoritmo completo de ADAMAX, una vez sustituido por la nueva regla de actualización, quedando así como una variante de Adam basada en un p que tiende a infinito.\n\nAdamax es a veces superior a Adam, especialmente en modelos con embeddings."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"response = requests.get('https://paperswithcode.com/media/methods/Screen_Shot_2020-05-28_at_6.15.37_PM_apRrZCo.png')\nimage_bytes = io.BytesIO(response.content)\n\nimg = PIL.Image.open(image_bytes)\nimg","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### SGD\n\nEste optimizador, realizara una actualización de parámetros para cada ejemplo de entrenamiento y su label.\n\nθ=θ−η⋅∇θJ(θ;x(i);y(i))\n\nLa fluctuación del SGD salta a mínimos locales nuevos y potencialmente mejores. Por otro lado, esto finalmente complica la convergencia al mínimo exacto, ya que SGD seguirá excediéndose. Sin embargo, se ha demostrado que cuando disminuimos lentamente el learning rate (la tasa de aprendizaje), SGD converge a un mínimo local o global para la optimización no convexa y convexa, respectivamente.\n\nComo queda bien ilustrado en el siguiente grafico."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"response = requests.get('https://www.jeremyjordan.me/content/images/2018/02/Screen-Shot-2018-02-24-at-11.47.09-AM.png')\nimage_bytes = io.BytesIO(response.content)\n\nimg = PIL.Image.open(image_bytes)\nimg","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"No nos excederemos más en este optimizador, ya que no es tan potente para el problema que estudiamos como los otros dos optimizadores."},{"metadata":{},"cell_type":"markdown","source":"### Fine Tuning"},{"metadata":{},"cell_type":"markdown","source":"Primero comprobaremos el **learning rate**.\n\nEsta parte es muy importante, puesto que:\n\n* Si la tasa de entrenamiento es demasiado pequeña, el entrenamiento, aún siendo más confiable, requiere de más steps.\n\n* Si la tasa de entrenamiento es demasiado grande, es posible que el entrenamiento no consiga la convergencia esperada, incluso podría divergir.\n\nSi bien en el Algorithm 1, del ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION de Diederik P. Kingma y Jimmy Lei Ba, muestra que una buena configuración predeterminada para los problemas de aprendizaje automático probados es lr = 0.001, β1 = 0.9, β2 = 0.999 and E = 10−8, en este problema en concreto, parece funcionar mejor un **lr = 2e-5**.\n"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fine_tuning_lr = pd.read_csv(\"../input/fine-tuning/table tuning lr.csv\", sep = ';')\n\nfine_tuning_lr","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Y ahora ajustaremos **epsilon**, al cual definiremos en 5e-08."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fine_tuning_lr = pd.read_csv(\"../input/fine-tuning/epsilon table.csv\", sep = ';')\n\nfine_tuning_lr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with strategy.scope():\n    # Primero cargamos la capa de codificador.\n    transformer_encoder = TFAutoModel.from_pretrained(model_xml_roberta)\n\n    # Definimos los inputs tokenizados\n    input_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_ids\")\n\n    # Ahora, codificamos los inputs segun el encoder que hemos definido anteriormente.\n    sequence_output = transformer_encoder(input_ids)[0]\n\n    # Extraemos los tokens utilizados para clasificar, en este caso <s>\n    cls_token = sequence_output[:, 0, :]\n\n    # La última capa es la que pasamos a través de softmax para la aplicación del uso de probabilidades. Con 3 niveles, ya que son 3 posibles resultados (0,1 y 2)\n    out = Dense(3, activation='softmax')(cls_token)\n\n    # Construimos y compilamos el modelo.\n    model = Model(inputs=input_ids, outputs=out)\n    model.compile(\n        Adam(lr=2e-5,beta_1=0.9, beta_2=0.999, epsilon=5e-08), \n        loss='sparse_categorical_crossentropy', \n        metrics=['accuracy']\n    )\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_steps = len(x_train) // batch_size\n\ntrain_history_xml_roberta_def = model.fit(\n    train_dataset,\n    steps_per_epoch=n_steps,\n    validation_data=valid_dataset,\n    epochs=3\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# list all data in history\nprint(train_history_xml_roberta_def.history.keys())\n# summarize history for loss\nplt.plot(train_history_xml_roberta_def.history['loss'])\nplt.plot(train_history_xml_roberta_def.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()\n# summarize history for accuracy\nplt.plot(train_history_xml_roberta_def.history['accuracy'])\nplt.plot(train_history_xml_roberta_def.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Podemos comprobar que las curvas tienen un comportamiento adecuado, y obtenemos un modelo con una precisión en el conjunto de validación cercana al 90% y con una función de pérdida muy baja, lo que pronostica un buen ajuste con los datos del test."},{"metadata":{},"cell_type":"markdown","source":"# viii. Conclusiones finales\n\nSe ha conseguido alcanzar tanto el objetivo general como los específicos. \n\nAlcanzando finalmente un modelo que pronostica la relación semántica entre las dos frases cualesquiera que le proporcionemos en cualquier idioma, con una precisión de casi un noventa por ciento.\n\nY también queda evidenciado que todo esto ha sido materialmente posible gracias a la implementación de unidades de procesamiento tensorial (TPUs), sin las cuales, realizar un análisis de este tipo requería una cantidad de tiempo prohibitiva, mientras que utilizando estas unidades, cada cálculo apenas ha necesitado unos pocos minutos."},{"metadata":{"_uuid":"fb1ff888-7684-4888-b861-3c31a3f360b7","_cell_guid":"87b18b05-30f3-45f9-9c3e-6f8184934bd0","trusted":true},"cell_type":"markdown","source":"# ix. Generating & Submitting Predictions"},{"metadata":{},"cell_type":"markdown","source":"## Test en Data Science y Test en Kaggle\n\nLlegados a este punto, se requiere aclarar la diferencia entre ambos conceptos:\n\n* Test en Data Science: Se utiliza para evaluar la capacidad de un modelo candidato a la hora de analizar datos, extraer información, sugerir conclusiones y respaldar la toma de decisiones, y los datos surgen de un segmento de datos del conjunto de datos a analizar. Dichos datos nunca son conocidos por el modelo en su fase de entrenamiento, tan solo son introducidos al final de esta para comprobar la calidad del mismo.\n\n* Test en Kaggle: Se trata de un conjunto de datos, sin los labels correspondientes, proporcionados por Kaggle para comprobar la puntuación obtenida en cada competición ofrecida por la plataforma.\n"},{"metadata":{"_uuid":"9e0e34fa-1ef7-4207-a5c3-c21863c7be27","_cell_guid":"add7302f-ae26-4e78-b69d-858cacb35991","trusted":true},"cell_type":"code","source":"test_preds = model.predict(test_dataset, verbose=1)\nsubmission['prediction'] = test_preds.argmax(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1d4999aa-10d2-4a88-962a-8f10bded837b","_cell_guid":"d4fdefdc-4839-4962-ae75-6807390a6de7","trusted":true},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b8463f7e-8b2f-4c24-8eb3-a35ec02a2d6e","_cell_guid":"84abe9e3-ef04-4dac-97d2-2308a0f11313","trusted":true},"cell_type":"code","source":"submission.to_csv(\"submission.csv\", index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# x. Bibliografía"},{"metadata":{},"cell_type":"markdown","source":"* Jeff Dean, Rajat Monga: TensorFlow. Disponible en:https://www.tensorflow.org/\n\n* Towards Data Science Inc. TowardsDataScience. Disponible en:https://towardsdatascience.com/how-to-use-dataset-in-tensorflow-c758ef9e4428\n\n* Diederik P. Kingma y Jimmy Lei Ba: \"ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION\", *a conference paper at ICLR 2015*, disponible en: https://arxiv.org/pdf/1412.6980.pdf\n\n* Tarang Shah: \"About Train, Validation and Test Sets in Machine Learning\", *towards data science*, 06/12/2017. Disponible en: https://towardsdatascience.com/train-validation-and-test-sets-72cb40cba9e7\n\n* Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, Veselin Stoyanov: \"Unsupervised Cross-lingual Representation Learning at Scale\", *Cornell University*, 05/11/2019. Disponible en: https://arxiv.org/abs/1911.02116\n\n* Hugging Face, Inc.https://huggingface.co/\n\n* Fernando Pérez: Colaboratory. Disponible en: https://colab.research.google.com/\n\n* Nick Doiron: \"A whole world of BERT\", *Medium*, 30/01/2020. Disponible en: https://medium.com/@mapmeld/a-whole-world-of-bert-f20d6bd47b2f\n\n* Rick Merritt: \"BERT Does Europe: AI Language Model Learns German, Swedish\", *Blogs Nvidia*, 23/12/2019. Disponible en: https://blogs.nvidia.com/blog/2019/12/23/bert-ai-german-swedish/\n\n* xhlulu: \"Concise Keras XLM-R on TPU\", *kaggle public notebooks*, 01/08/20. Disponible en: https://www.kaggle.com/xhlulu/contradictory-watson-concise-keras-xlm-r-on-tpu\n\n* AdityaMishra: \"NLI - Data Translation Augmentation\", *kaggle public notebooks*, 01/08/20. Disponible en: https://www.kaggle.com/aditya08/nli-data-translation-augmentation\n\n* Yih-Dar SHIEH: \"More NLI datasets - Hugging Face nlp library\", *kaggle public notebooks*, 01/08/20. Disponible en: https://www.kaggle.com/yihdarshieh/more-nli-datasets-hugging-face-nlp-library\n\n* Tom Dietterich: \"Overfitting and Undercomputing in Machine Learning\", *Departament of Computer Science, Oregon State University, Corvallis*. Disponible en: https://dl.acm.org/doi/pdf/10.1145/212094.212114?casa_token=vqzbFaLf3noAAAAA%3AN7JvZ6jkm5st8nq0j5uodFET_r6Cc7ompQBa5y6HoiiOjp0_Q5R2k8gU_5lH5K19MKZWPQP4mNOVuw\n\n* Sebastien C. Wong, Adam Gatt,Victor Stamatescu and Mark D. McDonnell: \"Understanding data augmentation for classification: when to warp?\", *2016 IEEE*. Disponible en: https://arxiv.org/pdf/1609.08764.pdf\n\n* Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzman, Edouard Grave, Myle Ott, Luke Zettlemoyer y Veselin Stoyanov: \"Unsupervised Cross-lingual Representation Learning at Scale\". Disponible en: https://arxiv.org/pdf/1911.02116.pdf\n\n* Sebastian Ruder: \"An overview of gradient descent optimization algorithms\", 19/01/2016. Disponible en: https://ruder.io/optimizing-gradient-descent/"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}