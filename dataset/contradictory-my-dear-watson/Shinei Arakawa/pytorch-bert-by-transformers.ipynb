{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport datetime\nfrom tqdm import tqdm\nimport random\nimport shutil\nimport matplotlib.pyplot as plt\n\nimport numpy as np\nimport pandas as pd\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as Func\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom transformers import BertTokenizer, BertModel, BertConfig, AdamW, get_linear_schedule_with_warmup","metadata":{"execution":{"iopub.status.busy":"2021-08-06T16:24:54.21758Z","iopub.execute_input":"2021-08-06T16:24:54.217902Z","iopub.status.idle":"2021-08-06T16:24:54.223612Z","shell.execute_reply.started":"2021-08-06T16:24:54.217873Z","shell.execute_reply":"2021-08-06T16:24:54.222463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Set hyporparameters.","metadata":{}},{"cell_type":"code","source":"RANDOM_SEED = None\nNUM_OF_CLASSES = 3\nMAX_LENGTH = 192\nTO_USE_POOLING_OUTPUT = False\nRAITO_OF_EVAL_DATA = 0.1\n\nMAX_EPOCH = 5\nTRAIN_BATCH_SIZE = 16\nEVAL_BATCH_SIZE = 16\nTEST_BATCH_SIZE = 16\n\nLEARNING_RATE = 8e-6\nWEIGHT_DECAY = 0.001\nDROPOUT_RATE = 0.3\nTO_USE_CLIP_GRAD = True","metadata":{"execution":{"iopub.status.busy":"2021-08-06T16:24:54.22547Z","iopub.execute_input":"2021-08-06T16:24:54.226055Z","iopub.status.idle":"2021-08-06T16:24:54.234332Z","shell.execute_reply.started":"2021-08-06T16:24:54.226017Z","shell.execute_reply":"2021-08-06T16:24:54.233605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Built preproccess unit.","metadata":{}},{"cell_type":"code","source":"class Preprocessor:\n    \"\"\"\n    preprocessing unit\n    \"\"\"\n    INDEX = \"id\"\n    PREMISE = \"premise\"\n    HYPOTHESIS = \"hypothesis\"\n    LANG_AVB = \"lang_avb\"\n    LANGUAGE = \"language\"\n    LAVEL = \"label\"\n\n    def __init__(self, trainDataPath='../input/contradictory-my-dear-watson/train.csv', testDataPath='../input/contradictory-my-dear-watson/test.csv', randomSeed=RANDOM_SEED):\n        self.trainDataPath = trainDataPath\n        self.testDataPath = testDataPath\n        self.csvTrainData = None\n        self.csvTestData = None\n        self.random = random\n        if randomSeed is not None:\n            self.random.seed(randomSeed)\n\n    def prepareTrainAndEvalData(self):\n        data = []\n        self.loadTrainData()\n        for index, item in self.csvTrainData.iterrows():\n            texts = []\n            texts.append(self.cleanTexts(item[self.PREMISE]))\n            texts.append(self.cleanTexts(item[self.HYPOTHESIS]))\n            texts.append(item[self.LAVEL])\n            data.append(texts)\n        \n        lenTrainData = int(len(data) * (1 - RAITO_OF_EVAL_DATA))\n        self.random.shuffle(data)\n        trainData = data[:lenTrainData]\n        evalData = data[lenTrainData:]\n\n        return trainData, evalData\n\n    def prepareTestData(self):\n        data = []\n        self.loadTestData()\n        for index, item in self.csvTestData.iterrows():\n            texts = []\n            texts.append(item[self.INDEX])\n            texts.append(self.cleanTexts(item[self.PREMISE]))\n            texts.append(self.cleanTexts(item[self.HYPOTHESIS]))\n            data.append(texts)\n        return data\n\n    def loadTrainData(self):\n        with open(self.trainDataPath) as file:\n            self.csvTrainData = pd.read_csv(file, header=0)\n    \n    def loadTestData(self):\n        with open(self.testDataPath) as file:\n            self.csvTestData = pd.read_csv(file, header=0)\n    \n    # If you want to do additional preproccessings, write them in this unit. \n    def cleanTexts(self, text):\n        text = text.replace('\\t', ' ')\n        \n        #for puctuation in string.punctuation:\n        #    if (puctuation == '.') or (puctuation == ','):\n        #        continue\n        #    else:\n        #        text = text.replace(puctuation, ' ')\n\n        return text","metadata":{"execution":{"iopub.status.busy":"2021-08-06T16:24:54.236953Z","iopub.execute_input":"2021-08-06T16:24:54.237386Z","iopub.status.idle":"2021-08-06T16:24:54.252701Z","shell.execute_reply.started":"2021-08-06T16:24:54.237347Z","shell.execute_reply":"2021-08-06T16:24:54.251815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Built dataloaders.","metadata":{}},{"cell_type":"code","source":"# For train data\nclass TrainDataSet(Dataset):\n    def __init__(self, trainData, tokenizer):\n        super(TrainDataSet, self).__init__()\n        self.tokenizer = tokenizer\n        self.trainData = trainData\n\n    def __len__(self):\n        return len(self.trainData)\n\n    def __getitem__(self, index):\n        return self.getData(index)\n\n    def encodeToTokenIds(self, text):\n        tokens = self.tokenizer.tokenize(text)\n        tokens.append(self.tokenizer.sep_token)\n        return self.tokenizer.convert_tokens_to_ids(tokens=tokens)\n\n    def getData(self, index):\n        outputData = []\n        premiseTokenIds = self.encodeToTokenIds(' '.join(self.trainData[index][0].split()))\n        hypothesisTokenIds = self.encodeToTokenIds(' '.join(self.trainData[index][1].split()))\n        labelIndex = torch.tensor(self.trainData[index][2], dtype=torch.long)\n        \n        inputs = self.tokenizer.encode_plus(     \n            premiseTokenIds,\n            hypothesisTokenIds,\n            add_special_tokens=True,\n            max_length=MAX_LENGTH,\n            pad_to_max_length=True,\n            truncation=True\n        )\n\n        outputData.append(torch.tensor(inputs['input_ids'], dtype=torch.long))\n        outputData.append(torch.tensor(inputs['token_type_ids'], dtype=torch.long))\n        outputData.append(torch.tensor(inputs['attention_mask'], dtype=torch.long))\n\n        return outputData, labelIndex\n\n    def getTokenizer(self):\n        return self.tokenizer\n\n# For eval data\nclass EvalDataSet(Dataset):\n    def __init__(self, evalData, tokenizer):\n        super(EvalDataSet, self).__init__()\n        self.tokenizer = tokenizer\n        self.evalData = evalData\n\n    def __len__(self):\n        return len(self.evalData)\n\n    def __getitem__(self, index):\n        return self.getData(index=index)\n\n    def encodeToTokenIds(self, text):\n        tokens = self.tokenizer.tokenize(text)\n        tokens.append(self.tokenizer.sep_token)\n        return self.tokenizer.convert_tokens_to_ids(tokens=tokens)\n\n    def getData(self, index):\n        outputData = []\n        premiseTokenIds = self.encodeToTokenIds(' '.join(self.evalData[index][0].split()))\n        hypothesisTokenIds = self.encodeToTokenIds(' '.join(self.evalData[index][1].split()))\n        labelIndex = torch.tensor(self.evalData[index][2], dtype=torch.long)\n        \n        inputs = self.tokenizer.encode_plus(     \n            premiseTokenIds,\n            hypothesisTokenIds,\n            add_special_tokens=True,\n            max_length=MAX_LENGTH,\n            pad_to_max_length=True,\n            truncation=True\n        )\n\n        outputData.append(torch.tensor(inputs['input_ids'], dtype=torch.long))\n        outputData.append(torch.tensor(inputs['token_type_ids'], dtype=torch.long))\n        outputData.append(torch.tensor(inputs['attention_mask'], dtype=torch.long))\n\n        return outputData, labelIndex\n\n# For test data\nclass TestDataSet(Dataset):\n    def __init__(self, testData, tokenizer):\n        super(TestDataSet, self).__init__()\n        self.tokenizer = tokenizer\n        self.testData = testData\n        self.labels = []\n\n    def __len__(self):\n        return len(self.testData)\n\n    def __getitem__(self, index):\n        return self.getData(index)\n\n    def encodeToTokenIds(self, text):\n        tokens = self.tokenizer.tokenize(text)\n        tokens.append(self.tokenizer.sep_token)\n        return self.tokenizer.convert_tokens_to_ids(tokens=tokens)\n\n    def getData(self, index):\n        outputData = []\n        self.labels.append(self.testData[index][0])\n        premiseTokenIds = self.encodeToTokenIds(' '.join(self.testData[index][1].split()))\n        hypothesisTokenIds = self.encodeToTokenIds(' '.join(self.testData[index][2].split()))\n        \n        inputs = self.tokenizer.encode_plus(     \n            premiseTokenIds,\n            hypothesisTokenIds,\n            add_special_tokens=True,\n            max_length=MAX_LENGTH,\n            pad_to_max_length=True,\n            truncation=True\n        )\n\n        outputData.append(torch.tensor(inputs['input_ids'], dtype=torch.long))\n        outputData.append(torch.tensor(inputs['token_type_ids'], dtype=torch.long))\n        outputData.append(torch.tensor(inputs['attention_mask'], dtype=torch.long))\n\n        return outputData\n\n# This method provides train and eval dataloaders. \nclass BertDataLoader:\n    def __init__(self):\n        self.tokenizer = BertTokenizer.from_pretrained('../input/bert-base-multilingual-cased/bert-base-multilingual-cased', local_files_only=True)\n        self.preprocessor = Preprocessor()\n        \n    def getTrainAndEvalDataLoader(self):\n        trainData, evalData = self.preprocessor.prepareTrainAndEvalData()\n\n        trainDataSet = TrainDataSet(trainData=trainData, tokenizer=self.tokenizer)\n        evalDataSet = EvalDataSet(evalData=evalData, tokenizer=self.tokenizer)\n        \n        trainDataLoader = DataLoader(trainDataSet, batch_size=TRAIN_BATCH_SIZE, shuffle=True, drop_last=True)\n        evalDataLoader = DataLoader(evalDataSet, batch_size=EVAL_BATCH_SIZE, shuffle=True)\n\n        dataLoadersDict = {'train': trainDataLoader, 'eval': evalDataLoader}\n        return dataLoadersDict","metadata":{"execution":{"iopub.status.busy":"2021-08-06T16:24:54.254324Z","iopub.execute_input":"2021-08-06T16:24:54.2547Z","iopub.status.idle":"2021-08-06T16:24:54.305957Z","shell.execute_reply.started":"2021-08-06T16:24:54.254664Z","shell.execute_reply":"2021-08-06T16:24:54.305105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make dataloaders.\ndataLoader = BertDataLoader()\ndataLoadersDict = dataLoader.getTrainAndEvalDataLoader()","metadata":{"execution":{"iopub.status.busy":"2021-08-06T16:24:54.307318Z","iopub.execute_input":"2021-08-06T16:24:54.307714Z","iopub.status.idle":"2021-08-06T16:24:55.508927Z","shell.execute_reply.started":"2021-08-06T16:24:54.307677Z","shell.execute_reply":"2021-08-06T16:24:55.508063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Define model.","metadata":{}},{"cell_type":"code","source":"class Model(nn.ModuleList):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.bertModel = SingleBERT(toUsePooling=TO_USE_POOLING_OUTPUT)\n\n        bertConfig = self.bertModel.getConfig()\n        hiddenSize = bertConfig.hidden_size\n        \n        self.classifier = Classifier(hiddenSize=hiddenSize, nClasses=NUM_OF_CLASSES, dropoutRate=DROPOUT_RATE)\n\n    def forward(self, input, tokenTypeIds, attentionMask):\n        output = self.bertModel(input=input, tokenTypeIds=tokenTypeIds, attentionMask=attentionMask)\n        output = self.classifier(output)\n        return output \n\nclass SingleBERT(nn.Module):\n    def __init__(self, toUsePooling=False):\n        super(SingleBERT, self).__init__()\n        self.toUsePooling = toUsePooling\n        self.bertConfig = BertConfig.from_pretrained('../input/bert-base-multilingual-cased/bert-base-multilingual-cased', local_files_only=True)\n        self.bertModel = BertModel.from_pretrained('../input/bert-base-multilingual-cased/bert-base-multilingual-cased', local_files_only=True)\n\n        for param in self.bertModel.parameters():\n            param.requires_grad = True\n\n    def forward(self, input, tokenTypeIds, attentionMask):\n        lastLayerOutPut, poolingOutput = self.bertModel(input, attention_mask=attentionMask, token_type_ids=tokenTypeIds, return_dict=False)\n        if self.toUsePooling:\n            return poolingOutput\n        return lastLayerOutPut[:, 0, :]\n\n    def getConfig(self):\n        return self.bertConfig\n\nclass Classifier(nn.Module):\n    def __init__(self, hiddenSize, nClasses, dropoutRate):\n        super(Classifier, self).__init__()\n        self.dropout1 = nn.Dropout(p=dropoutRate)\n        self.linear1 = nn.Linear(in_features=hiddenSize, out_features=hiddenSize)\n        self.batchNorm = nn.BatchNorm1d(num_features=hiddenSize, eps=1e-05, momentum=0.1, affine=False)\n        self.activation = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n        self.dropout2 = nn.Dropout(p=dropoutRate)\n        self.linear2 = nn.Linear(in_features=hiddenSize, out_features=nClasses)\n\n        nn.init.normal_(self.linear1.weight, std=0.04)\n        nn.init.normal_(self.linear2.weight, mean=0.5, std=0.04)\n        nn.init.normal_(self.linear1.bias, 0)\n        nn.init.normal_(self.linear2.bias, 0)\n\n    def forward(self, input):\n        output = self.dropout1(input)\n        output = self.linear1(output)\n        output = self.batchNorm(output)\n        output = self.activation(output)\n        output = self.dropout2(output)\n        output = self.linear2(output)\n        return output","metadata":{"execution":{"iopub.status.busy":"2021-08-06T16:24:55.510909Z","iopub.execute_input":"2021-08-06T16:24:55.511258Z","iopub.status.idle":"2021-08-06T16:24:55.526513Z","shell.execute_reply.started":"2021-08-06T16:24:55.511209Z","shell.execute_reply":"2021-08-06T16:24:55.525571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Model()","metadata":{"execution":{"iopub.status.busy":"2021-08-06T16:24:55.528457Z","iopub.execute_input":"2021-08-06T16:24:55.528856Z","iopub.status.idle":"2021-08-06T16:24:59.779008Z","shell.execute_reply.started":"2021-08-06T16:24:55.528817Z","shell.execute_reply":"2021-08-06T16:24:59.778166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Define loss function. I made the coding using classes in consideration of extensibility.","metadata":{}},{"cell_type":"code","source":"class LossFunction(nn.Module):\n    def __init__(self):\n        super(LossFunction, self).__init__()\n        self.loss = nn.CrossEntropyLoss()\n\n    def forward(self, inputTensor, lavelTensor):\n        return self.loss(inputTensor, lavelTensor)","metadata":{"execution":{"iopub.status.busy":"2021-08-06T16:24:59.780287Z","iopub.execute_input":"2021-08-06T16:24:59.780626Z","iopub.status.idle":"2021-08-06T16:24:59.787151Z","shell.execute_reply.started":"2021-08-06T16:24:59.780592Z","shell.execute_reply":"2021-08-06T16:24:59.786129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"criterion = LossFunction()","metadata":{"execution":{"iopub.status.busy":"2021-08-06T16:24:59.788415Z","iopub.execute_input":"2021-08-06T16:24:59.788816Z","iopub.status.idle":"2021-08-06T16:24:59.796411Z","shell.execute_reply.started":"2021-08-06T16:24:59.788778Z","shell.execute_reply":"2021-08-06T16:24:59.79557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Define train function.\nAlso, define some functions to record progress.","metadata":{}},{"cell_type":"code","source":"# To save model's weights\ndef saveWeights(model):\n    saveDirectoryPath = './weight'\n    if not os.path.exists(saveDirectoryPath):\n        os.makedirs(saveDirectoryPath)\n\n    time_now = datetime.datetime.now()\n    time_info = f'{time_now.year}-{time_now.month}-{time_now.day}_{time_now.hour}-{time_now.minute}-{time_now.second}'\n    savePath = saveDirectoryPath +'/'+ str(time_info) + '.pth'\n\n    try:\n        torch.save(model.state_dict(), savePath)\n        print('Parameters were successfully saved!')\n    except:\n        print('Parameters were not successfully saved!')\n    \n    return None\n\n# For drawing the loss and accuracy rate.\ndef saveLogs(logs):\n    time_now = datetime.datetime.now()\n    time_info = f'{time_now.year}-{time_now.month}-{time_now.day}_{time_now.hour}-{time_now.minute}-{time_now.second}'\n\n    saveDirectoryPath = './logs/' + str(time_info)\n    if not os.path.exists(saveDirectoryPath):\n        os.makedirs(saveDirectoryPath)\n\n    savePathLoss = saveDirectoryPath  + '/loss' + '.jpg'\n    savePathAcurracy = saveDirectoryPath  + '/accuracy' + '.jpg'\n\n    x = [num for num in range(MAX_EPOCH)]\n    epochTrainLosses = logs[0].tolist()\n    epochEvalLosses = logs[1].tolist()\n    epochTrainAccuracies = logs[2].tolist()\n    epochEvalAccuracies = logs[3].tolist()\n\n    # Loss\n    plt.plot(x, epochTrainLosses, color='red', label='Train Loss')\n    plt.plot(x, epochEvalLosses, color='blue', label='Eval Loss')\n\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend(loc='upper right')\n    plt.grid()\n    \n\n    plt.savefig(savePathLoss)\n    plt.clf()\n\n    # Accuracy\n    plt.plot(x, epochTrainAccuracies, color='red', label='Train Accuracy')\n    plt.plot(x, epochEvalAccuracies, color='blue', label='Eval Accuracy')\n\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.legend(loc='lower right')\n    plt.grid()\n \n    plt.savefig(savePathAcurracy)\n    \n    return None\n\n# Check available device.\ndef checkDevice():\n    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n    print(\"Device: \", device)\n    return device\n\ndef trainModel(model, dataLoadedrsDict, criterion, optimizer):\n    device = checkDevice()\n    model.to(device)\n    criterion.to(device)\n    torch.backends.cudnn.benchmark = True\n\n    total_steps = len(dataLoadedrsDict['train'].dataset) * MAX_EPOCH\n    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps = total_steps)\n    \n    # For progress record.\n    trainLossLogs = np.zeros(shape=MAX_EPOCH, dtype=np.float)\n    evalLossLogs = np.zeros(shape=MAX_EPOCH, dtype=np.float)\n    trainAccuracyLogs = np.zeros(shape=MAX_EPOCH, dtype=np.float)\n    evalAccuracyLogs = np.zeros(shape=MAX_EPOCH, dtype=np.float)\n    \n    # Core part of training\n    for epoch in range(MAX_EPOCH):\n        for phase in ['train', 'eval']:\n            print('-------------------------------------------------------------------------------------------------------------------------------------')\n            print(\"Phase: \", phase)\n\n            if phase == 'train':\n                model.train()\n            else:\n                model.eval()\n\n            epochLoss = 0.0\n            epochCorrects = 0\n            \n            for batch in tqdm(dataLoadedrsDict[phase]):\n                inputs = batch[0][0].to(device)\n                tokenTypeIds = batch[0][1].to(device)\n                attentionMask = batch[0][2].to(device)\n                labels = batch[1].to(device)\n\n                with torch.set_grad_enabled(phase == \"train\"):\n                    outputs = model(input=inputs, tokenTypeIds=tokenTypeIds, attentionMask=attentionMask)\n                    loss = criterion(outputs, labels)\n                    _, predictions = torch.max(outputs, dim=1)\n\n                    if phase == 'train':\n                        optimizer.zero_grad()\n                        loss.backward()\n                        if TO_USE_CLIP_GRAD:\n                            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n                        optimizer.step()\n                        scheduler.step()\n                        optimizer.zero_grad()\n                    \n                    epochLoss += loss.item() * TRAIN_BATCH_SIZE\n                    epochCorrects += torch.sum(predictions == labels, dim=0)\n\n            epochLoss = epochLoss / len(dataLoadedrsDict[phase].dataset)\n            epochAccuracy = epochCorrects.double() / len(dataLoadedrsDict[phase].dataset)\n\n            if phase == 'train':\n                trainLossLogs[epoch] = epochLoss\n                trainAccuracyLogs[epoch] = epochAccuracy\n            else:\n                evalLossLogs[epoch] = epochLoss\n                evalAccuracyLogs[epoch] = epochAccuracy\n\n            print('Epoch: {}/{}  |  Loss: {:.4f}  |  Acc: {:.4f}'.format(epoch+1, MAX_EPOCH, epochLoss, epochAccuracy))\n\n    saveWeights(model=model)\n    logs = [trainLossLogs, evalLossLogs, trainAccuracyLogs, evalAccuracyLogs]\n    saveLogs(logs=logs)\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-08-06T16:24:59.799233Z","iopub.execute_input":"2021-08-06T16:24:59.800081Z","iopub.status.idle":"2021-08-06T16:24:59.823734Z","shell.execute_reply.started":"2021-08-06T16:24:59.80004Z","shell.execute_reply":"2021-08-06T16:24:59.822875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Define optimizer. ","metadata":{}},{"cell_type":"code","source":"optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY, correct_bias=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-06T16:24:59.825149Z","iopub.execute_input":"2021-08-06T16:24:59.825575Z","iopub.status.idle":"2021-08-06T16:24:59.840053Z","shell.execute_reply.started":"2021-08-06T16:24:59.825535Z","shell.execute_reply":"2021-08-06T16:24:59.83927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Train the bert model.","metadata":{"execution":{"iopub.status.busy":"2021-06-28T10:52:53.856022Z","iopub.execute_input":"2021-06-28T10:52:53.856452Z","iopub.status.idle":"2021-06-28T10:52:53.869732Z","shell.execute_reply.started":"2021-06-28T10:52:53.85642Z","shell.execute_reply":"2021-06-28T10:52:53.868334Z"}}},{"cell_type":"code","source":"trainedModel = trainModel(model=model, dataLoadedrsDict=dataLoadersDict, criterion=criterion, optimizer=optimizer)","metadata":{"execution":{"iopub.status.busy":"2021-08-06T16:24:59.841422Z","iopub.execute_input":"2021-08-06T16:24:59.841796Z","iopub.status.idle":"2021-08-06T16:25:34.651831Z","shell.execute_reply.started":"2021-08-06T16:24:59.841768Z","shell.execute_reply":"2021-08-06T16:25:34.649853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The followings are functions to create a file for submission using the parameters of the trained model.","metadata":{}},{"cell_type":"code","source":"# A function for making inferences using learned parameters.\ndef predict(model, testDataLoader):\n    predictions = []\n    device = checkDevice()\n    model.to(device)\n\n    model.eval()\n\n    for batch in tqdm(testDataLoader):\n        inputs = batch[0].to(device)\n        tokenTypeIds = batch[1].to(device)\n        attentionMask = batch[2].to(device)\n\n        outputs = model(input=inputs, tokenTypeIds=tokenTypeIds, attentionMask=attentionMask)\n        _, prediction = torch.max(outputs, dim=1)\n        prediction = prediction.flatten().tolist()\n        predictions += prediction\n    \n    return predictions\n\n# functions for creating a submission file.\nclass Submitter:\n    def __init__(self, dataSet):\n        self.dataLoader = DataLoader(dataSet, batch_size=TEST_BATCH_SIZE, shuffle=False)\n        self.ids = dataSet.labels\n\n    def makeFile(self, model, weightPath):\n        if weightPath is not None:\n            weights = torch.load(weightPath, map_location={'cuda:0': 'cpu'})\n            model.load_state_dict(weights)\n\n\n        outputLabels = predict(model=model, testDataLoader=self.dataLoader)\n\n        saveDirectoryPath = './'\n        if not os.path.exists(saveDirectoryPath):\n            os.makedirs(saveDirectoryPath)\n\n        savePath =  'submission.csv'\n\n        print(len(self.ids))\n        print(len(outputLabels))\n        dataFrame = pd.DataFrame(list(zip(self.ids, outputLabels)), columns=['id', 'prediction'])\n\n        try:\n            dataFrame.to_csv(savePath, index=False)\n            print(\"Successed !!\")\n        except FileNotFoundError:\n            print(\"Failed !!\")\n","metadata":{"execution":{"iopub.status.busy":"2021-08-06T16:25:34.653037Z","iopub.status.idle":"2021-08-06T16:25:34.653743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Prepare dataset for test. \ntestData = Preprocessor().prepareTestData()\ntokenizer = BertTokenizer.from_pretrained('../input/bert-base-multilingual-cased/bert-base-multilingual-cased', local_files_only=True)\ndataSet = TestDataSet(testData=testData, tokenizer=tokenizer)\n\n# make a submittion file\nsubmitter = Submitter(dataSet=dataSet)\nsubmitter.makeFile(model=trainedModel, weightPath=None)","metadata":{"execution":{"iopub.status.busy":"2021-08-06T16:25:34.655042Z","iopub.status.idle":"2021-08-06T16:25:34.655821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}}]}