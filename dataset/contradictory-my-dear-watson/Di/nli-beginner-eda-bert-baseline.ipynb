{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport seaborn as sns\nfrom transformers import AutoTokenizer, BertTokenizer, TFAutoModel, TFBertModel\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom sklearn.model_selection import StratifiedKFold,train_test_split\nimport os\n\nos.environ[\"WANDB_API_KEY\"] = \"0\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Overview and EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.read_csv('../input/contradictory-my-dear-watson/train.csv')\ndf_test = pd.read_csv('../input/contradictory-my-dear-watson/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No null values in both train and test data"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"train_language_cnt = df_train['language'].value_counts(sort=False)\ntest_language_cnt = df_test['language'].value_counts(sort=False)[train_language_cnt.index]\nfig,axes = plt.subplots(nrows=1,ncols=2,figsize=(15,8),sharey=True)\nfor i,cnt in enumerate([train_language_cnt,test_language_cnt]):\n    axes[i].barh(cnt.index,cnt.values,\n            height=0.5, edgecolor='darkgrey',\n            color=sns.color_palette()[i],alpha=0.7)\n    axes[i].set_yticklabels(cnt.index,fontsize=15)\n    axes[i].set_xticks([])\n\n    for j in axes[i].patches:\n        axes[i].text(j.get_width()+.5, j.get_y()+.2,\n                str(j.get_width()), \n                fontsize=13, color='black')\n\n    for pos in ['left','right','bottom','top']:\n        axes[i].spines[pos].set_color(None)\n\n    #axes[i].grid(axis='x',linestyle='-',alpha=0.4)\n    #axes[i].invert_yaxis()\nfig.text(0.13, 0.95, 'Train vs Test: Language Count', fontsize=15, fontweight='bold') ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"English take the dominant place in both train and test data."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"label_cnt = df_train['label'].value_counts()\ntrace = go.Pie(labels = label_cnt.index, \n               values = label_cnt.values,\n               hoverinfo = 'percent+value+label',\n               textinfo = 'percent',\n               textposition = 'inside',\n               textfont = dict(size=14),\n               title = 'Label',\n               titlefont = dict(size=15),\n               hole = 0.3,\n               showlegend = True,\n               marker = dict(line=dict(color='black',width=2)))\nfig = go.Figure(data=[trace])\nfig.update_layout(height=500, width=500)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig = make_subplots(rows=5, cols=3, specs=[[{\"type\": \"domain\"}]*3]*5,\n                    horizontal_spacing=0.005,vertical_spacing=0.01)\nfor i,lang in enumerate(list(train_language_cnt.index)): \n    label_cnt = df_train[df_train.language == lang]['label'].value_counts()\n    fig.add_trace(go.Pie(labels = label_cnt.index, \n                   values = label_cnt.values,\n                   hoverinfo = 'percent+value+label',\n                   textinfo = 'percent',\n                   textposition = 'inside',\n                   textfont = dict(size=14),\n                   title = lang,\n                   titlefont = dict(size=15),\n                   hole = 0.5,\n                   showlegend = True,\n                   marker = dict(line=dict(color='black',width=2))),\n                   row=divmod(i,3)[0]+1,col=divmod(i,3)[1]+1)\nfig.update_layout(height=750, width=750)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The distribution of train labels is quite balanced on the whole and in differnt languages."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df_train['premise_word_cnt'] = df_train['premise'].apply(lambda x: len(x.split()))\ndf_train['hypothesis_word_cnt'] = df_train['hypothesis'].apply(lambda x: len(x.split()))\n\nfig,ax = plt.subplots(nrows=5,ncols=3,figsize=(15,20))\nfor i,lang in enumerate(list(train_language_cnt.index)):\n    sns.distplot(df_train[df_train.language == lang]['premise_word_cnt'],bins=20,\n                 color='red',label='Premise',kde=False, ax=ax[divmod(i,3)[0],divmod(i,3)[1]])\n    sns.distplot(df_train[df_train.language == lang]['hypothesis_word_cnt'],bins=20,\n                 color='blue',label='Hypothesis',kde=False, ax=ax[divmod(i,3)[0],divmod(i,3)[1]])\n    ax[divmod(i,3)[0],divmod(i,3)[1]].set_title(lang,fontsize=13)\n    ax[divmod(i,3)[0],divmod(i,3)[1]].legend()\n    ax[divmod(i,3)[0],divmod(i,3)[1]].set_xlabel('')\nfig.text(0.13, 0.95, 'Train: Premise & Hypothesis Word Count', fontsize=15, fontweight='bold') ","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df_test['premise_word_cnt'] = df_test['premise'].apply(lambda x: len(x.split()))\ndf_test['hypothesis_word_cnt'] = df_test['hypothesis'].apply(lambda x: len(x.split()))\n\nfig,ax = plt.subplots(nrows=5,ncols=3,figsize=(15,20))\nfor i,lang in enumerate(list(train_language_cnt.index)):\n    sns.distplot(df_test[df_test.language == lang]['premise_word_cnt'],bins=20,\n                 color='yellow',label='Premise',kde=False, ax=ax[divmod(i,3)[0],divmod(i,3)[1]])\n    sns.distplot(df_test[df_test.language == lang]['hypothesis_word_cnt'],bins=20,\n                 color='green',label='Hypothesis',kde=False, ax=ax[divmod(i,3)[0],divmod(i,3)[1]])\n    ax[divmod(i,3)[0],divmod(i,3)[1]].set_title(lang,fontsize=13)\n    ax[divmod(i,3)[0],divmod(i,3)[1]].legend()\n    ax[divmod(i,3)[0],divmod(i,3)[1]].set_xlabel('')\nfig.text(0.13, 0.95, 'Test: Premise & Hypothesis Word Count', fontsize=15, fontweight='bold')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In both training and test data, word count of hypothesis is less than that of premise for all the languages."},{"metadata":{},"cell_type":"markdown","source":"The max_len for sentences before encoding is around 60 so the in encoding phase, sequence length should be more than this value."},{"metadata":{},"cell_type":"markdown","source":"# Encode Input For Bert Base"},{"metadata":{},"cell_type":"markdown","source":"Thanks to the encoding method for Bert base in toturial [notebook](https://www.kaggle.com/anasofiauzsoy/tutorial-notebook)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def encode_sentence(s):\n    tokens = list(tokenizer.tokenize(s))\n    tokens.append('[SEP]')\n    return tokenizer.convert_tokens_to_ids(tokens)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def bert_encode(premise, hypothesis, tokenizer):\n    \n    sentence1 = tf.ragged.constant([encode_sentence(s) for s in np.array(premise)])\n    sentence2 = tf.ragged.constant([encode_sentence(s) for s in np.array(hypothesis)])\n    \n    cls = [tokenizer.convert_tokens_to_ids(['[CLS]'])]*(sentence1.shape[0])\n    input_word_ids = tf.concat([cls, sentence1, sentence2], axis=-1)\n\n    input_masks = tf.ones_like(input_word_ids).to_tensor()\n\n    type_cls = tf.zeros_like(cls)\n    type_s1 = tf.zeros_like(sentence1)\n    type_s2 = tf.ones_like(sentence2)\n    input_type_ids = tf.concat([type_cls, type_s1, type_s2], axis=-1).to_tensor()\n\n    inputs = {\n      'input_word_ids': input_word_ids.to_tensor(),\n      'input_masks': input_masks,\n      'input_type_ids': input_type_ids}\n    \n    return inputs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\ntrain_input = bert_encode(df_train.premise.values, df_train.hypothesis.values, tokenizer)\n#train_label = tf.keras.utils.to_categorical(df_train.label.values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Build Bert Baseline"},{"metadata":{"trusted":true},"cell_type":"code","source":"K.clear_session()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_bert_baseline(max_len=80):\n    bert_layer = TFBertModel.from_pretrained('bert-base-multilingual-cased')\n    input_word_ids = tf.keras.Input(shape = (max_len,),dtype =tf.int32, name='input_word_ids')\n    input_masks = tf.keras.Input(shape = (max_len,),dtype =tf.int32, name='input_masks')\n    input_type_ids = tf.keras.Input(shape = (max_len,),dtype =tf.int32, name='input_type_ids')\n    \n    sequence_output = bert_layer([input_word_ids, input_masks, input_type_ids])[0]\n    #sequence_output = clf_output[0]\n    output = tf.keras.layers.Dense(3, activation='softmax')(sequence_output[:,0,:])\n    \n    model = tf.keras.Model(inputs=[input_word_ids,input_masks,input_type_ids],outputs=output)\n    model.compile(tf.keras.optimizers.Adam(lr=3e-5),\n                  loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n    print(model.summary())\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = build_bert_baseline(max_len=80)\n#for layer in model.layers:\n    #print(layer.output_shape)\n    \nfilepath='best_weight.hdf5'\ncheckpoint = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=1, \n                                                save_best_only=True, save_weights_only=True, \n                                                mode='min',save_freq = 'epoch')\n#model.fit(train_input, df_train.label.values, epochs =3, \n#          batch_size = 16, callbacks=[checkpoint], validation_split=0.2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Build Bert with LSTM Head and Concat Pooling Result"},{"metadata":{},"cell_type":"markdown","source":"Model idea from [here](https://keras.io/examples/nlp/semantic_similarity_with_bert/)"},{"metadata":{"trusted":true},"cell_type":"code","source":"K.clear_session()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_bert_LSTM_head(max_len=80):\n    bert_layer = TFBertModel.from_pretrained('bert-base-multilingual-cased')\n    input_word_ids = tf.keras.Input(shape = (max_len,),dtype =tf.int32, name='input_word_ids')\n    input_masks = tf.keras.Input(shape = (max_len,),dtype =tf.int32, name='input_masks')\n    input_type_ids = tf.keras.Input(shape = (max_len,),dtype =tf.int32, name='input_type_ids')\n    \n    sequence_output = bert_layer([input_word_ids, input_masks, input_type_ids])[0]\n    bi_lstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True))(sequence_output)\n    avg_pool = tf.keras.layers.GlobalAveragePooling1D()(bi_lstm)\n    max_pool = tf.keras.layers.GlobalMaxPooling1D()(bi_lstm)\n    concat = tf.keras.layers.concatenate([avg_pool, max_pool])\n    dropout = tf.keras.layers.Dropout(0.3)(concat)\n    output = tf.keras.layers.Dense(3, activation=\"softmax\")(dropout)\n     \n    model = tf.keras.Model(inputs=[input_word_ids,input_masks,input_type_ids],outputs=output)\n    model.compile(tf.keras.optimizers.Adam(lr=3e-5),\n                  loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n    print(model.summary())\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bert_lstm_model = build_bert_LSTM_head(max_len=80)\n#for layer in model.layers:\n    #print(layer.output_shape)\n    \nfilepath='LSTM_best_weight.hdf5'\ncheckpoint = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=1, \n                                                save_best_only=True, save_weights_only=True, \n                                                mode='min',save_freq = 'epoch')\n#bert_lstm_model.fit(train_input, df_train.label.values, epochs =2, \n#          batch_size = 16, callbacks=[checkpoint], validation_split=0.2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The model performance did not improve after increasing complexity compared with baseline Bert model. Hence, There are two choices for me to boosting my results. The first is to do some translation augmentation and train the model again. The second is to switch to roBerta model."},{"metadata":{},"cell_type":"markdown","source":"# XLM-roBerta-Large"},{"metadata":{},"cell_type":"markdown","source":"Thanks to the [notebook](https://www.kaggle.com/xhlulu/contradictory-watson-concise-keras-xlm-r-on-tpu) as a complete guidance on TPU configuration and XLM-roBerta-Large fine tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"K.clear_session()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Initialize TPU"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Detect hardware, return appropriate distribution strategy\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n    # set: this is always the case on Kaggle.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def roberta_encode(text,tokenizer,max_len=80):\n    encoded_text = tokenizer.batch_encode_plus(text,\n                                               pad_to_max_length=True,\n                                               max_length=max_len)\n    return encoded_text","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained('jplu/tf-xlm-roberta-large')\n\ntrain_text = df_train[['premise', 'hypothesis']].values.tolist()\nencoded_train_text = roberta_encode(train_text,tokenizer,max_len=80)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"x_train, x_valid, y_train, y_valid = train_test_split(\n    encoded_train_text['input_ids'], \n    df_train.label.values, \n    test_size=0.2, random_state=98)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_SIZE = 16 * strategy.num_replicas_in_sync\nAUTO = tf.data.experimental.AUTOTUNE","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_tf_dataset(X, y,val,batch_size= BATCH_SIZE,auto=AUTO):\n    \n    dataset = tf.data.Dataset.from_tensor_slices((X,y)).shuffle(98)\n          \n    if not val:\n        dataset = dataset.repeat().batch(batch_size).prefetch(auto)\n    else:\n        dataset = dataset.batch(batch_size).prefetch(auto)\n\n    return dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf_train_dataset = create_tf_dataset(x_train,y_train,val=False)\ntf_val_dataset = create_tf_dataset(x_valid,y_valid,val=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf_train_dataset,tf_val_dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_roberta(max_len=80):\n    roberta_layer = TFAutoModel.from_pretrained('jplu/tf-xlm-roberta-large')\n    input_ids = tf.keras.Input(shape = (max_len,),dtype =tf.int32, name='input_ids')\n    \n    sequence_output = roberta_layer(input_ids)[0]\n    #sequence_output = clf_output[0]\n    output = tf.keras.layers.Dense(3, activation='softmax')(sequence_output[:,0,:])\n    \n    model = tf.keras.Model(inputs=input_ids, outputs=output)\n    model.compile(tf.keras.optimizers.Adam(lr=1e-5),\n                  loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n    print(model.summary())\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with strategy.scope():    \n    roberta_model = build_roberta(max_len=80)\n    #for layer in model.layers:\n        #print(layer.output_shape)\n\n    filepath='roberta_best_weight.hdf5'\n    checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=1, \n                                                    save_best_only=True, save_weights_only=True, \n                                                    mode='min',save_freq = 'epoch')\n    #roberta_model.fit(tf_train_dataset, epochs =10, \n              #batch_size = 16, callbacks=[checkpoint], validation_data=tf_val_dataset)\n    n_steps = len(x_train)//BATCH_SIZE\n    roberta_model.fit(tf_train_dataset,steps_per_epoch=n_steps,\n                      validation_data=tf_val_dataset,callbacks=[checkpoint],epochs=10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Cross Validation Score"},{"metadata":{"trusted":true},"cell_type":"code","source":"def model_cv_score(model):    \n    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=98)\n    E_cv = 0\n    for fold,(train_idx,val_idx) in enumerate(skf.split(df_train['premise'],df_train['label'])):\n        print('\\nFold %s:'% (fold))\n        K.clear_session()\n\n        train_premise = df_train['premise'].iloc[train_idx]\n        train_hypothesis = df_train['hypothesis'].iloc[train_idx]\n        train_label = df_train['label'].iloc[train_idx]\n\n        val_premise = df_train['premise'].iloc[val_idx]\n        val_hypothesis = df_train['hypothesis'].iloc[val_idx]\n        val_label = df_train['label'].iloc[val_idx]\n        \n        ## use encode method you define, this method is for bert base and \n        ## subject to change when switched to roBerta\n        cv_train_input = bert_encode(train_premise.values, train_hypothesis.values, tokenizer)\n        cv_val_input = bert_encode(val_premise.values, val_hypothesis.values, tokenizer)\n\n        #model = build_bert(max_len=55)\n        filepath='best_weight_fold%s.h5'%(fold)\n        checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=1, \n                                                    save_best_only=True, save_weights_only=True, \n                                                    mode='min',save_freq = 'epoch')\n        model.fit(cv_train_input, train_label.values, epochs =2, \n              batch_size = 16, callbacks=[checkpoint], \n              validation_data=(cv_val_input,val_label.values))\n\n        print('\\nPredicting OOF Error......')\n        model.load_weights('best_weight_fold%s.h5'%(fold))\n        val_pred = [np.argmax(p) for p in model.predict(cv_val_input)]\n        oof_error = np.sum([pred!=true for pred,true in zip(val_pred,val_label)])/len(val_label)\n        print('Fold_%s Error: %s' % (fold,oof_error))\n        E_cv += oof_error\n\n    print('\\nE_cv = %s'%(E_cv/5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Unbiased prediction of out of sample error for bert baseline is 0.35825. The best model performance is fold 0."},{"metadata":{},"cell_type":"markdown","source":"# Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"#test_input = bert_encode(df_test.premise.values, df_test.hypothesis.values, tokenizer)\n#model.load_weights('./best_weight_fold0.h5')\n#pred = [np.argmax(res) for res in model.predict(test_input)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_text = df_test[['premise', 'hypothesis']].values.tolist()\nencoded_test_text = roberta_encode(test_text,tokenizer,max_len=80)\ntf_test_dataset = (tf.data.Dataset\n                .from_tensor_slices(encoded_test_text['input_ids'])\n                .batch(BATCH_SIZE))\nroberta_model.load_weights('./roberta_best_weight.hdf5')\npred = [np.argmax(res) for res in roberta_model.predict(tf_test_dataset)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame({'id':df_test.id,'prediction':pred})\nsubmission.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head(10)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}