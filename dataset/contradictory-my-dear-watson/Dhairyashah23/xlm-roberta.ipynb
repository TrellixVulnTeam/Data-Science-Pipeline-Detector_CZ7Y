{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(\"../input/contradictory-my-dear-watson/train.csv\")\ntest = pd.read_csv(\"../input/contradictory-my-dear-watson/test.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept ValueError:\n    strategy = tf.distribute.get_strategy() # for CPU and single GPU\nprint('Number of replicas:', strategy.num_replicas_in_sync)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.info()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.id.nunique()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.language.nunique()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.language.unique()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(x='label',data=train)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15,8))\nsns.countplot(x='language',data=train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(x='lang_abv',data=train)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train_en = train.loc[train.language=='English'].copy()\nprint(df_train_en.shape)\ndf_train_fr = train.loc[train.language=='French'].copy()\nprint(df_train_fr.shape)\ndf_train_th = train.loc[train.language=='Thai'].copy()\nprint(df_train_th.shape)\ndf_train_tr = train.loc[train.language=='Turkish'].copy()\nprint(df_train_tr.shape)\ndf_train_ur = train.loc[train.language=='Urdu'].copy()\nprint(df_train_ur.shape)\ndf_train_ru = train.loc[train.language=='Russian'].copy()\nprint(df_train_ru.shape)\ndf_train_bg = train.loc[train.language=='Bulgarian'].copy()\nprint(df_train_bg.shape)\ndf_train_de = train.loc[train.language=='German'].copy()\nprint(df_train_de.shape)\ndf_train_ar = train.loc[train.language=='Arabic'].copy()\nprint(df_train_ar.shape)\ndf_train_zh = train.loc[train.language=='Chinese'].copy()\nprint(df_train_zh.shape)\ndf_train_hi = train.loc[train.language=='Hindi'].copy()\nprint(df_train_hi.shape)\ndf_train_sw = train.loc[train.language=='Swahili'].copy()\nprint(df_train_sw.shape)\ndf_train_vi = train.loc[train.language=='Vietnamese'].copy()\nprint(df_train_vi.shape)\ndf_train_es = train.loc[train.language=='Spanish'].copy()\nprint(df_train_es.shape)\ndf_train_ei = train.loc[train.language=='Greek'].copy()\nprint(df_train_ei.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text = \" \".join(txt for txt in train.hypothesis)\nstopwords = set(STOPWORDS)\n\nwordcloud = WordCloud(stopwords=stopwords, max_font_size=50, max_words=500,\n                      width = 600, height = 400,\n                      background_color=\"white\").generate(text)\nplt.figure(figsize=(12,8))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text = \" \".join(txt for txt in train.hypothesis)\nstopwords = set(STOPWORDS)\n\nwordcloud = WordCloud(stopwords=stopwords, max_font_size=50, max_words=500,\n                      width = 600, height = 400,\n                      background_color=\"white\").generate(text)\nplt.figure(figsize=(12,8))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['premise_len'] = pd.to_numeric(list(map(len, train.premise)))\ntrain['hypothesis_len'] = pd.to_numeric(list(map(len, train.hypothesis)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.premise_len.plot(kind='hist')\nplt.title('Length of premise')\nplt.grid()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.hypothesis_len.plot(kind='hist')\nplt.title('Length of hypothesis - Training (English)')\nplt.grid()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!pip install transformers\nfrom transformers import TFAutoModel,AutoTokenizer\nimport tensorflow as tf\n#!pip install sentencepiece\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer=AutoTokenizer.from_pretrained('joeddav/xlm-roberta-large-xnli')\ntrain_enc=tokenizer.batch_encode_plus(train[['premise','hypothesis']].values.tolist(),padding='max_length',max_length=100,truncation=True,return_attention_mask=True)\ntest_enc=tokenizer.batch_encode_plus(test[['premise','hypothesis']].values.tolist(),padding='max_length',max_length=100,truncation=True,return_attention_mask=True)\ntrain_tf1=tf.convert_to_tensor(train_enc['input_ids'],dtype=tf.int32)\ntrain_tf2=tf.convert_to_tensor(train_enc['attention_mask'],dtype=tf.int32)\ntrain_input={'input_word_ids':train_tf1,'input_mask':train_tf2}\ntest_tf1=tf.convert_to_tensor(test_enc['input_ids'],dtype=tf.int32)\ntest_tf2=tf.convert_to_tensor(test_enc['attention_mask'],dtype=tf.int32)\ntest_input={'input_word_ids':test_tf1,'input_mask':test_tf2}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_enc[100]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with strategy.scope():\n    input_ids = tf.keras.Input(shape = (100,), dtype = tf.int32,name='input_word_ids') \n    input_mask=tf.keras.Input(shape=(100,),dtype=tf.int32,name='input_mask')    \n    roberta = TFAutoModel.from_pretrained('joeddav/xlm-roberta-large-xnli')\n    roberta = roberta([input_ids,input_mask])[0]\n    out = tf.keras.layers.GlobalAveragePooling1D()(roberta)\n    out = tf.keras.layers.Dense(3, activation = 'softmax')(out)\n    model = tf.keras.Model(inputs = [input_ids,input_mask], outputs = out)\n    model.compile(\n                        optimizer = tf.keras.optimizers.Adam(lr = 1e-5), \n                        loss = 'sparse_categorical_crossentropy', \n                        metrics = ['accuracy']) \n    model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"strategy.num_replicas_in_sync\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"es=tf.keras.callbacks.EarlyStopping(patience=2,restore_best_weights=True)\nhistory=model.fit(train_input,train.label,validation_split=0.2,epochs=20,batch_size=10*strategy.num_replicas_in_sync,callbacks=[es],verbose=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\n# Plot training & validation loss values\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred=[np.argmax(i) for i in model.predict(test_input)]\npd.DataFrame(pred).value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame({'id':test.id,\n              'prediction':pred}).to_csv('submission.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}