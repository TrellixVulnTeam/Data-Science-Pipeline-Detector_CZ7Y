{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# What is BERT?\nBERT stands for Bidirectional Encoder Representation from Transformers. The pre-trained BERT model can be fine-tuned with just one output layer to create models for NLP tasks. **Bidirectional** means that BERT learns information from both the left and right side of a token's context during the training phase.\n\n**[CLS]** (classification task) token to put to the beginning of the sentence.\n**[SEP]** (seperator) token to put to the ending of the sentence.\nIn this case, since our model wants one input, we will concenate premise and hypothesis sentences and put [SEP] token between the two sentences. And [CLS] token to the beginning.\nLike this:\n> [CLS] I love transformers (premise). [SEP] I like transformers. [SEP]\n\n✓ All the input must be the same size.\n\n* Then we will convert inputs to tokens and encode them and outputs as embeddings of the word.\nSince we have multilingual data, I'll use XLM-RoBERTa model (train with more than 100 language) which has same architecture with BERT but it takes two input while BERT takes three.\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom transformers import AutoTokenizer,TFAutoModel\nimport tensorflow as tf\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2022-02-05T11:12:20.12544Z","iopub.execute_input":"2022-02-05T11:12:20.126402Z","iopub.status.idle":"2022-02-05T11:12:29.989116Z","shell.execute_reply.started":"2022-02-05T11:12:20.126224Z","shell.execute_reply":"2022-02-05T11:12:29.988136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = pd.read_csv(\"/kaggle/input/contradictory-my-dear-watson/train.csv\")\ntest_data = pd.read_csv(\"/kaggle/input/contradictory-my-dear-watson/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-02-05T11:12:29.990654Z","iopub.execute_input":"2022-02-05T11:12:29.991204Z","iopub.status.idle":"2022-02-05T11:12:30.218296Z","shell.execute_reply.started":"2022-02-05T11:12:29.991167Z","shell.execute_reply":"2022-02-05T11:12:30.217401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-05T11:12:30.219181Z","iopub.execute_input":"2022-02-05T11:12:30.219406Z","iopub.status.idle":"2022-02-05T11:12:30.24462Z","shell.execute_reply.started":"2022-02-05T11:12:30.219381Z","shell.execute_reply":"2022-02-05T11:12:30.243925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver() \n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu) \nexcept ValueError: \n    strategy = tf.distribute.get_strategy() \n    print('Number of replicas:', strategy.num_replicas_in_sync) ","metadata":{"execution":{"iopub.status.busy":"2022-02-05T11:12:30.246306Z","iopub.execute_input":"2022-02-05T11:12:30.246645Z","iopub.status.idle":"2022-02-05T11:12:36.063553Z","shell.execute_reply.started":"2022-02-05T11:12:30.246619Z","shell.execute_reply":"2022-02-05T11:12:36.062642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAX_LEN = 150\n#MAX_LEN = 227\n#lr = 1e-5\n#BATCH_SIZE = 32*strategy.num_replicas_in_sync\n#MAX_LEN = 236\n#MAX_LEN = 245\n\n#MAX_LEN = 120\nlr = 1e-6\nBATCH_SIZE = 64\nEPOCHS = 20","metadata":{"execution":{"iopub.status.busy":"2022-02-05T11:12:36.064767Z","iopub.execute_input":"2022-02-05T11:12:36.064982Z","iopub.status.idle":"2022-02-05T11:12:36.068811Z","shell.execute_reply.started":"2022-02-05T11:12:36.064956Z","shell.execute_reply":"2022-02-05T11:12:36.068142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_lang = train_data.language.value_counts()\ntest_lang = test_data.language.value_counts()\n\npx.pie(values=train_lang, names=train_lang.index)","metadata":{"execution":{"iopub.status.busy":"2022-02-05T11:12:36.070396Z","iopub.execute_input":"2022-02-05T11:12:36.07062Z","iopub.status.idle":"2022-02-05T11:12:37.095511Z","shell.execute_reply.started":"2022-02-05T11:12:36.070585Z","shell.execute_reply":"2022-02-05T11:12:37.09452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"px.pie(values=test_lang, names=test_lang.index)","metadata":{"execution":{"iopub.status.busy":"2022-02-05T11:12:37.096707Z","iopub.execute_input":"2022-02-05T11:12:37.097127Z","iopub.status.idle":"2022-02-05T11:12:37.149659Z","shell.execute_reply.started":"2022-02-05T11:12:37.097098Z","shell.execute_reply":"2022-02-05T11:12:37.148933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_label = train_data.label.value_counts()\n\npx.pie(values=train_label, names=train_label.index)","metadata":{"execution":{"iopub.status.busy":"2022-02-05T11:12:37.150723Z","iopub.execute_input":"2022-02-05T11:12:37.151051Z","iopub.status.idle":"2022-02-05T11:12:37.205354Z","shell.execute_reply.started":"2022-02-05T11:12:37.151025Z","shell.execute_reply":"2022-02-05T11:12:37.204578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# download models tokenizers\nmodel_name = \"joeddav/xlm-roberta-large-xnli\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# encode a sentence to have a list of the ID for each word and seperator\ndef encode_sentence(s):\n    tokens = list(tokenizer.tokenize(s))            # encode sentence with tokenizer\n    tokens.append(\"[SEP]\")                          # [SEP] token is added to the end of each sentence\n    return tokenizer.convert_tokens_to_ids(tokens)  # return a list of each token ID, not list of tokens","metadata":{"execution":{"iopub.status.busy":"2022-02-05T11:12:37.206281Z","iopub.execute_input":"2022-02-05T11:12:37.206996Z","iopub.status.idle":"2022-02-05T11:12:41.887502Z","shell.execute_reply.started":"2022-02-05T11:12:37.206964Z","shell.execute_reply":"2022-02-05T11:12:41.886684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_data.premise.map(lambda x: len(encode_sentence(x))).max())","metadata":{"execution":{"iopub.status.busy":"2022-02-05T11:12:41.890147Z","iopub.execute_input":"2022-02-05T11:12:41.890405Z","iopub.status.idle":"2022-02-05T11:12:45.057597Z","shell.execute_reply.started":"2022-02-05T11:12:41.890378Z","shell.execute_reply":"2022-02-05T11:12:45.056415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_data.hypothesis.map(lambda x: len(encode_sentence(x))).max())","metadata":{"execution":{"iopub.status.busy":"2022-02-05T11:12:45.058852Z","iopub.execute_input":"2022-02-05T11:12:45.059103Z","iopub.status.idle":"2022-02-05T11:12:47.062993Z","shell.execute_reply.started":"2022-02-05T11:12:45.059073Z","shell.execute_reply":"2022-02-05T11:12:47.061872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer.tokenize(\"Don't you love Transformers?\")        # tokenizers turn sequences of words into arrays of numbers","metadata":{"execution":{"iopub.status.busy":"2022-02-05T11:12:47.064326Z","iopub.execute_input":"2022-02-05T11:12:47.064563Z","iopub.status.idle":"2022-02-05T11:12:47.070995Z","shell.execute_reply.started":"2022-02-05T11:12:47.064535Z","shell.execute_reply":"2022-02-05T11:12:47.070056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encode_sentence(\"Don't you love Transformers?\")","metadata":{"execution":{"iopub.status.busy":"2022-02-05T11:12:47.072574Z","iopub.execute_input":"2022-02-05T11:12:47.073266Z","iopub.status.idle":"2022-02-05T11:12:47.084217Z","shell.execute_reply.started":"2022-02-05T11:12:47.073227Z","shell.execute_reply":"2022-02-05T11:12:47.083394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Input words ID:** The output list of encode will have a different length for each sentence from dataset. This means each sentence belong to premises and hypothesis must have the same length. So, we will have to add zeros at the end of each ID list until it has the same lenghth of the longest list in the dataset. This process is called *padding*.\n\n**Input masks:** We should tell BERT to which ID's to ignore and to embed. BERT should ignore paddings. The input mask variable has the same length as the ID lists. Contains 1 for each actual token ID, and 0 for each padding. which BERT should ignore. \n\n**Input type ID:** We won't need this for roBERTa model. BERT model needs this input argument since it was trained to predicting the likelihood that Sentence B belongs after Sentence A.","metadata":{}},{"cell_type":"code","source":"def roberta_encode(hypothesis, premise, tokenizer):\n        \n    #construct a constant ragged tensor since our entries has different lenghts\n    sentence_1 = tf.ragged.constant([\n        encode_sentence(s) for s in np.array(hypothesis)\n    ])\n    \n    sentence_2 = tf.ragged.constant([\n        encode_sentence(s) for s in np.array(premise)\n    ])\n    \n    # token [CLS] to denote each beginning of concenation of sentence_1 and _2\n    cls = [tokenizer.convert_tokens_to_ids(['[CLS]'])] * sentence_1.shape[0]\n    \n    # concenate sentences but each sentence still has different lengths\n    input_word_ids = tf.concat([cls, sentence_1, sentence_2], axis = -1)\n    \n    #a tensor with just ones with the same size as input_word_ids\n    input_mask = tf.ones_like(input_word_ids).to_tensor()\n    \n    inputs = {\n        'input_word_ids': input_word_ids.to_tensor(),\n        'input_mask': input_mask\n    }\n    \n    return inputs","metadata":{"execution":{"iopub.status.busy":"2022-02-05T11:12:47.085576Z","iopub.execute_input":"2022-02-05T11:12:47.08599Z","iopub.status.idle":"2022-02-05T11:12:47.094185Z","shell.execute_reply.started":"2022-02-05T11:12:47.08596Z","shell.execute_reply":"2022-02-05T11:12:47.093542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#inputs = roberta_encode(train_data.premise.values, train_data.hypothesis.values, tokenizer)\ninputs = roberta_encode(train_data.hypothesis.values, train_data.premise.values, tokenizer)","metadata":{"execution":{"iopub.status.busy":"2022-02-05T11:12:47.095088Z","iopub.execute_input":"2022-02-05T11:12:47.095353Z","iopub.status.idle":"2022-02-05T11:12:54.543625Z","shell.execute_reply.started":"2022-02-05T11:12:47.095313Z","shell.execute_reply":"2022-02-05T11:12:54.542759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inputs","metadata":{"execution":{"iopub.status.busy":"2022-02-05T11:12:54.544697Z","iopub.execute_input":"2022-02-05T11:12:54.544911Z","iopub.status.idle":"2022-02-05T11:12:54.60742Z","shell.execute_reply.started":"2022-02-05T11:12:54.544887Z","shell.execute_reply":"2022-02-05T11:12:54.606366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inputs['input_word_ids'].shape","metadata":{"execution":{"iopub.status.busy":"2022-02-05T11:12:54.608538Z","iopub.execute_input":"2022-02-05T11:12:54.608757Z","iopub.status.idle":"2022-02-05T11:12:54.614544Z","shell.execute_reply.started":"2022-02-05T11:12:54.608731Z","shell.execute_reply":"2022-02-05T11:12:54.613785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_model():\n    \n    # adjust the model\n    encoder = TFAutoModel.from_pretrained(model_name)\n    \n    # tell how our input looks like\n    input_word_ids = tf.keras.Input(shape=(MAX_LEN,), dtype=tf.int32, name=\"input_word_ids\")\n    \n    # other input, masks\n    input_mask = tf.keras.Input(shape=(MAX_LEN,), dtype=tf.int32, name=\"input_mask\")\n    \n    # feed model with input\n    # roberta encoder will return a tuple and the contextualized embeddings that we need are stored in the first element\n    embedding = encoder([input_word_ids, input_mask])[0]\n    out = tf.keras.layers.Dropout(.4)(embedding[:,0,:])\n    out = tf.keras.layers.Dense(32, activation=\"relu\")(out)\n    #output = tf.keras.layers.Dense(3, activation=\"softmax\")(embedding[:,0,:])\n    output = tf.keras.layers.Dense(3, activation=\"softmax\")(out)\n    model = tf.keras.Model(inputs=[input_word_ids, input_mask], outputs=output)\n    model.compile(tf.keras.optimizers.Adam(learning_rate=lr),\n                 loss = 'sparse_categorical_crossentropy',\n                 metrics = [\"accuracy\"])\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2022-02-05T11:12:54.615705Z","iopub.execute_input":"2022-02-05T11:12:54.616404Z","iopub.status.idle":"2022-02-05T11:12:54.62804Z","shell.execute_reply.started":"2022-02-05T11:12:54.61636Z","shell.execute_reply":"2022-02-05T11:12:54.627385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with strategy.scope():\n    model = build_model()\n    model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-02-05T11:12:54.629031Z","iopub.execute_input":"2022-02-05T11:12:54.629306Z","iopub.status.idle":"2022-02-05T11:14:50.061545Z","shell.execute_reply.started":"2022-02-05T11:12:54.629259Z","shell.execute_reply":"2022-02-05T11:14:50.060612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for key in inputs.keys():\n    inputs[key] = inputs[key][:, :MAX_LEN]","metadata":{"execution":{"iopub.status.busy":"2022-02-05T11:14:50.062993Z","iopub.execute_input":"2022-02-05T11:14:50.063235Z","iopub.status.idle":"2022-02-05T11:14:50.070053Z","shell.execute_reply.started":"2022-02-05T11:14:50.063204Z","shell.execute_reply":"2022-02-05T11:14:50.068849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inputs.keys()","metadata":{"execution":{"iopub.status.busy":"2022-02-05T11:14:50.071601Z","iopub.execute_input":"2022-02-05T11:14:50.071861Z","iopub.status.idle":"2022-02-05T11:14:50.086362Z","shell.execute_reply.started":"2022-02-05T11:14:50.071832Z","shell.execute_reply":"2022-02-05T11:14:50.085209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#n_steps = len(train_data) // BATCH_SIZE\nmodel.fit(inputs, train_data.label.values, epochs=EPOCHS, validation_split=.2,\n         batch_size=BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2022-02-05T11:14:50.087737Z","iopub.execute_input":"2022-02-05T11:14:50.088041Z","iopub.status.idle":"2022-02-05T11:32:11.225196Z","shell.execute_reply.started":"2022-02-05T11:14:50.088Z","shell.execute_reply":"2022-02-05T11:32:11.224357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.history.history\npx.line(\n    history, x=range(1, len(history['loss'])+1), y=['accuracy', 'val_accuracy'], \n    title='Model Accuracy', labels={'x': 'Epoch', 'value': 'Accuracy'}\n)","metadata":{"execution":{"iopub.status.busy":"2022-02-05T11:32:11.226775Z","iopub.execute_input":"2022-02-05T11:32:11.227029Z","iopub.status.idle":"2022-02-05T11:32:11.364838Z","shell.execute_reply.started":"2022-02-05T11:32:11.227Z","shell.execute_reply":"2022-02-05T11:32:11.363898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"px.line(\n    history, x=range(1, len(history['loss'])+1), y=['loss', 'val_loss'], \n    title='Model Loss', labels={'x': 'Epoch', 'value': 'Loss'}\n)","metadata":{"execution":{"iopub.status.busy":"2022-02-05T11:32:11.367427Z","iopub.execute_input":"2022-02-05T11:32:11.368057Z","iopub.status.idle":"2022-02-05T11:32:11.451551Z","shell.execute_reply.started":"2022-02-05T11:32:11.368019Z","shell.execute_reply":"2022-02-05T11:32:11.45093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_inputs = roberta_encode(test_data.hypothesis.values, test_data.premise.values, tokenizer)\nfor key in test_inputs.keys():\n    test_inputs[key] = test_inputs[key][:,:MAX_LEN]","metadata":{"execution":{"iopub.status.busy":"2022-02-05T11:32:11.452713Z","iopub.execute_input":"2022-02-05T11:32:11.453136Z","iopub.status.idle":"2022-02-05T11:32:14.813788Z","shell.execute_reply.started":"2022-02-05T11:32:11.453102Z","shell.execute_reply":"2022-02-05T11:32:14.812757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = model.predict(test_inputs)\npredictions = [np.argmax(i) for i in preds]\n#predictions","metadata":{"execution":{"iopub.status.busy":"2022-02-05T11:32:14.815583Z","iopub.execute_input":"2022-02-05T11:32:14.816573Z","iopub.status.idle":"2022-02-05T11:32:46.945074Z","shell.execute_reply.started":"2022-02-05T11:32:14.816529Z","shell.execute_reply":"2022-02-05T11:32:46.944033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = test_data.id.copy().to_frame()\nsubmission[\"prediction\"] = predictions\nsubmission.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-05T11:32:46.946808Z","iopub.execute_input":"2022-02-05T11:32:46.947137Z","iopub.status.idle":"2022-02-05T11:32:46.96464Z","shell.execute_reply.started":"2022-02-05T11:32:46.947098Z","shell.execute_reply":"2022-02-05T11:32:46.963776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-05T11:32:46.969102Z","iopub.execute_input":"2022-02-05T11:32:46.969722Z","iopub.status.idle":"2022-02-05T11:32:46.989963Z","shell.execute_reply.started":"2022-02-05T11:32:46.969683Z","shell.execute_reply":"2022-02-05T11:32:46.989262Z"},"trusted":true},"execution_count":null,"outputs":[]}]}