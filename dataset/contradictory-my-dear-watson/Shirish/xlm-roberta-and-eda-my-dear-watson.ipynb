{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h1 style='color:red'><strong>Importing Libraries :","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport os\nimport re\nimport plotly.express as px\nimport plotly.figure_factory as ff\nimport plotly.graph_objects as go\nfrom tqdm import tqdm\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras import backend as k\nfrom keras.utils import to_categorical\nimport transformers","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1 style='color:orange'><strong>TPU Initialization :","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"DEVICE = 'TPU'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if DEVICE == 'TPU':\n    print('Connecting to TPU...')\n    try:\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        print('Running on TPU :',tpu.master())\n    except ValueError:\n        print('Could not connect to TPU')\n        tpu = None\n        \n    if tpu:\n        try:\n            print('Initializing TPU...')\n            tf.config.experimental_connect_to_cluster(tpu)\n            tf.tpu.experimental.initialize_tpu_system(tpu)\n            strategy = tf.distribute.experimental.TPUStrategy(tpu)\n            print('TPU initialized!')\n            \n        except _:\n            print('Failed to initialized TPU')\n            \n    else:\n        DEVICE='GPU'\n\nif DEVICE != 'TPU':\n    print('Using default strategy for CPU and single GPU')\n    strategy = tf.distribute.get_strategy()\n\nif DEVICE == 'GPU':\n    print('Num GPUs available : ',len(tf.config.experimental.list_physical_devices('GPU')))\n    \nAUTO = tf.data.experimental.AUTOTUNE\nREPLICAS = strategy.num_replicas_in_sync\nprint('REPLICAS : ',REPLICAS)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Batch_size = 16 * strategy.num_replicas_in_sync\nepochs = 13\nAUTO = tf.data.experimental.AUTOTUNE\n\nMODEL = 'jplu/tf-xlm-roberta-large'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1 style='color:magenta'><strong>Reading Data :","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(r'../input/contradictory-my-dear-watson/train.csv')\ntest = pd.read_csv(r'../input/contradictory-my-dear-watson/test.csv')\nsubmission = pd.read_csv(r'../input/contradictory-my-dear-watson/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1 style='color:teal'><strong>Data Visualisation :","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"num_lang = train.groupby('language')['id'].count().sort_values(ascending=False).reset_index()\nnum_lang = pd.DataFrame(num_lang)\nnum_lang['count'] = num_lang['id']\nnum_lang = num_lang.drop('id',axis=1)\nnum_lang_data = num_lang.style.background_gradient(cmap='Greens')\nnum_lang_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_num_lang = test.groupby('language')['id'].count().sort_values(ascending=False).reset_index()\ntest_num_lang = pd.DataFrame(test_num_lang)\ntest_num_lang['count'] = test_num_lang['id']\ntest_num_lang = test_num_lang.drop('id',axis=1)\ntest_num_lang_data = test_num_lang.style.background_gradient(cmap='Oranges')\ntest_num_lang_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.pie(num_lang,values='count',names='language',title='Language and their percentage in the train data :',color_discrete_sequence=px.colors.sequential.GnBu)\nfig.update_traces(hoverinfo='label+percent', textfont_size=14,\n                  marker=dict(line=dict(color='#000000', width=1.2)))\nfig.show()\n\nfig = px.pie(test_num_lang,values='count',names='language',title='Language and their percentage in the test data :',color_discrete_sequence=px.colors.sequential.RdBu)\nfig.update_traces(hoverinfo='label+percent', textfont_size=14,\n                  marker=dict(line=dict(color='#000000', width=1.2)))\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.bar(num_lang,x='language',y='count')\nfig.update_traces(marker_color='ivory', marker_line_color='black',\n                  marker_line_width=1.3, opacity=0.5)\nfig.update_layout(title_text='Languages and their count in the data')\nfig.show()\n\nfig = px.bar(test_num_lang,x='language',y='count')\nfig.update_traces(marker_color='darkturquoise', marker_line_color='black',\n                  marker_line_width=1.3, opacity=0.5)\nfig.update_layout(title_text='Languages and their count in the data')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_words_train_h = [None] * len(train)\nfor i in range(len(train)):\n    num_words_train_h[i] = len(train['hypothesis'][i])\nnum_words_train_p = [None] * len(train)\nfor i in range(len(train)):\n    num_words_train_p[i] = len(train['premise'][i])\n    \nnum_words_test_h = [None] * len(test)\nfor i in range(len(test)):\n    num_words_test_h[i] = len(test['hypothesis'][i])\nnum_words_test_p = [None] * len(test)\nfor i in range(len(test)):\n    num_words_test_p[i] = len(test['premise'][i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Maximum and minimum number of words in a single sentence in hypothesis in the train data :',(max(num_words_train_h),min(num_words_train_h)))\nprint('Maximum and minimum number of words in a single sentence in hypothesis in the test data :',(max(num_words_test_h),min(num_words_test_h)))\nprint('Maximum and minimum number of words in a single sentence in premise in the train data :',(max(num_words_train_p),min(num_words_train_p)))\nprint('Maximum and minimum number of words in a single sentence in premise in the test data :',(max(num_words_test_p),min(num_words_test_p)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['num_words_hypothesis'] = num_words_train_h\ntrain['num_words_premise'] = num_words_train_p\ntest['num_words_hypothesis'] = num_words_test_h\ntest['num_words_premise'] = num_words_test_p","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"english_train = train[train['language']=='English']\nenglish_test = test[test['language']=='English']\n\n\nhist_data = [english_train['num_words_hypothesis'],english_train['num_words_premise']]\ngroup_labels = ['hypothesis','premise']\nfig = ff.create_distplot(hist_data,group_labels,colors=['ivory','teal'])\nfig.update_layout(title_text='Number of words for English Language in train data')\nfig.show()\n\nhist_data = [english_test['num_words_hypothesis'],english_test['num_words_premise']]\ngroup_labels = ['hypothesis','premise']\nfig = ff.create_distplot(hist_data,group_labels,colors=['red','greenyellow'])\nfig.update_layout(title_text='Number of words for English Language in test data')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = train['label'].sort_values().value_counts().reset_index()\nlabels = pd.DataFrame(labels)\nlabels.columns = ['label','count']\nlabels_ = labels.style.background_gradient(cmap='Blues')\nlabels_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1 style='color:red'><strong>Data Modeling :","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"target = train['label']\ntrain = train.drop('label',axis=1)\ntrain_text = [None] * len(train)\ntest_text = [None] * len(test)\nfor i in range(len(train)):\n    train_text[i] = train['premise'][i] + ' ' + train['hypothesis'][i]\nfor i in range(len(test)):\n    test_text[i] = test['premise'][i] + ' ' + test['hypothesis'][i]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = transformers.AutoTokenizer.from_pretrained(MODEL)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def roberta_encode(texts, maxlen=512):\n    enc_di = tokenizer.batch_encode_plus(\n        texts,  \n        pad_to_max_length=True,\n        max_length=maxlen\n    )\n    \n    return np.array(enc_di['input_ids'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_input_ids = roberta_encode(train_text,maxlen=100)\ntest_input_ids = roberta_encode(test_text,maxlen=100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1 style='color:orange'><strong>Train-Test Split :","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain_input_ids,validation_input_ids,train_labels,validation_labels = train_test_split(train_input_ids,target,test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_input_ids[7]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"validation_input_ids[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_input_ids[3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"validation_labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((train_input_ids,train_labels))\n    .repeat()\n    .shuffle(2048)\n    .batch(Batch_size)\n    .prefetch(AUTO)\n)\n\nvalidation_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((validation_input_ids, validation_labels))\n    .batch(Batch_size)\n    .cache()\n    .prefetch(AUTO)\n)\n\ntest_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices(test_input_ids)\n    .batch(Batch_size)\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1 style='color:brown'><strong>XLM-Roberta Model :","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model(bert_model):\n    input_ids = tf.keras.Input(shape=(100,),dtype='int32')\n  \n    output = bert_model(input_ids)[0]\n    output = output[:,0,:]\n    output = tf.keras.layers.Dense(3,activation='softmax')(output)\n    model = tf.keras.models.Model(inputs = input_ids,outputs = output)\n    model.compile(optimizer=tf.keras.optimizers.Adam(lr=1e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with strategy.scope():\n    bert_model = (\n        \n        transformers.TFAutoModel  \n        .from_pretrained(MODEL)    \n    )\n    model = create_model(bert_model)   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(train_dataset,\n                    validation_data = validation_dataset,\n                    epochs = epochs,   \n                    batch_size = Batch_size,\n                    steps_per_epoch = len(train_input_ids)//Batch_size\n                   )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,8))\nplt.plot(history.history['accuracy'],color='orange')\nplt.plot(history.history['val_accuracy'],color='green')\nplt.legend(loc='best',shadow=True)\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,8))\nplt.plot(history.history['loss'],color='orange')\nplt.plot(history.history['val_loss'],color='green')\nplt.legend(loc='best',shadow=True)\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1 style='color:gray'><strong>Predictions :","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = model.predict(test_dataset,verbose=1)\nprint(len(pred))\npred = pred.argmax(axis=1)\nsubmission.prediction = pred      \nsubmission.to_csv('submission.csv',index=False)    \nsubmission.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}