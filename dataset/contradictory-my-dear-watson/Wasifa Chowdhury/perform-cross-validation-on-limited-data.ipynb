{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Objective","metadata":{}},{"cell_type":"markdown","source":"#### Cross-validation is a resampling procedure that is used to evaluate machine learning models on limited dataset. This tutorial details how to cross-validate a transformer-based model on limited training data samples. \n\n#### The classification problem is to identify the semantic relationship between two sentences. Given a hypothesis and premise sentence-pairs, the task is to determine whether the premise `entails` the hypothesis statement, `contradicts` it, or neither (`neutral`). For more information on the problem, you can visit [Contradictory, My Dear Watson competition](https://www.kaggle.com/c/contradictory-my-dear-watson/overview) \n\n#### The provided [dataset](https://www.kaggle.com/c/contradictory-my-dear-watson/data) in the competition contains only 12,120 training examples. Hence *k*-fold cross-validation is used to reduce overfitting problems by dividing the training data into *k* random parts. The model is trained on *k-1* parts and tested with the remaining part.\n\n#### **Note**: The model is trained on Kaggle kernel with a TPU accelerator\n\nLet's get started!","metadata":{}},{"cell_type":"markdown","source":"## Install and Load Necessary Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os\nimport os.path\nfrom os import path\nfrom transformers import AutoTokenizer, AutoConfig, TFAutoModel    \nfrom transformers import XLMRobertaConfig, XLMRobertaTokenizer, TFXLMRobertaModel           \nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.layers import Input, Dropout, Dense, GlobalAveragePooling1D, LayerNormalization\nfrom tqdm import tqdm\nimport time\nimport glob\nfrom sklearn.model_selection import StratifiedKFold\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n\n# Resets all state generated by Keras\nK.clear_session()\n\n# For reproducibility\nnp.random.seed(0)\n\nos.environ[\"WANDB_API_KEY\"] = \"0\" # to silence warning","metadata":{"papermill":{"duration":11.59848,"end_time":"2021-02-09T22:37:59.720729","exception":false,"start_time":"2021-02-09T22:37:48.122249","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-19T09:52:32.800758Z","iopub.execute_input":"2022-05-19T09:52:32.801058Z","iopub.status.idle":"2022-05-19T09:52:32.812292Z","shell.execute_reply.started":"2022-05-19T09:52:32.801029Z","shell.execute_reply":"2022-05-19T09:52:32.81162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    print('Found TPU: ', tpu.master())\nexcept ValueError:\n    strategy = tf.distribute.get_strategy() # for CPU and single GPU\nprint('Number of replicas:', strategy.num_replicas_in_sync)","metadata":{"papermill":{"duration":5.453391,"end_time":"2021-02-09T22:38:05.194582","exception":false,"start_time":"2021-02-09T22:37:59.741191","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-19T09:52:32.81672Z","iopub.execute_input":"2022-05-19T09:52:32.81763Z","iopub.status.idle":"2022-05-19T09:52:41.00088Z","shell.execute_reply.started":"2022-05-19T09:52:32.817518Z","shell.execute_reply":"2022-05-19T09:52:40.999742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Read the Data Files","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv(\"../input/contradictory-my-dear-watson/train.csv\")\ntest_df = pd.read_csv(\"../input/contradictory-my-dear-watson/test.csv\")\n\n# check the number of rows and columns in the datasets\nprint(\"Training data shape: {}\".format(train_df.shape))\nprint(\"Test data shape: {}\\n\".format(test_df.shape))\n\ntrain_df.head()","metadata":{"papermill":{"duration":0.228832,"end_time":"2021-02-09T22:38:05.446368","exception":false,"start_time":"2021-02-09T22:38:05.217536","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-19T09:52:41.002932Z","iopub.execute_input":"2022-05-19T09:52:41.003182Z","iopub.status.idle":"2022-05-19T09:52:41.102472Z","shell.execute_reply.started":"2022-05-19T09:52:41.003154Z","shell.execute_reply":"2022-05-19T09:52:41.101582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load and Process the Data into Batches","metadata":{}},{"cell_type":"code","source":"# Configuration Settings\nEPOCHS = 5\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync\nMAX_LEN = 120\nPATIENCE = 1\nLEARNING_RATE = 1e-5\nAUTO = tf.data.experimental.AUTOTUNE","metadata":{"execution":{"iopub.status.busy":"2022-05-19T09:52:41.103941Z","iopub.execute_input":"2022-05-19T09:52:41.104759Z","iopub.status.idle":"2022-05-19T09:52:41.110989Z","shell.execute_reply.started":"2022-05-19T09:52:41.10471Z","shell.execute_reply":"2022-05-19T09:52:41.109933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"name = 'jplu/tf-xlm-roberta-large'\nPRETRAINED_MODEL_TYPES = {\n    'xlmroberta': (XLMRobertaConfig, TFXLMRobertaModel, XLMRobertaTokenizer, name)\n}\n\nconfig_class, model_class, tokenizer_class, model_name = PRETRAINED_MODEL_TYPES['xlmroberta']\n# Download vocabulary from huggingface.co and cache.\n# tokenizer = tokenizer_class.from_pretrained(model_name) \ntokenizer = AutoTokenizer.from_pretrained(model_name) #fast tokenizer\ntokenizer","metadata":{"papermill":{"duration":0.028911,"end_time":"2021-02-09T22:38:05.496872","exception":false,"start_time":"2021-02-09T22:38:05.467961","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-19T09:52:41.112343Z","iopub.execute_input":"2022-05-19T09:52:41.112678Z","iopub.status.idle":"2022-05-19T09:52:44.557149Z","shell.execute_reply.started":"2022-05-19T09:52:41.112637Z","shell.execute_reply":"2022-05-19T09:52:44.556182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def encode(df, tokenizer, max_len=50, cross_val=False):\n    \n    pairs = df[['premise','hypothesis']].values.tolist() #shape=[num_examples]\n    \n    print (\"Encoding...\")\n    encoded_dict = tokenizer.batch_encode_plus(pairs, max_length=max_len, padding=True, truncation=True, \n                                               add_special_tokens=True, return_attention_mask=True)\n    print (\"Complete\")\n    \n    if cross_val:\n        input_word_ids = np.array(encoded_dict['input_ids']) #shape=[num_examples, max_len])\n        input_mask = np.array(encoded_dict['attention_mask']) #shape=[num_examples, max_len]\n    else:\n        input_word_ids = tf.convert_to_tensor(encoded_dict['input_ids'], dtype=tf.int32) #shape=[num_examples, max_len])\n        input_mask = tf.convert_to_tensor(encoded_dict['attention_mask'], dtype=tf.int32) #shape=[num_examples, max_len]\n\n\n    inputs = {\n        'input_word_ids': input_word_ids,\n        'input_mask': input_mask}    \n    \n    return inputs","metadata":{"papermill":{"duration":0.036046,"end_time":"2021-02-09T22:38:09.437981","exception":false,"start_time":"2021-02-09T22:38:09.401935","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-19T09:52:44.559768Z","iopub.execute_input":"2022-05-19T09:52:44.560171Z","iopub.status.idle":"2022-05-19T09:52:44.570111Z","shell.execute_reply.started":"2022-05-19T09:52:44.560125Z","shell.execute_reply":"2022-05-19T09:52:44.569497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_dataset(features, labels, batch_size=BATCH_SIZE, validation=False):\n    dataset = tf.data.Dataset.from_tensor_slices((features, labels)).shuffle(len(features))\n    if validation:\n        dataset = dataset.batch(batch_size).prefetch(AUTO)\n    else:\n        dataset = dataset.repeat().batch(batch_size).prefetch(AUTO)\n    return dataset","metadata":{"papermill":{"duration":0.034168,"end_time":"2021-02-09T22:38:09.557769","exception":false,"start_time":"2021-02-09T22:38:09.523601","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-19T09:52:44.571722Z","iopub.execute_input":"2022-05-19T09:52:44.572779Z","iopub.status.idle":"2022-05-19T09:52:44.580518Z","shell.execute_reply.started":"2022-05-19T09:52:44.572733Z","shell.execute_reply":"2022-05-19T09:52:44.579694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_input = encode(train_df, tokenizer=tokenizer, max_len=MAX_LEN, cross_val=True)\ntrain_ids = train_input['input_word_ids'] #[9696, max_len]\ntrain_mask = train_input['input_mask'] #[9696, max_len]\ntrain_labels = train_df.label.values","metadata":{"papermill":{"duration":2.402471,"end_time":"2021-02-09T22:38:11.985087","exception":false,"start_time":"2021-02-09T22:38:09.582616","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-19T09:52:44.58165Z","iopub.execute_input":"2022-05-19T09:52:44.582001Z","iopub.status.idle":"2022-05-19T09:52:46.832103Z","shell.execute_reply.started":"2022-05-19T09:52:44.581972Z","shell.execute_reply":"2022-05-19T09:52:46.830962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Build the Model","metadata":{}},{"cell_type":"code","source":"def build_model(model_name, max_len=50):\n    \n    tf.random.set_seed(1234)\n    \n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    \n    # The bare XLM-RoBERTa Model transformer outputting raw hidden-states without any specific head on top.\n    base_model = model_class.from_pretrained(model_name)\n    output = base_model([input_word_ids, input_mask]) # output from xlmroberta model\n    sequence_output = output.pooler_output #shape: [batch_size, embed_size]\n    \n    # Add a classification layer\n    output = Dense(units=3, activation=\"softmax\")(sequence_output)\n    \n    model = tf.keras.Model(inputs=[input_word_ids, input_mask], outputs=output)\n    model.compile(tf.keras.optimizers.Adam(lr=LEARNING_RATE), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n    \n    return model","metadata":{"papermill":{"duration":0.036703,"end_time":"2021-02-09T22:38:09.499066","exception":false,"start_time":"2021-02-09T22:38:09.462363","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-19T09:52:46.833748Z","iopub.execute_input":"2022-05-19T09:52:46.834228Z","iopub.status.idle":"2022-05-19T09:52:46.842584Z","shell.execute_reply.started":"2022-05-19T09:52:46.834189Z","shell.execute_reply":"2022-05-19T09:52:46.841435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train and Cross-Validate the Model","metadata":{}},{"cell_type":"markdown","source":"Selecting the right number of folds or the *k* value is crucial in the model's ability to generalize well to unseen data. \n\nNote that the *k* value is a hyperparameter and is selected through experimentation. ","metadata":{}},{"cell_type":"code","source":"checkpoint_filepath='best_checkpoint.hdf5'\n\n# prepare cross validation\nfolds = 3\nkfold = StratifiedKFold(n_splits=folds, shuffle=True, random_state=1)\n\n# for plotting\nval_loss_list = []\nval_acc_list = []\ntrain_hist_list = []\n\n# callbacks = [EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=PATIENCE)]\ncallbacks = [EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=PATIENCE), ModelCheckpoint(filepath=checkpoint_filepath, save_best_only=True, save_weights_only=True, monitor='val_loss', mode='min', verbose=1)]\n\n# enumerate splits\nfor k, (train_idx, val_idx) in enumerate(kfold.split(train_ids, train_labels)):\n    print ('Fold {} of {}'.format(k+1, folds))\n    print('Train data shape: {}, Validation data shape: {}'.format(len(train_idx), len(val_idx)))\n    print('Train indices: {}, Validation indices: {}'.format(train_idx, val_idx))\n    \n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    \n    training_data = create_dataset((train_ids[train_idx], train_mask[train_idx]), train_labels[train_idx], batch_size=BATCH_SIZE, validation=False)\n    validation_data = create_dataset((train_ids[val_idx], train_mask[val_idx]), train_labels[val_idx], batch_size=BATCH_SIZE, validation=True)\n    \n    # instantiating the model in the strategy scope creates the model on the TPU\n    with strategy.scope():\n        model = build_model(model_name, MAX_LEN)\n#         model.summary()\n        \n    n_steps = int(len(train_idx)/BATCH_SIZE)\n    \n    train_history = model.fit(x=training_data, validation_data=validation_data, epochs=EPOCHS, verbose=1, steps_per_epoch=n_steps, callbacks=callbacks)\n    \n    avg_val_loss = np.mean(train_history.history['val_loss'])\n    avg_val_acc = np.mean(train_history.history['val_accuracy'])\n    \n    print ('Average Validation Loss: {}'.format(avg_val_loss))\n    print ('Average Validation Accuracy: {}'.format(avg_val_acc))\n    print('#############################################\\n')\n    \n    val_loss_list.append(avg_val_loss)\n    val_acc_list.append(avg_val_acc)\n    train_hist_list.append(train_history)\n    \n    del model #free up space\n            \n    # Resets all state generated by Keras before training a new model in the next fold\n    K.clear_session()","metadata":{"papermill":{"duration":1674.851104,"end_time":"2021-02-09T23:06:07.202444","exception":false,"start_time":"2021-02-09T22:38:12.35134","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-19T09:52:46.843867Z","iopub.execute_input":"2022-05-19T09:52:46.844651Z","iopub.status.idle":"2022-05-19T10:11:15.799482Z","shell.execute_reply.started":"2022-05-19T09:52:46.844588Z","shell.execute_reply":"2022-05-19T10:11:15.798617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Performance Analysis","metadata":{}},{"cell_type":"markdown","source":"*k*-fold cross-validation is performed on the model and the table below documents how the model performs across each individual validation fold when *k*=3. We can see that the validation performance is stable and doesn't fluctuate much with ±0.085 in average validation loss and ±0.074 in validation accuracy across all folds. Hence we can say that the model is consistent across the trained dataset.","metadata":{}},{"cell_type":"markdown","source":"<a id='table'></a>\n\n| Fold | Avg Val Loss | Avg Val Accuracy |\n| --- | --- | --- |\n| 1 | 0.789 | 0.635 | \n| 2 | 0.721 | 0.675 | \n| 3 | 0.806 | 0.601 |","metadata":{}},{"cell_type":"markdown","source":"Finally calculate the average performance of the metrics over all the iterations","metadata":{}},{"cell_type":"code","source":"mean_val_loss = round(np.mean(val_loss_list), 3)\nmean_val_acc = round(np.mean(val_acc_list), 3)\n\nprint('Average validation loss over all folds: {}'.format(mean_val_loss))\nprint('Average validation accuracy over all folds: {}'.format(mean_val_acc))","metadata":{"papermill":{"duration":0.514332,"end_time":"2021-02-09T23:06:08.217489","exception":false,"start_time":"2021-02-09T23:06:07.703157","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-19T10:11:15.800897Z","iopub.execute_input":"2022-05-19T10:11:15.801282Z","iopub.status.idle":"2022-05-19T10:11:15.807879Z","shell.execute_reply.started":"2022-05-19T10:11:15.801235Z","shell.execute_reply":"2022-05-19T10:11:15.807074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"That's it!\n\n<span style=\"color:blue\">If you find this notebook helpful, please leave your feedback or any suggestions, and kindly upvote. Thanks!</span>","metadata":{}}]}