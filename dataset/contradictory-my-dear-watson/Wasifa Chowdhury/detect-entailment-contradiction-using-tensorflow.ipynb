{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction","metadata":{}},{"cell_type":"markdown","source":"The notebook is a step-by-step tutorial on using Transformer models for Natural Language Inferencing (NLI). This includes how to load, fine-tune, and evaluate M-BERT and XLM-RoBERTa models with Tensorflow.","metadata":{}},{"cell_type":"markdown","source":"Natural Language Inferencing (NLI) is an exciting NLP (Natural Language Processing) problem to identify the semantic relationship between two sentences. Given a hypothesis and premise sentence-pairs, the task is to determine whether the premise `entails` the hypothesis statement, `contradicts` it, or neither (`neutral`). \n\nFor more information on the problem, you can visit [Contradictory, My Dear Watson competition](https://www.kaggle.com/c/contradictory-my-dear-watson/overview)","metadata":{}},{"cell_type":"markdown","source":"# Load Libraries and Dependencies","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os\nimport sys\nfrom transformers import BertTokenizer, TFBertModel\nfrom tokenizers import BertWordPieceTokenizer\nfrom transformers import AutoTokenizer, AutoConfig, TFAutoModel    \nfrom transformers import XLMRobertaConfig, XLMRobertaTokenizer, TFXLMRobertaModel  \nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.utils import plot_model\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix, ConfusionMatrixDisplay\nfrom itertools import product\n\n\n# Handle Warnings: Optional\nos.environ[\"WANDB_API_KEY\"] = \"0\" # to silence warning\nos.environ[\"TF_XLA_FLAGS\"] = \"--tf_xla_enable_xla_devices\" # enable xla devices\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\" \n\nnp.random.seed(0) # For reproducibility\n\n# Make sure to install the right version of Python and Tensorflow for reproducible results\nprint(\"Python version: {}\".format(sys.version))\nprint(\"Tensorflow version: {}\".format(tf.__version__))","metadata":{"execution":{"iopub.status.busy":"2022-05-13T07:33:40.337357Z","iopub.execute_input":"2022-05-13T07:33:40.337652Z","iopub.status.idle":"2022-05-13T07:33:40.348215Z","shell.execute_reply.started":"2022-05-13T07:33:40.337622Z","shell.execute_reply":"2022-05-13T07:33:40.347189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Configure TPU Settings","metadata":{}},{"cell_type":"code","source":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept ValueError:\n    strategy = tf.distribute.get_strategy() # for CPU and single GPU\nprint('Number of Replicas:', strategy.num_replicas_in_sync)","metadata":{"execution":{"iopub.status.busy":"2022-05-13T07:33:47.123646Z","iopub.execute_input":"2022-05-13T07:33:47.123929Z","iopub.status.idle":"2022-05-13T07:33:55.780724Z","shell.execute_reply.started":"2022-05-13T07:33:47.123902Z","shell.execute_reply":"2022-05-13T07:33:55.780173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load CSV Data files with Pandas","metadata":{}},{"cell_type":"markdown","source":"The dataset contains train and test files that includes premise-hypothesis pairs in fifteen different languages. \n\nThe classification of the relationship between the premise and hypothesis statements is as follows:\n\n- label==`0` for `entailment`\n- label==`1` for `neutral`\n- label==`2` for `contradiction`\n\nYou can look at [competition website](https://www.kaggle.com/c/contradictory-my-dear-watson/data) for elaboration on the dataset.","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(\"../input/contradictory-my-dear-watson/train.csv\")\ntest = pd.read_csv(\"../input/contradictory-my-dear-watson/test.csv\")\n\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-13T05:58:10.002863Z","iopub.execute_input":"2022-05-13T05:58:10.003148Z","iopub.status.idle":"2022-05-13T05:58:10.10462Z","shell.execute_reply.started":"2022-05-13T05:58:10.003117Z","shell.execute_reply":"2022-05-13T05:58:10.103684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Exploration and Analysis","metadata":{}},{"cell_type":"markdown","source":"Explore data & drop any incomplete rows of data.\n\nFind how many data points and features are in the original, provided training dataset.","metadata":{}},{"cell_type":"code","source":"# print out stats about data\n\nmissing_values_count = train.isnull().sum() # we get the number of missing data points per column\nprint(\"Number of missing data points per column:\\n\")\nprint (missing_values_count)","metadata":{"execution":{"iopub.status.busy":"2022-05-13T05:58:10.105719Z","iopub.execute_input":"2022-05-13T05:58:10.106243Z","iopub.status.idle":"2022-05-13T05:58:10.123153Z","shell.execute_reply.started":"2022-05-13T05:58:10.106214Z","shell.execute_reply":"2022-05-13T05:58:10.122116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Identify any duplicate examples in the dataset","metadata":{}},{"cell_type":"code","source":"train[\"is_duplicate\"] = train.duplicated()\ntrain[train[\"is_duplicate\"]==True].count() ","metadata":{"execution":{"iopub.status.busy":"2022-05-13T05:58:10.12559Z","iopub.execute_input":"2022-05-13T05:58:10.125857Z","iopub.status.idle":"2022-05-13T05:58:10.161018Z","shell.execute_reply.started":"2022-05-13T05:58:10.125826Z","shell.execute_reply":"2022-05-13T05:58:10.160027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Drop the duplicated examples from the dataset before splitting the data into train-validation-test subsets. This prevents any accidental test or validation data leakage into the train subset.","metadata":{}},{"cell_type":"code","source":"train.drop_duplicates(keep=False, inplace=True, ignore_index=True)\ntrain.drop(\"is_duplicate\", axis=1, inplace=True) \nprint(\"Number of data examples after dropping duplicates: {} \\n\".format(train.shape[0]))","metadata":{"execution":{"iopub.status.busy":"2022-05-13T05:58:10.162416Z","iopub.execute_input":"2022-05-13T05:58:10.162834Z","iopub.status.idle":"2022-05-13T05:58:10.191347Z","shell.execute_reply.started":"2022-05-13T05:58:10.162792Z","shell.execute_reply":"2022-05-13T05:58:10.190493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's look at the distribution of languages in the training set.","metadata":{}},{"cell_type":"code","source":"train.language.unique()\ntrain.language.value_counts(normalize=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-13T05:58:10.192491Z","iopub.execute_input":"2022-05-13T05:58:10.19271Z","iopub.status.idle":"2022-05-13T05:58:10.204417Z","shell.execute_reply.started":"2022-05-13T05:58:10.192685Z","shell.execute_reply":"2022-05-13T05:58:10.203569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that more than half of the training examples are in English as data resources are abundant in this language. Rest of the data is fairly shared between other 14 languages.","metadata":{}},{"cell_type":"markdown","source":"Let's now visualize the distribution of class labels over the training data","metadata":{}},{"cell_type":"code","source":"# check distribution of target classes in the augmented data\ncounts = train['label'].value_counts()\n\nclass_labels = ['Entailment', 'Neutral', 'Contradiction']\n\ncounts_per_class = [counts[0], counts[1], counts[2]]\n\nplt.figure(figsize = (10,10))\nplt.pie(counts_per_class, labels = class_labels, autopct = '%1.1f%%')\nplt.title(\"Training Data\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-13T05:58:10.205898Z","iopub.execute_input":"2022-05-13T05:58:10.206652Z","iopub.status.idle":"2022-05-13T05:58:10.327045Z","shell.execute_reply.started":"2022-05-13T05:58:10.206608Z","shell.execute_reply":"2022-05-13T05:58:10.326095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the chart above, we can see that the training data is fairly balanced over the 3 classes.","metadata":{}},{"cell_type":"markdown","source":"# Split the Training Data","metadata":{}},{"cell_type":"markdown","source":"We will be splitting the training dataset into two parts - the data we will train the model with and a validation set. We stratify data during train-valid split to preserve the original distribution of the target classes.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain, validation = train_test_split(train, stratify=train.label.values, \n                                                  random_state=42, \n                                                  test_size=0.2, shuffle=True)\n\n\ntrain.reset_index(drop=True, inplace=True)\nvalidation.reset_index(drop=True, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-13T05:58:10.328713Z","iopub.execute_input":"2022-05-13T05:58:10.329035Z","iopub.status.idle":"2022-05-13T05:58:10.350075Z","shell.execute_reply.started":"2022-05-13T05:58:10.328994Z","shell.execute_reply":"2022-05-13T05:58:10.349365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check the number of rows and columns after split\nprint(\"Train data: {} \\n\".format(train.shape))\nprint(\"Validation data: {} \\n\".format(validation.shape))","metadata":{"execution":{"iopub.status.busy":"2022-05-13T05:58:10.351268Z","iopub.execute_input":"2022-05-13T05:58:10.352166Z","iopub.status.idle":"2022-05-13T05:58:10.358192Z","shell.execute_reply.started":"2022-05-13T05:58:10.352125Z","shell.execute_reply":"2022-05-13T05:58:10.357368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Implement M-BERT Model","metadata":{}},{"cell_type":"markdown","source":"The Multilingual BERT or M-BERT is a single language model pre-trained on the concatenation of monolingual Wikipedia corpora from 104 languages. We will fine-tune this pretrained model on our training dataset to get the predictions for textual entailment recognition.","metadata":{}},{"cell_type":"markdown","source":"## Set up M-BERT Tokenizer","metadata":{}},{"cell_type":"markdown","source":"A pretrained model only performs properly if we feed it an input that was tokenized with the same rules that were used to tokenize its training data. The BERT multilingual model does not perform any normalization on the input (no lower casing, accent stripping, or Unicode normalization). Hence we also follow the same rules when tokenizing input data for our task. For more information on data pre-processing, visit [M-BERT github](https://github.com/google-research/bert/blob/master/multilingual.md).","metadata":{}},{"cell_type":"code","source":"PRETRAINED_MODEL_TYPES = {}\nPRETRAINED_MODEL_TYPES['bert'] = (TFBertModel, BertTokenizer, 'bert-base-multilingual-cased')\nmodel_class, tokenizer_class, model_name = PRETRAINED_MODEL_TYPES['bert']\n\ntokenizer = BertTokenizer.from_pretrained(model_name) # Save the slow pretrained tokenizer\nsave_path = '.'\nif not os.path.exists(save_path):\n    os.makedirs(save_path)\ntokenizer.save_pretrained(save_path) # Save the loaded tokenizer locally\ntokenizer = BertWordPieceTokenizer(\"vocab.txt\", lowercase=False, strip_accents=False) # Load the fast tokenizer from saved file\n\ntokenizer","metadata":{"execution":{"iopub.status.busy":"2022-05-13T05:58:10.360833Z","iopub.execute_input":"2022-05-13T05:58:10.361605Z","iopub.status.idle":"2022-05-13T05:58:11.722164Z","shell.execute_reply.started":"2022-05-13T05:58:10.361564Z","shell.execute_reply":"2022-05-13T05:58:11.721245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's look at the sequence length distribution (e.g. number of tokens in a sequence) for the input data. We will need this information later when setting the `max_len` value since a machine learning algorithm requires all the inputs in a batch to have the same length.","metadata":{}},{"cell_type":"code","source":"def plot(df, tokenizer):\n    \"\"\"\n    Plot histogram of lengths of input sequences\n    \"\"\"\n    all_text = df.premise.values.tolist() + df.hypothesis.values.tolist() # list of string texts\n    all_text_tokenized = tokenizer.encode_batch(all_text) # list of encoding objects\n    all_tokenized_len = [len(encoding.tokens) for encoding in all_text_tokenized] # list of token lengths\n       \n    plt.hist(all_tokenized_len, bins=30, alpha=0.5)\n    plt.title(' Histogram of lengths of input sequences')\n    plt.xlabel('Number of tokens')\n    plt.ylabel('Count')\n\n    plt.show()\n\nplot(train, tokenizer)","metadata":{"execution":{"iopub.status.busy":"2022-05-13T05:58:11.72346Z","iopub.execute_input":"2022-05-13T05:58:11.723713Z","iopub.status.idle":"2022-05-13T05:58:12.907097Z","shell.execute_reply.started":"2022-05-13T05:58:11.723682Z","shell.execute_reply":"2022-05-13T05:58:12.906084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the histogram above, we can see that majority of the input sequences have less than 50 tokens.","metadata":{}},{"cell_type":"markdown","source":"We can also calculate the mean and max input sequence lengths per language.","metadata":{}},{"cell_type":"code","source":"tokenized_premise = tokenizer.encode_batch(train.premise.values.tolist()) # list of encoding objects\ntrain['premise_seq_length'] = [len(encoding.tokens) for encoding in tokenized_premise] # list of lengths\n    \ntokenized_hypothesis = tokenizer.encode_batch(train.hypothesis.values.tolist()) # list of encoding objects\ntrain['hypothesis_seq_length'] = [len(encoding.tokens) for encoding in tokenized_hypothesis] # list of lengths\n\n# Calculate max and avg sequence length per language\ninfo_per_lang = train.groupby('language').agg({'premise_seq_length': ['mean', 'max', 'count'], 'hypothesis_seq_length': ['mean', 'max', 'count']})\nprint (info_per_lang)","metadata":{"execution":{"iopub.status.busy":"2022-05-13T05:58:12.908647Z","iopub.execute_input":"2022-05-13T05:58:12.9092Z","iopub.status.idle":"2022-05-13T05:58:13.727901Z","shell.execute_reply.started":"2022-05-13T05:58:12.909155Z","shell.execute_reply":"2022-05-13T05:58:13.726871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above table, we can see that the length of premise sentences are greater than those of the hypothesis sentences for all languages. \n\nHence, let's visualize the mean sequence length distribution over the languages for the premise inputs.","metadata":{}},{"cell_type":"code","source":"column_name = info_per_lang.columns.values[0] #premise mean column\ninfo_per_lang[column_name].plot(kind='bar')","metadata":{"execution":{"iopub.status.busy":"2022-05-13T05:58:13.729288Z","iopub.execute_input":"2022-05-13T05:58:13.729505Z","iopub.status.idle":"2022-05-13T05:58:14.064895Z","shell.execute_reply.started":"2022-05-13T05:58:13.729479Z","shell.execute_reply":"2022-05-13T05:58:14.06385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The length should be large enough such that we don’t lose much data. Additionally, a very big number would make the model complex.\n\nSince most of the inputs are shorter than 50 words, we can consider length 50 for each input type of hypothesis and premise. \n\nHence we set `MAX_LEN`=100.\n\n*Note*: The `MAX_LEN` hyperparameter can be taken as a parameter to be tuned to get optimal results.","metadata":{}},{"cell_type":"markdown","source":"## Configure Hyperparameter Settings","metadata":{}},{"cell_type":"code","source":"# Configuration\nEPOCHS = 3\nBATCH_SIZE = 64 \nMAX_LEN = 100\nPATIENCE = 1\nLEARNING_RATE = 1e-5","metadata":{"execution":{"iopub.status.busy":"2022-05-13T05:58:14.066478Z","iopub.execute_input":"2022-05-13T05:58:14.066825Z","iopub.status.idle":"2022-05-13T05:58:14.073849Z","shell.execute_reply.started":"2022-05-13T05:58:14.066781Z","shell.execute_reply":"2022-05-13T05:58:14.073015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Encode Input Sequences","metadata":{}},{"cell_type":"markdown","source":"For BERT model, the input is represented in the following format:\n\n`CLS` Premise `SEP` Hypothesis `SEP`\n\nThe `CLS` and `SEP` are special tokens, where `CLS` is used in the beginning of a sequence for sentence-level classification while `SEP` separates the sentence pairs.\n\nWe encode the training data by vectorizing the input strings and applying padding and truncation using `MAX_LEN` value.\n\nThe encoded input will include - input word IDs, input masks, and input type IDs","metadata":{}},{"cell_type":"code","source":"def encode(df, tokenizer, max_len=50):\n    \"\"\"\n    Encode the input sequences to feed into the MBERT model. \n    Note that encode_batch() is used as 'BertWordPieceTokenizer' object has no attribute 'batch_encode_plus'\n    \"\"\"\n    pairs = df[['premise','hypothesis']].values.tolist()\n\n    tokenizer.enable_truncation(max_len)\n    tokenizer.enable_padding()\n    \n    print (\"Encoding...\")\n    enc_list = tokenizer.encode_batch(pairs)\n    print (\"Complete\")\n    \n    input_word_ids = tf.ragged.constant([enc.ids for enc in enc_list], dtype=tf.int32) #shape=[num_examples, max_len])\n    input_mask = tf.ragged.constant([enc.attention_mask for enc in enc_list], dtype=tf.int32) #shape=[num_examples, max_len]\n    input_type_ids = tf.ragged.constant([enc.type_ids for enc in enc_list], dtype=tf.int32) #shape=[num_examples, max_len]\n   \n    inputs = {\n        'input_word_ids': input_word_ids.to_tensor(),\n        'input_mask': input_mask.to_tensor(),\n        'input_type_ids': input_type_ids.to_tensor()}\n    \n    return inputs ","metadata":{"execution":{"iopub.status.busy":"2022-05-13T05:58:14.075374Z","iopub.execute_input":"2022-05-13T05:58:14.075828Z","iopub.status.idle":"2022-05-13T05:58:14.084682Z","shell.execute_reply.started":"2022-05-13T05:58:14.075795Z","shell.execute_reply":"2022-05-13T05:58:14.083622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_input = encode(train, tokenizer=tokenizer, max_len=MAX_LEN)\ntrain_ids = train_input['input_word_ids'] #[9696, max_len]\ntrain_mask = train_input['input_mask'] #[9696, max_len]\ntrain_type = train_input['input_type_ids'] #[9696, max_len]\ntrain_labels = train.label.values","metadata":{"execution":{"iopub.status.busy":"2022-05-13T05:58:14.086443Z","iopub.execute_input":"2022-05-13T05:58:14.086915Z","iopub.status.idle":"2022-05-13T05:58:27.39347Z","shell.execute_reply.started":"2022-05-13T05:58:14.086874Z","shell.execute_reply":"2022-05-13T05:58:27.39264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"validation_input = encode(validation, tokenizer=tokenizer, max_len=MAX_LEN)\nvalidation_ids = validation_input['input_word_ids'] #[num_examples, max_len]\nvalidation_mask = validation_input['input_mask'] #[num_examples, max_len]\nvalidation_type = validation_input['input_type_ids'] #[num_examples, max_len]\nvalidation_labels = validation.label.values #[num_examples]","metadata":{"execution":{"iopub.status.busy":"2022-05-13T05:58:27.395167Z","iopub.execute_input":"2022-05-13T05:58:27.395426Z","iopub.status.idle":"2022-05-13T05:58:30.582485Z","shell.execute_reply.started":"2022-05-13T05:58:27.395388Z","shell.execute_reply":"2022-05-13T05:58:30.581476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load and Process the Data in Batches","metadata":{}},{"cell_type":"code","source":"def create_dataset(features, labels, batch_size=BATCH_SIZE, validation=False):\n    \"\"\"\n    Load and process input data into batches using TF Dataset \n    \"\"\"\n    AUTO = tf.data.experimental.AUTOTUNE\n    dataset = tf.data.Dataset.from_tensor_slices((features, labels)).shuffle(len(features))\n    if validation:\n        dataset = dataset.batch(batch_size).prefetch(AUTO)\n    else:\n        dataset = dataset.repeat().batch(batch_size).prefetch(AUTO)\n    return dataset","metadata":{"execution":{"iopub.status.busy":"2022-05-13T05:58:30.583948Z","iopub.execute_input":"2022-05-13T05:58:30.584247Z","iopub.status.idle":"2022-05-13T05:58:30.590688Z","shell.execute_reply.started":"2022-05-13T05:58:30.584214Z","shell.execute_reply":"2022-05-13T05:58:30.589874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_data = create_dataset((train_ids, train_mask, train_type), train_labels, batch_size=BATCH_SIZE)\nvalidation_data = create_dataset((validation_ids, validation_mask, validation_type), validation_labels, batch_size=BATCH_SIZE, validation=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-13T05:58:30.591698Z","iopub.execute_input":"2022-05-13T05:58:30.591895Z","iopub.status.idle":"2022-05-13T05:58:30.612352Z","shell.execute_reply.started":"2022-05-13T05:58:30.591871Z","shell.execute_reply":"2022-05-13T05:58:30.61157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create and Train Model","metadata":{}},{"cell_type":"markdown","source":"We extract the hidden state vector of the 'CLS' token in the final BERT layer and pass that as input to the classification layer for further training.","metadata":{}},{"cell_type":"code","source":"def build_model(model_name, model_class, max_len=50, add_input_type=False):\n    \"\"\"\n    Define the model architecture\n    \"\"\"\n    \n    tf.random.set_seed(123) # For reproducibility\n    \n    # The bare XLM-RoBERTa Model transformer outputting raw hidden-states without any specific head on top.\n    encoder = model_class.from_pretrained(model_name)\n#     encoder = TFAutoModel.from_pretrained(model_name)\n    \n    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    input_type_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_type_ids\")\n    \n    # Extract final layer feature vectors\n    if add_input_type:\n        features = encoder([input_word_ids, input_mask, input_type_ids])[0] # shape=(batch_size, max_len, output_size)\n    else:\n        features = encoder([input_word_ids, input_mask])[0] # shape=(batch_size, max_len, output_size)\n    \n    # We pass the vector of only the [cls] token (at index=0) to the classification layer\n    sequence = features[:,0,:] #shape=(batch_size, output_size)\n   \n    # Add a classification layer\n    output = tf.keras.layers.Dense(3, activation=\"softmax\")(sequence)  \n    \n    if add_input_type:\n        model = tf.keras.Model(inputs=[input_word_ids, input_mask, input_type_ids], outputs=output)\n    else:\n        model = tf.keras.Model(inputs=[input_word_ids, input_mask], outputs=output)\n        \n    model.compile(tf.keras.optimizers.Adam(lr=LEARNING_RATE), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-05-13T05:58:30.613845Z","iopub.execute_input":"2022-05-13T05:58:30.614372Z","iopub.status.idle":"2022-05-13T05:58:30.625652Z","shell.execute_reply.started":"2022-05-13T05:58:30.614329Z","shell.execute_reply":"2022-05-13T05:58:30.624583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# instantiating the model in the strategy scope creates the model on the TPU\nwith strategy.scope():\n    model = build_model(model_name, model_class, MAX_LEN, add_input_type=True)\n    model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-05-13T05:58:30.627374Z","iopub.execute_input":"2022-05-13T05:58:30.627822Z","iopub.status.idle":"2022-05-13T05:59:39.646761Z","shell.execute_reply.started":"2022-05-13T05:58:30.627779Z","shell.execute_reply":"2022-05-13T05:59:39.645778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The model will be trained on the training subset and early-stopping will be applied on validation subset to avoid overfitting. The best model checkpoint will be saved after `EPOCHS` iterations.","metadata":{}},{"cell_type":"code","source":"checkpoint_filepath='bert_best_checkpoint.hdf5'\n# callbacks = [EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=PATIENCE), ModelCheckpoint(filepath=checkpoint_filepath, save_best_only=True, save_weights_only=True, monitor='val_accuracy', mode='max', verbose=1)]\ncallbacks = [EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=PATIENCE), ModelCheckpoint(filepath=checkpoint_filepath, save_best_only=True, save_weights_only=True, monitor='val_loss', mode='min', verbose=1)]\n\nn_steps = int(train_ids.shape[0]/BATCH_SIZE)\ntrain_history = model.fit(x=training_data, validation_data=validation_data, epochs=EPOCHS, verbose=1, steps_per_epoch=n_steps, callbacks=callbacks)","metadata":{"execution":{"iopub.status.busy":"2022-05-13T05:59:39.648147Z","iopub.execute_input":"2022-05-13T05:59:39.648402Z","iopub.status.idle":"2022-05-13T06:01:52.215035Z","shell.execute_reply.started":"2022-05-13T05:59:39.648371Z","shell.execute_reply":"2022-05-13T06:01:52.213836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Plot Training and Validation Losses over all Epochs","metadata":{}},{"cell_type":"code","source":"def plot_loss(history):\n    ''' Plot loss history '''\n    plt.plot(history.history['loss'], label='train loss')\n    plt.plot(history.history['val_loss'], label='validation loss')\n    plt.title('Average Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-13T06:01:52.216825Z","iopub.execute_input":"2022-05-13T06:01:52.217899Z","iopub.status.idle":"2022-05-13T06:01:52.225273Z","shell.execute_reply.started":"2022-05-13T06:01:52.217851Z","shell.execute_reply":"2022-05-13T06:01:52.224613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_loss(train_history)","metadata":{"execution":{"iopub.status.busy":"2022-05-13T06:01:52.2264Z","iopub.execute_input":"2022-05-13T06:01:52.22673Z","iopub.status.idle":"2022-05-13T06:01:52.49407Z","shell.execute_reply.started":"2022-05-13T06:01:52.226702Z","shell.execute_reply":"2022-05-13T06:01:52.493027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Plot Training and Validation Accuracies over all Epochs","metadata":{}},{"cell_type":"code","source":"def plot_acc(history):\n    ''' Plot accuracy history '''\n    plt.plot(history.history['accuracy'], label='train accuracy')\n    plt.plot(history.history['val_accuracy'], label='validation accuracy')\n    plt.title('Average Accuracy')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-13T06:01:52.495755Z","iopub.execute_input":"2022-05-13T06:01:52.496097Z","iopub.status.idle":"2022-05-13T06:01:52.503244Z","shell.execute_reply.started":"2022-05-13T06:01:52.496059Z","shell.execute_reply":"2022-05-13T06:01:52.502079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_acc(train_history)","metadata":{"execution":{"iopub.status.busy":"2022-05-13T06:01:52.505101Z","iopub.execute_input":"2022-05-13T06:01:52.505606Z","iopub.status.idle":"2022-05-13T06:01:52.751215Z","shell.execute_reply.started":"2022-05-13T06:01:52.505563Z","shell.execute_reply":"2022-05-13T06:01:52.750349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above plots, we can see that there is a huge gap between the training and validation losses, which suggests that the M-BERT model is not quite good at generalizing to unseen data. The M-BERT model gives final validation accuracy of around 64-66%. In the next section, we'll look at another model, namely XLM-RoBERTa, which improves the validation accuracy and is much better at predictions on new data.  ","metadata":{}},{"cell_type":"code","source":"del model #to free up space","metadata":{"execution":{"iopub.status.busy":"2022-05-13T06:01:52.752511Z","iopub.execute_input":"2022-05-13T06:01:52.752737Z","iopub.status.idle":"2022-05-13T06:01:52.757384Z","shell.execute_reply.started":"2022-05-13T06:01:52.752711Z","shell.execute_reply":"2022-05-13T06:01:52.756229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Resets all state generated by Keras\nK.clear_session()","metadata":{"execution":{"iopub.status.busy":"2022-05-13T06:01:52.763663Z","iopub.execute_input":"2022-05-13T06:01:52.764173Z","iopub.status.idle":"2022-05-13T06:01:52.772139Z","shell.execute_reply.started":"2022-05-13T06:01:52.764094Z","shell.execute_reply":"2022-05-13T06:01:52.771299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Implement XLM-RoBERTa Model","metadata":{}},{"cell_type":"markdown","source":"The XLM-RoBERTa is based on Facebook’s RoBERTa model released in 2019. It is a large multi-lingual language model, trained on 2.5TB of filtered CommonCrawl data.","metadata":{}},{"cell_type":"markdown","source":"## Set up the Tokenizer","metadata":{}},{"cell_type":"code","source":"PRETRAINED_MODEL_TYPES['xlmroberta'] = (TFXLMRobertaModel, XLMRobertaTokenizer, 'jplu/tf-xlm-roberta-large')\nmodel_class, tokenizer_class, model_name = PRETRAINED_MODEL_TYPES['xlmroberta']\n\n# Download vocabulary from huggingface.co and cache.\n# tokenizer = tokenizer_class.from_pretrained(model_name) \ntokenizer = AutoTokenizer.from_pretrained(model_name) #fast tokenizer\n\ntokenizer","metadata":{"execution":{"iopub.status.busy":"2022-05-13T06:01:52.773747Z","iopub.execute_input":"2022-05-13T06:01:52.773978Z","iopub.status.idle":"2022-05-13T06:01:57.118654Z","shell.execute_reply.started":"2022-05-13T06:01:52.773946Z","shell.execute_reply":"2022-05-13T06:01:57.117976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Encode Input Sequences","metadata":{}},{"cell_type":"markdown","source":"The encoded input will include - input word IDs and input masks, as required by the XLM-RoBERTa model.","metadata":{}},{"cell_type":"code","source":"def encode(df, tokenizer, max_len=50):\n    \"\"\"\n    Encode the input sequences to feed into the XLM-Roberta model\n    \"\"\"\n    pairs = df[['premise','hypothesis']].values.tolist() #shape=[num_examples]\n    \n    print (\"Encoding...\")\n    encoded_dict = tokenizer.batch_encode_plus(pairs, max_length=max_len, padding=True, truncation=True, \n                                               add_special_tokens=True, return_attention_mask=True)\n    print (\"Complete\")\n    \n    input_word_ids = tf.convert_to_tensor(encoded_dict['input_ids'], dtype=tf.int32) #shape=[num_examples, max_len])\n    input_mask = tf.convert_to_tensor(encoded_dict['attention_mask'], dtype=tf.int32) #shape=[num_examples, max_len]\n    \n    inputs = {\n        'input_word_ids': input_word_ids,\n        'input_mask': input_mask}    \n    \n    return inputs","metadata":{"execution":{"iopub.status.busy":"2022-05-13T06:01:57.119821Z","iopub.execute_input":"2022-05-13T06:01:57.120264Z","iopub.status.idle":"2022-05-13T06:01:57.126933Z","shell.execute_reply.started":"2022-05-13T06:01:57.120229Z","shell.execute_reply":"2022-05-13T06:01:57.126293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will use the same train-validation split and hyperparameter settings as in the previous BERT model for results to be comparable.","metadata":{}},{"cell_type":"code","source":"train_input = encode(train, tokenizer=tokenizer, max_len=MAX_LEN)\ntrain_ids = train_input['input_word_ids'] #[9696, max_len]\ntrain_mask = train_input['input_mask'] #[9696, max_len]\ntrain_labels = train.label.values","metadata":{"execution":{"iopub.status.busy":"2022-05-13T06:01:57.127987Z","iopub.execute_input":"2022-05-13T06:01:57.128634Z","iopub.status.idle":"2022-05-13T06:01:59.221999Z","shell.execute_reply.started":"2022-05-13T06:01:57.128598Z","shell.execute_reply":"2022-05-13T06:01:59.220934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"validation_input = encode(validation, tokenizer=tokenizer, max_len=MAX_LEN)\nvalidation_ids = validation_input['input_word_ids'] #[num_examples, max_len]\nvalidation_mask = validation_input['input_mask'] #[num_examples, max_len]\nvalidation_labels = validation.label.values #[num_examples]","metadata":{"execution":{"iopub.status.busy":"2022-05-13T06:01:59.223481Z","iopub.execute_input":"2022-05-13T06:01:59.223835Z","iopub.status.idle":"2022-05-13T06:01:59.716853Z","shell.execute_reply.started":"2022-05-13T06:01:59.223795Z","shell.execute_reply":"2022-05-13T06:01:59.716122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load and Process Data into Batches","metadata":{}},{"cell_type":"code","source":"training_data = create_dataset((train_ids, train_mask), train_labels, batch_size=BATCH_SIZE, validation=False)\nvalidation_data = create_dataset((validation_ids, validation_mask), validation_labels, batch_size=BATCH_SIZE, validation=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-13T06:01:59.718307Z","iopub.execute_input":"2022-05-13T06:01:59.719253Z","iopub.status.idle":"2022-05-13T06:01:59.736466Z","shell.execute_reply.started":"2022-05-13T06:01:59.719195Z","shell.execute_reply":"2022-05-13T06:01:59.735588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create and Train Model","metadata":{}},{"cell_type":"code","source":"# instantiating the model in the strategy scope creates the model on the TPU\nwith strategy.scope():\n    model = build_model(model_name, model_class, MAX_LEN)\n    model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-05-13T06:01:59.737876Z","iopub.execute_input":"2022-05-13T06:01:59.738158Z","iopub.status.idle":"2022-05-13T06:05:03.598351Z","shell.execute_reply.started":"2022-05-13T06:01:59.738128Z","shell.execute_reply":"2022-05-13T06:05:03.597434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_model(model, to_file='model.png', show_shapes=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-13T06:12:12.00264Z","iopub.execute_input":"2022-05-13T06:12:12.003282Z","iopub.status.idle":"2022-05-13T06:12:12.604408Z","shell.execute_reply.started":"2022-05-13T06:12:12.003233Z","shell.execute_reply":"2022-05-13T06:12:12.603305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's now train the built model on training data and monitor the performance of the model on the validation data","metadata":{}},{"cell_type":"code","source":"checkpoint_filepath='xlmroberta_best_checkpoint.hdf5'\n\n# callbacks = [EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=PATIENCE), ModelCheckpoint(filepath=checkpoint_filepath, save_best_only=True, save_weights_only=True, monitor='val_accuracy', mode='max', verbose=1)]\ncallbacks = [EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=PATIENCE), ModelCheckpoint(filepath=checkpoint_filepath, save_best_only=True, save_weights_only=True, monitor='val_loss', mode='min', verbose=1)]\n\nn_steps = int(train_ids.shape[0]/BATCH_SIZE)\ntrain_history = model.fit(x=training_data, validation_data=validation_data, epochs=EPOCHS, verbose=1, steps_per_epoch=n_steps, callbacks=callbacks)","metadata":{"execution":{"iopub.status.busy":"2022-05-13T06:05:04.864517Z","iopub.execute_input":"2022-05-13T06:05:04.864795Z","iopub.status.idle":"2022-05-13T06:09:59.301572Z","shell.execute_reply.started":"2022-05-13T06:05:04.864757Z","shell.execute_reply":"2022-05-13T06:09:59.300151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It's always useful to visualize the loss and accuracy history for training and validation sets.","metadata":{}},{"cell_type":"markdown","source":"## Plot Loss against Epochs","metadata":{}},{"cell_type":"code","source":"plot_loss(train_history)","metadata":{"execution":{"iopub.status.busy":"2022-05-13T06:09:59.304075Z","iopub.execute_input":"2022-05-13T06:09:59.304481Z","iopub.status.idle":"2022-05-13T06:09:59.567643Z","shell.execute_reply.started":"2022-05-13T06:09:59.304427Z","shell.execute_reply":"2022-05-13T06:09:59.566585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Plot Accuracy against Epochs","metadata":{}},{"cell_type":"code","source":"plot_acc(train_history)","metadata":{"execution":{"iopub.status.busy":"2022-05-13T06:09:59.569218Z","iopub.execute_input":"2022-05-13T06:09:59.569548Z","iopub.status.idle":"2022-05-13T06:10:00.675834Z","shell.execute_reply.started":"2022-05-13T06:09:59.569506Z","shell.execute_reply":"2022-05-13T06:10:00.674856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above plots we can clearly see that the gap between the training and validation losses has decreased by a large margin and best validation accuracy with XLM-RoBERTa model is ~79-80%.","metadata":{}},{"cell_type":"markdown","source":"To further evaluate the performance of the XLM-RoBERTa model, we'll generate the confusion matrix and classification report on the validation data.","metadata":{}},{"cell_type":"code","source":"# The function plot_confusion_matrix() is from scikit-learn’s website to plot the confusion matrix for multiclass. \n# link: https://scikit-learn.org/0.18/auto_examples/model_selection/plot_confusion_matrix.html\ndef plot_confusion_matrix(cm, classes,\n                        normalize=False,\n                        title='Confusion matrix',\n                        cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = np.around(cm.astype('float') / cm.sum(axis=1)[:, np.newaxis], 2)\n#         print(\"Normalized confusion matrix\")\n#     else:\n#         print('Confusion matrix, without normalization')\n\n#     print(cm)\n\n    thresh = cm.max() / 2.\n    for i, j in product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n            horizontalalignment=\"center\",\n            color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\n\nvalidation_predictions = [np.argmax(i) for i in model.predict(validation_input)] #predictions\nvalidation_labels = validation.label.values.tolist() #ground truth labels\n\ncm_plot_labels = ['entailment','neutral', 'contradiction']\ncm = confusion_matrix(y_true=validation_labels, y_pred=validation_predictions)\nplot_confusion_matrix(cm=cm, classes=cm_plot_labels, title='Confusion Matrix Without Normalization')\n# plot_confusion_matrix(cm=cm, classes=cm_plot_labels, title='Confusion Matrix With Normalization', normalize=True)\n\ntarget_class = ['entailment' if label==0 else 'neutral' if label==1 else 'contradiction' for label in validation_labels]\nprediction_class = ['entailment' if label==0 else 'neutral' if label==1 else 'contradiction' for label in validation_predictions]\nprint('\\nClassification Report')\nprint(classification_report(y_true=target_class, y_pred=prediction_class))","metadata":{"execution":{"iopub.status.busy":"2022-05-13T06:10:00.677828Z","iopub.execute_input":"2022-05-13T06:10:00.678568Z","iopub.status.idle":"2022-05-13T06:10:29.666244Z","shell.execute_reply.started":"2022-05-13T06:10:00.67851Z","shell.execute_reply":"2022-05-13T06:10:29.665307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It would also be interesting to look at the number of correct predictions per language.","metadata":{}},{"cell_type":"code","source":"def accuracy(x):\n    \"\"\" Function to print accuracy per language \"\"\"\n    return round(float(x[2]/x[1]), 2)*100\n\nvalidation['predictions'] = validation_predictions\n\n# Calculate the total number of examples per language\nlang_counts = validation.language.value_counts().sort_index()\n\n# Calculate the number of correct predictions per language\ntp_per_lang = validation[validation['label'] == validation['predictions']].groupby('language').agg({'language': ['count']}).sort_index()\n\nlang_names = lang_counts.index.tolist()\nlang_tuples = list(zip(lang_names, lang_counts.values.tolist(), tp_per_lang.iloc[:, 0].values.tolist()))\nacc = map(accuracy, lang_tuples)\nfor i, score in enumerate(acc):\n    print (\"Accuracy of {} is {} \".format(lang_tuples[i][0], score))","metadata":{"execution":{"iopub.status.busy":"2022-05-13T06:10:29.66781Z","iopub.execute_input":"2022-05-13T06:10:29.668099Z","iopub.status.idle":"2022-05-13T06:10:29.702964Z","shell.execute_reply.started":"2022-05-13T06:10:29.66804Z","shell.execute_reply":"2022-05-13T06:10:29.702131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Generate Predictions on Test Data","metadata":{}},{"cell_type":"markdown","source":"Once we are satisfied with our model's performance, we can get test-data predictions for submission.","metadata":{}},{"cell_type":"code","source":"# The model weights (that are considered the best) are loaded into the model.\nmodel.load_weights(checkpoint_filepath)","metadata":{"execution":{"iopub.status.busy":"2022-05-13T06:10:29.704212Z","iopub.execute_input":"2022-05-13T06:10:29.704464Z","iopub.status.idle":"2022-05-13T06:11:08.209995Z","shell.execute_reply.started":"2022-05-13T06:10:29.704434Z","shell.execute_reply":"2022-05-13T06:11:08.208802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#encode the test-input sequences and get predictions\ntest_input = encode(test, tokenizer=tokenizer, max_len=MAX_LEN)\npredictions = [np.argmax(i) for i in model.predict(test_input)]","metadata":{"execution":{"iopub.status.busy":"2022-05-13T06:11:08.212113Z","iopub.execute_input":"2022-05-13T06:11:08.213197Z","iopub.status.idle":"2022-05-13T06:11:09.411196Z","shell.execute_reply.started":"2022-05-13T06:11:08.213112Z","shell.execute_reply":"2022-05-13T06:11:09.410386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submit the Predictions","metadata":{}},{"cell_type":"code","source":"submission = test.id.copy().to_frame()\nsubmission['prediction'] = predictions\n\nsubmission.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-13T06:11:19.990406Z","iopub.execute_input":"2022-05-13T06:11:19.990692Z","iopub.status.idle":"2022-05-13T06:11:20.009744Z","shell.execute_reply.started":"2022-05-13T06:11:19.990658Z","shell.execute_reply":"2022-05-13T06:11:20.009109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv(\"submission.csv\", index = False)","metadata":{"execution":{"iopub.status.busy":"2022-05-13T06:11:20.010752Z","iopub.execute_input":"2022-05-13T06:11:20.011755Z","iopub.status.idle":"2022-05-13T06:11:20.037551Z","shell.execute_reply.started":"2022-05-13T06:11:20.011699Z","shell.execute_reply":"2022-05-13T06:11:20.036647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"That's it! The submission file has been created, for more information on how to submit to the competition, please visit the following [link](https://www.kaggle.com/c/contradictory-my-dear-watson/overview/evaluation).\n\n\n\n<span style=\"color:blue\">If you find this notebook helpful, please leave your feedback or any suggestions, and kindly upvote. Thanks!</span>","metadata":{}}]}