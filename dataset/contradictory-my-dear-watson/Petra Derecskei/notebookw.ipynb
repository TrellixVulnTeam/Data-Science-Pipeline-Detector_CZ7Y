{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import classification_report\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install sentence_transformers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.utils.data import DataLoader\n#import math\nfrom sentence_transformers import LoggingHandler, util\nfrom sentence_transformers.cross_encoder import CrossEncoder\nfrom sentence_transformers.cross_encoder.evaluation import CESoftmaxAccuracyEvaluator\nimport logging\nfrom datetime import datetime\nimport os\nimport csv","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(\"../input/contradictory-my-dear-watson/train.csv\")\ntest_df = pd.read_csv(\"../input/contradictory-my-dear-watson/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeRegressor\ny = train_df[\"id\"]\nfeatures = [\"lang_abv\",\"language\",\"label\"]\nX = train_df[features]\nX","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label2int = {\"contradiction\": 0, \"entailment\": 1, \"neutral\":  2}\ntrain_samples = []\ndev_samples = []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install math","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport pandas.core as pc\nimport pandas as pd\nimport numpy as np\nimport argparse\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install dateutil\n!pip install pandas","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import math","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install math","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%capture\n!pip install datasets==1.0.2\n!pip install transformers\n\nimport datasets\nimport transformers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"T = train_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from cs231n.classifiers import LinearSVM","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svm = LinearSVM()\ntic = time.time()\nloss_hist = svm.train(X_train, y_train, learning_rate=1e-7, reg=2.5e4,num_iters=1500, verbose=True)\ntoc = time.time()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import BertTokenizer, BertForSequenceClassification\nimport torch\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n\ninputs = tokenizer(\"/Dokumentumok/tok.txt\", return_tensors=\"pt\")\nlabels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\noutputs = model(**inputs, labels=labels)\nloss = outputs.loss\nlogits = outputs.logits","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\nimport torch\n\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\nmodel = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n\ninputs = tokenizer(\"/Dokumentumok/tok.txt\", return_tensors=\"pt\")\nlabels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\noutputs = model(**inputs, labels=labels)\nloss = outputs.loss\nlogits = outputs.logits","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport argparse\npred = pd.DataFrame()\n\n\ninput_value = train_df['premise']\nimput_value2 = train_df['hypothesis']\nlabel = train_df['label']\ninput_values= [input_value,imput_value2,label]\ntrain_samples = []\ndev_samples = []\ntrain_data = train_df\ntrain_data['label'] = train_data['label'].replace([0,2],[2,0])\nfor id, row in train_data.iterrows():\n    label_id = int(row['label'])\n    train_samples.append(input_values)\n    \ntrain_batch_size = 16\nnum_epochs = 4\n\n#model = CrossEncoder('sentence_transformers/distilbert-base-nli-stsb-mean-tokens', num_labels=len(label2int))#\n#model = CrossEncoder('sentence-transformers/distilbert-multilingual-nli-stsb-quora-ranking', num_labels=len(label2int))\n#model = CrossEncoder('sentence-transformers/xlm-r-100langs-bert-base-nli-mean-tokens', num_labels=len(label2int))\n\n\nmodel = CrossEncoder('joeddav/xlm-roberta-large-xnli', num_labels=len(label2int))\ntrain_dataloader = DataLoader(train_samples, shuffle=True, batch_size=train_batch_size)\n\nevaluator = CESoftmaxAccuracyEvaluator.from_input_examples(dev_samples, name='ALLNLI-dev')\n\nup_steps = math.ceil(len(train_dataloader)* num_epochs * 0.1)\nlogging.info(r\"Warmup-steps: {}\".format(up_steps))\n\n#Train_the model\nmodel.fit(train_dataloader=train_dataloader,\n         evaluator=evaluator,\n         epochs = num_epochs,\n         evaluation_steps=10000,\n         warmup_steps=up_steps)\n        # output_path=model_save_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install pandas","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import BertTokenizer, BertForMultipleChoice\nimport torch\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertForMultipleChoice.from_pretrained('bert-base-uncased')\n\nbatch_size = 1\nprompt = train_df.shape\nchoice0 = \"premise\"\nchoice1 = \"hypothesis\"\nlabels = torch.tensor(0).unsqueeze(0)  # choice0 is correct (according to Wikipedia ;)), batch size 1\n\nencoding = tokenizer([[prompt, prompt], [choice0, choice1]], return_tensors='pt', padding=True)\noutputs = model(**{k: v.unsqueeze(0) for k,v in encoding.items()}, labels=labels)  # batch size is 1\n\n# the linear classifier still needs to be trained\nloss = outputs.loss\nlogits = outputs.logits","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_dataset = '../input/contradictory-my-dear-watson/test.csv'\ndf = pd.read_csv(test_dataset)\nsentence_pairs =[]\nids = []\nfor id, row in df.iterrows():\n    label_id = 0\n    ids.append(row['id'])\n    sentence_pairs.append([row['premise'], row['hypothesis']])\n    \npred_scores = model.predict(sentence_pairs, convert_to_numpy=True, show_progress_bar=False, batch_size=4)\npred_labels = np.argmax(pred_scores, axis=1)\npred_labels\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nout_df = pd.DataFrame([ids, pred_labels]).transpose()\nout_df = out_df.rename(columns={0: 'id', 1: 'predictio'})\nout_df ['prediction'] = out_df['prediction'].replace([2,0], [0,2])\nout_df.to_csv('submission.csv',index=False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}