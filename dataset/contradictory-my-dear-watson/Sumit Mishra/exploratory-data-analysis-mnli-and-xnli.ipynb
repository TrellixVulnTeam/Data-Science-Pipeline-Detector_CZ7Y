{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Importing the Libraries\n!pip install --quiet googletrans\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom wordcloud import WordCloud\nfrom nltk.corpus import stopwords\nimport time\nfrom string import punctuation\nfrom nltk.util import ngrams\nfrom nltk import word_tokenize\nfrom collections import Counter\nfrom spacy import displacy\nfrom googletrans import Translator\nfrom tqdm import tqdm\nfrom textblob import TextBlob\nimport warnings\n\nwarnings.filterwarnings('ignore')\ntrans = Translator()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/contradictory-my-dear-watson/train.csv\")\ndf_mnli = pd.read_csv('../input/back-mnli-xnli/df_mnli.csv')\ndf_xnli = pd.read_csv('../input/back-mnli-xnli/df_xnli.csv')\n\ndf = pd.concat([df, df_mnli, df_xnli], ignore_index = True)\ndf['hypothesis'] = df['hypothesis'].astype(str)\ndf.drop_duplicates(subset = ['premise', 'hypothesis'], inplace = True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### zh - Chinese, en - English, fr - French, es - Spanish, ar - Arabic, sw - Swahili, ur - Urdu, vi - Vietnamese, ru - Russian, hi - Hindi, el - Greek, th - Thai, de - German, tr - Turkish, bg - Bulgarian"},{"metadata":{},"cell_type":"markdown","source":"### Label Distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Label Distribution in the dataset\nplt.rcParams['figure.figsize'] = [10, 10.5]\nsns.set(style = 'darkgrid', font_scale = 1.2)\nlabel_sns = sns.countplot(df['label'], palette = 'spring')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Language Distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Languages given in the dataset\nplt.rcParams['figure.figsize'] = [15, 10]\nsns.set(style = 'darkgrid', font_scale = 1.2)\nlang_sns = sns.countplot(df['language'], palette = 'winter')\nplt.setp(lang_sns.get_xticklabels(), rotation = 45);\nprint (\"Max : \", df['lang_abv'].value_counts().max())\nprint (\"Min : \", df['lang_abv'].value_counts().min())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Part of Speech Tagging (English)"},{"metadata":{},"cell_type":"markdown","source":"For Part of speech tagging of other languages, visit the documentation and download the model for the particular language and load as given in the next code cell  - https://spacy.io/usage/models"},{"metadata":{"trusted":true},"cell_type":"code","source":"import spacy\nsp = spacy.load('en_core_web_sm')\n\ndef POS_tag(sentence):\n    sen = sp(sentence)\n    print (sentence, \"\\n\")\n    for word in sen:\n        print(f'{word.text:{12}} {word.pos_:{10}} {word.tag_:{8}} {spacy.explain(word.tag_)}')\n    displacy.render(sen, style='dep', jupyter=True, options={'distance': 85})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"examples = df.loc[df.lang_abv == 'en', 'premise']\nexamples = list(examples)\nsen_1 = examples[0]\nsen_2 = examples[1]\nsen_3 = examples[2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"POS_tag(sen_1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"POS_tag(sen_2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"POS_tag(sen_3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Sentiment (Polarity), Subjectivity and Objectivity of the Sentences"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Translating both Premise and Hypothesis in English for polarity and subjectivity (takes a long time to run)\n\n# converted_premise = []\n# converted_hypothesis = []\n# for i in tqdm(range(len(df))):\n#     if df['lang_abv'][i] != 'en':\n#         converted_text_p = trans.translate(df['premise'][i]).text\n#         converted_text_h = trans.translate(df['hypothesis'][i]).text\n#         converted_premise.append(converted_text_p)\n#         converted_hypothesis.append(converted_text_h)\n#     else:\n#         converted_premise.append(df['premise'][i])\n#         converted_hypothesis.append(df['hypothesis'][i])\n        \n\n# df['premise_conv'] = converted_premise\n# df['hypothesis_conv'] = converted_hypothesis\n\n# df.head(10)\n\npremise_eng = df.loc[df.lang_abv == 'en', 'premise'].values.tolist()\nhypothesis_eng = df.loc[df.lang_abv == 'en', 'hypothesis'].values.tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"li_pol_p = []\nli_sub_p = []\nfor i in tqdm(premise_eng):\n    text = TextBlob(i)\n    pol = text.sentiment.polarity\n    sub = text.sentiment.subjectivity\n    li_pol_p.append(pol)\n    li_sub_p.append(sub)\n    \nli_pol_h = []\nli_sub_h = []\nfor i in tqdm(hypothesis_eng):\n    text = TextBlob(i)\n    pol = text.sentiment.polarity\n    sub = text.sentiment.subjectivity\n    li_pol_h.append(pol)\n    li_sub_h.append(sub)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Polarity Distribution of Premise and Hypothesis\nplt.rcParams['figure.figsize'] = [20, 8]\nsns.set(style = 'whitegrid')\n\nplt.subplot(1, 2, 1)\nsns.distplot(li_pol_p, bins = 20, color = 'darkturquoise')\nplt.title('Polarity of Premise')\nplt.xlabel('Polarity')\nplt.ylabel('Frequency')\n\nplt.subplot(1, 2, 2)\nsns.distplot(li_pol_h, bins = 20, color = 'orange', kde_kws={'bw': 0.1})\nplt.title('Polarity of Hypothesis')\nplt.xlabel('Polarity');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Subjectivity Distribution of Premise and Hypothesis\nplt.rcParams['figure.figsize'] = [20, 8]\nsns.set(style = 'whitegrid')\n\nplt.subplot(1, 2, 1)\nsns.distplot(li_sub_p, bins = 20, color = 'darkturquoise')\nplt.title('Subjectivity of Premise')\nplt.xlabel('Subjectivity')\nplt.ylabel('Frequency')\n\nplt.subplot(1, 2, 2)\nsns.distplot(li_sub_h, bins = 20, color = 'orange')\nplt.title('Subjectivity of Hypothesis')\nplt.xlabel('Subjectivity');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Word Clouds"},{"metadata":{},"cell_type":"markdown","source":"#### Word Cloud for English"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Word Cloud Representation for the sentences in English\nfig, ax = plt.subplots(2, 1, figsize=(30, 15))\nstop_words_en = set(stopwords.words('english'))\n\nenglish_sent = df.loc[df.lang_abv == 'en', ['premise', 'hypothesis']].copy()\nenglish_sent['premise'] = english_sent['premise'].str.lower()\nenglish_sent['hypothesis'] = english_sent['hypothesis'].str.lower()\n\n# Removing the stopwords just for the visualization\nenglish_sent['premise'] = english_sent['premise'].apply(lambda x: \" \".join(word for word in x.split() if word not in stop_words_en))\nenglish_sent['hypothesis'] = english_sent['hypothesis'].apply(lambda x: \" \".join(word for word in x.split() if word not in stop_words_en))\n\ntext_hypo = (' '.join(english_sent['hypothesis']))\ntext = (' '.join(english_sent['premise']))\n\n# WORDCLOUD\ncloud = WordCloud(width = 1000, height = 500, background_color = 'white').generate(text)\nax[0].axis(\"off\")\nax[0].title.set_text(\"Premise\")\nax[0].imshow(cloud)\ntime.sleep(0.01)\n\ncloud_1 = WordCloud(width = 1000, height = 500, background_color = 'white').generate(text_hypo)\nax[1].axis(\"off\")\nax[1].title.set_text(\"Hypothesis\")\nax[1].imshow(cloud_1);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Word Cloud for Spanish"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Word Cloud Representation for the sentences in Spanish\nfig, ax = plt.subplots(2, 1, figsize=(30, 15))\nstop_words_es = set(stopwords.words('spanish'))\n\nenglish_sent = df.loc[df.lang_abv == 'es', ['premise', 'hypothesis']].copy()\nenglish_sent['premise'] = english_sent['premise'].str.lower()\nenglish_sent['hypothesis'] = english_sent['hypothesis'].str.lower()\n\n# Removing the stopwords just for the visualization\nenglish_sent['premise'] = english_sent['premise'].apply(lambda x: \" \".join(word for word in x.split() if word not in stop_words_es))\nenglish_sent['hypothesis'] = english_sent['hypothesis'].apply(lambda x: \" \".join(word for word in x.split() if word not in stop_words_es))\n\ntext_hypo = (' '.join(english_sent['hypothesis']))\ntext = (' '.join(english_sent['premise']))\n\n# WORDCLOUD\ncloud = WordCloud(width = 1000, height = 500, background_color = 'white').generate(text)\nax[0].axis(\"off\")\nax[0].title.set_text(\"Premise\")\nax[0].imshow(cloud)\ntime.sleep(0.01)\n\ncloud_1 = WordCloud(width = 1000, height = 500, background_color = 'white').generate(text_hypo)\nax[1].axis(\"off\")\nax[1].title.set_text(\"Hypothesis\")\nax[1].imshow(cloud_1);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Word Cloud for French"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Word Cloud Representation for the sentences in French\nfig, ax = plt.subplots(2, 1, figsize=(30, 15))\nstop_words_fr = set(stopwords.words('french'))\n\nenglish_sent = df.loc[df.lang_abv == 'es', ['premise', 'hypothesis']].copy()\nenglish_sent['premise'] = english_sent['premise'].str.lower()\nenglish_sent['hypothesis'] = english_sent['hypothesis'].str.lower()\n\n# Removing the stopwords just for the visualization\nenglish_sent['premise'] = english_sent['premise'].apply(lambda x: \" \".join(word for word in x.split() if word not in stop_words_fr))\nenglish_sent['hypothesis'] = english_sent['hypothesis'].apply(lambda x: \" \".join(word for word in x.split() if word not in stop_words_fr))\n\ntext_hypo = (' '.join(english_sent['hypothesis']))\ntext = (' '.join(english_sent['premise']))\n\n# WORDCLOUD\ncloud = WordCloud(width = 1000, height = 500, background_color = 'white').generate(text)\nax[0].axis(\"off\")\nax[0].title.set_text(\"Premise\")\nax[0].imshow(cloud)\ntime.sleep(0.01)\n\ncloud_1 = WordCloud(width = 1000, height = 500, background_color = 'white').generate(text_hypo)\nax[1].axis(\"off\")\nax[1].title.set_text(\"Hypothesis\")\nax[1].imshow(cloud_1);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Ngrams"},{"metadata":{},"cell_type":"markdown","source":"#### Unigrams"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Premise Unigrams\nfig, ax = plt.subplots(6, 2, figsize=(25, 50))\nsns.set(style = 'whitegrid')\nplt.rcParams['font.sans-serif'] = ['Source Han Sans TW', 'sans-serif']\nlanguages = list(df.language.unique())\nlanguages = [l for l in languages if l not in ('Thai', 'Chinese', 'Hindi')]\n\nfor i, lang in enumerate(languages):\n    lang_premise = df.loc[df.language == lang, 'premise']\n    text_ = ' '.join(lang_premise)\n    # Removing the punctuations\n    text = ''.join(word for word in text_ if word not in punctuation)\n    # Unigrams\n    unigram = ngrams(word_tokenize(text), 1)\n    count = Counter(unigram)\n    temp_df = pd.DataFrame(count.most_common(10))\n    # Plot\n    lng_plot = sns.barplot(x = temp_df[1], y = temp_df[0], ax = ax.flatten()[i], palette = 'spring')\n    ax.flatten()[i].set(xlabel='Count', ylabel='Unigrams', title = lang)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Hypothesis Unigrams\nfig, ax = plt.subplots(6, 2, figsize=(25, 50))\nsns.set(style = 'whitegrid')\nplt.rcParams['font.sans-serif'] = ['Source Han Sans TW', 'sans-serif']\n\nfor i, lang in enumerate(languages):\n    lang_premise = df.loc[df.language == lang, 'hypothesis']\n    text_ = ' '.join(lang_premise)\n    # Removing the punctuations\n    text = ''.join(word for word in text_ if word not in punctuation)\n    # Unigrams\n    unigram = ngrams(word_tokenize(text), 1)\n    count = Counter(unigram)\n    temp_df = pd.DataFrame(count.most_common(10))\n    # Plot\n    lng_plot = sns.barplot(x = temp_df[1], y = temp_df[0], ax = ax.flatten()[i], palette = 'winter')\n    ax.flatten()[i].set(xlabel='Count', ylabel='Unigrams', title = lang)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Bigrams"},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Premise Bigrams\n# fig, ax = plt.subplots(6, 2, figsize=(25, 50))\n# sns.set(style = 'whitegrid')\n# plt.rcParams['font.sans-serif'] = ['Source Han Sans TW', 'sans-serif']\n\n# for i, lang in enumerate(languages):\n#     lang_premise = df.loc[df.language == lang, 'premise']\n#     text_ = ' '.join(lang_premise)\n#     # Removing the punctuations\n#     text = ''.join(word for word in text_ if word not in punctuation)\n#     # Bigrams\n#     bigram = ngrams(word_tokenize(text), 2)\n#     count = Counter(bigram)\n#     temp_df = pd.DataFrame(count.most_common(10))\n#     # Plot\n#     lng_plot = sns.barplot(x = temp_df[1], y = temp_df[0], ax = ax.flatten()[i])\n#     ax.flatten()[i].set(xlabel='Count', ylabel='Bigrams', title = lang)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Hypothesis Bigrams\n# fig, ax = plt.subplots(6, 2, figsize=(25, 50))\n# sns.set(style = 'whitegrid')\n# plt.rcParams['font.sans-serif'] = ['Source Han Sans TW', 'sans-serif']\n\n# for i, lang in enumerate(languages):\n#     lang_premise = df.loc[df.language == lang, 'hypothesis']\n#     text_ = ' '.join(lang_premise)\n#     # Removing the punctuations\n#     text = ''.join(word for word in text_ if word not in punctuation)\n#     # Bigrams\n#     bigram = ngrams(word_tokenize(text), 2)\n#     count = Counter(bigram)\n#     temp_df = pd.DataFrame(count.most_common(10))\n#     # Plot\n#     lng_plot = sns.barplot(x = temp_df[1], y = temp_df[0], ax = ax.flatten()[i])\n#     ax.flatten()[i].set(xlabel='Count', ylabel='Bigrams', title = lang)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Trigrams"},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Premise Trigrams\n# fig, ax = plt.subplots(6, 2, figsize=(25, 50))\n# sns.set(style = 'whitegrid')\n# plt.rcParams['font.sans-serif'] = ['Source Han Sans TW', 'sans-serif']\n\n# for i, lang in enumerate(languages):\n#     lang_premise = df.loc[df.language == lang, 'premise']\n#     text_ = ' '.join(lang_premise)\n#     # Removing the punctuations\n#     text = ''.join(word for word in text_ if word not in punctuation)\n#     # Trigrams\n#     trigram = ngrams(word_tokenize(text), 3)\n#     count = Counter(trigram)\n#     temp_df = pd.DataFrame(count.most_common(10))\n#     # Plot\n#     lng_plot = sns.barplot(x = temp_df[1], y = temp_df[0], ax = ax.flatten()[i])\n#     ax.flatten()[i].set(xlabel='Count', ylabel='Trigrams', title = lang)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Hypothesis Trigrams\n# fig, ax = plt.subplots(6, 2, figsize=(25, 50))\n# sns.set(style = 'whitegrid')\n# plt.rcParams['font.sans-serif'] = ['Source Han Sans TW', 'sans-serif']\n\n# for i, lang in enumerate(languages):\n#     lang_premise = df.loc[df.language == lang, 'hypothesis']\n#     text_ = ' '.join(lang_premise)\n#     # Removing the punctuations\n#     text = ''.join(word for word in text_ if word not in punctuation)\n#     # Trigrams\n#     trigram = ngrams(word_tokenize(text), 3)\n#     count = Counter(trigram)\n#     temp_df = pd.DataFrame(count.most_common(10))\n#     # Plot\n#     lng_plot = sns.barplot(x = temp_df[1], y = temp_df[0], ax = ax.flatten()[i])\n#     ax.flatten()[i].set(xlabel='Count', ylabel='Trigrams', title = lang)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Word Frequency Plot (English)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Word Frequecy plot after removing stopwords in Premise\n\ntext_premise = \" \".join(df.loc[df.lang_abv == 'en', 'premise'])\ntext = ''.join(word for word in text_premise if word not in punctuation)\nt = \" \".join(word for word in text.split() if word not in stop_words_en)\ntokenized_premise = word_tokenize(t)\nfrequency_premise = Counter(tokenized_premise)\ndf_2 = pd.DataFrame(frequency_premise.most_common(30))\n\nsns.set(font_scale = 1.2, style = 'whitegrid')\nplt.rcParams['figure.figsize'] = [12, 14]\n\nword_count_2 = sns.barplot(x = df_2[1], y = df_2[0], color = 'mediumorchid')\nword_count_2.set(title = 'Top 30 most common words in the Premise (English)', xlabel = 'Frequency',\n                ylabel = 'Words');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Word Frequecy plot after removing stopwords in Hypothesis\n\ntext_hypothesis = \" \".join(df.loc[df.lang_abv == 'en', 'hypothesis'])\ntext_h = ''.join(word for word in text_hypothesis if word not in punctuation)\nt_h = \" \".join(word for word in text_h.split() if word not in stop_words_en)\ntokenized_hypothesis = word_tokenize(t_h)\nfrequency_hypothesis = Counter(tokenized_hypothesis)\ndf_2 = pd.DataFrame(frequency_hypothesis.most_common(30))\n\nsns.set(font_scale = 1.2, style = 'whitegrid')\nplt.rcParams['figure.figsize'] = [12, 14]\n\nword_count_2 = sns.barplot(x = df_2[1], y = df_2[0], color = 'mediumseagreen')\nword_count_2.set(title = 'Top 30 most common words in Hypothesis (English)', xlabel = 'Frequency',\n                ylabel = 'Words');","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}