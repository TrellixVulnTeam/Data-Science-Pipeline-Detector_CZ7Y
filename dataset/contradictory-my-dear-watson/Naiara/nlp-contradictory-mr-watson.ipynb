{"cells":[{"metadata":{},"cell_type":"markdown","source":"# NLP: Contradictory, Mr. Watson\n\nWe are going to analyse Contradictor, Mr. Watson competition from Kaggle"},{"metadata":{},"cell_type":"markdown","source":"## Index\n\n- [1. Import libraries and download data](#section1)\n- [2. Dataframe Analysis](#section2)\n- [3. Model](#section3)\n"},{"metadata":{},"cell_type":"markdown","source":"## 1. Import libraries and download data <a id='section1'></a>"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport string\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nimport nltk\nfrom nltk.corpus import stopwords\n\nimport sys\nfrom textblob import TextBlob \n\nimport re\n\nfrom IPython.display import display\nimport plotly.express as px\nimport spacy\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Input data files are available in the read-only \"../input/\" directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = '/kaggle/input/contradictory-my-dear-watson/'\n\ntrain = pd.read_csv(path + 'train.csv')\ntest = pd.read_csv(path + 'test.csv')\nsample_submission = pd.read_csv(path + 'sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Dataframe Analysis <a id='section2'></a>\n \n We are going to analyse the data that composes the different dataframes."},{"metadata":{},"cell_type":"markdown","source":"### 2.1. Shape and head"},{"metadata":{},"cell_type":"markdown","source":"- train\n\nTrain set is composed by 6 columns (5 features and 1 target) and 12120 rows. "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(\"  - Train: \\ntrain shape:\", train.shape)\nprint(\"Head:\")\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Test\n\nTest set is composed 5 columns (5 features) and  5191 rows."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(\"  - Test: \\ntest shape:\", test.shape)\nprint(\"Head:\")\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.2 Type of features and Nan values"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observing the previous values, train and test sets do not contain any NAN value, and the features' type\nis string and the target is integer (categorical variable)."},{"metadata":{},"cell_type":"markdown","source":"### 2.3 Features and Target\n\nIn this section, we want to visualise the variables, in order to have an idea what type of data is.\n"},{"metadata":{},"cell_type":"markdown","source":"- Target (label)\n\nWe draw a barplot that show us the frequency of each value that contain the target."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig, axes = plt.subplots(nrows=1,ncols=1, figsize=(12,5))\ntrain['label'].value_counts().plot.bar(ax=axes)\nplt.title('label - target')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Meaning of the label:\n\n0-> entailment\n\n1-> neutral\n\n2-> contradiction"},{"metadata":{},"cell_type":"markdown","source":"Looking at the plot, we see that the target has quite similar representation of each three groups of target."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- language\n\nThe data is multilingual, therefore we would like to see what type of languages and what proportion are represented. In order to see it, we plot one frequency barplot for train set and another for test. In addition, it is good to observe whether both sets have similar distribution."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def plot_table(data, name_data):\n    \n    pd_language_prob = round(pd.DataFrame(data.language.value_counts()).transpose()/data['language'].count(),2)\n    \n    lang = pd_language_prob.columns.tolist()\n    prob = pd_language_prob.values.tolist()[0]\n\n    table = [['prob'+'('+i+')',j] for (i,j) in zip(lang,prob)]\n\n    fig = plt.figure()\n    \n    # definitions for the axes\n    left, width = 0.10, 1.5\n    bottom, height = 0.1, .8\n    bottom_h = left_h = left + width + 0.02\n\n    rect_cones = [left, bottom, width, height]\n    rect_box = [left_h, bottom, 0.17, height]\n\n    # plot\n    ax1 = plt.axes(rect_cones)\n    data.groupby('language')['language'].agg(['count']).plot.bar(ax=ax1)\n   \n  \n    plt.title(\"Frequency of languages \" + name_data + \" set\")\n    \n    ax2 = plt.axes(rect_box)\n    my_table = ax2.table(cellText = table, loc ='right')\n    my_table.set_fontsize(40)\n    my_table.scale(4,4)\n    ax2.axis('off')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"\nplot_table(train,'train')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plot_table(test,'test')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The dataframes contain 15 languages. The highest frequency language is English. and the others 14 have the same  proportion of representation. Moreover, observing the distribution of both datasets, they are equal."},{"metadata":{},"cell_type":"markdown","source":"- languages and label\n\nNow we focus on how the target is distributed in the different languages. We plot a barplot, with three variables  every language that symbolise the three labels: entailment, neutral and contradiction."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"pd_language_label = pd.DataFrame(train[['language','label']].groupby(['language','label'])['label'].count())\nfig, axes = plt.subplots(nrows=1,ncols=1, figsize=(12,9))\npd_language_label.unstack().plot.barh(ax=axes)\nplt.title('label vs language')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The distribution of the three labels accross the different languages is quite uniform, in every language has the same behaviour."},{"metadata":{},"cell_type":"markdown","source":"#### 2.3.1. English\n\nIn this section, we focuss on the entries in English language only. After that, we study the main features: Hypothesis and Premise, because they will be used as \"inputs\" for our model. We try to visualise some variables in order to find some patterns."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_English = train[train['language']=='English']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Create an empty model\nnlp = spacy.load('en')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"class removing():\n    '''Clean text'''\n    def __init__(self):\n        self.text = texto\n    def lower(texto):\n        return(str(texto).lower())\n    def remove_url(texto):\n        return(re.sub(r'http://\\S+|https://\\S+','', texto))\n    def remove_emoji(texto):\n        emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n        return emoji_pattern.sub(r'', texto)\n    def remove_punctuation(texto):\n        return(re.sub(r'[^\\w\\s]','',texto))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"# cleaning text from premise and hypothesis and we create two new features\ndf_English.loc[:,'premise_modify'] = df_English.apply(lambda x: removing.lower(x.premise), axis=1)\ndf_English.loc[:,'premise_modify'] = df_English.apply(lambda x:removing.remove_url(x.premise_modify), axis=1)\ndf_English.loc[:,'premise_modify'] = df_English.apply(lambda x: removing.remove_punctuation(x.premise_modify),axis=1)\ndf_English.loc[:,'premise_modify'] = df_English.apply(lambda x: removing.remove_emoji(x.premise_modify), axis=1)\n\ndf_English.loc[:,'hypothesis_modify'] = df_English.apply(lambda x:removing.lower(x.hypothesis), axis=1)\ndf_English.loc[:,'hypothesis_modify'] = df_English.apply(lambda x:removing.remove_url(x.hypothesis_modify), axis=1)\ndf_English.loc[:,'hypothesis_modify'] = df_English.apply(lambda x: removing.remove_punctuation(x.hypothesis_modify), axis=1)\ndf_English.loc[:,'hypothesis_modify'] = df_English.apply(lambda x: removing.remove_emoji(x.hypothesis_modify), axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- number of words' histograms\n\nWe modify the text for premise and hypothesis because we only want to count the words, so we eliminate the punctuation and another symbols. Futhermore, we draw 6 histograms, distinguishin between the three labels (row) and between premise and hypothesis (column)."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def count_words(df, feature):\n    num_words = [] \n    for i in df[feature]:\n        aux = nlp(i)\n        num_words.append(len(aux))\n    return(num_words)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df_English.loc[:,'num_words_premise'] = count_words(df_English,'premise_modify')\ndf_English.loc[:,'num_words_hypothesis'] = count_words(df_English,'hypothesis_modify')\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(nrows=3,ncols=2, figsize=(12,8))\nplt.subplots_adjust(hspace = 0.5)\ndf_English[df_English.label == 0].num_words_premise.plot.hist(bins=40,  ax = axes[0][0], label = 'Fake', color='blue')\naxes[0][0].set_title('# Words Premise Histogram (Entailment)')\ndf_English[df_English.label == 0].num_words_hypothesis.plot.hist(bins=40,  ax = axes[0][1], label = 'Fake', color='orange')\naxes[0][1].set_title('# Words Hypothesis Histogram (Entailment)')\n\ndf_English[df_English.label == 1].num_words_premise.plot.hist(bins=40,  ax = axes[1][0], label = 'Fake', color='blue')\naxes[1][0].set_title('# Words Premise Histogram (Neutral)')\ndf_English[df_English.label == 1].num_words_hypothesis.plot.hist(bins=40,  ax = axes[1][1], label = 'Fake', color='orange')\naxes[1][1].set_title('# Words Hypothesis Histogram (Neutral)')\n\ndf_English[df_English.label == 2].num_words_premise.plot.hist(bins=40,  ax = axes[2][0], label = 'Fake', color='blue')\naxes[2][0].set_title('# Words Premise Histogram (Contradiction)')\ndf_English[df_English.label == 1].num_words_hypothesis.plot.hist(bins=40,  ax = axes[2][1], label = 'Fake', color='orange')\naxes[2][1].set_title('# Words Hypothesis Histogram (Contradiction)')\nplt.show()\n\n# dataframe\n\nmean_premise = []\nstd_premise = []\nmean_hypothesis = []\nstd_hypothesis = []\nfor i in range(3):\n    for j in ['num_words_premise', 'num_words_hypothesis']:\n        mean_aux = round(df_English[df_English.label == i][j].mean(),2)\n        std_aux = round(df_English[df_English.label == i][j].std(),2)\n        if j == 'num_words_premise':\n            mean_premise.append(mean_aux)\n            std_premise.append(std_aux)\n        else:\n            mean_hypothesis.append(mean_aux)\n            std_hypothesis.append(std_aux)\n\nindex_list = ['Entailment', 'Neutral', 'Contradiction']\n\npd_aux = pd.DataFrame({'mean_premise': mean_premise, 'std_premise':std_premise, \n                           'mean_hypothesis': mean_hypothesis, 'std_hypothesis': std_hypothesis}, index = index_list)\n\n\npd_aux","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Comparing the previous histograms, premise histograms have similar distribution between them. The same is true for the hypothesis histograms. Therefore, we can not observe any diffence between three labels, since they have similar behaviour in terms of number of words."},{"metadata":{},"cell_type":"markdown","source":"- stopwords\n\nWe would like to see how the stopwords are distributed, for this we select the most 20 popular stopwords and they are plotted in treemaps, distinguishing the different labels (entailment, neutral and contradiction) for premise and hypothesis. "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"stop = stopwords.words('english')\nclass stop_words():\n    def __init__(self):\n        self.text = texto\n    def get_stopwords(texto):\n        return(' '.join([x for x in texto.split(' ') if x in stop]))\n    def remove_stopwords(texto):\n        return(' '.join([x for x in texto.split(' ') if not x in stop]))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df_English.loc[:, 'stopwords_premise'] = df_English.premise_modify.apply(lambda x: stop_words.get_stopwords(x))\ndf_English.loc[:, 'stopwords_hypothesis'] = df_English.hypothesis_modify.apply(lambda x: stop_words.get_stopwords(x))\ndf_English.loc[:, 'premise_notstop'] = df_English.premise_modify.apply(lambda x: stop_words.remove_stopwords(x))\ndf_English.loc[:, 'hypothesis_notstop'] = df_English.hypothesis_modify.apply(lambda x: stop_words.remove_stopwords(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"\ndef count_list(list_pass):\n        '''you pass a str'''\n        count = {}\n        for word in list_pass:\n            if word in count :\n                count[word] += 1\n            else:\n                count[word] = 1\n        return(count)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def draw_treemap(serie, title_name):\n    '''It is a function that draw a treemap with the most 20 popular words from one serie\n    Note: the variable you have to pass is df[[feature]] and title of the plot \n    '''\n    list_aux = [w.split(' ') for w in serie]\n    list_aux = [item for sublist in list_aux for item in sublist]\n    dict_aux = count_list(list_aux)\n    df_aux = pd.DataFrame({'words': list(dict_aux.keys()), 'number':list(dict_aux.values())})\n    df_aux = df_aux.sort_values(by=['number'], ascending=False)[0:20]\n\n    fig = px.treemap(df_aux, path=['words'], values='number', width=900, height=400, title=title_name)\n    fig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#print('STOPWORDS_PREMISE:')\ndraw_treemap(df_English[df_English['label']==0].stopwords_premise, 'Treemap - StopWords_Premise(Entailment)(the 20 most popular)')\ndraw_treemap(df_English[df_English['label']==1].stopwords_premise, 'Treemap - StopWords_Premise(Neutral)(the 20 most popular)')\ndraw_treemap(df_English[df_English['label']==2].stopwords_premise, 'Treemap - StopWords_Premise(Contradiction)(the 20 most popular)')\n#print('-------------- STOPWORDS_hypothesis --------------')\ndraw_treemap(df_English[df_English['label']==0].stopwords_hypothesis, 'Treemap - StopWords_Hypothesis(Entailment)(the 20 most popular)')\ndraw_treemap(df_English[df_English['label']==1].stopwords_hypothesis, 'Treemap - StopWords_Hypothesis(Neutral)(the 20 most popular)')\ndraw_treemap(df_English[df_English['label']==2].stopwords_hypothesis, 'Treemap - StopWords_Hypothesis(Contradiction)(the 20 most popular)')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We observe that all treemaps contain the same words in a very similar proportions."},{"metadata":{},"cell_type":"markdown","source":"### 3. Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom transformers import AutoTokenizer, TFAutoModel, BertTokenizer, TFBertModel\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model \nfrom tensorflow.keras.callbacks import ModelCheckpoint\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TPU \ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    print('Running on TPU', tpu.master())\nexcept ValueError:\n    strategy = tf.distribute.get_strategy()\nprint(\"REPLICAS:\", strategy.num_replicas_in_sync)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#variables \nmodel_name = 'bert-base-multilingual-cased'\n\nmax_len = 40\n#batch size depend on replica\nbatch_size = 16 * strategy.num_replicas_in_sync\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained(model_name)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Preparing the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_valid, y_train, y_valid = train_test_split(train, \n                                                      train.label.values, \n                                                      test_size = 0.2, \n                                                     random_state = 245)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_text = X_train[['hypothesis', 'premise']].values.tolist()\nval_text = X_valid[['hypothesis', 'premise']].values.tolist()\ntest_text = test[['hypothesis', 'premise']].values.tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preparing_data(data):\n    '''this function you pass data that is a list with premise and hypothesis and \n        they have to be encoded in order to use in BERT model\n        RETURN: a dictionary with keys: 'input_ids', 'token_type_ids', 'attention_mask'\n    '''\n    # encoded the words (assign a number in every word) and also add '[SEP]'token between premise and hypothesis\n    # and add '[CLS]'token the begining of inputs\n    # and using padding in order to unify the length of the vectors (adding 0's)\n    text_encoded = tokenizer.batch_encode_plus(data, pad_to_max_length = True)\n    #create a dictionary with tokens\n    dict_input = {}\n    #convert list to tensor\n    dict_input['input_ids'] = tf.convert_to_tensor(text_encoded.input_ids)\n    dict_input['token_type_ids'] = tf.convert_to_tensor(text_encoded.token_type_ids)\n    dict_input['attention_mask'] = tf.convert_to_tensor(text_encoded.attention_mask)\n    return(dict_input)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_input = preparing_data(train_text)\nval_input = preparing_data(val_text)\ntest_input  = preparing_data(test_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with strategy.scope():\n    bert_encoder = TFBertModel.from_pretrained(model_name)\n    input_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name='input_ids')\n    attention_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name='attention_mask')\n    token_type_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name='token_type_ids')\n    \n    embedding = bert_encoder([input_ids, attention_mask, token_type_ids])[0]\n    output = tf.keras.layers.Dense(3, activation='softmax')(embedding[:,0,:])\n    \n    model = tf.keras.Model(inputs=[input_ids, attention_mask, token_type_ids], outputs=output)\n    model.compile(tf.keras.optimizers.Adam(lr=1e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n    \n   \n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_history = model.fit(train_input, y_train,\n                          epochs = 10, \n                          batch_size = 64,\n                          validation_data = (val_input, y_valid),\n                          verbose = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_history_df = pd.DataFrame(model_history.history, index = range(1, len(model_history.history['loss'])+1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(nrows=1,ncols=1, figsize=(12,5))\nmodel_history_df.plot(style=['b-','r-','b--','r--'], ax = axes)\nplt.title('Model Accuracy and Loss')\naxes.set_xlabel('epochs')\naxes.set_ylabel('Accuracy - Loss')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For the model, we use some hyperparameters, as example:\n    - max_len = 40\n    - batch_size = 64\n    - epochs = 10\n    \nWe plot the Accuracy and the Loss with respect to the accuracy. We observe that the Accuracy increases and the Loss decreases when we increase the number of epochs, however val_acuracy flattens after epoch = 2 whereas val_loss keeps increasing. This indicates that the model is overfitting after this value, therefore we select epoch = 2 for the final submission. "},{"metadata":{"trusted":true},"cell_type":"code","source":"model_history = model.fit(train_input, y_train,\n                          epochs = 2, \n                          batch_size = 64,\n                          validation_data = (val_input, y_valid),\n                          verbose = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predict on test\ntest_preds = model.predict(test_input, verbose = 1)\nsample_submission['prediction'] = test_preds.argmax(axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission.to_csv('submission.csv', index=False)\nsample_submission.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## References\n\nhttps://www.kaggle.com/kksienc/comprehensive-nlp-tutorial-3-bert"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}