{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import tensorflow as tf\ntry:\n   tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  \n   print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\nexcept ValueError:\n   tpu = None\nif tpu:\n   tf.config.experimental_connect_to_cluster(tpu)\n   tf.tpu.experimental.initialize_tpu_system(tpu)\n   strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n   strategy = tf.distribute.get_strategy()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"!pip install nlp\n!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n!python pytorch-xla-env-setup.py --version nightly  --apt-packages libomp5 libopenblas-dev","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n%autosave 60\n\nimport os\nos.environ['XLA_USE_BF16'] = \"1\"\nos.environ['XLA_TENSOR_ALLOCATOR_MAXSIZE'] = '100000000'\n\nimport gc\ngc.enable()\nimport time\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm \n\nimport nlp\nimport transformers\nfrom transformers import (AdamW, \n                          XLMRobertaTokenizer, \n                          XLMRobertaModel, \n                          get_cosine_schedule_with_warmup)\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.utils.data.distributed import DistributedSampler\n\nimport torch_xla\nimport torch_xla.core.xla_model as xm\nimport torch_xla.debug.metrics as met\nimport torch_xla.distributed.parallel_loader as pl\nimport torch_xla.distributed.xla_multiprocessing as xmp\nimport torch_xla.utils.serialization as xser\nimport torch_xla.version as xv\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nprint('PYTORCH:', xv.__torch_gitrev__)\nprint('XLA:', xv.__xla_gitrev__)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv('../input/contradictory-my-dear-watson/test.csv')\nsample_submission = pd.read_csv('../input/contradictory-my-dear-watson/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TRAIN_BATCH_SIZE = 16\nVALID_BATCH_SIZE = 16\nEPOCHS = 3\nMAX_LEN = 80\n# Scale learning rate to 8 TPU's\nLR = 2e-5 * xm.xrt_world_size() \nMETRICS_DEBUG = True\ntokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-large')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# encoding\ndef convert_to_features(batch):\n    input_pairs = list(zip(batch['premise'], batch['hypothesis']))\n    encodings = tokenizer.batch_encode_plus(input_pairs, \n                                            add_special_tokens=True, \n                                            pad_to_max_length=True, \n                                            max_length=MAX_LEN, \n                                            truncation=True, \n                                            return_attention_mask=True, \n                                            return_token_type_ids=True)\n    return encodings","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"test_dataset = nlp.load_dataset('csv', data_files=['../input/contradictory-my-dear-watson/test.csv'])['train']\ndrop_columns = test_dataset.column_names\nencoded_test_dataset = test_dataset.map(convert_to_features, batched=True, remove_columns=drop_columns)\nencoded_test_dataset.set_format(\"torch\", columns=['attention_mask', 'input_ids', 'token_type_ids']) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class XLMRoberta(nn.Module):\n    def __init__(self, num_labels, multisample):\n        super(XLMRoberta, self).__init__()\n        output_hidden_states = False\n        self.num_labels = num_labels\n        self.multisample= multisample\n        self.roberta = XLMRobertaModel.from_pretrained(\"xlm-roberta-large\", \n                                                       output_hidden_states=output_hidden_states, \n                                                       num_labels=1)\n        self.layer_norm = nn.LayerNorm(1024*2)\n        self.dropout = nn.Dropout(p=0.2)\n        self.high_dropout = nn.Dropout(p=0.5)        \n        self.classifier = nn.Linear(1024*2, self.num_labels)\n    \n    def forward(self,\n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None):\n        outputs = self.roberta(input_ids,\n                               attention_mask=attention_mask,\n                               token_type_ids=token_type_ids,\n                               position_ids=position_ids,\n                               head_mask=head_mask,\n                               inputs_embeds=inputs_embeds)\n        average_pool = torch.mean(outputs[0], 1)\n        max_pool, _ = torch.max(outputs[0], 1)\n        concatenate_layer = torch.cat((average_pool, max_pool), 1)\n        normalization = self.layer_norm(concatenate_layer)\n        if self.multisample:\n            # Multisample Dropout\n            logits = torch.mean(\n                torch.stack(\n                    [self.classifier(self.dropout(normalization)) for _ in range(5)],\n                    dim=0,\n                ),\n                dim=0,\n            )\n        else:\n            logits = self.dropout(normalization)\n            logits = self.classifier(logits)       \n        outputs = logits\n        return outputs  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TEST_BATCH_SIZE = 32\n\ntest_data_loader = DataLoader(\n    encoded_test_dataset, \n    batch_size=TEST_BATCH_SIZE,\n    drop_last=False,\n    num_workers=4,\n    shuffle=False\n)\n\nWRAPPED_MODEL = xmp.MpModelWrapper(XLMRoberta(num_labels=3, multisample=False))\n\ndevice = xm.xla_device()\nmodel = WRAPPED_MODEL.to(device).eval()\nmodel.load_state_dict(torch.load(\"../input/contradictorywatsonpublicxlmroberta/model.bin\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_preds = []\n\nfor i, data in tqdm(enumerate(test_data_loader), total=len(test_data_loader)):\n    ids = data[\"input_ids\"]\n    mask = data[\"attention_mask\"]\n    type_ids = data[\"token_type_ids\"]\n    ids = ids.to(device, dtype=torch.long)\n    mask = mask.to(device, dtype=torch.long)\n    type_ids = type_ids.to(device, dtype=torch.long)\n    outputs = model(\n        input_ids = ids,\n        attention_mask = mask,\n        token_type_ids = type_ids\n    )\n    outputs_np = outputs.cpu().detach().numpy().tolist()\n    test_preds.extend(outputs_np)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_preds = torch.FloatTensor(test_preds)\ntop1_prob, top1_label = torch.topk(test_preds, 1)\ny = top1_label.cpu().detach().numpy()\nsample_submission.prediction = y\nsample_submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}