{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Check TPU is available","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\ntry:\n   tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  \n   print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\nexcept ValueError:\n   tpu = None\nif tpu:\n   tf.config.experimental_connect_to_cluster(tpu)\n   tf.tpu.experimental.initialize_tpu_system(tpu)\n   strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n   strategy = tf.distribute.get_strategy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Setup Dependencies","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"!pip install git+https://github.com/ssut/py-googletrans.git\n!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n!python pytorch-xla-env-setup.py --version nightly  --apt-packages libomp5 libopenblas-dev","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"%%time\n%autosave 60\n\nimport os\nos.environ['XLA_USE_BF16'] = \"1\"\nos.environ['XLA_TENSOR_ALLOCATOR_MAXSIZE'] = '100000000'\n\nimport gc\ngc.enable()\nimport time\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm \nfrom googletrans import Translator\nfrom dask import bag, diagnostics\n\nimport transformers\nfrom transformers import (AdamW, \n                          DistilBertTokenizer, \n                          DistilBertModel, \n                          DistilBertTokenizerFast,                          \n                          get_cosine_schedule_with_warmup)\nfrom tokenizers import BertWordPieceTokenizer\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.utils.data.distributed import DistributedSampler\n\nimport torch_xla\nimport torch_xla.core.xla_model as xm\nimport torch_xla.debug.metrics as met\nimport torch_xla.distributed.parallel_loader as pl\nimport torch_xla.distributed.xla_multiprocessing as xmp\nimport torch_xla.utils.serialization as xser\nimport torch_xla.version as xv\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nprint('PYTORCH:', xv.__torch_gitrev__)\nprint('XLA:', xv.__xla_gitrev__)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Files","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/contradictory-my-dear-watson/train.csv')\ntest = pd.read_csv('../input/contradictory-my-dear-watson/test.csv')\nsample_submission = pd.read_csv('../input/contradictory-my-dear-watson/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Text Augmentation\n\n##### References - [JohnM's - Agmenting Data with Translations Kernel](https://www.kaggle.com/jpmiller/augmenting-data-with-translations)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def translate(words):\n    translator = Translator()\n    decoded = translator.translate(words, dest='en').text\n    return decoded\n\nother_langs = train.loc[train.lang_abv != \"en\"].copy()\n\n#TODO: use a dask dataframe instead of bags\npremise_bag = bag.from_sequence(other_langs.premise.tolist()).map(translate)\nhypo_bag =  bag.from_sequence(other_langs.hypothesis.tolist()).map(translate)\nwith diagnostics.ProgressBar():\n    premises = premise_bag.compute()\n    hypos = hypo_bag.compute()\n    \n    \nother_langs[['premise', 'hypothesis']] = list(zip(premises, hypos))\ntrain = train.append(other_langs)\ntrain.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Dataset Factory","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class DatasetRetriever(Dataset):\n    def __init__(self, df, ids, mask):\n        self.df = df\n        self.ids = ids\n        self.mask = mask\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):   \n        ids = self.ids[index]\n        mask = self.mask[index]\n        targets = self.df.iloc[index].label\n        return {\n            'ids':torch.tensor(ids),\n            'mask':torch.tensor(mask),\n            'targets':targets\n        }","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model Factory","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class DistillBERT(nn.Module):\n    def __init__(self, num_labels, multisample):\n        super(DistillBERT, self).__init__()\n        output_hidden_states = True\n        self.num_labels = num_labels\n        self.multisample= multisample\n        self.distillbert = DistilBertModel.from_pretrained(\"distilbert-base-multilingual-cased\", \n                                                           output_hidden_states=output_hidden_states,\n                                                           num_labels=1)\n        self.layer_norm = nn.LayerNorm(768*2)\n        self.dropout = nn.Dropout(p=0.2)\n        self.high_dropout = nn.Dropout(p=0.5)        \n        self.classifier = nn.Linear(768*2, self.num_labels)\n    \n    def forward(self,\n        input_ids=None,\n        attention_mask=None,\n        head_mask=None,\n        inputs_embeds=None):\n        outputs = self.distillbert(input_ids,\n                                   attention_mask=attention_mask,\n                                   head_mask=head_mask,\n                                   inputs_embeds=inputs_embeds)\n        average_pool = torch.mean(outputs[0], 1)\n        max_pool, _ = torch.max(outputs[0], 1)\n        concatenate_layer = torch.cat((average_pool, max_pool), 1)\n        normalization = self.layer_norm(concatenate_layer)\n        if self.multisample:\n            # Multisample Dropout\n            logits = torch.mean(\n                torch.stack(\n                    [self.classifier(self.dropout(normalization)) for _ in range(5)],\n                    dim=0,\n                ),\n                dim=0,\n            )\n        else:\n            logits = self.dropout(normalization)\n            logits = self.classifier(logits)       \n        outputs = F.log_softmax(logits, dim=1)\n        return outputs  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Metrics Factory","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class AverageMeter(object):\n    def __init__(self, name, fmt=':f'):\n        self.name = name\n        self.fmt = fmt\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n    def __str__(self):\n        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n        return fmtstr.format(**self.__dict__)\n\nclass ProgressMeter(object):\n    def __init__(self, num_batches, meters, prefix=\"\"):\n        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n        self.meters = meters\n        self.prefix = prefix\n\n    def display(self, batch):\n        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n        entries += [str(meter) for meter in self.meters]\n        print('\\t'.join(entries))\n\n    def _get_batch_fmtstr(self, num_batches):\n        num_digits = len(str(num_batches // 1))\n        fmt = '{:' + str(num_digits) + 'd}'\n        return '[' + fmt + '/' + fmt.format(num_batches) + ']'\n\ndef accuracy(output, target, topk=(1,)):\n    with torch.no_grad():\n        maxk = max(topk)\n        batch_size = target.size(0)\n\n        _, pred = output.topk(maxk, 1, True, True)\n        pred = pred.t()\n        correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n        res = []\n        for k in topk:\n            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n            res.append(correct_k.mul_(100.0 / batch_size))\n        return res","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Optimizer Factory","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_model_optimizer(model):\n    # Differential Learning Rate\n    def is_backbone(name):\n        return \"distillbert\" in name\n    \n    optimizer_grouped_parameters = [\n       {'params': [param for name, param in model.named_parameters() if is_backbone(name)], 'lr': LR},\n       {'params': [param for name, param in model.named_parameters() if not is_backbone(name)], 'lr': 1e-3} \n    ]\n    \n    optimizer = AdamW(\n        optimizer_grouped_parameters, lr=LR, weight_decay=1e-2\n    )\n    \n    return optimizer","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Loss Factory","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def loss_fn(outputs, targets):\n    return nn.NLLLoss()(outputs, targets)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Training","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_loop_fn(train_loader, model, optimizer, device, scheduler, epoch=None):\n    # Train\n    batch_time = AverageMeter('Time', ':6.3f')\n    data_time = AverageMeter('Data', ':6.3f')\n    losses = AverageMeter('Loss', ':.4e')\n    top1 = AverageMeter('Acc@1', ':6.2f')\n    progress = ProgressMeter(\n        len(train_loader),\n        [batch_time, data_time, losses, top1],\n        prefix=\"[xla:{}]Train:  Epoch: [{}]\".format(xm.get_ordinal(), epoch)\n    )\n    model.train()\n    end = time.time()\n    for i, data in enumerate(train_loader):\n        data_time.update(time.time()-end)\n        ids = data[\"ids\"]\n        mask = data[\"mask\"]\n        targets = data[\"targets\"]\n        ids = ids.to(device, dtype=torch.long)\n        mask = mask.to(device, dtype=torch.long)\n        targets = targets.to(device, dtype=torch.float)\n        optimizer.zero_grad()\n        outputs = model(\n            input_ids = ids,\n            attention_mask = mask\n        )\n        loss = loss_fn(outputs, targets)\n        loss.backward()\n        xm.optimizer_step(optimizer)\n        loss = loss_fn(outputs, targets)\n        acc1= accuracy(outputs, targets, topk=(1,))\n        losses.update(loss.item(), ids.size(0))\n        top1.update(acc1[0].item(), ids.size(0))\n        scheduler.step()\n        batch_time.update(time.time() - end)\n        end = time.time()\n        if i % 30 == 0:\n            progress.display(i)\n    del loss\n    del outputs\n    del ids\n    del mask\n    del targets\n    gc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Evaluation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def eval_loop_fn(validation_loader, model, device):\n    #Validation\n    model.eval()\n    batch_time = AverageMeter('Time', ':6.3f')\n    losses = AverageMeter('Loss', ':.4e')\n    top1 = AverageMeter('Acc@1', ':6.2f')\n    learning_rate = AverageMeter('LR',':2.8f')\n    progress = ProgressMeter(\n        len(validation_loader),\n        [batch_time, losses, top1],\n        prefix='[xla:{}]Validation: '.format(xm.get_ordinal()))\n    with torch.no_grad():\n        end = time.time()\n        for i, data in enumerate(validation_loader):\n            ids = data[\"ids\"]\n            mask = data[\"mask\"]\n            targets = data[\"targets\"]\n            ids = ids.to(device, dtype=torch.long)\n            mask = mask.to(device, dtype=torch.long)\n            targets = targets.to(device, dtype=torch.float)\n            outputs = model(\n                input_ids = ids,\n                attention_mask = mask\n            )\n            loss = loss_fn(outputs, targets)\n            acc1= accuracy(outputs, targets, topk=(1,))\n            losses.update(loss.item(), ids.size(0))\n            top1.update(acc1[0].item(), ids.size(0))\n            batch_time.update(time.time() - end)\n            end = time.time()\n            if i % 10 == 0:\n                progress.display(i)\n    del loss\n    del outputs\n    del ids\n    del mask\n    del targets\n    gc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Config","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"##### References - [Xhlulu's - Jigsaw TPU: DistilBERT with Huggingface and Keras Kernel](https://www.kaggle.com/xhlulu/jigsaw-tpu-distilbert-with-huggingface-and-keras)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def fast_encode(df, fast_tokenizer):\n    fast_tokenizer.enable_truncation(max_length=MAX_LEN)\n    fast_tokenizer.enable_padding(max_length=MAX_LEN)\n    \n    text = list(zip(df.premise, df.hypothesis))\n    encoded = fast_tokenizer.encode_batch(\n        text\n    )\n    \n    all_ids = []\n    all_masks = []\n    all_ids.extend([enc.ids for enc in encoded])\n    all_masks.extend([enc.attention_mask for enc in encoded])\n    \n    return np.array(all_ids), np.array(all_masks)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"TRAIN_BATCH_SIZE = 16\nVALID_BATCH_SIZE = 16\nEPOCHS = 40\nMAX_LEN = 80\n# Scale learning rate to 8 TPU's\nLR = 2e-5 * xm.xrt_world_size() \nMETRICS_DEBUG = True\n\nWRAPPED_MODEL = xmp.MpModelWrapper(DistillBERT(num_labels=3, multisample=False))\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-multilingual-cased')\n# Save the loaded tokenizer locally\ntokenizer.save_pretrained('.')\n# Reload it with the huggingface tokenizers library\nfast_tokenizer = BertWordPieceTokenizer('vocab.txt', lowercase=False)\n\n# Train Validation Split\nmask = np.random.rand(len(train)) < 0.95\ntrain_df = train[mask]\nvalid_df = train[~mask]\n\ntrain_ids, train_mask = fast_encode(train_df, fast_tokenizer)\nvalid_ids, valid_mask = fast_encode(valid_df, fast_tokenizer)\n\ntrain_dataset = DatasetRetriever(df=train_df, ids=train_ids, mask=train_mask)\nvalid_dataset = DatasetRetriever(df=valid_df, ids=valid_ids, mask=valid_mask)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Run","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def _run():\n    xm.master_print('Starting Run ...')\n    train_sampler = DistributedSampler(\n        train_dataset,\n        num_replicas=xm.xrt_world_size(),\n        rank=xm.get_ordinal(),\n        shuffle=False\n    )\n    \n    train_data_loader = DataLoader(\n        train_dataset,\n        batch_size=TRAIN_BATCH_SIZE,\n        sampler=train_sampler,\n        drop_last=True,\n        num_workers=0\n    )\n    xm.master_print('Train Loader Created.')\n    \n    valid_sampler = DistributedSampler(\n        valid_dataset,\n        num_replicas=xm.xrt_world_size(),\n        rank=xm.get_ordinal(),\n        shuffle=False\n    )\n    \n    valid_data_loader = DataLoader(\n        valid_dataset,\n        batch_size=VALID_BATCH_SIZE,\n        sampler=valid_sampler,\n        drop_last=True,\n        num_workers=0\n    )\n    xm.master_print('Valid Loader Created.')\n    \n    num_train_steps = int(len(train_df) / TRAIN_BATCH_SIZE / xm.xrt_world_size())\n    device = xm.xla_device()\n    model = WRAPPED_MODEL.to(device)\n    xm.master_print('Done Model Loading.')\n    optimizer = get_model_optimizer(model)\n    scheduler = get_cosine_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps = 0,\n        num_training_steps = num_train_steps * EPOCHS\n    )\n    xm.master_print(f'Num Train Steps= {num_train_steps}, XRT World Size= {xm.xrt_world_size()}.')\n    \n    for epoch in range(EPOCHS):\n        para_loader = pl.ParallelLoader(train_data_loader, [device])\n        xm.master_print('Parallel Loader Created. Training ...')\n        train_loop_fn(para_loader.per_device_loader(device),\n                      model,  \n                      optimizer, \n                      device, \n                      scheduler, \n                      epoch\n                     )\n        \n        xm.master_print(\"Finished training epoch {}\".format(epoch))\n            \n        para_loader = pl.ParallelLoader(valid_data_loader, [device])\n        xm.master_print('Parallel Loader Created. Validating ...')\n        eval_loop_fn(para_loader.per_device_loader(device), \n                     model,  \n                     device\n                    )\n        \n        # Serialized and Memory Reduced Model Saving\n        if epoch == EPOCHS-1:\n            xm.master_print('Saving Model ..')\n            xser.save(model.state_dict(), f\"model.bin\", master_only=True)\n            xm.master_print('Model Saved.')\n            \n    if METRICS_DEBUG:\n      xm.master_print(met.metrics_report(), flush=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"def _mp_fn(rank, flags):\n    # torch.set_default_tensor_type('torch.FloatTensor')\n    _run()\n\nFLAGS={}\nxmp.spawn(_mp_fn, args=(FLAGS,), nprocs=8, start_method='fork')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Test","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class TestDatasetRetriever(Dataset):\n    def __init__(self, df, ids, mask):\n        self.df = df\n        self.ids = ids\n        self.mask = mask\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):   \n        ids = self.ids[index]\n        mask = self.mask[index]\n        return {\n            'ids':torch.tensor(ids),\n            'mask':torch.tensor(mask)\n        }","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Test Config","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"TEST_BATCH_SIZE = 32\n\ntest_ids, test_mask = fast_encode(test, fast_tokenizer)\n\ntest_dataset = TestDatasetRetriever(test, test_ids, test_mask)\n\ntest_data_loader = DataLoader(\n    test_dataset, \n    batch_size=TEST_BATCH_SIZE,\n    drop_last=False,\n    num_workers=4,\n    shuffle=False\n)\n\n# Load Serialized Model\ndevice = xm.xla_device()\nmodel = WRAPPED_MODEL.to(device).eval()\nmodel.load_state_dict(xser.load(\"model.bin\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Test Prediction","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test_preds = []\n\nfor i, data in tqdm(enumerate(test_data_loader), total=len(test_data_loader)):\n    ids = data[\"ids\"]\n    mask = data[\"mask\"]\n    ids = ids.to(device, dtype=torch.long)\n    mask = mask.to(device, dtype=torch.long)\n    outputs = model(\n        input_ids = ids,\n        attention_mask = mask,\n    )\n    outputs_np = outputs.cpu().detach().numpy().tolist()\n    test_preds.extend(outputs_np)  \n    \ntest_preds = torch.FloatTensor(test_preds)\ntop1_prob, top1_label = torch.topk(test_preds, 1)\ny = top1_label.cpu().detach().numpy()\nsample_submission.prediction = y\nsample_submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}