{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom csv import reader\nimport os\nimport numpy as np\nimport pandas as pd\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.layers import concatenate, Dense, Input, Dropout, TimeDistributed, Embedding, BatchNormalization, Bidirectional, LSTM, GRU\nimport string\nimport tempfile","metadata":{"execution":{"iopub.status.busy":"2021-09-30T11:41:02.291975Z","iopub.execute_input":"2021-09-30T11:41:02.292561Z","iopub.status.idle":"2021-09-30T11:41:06.473277Z","shell.execute_reply.started":"2021-09-30T11:41:02.292479Z","shell.execute_reply":"2021-09-30T11:41:06.472548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"id":"1jnDwbo7wBJe","execution":{"iopub.status.busy":"2021-09-30T11:41:10.095833Z","iopub.execute_input":"2021-09-30T11:41:10.096392Z","iopub.status.idle":"2021-09-30T11:41:10.123755Z","shell.execute_reply.started":"2021-09-30T11:41:10.096354Z","shell.execute_reply":"2021-09-30T11:41:10.123041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#pip install -U deep_translator\n#from deep_translator import GoogleTranslator\n#translator = Translator()\n#def translate_sentence(x):\n#  return GoogleTranslator('auto', 'en').translate(x)\n#train.premise[train.lang_abv!= 'en']=train.premise[train.lang_abv!= 'en'].apply(lambda x: translate_sentence(x))\n#train.hypothesis[train.lang_abv!= 'en']=train.hypothesis[train.lang_abv!= 'en'].apply(lambda x: translate_sentence(x))\n#train.to_csv(\"/content/drive/MyDrive/Contradictory, My Dear Watson/train_translated.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-09-29T01:53:50.703757Z","iopub.execute_input":"2021-09-29T01:53:50.703997Z","iopub.status.idle":"2021-09-29T01:53:50.708943Z","shell.execute_reply.started":"2021-09-29T01:53:50.703968Z","shell.execute_reply":"2021-09-29T01:53:50.708093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Translation to English","metadata":{}},{"cell_type":"code","source":"# !pip install deep_translator","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from deep_translator import GoogleTranslator\n\n# def translate_sentence(x):\n#   return GoogleTranslator('auto', 'en').translate(x)\n\n# translate_sentence('hola mundo')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train = pd.read_csv(\"drive/MyDrive/Contradictory, My Dear Watson/train.csv\")\n# test = pd.read_csv(\"drive/MyDrive/Contradictory, My Dear Watson/test.csv\")\n\n# train.premise[train.lang_abv!= 'en']=train.premise[train.lang_abv!= 'en'].apply(lambda x: translate_sentence(x))\n# train.hypothesis[train.lang_abv!= 'en']=train.hypothesis[train.lang_abv!= 'en'].apply(lambda x: translate_sentence(x))\n# train.to_csv(\"/content/drive/MyDrive/Contradictory, My Dear Watson/train_translated.csv\")\n\n# test.premise[test.lang_abv!= 'en']=test.premise[test.lang_abv!= 'en'].apply(lambda x: translate_sentence(x))\n# test.hypothesis[test.lang_abv!= 'en']=test.hypothesis[test.lang_abv!= 'en'].apply(lambda x: translate_sentence(x))\n# test.to_csv(\"/content/drive/MyDrive/Contradictory, My Dear Watson/test_translated.csv\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# read csv file as a list of lists\nwith open('../input/train-translated/train_translated.csv', 'r') as read_obj:\n    # pass the file object to reader() to get the reader object\n    csv_reader = reader(read_obj)\n    # Pass reader object to list() to get a list of lists\n    train_translated = list(csv_reader)[1:]\n    print(train_translated[1][-1])","metadata":{"execution":{"iopub.status.busy":"2021-09-30T11:44:26.541728Z","iopub.execute_input":"2021-09-30T11:44:26.542008Z","iopub.status.idle":"2021-09-30T11:44:26.634348Z","shell.execute_reply.started":"2021-09-30T11:44:26.54198Z","shell.execute_reply":"2021-09-30T11:44:26.633376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Get the Data in the Appropriate Format for the Model","metadata":{}},{"cell_type":"code","source":"def get_data(raw_data, limit=None, test=False):\n    if not test:\n        premises = [' '.join(premise).translate(str.maketrans('', '', string.punctuation)).split() for n, _id, premise, hypothesis, lang_abv, language, l in raw_data]\n        hypotheses = [''.join(hypothesis).translate(str.maketrans('', '', string.punctuation)).split() for n, _id, premise, hypothesis, lang_abv, language, l in raw_data]\n        Y = np.array([int(l) for n, _id, premise, hypothesis, lang_abv, language, l in raw_data])\n        Y = to_categorical(Y, 3)\n        return (premises, hypotheses, Y)\n    else:\n        premises = [' '.join(premise).translate(str.maketrans('', '', string.punctuation)).split() for n, _id, premise, hypothesis, lang_abv, language in raw_data]\n        hypotheses = [''.join(hypothesis).translate(str.maketrans('', '', string.punctuation)).split() for n, _id, premise, hypothesis, lang_abv, language in raw_data]\n        return (premises, hypotheses)","metadata":{"execution":{"iopub.status.busy":"2021-09-30T11:44:29.77451Z","iopub.execute_input":"2021-09-30T11:44:29.775263Z","iopub.status.idle":"2021-09-30T11:44:29.784411Z","shell.execute_reply.started":"2021-09-30T11:44:29.775221Z","shell.execute_reply":"2021-09-30T11:44:29.783451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# os.remove(GLOVE_STORE + '.npy')","metadata":{"execution":{"iopub.status.busy":"2021-09-29T22:10:10.708651Z","iopub.execute_input":"2021-09-29T22:10:10.709099Z","iopub.status.idle":"2021-09-29T22:10:10.719233Z","shell.execute_reply.started":"2021-09-29T22:10:10.709064Z","shell.execute_reply":"2021-09-29T22:10:10.718546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## The Model","metadata":{}},{"cell_type":"code","source":"training = get_data(train_translated)\n\ntokenizer = Tokenizer(lower=False, filters='')\ntokenizer.fit_on_texts(training[0] + training[1])\n\n# Lowest index from the tokenizer is 1 - we need to include 0 in our vocab count\nVOCAB = len(tokenizer.word_counts) + 1\nLABELS = {'contradiction': 0, 'neutral': 1, 'entailment': 2}\n# RNN = LSTM\n# RNN = lambda *args, **kwargs: Bidirectional(LSTM(*args, **kwargs))\n# RNN = GRU\nRNN = lambda *args, **kwargs: Bidirectional(GRU(*args, **kwargs))\n# Summation of word embeddings\n# RNN = None\n# RNN = None\nLAYERS = 1\nUSE_GLOVE = True\nTRAIN_EMBED = False\nEMBED_HIDDEN_SIZE = 300\nSENT_HIDDEN_SIZE = 300\nBATCH_SIZE = 512\nPATIENCE = 4 # 8\nMAX_EPOCHS = 42\nMAX_LEN = 42\nDP = 0.4\nL2 = 4e-6\nACTIVATION = 'relu'\nOPTIMIZER = 'adam'\nprint('RNN / Embed / Sent = {}, {}, {}'.format(RNN, EMBED_HIDDEN_SIZE, SENT_HIDDEN_SIZE))\nprint('GloVe / Trainable Word Embeddings = {}, {}'.format(USE_GLOVE, TRAIN_EMBED))\n\nto_seq = lambda X: pad_sequences(tokenizer.texts_to_sequences(X), maxlen=MAX_LEN)\nprepare_data = lambda data: (to_seq(data[0]), to_seq(data[1]), data[2])\n\ntraining = prepare_data(training)\n\nprint('Build model...')\nprint('Vocab size =', VOCAB)\n\nGLOVE_STORE = 'precomputed_glove.weights'\nif USE_GLOVE:\n  if not os.path.exists(GLOVE_STORE + '.npy'):\n    print('Computing GloVe')\n  \n    embeddings_index = {}\n    f = open('../input/glove840b300dtxt/glove.840B.300d.txt')\n    for line in f:\n      values = line.split(' ')\n      word = values[0]\n      coefs = np.asarray(values[1:], dtype='float32')\n      embeddings_index[word] = coefs\n    f.close()\n    \n    # prepare embedding matrix\n    embedding_matrix = np.zeros((VOCAB, EMBED_HIDDEN_SIZE))\n    for word, i in tokenizer.word_index.items():\n      embedding_vector = embeddings_index.get(word)\n      if embedding_vector is not None:\n        # words not found in embedding index will be all-zeros.\n        embedding_matrix[i] = embedding_vector\n      else:\n        print('Missing from GloVe: {}'.format(word))\n  \n    np.save(GLOVE_STORE, embedding_matrix)\n\n  print('Loading GloVe')\n  embedding_matrix = np.load(GLOVE_STORE + '.npy')\n\n  print('Total number of null word embeddings:')\n  print(np.sum(np.sum(embedding_matrix, axis=1) == 0))\n\n  embed = Embedding(VOCAB, EMBED_HIDDEN_SIZE, weights=[embedding_matrix], input_length=MAX_LEN, trainable=TRAIN_EMBED)\nelse:\n  embed = Embedding(VOCAB, EMBED_HIDDEN_SIZE, input_length=MAX_LEN)\n\nrnn_kwargs = dict(units=SENT_HIDDEN_SIZE, dropout=DP, recurrent_dropout=DP)\nSumEmbeddings = keras.layers.Lambda(lambda x: K.sum(x, axis=1), output_shape=(SENT_HIDDEN_SIZE, ))\n\ntranslate = TimeDistributed(Dense(SENT_HIDDEN_SIZE, activation=ACTIVATION))\n\npremise = Input(shape=(MAX_LEN,), dtype='int32')\nhypothesis = Input(shape=(MAX_LEN,), dtype='int32')\n\nprem = embed(premise)\nhypo = embed(hypothesis)\n\nprem = translate(prem)\nhypo = translate(hypo)\n\nif RNN and LAYERS > 1:\n  for l in range(LAYERS - 1):\n    rnn = RNN(return_sequences=True, **rnn_kwargs)\n    prem = rnn(prem)\n    hypo = rnn(hypo)\n    prem = BatchNormalization()(prem)\n    hypo = BatchNormalization()(hypo)\nrnn = SumEmbeddings if not RNN else RNN(return_sequences=False, **rnn_kwargs)\nprem = rnn(prem)\nhypo = rnn(hypo)\nprem = BatchNormalization()(prem)\nhypo = BatchNormalization()(hypo)\n\n\njoint = concatenate([prem, hypo])\njoint = Dropout(DP)(joint)\nfor i in range(3):\n  joint = Dense(2 * SENT_HIDDEN_SIZE, activation=ACTIVATION, kernel_regularizer=l2(L2) if L2 else None)(joint)\n  joint = Dropout(DP)(joint)\n  joint = BatchNormalization()(joint)\n\npred = Dense(len(LABELS), activation='softmax')(joint)\n\nmodel = Model(inputs=[premise, hypothesis], outputs=pred)\nmodel.compile(optimizer=OPTIMIZER, loss='categorical_crossentropy', metrics=['accuracy'])\n\nmodel.summary()\n\nprint('_embeding')\ncheckpoint_filepath = './checkpoint'\n# Save the best model during validation and bail out of training early if we're not improving\ncallbacks = [EarlyStopping(patience=PATIENCE), ModelCheckpoint(checkpoint_filepath, save_best_only=True, save_weights_only=True, monitor='accuracy', mode='max')]\nmodel.fit([training[0], training[1]], training[2], batch_size=BATCH_SIZE, epochs=MAX_EPOCHS, callbacks=callbacks, validation_split=0.2)\n\n# Restore the best found model during validation\nmodel.load_weights(checkpoint_filepath)","metadata":{"execution":{"iopub.status.busy":"2021-09-30T11:59:16.097197Z","iopub.execute_input":"2021-09-30T11:59:16.097463Z","iopub.status.idle":"2021-09-30T12:03:49.743336Z","shell.execute_reply.started":"2021-09-30T11:59:16.097435Z","shell.execute_reply":"2021-09-30T12:03:49.742628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# read csv file as a list of lists\nwith open('../input/test-translated/test_translated.csv', 'r') as read_obj:\n    # pass the file object to reader() to get the reader object\n    csv_reader = reader(read_obj)\n    # Pass reader object to list() to get a list of lists\n    test_translated = list(csv_reader)[1:]","metadata":{"execution":{"iopub.status.busy":"2021-09-30T12:03:54.98837Z","iopub.execute_input":"2021-09-30T12:03:54.988655Z","iopub.status.idle":"2021-09-30T12:03:55.036157Z","shell.execute_reply.started":"2021-09-30T12:03:54.988627Z","shell.execute_reply":"2021-09-30T12:03:55.035428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"to_seq = lambda X: pad_sequences(tokenizer.texts_to_sequences(X), maxlen=MAX_LEN)\nprepare_data = lambda data: (to_seq(data[0]), to_seq(data[1]))\n\ntesting = get_data(test_translated, test=True)\n\ntesting = prepare_data(testing)\ntest_df = pd.read_csv('../input/test-translated/test_translated.csv')\npredictions = [np.argmax(i) for i in model.predict([testing[0], testing[1]])]\nsubmission = test_df.id.copy().to_frame()\nsubmission['prediction'] = predictions","metadata":{"execution":{"iopub.status.busy":"2021-09-30T12:03:57.042917Z","iopub.execute_input":"2021-09-30T12:03:57.043336Z","iopub.status.idle":"2021-09-30T12:04:09.73567Z","shell.execute_reply.started":"2021-09-30T12:03:57.043303Z","shell.execute_reply":"2021-09-30T12:04:09.734953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv(\"submission.csv\", index = False)","metadata":{"execution":{"iopub.status.busy":"2021-09-30T12:04:13.288031Z","iopub.execute_input":"2021-09-30T12:04:13.288678Z","iopub.status.idle":"2021-09-30T12:04:13.316908Z","shell.execute_reply.started":"2021-09-30T12:04:13.288363Z","shell.execute_reply":"2021-09-30T12:04:13.316251Z"},"trusted":true},"execution_count":null,"outputs":[]}]}