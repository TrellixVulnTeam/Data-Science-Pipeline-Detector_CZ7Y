{"cells":[{"metadata":{},"cell_type":"markdown","source":"Looks like the textual entailment problem is kind of a next sequence predicting problem except we need to classify the relation between sentences into three classes (entailment, neuter and contradiction) instead of two."},{"metadata":{},"cell_type":"markdown","source":"# Preparation"},{"metadata":{"id":"5fiDa3OTdZea","outputId":"ebc4431f-cac2-43e1-bbbe-50278288daf4","trusted":true},"cell_type":"code","source":"!pip install 'transformers==3.5.0'","execution_count":null,"outputs":[]},{"metadata":{"id":"lKKdrfMyC-oR","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nfrom tqdm import tqdm_notebook as tqdm\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport transformers\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First we have to set up our TPU. There is an educational notebook [\"TPUs in Colab\"](https://colab.research.google.com/notebooks/tpu.ipynb) and I took the following code from it:"},{"metadata":{"id":"qurNoNi-tRRH","outputId":"0e5abf21-ee9d-4422-8896-2f12acad3471","trusted":true},"cell_type":"code","source":"try:\n  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\nexcept ValueError:\n  raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n\ntf.config.experimental_connect_to_cluster(tpu)\ntf.tpu.experimental.initialize_tpu_system(tpu)\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)","execution_count":null,"outputs":[]},{"metadata":{"id":"Lh2AChA3Cjdj","trusted":true},"cell_type":"code","source":"DATA_DIR = '../input/contradictory-my-dear-watson/'\n\ntrain = pd.read_csv(DATA_DIR+'train.csv')\ntest = pd.read_csv(DATA_DIR+'test.csv')\nsubmission = pd.read_csv(DATA_DIR+'sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we need to choose our [pretrained model](https://huggingface.co/transformers/pretrained_models.html).\nLets take ``` bert-base-multilingual-cased ```  for now.\n\n*P.S. I also tried ```xlm-roberta-base``` but it didnt seem to be superiour for some reason.*"},{"metadata":{"trusted":true},"cell_type":"code","source":"MODEL_NAME = 'bert-base-multilingual-cased'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Tokenization"},{"metadata":{},"cell_type":"markdown","source":"We need our input sentenses to be in a single string divided by special marker to be fed into the BERT model. Let's find out how long the resulting string should be:"},{"metadata":{"id":"Qvcce9xtQPBG","outputId":"6c0bd809-91e2-43e1-937d-f540e4ad3724","trusted":true},"cell_type":"code","source":"premise_lengths = [len(train.premise[i].split()) for i in range(len(train))]\nhypothesis_lengths = [len(train.hypothesis[i].split()) for i in range(len(train))]\nprimise_hypothesis_combined_length = [len(train.premise[i].split()) + \n                                      len(train.hypothesis[i].split()) for i in range(len(train))]\n\nplt.figure(figsize=(20,5))\nsns.distplot(primise_hypothesis_combined_length);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we need to tokenize our data.\nFortunately Huggingface have some easy-to-use tokenizers. Fow now I'll use [BertTokenizerFast](https://huggingface.co/transformers/model_doc/bert.html#berttokenizerfast) and follow the [preprocessing](https://huggingface.co/transformers/preprocessing.html) overview:"},{"metadata":{"id":"ZSMBKcquc6s_","outputId":"28ac3e18-5bd9-4b5e-ceec-79938fab5293","trusted":true},"cell_type":"code","source":"# Creating an instance of tokenizer class and downloading the vocab for the model\ntokenizer = transformers.BertTokenizerFast.from_pretrained(MODEL_NAME);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Looks like 64 would be enough.\nMAX_LENGTH = 64","execution_count":null,"outputs":[]},{"metadata":{"id":"VgbBbWoMbaJL","trusted":true},"cell_type":"code","source":"train_encoded = tokenizer(text=list(train.premise.values),\n                    text_pair=list(train.hypothesis.values),\n                    add_special_tokens=True,\n                    max_length=MAX_LENGTH,\n                    truncation=True,\n                    padding=True,\n                    return_attention_mask=True,\n                    return_token_type_ids=True,\n                    return_tensors='tf'\n                    )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For some reasom ```.fit()``` of ```keras.Model```  method doesn't work with our Tensorflow ```Dataset```. So I had to split our data into *train* and *val* set beforehand."},{"metadata":{"id":"4IGE7HBKQGfd","trusted":true},"cell_type":"code","source":"BATCH_SIZE = 1024\nval_size = int(len(train)*0.2)\n\ndataset = tf.data.Dataset.from_tensor_slices((train_encoded.data, train.label.values))\nval_dataset = (dataset.take(val_size).batch(BATCH_SIZE))\ntrain_dataset = (dataset.skip(val_size).batch(BATCH_SIZE))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model building"},{"metadata":{"trusted":true},"cell_type":"code","source":"with tpu_strategy.scope():\n    transformer = transformers.TFAutoModel.from_pretrained(MODEL_NAME);","execution_count":null,"outputs":[]},{"metadata":{"id":"DVwTKYULZHjO","trusted":true},"cell_type":"code","source":"def create_model():\n\n    input_1 = tf.keras.Input(shape=(MAX_LENGTH,),name='input_ids', dtype='int32')\n    input_2 = tf.keras.Input(shape=(MAX_LENGTH,),name='attention_mask', dtype='int32')\n    input_3 = tf.keras.Input(shape=(MAX_LENGTH,),name='token_type_ids', dtype='int32')\n\n    x = transformer((input_1, input_2, input_3))[0]\n    x = tf.keras.layers.Dense(200, activation='relu')(x[:,0,:])\n    y = tf.keras.layers.Dense(3, activation='softmax', name='output_layer')(x)\n    model = tf.keras.Model(inputs=(input_1, input_2, input_3), outputs=y)\n    return model","execution_count":null,"outputs":[]},{"metadata":{"id":"JgpwwV5qPauI","outputId":"b3bd9578-c6dc-4790-dac8-c9ae86d0bbc4","trusted":true},"cell_type":"code","source":"with tpu_strategy.scope():\n    model = create_model()\n    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n    model.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modelling"},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_reduction = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor='val_loss',\n    factor=0.2,\n    patience=1,\n    min_lr=1e-7\n)\n\nhistory = model.fit(\n    train_dataset,\n    epochs = 20,\n    verbose = 2,\n    batch_size = BATCH_SIZE,\n    callbacks=[lr_reduction],\n    validation_data=val_dataset\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,5))\nplt.subplot(1,2,1)\nplt.plot(history.history['val_loss'])\nplt.plot(history.history['loss'])\nplt.subplot(1,2,2)\nplt.plot(history.history['val_accuracy'])\nplt.plot(history.history['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission"},{"metadata":{"id":"totrqk5k0PhH","outputId":"2a1c6599-e35b-4cbb-87c8-2567152eb634","trusted":true},"cell_type":"code","source":"test_encoded = tokenizer(text=list(test.premise.values),\n                    text_pair=list(test.hypothesis.values),\n                    add_special_tokens=True,\n                    max_length=MAX_LENGTH,\n                    truncation=True,\n                    padding=True,\n                    return_attention_mask=True,\n                    return_tensors='tf'\n                    )","execution_count":null,"outputs":[]},{"metadata":{"id":"PvCglQ0h0ld2","outputId":"2c00fb23-8362-4aa8-9b64-5642e45c7600","trusted":true},"cell_type":"code","source":"predictions = model.predict(test_encoded.data, batch_size=128, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"id":"mdWLy1YE2vEB","trusted":true},"cell_type":"code","source":"test_labels = np.argmax(predictions, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"id":"9ak4lnRhf4oR"},"cell_type":"markdown","source":"WARNING:tensorflow:Gradients do not exist for variables  https://github.com/tensorflow/tensorflow/issues/37501"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission['prediction'] = test_labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv(\"submission.csv\", index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}