{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install transformers==3.0.2","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\nimport gc\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport transformers\n\nfrom sklearn.model_selection import KFold","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept ValueError:\n    strategy = tf.distribute.get_strategy() # for CPU and single GPU\n    print('Number of replicas:', strategy.num_replicas_in_sync)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"transformers.__version__","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import AutoTokenizer, TFAutoModel","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/contradictory-my-dear-watson/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/contradictory-my-dear-watson/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = 3\nmaxlen = 50\n\nmodel_name = \"jplu/tf-xlm-roberta-large\"\n\nbatch_size = 16 * strategy.num_replicas_in_sync","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_name)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.premise.values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"[HF Tokeinzers](https://huggingface.co/transformers/main_classes/tokenizer.html#transformers.PreTrainedTokenizerFast.__call__)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"list(train_df.premise.values[:10]), list(train_df.hypothesis.values[:10])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntrain_encode = tokenizer(list(train_df.premise.values), list(train_df.hypothesis.values), \n                      max_length=maxlen, return_tensors=\"np\", padding=True, \n                      return_token_type_ids=True, return_attention_mask=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_model(maxlen=50):\n    \n    #base_model = TFDistilBertModel.from_pretrained(\"distilbert-base-multilingual-cased\")\n    \n    base_model = TFAutoModel.from_pretrained(model_name)\n    \n    input_ids = tf.keras.layers.Input(shape =(maxlen, ), dtype=tf.int32, name=\"input_ids\")\n    input_type = tf.keras.layers.Input(shape =(maxlen, ), dtype=tf.int32, name=\"token_type_ids\")\n    input_mask = tf.keras.layers.Input(shape =(maxlen, ), dtype=tf.int32, name=\"attention_mask\")\n    \n    \n    embedding = base_model([input_ids, input_mask, input_type])[0]\n    #embedding = base_model([input_ids, input_mask])[0]\n    \n    print(embedding.shape)\n    \n    output = tf.keras.layers.Dense(3, activation=\"softmax\")(embedding[:, 0, :])\n    \n    model = tf.keras.models.Model(inputs=[input_ids, input_mask, input_type], outputs = output)\n    \n    model.compile(tf.keras.optimizers.Adam(1e-5), \"sparse_categorical_crossentropy\", [\"accuracy\"])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with strategy.scope():\n    cls_model = get_model(maxlen)\n    cls_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nps = cls_model([train_encode['input_ids'][:10], train_encode['attention_mask'][:10], train_encode['token_type_ids'][:10]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fold = KFold(n_splits=3, shuffle=True, random_state=108)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nhists = []\nmodels = []\nfor i, (train_idx, val_idx) in enumerate(fold.split(np.arange(train_df.label.shape[0]))):\n    print(f\"----FOLD: {i+1}----\\n\",train_idx, val_idx)\n    \n    \n    x_train = [train_encode['input_ids'][train_idx], \n               train_encode['attention_mask'][train_idx], \n               train_encode['token_type_ids'][train_idx]]\n    \n    y_train = train_df.label.values[train_idx]\n    \n    x_val = [train_encode['input_ids'][val_idx],\n             train_encode['attention_mask'][val_idx],\n             train_encode['token_type_ids'][val_idx]]\n    y_val = train_df.label.values[val_idx]\n    \n    \n    hist=cls_model.fit(x_train, y_train,\n                       epochs=epochs, \n                       batch_size = batch_size,\n                       validation_data = (x_val, y_val),\n                      )\n    hists.append(hist)\n    #models.append(cls_model)\n    \n    gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntest_encode = tokenizer(list(test_df.premise.values), list(test_df.hypothesis.values), \n                      max_length=maxlen, return_tensors=\"tf\", padding=True, \n                      return_token_type_ids=True, return_attention_mask=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\npreds = []\nfor model in models:\n    ps = model.predict([test_encode['input_ids'], test_encode['attention_mask'], test_encode['token_type_ids']],\n                      verbose=1, batch_size=batch_size)\n    preds.append(ps)\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#ps = np.mean(np.stack(preds), 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ps = cls_model.predict([test_encode['input_ids'], test_encode['attention_mask'], test_encode['token_type_ids']],\n                      verbose=1, batch_size=batch_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = test_df.id.copy().to_frame()\nsubmission['prediction'] = np.argmax(ps, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv(\"submission.csv\", index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}