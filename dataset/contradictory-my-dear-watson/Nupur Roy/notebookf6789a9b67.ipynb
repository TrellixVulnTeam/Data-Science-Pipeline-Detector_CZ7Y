{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nimport gc\nimport random\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\ntorch.manual_seed(555)\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import roc_auc_score, accuracy_score\nimport transformers\nfrom transformers import BertTokenizer, BertForSequenceClassification \nfrom transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification\nfrom transformers import AdamW\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2021-06-10T21:52:55.668Z","iopub.execute_input":"2021-06-10T21:52:55.668394Z","iopub.status.idle":"2021-06-10T21:53:01.826659Z","shell.execute_reply.started":"2021-06-10T21:52:55.668354Z","shell.execute_reply":"2021-06-10T21:53:01.82564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = '../input/contradictory-my-dear-watson/train.csv'\ndf_train = pd.read_csv(path)\n\nprint(df_train.shape)\n\ndf_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-10T21:53:04.736825Z","iopub.execute_input":"2021-06-10T21:53:04.737224Z","iopub.status.idle":"2021-06-10T21:53:04.894038Z","shell.execute_reply.started":"2021-06-10T21:53:04.737192Z","shell.execute_reply":"2021-06-10T21:53:04.892996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a = df_train.shape\n\nprint(\"The shape of train the data is:\",a)\nprint(\"the attributes are\",df_train.columns)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T21:53:07.606636Z","iopub.execute_input":"2021-06-10T21:53:07.607017Z","iopub.status.idle":"2021-06-10T21:53:07.613306Z","shell.execute_reply.started":"2021-06-10T21:53:07.606981Z","shell.execute_reply":"2021-06-10T21:53:07.612301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.info()","metadata":{"execution":{"iopub.status.busy":"2021-06-10T21:53:10.23528Z","iopub.execute_input":"2021-06-10T21:53:10.235893Z","iopub.status.idle":"2021-06-10T21:53:10.264167Z","shell.execute_reply.started":"2021-06-10T21:53:10.235856Z","shell.execute_reply":"2021-06-10T21:53:10.263133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = '../input/contradictory-my-dear-watson/test.csv'\ndf_test = pd.read_csv(path)\n\nprint(df_test.shape)\n\ndf_test.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-10T21:53:13.105872Z","iopub.execute_input":"2021-06-10T21:53:13.106564Z","iopub.status.idle":"2021-06-10T21:53:13.192063Z","shell.execute_reply.started":"2021-06-10T21:53:13.106505Z","shell.execute_reply":"2021-06-10T21:53:13.191017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"b = df_test.shape\nprint(\"The shape of the test data is:\",b)\nprint(\"the attributes are\",df_test.columns)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T21:53:15.934438Z","iopub.execute_input":"2021-06-10T21:53:15.935009Z","iopub.status.idle":"2021-06-10T21:53:15.940607Z","shell.execute_reply.started":"2021-06-10T21:53:15.934957Z","shell.execute_reply":"2021-06-10T21:53:15.939755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.info()","metadata":{"execution":{"iopub.status.busy":"2021-06-10T21:53:19.280068Z","iopub.execute_input":"2021-06-10T21:53:19.280455Z","iopub.status.idle":"2021-06-10T21:53:19.299051Z","shell.execute_reply.started":"2021-06-10T21:53:19.280425Z","shell.execute_reply":"2021-06-10T21:53:19.297719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df_train.keys())\nprint(df_test.keys())","metadata":{"execution":{"iopub.status.busy":"2021-06-10T21:53:23.477263Z","iopub.execute_input":"2021-06-10T21:53:23.477845Z","iopub.status.idle":"2021-06-10T21:53:23.486138Z","shell.execute_reply.started":"2021-06-10T21:53:23.47779Z","shell.execute_reply":"2021-06-10T21:53:23.484483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['premise'].values[0]","metadata":{"execution":{"iopub.status.busy":"2021-06-10T21:53:25.909082Z","iopub.execute_input":"2021-06-10T21:53:25.909443Z","iopub.status.idle":"2021-06-10T21:53:25.916126Z","shell.execute_reply.started":"2021-06-10T21:53:25.909412Z","shell.execute_reply":"2021-06-10T21:53:25.914808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['hypothesis'].values[0]\n","metadata":{"execution":{"iopub.status.busy":"2021-06-10T21:53:28.539122Z","iopub.execute_input":"2021-06-10T21:53:28.539493Z","iopub.status.idle":"2021-06-10T21:53:28.547751Z","shell.execute_reply.started":"2021-06-10T21:53:28.539464Z","shell.execute_reply":"2021-06-10T21:53:28.545731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['label'].values[0]","metadata":{"execution":{"iopub.status.busy":"2021-06-10T21:53:30.772718Z","iopub.execute_input":"2021-06-10T21:53:30.773116Z","iopub.status.idle":"2021-06-10T21:53:30.77893Z","shell.execute_reply.started":"2021-06-10T21:53:30.773083Z","shell.execute_reply":"2021-06-10T21:53:30.777861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['premise'].values[1]","metadata":{"execution":{"iopub.status.busy":"2021-06-10T21:53:33.077309Z","iopub.execute_input":"2021-06-10T21:53:33.077696Z","iopub.status.idle":"2021-06-10T21:53:33.084166Z","shell.execute_reply.started":"2021-06-10T21:53:33.077663Z","shell.execute_reply":"2021-06-10T21:53:33.082952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['hypothesis'].values[1]","metadata":{"execution":{"iopub.status.busy":"2021-06-10T21:53:35.519902Z","iopub.execute_input":"2021-06-10T21:53:35.520299Z","iopub.status.idle":"2021-06-10T21:53:35.526554Z","shell.execute_reply.started":"2021-06-10T21:53:35.520268Z","shell.execute_reply":"2021-06-10T21:53:35.525064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['label'].values[1]","metadata":{"execution":{"iopub.status.busy":"2021-06-10T21:53:38.444834Z","iopub.execute_input":"2021-06-10T21:53:38.44525Z","iopub.status.idle":"2021-06-10T21:53:38.453012Z","shell.execute_reply.started":"2021-06-10T21:53:38.445217Z","shell.execute_reply":"2021-06-10T21:53:38.451363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-06-10T21:53:41.393925Z","iopub.execute_input":"2021-06-10T21:53:41.394332Z","iopub.status.idle":"2021-06-10T21:53:41.409418Z","shell.execute_reply.started":"2021-06-10T21:53:41.394288Z","shell.execute_reply":"2021-06-10T21:53:41.408373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip uninstall -y torch torchvision 1.7.0","metadata":{"execution":{"iopub.status.busy":"2021-06-10T21:49:31.041635Z","iopub.execute_input":"2021-06-10T21:49:31.042046Z","iopub.status.idle":"2021-06-10T21:49:33.337938Z","shell.execute_reply.started":"2021-06-10T21:49:31.042013Z","shell.execute_reply":"2021-06-10T21:49:33.337162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n!python pytorch-xla-env-setup.py --apt-packages libomp5 libopenblas-dev","metadata":{"execution":{"iopub.status.busy":"2021-06-10T21:50:17.644223Z","iopub.execute_input":"2021-06-10T21:50:17.644805Z","iopub.status.idle":"2021-06-10T21:51:24.882387Z","shell.execute_reply.started":"2021-06-10T21:50:17.644721Z","shell.execute_reply":"2021-06-10T21:51:24.881394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# imports pytorch\nimport torch\n\n# imports the torch_xla package\nimport torch_xla\nimport torch_xla.core.xla_model as xm","metadata":{"execution":{"iopub.status.busy":"2021-06-10T21:51:47.182768Z","iopub.execute_input":"2021-06-10T21:51:47.183177Z","iopub.status.idle":"2021-06-10T21:51:47.865279Z","shell.execute_reply.started":"2021-06-10T21:51:47.183143Z","shell.execute_reply":"2021-06-10T21:51:47.863683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import BertTokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T21:52:43.879872Z","iopub.execute_input":"2021-06-10T21:52:43.880334Z","iopub.status.idle":"2021-06-10T21:52:46.164781Z","shell.execute_reply.started":"2021-06-10T21:52:43.880295Z","shell.execute_reply":"2021-06-10T21:52:46.163593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer.tokenize((df_train['hypothesis'][0]))","metadata":{"execution":{"iopub.status.busy":"2021-06-10T21:53:56.526795Z","iopub.execute_input":"2021-06-10T21:53:56.527207Z","iopub.status.idle":"2021-06-10T21:53:56.534978Z","shell.execute_reply.started":"2021-06-10T21:53:56.527173Z","shell.execute_reply":"2021-06-10T21:53:56.534005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer(str(df_train), padding=True, truncation=True,return_tensors='pt')","metadata":{"execution":{"iopub.status.busy":"2021-06-10T21:54:07.084518Z","iopub.execute_input":"2021-06-10T21:54:07.084895Z","iopub.status.idle":"2021-06-10T21:54:07.124886Z","shell.execute_reply.started":"2021-06-10T21:54:07.08486Z","shell.execute_reply":"2021-06-10T21:54:07.123383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import BertModel \n\nmodel = BertModel.from_pretrained('bert-base-uncased')\nmodel","metadata":{"execution":{"iopub.status.busy":"2021-06-10T21:54:19.371321Z","iopub.execute_input":"2021-06-10T21:54:19.371736Z","iopub.status.idle":"2021-06-10T21:54:39.517545Z","shell.execute_reply.started":"2021-06-10T21:54:19.371706Z","shell.execute_reply":"2021-06-10T21:54:39.516491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n###padding, truncation\nencoded = tokenizer(str(df_train), padding=True, truncation=True, return_tensors='pt')\nout = model(**encoded)\nlast_hidden_states = out.last_hidden_state\nlast_hidden_states.shape, last_hidden_states","metadata":{"execution":{"iopub.status.busy":"2021-06-10T21:54:56.573824Z","iopub.execute_input":"2021-06-10T21:54:56.574683Z","iopub.status.idle":"2021-06-10T21:54:58.124356Z","shell.execute_reply.started":"2021-06-10T21:54:56.574624Z","shell.execute_reply":"2021-06-10T21:54:58.123153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import BertForSequenceClassification\n\nnum_labels = 3 \nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)\nmodel","metadata":{"execution":{"iopub.status.busy":"2021-06-10T21:55:10.902064Z","iopub.execute_input":"2021-06-10T21:55:10.902456Z","iopub.status.idle":"2021-06-10T21:55:15.295669Z","shell.execute_reply.started":"2021-06-10T21:55:10.902421Z","shell.execute_reply":"2021-06-10T21:55:15.294742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nencoded = tokenizer(str(df_train), padding=True, truncation=True, return_tensors='pt')\n\nout = model(**encoded)\n\nlogits = out.logits\nlogits.shape, logits","metadata":{"execution":{"iopub.status.busy":"2021-06-10T21:55:31.670116Z","iopub.execute_input":"2021-06-10T21:55:31.670536Z","iopub.status.idle":"2021-06-10T21:55:32.890456Z","shell.execute_reply.started":"2021-06-10T21:55:31.670507Z","shell.execute_reply":"2021-06-10T21:55:32.889346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import KFold, StratifiedKFold\n\ndf = shuffle(df_train)\nkf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1024)\ny = df['label']\nfold_list = list(kf.split(df, y))\n\ntrain_df_list = []\nval_df_list = []\n\nfor i, fold in enumerate(fold_list):\n\n    df_train = df[df.index.isin(fold[0])]\n    df_val = df[df.index.isin(fold[1])]\n    train_df_list.append(df_train)\n    val_df_list.append(df_val)\n    \n\nprint(len(train_df_list))\nprint(len(val_df_list))","metadata":{"execution":{"iopub.status.busy":"2021-06-10T21:55:45.285997Z","iopub.execute_input":"2021-06-10T21:55:45.286379Z","iopub.status.idle":"2021-06-10T21:55:45.343623Z","shell.execute_reply.started":"2021-06-10T21:55:45.286348Z","shell.execute_reply":"2021-06-10T21:55:45.342288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = train_df_list[0]\n\ndf_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-10T21:55:55.309583Z","iopub.execute_input":"2021-06-10T21:55:55.310117Z","iopub.status.idle":"2021-06-10T21:55:55.325454Z","shell.execute_reply.started":"2021-06-10T21:55:55.310065Z","shell.execute_reply":"2021-06-10T21:55:55.324153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_val = val_df_list[0]\n\ndf_val.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-10T21:56:05.083722Z","iopub.execute_input":"2021-06-10T21:56:05.084252Z","iopub.status.idle":"2021-06-10T21:56:05.10248Z","shell.execute_reply.started":"2021-06-10T21:56:05.084211Z","shell.execute_reply":"2021-06-10T21:56:05.100729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MODEL_TYPE = 'bert-base-uncased'\n\nNUM_FOLDS = 5\nNUM_FOLDS_TO_TRAIN = 3\nL_RATE = 1e-5\nMAX_LEN = 256\nNUM_EPOCHS = 3\nBATCH_SIZE = 32\nNUM_CORES = os.cpu_count()\n","metadata":{"execution":{"iopub.status.busy":"2021-06-10T21:56:25.525988Z","iopub.execute_input":"2021-06-10T21:56:25.526384Z","iopub.status.idle":"2021-06-10T21:56:25.532153Z","shell.execute_reply.started":"2021-06-10T21:56:25.526352Z","shell.execute_reply":"2021-06-10T21:56:25.530854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# For TPU\ndevice = xm.xla_device()\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T21:56:35.739708Z","iopub.execute_input":"2021-06-10T21:56:35.7401Z","iopub.status.idle":"2021-06-10T21:56:40.920575Z","shell.execute_reply.started":"2021-06-10T21:56:35.740071Z","shell.execute_reply":"2021-06-10T21:56:40.919788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import BertTokenizer\nprint('Loading BERT tokenizer...')\ntokenizer = BertTokenizer.from_pretrained(MODEL_TYPE, do_lower_case=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T21:56:52.647096Z","iopub.execute_input":"2021-06-10T21:56:52.647456Z","iopub.status.idle":"2021-06-10T21:56:53.772376Z","shell.execute_reply.started":"2021-06-10T21:56:52.647426Z","shell.execute_reply":"2021-06-10T21:56:53.771003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CompDataset(Dataset):\n\n    def __init__(self, df):\n        self.df_data = df\n\n\n\n    def __getitem__(self, index):\n\n        # get the sentence from the dataframe\n        sentence1 = self.df_data.loc[index, 'premise']\n        sentence2 = self.df_data.loc[index, 'hypothesis']\n\n        # Process the sentence\n        # ---------------------\n\n        encoded_dict = tokenizer.encode_plus(\n                    sentence1, sentence2,           # Sentences to encode.\n                    add_special_tokens = True,      # Add '[CLS]' and '[SEP]'\n                    max_length = MAX_LEN,           # Pad or truncate all sentences.\n                    pad_to_max_length = True,\n                    return_attention_mask = True,   # Construct attn. masks.\n                    return_tensors = 'pt',          # Return pytorch tensors.\n               )  \n        \n        # These are torch tensors already.\n        padded_token_list = encoded_dict['input_ids'][0]\n        att_mask = encoded_dict['attention_mask'][0]\n        token_type_ids = encoded_dict['token_type_ids'][0]\n        \n        # Convert the target to a torch tensor\n        target = torch.tensor(self.df_data.loc[index, 'label'])\n\n        sample = (padded_token_list, att_mask, token_type_ids, target)\n\n\n        return sample\n\n\n    def __len__(self):\n        return len(self.df_data)\n    \n","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:04:10.482853Z","iopub.execute_input":"2021-06-10T22:04:10.483269Z","iopub.status.idle":"2021-06-10T22:04:10.491976Z","shell.execute_reply.started":"2021-06-10T22:04:10.48323Z","shell.execute_reply":"2021-06-10T22:04:10.491014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TestDataset(Dataset):\n\n    def __init__(self, df):\n        self.df_data = df\n\n\n\n    def __getitem__(self, index):\n\n        # get the sentence from the dataframe\n        sentence1 = self.df_data.loc[index, 'premise']\n        sentence2 = self.df_data.loc[index, 'hypothesis']\n\n        # Process the sentence\n        # ---------------------\n\n        encoded_dict = tokenizer.encode_plus(\n                    sentence1, sentence2,           # Sentence to encode.\n                    add_special_tokens = True,      # Add '[CLS]' and '[SEP]'\n                    max_length = MAX_LEN,           # Pad or truncate all sentences.\n                    pad_to_max_length = True,\n                    return_attention_mask = True,   # Construct attn. masks.\n                    return_tensors = 'pt',          # Return pytorch tensors.\n               )\n        \n        # These are torch tensors already.\n        padded_token_list = encoded_dict['input_ids'][0]\n        att_mask = encoded_dict['attention_mask'][0]\n        token_type_ids = encoded_dict['token_type_ids'][0]\n               \n\n        sample = (padded_token_list, att_mask, token_type_ids)\n\n\n        return sample\n\n\n    def __len__(self):\n        return len(self.df_data)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:04:13.428377Z","iopub.execute_input":"2021-06-10T22:04:13.428876Z","iopub.status.idle":"2021-06-10T22:04:13.440198Z","shell.execute_reply.started":"2021-06-10T22:04:13.428834Z","shell.execute_reply":"2021-06-10T22:04:13.439079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = df_train.reset_index(drop=True)\ndf_val = df_val.reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:04:16.295394Z","iopub.execute_input":"2021-06-10T22:04:16.29576Z","iopub.status.idle":"2021-06-10T22:04:16.304614Z","shell.execute_reply.started":"2021-06-10T22:04:16.295727Z","shell.execute_reply":"2021-06-10T22:04:16.303383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = CompDataset(df_train)\nval_data = CompDataset(df_val)\ntest_data = TestDataset(df_test)\n\ntrain_dataloader = torch.utils.data.DataLoader(train_data,batch_size=BATCH_SIZE,shuffle=True,num_workers=NUM_CORES)\n\nval_dataloader = torch.utils.data.DataLoader(val_data,batch_size=BATCH_SIZE,shuffle=True,num_workers=NUM_CORES)\n\ntest_dataloader = torch.utils.data.DataLoader(test_data,batch_size=BATCH_SIZE,shuffle=False,num_workers=NUM_CORES)\n\n\n\nprint(len(train_dataloader))\nprint(len(val_dataloader))\nprint(len(test_dataloader))","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:04:18.190214Z","iopub.execute_input":"2021-06-10T22:04:18.190572Z","iopub.status.idle":"2021-06-10T22:04:18.202725Z","shell.execute_reply.started":"2021-06-10T22:04:18.190541Z","shell.execute_reply":"2021-06-10T22:04:18.201043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"padded_token_list, att_mask, token_type_ids, target = next(iter(train_dataloader))\n\nprint(padded_token_list.shape)\nprint(att_mask.shape)\nprint(token_type_ids.shape)\nprint(target.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:04:21.023549Z","iopub.execute_input":"2021-06-10T22:04:21.024116Z","iopub.status.idle":"2021-06-10T22:04:21.455106Z","shell.execute_reply.started":"2021-06-10T22:04:21.02408Z","shell.execute_reply":"2021-06-10T22:04:21.453169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get one train batch\n\npadded_token_list, att_mask, token_type_ids, target = next(iter(val_dataloader))\n\nprint(padded_token_list.shape)\nprint(att_mask.shape)\nprint(token_type_ids.shape)\nprint(target.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:05:50.33357Z","iopub.execute_input":"2021-06-10T22:05:50.335061Z","iopub.status.idle":"2021-06-10T22:05:50.889738Z","shell.execute_reply.started":"2021-06-10T22:05:50.33477Z","shell.execute_reply":"2021-06-10T22:05:50.888304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get one test batch\n\npadded_token_list, att_mask, token_type_ids = next(iter(test_dataloader))\n\nprint(padded_token_list.shape)\nprint(att_mask.shape)\nprint(token_type_ids.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:06:04.217035Z","iopub.execute_input":"2021-06-10T22:06:04.217505Z","iopub.status.idle":"2021-06-10T22:06:04.670979Z","shell.execute_reply.started":"2021-06-10T22:06:04.21746Z","shell.execute_reply":"2021-06-10T22:06:04.669381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get one val batch\n\npadded_token_list, att_mask, token_type_ids, target = next(iter(val_dataloader))\n\nprint(padded_token_list.shape)\nprint(att_mask.shape)\nprint(token_type_ids.shape)\nprint(target.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:06:14.087856Z","iopub.execute_input":"2021-06-10T22:06:14.088541Z","iopub.status.idle":"2021-06-10T22:06:14.534855Z","shell.execute_reply.started":"2021-06-10T22:06:14.088496Z","shell.execute_reply":"2021-06-10T22:06:14.533637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nmodel = BertForSequenceClassification.from_pretrained(\n    MODEL_TYPE, \n    num_labels = 3, \n    output_attentions = False,\n    output_hidden_states = False)\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:06:24.441197Z","iopub.execute_input":"2021-06-10T22:06:24.441576Z","iopub.status.idle":"2021-06-10T22:06:30.645623Z","shell.execute_reply.started":"2021-06-10T22:06:24.441543Z","shell.execute_reply":"2021-06-10T22:06:30.64438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataloader = torch.utils.data.DataLoader(train_data,\n                                        batch_size=8,\n                                        shuffle=True,\n                                       num_workers=NUM_CORES)\n\nbatch = next(iter(train_dataloader))\n\nb_input_ids = batch[0].to(device)\nb_input_mask = batch[1].to(device)\nb_token_type_ids = batch[2].to(device)\nb_labels = batch[3].to(device)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:06:46.089887Z","iopub.execute_input":"2021-06-10T22:06:46.090551Z","iopub.status.idle":"2021-06-10T22:06:46.481387Z","shell.execute_reply.started":"2021-06-10T22:06:46.090512Z","shell.execute_reply":"2021-06-10T22:06:46.479913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"outputs = model(b_input_ids, \n                token_type_ids=b_token_type_ids, \n                attention_mask=b_input_mask,\n                labels=b_labels)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:07:06.343545Z","iopub.execute_input":"2021-06-10T22:07:06.343998Z","iopub.status.idle":"2021-06-10T22:07:06.415951Z","shell.execute_reply.started":"2021-06-10T22:07:06.343961Z","shell.execute_reply":"2021-06-10T22:07:06.414884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"outputs","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:07:15.024483Z","iopub.execute_input":"2021-06-10T22:07:15.02489Z","iopub.status.idle":"2021-06-10T22:07:59.590463Z","shell.execute_reply.started":"2021-06-10T22:07:15.024854Z","shell.execute_reply":"2021-06-10T22:07:59.589292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(type(outputs[0]))\nprint(type(outputs[1]))","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:09:26.744525Z","iopub.execute_input":"2021-06-10T22:09:26.745126Z","iopub.status.idle":"2021-06-10T22:09:26.752721Z","shell.execute_reply.started":"2021-06-10T22:09:26.745079Z","shell.execute_reply":"2021-06-10T22:09:26.751493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n\n# Set a seed value.\nseed_val = 1024\n\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)\n\n\n\n# Store the accuracy scores for each fold model in this list.\n# [[model_0 scores], [model_1 scores], [model_2 scores], [model_3 scores], [model_4 scores]]\n# [[ecpoch 1, epoch 2, ...], [ecpoch 1, epoch 2, ...], [ecpoch 1, epoch 2, ...], [ecpoch 1, epoch 2, ...], [ecpoch 1, epoch 2, ...]]\n\n# Create a list of lists to store the val acc results.\n# The number of items in this list will correspond to\n# the number of folds that the model is being trained on.\nfold_val_acc_list = []\nfor i in range(0, NUM_FOLDS):\n    \n    # append an empty list\n    fold_val_acc_list.append([])\n    \n    \n    \n    \n\n# For each epoch...\nfor epoch in range(0, NUM_EPOCHS):\n    \n    print(\"\\nNum folds used for training:\", NUM_FOLDS_TO_TRAIN)\n    print('======== Epoch {:} / {:} ========'.format(epoch + 1, NUM_EPOCHS))\n    \n    # Get the number of folds\n    num_folds = len(train_df_list)\n\n    # For this epoch, store the val acc scores for each fold in this list.\n    # We will use this list to calculate the cv at the end of the epoch.\n    epoch_acc_scores_list = []\n    \n    # For each fold...\n    for fold_index in range(0, NUM_FOLDS_TO_TRAIN):\n        \n        print('\\n== Fold Model', fold_index)\n        \n        \n        # .........................\n        # Load the fold model\n        # .........................\n        \n        if epoch == 0:\n            \n            # define the model\n            model = BertForSequenceClassification.from_pretrained(\n            MODEL_TYPE, \n            num_labels = 3,       \n            output_attentions = False, \n            output_hidden_states = False,\n            )\n            \n            # Send the model to the device.\n            model.to(device)\n            \n            optimizer = AdamW(model.parameters(),\n              lr = L_RATE, \n              eps = 1e-8\n            )\n            \n        else:\n        \n            # Get the fold model\n            path_model = 'model_' + str(fold_index) + '.bin'\n            model.load_state_dict(torch.load(path_model))\n\n            # Send the model to the device.\n            model.to(device)\n        \n        \n        \n        # .....................................\n        # Set up the train and val dataloaders\n        # .....................................\n        \n        \n        # Intialize the fold dataframes\n        df_train = train_df_list[fold_index]\n        df_val = val_df_list[fold_index]\n        \n        # Reset the indices or the dataloader won't work.\n        df_train = df_train.reset_index(drop=True)\n        df_val = df_val.reset_index(drop=True)\n    \n        # Create the dataloaders\n        train_data = CompDataset(df_train)\n        val_data = CompDataset(df_val)\n\n        train_dataloader = torch.utils.data.DataLoader(train_data,\n                                                batch_size=BATCH_SIZE,\n                                                shuffle=True,\n                                               num_workers=NUM_CORES)\n\n        val_dataloader = torch.utils.data.DataLoader(val_data,\n                                                batch_size=BATCH_SIZE,\n                                                shuffle=True,\n                                               num_workers=NUM_CORES)\n    \n    \n    \n\n       \n\n        # ========================================\n        #               Training\n        # ========================================\n        \n        stacked_val_labels = []\n        targets_list = []\n\n        print('Training...')\n\n        # put the model into train mode\n        model.train()\n\n        # This turns gradient calculations on and off.\n        torch.set_grad_enabled(True)\n\n\n        # Reset the total loss for this epoch.\n        total_train_loss = 0\n\n        for i, batch in enumerate(train_dataloader):\n\n            train_status = 'Batch ' + str(i+1) + ' of ' + str(len(train_dataloader))\n\n            print(train_status, end='\\r')\n\n\n            b_input_ids = batch[0].to(device)\n            b_input_mask = batch[1].to(device)\n            b_token_type_ids = batch[2].to(device)\n            b_labels = batch[3].to(device)\n\n            model.zero_grad()        \n\n\n            outputs = model(b_input_ids, \n                        token_type_ids=b_token_type_ids, \n                        attention_mask=b_input_mask,\n                        labels=b_labels)\n\n            # Get the loss from the outputs tuple: (loss, logits)\n            loss = outputs[0]\n\n            # Convert the loss from a torch tensor to a number.\n            # Calculate the total loss.\n            total_train_loss = total_train_loss + loss.item()\n\n            # Zero the gradients\n            optimizer.zero_grad()\n\n            # Perform a backward pass to calculate the gradients.\n            loss.backward()\n            \n            # Clip the norm of the gradients to 1.0.\n            # This is to help prevent the \"exploding gradients\" problem.\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n            # Use the optimizer to update Weights\n            \n            # Optimizer for GPU\n            # optimizer.step() \n            \n            # Optimizer for TPU\n            # https://pytorch.org/xla/\n            xm.optimizer_step(optimizer, barrier=True)\n            \n           \n\n\n        print('Train loss:' ,total_train_loss)\n\n\n        # ========================================\n        #               Validation\n        # ========================================\n\n        print('\\nValidation...')\n\n        # Put the model in evaluation mode.\n        model.eval()\n\n        # Turn off the gradient calculations.\n        # This tells the model not to compute or store gradients.\n        # This step saves memory and speeds up validation.\n        torch.set_grad_enabled(False)\n\n\n        # Reset the total loss for this epoch.\n        total_val_loss = 0\n\n\n        for j, val_batch in enumerate(val_dataloader):\n\n            val_status = 'Batch ' + str(j+1) + ' of ' + str(len(val_dataloader))\n\n            print(val_status, end='\\r')\n\n            b_input_ids = val_batch[0].to(device)\n            b_input_mask = val_batch[1].to(device)\n            b_token_type_ids = val_batch[2].to(device)\n            b_labels = val_batch[3].to(device)      \n\n\n            outputs = model(b_input_ids, \n                    token_type_ids=b_token_type_ids, \n                    attention_mask=b_input_mask, \n                    labels=b_labels)\n\n            # Get the loss from the outputs tuple: (loss, logits)\n            loss = outputs[0]\n\n            # Convert the loss from a torch tensor to a number.\n            # Calculate the total loss.\n            total_val_loss = total_val_loss + loss.item()\n\n            # Get the preds\n            preds = outputs[1]\n\n\n            # Move preds to the CPU\n            val_preds = preds.detach().cpu().numpy()\n\n            # Move the labels to the cpu\n            targets_np = b_labels.to('cpu').numpy()\n\n            # Append the labels to a numpy list\n            targets_list.extend(targets_np)\n\n            if j == 0:  # first batch\n                stacked_val_preds = val_preds\n\n            else:\n                stacked_val_preds = np.vstack((stacked_val_preds, val_preds))\n                \n                \n                \n        # .........................................\n        # Calculate the val accuracy for this fold\n        # .........................................      \n\n\n        # Calculate the validation accuracy\n        y_true = targets_list\n        y_pred = np.argmax(stacked_val_preds, axis=1)\n\n        val_acc = accuracy_score(y_true, y_pred)\n        \n        \n        epoch_acc_scores_list.append(val_acc)\n\n\n        print('Val loss:' ,total_val_loss)\n        print('Val acc: ', val_acc)\n        \n        \n        # .........................\n        # Save the best model\n        # .........................\n        \n        if epoch == 0:\n            \n            # Save the Model\n            model_name = 'model_' + str(fold_index) + '.bin'\n            torch.save(model.state_dict(), model_name)\n            print('Saved model as ', model_name)\n            \n        if epoch != 0:\n        \n            val_acc_list = fold_val_acc_list[fold_index]\n            best_val_acc = max(val_acc_list)\n            \n            if val_acc > best_val_acc:\n                # save the model\n                model_name = 'model_' + str(fold_index) + '.bin'\n                torch.save(model.state_dict(), model_name)\n                print('Val acc improved. Saved model as ', model_name)\n                \n \n        fold_val_acc_list[fold_index].append(val_acc)\n        \n            \n\n        # Use the garbage collector to save memory.\n        gc.collect()\n\n        \n    # Print the average val accuracy for all 5 folds\n    cv_acc = sum(epoch_acc_scores_list)/NUM_FOLDS_TO_TRAIN\n    print(\"\\nCV Acc:\", cv_acc)\n    \n","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:10:08.79573Z","iopub.execute_input":"2021-06-10T22:10:08.796162Z","iopub.status.idle":"2021-06-10T22:36:57.811742Z","shell.execute_reply.started":"2021-06-10T22:10:08.79613Z","shell.execute_reply":"2021-06-10T22:36:57.810476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**XLM-RoBERTa Model**","metadata":{}},{"cell_type":"code","source":"MODEL_TYPE = 'xlm-roberta-base'","metadata":{"execution":{"iopub.status.busy":"2021-06-10T23:46:15.896597Z","iopub.execute_input":"2021-06-10T23:46:15.897216Z","iopub.status.idle":"2021-06-10T23:46:15.903155Z","shell.execute_reply.started":"2021-06-10T23:46:15.897154Z","shell.execute_reply":"2021-06-10T23:46:15.901778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = train_df_list[0]\n\ndf_train.head()\n\n","metadata":{"execution":{"iopub.status.busy":"2021-06-10T23:46:17.923972Z","iopub.execute_input":"2021-06-10T23:46:17.924355Z","iopub.status.idle":"2021-06-10T23:46:17.942059Z","shell.execute_reply.started":"2021-06-10T23:46:17.924322Z","shell.execute_reply":"2021-06-10T23:46:17.940623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_val = val_df_list[0]\n\ndf_val.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-10T23:46:22.469343Z","iopub.execute_input":"2021-06-10T23:46:22.469739Z","iopub.status.idle":"2021-06-10T23:46:22.483609Z","shell.execute_reply.started":"2021-06-10T23:46:22.469707Z","shell.execute_reply":"2021-06-10T23:46:22.48264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification\n\n# xlm-roberta-large\nprint('Loading XLMRoberta tokenizer...')\ntokenizer = XLMRobertaTokenizer.from_pretrained(MODEL_TYPE)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T23:46:26.233733Z","iopub.execute_input":"2021-06-10T23:46:26.234159Z","iopub.status.idle":"2021-06-10T23:46:28.322843Z","shell.execute_reply.started":"2021-06-10T23:46:26.234123Z","shell.execute_reply":"2021-06-10T23:46:28.321559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = df_train.reset_index(drop=True)\ndf_val = df_val.reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T23:46:35.292069Z","iopub.execute_input":"2021-06-10T23:46:35.292455Z","iopub.status.idle":"2021-06-10T23:46:35.301589Z","shell.execute_reply.started":"2021-06-10T23:46:35.292422Z","shell.execute_reply":"2021-06-10T23:46:35.300461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CompDataset(Dataset):\n\n    def __init__(self, df):\n        self.df_data = df\n\n\n\n    def __getitem__(self, index):\n\n        # get the sentence from the dataframe\n        sentence1 = self.df_data.loc[index, 'premise']\n        sentence2 = self.df_data.loc[index, 'hypothesis']\n\n        # Process the sentence\n        # ---------------------\n\n        encoded_dict = tokenizer.encode_plus(\n                    sentence1, sentence2,           # Sentences to encode.\n                    add_special_tokens = True,      # Add the special tokens.\n                    max_length = MAX_LEN,           # Pad & truncate all sentences.\n                    pad_to_max_length = True,\n                    return_attention_mask = True,   # Construct attn. masks.\n                    return_tensors = 'pt',          # Return pytorch tensors.\n               )\n        \n        # These are torch tensors.\n        padded_token_list = encoded_dict['input_ids'][0]\n        att_mask = encoded_dict['attention_mask'][0]\n        \n        # Convert the target to a torch tensor\n        target = torch.tensor(self.df_data.loc[index, 'label'])\n\n        sample = (padded_token_list, att_mask, target)\n\n\n        return sample\n\n\n    def __len__(self):\n        return len(self.df_data)\n    \n    \n    \n    \n    \n\nclass TestDataset(Dataset):\n\n    def __init__(self, df):\n        self.df_data = df\n\n\n\n    def __getitem__(self, index):\n\n        # get the sentence from the dataframe\n        sentence1 = self.df_data.loc[index, 'premise']\n        sentence2 = self.df_data.loc[index, 'hypothesis']\n\n        # Process the sentence\n        # ---------------------\n\n        encoded_dict = tokenizer.encode_plus(\n                    sentence1, sentence2,           # Sentence to encode.\n                    add_special_tokens = True,      # Add the special tokens.\n                    max_length = MAX_LEN,           # Pad & truncate all sentences.\n                    pad_to_max_length = True,\n                    return_attention_mask = True,   # Construct attn. masks.\n                    return_tensors = 'pt',          # Return pytorch tensors.\n               )\n        \n        # These are torch tensors.\n        padded_token_list = encoded_dict['input_ids'][0]\n        att_mask = encoded_dict['attention_mask'][0]\n        \n               \n\n        sample = (padded_token_list, att_mask)\n\n\n        return sample\n\n\n    def __len__(self):\n        return len(self.df_data)\n","metadata":{"execution":{"iopub.status.busy":"2021-06-10T23:46:37.897003Z","iopub.execute_input":"2021-06-10T23:46:37.89738Z","iopub.status.idle":"2021-06-10T23:46:37.923218Z","shell.execute_reply.started":"2021-06-10T23:46:37.897344Z","shell.execute_reply":"2021-06-10T23:46:37.92178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = CompDataset(df_train)\nval_data = CompDataset(df_val)\ntest_data = TestDataset(df_test)\n\ntrain_dataloader = torch.utils.data.DataLoader(train_data,\n                                        batch_size=BATCH_SIZE,\n                                        shuffle=True,\n                                       num_workers=NUM_CORES)\n\nval_dataloader = torch.utils.data.DataLoader(val_data,\n                                        batch_size=BATCH_SIZE,\n                                        shuffle=True,\n                                       num_workers=NUM_CORES)\n\ntest_dataloader = torch.utils.data.DataLoader(test_data,\n                                        batch_size=BATCH_SIZE,\n                                        shuffle=False,\n                                       num_workers=NUM_CORES)\n\n\n\nprint(len(train_dataloader))\nprint(len(val_dataloader))\nprint(len(test_dataloader))","metadata":{"execution":{"iopub.status.busy":"2021-06-10T23:46:42.13384Z","iopub.execute_input":"2021-06-10T23:46:42.134297Z","iopub.status.idle":"2021-06-10T23:46:42.144959Z","shell.execute_reply.started":"2021-06-10T23:46:42.134256Z","shell.execute_reply":"2021-06-10T23:46:42.143305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get one train batch\n\npadded_token_list, att_mask, target = next(iter(train_dataloader))\n\nprint(padded_token_list.shape)\nprint(att_mask.shape)\nprint(target.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T23:46:45.513608Z","iopub.execute_input":"2021-06-10T23:46:45.513984Z","iopub.status.idle":"2021-06-10T23:46:46.163416Z","shell.execute_reply.started":"2021-06-10T23:46:45.513952Z","shell.execute_reply":"2021-06-10T23:46:46.162188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get one val batch\n\npadded_token_list, att_mask, target = next(iter(val_dataloader))\n\nprint(padded_token_list.shape)\nprint(att_mask.shape)\nprint(target.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T23:46:48.973256Z","iopub.execute_input":"2021-06-10T23:46:48.973714Z","iopub.status.idle":"2021-06-10T23:46:49.662655Z","shell.execute_reply.started":"2021-06-10T23:46:48.973672Z","shell.execute_reply":"2021-06-10T23:46:49.660394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get one test batch\n\npadded_token_list, att_mask = next(iter(test_dataloader))\n\nprint(padded_token_list.shape)\nprint(att_mask.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T23:46:52.347173Z","iopub.execute_input":"2021-06-10T23:46:52.347622Z","iopub.status.idle":"2021-06-10T23:46:52.987549Z","shell.execute_reply.started":"2021-06-10T23:46:52.347585Z","shell.execute_reply":"2021-06-10T23:46:52.986209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import XLMRobertaForSequenceClassification\n\nmodel = XLMRobertaForSequenceClassification.from_pretrained(\n    MODEL_TYPE, \n    num_labels = 3, # The number of output labels. 2 for binary classification.\n)\n\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T23:46:55.669452Z","iopub.execute_input":"2021-06-10T23:46:55.669852Z","iopub.status.idle":"2021-06-10T23:47:16.20446Z","shell.execute_reply.started":"2021-06-10T23:46:55.669816Z","shell.execute_reply":"2021-06-10T23:47:16.203278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the optimizer\noptimizer = AdamW(model.parameters(),\n              lr = L_RATE, \n              eps = 1e-8 \n            )","metadata":{"execution":{"iopub.status.busy":"2021-06-10T23:47:24.293704Z","iopub.execute_input":"2021-06-10T23:47:24.294297Z","iopub.status.idle":"2021-06-10T23:47:24.309756Z","shell.execute_reply.started":"2021-06-10T23:47:24.29423Z","shell.execute_reply":"2021-06-10T23:47:24.308521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntrain_dataloader = torch.utils.data.DataLoader(train_data,\n                                        batch_size=BATCH_SIZE,\n                                        shuffle=True,\n                                       num_workers=NUM_CORES)\n\nb_input_ids, b_input_mask, b_labels = next(iter(train_dataloader))\n\nprint(b_input_ids.shape)\nprint(b_input_mask.shape)\nprint(b_labels.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T23:47:27.244636Z","iopub.execute_input":"2021-06-10T23:47:27.245057Z","iopub.status.idle":"2021-06-10T23:47:28.04087Z","shell.execute_reply.started":"2021-06-10T23:47:27.245022Z","shell.execute_reply":"2021-06-10T23:47:28.039554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Send the data to the device\nb_input_ids = batch[0].to(device)\nb_input_mask = batch[1].to(device)\nb_labels = batch[2].to(device)\n\noutputs = model(b_input_ids, \n                        attention_mask=b_input_mask, \n                        labels=b_labels)\n\n# The ouput is a tuple (loss, preds).\noutputs","metadata":{"execution":{"iopub.status.busy":"2021-06-10T23:47:30.745206Z","iopub.execute_input":"2021-06-10T23:47:30.745617Z","iopub.status.idle":"2021-06-10T23:47:34.118983Z","shell.execute_reply.started":"2021-06-10T23:47:30.745583Z","shell.execute_reply":"2021-06-10T23:47:34.118138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n\n# Set the seed.\nseed_val = 101\n\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)\n\n# Store the average loss after each epoch so we can plot them.\nloss_values = []\n\n\n# For each epoch...\nfor epoch in range(0, NUM_EPOCHS):\n    \n    print(\"\")\n    print('======== Epoch {:} / {:} ========'.format(epoch + 1, NUM_EPOCHS))\n    \n\n    stacked_val_labels = []\n    targets_list = []\n\n    # ========================================\n    #               Training\n    # ========================================\n    \n    print('Training...')\n    \n    # put the model into train mode\n    model.train()\n    \n    # This turns gradient calculations on and off.\n    torch.set_grad_enabled(True)\n\n\n    # Reset the total loss for this epoch.\n    total_train_loss = 0\n\n    for i, batch in enumerate(train_dataloader):\n        \n        train_status = 'Batch ' + str(i) + ' of ' + str(len(train_dataloader))\n        \n        print(train_status, end='\\r')\n\n\n        b_input_ids = batch[0].to(device)\n        b_input_mask = batch[1].to(device)\n        b_labels = batch[2].to(device)\n\n        model.zero_grad()        \n\n\n        outputs = model(b_input_ids, \n                    attention_mask=b_input_mask,\n                    labels=b_labels)\n        \n        # Get the loss from the outputs tuple: (loss, logits)\n        loss = outputs[0]\n        \n        # Convert the loss from a torch tensor to a number.\n        # Calculate the total loss.\n        total_train_loss = total_train_loss + loss.item()\n        \n        # Zero the gradients\n        optimizer.zero_grad()\n        \n        # Perform a backward pass to calculate the gradients.\n        loss.backward()\n        \n        \n        # Clip the norm of the gradients to 1.0.\n        # This is to help prevent the \"exploding gradients\" problem.\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        \n        \n        \n        # Use the optimizer to update the weights.\n        \n        # Optimizer for GPU\n        # optimizer.step() \n        \n        # Optimizer for TPU\n        # https://pytorch.org/xla/\n        xm.optimizer_step(optimizer, barrier=True)\n\n    \n    print('Train loss:' ,total_train_loss)\n\n\n    # ========================================\n    #               Validation\n    # ========================================\n    \n    print('\\nValidation...')\n\n    # Put the model in evaluation mode.\n    model.eval()\n\n    # Turn off the gradient calculations.\n    # This tells the model not to compute or store gradients.\n    # This step saves memory and speeds up validation.\n    torch.set_grad_enabled(False)\n    \n    \n    # Reset the total loss for this epoch.\n    total_val_loss = 0\n    \n\n    for j, batch in enumerate(val_dataloader):\n        \n        val_status = 'Batch ' + str(j) + ' of ' + str(len(val_dataloader))\n        \n        print(val_status, end='\\r')\n\n        b_input_ids = batch[0].to(device)\n        b_input_mask = batch[1].to(device)\n        b_labels = batch[2].to(device)      \n\n\n        outputs = model(b_input_ids, \n                attention_mask=b_input_mask, \n                labels=b_labels)\n        \n        # Get the loss from the outputs tuple: (loss, logits)\n        loss = outputs[0]\n        \n        # Convert the loss from a torch tensor to a number.\n        # Calculate the total loss.\n        total_val_loss = total_val_loss + loss.item()\n        \n\n        # Get the preds\n        preds = outputs[1]\n\n\n        # Move preds to the CPU\n        val_preds = preds.detach().cpu().numpy()\n        \n        # Move the labels to the cpu\n        targets_np = b_labels.to('cpu').numpy()\n\n        # Append the labels to a numpy list\n        targets_list.extend(targets_np)\n\n        if j == 0:  # first batch\n            stacked_val_preds = val_preds\n\n        else:\n            stacked_val_preds = np.vstack((stacked_val_preds, val_preds))\n\n    \n    # Calculate the validation accuracy\n    y_true = targets_list\n    y_pred = np.argmax(stacked_val_preds, axis=1)\n    \n    val_acc = accuracy_score(y_true, y_pred)\n    \n    \n    print('Val loss:' ,total_val_loss)\n    print('Val acc: ', val_acc)\n\n\n    # Save the Model\n    torch.save(model.state_dict(), 'model.pt')\n    \n    # Use the garbage collector to save memory.\n    gc.collect()\n","metadata":{"execution":{"iopub.status.busy":"2021-06-10T23:47:37.437202Z","iopub.execute_input":"2021-06-10T23:47:37.437983Z","iopub.status.idle":"2021-06-10T23:57:41.861162Z","shell.execute_reply.started":"2021-06-10T23:47:37.437896Z","shell.execute_reply":"2021-06-10T23:57:41.860039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Predictions**","metadata":{}},{"cell_type":"code","source":"for j, batch in enumerate(test_dataloader):\n        \n        inference_status = 'Batch ' + str(j+1) + ' of ' + str(len(test_dataloader))\n        \n        print(inference_status, end='\\r')\n\n        b_input_ids = batch[0].to(device)\n        b_input_mask = batch[1].to(device)\n\n\n        outputs = model(b_input_ids, \n                attention_mask=b_input_mask)\n        \n        \n        # Get the preds\n        preds = outputs[0]\n\n\n        # Move preds to the CPU\n        preds = preds.detach().cpu().numpy()\n        \n        # Move the labels to the cpu\n        targets_np = b_labels.to('cpu').numpy()\n\n        # Append the labels to a numpy list\n        targets_list.extend(targets_np)\n        \n        # Stack the predictions.\n\n        if j == 0:  # first batch\n            stacked_preds = preds\n\n        else:\n            stacked_preds = np.vstack((stacked_preds, preds))\n","metadata":{"execution":{"iopub.status.busy":"2021-06-10T23:57:59.22092Z","iopub.execute_input":"2021-06-10T23:57:59.221362Z","iopub.status.idle":"2021-06-10T23:58:20.313876Z","shell.execute_reply.started":"2021-06-10T23:57:59.221326Z","shell.execute_reply":"2021-06-10T23:58:20.312644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stacked_preds","metadata":{"execution":{"iopub.status.busy":"2021-06-10T23:58:48.660374Z","iopub.execute_input":"2021-06-10T23:58:48.660808Z","iopub.status.idle":"2021-06-10T23:58:48.670545Z","shell.execute_reply.started":"2021-06-10T23:58:48.660768Z","shell.execute_reply":"2021-06-10T23:58:48.669114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = np.argmax(stacked_preds, axis=1)\npreds","metadata":{"execution":{"iopub.status.busy":"2021-06-10T23:59:01.204091Z","iopub.execute_input":"2021-06-10T23:59:01.204485Z","iopub.status.idle":"2021-06-10T23:59:01.211882Z","shell.execute_reply.started":"2021-06-10T23:59:01.204421Z","shell.execute_reply":"2021-06-10T23:59:01.210704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = '../input/contradictory-my-dear-watson/sample_submission.csv'\n\ndf_sample = pd.read_csv(path)\n\nprint(df_sample.shape)\n\ndf_sample.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-10T23:59:15.494182Z","iopub.execute_input":"2021-06-10T23:59:15.494565Z","iopub.status.idle":"2021-06-10T23:59:15.528758Z","shell.execute_reply.started":"2021-06-10T23:59:15.494533Z","shell.execute_reply":"2021-06-10T23:59:15.527771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import BartForCausalLM, BartTokenizerFast\n\nMODEL_TYPE = 'facebook/bart-large-cnn'","metadata":{"execution":{"iopub.status.busy":"2021-06-11T00:04:40.855374Z","iopub.execute_input":"2021-06-11T00:04:40.855855Z","iopub.status.idle":"2021-06-11T00:04:40.962899Z","shell.execute_reply.started":"2021-06-11T00:04:40.855816Z","shell.execute_reply":"2021-06-11T00:04:40.962103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = train_df_list[0]\n\ndf_train.head()\n\n","metadata":{"execution":{"iopub.status.busy":"2021-06-11T00:21:06.657871Z","iopub.execute_input":"2021-06-11T00:21:06.658339Z","iopub.status.idle":"2021-06-11T00:21:06.676004Z","shell.execute_reply.started":"2021-06-11T00:21:06.658301Z","shell.execute_reply":"2021-06-11T00:21:06.674848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_val = val_df_list[0]\n\ndf_val.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-11T00:21:18.402231Z","iopub.execute_input":"2021-06-11T00:21:18.402637Z","iopub.status.idle":"2021-06-11T00:21:18.418777Z","shell.execute_reply.started":"2021-06-11T00:21:18.402602Z","shell.execute_reply":"2021-06-11T00:21:18.417668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import BartForCausalLM, BartTokenizerFast\n\n \n\nmodel_name = 'facebook/bart-large-cnn'\ntokenizer = BartTokenizerFast.from_pretrained(MODEL_TYPE)\n ","metadata":{"execution":{"iopub.status.busy":"2021-06-11T00:04:53.898809Z","iopub.execute_input":"2021-06-11T00:04:53.899509Z","iopub.status.idle":"2021-06-11T00:04:56.693791Z","shell.execute_reply.started":"2021-06-11T00:04:53.899462Z","shell.execute_reply":"2021-06-11T00:04:56.692455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = df_train.reset_index(drop=True)\ndf_val = df_val.reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T00:21:26.733777Z","iopub.execute_input":"2021-06-11T00:21:26.734286Z","iopub.status.idle":"2021-06-11T00:21:26.742594Z","shell.execute_reply.started":"2021-06-11T00:21:26.734224Z","shell.execute_reply":"2021-06-11T00:21:26.741518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CompDataset(Dataset):\n\n    def __init__(self, df):\n        self.df_data = df\n\n\n\n    def __getitem__(self, index):\n\n        # get the sentence from the dataframe\n        sentence1 = self.df_data.loc[index, 'premise']\n        sentence2 = self.df_data.loc[index, 'hypothesis']\n\n        # Process the sentence\n        # ---------------------\n\n        encoded_dict = tokenizer.encode_plus(\n                    sentence1, sentence2,           # Sentences to encode.\n                    add_special_tokens = True,      # Add the special tokens.\n                    max_length = MAX_LEN,           # Pad & truncate all sentences.\n                    pad_to_max_length = True,\n                    return_attention_mask = True,   # Construct attn. masks.\n                    return_tensors = 'pt',          # Return pytorch tensors.\n               )\n        \n        # These are torch tensors.\n        padded_token_list = encoded_dict['input_ids'][0]\n        att_mask = encoded_dict['attention_mask'][0]\n        \n        # Convert the target to a torch tensor\n        target = torch.tensor(self.df_data.loc[index, 'label'])\n\n        sample = (padded_token_list, att_mask, target)\n\n\n        return sample\n\n\n    def __len__(self):\n        return len(self.df_data)\n    \n    \n    \n    \n    \n\nclass TestDataset(Dataset):\n\n    def __init__(self, df):\n        self.df_data = df\n\n\n\n    def __getitem__(self, index):\n\n        # get the sentence from the dataframe\n        sentence1 = self.df_data.loc[index, 'premise']\n        sentence2 = self.df_data.loc[index, 'hypothesis']\n\n        # Process the sentence\n        # ---------------------\n\n        encoded_dict = tokenizer.encode_plus(\n                    sentence1, sentence2,           # Sentence to encode.\n                    add_special_tokens = True,      # Add the special tokens.\n                    max_length = MAX_LEN,           # Pad & truncate all sentences.\n                    pad_to_max_length = True,\n                    return_attention_mask = True,   # Construct attn. masks.\n                    return_tensors = 'pt',          # Return pytorch tensors.\n               )\n        \n        # These are torch tensors.\n        padded_token_list = encoded_dict['input_ids'][0]\n        att_mask = encoded_dict['attention_mask'][0]\n        \n               \n\n        sample = (padded_token_list, att_mask)\n\n\n        return sample\n\n\n    def __len__(self):\n        return len(self.df_data)\n","metadata":{"execution":{"iopub.status.busy":"2021-06-11T00:22:32.787435Z","iopub.execute_input":"2021-06-11T00:22:32.787898Z","iopub.status.idle":"2021-06-11T00:22:32.802899Z","shell.execute_reply.started":"2021-06-11T00:22:32.787864Z","shell.execute_reply":"2021-06-11T00:22:32.800738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = CompDataset(df_train)\nval_data = CompDataset(df_val)\ntest_data = TestDataset(df_test)\n\ntrain_dataloader = torch.utils.data.DataLoader(train_data,\n                                        batch_size=BATCH_SIZE,\n                                        shuffle=True,\n                                       num_workers=NUM_CORES)\n\nval_dataloader = torch.utils.data.DataLoader(val_data,\n                                        batch_size=BATCH_SIZE,\n                                        shuffle=True,\n                                       num_workers=NUM_CORES)\n\ntest_dataloader = torch.utils.data.DataLoader(test_data,\n                                        batch_size=BATCH_SIZE,\n                                        shuffle=False,\n                                       num_workers=NUM_CORES)\n\n\n\nprint(len(train_dataloader))\nprint(len(val_dataloader))\nprint(len(test_dataloader))","metadata":{"execution":{"iopub.status.busy":"2021-06-11T00:23:23.305598Z","iopub.execute_input":"2021-06-11T00:23:23.306162Z","iopub.status.idle":"2021-06-11T00:23:23.317034Z","shell.execute_reply.started":"2021-06-11T00:23:23.306114Z","shell.execute_reply":"2021-06-11T00:23:23.315708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get one train batch\n\npadded_token_list, att_mask, target = next(iter(train_dataloader))\n\nprint(padded_token_list.shape)\nprint(att_mask.shape)\nprint(target.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T00:24:02.674742Z","iopub.execute_input":"2021-06-11T00:24:02.675212Z","iopub.status.idle":"2021-06-11T00:24:03.742037Z","shell.execute_reply.started":"2021-06-11T00:24:02.675173Z","shell.execute_reply":"2021-06-11T00:24:03.738694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = BartForCausalLM.from_pretrained(model_name)\nmodel","metadata":{"execution":{"iopub.status.busy":"2021-06-11T00:16:23.76197Z","iopub.execute_input":"2021-06-11T00:16:23.762665Z","iopub.status.idle":"2021-06-11T00:16:34.556122Z","shell.execute_reply.started":"2021-06-11T00:16:23.762604Z","shell.execute_reply":"2021-06-11T00:16:34.555092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the optimizer\noptimizer = AdamW(model.parameters(),\n              lr = L_RATE, \n              eps = 1e-8 \n            )\n\n","metadata":{"execution":{"iopub.status.busy":"2021-06-11T00:26:00.495097Z","iopub.execute_input":"2021-06-11T00:26:00.495968Z","iopub.status.idle":"2021-06-11T00:26:00.519043Z","shell.execute_reply.started":"2021-06-11T00:26:00.49589Z","shell.execute_reply":"2021-06-11T00:26:00.517503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataloader = torch.utils.data.DataLoader(train_data,\n                                        batch_size=BATCH_SIZE,\n                                        shuffle=True,\n                                       num_workers=NUM_CORES)\n\nb_input_ids, b_input_mask, b_labels = next(iter(train_dataloader))\n\nprint(b_input_ids.shape)\nprint(b_input_mask.shape)\nprint(b_labels.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T00:32:13.145819Z","iopub.execute_input":"2021-06-11T00:32:13.146545Z","iopub.status.idle":"2021-06-11T00:32:14.01176Z","shell.execute_reply.started":"2021-06-11T00:32:13.146505Z","shell.execute_reply":"2021-06-11T00:32:14.009624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Send the data to the device\nb_input_ids = batch[0].to(device)\nb_input_mask = batch[1].to(device)\nb_labels = batch[2].to(device)\n\noutputs = model(b_input_ids, \n                        attention_mask=b_input_mask, labels=b_labels)                       \n# The ouput is a tuple (loss, preds).\noutputs","metadata":{"execution":{"iopub.status.busy":"2021-06-11T00:33:17.784253Z","iopub.execute_input":"2021-06-11T00:33:17.784651Z","iopub.status.idle":"2021-06-11T00:33:17.8649Z","shell.execute_reply.started":"2021-06-11T00:33:17.784618Z","shell.execute_reply":"2021-06-11T00:33:17.861404Z"},"trusted":true},"execution_count":null,"outputs":[]}]}