{"cells":[{"metadata":{},"cell_type":"markdown","source":"### How to set up TPU \n#turn the accelerator to TPU \n### detect and init the TPU\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n\n### instantiate a distribution strategy\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n\n### instantiating the model in the strategy scope creates the model on the TPU\nwith tpu_strategy.scope():\n    model = tf.keras.Sequential( … ) # define your model normally\n    model.compile( … )\n\n### train model normally\nmodel.fit(training_dataset, epochs=EPOCHS, steps_per_epoch=…)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n%matplotlib notebook\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport numpy as np \n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom keras.models import Sequential\nfrom keras.layers.recurrent import LSTM, GRU,SimpleRNN\nfrom keras.layers.core import Dense, Activation, Dropout\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.utils import np_utils\nfrom sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\nfrom keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\nfrom keras.preprocessing import sequence, text\nfrom keras.callbacks import EarlyStopping\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom plotly import graph_objs as go\nimport plotly.express as px\nimport plotly.figure_factory as ff\n\nfrom transformers import BertTokenizer, TFBertModel\nimport matplotlib.pyplot as plt\nimport tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I. Analyse des données "},{"metadata":{"trusted":true},"cell_type":"code","source":"model_name = 'bert-base-multilingual-cased'\ntokenizer = BertTokenizer.from_pretrained(model_name)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Detect hardware, return appropriate distribution strategy\n# Setting up TPU\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept ValueError:\n    strategy = tf.distribute.get_strategy() # for CPU and single GPU\n    print('Number of replicas:', strategy.num_replicas_in_sync)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def encode_sentence(s):\n   tokens = list(tokenizer.tokenize(s))\n   tokens.append('[SEP]')\n   return tokenizer.convert_tokens_to_ids(tokens)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encode_sentence(\"I love machine learning\"), encode_sentence(\"I love mach learning\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Constatations : \n- Aucune données ne manquent pour les 12120 entrées\n- Les données d'entrainement contiennent 15 langues différentes, d'abréviations 2 à 2 distinctes.\n- La distribution des (entraînemet,neutralité, contradiction) est presque équitable entre les données.\n- L'anglais est la langue la plus domainante (plus de 56 %).\n- les données d'hypothèses sont 2 à 2 distincts (à part \"I'm not sure.' de fréquence 2 ).\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/contradictory-my-dear-watson/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/contradictory-my-dear-watson/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train[\"lang_abv\"].unique()), len(train[\"language\"].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"set(test[\"lang_abv\"])== set(train[\"lang_abv\"]), set(test[\"language\"]) == set(train[\"language\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe(include = \"O\"), train.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"distribution des catégories "},{"metadata":{"trusted":true},"cell_type":"code","source":"#Label = train[\"label\"].map({0: \"entraînement\" , 1 : \"neutralité\" , 2 : \"contradiction\"})\n#Label = train[\"label\"].map({\"entraînement\":0 ,  \"neutralité\":  1, \"contradiction\":2})\n\n#train[\"label\"] = Label","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"label\"].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"lang_abv\"].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"lang_abv\"].groupby(train[\"lang_abv\"]).count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"languages = list(train[\"language\"].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.clf()\nplt.hist(train[train[\"language\"]==\"English\"][\"label\"], color =\"g\")\nplt.title(\"English\")\nplt.show()    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"colors = [\"b\",\"g\",\"r\",\"c\",\"m\"]\nfor i in range(len(languages)):\n    plt.subplot(3,5,i+1)\n    plt.hist(train[train[\"language\"]==languages[i]][\"label\"], color = colors[i%5])\n    plt.title(languages[i])\n    plt.show()    \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Distribution des phrases selon la langue "},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.clf()\nlanguages = train[\"language\"].unique()\nlang_freq = {i : len(train[train[\"language\"] == i]) for i in languages}\nplt.figure(figsize = (8.5,8.5))\nplt.pie(lang_freq.values(), labels = languages,autopct = '%1.0f%%')\nplt.title(\"Distribution des phrases selon la langue\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pour les données de test \nplt.clf()\nlanguages_test = test[\"language\"].unique()\nlang_freq_test = {i : len(test[test[\"language\"] == i]) for i in languages_test}\nplt.figure(figsize = (8.5,8.5))\nplt.pie(lang_freq_test.values(), labels = languages_test,autopct = '%1.0f%%')\nplt.title(\"Distribution des phrases selon la langue (données de test)\")\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test[\"language\"].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" ### Distribution des catégories selon la langue "},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train[\"lang_abv\"].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Changement des langues en valeurs numériques : \nLang_abv = train[\"lang_abv\"].map({train[\"lang_abv\"].unique()[i] : i for i in range(len(train[\"lang_abv\"].unique()))})\nLanguages = train[\"language\"].map({train[\"language\"].unique()[i] : i for i in range(len(train[\"lang_abv\"].unique()))})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"lang_abv\"] = Lang_abv\ntrain[\"language\"] = Languages","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sns.pairplot(train[train[\"language\"]==\"English\"], hue = \"lang_abv\")\nplt.clf()\ncorrMatrix = train.corr()\nsns.heatmap(corrMatrix, annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## A. Traitement des données "},{"metadata":{"trusted":true},"cell_type":"code","source":"train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"help(plt.hist)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# taille des phrases \nprem, hyp = train[\"premise\"], train[\"hypothesis\"]\ndict_label  ={0 :\"entrainement\", 1 : \"neutre\", 2 :\"contradictoire\"}\nfor i in range(3): \n    ls_prem = train[train[\"label\"]==i][\"premise\"].str.split().apply(lambda x: len(x))\n    ls_hyp = train[train[\"label\"]==i][\"hypothesis\"].str.split().apply(lambda x: len(x))\n    plt.subplot(3,2,1 +2*i)\n    plt.hist(ls_prem, bins = 20)\n    plt.xlabel(\"premise\")\n    plt.title(dict_label[i])\n    plt.subplot(3,2, 2 + 2*i)\n    plt.hist(ls_hyp, bins = 20)\n    plt.title(dict_label[i])\n    plt.xlabel(\"hypothesis\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On peut constater que les phrases sont plus longues pour les données premise par rapport a hypothesis et la distribution est presque la même pour les 3 classes. on peut s'en assurer en tracant la correlation "},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.clf()\ncopy_train = train\ncopy_train[\"length_sen_pre\"] = prem.str.split().apply(lambda x : len(x))\ncopy_train[\"length_sen_hyp\"] = hyp.str.split().apply(lambda x : len(x))\nmax_length = max(max(copy_train[\"length_sen_hyp\"]),max(copy_train[\"length_sen_pre\"]))\ncorrMatrix = copy_train.corr()\nsns.heatmap(corrMatrix, annot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# taille maximum des phrases\nmax_length","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Les stopwords \nOn peut éliminer tous les stopwords des phrases, mais puique notre classification ne se base pas que sur la frésuence des mots, mais la relation entre les phrases, on doit faire attention aux stopwords de négativité qui change complétement le sens."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Analyse de l'impact des stopwords\nimport gensim.parsing.preprocessing as g \nstopWords = g.STOPWORDS\n#on elimine les stopwords negatives\nneg = [\"no\" , \"not\" , \"has\" ,\"hasn't\", 'did',\n           'didn',\n           'do',\n           'does',\n           'doesn',\n           'doing',\n           'don', 'could',\n           'couldnt','can',\n           'cannot',\n           'cant', 'never', 'none']\nstop_without_neg  =[word for word in stopWords if word not in neg]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"help(g.STOPWORDS)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"copy_train[\"if_neg_exist_prem\"] = prem.str.split().apply(lambda x : len([word for word in neg if word in x]))\ncopy_train[\"if_neg_exist_hyp\"] = hyp.str.split().apply(lambda x : len([word for word in neg if word in x]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.clf()\nfor i in range(3): \n    plt.subplot(3,2,1 +2*i)\n    plt.hist(copy_train[copy_train[\"label\"] ==i][\"if_neg_exist_prem\"]) # anglais seulement\n    plt.xlabel(\"premise\")\n    plt.title(dict_label[i])\n    plt.subplot(3,2, 2 + 2*i)\n    plt.hist(copy_train[copy_train[\"label\"] ==i][\"if_neg_exist_hyp\"])\n    plt.title(dict_label[i])\n    plt.xlabel(\"hypothesis\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.clf()\ncorrMatrix = copy_train.corr()\nsns.heatmap(corrMatrix, annot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#before\ncopy_train[\"premise\"].iloc[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# eliminer les stopwords\n#pour l'anglais \ncopy_train[\"premise\"] = prem.str.split().apply(lambda x : (\" \").join([word for word in x if word not in stop_without_neg]))\ntest[\"premise\"] = test[\"premise\"].str.split().apply(lambda x : (\" \").join([word for word in x if word not in stop_without_neg]))\n# after\ncopy_train[\"premise\"].iloc[0]\ncopy_train[\"hypothesis\"] = hyp.str.split().apply(lambda x : (\" \").join([word for word in x if word not in stop_without_neg]))\ntest[\"hypothesis\"] = test[\"hypothesis\"].str.split().apply(lambda x : (\" \").join([word for word in x if word not in stop_without_neg]))\ncopy_train[\"length_sen_pre\"] = prem.str.split().apply(lambda x : len(x))\ncopy_train[\"length_sen_hyp\"] = hyp.str.split().apply(lambda x : len(x))\nmax_length = max(max(copy_train[\"length_sen_hyp\"]),max(copy_train[\"length_sen_pre\"]))"},{"metadata":{"trusted":true},"cell_type":"code","source":"#les autres \nfrom nltk.corpus import stopwords\nstop_english = stopwords.words('english')\nstop_french = stopwords.words('french')\nstop_arabic = stopwords.words('arabic')\n#stop_chinese = stopwords.words('chinese')\nstop = []\nstop.extend(stop_english)\nstop.extend(stop_french)\nstop.extend(stop_arabic)\ncopy_train[\"premise\"] = train_set[\"premise\"].apply(lambda x : [mot for mot in x.split(\" \") if mot not in stop])\ncopy_train[\"hypothesis\"] = train_set[\"hypothesis\"].apply(lambda x : [mot for mot in x.split(\" \") if mot not in stop])\ncopy_test[\"premise\"] = test_set[\"premise\"].apply(lambda x : [mot for mot in x.split(\" \") if mot not in stop])\ncopy_test[\"hypothesis\"] = test_set[\"hypothesis\"].apply(lambda x : [mot for mot in x.split(\" \") if mot not in stop])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# eliminer ponctuations \n# before \ncopy_train[\"premise\"].iloc[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"punc = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\ncopy_train[\"premise\"].iloc[0]\ndef eliminate_punct(string):\n    for elem in string : \n        if elem in punc : \n            string = string.replace(elem, \"\")\n    return string\ncopy_train[\"premise\"] = copy_train[\"premise\"].apply(lambda x : eliminate_punct(str(x)))\ncopy_train[\"hypothesis\"] = copy_train[\"hypothesis\"].apply(lambda x : eliminate_punct(str(x)))\ntest[\"premise\"] = test[\"premise\"].apply(lambda x : eliminate_punct(str(x)))\ntest[\"hypothesis\"] = test[\"hypothesis\"].apply(lambda x : eliminate_punct(str(x)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# after\ncopy_train[\"premise\"].iloc[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# before\ncopy_train[\"premise\"].iloc[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#eliminer majiscule\ncopy_train[\"premise\"] = copy_train[\"premise\"].apply(lambda x : x.lower())\ncopy_train[\"hypothesis\"] = copy_train[\"hypothesis\"].apply(lambda x : x.lower())\ntest[\"premise\"] = test[\"premise\"].apply(lambda x : x.lower())\ntest[\"hypothesis\"] = test[\"hypothesis\"].apply(lambda x : x.lower())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# after\ncopy_train[\"premise\"].iloc[1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Mots communs \nIntuitivement les phrases qui s'entraine ou sont contradictoire doivent avoir le nombre de mots communs important par rapport à celles neutres."},{"metadata":{"trusted":true},"cell_type":"code","source":"# mots communs \ndef commun_words(st1, st2):\n    listst1 = st1.split(\" \")\n    listst2 = st2.split(\" \")\n    nb = 0\n    for w in listst2: \n        if w in listst1: \n            nb += 1 \n            listst1.remove(w)\n    return nb \n            \ncommuns  = []\nfor i in range(len(copy_train[\"premise\"])):\n    communs.append(commun_words(copy_train[\"premise\"].iloc[i],copy_train[\"hypothesis\"].iloc[i]))\ncopy_train[\"length_communs\"] = communs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.clf()\nfor i in range(3): \n    plt.subplot(3,2,1 +2*i)\n    plt.hist(copy_train[copy_train[\"label\"]==i][\"length_communs\"], bins = 30) # anglais seulement\n    plt.title(dict_label[i])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.clf()\ncorrMatrix = copy_train[[\"length_communs\",\"label\"]].corr()\nsns.heatmap(corrMatrix, annot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Word Embeddings "},{"metadata":{},"cell_type":"markdown","source":"### Modèles\n--> simple RNN\n--> LSTM \n--> GRU \n--> BERT "},{"metadata":{},"cell_type":"markdown","source":"### BERT  "},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import BertTokenizer, TFBertModel\nimport matplotlib.pyplot as plt\nimport tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# BERT pour contextualiser les embeddings \n#model_name = 'bert-large-cased'\nmodel_name = \"bert-base-multilingual-cased\"\n#tokenizer = BertTokenizer.from_pretrained(model_name) # KM: this is the tokenizer we will use on our text data to tokenize it","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# on utilise le tokenasiation de bert qui prend en consideration le contexte des mots dans la phrase\ndef encode_sentence(s):\n    tokens = list(tokenizer.tokenize(s)) # KM: split the sentence into tokens that are either words or sub-words\n    tokens.append('[SEP]') # KM: a token called [SEP] (=separator) is added to mark end of each sentence\n    #return tokens\n    return tokenizer.convert_tokens_to_ids(tokens) # KM: instead of returning the list of tokens, a list of each token ID is returned","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_len= 210# valeur très grande\ndef bert_encode(hypotheses, premises, tokenizer):\n    \n    num_examples = len(hypotheses)\n    #tokenazitation de toutes les phrases du hypothese et premise\n    sentence1 = tf.ragged.constant([   # KM: constructs a constant ragged tensor. every entry has a different length (facilite de faire des operations sur less listes imbriquées)\n      encode_sentence(s)\n      for s in np.array(hypotheses)])\n    sentence2 = tf.ragged.constant([\n      encode_sentence(s)\n       for s in np.array(premises)])\n    # cls : indique le debut de phrase (apprait lg_phrase fois)\n    cls = [tokenizer.convert_tokens_to_ids(['[CLS]'])]*sentence1.shape[0] # KM: list of IDs for the token '[CLS]' to denote each beginning\n    input_word_ids = tf.concat([cls, sentence1, sentence2], axis=-1) # KM: put everything together. every row still has a different length.\n    # les mots inutils ajouté juste pour avoir des phrases de meme longeur sont codé en 0 , les autres en 1 \n    input_mask = tf.ones_like(input_word_ids).to_tensor(shape =(input_word_ids.shape[0], max_len)) # KM: first, a tensor with just ones in it is constructed in the same size as input_word_ids. Then, by applying to_tensor the ends of each row are padded with zeros to give every row the same length\n\n    type_cls = tf.zeros_like(cls) # KM: creates a tensor same shape as cls with only zeros in it\n    type_s1 = tf.zeros_like(sentence1)\n    type_s2 = tf.ones_like(sentence2) # KM: creates a tensor same shape as sentence2 with only ones in it to mark the 2nd sentence\n    input_type_ids = tf.concat(\n      [type_cls, type_s1, type_s2], axis=-1).to_tensor(shape =(input_word_ids.shape[0], max_len)) # KM: concatenates everything and again adds padding \n\n    inputs = {\n      'input_word_ids': input_word_ids.to_tensor(shape =(input_word_ids.shape[0], max_len)), # KM: input_word_ids hasn't been padded yet - do it here now\n      'input_mask': input_mask,\n      'input_type_ids': input_type_ids}\n\n    return inputs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"max_len = 64\ndef encode_sentence(s, tokenizer):\n    \"\"\"\n    Turn a sequence of words into and array of numbers using a selected tokenizer.\n    Args:\n        s (list of str) - Input string.\n        tokenizer - XLM-R tokenizer.\n    Returns:\n        (list of int) - Tokenized string.\n\n    \"\"\"\n    tokens = list(tokenizer.tokenize(s))\n    tokens.append(tokenizer.sep_token)\n    return tokenizer.convert_tokens_to_ids(tokens)\n\ndef tokenize(data, tokenizer, max_len):\n    \"\"\"\n    Encode hypotheses and premises into arrays of numbers using a selected tokenizer. \n    Args:\n        data - An array consisting of [hypothesis (str), premise (str)] pairs.\n        tokenizer - Tokenizer handle.\n        max_len - Max sequence length.\n    Returns: (dictionary of tensors)\n        input_word_ids - Indices of input sequence tokens in the vocabulary, truncated to max_len.\n        input_mask - Real input indices mapped to ones. Padding indices mapped to zeroes.\n        input_type_ids - Segment token indices to indicate first and second portions of the inputs.\n    \"\"\"\n\n    PAD_ID = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n \n    # Append a separator to each sentence, tokenize, and concatenate.\n    tokens1 = tf.ragged.constant([encode_sentence(s[0], tokenizer) for s in data], dtype=tf.int32) # ENCODED_SEQUENCE_A [SEP]\n    tokens2 = tf.ragged.constant([encode_sentence(s[1], tokenizer) for s in data], dtype=tf.int32) # ENCODED_SEQUENCE_B [SEP]\n    cls_label = [tokenizer.convert_tokens_to_ids([tokenizer.cls_token])]*tokens1.shape[0] # [CLS] ENCODED_SEQUENCE_A [SEP]\n    tokens = tf.concat([cls_label, tokens1, tokens2], axis=-1) # [CLS] ENCODED_SEQUENCE_A [SEP] ENCODED_SEQUENCE_B [SEP]\n\n    # Truncate to max_len.\n    tokens = tokens[:, :max_len]\n\n    # Pad with zeroes if len < max_len.\n    tokens = tokens.to_tensor(default_value=PAD_ID)\n    pad = max_len - tf.shape(tokens)[1]\n    tokens = tf.pad(tokens, [[0, 0], [0, pad]], constant_values=PAD_ID)\n    input_word_ids = tf.reshape(tokens, [-1, max_len])\n\n    # The input mask allows the model to cleanly differentiate between the content and the padding. \n    input_mask = tf.cast(input_word_ids != PAD_ID, tf.int32)\n    input_mask = tf.reshape(input_mask, [-1, max_len])\n\n    # Map tokens1 indices to zeroes and tokens2 indices to ones.\n    input_type_ids = tf.concat([tf.zeros_like(cls_label), tf.zeros_like(tokens1), tf.ones_like(tokens2)], axis=-1).to_tensor()\n\n\n    inputs = {\n      'input_word_ids': input_word_ids,\n      'input_mask': input_mask,\n      'input_type_ids': input_type_ids}\n\n    return inputs"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"x_train, y_train = copy_train[['premise', 'hypothesis']].values.tolist(), copy_train['label']\ntrain_input = tokenize(x_train,tokenizer, max_len)"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"len(copy_train.columns)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# on construit le modele \ndef build_model():\n    bert_encoder = TFBertModel.from_pretrained(model_name) # KM: constructs a BERT model pre-trained on the above described language model 'bert-base-multilingual-cased'\n    \n    # KM: now we adjust the model so that it can accept our input by telling the model what the input looks like:\n    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\") # KM: tf.keras.Input constructs a symbolic tensor object whith certain attributes: \"shape\" tells it that the expected input will be in batches of max_len-dimensional vectors; \"dtype\" tells it that the data type will be int32; \"name\" will be the name string for the input layer\n    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\") # KM: repeat the same for the other two input variables\n    input_type_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_type_ids\")\n    \n    # KM: now follows, what we want to happen with our input:\n    embedding = bert_encoder([input_word_ids, input_mask, input_type_ids])[0] # KM: first, our input goes into the BERT model bert_encoder. It will return a tuple and the contextualized embeddings that we need are stored in the first element of that tuple\n    x = tf.keras.layers.Dense(512, activation =\"relu\")(embedding[:,0,:])\n    output = tf.keras.layers.Dense(3, activation='softmax')(x) # KM: as described below, we only need the output corresponding to the first token [CLS], which is a 2D-tensor with size (#sentence pairs, 768) and is accessd with embedding[:,0,:]. This will be input for our classifier, which is a regular densely-connected neural network constructed through tf.keras.layers.Dense. The inputs mean: \"3\" is the dimensionality of the output space, which means that the output has shape (#sentence pairs,3). More practically speaking, for each sentence pair that we input, the output will have 3 probability values for each of the 3 possible labels (entailment, neutral, contradiction). They will be in range(0,1) and add up to 1; \"activation\" denotes the activation function, in this case 'softmax', which connects a real vector to a vector of categorical possibilities.\n    \n    model = tf.keras.Model(inputs=[input_word_ids, input_mask, input_type_ids], outputs=output) # KM: based on the code in the lines above, a model is now constructed and passed into the variable model\n    model.compile(tf.keras.optimizers.Adam(lr=1e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy']) # KM: we tell the model how we want it to train and evaluate: \"tf.keras.optimizers.Adam\": use an optimizer that implements the Adam algorithm. \"lr\" denotes the learning rate; \"loss\" denotes the loss function to use; \"metrics\" specifies which kind of metrics to use for training and testing\n    \n    return model #KM","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Roberta XLM "},{"metadata":{"trusted":true},"cell_type":"markdown","source":"from transformers import BertTokenizer, AutoTokenizer, TFBertModel, TFXLMRobertaModel\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, LSTM, Embedding, GlobalAveragePooling1D"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"encoder_handle = 'jplu/tf-xlm-roberta-large'\ntokenizer = AutoTokenizer.from_pretrained(encoder_handle)\nrandom_seed = 11887\nlearning_rate = 1e-5 # Controls how large a step is taken when updating model weights during training."},{"metadata":{"trusted":true},"cell_type":"markdown","source":"def build_model2():\n    tf.keras.backend.clear_session()\n    tf.random.set_seed(random_seed)\n    \n    with strategy.scope():\n        input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n        input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n        # RoBERTa doesn’t use token_type_ids.\n\n        #  Create an instance of a model defined in encoder_handle\n        roberta = TFXLMRobertaModel.from_pretrained(encoder_handle)\n        roberta = roberta([input_word_ids, input_mask])[0]\n        out = GlobalAveragePooling1D()(roberta)\n        out = Dense(3, activation='softmax')(out)\n\n        model = tf.keras.Model(inputs=[input_word_ids, input_mask], outputs = out)\n        model.compile(optimizer=tf.keras.optimizers.Adam(lr=learning_rate), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n    \n    return model"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = build_model2() # KM: our model is being built\nmodel.summary()       # KM: let's look at some of its properties","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(train_input, train.label.values, epochs = 5, verbose = 1, batch_size = 16 * strategy.num_replicas_in_sync, validation_split = 0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"x_test = test[['premise','hypothesis']].values.tolist()\ntest_input = tokenize(x_test, tokenizer, max_len)"},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = [np.argmax(i) for i in model.predict(test_input)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"submission = test.id.copy().to_frame()\nsubmission['prediction'] = predictions\nsubmission.to_csv(\"submission.csv\", index = False)"},{"metadata":{},"cell_type":"markdown","source":"#### simple RNN"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom keras.models import Sequential\nfrom keras.layers.recurrent import LSTM, GRU,SimpleRNN\nfrom keras.layers.core import Dense, Activation, Dropout\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.utils import np_utils\nfrom sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\nfrom keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\nfrom keras.preprocessing import sequence, text\nfrom keras.callbacks import EarlyStopping\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom plotly import graph_objs as go\nimport plotly.express as px\nimport plotly.figure_factory as ff\n\nfrom transformers import BertTokenizer, TFBertModel\nimport matplotlib.pyplot as plt\nimport tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Detect hardware, return appropriate distribution strategy\n# Setting up TPU\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept ValueError:\n    strategy = tf.distribute.get_strategy() # for CPU and single GPU\n    print('Number of replicas:', strategy.num_replicas_in_sync)"},{"metadata":{},"cell_type":"markdown","source":"tokenization"},{"metadata":{"trusted":true},"cell_type":"code","source":"token = text.Tokenizer(num_words=None)\nmax_len = 250 # taille maximale de phrases\nseparator = ' [SEP] ' #separer les deux phrases \ncopy_train[\"concatenated\"] = copy_train[\"premise\"] + separator + copy_train[\"hypothesis\"]\ntest[\"concatenated\"] = test[\"premise\"] + separator + test[\"hypothesis\"]\ntoken.fit_on_texts(list(copy_train[\"concatenated\"]) + list(test[\"concatenated\"]))\ntest_seq = token.texts_to_sequences(list(test[\"concatenated\"]))\n#zero pad the sequences : toutes les phrases deviennet de meme longueur(fit avec 0 )\ntest_pad = sequence.pad_sequences(test_seq, maxlen=max_len)\n#A revoir !! \ntrain_seq = token.texts_to_sequences(list(copy_train[\"concatenated\"]))\n#zero pad the sequences : toutes les phrases deviennet de meme longueur(fit avec 0 )\ntrain_pad = sequence.pad_sequences(train_seq, maxlen=max_len)\nword_index = token.word_index ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(word_index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"model = Sequential()\nmodel.add(Embedding(len(word_index) + 1, 300, input_length=max_len))\nmodel.add(SimpleRNN(200))\nmodel.add(Dense(3, activation='softmax'))\nmodel.compile(loss=\"categorical_crossentropy\", optimizer='adam', metrics=['accuracy'])\nmodel.summary()"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"from keras.utils import to_categorical \n# on fait une premiere classification binaire (neutre ou en relation) \n#copy_train[\"label_bin\"] = copy_train[\"label\"].apply(lambda x : x%2) # 2 ==> 0\nlabel =  to_categorical(copy_train[\"label\"],3)\nmodel.fit(train_pad, label, epochs=5, batch_size=64*strategy.num_replicas_in_sync) #Multiplying by Strategy to run on TPU's"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"predictions = [np.argmax(i) for i in model.predict(test_pad)]"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"submission = test.id.copy().to_frame()\nsubmission['prediction'] = predictions \nsubmission.to_csv(\"submission.csv\", index = False)"},{"metadata":{},"cell_type":"markdown","source":"### LSTM "},{"metadata":{},"cell_type":"markdown","source":"simple RNN donne des résultats très médiocres vue que les phrases sont très longues , on essaiera d'améliorer cela en utilisatn un modèle à base LSTM "},{"metadata":{},"cell_type":"markdown","source":"#### Embeddings "},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.utils import to_categorical ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"glove_100d = open('/kaggle/input//glove-global-vectors-for-word-representation/glove.6B.100d.txt','r',encoding='utf-8') \nembeddings = {} \nfor line in glove_100d : \n    vector = line.split(\" \") \n    word = vector[0] \n    coefs = np.asarray([float(val) for val in vector[1:]])\n    embeddings[word] = coefs \nglove_100d.close()"},{"metadata":{"trusted":true},"cell_type":"code","source":"# travailler avec Elmo à la place \n!conda remove -y greenlet\n!pip install allennlp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# create an embedding matrix for the words we have in the dataset\nembedding_matrix = np.zeros((len(word_index) + 1, 100))\nfor word, i in tqdm(word_index.items()):\n    embedding_vector = embeddings.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector"},{"metadata":{"trusted":true},"cell_type":"code","source":"on calcule la similarité cosinus entre premise et hypothesis et on montre que pour neutre, c'est faible ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"import math\n#source: https://webdevdesigner.com/q/calculate-cosine-similarity-given-2-sentence-strings-67720/\ndef get_cosine(vec1, vec2):\n     intersection = set(vec1) & set(vec2)\n     numerator = sum([vec1.count(x) * vec2.count(x) for x in intersection])\n\n     sum1 = sum([vec1.count(x)**2 for x in vec1])\n     sum2 = sum([vec2.count(x)**2 for x in vec2])\n     denominator = math.sqrt(sum1) * math.sqrt(sum2)\n\n     if not denominator:\n        return 0.0\n     else:\n        return float(numerator) / denominator"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"similarity_cos = []\nfor x in copy_train.itertuples():\n    encode_prem = encode_sentence(x[2])\n    encode_hyp = encode_sentence(x[3])\n    similarity_cos.append(get_cosine(encode_prem, encode_hyp))\ncopy_train[\"similarité\"] = similarity_cos"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"for i in range(3): \n    plt.subplot(3,2,1 +2*i)\n    plt.hist(copy_train[copy_train['label'] == i][\"similarité\"], bins = 20)\n    plt.title(dict_label[i])"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"print(len(similarity_cos))\n"},{"metadata":{},"cell_type":"markdown","source":"### Comparaison des embedidngs des mots (phrases) selon le label "},{"metadata":{},"cell_type":"markdown","source":"with strategy.scope():\n    # A simple LSTM with glove embeddings and one dense layer\n    model1 = Sequential()\n    model1.add(Embedding(len(word_index) + 1,\n                     100,\n                     weights=[embedding_matrix],\n                     input_length=max_len,\n                     trainable=False))\n\n    model1.add(LSTM(100, dropout=0.3, recurrent_dropout=0.3))\n    model1.add(Dense(3, activation='softmax'))\n    model1.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n    \nmodel1.summary()"},{"metadata":{},"cell_type":"markdown","source":"label = to_categorical(copy_train[\"label\"],3)\nmodel1.fit(train_pad, label, epochs=5, batch_size=64*strategy.num_replicas_in_sync)"},{"metadata":{},"cell_type":"markdown","source":"predictions = [np.argmax(i) for i in model1.predict(test_pad)]"},{"metadata":{},"cell_type":"markdown","source":"submission = test.id.copy().to_frame()\nsubmission['prediction'] = predictions \nsubmission.to_csv(\"submission.csv\", index = False)"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# GRU with glove embeddings and two dense layers\nmodel2 = Sequential()\nmodel2.add(Embedding(len(word_index) + 1,\n             100,\n             weights=[embedding_matrix],\n             input_length=max_len,\n             trainable=False))\nmodel2.add(SpatialDropout1D(0.3))\nmodel2.add(GRU(300))\nmodel2.add(Dense(3, activation='softmax'))\nmodel2.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])   \nmodel2.summary()"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"label = to_categorical(copy_train[\"label\"],3)\nmodel2.fit(train_pad, label, epochs=5, batch_size=64*strategy.num_replicas_in_sync)\npredictions = [np.argmax(i) for i in model2.predict(test_pad)]\nsubmission = test.id.copy().to_frame()\nsubmission['prediction'] = predictions \nsubmission.to_csv(\"submission.csv\", index = False)"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# On entraine maintenant le modele que sur les classes 1 et 2 ( de facon binaire)\nsub_copy = copy_train[copy_train[\"label_bin\"]==0]\nidx_to_change = [i for i in range(len(predictions)) if predictions[i] == 0]"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"token.fit_on_texts(list(sub_copy[\"concatenated\"]))\ntrain_sub_seq = token.texts_to_sequences(list(sub_copy[\"concatenated\"]))\n#zero pad the sequences : toutes les phrases deviennet de meme longueur(fit avec 0 )\ntrain_sub_pad = sequence.pad_sequences(train_sub_seq, maxlen=max_len)\nword_index = token.word_index \nmodel = Sequential()\nmodel.add(Embedding(len(word_index) + 1, 300, input_length=max_len))\nmodel.add(SimpleRNN(100))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss=\"binary_crossentropy\", optimizer='adam', metrics=['accuracy'])\nmodel.summary()\nsub_copy[\"label\"] = sub_copy[\"label\"].apply(lambda x : max(0,x-1))\nmodel.fit(train_sub_pad, sub_copy[\"label\"], epochs=5, batch_size=64*strategy.num_replicas_in_sync) #Multiplying by Strategy to run on TPU's"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"predict2 =[round(float(i)) for i in model.predict(test_pad[idx_to_change])]"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"predict2 = [2*i for i in predict2] #[contradictoire == 2]"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"predictions = pd.Series(predictions)\npredictions[idx_to_change] =predict2"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}