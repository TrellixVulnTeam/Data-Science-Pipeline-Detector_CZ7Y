{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import torch\nfrom transformers import BertTokenizer,BertForSequenceClassification,AdamW,get_linear_schedule_with_warmup\nimport tensorflow as tf\nimport os\nimport pandas as pd\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Get GPU name\ngpu=tf.test.gpu_device_name()\nprint(gpu)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# If GPU available\nif torch.cuda.is_available():    \n    # Tell PyTorch to use the GPU.    \n    device=torch.device(\"cuda\")\n    print('There are %d GPU(s) available.' %torch.cuda.device_count())\n    print('We will use the GPU:',torch.cuda.get_device_name(0))\nelse:\n    print('No GPU available, using the CPU.')\n    device = torch.device(\"cpu\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Get training and validation data\ntrain=pd.read_csv(\"/kaggle/input/contradictory-my-dear-watson/train.csv\")\ntest=pd.read_csv(\"/kaggle/input/contradictory-my-dear-watson/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.id","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.groupby('language')['id'].count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Initialize BertTokenzier\ntokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Data Preprocessing and tensor generation\nseed=2\n\n#train=train.sample(n=10000,random_state=seed)\ntokenized_train=tokenizer.batch_encode_plus(train.iloc[:,1:3].to_numpy().tolist(),max_length=128,pad_to_max_length=True,return_tensors='pt')\nlabels_train=torch.tensor(train.label.values[:])\n\n#val=val.sample(n=2000,random_state=seed)\ntokenized_val=tokenizer.batch_encode_plus(train.iloc[-1000:,1:3].to_numpy().tolist(),max_length=128,pad_to_max_length=True,return_tensors='pt')\nlabels_val=torch.tensor(train.label.values[-1000:])\n\ntokenized_test=tokenizer.batch_encode_plus(test.iloc[:,1:3].to_numpy().tolist(),max_length=128,pad_to_max_length=True,return_tensors='pt')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n\nbatch_size=32\n\ntrain_data=TensorDataset(tokenized_train['input_ids'],tokenized_train['attention_mask'],tokenized_train['token_type_ids'],labels_train)\ntrain_sampler=RandomSampler(train_data)\ntrain_dataloader=DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n\nval_data=TensorDataset(tokenized_val['input_ids'],tokenized_val['attention_mask'],tokenized_val['token_type_ids'],labels_val)\nval_sampler=RandomSampler(val_data)\nval_dataloader=DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n\ntest_data=TensorDataset(tokenized_test['input_ids'],tokenized_test['attention_mask'],tokenized_test['token_type_ids'])\n# test_sampler=RandomSampler(test_data)\ntest_dataloader=DataLoader(test_data, batch_size=batch_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Bert Model transformer with a single sequence classification layer on top\nmodel=BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased',num_labels=3,output_attentions=False,output_hidden_states=False)\nmodel.cuda()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Set learning rate\noptimizer=AdamW(model.parameters(),lr=2e-5)\nepochs=8\n\n#Training steps is no_of_batches*no_of_epochs\ntotal_steps=len(train_dataloader)*epochs\n\n#Learning rate scheduler\nscheduler=get_linear_schedule_with_warmup(optimizer,num_warmup_steps=0,num_training_steps = total_steps)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function to calculate the accuracy of predictions\ndef flat_accuracy(preds, labels):\n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return np.sum(pred_flat == labels_flat) / len(labels_flat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\n\nseed_val = 12\n\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)\n\nloss_values = []\n\nfor epoch_i in range(0, epochs):\n    \n    #Put model into training mode\n    model.train()\n\n    total_loss=0\n\n    for step, batch in enumerate(train_dataloader):\n\n        #Unpack the training batch\n        b_input_ids = batch[0].to(device)\n        b_attention_mask=batch[1].to(device)\n        b_token_type = batch[2].to(device)\n        b_labels = batch[3].to(device)\n\n        #Clear previously calculated gradients before performing a backward pass\n        #model.zero_grad()        //Not sure if useful or not\n\n        #Perform a forward pass and get the loss\n        outputs=model(b_input_ids,token_type_ids=b_token_type,attention_mask=b_attention_mask,labels=b_labels)\n        \n        loss = outputs[0]\n        total_loss += loss.item()\n\n        #Perform backward pass to calculate gradients\n        loss.backward()\n\n        #Clip the norm of the gradients to 1.0.\n        #Used to prevent the \"exploding gradients\" problem.\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n        #Update weights\n        optimizer.step()\n\n        #Update learning rate\n        scheduler.step()\n\n    #Avg loss over training data for an epoch\n    avg_train_loss = total_loss / len(train_dataloader)\n\n    print(\"\")\n    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n\n    #### Validation\n\n    #Evaluation mode\n    model.eval()\n\n    #Tracking variables \n    val_accuracy=0\n    nb_val_steps=0\n\n    #Evaluate data for one epoch\n    for batch in val_dataloader:\n        \n        #Add batch to GPU\n        batch = tuple(t.to(device) for t in batch)\n        \n        #Unpack the inputs from dataloader\n        b_input_ids,b_attention_mask, b_token_type, b_labels = batch\n        \n        #Telling the model not to compute or store gradients, saving memory and speeding up validation\n        with torch.no_grad():        \n            outputs = model(b_input_ids,token_type_ids=b_token_type,attention_mask=b_attention_mask)\n        \n        #Get the \"logits\" output by the model. The \"logits\" are output values prior to applying an activation function like the softmax.\n        logits = outputs[0]\n        \n        #Move logits and labels to CPU\n        logits = logits.detach().cpu().numpy()\n        label_ids = b_labels.to('cpu').numpy()\n        \n        #Calculate the accuracy for this batch of test sentences.\n        tmp_val_accuracy = flat_accuracy(logits, label_ids)\n        \n        #Accumulate the total accuracy.\n        val_accuracy += tmp_val_accuracy\n\n        #Track the number of batches\n        nb_val_steps += 1\n\n    # Report the final accuracy for this validation run.\n    print(\"  Accuracy: {0:.2f}\".format(val_accuracy/nb_val_steps))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_output = []\nfor batch in test_dataloader:\n  #Add batch to GPU\n  batch = tuple(t.to(device) for t in batch)\n  \n  #Unpack the inputs from dataloader\n  b_input_ids,b_attention_mask, b_token_type= batch\n  \n  #Telling the model not to compute or store gradients, saving memory and speeding up validation\n  with torch.no_grad():        \n      outputs = model(b_input_ids,token_type_ids=b_token_type,attention_mask=b_attention_mask)\n  \n  #Get the \"logits\" output by the model. The \"logits\" are output values prior to applying an activation function like the softmax.\n  logits = outputs[0]\n  final_output.extend(np.argmax(logits.detach().cpu().numpy(), axis=1).flatten())\n  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output = pd.DataFrame({'id': test.id,\n                       'prediction': final_output})\noutput.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}