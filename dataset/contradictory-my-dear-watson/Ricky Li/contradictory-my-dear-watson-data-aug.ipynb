{"cells":[{"metadata":{},"cell_type":"markdown","source":"The english text contains some errors. We'll see how they came about, and try to correct them.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Data cleanup","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"First we need to download and import the necessary libraries, and the datasets.","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"pip install pyspellchecker","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nfrom spellchecker import SpellChecker\nimport regex as re","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/contradictory-my-dear-watson/train.csv')\ntest = pd.read_csv('../input/contradictory-my-dear-watson/test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we'll just be working with the english entries. We could also do this for the non-english entries after translating.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eng = train.loc[train.language == 'English']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we'll load the spellchecker. The default list is incomplete so we'll load a dictionary from a separate text file from https://github.com/dwyl/english-words","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"spell = SpellChecker()  # loads default word frequency list\nspell.word_frequency.load_text_file('../input/english-words/words_alpha.txt')\nspell.word_frequency.load_words([''])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We want to check for misspelled words, but first we split on punctuation to get just the words back. We also remove things like numbers and dollar signs as things like $10 will be flagged as misspelled. Additionally, we remove the square brackets and apostrophes as this makes processing the words into the spellchecker easier.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"eng['premise_misspelled'] = eng.premise.apply(lambda sentence: tuple(spell.unknown(re.split('[!\\:;,.\\-\\% \\b\\s()\\\"/$0-9]',re.sub('[\\'\\[\\]]', '', sentence)))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"misspelled_df = eng.loc[eng.premise_misspelled != ()]\nlist(misspelled_df.premise_misspelled)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By looking through the list, we see one source of misspellings is when we have a double question mark, which we will see comes from accented characters. Another source is a '?\\xad' (which specifies a soft hyphen).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_colwidth', -1) #This allows us to see the full sentences of the dataframes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"err1 = misspelled_df.loc[eng.premise.str.contains('\\?\\?', case = False)] #Let the double question mark errors be err1\nerr2 = misspelled_df.loc[eng.premise.str.contains(\"\\xad\", case = False)] #and those containing \\xad be err2\nprint(len(err1), len(err2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are not too many of these, and we can fix them individually. Let's see what is left after removing these guys.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"reduced_1 = pd.concat([misspelled_df, err1, err1]).drop_duplicates(keep=False)\nreduced_2 = pd.concat([reduced_1, err2, err2]).drop_duplicates(keep=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can look through misspellings once the common errors are removed.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"list(reduced_2.premise_misspelled)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking through, we see that the marked mistakes are mostly either an actual word followed by a question mark, proper nouns, acronyms or words joined together. Most of these we are fine to leave as is. Only the words joined together might be worth correcting, but after tokenisation we hope that the meaning is mostly captured there. \n\nRegarding spelling errors, I've only spotted one so far: in index 10307, we have 'Behind the cathedral, croseover the Rue de la Republique to the 15th-century Eglise Saint-Maclou, the richest example of Flam­boy­ant Gothic in the country.', where 'croseover' should be 'cross over', but that's not to say there aren't more errors.\n\nSo only the mistakes identified should change the meaning.\nFrom here, we just need to correct mistakes of the first kind:","execution_count":null},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"err1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Replacing the ?? with an 'e' fixes most of these, there are just 9 corrections left: rhone (occurring three times), ataturk (occurring twice), madrileno (occurring twice), alacahoyuk and alcudia. We also see why these mistakes appeared - all of these should be accented. Most of these are the French 'é', but we also should have 'Rhône', 'Atatürk', 'Madrileño', 'Alacahöyük' and 'Alcúdia'. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"correction = err1.premise.apply(lambda sentence: re.sub('\\?\\?', 'e', sentence))\ncorrect_rhone = correction.apply(lambda sentence: re.sub('Rhene', 'Rhone', sentence))\ncorrect_ataturk = correct_rhone.apply(lambda sentence: re.sub('Ataterk', 'Ataturk', sentence))\ncorrect_madrileno = correct_ataturk.apply(lambda sentence: re.sub('Madrileeo', 'Madrileno', sentence))\ncorrect_alacahoyuk = correct_madrileno.apply(lambda sentence: re.sub('Alacaheyek', 'Alacahoyuk', sentence))\ncorrect_alcudia = correct_alacahoyuk.apply(lambda sentence: re.sub('Alcedia', 'Alcudia', sentence))\ncorrection1 = correct_alcudia","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And then we update the dataframe","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in correction1.index:\n    eng.loc[eng.index == i, 'premise'] = correction1.loc[correction1.index == i]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And now for corrections of the second kind","execution_count":null},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"err2 = eng.loc[eng.premise.str.contains(\"\\?\\xad\", case = False)]\nerr2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It looks like somehow one of the double question marks was missed, in index 7534, 'Cham??bord'! Fortunately, it seems that we can simply remove the question marks, as with all the other question marks. (The only iffy one is 'Arab?', in index 1449, which should be Punta Arabí, but it looks like the mistake has been carried over into the test, so we won't bother.)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"correction2 = err2.premise.apply(lambda sentence: re.sub('\\?\\xad', '', sentence))\nfor i in correction2.index:\n    eng.loc[eng.index == i, 'premise'] = correction2.loc[correction2.index == i]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"How about the hypotheses?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"eng['hypothesis_misspelled'] = eng.hypothesis.apply(lambda sentence: tuple(spell.unknown(re.split('[\\?!\\:;,.\\-\\% \\b\\s()\\\"/$0-9]',re.sub('[\\'\\[\\]]', '', sentence)))))\nmisspelled_hyp_df = eng.loc[eng.hypothesis_misspelled != ()]\nlist(misspelled_hyp_df.hypothesis_misspelled) #contains the 'misspelled' words","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Again, some typos can be spotted, like 'availalbe' at id 11997 and 'asssess'. We'll hope that whatever encoding is being used will not get too tripped up by these. Also by taking len gives 636 mistakes, much shorter than premise.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"After searching for mistakes of the kind we fixed in the 'premise', it looks like we don't get any errors of the form we corrected earlier. There is a small correction worth making for the hypotheses though: 'Ile de Re' is recorded as 'Ile de R' in the hypotheses where the isle is mentioned.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"eng.loc[eng.hypothesis.str.contains('Ile de R', case = False)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eng.loc[eng.hypothesis == 'Ile de R is no longer part of the attraction.', 'hypothesis'] = 'Ile de Re is no longer part of the attraction.'\neng.loc[eng.hypothesis == 'Ile de R.', 'hypothesis'] = 'Ile de Re.'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"One last thing we can do is remove brackets where they occur, noting that the meaning doesn't change if they are taken out, and remove the &amp from the premise and replace with the word 'and'.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"square_brackets = eng.loc[eng.premise.str.contains('[\\[\\]]', case = False)]\nfor i in square_brackets.index:\n     eng.loc[eng.index == i, 'premise'] = re.sub('[\\[\\]]', '', str(eng.loc[eng.index == i].premise.values[0]))\nampersands = eng.loc[eng.premise.str.contains('\\&amp', case = False)]\nfor i in ampersands.index:\n    eng.loc[eng.index == i, 'premise'] = re.sub('\\&amp', ' and ', str(eng.loc[eng.index == i].premise.values[0]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, we drop the misspelled columns and update the training frame with the cleaned data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"eng.drop(columns=['premise_misspelled', 'hypothesis_misspelled'])\nfor i in eng.index:\n    train.loc[train.index == i] = eng.loc[eng.index == i]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Cleaning the test set**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"eng_test = test.loc[test.language == 'English']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"err1 = eng_test.loc[eng_test.premise.str.contains('\\?\\?', case = False)]\ncorrection = err1.premise.apply(lambda sentence: re.sub('\\?\\?', 'e', sentence))\ncorrect_alacahoyuk = correction.apply(lambda sentence: re.sub('Alacaheyek', 'Alacahoyuk', sentence))\ncorrect_madrileno = correct_alacahoyuk.apply(lambda sentence: re.sub('Madrileeo', 'Madrileno', sentence))\ncorrect_alcudia = correct_madrileno.apply(lambda sentence: re.sub('Alcedia', 'Alcudia', sentence))\ncorrect_ataturk = correct_alcudia.apply(lambda sentence: re.sub('Ataterk', 'Ataturk', sentence))\nfor i in correct_ataturk.index:\n    eng_test.loc[eng_test.index == i, 'premise'] = correct_ataturk.loc[correct_ataturk.index == i]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"err2 = eng_test.loc[eng_test.premise.str.contains('\\?\\xad', case = False)]\ncorrection = err2.premise.apply(lambda sentence: re.sub('\\?\\xad', '', sentence))\nfor i in correction.index:\n    eng_test.loc[eng_test.index == i, 'premise'] = correction.loc[correction.index == i]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"square_brackets = eng_test.loc[eng_test.premise.str.contains('[\\[\\]]', case = False)]\nfor i in square_brackets.index:\n     eng_test.loc[eng_test.index == i, 'premise'] = re.sub('[\\[\\]]', '', str(eng_test.loc[eng_test.index == i].premise.values[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ampersands = eng_test.loc[eng_test.premise.str.contains('\\&amp', case = False)]\nfor i in ampersands.index:\n    eng_test.loc[eng_test.index == i, 'premise'] = re.sub('\\&amp', ' and ', str(eng_test.loc[eng_test.index == i].premise.values[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in eng_test.index:\n    test.loc[test.index == i] = eng_test.loc[eng_test.index == i]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cool, so that's english. I wonder what the other languages are like regarding the two main errors identified?\nIt turns out that after checking the other languages, we don't get the same kinds of mistakes.\nLet's save our cleaned dataframes.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.to_csv('train_cleaned.csv',index=False)\ntest.to_csv('test_cleaned.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Augmentation","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Translation augmentation**\n\nOne way we can augment the data is by translating the premise-hypothesis pairs into a different language, following JohnM's notebook.","execution_count":null},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"!pip install git+https://github.com/ssut/py-googletrans.git","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from googletrans import Translator\nfrom dask import bag, diagnostics\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def translate(words, dest):\n    dest_choices = ['zh-cn',\n                    'ar',\n                    'fr',\n                    'sw',\n                    'ur',\n                    'vi',\n                    'ru',\n                    'hi',\n                    'el',\n                    'th',\n                    'es',\n                    'de',\n                    'tr',\n                    'bg'\n                    ]\n    if not dest:\n        dest = np.random.choice(dest_choices)\n        \n    translator = Translator()\n    decoded = translator.translate(words, dest=dest).text\n    return decoded\n\n\n#TODO: use a dask dataframe instead of all this\ndef trans_parallel(df, dest):\n    premise_bag = bag.from_sequence(df.premise.tolist()).map(translate, dest)\n    hypo_bag =  bag.from_sequence(df.hypothesis.tolist()).map(translate, dest)\n    with diagnostics.ProgressBar():\n        premises = premise_bag.compute()\n        hypos = hypo_bag.compute()\n    df[['premise', 'hypothesis']] = list(zip(premises, hypos))\n    return df\n\n    \neng_trans = train.loc[train.lang_abv == \"en\"].copy() \\\n           .pipe(trans_parallel, dest=None)\n\nnon_eng_trans =  train.loc[train.lang_abv != \"en\"].copy() \\\n                .pipe(trans_parallel, dest='en')\n\n#These two lines are not in JohnM's notebook and are here to update the language and lang_abv column for the new dataframes\neng_trans[['lang_abv', 'language']] = [['mx', 'Mixed']]*len(eng)\nnon_eng_trans[['lang_abv', 'language']] = [['en', 'English']]*len(non_eng_trans)\n\ntrain = train.append([eng_trans, non_eng_trans])\ntrain.reset_index\ntrain.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Synonym Augmentation**\n\nWe could also make new samples by swapping out words for synonyms, where synonymous words have been learned by some selected language model. We'll use the nlpaug library to do this.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install git+https://github.com/makcedward/nlpaug","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import nlpaug.augmenter.word as naw\nimport nlpaug.flow as nafc\nfrom nlpaug.util import Action","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There's a number of different things we can do - insert or substitute words based on contextual embeddings, replace by synonyms, etc. However we have to be careful not to change the meaning of the sentence too much. Substitution based on contextual embeddings *could* give a sentence with a similar meaning, but could also change the meaning completely, especially if the word being substituted is integral to the meaning of the sentence. \n\nThe two types of augmentation that are likely to not change the meaning too much are synonym substitution and contextual insertion. Let's take a look at it in action.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Insertion\n\nThere's a large number of models that nlpaug can use for contextual insertion: BERT, DistilBERT, RoBERTa and XLNet and within these models you may choose cased or uncased, the size of the model, etc.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"text = train.premise.values[0]\nmodel = 'distilbert-base-uncased'\nins_aug = naw.ContextualWordEmbsAug(\n    model_path=model, action=\"insert\")\nprint(\"Original:\")\nprint(text)\nprint(\"Augmented Text:\")\nfor i in range(10):\n    augmented_text = ins_aug.augment(text)\n    print(augmented_text)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It looks as though most of these have the same meaning, but a couple have an opposite meaning, and some might not make sense. Additionally the tokens can be split introducing a text error. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Synonym","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"syn_aug = naw.SynonymAug(aug_src='wordnet')\naugmented_text = syn_aug.augment(text)\nprint(\"Original:\")\nprint(text)\nprint(\"Augmented Text:\")\nfor i in range(10):\n    augmented_text = syn_aug.augment(text)\n    print(augmented_text)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Synonym substitution is less likely to negate the meaning: negation is more likely with the contextual insertion as there the completion will sound logical, but the negative of the statement could be a completely logical completion. Here, we are more likely to get nonsense though, as the synonym is blindly selected and doesn't take into account different usage of the same word.\n\nThe most striking example I found was when 'he' was substituted for 'atomic number 2', having interpreted 'he' as helium! \n\nHowever, we'll press on. After all, machine learning is more of an art than a science!","execution_count":null},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"\ndef insert_augment(words, model):\n    ins_aug = naw.ContextualWordEmbsAug(\n        model_path=model, action=\"insert\")    \n    augmented_text = ins_aug.augment(words)\n    return augmented_text\n\ndef ins_aug_parallel(df, model):\n    premise_bag = bag.from_sequence(df.premise.tolist()).map(insert_augment, model)\n    hypo_bag =  bag.from_sequence(df.hypothesis.tolist()).map(insert_augment, model)\n    with diagnostics.ProgressBar():\n        premises = premise_bag.compute()\n        hypos = hypo_bag.compute()\n    df[['premise', 'hypothesis']] = list(zip(premises, hypos))\n    return df\n\neng_ins_aug = train.loc[train.lang_abv == \"en\"].copy() \\\n           .pipe(ins_aug_parallel, model='distilbert-base-uncased')\n\ntrain = train.append([eng_ins_aug])\n\ntrain.to_csv('train_cleaned_ins.csv',index=False)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef synonym_augment(words):\n    syn_aug = naw.SynonymAug(\n        aug_src = 'wordnet')    \n    augmented_text = syn_aug.augment(words)\n    return augmented_text\n\ndef syn_aug_parallel(df):\n    premise_bag = bag.from_sequence(df.premise.tolist()).map(synonym_augment)\n    hypo_bag =  bag.from_sequence(df.hypothesis.tolist()).map(synonym_augment)\n    with diagnostics.ProgressBar():\n        premises = premise_bag.compute()\n        hypos = hypo_bag.compute()\n    df[['premise', 'hypothesis']] = list(zip(premises, hypos))\n    return df\n\neng_syn_aug = train.loc[train.lang_abv == \"en\"].copy() \\\n           .pipe(syn_aug_parallel)\n\ntrain = train.append([eng_syn_aug])\n\ntrain.to_csv('train_cleaned_ins_syn.csv',index=False)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train using TPU","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We'll follow Shahules' kernel.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Import libraries:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom kaggle_datasets import KaggleDatasets\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport transformers\nfrom transformers import TFAutoModel, AutoTokenizer\nfrom sklearn.model_selection import StratifiedKFold\nplt.style.use('fivethirtyeight')\nimport warnings\nwarnings.filterwarnings('ignore')\nimport os","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Set up TPUs:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Detect hardware, return appropriate distribution strategy\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n    # set: this is always the case on Kaggle.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Define variables:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"MODEL = 'jplu/tf-xlm-roberta-large'\nEPOCHS = 8\nMAX_LEN = 96\n\n# Our batch size will depend on number of replicas\nBATCH_SIZE= 16 * strategy.num_replicas_in_sync\nAUTO = tf.data.experimental.AUTOTUNE","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Skip loading datasets as we already have the ones we want.\n\nSo next we encode the training data:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Load the tokenizer\ntokenizer = AutoTokenizer.from_pretrained(MODEL)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def quick_encode(df,maxlen=100):\n    \n    values = df[['premise','hypothesis']].values.tolist()\n    tokens=tokenizer.batch_encode_plus(values,max_length=maxlen,pad_to_max_length=True)\n    \n    return np.array(tokens['input_ids'])\n\nx_train = quick_encode(train)\nx_test = quick_encode(test)\ny_train = train.label.values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Convert to tf.data.Dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_dist_dataset(X, y,val,batch_size= BATCH_SIZE):\n    \n    \n    dataset = tf.data.Dataset.from_tensor_slices((X,y)).shuffle(len(X))\n          \n    if not val:\n        dataset = dataset.repeat().batch(batch_size).prefetch(AUTO)\n    else:\n        dataset = dataset.batch(batch_size).prefetch(AUTO)\n\n    \n    \n    return dataset\n\n\n\ntest_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((x_test))\n    .batch(BATCH_SIZE)\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Train the model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model(transformer,max_len):\n    \n    input_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_ids\")\n    sequence_output = transformer(input_ids)[0]\n    cls_token = sequence_output[:, 0, :]\n    out = Dense(3, activation='softmax')(cls_token)\n\n    # It's time to build and compile the model\n    model = Model(inputs=input_ids, outputs=out)\n    model.compile(\n        Adam(lr=1e-5), \n        loss='sparse_categorical_crossentropy', \n        metrics=['accuracy']\n    )\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_steps = len(x_train) // batch_size\n\ntrain_history = model.fit(\n    train_dataset,\n    steps_per_epoch=n_steps,\n    validation_data=valid_dataset,\n    epochs=n_epochs\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# K-fold validation","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Following Shahules' kernel","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_test=np.zeros((test.shape[0],3))\nskf = StratifiedKFold(n_splits=5,shuffle=True,random_state=777)\nval_score=[]\nhistory=[]\n\n\nfor fold,(train_ind,valid_ind) in enumerate(skf.split(x_train,y_train)):\n    \n    if fold < 4:\n    \n        print(\"fold\",fold+1)\n        \n       \n        tf.tpu.experimental.initialize_tpu_system(tpu)\n        \n        train_data = create_dist_dataset(x_train[train_ind],y_train[train_ind],val=False)\n        valid_data = create_dist_dataset(x_train[valid_ind],y_train[valid_ind],val=True)\n    \n        Checkpoint=tf.keras.callbacks.ModelCheckpoint(f\"roberta_base.h5\", monitor='val_loss', verbose=0, save_best_only=True,\n        save_weights_only=True, mode='min')\n        \n        with strategy.scope():\n            transformer_layer = TFAutoModel.from_pretrained(MODEL)\n            model = build_model(transformer_layer, max_len=MAX_LEN)\n            \n        \n\n        n_steps = len(train_ind)//BATCH_SIZE\n        print(\"training model {} \".format(fold+1))\n\n        train_history = model.fit(\n        train_data,\n        steps_per_epoch=n_steps,\n        validation_data=valid_data,\n        epochs=EPOCHS,callbacks=[Checkpoint],verbose=0)\n        \n        print(\"Loading model...\")\n        model.load_weights(f\"roberta_base.h5\")\n        \n        \n\n        print(\"fold {} validation acc {}\".format(fold+1,np.mean(train_history.history['val_accuracy'])))\n        print(\"fold {} validation acc {}\".format(fold+1,np.mean(train_history.history['val_loss'])))\n        \n        history.append(train_history)\n\n        val_score.append(np.mean(train_history.history['val_accuracy']))\n        \n        print('predict on test....')\n        preds=model.predict(test_dataset,verbose=1)\n\n        pred_test+=preds/4","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Evaluation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\n\nfor i,hist in enumerate(history):\n\n    plt.subplot(2,2,i+1)\n    plt.plot(np.arange(EPOCHS),hist.history['accuracy'],label='train accu')\n    plt.plot(np.arange(EPOCHS),hist.history['val_accuracy'],label='validation acc')\n    plt.gca().title.set_text(f'Fold {i+1} accuracy curve')\n    plt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\n\nfor i,hist in enumerate(history):\n\n    plt.subplot(2,2,i+1)\n    plt.plot(np.arange(EPOCHS),hist.history['loss'],label='train loss')\n    plt.plot(np.arange(EPOCHS),hist.history['val_loss'],label='validation loss')\n    plt.gca().title.set_text(f'Fold {i+1} loss curve')\n    plt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prediction","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv('/kaggle/input/contradictory-my-dear-watson/sample_submission.csv')\nsubmission['prediction'] = np.argmax(pred_test,axis=1)\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}