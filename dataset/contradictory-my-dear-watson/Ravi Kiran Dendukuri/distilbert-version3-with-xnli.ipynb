{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.graph_objects as go\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam, Adamax\n\nfrom transformers import DistilBertTokenizer, TFDistilBertModel\n\nfrom datasets import load_dataset, list_datasets\n\nfrom sklearn.model_selection import train_test_split\n\nimport gc\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\nimport warnings,json\nwarnings.filterwarnings('ignore')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-07T01:15:20.439735Z","iopub.execute_input":"2022-05-07T01:15:20.440185Z","iopub.status.idle":"2022-05-07T01:15:30.23419Z","shell.execute_reply.started":"2022-05-07T01:15:20.440139Z","shell.execute_reply":"2022-05-07T01:15:30.233337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.environ[\"WANDB_API_KEY\"] = \"0\"","metadata":{"execution":{"iopub.status.busy":"2022-05-07T01:15:30.236096Z","iopub.execute_input":"2022-05-07T01:15:30.236407Z","iopub.status.idle":"2022-05-07T01:15:30.241613Z","shell.execute_reply.started":"2022-05-07T01:15:30.236367Z","shell.execute_reply":"2022-05-07T01:15:30.240357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Init_TPU():  \n\n    try:\n        resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n        tf.config.experimental_connect_to_cluster(resolver)\n        tf.tpu.experimental.initialize_tpu_system(resolver)\n        strategy = tf.distribute.experimental.TPUStrategy(resolver)\n        REPLICAS = strategy.num_replicas_in_sync\n        print(\"Connected to TPU Successfully:\\n TPUs Initialised with Replicas:\",REPLICAS)\n        \n        return strategy\n    \n    except ValueError:\n        \n        print(\"Connection to TPU Falied\")\n        print(\"Using default strategy for CPU and single GPU\")\n        strategy = tf.distribute.get_strategy()\n        \n        return strategy\n    \nstrategy=Init_TPU()","metadata":{"execution":{"iopub.status.busy":"2022-05-07T01:15:30.243403Z","iopub.execute_input":"2022-05-07T01:15:30.244361Z","iopub.status.idle":"2022-05-07T01:15:35.907949Z","shell.execute_reply.started":"2022-05-07T01:15:30.244325Z","shell.execute_reply":"2022-05-07T01:15:35.907177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = '../input/contradictory-my-dear-watson/'","metadata":{"execution":{"iopub.status.busy":"2022-05-07T01:15:35.908896Z","iopub.execute_input":"2022-05-07T01:15:35.90912Z","iopub.status.idle":"2022-05-07T01:15:35.913433Z","shell.execute_reply.started":"2022-05-07T01:15:35.909094Z","shell.execute_reply":"2022-05-07T01:15:35.912454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntrain_url = os.path.join(path,'train.csv')\ntrain_data = pd.read_csv(train_url, header='infer')\n\nsample_sub_url = os.path.join(path,'sample_submission.csv')\nsample_sub = pd.read_csv(sample_sub_url, header='infer')\n\ntest_url = os.path.join(path,'test.csv')\ntest_data = pd.read_csv(test_url, header='infer')","metadata":{"execution":{"iopub.status.busy":"2022-05-07T01:15:35.915554Z","iopub.execute_input":"2022-05-07T01:15:35.915782Z","iopub.status.idle":"2022-05-07T01:15:36.146705Z","shell.execute_reply.started":"2022-05-07T01:15:35.915754Z","shell.execute_reply":"2022-05-07T01:15:36.145928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-07T01:15:36.149673Z","iopub.execute_input":"2022-05-07T01:15:36.149991Z","iopub.status.idle":"2022-05-07T01:15:36.170936Z","shell.execute_reply.started":"2022-05-07T01:15:36.14995Z","shell.execute_reply":"2022-05-07T01:15:36.170366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-05-07T01:15:36.172163Z","iopub.execute_input":"2022-05-07T01:15:36.172396Z","iopub.status.idle":"2022-05-07T01:15:36.374305Z","shell.execute_reply.started":"2022-05-07T01:15:36.17237Z","shell.execute_reply":"2022-05-07T01:15:36.373511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Transformer Model Name\ntransformer_model = 'distilbert-base-multilingual-cased'\n\n# Define Tokenizer\ntokenizer = DistilBertTokenizer.from_pretrained(transformer_model)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T01:15:36.375865Z","iopub.execute_input":"2022-05-07T01:15:36.376099Z","iopub.status.idle":"2022-05-07T01:15:38.577093Z","shell.execute_reply.started":"2022-05-07T01:15:36.376072Z","shell.execute_reply":"2022-05-07T01:15:38.576253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking the output of tokenizer\ntokenizer.convert_tokens_to_ids(list(tokenizer.tokenize(\"Elementary, My Dear Watson!\")))","metadata":{"execution":{"iopub.status.busy":"2022-05-07T01:15:38.578535Z","iopub.execute_input":"2022-05-07T01:15:38.578851Z","iopub.status.idle":"2022-05-07T01:15:38.587491Z","shell.execute_reply.started":"2022-05-07T01:15:38.578811Z","shell.execute_reply":"2022-05-07T01:15:38.586579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = load_dataset('xnli', 'all_languages', split='train+validation+test') #returns a Dataset object  \nprint(dataset)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T01:15:38.58867Z","iopub.execute_input":"2022-05-07T01:15:38.589003Z","iopub.status.idle":"2022-05-07T01:19:04.251703Z","shell.execute_reply.started":"2022-05-07T01:15:38.58886Z","shell.execute_reply":"2022-05-07T01:19:04.250692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\nentries = []   \nfor entry in tqdm(dataset): \n    hypothesis_langs = entry['hypothesis']['language'] #list of 15 lang string values\n    hypothesis_values = entry['hypothesis']['translation'] #list of 15 hypothesis string values\n\n    premise_langs = list(entry['premise'].keys()) #list of 15 lang string values\n    premise_values = list(entry['premise'].values()) #list of 15 premise string values\n\n    labels = [entry['label']]*len(hypothesis_langs) #all 15 languages for the same example have same label \n\n    if premise_langs == hypothesis_langs: #the languages in premise and hypothesis are in same order\n#             values = list(zip(premise_values, hypothesis_values, hypothesis_langs, labels))\n        values = list(zip(premise_values, hypothesis_values, hypothesis_langs,labels))\n        entries += values\n\n#     xnli_df = pd.DataFrame(entries, columns=['premise', 'hypothesis', 'lang_abv', 'label']) #create dataframe for each key\nxnli_df = pd.DataFrame(entries, columns=['premise', 'hypothesis', 'lang_abv','label']) ","metadata":{"execution":{"iopub.status.busy":"2022-05-07T01:19:04.253706Z","iopub.execute_input":"2022-05-07T01:19:04.254415Z","iopub.status.idle":"2022-05-07T01:20:57.642215Z","shell.execute_reply.started":"2022-05-07T01:19:04.254378Z","shell.execute_reply":"2022-05-07T01:20:57.641091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"missing_values_xnli = xnli_df.isnull().sum() \n\nprint(\"Number of missing data points per column in XNLI corpus:\")\nprint (missing_values_xnli)\n\n# Drop the missing value rows\nxnli_df.dropna(axis=0, inplace=True)\nprint(\"Total number of data examples in XNLI corpus after dropping NA values: {}\".format(xnli_df.shape[0]))","metadata":{"execution":{"iopub.status.busy":"2022-05-07T01:20:57.643758Z","iopub.execute_input":"2022-05-07T01:20:57.644037Z","iopub.status.idle":"2022-05-07T01:21:04.054206Z","shell.execute_reply.started":"2022-05-07T01:20:57.644003Z","shell.execute_reply":"2022-05-07T01:21:04.052973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xnli_df","metadata":{"execution":{"iopub.status.busy":"2022-05-07T01:21:04.055859Z","iopub.execute_input":"2022-05-07T01:21:04.0562Z","iopub.status.idle":"2022-05-07T01:21:04.076578Z","shell.execute_reply.started":"2022-05-07T01:21:04.056156Z","shell.execute_reply":"2022-05-07T01:21:04.075316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xnli_df = xnli_df[['premise', 'hypothesis' ,'label']]\nxnli_df","metadata":{"execution":{"iopub.status.busy":"2022-05-07T01:21:37.330912Z","iopub.execute_input":"2022-05-07T01:21:37.331812Z","iopub.status.idle":"2022-05-07T01:21:37.737515Z","shell.execute_reply.started":"2022-05-07T01:21:37.331755Z","shell.execute_reply":"2022-05-07T01:21:37.736603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data=pd.concat([train_data, xnli_df.loc[:20000]], axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T01:21:41.799854Z","iopub.execute_input":"2022-05-07T01:21:41.800202Z","iopub.status.idle":"2022-05-07T01:21:41.841616Z","shell.execute_reply.started":"2022-05-07T01:21:41.800171Z","shell.execute_reply":"2022-05-07T01:21:41.840701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create seperate list from Train & Test Dataframes with only Premise & Hypothesis\ntrain = train_data[['premise','hypothesis']].values.tolist()\ntest = test_data[['premise','hypothesis']].values.tolist()","metadata":{"execution":{"iopub.status.busy":"2022-05-07T01:21:43.917701Z","iopub.execute_input":"2022-05-07T01:21:43.918612Z","iopub.status.idle":"2022-05-07T01:21:43.953253Z","shell.execute_reply.started":"2022-05-07T01:21:43.918565Z","shell.execute_reply":"2022-05-07T01:21:43.952292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define Max Length\nmax_len = 80   # << change if you wish\n\n# Encode the training & test data \ntrain_encode = tokenizer.batch_encode_plus(train, pad_to_max_length=True, max_length=max_len)\ntest_encode = tokenizer.batch_encode_plus(test, pad_to_max_length=True, max_length=max_len)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T01:21:45.933717Z","iopub.execute_input":"2022-05-07T01:21:45.934062Z","iopub.status.idle":"2022-05-07T01:22:26.298801Z","shell.execute_reply.started":"2022-05-07T01:21:45.93403Z","shell.execute_reply":"2022-05-07T01:22:26.297912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split the Training Data into Training (90%) & Validation (10%)\n\ntest_size = 0.1  # << change if you wish\nx_train, x_val, y_train, y_val = train_test_split(train_encode['input_ids'], train_data.label.values, test_size=test_size)\n\n\n# Split Test Data\nx_test = test_encode['input_ids']","metadata":{"execution":{"iopub.status.busy":"2022-05-07T01:22:26.301004Z","iopub.execute_input":"2022-05-07T01:22:26.301288Z","iopub.status.idle":"2022-05-07T01:22:26.324324Z","shell.execute_reply.started":"2022-05-07T01:22:26.301257Z","shell.execute_reply":"2022-05-07T01:22:26.323142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#garbage collect\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-05-07T01:22:26.326432Z","iopub.execute_input":"2022-05-07T01:22:26.326816Z","iopub.status.idle":"2022-05-07T01:22:26.9973Z","shell.execute_reply.started":"2022-05-07T01:22:26.326766Z","shell.execute_reply":"2022-05-07T01:22:26.996113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loading Data Into TensorFlow Dataset\nAUTO = tf.data.experimental.AUTOTUNE\nbatch_size = 16 * strategy.num_replicas_in_sync\n\ntrain_ds = (tf.data.Dataset.from_tensor_slices((x_train, y_train)).repeat().shuffle(3072).batch(batch_size).prefetch(AUTO))\nval_ds = (tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(batch_size).prefetch(AUTO))\n\ntest_ds = (tf.data.Dataset.from_tensor_slices(x_test).batch(batch_size))","metadata":{"execution":{"iopub.status.busy":"2022-05-07T01:22:26.999878Z","iopub.execute_input":"2022-05-07T01:22:27.000374Z","iopub.status.idle":"2022-05-07T01:22:37.678014Z","shell.execute_reply.started":"2022-05-07T01:22:27.000316Z","shell.execute_reply":"2022-05-07T01:22:37.676866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Garbage Collection\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-05-07T01:22:37.679641Z","iopub.execute_input":"2022-05-07T01:22:37.679948Z","iopub.status.idle":"2022-05-07T01:22:38.350215Z","shell.execute_reply.started":"2022-05-07T01:22:37.679913Z","shell.execute_reply":"2022-05-07T01:22:38.34935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_model(strategy,transformer):\n    with strategy.scope():\n        transformer_encoder = TFDistilBertModel.from_pretrained(transformer)  #Pretrained BERT Transformer Model\n        \n        input_layer = Input(shape=(max_len,), dtype=tf.int32, name=\"input_layer\")\n        \n        sequence_output = transformer_encoder(input_layer)[0]\n        \n        cls_token = sequence_output[:, 0, :]\n        \n        output_layer = Dense(3, activation='softmax')(cls_token)\n        \n        model = Model(inputs=input_layer, outputs=output_layer)\n        \n        model.compile(\n            Adamax(lr=1e-5), \n            loss='sparse_categorical_crossentropy', \n            metrics=['accuracy']\n        )\n        \n        return model\n    \n\n# Applying the build model function\nmodel = build_model(strategy,transformer_model)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T01:22:38.351302Z","iopub.execute_input":"2022-05-07T01:22:38.351538Z","iopub.status.idle":"2022-05-07T01:23:31.886554Z","shell.execute_reply.started":"2022-05-07T01:22:38.351509Z","shell.execute_reply":"2022-05-07T01:23:31.885411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model Summary\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-05-07T01:23:31.888521Z","iopub.execute_input":"2022-05-07T01:23:31.888861Z","iopub.status.idle":"2022-05-07T01:23:31.905291Z","shell.execute_reply.started":"2022-05-07T01:23:31.888816Z","shell.execute_reply":"2022-05-07T01:23:31.904121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the Model\n\nepochs = 20  # < change if you wish\nn_steps = len(train_data) // batch_size \n\nmodel.fit(train_ds, \n          steps_per_epoch = n_steps, \n          validation_data = val_ds,\n          epochs = epochs)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T01:23:31.907376Z","iopub.execute_input":"2022-05-07T01:23:31.907765Z","iopub.status.idle":"2022-05-07T01:32:02.375462Z","shell.execute_reply.started":"2022-05-07T01:23:31.907715Z","shell.execute_reply":"2022-05-07T01:32:02.374317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Garbage Collection\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-05-07T01:32:02.378825Z","iopub.execute_input":"2022-05-07T01:32:02.379618Z","iopub.status.idle":"2022-05-07T01:32:03.597729Z","shell.execute_reply.started":"2022-05-07T01:32:02.379574Z","shell.execute_reply":"2022-05-07T01:32:03.596616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction = model.predict(test_ds, verbose=0)\nsample_sub['prediction'] = prediction.argmax(axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T01:32:03.613991Z","iopub.execute_input":"2022-05-07T01:32:03.614351Z","iopub.status.idle":"2022-05-07T01:32:12.855625Z","shell.execute_reply.started":"2022-05-07T01:32:03.614311Z","shell.execute_reply":"2022-05-07T01:32:12.854529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_sub.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T01:32:12.85717Z","iopub.execute_input":"2022-05-07T01:32:12.857479Z","iopub.status.idle":"2022-05-07T01:32:12.881841Z","shell.execute_reply.started":"2022-05-07T01:32:12.857433Z","shell.execute_reply":"2022-05-07T01:32:12.880806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_sub.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-07T01:32:12.883208Z","iopub.execute_input":"2022-05-07T01:32:12.883636Z","iopub.status.idle":"2022-05-07T01:32:12.895034Z","shell.execute_reply.started":"2022-05-07T01:32:12.8836Z","shell.execute_reply":"2022-05-07T01:32:12.89422Z"},"trusted":true},"execution_count":null,"outputs":[]}]}