{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\nimport sys\nimport os\nimport math\nimport subprocess\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nimport random\nimport torch\nimport torch.nn as nn","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-08T01:25:56.546115Z","iopub.execute_input":"2022-05-08T01:25:56.546546Z","iopub.status.idle":"2022-05-08T01:25:56.559871Z","shell.execute_reply.started":"2022-05-08T01:25:56.546516Z","shell.execute_reply":"2022-05-08T01:25:56.559155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def random_init(**kwargs):\n    random.seed(kwargs['seed'])\n    torch.manual_seed(kwargs['seed'])\n    torch.cuda.manual_seed(kwargs['seed'])\n    torch.backends.cudnn.deterministic = True","metadata":{"execution":{"iopub.status.busy":"2022-05-08T01:25:56.561275Z","iopub.execute_input":"2022-05-08T01:25:56.561638Z","iopub.status.idle":"2022-05-08T01:25:56.568362Z","shell.execute_reply.started":"2022-05-08T01:25:56.56161Z","shell.execute_reply":"2022-05-08T01:25:56.567141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def normalise(text):\n    chars = list('ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789')\n    text = text.upper()\n    words=[]\n    for w in text.strip().split():\n        if w.startswith('HTTP'):\n            continue\n        while len(w)>0 and w[0] not in chars:\n            w = w[1:]\n        while len(w)>0 and w[-1] not in chars:\n            w = w[:-1]\n        if len(w) == 0:\n            continue\n        words.append(w)\n    text=' '.join(words)\n    return text","metadata":{"execution":{"iopub.status.busy":"2022-05-08T01:25:56.571602Z","iopub.execute_input":"2022-05-08T01:25:56.572166Z","iopub.status.idle":"2022-05-08T01:25:56.58782Z","shell.execute_reply.started":"2022-05-08T01:25:56.572113Z","shell.execute_reply":"2022-05-08T01:25:56.586889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def read_vocabulary(train_text, **kwargs):\n    vocab = dict()\n    counts = dict()\n    num_words = 0\n    for line in train_text:\n        line = (list(line.strip()) if kwargs['characters'] else line.strip().split())\n        for char in line:\n            if char not in vocab:\n                vocab[char] = num_words\n                counts[char] = 0\n                num_words+=1\n            counts[char] += 1\n    num_words = 0\n    vocab2 = dict()\n    if not kwargs['characters']:\n        for w in vocab:\n            if counts[w] >= args['min_count']:\n                vocab2[w] = num_words\n                num_words += 1\n    vocab = vocab2\n    for word in [kwargs['start_token'],kwargs['end_token'],kwargs['unk_token']]:\n        if word not in vocab:\n            vocab[word] = num_words\n            num_words += 1\n    return vocab","metadata":{"execution":{"iopub.status.busy":"2022-05-08T01:25:56.589505Z","iopub.execute_input":"2022-05-08T01:25:56.589822Z","iopub.status.idle":"2022-05-08T01:25:56.602037Z","shell.execute_reply.started":"2022-05-08T01:25:56.589785Z","shell.execute_reply":"2022-05-08T01:25:56.601229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_data(premise, hypothesis, targets=None, cv=False, **kwargs):\n    assert len(premise) == len(hypothesis)\n    num_seq = len(premise)\n    max_words = max([len(t) for t in premise+hypothesis])+2\n    dataset = len(kwargs['vocab'])*torch.ones((2,max_words,num_seq),dtype=torch.long)\n    labels = torch.zeros((num_seq),dtype=torch.uint8)\n    idx = 0\n    utoken_value = kwargs['vocab'][kwargs['unk_token']]\n    for i,line in tqdm(enumerate(premise),desc='Allocating data memory',disable=(kwargs['verbose']<2)):\n        words = (list(line.strip()) if kwargs['characters'] else line.strip().split())\n        if len(words)==0 or words[0] != kwargs['start_token']:\n            words.insert(0,kwargs['start_token'])\n        if words[-1] != kwargs['end_token']:\n            words.append(kwargs['end_token'])\n        for jdx,word in enumerate(words):\n            dataset[0,jdx,idx] = kwargs['vocab'].get(word,utoken_value)\n        line=hypothesis[i]\n        words = (list(line.strip()) if kwargs['characters'] else line.strip().split())\n        if len(words)==0 or words[0] != kwargs['start_token']:\n            words.insert(0,kwargs['start_token'])\n        if words[-1] != kwargs['end_token']:\n            words.append(kwargs['end_token'])\n        for jdx,word in enumerate(words):\n            dataset[1,jdx,idx] = kwargs['vocab'].get(word,utoken_value)\n        if targets is not None:\n            labels[idx] = targets[i]\n        idx += 1\n\n    if cv == False:\n        return dataset, labels\n\n    idx = [i for i in range(num_seq)]\n    random.shuffle(idx)\n    trainset = dataset[:,:,idx[0:int(num_seq*(1-kwargs['cv_percentage']))]]\n    trainlabels = labels[idx[0:int(num_seq*(1-kwargs['cv_percentage']))]]\n    validset = dataset[:,:,idx[int(num_seq*(1-kwargs['cv_percentage'])):]]\n    validlabels = labels[idx[int(num_seq*(1-kwargs['cv_percentage'])):]]\n    return trainset, validset, trainlabels, validlabels","metadata":{"execution":{"iopub.status.busy":"2022-05-08T01:25:56.603143Z","iopub.execute_input":"2022-05-08T01:25:56.603662Z","iopub.status.idle":"2022-05-08T01:25:56.621843Z","shell.execute_reply.started":"2022-05-08T01:25:56.603631Z","shell.execute_reply":"2022-05-08T01:25:56.620695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class LSTMEncoder(nn.Module):\n    def __init__(self, **kwargs):\n        \n        super(LSTMEncoder, self).__init__()\n        #Base variables\n        self.vocab = kwargs['vocab']\n        self.in_dim = len(self.vocab)\n        self.start_token = kwargs['start_token']\n        self.end_token = kwargs['end_token']\n        self.unk_token = kwargs['unk_token']\n        self.characters = kwargs['characters']\n        self.embed_dim = kwargs['embedding_size']\n        self.hid_dim = kwargs['hidden_size']\n        self.n_layers = kwargs['num_layers']\n        \n        #Define the embedding layer\n        self.embed = nn.Embedding(self.in_dim+1,self.embed_dim,padding_idx=self.in_dim)\n        #Define the lstm layer\n        self.lstm = nn.LSTM(input_size=self.embed_dim,hidden_size=self.hid_dim,num_layers=self.n_layers)\n    \n    def forward(self, inputs, lengths):\n        #Inputs are size (LxBx1)\n        #Forward embedding layer\n        emb = self.embed(inputs)\n        #Embeddings are size (LxBxself.embed_dim)\n\n        #Pack the sequences for GRU\n        packed = torch.nn.utils.rnn.pack_padded_sequence(emb, lengths)\n        #Forward the GRU\n        packed_rec, self.hidden = self.lstm(packed,self.hidden)\n        #Unpack the sequences\n        rec, _ = torch.nn.utils.rnn.pad_packed_sequence(packed_rec)\n        #Hidden outputs are size (LxBxself.hidden_size)\n        \n        #Get last embeddings\n        out = rec[lengths-1,list(range(rec.shape[1])),:]\n        #Outputs are size (Bxself.hid_dim)\n        \n        return out\n    \n    def init_hidden(self, bsz):\n        #Initialise the hidden state\n        weight = next(self.parameters())\n        self.hidden = (weight.new_zeros(self.n_layers, bsz, self.hid_dim),weight.new_zeros(self.n_layers, bsz, self.hid_dim))\n\n    def detach_hidden(self):\n        #Detach the hidden state\n        self.hidden=(self.hidden[0].detach(),self.hidden[1].detach())\n\n    def cpu_hidden(self):\n        #Set the hidden state to CPU\n        self.hidden=(self.hidden[0].detach().cpu(),self.hidden[1].detach().cpu())","metadata":{"execution":{"iopub.status.busy":"2022-05-08T01:31:20.426681Z","iopub.execute_input":"2022-05-08T01:31:20.427005Z","iopub.status.idle":"2022-05-08T01:31:20.441371Z","shell.execute_reply.started":"2022-05-08T01:31:20.426974Z","shell.execute_reply":"2022-05-08T01:31:20.440702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Predictor(nn.Module):\n    def __init__(self, **kwargs):\n        \n        super(Predictor, self).__init__()\n        self.hid_dim = kwargs['hidden_size']*2\n        self.out_dim = 3\n        #Define the output layer and softmax\n        self.linear = nn.Linear(self.hid_dim,self.out_dim)\n        self.softmax = nn.LogSoftmax(dim=1)\n        \n    def forward(self,input1,input2):\n        #Outputs are size (Bxself.hid_dim)\n        inputs = torch.cat((input1,input2),dim=1)\n        out = self.softmax(self.linear(inputs))\n        return out","metadata":{"execution":{"iopub.status.busy":"2022-05-08T01:25:56.66335Z","iopub.execute_input":"2022-05-08T01:25:56.663617Z","iopub.status.idle":"2022-05-08T01:25:56.675318Z","shell.execute_reply.started":"2022-05-08T01:25:56.663554Z","shell.execute_reply":"2022-05-08T01:25:56.674666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_model(trainset,trainlabels,encoder,predictor,optimizer,criterion,**kwargs):\n    trainlen = trainset.shape[2]\n    nbatches = math.ceil(trainlen/kwargs['batch_size'])\n    total_loss = 0\n    total_backs = 0\n    with tqdm(total=nbatches,disable=(kwargs['verbose']<2)) as pbar:\n        encoder = encoder.train()\n        for b in range(nbatches):\n            #Data batch\n            X1 = trainset[0,:,b*kwargs['batch_size']:min(trainlen,(b+1)*kwargs['batch_size'])].clone().long().to(kwargs['device'])\n            mask1 = torch.clamp(len(kwargs['vocab'])-X1,max=1)\n            seq_length1 = torch.sum(mask1,dim=0)\n            ordered_seq_length1, dec_index1 = seq_length1.sort(descending=True)\n            max_seq_length1 = torch.max(seq_length1)\n            X1 = X1[:,dec_index1]\n            X1 = X1[0:max_seq_length1]\n            rev_dec_index1 = list(range(seq_length1.shape[0]))\n            for i,j in enumerate(dec_index1):\n                rev_dec_index1[j] = i\n            X2 = trainset[1,:,b*kwargs['batch_size']:min(trainlen,(b+1)*kwargs['batch_size'])].clone().long().to(kwargs['device'])\n            mask2 = torch.clamp(len(kwargs['vocab'])-X2,max=1)\n            seq_length2 = torch.sum(mask2,dim=0)\n            ordered_seq_length2, dec_index2 = seq_length2.sort(descending=True)\n            max_seq_length2 = torch.max(seq_length2)\n            X2 = X2[:,dec_index2]\n            X2 = X2[0:max_seq_length2]\n            rev_dec_index2 = list(range(seq_length2.shape[0]))\n            for i,j in enumerate(dec_index2):\n                rev_dec_index2[j] = i\n            Y = trainlabels[b*kwargs['batch_size']:min(trainlen,(b+1)*kwargs['batch_size'])].clone().long().to(kwargs['device'])\n            #Forward pass\n            encoder.init_hidden(X1.size(1))\n            embeddings1 = encoder(X1,ordered_seq_length1)\n            encoder.detach_hidden()\n            encoder.init_hidden(X2.size(1))\n            embeddings2 = encoder(X2,ordered_seq_length2)\n            embeddings1 = embeddings1[rev_dec_index1]\n            embeddings2 = embeddings2[rev_dec_index2]\n            posteriors = predictor(embeddings1,embeddings2)\n            loss = criterion(posteriors,Y)\n            #Backpropagate\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            #Estimate the latest loss\n            if total_backs == 100:\n                total_loss = total_loss*0.99+loss.detach().cpu().numpy()\n            else:\n                total_loss += loss.detach().cpu().numpy()\n                total_backs += 1\n            encoder.detach_hidden()\n            pbar.set_description(f'Training epoch. Loss {total_loss/(total_backs+1):.2f}')\n            pbar.update()\n    return total_loss/(total_backs+1)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T01:32:05.890938Z","iopub.execute_input":"2022-05-08T01:32:05.891461Z","iopub.status.idle":"2022-05-08T01:32:05.909835Z","shell.execute_reply.started":"2022-05-08T01:32:05.891427Z","shell.execute_reply":"2022-05-08T01:32:05.908922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate_model(testset,encoder,predictor,**kwargs):\n    testlen = testset.shape[2]\n    nbatches = math.ceil(testlen/kwargs['batch_size'])\n    predictions = np.zeros((testlen,))\n    with torch.no_grad():\n        encoder = encoder.eval()\n        for b in range(nbatches):\n            #Data batch\n            X1 = testset[0,:,b*kwargs['batch_size']:min(testlen,(b+1)*kwargs['batch_size'])].clone().long().to(kwargs['device'])\n            mask1 = torch.clamp(len(kwargs['vocab'])-X1,max=1)\n            seq_length1 = torch.sum(mask1,dim=0)\n            ordered_seq_length1, dec_index1 = seq_length1.sort(descending=True)\n            max_seq_length1 = torch.max(seq_length1)\n            X1 = X1[:,dec_index1]\n            X1 = X1[0:max_seq_length1]\n            rev_dec_index1 = list(range(seq_length1.shape[0]))\n            for i,j in enumerate(dec_index1):\n                rev_dec_index1[j] = i\n            X2 = testset[1,:,b*kwargs['batch_size']:min(testlen,(b+1)*kwargs['batch_size'])].clone().long().to(kwargs['device'])\n            mask2 = torch.clamp(len(kwargs['vocab'])-X2,max=1)\n            seq_length2 = torch.sum(mask2,dim=0)\n            ordered_seq_length2, dec_index2 = seq_length2.sort(descending=True)\n            max_seq_length2 = torch.max(seq_length2)\n            X2 = X2[:,dec_index2]\n            X2 = X2[0:max_seq_length2]\n            rev_dec_index2 = list(range(seq_length2.shape[0]))\n            for i,j in enumerate(dec_index2):\n                rev_dec_index2[j] = i\n            #Forward pass\n            encoder.init_hidden(X1.size(1))\n            embeddings1 = encoder(X1,ordered_seq_length1)\n            encoder.init_hidden(X2.size(1))\n            embeddings2 = encoder(X2,ordered_seq_length2)\n            embeddings1 = embeddings1[rev_dec_index1]\n            embeddings2 = embeddings2[rev_dec_index2]\n            posteriors = predictor(embeddings1,embeddings2)\n            #posteriors = model(X,ordered_seq_length)\n            estimated = torch.argmax(posteriors,dim=1)\n            predictions[b*kwargs['batch_size']:min(testlen,(b+1)*kwargs['batch_size'])] = estimated.detach().cpu().numpy()\n    return predictions","metadata":{"execution":{"iopub.status.busy":"2022-05-08T01:25:56.698064Z","iopub.execute_input":"2022-05-08T01:25:56.698458Z","iopub.status.idle":"2022-05-08T01:25:56.717288Z","shell.execute_reply.started":"2022-05-08T01:25:56.69843Z","shell.execute_reply":"2022-05-08T01:25:56.716541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nargs = {\n    'cv_percentage': 0.1,\n    'epochs': 20,\n    'batch_size': 128,\n    'embedding_size': 16,\n    'hidden_size': 64,\n    'num_layers': 1,\n    'learning_rate': 0.01,\n    'seed': 0,\n    'start_token': '<s>',\n    'end_token': '<\\s>',\n    'unk_token': '<UNK>',\n    'verbose': 1,\n    'characters': False,\n    'min_count': 15,\n    'device': torch.device(('cuda:0' if torch.cuda.is_available() else 'cpu'))\n    }\n","metadata":{"execution":{"iopub.status.busy":"2022-05-08T01:32:11.251964Z","iopub.execute_input":"2022-05-08T01:32:11.252263Z","iopub.status.idle":"2022-05-08T01:32:11.258946Z","shell.execute_reply.started":"2022-05-08T01:32:11.252231Z","shell.execute_reply":"2022-05-08T01:32:11.257686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Read data\ntrain_data = pd.read_csv('/kaggle/input/contradictory-my-dear-watson/train.csv')\ntest_data = pd.read_csv('/kaggle/input/contradictory-my-dear-watson/test.csv')\n#Extract only English language cases\ntrain_data = train_data.loc[train_data['language']=='English']\ntest_data = test_data.loc[test_data['language']=='English']\n#Extract premises and hypothesis\ntrain_premise = [normalise(v) for v in train_data.premise.values]\ntrain_hypothesis = [normalise(v) for v in train_data.hypothesis.values]\ntest_premise = [normalise(v) for v in test_data.premise.values]\ntest_hypothesis = [normalise(v) for v in test_data.hypothesis.values]\ntrain_targets = train_data.label.values\nprint('Training: {0:d} pairs in English. Evaluation: {1:d} pairs in English'.format(len(train_premise),len(test_premise)))\nprint('Label distribution in training set: {0:s}'.format(str({i:'{0:.2f}%'.format(100*len(np.where(train_targets==i)[0])/len(train_targets)) for i in [0,1,2]})))\n\nbatch_sizes = [64,128,256]\nmin_counts = [5,15,25]\n\nit_idx = 0\nvalid_predictions = dict()\ntest_predictions = dict()\nvalid_accuracies = dict()\n\nfor batch_size in batch_sizes:\n    for min_count in min_counts:\n        args['batch_size'] = batch_size\n        args['min_count'] = min_count\n    \n        random_init(**args)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T01:32:13.775525Z","iopub.execute_input":"2022-05-08T01:32:13.776289Z","iopub.status.idle":"2022-05-08T01:32:14.415369Z","shell.execute_reply.started":"2022-05-08T01:32:13.776246Z","shell.execute_reply":"2022-05-08T01:32:14.414349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Make vocabulary and load data\nargs['vocab'] = read_vocabulary(train_premise+train_hypothesis, **args)\n#print('Vocabulary size: {0:d} tokens'.format(len(args['vocab'])))\ntrainset, validset, trainlabels, validlabels = load_data(train_premise, train_hypothesis, train_targets, cv=True, **args)\ntestset, _ = load_data(test_premise, test_hypothesis, None, cv=False, **args)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-08T01:32:17.040041Z","iopub.execute_input":"2022-05-08T01:32:17.040337Z","iopub.status.idle":"2022-05-08T01:32:20.457727Z","shell.execute_reply.started":"2022-05-08T01:32:17.040304Z","shell.execute_reply":"2022-05-08T01:32:20.456826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Create model, optimiser and criterion\nencoder = LSTMEncoder(**args).to(args['device'])\npredictor = Predictor(**args).to(args['device'])\noptimizer = torch.optim.Adam(list(encoder.parameters())+list(predictor.parameters()),lr=args['learning_rate'])\ncriterion = nn.NLLLoss(reduction='mean').to(args['device'])","metadata":{"execution":{"iopub.status.busy":"2022-05-08T01:32:20.460973Z","iopub.execute_input":"2022-05-08T01:32:20.46124Z","iopub.status.idle":"2022-05-08T01:32:20.471521Z","shell.execute_reply.started":"2022-05-08T01:32:20.461209Z","shell.execute_reply":"2022-05-08T01:32:20.470696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Train epochs\nbest_acc = 0.0\nfor ep in range(1,args['epochs']+1):\n    loss = train_model(trainset,trainlabels,encoder,predictor,optimizer,criterion,**args)\n    val_pred = evaluate_model(validset,encoder,predictor,**args)\n    test_pred = evaluate_model(testset,encoder,predictor,**args)\n    acc = 100*len(np.where((val_pred-validlabels.numpy())==0)[0])/validset.shape[2]\n    if (acc >= best_acc):\n        best_epoch = ep\n        best_loss = loss\n        best_acc = acc\n        valid_predictions[it_idx] = val_pred\n        valid_accuracies[it_idx] = acc\n        test_predictions[it_idx] = test_pred\nprint('Run {0:d}. Best epoch: {1:d} of {2:d}. Training loss: {3:.2f}, validation accuracy: {4:.2f}%, test label distribution: {5:s}'.format(it_idx+1,best_epoch,args['epochs'],best_loss,best_acc,str({i:'{0:.2f}%'.format(100*len(np.where(test_pred==i)[0])/len(test_pred)) for i in [0,1,2]})))\nit_idx += 1","metadata":{"execution":{"iopub.status.busy":"2022-05-08T01:32:21.270432Z","iopub.execute_input":"2022-05-08T01:32:21.270738Z","iopub.status.idle":"2022-05-08T01:34:30.853286Z","shell.execute_reply.started":"2022-05-08T01:32:21.270708Z","shell.execute_reply":"2022-05-08T01:34:30.85231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Do the score combination\nbest_epochs = np.argsort([valid_accuracies[ep] for ep in range(it_idx)])[::-1]\nval_pred = np.array([valid_predictions[ep] for ep in best_epochs[0:5]])\nval_pred = np.argmax(np.array([np.sum((val_pred==i).astype(int),axis=0) for i in [0,1,2]]),axis=0)\ntest_pred = np.array([test_predictions[ep] for ep in best_epochs[0:5]])\ntest_pred = np.argmax(np.array([np.sum((test_pred==i).astype(int),axis=0) for i in [0,1,2]]),axis=0)\nacc = 100*len(np.where((val_pred-validlabels.numpy())==0)[0])/validset.shape[2]\nprint('Ensemble. Cross-validation accuracy: {0:.2f}%, test label distribution: {1:s}'.format(acc,str({i:'{0:.2f}%'.format(100*len(np.where(test_pred==i)[0])/len(test_pred)) for i in [0,1,2]})))\n#Set all predictions to the majority category\ndf_out = pd.DataFrame({'id': pd.read_csv('/kaggle/input/contradictory-my-dear-watson/test.csv')['id'], 'prediction': np.argmax([len(np.where(train_targets==i)[0]) for i in [0,1,2]])})\n#Set only English language cases to the predicted labels\ndf_out.loc[df_out['id'].isin(test_data['id']),'prediction']=test_pred\ndf_out.to_csv('/kaggle/working/submission.csv'.format(it_idx,acc),index=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T01:36:15.153049Z","iopub.execute_input":"2022-05-08T01:36:15.153352Z","iopub.status.idle":"2022-05-08T01:36:15.223509Z","shell.execute_reply.started":"2022-05-08T01:36:15.153323Z","shell.execute_reply":"2022-05-08T01:36:15.222675Z"},"trusted":true},"execution_count":null,"outputs":[]}]}