{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom transformers import BertTokenizer, TFBertModel, AutoTokenizer,TFAutoModel\nimport tensorflow as tf\nfrom datasets import load_dataset\nimport kerastuner as kt\n#import plotlib as plt\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk(\"/kaggle/input\"):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\nimport warnings\n\nwarnings.filterwarnings('ignore') # ignore Jupiter warnings","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept ValueError:\n    strategy = tf.distribute.get_strategy() # for CPU and single GPU\n    print('Number of replicas:', strategy.num_replicas_in_sync)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T18:01:29.922926Z","iopub.status.idle":"2022-05-08T18:01:29.923585Z","shell.execute_reply.started":"2022-05-08T18:01:29.923309Z","shell.execute_reply":"2022-05-08T18:01:29.923337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_Bert = 'bert-base-multilingual-cased'\ntokenizer = AutoTokenizer.from_pretrained(model_Bert)\nmodel = TFAutoModel.from_pretrained(model_Bert)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T18:01:29.924777Z","iopub.status.idle":"2022-05-08T18:01:29.926209Z","shell.execute_reply.started":"2022-05-08T18:01:29.925834Z","shell.execute_reply":"2022-05-08T18:01:29.9259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer","metadata":{"execution":{"iopub.status.busy":"2022-05-08T18:01:29.92736Z","iopub.status.idle":"2022-05-08T18:01:29.928389Z","shell.execute_reply.started":"2022-05-08T18:01:29.928121Z","shell.execute_reply":"2022-05-08T18:01:29.928155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.environ[\"WANDB_API_KEY\"] = \"0\" ","metadata":{"execution":{"iopub.status.busy":"2022-05-08T18:01:29.929375Z","iopub.status.idle":"2022-05-08T18:01:29.930022Z","shell.execute_reply.started":"2022-05-08T18:01:29.929786Z","shell.execute_reply":"2022-05-08T18:01:29.929809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(\"../input/contradictory-my-dear-watson/train.csv\")\ntest = pd.read_csv(\"../input/contradictory-my-dear-watson/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-05-08T18:01:29.931082Z","iopub.status.idle":"2022-05-08T18:01:29.931791Z","shell.execute_reply.started":"2022-05-08T18:01:29.931605Z","shell.execute_reply":"2022-05-08T18:01:29.931629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-08T18:01:29.932987Z","iopub.status.idle":"2022-05-08T18:01:29.93374Z","shell.execute_reply.started":"2022-05-08T18:01:29.933441Z","shell.execute_reply":"2022-05-08T18:01:29.933469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-08T18:01:29.93508Z","iopub.status.idle":"2022-05-08T18:01:29.935421Z","shell.execute_reply.started":"2022-05-08T18:01:29.935241Z","shell.execute_reply":"2022-05-08T18:01:29.935263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nlabels, frequencies =np.unique(train['language'], return_counts=True)\nplt.figure(figsize=[10, 10])\nplt.pie(frequencies, labels=labels, autopct='%1.1f%%')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-08T18:01:29.936838Z","iopub.status.idle":"2022-05-08T18:01:29.937674Z","shell.execute_reply.started":"2022-05-08T18:01:29.937333Z","shell.execute_reply":"2022-05-08T18:01:29.937363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train[['premise', 'hypothesis', 'lang_abv', 'label']]\ntrain","metadata":{"execution":{"iopub.status.busy":"2022-05-08T18:01:29.938749Z","iopub.status.idle":"2022-05-08T18:01:29.939423Z","shell.execute_reply.started":"2022-05-08T18:01:29.939196Z","shell.execute_reply":"2022-05-08T18:01:29.939228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SEQ_LEN = 236  \n\ndef bert_encode(df, tokenizer):    \n    batch_premises = df['premise'].tolist()\n    batch_hypothesis = df['hypothesis'].tolist()\n\n    tokens = tokenizer(batch_premises, batch_hypothesis, max_length = SEQ_LEN,\n                   truncation=True, padding='max_length',\n                   add_special_tokens=True, return_attention_mask=True,\n                   return_token_type_ids=True,\n                   return_tensors='tf')\n    inputs = {\n          'input_ids': tokens['input_ids'], \n          'attention_mask': tokens['attention_mask'],\n          'token_type_ids': tokens['token_type_ids']  }  \n    return inputs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_input = bert_encode(train, tokenizer)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_input","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras import regularizers\n\ndef build_model(): \n   \n    encoder = TFAutoModel.from_pretrained(model_Bert)\n    input_ids = tf.keras.Input(shape=(SEQ_LEN,), dtype=tf.int32, name=\"input_ids\")\n    attention_mask = tf.keras.Input(shape=(SEQ_LEN,), dtype=tf.int32, name=\"attention_mask\")\n    token_type_ids = tf.keras.Input(shape=(SEQ_LEN,), \n                                    dtype=tf.int32,  name=\"token_type_ids\")\n        \n    embedding = encoder([input_ids, attention_mask , token_type_ids])[0] \n    inputs=[input_ids, attention_mask  , token_type_ids ] \n    hp_units1 = 64 \n    hp_units2 = 32 \n    x = tf.keras.layers.Dense(units = hp_units1, activation=tf.nn.relu)(embedding[:,0,:])\n    x = tf.keras.layers.Dense(units = hp_units2, activation=tf.nn.relu, kernel_regularizer=regularizers.l2(l2=1e-4))(x)\n    output = tf.keras.layers.Dense(3, activation='softmax')(x)\n      \n    model = tf.keras.Model(inputs=inputs, outputs=output)\n    hp_learning_rate = 1e-6\n    model.compile(tf.keras.optimizers.Adam(learning_rate = hp_learning_rate), loss='sparse_categorical_crossentropy', metrics=['accuracy'])   \n    return model ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with strategy.scope(): \n    model = build_model()\n    model.summary()   ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for key in train_input.keys():\n    train_input[key] = train_input[key][:,:SEQ_LEN]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nhistory = model.fit(train_input, train['label'], epochs = 5,verbose =1, batch_size=30, \n                    validation_split = 0.2)#,callbacks=[hist]) verbose = 1,  ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(loss) + 1)\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.clf() \nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_input = bert_encode(test, tokenizer) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for key in test_input.keys():\n    test_input[key] = test_input[key][:,:SEQ_LEN]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = [np.argmax(i) for i in model.predict(test_input)]\nmodel.evaluate(test_input)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = test.id.copy().to_frame()\nsubmission['prediction'] = predictions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv(\"./submission.csv\", index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}