{"cells":[{"metadata":{},"cell_type":"markdown","source":"# 모델을 바꿔서 성능을 높여보자.\n- [Contradictory, My Dear Watson using XLNI Robert2](https://www.kaggle.com/rahulbana/contradictory-my-dear-watson-using-xlni-robert2)\n  - https://huggingface.co/joeddav/xlm-roberta-large-xnli 모델 사용"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/contradictory-my-dear-watson/train.csv')\ntest = pd.read_csv('../input/contradictory-my-dear-watson/test.csv')\nsubmission = pd.read_csv('../input/contradictory-my-dear-watson/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['language'].value_counts(normalize=True)*100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['language'].value_counts(normalize=True)*100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.language.unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['label_str'] = train['label'].map({0 : \"entailment\", 1 : \"neutral\", 2 : \"contradiction\"})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns \n\nplt.figure(figsize=(8,5))\nsns.countplot(y ='label_str', data = train, alpha=.5, palette=\"muted\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,5))\nsns.countplot(y ='language', hue = \"label_str\", data = train, alpha=.5, palette=\"muted\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 모델 고르기\n\nMulti-lingual model 을 사용하기 위해서 [huggingface](https://huggingface.co/transformers/multilingual.html) 사이트를 참고하였습니다. multilingual model 이 제공해주는 언어가 XNLI 인 경우에는 [facebook 의 XNLI github](https://github.com/facebookresearch/XNLI) 을 보면 되는데, 다음과 같은 14개의 언어를 타겟으로 하고 있다고 하고 해당 언어들은 이 competition 에서 제공하는 train set 과 일치합니다.\n- French, Spanish, German, Greek, Bulgarian, Russian, Turkish, Arabic, Vietnamese, Thai, Chinese, Hindi, Swahili and Urdu\n\n이 중에서 저는 다음과 같은 XLM-RoBERTa 모델을 사용해보고자 합니다.\n- 100개의 언어로 된 새롭게 생성된 깨끗한 CommonCrawl data 2.5TB 를 기반으로 학습되었습니다.\n- mBEERT, XLM 과 같은 언어모델보다 downstream tasks 에 강점을 가진다고 합니다. (분류, sequence labeling, question answering)\n- 2개의 모델이 존재합니다: xlm-roberta-base, xlm-roberta-large\n\n저는 large 를 사용해보도록 하겠습니다.\n[이 사이트](https://huggingface.co/transformers/model_doc/xlmroberta.html) 를 살펴보면 대략적인 사용방법에 대해서 알 수 있습니다. 너무 어려워서 [이 노트북](https://www.kaggle.com/jbagdon/predict-with-tf-xlm-roberta-large) 을 따라하기로 하였습니다."},{"metadata":{"trusted":true},"cell_type":"code","source":"# !pip install -q transformers==3.0.2\n!pip install -q nlp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import BertTokenizer, AutoTokenizer, TFBertModel, TFXLMRobertaModel, TFAutoModel\nimport tensorflow as tf\nfrom tensorflow.keras import Input, Model, Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, LSTM, Embedding, GlobalAveragePooling1D\nfrom keras.optimizers import Adam\n\nfrom nlp import load_dataset\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# TPU 사용 준비"},{"metadata":{"trusted":true},"cell_type":"code","source":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print(f'Running on TPU: {tpu.master()}')\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept ValueError:\n    strategy = tf.distribute.get_strategy() # for CPU and single GPU\nprint('Number of replicas:', strategy.num_replicas_in_sync)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 모델 준비"},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://huggingface.co/jplu/tf-xlm-roberta-large\n# encoder_handle = 'jplu/tf-xlm-roberta-large'\nencoder_handle = 'joeddav/xlm-roberta-large-xnli'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://huggingface.co/jplu/tf-xlm-roberta-large/raw/main/config.json 가능\n!curl https://s3.amazonaws.com/models.huggingface.co/bert/jplu/tf-xlm-roberta-large/config.json","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(encoder_handle)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 너무 작은 건 아닌가?\nmax_len = 120 # max sequence length\n# random_seed = 2021\nrandom_seed = 11887\nlearning_rate = 1e-5 # Controls how large a step is taken when updating model weights during training.\nepochs = 3\nbatch_size = 16 * strategy.num_replicas_in_sync # The number of examples that will be processed in parallel during training. Tailored for TPUs.\nloss = 'sparse_categorical_crossentropy'\nmetrics = ['accuracy']\n# steps_per_epoch = 1000\n\nauto = tf.data.experimental.AUTOTUNE","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# encoding 함수\n\nragged: 누더기가 된\n\ntf.ragged.constant: 데이터는 다양한 형태로 제공됩니다; 텐서도 마찬가지입니다. 비정형 텐서는 중첩 가변 길이 목록에 해당하는 텐서플로입니다. 다음을 포함하여 균일하지 않은 모양으로 데이터를 쉽게 저장하고 처리할 수 있습니다.\n- 일련의 영화의 배우들과 같은 가변 길이 기능\n- 문장이나 비디오 클립과 같은 가변 길이 순차적 입력의 배치\n- 절, 단락, 문장 및 단어로 세분화된 텍스트 문서와 같은 계층적 입력\n- 프로토콜 버퍼와 같은 구조화된 입력의 개별 필드\n\nragged tensor를 to_tensor 를 통해서 일반 tensor로 바꾸게 되면, 내부의 모든 데이터가 같은 길이를 갖게 된다. 이 때 짧은 값들이 길어지게 되면서 새로운 값들이 채워지게 되는데 그 때 사용되는 값이 default_value 값이다."},{"metadata":{"trusted":true},"cell_type":"code","source":"def encode_sentence(s, tokenizer):\n    \"\"\"\n    Turn a sequence of words into and array of numbers using a selected tokenizer.\n    Args:\n        s (list of str) - Input string.\n        tokenizer - XLM-R tokenizer.\n    Returns:\n        (list of int) - Tokenized string.\n\n    \"\"\"\n    tokens = list(tokenizer.tokenize(s))\n    tokens.append(tokenizer.sep_token)\n    return tokenizer.convert_tokens_to_ids(tokens)\n\ndef tokenize(data, tokenizer, max_len):\n    \"\"\"\n    Encode hypotheses and premises into arrays of numbers using a selected tokenizer. \n    Args:\n        data - An array consisting of [hypothesis (str), premise (str)] pairs.\n        tokenizer - Tokenizer handle.\n        max_len - Max sequence length.\n    Returns: (dictionary of tensors)\n        input_word_ids - Indices of input sequence tokens in the vocabulary, truncated to max_len.\n        input_mask - Real input indices mapped to ones. Padding indices mapped to zeroes.\n        input_type_ids - Segment token indices to indicate first and second portions of the inputs.\n    \"\"\"\n\n    PAD_ID = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n \n    # Append a separator to each sentence, tokenize, and concatenate.\n    tokens1 = tf.ragged.constant([encode_sentence(s[0], tokenizer) for s in data], dtype=tf.int32) # ENCODED_SEQUENCE_A [SEP]\n    tokens2 = tf.ragged.constant([encode_sentence(s[1], tokenizer) for s in data], dtype=tf.int32) # ENCODED_SEQUENCE_B [SEP]\n    cls_label = [tokenizer.convert_tokens_to_ids([tokenizer.cls_token])]*tokens1.shape[0] # [CLS] ENCODED_SEQUENCE_A [SEP]\n    tokens = tf.concat([cls_label, tokens1, tokens2], axis=-1) # [CLS] ENCODED_SEQUENCE_A [SEP] ENCODED_SEQUENCE_B [SEP]\n\n    # Truncate to max_len.\n    tokens = tokens[:, :max_len]\n\n    # Pad with zeroes if len < max_len.\n    tokens = tokens.to_tensor(default_value=PAD_ID)\n    pad = max_len - tf.shape(tokens)[1]\n    tokens = tf.pad(tokens, [[0, 0], [0, pad]], constant_values=PAD_ID)\n    input_word_ids = tf.reshape(tokens, [-1, max_len])\n\n    # The input mask allows the model to cleanly differentiate between the content and the padding. \n    input_mask = tf.cast(input_word_ids != PAD_ID, tf.int32)\n    input_mask = tf.reshape(input_mask, [-1, max_len])\n\n    # Map tokens1 indices to zeroes and tokens2 indices to ones.\n    input_type_ids = tf.concat([tf.zeros_like(cls_label), tf.zeros_like(tokens1), tf.ones_like(tokens2)], axis=-1).to_tensor()\n\n\n    inputs = {\n      'input_word_ids': input_word_ids,\n      'input_mask': input_mask,\n      'input_type_ids': input_type_ids}\n\n    return inputs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_dataset(x, y, mode, batch_size):\n    \"\"\"\n    Build a batched TF training, validation, or test dataset.\n    \n    (This function is borrowed from some of the other notebooks in this competition -\n    not sure who to credit exactly so thanks all!)\n    \"\"\"\n    if mode == \"train\":\n        dataset = (\n            tf.data.Dataset\n            .from_tensor_slices((x, y))\n            .repeat()\n            .shuffle(5678)\n            .batch(batch_size)\n            .prefetch(auto)\n        )\n    elif mode == \"valid\":\n        dataset = (\n            tf.data.Dataset\n            .from_tensor_slices((x, y))\n            .batch(batch_size)\n            .cache()\n            .prefetch(auto)\n        )\n    elif mode == \"test\":\n        dataset = (\n            tf.data.Dataset\n            .from_tensor_slices(x)\n            .batch(batch_size)\n            )\n    else:\n        raise NotImplementedError\n    return dataset","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train, validation dataset 준비\n\n- mnli 데이터셋도 함께 사용하면 성능이 올라가는 것으로 보인다. (참고: https://www.kaggle.com/rahulbana/contradictory-my-dear-watson-using-xlni-robert2)\n하지만 다음의 코드는 왠지모르게 작동을 잘 안함\n\n```\n\n```\n\n그래서 huggingface 에 [dataset](https://github.com/huggingface/datasets) 을 이용하기로 함."},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip3 install datasets","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from datasets import load_dataset\n\ndef load_mnli(use_validation=True):\n    result = []\n    dataset = load_dataset('multi_nli')\n    print(dataset['train'])\n    keys = ['train', 'validation_matched','validation_mismatched'] if use_validation else ['train']\n    for k in keys:\n        for record in dataset[k]:\n            c1, c2, c3 = record['premise'], record['hypothesis'], record['label']\n            if c1 and c2 and c3 in {0,1,2}:\n                result.append((c1,c2,c3,'en'))\n    result = pd.DataFrame(result, columns=['premise','hypothesis','label','lang_abv'])\n    return result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mnli = load_mnli()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total_train = train[['premise', 'hypothesis', 'label']]\ntotal_train = pd.concat([total_train, mnli], axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train, x_valid, y_train, y_valid = train_test_split(total_train[['premise', 'hypothesis']].values.tolist(), total_train['label'], test_size=0.25, random_state=12345)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train_ = tokenize(x_train, tokenizer, max_len)\nx_valid_ = tokenize(x_valid, tokenizer, max_len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset = build_dataset(x_train_, y_train, \"train\", batch_size)\nvalid_dataset = build_dataset(x_valid_, y_valid, \"valid\", batch_size)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train model\n\n[transformers.XLMRobertaModel](https://huggingface.co/transformers/model_doc/xlmroberta.html#tfxlmrobertamodel) 은 tf.keras.Model 의 subclass 로 만들어 졌으며, TF2.0 Keras model 로 만들어졌다. Return 은 return_dict=True 인 경우에는 TFBaseModelOutputWithPooling 이 나오며, 그렇지 않으면 tf.Tensor 가 반환된다. roberta 가 뱉은 output 중\n- last_hidden_state 는 `(batch_size, sequence_length, hidden_size)` 의 shape 을 갖는다. \n- pooler_outpout 은 `(batch_size, hidden_size)` 의 크기를 갖으며 classification token 인 sequence 의 첫번째 토큰의 hidden-state 이다.   \n  - pooler_output (tf.Tensor of shape (batch_size, hidden_size)) – Last layer hidden-state of the first token of the sequence (classification token) further processed by a Linear layer and a Tanh activation function. The Linear layer weights are trained from the next sentence prediction (classification) objective during pretraining.\n  - This output is usually not a good summary of the semantic content of the input, **you’re often better with averaging or pooling the sequence of hidden-states for the whole input sequence.**\n\nroberta 의 output 을 3개의 unit 에 대한 softmax 로 만드는 것이 목표인데, 이 경우 보통은 Flatten 을 하고 Dense 를 쌓는 방식으로 가는데 여기서는 특이하게도 GlobalAveragePooling1D 를 사용하였다. 아마 위에서 hidden-states 를 averaging 하거나 pooling 하는게 좋다고 해서 그런것 같다."},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model(encoder_handle, random_seed, learning_rate, loss, metrics, max_len):\n    \n    tf.keras.backend.clear_session()\n    tf.random.set_seed(random_seed)\n    \n    with strategy.scope():\n        \n        input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n#          input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n        # RoBERTa doesn’t use token_type_ids.\n        \n        #  Create an instance of a model defined in encoder_handle\n#         roberta = TFXLMRobertaModel.from_pretrained(encoder_handle)\n#         roberta = roberta([input_word_ids, input_mask])[0]\n        roberta = TFAutoModel.from_pretrained(encoder_handle)\n        roberta = roberta([input_word_ids])[0]\n        out = GlobalAveragePooling1D()(roberta)\n        out = Dense(3, activation='softmax')(out)\n        \n        model = Model(inputs=[input_word_ids], outputs = out)\n        model.compile(optimizer=Adam(lr=learning_rate), loss=loss, metrics=metrics)\n    \n    model.summary()\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = build_model(encoder_handle, random_seed, learning_rate, loss, metrics, max_len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n                                                  verbose=1,\n                                                  patience=2,\n                                                  mode='min',\n                                                  restore_best_weights=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"steps_per_epoch = len(x_train) // batch_size\nhistory = model.fit(train_dataset,\n                    validation_data=valid_dataset,\n                    steps_per_epoch=steps_per_epoch,\n                    epochs=epochs,\n                    callbacks=[early_stopping])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(history.history.keys())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\n\n# summarize history for loss\nep_nbr = np.arange(1, len(history.history['accuracy']) + 1)\nplt.plot(ep_nbr, history.history['loss'])\nplt.plot(ep_nbr, history.history['val_loss'])\nplt.title('Unadjusted Loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()\n\n# Training loss is continually reported over the course of an entire epoch.\n# Validation metrics are computed over the validation set only once the current training epoch is completed.\n# This implies, that on average, training losses are measured half an epoch earlier.\n\n# plot the *shifted* training and validation loss\nplt.plot(ep_nbr - 0.5, history.history['loss'], label=\"train_loss\")\nplt.plot(ep_nbr, history.history['val_loss'], label=\"val_loss\")\nplt.title(\"Shifted Loss\")\nplt.xlabel(\"epoch\")\nplt.ylabel(\"loss\")\nplt.legend()\nplt.show()\n\n# summarize history for accuracy\nplt.plot(ep_nbr, history.history['accuracy'])\nplt.plot(ep_nbr, history.history['val_accuracy'])\nplt.title('Unadjusted Accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()\n\n# plot the *shifted* training and validation accuracy\nplt.plot(ep_nbr - 0.5, history.history['accuracy'], label=\"train_accuracy\")\nplt.plot(ep_nbr, history.history['val_accuracy'], label=\"val_accuracy\")\nplt.title(\"Shifted Accuracy\")\nplt.xlabel(\"epoch\")\nplt.ylabel(\"loss\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test = tokenize(test[['premise', 'hypothesis']].values.tolist(), tokenizer, max_len)\ntest_dataset  = build_dataset(x_test, None, \"test\", batch_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\n\npredictions_prob = model.predict(test_dataset)\nfinal = predictions_prob.argmax(axis=-1)   \n\nsubmission = pd.DataFrame()    \nsubmission['id'] = test['id']\nsubmission['prediction'] = final.astype(np.int32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv(\"/kaggle/working/submission.csv\", index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}