{"cells":[{"metadata":{},"cell_type":"markdown","source":"Notebook for competition: https://www.kaggle.com/c/contradictory-my-dear-watson"},{"metadata":{},"cell_type":"markdown","source":"## Load data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from pandas import read_csv, DataFrame\n\ndata = read_csv(\"../input/contradictory-my-dear-watson/train.csv\")\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Given data is not enough to get good result"},{"metadata":{},"cell_type":"markdown","source":"## Get extra data"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install nlp","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"MNLI data set: https://cims.nyu.edu/~sbowman/multinli/"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nlp import load_dataset\n\nextra_data = load_dataset(path='glue', name='mnli')\nmnli = []\nfor sample in extra_data['train']:\n    mnli.append([sample['premise'], sample['hypothesis'], sample['label']])\ndel extra_data\nmnli = DataFrame(mnli, columns=['premise','hypothesis','label'])\nlen(mnli)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Set up TPU\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.distribute.cluster_resolver import TPUClusterResolver\nfrom tensorflow.config import experimental_connect_to_cluster\nfrom tensorflow.tpu.experimental import initialize_tpu_system\nfrom tensorflow.distribute.experimental import TPUStrategy\nfrom tensorflow.distribute import get_strategy\n\ntry:\n    tpu = TPUClusterResolver()\n    experimental_connect_to_cluster(tpu)\n    initialize_tpu_system(tpu)\n    strategy = TPUStrategy(tpu)\n    print('using TPU...')\nexcept ValueError:\n    strategy = get_strategy() # for CPU and single GPU\n    print('not using TPU...')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Convert data into Tensorflow dataset"},{"metadata":{},"cell_type":"markdown","source":"pre-trained model from https://huggingface.co/jplu/tf-xlm-roberta-large"},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import AutoTokenizer, TFAutoModel\n\npretrained = 'jplu/tf-xlm-roberta-large'\ntokenizer = AutoTokenizer.from_pretrained(pretrained)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow import data as d\nfrom tensorflow.data.experimental import AUTOTUNE\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n# average length of encoded sequences in test set is ~44\n# bigger maximum length might need more than given 16GB of RAM\nmax_len = 256\n\ndef generator(dataset, batch_size):\n        \n    texts = dataset[['premise', 'hypothesis']].values.tolist()\n    encoded = tokenizer.batch_encode_plus(texts, max_length=max_len, padding=True, truncation=True)['input_ids']\n    label = dataset['label'].values\n    \n    # padding with 0\n    data_tensor = d.Dataset.from_tensor_slices((pad_sequences(encoded, padding='post', value=0), label))\n    \n    return data_tensor.shuffle(2048).batch(batch_size).prefetch(AUTOTUNE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"# bigger batch size might need more RAM\ntrain = generator(mnli, 256)\nval = generator(data, 256)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from gc import collect\n\ndel data, mnli\ncollect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Build model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.layers import Input, Dropout, Dense, GlobalAveragePooling1D\nfrom tensorflow.keras import Model\nfrom tensorflow import int32\nfrom tensorflow.keras.metrics import CategoricalAccuracy\n\ndef build(dropout_rate, optimizer, loss):\n    inputs = Input(shape=(max_len,), dtype=int32)\n    layers = TFAutoModel.from_pretrained(pretrained)(inputs)[0]\n#     layers = Dropout(rate=dropout_rate)(layers)\n    layers = GlobalAveragePooling1D()(layers)\n    outputs = Dense(3, activation='softmax')(layers)\n    model = Model(inputs=inputs, outputs=outputs)\n    model.layers[1].trainable = False\n    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.optimizers import Adam\n\nwith strategy.scope():\n    dropout_rate = 0\n    optimizer = Adam(lr=1e-5)\n    loss = 'sparse_categorical_crossentropy'\n    model = build(dropout_rate=dropout_rate, optimizer='adam',loss=loss)\n    model.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Time limit is 2 hours\n# Theoretically, one could train for infinite time by saving weights after each session then using the final weights for evaluation\nhst = model.fit(train, epochs=20, verbose=1, validation_data=val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef visualize(metric):\n    plt.plot(hst.history[metric])\n    plt.plot(hst.history['val_' + metric])\n    plt.title(metric)\n    plt.ylabel(metric)\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"visualize('loss')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"visualize('accuracy')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Predict"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_test = read_csv(\"../input/contradictory-my-dear-watson/test.csv\")\ntexts = data_test[['premise', 'hypothesis']].values.tolist()\nencoded = tokenizer.batch_encode_plus(texts, max_length=max_len, padding='max_length', truncation=True)['input_ids']\nencoded_data_test = d.Dataset.from_tensor_slices(pad_sequences(encoded, padding='post', value=0))\nencoded_data_test = encoded_data_test.batch(256)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction = model.predict(encoded_data_test, verbose=1)\nprediction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = data_test.id.copy().to_frame()\nsubmission['prediction'] = prediction.argmax(1)\nsubmission.to_csv(\"submission.csv\", index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}