{"cells":[{"metadata":{},"cell_type":"markdown","source":"[Training code is from this repo of mine](https://github.com/ceshine/finetuning-t5/tree/mt5-classifier-single-token/mnli). Currently my fine-tunes mT5 models still underperforms comparing to the BERT and XLM models.\n\nThe mT5-base model was [trained on the MNLI dataset (Enlgish-only)](https://www.kaggle.com/ceshine/preprocess-and-finetune-mt5?scriptVersionId=52689872). The result is slightly better than [the zero-shot one](https://www.kaggle.com/ceshine/mt5-base-mnli-zero-shot).\n\nThe embedding matrix in the encoder is frozen when training.\n\nUpdate in version 8: Use AdaFactor optimizer; update the maxlen trucation scheme; use the Sortish sampler. Train all weights."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"!pip install --upgrade pip\n!pip uninstall -y allennlp\n!pip install transformers==4.1.1 typer\n!pip install -U pytorch-lightning\n!pip install https://github.com/veritable-tech/pytorch-lightning-spells/archive/master.zip","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"!mkdir -p /src/finetuning-t5\n!git clone https://github.com/ceshine/finetuning-t5.git -b master /src/finetuning-t5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%cd /src/finetuning-t5/mnli","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!mkdir -p data/kaggle\n!cp -r /kaggle/input/contradictory-my-dear-watson/* data/kaggle\n!ls data/kaggle/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!mkdir -p data/mt5-base-mnli\n!cp -r /kaggle/input/mt5-base-mnli-pretrained/mt5-base_best/* data/mt5-base-mnli\n!ls data/mt5-base-mnli/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!mkdir -p cache/kaggle/\n!python preprocess/preprocess_kaggle.py\n!python preprocess/tokenize_dataset.py kaggle --tokenizer-name data/mt5-base-mnli","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!SEED=9923 python train.py --t5-model data/mt5-base-mnli --batch-size 64 --grad-accu 1 \\\n        --epochs 2 --lr 1e-4 --disable-progress-bar --dataset kaggle \\\n        --max-len 128 --freeze-embeddings --valid-frequency 0.25 ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls cache/\n!mv cache/tb_logs /kaggle/working\n!mv cache/mt5-base-mnli_best /kaggle/working","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!python kaggle_inference.py /kaggle/working/mt5-base-mnli_best\n!cp submission.csv /kaggle/working","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}