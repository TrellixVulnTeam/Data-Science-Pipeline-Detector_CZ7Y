{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"id":"1jnDwbo7wBJe","execution":{"iopub.status.busy":"2021-09-29T01:47:25.63624Z","iopub.execute_input":"2021-09-29T01:47:25.637138Z","iopub.status.idle":"2021-09-29T01:47:25.654616Z","shell.execute_reply.started":"2021-09-29T01:47:25.637095Z","shell.execute_reply":"2021-09-29T01:47:25.653961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.environ[\"WANDB_API_KEY\"] = \"0\" ## to silence warning","metadata":{"id":"WbsulzuHwJEO","execution":{"iopub.status.busy":"2021-09-29T01:47:25.655996Z","iopub.execute_input":"2021-09-29T01:47:25.656809Z","iopub.status.idle":"2021-09-29T01:47:25.661442Z","shell.execute_reply.started":"2021-09-29T01:47:25.65675Z","shell.execute_reply":"2021-09-29T01:47:25.660552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install transformers","metadata":{"id":"XAQPYzgNwOzv","outputId":"3a48b92f-8e10-4dcf-835b-e34884eeef21","execution":{"iopub.status.busy":"2021-09-29T01:47:25.664792Z","iopub.execute_input":"2021-09-29T01:47:25.665127Z","iopub.status.idle":"2021-09-29T01:47:35.105596Z","shell.execute_reply.started":"2021-09-29T01:47:25.665086Z","shell.execute_reply":"2021-09-29T01:47:35.104513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import BertTokenizer, TFBertModel\nimport matplotlib.pyplot as plt\nimport tensorflow as tf","metadata":{"id":"Mudus40QwLfO","execution":{"iopub.status.busy":"2021-09-29T01:47:35.107949Z","iopub.execute_input":"2021-09-29T01:47:35.108338Z","iopub.status.idle":"2021-09-29T01:47:41.159402Z","shell.execute_reply.started":"2021-09-29T01:47:35.108289Z","shell.execute_reply":"2021-09-29T01:47:41.158588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept ValueError:\n    strategy = tf.distribute.get_strategy() # for CPU and single GPU\n    print('Number of replicas:', strategy.num_replicas_in_sync)","metadata":{"id":"ObHUz5t3wZNW","outputId":"3f394ca9-90bd-4337-f54b-2b1b11e3d971","execution":{"iopub.status.busy":"2021-09-29T01:47:41.160734Z","iopub.execute_input":"2021-09-29T01:47:41.160997Z","iopub.status.idle":"2021-09-29T01:47:46.527373Z","shell.execute_reply.started":"2021-09-29T01:47:41.160966Z","shell.execute_reply":"2021-09-29T01:47:46.526383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from google.colab import drive\n# drive.mount('/content/drive', force_remount=True)","metadata":{"id":"mb5T9-XowaAX","outputId":"f6f0f367-1003-4e88-f254-63bd0b7570fe","execution":{"iopub.status.busy":"2021-09-29T01:47:46.529503Z","iopub.execute_input":"2021-09-29T01:47:46.529767Z","iopub.status.idle":"2021-09-29T01:47:46.53368Z","shell.execute_reply.started":"2021-09-29T01:47:46.529737Z","shell.execute_reply":"2021-09-29T01:47:46.532863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/contradictory-my-dear-watson/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/contradictory-my-dear-watson/test.csv\")","metadata":{"id":"Ooit0VkCwd9G","execution":{"iopub.status.busy":"2021-09-29T01:47:46.535253Z","iopub.execute_input":"2021-09-29T01:47:46.535547Z","iopub.status.idle":"2021-09-29T01:47:46.762854Z","shell.execute_reply.started":"2021-09-29T01:47:46.535511Z","shell.execute_reply":"2021-09-29T01:47:46.762031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels, frequencies = np.unique(train.language.values, return_counts = True)\n\nplt.figure(figsize = (10,10))\nplt.pie(frequencies,labels = labels, autopct = '%1.1f%%')\nplt.show()","metadata":{"id":"ncblGqmKwye-","outputId":"8efc2841-931a-4f25-dc32-363d1502cbc9","execution":{"iopub.status.busy":"2021-09-29T01:47:46.764093Z","iopub.execute_input":"2021-09-29T01:47:46.764329Z","iopub.status.idle":"2021-09-29T01:47:47.050365Z","shell.execute_reply.started":"2021-09-29T01:47:46.764302Z","shell.execute_reply":"2021-09-29T01:47:47.049499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Bert**\n\n\n","metadata":{"id":"LuBujhplh-FY"}},{"cell_type":"code","source":"model_name = 'bert-base-multilingual-cased'\ntokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')","metadata":{"id":"TTdJmrvaw3yO","execution":{"iopub.status.busy":"2021-09-29T01:47:47.051764Z","iopub.execute_input":"2021-09-29T01:47:47.052079Z","iopub.status.idle":"2021-09-29T01:47:49.117717Z","shell.execute_reply.started":"2021-09-29T01:47:47.052034Z","shell.execute_reply":"2021-09-29T01:47:49.116878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def encode_sentence(s):\n#    tokens = list(tokenizer.tokenize(s))\n#    tokens.append('[SEP]')\n   \n#    return tokenizer.convert_tokens_to_ids(tokens)","metadata":{"id":"ppT3kaOwy6w4","execution":{"iopub.status.busy":"2021-09-29T01:47:49.118987Z","iopub.execute_input":"2021-09-29T01:47:49.119944Z","iopub.status.idle":"2021-09-29T01:47:49.124562Z","shell.execute_reply.started":"2021-09-29T01:47:49.119896Z","shell.execute_reply":"2021-09-29T01:47:49.123627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# encode_sentence(\"I love machine learning\")","metadata":{"id":"CCWA2rx-c8Ov","outputId":"fb604919-d66a-4e0b-cb41-5d02fe899c53","execution":{"iopub.status.busy":"2021-09-29T01:47:49.12587Z","iopub.execute_input":"2021-09-29T01:47:49.12613Z","iopub.status.idle":"2021-09-29T01:47:49.144737Z","shell.execute_reply.started":"2021-09-29T01:47:49.126103Z","shell.execute_reply":"2021-09-29T01:47:49.144013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lengths = []\n\ndef encode_sentence(s):\n  tokens = list(tokenizer.tokenize(s))\n  # lengths.append(len(tokens))\n  padding  = ['[PAD]']*(150-len(tokens))\n  tokens = tokens + padding\n  tokens = tokens[:150]\n  lengths.append(len(tokens))\n  return tokenizer.convert_tokens_to_ids(tokens)","metadata":{"id":"HCdCI2jmb1m_","execution":{"iopub.status.busy":"2021-09-29T01:47:49.146863Z","iopub.execute_input":"2021-09-29T01:47:49.14788Z","iopub.status.idle":"2021-09-29T01:47:49.157254Z","shell.execute_reply.started":"2021-09-29T01:47:49.147826Z","shell.execute_reply":"2021-09-29T01:47:49.156621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def bert_encode(hypotheses, premises, tokenizer):\n    \n  num_examples = len(hypotheses)\n  \n  # TRIAL #1\n  # sentence1 = tf.ragged.constant([\n  #     encode_sentence(s)\n  #     for s in np.array(hypotheses)])\n  # sentence2 = tf.ragged.constant([\n  #     encode_sentence(s)\n  #      for s in np.array(premises)])\n\n  # TRIAL #2\n  # sentence = tf.ragged.constant([\n  #   encode_sentence(s1,s2)\n  #   for s1,s2 in zip(hypothesis, premises)\n  # ])\n\n  # TRIAL #3\n  cls = [tokenizer.convert_tokens_to_ids(['[CLS]'])]*len(hypotheses) # wont error\n  \n  sentence1 = tf.ragged.constant([\n      encode_sentence(s)\n      for s in np.array(hypotheses)])\n  \n  sep = [tokenizer.convert_tokens_to_ids(['[SEP]'])]*len(hypotheses) # wont error\n\n  sentence2 = tf.ragged.constant([\n      encode_sentence(s)\n      for s in np.array(premises)])\n  \n  sentence1 = tf.concat([sentence1, sep], axis=-1)\n  sentence2 = tf.concat([sentence2, sep], axis=-1)\n\n  input_word_ids = tf.concat([cls, sentence1, sentence2], axis=-1)\n\n  input_mask = tf.ones_like(input_word_ids).to_tensor()\n\n  type_cls = tf.zeros_like(cls)\n  type_s1 = tf.zeros_like(sentence1)\n  type_s2 = tf.ones_like(sentence2)\n  \n  input_type_ids = tf.concat(\n      [type_cls, type_s1, type_s2], axis=-1).to_tensor()\n\n  inputs = {\n      'input_word_ids': input_word_ids.to_tensor(),\n      'input_mask': input_mask,\n      'input_type_ids': input_type_ids}\n\n  return inputs","metadata":{"id":"SIyHHIjAw8nO","execution":{"iopub.status.busy":"2021-09-29T01:47:49.158699Z","iopub.execute_input":"2021-09-29T01:47:49.159175Z","iopub.status.idle":"2021-09-29T01:47:49.171838Z","shell.execute_reply.started":"2021-09-29T01:47:49.159133Z","shell.execute_reply":"2021-09-29T01:47:49.170885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lengths = []\ntrain_input = bert_encode(train.hypothesis.values, train.premise.values, tokenizer)\ntest_input = bert_encode(test.hypothesis.values, test.premise.values, tokenizer)","metadata":{"id":"Fwb4kmZQxVvI","execution":{"iopub.status.busy":"2021-09-29T01:47:49.173289Z","iopub.execute_input":"2021-09-29T01:47:49.173548Z","iopub.status.idle":"2021-09-29T01:48:33.28317Z","shell.execute_reply.started":"2021-09-29T01:47:49.173517Z","shell.execute_reply":"2021-09-29T01:48:33.282286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(lengths)","metadata":{"id":"NM4d7UVto0Nx","outputId":"71a428e7-58d1-458b-a6e4-3f2a8d0fbe5d","execution":{"iopub.status.busy":"2021-09-29T01:48:33.284285Z","iopub.execute_input":"2021-09-29T01:48:33.285321Z","iopub.status.idle":"2021-09-29T01:48:33.530868Z","shell.execute_reply.started":"2021-09-29T01:48:33.285284Z","shell.execute_reply":"2021-09-29T01:48:33.529979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_len = 303\n\ndef build_model():\n    bert_encoder = TFBertModel.from_pretrained(model_name)\n    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    input_type_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_type_ids\")\n    \n    embedding = bert_encoder([input_word_ids, input_mask, input_type_ids])[0]\n    # Adding more FC layers\n    FC1 = tf.keras.layers.Dense(2048)(embedding[:,0,:])\n    FC2 = tf.keras.layers.Dense(512)(FC1)\n    output = tf.keras.layers.Dense(3, activation='softmax')(FC2)\n    \n    model = tf.keras.Model(inputs=[input_word_ids, input_mask, input_type_ids], outputs=output)\n    model.compile(tf.keras.optimizers.Adam(lr=1e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n    # cross categorical vs sparse cat\n    return model","metadata":{"id":"IL1u8ompxf9g","execution":{"iopub.status.busy":"2021-09-29T01:48:33.531942Z","iopub.execute_input":"2021-09-29T01:48:33.532169Z","iopub.status.idle":"2021-09-29T01:48:33.541885Z","shell.execute_reply.started":"2021-09-29T01:48:33.532143Z","shell.execute_reply":"2021-09-29T01:48:33.541079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with strategy.scope():\n    model = build_model()\n    model.summary()","metadata":{"id":"Ff9ibETTxifn","outputId":"7870a2cd-eb93-42c4-c5d4-d81625807537","execution":{"iopub.status.busy":"2021-09-29T01:48:33.543338Z","iopub.execute_input":"2021-09-29T01:48:33.543951Z","iopub.status.idle":"2021-09-29T01:49:31.702352Z","shell.execute_reply.started":"2021-09-29T01:48:33.543898Z","shell.execute_reply":"2021-09-29T01:49:31.701731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.label.values.size","metadata":{"id":"0MWtBvrdznWe","outputId":"ddce2680-ca38-477f-d342-f7bde69e66db","execution":{"iopub.status.busy":"2021-09-29T01:49:31.703388Z","iopub.execute_input":"2021-09-29T01:49:31.704187Z","iopub.status.idle":"2021-09-29T01:49:31.709695Z","shell.execute_reply.started":"2021-09-29T01:49:31.704157Z","shell.execute_reply":"2021-09-29T01:49:31.708797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(train_input, train.label.values, epochs = 5, verbose = 1, batch_size = 64, validation_split = 0.2)","metadata":{"id":"ye66oWltx5pN","outputId":"91033e74-0932-41a7-a0b6-fdc47bf5c575","execution":{"iopub.status.busy":"2021-09-29T01:49:31.711019Z","iopub.execute_input":"2021-09-29T01:49:31.711313Z","iopub.status.idle":"2021-09-29T01:53:25.485355Z","shell.execute_reply.started":"2021-09-29T01:49:31.711283Z","shell.execute_reply":"2021-09-29T01:53:25.484533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = [np.argmax(i) for i in model.predict(test_input)]\nsubmission = test.id.copy().to_frame()\nsubmission['prediction'] = predictions","metadata":{"id":"l7NCfqmi3EAZ","outputId":"f20feb71-90d1-4a29-dbf8-a784def1add1","execution":{"iopub.status.busy":"2021-09-29T01:53:25.487061Z","iopub.execute_input":"2021-09-29T01:53:25.487367Z","iopub.status.idle":"2021-09-29T01:53:42.247613Z","shell.execute_reply.started":"2021-09-29T01:53:25.487328Z","shell.execute_reply":"2021-09-29T01:53:42.246825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv(\"submission.csv\", index = False)","metadata":{"execution":{"iopub.status.busy":"2021-09-29T01:53:42.248868Z","iopub.execute_input":"2021-09-29T01:53:42.249108Z","iopub.status.idle":"2021-09-29T01:53:42.269634Z","shell.execute_reply.started":"2021-09-29T01:53:42.249081Z","shell.execute_reply":"2021-09-29T01:53:42.268639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install transformers","metadata":{"execution":{"iopub.status.busy":"2021-09-29T01:53:42.271014Z","iopub.execute_input":"2021-09-29T01:53:42.271287Z","iopub.status.idle":"2021-09-29T01:53:50.702296Z","shell.execute_reply.started":"2021-09-29T01:53:42.271257Z","shell.execute_reply":"2021-09-29T01:53:50.70131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#pip install -U deep_translator\n#from deep_translator import GoogleTranslator\n#translator = Translator()\n#def translate_sentence(x):\n#  return GoogleTranslator('auto', 'en').translate(x)\n#train.premise[train.lang_abv!= 'en']=train.premise[train.lang_abv!= 'en'].apply(lambda x: translate_sentence(x))\n#train.hypothesis[train.lang_abv!= 'en']=train.hypothesis[train.lang_abv!= 'en'].apply(lambda x: translate_sentence(x))\n#train.to_csv(\"/content/drive/MyDrive/Contradictory, My Dear Watson/train_translated.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-09-29T01:53:50.703757Z","iopub.execute_input":"2021-09-29T01:53:50.703997Z","iopub.status.idle":"2021-09-29T01:53:50.708943Z","shell.execute_reply.started":"2021-09-29T01:53:50.703968Z","shell.execute_reply":"2021-09-29T01:53:50.708093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_translated = pd.read_csv(\"../input/train-translated/train_translated.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-09-29T05:32:38.566793Z","iopub.execute_input":"2021-09-29T05:32:38.567293Z","iopub.status.idle":"2021-09-29T05:32:38.64106Z","shell.execute_reply.started":"2021-09-29T05:32:38.567259Z","shell.execute_reply":"2021-09-29T05:32:38.640241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.regularizers import l2","metadata":{"execution":{"iopub.status.busy":"2021-09-29T02:05:30.842405Z","iopub.execute_input":"2021-09-29T02:05:30.842728Z","iopub.status.idle":"2021-09-29T02:05:30.848605Z","shell.execute_reply.started":"2021-09-29T02:05:30.842695Z","shell.execute_reply":"2021-09-29T02:05:30.847737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def extract_tokens_from_binary_parse(parse):\n    return parse.replace('(', ' ').replace(')', ' ').replace('-LRB-', '(').replace('-RRB-', ')').split()\ndef yield_examples(fn, skip_no_majority=True, limit=None):\n  for i, line in enumerate(open(fn)):\n    if limit and i > limit:\n      break\n    data = json.loads(line)\n    label = data['gold_label']\n    s1 = ' '.join(extract_tokens_from_binary_parse(data['sentence1_binary_parse']))\n    s2 = ' '.join(extract_tokens_from_binary_parse(data['sentence2_binary_parse']))\n    if skip_no_majority and label == '-':\n      continue\n    yield (label, s1, s2)\n\ndef get_data(fn, limit=None):\n  raw_data = list(yield_examples(fn=fn, limit=limit))\n  left = [s1 for _, s1, s2 in raw_data]\n  right = [s2 for _, s1, s2 in raw_data]\n  print(max(len(x.split()) for x in left))\n  print(max(len(x.split()) for x in right))\n  LABELS = {'contradiction': 0, 'neutral': 1, 'entailment': 2}\n  Y = np.array([LABELS[l] for l, s1, s2 in raw_data])\n  Y = np_utils.to_categorical(Y, len(LABELS))\n\n  return left, right, Y","metadata":{"execution":{"iopub.status.busy":"2021-09-29T02:05:43.846602Z","iopub.execute_input":"2021-09-29T02:05:43.846895Z","iopub.status.idle":"2021-09-29T02:05:43.858714Z","shell.execute_reply.started":"2021-09-29T02:05:43.846866Z","shell.execute_reply":"2021-09-29T02:05:43.857555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_translated.premise[0])","metadata":{"execution":{"iopub.status.busy":"2021-09-29T02:08:41.163002Z","iopub.execute_input":"2021-09-29T02:08:41.163306Z","iopub.status.idle":"2021-09-29T02:08:41.168573Z","shell.execute_reply.started":"2021-09-29T02:08:41.163275Z","shell.execute_reply":"2021-09-29T02:08:41.167606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from gensim.models import word2vec\ncorpus = [\n          'Text of the first document.',\n          'Text of the second document made longer.',\n          'Number three.',\n          'This is number four.',\n]\n# we need to pass splitted sentences to the model\ntokenized_sentences = [sentence.split() for sentence in corpus]\nmodel1 = word2vec.Word2Vec(tokenized_sentences, min_count=1)","metadata":{"execution":{"iopub.status.busy":"2021-09-29T04:24:28.680318Z","iopub.execute_input":"2021-09-29T04:24:28.681Z","iopub.status.idle":"2021-09-29T04:24:28.700299Z","shell.execute_reply.started":"2021-09-29T04:24:28.680957Z","shell.execute_reply":"2021-09-29T04:24:28.699304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2021-09-29T04:42:44.690424Z","iopub.execute_input":"2021-09-29T04:42:44.691112Z","iopub.status.idle":"2021-09-29T04:42:45.213701Z","shell.execute_reply.started":"2021-09-29T04:42:44.691067Z","shell.execute_reply":"2021-09-29T04:42:45.212735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training = train_translated\nVOCAB = len(tokenizer.word_counts) + 1\nLABELS = {'contradiction': 0, 'neutral': 1, 'entailment': 2}\ntraining_premise_seq_vec = []\ntraining_hypothesis_seq_vec = []\ntraining = train_translated\ntokenizer = Tokenizer(lower=False, filters='')\ntokenizer.fit_on_texts(training.premise)\nfor words in training.premise[:20]:\n    training_premise_seq = tokenizer.texts_to_sequences(words)\n    training_premise_seq_vec.append(training_premise_seq)\nprint(len(training_premise_seq_vec))\n\ntokenizer2 = Tokenizer(lower=False, filters='')\ntokenizer2.fit_on_texts(training.hypothesis)\nfor words in training.hypothesis[:20]:\n    training_hypothesis_seq = tokenizer2.texts_to_sequences(words)\n    training_hypothesis_seq_vec.append(training_hypothesis_seq)\nprint(len(training_hypothesis_seq_vec))\n\ntraining_label = str(training.label)\nprint(training_label)\n\nlabels1 = []\ntokenizer3 = Tokenizer(lower=False, filters='')\ntokenizer3.fit_on_texts(training_label)\nfor mylabels in training_label[:20]:\n    mylabels1 = tokenizer3.texts_to_sequences(mylabels)\n    labels1.append(mylabels1)\nprint(labels1)","metadata":{"execution":{"iopub.status.busy":"2021-09-29T05:38:18.388802Z","iopub.execute_input":"2021-09-29T05:38:18.389081Z","iopub.status.idle":"2021-09-29T05:38:18.915663Z","shell.execute_reply.started":"2021-09-29T05:38:18.389052Z","shell.execute_reply":"2021-09-29T05:38:18.914571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.layers import BatchNormalization\nmax_tokens = 1000\nMAX_LEN = 1000\nembed = Embedding(VOCAB, EMBED_HIDDEN_SIZE, input_length=MAX_LEN)\n\nprem1 = training_premise_seq_vec\nhypo1 = training_hypothesis_seq_vec\n\npremise = Input(shape=(MAX_LEN,),dtype='int32')\nhypothesis = Input(shape=(MAX_LEN,),dtype='int32')\nprem = embed(premise)\nhypo = embed(hypothesis)\n\njoint = keras.layers.concatenate([prem, hypo],dtype='float32')\njoint = Dropout(DP, dtype='float32')(joint)\nfor i in range(3):\n  joint = Dense(2 * SENT_HIDDEN_SIZE, activation=ACTIVATION)(joint)\n  joint = Dropout(DP)(joint)\n  joint = BatchNormalization()(joint)\npred = Dense(3, activation='softmax')(joint)\n\nmodel = keras.Model([premise, hypothesis], pred)\n\n[print(i.shape) for i in model.inputs]\n[print(l.name, l.input_shape, l.dtype) for l in model.layers]\n\nmodel.compile(\n  optimizer='adam',\n  loss='binary_crossentropy',\n  metrics=['accuracy'],\n)\nmodel.summary()\nmodel.fit([prem1, hypo1], np.array(labels1))","metadata":{"execution":{"iopub.status.busy":"2021-09-29T05:38:54.768667Z","iopub.execute_input":"2021-09-29T05:38:54.768988Z","iopub.status.idle":"2021-09-29T05:38:55.083022Z","shell.execute_reply.started":"2021-09-29T05:38:54.768956Z","shell.execute_reply":"2021-09-29T05:38:55.081432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.layers import Dense\nmax_tokens = 1000\n\nprem = training_premise_seq_vec\nhypo = training_hypothesis_seq_vec\n\n\nmodel = keras.Sequential()\nmodel.add(Embedding(max_tokens + 1, 128))\nmodel.add(LSTM(64))\nmodel.add(Dense(32, activation=\"relu\"))\nmodel.add(Dense(1, activation=\"sigmoid\"))\nmodel.compile(\n  optimizer='adam',\n  loss='binary_crossentropy',\n  metrics=['accuracy'],\n)\nmodel.fit([prem, hypo], epochs=2,validation_split=0.2, batch_size = 512)\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Lowest index from the tokenizer is 1 - we need to include 0 in our vocab count\n\nprint('Build model...')\nprint('Vocab size =', VOCAB)\nRNN = None\nLAYERS = 1\nUSE_GLOVE = True\nTRAIN_EMBED = False\nEMBED_HIDDEN_SIZE = 300\nSENT_HIDDEN_SIZE = 300\nBATCH_SIZE = 512\nPATIENCE = 4 # 8\nMAX_EPOCHS = 42\nMAX_LEN = 42\nDP = 0.2\nL2 = 4e-6\nACTIVATION = 'relu'\n\npremise = Input(shape=(MAX_LEN,), dtype='int32')\nhypothesis = Input(shape=(MAX_LEN,), dtype='int32')\n\nif RNN and LAYERS > 1:\n  for l in range(LAYERS - 1):\n    rnn = RNN(return_sequences=True, **rnn_kwargs)\n    prem = BatchNormalization()(rnn(prem))\n    hypo = BatchNormalization()(rnn(hypo))\nrnn = SumEmbeddings if not RNN else RNN(return_sequences=False, **rnn_kwargs)\nprem = rnn(prem)\nhypo = rnn(hypo)\nprem = BatchNormalization()(prem)\nhypo = BatchNormalization()(hypo)\n\njoint = merge([prem, hypo], mode='concat')\njoint = Dropout(DP)(joint)\nfor i in range(3):\n  joint = Dense(2 * SENT_HIDDEN_SIZE, activation=ACTIVATION, W_regularizer=l2(L2) if L2 else None)(joint)\n  joint = Dropout(DP)(joint)\n  joint = BatchNormalization()(joint)\n\npred = Dense(len(LABELS), activation='softmax')(joint)\n\nmodel = word2vec.Word2Vec(input=[premise, hypothesis], output=pred)\nmodel.compile(optimizer=OPTIMIZER, loss='categorical_crossentropy', metrics=['accuracy'])\n\nmodel.summary()\n\nprint('Training')\n_, tmpfn = tempfile.mkstemp()\n# Save the best model during validation and bail out of training early if we're not improving\ncallbacks = [EarlyStopping(patience=PATIENCE), ModelCheckpoint(tmpfn, save_best_only=True, save_weights_only=True)]\nmodel.fit([training[0], training[1]], training[2], batch_size=BATCH_SIZE, nb_epoch=MAX_EPOCHS, validation_data=([validation[0], validation[1]], validation[2]), callbacks=callbacks)\n\n# Restore the best found model during validation\nmodel.load_weights(tmpfn)\n\nloss, acc = model.evaluate([test[0], test[1]], test[2], batch_size=BATCH_SIZE)\nprint('Test loss / test accuracy = {:.4f} / {:.4f}'.format(loss, acc))","metadata":{"execution":{"iopub.status.busy":"2021-09-29T02:34:47.478366Z","iopub.status.idle":"2021-09-29T02:34:47.478757Z","shell.execute_reply.started":"2021-09-29T02:34:47.478551Z","shell.execute_reply":"2021-09-29T02:34:47.478575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}