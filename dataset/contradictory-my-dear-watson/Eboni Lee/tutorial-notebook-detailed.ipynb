{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### **Hey everyone! I just want to make sure I give credit where it is due and explain the purpose of this notebook. I'm taking the Tutorial Notebook done by [Ana Sofia Uzsoy](https://www.kaggle.com/anasofiauzsoy), [Amy Jang](https://www.kaggle.com/amyjang), & [Phil Culliton](https://www.kaggle.com/philculliton) and just adding more explanations and background information about why the code is set up the way it is. Originally, I was just meaning to try and understand NLI and this notebook, but thought that it could be helpful to others if I broke things down even more than the original notebook. I've also added a few additions since some things didn't work for me when using the original workbook and have changed some of the model parameters to see if it will give me better accuracy.**\n\n#### **Any words that I've added will be bolded or added as comments in code blocks starting with \"First, let's import the libraries we'll need\"**\n\n##### *The original notebook can be found [here](https://www.kaggle.com/anasofiauzsoy/tutorial-notebook)*","metadata":{}},{"cell_type":"markdown","source":"Natural Language Inferencing (NLI) is a classic NLP (Natural Language Processing) problem that involves taking two sentences (the _premise_ and the _hypothesis_ ), and deciding how they are related- if the premise entails the hypothesis, contradicts it, or neither.\n\nIn this tutorial we'll look at the _Contradictory, My Dear Watson_ competition dataset, build a preliminary model using Tensorflow 2, Keras, and BERT, and prepare a submission file.","metadata":{"_uuid":"7edff2eb-50cb-4285-8255-b3262dbd5161","_cell_guid":"5c1b75ee-94f3-40c8-adbe-4ca8325b9f9c","trusted":true}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will \n# list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output\n# when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current\n# session","metadata":{"_uuid":"d3a20929-8e1d-48d2-869c-cc57f8c63cc9","_cell_guid":"20666a1f-e31b-4134-94f8-fea9a50998d3","execution":{"iopub.status.busy":"2021-06-10T22:18:28.837452Z","iopub.execute_input":"2021-06-10T22:18:28.83788Z","iopub.status.idle":"2021-06-10T22:18:28.855952Z","shell.execute_reply.started":"2021-06-10T22:18:28.837797Z","shell.execute_reply":"2021-06-10T22:18:28.854898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.environ[\"WANDB_API_KEY\"] = \"0\" ## to silence warning","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:18:28.85765Z","iopub.execute_input":"2021-06-10T22:18:28.85824Z","iopub.status.idle":"2021-06-10T22:18:28.863739Z","shell.execute_reply.started":"2021-06-10T22:18:28.858192Z","shell.execute_reply":"2021-06-10T22:18:28.862708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **First, let's import the libraries we'll need**","metadata":{}},{"cell_type":"code","source":"# importing libraries\n\nfrom transformers import BertTokenizer, TFBertModel\nimport matplotlib.pyplot as plt\nimport tensorflow as tf","metadata":{"_uuid":"e3dd507b-c502-488c-9dd0-419c5d73c159","_cell_guid":"863de620-d4b7-4711-b587-e1c75be9e36f","execution":{"iopub.status.busy":"2021-06-10T22:18:28.865741Z","iopub.execute_input":"2021-06-10T22:18:28.866086Z","iopub.status.idle":"2021-06-10T22:18:35.198245Z","shell.execute_reply.started":"2021-06-10T22:18:28.866039Z","shell.execute_reply":"2021-06-10T22:18:35.197199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's set up our TPU.","metadata":{"_uuid":"58fb9ec5-c099-494c-bf59-6ec9d6e64628","_cell_guid":"6c5122f9-8c39-4892-81a9-1f8830b64484","trusted":true}},{"cell_type":"markdown","source":"**The code below was taken from [Watson :: XLM-R & NLI :: inference](https://www.kaggle.com/alturutin/watson-xlm-r-nli-inference). This was the notebook I was originally looking to add additional explanations to and break down further, but it went a little over my head in most places so I decided to start with a simpler notebook instead.**\n\n**If you go to Kaggle's documentation on [TPUs](https://www.kaggle.com/docs/tpu) you'll see that some of the code in the function is taken directly from there. TPUs, Tensor Processing Units, were specifically created to work with TensorFlow, a ML library. The original poster [novichok](https://www.kaggle.com/alturutin) just turned it into a function that could tell whether the notebook is running a harware accelarator, such as a TPU, GPU, or a CPU (which is just normal). I thought it would be a nice addition. Want more info on the differences between the three? Check out this [link](https://serverguy.com/comparison/cpu-vs-gpu-vs-tpu/)**\n\n**Last Note: I've tried my best to add pertinent info regarding each function and its purpose. I was taught that it's good practice to do so and so you'll see it in red right between the start of the function and the first line of code.**","metadata":{}},{"cell_type":"code","source":"def init_strategy():\n    '''function that determines whethere a TPU is running or a CPU/GPU'''\n    try:\n        # detect the TPU\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        \n        # initiate the TPU\n        tf.config.experimental_connect_to_cluster(tpu)\n        tf.tpu.experimental.initialize_tpu_system(tpu)\n        \n        # instantiate a distribution strategy\n        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n        print(\"Init TPU strategy\")\n    except ValueError:\n        strategy = tf.distribute.get_strategy() # for CPU and single GPU\n        print(\"Init CPU/GPU strategy\")\n    return strategy\n\nstrategy = init_strategy()\nstrategy","metadata":{"_uuid":"386d0823-63ab-4765-9561-32c5f382e71d","_cell_guid":"ca2729e3-9275-4a9c-b150-592320bd3e54","execution":{"iopub.status.busy":"2021-06-10T22:18:35.199704Z","iopub.execute_input":"2021-06-10T22:18:35.199975Z","iopub.status.idle":"2021-06-10T22:18:40.84154Z","shell.execute_reply.started":"2021-06-10T22:18:35.199948Z","shell.execute_reply":"2021-06-10T22:18:40.840455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Downloading Data","metadata":{"_uuid":"0b64130f-530a-4560-8afd-462a55fc14b3","_cell_guid":"44a1d22f-053c-4188-b25f-d714aa745016","trusted":true}},{"cell_type":"markdown","source":"The training set contains a premise, a hypothesis, a label (0 = entailment, 1 = neutral, 2 = contradiction), and the language of the text. For more information about what these mean and how the data is structured, check out the data page: https://www.kaggle.com/c/contradictory-my-dear-watson/data","metadata":{"_uuid":"b9285071-38f2-421b-9b1c-9c44d41c7365","_cell_guid":"6fb2939c-b14a-450f-85dd-7220439eaf55","trusted":true}},{"cell_type":"code","source":"train = pd.read_csv(\"../input/contradictory-my-dear-watson/train.csv\")","metadata":{"_uuid":"6e60d19f-aeae-417a-a10a-50cc1d5ee685","_cell_guid":"f3ad567b-a156-4ffc-a6f8-1f5e6e989a4e","execution":{"iopub.status.busy":"2021-06-10T22:18:40.842822Z","iopub.execute_input":"2021-06-10T22:18:40.843133Z","iopub.status.idle":"2021-06-10T22:18:40.978002Z","shell.execute_reply.started":"2021-06-10T22:18:40.843102Z","shell.execute_reply":"2021-06-10T22:18:40.976971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can use the pandas head() function to take a quick look at the training set.\n\n**A very good practice when working with a new data set**","metadata":{}},{"cell_type":"code","source":"train.head()","metadata":{"_uuid":"82e6d183-b8e1-412b-816f-c9020bac1428","_cell_guid":"d3b9a632-7abb-4bef-acd3-9cba577dc2c0","execution":{"iopub.status.busy":"2021-06-10T22:18:40.979378Z","iopub.execute_input":"2021-06-10T22:18:40.979676Z","iopub.status.idle":"2021-06-10T22:18:41.007288Z","shell.execute_reply.started":"2021-06-10T22:18:40.979648Z","shell.execute_reply":"2021-06-10T22:18:41.006284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's look at one of the pairs of sentences.","metadata":{"_uuid":"abfb11e9-865c-4b50-b0cc-9d25851618ff","_cell_guid":"817c2470-9778-46e8-be86-fdd1b9449b93","trusted":true}},{"cell_type":"code","source":"train.premise.values[1]","metadata":{"_uuid":"dea716f0-9698-4a10-a7f1-1a0bb36054db","_cell_guid":"bcca3548-bb15-4868-86ec-95fe084d6e06","execution":{"iopub.status.busy":"2021-06-10T22:18:41.008453Z","iopub.execute_input":"2021-06-10T22:18:41.008754Z","iopub.status.idle":"2021-06-10T22:18:41.016617Z","shell.execute_reply.started":"2021-06-10T22:18:41.008726Z","shell.execute_reply":"2021-06-10T22:18:41.01549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.hypothesis.values[1]","metadata":{"_uuid":"5b639eff-aeea-4dca-b516-7e729cdeb741","_cell_guid":"15dcbe6c-4914-4400-a8fd-65b6e4cbf652","execution":{"iopub.status.busy":"2021-06-10T22:18:41.018222Z","iopub.execute_input":"2021-06-10T22:18:41.018649Z","iopub.status.idle":"2021-06-10T22:18:41.030451Z","shell.execute_reply.started":"2021-06-10T22:18:41.018592Z","shell.execute_reply":"2021-06-10T22:18:41.029503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.label.values[1]","metadata":{"_uuid":"c63a91e0-05f7-4a67-acf8-2131ef50f054","_cell_guid":"e16e8879-c565-4a29-bacc-26648659da29","execution":{"iopub.status.busy":"2021-06-10T22:18:41.034009Z","iopub.execute_input":"2021-06-10T22:18:41.034467Z","iopub.status.idle":"2021-06-10T22:18:41.044481Z","shell.execute_reply.started":"2021-06-10T22:18:41.034432Z","shell.execute_reply":"2021-06-10T22:18:41.043655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"These statements are contradictory, and the label shows that.\n\nLet's look at the distribution of languages in the training set.","metadata":{"_uuid":"216dc9c2-1699-4b89-af27-443d1b4c7289","_cell_guid":"8c9122ea-797d-48f1-b4ba-85bc2e0e6f18","trusted":true}},{"cell_type":"code","source":"labels, frequencies = np.unique(train.language.values, return_counts = True)\n\nplt.figure(figsize = (10,10))\nplt.pie(frequencies,labels = labels, autopct = '%1.1f%%')\nplt.show()","metadata":{"_uuid":"7243f511-d81e-436c-971f-6328b7c0cf43","_cell_guid":"7d9b43ef-4ac1-40d5-aebc-a162f1b9a6c0","execution":{"iopub.status.busy":"2021-06-10T22:18:41.045719Z","iopub.execute_input":"2021-06-10T22:18:41.045979Z","iopub.status.idle":"2021-06-10T22:18:41.342116Z","shell.execute_reply.started":"2021-06-10T22:18:41.045953Z","shell.execute_reply":"2021-06-10T22:18:41.341264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observation: most of the statements are in English with all other languages appearing at almost the same rate (about 3%)**","metadata":{}},{"cell_type":"markdown","source":"## Preparing Data for Input","metadata":{"_uuid":"68cee874-838e-4f0e-9f80-bfebc4a295d0","_cell_guid":"a7f5d429-083e-4d81-883f-e032dfb0e236","trusted":true}},{"cell_type":"markdown","source":"To start out, we can use a pretrained model. Here, we'll use a multilingual BERT model from huggingface. For more information about BERT, see: https://github.com/google-research/bert/blob/master/multilingual.md\n\nFirst, we download the tokenizer.","metadata":{"_uuid":"27e66828-76b9-44f4-ba79-66fe0cb8f922","_cell_guid":"f20a33db-a377-43b6-825e-157456ed1092","trusted":true}},{"cell_type":"code","source":"model_name = 'bert-base-multilingual-cased'\ntokenizer = BertTokenizer.from_pretrained(model_name)","metadata":{"_uuid":"0af7b876-e719-474a-b6a5-fef500624b83","_cell_guid":"ddc3e65e-24a4-43dc-98f0-194655d17cfd","execution":{"iopub.status.busy":"2021-06-10T22:18:41.343149Z","iopub.execute_input":"2021-06-10T22:18:41.343433Z","iopub.status.idle":"2021-06-10T22:18:43.650116Z","shell.execute_reply.started":"2021-06-10T22:18:41.343405Z","shell.execute_reply":"2021-06-10T22:18:43.649034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Tokenizers turn sequences of words into arrays of numbers. Let's look at an example:","metadata":{"_uuid":"a6088f3d-8778-4a6c-82b0-db6a5461ec95","_cell_guid":"f6f4ab91-c9b5-4861-a400-89969200b7f4","trusted":true}},{"cell_type":"code","source":"def encode_sentence(s):\n    '''Function to tokenize words'''\n    tokens = list(tokenizer.tokenize(s))\n    tokens.append('[SEP]')\n    return tokenizer.convert_tokens_to_ids(tokens)","metadata":{"_uuid":"2cd597c4-6204-44ba-869a-fbf7c32c711d","_cell_guid":"55838dc3-c459-4097-b5b2-44d049429e25","execution":{"iopub.status.busy":"2021-06-10T22:18:43.651485Z","iopub.execute_input":"2021-06-10T22:18:43.651804Z","iopub.status.idle":"2021-06-10T22:18:43.657463Z","shell.execute_reply.started":"2021-06-10T22:18:43.651774Z","shell.execute_reply":"2021-06-10T22:18:43.656228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **OK, now let's break the function down into smaller parts.**\n\n**The first line of code in the function turns the statement you enter into a list of words. The tokenizer takes care of splitting the sequence into tokens available in the tokenizer vocabulary, in this case the multilingual BERT. The tokens can be either words or subwords.**\n\n**Let's look at the example below using the phrase _\"I love machine learning\"_**","metadata":{}},{"cell_type":"code","source":"tokens = list(tokenizer.tokenize(\"I love machine learning\"))\ntokens","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:18:43.659169Z","iopub.execute_input":"2021-06-10T22:18:43.659505Z","iopub.status.idle":"2021-06-10T22:18:43.678441Z","shell.execute_reply.started":"2021-06-10T22:18:43.659472Z","shell.execute_reply":"2021-06-10T22:18:43.677287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Oh look, our sentence is now broken up into a list of 4 words**","metadata":{}},{"cell_type":"markdown","source":"**The second line of code below is explained by the original authors below, but if you want more detailed information you can check out this [link](https://github.com/google-research/bert/blob/master/run_classifier.py) around line 400.**\n","metadata":{}},{"cell_type":"code","source":"tokens.append('[SEP]')\ntokens","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:18:43.679738Z","iopub.execute_input":"2021-06-10T22:18:43.680055Z","iopub.status.idle":"2021-06-10T22:18:43.700124Z","shell.execute_reply.started":"2021-06-10T22:18:43.680023Z","shell.execute_reply":"2021-06-10T22:18:43.69885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Hmmm, so we just added [SEP] to denote that it's the end of the statement**","metadata":{}},{"cell_type":"markdown","source":"**The last line of code uses the multilingual BERT model to turn each token into an ID, which are then understandable by the model.**","metadata":{}},{"cell_type":"code","source":"tokenizer.convert_tokens_to_ids(tokens)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:18:43.701773Z","iopub.execute_input":"2021-06-10T22:18:43.70215Z","iopub.status.idle":"2021-06-10T22:18:43.720344Z","shell.execute_reply.started":"2021-06-10T22:18:43.702107Z","shell.execute_reply":"2021-06-10T22:18:43.719426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Would you look at that, all of the words have been turned into ID numbers. I also want to note that [SEP] is encoded as ID number 102. So if you see that number I assume it will always represent the end of a sentence or statement.**","metadata":{}},{"cell_type":"code","source":"# now, let's run the original function and see what we get\nencode_sentence(\"I love machine learning\")","metadata":{"_uuid":"797de062-6743-4455-9ead-73527bf6c3b6","_cell_guid":"f05a9baa-b7fa-44da-89dd-04fdfc149cba","execution":{"iopub.status.busy":"2021-06-10T22:18:43.721777Z","iopub.execute_input":"2021-06-10T22:18:43.722379Z","iopub.status.idle":"2021-06-10T22:18:43.736604Z","shell.execute_reply.started":"2021-06-10T22:18:43.722336Z","shell.execute_reply":"2021-06-10T22:18:43.735616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"BERT uses three kind of input data- input word IDs, input masks, and input type IDs.\n\nThese allow the model to know that the premise and hypothesis are distinct sentences, and also to ignore any padding from the tokenizer.\n\nWe add a [CLS] token to denote the beginning of the inputs, and a [SEP] token to denote the separation between the premise and the hypothesis. We also need to pad all of the inputs to be the same size. For more information about BERT inputs, see: https://huggingface.co/transformers/model_doc/bert.html#tfbertmodel\n\nNow, we're going to encode all of our premise/hypothesis pairs for input into BERT.","metadata":{"_uuid":"1fb6acf3-3c19-494e-83bd-84d6d933e6f0","_cell_guid":"e80e3fcd-513e-4d14-88fd-9417b746c107","trusted":true}},{"cell_type":"code","source":"def bert_encode(hypotheses, premises, tokenizer):\n    '''Function that formats the hypothesis and premise data so it can be input into the model'''\n    \n    num_examples = len(hypotheses)\n    \n    sentence1 = tf.ragged.constant([\n        encode_sentence(s)\n        for s in np.array(hypotheses)])\n    \n    sentence2 = tf.ragged.constant([\n        encode_sentence(s)\n        for s in np.array(premises)])\n    \n    cls = [tokenizer.convert_tokens_to_ids(['[CLS]'])]*sentence1.shape[0]\n    input_word_ids = tf.concat([cls, sentence1, sentence2], axis=-1)\n    \n    input_mask = tf.ones_like(input_word_ids).to_tensor()\n    \n    type_cls = tf.zeros_like(cls)\n    type_s1 = tf.zeros_like(sentence1)\n    type_s2 = tf.ones_like(sentence2)\n    \n    input_type_ids = tf.concat(\n        [type_cls, type_s1, type_s2], axis=-1).to_tensor()\n    \n    inputs = {\n        'input_word_ids': input_word_ids.to_tensor(),\n        'input_mask': input_mask,\n        'input_type_ids': input_type_ids}\n    return inputs","metadata":{"_uuid":"dedb18d0-63cb-492b-8fe0-b4ebb8819e4c","_cell_guid":"037b0a29-3e6d-42b6-b13a-328fab19d15d","execution":{"iopub.status.busy":"2021-06-10T22:18:43.73842Z","iopub.execute_input":"2021-06-10T22:18:43.739128Z","iopub.status.idle":"2021-06-10T22:18:43.752787Z","shell.execute_reply.started":"2021-06-10T22:18:43.739077Z","shell.execute_reply":"2021-06-10T22:18:43.751683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **You know the drill, let's break this function down!**\n\n**First, I'm going to assign the hypothesis and premise values from the training data to variables. It'll just make it easier when running the code in each step.**","metadata":{}},{"cell_type":"code","source":"hypothesis = train.hypothesis.values\npremises = train.premise.values","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:18:43.754467Z","iopub.execute_input":"2021-06-10T22:18:43.755024Z","iopub.status.idle":"2021-06-10T22:18:43.76708Z","shell.execute_reply.started":"2021-06-10T22:18:43.75497Z","shell.execute_reply":"2021-06-10T22:18:43.765898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Line 1: determines how many rows there are. It should be the same number whether you're looking at hypothesis or premise since they are present at a ratio of 1:1**","metadata":{}},{"cell_type":"code","source":"num_examples = len(premises)\nnum_examples","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:18:43.768466Z","iopub.execute_input":"2021-06-10T22:18:43.769131Z","iopub.status.idle":"2021-06-10T22:18:43.782646Z","shell.execute_reply.started":"2021-06-10T22:18:43.769076Z","shell.execute_reply":"2021-06-10T22:18:43.781846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### sentence1 and sentence2 is essentially all of the labeled statements tokenized with a SEP at the end of each list represented by the id number 102; each tokenized sentence is then put into an array\n\n#### looking at TF documentation: https://www.tensorflow.org/guide/ragged_tensor\n\n> Your data comes in many shapes; your tensors should too. Ragged tensors are the TensorFlow equivalent of nested variable-length lists. They make it easy to store and process data with non-uniform shapes, including: Variable-length features, such as the set of actors in a movie. Batches of variable-length sequential inputs, such as sentences or video clips. Hierarchical inputs, such as text documents that are subdivided into sections, paragraphs, sentences, and words. Individual fields in structured inputs,such as protocol buffers.\n\n#### The simplest way to construct a ragged tensor is using tf.ragged.constant, which builds the RaggedTensor corresponding to a given nested Python list or numpy array\n\n#### As with normal Tensors, the values in a RaggedTensor must all have the same type; and the values must all be at the same nesting depth (the rank of the tensor)","metadata":{}},{"cell_type":"code","source":"sentence1 = tf.ragged.constant([\n        encode_sentence(s)\n        for s in np.array(hypothesis)])\n\nsentence1[0]","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:18:43.783767Z","iopub.execute_input":"2021-06-10T22:18:43.784219Z","iopub.status.idle":"2021-06-10T22:18:48.69777Z","shell.execute_reply.started":"2021-06-10T22:18:43.784173Z","shell.execute_reply":"2021-06-10T22:18:48.69704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentence2 = tf.ragged.constant([\n      encode_sentence(s)\n       for s in np.array(premises)])\nsentence2[0]","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:18:48.698819Z","iopub.execute_input":"2021-06-10T22:18:48.699285Z","iopub.status.idle":"2021-06-10T22:18:57.752304Z","shell.execute_reply.started":"2021-06-10T22:18:48.699247Z","shell.execute_reply":"2021-06-10T22:18:57.751181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Great, looks like all of the hypotheses and premises are encoded/tokenized**","metadata":{}},{"cell_type":"code","source":"sentence1.shape[0]","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:18:57.753523Z","iopub.execute_input":"2021-06-10T22:18:57.753817Z","iopub.status.idle":"2021-06-10T22:18:57.759531Z","shell.execute_reply.started":"2021-06-10T22:18:57.753788Z","shell.execute_reply":"2021-06-10T22:18:57.758502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentence2.shape[0]","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:18:57.76133Z","iopub.execute_input":"2021-06-10T22:18:57.761642Z","iopub.status.idle":"2021-06-10T22:18:57.786089Z","shell.execute_reply.started":"2021-06-10T22:18:57.76161Z","shell.execute_reply":"2021-06-10T22:18:57.785047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Because we're doing sequence classification, the model requires two different sequences to be joined in a single “input_ids” entry, which usually is performed with the help of special tokens, such as the classifier ([CLS]) and separator ([SEP]) tokens. For example, the BERT model builds its two sequence input as such:**\n\n```# [CLS] SEQUENCE_A [SEP] SEQUENCE_B [SEP]```\n\n**We can use our tokenizer to automatically generate such a sentence by passing the two sequences to the tokenizer as two arguments (and not a list, like before) like this:**\n\n```from transformers import BertTokenizer\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\nsequence_a = \"HuggingFace is based in NYC\"\nsequence_b = \"Where is HuggingFace based?\"\nencoded_dict = tokenizer(sequence_a, sequence_b)\ndecoded = tokenizer.decode(encoded_dict[\"input_ids\"])```\n\n**which will return:**\n\n```[CLS] HuggingFace is based in NYC [SEP] Where is HuggingFace based? [SEP]```\n\n**This is enough for some models to understand where one sequence ends and where another begins. However, for BERT we also need to deploy token type IDs (also called segment IDs). They are represented as a binary mask identifying the two types of sequence in the model.**\n\n**The tokenizer returns this mask as the “token_type_ids” entry:**\n\n``` encoded_dict['token_type_ids']\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]```\n\n**The first sequence, the “context” used for the question, has all its tokens represented by a ```0```, whereas the second sequence, corresponding to the “question”, has all its tokens represented by a ```1```.**\n\n**This example comes from Hugging Face and can be found [here](https://huggingface.co/transformers/v2.4.0/glossary.html#token-type-idshttps://huggingface.co/transformers/v2.4.0/glossary.html#token-type-ids)**","metadata":{}},{"cell_type":"markdown","source":"**I will break the ```cls``` variable into two parts.**","metadata":{}},{"cell_type":"code","source":"# just converting the CLS token into an id number \ncls = [tokenizer.convert_tokens_to_ids(['[CLS]'])]","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:20:29.297684Z","iopub.execute_input":"2021-06-10T22:20:29.298048Z","iopub.status.idle":"2021-06-10T22:20:29.302677Z","shell.execute_reply.started":"2021-06-10T22:20:29.298015Z","shell.execute_reply":"2021-06-10T22:20:29.301325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# converting the CLS token into an id number, then repeating it\n# to equal the number of statements pairs \ncls = cls*sentence1.shape[0]\ncls[0:10]","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:20:30.15469Z","iopub.execute_input":"2021-06-10T22:20:30.155228Z","iopub.status.idle":"2021-06-10T22:20:30.161201Z","shell.execute_reply.started":"2021-06-10T22:20:30.155179Z","shell.execute_reply":"2021-06-10T22:20:30.160169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Concatenates tensors along one dimension\ninput_word_ids = tf.concat([cls, sentence1, sentence2], axis=-1)\ninput_word_ids[0]","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:20:30.971668Z","iopub.execute_input":"2021-06-10T22:20:30.972348Z","iopub.status.idle":"2021-06-10T22:20:31.051317Z","shell.execute_reply.started":"2021-06-10T22:20:30.972308Z","shell.execute_reply":"2021-06-10T22:20:31.050166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The code above is essentially just taking the CLS which marks where one sequence/ statement begins and adds it to an array that also includes the tokenized hypothesis and premise**","metadata":{}},{"cell_type":"code","source":"input_mask = tf.ones_like(input_word_ids).to_tensor()\ninput_mask[0]","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:20:36.521679Z","iopub.execute_input":"2021-06-10T22:20:36.522015Z","iopub.status.idle":"2021-06-10T22:20:36.547329Z","shell.execute_reply.started":"2021-06-10T22:20:36.521985Z","shell.execute_reply":"2021-06-10T22:20:36.546339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The code above is creating a binary mask identifying the two types of sequence in the model, premise and hypothesis. The mask allows the model to cleanly differentiate between the content and the padding. The mask makes it so that the input_word_ids are all the same shape, and contains a 1 anywhere the input_word_ids is not padding.**","metadata":{}},{"cell_type":"code","source":"# Creates a tensor with all elements, in this case CLS, set to zeros\ntype_cls = tf.zeros_like(cls)\ntype_cls","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:20:40.145097Z","iopub.execute_input":"2021-06-10T22:20:40.145588Z","iopub.status.idle":"2021-06-10T22:20:40.165403Z","shell.execute_reply.started":"2021-06-10T22:20:40.145556Z","shell.execute_reply":"2021-06-10T22:20:40.164326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creates a tensor with all elements, in this case the hypotheses, set to zeros\ntype_s1 = tf.zeros_like(sentence1)\ntype_s1[0]","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-06-10T22:21:30.82849Z","iopub.execute_input":"2021-06-10T22:21:30.828948Z","iopub.status.idle":"2021-06-10T22:21:30.843686Z","shell.execute_reply.started":"2021-06-10T22:21:30.828901Z","shell.execute_reply":"2021-06-10T22:21:30.84264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creates a tensor with all elements, in this case the premises, set to ones\ntype_s2 = tf.ones_like(sentence2)\ntype_s2[0]","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:21:41.424355Z","iopub.execute_input":"2021-06-10T22:21:41.424715Z","iopub.status.idle":"2021-06-10T22:21:41.437658Z","shell.execute_reply.started":"2021-06-10T22:21:41.424677Z","shell.execute_reply":"2021-06-10T22:21:41.436828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_type_ids = tf.concat(\n    [type_cls, type_s1, type_s2], axis=-1).to_tensor()\ninput_type_ids[0]","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:21:42.280352Z","iopub.execute_input":"2021-06-10T22:21:42.281031Z","iopub.status.idle":"2021-06-10T22:21:42.337638Z","shell.execute_reply.started":"2021-06-10T22:21:42.28098Z","shell.execute_reply":"2021-06-10T22:21:42.336553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The code above is doing what we did before for the input_word_ids except this time when looking inside the non-padded region, it contains a 0 or a 1 that indicates which sentence the token is a part of. The mask makes it so that the input_type_ids are again all the same shape**","metadata":{}},{"cell_type":"code","source":" inputs = {\n      'input_word_ids': input_word_ids.to_tensor(),\n      'input_mask': input_mask,\n      'input_type_ids': input_type_ids}\ninputs","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:21:56.135295Z","iopub.execute_input":"2021-06-10T22:21:56.135816Z","iopub.status.idle":"2021-06-10T22:21:56.251208Z","shell.execute_reply.started":"2021-06-10T22:21:56.135783Z","shell.execute_reply":"2021-06-10T22:21:56.249957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Woohoo! We now have all of the information in the correct format to train and run our BERT model below**","metadata":{}},{"cell_type":"markdown","source":"## Creating & Training Model","metadata":{"_uuid":"e1bf9ee4-e872-45d8-a118-a72eb7917d8b","_cell_guid":"3dbd7066-6469-4823-8e09-e2dfa0a68bcc","trusted":true}},{"cell_type":"markdown","source":"Now, we can incorporate the BERT transformer into a Keras Functional Model. For more information about the Keras Functional API, see: https://www.tensorflow.org/guide/keras/functional.\n\nThis model was inspired by the model in this notebook: https://www.kaggle.com/tanulsingh077/deep-learning-for-nlp-zero-to-transformers-bert#BERT-and-Its-Implementation-on-this-Competition, which is a wonderful introduction to NLP!","metadata":{"_uuid":"33811793-9df3-4d97-84c3-c5367a130cd7","_cell_guid":"7ebc4032-48bf-4bbd-8244-6b4ee38a1f56","trusted":true}},{"cell_type":"markdown","source":"**I had issues with the original build_model function below. The hack directly below this cell came from Zakaria MESSIA and is directly linked [here](https://www.kaggle.com/anasofiauzsoy/tutorial-notebook/comments#1223205)**","metadata":{}},{"cell_type":"code","source":"def bert_encode(hypotheses, premises, tokenizer, max_length=150):\n\n    x = [h + ' [SEP] ' + p for h, p in zip(np.array(hypotheses), np.array(premises))]\n    x = tokenizer(x, padding=True, truncation=True, max_length=max_length)\n\n    inputs = {\n          'input_word_ids':tf.ragged.constant(x['input_ids']).to_tensor(),\n          'input_mask': tf.ragged.constant(x['attention_mask']).to_tensor(),\n          'input_type_ids': tf.ragged.constant(x['token_type_ids']).to_tensor()}\n\n    return inputs","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:24:05.262395Z","iopub.execute_input":"2021-06-10T22:24:05.262781Z","iopub.status.idle":"2021-06-10T22:24:05.269882Z","shell.execute_reply.started":"2021-06-10T22:24:05.262743Z","shell.execute_reply":"2021-06-10T22:24:05.268638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Since just having max_len in the model that we built below was causing errors, the code above specifies a max_len and a few other parameters when the statements are being tokenized. You can also check out this webpage by Hugging Face on preprocessing that explains the code above [here](https://huggingface.co/transformers/preprocessing.html#preprocessing-pairs-of-sentences).**","metadata":{}},{"cell_type":"code","source":"train_input = bert_encode(train.premise.values, train.hypothesis.values, tokenizer)","metadata":{"_uuid":"697579c5-9f89-421c-8cbc-4321a7c93179","_cell_guid":"fc14d779-d0c6-4585-8e80-ebf539bd132c","execution":{"iopub.status.busy":"2021-06-10T22:24:08.943044Z","iopub.execute_input":"2021-06-10T22:24:08.943455Z","iopub.status.idle":"2021-06-10T22:24:43.464248Z","shell.execute_reply.started":"2021-06-10T22:24:08.943425Z","shell.execute_reply":"2021-06-10T22:24:43.462799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Now it's time to build our model! The code below is a pretty standard TF model set-up so I've decided not to break it down**","metadata":{}},{"cell_type":"code","source":"max_len = 150\n\ndef build_model():\n    bert_encoder = TFBertModel.from_pretrained(model_name)\n    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    input_type_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_type_ids\")\n    \n    embedding = bert_encoder([input_word_ids, input_mask, input_type_ids])[0]\n    output = tf.keras.layers.Dense(3, activation='softmax')(embedding[:,0,:])\n    \n    model = tf.keras.Model(inputs=[input_word_ids, input_mask, input_type_ids], outputs=output)\n    model.compile(tf.keras.optimizers.Adam(lr=1e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n    \n    return model","metadata":{"_uuid":"1094f3ff-6b5b-4b15-9d93-b0065ae586bd","_cell_guid":"69f90af1-4d66-4b83-a264-df23a3a68e26","execution":{"iopub.status.busy":"2021-06-10T02:21:46.705373Z","iopub.execute_input":"2021-06-10T02:21:46.705783Z","iopub.status.idle":"2021-06-10T02:21:46.712784Z","shell.execute_reply.started":"2021-06-10T02:21:46.705753Z","shell.execute_reply":"2021-06-10T02:21:46.712055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# just telling it to use the TPU \nwith strategy.scope():\n    model = build_model()\n    model.summary()","metadata":{"_uuid":"47c8cc2f-a7b9-4bae-971b-e41277b45c8b","_cell_guid":"97885b33-70f1-4d14-b788-fd1a650d5a57","execution":{"iopub.status.busy":"2021-06-10T02:21:46.713703Z","iopub.execute_input":"2021-06-10T02:21:46.713979Z","iopub.status.idle":"2021-06-10T02:23:08.363356Z","shell.execute_reply.started":"2021-06-10T02:21:46.713952Z","shell.execute_reply":"2021-06-10T02:23:08.362373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(train_input, train.label.values, epochs = 5, verbose = 1, batch_size = 128, validation_split = 0.3)","metadata":{"_uuid":"316f4376-9df7-40ea-bbbd-ae46eb5562e5","_cell_guid":"0423d993-8ff5-4ab9-bc47-87e1cb74c0c5","execution":{"iopub.status.busy":"2021-06-10T02:23:08.364771Z","iopub.execute_input":"2021-06-10T02:23:08.365062Z","iopub.status.idle":"2021-06-10T02:26:00.140183Z","shell.execute_reply.started":"2021-06-10T02:23:08.365033Z","shell.execute_reply":"2021-06-10T02:26:00.13909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv(\"../input/contradictory-my-dear-watson/test.csv\")\ntest_input = bert_encode(test.premise.values, test.hypothesis.values, tokenizer)","metadata":{"_uuid":"3bfd96e9-cf2c-41ad-bdcc-8c31408f05bd","_cell_guid":"e26cba08-7fe9-4e2e-ab2e-062a26a5fcac","execution":{"iopub.status.busy":"2021-06-10T02:26:00.141923Z","iopub.execute_input":"2021-06-10T02:26:00.142357Z","iopub.status.idle":"2021-06-10T02:26:14.963683Z","shell.execute_reply.started":"2021-06-10T02:26:00.142313Z","shell.execute_reply":"2021-06-10T02:26:14.962675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head()","metadata":{"_uuid":"9f040800-653f-4f51-9f60-0456d89068be","_cell_guid":"4b871234-9a42-4f5e-9792-7fc90615b808","execution":{"iopub.status.busy":"2021-06-10T02:26:14.964969Z","iopub.execute_input":"2021-06-10T02:26:14.965252Z","iopub.status.idle":"2021-06-10T02:26:14.979059Z","shell.execute_reply.started":"2021-06-10T02:26:14.965224Z","shell.execute_reply":"2021-06-10T02:26:14.978012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Generating & Submitting Predictions","metadata":{"_uuid":"fb1ff888-7684-4888-b861-3c31a3f360b7","_cell_guid":"87b18b05-30f3-45f9-9c3e-6f8184934bd0","trusted":true}},{"cell_type":"code","source":"predictions = [np.argmax(i) for i in model.predict(test_input)]","metadata":{"_uuid":"f9db46a1-1f83-4ddb-85bf-da8f40afe623","_cell_guid":"a7ab0c33-0377-4cb2-b4f9-fc489383fdb7","execution":{"iopub.status.busy":"2021-06-10T02:26:14.980445Z","iopub.execute_input":"2021-06-10T02:26:14.980784Z","iopub.status.idle":"2021-06-10T02:26:28.488806Z","shell.execute_reply.started":"2021-06-10T02:26:14.980754Z","shell.execute_reply":"2021-06-10T02:26:28.487748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The submission file will consist of the ID column and a prediction column. We can just copy the ID column from the test file, make it a dataframe, and then add our prediction column.","metadata":{"_uuid":"7b489c00-896d-44af-9371-16c2cf222365","_cell_guid":"1d0e8e42-2746-4331-95a5-3eb78ca6861c","trusted":true}},{"cell_type":"code","source":"submission = test.id.copy().to_frame()\nsubmission['prediction'] = predictions","metadata":{"_uuid":"9e0e34fa-1ef7-4207-a5c3-c21863c7be27","_cell_guid":"add7302f-ae26-4e78-b69d-858cacb35991","execution":{"iopub.status.busy":"2021-06-10T02:26:28.490076Z","iopub.execute_input":"2021-06-10T02:26:28.490379Z","iopub.status.idle":"2021-06-10T02:26:28.499109Z","shell.execute_reply.started":"2021-06-10T02:26:28.490349Z","shell.execute_reply":"2021-06-10T02:26:28.498213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.head()","metadata":{"_uuid":"1d4999aa-10d2-4a88-962a-8f10bded837b","_cell_guid":"d4fdefdc-4839-4962-ae75-6807390a6de7","execution":{"iopub.status.busy":"2021-06-10T02:26:28.50045Z","iopub.execute_input":"2021-06-10T02:26:28.500779Z","iopub.status.idle":"2021-06-10T02:26:28.511505Z","shell.execute_reply.started":"2021-06-10T02:26:28.500751Z","shell.execute_reply":"2021-06-10T02:26:28.51056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv(\"submission.csv\", index = False)","metadata":{"_uuid":"b8463f7e-8b2f-4c24-8eb3-a35ec02a2d6e","_cell_guid":"84abe9e3-ef04-4dac-97d2-2308a0f11313","execution":{"iopub.status.busy":"2021-06-10T02:26:28.512731Z","iopub.execute_input":"2021-06-10T02:26:28.513008Z","iopub.status.idle":"2021-06-10T02:26:28.536501Z","shell.execute_reply.started":"2021-06-10T02:26:28.512982Z","shell.execute_reply":"2021-06-10T02:26:28.535746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And now we've created our submission file, which can be submitted to the competition. Good luck!","metadata":{"_uuid":"1af8c748-a13d-4274-9208-94d9895fdc19","_cell_guid":"4d057f53-824d-400f-b9f0-5adfc06322ef","trusted":true}}]}