{"cells":[{"metadata":{},"cell_type":"markdown","source":"Natural Language Inferencing (NLI) is a classic NLP (Natural Language Processing) problem that involves taking two sentences (the premise and the hypothesis ), and deciding how they are related- if the premise entails the hypothesis, contradicts it, or neither.\n\nIn this tutorial we'll look at the Contradictory, My Dear Watson competition dataset, build a preliminary model using Tensorflow 2, Keras, and BERT, and prepare a submission file.","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import BertTokenizer, TFBertModel\nfrom transformers import RobertaTokenizer, TFRobertaModel\nfrom transformers import XLMRobertaTokenizer, TFXLMRobertaModel\nfrom transformers import AutoTokenizer\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom dask import bag, diagnostics\nfrom sklearn.utils import shuffle","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install --quiet googletrans\nfrom googletrans import Translator","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's set up our TPU.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept ValueError:\n    strategy = tf.distribute.get_strategy() # for CPU and single GPU\n    print('Number of replicas:', strategy.num_replicas_in_sync)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Downloading data**\nThe training set contains a premise, a hypothesis, a label (0 = entailment, 1 = neutral, 2 = contradiction), and the language of the text. For more information about what these mean and how the data is structured, check out the data page: https://www.kaggle.com/c/contradictory-my-dear-watson/data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv(\"/kaggle/input/output/submission (2).csv\")\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv(\"submission1.csv\", index = False)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/contradictory-my-dear-watson/train.csv\")\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Let us see the distribution of languages in the training dataset.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train['language'].value_counts(normalize = True).plot(kind = 'bar', alpha = 0.7)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preparing Data for Input\nTo start out, we can use a pretrained model. Here, we'll use a multilingual BERT model from huggingface. For more information about BERT, see: https://github.com/google-research/bert/blob/master/multilingual.md\n\nFirst, we download the tokenizer.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# model_name = 'bert-base-multilingual-cased'\n# tokenizer = BertTokenizer.from_pretrained(model_name)\n#tokenizer = RobertaTokenizer.from_pretrained('roberta-large-mnli')\n#tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-large')\ntokenizer = AutoTokenizer.from_pretrained('jplu/tf-xlm-roberta-large')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Tokenizers turn sequences of words into arrays of numbers. Let's look at an example:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def encode_sentence(s):\n    tokens = list(tokenizer.tokenize(s))\n    tokens.append('[SEP]')\n    return tokenizer.convert_tokens_to_ids(tokens)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"s = \"I love machine learning\"\nencode_sentence(s)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data augmentation by Translation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def data_translate(source_data,dest_language):\n    translator = Translator()\n    if dest_language == 'zh':\n        dest_language = 'zh-cn'\n    dest_data = translator.translate(source_data, dest = dest_language).text \n    return dest_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def translation_augment(source_data, languages, fraction):\n    \n    new_df = pd.DataFrame()\n    \n    for lang in languages:\n        print(lang)\n        sampled_rows = source_data.sample(frac=fraction, replace = False)\n        prem_bag = bag.from_sequence(sampled_rows['premise'].tolist()).map(data_translate, lang)\n        hypothesis_bag = bag.from_sequence(sampled_rows['hypothesis'].tolist()).map(data_translate, lang)\n        \n        with diagnostics.ProgressBar():\n            prems = prem_bag.compute()\n            hyps = hypothesis_bag.compute()\n            \n        aug_df = pd.DataFrame({'id': pd.Series([None]*len(sampled_rows)),\n                                'premise': pd.Series(prems),\n                                'hypothesis': pd.Series(hyps),\n                                'lang_abv': pd.Series([lang]*len(sampled_rows)),\n                                'language': pd.Series([None]*len(sampled_rows)),\n                                'label': pd.Series(sampled_rows['label'].values)                              \n                              })\n        new_df = new_df.append(aug_df)\n    new_df = shuffle(new_df)\n    return new_df\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def data_augment(train_df, fraction):\n    \n    english_df = train.loc[train.lang_abv == 'en']\n    languages = list(set(train.lang_abv.values))\n    languages.remove('en')\n\n#     languages = ['fr', 'th', 'tr', 'ur', 'ru', 'bg', 'de', 'ar', 'zh-cn', 'hi',\n#                  'sw', 'vi', 'es', 'el']\n\n    print(languages)    \n    translated_df = translation_augment(english_df,languages, fraction)\n    train_df = train_df.append(translated_df)\n    train_df = shuffle(train_df)\n    return train_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/augment-data20/augmented_data_20percent.csv')\ntrain.head()\nlen(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print(\"Length of training data before augmentation\", len(train))\n# train = data_augment(train, fraction = 0.6)\n# print(\"Length of training data after augmentation\", len(train))\n\n# train['lang_abv'].value_counts(normalize = True).plot(kind = 'bar', alpha = 0.7)\n# plt.show()\n# train.to_csv('augmented_data_60_percent.csv', index=False)\n# train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"BERT uses three kind of input data- input word IDs, input masks, and input type IDs.\n\nThese allow the model to know that the premise and hypothesis are distinct sentences, and also to ignore any padding from the tokenizer.\n\nWe add a [CLS] token to denote the beginning of the inputs, and a [SEP] token to denote the separation between the premise and the hypothesis. We also need to pad all of the inputs to be the same size. For more information about BERT inputs, see: https://huggingface.co/transformers/model_doc/bert.html#tfbertmodel\n\nNow, we're going to encode all of our premise/hypothesis pairs for input into BERT.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def bert_encode(premises, hypotheses, tokenizer):\n    num_examples = len(premises)\n    sen1 = tf.ragged.constant([encode_sentence(s) for s in np.array(premises)])\n    sen2 = tf.ragged.constant([encode_sentence(s) for s in np.array(hypotheses)])\n    cls = [tokenizer.convert_tokens_to_ids(['CLS'])]*sen1.shape[0]\n    \n    input_word_ids = tf.concat([cls, sen1, sen2], axis = -1)\n    input_mask = tf.ones_like(input_word_ids).to_tensor()\n    \n    type_cls = tf.zeros_like(cls)\n    type_sen1 = tf.zeros_like(sen1)\n    type_sen2 = tf.ones_like(sen2)\n    input_type_ids = tf.concat([type_cls, type_sen1, type_sen2], axis = -1).to_tensor()\n    \n    inputs = {\n        \n        'input_word_ids' : input_word_ids.to_tensor(),\n        'input_mask': input_mask,\n        'input_type_ids': input_type_ids\n        \n    }\n    \n    return inputs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_input = bert_encode(train.premise.values, train.hypothesis.values, tokenizer)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Creating & Training Model**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"max_len = 80\ndef build_model():\n    #bert_encoder = TFBertModel.from_pretrained(model_name)\n    bert_encoder = TFRobertaModel.from_pretrained('jplu/tf-xlm-roberta-large')\n    #bert_encoder = TFXLMRobertaModel.from_pretrained('xlm-mlm-100-1280')\n    input_word_ids = tf.keras.Input(shape =(max_len, ), dtype =tf.int32, name = \"input_word_ids\")\n    input_mask = tf.keras.Input(shape = (max_len, ), dtype= tf.int32, name = \"input_mask\")\n    input_type_ids = tf.keras.Input(shape= (max_len, ), dtype= tf.int32, name=\"input_type_ids\")\n    embedding = bert_encoder([input_word_ids, input_mask, input_type_ids])[0]\n    output = tf.keras.layers.Dense(3, activation = 'softmax')(embedding[:,0,:])\n    \n    model = tf.keras.Model(inputs= [input_word_ids, input_mask, input_type_ids], outputs=output)\n    model.compile(tf.keras.optimizers.Adam(lr=1e-5), loss='sparse_categorical_crossentropy', metrics = 'accuracy')\n    \n    return model   \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with strategy.scope():\n    model = build_model()\n    model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(train_input, train.label.values, epochs=3, verbose=1, batch_size=16, validation_split=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save_weights('RoBertamodel_augmented_data_20_percent_adam_sparse_categorical_entropy.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/contradictory-my-dear-watson/test.csv')\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_input = bert_encode(test.premise.values, test.hypothesis.values, tokenizer)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Generating & Submitting Predictions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = [np.argmax(i) for i in model.predict(test_input) ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = test.id.copy().to_frame()\nsubmission.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission['prediction'] = predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv(\"submission.csv\", index = False)\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}