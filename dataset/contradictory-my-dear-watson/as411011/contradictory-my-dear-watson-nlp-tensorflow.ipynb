{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1. Problem Formulation\nOur task is to create an NLI model that assigns labels of 0, 1, or 2 (corresponding to entailment, neutral, and contradiction) to pairs of premises and hypotheses. To make things more interesting, the train and test set include text in fifteen different languages! ","metadata":{}},{"cell_type":"markdown","source":"# 2. Import libraries","metadata":{}},{"cell_type":"code","source":"# 匯入套件\nimport os\nimport numpy as np # 線性運算\nimport pandas as pd #處理資料\nfrom transformers import BertTokenizer, TFBertModel # 語言模型\nimport matplotlib.pyplot as plt # 畫圖\nimport seaborn as sns # 畫圖\nimport tensorflow as tf # 類神經網路","metadata":{"execution":{"iopub.status.busy":"2022-06-15T09:00:14.110699Z","iopub.execute_input":"2022-06-15T09:00:14.11162Z","iopub.status.idle":"2022-06-15T09:00:14.117108Z","shell.execute_reply.started":"2022-06-15T09:00:14.111569Z","shell.execute_reply":"2022-06-15T09:00:14.116541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.environ[\"WANDB_API_KEY\"] = \"0\" # to silence warning","metadata":{"execution":{"iopub.status.busy":"2022-06-15T09:00:14.247634Z","iopub.execute_input":"2022-06-15T09:00:14.248309Z","iopub.status.idle":"2022-06-15T09:00:14.253065Z","shell.execute_reply.started":"2022-06-15T09:00:14.24827Z","shell.execute_reply":"2022-06-15T09:00:14.252372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Turn on TPU\nTPUs are powerful hardware accelerators specialized in deep learning tasks, including Natural Language Processing. Kaggle provides all users TPU Quota at no cost, which you can use to explore this competition. ","metadata":{}},{"cell_type":"code","source":"#偵測錯誤\ntry:                                                                  # 使用 try，測試內容是否正確\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept ValueError:                                                    # 如果 try 的內容發生錯誤，就執行 except 裡的內容\n    strategy = tf.distribute.get_strategy() # for CPU and single GPU\n    print('Number of replicas:', strategy.num_replicas_in_sync)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T09:00:14.410503Z","iopub.execute_input":"2022-06-15T09:00:14.411132Z","iopub.status.idle":"2022-06-15T09:00:20.491741Z","shell.execute_reply.started":"2022-06-15T09:00:14.411083Z","shell.execute_reply":"2022-06-15T09:00:20.490916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Read data","metadata":{}},{"cell_type":"code","source":"# 導入資料\ndf_train = pd.read_csv(\"../input/contradictory-my-dear-watson/train.csv\")\ndf_test = pd.read_csv(\"../input/contradictory-my-dear-watson/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-06-15T09:00:20.493462Z","iopub.execute_input":"2022-06-15T09:00:20.493699Z","iopub.status.idle":"2022-06-15T09:00:20.6074Z","shell.execute_reply.started":"2022-06-15T09:00:20.493673Z","shell.execute_reply":"2022-06-15T09:00:20.606496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head(7) # check data (first 7 rows) 顯示前七行","metadata":{"execution":{"iopub.status.busy":"2022-06-15T09:00:20.608839Z","iopub.execute_input":"2022-06-15T09:00:20.609123Z","iopub.status.idle":"2022-06-15T09:00:20.622066Z","shell.execute_reply.started":"2022-06-15T09:00:20.609093Z","shell.execute_reply":"2022-06-15T09:00:20.621161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.head(7) # check data (first 7 rows) 顯示前七行","metadata":{"execution":{"iopub.status.busy":"2022-06-15T09:00:20.624169Z","iopub.execute_input":"2022-06-15T09:00:20.624521Z","iopub.status.idle":"2022-06-15T09:00:20.638393Z","shell.execute_reply.started":"2022-06-15T09:00:20.624491Z","shell.execute_reply":"2022-06-15T09:00:20.637841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.shape # check data 總共有多少行與列","metadata":{"execution":{"iopub.status.busy":"2022-06-15T09:00:20.639381Z","iopub.execute_input":"2022-06-15T09:00:20.639709Z","iopub.status.idle":"2022-06-15T09:00:20.649534Z","shell.execute_reply.started":"2022-06-15T09:00:20.639682Z","shell.execute_reply":"2022-06-15T09:00:20.648955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.shape # check data 總共有多少行與列","metadata":{"execution":{"iopub.status.busy":"2022-06-15T09:00:20.650549Z","iopub.execute_input":"2022-06-15T09:00:20.650847Z","iopub.status.idle":"2022-06-15T09:00:20.659564Z","shell.execute_reply.started":"2022-06-15T09:00:20.650822Z","shell.execute_reply":"2022-06-15T09:00:20.658767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's check some rows more in more detailed way. As example, I'll use the row with 0 index.\n\n- **Premise** - a previous statement or proposition from which another is inferred or follows as a conclusion.\n- **Hypothesis** - a supposition or proposed explanation made on the basis of limited evidence as a starting point for further investigation.","metadata":{}},{"cell_type":"code","source":"# \"premise\" column\ndf_train[\"premise\"].values[0] # colume\"premise\"中的第一個row","metadata":{"execution":{"iopub.status.busy":"2022-06-15T09:00:20.6607Z","iopub.execute_input":"2022-06-15T09:00:20.660895Z","iopub.status.idle":"2022-06-15T09:00:20.670298Z","shell.execute_reply.started":"2022-06-15T09:00:20.660872Z","shell.execute_reply":"2022-06-15T09:00:20.669645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# \"hypothesis\" column\ndf_train[\"hypothesis\"].values[0] #colume\"hypothesis\"中的第一個row","metadata":{"execution":{"iopub.status.busy":"2022-06-15T09:00:20.673267Z","iopub.execute_input":"2022-06-15T09:00:20.673522Z","iopub.status.idle":"2022-06-15T09:00:20.680429Z","shell.execute_reply.started":"2022-06-15T09:00:20.673465Z","shell.execute_reply":"2022-06-15T09:00:20.679864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Vizualize data\nHere we can build some plots and visualizations.","metadata":{}},{"cell_type":"code","source":"# 畫圖\nfig, axes = plt.subplots(nrows=1, ncols=1, figsize=(20,10))\ncount_classes = df_train['language'].value_counts() # count sentenses in each language\nplt.title(\"Languages in df_train\".upper())\ncolors = ['#ff9999','#66b3ff','#99ff99',\n          '#ffcc99', '#ffccf9', '#ff99f8', \n          '#ff99af', '#ffe299', '#a8ff99',\n          '#cc99ff', '#9e99ff', '#99c9ff',\n          '#99f5ff', '#99ffe4', '#99ffaf']\n\nplt.pie(count_classes, \n        labels = count_classes.index, \n        autopct='%1.1f%%',\n        shadow=True, \n        startangle=90, \n        colors=colors)\n\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-15T09:00:20.681712Z","iopub.execute_input":"2022-06-15T09:00:20.682088Z","iopub.status.idle":"2022-06-15T09:00:20.995371Z","shell.execute_reply.started":"2022-06-15T09:00:20.68206Z","shell.execute_reply":"2022-06-15T09:00:20.994668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After that, we can build some more stats:","metadata":{}},{"cell_type":"code","source":"# 建立函數，方便畫圖\ndef plot_stats(df, column, ax, color, angle):\n    \"\"\" PLOT STATS OF DIFFERENT COLUMNS \"\"\"\n    count_classes = df[column].value_counts()\n    ax = sns.barplot(x=count_classes.index, y=count_classes, ax=ax, palette=color)\n    ax.set_title(column.upper(), fontsize=18)\n    for tick in ax.get_xticklabels():\n        tick.set_rotation(angle)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-15T09:00:20.997941Z","iopub.execute_input":"2022-06-15T09:00:20.99834Z","iopub.status.idle":"2022-06-15T09:00:21.00421Z","shell.execute_reply.started":"2022-06-15T09:00:20.998302Z","shell.execute_reply":"2022-06-15T09:00:21.003619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we can see stats for **\"label\"**:","metadata":{}},{"cell_type":"code","source":"# 使用函數來畫圖\nfig, axes = plt.subplots(nrows=1, ncols=1, figsize=(15,5))\nfig.autofmt_xdate()\nplot_stats(df_train, \"label\", axes, \"viridis\", 45)\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-15T09:00:21.005572Z","iopub.execute_input":"2022-06-15T09:00:21.005973Z","iopub.status.idle":"2022-06-15T09:00:21.218052Z","shell.execute_reply.started":"2022-06-15T09:00:21.005946Z","shell.execute_reply":"2022-06-15T09:00:21.217467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Download pretrained models\nTo start out, we can use a pretrained model. Here, we'll use a multilingual BERT model from huggingface. For more information about BERT, see [here](https://github.com/google-research/bert/blob/master/multilingual.md).","metadata":{}},{"cell_type":"markdown","source":"## 4.1 Download tokenizer\nTokenizers turn sequences of words into arrays of numbers. ","metadata":{}},{"cell_type":"code","source":"model_name = 'bert-base-multilingual-cased' \ntokenizer = BertTokenizer.from_pretrained(model_name) # 匯入已訓練過的語言模型","metadata":{"execution":{"iopub.status.busy":"2022-06-15T09:00:21.219346Z","iopub.execute_input":"2022-06-15T09:00:21.219753Z","iopub.status.idle":"2022-06-15T09:00:22.28952Z","shell.execute_reply.started":"2022-06-15T09:00:21.219722Z","shell.execute_reply":"2022-06-15T09:00:22.288767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(tokenizer.vocab) # check the vocabulary size 總長度","metadata":{"execution":{"iopub.status.busy":"2022-06-15T09:00:22.290992Z","iopub.execute_input":"2022-06-15T09:00:22.291457Z","iopub.status.idle":"2022-06-15T09:00:22.296035Z","shell.execute_reply.started":"2022-06-15T09:00:22.291424Z","shell.execute_reply":"2022-06-15T09:00:22.295552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's check the tokenizer. As example, we can take the sentence **\"you know they can't really defend themselves\"**.\n\nThe model expects its two inputs sentences to be concatenated together. This input is expected to start with a [CLS] \"This is a classification problem\" token, and each sentence should end with a [SEP] \"Separator\" token.","metadata":{}},{"cell_type":"code","source":"#建立函數，將句子中的字轉成連續性的向量\ndef encode_sentence(s):\n    \"\"\" ENCODE SENTENCES WITH TOKENIZER\"\"\"\n    tokens = list(tokenizer.tokenize(s))\n    tokens.append('[SEP]')\n    return tokenizer.convert_tokens_to_ids(tokens)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-15T09:00:22.29703Z","iopub.execute_input":"2022-06-15T09:00:22.297739Z","iopub.status.idle":"2022-06-15T09:00:22.307099Z","shell.execute_reply.started":"2022-06-15T09:00:22.297711Z","shell.execute_reply":"2022-06-15T09:00:22.30631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encode_sentence(\"you know they can't really defend themselves\")","metadata":{"execution":{"iopub.status.busy":"2022-06-15T09:00:22.30818Z","iopub.execute_input":"2022-06-15T09:00:22.308447Z","iopub.status.idle":"2022-06-15T09:00:22.320663Z","shell.execute_reply.started":"2022-06-15T09:00:22.308407Z","shell.execute_reply":"2022-06-15T09:00:22.319832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.2 BERT\n[BERT](https://huggingface.co/transformers/model_doc/bert.html#tfbertmodel) **uses three kind of input data** - input word IDs, input masks, and input type IDs.\n\nThese allow the model to know that **the premise and hypothesis are distinct sentences**, and also to ignore any padding from the tokenizer.\n\nWe add a [CLS] token to denote **the beginning of the inputs**, and a [SEP] token to denote the separation between **the premise and the hypothesis**. \n\nWe also **need to pad** all of the inputs to be the same size.\n\nNow, we're going to **encode all of our premise/hypothesis pairs** for input into BERT.","metadata":{}},{"cell_type":"code","source":"#建立函數，將輸入所有輸入的長度轉為一樣長\ndef bert_encode(hypotheses, premises, tokenizer):\n    \"\"\" ENCODE DATA FOR BERT\"\"\"\n    num_examples = len(hypotheses)\n    print(\"num_examples = \", num_examples)\n    sentence1 = tf.ragged.constant([encode_sentence(s) for s in np.array(hypotheses)])\n    print(\"sentence1.shape = \", sentence1.shape)\n    sentence2 = tf.ragged.constant([encode_sentence(s) for s in np.array(premises)])\n    print(\"sentence2.shape = \", sentence2.shape)\n    cls_ = [tokenizer.convert_tokens_to_ids(['[CLS]'])] * sentence1.shape[0]\n    input_word_ids = tf.concat([cls_, sentence1, sentence2], axis=-1)\n    print(\"input_word_ids.shape = \", input_word_ids.shape)\n    # 300 - as my example\n    # because we have train_input (12120; 259), test_input (5159; 234)\n    # and shape[1] should be the same in each dataset\n    # that is why we creating (xxx; 300) shape in to_tensor() functions  \n    input_mask = tf.ones_like(input_word_ids).to_tensor(shape=(input_word_ids.shape[0], 300)) \n    print(\"input_mask.shape = \", input_mask.shape)\n    \n    type_cls = tf.zeros_like(cls_)\n    type_s1 = tf.zeros_like(sentence1)\n    type_s2 = tf.ones_like(sentence2)\n    \n    input_type_ids = tf.concat([type_cls, type_s1, type_s2], axis=-1).to_tensor(shape=(input_word_ids.shape[0], 300))\n    \n    inputs = {'input_word_ids': input_word_ids.to_tensor(shape=(input_word_ids.shape[0], 300)),\n              'input_mask': input_mask,\n              'input_type_ids': input_type_ids}\n    print()\n    \n    return inputs\n    ","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-15T09:00:22.322042Z","iopub.execute_input":"2022-06-15T09:00:22.322289Z","iopub.status.idle":"2022-06-15T09:00:22.333103Z","shell.execute_reply.started":"2022-06-15T09:00:22.322252Z","shell.execute_reply":"2022-06-15T09:00:22.332288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# encode data\ntrain_input = bert_encode(df_train[\"premise\"].values, df_train[\"hypothesis\"].values, tokenizer)\ntest_input = bert_encode(df_test[\"premise\"].values, df_test[\"hypothesis\"].values, tokenizer)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T09:00:22.334239Z","iopub.execute_input":"2022-06-15T09:00:22.334497Z","iopub.status.idle":"2022-06-15T09:00:42.270738Z","shell.execute_reply.started":"2022-06-15T09:00:22.334447Z","shell.execute_reply":"2022-06-15T09:00:42.269952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_input # check train input 顯示","metadata":{"execution":{"iopub.status.busy":"2022-06-15T09:00:42.272604Z","iopub.execute_input":"2022-06-15T09:00:42.273203Z","iopub.status.idle":"2022-06-15T09:00:42.339717Z","shell.execute_reply.started":"2022-06-15T09:00:42.273151Z","shell.execute_reply":"2022-06-15T09:00:42.338892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_input # check test input 顯示","metadata":{"execution":{"iopub.status.busy":"2022-06-15T09:00:42.341229Z","iopub.execute_input":"2022-06-15T09:00:42.34168Z","iopub.status.idle":"2022-06-15T09:00:42.370754Z","shell.execute_reply.started":"2022-06-15T09:00:42.341632Z","shell.execute_reply":"2022-06-15T09:00:42.369826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Train Neural Network Model\nNow, we can incorporate the BERT transformer into a Keras Functional Model. ","metadata":{}},{"cell_type":"code","source":"max_len = train_input[\"input_word_ids\"].shape[1] # 輸入的大小\n# 建立模型\ndef create_model():\n    \"\"\" BUILD MODEL \"\"\"\n    bert_encoder = TFBertModel.from_pretrained(model_name)\n    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    input_type_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_type_ids\")\n\n    embedding = bert_encoder([input_word_ids, input_mask, input_type_ids])[0]\n    output = tf.keras.layers.Dense(3, activation='softmax')(embedding[:,0,:])\n\n    model = tf.keras.Model(inputs=[input_word_ids, input_mask, input_type_ids], outputs=output)\n    model.compile(tf.keras.optimizers.Adam(lr=1e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-06-15T09:00:42.372149Z","iopub.execute_input":"2022-06-15T09:00:42.373093Z","iopub.status.idle":"2022-06-15T09:00:42.383518Z","shell.execute_reply.started":"2022-06-15T09:00:42.37305Z","shell.execute_reply":"2022-06-15T09:00:42.382578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with strategy.scope():\n    model = create_model()\n    model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-06-15T09:00:42.38489Z","iopub.execute_input":"2022-06-15T09:00:42.385224Z","iopub.status.idle":"2022-06-15T09:01:00.306688Z","shell.execute_reply.started":"2022-06-15T09:00:42.385182Z","shell.execute_reply":"2022-06-15T09:01:00.305777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 訓練模型\nmodel_history = model.fit(train_input, \n                          df_train[\"label\"].values, \n                          epochs = 20, \n                          verbose = 1,\n                          batch_size = 128, \n                          validation_split = 0.2)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T09:01:00.308056Z","iopub.execute_input":"2022-06-15T09:01:00.308307Z","iopub.status.idle":"2022-06-15T09:08:32.017579Z","shell.execute_reply.started":"2022-06-15T09:01:00.308276Z","shell.execute_reply":"2022-06-15T09:08:32.016576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Also, we can build **training history**:","metadata":{}},{"cell_type":"code","source":"# 建立函數以致畫圖：模型的準確度與誤差\ndef plot_NN_history(model_history, suptitle):\n    # plot data\n    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15,6))\n    fig.suptitle(suptitle, fontsize=18)\n    \n    axes[0].plot(model_history.history['accuracy'], label='train accuracy', color='g', axes=axes[0])\n    axes[0].plot(model_history.history['val_accuracy'], label='val accuracy', color='r', axes=axes[0])\n    axes[0].set_title(\"Model Accuracy\", fontsize=16) \n    axes[0].legend(loc='upper left')\n\n    axes[1].plot(model_history.history['loss'], label='train loss', color='g', axes=axes[1])\n    axes[1].plot(model_history.history['val_loss'], label='val loss', color='r', axes=axes[1])\n    axes[1].set_title(\"Model Loss\", fontsize=16) \n    axes[1].legend(loc='upper left')\n\n    plt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-15T09:08:32.019188Z","iopub.execute_input":"2022-06-15T09:08:32.019467Z","iopub.status.idle":"2022-06-15T09:08:32.028851Z","shell.execute_reply.started":"2022-06-15T09:08:32.019434Z","shell.execute_reply":"2022-06-15T09:08:32.028096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_NN_history(model_history, \"BERT\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-15T09:08:32.030006Z","iopub.execute_input":"2022-06-15T09:08:32.030354Z","iopub.status.idle":"2022-06-15T09:08:32.410233Z","shell.execute_reply.started":"2022-06-15T09:08:32.030327Z","shell.execute_reply":"2022-06-15T09:08:32.409314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. Test Neural Network","metadata":{"execution":{"iopub.status.busy":"2021-12-03T19:22:22.504812Z","iopub.execute_input":"2021-12-03T19:22:22.505161Z","iopub.status.idle":"2021-12-03T19:22:22.508925Z","shell.execute_reply.started":"2021-12-03T19:22:22.505124Z","shell.execute_reply":"2021-12-03T19:22:22.508094Z"}}},{"cell_type":"code","source":"# get the probabilities　＃預測\ny_prob = model.predict(test_input)\n# get the classes　＃將預測結果轉換成類別０－１－２已至submission\ny_hat = y_prob.argmax(axis=-1)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T09:08:32.411534Z","iopub.execute_input":"2022-06-15T09:08:32.411791Z","iopub.status.idle":"2022-06-15T09:08:51.172074Z","shell.execute_reply.started":"2022-06-15T09:08:32.411759Z","shell.execute_reply":"2022-06-15T09:08:51.170857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 7. Make submission","metadata":{}},{"cell_type":"code","source":"submission = df_test.id.copy().to_frame()\nsubmission['prediction'] = y_hat\nsubmission.head() # check submission\nsubmission.to_csv(\"submission.csv\", index = False) # save file","metadata":{"execution":{"iopub.status.busy":"2022-06-15T09:08:51.173776Z","iopub.execute_input":"2022-06-15T09:08:51.174634Z","iopub.status.idle":"2022-06-15T09:08:51.196168Z","shell.execute_reply.started":"2022-06-15T09:08:51.174595Z","shell.execute_reply":"2022-06-15T09:08:51.195411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 8. Conclusion\nThank you for reading my new article! Hope, you liked it and it was interesting for you! There are some more my articles:\n* [SMS spam with NBC | NLP | sklearn](https://www.kaggle.com/maricinnamon/sms-spam-with-nbc-nlp-sklearn)\n* [House Prices Regression sklearn](https://www.kaggle.com/maricinnamon/house-prices-regression-sklearn)\n* [Automobile Customer Clustering (K-means & PCA)](https://www.kaggle.com/maricinnamon/automobile-customer-clustering-k-means-pca)\n* [Credit Card Fraud detection sklearn](https://www.kaggle.com/maricinnamon/credit-card-fraud-detection-sklearn)\n* [Market Basket Analysis for beginners](https://www.kaggle.com/maricinnamon/market-basket-analysis-for-beginners)\n* [Neural Network for beginners with keras](https://www.kaggle.com/maricinnamon/neural-network-for-beginners-with-keras)\n* [Fetal Health Classification for beginners sklearn](https://www.kaggle.com/maricinnamon/fetal-health-classification-for-beginners-sklearn)\n* [Retail Trade Report Department Stores (LSTM)](https://www.kaggle.com/maricinnamon/retail-trade-report-department-stores-lstm)","metadata":{}}]}