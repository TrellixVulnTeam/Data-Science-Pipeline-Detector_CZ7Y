{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\ntrain_df = pd.read_csv(\"../input/contradictory-my-dear-watson/train.csv\")\ntest_df = pd.read_csv(\"../input/contradictory-my-dear-watson/test.csv\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport gc\nimport tensorflow as tf\nimport transformers\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import StratifiedKFold","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ACCELERATOR = 'TPU'\n\ntrain_splits = 5\nbatch_size = 32\nepochs = 10\nmax_length = 80\nmodel_name = 'jplu/tf-xlm-roberta-large'\ntokenizer = transformers.AutoTokenizer.from_pretrained(model_name)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking TPU first\nif ACCELERATOR == \"TPU\":\n    print(\"Connecting to TPU\")\n    try:\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        print(f\"Running on TPU {tpu.master()}\")\n        print(\"Initializing TPU\")\n        tf.config.experimental_connect_to_cluster(tpu)\n        tf.tpu.experimental.initialize_tpu_system(tpu)\n        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n        print(\"TPU initialized\")\n    except ValueError:\n        print(\"Could not connect to TPU\")\n\n# Default for CPU and GPU otherwise\nelse:\n    print(\"Using default strategy for CPU and single GPU\")\n    strategy = tf.distribute.get_strategy()\n\n# Checking GPUs\nif ACCELERATOR == \"GPU\":\n    print(f\"GPUs Available: {len(tf.config.experimental.list_physical_devices('GPU'))}\")\n\n# Defining replicas\nREPLICAS = strategy.num_replicas_in_sync\nprint(f\"REPLICAS: {REPLICAS}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_model():\n    # Defining the encoded inputs\n    input_ids = tf.keras.layers.Input(shape = (max_length,), dtype = tf.int32, name = \"input_ids\")\n    \n    # Loading pretrained transformer model\n    transformer_model = transformers.TFAutoModel.from_pretrained(model_name)\n    transformer_model.trainable = False\n\n    # Defining the data embedding using the loaded model\n    bert_output = transformer_model(input_ids)\n    transformer_embeddings = bert_output[0]\n    cls_token = transformer_embeddings[:, 0, :]\n    \n    sequence_output = bert_output.last_hidden_state\n    pooled_output = bert_output.pooler_output\n    \n    # Add trainable layers on top of frozen layers to adapt the pretrained features on the new data.\n    bi_lstm = tf.keras.layers.Bidirectional(\n        tf.keras.layers.LSTM(64, return_sequences=True)\n    )(sequence_output)\n    # Applying hybrid pooling approach to bi_lstm sequence output.\n    avg_pool = tf.keras.layers.GlobalAveragePooling1D()(bi_lstm)\n    max_pool = tf.keras.layers.GlobalMaxPooling1D()(bi_lstm)\n    concat = tf.keras.layers.concatenate([avg_pool, max_pool])\n    dropout = tf.keras.layers.Dropout(0.3)(concat)\n    batch_norm = tf.keras.layers.BatchNormalization()(dropout)\n    \n    # Defining the classifier layer\n    output = tf.keras.layers.Dense(3, activation=\"softmax\")(batch_norm)\n\n    model = tf.keras.models.Model(inputs = input_ids, outputs=output)\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(),\n        loss=\"categorical_crossentropy\",\n        metrics=[\"acc\"],\n    )\n\n    return model, transformer_model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def encode(df):\n    \"\"\"\n    Using tokenizer to encode text samples.\n    \"\"\"\n\n    sentences = df[[\"premise\", \"hypothesis\"]].values.astype(\"str\").tolist()\n    sentences_encoded = tokenizer.batch_encode_plus(\n        sentences,\n        add_special_tokens=True,\n        max_length = max_length,\n        padding = 'max_length',\n        truncation = 'longest_first',\n        return_attention_mask=True,\n        return_token_type_ids=True,\n    )\n\n    return sentences_encoded","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def to_tfds(X, y, labelled=True, repeat=False, shuffle=False, batch_size=128):\n    # Train data\n    if labelled:\n        ds = (tf.data.Dataset.from_tensor_slices((X[\"input_ids\"], y)))\n    # Test data\n    else:\n        ds = (tf.data.Dataset.from_tensor_slices(X[\"input_ids\"]))\n\n    # Optional repeat or shuffle\n    if repeat:\n        ds = ds.repeat()\n    if shuffle:\n        ds = ds.shuffle(2048)\n\n    # Fetch batch\n    ds = ds.batch(batch_size)\n    ds = ds.prefetch(tf.data.experimental.AUTOTUNE)\n\n    return ds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# adding language column for stratified splitting\ntrain_df[\"language_label\"] = train_df.language.astype(str) + \"_\" + train_df.label.astype(str)\n\n# stratified K-fold on language and label for balance\nskf = StratifiedKFold(n_splits=train_splits, shuffle=True, random_state=2021)\n\npreds_oof = np.zeros((train_df.shape[0], 3))\ntest_predictions = np.zeros((test_df.shape[0], 3))\nacc_oof = []\n    \n# Fine-tuning avec K-fold\nfor (fold, (train_index, valid_index)) in enumerate(skf.split(train_df, train_df.language_label)):\n    print(f\"--- Fold {fold} ---\")\n    \n    # Construction du modèle (dans la boucle pour que chaque fold ait un modèle 'neuf')\n    tf.keras.backend.clear_session()\n    if ACCELERATOR == \"TPU\":\n        tf.tpu.experimental.initialize_tpu_system(tpu)\n    with strategy.scope():\n        model, transformer_model = get_model()\n        if fold == 0:\n            print(model.summary())\n    \n    train = train_df.iloc[train_index]\n    valid = train_df.iloc[valid_index]\n\n    train_labels = tf.keras.utils.to_categorical(train.label, num_classes=3)\n    valid_labels = tf.keras.utils.to_categorical(valid.label, num_classes=3)\n    \n    # Encoding text data using tokenizer\n    train_encoded = encode(df=train)\n    valid_encoded = encode(df=valid)\n\n    # Creating TF Datasets for TPU\n    ds_train = to_tfds(train_encoded, train_labels, repeat=True, shuffle=True, batch_size=batch_size * REPLICAS)\n    ds_valid = to_tfds(valid_encoded, valid_labels, batch_size=batch_size * REPLICAS * 4)\n\n    train_size = train.shape[0]\n    \n    # Only need to encode test data once\n    if fold == 0:\n        test_encoded = encode(df=test_df)\n\n    # Pre-compilation\n    model.fit(\n        ds_train,\n        epochs=epochs,\n        steps_per_epoch=train_size / batch_size // REPLICAS,\n        validation_data=ds_valid,\n        verbose=1\n    )\n        \n    transformer_model.trainable = True\n    # Recompile the model to make the change effective.\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(1e-5),\n        loss=\"categorical_crossentropy\",\n        metrics=[\"accuracy\"],\n    )\n        \n    # Defining checkpoint callback\n    checkpoint = tf.keras.callbacks.ModelCheckpoint(\n        \"model.h5\",\n        monitor=\"val_accuracy\",\n        verbose=1,\n        save_best_only=True,\n        save_weights_only=True,\n        mode=\"max\",\n        save_freq=\"epoch\"\n    )\n    callbacks = [checkpoint]\n\n    print(\"Training...\")\n    model_history = model.fit(\n        ds_train,\n        epochs=epochs,\n        callbacks=callbacks,\n        steps_per_epoch=train_size / batch_size // REPLICAS,\n        validation_data=ds_valid,\n        verbose=1\n    )\n    print(model_history.history)\n\n    print(\"Validating...\")\n    # Scoring validation data\n    # we get the same acc score as val_accuracy for the best model (fit)\n    model.load_weights(\"model.h5\")\n    ds_valid = to_tfds(valid_encoded, -1, labelled=False, batch_size=batch_size * REPLICAS * 4)\n    print(ds_valid)\n    preds_valid = model.predict(ds_valid, verbose=1)\n    acc = accuracy_score(valid.label, np.argmax(preds_valid, axis = 1))\n\n    preds_oof[valid_index] = preds_valid\n    acc_oof.append(acc)\n\n    print(\"Testing...\")\n    # Scoring test data\n    ds_test = to_tfds(test_encoded, -1, labelled=False, batch_size=batch_size * REPLICAS * 4)\n    test_predictions += model.predict(ds_test, verbose=1) / train_splits\n\n    print(f\"Fold {fold} Accuracy: {round(acc, 4)}\")\n\n    g = gc.collect()\n\n# overall CV score and standard deviation\nprint(f\"CV Mean Accuracy: {round(np.mean(acc_oof), 4)}\")\nprint(f\"CV StdDev Accuracy: {round(np.std(acc_oof), 4)}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df = pd.DataFrame({\"id\": test_df.id.values, \"prediction\": np.argmax(test_predictions, axis = 1)})\nsubmission_df.to_csv(\"submission.csv\", index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}