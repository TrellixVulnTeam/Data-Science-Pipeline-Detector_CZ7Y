{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install --upgrade pip\n!pip install --upgrade allennlp\n!pip install transformers==4.0.1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for TPU\n!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n!python pytorch-xla-env-setup.py --apt-packages libomp5 libopenblas-dev","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import transformers\nimport pandas as pd\nimport torch","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for TPU\nimport torch_xla\nimport torch_xla.core.xla_model as xm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Transformers version: ', transformers.__version__)\nprint('Pytorch version: ', torch.__version__)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Import Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dir = '/kaggle/input/contradictory-my-dear-watson/'\ntrain_df = pd.read_csv(data_dir+'train.csv').sample(frac=1, random_state=100)\ntest_df = pd.read_csv(data_dir+'test.csv')\nprint(train_df['label'].value_counts())\ntrain_df.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Tokenization & Make input"},{"metadata":{"trusted":true},"cell_type":"code","source":"#PRE_TRAINED_MODEL = 'bert-base-multilingual-cased'\nPRE_TRAINED_MODEL = 'xlm-roberta-large'\ntokenizer = transformers.AutoTokenizer.from_pretrained(PRE_TRAINED_MODEL)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_LEN = 84\nsplit_idx = int(train_df.shape[0] * 0.8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_encode (data):\n    tokenized_data=tokenizer(\n        text=list(data['premise']), text_pair=list(data['hypothesis']),\n                                            max_length=MAX_LEN,\n                                            pad_to_max_length=True,\n                                            add_special_tokens=True,\n                                            truncation=True, \n                                            return_attention_mask=True, \n                                            return_token_type_ids=True,\n                                             return_tensors='pt')\n    return tokenized_data\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Data Preprocessing and tensor generation\nseed=2\n\ntokenized_train=get_encode(train_df[:split_idx])\nlabels_train=torch.tensor(train_df.label.values[:split_idx])\n\ntokenized_valid=get_encode(train_df[split_idx:])\nlabels_valid=torch.tensor(train_df.label.values[split_idx:])\n\ntokenized_test=get_encode(test_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print (tokenized_train['input_ids'][0])\nprint (tokenized_train['token_type_ids'][0])\nprint (tokenized_train['attention_mask'][0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n\nbatch_size=64\n\ntrain_data=TensorDataset(torch.tensor(tokenized_train['input_ids']),\n                         torch.tensor(tokenized_train['token_type_ids']),torch.tensor(tokenized_train['attention_mask'])\n                         ,labels_train)\ntrain_sampler=RandomSampler(train_data)\ntrain_dataloader=DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n\nvalid_data=TensorDataset(torch.tensor(tokenized_valid['input_ids']),\n                         torch.tensor(tokenized_valid['token_type_ids']),\n                         torch.tensor(tokenized_valid['attention_mask'])\n                         ,labels_valid)\nvalid_sampler=SequentialSampler(valid_data)\nvalid_dataloader=DataLoader(valid_data, sampler=valid_sampler, batch_size=batch_size)\n\ntest_data=TensorDataset(torch.tensor(tokenized_test['input_ids']),\n                        torch.tensor(tokenized_test['token_type_ids']),\n                        torch.tensor(tokenized_test['attention_mask']))\ntest_dataloader=DataLoader(test_data, batch_size=batch_size)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Fine Tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification\n\nmodel = AutoModelForSequenceClassification.from_pretrained(PRE_TRAINED_MODEL,\n                                                      num_labels = 3,\n                                                      output_attentions = False,\n                                                      output_hidden_states = False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for GPU / CPU\n'''\nif torch.cuda.is_available():\n    print(model.cuda())\nelse :\n    print(model.cpu())\n\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\n    print ('%d GPU(s) available' % torch.cuda.device_count())\nelse:\n    device = torch.device(\"cpu\")\n    print ('No GPU avaailable, using CPU.')\n'''\n    \n# for TPU\ndevice = xm.xla_device()\ntorch.set_default_tensor_type('torch.FloatTensor')\nprint(model.to(device))\nprint ('TPU available')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def accuracy(predictions, labels):\n    prediction_flat = np.argmax(predictions, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return np.sum(prediction_flat == labels_flat) / len(labels_flat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import AdamW\nfrom transformers import get_linear_schedule_with_warmup\n\noptimizer = AdamW(model.parameters(),\n                  lr = 2e-5\n                 )\nepochs = 20\ntotal_steps = len(train_dataloader) * epochs\nscheduler = get_linear_schedule_with_warmup(optimizer, \n                                            num_warmup_steps = 0,\n                                            num_training_steps = total_steps)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import datetime\nimport random\n\nrandom.seed(10)\nnp.random.seed(10)\ntorch.manual_seed(10)\ntorch.cuda.manual_seed_all(10)\n\nlosses = []\n\nfor i in range(0, epochs):\n    print ('Epoch {:} of {:} Training...'.format(i+1, epochs))\n    \n    total_loss = 0\n    model.train()\n    for step, batch in enumerate(train_dataloader):\n        if step % 10 == 0 and step != 0:\n            print ('[{}] Batch {:>5,} of {:>5,}'\n               .format(datetime.datetime.now().strftime('%H:%M:%S'), step, len(train_dataloader)))\n        train_batch_input = batch[0].to(device)\n        train_batch_input_types = batch[1].to(device)\n        train_batch_mask = batch[2].to(device)\n        train_batch_label = batch[3].to(device)\n        \n        model.zero_grad()\n        outputs = model(train_batch_input, token_type_ids = train_batch_input_types, \n                        attention_mask = train_batch_mask, labels = train_batch_label)\n        loss = outputs[0]        \n        \n        total_loss += loss.item()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        # for GPU / CPU\n        #optimizer.step()\n        # for TPU\n        xm.optimizer_step(optimizer, barrier=True)\n        scheduler.step()\n    \n    average_train_loss = total_loss / len(train_dataloader)\n    losses.append(average_train_loss)\n    print ('Training loss={:.2f}'.format(average_train_loss))\n    \n    model.eval()\n    eval_accuracy = 0\n    eval_count = 0\n    \n    for batch in valid_dataloader:\n        valid_batch_input = batch[0].to(device)\n        valid_batch_input_types = batch[1].to(device)\n        valid_batch_mask = batch[2].to(device)\n        valid_batch_labels = batch[3].to(device)\n        \n        with torch.no_grad():\n            outputs = model(valid_batch_input, token_type_ids = valid_batch_input_types,\n                           attention_mask = valid_batch_mask)\n        \n        logits = outputs[0]\n        logits = logits.detach().cpu().numpy()\n        label_ids = valid_batch_labels.to('cpu').numpy()\n        batch_accuracy = accuracy(logits, label_ids)\n        eval_accuracy += batch_accuracy\n        eval_count += 1\n\n    print ('Validation accuracy={:.2f}'.format(eval_accuracy / eval_count) )\n    print ('')\n    \n    if i == 4:\n        break\n\nprint (\"Training done\")\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Make submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"model.eval()\nsubmissions = []\nfor batch in test_dataloader:\n    test_batch_input = batch[0].to(device)\n    test_batch_input_types = batch[1].to(device)\n    test_batch_mask = batch[2].to(device)\n    \n    with torch.no_grad():\n        outputs = model(test_batch_input, token_type_ids = test_batch_input_types,\n                       attention_mask = test_batch_mask)\n        \n    logits = outputs[0]\n    submissions.extend(np.argmax(logits.detach().cpu().numpy(), axis=1).flatten())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output = pd.DataFrame({'id': test_df.id,\n                       'prediction': submissions})\noutput.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}