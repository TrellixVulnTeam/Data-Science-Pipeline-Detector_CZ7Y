{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-17T20:13:53.856697Z","iopub.execute_input":"2022-01-17T20:13:53.857058Z","iopub.status.idle":"2022-01-17T20:13:53.890948Z","shell.execute_reply.started":"2022-01-17T20:13:53.856965Z","shell.execute_reply":"2022-01-17T20:13:53.889834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.environ[\"WANDB_API_KEY\"] = \"0\"","metadata":{"execution":{"iopub.status.busy":"2022-01-17T20:13:53.893358Z","iopub.execute_input":"2022-01-17T20:13:53.893891Z","iopub.status.idle":"2022-01-17T20:13:53.900304Z","shell.execute_reply.started":"2022-01-17T20:13:53.893843Z","shell.execute_reply":"2022-01-17T20:13:53.898902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport tensorflow as tf","metadata":{"execution":{"iopub.status.busy":"2022-01-17T20:13:53.902122Z","iopub.execute_input":"2022-01-17T20:13:53.902484Z","iopub.status.idle":"2022-01-17T20:14:00.653443Z","shell.execute_reply.started":"2022-01-17T20:13:53.90243Z","shell.execute_reply":"2022-01-17T20:14:00.652659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(\"../input/contradictory-my-dear-watson/train.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-01-17T20:14:00.655126Z","iopub.execute_input":"2022-01-17T20:14:00.656151Z","iopub.status.idle":"2022-01-17T20:14:00.822076Z","shell.execute_reply.started":"2022-01-17T20:14:00.656112Z","shell.execute_reply":"2022-01-17T20:14:00.821366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-17T20:14:00.823462Z","iopub.execute_input":"2022-01-17T20:14:00.823995Z","iopub.status.idle":"2022-01-17T20:14:00.850784Z","shell.execute_reply.started":"2022-01-17T20:14:00.82395Z","shell.execute_reply":"2022-01-17T20:14:00.848498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install nlp\nfrom nlp import load_dataset","metadata":{"execution":{"iopub.status.busy":"2022-01-17T20:14:00.852025Z","iopub.execute_input":"2022-01-17T20:14:00.852328Z","iopub.status.idle":"2022-01-17T20:14:14.936137Z","shell.execute_reply.started":"2022-01-17T20:14:00.852296Z","shell.execute_reply":"2022-01-17T20:14:14.934857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_mnli(use_validation=True):\n    result = []\n    dataset = load_dataset('multi_nli')\n    print(dataset['train'])\n    keys = ['train', 'validation_matched','validation_mismatched'] if use_validation else ['train']\n    for k in keys:\n        for record in dataset[k]:\n            c1, c2, c3 = record['premise'], record['hypothesis'], record['label']\n            if c1 and c2 and c3 in {0,1,2}:\n                result.append((c1,c2,c3,'en'))\n    result = pd.DataFrame(result, columns=['premise','hypothesis', 'label','lang_abv'])\n    return result","metadata":{"execution":{"iopub.status.busy":"2022-01-17T20:14:14.938161Z","iopub.execute_input":"2022-01-17T20:14:14.938599Z","iopub.status.idle":"2022-01-17T20:14:14.949776Z","shell.execute_reply.started":"2022-01-17T20:14:14.938546Z","shell.execute_reply":"2022-01-17T20:14:14.948373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mnli = load_mnli()","metadata":{"execution":{"iopub.status.busy":"2022-01-17T20:14:14.951693Z","iopub.execute_input":"2022-01-17T20:14:14.952057Z","iopub.status.idle":"2022-01-17T20:14:59.233813Z","shell.execute_reply.started":"2022-01-17T20:14:14.952011Z","shell.execute_reply":"2022-01-17T20:14:59.232843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_train = train[['id', 'premise', 'hypothesis','lang_abv', 'language', 'label']]\ntotal_train","metadata":{"execution":{"iopub.status.busy":"2022-01-17T20:14:59.235138Z","iopub.execute_input":"2022-01-17T20:14:59.235417Z","iopub.status.idle":"2022-01-17T20:14:59.263259Z","shell.execute_reply.started":"2022-01-17T20:14:59.235388Z","shell.execute_reply":"2022-01-17T20:14:59.2625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mnli = mnli[['premise', 'hypothesis', 'lang_abv', 'label']]\nmnli.insert(0, 'language', 'English')\nmnli = mnli[['premise', 'hypothesis', 'lang_abv', 'language', 'label']]\nmnli.insert(0, 'id', 'xxx')\nmnli","metadata":{"execution":{"iopub.status.busy":"2022-01-17T20:14:59.266388Z","iopub.execute_input":"2022-01-17T20:14:59.267162Z","iopub.status.idle":"2022-01-17T20:14:59.4626Z","shell.execute_reply.started":"2022-01-17T20:14:59.26711Z","shell.execute_reply":"2022-01-17T20:14:59.461352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_train = pd.concat([total_train, mnli], axis = 0)\ntotal_train","metadata":{"execution":{"iopub.status.busy":"2022-01-17T20:14:59.46415Z","iopub.execute_input":"2022-01-17T20:14:59.464439Z","iopub.status.idle":"2022-01-17T20:14:59.532617Z","shell.execute_reply.started":"2022-01-17T20:14:59.464408Z","shell.execute_reply":"2022-01-17T20:14:59.531731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_train.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2022-01-17T20:14:59.534046Z","iopub.execute_input":"2022-01-17T20:14:59.534396Z","iopub.status.idle":"2022-01-17T20:14:59.801494Z","shell.execute_reply.started":"2022-01-17T20:14:59.534356Z","shell.execute_reply":"2022-01-17T20:14:59.799887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_train.columns","metadata":{"execution":{"iopub.status.busy":"2022-01-17T20:14:59.803692Z","iopub.execute_input":"2022-01-17T20:14:59.804048Z","iopub.status.idle":"2022-01-17T20:14:59.812898Z","shell.execute_reply.started":"2022-01-17T20:14:59.804006Z","shell.execute_reply":"2022-01-17T20:14:59.811894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels, frequencies = np.unique(total_train.language.values, return_counts = True)\n\nplt.figure(figsize = (10,10))\nplt.barh(labels, frequencies)\nplt.xlabel(\"Languages\")\nplt.ylabel(\"percentage\")\nplt.title(\"Language distribution\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-17T20:14:59.814627Z","iopub.execute_input":"2022-01-17T20:14:59.815205Z","iopub.status.idle":"2022-01-17T20:15:00.738108Z","shell.execute_reply.started":"2022-01-17T20:14:59.815161Z","shell.execute_reply":"2022-01-17T20:15:00.737378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_train.label.value_counts().plot(kind='bar', color=['#003049', '#d62828', '#f77f00'])\nplt.xlabel(\"Classes\")\nplt.ylabel(\"nbr of occurences\")\nplt.title(\"Classes distribution\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-17T20:15:00.74149Z","iopub.execute_input":"2022-01-17T20:15:00.741784Z","iopub.status.idle":"2022-01-17T20:15:00.977298Z","shell.execute_reply.started":"2022-01-17T20:15:00.741753Z","shell.execute_reply":"2022-01-17T20:15:00.976368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_train.describe(include='all')","metadata":{"execution":{"iopub.status.busy":"2022-01-17T20:15:00.97888Z","iopub.execute_input":"2022-01-17T20:15:00.979187Z","iopub.status.idle":"2022-01-17T20:15:02.368191Z","shell.execute_reply.started":"2022-01-17T20:15:00.979148Z","shell.execute_reply":"2022-01-17T20:15:02.367073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels, frequencies = np.unique(train.language.values, return_counts = True)\n\nplt.figure(figsize = (10,10))\nplt.pie(frequencies,labels = labels, autopct = '%1.1f%%')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-17T20:15:02.36944Z","iopub.execute_input":"2022-01-17T20:15:02.369684Z","iopub.status.idle":"2022-01-17T20:15:02.652411Z","shell.execute_reply.started":"2022-01-17T20:15:02.369657Z","shell.execute_reply":"2022-01-17T20:15:02.65141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\n#explore the distribution of classes and languages\nfig, ax = plt.subplots(figsize = (12,5))\n\n#for maximum aesthetics\npalette = sns.cubehelix_palette(8, start=2, rot=0, dark=0, light=.95, reverse=True)\n\ngraph1 = sns.countplot(train['language'], hue = train['label'])#, palette = palette)\n\n#set title\ngraph1.set_title('Distribution of Languages and Labels')\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-17T20:15:02.654135Z","iopub.execute_input":"2022-01-17T20:15:02.654573Z","iopub.status.idle":"2022-01-17T20:15:03.98685Z","shell.execute_reply.started":"2022-01-17T20:15:02.654525Z","shell.execute_reply":"2022-01-17T20:15:03.985768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import BertTokenizer, TFBertModel, TFAutoModel,AutoTokenizer\n\n#model_name = \"bert-base-multilingual-cased\"\n#tokenizer = BertTokenizer.from_pretrained(model_name) # FC: this is the tokenizer we will use on our text data to tokenize it\n\nmodel_name = \"joeddav/xlm-roberta-large-xnli\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)","metadata":{"execution":{"iopub.status.busy":"2022-01-17T20:15:03.988656Z","iopub.execute_input":"2022-01-17T20:15:03.988902Z","iopub.status.idle":"2022-01-17T20:15:09.802084Z","shell.execute_reply.started":"2022-01-17T20:15:03.988874Z","shell.execute_reply":"2022-01-17T20:15:09.801112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def encode_sentence(s):\n   tokens = list(tokenizer.tokenize(s)) # FC: split the sentence into tokens that are either words or sub-words\n   tokens.append('[SEP]') # FC: a token called [SEP] (=separator) is added to mark end of each sentence\n   return tokenizer.convert_tokens_to_ids(tokens) # FC: instead of returning the list of tokens, a list of each token ID is returned","metadata":{"execution":{"iopub.status.busy":"2022-01-17T20:15:09.803575Z","iopub.execute_input":"2022-01-17T20:15:09.803877Z","iopub.status.idle":"2022-01-17T20:15:09.809491Z","shell.execute_reply.started":"2022-01-17T20:15:09.80384Z","shell.execute_reply":"2022-01-17T20:15:09.808348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def bert_encode(hypotheses, premises, tokenizer): # FC: for RoBERTa we remove the input_type_ids from the inputs of the model\n    \n  num_examples = len(hypotheses)\n  \n  sentence1 = tf.ragged.constant([   # FC: constructs a constant ragged tensor. every entry has a different length\n      encode_sentence(s) for s in np.array(hypotheses)])\n  \n  sentence2 = tf.ragged.constant([\n      encode_sentence(s) for s in np.array(premises)])\n  \n  cls = [tokenizer.convert_tokens_to_ids(['[CLS]'])]*sentence1.shape[0] # FC: list of IDs for the token '[CLS]' to denote each beginning\n  \n  input_word_ids = tf.concat([cls, sentence1, sentence2], axis=-1) # FC: put everything together. every row still has a different length.\n  \n  #input_word_ids2 = tf.concat([cls, sentence2, sentence1], axis=-1)\n  \n  #input_word_ids = tf.concat([input_word_ids1, input_word_ids2], axis=0) # we duplicate the dataset inverting sentence 1 and 2\n    \n  input_mask = tf.ones_like(input_word_ids).to_tensor() # FC: first, a tensor with just ones in it is constructed in the same size as input_word_ids. Then, by applying to_tensor the ends of each row are padded with zeros to give every row the same length\n\n  # type is not need for the RoBERTa model it will not be include in the output of this function\n  type_cls = tf.zeros_like(cls) # FC: creates a tensor same shape as cls with only zeros in it\n  \n  type_s1 = tf.zeros_like(sentence1)\n  \n  type_s2 = tf.ones_like(sentence2) # FC: creates a tensor same shape as sentence2 with only ones in it to mark the 2nd sentence\n  \n  input_type_ids = tf.concat(\n      [type_cls, type_s1, type_s2], axis=-1).to_tensor() # FC: concatenates everything and again adds padding \n  \n  inputs = {\n      'input_word_ids': input_word_ids.to_tensor(), # FC: input_word_ids hasn't been padded yet - do it here now\n      'input_mask': input_mask\n      \n      #,'input_type_ids': input_type_ids\n  }\n\n  return inputs","metadata":{"execution":{"iopub.status.busy":"2022-01-17T20:15:09.810939Z","iopub.execute_input":"2022-01-17T20:15:09.811231Z","iopub.status.idle":"2022-01-17T20:15:09.83606Z","shell.execute_reply.started":"2022-01-17T20:15:09.811184Z","shell.execute_reply":"2022-01-17T20:15:09.835329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_input = bert_encode(train.premise.values, train.hypothesis.values, tokenizer)","metadata":{"execution":{"iopub.status.busy":"2022-01-17T20:15:09.837102Z","iopub.execute_input":"2022-01-17T20:15:09.837498Z","iopub.status.idle":"2022-01-17T20:15:18.028366Z","shell.execute_reply.started":"2022-01-17T20:15:09.837462Z","shell.execute_reply":"2022-01-17T20:15:18.027499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_train_input = bert_encode(total_train.premise.values, total_train.hypothesis.values, tokenizer)","metadata":{"execution":{"iopub.status.busy":"2022-01-17T20:15:18.029579Z","iopub.execute_input":"2022-01-17T20:15:18.029832Z","iopub.status.idle":"2022-01-17T20:19:44.042428Z","shell.execute_reply.started":"2022-01-17T20:15:18.029802Z","shell.execute_reply":"2022-01-17T20:19:44.041308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n\n# Fixing random state for reproducibility\nnp.random.seed(19680801)\n\n\nx = np.count_nonzero(train_input['input_word_ids'], axis=1)\n\n# the histogram of the data\nn, bins, patches = plt.hist(x, 50, density=True, facecolor='b', alpha=0.75)\n\n\nplt.xlabel('input word lenght')\nplt.ylabel('Probability')\nplt.title('Distribution of word length on the train set')\nplt.text(60, .021, r'max_length=245')\nplt.xlim(0, 250)\n#plt.ylim(0, 0.03)\nplt.grid(True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-17T20:19:44.044328Z","iopub.execute_input":"2022-01-17T20:19:44.044568Z","iopub.status.idle":"2022-01-17T20:19:44.362283Z","shell.execute_reply.started":"2022-01-17T20:19:44.044541Z","shell.execute_reply":"2022-01-17T20:19:44.361336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n\n# Fixing random state for reproducibility\nnp.random.seed(19680801)\n\n\nx = np.count_nonzero(total_train_input['input_word_ids'], axis=1)\n\n# the histogram of the data\nn, bins, patches = plt.hist(x, 50, density=True, facecolor='b', alpha=0.75)\n\n\nplt.xlabel('input word lenght')\nplt.ylabel('Probability')\nplt.title('Distribution of word length on the test set')\nplt.text(60, .021, r'max_length=236')\nplt.xlim(0, 250)\n#plt.ylim(0, 0.03)\nplt.grid(True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-17T20:19:44.363744Z","iopub.execute_input":"2022-01-17T20:19:44.363972Z","iopub.status.idle":"2022-01-17T20:19:45.075575Z","shell.execute_reply.started":"2022-01-17T20:19:44.363945Z","shell.execute_reply":"2022-01-17T20:19:45.074553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_len = 236 #: FC 50 in the initial tutorial\n\ndef build_model():\n    #encoder = TFBertModel.from_pretrained(model_name) \n    # FC: constructs a RoBERTa model pre-trained on the above described language model 'xlm-roberta-large-xnli'\n    encoder = TFAutoModel.from_pretrained('joeddav/xlm-roberta-large-xnli')\n    # FC: now we adjust the model so that it can accept our input by telling the model what the input looks like:\n    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\") # FC: tf.keras.Input constructs a symbolic tensor object whith certain attributes: \"shape\" tells it that the expected input will be in batches of max_len-dimensional vectors; \"dtype\" tells it that the data type will be int32; \"name\" will be the name string for the input layer\n    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\") # FC: repeat the same for the other two input variables\n    # FC: the input type is only needed for the BERT model\n    #input_type_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_type_ids\")\n    \n    # FC: now follows, what we want to happen with our input:\n    # FC: first, our input goes into the BERT model bert_encoder. It will return a tuple and the contextualized embeddings that we need are stored in the first element of that tuple\n    embedding = encoder([input_word_ids, input_mask])[0] # FC: add_input_type_ids for the BERT model\n    # FC: we only need the output corresponding to the first token [CLS], which is a 2D-tensor with size (#sentence pairs, 768) and is accessd with embedding[:,0,:]. This will be input for our classifier, which is a regular densely-connected neural network constructed through tf.keras.layers.Dense. The inputs mean: \"3\" is the dimensionality of the output space, which means that the output has shape (#sentence pairs,3). More practically speaking, for each sentence pair that we input, the output will have 3 probability values for each of the 3 possible labels (entailment, neutral, contradiction). They will be in range(0,1) and add up to 1; \"activation\" denotes the activation function, in this case 'softmax', which connects a real vector to a vector of categorical possibilities.\n    \n    # I tried to put another layer put it doesn't help in performance\n    #output = tf.keras.layers.Dense(10, activation='softmax')(embedding[:,0,:]) #FC: no need of a GlobalAveragePooling for BERT\n    \n    output = tf.keras.layers.Dense(3, activation='softmax')(embedding[:,0,:])\n    # FC: we also have the posibility of making a globalAveragepooling of all the embeddings, but the resuls are not better \n    #output = tf.keras.layers.GlobalAveragePooling1D()(embedding) \n    #output = tf.keras.layers.Dense(3, activation='softmax')(output) \n    \n       \n    model = tf.keras.Model(inputs=[input_word_ids, input_mask], outputs=output) # FC: based on the code in the lines above, a model is now constructed and passed into the variable model\n    model.compile(tf.keras.optimizers.Adam(lr=1e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy']) # FC: we tell the model how we want it to train and evaluate: \"tf.keras.optimizers.Adam\": use an optimizer that implements the Adam algorithm. \"lr\" denotes the learning rate; \"loss\" denotes the loss function to use; \"metrics\" specifies which kind of metrics to use for training and testing\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2022-01-17T20:19:45.076994Z","iopub.execute_input":"2022-01-17T20:19:45.077516Z","iopub.status.idle":"2022-01-17T20:19:45.089482Z","shell.execute_reply.started":"2022-01-17T20:19:45.077478Z","shell.execute_reply":"2022-01-17T20:19:45.088376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"try:\n    # detect and init the TPU\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver() # FC: detect and init the TPU: TPUClusterResolver() locates the TPUs on the network\n    # instantiate a distribution strategy\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    \n    strategy = tf.distribute.experimental.TPUStrategy(tpu) # FC: \"strategy\" contains the necessary distributed training code that will work on the TPUs\nexcept ValueError: # FC: in case Accelerator is not set to TPU in the Notebook Settings\n    strategy = tf.distribute.get_strategy() # for CPU and single GPU\n    print('Number of replicas:', strategy.num_replicas_in_sync) # FC: returns the number of cores","metadata":{"execution":{"iopub.status.busy":"2022-01-17T20:19:45.09172Z","iopub.execute_input":"2022-01-17T20:19:45.09244Z","iopub.status.idle":"2022-01-17T20:19:51.187551Z","shell.execute_reply.started":"2022-01-17T20:19:45.09239Z","shell.execute_reply":"2022-01-17T20:19:51.186772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"try:\n  tpu = tf.distribute.cluster_resolver.TPUClusterResolver() # TPU detection\nexcept ValueError:\n  tpu = None\n  gpus = tf.config.experimental.list_logical_devices(\"GPU\")\n\nif tpu:\n  tf.tpu.experimental.initialize_tpu_system(tpu)\n  strategy = tf.distribute.experimental.TPUStrategy(tpu,) # Going back and forth between TPU and host is expensive. Better to run 128 batches on the TPU before reporting back.\n  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\nelif len(gpus) > 1:\n  strategy = tf.distribute.MirroredStrategy([gpu.name for gpu in gpus])\n  print('Running on multiple GPUs ', [gpu.name for gpu in gpus])\nelif len(gpus) == 1:\n  strategy = tf.distribute.get_strategy() # default strategy that works on CPU and single GPU\n  print('Running on single GPU ', gpus[0].name)\nelse:\n  strategy = tf.distribute.get_strategy() # default strategy that works on CPU and single GPU\n  print('Running on CPU')\nprint(\"Number of accelerators: \", strategy.num_replicas_in_sync)","metadata":{"execution":{"iopub.status.busy":"2022-01-17T20:19:51.192293Z","iopub.execute_input":"2022-01-17T20:19:51.192588Z","iopub.status.idle":"2022-01-17T20:19:59.376522Z","shell.execute_reply.started":"2022-01-17T20:19:51.192556Z","shell.execute_reply":"2022-01-17T20:19:59.375532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# instantiating the model in the strategy scope creates the model on the TPU\n\nwith strategy.scope(): # FC: defines the compute distribution policy for building the model. or in other words: makes sure that the model is created on the TPU/GPU/CPU, depending on to what the Accelerator is set in the Notebook Settings\n    model = build_model() # FC: our model is being built\n    model.summary()       # FC: let's look at some of its properties\n\ntf.keras.utils.plot_model(model, \"my_model.png\", show_shapes=True) # FC: I added this line because it gives a nice visualization showing the individual components of our model","metadata":{"execution":{"iopub.status.busy":"2022-01-17T20:19:59.377651Z","iopub.execute_input":"2022-01-17T20:19:59.377882Z","iopub.status.idle":"2022-01-17T20:22:14.462418Z","shell.execute_reply.started":"2022-01-17T20:19:59.377855Z","shell.execute_reply":"2022-01-17T20:22:14.461312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We can freeze the RoBERTa weights in order to save some time\nprint(model.layers[2])\nmodel.layers[2].trainable=True","metadata":{"execution":{"iopub.status.busy":"2022-01-17T20:22:14.464746Z","iopub.execute_input":"2022-01-17T20:22:14.465022Z","iopub.status.idle":"2022-01-17T20:22:14.492494Z","shell.execute_reply.started":"2022-01-17T20:22:14.464989Z","shell.execute_reply":"2022-01-17T20:22:14.491255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We need to put the train set with the same size of the model\nfor key in train_input.keys():\n    train_input[key] = train_input[key][:,:max_len]","metadata":{"execution":{"iopub.status.busy":"2022-01-17T20:22:14.494389Z","iopub.execute_input":"2022-01-17T20:22:14.495476Z","iopub.status.idle":"2022-01-17T20:22:14.526602Z","shell.execute_reply.started":"2022-01-17T20:22:14.495422Z","shell.execute_reply":"2022-01-17T20:22:14.522463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We need to put the train set with the same size of the model\nfor key in total_train_input.keys():\n    total_train_input[key] = total_train_input[key][:,:max_len]","metadata":{"execution":{"iopub.status.busy":"2022-01-17T20:22:14.529048Z","iopub.execute_input":"2022-01-17T20:22:14.530654Z","iopub.status.idle":"2022-01-17T20:22:15.958721Z","shell.execute_reply.started":"2022-01-17T20:22:14.530603Z","shell.execute_reply":"2022-01-17T20:22:15.956388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"early_stop = tf.keras.callbacks.EarlyStopping(patience=3,restore_best_weights=True)\n# FC: make sure that TPU in Accelerator under Notebook Settings is turned on so that model trains on the TPU. Otherwise this line will crash\nmodel.fit(total_train_input, total_train.label.values, epochs = 10, verbose = 1, validation_split = 0.01,\n         batch_size=16*strategy.num_replicas_in_sync\n          ,callbacks=[early_stop]\n         ) # FC: now we fit the model to our training data that we prepared before. The number of training epochs is 2, verbose = 1 shows progress bar, # of rows in each batch is 64, and 20% of the data is used for validation","metadata":{"execution":{"iopub.status.busy":"2022-01-17T20:22:15.960555Z","iopub.execute_input":"2022-01-17T20:22:15.960906Z","iopub.status.idle":"2022-01-17T21:51:10.524937Z","shell.execute_reply.started":"2022-01-17T20:22:15.960864Z","shell.execute_reply":"2022-01-17T21:51:10.523902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv(\"../input/contradictory-my-dear-watson/test.csv\")\ntest_input = bert_encode(test.premise.values, test.hypothesis.values, tokenizer)","metadata":{"execution":{"iopub.status.busy":"2022-01-17T21:51:10.526786Z","iopub.execute_input":"2022-01-17T21:51:10.527137Z","iopub.status.idle":"2022-01-17T21:51:14.100236Z","shell.execute_reply.started":"2022-01-17T21:51:10.527092Z","shell.execute_reply":"2022-01-17T21:51:14.099279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# same for the test set we need to put it in the same size of the model\nfor key in test_input.keys():\n    test_input[key] = test_input[key][:,:max_len]","metadata":{"execution":{"iopub.status.busy":"2022-01-17T21:51:14.102011Z","iopub.execute_input":"2022-01-17T21:51:14.10236Z","iopub.status.idle":"2022-01-17T21:51:14.111044Z","shell.execute_reply.started":"2022-01-17T21:51:14.102317Z","shell.execute_reply":"2022-01-17T21:51:14.109868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-17T21:51:14.112443Z","iopub.execute_input":"2022-01-17T21:51:14.112696Z","iopub.status.idle":"2022-01-17T21:51:14.140908Z","shell.execute_reply.started":"2022-01-17T21:51:14.112667Z","shell.execute_reply":"2022-01-17T21:51:14.139775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = [np.argmax(i) for i in model.predict(test_input)]","metadata":{"execution":{"iopub.status.busy":"2022-01-17T21:51:14.142359Z","iopub.execute_input":"2022-01-17T21:51:14.142634Z","iopub.status.idle":"2022-01-17T21:51:48.462126Z","shell.execute_reply.started":"2022-01-17T21:51:14.142602Z","shell.execute_reply":"2022-01-17T21:51:48.461013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = test.id.copy().to_frame()\nsubmission['prediction'] = predictions","metadata":{"execution":{"iopub.status.busy":"2022-01-17T21:51:48.464098Z","iopub.execute_input":"2022-01-17T21:51:48.464496Z","iopub.status.idle":"2022-01-17T21:51:48.477713Z","shell.execute_reply.started":"2022-01-17T21:51:48.464443Z","shell.execute_reply":"2022-01-17T21:51:48.476957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-17T21:51:48.478979Z","iopub.execute_input":"2022-01-17T21:51:48.479733Z","iopub.status.idle":"2022-01-17T21:51:48.497739Z","shell.execute_reply.started":"2022-01-17T21:51:48.479694Z","shell.execute_reply":"2022-01-17T21:51:48.496885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv(\"submission.csv\", index = False)","metadata":{"execution":{"iopub.status.busy":"2022-01-17T21:51:48.498979Z","iopub.execute_input":"2022-01-17T21:51:48.499324Z","iopub.status.idle":"2022-01-17T21:51:48.528555Z","shell.execute_reply.started":"2022-01-17T21:51:48.499289Z","shell.execute_reply":"2022-01-17T21:51:48.527357Z"},"trusted":true},"execution_count":null,"outputs":[]}]}