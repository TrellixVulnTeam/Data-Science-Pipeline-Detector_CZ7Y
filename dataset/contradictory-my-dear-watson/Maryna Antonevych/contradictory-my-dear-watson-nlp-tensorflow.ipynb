{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Contradictory, My Dear Watson\nHello, everyone! In this my new notebook we are going to solve the [\"Contradictory, My Dear Watson\"](https://www.kaggle.com/c/contradictory-my-dear-watson) problem. \n\n> \"â€¦when you have eliminated the impossible, whatever remains, however improbable, must be the truth\" - Sir Arthur Conan Doyle\n\n\n#### References\nFor this notebook I would like to say thank you some authors for their notebooks that have inspired me to write own notebook:\n1. [ANA SOFIA UZSOY. Tutorial Notebook](https://www.kaggle.com/anasofiauzsoy/tutorial-notebook)\n\n#### Natural language processing (NLP) \nNatural language processing (NLP) has grown increasingly elaborate over the past few years. Machine learning models tackle question answering, text extraction, sentence generation, and many other complex tasks. But, can machines determine the relationships between sentences, or is that still left to humans? If NLP can be applied between sentences, this could have profound implications for fact-checking, identifying fake news, analyzing text, and much more.\n\n![detective](https://cdn0.iconfinder.com/data/icons/streamline-emoji-1/48/192-detective-2-512.png)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"# 1. Problem Formulation\nOur task is to create an NLI model that assigns labels of 0, 1, or 2 (corresponding to entailment, neutral, and contradiction) to pairs of premises and hypotheses. To make things more interesting, the train and test set include text in fifteen different languages! ","metadata":{}},{"cell_type":"markdown","source":"# 2. Import libraries","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nfrom transformers import BertTokenizer, TFBertModel\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf","metadata":{"execution":{"iopub.status.busy":"2021-12-03T21:23:00.791144Z","iopub.execute_input":"2021-12-03T21:23:00.791456Z","iopub.status.idle":"2021-12-03T21:23:00.800673Z","shell.execute_reply.started":"2021-12-03T21:23:00.791425Z","shell.execute_reply":"2021-12-03T21:23:00.799672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.environ[\"WANDB_API_KEY\"] = \"0\" # to silence warning","metadata":{"execution":{"iopub.status.busy":"2021-12-03T21:23:00.802759Z","iopub.execute_input":"2021-12-03T21:23:00.803297Z","iopub.status.idle":"2021-12-03T21:23:00.810611Z","shell.execute_reply.started":"2021-12-03T21:23:00.803254Z","shell.execute_reply":"2021-12-03T21:23:00.809647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Turn on TPU\nTPUs are powerful hardware accelerators specialized in deep learning tasks, including Natural Language Processing. Kaggle provides all users TPU Quota at no cost, which you can use to explore this competition. ","metadata":{}},{"cell_type":"code","source":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept ValueError:\n    strategy = tf.distribute.get_strategy() # for CPU and single GPU\n    print('Number of replicas:', strategy.num_replicas_in_sync)","metadata":{"execution":{"iopub.status.busy":"2021-12-03T21:23:00.812003Z","iopub.execute_input":"2021-12-03T21:23:00.812791Z","iopub.status.idle":"2021-12-03T21:23:08.536954Z","shell.execute_reply.started":"2021-12-03T21:23:00.812748Z","shell.execute_reply":"2021-12-03T21:23:08.53613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Read data","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv(\"../input/contradictory-my-dear-watson/train.csv\")\ndf_test = pd.read_csv(\"../input/contradictory-my-dear-watson/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-12-03T21:23:08.539541Z","iopub.execute_input":"2021-12-03T21:23:08.539842Z","iopub.status.idle":"2021-12-03T21:23:08.627146Z","shell.execute_reply.started":"2021-12-03T21:23:08.539804Z","shell.execute_reply":"2021-12-03T21:23:08.626532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head(7) # check data (first 7 rows)","metadata":{"execution":{"iopub.status.busy":"2021-12-03T21:23:08.628044Z","iopub.execute_input":"2021-12-03T21:23:08.62884Z","iopub.status.idle":"2021-12-03T21:23:08.641092Z","shell.execute_reply.started":"2021-12-03T21:23:08.628805Z","shell.execute_reply":"2021-12-03T21:23:08.640533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.head(7) # check data (first 7 rows)","metadata":{"execution":{"iopub.status.busy":"2021-12-03T21:23:08.642955Z","iopub.execute_input":"2021-12-03T21:23:08.643186Z","iopub.status.idle":"2021-12-03T21:23:08.65888Z","shell.execute_reply.started":"2021-12-03T21:23:08.643161Z","shell.execute_reply":"2021-12-03T21:23:08.658302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.shape # check data","metadata":{"execution":{"iopub.status.busy":"2021-12-03T21:23:08.659746Z","iopub.execute_input":"2021-12-03T21:23:08.660376Z","iopub.status.idle":"2021-12-03T21:23:08.671827Z","shell.execute_reply.started":"2021-12-03T21:23:08.660348Z","shell.execute_reply":"2021-12-03T21:23:08.670996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.shape # check data","metadata":{"execution":{"iopub.status.busy":"2021-12-03T21:23:08.673169Z","iopub.execute_input":"2021-12-03T21:23:08.673436Z","iopub.status.idle":"2021-12-03T21:23:08.683725Z","shell.execute_reply.started":"2021-12-03T21:23:08.673409Z","shell.execute_reply":"2021-12-03T21:23:08.682838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's check some rows more in more detailed way. As example, I'll use the row with 0 index.\n\n- **Premise** - a previous statement or proposition from which another is inferred or follows as a conclusion.\n- **Hypothesis** - a supposition or proposed explanation made on the basis of limited evidence as a starting point for further investigation.","metadata":{}},{"cell_type":"code","source":"# \"premise\" column\ndf_train[\"premise\"].values[0]","metadata":{"execution":{"iopub.status.busy":"2021-12-03T21:23:08.685504Z","iopub.execute_input":"2021-12-03T21:23:08.685804Z","iopub.status.idle":"2021-12-03T21:23:08.696534Z","shell.execute_reply.started":"2021-12-03T21:23:08.685766Z","shell.execute_reply":"2021-12-03T21:23:08.695694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# \"hypothesis\" column\ndf_train[\"hypothesis\"].values[0]","metadata":{"execution":{"iopub.status.busy":"2021-12-03T21:23:08.700057Z","iopub.execute_input":"2021-12-03T21:23:08.700438Z","iopub.status.idle":"2021-12-03T21:23:08.70827Z","shell.execute_reply.started":"2021-12-03T21:23:08.700407Z","shell.execute_reply":"2021-12-03T21:23:08.707392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Vizualize data\nHere we can build some plots and visualizations.","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(20,10))\ncount_classes = df_train['language'].value_counts() # count sentenses in each language\nplt.title(\"Languages in df_train\".upper())\ncolors = ['#ff9999','#66b3ff','#99ff99',\n          '#ffcc99', '#ffccf9', '#ff99f8', \n          '#ff99af', '#ffe299', '#a8ff99',\n          '#cc99ff', '#9e99ff', '#99c9ff',\n          '#99f5ff', '#99ffe4', '#99ffaf']\n\nplt.pie(count_classes, \n        labels = count_classes.index, \n        autopct='%1.1f%%',\n        shadow=True, \n        startangle=90, \n        colors=colors)\n\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-03T21:23:08.709751Z","iopub.execute_input":"2021-12-03T21:23:08.710069Z","iopub.status.idle":"2021-12-03T21:23:09.034238Z","shell.execute_reply.started":"2021-12-03T21:23:08.709972Z","shell.execute_reply":"2021-12-03T21:23:09.033322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After that, we can build some more stats:","metadata":{}},{"cell_type":"code","source":"def plot_stats(df, column, ax, color, angle):\n    \"\"\" PLOT STATS OF DIFFERENT COLUMNS \"\"\"\n    count_classes = df[column].value_counts()\n    ax = sns.barplot(x=count_classes.index, y=count_classes, ax=ax, palette=color)\n    ax.set_title(column.upper(), fontsize=18)\n    for tick in ax.get_xticklabels():\n        tick.set_rotation(angle)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-03T21:23:09.035549Z","iopub.execute_input":"2021-12-03T21:23:09.035866Z","iopub.status.idle":"2021-12-03T21:23:09.042779Z","shell.execute_reply.started":"2021-12-03T21:23:09.035826Z","shell.execute_reply":"2021-12-03T21:23:09.041838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we can see stats for **\"label\"**:","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(15,5))\nfig.autofmt_xdate()\nplot_stats(df_train, \"label\", axes, \"viridis\", 45)\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-03T21:23:09.044453Z","iopub.execute_input":"2021-12-03T21:23:09.044749Z","iopub.status.idle":"2021-12-03T21:23:09.253081Z","shell.execute_reply.started":"2021-12-03T21:23:09.044707Z","shell.execute_reply":"2021-12-03T21:23:09.252239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Download pretrained models\nTo start out, we can use a pretrained model. Here, we'll use a multilingual BERT model from huggingface. For more information about BERT, see [here](https://github.com/google-research/bert/blob/master/multilingual.md).","metadata":{}},{"cell_type":"markdown","source":"## 4.1 Download tokenizer\nTokenizers turn sequences of words into arrays of numbers. ","metadata":{}},{"cell_type":"code","source":"model_name = 'bert-base-multilingual-cased'\ntokenizer = BertTokenizer.from_pretrained(model_name)","metadata":{"execution":{"iopub.status.busy":"2021-12-03T21:23:09.254774Z","iopub.execute_input":"2021-12-03T21:23:09.255083Z","iopub.status.idle":"2021-12-03T21:23:10.427066Z","shell.execute_reply.started":"2021-12-03T21:23:09.255044Z","shell.execute_reply":"2021-12-03T21:23:10.426194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(tokenizer.vocab) # check the vocabulary size","metadata":{"execution":{"iopub.status.busy":"2021-12-03T21:23:10.428302Z","iopub.execute_input":"2021-12-03T21:23:10.428547Z","iopub.status.idle":"2021-12-03T21:23:10.43441Z","shell.execute_reply.started":"2021-12-03T21:23:10.428519Z","shell.execute_reply":"2021-12-03T21:23:10.433584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's check the tokenizer. As example, we can take the sentence **\"you know they can't really defend themselves\"**.\n\nThe model expects its two inputs sentences to be concatenated together. This input is expected to start with a [CLS] \"This is a classification problem\" token, and each sentence should end with a [SEP] \"Separator\" token.","metadata":{}},{"cell_type":"code","source":"def encode_sentence(s):\n    \"\"\" ENCODE SENTENCES WITH TOKENIZER\"\"\"\n    tokens = list(tokenizer.tokenize(s))\n    tokens.append('[SEP]')\n    return tokenizer.convert_tokens_to_ids(tokens)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-03T21:23:10.435709Z","iopub.execute_input":"2021-12-03T21:23:10.43655Z","iopub.status.idle":"2021-12-03T21:23:10.444664Z","shell.execute_reply.started":"2021-12-03T21:23:10.436517Z","shell.execute_reply":"2021-12-03T21:23:10.443773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encode_sentence(\"you know they can't really defend themselves\")","metadata":{"execution":{"iopub.status.busy":"2021-12-03T21:23:10.446069Z","iopub.execute_input":"2021-12-03T21:23:10.446418Z","iopub.status.idle":"2021-12-03T21:23:10.457541Z","shell.execute_reply.started":"2021-12-03T21:23:10.446379Z","shell.execute_reply":"2021-12-03T21:23:10.456876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.2 BERT\n[BERT](https://huggingface.co/transformers/model_doc/bert.html#tfbertmodel) **uses three kind of input data** - input word IDs, input masks, and input type IDs.\n\nThese allow the model to know that **the premise and hypothesis are distinct sentences**, and also to ignore any padding from the tokenizer.\n\nWe add a [CLS] token to denote **the beginning of the inputs**, and a [SEP] token to denote the separation between **the premise and the hypothesis**. \n\nWe also **need to pad** all of the inputs to be the same size.\n\nNow, we're going to **encode all of our premise/hypothesis pairs** for input into BERT.","metadata":{}},{"cell_type":"code","source":"def bert_encode(hypotheses, premises, tokenizer):\n    \"\"\" ENCODE DATA FOR BERT\"\"\"\n    num_examples = len(hypotheses)\n    print(\"num_examples = \", num_examples)\n    sentence1 = tf.ragged.constant([encode_sentence(s) for s in np.array(hypotheses)])\n    print(\"sentence1.shape = \", sentence1.shape)\n    sentence2 = tf.ragged.constant([encode_sentence(s) for s in np.array(premises)])\n    print(\"sentence2.shape = \", sentence2.shape)\n    cls_ = [tokenizer.convert_tokens_to_ids(['[CLS]'])] * sentence1.shape[0]\n    input_word_ids = tf.concat([cls_, sentence1, sentence2], axis=-1)\n    print(\"input_word_ids.shape = \", input_word_ids.shape)\n    # 300 - as my example\n    # because we have train_input (12120; 259), test_input (5159; 234)\n    # and shape[1] should be the same in each dataset\n    # that is why we creating (xxx; 300) shape in to_tensor() functions  \n    input_mask = tf.ones_like(input_word_ids).to_tensor(shape=(input_word_ids.shape[0], 300)) \n    print(\"input_mask.shape = \", input_mask.shape)\n    \n    type_cls = tf.zeros_like(cls_)\n    type_s1 = tf.zeros_like(sentence1)\n    type_s2 = tf.ones_like(sentence2)\n    \n    input_type_ids = tf.concat([type_cls, type_s1, type_s2], axis=-1).to_tensor(shape=(input_word_ids.shape[0], 300))\n    \n    inputs = {'input_word_ids': input_word_ids.to_tensor(shape=(input_word_ids.shape[0], 300)),\n              'input_mask': input_mask,\n              'input_type_ids': input_type_ids}\n    print()\n    \n    return inputs\n    ","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-03T21:27:51.83612Z","iopub.execute_input":"2021-12-03T21:27:51.837138Z","iopub.status.idle":"2021-12-03T21:27:51.849161Z","shell.execute_reply.started":"2021-12-03T21:27:51.837088Z","shell.execute_reply":"2021-12-03T21:27:51.848232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# encode data\ntrain_input = bert_encode(df_train[\"premise\"].values, df_train[\"hypothesis\"].values, tokenizer)\ntest_input = bert_encode(df_test[\"premise\"].values, df_test[\"hypothesis\"].values, tokenizer)","metadata":{"execution":{"iopub.status.busy":"2021-12-03T21:23:10.471051Z","iopub.execute_input":"2021-12-03T21:23:10.47144Z","iopub.status.idle":"2021-12-03T21:23:31.231089Z","shell.execute_reply.started":"2021-12-03T21:23:10.471407Z","shell.execute_reply":"2021-12-03T21:23:31.230357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_input # check train input","metadata":{"execution":{"iopub.status.busy":"2021-12-03T21:23:31.232404Z","iopub.execute_input":"2021-12-03T21:23:31.232646Z","iopub.status.idle":"2021-12-03T21:23:31.283808Z","shell.execute_reply.started":"2021-12-03T21:23:31.232608Z","shell.execute_reply":"2021-12-03T21:23:31.282964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_input # check test input","metadata":{"execution":{"iopub.status.busy":"2021-12-03T21:23:31.285092Z","iopub.execute_input":"2021-12-03T21:23:31.28533Z","iopub.status.idle":"2021-12-03T21:23:31.311399Z","shell.execute_reply.started":"2021-12-03T21:23:31.285304Z","shell.execute_reply":"2021-12-03T21:23:31.310266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Train Neural Network Model\nNow, we can incorporate the BERT transformer into a Keras Functional Model. ","metadata":{}},{"cell_type":"code","source":"max_len = train_input[\"input_word_ids\"].shape[1]\n\ndef create_model():\n    \"\"\" BUILD MODEL \"\"\"\n    bert_encoder = TFBertModel.from_pretrained(model_name)\n    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    input_type_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_type_ids\")\n\n    embedding = bert_encoder([input_word_ids, input_mask, input_type_ids])[0]\n    output = tf.keras.layers.Dense(3, activation='softmax')(embedding[:,0,:])\n\n    model = tf.keras.Model(inputs=[input_word_ids, input_mask, input_type_ids], outputs=output)\n    model.compile(tf.keras.optimizers.Adam(lr=1e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-12-03T21:23:31.312531Z","iopub.execute_input":"2021-12-03T21:23:31.312743Z","iopub.status.idle":"2021-12-03T21:23:31.322509Z","shell.execute_reply.started":"2021-12-03T21:23:31.312718Z","shell.execute_reply":"2021-12-03T21:23:31.321519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with strategy.scope():\n    model = create_model()\n    model.summary()","metadata":{"execution":{"iopub.status.busy":"2021-12-03T21:23:31.324006Z","iopub.execute_input":"2021-12-03T21:23:31.324869Z","iopub.status.idle":"2021-12-03T21:23:49.213394Z","shell.execute_reply.started":"2021-12-03T21:23:31.324824Z","shell.execute_reply":"2021-12-03T21:23:49.212526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_history = model.fit(train_input, \n                          df_train[\"label\"].values, \n                          epochs = 20, \n                          verbose = 1,\n                          batch_size = 128, \n                          validation_split = 0.2)","metadata":{"execution":{"iopub.status.busy":"2021-12-03T21:23:49.214875Z","iopub.execute_input":"2021-12-03T21:23:49.215365Z","iopub.status.idle":"2021-12-03T21:26:38.808184Z","shell.execute_reply.started":"2021-12-03T21:23:49.21532Z","shell.execute_reply":"2021-12-03T21:26:38.807279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Also, we can build **training history**:","metadata":{}},{"cell_type":"code","source":"def plot_NN_history(model_history, suptitle):\n    # plot data\n    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15,6))\n    fig.suptitle(suptitle, fontsize=18)\n    \n    axes[0].plot(model_history.history['accuracy'], label='train accuracy', color='g', axes=axes[0])\n    axes[0].plot(model_history.history['val_accuracy'], label='val accuracy', color='r', axes=axes[0])\n    axes[0].set_title(\"Model Accuracy\", fontsize=16) \n    axes[0].legend(loc='upper left')\n\n    axes[1].plot(model_history.history['loss'], label='train loss', color='g', axes=axes[1])\n    axes[1].plot(model_history.history['val_loss'], label='val loss', color='r', axes=axes[1])\n    axes[1].set_title(\"Model Loss\", fontsize=16) \n    axes[1].legend(loc='upper left')\n\n    plt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-03T21:26:38.809425Z","iopub.execute_input":"2021-12-03T21:26:38.809678Z","iopub.status.idle":"2021-12-03T21:26:38.820022Z","shell.execute_reply.started":"2021-12-03T21:26:38.80965Z","shell.execute_reply":"2021-12-03T21:26:38.819301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_NN_history(model_history, \"BERT\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-03T21:26:38.821116Z","iopub.execute_input":"2021-12-03T21:26:38.821423Z","iopub.status.idle":"2021-12-03T21:26:39.201697Z","shell.execute_reply.started":"2021-12-03T21:26:38.821393Z","shell.execute_reply":"2021-12-03T21:26:39.200893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. Test Neural Network","metadata":{"execution":{"iopub.status.busy":"2021-12-03T19:22:22.504812Z","iopub.execute_input":"2021-12-03T19:22:22.505161Z","iopub.status.idle":"2021-12-03T19:22:22.508925Z","shell.execute_reply.started":"2021-12-03T19:22:22.505124Z","shell.execute_reply":"2021-12-03T19:22:22.508094Z"}}},{"cell_type":"code","source":"def calculate_results(y_true, y_pred):\n    \"\"\" CALCULATE RESULTS\"\"\"\n    # Calculate model accuracy\n    model_accuracy = accuracy_score(y_true, y_pred) * 100\n    # Calculate model precision, recall and f1 score using \"weighted\" average\n    model_precision, model_recall, model_f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\")\n    model_results = {\"accuracy\": model_accuracy,\n                     \"precision\": model_precision,\n                     \"recall\": model_recall,\n                     \"f1\": model_f1}\n    return model_results","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-03T21:26:39.203023Z","iopub.execute_input":"2021-12-03T21:26:39.203341Z","iopub.status.idle":"2021-12-03T21:26:39.208995Z","shell.execute_reply.started":"2021-12-03T21:26:39.20331Z","shell.execute_reply":"2021-12-03T21:26:39.208122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get the probabilities\ny_prob = model.predict(test_input)\n# get the classes\ny_hat = y_prob.argmax(axis=-1)","metadata":{"execution":{"iopub.status.busy":"2021-12-03T21:26:39.212426Z","iopub.execute_input":"2021-12-03T21:26:39.213338Z","iopub.status.idle":"2021-12-03T21:26:55.345929Z","shell.execute_reply.started":"2021-12-03T21:26:39.213303Z","shell.execute_reply":"2021-12-03T21:26:55.34475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 7. Make submission","metadata":{}},{"cell_type":"code","source":"submission = df_test.id.copy().to_frame()\nsubmission['prediction'] = y_hat\nsubmission.head() # check submission\nsubmission.to_csv(\"submission.csv\", index = False) # save file","metadata":{"execution":{"iopub.status.busy":"2021-12-03T21:26:55.347519Z","iopub.execute_input":"2021-12-03T21:26:55.347779Z","iopub.status.idle":"2021-12-03T21:26:55.354138Z","shell.execute_reply.started":"2021-12-03T21:26:55.347751Z","shell.execute_reply":"2021-12-03T21:26:55.353284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 8. Conclusion\nThank you for reading my new article! Hope, you liked it and it was interesting for you! There are some more my articles:\n* [SMS spam with NBC | NLP | sklearn](https://www.kaggle.com/maricinnamon/sms-spam-with-nbc-nlp-sklearn)\n* [House Prices Regression sklearn](https://www.kaggle.com/maricinnamon/house-prices-regression-sklearn)\n* [Automobile Customer Clustering (K-means & PCA)](https://www.kaggle.com/maricinnamon/automobile-customer-clustering-k-means-pca)\n* [Credit Card Fraud detection sklearn](https://www.kaggle.com/maricinnamon/credit-card-fraud-detection-sklearn)\n* [Market Basket Analysis for beginners](https://www.kaggle.com/maricinnamon/market-basket-analysis-for-beginners)\n* [Neural Network for beginners with keras](https://www.kaggle.com/maricinnamon/neural-network-for-beginners-with-keras)\n* [Fetal Health Classification for beginners sklearn](https://www.kaggle.com/maricinnamon/fetal-health-classification-for-beginners-sklearn)\n* [Retail Trade Report Department Stores (LSTM)](https://www.kaggle.com/maricinnamon/retail-trade-report-department-stores-lstm)","metadata":{}}]}