{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **Watson using Bert**"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from transformers import BertTokenizer, TFBertModel\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport numpy as np\nimport pandas as pd\nimport re\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Prepare TPU**"},{"metadata":{"trusted":true},"cell_type":"code","source":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept ValueError:\n    strategy = tf.distribute.get_strategy()\n    print('Number of replicas:', strategy.num_replicas_in_sync)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Read dataset**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/contradictory-my-dear-watson/train.csv')\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Show the percentage of languages used**"},{"metadata":{"trusted":true},"cell_type":"code","source":"labels, frequencies = np.unique(train_df.language.values, return_counts = True)\nplt.figure(figsize = (8,8))\nplt.pie(frequencies,labels = labels, autopct = '%1.1f%%')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Get ready for tokenizer**"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_name = 'bert-base-multilingual-cased'\ntokenizer = BertTokenizer.from_pretrained(model_name)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def encode_sentence(s):\n    tokens = list(tokenizer.tokenize(s))\n    tokens.append('[SEP]')\n    return tokenizer.convert_tokens_to_ids(tokens)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Preprocess data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"premise = train_df['premise'].values\nhypothesis = train_df['hypothesis'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"premise = [re.sub('\\d+', '0', s) for s in premise]       #Set all numbers to 0\npremise = [s.lower() for s in premise]                   #English should be all lowercase\nhypothesis = [re.sub('\\d+', '0', s) for s in hypothesis] #Set all numbers to 0\nhypothesis = [s.lower() for s in hypothesis]             #English should be all lowercase","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def bert_encode(premise, hypothesis, tokenizer):\n    num_examples = len(premise)\n    sentence1 = tf.ragged.constant([\n        encode_sentence(s)\n        for s in np.array(premise)])\n    sentence2 = tf.ragged.constant([\n        encode_sentence(s)\n        for s in np.array(hypothesis)])\n    cls = [tokenizer.convert_tokens_to_ids(['[CLS]'])] * sentence1.shape[0]\n    input_word_ids = tf.concat([cls, sentence1, sentence2], axis =- 1)\n    input_mask = tf.ones_like(input_word_ids).to_tensor()\n    type_cls = tf.zeros_like(cls)\n    type_s1 = tf.zeros_like(sentence1)\n    type_s2 = tf.ones_like(sentence2)\n    input_type_ids = tf.concat([type_cls, type_s1, type_s2], axis =- 1).to_tensor()\n    inputs = {\n            'input_word_ids': input_word_ids.to_tensor(),\n            'input_mask': input_mask,\n            'input_type_ids': input_type_ids}\n    return inputs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = bert_encode(premise, hypothesis, tokenizer)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Build Model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"max_len = 20\ndef build_model():\n    bert_encoder = TFBertModel.from_pretrained(model_name)\n    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    input_type_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_type_ids\")\n    embedding = bert_encoder([input_word_ids, input_mask, input_type_ids])[0]\n    output = tf.keras.layers.Dense(3, activation='softmax')(embedding[:,0,:])\n    model = tf.keras.Model(inputs=[input_word_ids, input_mask, input_type_ids], outputs=output)\n    model.compile(tf.keras.optimizers.Adam(lr=1e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with strategy.scope():\n    model = build_model()\n    model.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Train using the TF Bert Model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(x_train, train_df.label.values, epochs = 8, batch_size = 64)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Read dataset**"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = pd.read_csv(\"../input/contradictory-my-dear-watson/test.csv\")\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Show the percentage of languages used**"},{"metadata":{"trusted":true},"cell_type":"code","source":"labels, frequencies = np.unique(test_df.language.values, return_counts = True)\nplt.figure(figsize = (8,8))\nplt.pie(frequencies,labels = labels, autopct = '%1.1f%%')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Preprocess data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test = bert_encode(test_df.premise.values, test_df.hypothesis.values, tokenizer)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Predict the answer**"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test = [np.argmax(i) for i in model.predict(x_test)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.DataFrame({'id': test_df['id'].values, 'prediction': y_test})\nsub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.to_csv('submission.csv', index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}