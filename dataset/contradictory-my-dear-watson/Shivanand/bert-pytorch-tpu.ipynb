{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"VERSION = \"nightly\"  #@param [\"1.5\" , \"20200325\", \"nightly\"]\n!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n!python pytorch-xla-env-setup.py --version $VERSION ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch \nimport transformers\nimport torch_xla\nimport torch_xla.core.xla_model as xm\nimport torch_xla.distributed.parallel_loader as pl \nimport torch_xla.distributed.xla_multiprocessing as xmp\nfrom tqdm import tqdm\nimport torch.nn as nn \nfrom sklearn import metrics, model_selection\nimport numpy as np\nimport pandas as pd \nfrom transformers import AdamW, get_linear_schedule_with_warmup ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class BERTDataset:\n    def __init__(self, premise, hypothesis,label):\n        self.premise = premise\n        self.hypothesis = hypothesis\n        self.tokenizer = TOKENIZER\n        self.max_len = MAX_LEN\n        self.label = label\n\n    def __len__(self):\n        return len(self.premise)\n\n    def __getitem__(self, item):\n        premise = str(self.premise[item])\n        hypothesis = str(self.hypothesis[item])\n        inputs = self.tokenizer.encode_plus(\n            premise,\n            hypothesis,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            pad_to_max_length=True,\n            truncation=True\n        )\n        ids = inputs['input_ids']\n        mask = inputs['attention_mask']\n        token_type_ids = inputs['token_type_ids']\n\n        return {\n            'ids': torch.tensor(ids, dtype=torch.long),\n            'mask': torch.tensor(mask, dtype=torch.long),\n            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n            'label': torch.tensor(self.label[item], dtype=torch.float)            \n        }\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nclass BERTBaseMultilingualCased(nn.Module):\n    def __init__(self):\n        super(BERTBaseMultilingualCased, self).__init__()\n        self.bert = transformers.BertModel.from_pretrained(\n            BERT_PATH\n        )\n        self.bert_drop = nn.Dropout(0.3)\n        \n        self.out = nn.Linear(768, 3)\n\n\n    def forward(self, ids, mask, token_type_ids):\n        _, out2 = self.bert(\n            input_ids=ids,\n            attention_mask=mask,\n            token_type_ids=token_type_ids,\n        ) \n        bo = self.bert_drop(out2) \n        output = self.out(bo)\n\n        return output\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef loss_fn(output, target):\n    return nn.CrossEntropyLoss()(output, target)\n\n\ndef train_fn(data_loader, model, device, optimizer,scheduler):\n    model.train()\n    for b, d in tqdm(enumerate(data_loader), total=len(data_loader)):\n        ids = d['ids']\n        mask = d['mask']\n        token_type_ids = d['token_type_ids']\n        target = d['label']\n        ids = ids.to(device, dtype=torch.long)\n        mask = mask.to(device, dtype=torch.long)\n        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n        target = target.to(device, dtype=torch.long)\n        optimizer.zero_grad()\n        output = model(ids, mask, token_type_ids)\n        loss = loss_fn(output, target)\n        loss.backward()\n        xm.optimizer_step(optimizer)\n        if scheduler is not None:\n          scheduler.step()\n\n\ndef eval_fn(data_loader, model, device):\n    model.eval()\n    fin_target = []\n    fin_output = []\n    with torch.no_grad():\n        for b, d in tqdm(enumerate(data_loader), total=len(data_loader)):\n            ids = d['ids']\n            mask = d['mask']\n            token_type_ids = d['token_type_ids']\n            target = d['label']\n            ids = ids.to(device, dtype=torch.long)\n            mask = mask.to(device, dtype=torch.long)\n            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n            target = target.to(device, dtype=torch.long)\n            output = model(ids, mask, token_type_ids)\n            output = torch.log_softmax(output,dim=1)\n            output = torch.argmax(output,dim=1)\n            fin_target.extend(target.cpu().detach().numpy().tolist())\n            fin_output.extend(output.cpu().detach().numpy().tolist())\n    return fin_target, fin_output \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EPOCHS = 10\nTRAIN_BATCH_SIZE = 8\nVALID_BATCH_SIZE = 4\nBERT_PATH = 'bert-base-multilingual-cased'\nTOKENIZER = transformers.BertTokenizer.from_pretrained(BERT_PATH, do_lower_case=False)\nMAX_LEN = 260\nMODEL_PATH = './model.bin'\nTRAINING_FILE = '/kaggle/input/contradictory-my-dear-watson/train.csv'\nTESTING_FILE = '/kaggle/input/contradictory-my-dear-watson/test.csv'\n\ndef run():\n    dfx = pd.read_csv(TRAINING_FILE).fillna('none')\n    df_train, df_valid = model_selection.train_test_split(\n        dfx, test_size=0.1, stratify=dfx.label.values, random_state=42)\n    \n    df_train = df_train.reset_index(drop=True)\n    df_valid = df_valid.reset_index(drop=True)\n    \n\n    def get_dataset():\n        train_dataset = BERTDataset(\n          df_train.premise.values, df_train.hypothesis.values, df_train.label.values)\n        valid_dataset = BERTDataset(\n          df_valid.premise.values, df_valid.hypothesis.values, df_valid.label.values)  \n        return train_dataset,valid_dataset\n    SERIAL_EXEC = xmp.MpSerialExecutor()\n    train_dataset,valid_dataset = SERIAL_EXEC.run(get_dataset)\n\n    \n    train_sampler = torch.utils.data.distributed.DistributedSampler(\n        train_dataset,\n        num_replicas = xm.xrt_world_size(),\n        rank = xm.get_ordinal(),\n        shuffle = True \n    )\n    \n    train_data_loader = torch.utils.data.DataLoader(\n        train_dataset, batch_size=TRAIN_BATCH_SIZE, sampler = train_sampler)\n     \n    valid_sampler = torch.utils.data.distributed.DistributedSampler(\n        valid_dataset,\n        num_replicas = xm.xrt_world_size(),\n        rank = xm.get_ordinal(),\n        shuffle = True \n    )\n    \n    valid_data_loader = torch.utils.data.DataLoader(\n        valid_dataset, batch_size=VALID_BATCH_SIZE,sampler = valid_sampler) \n    \n    MODEL_WRAPPER = xmp.MpModelWrapper(BERTBaseMultilingualCased())\n    device = xm.xla_device()\n    model = MODEL_WRAPPER.to(device)\n    LR = 3e-5*xm.xrt_world_size()\n    num_steps = int(len(train_dataset)/TRAIN_BATCH_SIZE/xm.xrt_world_size() * EPOCHS)\n    optimizer = AdamW(model.parameters(),lr=LR)  \n    scheduler = get_linear_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps = 0,\n        num_training_steps = num_steps\n    )\n    best_accuracy = 0\n    for EPOCH in range(EPOCHS): \n        para_train_loader = pl.ParallelLoader(train_data_loader,[device])\n        train_fn(para_train_loader.per_device_loader(device), model, device, optimizer,scheduler=scheduler)\n        para_valid_loader = pl.ParallelLoader(valid_data_loader, [device])\n        target, output = eval_fn(para_valid_loader.per_device_loader(device), model, device)\n        accuracy = metrics.accuracy_score(target, output)\n        xm.master_print(f'{EPOCH+1}: Accuracy Score: {accuracy}')\n        if accuracy >= best_accuracy:\n            xm.save(model.state_dict(), MODEL_PATH)\n            best_accuracy = accuracy\n    return f'Best Accuracy: {best_accuracy}'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xmp.spawn(run(),nprocs=8) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy\nclass BERTInferenceDataset:\n    def __init__(self, premise, hypothesis):\n        self.premise = premise\n        self.hypothesis = hypothesis\n        self.tokenizer = TOKENIZER\n        self.max_len = MAX_LEN\n\n    def __len__(self):\n        return len(self.premise)\n\n    def __getitem__(self, item):\n        premise = str(self.premise[item])\n        hypothesis = str(self.hypothesis[item])\n        inputs = self.tokenizer.encode_plus(\n            premise,\n            hypothesis,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            pad_to_max_length=True,\n            truncation=True\n        )\n        ids = inputs['input_ids']\n        mask = inputs['attention_mask']\n        token_type_ids = inputs['token_type_ids']\n\n        return {\n            'ids': torch.tensor(ids, dtype=torch.long),\n            'mask': torch.tensor(mask, dtype=torch.long),\n            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long)           \n        }\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch \nfrom tqdm import tqdm\nimport torch.nn as nn  \nimport numpy as np\nimport pandas as pd \nimport transformers\nfrom transformers import AdamW, get_linear_schedule_with_warmup \nTESTING_FILE = '/kaggle/input/contradictory-my-dear-watson/test.csv'\nBERT_PATH = 'bert-base-multilingual-cased'\nTOKENIZER = transformers.BertTokenizer.from_pretrained(BERT_PATH, do_lower_case=False)\nMAX_LEN = 260\nMODEL_PATH = './model.bin'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(TESTING_FILE).fillna('none')\ntest_dataset = BERTInferenceDataset(df.premise,df.hypothesis)\ntest_data_loader = torch.utils.data.DataLoader(test_dataset,batch_size = 8)\nmodel = BERTBaseMultilingualCased() \ndevice = 'cpu'\nmodel.load_state_dict(torch.load(MODEL_PATH)) \nmodel.to(device)\nfin_output = []\nmodel.eval()\nwith torch.no_grad():\n    for b, d in tqdm(enumerate(test_data_loader), total=len(test_data_loader)):\n        ids = d['ids']\n        mask = d['mask']\n        token_type_ids = d['token_type_ids']\n        ids = ids.to(device, dtype=torch.long)\n        mask = mask.to(device, dtype=torch.long)\n        token_type_ids = token_type_ids.to(device, dtype=torch.long) \n        output = model(ids, mask, token_type_ids)\n        output = torch.log_softmax(output,dim=1) \n        output = torch.argmax(output,dim=1)\n        fin_output.extend(output.cpu().detach().numpy().tolist())\nsubmission = pd.DataFrame({'id': df['id'], 'prediction': fin_output}) \nsubmission.to_csv('submission.csv', index=False) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}