{"cells":[{"metadata":{},"cell_type":"markdown","source":"## <font size='5' color='red'>Introduction</font>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Natural language processing (NLP) has grown increasingly elaborate over the past few years. Machine learning models tackle question answering, text extraction, sentence generation, and many other complex tasks. But, can machines determine the relationships between sentences, or is that still left to humans? If NLP can be applied between sentences, this could have profound implications for fact-checking, identifying fake news, analyzing text, and much more. \n\n![](https://media.giphy.com/media/ZkwSxuckDvf7q/giphy.gif)\n\n\n- In this notebook,I show how to do KFold Cross validation on TPU with XLM Roberta.I will further evaluate and tune the model in the upcoming updates.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## <font size='4' color='blue'>Import Important packages</font>","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport seaborn as sns\nfrom kaggle_datasets import KaggleDatasets\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input,Dropout\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport transformers\nfrom transformers import TFAutoModel, AutoTokenizer\nfrom sklearn.model_selection import StratifiedKFold,KFold\nplt.style.use('fivethirtyeight')\nimport warnings\nwarnings.filterwarnings('ignore')\nimport os","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path=\"../input/contradictory-my-dear-watson\"\nos.listdir(path)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <font size='4' color='blue'>Getting Basic Idea</font>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train=pd.read_csv(os.path.join(path,\"train.csv\"))\ndf_test=pd.read_csv(os.path.join(path,\"test.csv\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('there are {} rows and {} columns in the train'.format(df_train.shape[0],df_train.shape[1]))\nprint('there are {} rows and {} columns in the test'.format(df_test.shape[0],df_test.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <font size='3' color='blue'>Language distribution<font>","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"\n\nlangs = df_train.language.unique()\n\nfig = go.Figure()\nfig.add_trace(go.Bar(\n    x=langs,\n    y=df_train.language.value_counts().values,\n    name='train',\n    marker_color='indianred'\n))\nfig.add_trace(go.Bar(\n    x=langs,\n    y=df_test.language.value_counts().values,\n    name='test',\n    marker_color='lightsalmon'\n))\n\n# Here we modify the tickangle of the xaxis, resulting in rotated labels.\nfig.update_layout(barmode='group', xaxis_tickangle=-45,title=\"language distribution in dataset\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- The majoity of both the train and test set is in English.\n- All other language samples are under 100 per language.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### <font size='3' color='blue'>Class distribution</font>\n","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"\n\nlangs = df_train.label.unique()\n\nfig = go.Figure()\n\nfig.add_trace(go.Bar(\n    x=langs,\n    y=df_train.label.value_counts().values,\n    name='test',\n    marker_color=[ 'steelblue', 'tan', 'teal']\n))\n\n# Here we modify the tickangle of the xaxis, resulting in rotated labels.\nfig.update_layout(xaxis_tickangle=-45,title=\"Target distribution in train dataset\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- The distribution of targets seems to be almost equal.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## <font size='4' color='blue'>TPU Config</font>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Detect hardware, return appropriate distribution strategy\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n    # set: this is always the case on Kaggle.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MODEL = 'jplu/tf-xlm-roberta-large'\nEPOCHS = 10\nMAX_LEN = 96\n\n# Our batch size will depend on number of replic\nBATCH_SIZE= 16 * strategy.num_replicas_in_sync\nAUTO = tf.data.experimental.AUTOTUNE\ntokenizer = AutoTokenizer.from_pretrained(MODEL)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <font size='4' color='blue'>Fast Encoder</font>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def quick_encode(df,maxlen=100):\n    \n    values = df[['premise','hypothesis']].values.tolist()\n    tokens=tokenizer.batch_encode_plus(values,max_length=maxlen,pad_to_max_length=True)\n    \n    return np.array(tokens['input_ids'])\n\nx_train = quick_encode(df_train)\nx_test = quick_encode(df_test)\ny_train = df_train.label.values\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <font size='4' color='blue'>Dataset </font>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ndef create_dist_dataset(X, y,val,batch_size= BATCH_SIZE):\n    \n    \n    dataset = tf.data.Dataset.from_tensor_slices((X,y)).shuffle(len(X))\n          \n    if not val:\n        dataset = dataset.repeat().batch(batch_size).prefetch(AUTO)\n    else:\n        dataset = dataset.batch(batch_size).prefetch(AUTO)\n\n    \n    \n    return dataset\n\n\n\ntest_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((x_test))\n    .batch(BATCH_SIZE)\n)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <font size='4' color='blue'>Model</font>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model(transformer,max_len):\n    \n    input_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_ids\")\n    sequence_output = transformer(input_ids)[0]\n    cls_token = sequence_output[:, 0, :]\n    cls_token = Dropout(0.2)(cls_token)\n    cls_token = Dense(32,activation='relu')(cls_token)\n    out = Dense(3, activation='softmax')(cls_token)\n\n    # It's time to build and compile the model\n    model = Model(inputs=input_ids, outputs=out)\n    model.compile(\n        Adam(lr=1e-5), \n        loss='sparse_categorical_crossentropy', \n        metrics=['accuracy']\n    )\n    \n    return model\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <font size='4' color='blue'>LR Scheduler</font>\nsource :https://www.kaggle.com/miklgr500/jigsaw-tpu-bert-with-huggingface-and-keras","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_lrfn(lr_start=0.00001, lr_max=0.00003, \n               lr_min=0.000001, lr_rampup_epochs=3, \n               lr_sustain_epochs=0, lr_exp_decay=.6):\n    lr_max = lr_max * strategy.num_replicas_in_sync\n\n    def lrfn(epoch):\n        if epoch < lr_rampup_epochs:\n            lr = (lr_max - lr_start) / lr_rampup_epochs * epoch + lr_start\n        elif epoch < lr_rampup_epochs + lr_sustain_epochs:\n            lr = lr_max\n        else:\n            lr = (lr_max - lr_min) * lr_exp_decay**(epoch - lr_rampup_epochs - lr_sustain_epochs) + lr_min\n        return lr\n    \n    return lrfn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nplt.figure(figsize=(10, 7))\n\n_lrfn = build_lrfn()\nplt.plot([i for i in range(10)], [_lrfn(i) for i in range(10)]);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lrfn = build_lrfn()\nlr_schedule = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <font size='4' color='blue'>Kfold CV</font>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"skf = StratifiedKFold(n_splits=5,shuffle=True,random_state=777)\nval_score=[]\nhistory=[]\n\n\nfor fold,(train_ind,valid_ind) in enumerate(skf.split(x_train,y_train)):\n    \n    if fold < 4:\n    \n        print(\"fold\",fold+1)\n        \n       \n        tf.tpu.experimental.initialize_tpu_system(tpu)\n        \n        train_data = create_dist_dataset(x_train[train_ind],y_train[train_ind],val=False)\n        valid_data = create_dist_dataset(x_train[valid_ind],y_train[valid_ind],val=True)\n    \n        Checkpoint=tf.keras.callbacks.ModelCheckpoint(f\"roberta_base.h5\", monitor='val_loss', verbose=0, save_best_only=True,\n        save_weights_only=True, mode='min')\n        \n        with strategy.scope():\n            transformer_layer = TFAutoModel.from_pretrained(MODEL)\n            model = build_model(transformer_layer, max_len=MAX_LEN)\n            \n        \n\n        n_steps = len(train_ind)//BATCH_SIZE\n        print(\"training model {} \".format(fold+1))\n\n        train_history = model.fit(\n        train_data,\n        steps_per_epoch=n_steps,\n        validation_data=valid_data,\n        epochs=EPOCHS,callbacks=[Checkpoint],verbose=1)\n        \n        print(\"Loading model...\")\n        model.load_weights(f\"roberta_base.h5\")\n        \n        \n\n        print(\"fold {} validation accuracy {}\".format(fold+1,np.mean(train_history.history['val_accuracy'])))\n        print(\"fold {} validation loss {}\".format(fold+1,np.mean(train_history.history['val_loss'])))\n        \n        val_score.append(train_history.history['val_accuracy'])\n        history.append(train_history)\n\n        val_score.append(np.mean(train_history.history['val_accuracy']))\n        \n        print('predict on test....')\n        preds=model.predict(test_dataset,verbose=1)\n        \n        pred_test+=preds/4\n        \n\n        \nprint(\"Mean Validation accuracy : \",np.mean(val_score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <font size='4' color='blue'>Evaluation</font>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nplt.figure(figsize=(15,10))\n\nfor i,hist in enumerate(history):\n\n    plt.subplot(2,2,i+1)\n    plt.plot(np.arange(EPOCHS),hist.history['accuracy'],label='train accu')\n    plt.plot(np.arange(EPOCHS),hist.history['val_accuracy'],label='validation acc')\n    plt.gca().title.set_text(f'Fold {i+1} accuracy curve')\n    plt.legend()\n\n\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nplt.figure(figsize=(15,10))\n\nfor i,hist in enumerate(history):\n\n    plt.subplot(2,2,i+1)\n    plt.plot(np.arange(EPOCHS),hist.history['loss'],label='train loss')\n    plt.plot(np.arange(EPOCHS),hist.history['val_loss'],label='validation loss')\n    plt.gca().title.set_text(f'Fold {i+1} loss curve')\n    plt.legend()\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <font size='4' color='blue'>Submission</font>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv(os.path.join(path,'sample_submission.csv'))\nsubmission['prediction'] = np.argmax(pred_test,axis=1)\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <font size='4' color='red'>Work in Progress! Please upvote if you think this was helpful.</font>","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}