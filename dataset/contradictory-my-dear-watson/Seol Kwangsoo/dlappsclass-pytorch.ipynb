{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# DL Apps Class Project(Kaggle track)\n### Competition: Contradictory, My Dear Watson","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport time\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport torch\nimport random\nfrom torch.utils.data import Dataset, TensorDataset, DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\nimport pickle\nimport matplotlib.pyplot as plt\nimport os\nimport tqdm\nfrom transformers import BertTokenizer\nfrom transformers import BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\nfrom argparse import Namespace\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2022-06-15T04:22:52.611234Z","iopub.execute_input":"2022-06-15T04:22:52.611663Z","iopub.status.idle":"2022-06-15T04:22:52.626896Z","shell.execute_reply.started":"2022-06-15T04:22:52.611631Z","shell.execute_reply":"2022-06-15T04:22:52.62563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# set random seed for reproduce\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\nset_seed(42)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T04:22:52.654762Z","iopub.execute_input":"2022-06-15T04:22:52.6552Z","iopub.status.idle":"2022-06-15T04:22:52.663116Z","shell.execute_reply.started":"2022-06-15T04:22:52.655166Z","shell.execute_reply":"2022-06-15T04:22:52.660952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Get Dataset","metadata":{}},{"cell_type":"code","source":"full_dataset = pd.read_csv(\"../input/contradictory-my-dear-watson/train.csv\")\ntest_dataset = pd.read_csv(\"../input/contradictory-my-dear-watson/test.csv\")\n\n# split train dataset into train dataset and validation dataset\ntrain_dataset = full_dataset.sample(frac=0.8,random_state=200)\nval_dataset = full_dataset.drop(train_dataset.index).reset_index(drop=True)\ntrain_dataset = train_dataset.reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T04:22:52.698375Z","iopub.execute_input":"2022-06-15T04:22:52.698799Z","iopub.status.idle":"2022-06-15T04:22:52.792829Z","shell.execute_reply.started":"2022-06-15T04:22:52.698765Z","shell.execute_reply":"2022-06-15T04:22:52.791672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check the dataset\nprint(train_dataset.head(3))\nprint(test_dataset.head(3))","metadata":{"execution":{"iopub.status.busy":"2022-06-15T04:22:52.795138Z","iopub.execute_input":"2022-06-15T04:22:52.795602Z","iopub.status.idle":"2022-06-15T04:22:52.809431Z","shell.execute_reply.started":"2022-06-15T04:22:52.795556Z","shell.execute_reply":"2022-06-15T04:22:52.807762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# training dataset info\nprint('full data num:', len(full_dataset.premise.values))\nprint('train data num:', len(train_dataset.premise.values))\nprint('val data num:', len(val_dataset.premise.values))","metadata":{"execution":{"iopub.status.busy":"2022-06-15T04:22:52.818859Z","iopub.execute_input":"2022-06-15T04:22:52.819537Z","iopub.status.idle":"2022-06-15T04:22:52.827523Z","shell.execute_reply.started":"2022-06-15T04:22:52.819496Z","shell.execute_reply":"2022-06-15T04:22:52.825877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# see language distribution of the datasets\nlabels, frequencies = np.unique(train_dataset.language.values, return_counts = True)\n\nprint('train_dataset')\nplt.figure(figsize = (10,10))\nplt.pie(frequencies,labels = labels, autopct = '%1.1f%%')\nplt.show()\n\nlabels, frequencies = np.unique(val_dataset.language.values, return_counts = True)\n\nprint('val_dataset')\nplt.figure(figsize = (10,10))\nplt.pie(frequencies,labels = labels, autopct = '%1.1f%%')\nplt.show()\n\nlabels, frequencies = np.unique(test_dataset.language.values, return_counts = True)\n\nprint('test_dataset')\nplt.figure(figsize = (10,10))\nplt.pie(frequencies,labels = labels, autopct = '%1.1f%%')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-15T04:22:52.859125Z","iopub.execute_input":"2022-06-15T04:22:52.859906Z","iopub.status.idle":"2022-06-15T04:22:53.774529Z","shell.execute_reply.started":"2022-06-15T04:22:52.85987Z","shell.execute_reply":"2022-06-15T04:22:53.773503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# see label distribution of the datasets \nprint('train_dataset')\nlabels, frequencies = np.unique(train_dataset.label.values, return_counts = True)\n\nplt.figure(figsize = (5,5))\n\nlabel_name = ['entailment', 'neutral', 'contradiction'] \n\nplt.bar(labels, frequencies, width=0.5)\nplt.xticks(labels, label_name)\n\nplt.show()\n\nprint('val_dataset')\nlabels, frequencies = np.unique(val_dataset.label.values, return_counts = True)\n\nplt.figure(figsize = (5,5))\n\nlabel_name = ['entailment', 'neutral', 'contradiction'] \n\nplt.bar(labels, frequencies, width=0.5)\nplt.xticks(labels, label_name)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-15T04:22:53.776557Z","iopub.execute_input":"2022-06-15T04:22:53.777489Z","iopub.status.idle":"2022-06-15T04:22:54.145939Z","shell.execute_reply.started":"2022-06-15T04:22:53.777441Z","shell.execute_reply":"2022-06-15T04:22:54.144905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data preprocessing","metadata":{}},{"cell_type":"code","source":"class DataBert(Dataset):\n\n    def __init__(self, train_df, val_df, test_df, tokenizer):\n        self.label_dict = {'entailment': 0, 'neutral': 1, 'contradiction': 2}\n\n        self.train_df = train_df\n        self.val_df = val_df\n        self.test_df = test_df\n        \n        self.tokenizer = tokenizer\n        self.train_data = None\n        self.val_data = None\n        self.test_data = None\n        self.init_data()\n\n    def init_data(self):\n        self.train_data = self.load_data(self.train_df)\n        self.val_data = self.load_data(self.val_df)\n        self.test_data = self.load_data(self.test_df, train=False)\n\n    def load_data(self, df, train=True):\n        MAX_LEN = 512\n        token_ids = []\n        mask_ids = []\n        seg_ids = []\n        y = []\n\n        if train:\n            hypothesis_list = df['hypothesis'].to_list()\n            premise_list = df['premise'].to_list()\n            label_list = df['label'].to_list()\n\n            for (hypothesis, premise, label) in zip(hypothesis_list, premise_list, label_list):\n                hypothesis_id = self.tokenizer.encode(hypothesis, add_special_tokens = False)\n                premise_id = self.tokenizer.encode(premise, add_special_tokens = False)\n                pair_token_ids = [self.tokenizer.cls_token_id] + hypothesis_id + [self.tokenizer.sep_token_id] + premise_id + [self.tokenizer.sep_token_id]\n                hypothesis_len = len(hypothesis_id)\n                premise_len = len(premise_id)\n\n                segment_ids = torch.tensor([0] * (hypothesis_len + 1) + [1] * (premise_len + 2))  # sentence 0 and sentence 1\n                attention_mask_ids = torch.tensor([1] * (premise_len + hypothesis_len + 3))  # mask padded values\n\n                token_ids.append(torch.tensor(pair_token_ids))\n                seg_ids.append(segment_ids)\n                mask_ids.append(attention_mask_ids)\n                y.append(label)\n\n            token_ids = pad_sequence(token_ids, batch_first=True)\n            mask_ids = pad_sequence(mask_ids, batch_first=True)\n            seg_ids = pad_sequence(seg_ids, batch_first=True)\n            y = torch.tensor(y)\n            dataset = TensorDataset(token_ids, mask_ids, seg_ids, y)\n        else:\n            hypothesis_list = df['hypothesis'].to_list()\n            premise_list = df['premise'].to_list()\n\n            for (hypothesis, premise) in zip(hypothesis_list, premise_list):\n                hypothesis_id = self.tokenizer.encode(hypothesis, add_special_tokens = False)\n                premise_id = self.tokenizer.encode(premise, add_special_tokens = False)\n                pair_token_ids = [self.tokenizer.cls_token_id] + hypothesis_id + [self.tokenizer.sep_token_id] + premise_id + [self.tokenizer.sep_token_id]\n                hypothesis_len = len(hypothesis_id)\n                premise_len = len(premise_id)\n\n                segment_ids = torch.tensor([0] * (hypothesis_len + 1) + [1] * (premise_len + 2))  # sentence 0 and sentence 1\n                attention_mask_ids = torch.tensor([1] * (premise_len + hypothesis_len + 3))  # mask padded values\n\n                token_ids.append(torch.tensor(pair_token_ids))\n                seg_ids.append(segment_ids)\n                mask_ids.append(attention_mask_ids)\n\n            token_ids = pad_sequence(token_ids, batch_first=True)\n            mask_ids = pad_sequence(mask_ids, batch_first=True)\n            seg_ids = pad_sequence(seg_ids, batch_first=True)\n            dataset = TensorDataset(token_ids, mask_ids, seg_ids)\n\n        return dataset\n\n    def get_data_loaders(self, batch_size=32, shuffle=True):\n        train_loader = DataLoader(\n          self.train_data,\n          shuffle=shuffle,\n          batch_size=batch_size\n        )\n        val_loader = DataLoader(\n          self.val_data,\n          shuffle=shuffle,\n          batch_size=batch_size\n        )\n        test_loader = DataLoader(\n          self.test_data,\n          batch_size=1\n        )\n        return train_loader, val_loader, test_loader","metadata":{"execution":{"iopub.status.busy":"2022-06-15T04:22:54.148331Z","iopub.execute_input":"2022-06-15T04:22:54.149288Z","iopub.status.idle":"2022-06-15T04:22:54.174152Z","shell.execute_reply.started":"2022-06-15T04:22:54.149241Z","shell.execute_reply":"2022-06-15T04:22:54.172929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True) # Using a pre-trained BERT tokenizer to encode sentences\n\nnli_dataset = DataBert(train_dataset, val_dataset, test_dataset, bert_tokenizer)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T04:22:54.177889Z","iopub.execute_input":"2022-06-15T04:22:54.178301Z","iopub.status.idle":"2022-06-15T04:23:25.913252Z","shell.execute_reply.started":"2022-06-15T04:22:54.17827Z","shell.execute_reply":"2022-06-15T04:23:25.911969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Get BERT model","metadata":{}},{"cell_type":"code","source":"device = \"cuda\"\nmodel = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=3)\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T04:23:25.915263Z","iopub.execute_input":"2022-06-15T04:23:25.915746Z","iopub.status.idle":"2022-06-15T04:23:28.426885Z","shell.execute_reply.started":"2022-06-15T04:23:25.915699Z","shell.execute_reply":"2022-06-15T04:23:28.425747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"# helper function\ndef get_acc(pred, gt):\n    gt = gt.to('cpu')\n    acc = (torch.tensor(pred) == gt).sum().float() / float(gt.size(0))\n    return acc","metadata":{"execution":{"iopub.status.busy":"2022-06-15T04:23:28.428443Z","iopub.execute_input":"2022-06-15T04:23:28.429617Z","iopub.status.idle":"2022-06-15T04:23:28.437146Z","shell.execute_reply.started":"2022-06-15T04:23:28.429571Z","shell.execute_reply":"2022-06-15T04:23:28.435883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EPOCHS = 7\n\ndef train(model, train_loader, val_loader, optimizer, scheduler=None):  \n#     total_step = len(train_loader)\n    device = \"cuda\"\n    train_acc_history = []\n    train_loss_history = []\n    val_acc_history = []\n    val_loss_history = []\n    prev_loss = 100\n    prev_acc = 0\n    for epoch in range(EPOCHS):\n        start = time.time()\n        model.train()\n        total_train_loss = 0\n        total_train_acc  = 0\n        for batch_idx, (pair_token_ids, mask_ids, seg_ids, y) in enumerate(train_loader):\n            optimizer.zero_grad()\n            pair_token_ids = pair_token_ids.to(device)\n            mask_ids = mask_ids.to(device)\n            seg_ids = seg_ids.to(device)\n            labels = y.to(device)\n\n            outputs = model(pair_token_ids, \n                            token_type_ids=seg_ids, \n                            attention_mask=mask_ids,\n                            labels=labels)\n            prediction = [torch.argmax(logit).cpu().detach().item() for logit in outputs.logits]\n            loss = outputs.loss\n            total_train_loss += loss.item()\n            total_train_acc += get_acc(prediction, labels).item()            \n\n            loss.backward()\n            optimizer.step()\n            if scheduler is not None:\n                scheduler.step()\n            \n\n        train_acc  = total_train_acc/len(train_loader)\n        train_loss = total_train_loss/len(train_loader)\n\n        train_acc_history.append(train_acc)\n        train_loss_history.append(train_loss)\n        \n        model.eval()\n        total_val_acc  = 0\n        total_val_loss = 0\n        with torch.no_grad():\n            for batch_idx, (pair_token_ids, mask_ids, seg_ids, y) in enumerate(val_loader):\n                optimizer.zero_grad()\n                pair_token_ids = pair_token_ids.to(device)\n                mask_ids = mask_ids.to(device)\n                seg_ids = seg_ids.to(device)\n                labels = y.to(device)\n\n                outputs = model(pair_token_ids, \n                                token_type_ids=seg_ids, \n                                attention_mask=mask_ids,\n                                labels=labels)\n\n                prediction = [torch.argmax(logit).cpu().detach().item() for logit in outputs.logits]\n\n                loss = outputs.loss\n                total_val_loss += loss.item()\n                total_val_acc += get_acc(prediction, labels).item()   \n\n        val_acc  = total_val_acc/len(val_loader)\n        val_loss = total_val_loss/len(val_loader)\n\n        val_acc_history.append(val_acc)\n        val_loss_history.append(val_loss)\n        \n        end = time.time()\n        hours, rem = divmod(end-start, 3600)\n        minutes, seconds = divmod(rem, 60)\n\n        print(f'Epoch {epoch+1}: train_loss: {train_loss:.4f} train_acc: {train_acc:.4f} | val_loss: {val_loss:.4f} val_acc: {val_acc:.4f}')\n        print(\"{:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))\n        # best loss?\n        if val_loss > prev_loss:\n            return epoch, train_acc_history, train_loss_history, val_acc_history, val_loss_history\n#         # best acc?\n#         if val_acc < prev_acc:\n#             return epoch, train_acc_history, train_loss_history, val_acc_history, val_loss_history\n        prev_loss = val_loss\n        prev_acc = val_acc\n    return train_acc_history, train_loss_history, val_acc_history, val_loss_history","metadata":{"execution":{"iopub.status.busy":"2022-06-15T04:23:28.438539Z","iopub.execute_input":"2022-06-15T04:23:28.439045Z","iopub.status.idle":"2022-06-15T04:23:28.462512Z","shell.execute_reply.started":"2022-06-15T04:23:28.438979Z","shell.execute_reply":"2022-06-15T04:23:28.461247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loader, val_loader, test_loader = nli_dataset.get_data_loaders(batch_size=8)\n\n# optimizer\noptimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n\n# lr decay\nscheduler = get_linear_schedule_with_warmup(\n    optimizer, num_warmup_steps=0, num_training_steps=len(train_loader)*EPOCHS)\n\nlast_epoch, t_a_his, t_l_his, v_a_his, v_l_his = train(model, train_loader, val_loader, optimizer, scheduler)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T04:23:28.464608Z","iopub.execute_input":"2022-06-15T04:23:28.465172Z","iopub.status.idle":"2022-06-15T04:48:08.427813Z","shell.execute_reply.started":"2022-06-15T04:23:28.465118Z","shell.execute_reply":"2022-06-15T04:48:08.42671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# show the history\n# accuracy\nplt.plot(range(1, last_epoch + 2), t_a_his)\nplt.plot(range(1, last_epoch + 2), v_a_his)\nplt.xlabel('epochs')\nplt.ylabel('accuracy')\nplt.title('Accuracy')\nplt.legend(['train', 'validation'])\nplt.show()\n\n# loss\nplt.plot(range(1, last_epoch + 2), t_l_his)\nplt.plot(range(1, last_epoch + 2), v_l_his)\nplt.xlabel('epochs')\nplt.ylabel('loss')\nplt.title('Loss')\nplt.legend(['train', 'validation'])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-15T04:58:15.136039Z","iopub.execute_input":"2022-06-15T04:58:15.136468Z","iopub.status.idle":"2022-06-15T04:58:15.623382Z","shell.execute_reply.started":"2022-06-15T04:58:15.136434Z","shell.execute_reply":"2022-06-15T04:58:15.622314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Generating prediction","metadata":{}},{"cell_type":"code","source":"def eval(model, test_loader, scheduler=None):\n    model.eval()\n    start = time.time()\n    pred = []\n    with torch.no_grad():\n        for batch_idx, (pair_token_ids, mask_ids, seg_ids) in enumerate(test_loader):\n            optimizer.zero_grad()\n            pair_token_ids = pair_token_ids.to(device)\n            mask_ids = mask_ids.to(device)\n            seg_ids = seg_ids.to(device)\n\n            outputs = model(pair_token_ids, \n                            token_type_ids=seg_ids, \n                            attention_mask=mask_ids)\n            \n            prediction = [torch.argmax(logit).cpu().detach().item() for logit in outputs.logits]\n            \n            for i in range(len(prediction)):\n                pred.append(prediction[i])\n\n    end = time.time()\n    hours, rem = divmod(end-start, 3600)\n    minutes, seconds = divmod(rem, 60)\n    print(\"{:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))\n    return np.array(pred, dtype=np.int64)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T04:58:25.849586Z","iopub.execute_input":"2022-06-15T04:58:25.850012Z","iopub.status.idle":"2022-06-15T04:58:25.860994Z","shell.execute_reply.started":"2022-06-15T04:58:25.849979Z","shell.execute_reply":"2022-06-15T04:58:25.859343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = eval(model, test_loader)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T04:58:26.411927Z","iopub.execute_input":"2022-06-15T04:58:26.412932Z","iopub.status.idle":"2022-06-15T04:59:51.72736Z","shell.execute_reply.started":"2022-06-15T04:58:26.412887Z","shell.execute_reply":"2022-06-15T04:59:51.726248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = test_dataset.id.copy().to_frame()\nsubmission['prediction'] = preds\nsubmission.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-15T04:59:51.729486Z","iopub.execute_input":"2022-06-15T04:59:51.730244Z","iopub.status.idle":"2022-06-15T04:59:51.746101Z","shell.execute_reply.started":"2022-06-15T04:59:51.730197Z","shell.execute_reply":"2022-06-15T04:59:51.744678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv(\"submission.csv\", index = False)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T04:59:51.747865Z","iopub.execute_input":"2022-06-15T04:59:51.749178Z","iopub.status.idle":"2022-06-15T04:59:51.767585Z","shell.execute_reply.started":"2022-06-15T04:59:51.749131Z","shell.execute_reply":"2022-06-15T04:59:51.766546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}