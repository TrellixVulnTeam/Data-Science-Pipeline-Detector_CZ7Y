{"cells":[{"metadata":{"_uuid":"7edff2eb-50cb-4285-8255-b3262dbd5161","_cell_guid":"5c1b75ee-94f3-40c8-adbe-4ca8325b9f9c","trusted":true},"cell_type":"markdown","source":"Here is a notebook for NLI using PyTorch/Pytorch Lightning & HuggingFace Transformers & NLP libraries.  So far the results are somewhat simular with what I see out of the Tutorial Notebook Both methods seems to perform simularly badly.  Below I am using a HuggingFace model with sequence classification head.  I also tried using basic BERT for embeddings with a custom fully connected head on top and didn't get any improvements.  Comments are welcome and appreciated!","execution_count":null},{"metadata":{"_uuid":"d3a20929-8e1d-48d2-869c-cc57f8c63cc9","_cell_guid":"20666a1f-e31b-4134-94f8-fea9a50998d3","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.environ[\"WANDB_API_KEY\"] = \"0\" ## to silence warning","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"58fb9ec5-c099-494c-bf59-6ec9d6e64628","_cell_guid":"6c5122f9-8c39-4892-81a9-1f8830b64484","trusted":true},"cell_type":"markdown","source":"Let's set up our TPU.","execution_count":null},{"metadata":{"_uuid":"386d0823-63ab-4765-9561-32c5f382e71d","_cell_guid":"ca2729e3-9275-4a9c-b150-592320bd3e54","trusted":true},"cell_type":"code","source":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept ValueError:\n    strategy = tf.distribute.get_strategy() # for CPU and single GPU\n    print('Number of replicas:', strategy.num_replicas_in_sync)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0b64130f-530a-4560-8afd-462a55fc14b3","_cell_guid":"44a1d22f-053c-4188-b25f-d714aa745016","trusted":true},"cell_type":"markdown","source":"## Downloading Data<br>\n*Straight from the Tutorial, i didn't change anything here*","execution_count":null},{"metadata":{"_uuid":"b9285071-38f2-421b-9b1c-9c44d41c7365","_cell_guid":"6fb2939c-b14a-450f-85dd-7220439eaf55","trusted":true},"cell_type":"markdown","source":"The training set contains a premise, a hypothesis, a label (0 = entailment, 1 = neutral, 2 = contradiction), and the language of the text. For more information about what these mean and how the data is structured, check out the data page: https://www.kaggle.com/c/contradictory-my-dear-watson/data","execution_count":null},{"metadata":{"_uuid":"6e60d19f-aeae-417a-a10a-50cc1d5ee685","_cell_guid":"f3ad567b-a156-4ffc-a6f8-1f5e6e989a4e","trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/contradictory-my-dear-watson/train.csv\", nrows = 500)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can use the pandas head() function to take a quick look at the training set.","execution_count":null},{"metadata":{"_uuid":"82e6d183-b8e1-412b-816f-c9020bac1428","_cell_guid":"d3b9a632-7abb-4bef-acd3-9cba577dc2c0","trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"abfb11e9-865c-4b50-b0cc-9d25851618ff","_cell_guid":"817c2470-9778-46e8-be86-fdd1b9449b93","trusted":true},"cell_type":"markdown","source":"Let's look at one of the pairs of sentences.","execution_count":null},{"metadata":{"_uuid":"dea716f0-9698-4a10-a7f1-1a0bb36054db","_cell_guid":"bcca3548-bb15-4868-86ec-95fe084d6e06","trusted":true},"cell_type":"code","source":"train.premise.values[1]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5b639eff-aeea-4dca-b516-7e729cdeb741","_cell_guid":"15dcbe6c-4914-4400-a8fd-65b6e4cbf652","trusted":true},"cell_type":"code","source":"train.hypothesis.values[1]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c63a91e0-05f7-4a67-acf8-2131ef50f054","_cell_guid":"e16e8879-c565-4a29-bacc-26648659da29","trusted":true},"cell_type":"code","source":"train.label.values[1]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"216dc9c2-1699-4b89-af27-443d1b4c7289","_cell_guid":"8c9122ea-797d-48f1-b4ba-85bc2e0e6f18","trusted":true},"cell_type":"markdown","source":"These statements are contradictory, and the label shows that.\n\nLet's look at the distribution of languages in the training set.","execution_count":null},{"metadata":{"_uuid":"7243f511-d81e-436c-971f-6328b7c0cf43","_cell_guid":"7d9b43ef-4ac1-40d5-aebc-a162f1b9a6c0","trusted":true},"cell_type":"code","source":"labels, frequencies = np.unique(train.language.values, return_counts = True)\n\nplt.figure(figsize = (10,10))\nplt.pie(frequencies,labels = labels, autopct = '%1.1f%%')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **PyTorch Lightning & HuggingFace Based Model**<br>\n*I did all the work in Colab so I effectively read the csv file twice in this notebook it obviously an be optimized re-using the dataframe read in the above section*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport time","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Setting TPU if working in Google Colab\n%%capture\n!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n!python pytorch-xla-env-setup.py --version nightly --apt-packages libomp5 libopenblas-dev","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%capture\n!pip install pytorch_lightning\n!pip install transformers\n!pip install nlp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch as th\nimport pytorch_lightning as pl\nimport nlp\nimport transformers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import BertTokenizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# using basic BERT for sequence classification\nLOSS = []\nACC = []\nclass Sentences(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.brt = transformers.BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels = 3)\n\n    def prepare_data(self):\n        tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n\n        def _tokenize(x):\n            print('in tokenize')\n            return tokenizer(\n                    x['premise'],\n                    x['hypothesis'],\n                    max_length=64, \n                    truncation = True,\n                    pad_to_max_length=True)\n            \n        def _prepare_ds():\n            df = pd.read_csv('../input/contradictory-my-dear-watson/train.csv')\n            df_filtered  = df.filter(['premise','hypothesis','label'])\n            dataset = nlp.Dataset.from_pandas(df_filtered)\n            ds_flt = dataset.train_test_split(test_size=0.1)\n\n            ds_flt['train'] = ds_flt['train'].map(_tokenize, batched=True)\n            ds_flt['train'].set_format(type='torch',columns = ['input_ids','token_type_ids','label','attention_mask'])\n\n            ds_flt['test'] = ds_flt['test'].map(_tokenize, batched=True)\n            ds_flt['test'].set_format(type='torch',columns = ['input_ids','token_type_ids','label','attention_mask'])\n\n            return ds_flt['train'], ds_flt['test']\n\n        self.train_ds, self.test_ds = _prepare_ds()      \n\n    def forward(self, input_ids, masks, token_type_ids, labels):\n        out = self.brt(input_ids, masks, token_type_ids, labels = labels)\n        #print('forward brt output - {}'.format(out))\n        return out\n\n    def training_step(self, batch, batch_idx):\n        res = self.forward(batch['input_ids'],batch['attention_mask'], batch['token_type_ids'], batch['label'])\n        LOSS.append(res[0].tolist())\n        training_loss = {'train_loss': res[0]}\n        return {'loss': res[0], 'log': training_loss} \n\n    def validation_step(self, batch, batch_idx):\n        res = self.forward(batch['input_ids'], batch['attention_mask'], batch['token_type_ids'], batch['label'])\n        #print('validation step input - {}'.format(res))\n        loss = res[0]\n        acc = (res[1].argmax(-1) == batch['label']).float() # argmax(1) or argmax(dim=1) produces the same result\n        ACC.append(th.mean(acc))\n        out = {'val_loss': loss, 'val_acc': acc}\n        print('validation step val_loss & val_acc - {}'.format(out))\n        result = pl.EvalResult(checkpoint_on=loss)\n        result.log_dict({'val_acc': acc, 'val_loss': loss})\n        #return result\n        return {'loss': loss, 'acc': acc}\n\n    def validation_epoch_end(self, outputs):\n        #loss = th.cat([o['loss'] for o in outputs], 0).mean()\n        for item in outputs:\n          loss = item['loss']\n        acc = th.cat([o['acc'] for o in outputs], 0).mean()\n        out = {'val_loss': loss, 'val_acc': acc}\n        print('in validation epoc end: out: {}'.format(out)) \n        return {**out, 'log': out}\n\n    def train_dataloader(self):\n        return th.utils.data.DataLoader(\n                self.train_ds,\n                batch_size=32,\n                drop_last=True,\n                shuffle=True,\n                )\n\n    def val_dataloader(self):\n        return th.utils.data.DataLoader(\n                self.test_ds,\n                batch_size=16,\n                drop_last=False,\n                shuffle=False,\n                )\n\n    def configure_optimizers(self):\n        return th.optim.SGD(\n            self.parameters(),\n            lr=0.01,\n            momentum=0.9,\n        )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def model_fit():\n    model = Sentences()\n    trainer = pl.Trainer(\n        #default_root_dir='logs',\n        gpus=(1 if th.cuda.is_available() else 0),\n        #tpu_cores = 1, #uncomment if using TPU on Colab\n        max_epochs=10,\n        fast_dev_run=False,\n        logger=pl.loggers.TensorBoardLogger('logs_bert/', name='pretrained'),\n    )\n    trainer.fit(model)\n\n!rm -rf ./logs_bert/ # these are tensorboard logs\n\nstart_time = time.time()\nmodel_fit()\nexecution_time = time.time() - start_time","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#the Tensorboard is set for Colab, I haven't tried it in Kaggle\n%load_ext tensorboard\n%tensorboard --logdir logs_bert/","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fb1ff888-7684-4888-b861-3c31a3f360b7","_cell_guid":"87b18b05-30f3-45f9-9c3e-6f8184934bd0","trusted":true},"cell_type":"markdown","source":"## Generating & Submitting Predictions","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The below code is from the Tutorial notebook and not yet hooked up to the code up above","execution_count":null},{"metadata":{"_uuid":"f9db46a1-1f83-4ddb-85bf-da8f40afe623","_cell_guid":"a7ab0c33-0377-4cb2-b4f9-fc489383fdb7","trusted":true},"cell_type":"code","source":"predictions = [np.argmax(i) for i in model.predict(test_input)]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7b489c00-896d-44af-9371-16c2cf222365","_cell_guid":"1d0e8e42-2746-4331-95a5-3eb78ca6861c","trusted":true},"cell_type":"markdown","source":"The submission file will consist of the ID column and a prediction column. We can just copy the ID column from the test file, make it a dataframe, and then add our prediction column.","execution_count":null},{"metadata":{"_uuid":"9e0e34fa-1ef7-4207-a5c3-c21863c7be27","_cell_guid":"add7302f-ae26-4e78-b69d-858cacb35991","trusted":true},"cell_type":"code","source":"submission = test.id.copy().to_frame()\nsubmission['prediction'] = predictions","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1d4999aa-10d2-4a88-962a-8f10bded837b","_cell_guid":"d4fdefdc-4839-4962-ae75-6807390a6de7","trusted":true},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b8463f7e-8b2f-4c24-8eb3-a35ec02a2d6e","_cell_guid":"84abe9e3-ef04-4dac-97d2-2308a0f11313","trusted":true},"cell_type":"code","source":"submission.to_csv(\"submission.csv\", index = False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1af8c748-a13d-4274-9208-94d9895fdc19","_cell_guid":"4d057f53-824d-400f-b9f0-5adfc06322ef","trusted":true},"cell_type":"markdown","source":"And now we've created our submission file, which can be submitted to the competition. Good luck!","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}