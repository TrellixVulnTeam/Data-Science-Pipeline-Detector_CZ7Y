{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os, time, random, warnings\nfrom tqdm import tqdm\n\nfrom plotly.offline import init_notebook_mode, iplot, plot\nimport plotly as py\nimport plotly.express as px\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import Dense,Input,Dropout\nfrom tensorflow.keras.models import Model\nimport tensorflow_addons as tfa\n\nfrom transformers import BertTokenizer, TFBertModel\n\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept ValueError:\n    strategy = tf.distribute.get_strategy() # for CPU and single GPU\n    print('Number of replicas:', strategy.num_replicas_in_sync)\n    \n\nCFG = {\n'epochs' : 20,\n'batch_size' : 32*strategy.num_replicas_in_sync,\n'random_seed' : 42,\n'max_len' : 100,\n'validation_split':0.2\n}\n\n\n\nnp.random.seed(CFG['random_seed'])\ntf.random.set_seed(CFG['random_seed'])\nos.environ[\"WANDB_API_KEY\"] = \"0\" ## to silence warning\nwarnings.simplefilter(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/contradictory-my-dear-watson/train.csv')\ntrain_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"iplot(px.pie(train_df,names='language',title='Languages counts'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"iplot(px.pie(train_df,names='label',title='Languages counts'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.bar(['premise','hypothesis'],[np.mean([len(i) for i in train_df['premise'].apply(lambda x: x.split())]),np.mean([len(i) for i in train_df['hypothesis'].apply(lambda x: x.split())])])\nplt.title('avg sentence len')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_name = 'bert-base-multilingual-cased'\ntokenizer = BertTokenizer.from_pretrained(model_name)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def encode_sentence(s):\n    tokens = list(tokenizer.tokenize(s))\n    tokens.append('[SEP]')\n    return tokenizer.convert_tokens_to_ids(tokens)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def bert_encode(premises,hypothesis,tokenize):\n    \n    num_examples = len(hypothesis)\n    \n    sentence1 = tf.ragged.constant([\n        encode_sentence(s) for s in np.array(premises)\n    ])\n    \n    sentence2 = tf.ragged.constant([\n        encode_sentence(s) for s in np.array(hypothesis)\n    ])\n    \n    cls = [tokenizer.convert_tokens_to_ids(['[SEP]'])]*sentence1.shape[0]\n    \n    input_words_ids = tf.concat([cls,sentence1,sentence2],axis=-1)\n    \n    input_mask=tf.ones_like(input_words_ids)\n    \n    type_cls = tf.zeros_like(cls)\n    type_sentence1 = tf.zeros_like(sentence1)\n    type_sentence2 = tf.ones_like(sentence2)\n    \n    input_type_ids = tf.concat([type_cls,type_sentence1,type_sentence2],axis=-1)\n    \n    inputs={\n        'input_words_ids' :input_words_ids.to_tensor(),\n        'input_mask' : input_mask.to_tensor(),\n        'input_type_ids' : input_type_ids.to_tensor()\n    }\n    \n    return inputs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_train = bert_encode(train_df.premise.values,train_df.hypothesis.values,tokenizer)\n\n# input_val = dict()\n# val_start = int(len(train_df.label.values)*CFG['validation_split'])\n\n# for key in input_train.keys():\n#     input_val[key] =input_train[key][val_start:]\n#     input_train[key] = input_train[key][:val_start]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"maxlen = CFG['max_len']\n\ndef build_model(l):\n    bert_encoder = TFBertModel.from_pretrained(model_name)\n    \n    input_words_ids = Input(shape=(maxlen),dtype=tf.int32,name='input_words_ids')\n    input_mask = Input(shape=(maxlen,),dtype=tf.int32,name='input_mask')\n    input_type_ids = Input(shape=(maxlen,),dtype=tf.int32,name='input_type_ids')\n    \n    embedding = bert_encoder([input_words_ids,input_mask,input_type_ids])[0]\n    \n    fc1 = Dense(1024,activation='relu')(embedding[:,0,:])\n    drop = Dropout(0.2)(fc1)\n    \n    output = Dense(3,activation='softmax')(drop)\n    \n    \n    model = Model(inputs=[input_words_ids,input_mask,input_type_ids],outputs=output)\n    \n\n    model.compile(\n        optimizer = keras.optimizers.Adam(lr=l),\n        loss='sparse_categorical_crossentropy',\n        metrics=['accuracy']\n    )\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for e in range(3,4):\n#     for l in [5e-5,3e-5,2e-5]:\n        \n#         with strategy.scope():\n#             model = build_model(l)\n        \n#         print(f\"----------------- epochs: {e} ,learning rate: {l} -----------------\")\n        \n#         hist = model.fit(input_train,train_df.label.values,batch_size=CFG['batch_size'],epochs=e,verbose=1,validation_split=CFG['validation_split'])\n        \n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with strategy.scope():\n    model = build_model(l=1e-5)\n    model.summary()\n    \nhist = model.fit(input_train,train_df.label.values,batch_size=CFG['batch_size'],epochs=CFG['epochs'],verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv('../input/contradictory-my-dear-watson/test.csv')\ninput_test = bert_encode(test.premise.values,test.hypothesis.values,tokenizer)\n\npreds = model.predict(input_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = [np.argmax(i) for i in preds]\npreds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv('../input/contradictory-my-dear-watson/sample_submission.csv')\nsubmission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission['prediction'] = preds\nsubmission\nsubmission.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}