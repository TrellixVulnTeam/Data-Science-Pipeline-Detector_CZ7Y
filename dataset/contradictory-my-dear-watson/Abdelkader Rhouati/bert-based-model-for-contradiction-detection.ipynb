{"cells":[{"metadata":{"_uuid":"b98bd9e8-e8b2-4b4d-9a09-733220991729","_cell_guid":"7b9d05f4-74c6-47a2-96a5-f4e16e077e22","trusted":true},"cell_type":"markdown","source":"# **Detecting contradiction and entailment in multilingual text using TPUs**\n\nThe Challenge for this competition is to create an NLI model for contradiction of two sentences. However, NLI (for Natural Language Inferencing) is a NLP problem which analyse a relation between a pairs of sentences. In fact, There are three ways that a pair of sentences could be related: one could entail the other (= label equal to 0), they could be unrelated (= label equal to 1), or one could contradict the other (= label equal to 2).\n\nFor this competition, the train and test dataset include text in fifteen different languages !\n\n> Disclaimer: The dataset for this competition contains text that may be considered profane, vulgar, or offensive."},{"metadata":{"_uuid":"a59265d8-2fca-4f83-a526-6e38d0e8d606","_cell_guid":"1c0aa92d-6a29-4783-9b3f-bcdc717e3e8b","trusted":true},"cell_type":"markdown","source":"# 1. Import Needed Package"},{"metadata":{"_uuid":"3147b043-ecf9-4987-b596-e1cc402c9500","_cell_guid":"6aa8a517-c8a4-4e79-97c7-7e7b1c3f5932","trusted":true},"cell_type":"code","source":"# install need package\n!pip install googletrans textAugment","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4c3e235e-27a0-4916-9a86-bac37079e5d4","_cell_guid":"5bea9baa-e67c-420a-86fa-df7f4071d013","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # lib for gsraph plot\nimport os\nimport re # Regular Exprexion lib\n\n\nos.environ[\"WANDB_API_KEY\"] = \"0\" ## to silence warning\n\n# lib for Machine learning models (BERT)\nfrom transformers import TFXLMRobertaModel, XLMRobertaTokenizer\nfrom transformers import TFRobertaModel, RobertaTokenizer\nimport tensorflow as tf\n\nfrom textaugment import EDA\nfrom googletrans import Translator\n\nimport multiprocessing as mp\nfrom tqdm import tqdm_notebook\n\nimport gc\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"740551de-0bf7-4f13-9c97-d048cae590c1","_cell_guid":"4732d8f7-6bec-47a4-a48b-eb7c9c978b1b","trusted":true},"cell_type":"code","source":"# TPU detection. No parameters necessary if TPU_NAME environment variable is\n# set: this is always the case on Kaggle.\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    strategy = tf.distribute.get_strategy() # for CPU and single GPU\n\nprint('Number of replicas:', strategy.num_replicas_in_sync)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e9741801-0409-49b5-921d-507c4f900900","_cell_guid":"b079228e-9069-4406-bdfd-32de75e4f664","trusted":true},"cell_type":"markdown","source":"# 2. Data Reviews\n\nTo better understand the issue, we first make a deep review of our input data"},{"metadata":{"_uuid":"c8668d5e-a5a8-4ba5-a677-1460695e1934","_cell_guid":"30c035fb-2e12-40c9-86c5-239725a63e9a","trusted":true},"cell_type":"code","source":"# List of csv data files\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5ab4247e-81f9-415b-b801-dd87f769c185","_cell_guid":"2614038e-cf1b-4054-862f-464d1ce84505","trusted":true},"cell_type":"markdown","source":"**Load training and testing sets from each corresponding files :**"},{"metadata":{"_uuid":"84e1bd77-ce87-49d9-8425-c462d7ac6fbe","_cell_guid":"87ca1fd2-a65a-4c19-8a16-e6ea78bbdb06","trusted":true},"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/input/contradictory-my-dear-watson/train.csv')\ndf_test = pd.read_csv('/kaggle/input/contradictory-my-dear-watson/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f77bc66d-82b7-4395-aed5-db13e9ebc00b","_cell_guid":"fbdcc573-a6db-46de-9606-ddc9d66d1d37","trusted":true},"cell_type":"markdown","source":"**A brief review of data structure :**"},{"metadata":{"_uuid":"05a7acae-3c96-4260-aff5-157f5cd3f78a","_cell_guid":"444d6717-d8c6-482c-861c-4721b85b914b","trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"00c7cdcc-5587-4f9f-989c-819785419959","_cell_guid":"d55be720-1fff-4ee9-bc26-794720dd3ba3","trusted":true},"cell_type":"markdown","source":"The training set contains a premise, a hypothesis, a label (0 = entailment, 1 = neutral, 2 = contradiction), and the language of the text."},{"metadata":{"_uuid":"73819e96-c321-449f-b36f-3961ebdebd4c","_cell_guid":"4a6d0ac2-587d-4ad4-9342-caae8fa1b6ed","trusted":true},"cell_type":"markdown","source":"**Distribution of languages in training and testing sets**"},{"metadata":{"_uuid":"c31ea643-865d-4077-97b8-f7252085ebe7","_cell_guid":"58290d2d-6670-4e38-8a29-42af8ca104b5","trusted":true},"cell_type":"code","source":"labels, frequencies = np.unique(df_train.language.values, return_counts = True)\n\nplt.figure(figsize = (10,10))\nplt.pie(frequencies,labels = labels, autopct = '%1.1f%%')\nplt.title('language distribution in Training Set')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bb940125-7ae7-43a5-af02-ca6003292d15","_cell_guid":"ae9c98b5-92de-49e7-8d6b-bfa13c98a67a","trusted":true},"cell_type":"code","source":"labels, frequencies = np.unique(df_test.language.values, return_counts = True)\n\nplt.figure(figsize = (10,10))\nplt.pie(frequencies,labels = labels, autopct = '%1.1f%%')\nplt.title('language distribution in Testing Set')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e99ac1c2-db59-4b3f-98d1-80c3e8e057a1","_cell_guid":"6d98d5e3-1a37-4d1a-96db-99d8a686ec48","trusted":true},"cell_type":"markdown","source":"**Distribution of sentence's relations in training set**"},{"metadata":{"_uuid":"c6b02f36-58d7-4f24-bab1-da48b8c31a91","_cell_guid":"f4678e54-84ab-4753-9b69-b5db5ebb4709","trusted":true},"cell_type":"code","source":"labels, freq_labels = np.unique(df_train.label.values, return_counts = True)\n\nplt.figure(figsize = (10,10))\nplt.pie(freq_labels,labels = labels, autopct = '%1.1f%%')\nplt.title('labels distribution in Training Set')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fdbc058b-fe02-431a-bd26-d16b452f3f04","_cell_guid":"1edf86f1-d93f-4663-824b-9552a684ffc3","trusted":true},"cell_type":"markdown","source":"**Data cleansing**"},{"metadata":{"_uuid":"5256f4b1-298a-433b-bf89-962a74444ed1","_cell_guid":"63f8f81e-da36-4da7-90df-b6be0c078e71","trusted":true},"cell_type":"code","source":"def clean_word(value):\n    language = value[0]\n    word = value[1]\n    if language != 'English':\n        word = word.lower()\n        return word\n    word = word.lower()\n    word = re.sub(r'\\?\\?', 'e', word)\n    word = re.sub('\\.\\.\\.', '.', word)\n    word = re.sub('\\/', ' ', word)\n    word = re.sub('--', ' ', word)\n    word = re.sub('/\\xad', '', word)\n    word = word.strip(' ')\n    return word\n\ndf_train['premise'] = df_train[['language', 'premise']].apply(lambda v: clean_word(v), axis=1)\ndf_train['hypothesis'] = df_train[['language', 'hypothesis']].apply(lambda v: clean_word(v), axis=1)\ndf_test['premise'] = df_test[['language', 'premise']].apply(lambda v: clean_word(v), axis=1)\ndf_test['hypothesis'] = df_test[['language', 'hypothesis']].apply(lambda v: clean_word(v), axis=1)\n\nlanguages = [ 'zh-cn' if lang == 'zh' else lang for lang in df_train['lang_abv'].unique()]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d404352e-8df2-4ba9-becb-e01d2c7fd196","_cell_guid":"61094731-9637-4e0b-aad3-fe53188cda82","trusted":true},"cell_type":"markdown","source":"# 4. Building Model"},{"metadata":{"_uuid":"7a7b17f9-f2b2-4c4c-958c-b6b006a0835f","_cell_guid":"0ef89cc0-d075-4ca7-86e4-71968c034608","trusted":true},"cell_type":"markdown","source":"## 4.1 initialize model architecture"},{"metadata":{"_uuid":"fe69e2c7-e8d4-4f3c-b135-dbf615f1b457","_cell_guid":"dcd51a69-fc79-4cfd-89cf-3ae5ce01c1fa","trusted":true},"cell_type":"code","source":"seed = 42\ntf.random.set_seed(seed)\n\nmodel_name = 'jplu/tf-xlm-roberta-large' # pretrained model' name\ntokenizer = XLMRobertaTokenizer.from_pretrained(model_name) # tokenizer init\n\n#model_name = 'roberta-large'\n#tokenizer = RobertaTokenizer.from_pretrained(model_name) # tokenizer init\n\ndef build_model():\n    with strategy.scope():\n        \n        bert_encoder = TFXLMRobertaModel.from_pretrained(model_name)\n        #bert_encoder = TFRobertaModel.from_pretrained(model_name)\n        \n        # define tensors for inputs\n        input_word_ids = tf.keras.Input(shape=(None,), dtype=tf.int32, name=\"input_word_ids\")\n        input_mask = tf.keras.Input(shape=(None,), dtype=tf.int32, name=\"input_mask\")\n        \n        # Define model for fine-tuning XLMRoberta\n        \n        ### Layer 1 is a pretrained XLMRoberta Transformer\n        embedding = bert_encoder([input_word_ids, input_mask])[0]\n        \n        ### 5 Layers before for Classification task\n        output_layer = tf.keras.layers.Dropout(0.25)(embedding)\n        output_layer = tf.keras.layers.GlobalAveragePooling1D()(output_layer)\n        output_dense_layer = tf.keras.layers.Dense(64, activation='relu')(output_layer)\n        output_dense_layer = tf.keras.layers.Dense(32, activation='relu')(output_dense_layer)\n        output = tf.keras.layers.Dense(3, activation='softmax')(output_dense_layer)\n\n        # Define Training parameters\n        ## Optimizer is ADAM\n        ## Function Loss is CrossEntropy\n        ## Metric for evaluation is a standard accuracy\n        model = tf.keras.Model(inputs=[input_word_ids, input_mask], outputs=output)\n        model.compile(tf.keras.optimizers.Adam(lr=1e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n        return model\n\n# Init DeepLearning Model \nwith strategy.scope():\n    model = build_model()\n    model.summary() # this describe model architecture and layers","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"25e15aa2-9dc0-4168-8cf1-c8923789f6db","_cell_guid":"49c62a03-6c18-48e6-a8f6-77d3f800c1c8","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import confusion_matrix, accuracy_score, ConfusionMatrixDisplay\n\nbatch_size = 8 * strategy.num_replicas_in_sync\nnum_splits = 5\ntest_input = None","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"76521ae0-682a-43ff-b52f-02e0b2281251","_cell_guid":"ac09ad7f-698d-49f3-8ead-89f6337cc386","trusted":true},"cell_type":"markdown","source":"## 4.2 Data Preprocessing"},{"metadata":{"_uuid":"a04b272a-64cb-4d86-853a-09690c59f629","_cell_guid":"8540a4e9-6bf2-4a0e-b886-6f7962074631","trusted":true},"cell_type":"code","source":"auto = tf.data.experimental.AUTOTUNE\n\ndef make_dataset(train_input, train_label):\n    dataset = tf.data.Dataset.from_tensor_slices(\n        (\n            train_input,\n            train_label\n        )\n    ).repeat().shuffle(batch_size).batch(batch_size).prefetch(auto)\n    return dataset\n\n\ndef xlm_roberta_encode(hypotheses, premises, src_langs, augmentation=False):\n    num_examples = len(hypotheses)\n\n    sentence_1 = [tokenizer.encode(s) for s in premises]\n    sentence_2 = [tokenizer.encode(s) for s in hypotheses]\n    input_word_ids = list(map(lambda x: x[0]+x[1], list(zip(sentence_1,sentence_2))))\n    input_mask = [np.ones_like(x) for x in input_word_ids]\n    inputs = {\n        'input_word_ids': tf.keras.preprocessing.sequence.pad_sequences(input_word_ids, padding='post'),\n        'input_mask': tf.keras.preprocessing.sequence.pad_sequences(input_mask, padding='post')\n    }\n    return inputs\n\n# splite training data into train and valdiation\ntrain_df, validation_df = train_test_split(df_train, test_size=0.1)\n\ndf_train['prediction'] = 0\nnum_augmentation = 1\n\n# encoding training data\ntrain_input = xlm_roberta_encode(train_df.hypothesis.values,train_df.premise.values, train_df.lang_abv.values, augmentation=False)\ntrain_label = train_df.label.values\n\n# create data Iterator for training \ntrain_sequence = make_dataset(train_input, train_label)\n\n# encoding validation data\nvalidation_input = xlm_roberta_encode(validation_df.hypothesis.values, validation_df.premise.values,validation_df.lang_abv.values, augmentation=False)\nvalidation_label = validation_df.label.values\ntf.keras.backend.clear_session()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a52bf8e0-d158-4807-914e-b2a72c806fd8","_cell_guid":"5dd098d0-32ed-4ee4-958d-6f9f8320bba1","trusted":true},"cell_type":"code","source":"# splite training data into train and valdiation\ntrain_df, validation_df = train_test_split(df_train, test_size=0.1)\n\ndf_train['prediction'] = 0\nnum_augmentation = 1\n\n# encoding training data\ntrain_input = xlm_roberta_encode(train_df.hypothesis.values,train_df.premise.values, train_df.lang_abv.values, augmentation=False)\ntrain_label = train_df.label.values\n\n# create data Iterator for training \ntrain_sequence = make_dataset(train_input, train_label)\n\n# encoding validation data\nvalidation_input = xlm_roberta_encode(validation_df.hypothesis.values, validation_df.premise.values,validation_df.lang_abv.values, augmentation=False)\nvalidation_label = validation_df.label.values\ntf.keras.backend.clear_session()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0d32202a-8d74-4f2a-b240-5ef25343c3ae","_cell_guid":"40678862-f945-465d-9e5a-22664a08bc61","trusted":true},"cell_type":"markdown","source":"## 4.3 Training the model"},{"metadata":{"_uuid":"eaf3e210-55ee-4acf-8893-a601cb4b0450","_cell_guid":"6f795706-b15f-4196-99e1-18beb74ed3bd","trusted":true},"cell_type":"code","source":"n_steps = (len(train_label)) // batch_size\n\nwith strategy.scope():\n    history = model.fit(\n        train_sequence, shuffle=True, steps_per_epoch=n_steps, \n        validation_data = (validation_input, validation_label), epochs=50, verbose=1,\n        callbacks=[\n            tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=10),\n            tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', factor=0.1, patience=5),\n            tf.keras.callbacks.ModelCheckpoint(\n                'model.h5', monitor='val_accuracy', save_best_only=True,save_weights_only=True)\n        ]\n    ) \n\n# save trained model\nmodel.load_weights('model.h5')\n\n# calcul of validation Accuracy\nvalidation_predictions = model.predict(validation_input)\nvalidation_predictions = np.argmax(validation_predictions, axis=-1)\nvalidation_df['predictions'] = validation_predictions\nacc = accuracy_score(validation_label, validation_predictions)\nprint('Accuracy: {}'.format(acc))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2f1782f3-b561-4a91-805f-7f0869c74b35","_cell_guid":"6220d572-872f-41e4-9bfe-42e927f1f0a4","trusted":true},"cell_type":"markdown","source":"# 5. Generate Prediction and Submission"},{"metadata":{"_uuid":"088d1925-d97d-4ed1-b033-8ff31238d607","_cell_guid":"18919c37-2ddb-496b-8fc1-4f74581ce270","trusted":true},"cell_type":"code","source":"# encoding test data for prediction and submission\nif test_input is None:\n    test_input = xlm_roberta_encode(df_test.hypothesis.values, df_test.premise.values, df_test.lang_abv.values,augmentation=False)\n\n# prediction using trained model\ntest_split_predictions = model.predict(test_input)\npredictions = np.argmax(test_split_predictions, axis=-1)\n\n# create submission file\nsubmission = df_test.id.copy().to_frame()\nsubmission['prediction'] = predictions\nsubmission.head()\n\n# submission to challenge\nsubmission.to_csv(\"submission.csv\", index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}