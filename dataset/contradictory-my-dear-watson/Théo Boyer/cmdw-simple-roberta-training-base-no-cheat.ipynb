{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Simple RoBERTa Training\n\nHello, this notebook is a simple baseline using HuggingFace and Keras that scores ~0.90 on LB in less than 150 lines of code and trying not to cheat. <br/>\nOur work is inspired by many public notebooks so a big thank you to everyone that shared a notebook in this little competition. <br/>\nWe tried our best to avoid cheating but there may still be room for improvement. If you have any suggestions, please let us know in the comment section !\n\nThe co-authors of this work are:\n* [Nicolas Vincent](https://www.kaggle.com/nicovincx2)\n* [Adrien ArgentoAdrien Argento](https://www.kaggle.com/adrienargento)\n* [Hela Siala](https://www.kaggle.com/helasiala)\n* [Th√©o Boyer](https://www.kaggle.com/wolfy73)","metadata":{}},{"cell_type":"code","source":"\"\"\"\n    Imports\n\"\"\"\n\nimport numpy as np\nimport pandas as pd\n\nfrom transformers import TFAutoModel, AutoTokenizer\nfrom tensorflow.keras.layers import Dense, Input, GlobalAveragePooling1D\n\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport tensorflow.keras.backend as K","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\n    Let's use the TPU if possible\n\"\"\"\n\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept ValueError:\n    strategy = tf.distribute.get_strategy() # for CPU and single GPU\n    print('Number of replicas:', strategy.num_replicas_in_sync)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\n    Parameters\n\"\"\"\n\nmodel_name = 'jplu/tf-xlm-roberta-large'\nEPOCHS = 5\nBATCH_SIZE = 64","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\n    A function that tries to avoid cheat\n\"\"\"\n\ndef clean_data(train, val):\n    \"\"\"\n        Return train without the examples that are in val\n    \"\"\"\n    initial_ds = train\n    train = train.copy()\n    n = len(train)\n    # We concatenate premise and hypothesis in a copy of both datasets\n    train = pd.DataFrame((train['premise'].str.lower() + ' | ' + train['hypothesis'].str.lower()).rename(\"data\"))\n    val = pd.DataFrame((val['premise'].str.lower() + ' | ' + val['hypothesis'].str.lower()).rename(\"data\"))\n    # Save the index, we're gonna need it later\n    train['index'] = train.index\n    val['index'] = val.index\n    # This is in essence equivalent to a left join in SQL. This alow us to find the indexes of pairs tha occurs in both datasets\n    train_in_val_idxs = pd.merge(val, train, how='left', on='data')['index_y'].values\n    train_in_val_idxs = train_in_val_idxs[train_in_val_idxs == train_in_val_idxs].astype(np.int)\n    # Remove the examples present in both datasets\n    train_clean = initial_ds.drop(train_in_val_idxs)\n    train_clean = train_clean.reset_index().drop('index', axis=1)\n    n_removed = n - len(train_clean)\n    print(\"Cleaned ! removed {} examples ({:.4f}% of the total dataset)\".format(n_removed, 100 * n_removed/n))\n    return train_clean","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\n    Let's use a little class to manage our data\n\"\"\"\n\nclass WatsonAugmentedDataset:\n    def __init__(self):\n        self.maxlen = None\n        # Tokenizer from huggingface\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        \n        ## The competition datasets\n        # We are gonna validate on the training data of the competition\n        self.validation_data = pd.read_csv(\"../input/contradictory-my-dear-watson/train.csv\")\n        self.test_data = pd.read_csv(\"../input/contradictory-my-dear-watson/test.csv\")\n        # XNLI: in the languages we need, but everything is balanced\n        self.xnli = pd.read_csv(\"../input/watsoncsvda/xnli.csv\")\n        # Let's clean XNLI from the test and train data\n        print(\"Cleaning XNLI from train examples\")\n        self.xnli = clean_data(self.xnli, self.validation_data)\n        print(\"Cleaning XNLI from test examples\")\n        self.xnli = clean_data(self.xnli, self.test_data)\n        \n        ## Now we want to balance the languages in the dataset.\n        # Count the number of examples in each language\n        test_n = self.test_data['language'].value_counts().values\n        xnli_n = self.xnli['language'].value_counts().values\n        # Number of examples in english in XNLI\n        n_en_xnli = self.xnli['language'].value_counts().values.max()\n        # Number of examples in english in the test set\n        n_en_test = test_n.max()\n        # Number of examples that are not in english in XNLI\n        n_others_xnli = np.sum(xnli_n) - n_en_xnli\n        # fraction of the test set in english\n        p_en_test = n_en_test / np.sum(test_n)\n        # Calculate the number of english examples to add to XNLI to end up with a similare languages distribution than in the test set\n        n_mnli = int(round(((p_en_test - 1) * n_en_xnli + p_en_test * n_others_xnli) / (1 - p_en_test)))\n\n        # MNLI: All examples are in english\n        self.mnli = pd.read_csv(\"../input/watsoncsvda/mnli.csv\").dropna()\n        # Let's clean MNLI\n        print(\"Cleaning MNLI from train examples\")\n        self.mnli = clean_data(self.mnli, self.validation_data)\n        print(\"Cleaning MNLI from test examples\")\n        self.mnli = clean_data(self.mnli, self.test_data)\n        # We sample from MNLI the number of examples we need to balance our training set\n        self.mnli = self.mnli.sample(n_mnli)\n        # Concatenate everything\n        self.train_data = pd.concat([self.mnli, self.xnli], ignore_index=True).sample(frac=1)\n        # Save the label\n        self.train_label = self.train_data.label.values\n        self.val_label = self.validation_data.label.values\n        # Tokenize the data\n        self.test_data = self.df_to_token_tensor(self.test_data)\n        maxlen_test = self.test_data['input_ids'].shape[-1]\n        self.maxlen = maxlen_test\n        self.validation_data = self.df_to_token_tensor(self.validation_data)\n        maxlen_val = self.validation_data['input_ids'].shape[-1]\n        self.train_data = self.df_to_token_tensor(self.train_data)\n        maxlen_train = self.train_data['input_ids'].shape[-1]\n        \n        assert maxlen_val == maxlen_test == maxlen_train, \"Inconsistent maxlen between val, train and test ({}, {}, {})\".format(maxlen_val, maxlen_test, maxlen_train)\n        \n    def df_to_token_tensor(self, data):\n        return self.tokenizer(\n            data[[\"premise\", \"hypothesis\"]].astype(str).values.tolist(),\n            return_attention_mask=False,\n            #return_token_type_ids=False,\n            padding='longest' if self.maxlen is None else 'max_length', \n            truncation=True, \n            return_tensors='tf', \n            max_length=self.maxlen\n        )\n        \n\ndata = WatsonAugmentedDataset()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\n    Build the model\n\"\"\"\n\nwith strategy.scope(): # Using the predefined hardware\n    # We load the pre-trained model from his name (how cool is that)\n    bert_encoder = TFAutoModel.from_pretrained(model_name)\n    # This is a symbolic tensor that represents out input data\n    input_ids = tf.keras.Input(shape=(data.maxlen,), dtype=tf.int32, name=\"input_ids\")\n    # We feed the data to the pretrained model and get the high level features\n    features = bert_encoder(input_ids)[0]\n    # Pooling to reduce dimensions\n    features = GlobalAveragePooling1D()(features)\n    # Classification layer\n    output = Dense(3, activation='softmax')(features)\n    # Make the model\n    model = tf.keras.Model(inputs=input_ids, outputs=output)\n    # Compile it with a little learning rate\n    model.compile(\n        tf.keras.optimizers.Adam(lr=1e-5),\n        loss='sparse_categorical_crossentropy',\n        metrics=['accuracy']\n    )\n    model.summary()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\n    And now we can train the model !\n\"\"\"\n# We want to save the model that gives us the best validation results\nmcp_save = tf.keras.callbacks.ModelCheckpoint('best_model.hdf5', save_best_only=True, save_weights_only=True, monitor='val_loss', mode='min')\n# And we train everything\nmodel.fit(\n    data.train_data.values(),\n    data.train_label,\n    epochs=EPOCHS,\n    verbose=1,\n    batch_size=BATCH_SIZE,\n    callbacks=[mcp_save],\n    validation_data=(data.validation_data.values(), data.val_label)\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\n    Now we can make the predictions on the test set for the submission\n\"\"\"\n\npredictions = [np.argmax(i) for i in model.predict(data.test_data.values())]\nsubmission = pd.read_csv(\"../input/contradictory-my-dear-watson/sample_submission.csv\")\nsubmission['prediction'] = predictions\nsubmission.to_csv(\"submission.csv\", index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}