{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\ntrain_dataset = pd.read_csv(\"../input/contradictory-my-dear-watson/train.csv\")\nlang_abv = train_dataset.groupby(['lang_abv']).size()\nlang_abv_keys = lang_abv.keys().tolist()\nlang_abv_vals = lang_abv.tolist()\nprint(lang_abv_keys)\nprint(lang_abv_vals)\nprint(len(lang_abv_keys))\nprint(len(lang_abv_vals))\ny_pos = np.arange(len(lang_abv_keys))\nplt.bar(y_pos, lang_abv_vals)\nplt.xticks(y_pos, lang_abv_keys)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom transformers import BertTokenizer, TFBertModel\nfrom tensorflow import keras\n\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept ValueError:\n    strategy = tf.distribute.get_strategy() # for CPU and single GPU\n    print('Number of replicas:', strategy.num_replicas_in_sync)\n\n#https://huggingface.co/transformers/glossary.html#attention-mask\nclass BertClassifier:\n\n    MODEL_FILE_PATH = './model/bert_model.pkl'\n    EPOCHS = 1\n    MAX_LEN = 300\n\n    def __init__(self):\n        self.tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n        self.model = TFBertModel.from_pretrained('bert-base-multilingual-cased')\n        self.build_model()\n\n    def pre_process_hyp_prem_pairs(self, premiseList, hypothesisList):\n        premise_ids = []\n        hypothesis_ids = []\n        for i,premise in enumerate(premiseList):\n            premise_tokens = self.tokenizer.tokenize(\"[CLS]\" + premise + \"[SEP]\")\n            hypothesis_tokens = self.tokenizer.tokenize(hypothesisList[i] + \"[SEP]\")\n            premise_ids.append(self.tokenizer.convert_tokens_to_ids(premise_tokens))\n            hypothesis_ids.append(self.tokenizer.convert_tokens_to_ids(hypothesis_tokens))\n\n        premise_ids_tensor = tf.ragged.constant(premise_ids)\n        hypthesis_ids_tensor = tf.ragged.constant(hypothesis_ids)\n        premise_hypothesis_tensor = tf.concat([premise_ids_tensor, hypthesis_ids_tensor], axis=-1)\n\n        input_mask = tf.ones_like(premise_hypothesis_tensor).to_tensor()\n        type_s1 = tf.zeros_like(premise_ids_tensor)\n        type_s2 = tf.ones_like(hypthesis_ids_tensor)\n        input_type_ids = tf.concat([type_s1, type_s2], axis=-1).to_tensor()\n\n        inputs = {\n                    'input_word_ids': premise_hypothesis_tensor.to_tensor(),\n                    'input_mask': input_mask,\n                    'input_type_ids': input_type_ids\n                 }\n        return inputs\n\n    def build_model(self):\n        input_word_ids = tf.keras.Input(shape=(self.MAX_LEN,), dtype=tf.int32, name=\"input_word_ids\")\n        input_mask = tf.keras.Input(shape=(self.MAX_LEN,), dtype=tf.int32, name=\"input_mask\")\n        input_type_ids = tf.keras.Input(shape=(self.MAX_LEN,), dtype=tf.int32, name=\"input_type_ids\")\n\n        embedding = self.model([input_word_ids, input_mask, input_type_ids])[0]\n        output = tf.keras.layers.Dense(3, activation='softmax')(embedding[:,0,:])\n        self.nn_model = keras.Model(inputs=[input_word_ids, input_mask, input_type_ids], outputs=output)\n        self.nn_model.compile(tf.keras.optimizers.Adam(lr=1e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n#         self.nn_model.summary()\n\n    def train(self, inputs, labels):\n        self.nn_model.fit(inputs, labels, epochs = 2, verbose = 1, batch_size = 64, validation_split = 0.2)\n#         keras.models.save_model(self.nn_model, self.MODEL_FILE_PATH)\n\n    def evaluate(self, inputs, labels):\n#         prepared_model = keras.models.load_model(self.MODEL_FILE_PATH)\n#         if prepared_model:\n#             self.nn_model = prepared_model\n        test_loss, test_acc = self.nn_model.evaluate(inputs, labels, verbose=2)\n        return test_acc\n\n    def predict(self, inputs):\n#         prepared_model = keras.models.load_model(self.MODEL_FILE_PATH)\n#         if prepared_model:\n#             self.nn_model = prepared_model\n        predictions = self.nn_model.predict(inputs)\n        return predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\n\nclassifier = BertClassifier()\n\ndef load_sentences(file_path):\n    df = pd.read_csv(file_path)\n    return df[\"premise\"], df[\"hypothesis\"], df[\"id\"], df[\"lang_abv\"], (df[\"label\"] if \"label\" in df else None)\n\ndef prepare_dataset(is_training):\n    if is_training:\n        premises, hypothesis, ids, lang_abv, labels = load_sentences(\"../input/contradictory-my-dear-watson/train.csv\")\n    else:\n        premises, hypothesis, ids, lang_abv, labels = load_sentences(\"../input/contradictory-my-dear-watson/test.csv\")\n    inputs = classifier.pre_process_hyp_prem_pairs(premises, hypothesis)\n    train_labels = np.array(labels) if labels is not None else None\n    outcome = {\n        'inputs': inputs,\n        'labels': train_labels,\n        'ids': ids,\n        'lang_abv': lang_abv\n    }\n    return outcome\n\ndef train_and_evaluate():\n    # prepare_train_dataset()\n    outcome = prepare_dataset(True)\n    classifier.train(outcome['inputs'], outcome['labels'])\n\n    # eval_records = train_data[total_eval_len:]\n    # eval_record_labels = train_labels[total_eval_len:]\n    #\n    # classifier.train(train_records, train_record_labels)\n    # accuracy = classifier.evaluate(eval_records, eval_record_labels)\n    # print(accuracy)\n\ndef predict_outcomes():\n    outcomes = prepare_dataset(False)\n    test_inputs = outcomes['inputs']\n    print(test_inputs)\n    ids = outcomes['ids']\n    print(\"test ids\")\n    print(len(ids))\n    results = classifier.predict(test_inputs)\n    print(results)\n    predictions = [np.argmax(i) for i in results]\n    submission = pd.DataFrame(ids, columns=['id'])\n    # print(submission)\n    # print(predictions)\n    submission['prediction'] = predictions\n    submission.to_csv(\"submission.csv\", index=False)\n\n# prepare_dataset(True)\n# train_and_evaluate()\n# predict_outcomes()\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}