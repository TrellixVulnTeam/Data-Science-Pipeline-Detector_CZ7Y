{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n# pd.set_option('display.max_columns', None)\nimport scipy\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import preprocessing\nfrom sklearn.naive_bayes import MultinomialNB\nfrom  sklearn.metrics  import accuracy_score\npd.options.plotting.backend = \"plotly\"\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-09T00:50:39.502417Z","iopub.execute_input":"2021-06-09T00:50:39.502985Z","iopub.status.idle":"2021-06-09T00:50:39.518426Z","shell.execute_reply.started":"2021-06-09T00:50:39.50293Z","shell.execute_reply":"2021-06-09T00:50:39.517124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tratamento de Dados\n---","metadata":{}},{"cell_type":"markdown","source":"Funções do sklearn pra tratamento de texto\nTfidVectorizer - https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html?highlight=vectorizer#sklearn.feature_extraction.text.TfidfVectorizer\n\nEsse tmb é de categorização, não da pra usar LabelEncoder - https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html#sklearn.preprocessing.LabelEncoder\n\nNão da pra usar o OneHotEncoder no nosso problema, já que ele é mais focado pra categorizar labels, não frases\nOneHotEncoder - https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html#sklearn.preprocessing.OneHotEncoder\n","metadata":{}},{"cell_type":"code","source":"#substituir todos 1 por 0 em label\nlabels = {0:\"entailed\",\n          1:\"neutral\",\n          2:\"contradiction\"}\ntrain = pd.read_csv(\"../input/contradictory-my-dear-watson/train.csv\")\ntrain.label = train.label.replace(0,1)\nprint(train.label.unique())\ntrain_en = train[train[\"lang_abv\"]==\"en\"]\n\ntest = pd.read_csv(\"../input/contradictory-my-dear-watson/test.csv\")\ntest_en = test[test[\"lang_abv\"]==\"en\"]\n\n# print(\"TRAIN\")\n# print(train.iloc[0])\n# print(\"\\n\\n\")\n# print(\"TEST\")\n# print(test.iloc[0])\n\n\npremise = train_en.premise\nhypothesis = train_en.hypothesis\n\npremise_train = train_en.premise[:5000]\nhypothesis_train = train_en.hypothesis[:5000]\n\npremise_test = train_en.premise[5000:]\n# print(len(premise_test))\nhypothesis_test = train_en.hypothesis[5000:]\n# print(len(hypothesis_test))\ntfidv = TfidfVectorizer()\ntokenizer = tfidv.build_tokenizer()","metadata":{"execution":{"iopub.status.busy":"2021-06-09T01:17:54.584988Z","iopub.execute_input":"2021-06-09T01:17:54.585343Z","iopub.status.idle":"2021-06-09T01:17:54.684997Z","shell.execute_reply.started":"2021-06-09T01:17:54.585313Z","shell.execute_reply":"2021-06-09T01:17:54.684045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# labelCount = train_en.label.value_counts()\n# labelCount.plot.barh()","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import decomposition\n\nx_train = tfidv.fit_transform(premise_train+\" \"+hypothesis_train)\nx_test = tfidv.transform(premise_test+\" \"+hypothesis_test)\n\ny_train = train_en.label[:5000].values\ny_test = train_en.label[5000:].values\n\n#testei algumas max_depth e outros argumentos opcionais mas nao mudou mto a precisao\nclf = RandomForestClassifier(max_depth=2)\nclf.fit(x_train, y_train)\n\npredicted = clf.predict(x_test)\nacc = accuracy_score(y_test,predicted)*100\nprint(\"RandomForest: %.1f\"%acc+\"%\")","metadata":{"execution":{"iopub.status.busy":"2021-06-09T01:11:00.062357Z","iopub.execute_input":"2021-06-09T01:11:00.062774Z","iopub.status.idle":"2021-06-09T01:11:00.822065Z","shell.execute_reply.started":"2021-06-09T01:11:00.062728Z","shell.execute_reply":"2021-06-09T01:11:00.821043Z"},"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import decomposition\nfrom sklearn.preprocessing import Normalizer\n\nx_train = tfidv.fit_transform(premise_train+\" \"+hypothesis_train)\nx_test = tfidv.transform(premise_test+\" \"+hypothesis_test)\n\n#utilizar um PCA aqui dps do tfid\ntransformer = Normalizer().fit(x_train)\nx_train = transformer.transform(x_train)\nprint(\"Normalizacao finalizada...\")\npca = decomposition.PCA(n_components = 1000)\npca.fit(x_train.todense())\nx_train = pca.transform(x_train.todense())\nx_test = pca.transform(x_test.todense())\nprint(\"PCA finalizado...\")\ny_train = train_en.label[:5000].values\ny_test = train_en.label[5000:].values\n\n#pegar um modelo melhor\nclf = RandomForestClassifier(max_depth=2)\nclf.fit(x_train, y_train)\n# print(clf.score(x_test,y_test))\npredicted = clf.predict(x_test)\nacc = accuracy_score(y_test,predicted)*100\nprint(\"RandomForest: %.1f\"%acc+\"%\")","metadata":{"execution":{"iopub.status.busy":"2021-06-09T01:16:12.434851Z","iopub.execute_input":"2021-06-09T01:16:12.435231Z","iopub.status.idle":"2021-06-09T01:16:44.47978Z","shell.execute_reply.started":"2021-06-09T01:16:12.435203Z","shell.execute_reply":"2021-06-09T01:16:44.478909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train = tfidv.fit_transform(premise_train+\" \"+hypothesis_train)\nx_test = tfidv.transform(premise_test+\" \"+hypothesis_test)\n\n#utilizar um PCA aqui dps do tfid\n\ny_train = train_en.label[:5000].values\ny_test = train_en.label[5000:].values\n\n#pegar um modelo melhor\nclf = MultinomialNB()\nclf.fit(x_train, y_train)\n# print(clf.score(x_test,y_test))\npredicted = clf.predict(x_test)\nacc = accuracy_score(y_test,predicted)*100\nprint(\"MultinomialNB: %.1f\"%acc+\"%\")","metadata":{"execution":{"iopub.status.busy":"2021-06-07T20:56:38.1668Z","iopub.execute_input":"2021-06-07T20:56:38.167175Z","iopub.status.idle":"2021-06-07T20:56:38.482047Z","shell.execute_reply.started":"2021-06-07T20:56:38.167144Z","shell.execute_reply":"2021-06-07T20:56:38.480979Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import SGDClassifier\nx_train = tfidv.fit_transform(premise_train+\" \"+hypothesis_train)\nx_test = tfidv.transform(premise_test+\" \"+hypothesis_test)\n\n\n#utilizar um PCA aqui dps do tfid\nfrom sklearn.preprocessing import Normalizer\ntransformer = Normalizer().fit(x_train)\nx_train = transformer.transform(x_train)\nprint(\"normalizer finalizado...\")\n\npca = decomposition.PCA(n_components = 1000)\npca.fit(x_train.todense())\nx_train = pca.transform(x_train.todense())\nx_test = pca.transform(x_test.todense())\nprint(\"PCA finalizado...\")\n\ny_train = train_en.label[:5000].values\ny_test = train_en.label[5000:].values\n\n#pegar um modelo melhor\nclf = SGDClassifier()\nclf.fit(x_train, y_train)\nprint(\"Treinamento finalizado...\")\n# print(clf.score(x_test,y_test))\npredicted = clf.predict(x_test)\nacc1 = accuracy_score(y_test,predicted)*100\nprint(\"SGDClassifier: %.1f\"%acc1+\"%\")","metadata":{"execution":{"iopub.status.busy":"2021-06-09T01:17:58.640014Z","iopub.execute_input":"2021-06-09T01:17:58.640344Z","iopub.status.idle":"2021-06-09T01:18:28.898958Z","shell.execute_reply.started":"2021-06-09T01:17:58.640316Z","shell.execute_reply":"2021-06-09T01:18:28.897806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(\"frase tokenizada: \\n\",tokenizer(train_en.premise.iloc[0]),\"\\n\")\n# x_train = premise+\"[SEP]\"+hypothesis\n# print(len(x_train))\n# x_train = x_train.apply(tokenizer)\n# print(\"Premissa e hipotese com separador e tokenizado:\\n\",x_train.iloc[0])\n# # nao consegui dar fit no modelo com os dados tokenizados","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#tentei esse aqui mas ele da esse erro:\n# TypeError: A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array.\n# deixei essa operacao .toarray() por 10 minutos e nao rodou, entao desisti\n# sPCA = decomposition.SparsePCA()\n# sPCA.fit(x_train)\n# x_train = sPCA.transform(x_train)\n\n#TypeError: PCA does not support sparse input. See TruncatedSVD for a possible alternative.\n# pca = decomposition.PCA()\n# pca.fit(x_train)\n# x_train = pca.transform(x_train)\n\n# svd = decomposition.TruncatedSVD()\n# svd.fit(x_train)\n# x_train = svd.transform(x_train)","metadata":{},"execution_count":null,"outputs":[]}]}