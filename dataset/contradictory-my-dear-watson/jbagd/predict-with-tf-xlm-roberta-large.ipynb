{"cells":[{"metadata":{"_uuid":"7edff2eb-50cb-4285-8255-b3262dbd5161","_cell_guid":"5c1b75ee-94f3-40c8-adbe-4ca8325b9f9c","trusted":true},"cell_type":"markdown","source":"Natural Language Inferencing (NLI) is a classic NLP (Natural Language Processing) problem that involves taking two sentences (the _premise_ and the _hypothesis_ ), and deciding how they are related- if the premise entails the hypothesis, contradicts it, or neither."},{"metadata":{},"cell_type":"markdown","source":"* [Imports](#imports)\n* [Download competition data](#download-data)\n* [Exploratory data analysis](#eda)\n    - [Sanity checks](#sanity-checks-eda)\n    - [Distributions](#data-dist-eda)\n    - [Premise-hypothesis length relationship](#premise-hypothesis-eda)\n* [Download more data (caveats)](#download-more-data)\n* [Set up TPU](#set-up-tpu)\n* [Parameters](#parameters)\n* [Prepare training data](#prepare-data)\n* [Create & train model](#create-train-model)\n    - [Small scale training](#small-training)\n    - [Main model training](#main-training)\n* [Generate & submit predictions](#submit-predictions)"},{"metadata":{},"cell_type":"markdown","source":"## Imports \n<a id='Imports'></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install -q transformers==3.0.2\n!pip install -q nlp","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d3a20929-8e1d-48d2-869c-cc57f8c63cc9","_cell_guid":"20666a1f-e31b-4134-94f8-fea9a50998d3","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport gc\nimport os\n\nfrom transformers import BertTokenizer, AutoTokenizer, TFBertModel, TFXLMRobertaModel\nimport tensorflow as tf\nfrom tensorflow.keras import Input, Model, Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, LSTM, Embedding, GlobalAveragePooling1D\nfrom keras.optimizers import Adam\n\nfrom nlp import load_dataset\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle\n\nnp.random.seed(12345)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_colwidth', 100) ## to display more characters in a pandas dataframe column \nos.environ[\"WANDB_API_KEY\"] = \"0\" ## to silence warning\nsns.set_context(\"talk\", font_scale=1.05)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0b64130f-530a-4560-8afd-462a55fc14b3","_cell_guid":"44a1d22f-053c-4188-b25f-d714aa745016","trusted":true},"cell_type":"markdown","source":"<a id='download-data'></a>\n## Download competition data"},{"metadata":{"_uuid":"b9285071-38f2-421b-9b1c-9c44d41c7365","_cell_guid":"6fb2939c-b14a-450f-85dd-7220439eaf55","trusted":true},"cell_type":"markdown","source":"The training set contains a premise, a hypothesis, a label (0 = entailment, 1 = neutral, 2 = contradiction), and the language of the text. For more information about what these mean and how the data is structured, check out the data page: https://www.kaggle.com/c/contradictory-my-dear-watson/data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Input data files are available in the read-only \"../input/\" directory\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6e60d19f-aeae-417a-a10a-50cc1d5ee685","_cell_guid":"f3ad567b-a156-4ffc-a6f8-1f5e6e989a4e","trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/contradictory-my-dear-watson/train.csv\")\ntest = pd.read_csv(\"../input/contradictory-my-dear-watson/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"82e6d183-b8e1-412b-816f-c9020bac1428","_cell_guid":"d3b9a632-7abb-4bef-acd3-9cba577dc2c0","trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='eda'></a>\n## Exploratory data analysis"},{"metadata":{},"cell_type":"markdown","source":"<a id='sanity-checks-eda'></a>\n### Sanity checks"},{"metadata":{"trusted":true},"cell_type":"code","source":"## check for duplicate ids\nprint(\"Any duplicate rows (train or test): \", \n      max(train['id'].nunique() != train.shape[0], test['id'].nunique() != test.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Train and test datasets have unique, non-overlapping ids: \",\n      pd.merge(train['id'], test['id'], on = 'id', how = 'inner').shape[0] == 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Training data contains missing values: \", train.dropna().shape != train.shape)\nprint(\"Test data contains missing values: \", test.dropna().shape != test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='data-dist-eda'></a>\n### Distributions"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"# of examples in the training dataset: \", train.shape[0])\nprint(\"# of examples in the test dataset: \", test.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"% distribution by language - training dataset\")\ntrain['language'].value_counts(normalize=True) * 100.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"% distribution by language - test dataset\")\ntest['language'].value_counts(normalize=True) * 100.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## convert numeric labels to strings\ntrain['label_str'] = train['label'].map({0 : \"entailment\", 1 : \"neutral\", 2 : \"contradiction\"})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,5))\nsns.countplot(y ='label_str', data = train, alpha=.5, palette=\"muted\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,5))\nsns.countplot(y ='language', hue = \"label_str\", data = train, alpha=.5, palette=\"muted\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='premise-hypothesis-eda'></a>\n### Premise-hypothesis length relationship"},{"metadata":{"trusted":true},"cell_type":"code","source":"def _get_word_count(snt):\n    return len(str(snt).split())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['premise_len'] = train['premise'].apply(lambda x: _get_word_count(x))\ntrain['hypothesis_len'] = train['hypothesis'].apply(lambda x: _get_word_count(x))\ntrain['relative_diff'] = (train['hypothesis_len'] - train['premise_len']) * 1. / train['premise_len']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[['premise_len', 'hypothesis_len', 'relative_diff']].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for label in ('entailment', 'neutral', 'contradiction'):\n    g = sns.jointplot(x=\"premise_len\", y=\"hypothesis_len\",\n                      kind='scatter', alpha=.5, \n                      ylim= [-5, 50], xlim= [-5, 225],\n                      height=6, data=train[train['label_str'] == label])\n    g.fig.subplots_adjust(top=0.9)\n    g.fig.suptitle(\"Sentence lengths in case of \" + label, fontsize=16)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8, 5))\nat_premise_length = 7\nfor label in ('entailment', 'neutral', 'contradiction'):\n    ax = sns.distplot(train[(train['label_str'] == label) & (train['premise_len'] < at_premise_length)]['relative_diff'], \n                 hist = False, \n                 kde = True, \n                 kde_kws = {'cumulative': True}, \n                 label = label)\nplt.axvline(x=0, color='k', linestyle='--')\nax.set_xlim([-10, 25])\nplt.ylabel('CDF(relative_diff)')\nplt.title(\"Premise < \" + str(at_premise_length) + \" words\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If the hypothesis is shorter than the premise, the sentence pair is more likely to be tagged as \"entailment\" or \"contradiction\". As the hypothesis gets longer with respect to the premise, the sentence pair is more likely to be tagged as \"neutral\"."},{"metadata":{"trusted":true},"cell_type":"code","source":"train[(train['label_str'] == 'neutral') & (train['relative_diff'] > 2) & (train['language'] == 'English')].tail(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='download-more-data'></a>\n## Download more data (caveats)"},{"metadata":{},"cell_type":"markdown","source":"Caveats: Adding SNLI data led to pretty erratic behavior during validation and also resulted in a pretty poor accuracy in the submission, possibly due to some differences in how the data was generated. Other datasets, such as XNLI, overlap with the competition dataset so it wouldn't be fair to use them."},{"metadata":{"trusted":true},"cell_type":"code","source":"# About SNLI: https://nlp.stanford.edu/projects/snli/\nsnli = load_dataset(path='snli')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result_snli = []\nfor k in ['train', 'validation']:\n    for record in snli[k]:\n        c1, c2, c3 = record['premise'], record['hypothesis'], record['label']\n        if c1 and c2 and c3 in {0,1,2}:\n            result_snli.append((c1,c2,'en','English',c3))\nsnli_df = pd.DataFrame(result_snli, columns=['premise','hypothesis','lang_abv', 'language', 'label'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## To avoid duplication, check if premises in SNLI and the Kaggle training dataset overlap.\npd.merge(train['premise'], snli_df['premise'], on = 'premise', how='inner').shape[0] != 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## if we wanted to combine the datasets\n# combined_df = pd.concat([train.drop(columns=['id', 'label_str', 'premise_len', 'hypothesis_len', 'relative_diff']), snli_df], axis=0)\n# combined_df = shuffle(combined_df).reset_index(drop = True)\n# assert combined_df.shape[0] == train.shape[0] + snli_df.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## sticking to the data provided by Kaggle\nfinal_df = shuffle(train.drop(columns=['id', 'label_str', 'premise_len', 'hypothesis_len', 'relative_diff'])).reset_index(drop = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X, y = final_df[['premise', 'hypothesis']].values.tolist(), final_df['label']\nx_train, x_valid, y_train, y_valid = train_test_split(X, y, test_size=0.25, random_state=12345)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## delete snli\ndel snli\n## collect garbage\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='set-up-tpu'></a>\n## Set up TPU "},{"metadata":{"trusted":true},"cell_type":"code","source":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept ValueError:\n    strategy = tf.distribute.get_strategy() # for CPU and single GPU\n    print('Number of replicas:', strategy.num_replicas_in_sync)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='parameters'></a>\n## Parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder_handle = 'jplu/tf-xlm-roberta-large'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!curl https://s3.amazonaws.com/models.huggingface.co/bert/jplu/tf-xlm-roberta-large/config.json","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0af7b876-e719-474a-b6a5-fef500624b83","_cell_guid":"ddc3e65e-24a4-43dc-98f0-194655d17cfd","trusted":true},"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(encoder_handle)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_len = 64 # max sequence length\n# random_seed = 2021\nrandom_seed = 11887\nlearning_rate = 1e-5 # Controls how large a step is taken when updating model weights during training.\nepochs = 5\nbatch_size = 16 * strategy.num_replicas_in_sync # The number of examples that will be processed in parallel during training. Tailored for TPUs.\nloss = 'sparse_categorical_crossentropy'\nmetrics = ['accuracy']\nsteps_per_epoch = 1000\n\nauto = tf.data.experimental.AUTOTUNE","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(batch_size)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"68cee874-838e-4f0e-9f80-bfebc4a295d0","_cell_guid":"a7f5d429-083e-4d81-883f-e032dfb0e236","trusted":true},"cell_type":"markdown","source":"<a id='prepare-data'></a>\n## Prepare training data"},{"metadata":{"_uuid":"dedb18d0-63cb-492b-8fe0-b4ebb8819e4c","_cell_guid":"037b0a29-3e6d-42b6-b13a-328fab19d15d","trusted":true},"cell_type":"code","source":"def encode_sentence(s, tokenizer):\n    \"\"\"\n    Turn a sequence of words into and array of numbers using a selected tokenizer.\n    Args:\n        s (list of str) - Input string.\n        tokenizer - XLM-R tokenizer.\n    Returns:\n        (list of int) - Tokenized string.\n\n    \"\"\"\n    tokens = list(tokenizer.tokenize(s))\n    tokens.append(tokenizer.sep_token)\n    return tokenizer.convert_tokens_to_ids(tokens)\n\ndef tokenize(data, tokenizer, max_len):\n    \"\"\"\n    Encode hypotheses and premises into arrays of numbers using a selected tokenizer. \n    Args:\n        data - An array consisting of [hypothesis (str), premise (str)] pairs.\n        tokenizer - Tokenizer handle.\n        max_len - Max sequence length.\n    Returns: (dictionary of tensors)\n        input_word_ids - Indices of input sequence tokens in the vocabulary, truncated to max_len.\n        input_mask - Real input indices mapped to ones. Padding indices mapped to zeroes.\n        input_type_ids - Segment token indices to indicate first and second portions of the inputs.\n    \"\"\"\n\n    PAD_ID = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n \n    # Append a separator to each sentence, tokenize, and concatenate.\n    tokens1 = tf.ragged.constant([encode_sentence(s[0], tokenizer) for s in data], dtype=tf.int32) # ENCODED_SEQUENCE_A [SEP]\n    tokens2 = tf.ragged.constant([encode_sentence(s[1], tokenizer) for s in data], dtype=tf.int32) # ENCODED_SEQUENCE_B [SEP]\n    cls_label = [tokenizer.convert_tokens_to_ids([tokenizer.cls_token])]*tokens1.shape[0] # [CLS] ENCODED_SEQUENCE_A [SEP]\n    tokens = tf.concat([cls_label, tokens1, tokens2], axis=-1) # [CLS] ENCODED_SEQUENCE_A [SEP] ENCODED_SEQUENCE_B [SEP]\n\n    # Truncate to max_len.\n    tokens = tokens[:, :max_len]\n\n    # Pad with zeroes if len < max_len.\n    tokens = tokens.to_tensor(default_value=PAD_ID)\n    pad = max_len - tf.shape(tokens)[1]\n    tokens = tf.pad(tokens, [[0, 0], [0, pad]], constant_values=PAD_ID)\n    input_word_ids = tf.reshape(tokens, [-1, max_len])\n\n    # The input mask allows the model to cleanly differentiate between the content and the padding. \n    input_mask = tf.cast(input_word_ids != PAD_ID, tf.int32)\n    input_mask = tf.reshape(input_mask, [-1, max_len])\n\n    # Map tokens1 indices to zeroes and tokens2 indices to ones.\n    input_type_ids = tf.concat([tf.zeros_like(cls_label), tf.zeros_like(tokens1), tf.ones_like(tokens2)], axis=-1).to_tensor()\n\n\n    inputs = {\n      'input_word_ids': input_word_ids,\n      'input_mask': input_mask,\n      'input_type_ids': input_type_ids}\n\n    return inputs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_dataset(x, y, mode, batch_size):\n    \"\"\"\n    Build a batched TF training, validation, or test dataset.\n    \n    (This function is borrowed from some of the other notebooks in this competition -\n    not sure who to credit exactly so thanks all!)\n    \"\"\"\n    if mode == \"train\":\n        dataset = (\n            tf.data.Dataset\n            .from_tensor_slices((x, y))\n            .repeat()\n            .shuffle(5678)\n            .batch(batch_size)\n            .prefetch(auto)\n        )\n    elif mode == \"valid\":\n        dataset = (\n            tf.data.Dataset\n            .from_tensor_slices((x, y))\n            .batch(batch_size)\n            .cache()\n            .prefetch(auto)\n        )\n    elif mode == \"test\":\n        dataset = (\n            tf.data.Dataset\n            .from_tensor_slices(x)\n            .batch(batch_size)\n            )\n    else:\n        raise NotImplementedError\n    return dataset","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"697579c5-9f89-421c-8cbc-4321a7c93179","_cell_guid":"fc14d779-d0c6-4585-8e80-ebf539bd132c","trusted":true},"cell_type":"code","source":"x_train_ = tokenize(x_train, tokenizer, max_len)\nx_valid_ = tokenize(x_valid, tokenizer, max_len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Shape Word Ids : ', x_train_['input_word_ids'].shape)\nprint('Word Ids       : ', x_train_['input_word_ids'][0, :max_len])\nprint('Shape Mask     : ', x_train_['input_mask'].shape)\nprint('Input Mask     : ', x_train_['input_mask'][0, :max_len])\nprint('Shape Type Ids : ', x_train_['input_type_ids'].shape)\nprint('Type Ids       : ', x_train_['input_type_ids'][0, :max_len])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset = build_dataset(x_train_, y_train, \"train\", batch_size)\nvalid_dataset = build_dataset(x_valid_, y_valid, \"valid\", batch_size)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e1bf9ee4-e872-45d8-a118-a72eb7917d8b","_cell_guid":"3dbd7066-6469-4823-8e09-e2dfa0a68bcc","trusted":true},"cell_type":"markdown","source":"<a id='create-train-model'></a>\n## Create & train model"},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model(encoder_handle, random_seed, learning_rate, loss, metrics, max_len):\n    \n    tf.keras.backend.clear_session()\n    tf.random.set_seed(random_seed)\n    \n    with strategy.scope():\n        \n        input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n        input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n        # RoBERTa doesn’t use token_type_ids.\n        \n        #  Create an instance of a model defined in encoder_handle\n        roberta = TFXLMRobertaModel.from_pretrained(encoder_handle)\n        roberta = roberta([input_word_ids, input_mask])[0]\n        out = GlobalAveragePooling1D()(roberta)\n        out = Dense(3, activation='softmax')(out)\n        \n        model = Model(inputs=[input_word_ids, input_mask], outputs = out)\n        model.compile(optimizer=Adam(lr=learning_rate), loss=loss, metrics=metrics)\n    \n    model.summary()\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = build_model(encoder_handle, random_seed, learning_rate, loss, metrics, max_len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Early stopping is a technique used to prevent machine learning models from overfitting to training data. \n# The general idea is to terminate training once the model stops improving its performance on the validation/test data. \n# The patience is how many steps to wait before termination. With a patience of 2, we will terminate training \n# if the evaluation loss does not improve for 2 consecutive evaluations.\nearly_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n                                                  verbose=1,\n                                                  patience=2,\n                                                  mode='min',\n                                                  restore_best_weights=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='small-training'></a>\n### Small scale training"},{"metadata":{},"cell_type":"markdown","source":"Sanity check: Working with a very small dataset, we should achieve perfect classification."},{"metadata":{"trusted":true},"cell_type":"code","source":"x_small_train, y_small_train = X[:batch_size], y[:batch_size]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_small_train = tokenize(x_small_train, tokenizer, max_len)\nsmall_train_dataset = build_dataset(x_small_train, y_small_train, \"train\", batch_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Shape Word Ids : ', x_small_train['input_word_ids'].shape)\nprint('Word Ids       : ', x_small_train['input_word_ids'][0, :max_len])\nprint('Shape Mask     : ', x_small_train['input_mask'].shape)\nprint('Input Mask     : ', x_small_train['input_mask'][0, :max_len])\nprint('Shape Type Ids : ', x_small_train['input_type_ids'].shape)\nprint('Type Ids       : ', x_small_train['input_type_ids'][0, :max_len])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history_small_train = model.fit(small_train_dataset,\n                                steps_per_epoch=100,\n                                epochs=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ep_nbr = np.arange(1, len(history_small_train.history['loss']) + 1)\nplt.plot(ep_nbr, history_small_train.history['loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(ep_nbr, history_small_train.history['accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='main-training'></a>\n### Main model training"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = build_model(encoder_handle, random_seed, learning_rate, loss, metrics, max_len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(train_dataset,\n                    validation_data=valid_dataset,\n                    steps_per_epoch=steps_per_epoch,\n                    epochs=epochs,\n                    callbacks=[early_stopping])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# list all data in history\nprint(history.history.keys())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# summarize history for loss\nep_nbr = np.arange(1, len(history.history['accuracy']) + 1)\nplt.plot(ep_nbr, history.history['loss'])\nplt.plot(ep_nbr, history.history['val_loss'])\nplt.title('Unadjusted Loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()\n\n# Training loss is continually reported over the course of an entire epoch.\n# Validation metrics are computed over the validation set only once the current training epoch is completed.\n# This implies, that on average, training losses are measured half an epoch earlier.\n\n# plot the *shifted* training and validation loss\nplt.plot(ep_nbr - 0.5, history.history['loss'], label=\"train_loss\")\nplt.plot(ep_nbr, history.history['val_loss'], label=\"val_loss\")\nplt.title(\"Shifted Loss\")\nplt.xlabel(\"epoch\")\nplt.ylabel(\"loss\")\nplt.legend()\nplt.show()\n\n# summarize history for accuracy\nplt.plot(ep_nbr, history.history['accuracy'])\nplt.plot(ep_nbr, history.history['val_accuracy'])\nplt.title('Unadjusted Accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()\n\n# plot the *shifted* training and validation accuracy\nplt.plot(ep_nbr - 0.5, history.history['accuracy'], label=\"train_accuracy\")\nplt.plot(ep_nbr, history.history['val_accuracy'], label=\"val_accuracy\")\nplt.title(\"Shifted Accuracy\")\nplt.xlabel(\"epoch\")\nplt.ylabel(\"loss\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fb1ff888-7684-4888-b861-3c31a3f360b7","_cell_guid":"87b18b05-30f3-45f9-9c3e-6f8184934bd0","trusted":true},"cell_type":"markdown","source":"<a id='submit-predictions'></a>\n## Generate & submitting predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head(2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3bfd96e9-cf2c-41ad-bdcc-8c31408f05bd","_cell_guid":"e26cba08-7fe9-4e2e-ab2e-062a26a5fcac","trusted":true},"cell_type":"code","source":"x_test = tokenize(test[['premise', 'hypothesis']].values.tolist(), tokenizer, max_len)\ntest_dataset  = build_dataset(x_test, None, \"test\", batch_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#model predictions\npredictions_prob = model.predict(test_dataset)\nfinal = predictions_prob.argmax(axis=-1)   \n\nsubmission = pd.DataFrame()    \nsubmission['id'] = test['id']\nsubmission['prediction'] = final.astype(np.int32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"assert submission.shape[0] == test.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b8463f7e-8b2f-4c24-8eb3-a35ec02a2d6e","_cell_guid":"84abe9e3-ef04-4dac-97d2-2308a0f11313","trusted":true},"cell_type":"code","source":"submission.to_csv(\"submission.csv\", index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}