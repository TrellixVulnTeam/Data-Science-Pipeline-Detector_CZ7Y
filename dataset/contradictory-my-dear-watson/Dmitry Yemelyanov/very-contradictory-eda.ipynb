{"cells":[{"metadata":{},"cell_type":"markdown","source":"![](https://storage.googleapis.com/www.forwardit.lv/kaggle/headline.png)"},{"metadata":{},"cell_type":"markdown","source":"## con‧tra‧dic‧tion /ˌkɒntrəˈdɪkʃən /\n\n> the fact of something being the complete opposite of something else or very different from something else, so that one of them must be wrong \n\n—Cambrigde dictionary\n\n\n> a difference between two statements, beliefs, or ideas about something that means they cannot both be true \n\n—Longman dictionary\n\n> The famous pipe. How people reproached me for it! And yet, could you stuff my pipe? No, it's just a representation, is it not? So if I had written on my picture \"This is a pipe\", I'd have been lying!\n\n— René Magritte\n"},{"metadata":{},"cell_type":"markdown","source":"# Welcome\n\nWelcome to a Very Contradictory EDA! This notebook is inspired by a quote by Kaggle Grandmaster [Agnis Liukis](https://www.kaggle.com/alijs1):\n> To stand out and get some real advantage, it is necessary to do something different, find something that others didn’t notice. \n\nSo the purpose of this EDA is to find something interesting and not so straightforward about this dataset. Let's go!"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy import stats","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/contradictory-my-dear-watson/train.csv\")\ntest = pd.read_csv('/kaggle/input/contradictory-my-dear-watson/test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random exploration"},{"metadata":{},"cell_type":"markdown","source":"Let's start by looking at the raw data. Instead of the common approach - taking head of the dataset, we will take random samples:"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train.sample(frac=0.001, replace=True, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.sample(frac=0.001, replace=True, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This gives us a chance to find some non-topmost samples in training and test set that might be interesting to investigate deeper:"},{"metadata":{"trusted":true},"cell_type":"code","source":"test[(test[\"id\"]==\"40a9b0f08e\") | (test[\"id\"]==\"4e9266e800\")]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's inspect one of the sentence pairs consisting of: \"Yes, sir\" and \"I will take care of that right away Sir\". Given this sentence pair without prior knowledge which is a premise and hypothesis:\n\n* **Would you choose premise and hypothesis the same way as in the dataset?**\n* **Would you label these pairs differently depending on that assignment?**\n\n**Case 1 (neutral)**\n\n* Premise: \"Yes, sir.\"\n* Hypothesis: \"I will take care of that right away Sir.\"\n\n\n**Case 2 (entailed)**\n\n* Premise: \"I will take care of that right away Sir.\"\n* Hypothesis: \"Yes, sir.\""},{"metadata":{},"cell_type":"markdown","source":"# Annotation Artifacts? Yes sir!\n\nDespite having very similar meaning, the resulting label is different depending on sentence order! Why is this happening? Word count. **Longer sentence naturally conveys more information than a shorter one**\n1. In case longer sentence is a hypothesis (as in Case 1), we could assume the pair is more likely to be contradictory (or at least neutral), because longer sentence have more chances to contain information that contradicts a premise.\n2. In case longer sentence is a premise (as in Case 2) the pair is more likely to be entailed, just because shorter hypothesis has fewer chances to be contradictory.\n\nLet's check this idea by introducing additional feature - **word count ratio**:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_word_count(sentence):\n    return len(str(sentence).split())\n    \ndef get_word_count_ratio(premise, hypothesis):\n    return get_word_count(premise) / get_word_count(hypothesis)\n\ntrain['word_count_ratio'] = train[['premise', 'hypothesis']].apply(lambda x: get_word_count_ratio(*x), axis=1)\ntrain.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's check the samples with hypothesis being at least twice as long as the premise:"},{"metadata":{"trusted":true},"cell_type":"code","source":"longer_hypotheses = train[(train['word_count_ratio'] < 0.5)]\nlonger_hypotheses_en = longer_hypotheses.loc[longer_hypotheses[\"language\"] == \"English\"]\nlonger_hypotheses_en.head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"![](https://storage.googleapis.com/www.forwardit.lv/kaggle/Jerusalem.png)\n\nThe one about proximity to Jerusalem is of particular interest:"},{"metadata":{"trusted":true},"cell_type":"code","source":"train[train[\"id\"]==\"1be4c67e65\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n\nLet's check out all the samples with the same premise:"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.loc[train[\"premise\"] == \"Near Jerusalem\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Recalling the labels: 0 = entailment, 1 = neutral, 2 = contradiction\n\nIsn't *three miles away from Jerusalem* still being close to it? I would rather label this pair as *entailement*, however training set says it is *neutral*.\n\nClearly, we have found some pattern: samples with short premises are most *contradictory* to look at.  "},{"metadata":{},"cell_type":"markdown","source":"# The defeat of Napoleon\n\nLet's continue by filtering out samples with premises consisting of just one word:"},{"metadata":{"trusted":true},"cell_type":"code","source":"one_word_premises = train.loc[train[\"premise\"].apply(lambda premise: get_word_count(premise) == 1)]\none_word_premises_en = one_word_premises.loc[one_word_premises[\"language\"] == \"English\"]\none_word_premises_en.head(30)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The following pairs are great examples of what could go wrong with the model training for Natural Language Inference - some of these sentences require very specific domain knowledge:\n* *Dr Bauerstein* and *Alfred Inglethorp* are fictional characters in *Agatha Christie*'s detective novel *The Mysterious Affair at Styles*.\n* *Saint-Paul-de-Vence* is a commune in the Alpes-Maritimes department in the Provence-Alpes-Côte d'Azur region of Southeastern France. \n* *D-Day* is the name of the Normandy landings operation during World War II, on June 6, 1944.\n* *Waterloo* is a municipality in Belgium from which the famous *Waterloo battle* took its name.\n* *Melatonin* is a hormone made by the pineal gland. It helps your body know when it's time to sleep and wake up."},{"metadata":{},"cell_type":"markdown","source":"![](https://storage.googleapis.com/www.forwardit.lv/kaggle/Regiment-Charles-Ewart.png)\n\n\n1. **Is Waterloo is the defeat of Napoleon?** It depends on how one understands the word \"Waterloo\". Despite being commonly referred as *Waterloo battle*, direct meaning of this word is the municipality, not the famous Napoleon's combat. Thus Waterloo might be a battle, but primarily it is a geographic name, thus the pair is *neutral*.\n2. **Is Bauerstein a doctor?** Given you have just read Agatha Cristie's novel *The Mysterious Affair at Styles*, you may recall Dr. Bauerstein and give a positive answer. We should account for no prior knowledge of Agatha Cristie and in this case Bauersten is just a random surname, so the pair is *neutral*.\n3. **Are Alfred Inglethorp and Bauerstein the same person?** Obviously they are not, given you are familiar with Agatha Cristie's *The Mysterious Affair at Styles*. On the other hand, the pair is not \"Agatha Cristie's Bauerstein\" vs \"Agatha Cristie's Alfred Inglethorp\", so we should treat those names without any connection to the original novel. In this case those are just random surnames, which are different and are thus *contradictionary*.\n\nOne might argue the last statement, as sometimes a person has two names. For example, how would you label the pair *Agatha Cristie* and *Mary Westmacott*? (the latter is one of her pseudonyms)\n* Following the logic of *Alfred Inglethorp vs Bauerstein*, as the names are different, we should say it is *contradictionary*.\n* Given the knowledge that Mary Westmacott is Agatha Cristie's pseudonym, we should say it is an *entailement*.\n* Given the knowledge that there are thousands of people with the very same name, Agatha Christie, the pair might be *neutral*.\n\nThis leads to a conclusion, that humans judge on the meaning of the sentence pairs given **prior knowledge** and **context**. \n\nIn certain cases one could interprete the very same pairs of sentences differently given different knowledge and context.\n\n> Should our model be aware of the Agatha Cristie and Napoleon? \n\n![](https://storage.googleapis.com/www.forwardit.lv/kaggle/napoleon_vs_agatha_cristie.png)"},{"metadata":{},"cell_type":"markdown","source":"Let's recall the sample about Waterloo again. Would *The defeat of Napoleon* be an *entailement* if the premise would explicitly say about Waterloo being referred as a battle, e.g. *The battle of Waterloo*? It depends on our personal judgement on some historic event. \n\nThe great example of this paradox is *The Battle of Borodino*, which has very diverse set of opinions expressed by different historians:\n* Some claim it is a victory for the French\n* Others say it is a victory for the Russians\n* The third group exists saying it was a *Pyrrhic victory*, as it ultimately cost Napoleon his army\n\nSo the other important question arises: \n\n> How could one define the crossing line between natual language understanding and having a certain (probably biased) opinion on some facts?"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.label.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = {\n    0: \"Entailment\",\n    1: \"Neutral\",\n    2: \"Contradiction\"\n}\ndef dist_plot(df): \n    plt.figure(figsize=(16, 10))\n    for label in df.label.unique():\n        subset = df[train['label'] == label]\n\n        # Draw the density plot\n        sns.distplot(subset['word_count_ratio'], hist = False, kde = True,\n                     kde_kws = {'linewidth': 3},\n                     label = labels[label])\n\n    # Plot formatting\n    plt.legend(prop={'size': 16}, title = 'Label')\n    plt.title('Word count vs Label')\n    plt.xlabel('Word Count Ratio')\n    plt.ylabel('Density')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dist_plot(longer_hypotheses_en)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,10))\ncor = train.corr()\nsns.heatmap(cor, annot=True, cmap=plt.cm.Reds)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"One would also note the sentences in these \"strange\" samples are of a very different length. With that we will formulate our own set of hypotheseses based on word count in respective sentences:\n1. Sentences of the same length are more likely to be neutral\n2. Sentences of the different length are more likely to either contradict or entail\n3. In case hypothesis is longer than a premise, it has higher entropy containing more information which in turns increases chances of a contradiction\n4. In case hypothesis is shorter than a premise, it has lower entropy and have more chances to be a \"summary\": either neutral or entailement"},{"metadata":{},"cell_type":"markdown","source":" word overlap is measured by the percentage of tokens from the question that appear in the evidence."},{"metadata":{},"cell_type":"markdown","source":"We will check that assumption on a training set by introducing anothe feature: **word count ratio**:"},{"metadata":{},"cell_type":"markdown","source":"Let's check what part of a training has hypotheses longer than premises:"},{"metadata":{"trusted":true},"cell_type":"code","source":"longer_hypotheses = train[(train['word_count_ratio'] < 0.8)]\nlen(longer_hypotheses) / len(train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's check how this correlates to the label (our assumption is that contradiction will prevail)"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(longer_hypotheses['label'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(train['label'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# About the author\n\nThis notebook is published under the **Data Science DJ** initiative with the goal of giving you distilled pieces of valuable information, short and concise, easy to comprehend. \n\nI spend a few hours every day to write a single post about a single concept. You can find them by:\n\n* [Joining my Telegram channel](https://t.me/datasciencedj)\n* [Following my LinkedIn tag](https://www.linkedin.com/feed/hashtag/?keywords=datasciencedj)\n\nIf this work gives you joy, or maybe even inspiration, please consider contributing to my [Patreon account](https://www.patreon.com/datasciencedj).\nThank you!"},{"metadata":{},"cell_type":"markdown","source":"# Resources\n\n1. https://www.arxiv-vanity.com/papers/1803.02324/"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}