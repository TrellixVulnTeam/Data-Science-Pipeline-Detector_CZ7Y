{"cells":[{"metadata":{},"cell_type":"markdown","source":"# NLP Contradictory - LASER Embeddings + Keras\n\nThis notebook is a port of my [NLP Disaster Tweets - LASER Embeddings + Keras](https://www.kaggle.com/jamesmcguigan/nlp-laser-embeddings-keras) notebook, which encodes the tweets using [LASER](https://github.com/yannvgn/laserembeddings) multilingual sentence embeddings,\nfollowed by a [TF Keras](https://www.tensorflow.org/api_docs/python/tf/keras) dense neural network."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"!pip install -q laserembeddings laserembeddings[zh] laserembeddings[ja]\n!pip install -q ftfy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import fasttext\nimport ftfy\nimport html\nimport laserembeddings\nimport numpy as np\nimport pandas as pd\nimport re\nimport tensorflow as tf\nimport sys\n\nfrom fastcache import clru_cache\nfrom laserembeddings import Laser\nfrom typing import List, Union\nfrom urllib.parse import unquote\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.preprocessing import OneHotEncoder","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.read_csv('../input/contradictory-my-dear-watson/train.csv', index_col=0).fillna('')\ndf_test  = pd.read_csv('../input/contradictory-my-dear-watson/test.csv',  index_col=0).fillna('')\ndf_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# LASER Embeddings\n\nThis encodes each of the strings as a LASER embedding (1024 dimentional vector)"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%bash\n# DOCS: https://github.com/facebookresearch/LASER/blob/master/install_models.sh\n\nmkdir -p models/laser/\n# for FILE in bilstm.eparl21.2018-11-19.pt eparl21.fcodes eparl21.fvocab bilstm.93langs.2018-12-26.pt 93langs.fcodes 93langs.fvocab; do\nfor FILE in bilstm.93langs.2018-12-26.pt 93langs.fcodes 93langs.fvocab; do\n    wget -cq https://dl.fbaipublicfiles.com/laser/models/$FILE -O models/laser/$FILE\ndone","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from config import config\n# from src.utils.fasttest_model import language_detect\n# from src.utils.punkt_tokenizer import punkt_tokenize_sentences\n\nconfig = {\n    \"laser\": {\n        \"base_dir\":  \"./models/laser\",\n        \"bpe_codes\": \"./models/laser/93langs.fcodes\",\n        \"bpe_vocab\": \"./models/laser/93langs.fvocab\",\n        \"encoder\":   \"./models/laser/bilstm.93langs.2018-12-26.pt\",\n    }\n}\n\n# Instantiate encoder\n# BUG: CUDA GPU memory is exceeded if both laser and labse are loaded together \n@clru_cache(None)\ndef get_laser_model():\n    laser_model = Laser(\n        bpe_codes = config['laser']['bpe_codes'],\n        bpe_vocab = config['laser']['bpe_vocab'],\n        encoder   = config['laser']['encoder'],\n        tokenizer_options = None,\n        embedding_options = None\n    )\n    return laser_model\n\n\ndef laser_encode(text: Union[str, List[str]], lang='en', normalize=True) -> np.ndarray:\n    \"\"\"\n    Encodes a corpus of text using LASER\n    :param text: Large block of text (will be tokenized), or list of pre-tokenized sentences\n    :param lang: 2 digit language code (optional autodetect)\n    :return:     embedding matrix\n    \"\"\"\n    laser_model = get_laser_model()\n    \n    # lang = lang or language_detect(text, threshold=0.0)\n    if isinstance(text, str):\n        # sentences = punkt_tokenize_sentences(text, lang=lang)\n        sentences = [ text ]\n    else:\n        sentences = list(text)\n\n    embedding = laser_model.embed_sentences(sentences, lang=lang)\n    \n    if normalize:\n        embedding = embedding / np.sqrt(np.sum(embedding**2, axis=1)).reshape(-1,1)\n        \n    return embedding","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def encode_X(df):\n    premise    = laser_encode(df['premise'],    lang=df['lang_abv'])\n    hypothesis = laser_encode(df['hypothesis'], lang=df['lang_abv'])\n    cosine = np.array([\n        cosine_similarity( premise[n].reshape(1,-1), hypothesis[n].reshape(1,-1) )\n        for n in range(len(df))\n    ]).reshape(-1,1)\n    X = np.array([\n        np.concatenate([ premise[n], hypothesis[n], cosine[n] ])\n        for n in range(len(df))\n    ])\n\n    # print('premise.shape    ', premise.shape)     # (12120, 1024)\n    # print('hypothesis.shape ', hypothesis.shape)  # (12120, 1024)\n    # print('cosine.shape     ', cosine.shape)      # (12120, 1)\n    # print('X.shape          ', X.shape)           # (12120, 2049)\n    return X\n    \n\ndef encode_Y(df):\n    encoder = OneHotEncoder().fit([ [0], [1], [2] ])\n    return encoder.transform( df['label'].to_numpy().reshape(-1,1) ).toarray()\n\ndef decode_Y(one_hot_encoded):\n    decoded = tf.argmax(one_hot_encoded, axis=1)\n    return decoded.numpy().astype(np.int32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nX_train = encode_X(df_train)\nY_train = encode_Y(df_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Neural Network - TF Keras\n\nDefine and train a dense neural network. \n\nThis inputs a 1024 LASER embedding and outputs a 1 bit classification prediction.\n\nA triangular shaped architecture is used, including Dropout and BatchNorm."},{"metadata":{"trusted":true},"cell_type":"code","source":"# DOCS: https://keras.io/examples/keras_recipes/antirectifier/\n\ndef model_compile_fit(\n    X, Y, \n    model      = None,\n    test_size  = 0.2,\n    epochs     = 1000, \n    batch_size = 32, \n    verbose    = 2,\n):\n    # Build the model\n    if model is None:\n        model = tf.keras.Sequential([\n            tf.keras.Input(shape=(X.shape[1],)),\n            tf.keras.layers.Dense(512, activation=tf.keras.layers.LeakyReLU(alpha=0.1)),\n                tf.keras.layers.BatchNormalization(),\n                tf.keras.layers.Dropout(0.25),\n            tf.keras.layers.Dense(128, activation=tf.keras.layers.LeakyReLU(alpha=0.1)),\n                tf.keras.layers.BatchNormalization(),\n                tf.keras.layers.Dropout(0.25),\n            tf.keras.layers.Dense(32, activation=tf.keras.layers.LeakyReLU(alpha=0.1)),\n                tf.keras.layers.BatchNormalization(),\n                tf.keras.layers.Dropout(0.25),\n            tf.keras.layers.Dense(8, activation=tf.keras.layers.LeakyReLU(alpha=0.1)),\n                tf.keras.layers.BatchNormalization(),\n                tf.keras.layers.Dropout(0.25),\n            tf.keras.layers.Dense(Y.shape[1], activation=tf.keras.activations.sigmoid),\n        ])\n        model.summary()\n            \n    # Compile the model\n    model.compile(\n        loss      = tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n        optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5),\n        metrics   = [ tf.keras.metrics.CategoricalAccuracy() ],\n    )\n    \n    # Train the model\n    if test_size:\n        X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size)\n        \n        model.fit(\n            X_train, Y_train, \n            batch_size = batch_size, \n            epochs     = epochs, \n            validation_split = test_size,\n            callbacks = [\n                tf.keras.callbacks.EarlyStopping(\n                    monitor  = 'val_loss', \n                    mode     = 'min', \n                    verbose  = 0, \n                    patience = 100\n                ),\n                tf.keras.callbacks.ModelCheckpoint(\n                    'model.h5', \n                    monitor = 'val_categorical_accuracy', \n                    mode    = 'max', \n                    verbose = 0, \n                    save_best_only = True\n                )\n            ],\n            verbose = verbose\n        )\n    else:\n        X_train, Y_train = X, Y\n\n        model.fit(\n            X, Y, \n            batch_size = batch_size, \n            epochs     = epochs, \n            verbose    = verbose,\n        )\n        \n        \n    print()\n    print('Train Accuracy')\n    model.evaluate(X_train, Y_train)\n\n    if test_size:\n        print('Test Accuracy')\n        model.evaluate(X_test, Y_test)\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nmodel = model_compile_fit(X_train, Y_train, test_size=0.2)\nmodel = model_compile_fit(X_train, Y_train, test_size=0, epochs=1000, model=model, verbose=0)\nmodel.save('model.h5')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time \n\nX_test = encode_X( df_test)\nY_test = decode_Y( model.predict(X_test) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_submission = pd.read_csv('../input/contradictory-my-dear-watson/sample_submission.csv', index_col=0).fillna('')\ndf_submission['prediction'] = Y_test\ndf_submission.to_csv('submission.csv')\n!head submission.csv","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Further Reading\n\nThis notebook is part of a series exploring Natural Language Processing\n\nNLP Disaster Tweets\n- 0.74164 - [NLP Logistic Regression](https://www.kaggle.com/jamesmcguigan/disaster-tweets-logistic-regression)\n- 0.77536 - [NLP TF-IDF Classifier](https://www.kaggle.com/jamesmcguigan/disaster-tweets-tf-idf-classifier)\n- 0.78302 - [NLP LASER Embeddings + Keras](https://www.kaggle.com/jamesmcguigan/nlp-laser-embeddings-keras)\n- 0.79742 - [NLP Naive Bayes](https://www.kaggle.com/jamesmcguigan/nlp-naive-bayes)\n\nContradictory, My Dear Watson\n- 0.50779 - [NLP Contradictory - LASER Embeddings + Keras](https://www.kaggle.com/jamesmcguigan/nlp-contradictory-laser-embeddings-keras)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}