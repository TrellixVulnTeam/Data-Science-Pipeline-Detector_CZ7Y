{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf\nfrom transformers import BertTokenizer\nimport os\nfrom transformers import BertTokenizer, TFBertModel\nimport tensorflow as tf\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-16T13:45:47.52836Z","iopub.execute_input":"2022-03-16T13:45:47.529021Z","iopub.status.idle":"2022-03-16T13:45:47.538723Z","shell.execute_reply.started":"2022-03-16T13:45:47.528981Z","shell.execute_reply":"2022-03-16T13:45:47.537833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train= pd.read_csv('/kaggle/input/contradictory-my-dear-watson/train.csv')\ndf_test = pd.read_csv(\"../input/contradictory-my-dear-watson/test.csv\")\ndf_train.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-03-16T13:45:47.540778Z","iopub.execute_input":"2022-03-16T13:45:47.541277Z","iopub.status.idle":"2022-03-16T13:45:47.655769Z","shell.execute_reply.started":"2022-03-16T13:45:47.541223Z","shell.execute_reply":"2022-03-16T13:45:47.654814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train= df_train.drop(['id', 'lang_abv', 'language'], axis=1)\ndf_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-16T13:45:47.657381Z","iopub.execute_input":"2022-03-16T13:45:47.657922Z","iopub.status.idle":"2022-03-16T13:45:47.671541Z","shell.execute_reply.started":"2022-03-16T13:45:47.657878Z","shell.execute_reply":"2022-03-16T13:45:47.670806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_name = 'bert-base-multilingual-cased'\ntokenizer = BertTokenizer.from_pretrained(model_name)","metadata":{"execution":{"iopub.status.busy":"2022-03-16T13:45:47.67316Z","iopub.execute_input":"2022-03-16T13:45:47.673638Z","iopub.status.idle":"2022-03-16T13:45:48.775404Z","shell.execute_reply.started":"2022-03-16T13:45:47.673601Z","shell.execute_reply":"2022-03-16T13:45:48.774308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(tokenizer.vocab) # check the vocabulary size","metadata":{"execution":{"iopub.status.busy":"2022-03-16T13:45:48.777516Z","iopub.execute_input":"2022-03-16T13:45:48.777772Z","iopub.status.idle":"2022-03-16T13:45:48.784097Z","shell.execute_reply.started":"2022-03-16T13:45:48.777745Z","shell.execute_reply":"2022-03-16T13:45:48.783183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def encode_sentence(s):\n    \"\"\" ENCODE SENTENCES WITH TOKENIZER\"\"\"\n    tokens = list(tokenizer.tokenize(s))\n    tokens.append('[SEP]')\n    return tokenizer.convert_tokens_to_ids(tokens)","metadata":{"execution":{"iopub.status.busy":"2022-03-16T13:45:48.785332Z","iopub.execute_input":"2022-03-16T13:45:48.785604Z","iopub.status.idle":"2022-03-16T13:45:48.795054Z","shell.execute_reply.started":"2022-03-16T13:45:48.785575Z","shell.execute_reply":"2022-03-16T13:45:48.794486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def bert_encode(hypotheses, premises, tokenizer):\n    \"\"\" ENCODE DATA FOR BERT\"\"\"\n    num_examples = len(hypotheses)\n    print(\"num_examples = \", num_examples)\n    sentence1 = tf.ragged.constant([encode_sentence(s) for s in np.array(hypotheses)])\n    print(\"sentence1.shape = \", sentence1.shape)\n    sentence2 = tf.ragged.constant([encode_sentence(s) for s in np.array(premises)])\n    print(\"sentence2.shape = \", sentence2.shape)\n    cls_ = [tokenizer.convert_tokens_to_ids(['[CLS]'])] * sentence1.shape[0]\n    input_word_ids = tf.concat([cls_, sentence1, sentence2], axis=-1)\n    print(\"input_word_ids.shape = \", input_word_ids.shape)\n    # 300 - as my example\n    # because we have train_input (12120; 259), test_input (5159; 234)\n    # and shape[1] should be the same in each dataset\n    # that is why we creating (xxx; 300) shape in to_tensor() functions  \n    input_mask = tf.ones_like(input_word_ids).to_tensor(shape=(input_word_ids.shape[0], 300)) \n    print(\"input_mask.shape = \", input_mask.shape)\n    \n    type_cls = tf.zeros_like(cls_)\n    type_s1 = tf.zeros_like(sentence1)\n    type_s2 = tf.ones_like(sentence2)\n    \n    input_type_ids = tf.concat([type_cls, type_s1, type_s2], axis=-1).to_tensor(shape=(input_word_ids.shape[0], 300))\n    \n    inputs = {'input_word_ids': input_word_ids.to_tensor(shape=(input_word_ids.shape[0], 300)),\n              'input_mask': input_mask,\n              'input_type_ids': input_type_ids}\n    print()\n    \n    return inputs","metadata":{"execution":{"iopub.status.busy":"2022-03-16T13:45:48.796126Z","iopub.execute_input":"2022-03-16T13:45:48.796474Z","iopub.status.idle":"2022-03-16T13:45:48.808737Z","shell.execute_reply.started":"2022-03-16T13:45:48.796444Z","shell.execute_reply":"2022-03-16T13:45:48.807879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# encode data\ntrain_input = bert_encode(df_train[\"premise\"].values, df_train[\"hypothesis\"].values, tokenizer)\ntest_input = bert_encode(df_test[\"premise\"].values, df_test[\"hypothesis\"].values, tokenizer)","metadata":{"execution":{"iopub.status.busy":"2022-03-16T13:45:48.809864Z","iopub.execute_input":"2022-03-16T13:45:48.810208Z","iopub.status.idle":"2022-03-16T13:46:09.65617Z","shell.execute_reply.started":"2022-03-16T13:45:48.810182Z","shell.execute_reply":"2022-03-16T13:46:09.655296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_len = train_input[\"input_word_ids\"].shape[1]\n\ndef create_model():\n    \"\"\" BUILD MODEL \"\"\"\n    bert_encoder = TFBertModel.from_pretrained(model_name)\n    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    input_type_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_type_ids\")\n\n    embedding = bert_encoder([input_word_ids, input_mask, input_type_ids])[0]\n    output = tf.keras.layers.Dense(3, activation='softmax')(embedding[:,0,:])\n\n    model = tf.keras.Model(inputs=[input_word_ids, input_mask, input_type_ids], outputs=output)\n    model.compile(tf.keras.optimizers.Adam(lr=1e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-03-16T13:46:09.657594Z","iopub.execute_input":"2022-03-16T13:46:09.657986Z","iopub.status.idle":"2022-03-16T13:46:09.668282Z","shell.execute_reply.started":"2022-03-16T13:46:09.657942Z","shell.execute_reply":"2022-03-16T13:46:09.66746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# detect and init the TPU\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n\n# instantiate a distribution strategy\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n\n# instantiating the model in the strategy scope creates the model on the TPU\nwith tpu_strategy.scope():\n    model = create_model()\n    model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-03-16T13:46:09.669617Z","iopub.execute_input":"2022-03-16T13:46:09.669839Z","iopub.status.idle":"2022-03-16T13:46:35.635317Z","shell.execute_reply.started":"2022-03-16T13:46:09.669815Z","shell.execute_reply":"2022-03-16T13:46:35.634429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n# train model normally\nmodel_history = model.fit(train_input, \n                          df_train[\"label\"].values, \n                          epochs = 20, \n                          verbose = 1,\n                          batch_size = 128, \n                          validation_split = 0.2)","metadata":{"execution":{"iopub.status.busy":"2022-03-16T13:46:35.636637Z","iopub.execute_input":"2022-03-16T13:46:35.636853Z","iopub.status.idle":"2022-03-16T13:54:11.004568Z","shell.execute_reply.started":"2022-03-16T13:46:35.636828Z","shell.execute_reply":"2022-03-16T13:54:11.003715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calculate_results(y_true, y_pred):\n    \"\"\" CALCULATE RESULTS\"\"\"\n    # Calculate model accuracy\n    model_accuracy = accuracy_score(y_true, y_pred) * 100\n    # Calculate model precision, recall and f1 score using \"weighted\" average\n    model_precision, model_recall, model_f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\")\n    model_results = {\"accuracy\": model_accuracy,\n                     \"precision\": model_precision,\n                     \"recall\": model_recall,\n                     \"f1\": model_f1}\n    return model_results","metadata":{"execution":{"iopub.status.busy":"2022-03-16T13:54:11.00609Z","iopub.execute_input":"2022-03-16T13:54:11.006481Z","iopub.status.idle":"2022-03-16T13:54:11.012931Z","shell.execute_reply.started":"2022-03-16T13:54:11.006434Z","shell.execute_reply":"2022-03-16T13:54:11.012074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get the probabilities\ny_prob = model.predict(test_input)\n# get the classes\ny_hat = y_prob.argmax(axis=-1)","metadata":{"execution":{"iopub.status.busy":"2022-03-16T13:54:11.014766Z","iopub.execute_input":"2022-03-16T13:54:11.015008Z","iopub.status.idle":"2022-03-16T13:54:27.761849Z","shell.execute_reply.started":"2022-03-16T13:54:11.014981Z","shell.execute_reply":"2022-03-16T13:54:27.760871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = df_test.id.copy().to_frame()\nsubmission['prediction'] = y_hat\nsubmission.head() # check submission\nsubmission.to_csv(\"submission.csv\", index = False) # save file","metadata":{"execution":{"iopub.status.busy":"2022-03-16T13:54:27.764696Z","iopub.execute_input":"2022-03-16T13:54:27.765313Z","iopub.status.idle":"2022-03-16T13:54:27.786116Z","shell.execute_reply.started":"2022-03-16T13:54:27.765281Z","shell.execute_reply":"2022-03-16T13:54:27.785393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}