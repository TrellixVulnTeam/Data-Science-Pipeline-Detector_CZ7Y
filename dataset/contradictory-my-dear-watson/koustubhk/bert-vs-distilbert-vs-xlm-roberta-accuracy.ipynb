{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Libraries","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Generic\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os, warnings\nwarnings.filterwarnings('ignore')\n\n# TensorFlow\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\n\n# Transformer Model\nfrom transformers import BertTokenizer, TFBertModel               #BERT\nfrom transformers import DistilBertTokenizer, TFDistilBertModel    #DistilBERT\nfrom transformers import XLMRobertaTokenizer, TFXLMRobertaModel    #XLM-RoBERTa\n\n\n# SKLearn Library\nfrom sklearn.model_selection import train_test_split\n\n# Garbage Collector\nimport gc\n\n# Tabulate\nfrom tabulate import tabulate\n\nos.environ[\"WANDB_API_KEY\"] = \"0\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initialize TPU\n\ndef Init_TPU():  \n\n    try:\n        resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n        tf.config.experimental_connect_to_cluster(resolver)\n        tf.tpu.experimental.initialize_tpu_system(resolver)\n        strategy = tf.distribute.experimental.TPUStrategy(resolver)\n        REPLICAS = strategy.num_replicas_in_sync\n        print(\"Connected to TPU Successfully:\\n TPUs Initialised with Replicas:\",REPLICAS)\n        \n        return strategy\n    \n    except ValueError:\n        \n        print(\"Connection to TPU Falied\")\n        print(\"Using default strategy for CPU and single GPU\")\n        strategy = tf.distribute.get_strategy()\n        \n        return strategy\n    \nstrategy=Init_TPU()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define Dataset Path\npath = '../input/contradictory-my-dear-watson/'","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Load Training Data\ntrain_url = os.path.join(path,'train.csv')\ntrain_data = pd.read_csv(train_url, header='infer')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Garbage Collection\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Prep\n\n* BERT Model = bert-base-multilingual-cased\n* DistilBERT Model = distilbert-base-multilingual-cased\n* XLM-RoBERTa Model = xlm-roberta-base","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# Transformer Model Name\nBert_model = 'bert-base-multilingual-cased'\ndistilBert_model = 'distilbert-base-multilingual-cased'\nxlmRoberta_model = 'jplu/tf-xlm-roberta-base'\n\n# Define Tokenizer for each\nBert_toknzr = BertTokenizer.from_pretrained(Bert_model)\ndistilBert_toknzr = DistilBertTokenizer.from_pretrained(distilBert_model)\nxlmRoberta_toknzr = XLMRobertaTokenizer.from_pretrained(xlmRoberta_model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the output of tokenizer\nsentence = 'Elementary, My Dear Watson!'\n\nprint(\"BERT Model Tokenizer Output:\",Bert_toknzr.convert_tokens_to_ids(list(Bert_toknzr.tokenize(sentence))))\nprint(\"DistilBERT Model Tokenizer Output:\",distilBert_toknzr.convert_tokens_to_ids(list(distilBert_toknzr.tokenize(sentence))))\nprint(\"XLM-RoBERTa Model Tokenizer Output:\",xlmRoberta_toknzr.convert_tokens_to_ids(list(xlmRoberta_toknzr.tokenize(sentence))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create seperate list from Train & Test Dataframes with only Premise & Hypothesis\ntrain = train_data[['premise','hypothesis']].values.tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define Max Length\nmax_len = 80   # << change if you wish\n\n# Encode the training & test data - BERT\ntrain_encode_Bert = Bert_toknzr.batch_encode_plus(train, pad_to_max_length=True, max_length=max_len)\n\n# Encode the training & test data - DistilBERT\ntrain_encode_DistilBert = distilBert_toknzr.batch_encode_plus(train, pad_to_max_length=True, max_length=max_len)\n\n# Encode the training & test data - XLM-RoBERTa\ntrain_encode_XlmRoberta = xlmRoberta_toknzr.batch_encode_plus(train, pad_to_max_length=True, max_length=max_len)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split the Training Data into Training (90%) & Validation (10%)\n\ntest_size = 0.1  # << change if you wish\n\n# BERT\nx_tr_bert, x_val_bert, y_tr_bert, y_val_bert = train_test_split(train_encode_Bert['input_ids'], train_data.label.values, test_size=test_size)\n\n# DistilBERT\nx_tr_Dbert, x_val_Dbert, y_tr_Dbert, y_val_Dbert = train_test_split(train_encode_DistilBert['input_ids'], train_data.label.values, test_size=test_size)\n\n# XLM-RoBERTa\nx_tr_XR, x_val_XR, y_tr_XR, y_val_XR = train_test_split(train_encode_XlmRoberta['input_ids'], train_data.label.values, test_size=test_size)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#garbage collect\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loading Data Into TensorFlow Dataset\nAUTO = tf.data.experimental.AUTOTUNE\nbatch_size = 16 * strategy.num_replicas_in_sync\n\n#BERT\ntr_ds_bert = (tf.data.Dataset.from_tensor_slices((x_tr_bert, y_tr_bert)).repeat().shuffle(2048).batch(batch_size).prefetch(AUTO))\nval_ds_bert = (tf.data.Dataset.from_tensor_slices((x_val_bert, y_val_bert)).batch(batch_size).prefetch(AUTO))\n\n#DistilBERT\ntr_ds_Dbert = (tf.data.Dataset.from_tensor_slices((x_tr_Dbert, y_tr_Dbert)).repeat().shuffle(2048).batch(batch_size).prefetch(AUTO))\nval_ds_Dbert = (tf.data.Dataset.from_tensor_slices((x_val_Dbert, y_val_Dbert)).batch(batch_size).prefetch(AUTO))\n\n#XLM-RoBERTa\ntr_ds_XR = (tf.data.Dataset.from_tensor_slices((x_tr_XR, y_tr_XR)).repeat().shuffle(2048).batch(batch_size).prefetch(AUTO))\nval_ds_XR = (tf.data.Dataset.from_tensor_slices((x_val_XR, y_val_XR)).batch(batch_size).prefetch(AUTO))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Garbage Collection\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Build & Train Model\n\nNow we shall build a model with the pre-trained BERT transformer model into Keras Functional Model","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"def build_model(strategy):\n    with strategy.scope():\n        bert_encoder = TFBertModel.from_pretrained(Bert_model)  #BERT\n        DistilBert_encoder = TFDistilBertModel.from_pretrained(distilBert_model)  #DistilBERT\n        XLMRoberta_encoder = TFXLMRobertaModel.from_pretrained(xlmRoberta_model)  #XLM-RoBERTa\n        \n        input_layer = Input(shape=(max_len,), dtype=tf.int32, name=\"input_layer\")\n        \n        sequence_output_bert = bert_encoder(input_layer)[0]\n        sequence_output_Dbert = DistilBert_encoder(input_layer)[0]\n        sequence_output_XR = XLMRoberta_encoder(input_layer)[0]\n        \n        cls_token_bert = sequence_output_bert[:, 0, :]\n        cls_token_Dbert = sequence_output_Dbert[:, 0, :]\n        cls_token_XR = sequence_output_XR[:, 0, :]\n                \n        output_layer_bert = Dense(3, activation='softmax')(cls_token_bert)\n        output_layer_Dbert = Dense(3, activation='softmax')(cls_token_Dbert)\n        output_layer_XR = Dense(3, activation='softmax')(cls_token_XR)\n        \n        model1 = Model(inputs=input_layer, outputs=output_layer_bert)\n        model2 = Model(inputs=input_layer, outputs=output_layer_Dbert)\n        model3 = Model(inputs=input_layer, outputs=output_layer_XR)\n        \n        \n        model1.compile(\n            Adam(lr=1e-5), \n            loss='sparse_categorical_crossentropy', \n            metrics=['accuracy']\n        )\n        \n        model2.compile(\n            Adam(lr=1e-5), \n            loss='sparse_categorical_crossentropy', \n            metrics=['accuracy']\n        )\n            \n        model3.compile(\n            Adam(lr=1e-5), \n            loss='sparse_categorical_crossentropy', \n            metrics=['accuracy']\n        )\n        \n        \n        return model1, model2, model3\n    \n\n# Applying the build model function\nmodel_bert, model_Dbert, model_XLMRoberta = build_model(strategy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# Train the Model\n\nepochs = 30  # < change if you wish\nn_steps = len(train_data) // batch_size ","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true,"collapsed":true},"cell_type":"code","source":"# Train BERT Model\n\nmodel_bert.fit(tr_ds_bert, \n          steps_per_epoch = n_steps, \n          validation_data = val_ds_bert,\n          epochs = epochs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Garbage Collection\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true,"collapsed":true},"cell_type":"code","source":"# Train DistilBERT Model\n\nmodel_Dbert.fit(tr_ds_Dbert, \n          steps_per_epoch = n_steps, \n          validation_data = val_ds_Dbert,\n          epochs = epochs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Garbage Collection\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true,"collapsed":true},"cell_type":"code","source":"# Train XLM-RobERTa Model\n\nmodel_XLMRoberta.fit(tr_ds_XR, \n          steps_per_epoch = n_steps, \n          validation_data = val_ds_XR,\n          epochs = epochs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Garbage Collection\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Evaluation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Evaluate BERT\nres_bert = model_bert.evaluate(val_ds_bert, verbose=0)\n\n# Evaluate DistilBERT\nres_Dbert = model_Dbert.evaluate(val_ds_Dbert, verbose=0)\n\n# Evaluate XLM-RoBERTa\nres_XlmRoberta = model_XLMRoberta.evaluate(val_ds_XR, verbose=0)\n\n#Tabulate Data\ntab_data = [[\"BERT\",\"30\",\"Adam\",\"128\",\"1e-5\",'{:.2%}'.format(res_bert[1])],\n            [\"DistilBERT\",\"30\",\"Adam\",\"128\",\"1e-5\",'{:.2%}'.format(res_Dbert[1])],\n            [\"XLM-RoBERTa\",\"30\",\"Adam\",\"128\",\"1e-5\",'{:.2%}'.format(res_XlmRoberta[1])]]   \n    \nprint(tabulate(tab_data, headers=['Models','Epochs','Optimizer','Batch Size','Learning Rate','Accuracy'], tablefmt='pretty'))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}