{"cells":[{"metadata":{},"cell_type":"markdown","source":"Submission for NLI (Natural Language Inferencing) problem based on the [Contradictory, My Dear Watson](https://www.kaggle.com/c/contradictory-my-dear-watson/overview) dataset. \n\nThis notebook is inspired from [here](https://www.kaggle.com/tkrsh09/nlp-starter-complete-tpu-bert-guide-keras) but with slight tweaks & changes. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Libraries & Data \n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generic\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os, warnings,json\nwarnings.filterwarnings('ignore')\n\n# Plot\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.graph_objects as go\n\n# TensorFlow & Transformers\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam, Adamax\n\n# Transformer Model\nfrom transformers import BertTokenizer, TFBertModel\nfrom transformers import TFAutoModel, AutoTokenizer\n\n#SK Learn\nfrom sklearn.model_selection import train_test_split\n\n# Garbage Collector\nimport gc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = pd.read_csv(\"../input/contradictory-my-dear-watson/train.csv\")\ntest_data = pd.read_csv(\"../input/contradictory-my-dear-watson/test.csv\")\nsample_sub = pd.read_csv(\"../input/contradictory-my-dear-watson/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Initialise TPU","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def Utilize_TPUs():  \n    \"\"\"\n    Initialize training strategy using TPU if available else using default strategy for CPU and  single GPU\n    \n    After the TPU is initialized, you can also use manual device placement to place the computation on a single TPU device.\n\n    \"\"\"\n    try:\n        \n        resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n        tf.config.experimental_connect_to_cluster(resolver)\n        tf.tpu.experimental.initialize_tpu_system(resolver)\n        strategy = tf.distribute.experimental.TPUStrategy(resolver)\n        REPLICAS = strategy.num_replicas_in_sync\n        print(\"Connected to TPU Successfully:\\n TPUs Initialised with Replicas:\",REPLICAS)\n        \n        return strategy\n    \n    except ValueError:\n        \n        print(\"Connection to TPU Falied\")\n        print(\"Using default strategy for CPU and single GPU\")\n        strategy = tf.distribute.get_strategy()\n        \n        return strategy\n    \nstrategy=Utilize_TPUs()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# AutoModels\n* In many cases, the architecture you want to use can be guessed from the name or the path of the pretrained model you are supplying to the from_pretrained method.\n\n* AutoClasses are here to do this job for you so that you automatically retrieve the relevant model given the name/path to the pretrained weights/config/vocabulary:\n\n* Instantiating one of AutoModel, AutoConfig and AutoTokenizer will directly create a class of the relevant architecture (ex: model = AutoModel.from_pretrained('bert-base-cased') will create a instance of BertModel).","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Defining Parameters","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"the_chosen_one=\"jplu/tf-xlm-roberta-base\"\nmax_len =80\nbatch_size = 16 * strategy.num_replicas_in_sync\n\nAUTO = tf.data.experimental.AUTOTUNE\nepochs = 30\nn_steps = len(train_data) // batch_size","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Define Build & Compile Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def model_baseline(strategy,transformer):\n    with strategy.scope():\n        transformer_encoder = TFAutoModel.from_pretrained(transformer)\n        input_layer = Input(shape=(max_len,), dtype=tf.int32, name=\"input_layer\")\n        sequence_output = transformer_encoder(input_layer)[0]\n        cls_token = sequence_output[:, 0, :]\n        output_layer = Dense(3, activation='softmax')(cls_token)\n        model = Model(inputs=input_layer, outputs=output_layer)\n        model.compile(\n            Adamax(lr=1e-5), \n            loss='sparse_categorical_crossentropy', \n            metrics=['accuracy']\n        )\n        return model\n    \nmodel=model_baseline(strategy,the_chosen_one)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# EDA & Data Preprocessing ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Pie Chart\ndf = pd.DataFrame({\"count\": train_data.language.value_counts() })\nfig = px.pie(df, values='count', names=df.index, title='Language Count %',\n             labels={'index':'lang'}, color_discrete_sequence=px.colors.diverging.Earth)\nfig.update_traces(textinfo='percent')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Bar Plot - Label Count per Language \nfig, ax = plt.subplots(figsize=(20,10))\ntrain_data.groupby(['language','label']).count()['premise'].unstack().plot(ax=ax,kind='bar', cmap='cividis')\nplt.grid(color='gray',linestyle='--',linewidth=0.2)\nax.set_facecolor('#d8dcd6')\nplt.title(\"Label Count per Language\", fontsize='18')\nplt.xticks(rotation=45)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(the_chosen_one)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = train_data[['premise', 'hypothesis']].values.tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = test_data[['premise', 'hypothesis']].values.tolist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Encoding Data \n\nNumberically representing  text data such that It can be feed to the model ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_encoded=tokenizer.batch_encode_plus(train_df,pad_to_max_length=True,max_length=max_len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_encoded=tokenizer.batch_encode_plus(test_df,pad_to_max_length=True,max_length=max_len)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Validation Split ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train, x_valid, y_train, y_valid = train_test_split(train_encoded['input_ids'], train_data.label.values, test_size=0.1)\n\nx_test = test_encoded['input_ids']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loading Data Into tf.Data.Dataset ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset = (tf.data.Dataset.from_tensor_slices((x_train, y_train)).repeat().shuffle(2048).batch(batch_size).prefetch(AUTO))\n\nvalid_dataset = (tf.data.Dataset.from_tensor_slices((x_valid, y_valid)).batch(batch_size).cache().prefetch(AUTO))\n\ntest_dataset = (tf.data.Dataset.from_tensor_slices(x_test).batch(batch_size))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training Base Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(train_dataset,steps_per_epoch=n_steps,validation_data=valid_dataset,epochs=epochs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Saving","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = model.predict(test_dataset, verbose=1)\nsample_sub['prediction'] = predictions.argmax(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_sub.to_csv(\"submission.csv\",index= False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}