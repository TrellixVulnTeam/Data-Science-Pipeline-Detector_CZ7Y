{"cells":[{"metadata":{},"cell_type":"markdown","source":"We have used XLM-RoBERTA to achieve better validation accuracy of the model.","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.environ[\"WANDB_API_KEY\"] = \"0\" ## to silence warning","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Setting the contants","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"SEED=42\nmax_len = 48  #it was 50\nEPOCHS = 3   #it was 3; highest 10 gives no improvement\nBATCH_SIZE = 64  # it was 128\nLR = 1e-5  #it was 3e-5","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Importing libraries**","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from transformers import BertTokenizer, TFBertModel,TFAutoModel,TFXLMRobertaModel, AutoTokenizer\nimport matplotlib.pyplot as plt\nimport tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Let's setup our TPU**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept ValueError:\n    strategy = tf.distribute.get_strategy() # for CPU and single GPU\n    print('Number of replicas:', strategy.num_replicas_in_sync)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Downloading data**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/contradictory-my-dear-watson/train.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Let's look at the pairs of sentences**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.premise.values[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.hypothesis.values[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.label.values[1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These statements are contradictory, and the label shows that.\n\nLet's look at the distribution of languages in the training set.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"labels, frequencies = np.unique(train.language.values, return_counts = True)\n\nplt.figure(figsize = (10,10))\nplt.pie(frequencies,labels = labels, autopct = '%1.1f%%')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Preparing data for the model**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We are using pre-trained XLM-RoBERTA (cross-language Modeling) from huggingface","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Downloading the tokenizer\n#model_name = 'bert-base-multilingual-cased'\nmodel_name = 'jplu/tf-xlm-roberta-large'\n#tokenizer = BertTokenizer.from_pretrained(model_name)\n#transformer_layer = TFXLMRobertaModel.from_pretrained(MODEL)\n#model = create_model(transformer_layer)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Tokenizers turn the sequences of words into arrays of numbers.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def encode_sentence(s):\n   tokens = list(tokenizer.tokenize(s))\n   tokens.append('[SEP]')\n   return tokenizer.convert_tokens_to_ids(tokens)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encode_sentence(\"I love machine learning\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"XLM RoBERTA uses three kind of input data- input word IDs, input masks, and input type IDs.\n\nThese allow the model to know that the premise and hypothesis are distinct sentences, and also to ignore any padding from the tokenizer.\n\nWe add a [CLS] token to denote the beginning of the inputs, and a [SEP] token to denote the separation between the premise and the hypothesis. We also need to pad all of the inputs to be the same size.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We are going to encode all of our premise/hypothesis pairs for input into XLM RoBERTA","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def bert_encode(hypotheses, premises, tokenizer):\n    \n  num_examples = len(hypotheses)\n  \n  sentence1 = tf.ragged.constant([\n      encode_sentence(s)\n      for s in np.array(hypotheses)])\n  sentence2 = tf.ragged.constant([\n      encode_sentence(s)\n       for s in np.array(premises)])\n\n  cls = [tokenizer.convert_tokens_to_ids(['[CLS]'])]*sentence1.shape[0]\n  input_word_ids = tf.concat([cls, sentence1, sentence2], axis=-1)\n\n  input_mask = tf.ones_like(input_word_ids).to_tensor()\n\n  type_cls = tf.zeros_like(cls)\n  type_s1 = tf.zeros_like(sentence1)\n  type_s2 = tf.ones_like(sentence2)\n  input_type_ids = tf.concat(\n      [type_cls, type_s1, type_s2], axis=-1).to_tensor()\n\n  inputs = {\n      'input_word_ids': input_word_ids.to_tensor(),\n      'input_mask': input_mask,\n      'input_type_ids': input_type_ids}\n\n  return inputs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_input = bert_encode(train.premise.values, train.hypothesis.values, tokenizer)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Building and Training the Model**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We will incorporate XLM RoBERTA Transformer into Keras Functional Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model():\n#    bert_encoder = TFBertModel.from_pretrained(model_name)\n    bert_encoder = TFXLMRobertaModel.from_pretrained(model_name)\n#    bert_encoder = TFAutoModel.from_pretrained(model_name)\n    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    input_type_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_type_ids\")\n    \n    embedding = bert_encoder([input_word_ids, input_mask, input_type_ids])[0]\n    output = tf.keras.layers.Dense(3, activation='softmax')(embedding[:,0,:])   #Instead of softmax(), we can try out sigmoid()\n    \n    model = tf.keras.Model(inputs=[input_word_ids, input_mask, input_type_ids], outputs=output)\n    model.compile(tf.keras.optimizers.Adam(lr=LR), loss='sparse_categorical_crossentropy', metrics=['accuracy']) \n    #Check to see the learning rate as 1e-5, 3e-5 and 5e-5 and see the change\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with strategy.scope():\n    model = build_model()\n    model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# callback for early stoppong\nfrom tensorflow.keras.callbacks import EarlyStopping\neas = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=3,\n                    verbose=1, mode='min', baseline=None, restore_best_weights=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(train_input, train.label.values, epochs = EPOCHS, verbose = 1, batch_size = BATCH_SIZE, validation_split = 0.2, callbacks = [eas])   \n#instead of 2 we can try out 3-4 epochs to see if accuracy is improved","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv(\"../input/contradictory-my-dear-watson/test.csv\")\ntest_input = bert_encode(test.premise.values, test.hypothesis.values, tokenizer)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Using Test data**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Generating & Submitting Predictions**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = [np.argmax(i) for i in model.predict(test_input)]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = test.id.copy().to_frame()\nsubmission['prediction'] = predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv(\"submission.csv\", index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}