{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers\n!pip install datasets\n!pip install transformers[sentencepiece]","metadata":{"execution":{"iopub.status.busy":"2022-02-04T15:10:50.755076Z","iopub.execute_input":"2022-02-04T15:10:50.755461Z","iopub.status.idle":"2022-02-04T15:11:19.944062Z","shell.execute_reply.started":"2022-02-04T15:10:50.755369Z","shell.execute_reply":"2022-02-04T15:11:19.94276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input/contradictory-my-dear-watson/'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-04T15:11:19.947098Z","iopub.execute_input":"2022-02-04T15:11:19.947844Z","iopub.status.idle":"2022-02-04T15:11:19.95834Z","shell.execute_reply.started":"2022-02-04T15:11:19.947781Z","shell.execute_reply":"2022-02-04T15:11:19.957211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Imports and declarations\nimport torch\nimport gc\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.cuda.amp import GradScaler, autocast\nfrom transformers import AutoTokenizer, AutoModel, logging, AdamW, get_scheduler\nfrom datasets import load_dataset\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n%matplotlib inline\nlogging.set_verbosity_warning()\nlogging.set_verbosity_error()","metadata":{"execution":{"iopub.status.busy":"2022-02-04T15:11:19.959458Z","iopub.execute_input":"2022-02-04T15:11:19.959708Z","iopub.status.idle":"2022-02-04T15:11:29.978997Z","shell.execute_reply.started":"2022-02-04T15:11:19.959661Z","shell.execute_reply":"2022-02-04T15:11:29.978033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# preprocessing function\ndef preprocess(data, features, label):\n  return data.drop(features, axis=1), data[label]\n\n\n# load the train data and have a look\ntrain = pd.read_csv('/kaggle/input/contradictory-my-dear-watson/train.csv')\nfeatures, labels = preprocess(train, ['label', 'lang_abv', 'id', 'language'], 'label')","metadata":{"execution":{"iopub.status.busy":"2022-02-04T15:11:29.9814Z","iopub.execute_input":"2022-02-04T15:11:29.981937Z","iopub.status.idle":"2022-02-04T15:11:30.171571Z","shell.execute_reply.started":"2022-02-04T15:11:29.981897Z","shell.execute_reply":"2022-02-04T15:11:30.170509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# take a look at the features and labels\nfeatures.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-04T15:11:30.172912Z","iopub.execute_input":"2022-02-04T15:11:30.17316Z","iopub.status.idle":"2022-02-04T15:11:30.191783Z","shell.execute_reply.started":"2022-02-04T15:11:30.173129Z","shell.execute_reply":"2022-02-04T15:11:30.190691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-04T15:11:30.194493Z","iopub.execute_input":"2022-02-04T15:11:30.19531Z","iopub.status.idle":"2022-02-04T15:11:30.204721Z","shell.execute_reply.started":"2022-02-04T15:11:30.195259Z","shell.execute_reply":"2022-02-04T15:11:30.204056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dataset class in pytorch\nclass BertyDataset(Dataset):\n    \n    def __init__(self, features, max_length, bert_model, tokenizer, labels=None, indexes=False):\n        self.features = features\n        self.labels = labels\n        self.max_length = max_length\n        self.model = bert_model\n        self.tokenizer = tokenizer\n        self.indexes = indexes\n        \n    def __len__(self):\n        return len(self.features)\n    \n    def __getitem__(self, idx):\n        # select the sentences at the specified index (idx)\n        sent1 = str(self.features.loc[idx, 'premise'])\n        sent2 = str(self.features.loc[idx, 'hypothesis'])\n        \n        # tokenize the sentences\n        batch = self.tokenizer(\n            sent1, sent2,\n            max_length=self.max_length,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_token_type_ids=True,\n            return_tensors='pt'\n        )\n        \n        # in case we won't use labels\n        if type(self.labels) == None:            \n            return batch['input_ids'].squeeze(0), batch['attention_mask'].squeeze(0), batch['token_type_ids'].squeeze(0)\n        \n        if self.indexes:\n            label = self.labels.loc[idx]\n            \n            return batch['input_ids'].squeeze(0), batch['attention_mask'].squeeze(0), batch['token_type_ids'].squeeze(0), torch.tensor(label).squeeze(0)\n        \n        label = self.labels.loc[idx]['label']\n            \n        return batch['input_ids'].squeeze(0), batch['attention_mask'].squeeze(0), batch['token_type_ids'].squeeze(0), torch.tensor(label).squeeze(0)","metadata":{"execution":{"iopub.status.busy":"2022-02-04T15:11:30.207633Z","iopub.execute_input":"2022-02-04T15:11:30.208122Z","iopub.status.idle":"2022-02-04T15:11:30.221575Z","shell.execute_reply.started":"2022-02-04T15:11:30.208088Z","shell.execute_reply":"2022-02-04T15:11:30.220773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# initialzie the pretrained model\nroberta = AutoModel.from_pretrained('joeddav/xlm-roberta-large-xnli')\n\n# initialzie the tokenizer\ntokenizer = AutoTokenizer.from_pretrained('joeddav/xlm-roberta-large-xnli')","metadata":{"execution":{"iopub.status.busy":"2022-02-04T15:11:30.223026Z","iopub.execute_input":"2022-02-04T15:11:30.223327Z","iopub.status.idle":"2022-02-04T15:12:50.829527Z","shell.execute_reply.started":"2022-02-04T15:11:30.223293Z","shell.execute_reply":"2022-02-04T15:12:50.828179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load mnli dataset\ndef load_mnli():\n    \"\"\"Load mnli data\"\"\"\n    result = []\n    dataset = load_dataset(path=\"glue\", name=\"mnli\")\n    keys = ['train']\n    \n    for k in keys:\n        for record in dataset[k]:\n            c1, c2, c3 = record['premise'], record['hypothesis'], record['label']\n    \n            if c1 and c2 and c3 in {0,1,2}:\n                result.append((c1,c2,c3,'en'))\n    \n    result = pd.DataFrame(result, columns=['premise', 'hypothesis', 'label', 'lang_abv'])\n    \n    return result\n\n# load xnli dataset\ndef load_xnli():\n    \"\"\"Load xnli data\"\"\"\n    result = []\n    dataset = load_dataset('xnli', 'all_languages')\n    \n    for k in dataset.keys():\n        for record in dataset[k]:\n            hp, pr, lb = record['hypothesis'], record['premise'], record['label']\n            \n            if hp and pr and lb in {0,1,2}:\n                    \n                for lang, translation in zip(hp['language'], hp['translation']):\n                    pr_lang = pr.get(lang, None)\n                        \n                    if pr_lang is None:\n                        continue\n                            \n                    result.append((pr_lang, translation, lb,lang))\n    \n    result = pd.DataFrame(result, columns=['premise', 'hypothesis', 'label', 'lang_abv'])\n    \n    return result\n\n\n# take 25 000 samples from mnli and xnli datasets\nmnli = load_mnli().loc[:24999]\nmnli.drop(['lang_abv'], axis=1, inplace=True) # drop the lang_abv col\nxnli = load_xnli().loc[:24999]\nxnli.drop(['lang_abv'], axis=1, inplace=True) # drop the lang_abv col","metadata":{"execution":{"iopub.status.busy":"2022-02-04T15:12:50.83205Z","iopub.execute_input":"2022-02-04T15:12:50.833146Z","iopub.status.idle":"2022-02-04T15:19:55.206512Z","shell.execute_reply.started":"2022-02-04T15:12:50.833102Z","shell.execute_reply":"2022-02-04T15:19:55.205365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# take a look at mnli and xnli datasets\nprint('MNLI dataset: ', mnli.shape)\nmnli.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-04T15:19:55.211627Z","iopub.execute_input":"2022-02-04T15:19:55.211949Z","iopub.status.idle":"2022-02-04T15:19:55.226441Z","shell.execute_reply.started":"2022-02-04T15:19:55.211913Z","shell.execute_reply":"2022-02-04T15:19:55.225112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('XNLI dataset: ', xnli.shape)\nxnli.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-04T15:19:55.228415Z","iopub.execute_input":"2022-02-04T15:19:55.228661Z","iopub.status.idle":"2022-02-04T15:19:55.24878Z","shell.execute_reply.started":"2022-02-04T15:19:55.228632Z","shell.execute_reply":"2022-02-04T15:19:55.247866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# implementing the.... MASTER DATASET!!!!!! a.k.a. concatenate both mnli and xnli\nmaster_data = pd.concat([mnli, xnli], axis=0)\n# take a look at the dataset\nprint('THE MASTER DATA: ', master_data.shape)\nmaster_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-04T15:19:55.250715Z","iopub.execute_input":"2022-02-04T15:19:55.251443Z","iopub.status.idle":"2022-02-04T15:19:55.270918Z","shell.execute_reply.started":"2022-02-04T15:19:55.251406Z","shell.execute_reply":"2022-02-04T15:19:55.270306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# split the data into train and val sets\nx_train, x_val, y_train, y_val = train_test_split(master_data.drop(['label'], axis=1), master_data['label'], test_size=0.2, random_state=2022)","metadata":{"execution":{"iopub.status.busy":"2022-02-04T15:19:55.272061Z","iopub.execute_input":"2022-02-04T15:19:55.272671Z","iopub.status.idle":"2022-02-04T15:19:55.299107Z","shell.execute_reply.started":"2022-02-04T15:19:55.272638Z","shell.execute_reply":"2022-02-04T15:19:55.298304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# take a look at the training features and labels\nprint('X train: ', x_train.shape)\nx_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-04T15:19:55.301289Z","iopub.execute_input":"2022-02-04T15:19:55.302095Z","iopub.status.idle":"2022-02-04T15:19:55.315132Z","shell.execute_reply.started":"2022-02-04T15:19:55.302047Z","shell.execute_reply":"2022-02-04T15:19:55.314446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Y train: ', y_train.shape)\ny_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-04T15:19:55.316544Z","iopub.execute_input":"2022-02-04T15:19:55.317428Z","iopub.status.idle":"2022-02-04T15:19:55.326265Z","shell.execute_reply.started":"2022-02-04T15:19:55.317392Z","shell.execute_reply":"2022-02-04T15:19:55.325086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create dataset and dataloader with the merged dataset for training and validation\nbatch_size = 16\nmax_length = 128\nx_mnli_dataset_train = BertyDataset(x_train.reset_index().drop(['index'], axis=1),\n                                    labels=y_train.reset_index().drop(['index'], axis=1),\n                                    max_length=max_length,\n                                    bert_model=roberta,\n                                    tokenizer=tokenizer\n                                   )\nx_mnli_dataset_val = BertyDataset(x_val.reset_index().drop(['index'], axis=1),\n                                  labels=y_val.reset_index().drop(['index'], axis=1),\n                                  max_length=max_length,\n                                  bert_model=roberta,\n                                  tokenizer=tokenizer\n                                 )\nx_mnli_dataloader_train = DataLoader(x_mnli_dataset_train, batch_size=batch_size)\nx_mnli_dataloader_val = DataLoader(x_mnli_dataset_val, batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2022-02-04T15:19:55.328401Z","iopub.execute_input":"2022-02-04T15:19:55.329049Z","iopub.status.idle":"2022-02-04T15:19:55.363758Z","shell.execute_reply.started":"2022-02-04T15:19:55.328999Z","shell.execute_reply":"2022-02-04T15:19:55.362872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# implement model class in pytorch\nclass Berty(torch.nn.Module):\n    \n    def __init__(self, bert_model, freeze_bert=False):\n        super(Berty, self).__init__()\n        \n        # instance of BERT model\n        self.bert = bert_model\n        \n        # freezing BERT's parameters\n        if freeze_bert:\n            for param in self.bert.parameters():\n                param.requires_grad = False\n        \n        # BERT's hidden size, hidden size of our classifier, number of output labels\n        self.hid_size_bert = 1024\n        self.output = 3\n        \n        # our classification layer\n        self.classifier = torch.nn.Sequential(\n            torch.nn.Dropout(p=0.2),\n            torch.nn.Linear(self.hid_size_bert, self.output) \n        )\n        \n    def forward(self, input_ids, attention_mask, token_type_ids): \n        # feed inputs into BERT\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n        \n        # extract the last hidden state of the '[CLS]' token for classification tasks\n        last_hidden_state_cls = outputs[0][:, 0, :]\n        \n        # feed input into the classifier to compute logits\n        logits = self.classifier(last_hidden_state_cls)\n        \n        return logits","metadata":{"execution":{"iopub.status.busy":"2022-02-04T15:19:55.365816Z","iopub.execute_input":"2022-02-04T15:19:55.366521Z","iopub.status.idle":"2022-02-04T15:19:55.377384Z","shell.execute_reply.started":"2022-02-04T15:19:55.366475Z","shell.execute_reply":"2022-02-04T15:19:55.376608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# example data - see the data we are going to feed into BERT\ninput_ids = x_mnli_dataset_val.__getitem__(21)[0]\nattn_mask = x_mnli_dataset_val.__getitem__(21)[1]\nsegment_ids = x_mnli_dataset_val.__getitem__(21)[2]\nlabel = x_mnli_dataset_val.__getitem__(21)[3]\nprint('Input IDs: ', input_ids)\nprint('\\nAttention mask: ', attn_mask)\nprint('\\nSegment IDs: ', segment_ids)\nprint('\\nLabel: ', label)","metadata":{"execution":{"iopub.status.busy":"2022-02-04T15:19:55.379127Z","iopub.execute_input":"2022-02-04T15:19:55.380063Z","iopub.status.idle":"2022-02-04T15:19:55.430382Z","shell.execute_reply.started":"2022-02-04T15:19:55.380013Z","shell.execute_reply":"2022-02-04T15:19:55.429298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# training function\ndef train(model, train_dataloader, val_dataloader, epochs=4, gradient_accumulations=16, debug=False):\n    \"\"\"Train the BERT classifier model.\"\"\"\n    \n    print('Start training...\\n')\n\n    # put the model in training mode\n    model.to(device).train()\n\n    # init gradient scaler\n    scaler = GradScaler()\n    max_train_acc = 0\n    \n    for epoch_i in range(epochs):   \n        # initialize monitoring variables\n        # batch_loss and batch_count are added because\n        # we need to normalize the loss with respect to\n        # the number of batches we've accumulated\n        epoch_loss = 0\n        epoch_acc = []\n        batch_loss = 0\n        batch_count = 0\n        batch_acc = []\n        \n        for step, batch in enumerate(train_dataloader):\n            # load batch on GPU/CPU\n            batch_input_ids, batch_attn_mask, batch_segment_ids, batch_labels = tuple(item.to(device) for item in batch)\n\n            with autocast():\n              logits = model(input_ids=batch_input_ids, attention_mask=batch_attn_mask, token_type_ids=batch_segment_ids)\n              loss = loss_fn(logits, batch_labels)\n\n            # scale the gradients and perform backward propagation\n            optim.zero_grad()\n            scaler.scale(loss / gradient_accumulations).backward()\n            preds = torch.argmax(logits, dim=1).flatten()\n            accuracy = (preds == batch_labels).cpu().numpy().mean() * 100\n                        \n            # clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            \n            # update monitoring variables\n            epoch_loss += loss.item()\n            batch_loss += loss.item()\n            batch_acc.append(accuracy)\n            epoch_acc.append(accuracy)\n            batch_count += 1\n\n            # update with the accumulated gradients, step with the scheduler and zero the gradients of the model\n            if (step + 1) % gradient_accumulations == 0:\n              scaler.step(optim)\n              lr_scheduler.step()\n              scaler.update()\n\n            # monitor every 100 steps\n            if (step % 100 == 0 and step != 0) or (step == len(train_dataloader)-1):\n                \n                if debug:\n                    print(\"Evaluation for epoch: {}/{}, batch: {}/{}\".format(epoch_i+1, epochs, step, len(train_dataloader)))\n                    print(\"Train acc: {:.2f} \\nTrain loss: {:.2f}\\n\".format(np.mean(batch_acc), batch_loss/batch_count))\n                \n                if max_train_acc < np.mean(batch_acc):\n                    max_train_acc = np.mean(batch_acc)\n                    torch.save(model, \"model.pt\")\n                \n                batch_count = 0\n                batch_loss = 0\n                batch_acc = []\n            \n        # evaluate for the current epoch\n        avg_train_loss = epoch_loss / len(train_dataloader)\n        avg_train_acc = np.mean(epoch_acc)\n        val_acc, val_loss = evaluate(model, val_dataloader)\n\n        print('****EVALUATION****')\n        print(\"Epoch: {}/{} \\nTrain loss: {:.4f} \\nTrain accuracy: {:.2f} \\nVal loss: {:.4f} \\nVal accuracy: {:.2f}\\n\"\\\n              .format(epoch_i+1, epochs, avg_train_loss, avg_train_acc, val_loss, val_acc))\n        \n    return model\n\n        \n# evaluation function\ndef evaluate(model, val_dataloader):\n    \"\"\"Evaluate trained model.\"\"\"\n    \n    model.to(device).eval()\n    \n    # initialize monitoring variables\n    val_accuracy, val_loss = [], []\n    \n    for step, batch in enumerate(val_dataloader):\n        batch_input_ids, batch_attn_mask, batch_segment_ids, batch_labels = tuple(item.to(device) for item in batch)\n        \n        with torch.no_grad():\n            logits = model(input_ids=batch_input_ids, attention_mask=batch_attn_mask, token_type_ids=batch_segment_ids)\n            \n        loss = loss_fn(logits, batch_labels)\n        preds = torch.argmax(logits, dim=1).flatten()\n        accuracy = (preds == batch_labels).cpu().numpy().mean() * 100\n        \n        # update monitoring variables\n        val_loss.append(loss.item())\n        val_accuracy.append(accuracy)\n        \n    # return average accuracy and loss for the validation dataset\n    return np.mean(val_accuracy), np.mean(val_loss)","metadata":{"execution":{"iopub.status.busy":"2022-02-04T15:19:55.432529Z","iopub.execute_input":"2022-02-04T15:19:55.432957Z","iopub.status.idle":"2022-02-04T15:19:55.462114Z","shell.execute_reply.started":"2022-02-04T15:19:55.432906Z","shell.execute_reply":"2022-02-04T15:19:55.461322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# init the model\nbert_classifier = Berty(roberta)\n\n# define an optimizer\noptim = AdamW(\n    bert_classifier.parameters(),\n    lr=2e-5, # recommended values are 5e-5, 3e-5 or 2e-5\n    eps=1e-8, # default epsilon value is 1e-8\n    #weight_decay=1e-2\n)\n\n# define the learning rate scheduler\nepochs = 3 # authors' recommendation is 2, 3 or 4 for fine-tuning\ngradient_accumulations = 16\ntotal_steps = len(x_mnli_dataloader_train) // gradient_accumulations * epochs # all training steps\n# without the gradient accumulation the total training steps would be total_steps = len(dataloader) * total_epochs \nlr_scheduler = get_scheduler(\n    'linear',\n    optimizer=optim,\n    num_warmup_steps=0, # default value\n    num_training_steps=total_steps\n)\n\n# define the loss function\nloss_fn = torch.nn.CrossEntropyLoss()\n\n# clear cuda cache\ntorch.cuda.empty_cache()\n\n# training the model\nmodel = train(bert_classifier, x_mnli_dataloader_train, x_mnli_dataloader_val, epochs=epochs, gradient_accumulations=gradient_accumulations)\n#model = torch.load(os.path.join('/kaggle/input/contradictory-my-dear-bertson-model', 'model (1).pt'), map_location=torch.device('cpu'))","metadata":{"execution":{"iopub.status.busy":"2022-02-04T15:19:55.463624Z","iopub.execute_input":"2022-02-04T15:19:55.464683Z","iopub.status.idle":"2022-02-04T15:20:26.168468Z","shell.execute_reply.started":"2022-02-04T15:19:55.464632Z","shell.execute_reply":"2022-02-04T15:20:26.167472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load test data for submission\ntest = pd.read_csv('/kaggle/input/contradictory-my-dear-watson/test.csv')\nprint('Test set: ', test.shape)\ntest.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-04T15:20:26.17029Z","iopub.execute_input":"2022-02-04T15:20:26.170559Z","iopub.status.idle":"2022-02-04T15:20:26.253152Z","shell.execute_reply.started":"2022-02-04T15:20:26.170526Z","shell.execute_reply":"2022-02-04T15:20:26.252094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# preprocess test data\ntest, test_ids = preprocess(test, ['lang_abv', 'language', 'id'], 'id')\nprint('Test IDs: ', test_ids.shape)\ntest_ids.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-04T15:20:26.254781Z","iopub.execute_input":"2022-02-04T15:20:26.255105Z","iopub.status.idle":"2022-02-04T15:20:26.266036Z","shell.execute_reply.started":"2022-02-04T15:20:26.255062Z","shell.execute_reply":"2022-02-04T15:20:26.265131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Test set: ', test.shape)\ntest.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-04T15:20:26.267309Z","iopub.execute_input":"2022-02-04T15:20:26.267549Z","iopub.status.idle":"2022-02-04T15:20:26.284261Z","shell.execute_reply.started":"2022-02-04T15:20:26.26752Z","shell.execute_reply":"2022-02-04T15:20:26.283076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# make a submission\nfrom tqdm import tqdm\n\n# make a prediction function\ndef predict(model, data, tokenizer):\n    \"\"\"Prediction function for Berty model\"\"\"\n    predicted = []\n    model.eval()\n    \n    for i in tqdm(range(data.shape[0])):\n        # tokenize input\n        batch = tokenizer(\n                data['premise'].loc[i], data['hypothesis'].loc[i],\n                max_length=max_length,\n                padding='max_length',\n                truncation=True,\n                return_attention_mask=True,\n                return_token_type_ids=True,\n                return_tensors='pt'\n            )\n        \n        # loading batches on the same device with the model\n        input_ids, attn_mask, segment_ids = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['token_type_ids'].to(device)\n\n        # predict results\n        with torch.no_grad():\n            logits = model(input_ids, attn_mask, segment_ids)\n        \n        preds = torch.argmax(logits, dim=1).flatten()\n        predicted.append(preds.item())\n    \n    return predicted\n\npreds = predict(model, test, tokenizer=tokenizer)","metadata":{"execution":{"iopub.status.busy":"2022-02-04T15:20:26.28578Z","iopub.execute_input":"2022-02-04T15:20:26.286935Z","iopub.status.idle":"2022-02-04T16:43:24.642561Z","shell.execute_reply.started":"2022-02-04T15:20:26.286882Z","shell.execute_reply":"2022-02-04T16:43:24.641337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# make the submission csv and save it\nsubmission = pd.concat([test_ids, pd.DataFrame({'prediction': preds})], axis=1)\nsubmission.to_csv('/kaggle/working/submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-04T16:43:24.645134Z","iopub.execute_input":"2022-02-04T16:43:24.645497Z","iopub.status.idle":"2022-02-04T16:43:24.681715Z","shell.execute_reply.started":"2022-02-04T16:43:24.645453Z","shell.execute_reply":"2022-02-04T16:43:24.680865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 0 == entailment, 1 == neutral, 2 == contradiction\nsubmission","metadata":{"execution":{"iopub.status.busy":"2022-02-04T16:43:24.683483Z","iopub.execute_input":"2022-02-04T16:43:24.684002Z","iopub.status.idle":"2022-02-04T16:43:24.700754Z","shell.execute_reply.started":"2022-02-04T16:43:24.68395Z","shell.execute_reply":"2022-02-04T16:43:24.699595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission = pd.read_csv('/kaggle/input/contradictory-my-dear-watson/sample_submission.csv')\nsample_submission.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-04T16:43:24.719558Z","iopub.execute_input":"2022-02-04T16:43:24.720585Z","iopub.status.idle":"2022-02-04T16:43:24.763179Z","shell.execute_reply.started":"2022-02-04T16:43:24.720538Z","shell.execute_reply":"2022-02-04T16:43:24.762439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# just to take a look\nclasses = {0: 'entailment', 1: 'neutral', 2: 'contradiction'}\nprem = 'Premise: '+test.loc[5191][0]\nhyp = 'Hypothesis: '+test.loc[5191][1]\nprint(prem)\nprint(hyp)\nprint('Prediction: ', classes[submission.loc[5191][1]])","metadata":{"execution":{"iopub.status.busy":"2022-02-04T16:43:24.702909Z","iopub.execute_input":"2022-02-04T16:43:24.70318Z","iopub.status.idle":"2022-02-04T16:43:24.714593Z","shell.execute_reply.started":"2022-02-04T16:43:24.703148Z","shell.execute_reply":"2022-02-04T16:43:24.713391Z"},"trusted":true},"execution_count":null,"outputs":[]}]}