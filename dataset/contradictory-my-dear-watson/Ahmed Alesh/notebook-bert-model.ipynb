{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nseed = 42\nnp.random.seed(seed)\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"!pip install googletrans textAugment","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n    # set: this is always the case on Kaggle.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/input/contradictory-my-dear-watson/train.csv')\ndf_test = pd.read_csv('/kaggle/input/contradictory-my-dear-watson/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from textaugment import EDA\nfrom googletrans import Translator","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for lang in df_train['language'].unique():\n    number_of_training_samples = df_train[df_train['language'] == lang].shape[0] / df_train.shape[0]\n    number_of_testing_samples = df_test[df_test['language'] == lang].shape[0] / df_test.shape[0]\n    print('distribution of {} in training samples: {} and in testing samples {}'.format(lang, number_of_training_samples, number_of_testing_samples))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"idx2label = {0: 'entailment', 1 :'neutral', 2: 'contradiction'}\nfor label in df_train['label'].unique():\n    number_of_training_samples = df_train[df_train['label'] == label].shape[0] / df_train.shape[0]\n    print('distribution of {} in training samples: {}'.format(idx2label[label], number_of_training_samples))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import BertTokenizer, TFBertModel\nfrom transformers import RobertaTokenizer, TFRobertaModel\nfrom transformers import TFXLMRobertaModel, XLMRobertaTokenizer\ntf.random.set_seed(seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_name = 'jplu/tf-xlm-roberta-large'\ntokenizer = XLMRobertaTokenizer.from_pretrained(model_name)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\n\ndef clean_word(value):\n    language = value[0]\n    word = value[1]\n    if language != 'English':\n        word = word.lower()\n        return word\n    word = word.lower()\n    word = re.sub(r'\\?\\?', 'e', word)\n    word = re.sub('\\.\\.\\.', '.', word)\n    word = re.sub('\\/', ' ', word)\n    word = re.sub('--', ' ', word)\n    word = re.sub('/\\xad', '', word)\n    word = word.strip(' ')\n    return word\n\ndf_train['premise'] = df_train[['language', 'premise']].apply(lambda v: clean_word(v), axis=1)\ndf_train['hypothesis'] = df_train[['language', 'hypothesis']].apply(lambda v: clean_word(v), axis=1)\ndf_test['premise'] = df_test[['language', 'premise']].apply(lambda v: clean_word(v), axis=1)\ndf_test['hypothesis'] = df_test[['language', 'hypothesis']].apply(lambda v: clean_word(v), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model():\n    with strategy.scope():\n        bert_encoder = TFXLMRobertaModel.from_pretrained(model_name)\n        input_word_ids = tf.keras.Input(shape=(None,), dtype=tf.int32, name=\"input_word_ids\")\n        input_mask = tf.keras.Input(shape=(None,), dtype=tf.int32, name=\"input_mask\")\n        embedding = bert_encoder([input_word_ids, input_mask])[0]\n        output_layer = tf.keras.layers.Dropout(0.25)(embedding)\n        output_layer = tf.keras.layers.GlobalAveragePooling1D()(output_layer)\n        output_dense_layer = tf.keras.layers.Dense(64, activation='relu')(output_layer)\n        output_dense_layer = tf.keras.layers.Dense(32, activation='relu')(output_dense_layer)\n        output = tf.keras.layers.Dense(3, activation='softmax')(output_dense_layer)\n\n        model = tf.keras.Model(inputs=[input_word_ids, input_mask], outputs=output)\n        model.compile(tf.keras.optimizers.Adam(lr=1e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n        return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with strategy.scope():\n    model = build_model()\n    model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import confusion_matrix, accuracy_score, ConfusionMatrixDisplay\n\nbatch_size = 8 * strategy.num_replicas_in_sync\nnum_splits = 5\ntest_input = None","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"auto = tf.data.experimental.AUTOTUNE\nlanguages = [ 'zh-cn' if lang == 'zh' else lang for lang in df_train['lang_abv'].unique()]\n\ndef make_dataset(train_input, train_label):\n    dataset = tf.data.Dataset.from_tensor_slices(\n        (\n            train_input,\n            train_label\n        )\n    ).repeat().shuffle(batch_size).batch(batch_size).prefetch(auto)\n    return dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import multiprocessing as mp\nfrom tqdm import tqdm_notebook\n\ndef xlm_roberta_encode(hypotheses, premises, src_langs, augmentation=False):\n    num_examples = len(hypotheses)\n\n    sentence_1 = [tokenizer.encode(s) for s in premises]\n    sentence_2 = [tokenizer.encode(s) for s in hypotheses]\n    input_word_ids = list(map(lambda x: x[0]+x[1], list(zip(sentence_1,sentence_2))))\n    input_mask = [np.ones_like(x) for x in input_word_ids]\n    inputs = {\n        'input_word_ids': tf.keras.preprocessing.sequence.pad_sequences(input_word_ids, padding='post'),\n        'input_mask': tf.keras.preprocessing.sequence.pad_sequences(input_mask, padding='post')\n    }\n    return inputs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gc\nfrom sklearn.model_selection import train_test_split\ntrain_df, validation_df = train_test_split(df_train, test_size=0.1)\nif test_input is None:\n    test_input = xlm_roberta_encode(df_test.hypothesis.values, df_test.premise.values, df_test.lang_abv.values,augmentation=False)\ndf_train['prediction'] = 0\nnum_augmentation = 1\ntrain_input = xlm_roberta_encode(train_df.hypothesis.values,train_df.premise.values, train_df.lang_abv.values, augmentation=False)\ntrain_label = train_df.label.values\ntrain_sequence = make_dataset(train_input, train_label)\nn_steps = (len(train_label)) // batch_size\nvalidation_input = xlm_roberta_encode(validation_df.hypothesis.values, validation_df.premise.values,validation_df.lang_abv.values, augmentation=False)\nvalidation_label = validation_df.label.values\ntf.keras.backend.clear_session()\nwith strategy.scope():\n    model = build_model()\n    history = model.fit(\n        train_sequence, shuffle=True, steps_per_epoch=n_steps, \n        validation_data = (validation_input, validation_label), epochs=50, verbose=1,\n        callbacks=[\n            tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=10),\n            tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', factor=0.1, patience=5),\n            tf.keras.callbacks.ModelCheckpoint(\n                'model.h5', monitor='val_accuracy', save_best_only=True,save_weights_only=True)\n        ]\n    )\nmodel.load_weights('model.h5')\nvalidation_predictions = model.predict(validation_input)\nvalidation_predictions = np.argmax(validation_predictions, axis=-1)\nvalidation_df['predictions'] = validation_predictions\nacc = accuracy_score(validation_label, validation_predictions)\nprint('Accuracy: {}'.format(acc))\ntest_split_predictions = model.predict(test_input)\ndel train_input, train_label, validation_input, validation_label, model, train_sequence\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, accuracy_score, ConfusionMatrixDisplay\nimport matplotlib.pyplot as plt\n\nfig, axes = plt.subplots(nrows=4, ncols=4, figsize=(15,10))\nlabels = list(idx2label.values())\nfor ax,language in zip(axes.flatten()[:validation_df['language'].nunique()],validation_df['language'].unique()):\n    y_pred = validation_df[validation_df['language'] == language]['prediction'].values\n    y_true = validation_df[validation_df['language'] == language]['label'].values\n    lang_acc = accuracy_score(y_true, y_pred)\n    print('Language {} has accuracy {}'.format(language, lang_acc))\n    cm = confusion_matrix(np.array([idx2label[v] for v in y_true]), \n                      np.array([idx2label[v] for v in y_pred]), \n                      labels=labels)  \n    cax = ConfusionMatrixDisplay(cm, display_labels=labels)\n    cax.plot(ax=ax)\n    ax.set_title(language)\n    #fig.colorbar(cax)\n    ax.set_xticklabels([''] + labels)\n    ax.set_yticklabels([''] + labels)\nplt.title('Confusion matrix of the classifier')\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.tight_layout()  \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm = confusion_matrix(np.array([idx2label[v] for v in validation_df.label.values]), \n                      np.array([idx2label[v] for v in validation_df.prediction.values]), \n                      labels=labels)\nprint(cm)\nfig = plt.figure()\nax = fig.add_subplot(111)\ncax = ConfusionMatrixDisplay(cm, display_labels=labels)\ncax.plot(ax=ax)\nplt.title('Confusion matrix of the classifier')\nax.set_xticklabels([''] + labels)\nax.set_yticklabels([''] + labels)\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = np.argmax(test_split_predictions, axis=-1)\nsubmission = df_test.id.copy().to_frame()\nsubmission['prediction'] = predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head()\nsubmission.to_csv(\"submission.csv\", index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}