{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install -q tf-models-official==2.2.0\n!pip install -q nlp","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport tensorflow as tf\nimport tensorflow_hub as hub\n\nfrom official.nlp import bert\nfrom official import nlp\n\nimport official.nlp.bert.tokenization\nimport official.nlp.optimization\n\nimport nlp as an_nlp\n\nfrom kaggle_datasets import KaggleDatasets\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nfrom collections import Counter\nimport json\nimport time\nimport unicodedata\nimport re\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# detect and init the TPU and instantiate a distribution strategy"},{"metadata":{"trusted":true},"cell_type":"code","source":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept ValueError:\n    strategy = tf.distribute.get_strategy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"GCS_PATH_TO_SAVEDMODEL = KaggleDatasets().get_gcs_path(\"bert-multi-cased-l12-h768-a12-2\")\nload_model = tf.saved_model.load(GCS_PATH_TO_SAVEDMODEL)\n\n\nbert_layer = hub.KerasLayer(load_model)\nvocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = bert.tokenization.FullTokenizer(vocab_file, do_lower_case)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# set seed"},{"metadata":{"trusted":true},"cell_type":"code","source":"seed = 123\ntf.random.set_seed(seed)\nnp.random.seed(seed)\n\nstart_time = time.time()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/contradictory-my-dear-watson/train.csv')\ndata.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(6, 15))\nsns.countplot(y='language', hue='label', data=data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = data[['premise', 'hypothesis', 'label']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Augment dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"# multigenre_data = an_nlp.load_dataset(path='glue', name='mnli')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"premise = []\nhypothesis = []\nlabel = []\n\n# for example in multigenre_data['train']:\n#     premise.append(example['premise'])\n#     hypothesis.append(example['hypothesis'])\n#     label.append(example['label'])\n\n# multigenre_df = pd.DataFrame(data={\n#     'premise': premise,\n#     'hypothesis': hypothesis,\n#     'label': label\n# })","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"adversarial_data = an_nlp.load_dataset(path='anli')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"premise = []\nhypothesis = []\nlabel = []\n\nfor example in adversarial_data['train_r1']:\n    premise.append(example['premise'])\n    hypothesis.append(example['hypothesis'])\n    label.append(example['label'])\n    \nfor example in adversarial_data['train_r2']:\n    premise.append(example['premise'])\n    hypothesis.append(example['hypothesis'])\n    label.append(example['label'])\n    \nfor example in adversarial_data['train_r3']:\n    premise.append(example['premise'])\n    hypothesis.append(example['hypothesis'])\n    label.append(example['label'])\n    \nadversarial_df = pd.DataFrame(data={\n    'premise': premise,\n    'hypothesis': hypothesis,\n    'label': label\n})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = dataset.sample(frac=0.99)\nval = dataset[~dataset.index.isin(train.index)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.concat([train, adversarial_df])\ntrain.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing text"},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer.tokenize('and these comments were considered in formulat')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def unicode_to_ascii(s):\n    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n\ndef preprocessing_text(s):\n    s = unicode_to_ascii(s.lower().strip())\n    s = re.sub(r'[\" \"0-9]+', \" \", s)\n    s = s.rstrip().strip()\n    return s\n\ndef encode_sentence(s, tokenizer):\n    s = preprocessing_text(s)\n    tokens = list(tokenizer.tokenize(s))\n    tokens.append('[SEP]')\n    return tokenizer.convert_tokens_to_ids(tokens)\n\ndef bert_encode(glue_dict, tokenizer):\n    sentence1 = tf.ragged.constant([encode_sentence(s, tokenizer) for s in np.array(glue_dict['premise'])])\n    sentence2 = tf.ragged.constant([encode_sentence(s, tokenizer) for s in np.array(glue_dict['hypothesis'])])\n    \n    cls = [tokenizer.convert_tokens_to_ids(['[CLS]'])]*sentence1.shape[0]\n    input_word_ids = tf.concat([cls, sentence1, sentence2], axis=-1)\n    \n    input_mask = tf.ones_like(input_word_ids).to_tensor()\n    \n    type_cls = tf.zeros_like(cls)\n    type_s1 = tf.zeros_like(sentence1)\n    type_s2 = tf.ones_like(sentence2)\n    input_type_ids = tf.concat([type_cls, type_s1, type_s2], axis=-1).to_tensor()\n    \n    return {\n        \"input_word_ids\": input_word_ids.to_tensor(),\n        \"input_mask\": input_mask,\n        \"input_type_ids\": input_type_ids\n    }","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# split train and val"},{"metadata":{"trusted":true},"cell_type":"code","source":"glue_train = bert_encode(train, tokenizer)\nglue_train_labels = train['label']\n\nglue_val = bert_encode(val, tokenizer)\nglue_val_labels = val['label']\n\nfor key, value in glue_train.items():\n    print(f\"{key:15s} shape: {value.shape}\")\nprint(f\"glue_train_labels shape: {glue_train_labels.shape}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 设置优化器"},{"metadata":{},"cell_type":"markdown","source":"### set up epochs and steps"},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = 3\nbatch_size = 16\n\ntrain_data_size = len(glue_train_labels)\nsteps_per_epoch = int(train_data_size / batch_size)\nnum_train_steps = steps_per_epoch * epochs\nwarmup_steps = int(epochs * train_data_size * 0.1 / batch_size)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 构建模型"},{"metadata":{"trusted":true},"cell_type":"code","source":"class ClassifierModel(tf.keras.Model):\n    def __init__(self, num_labels, max_seq_length=None, rate=0.5):\n        super(ClassifierModel, self).__init__()\n        self.bert_layer = hub.KerasLayer(load_model)\n        self.dropout = tf.keras.layers.Dropout(rate=rate)\n        self.output_layer = tf.keras.layers.Dense(num_labels)\n    \n    def call(self, inputs, training=True):\n        input_word_ids = inputs['input_word_ids']\n        input_mask = inputs['input_mask']\n        input_type_ids = inputs['input_type_ids']\n        \n        self.bert_layer.trainable = training\n        pooled_output, _ = self.bert_layer([input_word_ids, input_mask, input_type_ids])\n        output = self.dropout(pooled_output, training=training)\n        output = self.output_layer(output)\n        \n        return output","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### creates an optimizer with learning rate schedule"},{"metadata":{"trusted":true},"cell_type":"code","source":"with strategy.scope():\n    multi_bert_classifier = ClassifierModel(num_labels=3, max_seq_length=None, rate=0.5)\n\n    optimizer = nlp.optimization.create_optimizer(2e-5, num_train_steps=num_train_steps, num_warmup_steps=warmup_steps)\n\n    metrics = [tf.keras.metrics.SparseCategoricalAccuracy('accuracy', dtype=tf.float32)]\n    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n\n    multi_bert_classifier.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n#     def train_step(inputs, labels):\n#         with tf.GradientTape() as tape:\n#             predictions = multi_bert_classifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"multi_bert_classifier.fit(glue_train, glue_train_labels, validation_data=(glue_val, glue_val_labels), batch_size=batch_size, epochs=epochs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# read test"},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/contradictory-my-dear-watson/test.csv')\ntest.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_language = dict(Counter(data['language']).most_common())\ntest_language = dict(Counter(test['language']).most_common())\n\nlanguage_name = np.concatenate([list(data_language.keys()), list(test_language.keys())])\nlanguage_name = list(set(language_name))\n\ndata_num = []\ntest_num = []\nfor index, value in enumerate(language_name):\n    if value in data_language.keys():\n        data_num.append(data_language[value])\n    else:\n        data_num.append(0)\n    \n    if value in test_language.keys():\n        test_num.append(test_language[value])\n    else:\n        test_num.append(0)\n\nlanguage_num = pd.DataFrame({'language': language_name, 'data': data_num, 'test': test_num})\n\nfig, ax = plt.subplots(figsize=(6, 8))\nsns.set_color_codes(\"pastel\")\nsns.barplot(x='data' , y='language', data=language_num, label=\"data\", color=\"b\")\nsns.set_color_codes(\"muted\")\nsns.barplot(x='test' , y='language', data=language_num, label=\"test\", color=\"b\")\nax.legend(ncol=2, loc=\"lower right\", frameon=True)\nsns.despine(left=True, bottom=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"glue_test = bert_encode(test, tokenizer)\npredictions = multi_bert_classifier.predict(glue_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = tf.math.argmax(predictions, axis=-1)\nresults = pd.DataFrame({'id': test['id'], 'prediction': predictions.numpy()})\nresults.to_csv('/kaggle/working/submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}