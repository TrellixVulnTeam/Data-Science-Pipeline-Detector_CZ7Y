{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input/contradictory-my-dear-watson/train.csv'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-10T02:36:01.100785Z","iopub.execute_input":"2021-06-10T02:36:01.101143Z","iopub.status.idle":"2021-06-10T02:36:01.109077Z","shell.execute_reply.started":"2021-06-10T02:36:01.101105Z","shell.execute_reply":"2021-06-10T02:36:01.107922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n\nfrom sklearn import ensemble, metrics, model_selection\nimport os\n\n\n\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import roc_auc_score, accuracy_score\n#!pip install transformers\nimport transformers\nfrom transformers import BertTokenizer,AutoTokenizer, TFAutoModel,BertForSequenceClassification, TFBertModel\nfrom transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification\nfrom transformers import AdamW\n\n#import nlp\nimport transformers\nfrom transformers import (AdamW, \n                          XLMRobertaTokenizer, \n                          XLMRobertaModel, \n                          get_cosine_schedule_with_warmup)\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.utils.data.distributed import DistributedSampler\n\n\n#!pip install googletrans\nimport copy\nimport copy\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2021-06-10T02:36:01.111158Z","iopub.execute_input":"2021-06-10T02:36:01.111826Z","iopub.status.idle":"2021-06-10T02:36:01.127487Z","shell.execute_reply.started":"2021-06-10T02:36:01.111787Z","shell.execute_reply":"2021-06-10T02:36:01.126606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\n\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2021-06-10T02:36:01.129491Z","iopub.execute_input":"2021-06-10T02:36:01.129844Z","iopub.status.idle":"2021-06-10T02:36:01.138795Z","shell.execute_reply.started":"2021-06-10T02:36:01.12981Z","shell.execute_reply":"2021-06-10T02:36:01.138001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install transformers","metadata":{"execution":{"iopub.status.busy":"2021-06-10T02:36:01.141841Z","iopub.execute_input":"2021-06-10T02:36:01.14212Z","iopub.status.idle":"2021-06-10T02:36:07.194658Z","shell.execute_reply.started":"2021-06-10T02:36:01.142088Z","shell.execute_reply":"2021-06-10T02:36:07.193674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import transformers","metadata":{"execution":{"iopub.status.busy":"2021-06-10T02:36:07.198218Z","iopub.execute_input":"2021-06-10T02:36:07.198526Z","iopub.status.idle":"2021-06-10T02:36:07.204961Z","shell.execute_reply.started":"2021-06-10T02:36:07.198482Z","shell.execute_reply":"2021-06-10T02:36:07.204103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = pd.read_csv(\"../input/contradictory-my-dear-watson/train.csv\")\ntrain_data.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T02:36:07.207341Z","iopub.execute_input":"2021-06-10T02:36:07.207883Z","iopub.status.idle":"2021-06-10T02:36:07.262324Z","shell.execute_reply.started":"2021-06-10T02:36:07.207706Z","shell.execute_reply":"2021-06-10T02:36:07.261407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a = train_data.shape\n\nprint(\"The shape of train the data is:\",a)\nprint(\"the attributes are\",train_data.columns)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T02:36:07.263852Z","iopub.execute_input":"2021-06-10T02:36:07.264226Z","iopub.status.idle":"2021-06-10T02:36:07.270102Z","shell.execute_reply.started":"2021-06-10T02:36:07.264187Z","shell.execute_reply":"2021-06-10T02:36:07.269288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.info()","metadata":{"execution":{"iopub.status.busy":"2021-06-10T02:36:07.273317Z","iopub.execute_input":"2021-06-10T02:36:07.273939Z","iopub.status.idle":"2021-06-10T02:36:07.292807Z","shell.execute_reply.started":"2021-06-10T02:36:07.2739Z","shell.execute_reply":"2021-06-10T02:36:07.291833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data = pd.read_csv(\"../input/contradictory-my-dear-watson/test.csv\")\ntest_data.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T02:36:07.294938Z","iopub.execute_input":"2021-06-10T02:36:07.295368Z","iopub.status.idle":"2021-06-10T02:36:07.328573Z","shell.execute_reply.started":"2021-06-10T02:36:07.295332Z","shell.execute_reply":"2021-06-10T02:36:07.327649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"b = test_data.shape\nprint(\"The shape of the test data is:\",b)\nprint(\"the attributes are\",test_data.columns)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T02:36:07.329895Z","iopub.execute_input":"2021-06-10T02:36:07.330236Z","iopub.status.idle":"2021-06-10T02:36:07.335939Z","shell.execute_reply.started":"2021-06-10T02:36:07.330198Z","shell.execute_reply":"2021-06-10T02:36:07.334808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.info()","metadata":{"execution":{"iopub.status.busy":"2021-06-10T02:36:07.337289Z","iopub.execute_input":"2021-06-10T02:36:07.337758Z","iopub.status.idle":"2021-06-10T02:36:07.356959Z","shell.execute_reply.started":"2021-06-10T02:36:07.337723Z","shell.execute_reply":"2021-06-10T02:36:07.3562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_data.keys())\nprint(test_data.keys())\n\ntrain_data","metadata":{"execution":{"iopub.status.busy":"2021-06-10T02:36:07.358114Z","iopub.execute_input":"2021-06-10T02:36:07.358595Z","iopub.status.idle":"2021-06-10T02:36:07.377867Z","shell.execute_reply.started":"2021-06-10T02:36:07.358557Z","shell.execute_reply":"2021-06-10T02:36:07.376949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import BertTokenizer\n\n# Instantiate the Bert tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n#tokenizer.tokenize(str(train_data['premise'][0]))","metadata":{"execution":{"iopub.status.busy":"2021-06-10T02:36:07.379214Z","iopub.execute_input":"2021-06-10T02:36:07.379606Z","iopub.status.idle":"2021-06-10T02:36:09.631645Z","shell.execute_reply.started":"2021-06-10T02:36:07.379568Z","shell.execute_reply":"2021-06-10T02:36:09.630778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer.tokenize((train_data['hypothesis'][0]))","metadata":{"execution":{"iopub.status.busy":"2021-06-10T02:36:09.633018Z","iopub.execute_input":"2021-06-10T02:36:09.633352Z","iopub.status.idle":"2021-06-10T02:36:09.640315Z","shell.execute_reply.started":"2021-06-10T02:36:09.633315Z","shell.execute_reply":"2021-06-10T02:36:09.639455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer(str(train_data), padding=True, truncation=True,return_tensors='pt')","metadata":{"execution":{"iopub.status.busy":"2021-06-10T02:36:09.641763Z","iopub.execute_input":"2021-06-10T02:36:09.642345Z","iopub.status.idle":"2021-06-10T02:36:09.676668Z","shell.execute_reply.started":"2021-06-10T02:36:09.642306Z","shell.execute_reply":"2021-06-10T02:36:09.675702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import BertModel \n\n# raw, unconfigured BERT\nmodel = BertModel.from_pretrained('bert-base-uncased')\nmodel","metadata":{"execution":{"iopub.status.busy":"2021-06-10T02:36:09.678187Z","iopub.execute_input":"2021-06-10T02:36:09.678581Z","iopub.status.idle":"2021-06-10T02:36:23.679502Z","shell.execute_reply.started":"2021-06-10T02:36:09.678541Z","shell.execute_reply":"2021-06-10T02:36:23.678664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# which we can use as a feature extractor\n\n## encode some raw text with padding, truncation, and as PyTorch tesnors\nencoded = tokenizer(str(train_data), padding=True, truncation=True, return_tensors='pt')\n\n## our model expects parameters that are named exactly as our output \n## from our tokenizer... as such, we can unpack the encoded input directly\n## with a ** operator:\nout = model(**encoded)\n\n## take a specific part of the output\nlast_hidden_states = out.last_hidden_state\n\nlast_hidden_states.shape, last_hidden_states","metadata":{"execution":{"iopub.status.busy":"2021-06-10T02:36:23.680825Z","iopub.execute_input":"2021-06-10T02:36:23.681335Z","iopub.status.idle":"2021-06-10T02:36:24.811992Z","shell.execute_reply.started":"2021-06-10T02:36:23.681294Z","shell.execute_reply":"2021-06-10T02:36:24.810989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import BertForSequenceClassification\n\nnum_labels = 3  # need to specify output space now!\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)\nmodel","metadata":{"execution":{"iopub.status.busy":"2021-06-10T02:36:24.813404Z","iopub.execute_input":"2021-06-10T02:36:24.813952Z","iopub.status.idle":"2021-06-10T02:36:29.508485Z","shell.execute_reply.started":"2021-06-10T02:36:24.813906Z","shell.execute_reply":"2021-06-10T02:36:29.507594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# which we can use as a feature extractor\n\n## encode some raw text with padding, truncation, and as PyTorch tesnors\nencoded = tokenizer(str(train_data), padding=True, truncation=True, return_tensors='pt')\n\n## our model expects parameters that are named exactly as our output \n## from our tokenizer... as such, we can unpack the encoded input directly\n## with a ** operator:\nout = model(**encoded)\n\n## take a specific part of the output\nlogits = out.logits\n\nlogits.shape, logits","metadata":{"execution":{"iopub.status.busy":"2021-06-10T02:36:29.509844Z","iopub.execute_input":"2021-06-10T02:36:29.510216Z","iopub.status.idle":"2021-06-10T02:36:30.492066Z","shell.execute_reply.started":"2021-06-10T02:36:29.510177Z","shell.execute_reply":"2021-06-10T02:36:30.491062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import BertForSequenceClassification, BertTokenizerFast\n\nmodel_name = 'bert-base-uncased'\n\ntokenizer = BertTokenizerFast.from_pretrained(model_name)\n\nmodel = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n#model = to_gpu(model)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T02:36:30.493552Z","iopub.execute_input":"2021-06-10T02:36:30.493911Z","iopub.status.idle":"2021-06-10T02:36:37.221685Z","shell.execute_reply.started":"2021-06-10T02:36:30.493873Z","shell.execute_reply":"2021-06-10T02:36:37.220865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.keys()","metadata":{"execution":{"iopub.status.busy":"2021-06-10T02:36:37.22296Z","iopub.execute_input":"2021-06-10T02:36:37.22331Z","iopub.status.idle":"2021-06-10T02:36:37.229198Z","shell.execute_reply.started":"2021-06-10T02:36:37.223271Z","shell.execute_reply":"2021-06-10T02:36:37.228291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 32\n\ntrain = torch.utils.data.DataLoader(train_data.keys(), batch_size=batch_size, shuffle=True)\n#val = torch.utils.data.DataLoader(sst_data['validation'], batch_size=batch_size, shuffle=True)\ntest = torch.utils.data.DataLoader(test_data.keys(), batch_size=batch_size, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T02:36:37.230618Z","iopub.execute_input":"2021-06-10T02:36:37.231233Z","iopub.status.idle":"2021-06-10T02:36:37.239064Z","shell.execute_reply.started":"2021-06-10T02:36:37.231192Z","shell.execute_reply":"2021-06-10T02:36:37.238296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss_fn = torch.nn.CrossEntropyLoss(reduction='sum')\n\nlr = 5e-05\nopt = torch.optim.AdamW(model.parameters(), lr=lr)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T02:36:37.24358Z","iopub.execute_input":"2021-06-10T02:36:37.2439Z","iopub.status.idle":"2021-06-10T02:36:37.252803Z","shell.execute_reply.started":"2021-06-10T02:36:37.243876Z","shell.execute_reply":"2021-06-10T02:36:37.251901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import get_scheduler\n\n# defining a linearly decreasing learning rate schedule\nnum_epochs = 3\nnum_training_steps = num_epochs * len(train)\nlr_scheduler = get_scheduler(\n    \"linear\",\n    optimizer=opt,\n    num_warmup_steps=0,\n    num_training_steps=num_training_steps\n)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T02:36:37.254626Z","iopub.execute_input":"2021-06-10T02:36:37.25497Z","iopub.status.idle":"2021-06-10T02:36:37.264257Z","shell.execute_reply.started":"2021-06-10T02:36:37.254935Z","shell.execute_reply":"2021-06-10T02:36:37.263554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = '../input/contradictory-my-dear-watson/train.csv'\ndf_train = pd.read_csv(path)\n\nprint(df_train.shape)\n\ndf_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-10T02:36:37.265289Z","iopub.execute_input":"2021-06-10T02:36:37.265578Z","iopub.status.idle":"2021-06-10T02:36:37.322361Z","shell.execute_reply.started":"2021-06-10T02:36:37.265549Z","shell.execute_reply":"2021-06-10T02:36:37.321382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the test data.\n\npath = '../input/contradictory-my-dear-watson/test.csv'\ndf_test = pd.read_csv(path)\n\nprint(df_test.shape)\n\ndf_test.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-10T02:36:37.323855Z","iopub.execute_input":"2021-06-10T02:36:37.324206Z","iopub.status.idle":"2021-06-10T02:36:37.35761Z","shell.execute_reply.started":"2021-06-10T02:36:37.324169Z","shell.execute_reply":"2021-06-10T02:36:37.356653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import KFold, StratifiedKFold\n\n# shuffle\ndf = shuffle(df_train)\n\n# initialize kfold\nkf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1024)\n\n# for stratification\ny = df['label']\n\n# Note:\n# Each fold is a tuple ([train_index_values], [val_index_values])\n# fold_0, fold_1, fold_2, fold_3, fold_5 = kf.split(df, y)\n\n# Put the folds into a list. This is a list of tuples.\nfold_list = list(kf.split(df, y))\n\ntrain_df_list = []\nval_df_list = []\n\nfor i, fold in enumerate(fold_list):\n\n    # map the train and val index values to dataframe rows\n    df_train = df[df.index.isin(fold[0])]\n    df_val = df[df.index.isin(fold[1])]\n    \n    train_df_list.append(df_train)\n    val_df_list.append(df_val)\n    \n    \n\nprint(len(train_df_list))\nprint(len(val_df_list))","metadata":{"execution":{"iopub.status.busy":"2021-06-10T02:36:37.35912Z","iopub.execute_input":"2021-06-10T02:36:37.359473Z","iopub.status.idle":"2021-06-10T02:36:37.389496Z","shell.execute_reply.started":"2021-06-10T02:36:37.359435Z","shell.execute_reply":"2021-06-10T02:36:37.388603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = train_df_list[0]\n\ndf_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-10T02:36:37.390744Z","iopub.execute_input":"2021-06-10T02:36:37.391234Z","iopub.status.idle":"2021-06-10T02:36:37.406665Z","shell.execute_reply.started":"2021-06-10T02:36:37.391195Z","shell.execute_reply":"2021-06-10T02:36:37.405884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_val = val_df_list[0]\n\ndf_val.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-10T02:36:37.407798Z","iopub.execute_input":"2021-06-10T02:36:37.408147Z","iopub.status.idle":"2021-06-10T02:36:37.420712Z","shell.execute_reply.started":"2021-06-10T02:36:37.408111Z","shell.execute_reply":"2021-06-10T02:36:37.419648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MODEL_TYPE = 'bert-base-uncased'\n\nNUM_FOLDS = 5\n\n# Saving 5 TPU models will exceed the 4.9GB disk space.\n# Therefore, will will only train on 3 folds.\nNUM_FOLDS_TO_TRAIN = 3 \n\nL_RATE = 1e-5\nMAX_LEN = 256\nNUM_EPOCHS = 3\nBATCH_SIZE = 32\nNUM_CORES = os.cpu_count()\n\nNUM_CORES\n\n","metadata":{"execution":{"iopub.status.busy":"2021-06-10T02:36:37.42233Z","iopub.execute_input":"2021-06-10T02:36:37.422822Z","iopub.status.idle":"2021-06-10T02:36:37.430955Z","shell.execute_reply.started":"2021-06-10T02:36:37.422763Z","shell.execute_reply":"2021-06-10T02:36:37.429858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# For GPU\nimport torch\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\nprint(device)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-06-10T02:36:37.43259Z","iopub.execute_input":"2021-06-10T02:36:37.432973Z","iopub.status.idle":"2021-06-10T02:36:37.439584Z","shell.execute_reply.started":"2021-06-10T02:36:37.432936Z","shell.execute_reply":"2021-06-10T02:36:37.438463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import BertTokenizer\n\n# Load the BERT tokenizer.\nprint('Loading BERT tokenizer...')\ntokenizer = BertTokenizer.from_pretrained(MODEL_TYPE, do_lower_case=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T02:36:37.441508Z","iopub.execute_input":"2021-06-10T02:36:37.441942Z","iopub.status.idle":"2021-06-10T02:36:39.720942Z","shell.execute_reply.started":"2021-06-10T02:36:37.441902Z","shell.execute_reply":"2021-06-10T02:36:39.719997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CompDataset(Dataset):\n\n    def __init__(self, df):\n        self.df_data = df\n\n\n\n    def __getitem__(self, index):\n\n        # get the sentence from the dataframe\n        sentence1 = self.df_data.loc[index, 'premise']\n        sentence2 = self.df_data.loc[index, 'hypothesis']\n\n        # Process the sentence\n        # ---------------------\n\n        encoded_dict = tokenizer.encode_plus(\n                    sentence1, sentence2,           # Sentences to encode.\n                    add_special_tokens = True,      # Add '[CLS]' and '[SEP]'\n                    max_length = MAX_LEN,           # Pad or truncate all sentences.\n                    pad_to_max_length = True,\n                    return_attention_mask = True,   # Construct attn. masks.\n                    return_tensors = 'pt',          # Return pytorch tensors.\n               )  \n         # These are torch tensors already.\n        padded_token_list = encoded_dict['input_ids'][0]\n        att_mask = encoded_dict['attention_mask'][0]\n        token_type_ids = encoded_dict['token_type_ids'][0]\n        \n        # Convert the target to a torch tensor\n        target = torch.tensor(self.df_data.loc[index, 'label'])\n\n        sample = (padded_token_list, att_mask, token_type_ids, target)\n\n\n        return sample\n\n\n    def __len__(self):\n        return len(self.df_data)\n    \n    \nclass TestDataset(Dataset):\n\n    def __init__(self, df):\n        self.df_data = df\n\n\n\n    def __getitem__(self, index):\n\n        # get the sentence from the dataframe\n        sentence1 = self.df_data.loc[index, 'premise']\n        sentence2 = self.df_data.loc[index, 'hypothesis']\n\n        # Process the sentence\n        # ---------------------\n\n        encoded_dict = tokenizer.encode_plus(\n                    sentence1, sentence2,           # Sentence to encode.\n                    add_special_tokens = True,      # Add '[CLS]' and '[SEP]'\n                    max_length = MAX_LEN,           # Pad or truncate all sentences.\n                    pad_to_max_length = True,\n                    return_attention_mask = True,   # Construct attn. masks.\n                    return_tensors = 'pt',          # Return pytorch tensors.\n               )\n         # These are torch tensors already.\n        padded_token_list = encoded_dict['input_ids'][0]\n        att_mask = encoded_dict['attention_mask'][0]\n        token_type_ids = encoded_dict['token_type_ids'][0]\n               \n\n        sample = (padded_token_list, att_mask, token_type_ids)\n\n\n        return sample\n\n\n    def __len__(self):\n        return len(self.df_data)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T02:36:39.72234Z","iopub.execute_input":"2021-06-10T02:36:39.722702Z","iopub.status.idle":"2021-06-10T02:36:39.736313Z","shell.execute_reply.started":"2021-06-10T02:36:39.722665Z","shell.execute_reply":"2021-06-10T02:36:39.735561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = df_train.reset_index(drop=True)\ndf_val = df_val.reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T02:36:39.737756Z","iopub.execute_input":"2021-06-10T02:36:39.738346Z","iopub.status.idle":"2021-06-10T02:36:39.743915Z","shell.execute_reply.started":"2021-06-10T02:36:39.738308Z","shell.execute_reply":"2021-06-10T02:36:39.743031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = CompDataset(df_train)\nval_data = CompDataset(df_val)\ntest_data = TestDataset(df_test)\n\n\n\ntrain_dataloader = torch.utils.data.DataLoader(train_data,\n                                        batch_size=BATCH_SIZE,\n                                        shuffle=True,\n                                       num_workers=NUM_CORES)\n\nval_dataloader = torch.utils.data.DataLoader(val_data,\n                                        batch_size=BATCH_SIZE,\n                                        shuffle=True,\n                                       num_workers=NUM_CORES)\n\ntest_dataloader = torch.utils.data.DataLoader(test_data,\n                                        batch_size=BATCH_SIZE,\n                                        shuffle=False,\n                                       num_workers=NUM_CORES)\n\nprint(len(train_dataloader))\nprint(len(val_dataloader))\nprint(len(test_dataloader))","metadata":{"execution":{"iopub.status.busy":"2021-06-10T02:36:39.745216Z","iopub.execute_input":"2021-06-10T02:36:39.745752Z","iopub.status.idle":"2021-06-10T02:36:39.755877Z","shell.execute_reply.started":"2021-06-10T02:36:39.745709Z","shell.execute_reply":"2021-06-10T02:36:39.754685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get one train batch\n\npadded_token_list, att_mask, token_type_ids, target = next(iter(train_dataloader))\n\nprint(padded_token_list.shape)\nprint(att_mask.shape)\nprint(token_type_ids.shape)\nprint(target.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T02:36:39.757273Z","iopub.execute_input":"2021-06-10T02:36:39.75803Z","iopub.status.idle":"2021-06-10T02:36:40.019618Z","shell.execute_reply.started":"2021-06-10T02:36:39.757977Z","shell.execute_reply":"2021-06-10T02:36:40.018713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get one val batch\n\npadded_token_list, att_mask, token_type_ids, target = next(iter(val_dataloader))\n\nprint(padded_token_list.shape)\nprint(att_mask.shape)\nprint(token_type_ids.shape)\nprint(target.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T02:36:40.021231Z","iopub.execute_input":"2021-06-10T02:36:40.021613Z","iopub.status.idle":"2021-06-10T02:36:40.294669Z","shell.execute_reply.started":"2021-06-10T02:36:40.02157Z","shell.execute_reply":"2021-06-10T02:36:40.293737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get one test batch\n\npadded_token_list, att_mask, token_type_ids = next(iter(test_dataloader))\n\nprint(padded_token_list.shape)\nprint(att_mask.shape)\nprint(token_type_ids.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T02:36:40.296325Z","iopub.execute_input":"2021-06-10T02:36:40.296738Z","iopub.status.idle":"2021-06-10T02:36:40.568173Z","shell.execute_reply.started":"2021-06-10T02:36:40.296693Z","shell.execute_reply":"2021-06-10T02:36:40.56723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load BertForSequenceClassification, the pretrained BERT model with a single \n# linear classification layer on top. \nmodel = BertForSequenceClassification.from_pretrained(\n    MODEL_TYPE, \n    num_labels = 3, \n    output_attentions = False,\n    output_hidden_states = False)\n\n# Send the model to the device.\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T02:36:40.569856Z","iopub.execute_input":"2021-06-10T02:36:40.570227Z","iopub.status.idle":"2021-06-10T02:36:50.806171Z","shell.execute_reply.started":"2021-06-10T02:36:40.570185Z","shell.execute_reply":"2021-06-10T02:36:50.805254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get one train batch\n\ntrain_dataloader = torch.utils.data.DataLoader(train_data,\n                                        batch_size=8,\n                                        shuffle=True,\n                                       num_workers=NUM_CORES)\n\nbatch = next(iter(train_dataloader))\n\nb_input_ids = batch[0].to(device)\nb_input_mask = batch[1].to(device)\nb_token_type_ids = batch[2].to(device)\nb_labels = batch[3].to(device)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T02:36:50.807449Z","iopub.execute_input":"2021-06-10T02:36:50.807795Z","iopub.status.idle":"2021-06-10T02:36:51.030613Z","shell.execute_reply.started":"2021-06-10T02:36:50.807767Z","shell.execute_reply":"2021-06-10T02:36:51.029621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"outputs = model(b_input_ids, \n                token_type_ids=b_token_type_ids, \n                attention_mask=b_input_mask,\n                labels=b_labels)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T02:36:51.034048Z","iopub.execute_input":"2021-06-10T02:36:51.034337Z","iopub.status.idle":"2021-06-10T02:36:51.812042Z","shell.execute_reply.started":"2021-06-10T02:36:51.034306Z","shell.execute_reply":"2021-06-10T02:36:51.811144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"outputs","metadata":{"execution":{"iopub.status.busy":"2021-06-10T02:36:51.813436Z","iopub.execute_input":"2021-06-10T02:36:51.813805Z","iopub.status.idle":"2021-06-10T02:36:51.854458Z","shell.execute_reply.started":"2021-06-10T02:36:51.813763Z","shell.execute_reply":"2021-06-10T02:36:51.853684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = outputs[1].detach().cpu().numpy()\n\ny_true = b_labels.detach().cpu().numpy()\ny_pred = np.argmax(preds, axis=1)\n\ny_pred","metadata":{"execution":{"iopub.status.busy":"2021-06-10T02:36:51.855638Z","iopub.execute_input":"2021-06-10T02:36:51.856011Z","iopub.status.idle":"2021-06-10T02:36:51.868093Z","shell.execute_reply.started":"2021-06-10T02:36:51.855975Z","shell.execute_reply":"2021-06-10T02:36:51.867066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_acc = accuracy_score(y_true, y_pred)\n\nval_acc","metadata":{"execution":{"iopub.status.busy":"2021-06-10T02:36:51.869575Z","iopub.execute_input":"2021-06-10T02:36:51.869939Z","iopub.status.idle":"2021-06-10T02:36:51.876608Z","shell.execute_reply.started":"2021-06-10T02:36:51.869902Z","shell.execute_reply":"2021-06-10T02:36:51.875751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the optimizer\noptimizer = AdamW(model.parameters(),\n              lr = L_RATE, \n              eps = 1e-8 \n            )","metadata":{"execution":{"iopub.status.busy":"2021-06-10T02:36:51.878035Z","iopub.execute_input":"2021-06-10T02:36:51.878725Z","iopub.status.idle":"2021-06-10T02:36:51.887086Z","shell.execute_reply.started":"2021-06-10T02:36:51.878683Z","shell.execute_reply":"2021-06-10T02:36:51.886346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = CompDataset(df_train)\nval_data = CompDataset(df_val)\ntest_data = TestDataset(df_test)\n\ntrain_dataloader = torch.utils.data.DataLoader(train_data,\n                                        batch_size=BATCH_SIZE,\n                                        shuffle=True,\n                                       num_workers=NUM_CORES)\n\nval_dataloader = torch.utils.data.DataLoader(val_data,\n                                        batch_size=BATCH_SIZE,\n                                        shuffle=True,\n                                       num_workers=NUM_CORES)\n\ntest_dataloader = torch.utils.data.DataLoader(test_data,\n                                        batch_size=BATCH_SIZE,\n                                        shuffle=False,\n                                       num_workers=NUM_CORES)\n\nprint(len(train_dataloader))\nprint(len(val_dataloader))\nprint(len(test_dataloader))","metadata":{"execution":{"iopub.status.busy":"2021-06-10T02:36:51.888643Z","iopub.execute_input":"2021-06-10T02:36:51.889196Z","iopub.status.idle":"2021-06-10T02:36:51.899014Z","shell.execute_reply.started":"2021-06-10T02:36:51.889157Z","shell.execute_reply":"2021-06-10T02:36:51.897885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nimport random\nimport numpy as np \n\n# Set the seed.\nseed_val = 101\n\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)\n\n# Store the average loss after each epoch so we can plot them.\nloss_values = []\n\n\n# For each epoch...\nfor epoch in range(0, NUM_EPOCHS):\n    \n    print(\"\")\n    print('======== Epoch {:} / {:} ========'.format(epoch + 1, NUM_EPOCHS))\n    \n    stacked_val_labels = []\n    targets_list = []\n\n    # ========================================\n    #               Training\n    # ========================================\n    \n    print('Training...')\n    \n    # put the model into train mode\n    model.train()\n    \n    # This turns gradient calculations on and off.\n    torch.set_grad_enabled(True)\n\n\n    # Reset the total loss for this epoch.\n    total_train_loss = 0\n\n    for i, batch in enumerate(train_dataloader):\n        \n        train_status = 'Batch ' + str(i) + ' of ' + str(len(train_dataloader))\n        print(train_status, end='\\r')\n\n\n        b_input_ids = batch[0].to(device)\n        b_input_mask = batch[1].to(device)\n        b_labels = batch[2].to(device)\n\n        model.zero_grad()        \n\n\n        outputs = model(b_input_ids, \n                    attention_mask=b_input_mask,\n                    labels=b_labels)\n        \n        # Get the loss from the outputs tuple: (loss, logits)\n        loss = outputs[0]\n        \n        # Convert the loss from a torch tensor to a number.\n        # Calculate the total loss.\n        total_train_loss = total_train_loss + loss.item()\n        \n        # Zero the gradients\n        optimizer.zero_grad()\n        \n        # Perform a backward pass to calculate the gradients.\n        loss.backward()\n        \n        # Clip the norm of the gradients to 1.0.\n        # This is to help prevent the \"exploding gradients\" problem.\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        \n        \n        \n        # Use the optimizer to update the weights.\n        \n        # Optimizer for GPU\n        # optimizer.step() \n        \n        # Optimizer for TPU\n        # https://pytorch.org/xla/\n        xm.optimizer_step(optimizer, barrier=True)\n\n    \n    print('Train loss:' ,total_train_loss)\n\n\n    # ========================================\n    #               Validation\n    # ========================================\n    print('\\nValidation...')\n\n    # Put the model in evaluation mode.\n    model.eval()\n\n    # Turn off the gradient calculations.\n    # This tells the model not to compute or store gradients.\n    # This step saves memory and speeds up validation.\n    torch.set_grad_enabled(False)\n    \n    \n    # Reset the total loss for this epoch.\n    total_val_loss = 0\n    \n    \n    for j, batch in enumerate(val_dataloader):\n        \n        val_status = 'Batch ' + str(j) + ' of ' + str(len(val_dataloader))\n        \n        print(val_status, end='\\r')\n\n        b_input_ids = batch[0].to(device)\n        b_input_mask = batch[1].to(device)\n        b_labels = batch[2].to(device)      \n\n\n        outputs = model(b_input_ids, \n                attention_mask=b_input_mask, \n                labels=b_labels)\n        \n        # Get the loss from the outputs tuple: (loss, logits)\n        loss = outputs[0]\n        \n        # Convert the loss from a torch tensor to a number.\n        # Calculate the total loss.\n        total_val_loss = total_val_loss + loss.item()\n        \n\n        # Get the preds\n        preds = outputs[1]\n        \n        \n        \n        # Move preds to the CPU\n        val_preds = preds.detach().cpu().numpy()\n        \n        # Move the labels to the cpu\n        targets_np = b_labels.to('cpu').numpy()\n\n        # Append the labels to a numpy list\n        targets_list.extend(targets_np)\n\n        if j == 0:  # first batch\n            stacked_val_preds = val_preds\n\n        else:\n            stacked_val_preds = np.vstack((stacked_val_preds, val_preds))\n\n    \n    # Calculate the validation accuracy\n    y_true = targets_list\n    y_pred = np.argmax(stacked_val_preds, axis=1)\n    \n    val_acc = accuracy_score(y_true, y_pred)\n    \n    \n    print('Val loss:' ,total_val_loss)\n    print('Val acc: ', val_acc)\n\n\n    # Save the Model\n    torch.save(model.state_dict(), 'model.pt')\n    \n    # Use the garbage collector to save memory.\n    gc.collect()\n    \n    \ncv_acc = sum(epoch_acc_scores_list)/NUM_FOLDS_TO_TRAIN\nprint(\"\\nCV Acc:\", cv_acc)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T02:36:51.900907Z","iopub.execute_input":"2021-06-10T02:36:51.901341Z"},"trusted":true},"execution_count":null,"outputs":[]}]}