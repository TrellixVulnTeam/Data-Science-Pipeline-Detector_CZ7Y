{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Exploration\n---","metadata":{}},{"cell_type":"markdown","source":"## Importando o DataSet","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/contradictory-my-dear-watson/train.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-06-12T20:27:39.034186Z","iopub.execute_input":"2021-06-12T20:27:39.034687Z","iopub.status.idle":"2021-06-12T20:27:39.118915Z","shell.execute_reply.started":"2021-06-12T20:27:39.034618Z","shell.execute_reply":"2021-06-12T20:27:39.1179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head(20)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T12:38:36.407513Z","iopub.execute_input":"2021-06-12T12:38:36.4079Z","iopub.status.idle":"2021-06-12T12:38:36.452067Z","shell.execute_reply.started":"2021-06-12T12:38:36.40787Z","shell.execute_reply":"2021-06-12T12:38:36.451273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.tail()","metadata":{"execution":{"iopub.status.busy":"2021-06-12T12:38:38.437791Z","iopub.execute_input":"2021-06-12T12:38:38.438394Z","iopub.status.idle":"2021-06-12T12:38:38.451209Z","shell.execute_reply.started":"2021-06-12T12:38:38.438357Z","shell.execute_reply":"2021-06-12T12:38:38.450328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2021-06-12T12:38:39.750152Z","iopub.execute_input":"2021-06-12T12:38:39.750565Z","iopub.status.idle":"2021-06-12T12:38:39.756316Z","shell.execute_reply.started":"2021-06-12T12:38:39.750531Z","shell.execute_reply":"2021-06-12T12:38:39.755497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Verificando se há dados nulos ou vazios","metadata":{}},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-06-12T12:38:43.682907Z","iopub.execute_input":"2021-06-12T12:38:43.683292Z","iopub.status.idle":"2021-06-12T12:38:43.69944Z","shell.execute_reply.started":"2021-06-12T12:38:43.683258Z","shell.execute_reply":"2021-06-12T12:38:43.6986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Verificar quantas e quais as línguas presentes DF","metadata":{}},{"cell_type":"code","source":"print(\n    f\"number of languages: {len(list(df.language.unique()))}\\n\\n\"\n    f\"{list(df.language.unique())}\")\n","metadata":{"execution":{"iopub.status.busy":"2021-06-12T12:39:07.440352Z","iopub.execute_input":"2021-06-12T12:39:07.440708Z","iopub.status.idle":"2021-06-12T12:39:07.448275Z","shell.execute_reply.started":"2021-06-12T12:39:07.440669Z","shell.execute_reply":"2021-06-12T12:39:07.447171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Relacionando a quantidade de cada língua dentro do DF","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\ny = df[\"language\"].value_counts()\n\nplt.figure(figsize=(15,5))\nsns.barplot(x=y.index, y=y)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-12T12:43:26.558804Z","iopub.execute_input":"2021-06-12T12:43:26.559185Z","iopub.status.idle":"2021-06-12T12:43:26.842326Z","shell.execute_reply.started":"2021-06-12T12:43:26.559152Z","shell.execute_reply":"2021-06-12T12:43:26.841498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label, frequencies = np.unique(df.language, return_counts=True)\nplt.figure(figsize=(10,10))\nplt.pie(frequencies, labels=label, autopct='%.1f%%')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-12T12:43:41.029575Z","iopub.execute_input":"2021-06-12T12:43:41.029974Z","iopub.status.idle":"2021-06-12T12:43:41.305899Z","shell.execute_reply.started":"2021-06-12T12:43:41.029939Z","shell.execute_reply":"2021-06-12T12:43:41.30514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### OBS:\nAssim pode-se observar que o dataset é majoritariamente composto por premissas e hipóteses na língua inglesas enquanto que o restante das frases são igualmente distribuídas entre as outras 14 línguas.","metadata":{}},{"cell_type":"markdown","source":"## Relacionando a quantidade de labels em cada língua","metadata":{}},{"cell_type":"code","source":"pd.options.plotting.backend = \"matplotlib\"\ndf.label.hist()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-12T20:27:46.257085Z","iopub.execute_input":"2021-06-12T20:27:46.25744Z","iopub.status.idle":"2021-06-12T20:27:46.476438Z","shell.execute_reply.started":"2021-06-12T20:27:46.25741Z","shell.execute_reply":"2021-06-12T20:27:46.47533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nsns.catplot(data=df, x=\"label\", col=\"lang_abv\", col_wrap=5, kind=\"count\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-12T12:47:21.480417Z","iopub.execute_input":"2021-06-12T12:47:21.480787Z","iopub.status.idle":"2021-06-12T12:47:23.935768Z","shell.execute_reply.started":"2021-06-12T12:47:21.480754Z","shell.execute_reply":"2021-06-12T12:47:23.934864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### OBS:\nAs labels no total são levemente desbalanceadas.Esse desbalaço está presente em todos os idiomas. ","metadata":{}},{"cell_type":"markdown","source":"# Pre-Processamento\n---","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ndf = df[[\"premise\", \"hypothesis\", \"lang_abv\", \"label\"]]\nX = df.iloc[:, :-1]\ny = df.label\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)  ","metadata":{"execution":{"iopub.status.busy":"2021-06-12T17:53:26.428535Z","iopub.execute_input":"2021-06-12T17:53:26.428893Z","iopub.status.idle":"2021-06-12T17:53:26.452168Z","shell.execute_reply.started":"2021-06-12T17:53:26.428862Z","shell.execute_reply":"2021-06-12T17:53:26.451095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Bag of Words:","metadata":{}},{"cell_type":"markdown","source":"* ### Separando premissas e hipoteses por idioma:","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import PCA\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import Normalizer\n\ndef preprossecing1(train, test, train_label, test_label, lan):\n    \n    vec_p = TfidfVectorizer()\n    vec_h = TfidfVectorizer()\n    pca = PCA(0.9)\n    normalizer = Normalizer(copy=False) # para aplicar método PCA normalização dos dados é reconmendável\n    pipe = make_pipeline(normalizer, pca)\n\n    # filtro de linguagem\n    index_train = train[train.lang_abv==lan].index\n    train_label_final = train_label.loc[index_train]\n    \n    index_test = test[test.lang_abv==lan].index\n    test_label_final = test_label.loc[index_test]\n    \n    # premissas\n    train_p = vec_p.fit_transform(train[train.lang_abv==lan].premise)\n    df_train_p = pd.DataFrame.sparse.from_spmatrix(train_p, columns=[\"p_\"+k for k in vec_p.get_feature_names()])\n    \n    test_p = vec_p.transform(test[test.lang_abv==lan].premise)\n    df_test_p = pd.DataFrame.sparse.from_spmatrix(test_p, columns=[\"p_\"+k for k in vec_p.get_feature_names()])\n\n    #hipotesis\n    train_h = vec_h.fit_transform(train[train.lang_abv==lan].hypothesis)\n    df_train_h = pd.DataFrame.sparse.from_spmatrix(train_h, columns=[\"h_\"+k for k in vec_h.get_feature_names()])\n    \n    test_h = vec_h.transform(test[test.lang_abv==lan].hypothesis)\n    df_test_h = pd.DataFrame.sparse.from_spmatrix(test_h, columns=[\"h_\"+k for k in vec_h.get_feature_names()])\n    \n    #concatenando..\n    train_final = pd.concat([df_train_p, df_train_h], axis=1)\n    test_final = pd.concat([df_test_p, df_test_h], axis=1)\n    \n    train_final = pipe.fit_transform(train_final.to_numpy())\n    test_final = pipe.transform(test_final.to_numpy())\n\n    return train_final, test_final, train_label_final, test_label_final","metadata":{"execution":{"iopub.status.busy":"2021-06-12T19:53:53.417032Z","iopub.execute_input":"2021-06-12T19:53:53.417441Z","iopub.status.idle":"2021-06-12T19:53:53.430745Z","shell.execute_reply.started":"2021-06-12T19:53:53.417399Z","shell.execute_reply":"2021-06-12T19:53:53.42974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* ### Agrupando premissa e hipótese por idioma","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import PCA\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import Normalizer\n\ndef preprossecing2(train, test, train_label, test_label, lan):\n    \n    vec = TfidfVectorizer()\n    pca = PCA(0.9)\n    normalizer = Normalizer(copy=False) # para aplicar método PCA normalização dos dados é reconmendável\n    pipe = make_pipeline(normalizer, pca)\n\n    # filtro de linguagem\n    index_train = train[train.lang_abv==lan].index\n    train_label_final = train_label.loc[index_train]\n    \n    index_test = test[test.lang_abv==lan].index\n    test_label_final = test_label.loc[index_test]\n    \n    #juntando premissas e hipotesis\n    train_text = [p+\" \"+h for p, h in zip(train[train.lang_abv==lan].premise, train[train.lang_abv==lan].hypothesis)]\n    \n    test_text = [p+\" \"+h for p, h in zip(test[test.lang_abv==lan].premise, test[test.lang_abv==lan].hypothesis)]\n    \n\n    #tfidvec\n    train_h = vec.fit_transform(train_text)\n    train_final = pd.DataFrame.sparse.from_spmatrix(train_h, columns=[k for k in vec.get_feature_names()])\n    \n    test_h = vec.transform(test_text)\n    test_final = pd.DataFrame.sparse.from_spmatrix(test_h, columns=[k for k in vec.get_feature_names()])\n    \n    \n    train_final = pipe.fit_transform(train_final.to_numpy())\n    test_final = pipe.transform(test_final.to_numpy())\n\n    return train_final, test_final, train_label_final, test_label_final","metadata":{"execution":{"iopub.status.busy":"2021-06-12T19:53:53.432323Z","iopub.execute_input":"2021-06-12T19:53:53.43295Z","iopub.status.idle":"2021-06-12T19:53:53.452932Z","shell.execute_reply.started":"2021-06-12T19:53:53.432904Z","shell.execute_reply":"2021-06-12T19:53:53.452057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modelo de Machine Learning e Validação:\n---","metadata":{}},{"cell_type":"markdown","source":"## Random Forest Classifier:","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn import metrics\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Separando premissas e hipoteses por idioma\ndef predictRFC(X_train, X_test, y_train, y_test, inteiro):\n    for lan in X_train.lang_abv.unique():\n            \n        rfc = RandomForestClassifier(n_estimators=1000, max_depth=2)\n\n        if inteiro==0:\n            train, test, train_label, test_label = preprossecing1(X_train, y_train, X_test, y_test, lan)\n        else:\n            train, test, train_label, test_label = preprossecing2(X_train, y_train, X_test, y_test, lan)\n            \n        fig, axs = plt.subplots(1,3, figsize=(15,6))\n        fig.suptitle(f\"Language: {lan}\")\n        for n in [0,1,2]:\n            rfc.fit(train, train_label==n)\n            pred = rfc.predict(test)\n\n            accuracy = metrics.accuracy_score(pred, test_label==n)\n            v = metrics.v_measure_score(pred, test_label==n)\n            \n            cm = confusion_matrix(test_label==n, pred, normalize=\"true\")\n            axs[n].imshow(cm)\n            axs[n].set_xticks(np.arange(2))\n            axs[n].set_yticks(np.arange(2))\n            axs[n].set_xticklabels([\"False\", \"True\"])\n            axs[n].set_yticklabels([\"False\", \"True\"])\n            axs[n].set_title(f\"Label {n}\\naccuracy: {accuracy:.3f}\\nv_score: {v:.3f}\")\n            for i in range(2):\n                for j in range(2):\n                    text = axs[n].text(j, i, cm[i, j],\n                                   ha=\"center\", va=\"center\", color=\"k\")\n            \n        plt.show() ","metadata":{"execution":{"iopub.status.busy":"2021-06-12T19:26:36.354819Z","iopub.execute_input":"2021-06-12T19:26:36.35517Z","iopub.status.idle":"2021-06-12T19:31:50.790264Z","shell.execute_reply.started":"2021-06-12T19:26:36.355139Z","shell.execute_reply":"2021-06-12T19:31:50.789141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# separado\n# predictRFC(X_train, y_train, X_test, y_test, 0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# agrupado\npredictRFC(X_train, y_train, X_test, y_test, 1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## XGBoost:","metadata":{}},{"cell_type":"code","source":"import xgboost as xgb\n\ndef predictXGB(X_train, X_test, y_train, y_test, inteiro):\n    for lan in X_train.lang_abv.unique():\n            \n        xgboost = xgb.XGBClassifier(n_estimators=500, max_detph=2, scale_pos_weight=10)\n\n        if inteiro==0:\n            train, test, train_label, test_label = preprossecing1(X_train, y_train, X_test, y_test, lan)\n        else:\n            train, test, train_label, test_label = preprossecing2(X_train, y_train, X_test, y_test, lan)\n    \n        fig, axs = plt.subplots(1,3, figsize=(15,8))\n        fig.suptitle(f\"Language: {lan}\")\n        for n in [0,1,2]:\n            xgboost.fit(train, train_label==n)\n            pred = xgboost.predict(test)\n\n            accuracy = metrics.accuracy_score(pred, test_label==n)\n            v = metrics.v_measure_score(pred, test_label==n)\n            \n            cm = confusion_matrix(test_label==n, pred, normalize=\"true\")\n            axs[n].imshow(cm)\n            axs[n].set_xticks(np.arange(2))\n            axs[n].set_yticks(np.arange(2))\n            axs[n].set_xticklabels([\"False\", \"True\"])\n            axs[n].set_yticklabels([\"False\", \"True\"])\n            axs[n].set_title(f\"Label {n}\\naccuracy: {accuracy:.3f}\\nv_score: {v:.3f}\")\n            for i in range(2):\n                for j in range(2):\n                    text = axs[n].text(j, i, cm[i, j],\n                                   ha=\"center\", va=\"center\", color=\"k\")\n            \n        plt.show() ","metadata":{"execution":{"iopub.status.busy":"2021-06-12T19:33:29.104631Z","iopub.execute_input":"2021-06-12T19:33:29.105014Z","iopub.status.idle":"2021-06-12T19:53:53.414558Z","shell.execute_reply.started":"2021-06-12T19:33:29.104981Z","shell.execute_reply":"2021-06-12T19:53:53.413316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# separado\n# predictXGB(X_train, y_train, X_test, y_test, 0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# agrupado\npredictXGB(X_train, y_train, X_test, y_test, 1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusão\n---\n\n## Modelos\nApesar dos bons resultados de acurácia do RandomForest, o resultado não é confiável uma vez que o modelo gera a mesma resposta independente da label ou do idioma. Já com o método XGBosst, obteve-se resultado mais consistente com variação das predições em função tanto da label quanto do idioma.\n\n## Abordagem\nA aboradagem utilizada foi o 'bag of words', no qual considera-se apenas a frequência das palavras presentes, transformando textos em vetores. Entretanto, abordagens com 'word embeddings' aparentemente seriam melhores uma vez que ela considera o valor do textos, permintindo estabelecer comparações entre elas, o que, neste caso, seria uma ótima alternativa.","metadata":{}}]}