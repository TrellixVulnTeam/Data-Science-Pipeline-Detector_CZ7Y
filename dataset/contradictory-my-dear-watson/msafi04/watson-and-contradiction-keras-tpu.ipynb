{"cells":[{"metadata":{},"cell_type":"markdown","source":"__Competition Challenge__\n\nCreate an NLI model that assigns labels of 0, 1, or 2 (corresponding to entailment, neutral, and contradiction) to pairs of premises and hypotheses in the testset.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"__Competition Metric__\n\n*Accuracy*","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"__Competition Rules__\n\n- Notebook competition\n- CPU, GPU, or TPU all allowed\n- Submission file name: *sumbmission.csv*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install -q transformers==3.0.2","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport gc\nfrom sklearn.model_selection import KFold, StratifiedKFold, train_test_split\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\n\nos.environ[\"WANDB_API_KEY\"] = \"0\"\n\nimport transformers\nfrom transformers import AutoTokenizer, TFAutoModel\n\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nfrom time import time, strftime, gmtime\n\nstart = time()\n#print(start)\n\nimport datetime\nprint(str(datetime.datetime.now()))\n\nprint(tf.version.VERSION)\nprint(transformers.__version__)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/contradictory-my-dear-watson/train.csv')\ntrain","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/contradictory-my-dear-watson/test.csv')\ntest","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv('/kaggle/input/contradictory-my-dear-watson/sample_submission.csv')\nsub","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"__Target Label Distribution__","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (10, 10))\nsns.countplot(train['label'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"__Language Distribution__","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lbls, freqs = np.unique(train['language'].values, return_counts = True)\n#print(list(zip(lbls, freqs)))\n\nplt.figure(figsize = (10, 10))\nplt.title('Train')\nplt.pie(freqs, labels = lbls, autopct = '%1.1f%%', shadow = False, startangle = 90)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lbls, freqs = np.unique(test['language'].values, return_counts = True)\n#print(list(zip(lbls, freqs)))\n\nplt.figure(figsize = (10, 10))\nplt.title('Test')\nplt.pie(freqs, labels = lbls, autopct = '%1.1f%%', shadow = False, startangle = 90)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"__TPU Config__","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept ValueError:\n    strategy = tf.distribute.get_strategy() # for CPU and single GPU\n    print('Number of replicas:', strategy.num_replicas_in_sync)\n\nreplicas = strategy.num_replicas_in_sync\nbatch_size = 8 * replicas\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)\nprint('Batch_size: ', batch_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_name = 'jplu/tf-xlm-roberta-large'\nepochs = 4\nmaxlen = 80\n\nAUTO = tf.data.experimental.AUTOTUNE","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"__Encoding text data using tokenizer__","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_name)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def display_training_curves(training, validation, title, subplot):\n    \"\"\"\n    Source: https://www.kaggle.com/mgornergoogle/getting-started-with-100-flowers-on-tpu\n    \"\"\"\n    if subplot%10==1: # set up the subplots on the first call\n        plt.subplots(figsize = (20, 15), facecolor='#F0F0F0')\n        plt.tight_layout()\n    ax = plt.subplot(subplot)\n    ax.set_facecolor('#F8F8F8')\n    ax.plot(training)\n    ax.plot(validation)\n    ax.set_title('model '+ title)\n    ax.set_ylabel(title)\n    ax.set_xlabel('epoch')\n    ax.legend(['train', 'valid.'])\n    #plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"__Creating TF Dataset__","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_training_dataset(idx, df = train, is_train = True):\n    text = df[['premise', 'hypothesis']].values[idx].tolist()\n    text_enc = tokenizer.batch_encode_plus(\n                            text,\n                            pad_to_max_length = True,\n                            max_length = maxlen\n                        )\n    dataset = tf.data.Dataset.from_tensor_slices((text_enc['input_ids'], df['label'][idx].values))\n    dataset = dataset.repeat()\n    dataset = dataset.shuffle(2020)\n    dataset = dataset.batch(batch_size)\n    dataset = dataset.prefetch(AUTO)\n    return dataset\n\ndef get_valid_dataset(idx, df = train, is_train = False):\n    text = df[['premise', 'hypothesis']].values[idx].tolist()\n    text_enc = tokenizer.batch_encode_plus(\n                            text,\n                            pad_to_max_length = True,\n                            max_length = maxlen\n                        )\n    dataset = tf.data.Dataset.from_tensor_slices((text_enc['input_ids'], df['label'][idx].values))\n    dataset = dataset.batch(batch_size)\n    dataset = dataset.prefetch(AUTO)\n    return dataset\n\ndef get_test_dataset(df = test, is_train = False):\n    text = df[['premise', 'hypothesis']].values.tolist()\n    text_enc = tokenizer.batch_encode_plus(\n                            text,\n                            pad_to_max_length = True,\n                            max_length = maxlen\n                        )\n    dataset = tf.data.Dataset.from_tensor_slices(text_enc['input_ids'])\n    dataset = dataset.batch(batch_size)\n    return dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model(maxlen, model_name):\n    with strategy.scope():\n        #Load Transformer model\n        base_model = TFAutoModel.from_pretrained(model_name)\n\n        input_word_ids = tf.keras.Input(shape = (maxlen, ), dtype = tf.int32, name = \"input_word_ids\")\n\n        #Encoding the input with the model\n        embedding = base_model(input_word_ids)[0]\n\n        #Extract the token used for classification, which is <s> and pass it to softmax (3 possible labels)\n        out_tokens = embedding[:, 0, :]\n\n        output = tf.keras.layers.Dense(3, activation = 'softmax')(out_tokens)\n\n        model = tf.keras.Model(inputs = input_word_ids, outputs = output)\n\n        model.compile(tf.keras.optimizers.Adam(lr = 1e-5), \n                      loss = 'sparse_categorical_crossentropy', \n                      metrics = ['accuracy'])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = build_model(maxlen, model_name)\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"folds = 3\nkf = KFold(n_splits = folds, shuffle = True, random_state = 777)\nmodels = []\nhistories = []\npredictions = np.zeros((test.shape[0], 3))\n\nfor fold, (trn_idx, val_idx) in enumerate(kf.split(np.arange(train['label'].shape[0]))):\n    print('\\n')\n    print('-'*50)\n    print(f'Training fold {fold + 1}')\n    train_dataset = get_training_dataset(trn_idx, df = train, is_train = True)\n    valid_dataset = get_valid_dataset(val_idx, df = train, is_train = False)\n    K.clear_session()\n    model = build_model(maxlen, model_name)\n    checkpoint = tf.keras.callbacks.ModelCheckpoint(\n                'XLM-R_fold-%i.h5'%fold, monitor = 'val_loss', verbose = 1, save_best_only = True,\n                save_weights_only = True, mode = 'min', save_freq = 'epoch'\n                )\n    print('Model Training.....')\n    STEPS_PER_EPOCH = len(trn_idx) // batch_size\n    history = model.fit(\n                train_dataset, epochs = epochs, verbose = 1, \n                steps_per_epoch = STEPS_PER_EPOCH,\n                batch_size = batch_size, \n                validation_data = valid_dataset\n            )\n    \n    display_training_curves(\n                history.history['loss'], \n                history.history['val_loss'], \n                'loss', 311\n                )\n    display_training_curves(\n                history.history['accuracy'], \n                history.history['val_accuracy'], \n                'accuracy', 312\n                )\n    histories.append(history)\n    models.append(model)\n    print('Prediting on test data..')\n    test_dataset = get_test_dataset(test, is_train = False)\n    pred = model.predict(test_dataset, verbose = 1)\n    \n    predictions += pred / folds\n    \n    del history, train_dataset, valid_dataset, model\n    gc.collect()\nprint('\\n')\nprint('-'*50)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"__Predicting on testset__","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sub['prediction'] = np.argmax(predictions, axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.to_csv('./submission.csv', index = False)\nsub","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (10, 10))\nsns.countplot(sub['prediction'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"finish = time()\nprint(strftime(\"%H:%M:%S\", gmtime(finish - start)))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}