{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport keras\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style='darkgrid')\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"traindf = pd.read_csv(\"/kaggle/input/contradictory-my-dear-watson/train.csv\")\ntestdf = pd.read_csv(\"/kaggle/input/contradictory-my-dear-watson/test.csv\")\nprint(\"Number of rows and columns in train data : \",traindf.shape)\nprint(\"Number of rows and columns in test data : \",testdf.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"traindf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"traindf[\"lang_abv\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,5))\nsns.countplot(x='label', data=traindf,\n                   order=list(traindf['label'].value_counts().sort_index().index) ,\n                   color='black')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Well Balanced on Label Distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Number of different Langauges: {len(traindf[\"language\"].unique())}')\nplt.figure(figsize=(20,5))\nsns.countplot(x='language', data=traindf,\n                   order=list(traindf['language'].value_counts().sort_index().index) ,\n                   color='black')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,5))\nsns.countplot(traindf['language'], hue = traindf['label'] ,\n                   color='black')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Labels are also well distributed across multiple langauges"},{"metadata":{"trusted":true},"cell_type":"code","source":"langdf=pd.DataFrame()\nlangdf['Name']=traindf.language.value_counts().index\nlangdf['Count']=traindf.language.value_counts().values\n\nlangdf_test=pd.DataFrame()\nlangdf_test['Name']=testdf.language.value_counts().index\nlangdf_test['Count']=testdf.language.value_counts().values\n\nlangdf['Key'] = 'train'\nlangdf_test['Key'] = 'test'\nDF = pd.concat([langdf,langdf_test],keys=['train','test'])\nDF.groupby(['Name','Key']).sum().unstack('Key').plot(kind='bar',figsize=(20, 5),color='black')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"traindf[\"lang_abv\"]= traindf[\"lang_abv\"].replace(\"zh\",\"zh-tw\") \ntestdf[\"lang_abv\"]= testdf[\"lang_abv\"].replace(\"zh\",\"zh-tw\") ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h5>Using Google translate API for handling data from different Languages</h5>\nFor more Info please refer - https://stackabuse.com/text-translation-with-google-translate-api-in-python/"},{"metadata":{"trusted":true},"cell_type":"code","source":"pip install googletrans","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import googletrans\nprint(googletrans.LANGUAGES)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from googletrans import Translator\ntranslator = Translator()\nresult = translator.translate('Main acha hoon', src='hi')\nprint(result.src)\nprint(result.dest)\nprint(result.origin)\nprint(result.text)\nprint(result.pronunciation)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def To_English(language,textstring):\n    if language!=\"en\":\n        translator = Translator()\n        return translator.translate(textstring,dest = \"en\").text\n    else:\n        return textstring","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept ValueError:\n    strategy = tf.distribute.get_strategy() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"strategy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def Translate(x):\n    translator = Translator()\n    return translator.translate(x).text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#traindf.premise[traindf.lang_abv!= 'en']=traindf.premise[traindf.lang_abv!= 'en'].apply(lambda x: Translate(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#traindf.hypothesis[traindf.lang_abv!= 'en']=traindf.hypothesis[traindf.lang_abv!= 'en'].apply(lambda x: Translate(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#traindf.to_csv(\"traindf.csv\",index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#testdf.hypothesis[testdf.lang_abv!= 'en']=testdf.hypothesis[testdf.lang_abv!= 'en'].apply(lambda x: Translate(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#testdf.premise[testdf.lang_abv!= 'en']=testdf.premise[testdf.lang_abv!= 'en'].apply(lambda x: Translate(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#testdf.to_csv(\"testdf.csv\",index=True)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true},"cell_type":"markdown","source":"Please find the translated data <a href=\"https://www.kaggle.com/krsna540/translated-data\">here</a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"Updatedtraindf = pd.read_csv(\"../input/translated-data/traindf.csv\")\nUpdatedtestdf = pd.read_csv(\"../input/translated-data/testdf.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Updatedtraindf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LangDf = pd.DataFrame()\nLangDf['premise'] = Updatedtraindf['premise']\nLangDf['hypothesis'] = Updatedtraindf['hypothesis']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords\nstop_words=stopwords.words('english')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(stop_words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LangDf['premise'][0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For a given sentence in the training set, we randomly choose and perform one of the following operations:\n1. Synonym Replacement (SR): <br/>\nRandomly choose n words from the sentence that are not stop words. Replace each of these words with one of its synonyms chosen at random.<br/>\n2. Random Insertion (RI): <br/>\nFind a random synonym of a random word in the sentence that is not a stop word. Insert that synonym into a random position in the sentence. Do this n times.<br/>\n3. Random Swap (RS):<br/>\nRandomly choose two words in the sentence and swap their positions. Do this n times.<br/>\n4. Random Deletion (RD):<br/>\nRandomly remove each word in the sentence with probability p"},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\nfrom random import shuffle\nrandom.seed(1)\n\n# import these modules \nfrom nltk.stem import WordNetLemmatizer \nfrom nltk.tokenize import sent_tokenize, word_tokenize\n#cleaning up text\nimport re\ndef Preprocess_text(line):\n\n    clean_line = \"\"\n\n    line = line.replace(\"â€™\", \"\")\n    line = line.replace(\"'\", \"\")\n    line = line.replace(\"-\", \" \") #replace hyphens with spaces\n    line = line.replace(\"\\t\", \" \")\n    line = line.replace(\"\\n\", \" \")\n    line = line.lower()\n\n    for char in line:\n        if char in 'qwertyuiopasdfghjklzxcvbnm ':\n            clean_line += char\n        else:\n            clean_line += ' '\n\n    clean_line = re.sub(' +',' ',clean_line) #delete extra spaces\n    if clean_line[0] == ' ':\n        clean_line = clean_line[1:]\n    \n    #Removing stop words and convert words to base forms\n    clean_line=LemmaSentence(clean_line)\n    return clean_line\n\ndef LemmaSentence(sentence):\n    token_words=word_tokenize(sentence)\n    token_words\n    New_sentence=[]\n    updated_word_list = list(set([word for word in token_words if word not in stop_words]))\n    for word in token_words:\n        lemmatizer = WordNetLemmatizer()\n        New_sentence.append(lemmatizer.lemmatize(word))\n        New_sentence.append(\" \")\n        \n    return \"\".join(New_sentence)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################################################################\n# Synonym replacement\n# Replace n words in the sentence with synonyms from wordnet\n########################################################################\n\n#for the first time you use wordnet\n#import nltk\n#nltk.download('wordnet')\nfrom nltk.corpus import wordnet \n\ndef synonym_replacement(words, n):\n    new_words = words.copy()\n    random_word_list = list(set([word for word in words if word not in stop_words]))\n    random.shuffle(random_word_list)\n    num_replaced = 0\n    for random_word in random_word_list:\n        synonyms = get_synonyms(random_word)\n        if len(synonyms) >= 1:\n            synonym = random.choice(list(synonyms))\n            new_words = [synonym if word == random_word else word for word in new_words]\n            #print(\"replaced\", random_word, \"with\", synonym)\n            num_replaced += 1\n        if num_replaced >= n: #only replace up to n words\n            break\n\n#this is stupid but we need it, trust me\n    sentence = ' '.join(new_words)\n    new_words = sentence.split(' ')\n\n    return new_words\n\ndef get_synonyms(word):\n    synonyms = set()\n    for syn in wordnet.synsets(word): \n        for l in syn.lemmas(): \n            synonym = l.name().replace(\"_\", \" \").replace(\"-\", \" \").lower()\n            synonym = \"\".join([char for char in synonym if char in ' qwertyuiopasdfghjklzxcvbnm'])\n            synonyms.add(synonym) \n    if word in synonyms:\n        synonyms.remove(word)\n    return list(synonyms)\n\n########################################################################\n# Random deletion\n# Randomly delete words from the sentence with probability p\n########################################################################\n\ndef random_deletion(words, p):\n\n#obviously, if there's only one word, don't delete it\n    if len(words) == 1:\n        return words\n\n#randomly delete words with probability p\n    new_words = []\n    for word in words:\n        r = random.uniform(0, 1)\n        if r > p:\n            new_words.append(word)\n\n#if you end up deleting all words, just return a random word\n    if len(new_words) == 0:\n        rand_int = random.randint(0, len(words)-1)\n        return [words[rand_int]]\n    return new_words\n\n########################################################################\n# Random swap\n# Randomly swap two words in the sentence n times\n########################################################################\n\ndef random_swap(words, n):\n    new_words = words.copy()\n    for _ in range(n):\n        new_words = swap_word(new_words)\n    return new_words\n\ndef swap_word(new_words):\n    random_idx_1 = random.randint(0, len(new_words)-1)\n    random_idx_2 = random_idx_1\n    counter = 0\n    while random_idx_2 == random_idx_1:\n        random_idx_2 = random.randint(0, len(new_words)-1)\n        counter += 1\n        if counter > 3:\n            return new_words\n    new_words[random_idx_1], new_words[random_idx_2] = new_words[random_idx_2], new_words[random_idx_1] \n    return new_words\n\n########################################################################\n# Random insertion\n# Randomly insert n words into the sentence\n########################################################################\n\ndef random_insertion(words, n):\n    new_words = words.copy()\n    for _ in range(n):\n        add_word(new_words)\n    return new_words\n\ndef add_word(new_words):\n    synonyms = []\n    counter = 0\n    while len(synonyms) < 1:\n        random_word = new_words[random.randint(0, len(new_words)-1)]\n        synonyms = get_synonyms(random_word)\n        counter += 1\n        if counter >= 10:\n            return\n    random_synonym = synonyms[0]\n    random_idx = random.randint(0, len(new_words)-1)\n    new_words.insert(random_idx, random_synonym)\n\n########################################################################\n# main data augmentation function\n########################################################################\n\ndef eda(sentence, alpha_sr=0.1, alpha_ri=0.1, alpha_rs=0.1, p_rd=0.1, num_aug=7):\n    words = sentence.split(' ')\n    words = [word for word in words if word is not '']\n    num_words = len(words)\n    augmented_sentences = []\n    num_new_per_technique = int(num_aug/4)+1\n    n_sr = max(1, int(alpha_sr*num_words))\n    n_ri = max(1, int(alpha_ri*num_words))\n    n_rs = max(1, int(alpha_rs*num_words))\n\n#sr\n    for _ in range(num_new_per_technique):\n        a_words = synonym_replacement(words, n_sr)\n        augmented_sentences.append(' '.join(a_words))\n\n#ri\n    for _ in range(num_new_per_technique):\n        a_words = random_insertion(words, n_ri)\n        augmented_sentences.append(' '.join(a_words))\n\n#rs\n    for _ in range(num_new_per_technique):\n        a_words = random_swap(words, n_rs)\n        augmented_sentences.append(' '.join(a_words))\n\n    #rd\n    for _ in range(num_new_per_technique):\n        a_words = random_deletion(words, p_rd)\n        augmented_sentences.append(' '.join(a_words))\n\n    augmented_sentences = [sentence for sentence in augmented_sentences]\n    shuffle(augmented_sentences)\n\n    #trim so that we have the desired number of augmented sentences\n    if num_aug >= 1:\n        augmented_sentences = augmented_sentences[:num_aug]\n    else:\n        keep_prob = num_aug / len(augmented_sentences)\n        augmented_sentences = [s for s in augmented_sentences if random.uniform(0, 1) < keep_prob]\n\n    #append the original sentence\n    augmented_sentences.append(sentence)\n    return augmented_sentences","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Actual text - {Preprocess_text(LangDf['premise'][1])}\")\nprint(\"Augmented text -\")\nfor x in eda(Preprocess_text(LangDf['premise'][1])):\n    print(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Updatedtraindf=Updatedtraindf.drop([\"id\",\"language\",\"lang_abv\",\"Unnamed: 0\"],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(Updatedtraindf.shape)\nUpdatedtraindf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"augmenteddf = pd.DataFrame(columns = ['premise', 'hypothesis', 'label']) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rows=[]\ndef augment_data(premise,hypothesis,label):\n     # Pass a series in append() to append a row in dataframe  \n    rows.append([premise,hypothesis,label])\n    for x in eda(premise):\n        rows.append([x,hypothesis,label])\n    for y in eda(hypothesis):\n        rows.append([premise,y,label])    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in Updatedtraindf.index: \n    augment_data(Preprocess_text(Updatedtraindf['premise'][i]),Preprocess_text(Updatedtraindf['hypothesis'][i]), Updatedtraindf['label'][i])\n    \naugmenteddf=pd.DataFrame(rows, columns=['premise', 'hypothesis', 'label'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(Updatedtraindf.shape)\nprint(augmenteddf.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.utils import shuffle\naugmenteddf = shuffle(augmenteddf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"augmenteddf.to_csv(\"AugmentedTrain.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Please find the augmented data <a href=\"https://www.kaggle.com/krsna540/translated-data\"> here </a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def bert_encode(hypotheses, premises, tokenizer):\n    \n  num_examples = len(hypotheses)\n  \n  sentence1 = tf.ragged.constant([encode_sentence(s) for s in np.array(hypotheses)])\n  sentence2 = tf.ragged.constant([encode_sentence(s) for s in np.array(premises)])\n\n  cls = [tokenizer.convert_tokens_to_ids(['[CLS]'])]*sentence1.shape[0]\n  input_word_ids = tf.concat([cls, sentence1, sentence2], axis=-1)\n\n  input_mask = tf.ones_like(input_word_ids).to_tensor()\n\n  type_cls = tf.zeros_like(cls)\n  type_s1 = tf.zeros_like(sentence1)\n  type_s2 = tf.ones_like(sentence2)\n  input_type_ids = tf.concat(\n      [type_cls, type_s1, type_s2], axis=-1).to_tensor()\n\n  inputs = {\n      'input_word_ids': input_word_ids.to_tensor(),\n      'input_mask': input_mask,\n      'input_type_ids': input_type_ids}\n\n  return inputs\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def encode_sentence(s):\n   tokens = list(tokenizer.tokenize(s))\n   tokens.append('[SEP]')\n   return tokenizer.convert_tokens_to_ids(tokens)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.environ[\"WANDB_API_KEY\"] = \"0\" ## to silence warning","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import BertTokenizer, TFBertModel\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nmodel_name = 'bert-base-multilingual-cased'\ntokenizer = BertTokenizer.from_pretrained(model_name)\ntrain_input = bert_encode(augmenteddf.premise.values, augmenteddf.hypothesis.values, tokenizer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_len = 30\n\ndef build_model():\n    bert_encoder = TFBertModel.from_pretrained(model_name)\n    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    input_type_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_type_ids\")\n    \n    embedding = bert_encoder([input_word_ids, input_mask, input_type_ids])[0]\n    output = tf.keras.layers.Dense(3, activation='softmax')(embedding[:,0,:])\n    \n    model = tf.keras.Model(inputs=[input_word_ids, input_mask, input_type_ids], outputs=output)\n    model.compile(tf.keras.optimizers.Adam(lr=1e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with strategy.scope():\n    model = build_model()\n    model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping\nearly_stopping=EarlyStopping(monitor='val_accuracy',mode='max',patience=5,min_delta=0.01)\nmodel.fit(train_input, augmenteddf.label.values, epochs = 10, verbose = 1, batch_size = 64, validation_split = 0.3,callbacks=[early_stopping])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_input=bert_encode(Updatedtestdf.premise.values, Updatedtestdf.hypothesis.values, tokenizer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = [np.argmax(i) for i in model.predict(test_input)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = Updatedtestdf.id.copy().to_frame()\nsubmission['prediction'] = predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission.csv', header=True, index=False) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<center><H1> In Progress </h1></center>"},{"metadata":{},"cell_type":"markdown","source":"<h5>References:</h5>\n\n* https://arxiv.org/pdf/1901.11196.pdf\n* https://github.com/jasonwei20/eda_nlp\n* https://arxiv.org/abs/1706.03762\n* https://github.com/google-research/bert\n* https://openai.com/blog/better-language-models/\n* https://arxiv.org/pdf/1906.08237.pdf\n* https://blog.einstein.ai/introducing-a-conditional-transformer-language-model-for-controllable-generation/\n* https://developer.nvidia.com/blog/training-bert-with-gpus/\n* https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}