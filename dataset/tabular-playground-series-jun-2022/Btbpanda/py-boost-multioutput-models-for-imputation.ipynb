{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Py-Boost\n\nIn this kernel I want to introduce my boosting framework called **Py-Boost** and the example of its usage for imputation task. \n\nThe main idea of **Py-Boost** is to make a simple and fast boosting for the researchers that could be easily customized and convinient to implement your own ideas. Key feature of **Py-Boost** is that it is written on Python and, despite this fact, is fast enough (comparable by speed with state of art GPU implementations such as **XGBoost** and **CatBoost**) because it uses GPU computation frameworks such as **CuPy** and **Numba**.\n\nTo learn more, visit our [Github repo](https://github.com/sb-ai-lab/Py-Boost). Here you will find some more usage tutorials. If you like this tool, you also can star us :)\n\nAlso there is an example of training **Py-Boost** on a simple binary task in [this kernel](https://www.kaggle.com/code/btbpanda/fast-metric-and-py-boost-baseline)\n\nMy own research today is mainly focused on applying GBDTs to the **multioutput tasks (multiclass, multilabel, and multitask regression)**, so another important thing is that there are few features that could be helpful for this competition. To learn more you can check our [multioutput tutorial](https://github.com/sb-ai-lab/Py-Boost/blob/master/tutorials/Tutorial_2_Advanced_multioutput.ipynb).\n\n## Gradient Boosting for imputation idea\n\nThe imputation task is actually very similar to multitask regression, where Py-Boost can be very efficient. In both tasks the model should take an array of features and output an array of target predictions (not a single value). The only difference is that the input and the output for imputation are the same arrays. \n\nBut the obvious fact is that if we will fit the model like `model.fit(X, X)` we will definetly be overfitted. We need to modify somehow the classic boosting scheme to prevent that. And here is the solution:\n\n**At each boosting step let's split all X columns into the 2 parts - randomly decide which columns are the features and which are the targets**. So each particular tree will not use the column to predict itself. It is very similar to the common colsample strategy, but we also sample the targets simultaneously. \n\nAnd here I show you that Py-Boost is flexible enought to create such custom scheme easily :)\n\n## Installation and imports","metadata":{}},{"cell_type":"code","source":"%%capture\n!pip install py-boost","metadata":{"execution":{"iopub.status.busy":"2022-06-29T07:24:55.794964Z","iopub.execute_input":"2022-06-29T07:24:55.795588Z","iopub.status.idle":"2022-06-29T07:25:05.554598Z","shell.execute_reply.started":"2022-06-29T07:24:55.79555Z","shell.execute_reply":"2022-06-29T07:25:05.55335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom pandas import Series, DataFrame\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport cudf\nimport cupy as cp\nimport numpy as np\n\nimport joblib\n\nfrom collections import defaultdict\n\nfrom py_boost import GradientBoosting # basic GradientBoosting class\nfrom py_boost.gpu.losses import * # utils for the custom loss\nfrom py_boost.multioutput.sketching import * # utils for multioutput\nfrom py_boost.sampling.bagging import BaseSampler # utils for custom sampler\nfrom py_boost.callbacks.callback import Callback # other customization via callback\n\nfrom IPython.core.display import HTML","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-29T10:21:17.738548Z","iopub.execute_input":"2022-06-29T10:21:17.738979Z","iopub.status.idle":"2022-06-29T10:21:17.752301Z","shell.execute_reply.started":"2022-06-29T10:21:17.738944Z","shell.execute_reply":"2022-06-29T10:21:17.751238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data processing part","metadata":{}},{"cell_type":"code","source":"%%time\n\ndef get_dtypes(path):\n    \"\"\"\n    Get data types\n    \"\"\"\n    df = pd.read_csv(path, nrows=2)\n    dtypes = {x: np.float32 for x in df.columns}\n    dtypes['row_id'] = np.int64\n    \n    return dtypes\n\npath = '../input/tabular-playground-series-jun-2022/data.csv'\ndata = cudf.read_csv(path, index_col='row_id', dtype=get_dtypes(path))\n# save features names\ncols = data.columns.tolist()","metadata":{"execution":{"iopub.status.busy":"2022-06-29T08:20:48.721373Z","iopub.execute_input":"2022-06-29T08:20:48.721722Z","iopub.status.idle":"2022-06-29T08:20:49.059778Z","shell.execute_reply.started":"2022-06-29T08:20:48.721695Z","shell.execute_reply":"2022-06-29T08:20:49.0587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Similar to other public kernels we will deal with F_4 feature group only. Others will be just filled with the mean values","metadata":{}},{"cell_type":"code","source":"%%time\n# columns to impute with means\nfillna_cols = [x for x in data.columns if int(x.split('_')[1]) in [1, 2, 3]]\nfillna_index = [cols.index(x) for x in fillna_cols]\nmeans = data[fillna_cols].mean().to_pandas()\n\n# features to process\nfeatures_cols = [x for x in data.columns if int(x.split('_')[1]) in [4]]\nfeatures_index = [cols.index(x) for x in features_cols]\n\n# cut dataset\ndata = data[features_cols]","metadata":{"execution":{"iopub.status.busy":"2022-06-29T08:20:49.313712Z","iopub.execute_input":"2022-06-29T08:20:49.314383Z","iopub.status.idle":"2022-06-29T08:20:49.399685Z","shell.execute_reply.started":"2022-06-29T08:20:49.31434Z","shell.execute_reply":"2022-06-29T08:20:49.398423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we separate the columns, that can be NULL from columns that are not NULL. In our case all F_4 are nullable, so this cell just represents the general case usage - for example if you are going to train the model on both F_2 (not nullable group) and F_4\n\nNullable columnns will be used both as features and targets, not nullable - only as features","metadata":{}},{"cell_type":"code","source":"%%time\nnull_stats = data.isnull().sum()\ncols = data.columns.tolist()\n\n# columns that can not  be NULL (empty list in this particular case) - only used as features\nnot_null_cols = null_stats[null_stats == 0].index.to_pandas().tolist()\n\n# columns that cab be null (all inn our case) - used both as features and targets\nnull_cols = null_stats[null_stats > 0].index.to_pandas().tolist()\n\n# save numeric indices of targets\ntarget_index = [cols.index(x) for x in null_cols]\n\n# move to NumPy as clean GPU memory\nX = data.to_pandas().values\nindex = data.index.to_pandas()\n\ndel data","metadata":{"execution":{"iopub.status.busy":"2022-06-29T08:20:50.377883Z","iopub.execute_input":"2022-06-29T08:20:50.378591Z","iopub.status.idle":"2022-06-29T08:20:50.527839Z","shell.execute_reply.started":"2022-06-29T08:20:50.378553Z","shell.execute_reply":"2022-06-29T08:20:50.526756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Separate by NaN count\n\nIn previous kernels it was shown, that building separate models for different NaN count in row helps training. Despite the fact we will build single multioutput model for ALL features, we still will be seaparating the models by NaN count. As I checked locally, it is also helpful for my approach too. So finally, we will have 4 models for the final solution (4 and 5 NaNs group will be merged together). ","metadata":{}},{"cell_type":"code","source":"nan_cnt = np.isnan(X).sum(axis=1)\nSeries(nan_cnt).value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-06-29T08:20:51.55484Z","iopub.execute_input":"2022-06-29T08:20:51.555623Z","iopub.status.idle":"2022-06-29T08:20:51.596177Z","shell.execute_reply.started":"2022-06-29T08:20:51.55557Z","shell.execute_reply":"2022-06-29T08:20:51.595243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Split on train/test","metadata":{}},{"cell_type":"code","source":"%%time\nX_train = X[nan_cnt == 0]\n\ntest_sl = nan_cnt > 0\nX_test = X[test_sl]\n\ntest_index = index[test_sl]\ntest_nan = nan_cnt[test_sl]","metadata":{"execution":{"iopub.status.busy":"2022-06-29T08:22:22.111117Z","iopub.execute_input":"2022-06-29T08:22:22.111777Z","iopub.status.idle":"2022-06-29T08:22:22.174992Z","shell.execute_reply.started":"2022-06-29T08:22:22.111739Z","shell.execute_reply":"2022-06-29T08:22:22.174065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Customizing the Py-Boost\n\nHere I show how to customize Py-Boost to implement the idea of imputation boosting described at the begining. Custom classes should be implemented via modifying the followinng methods:\n\n- **.before_iteration**\n- **.before_iteration**\n- **.before_train**\n- **.after_train**\n\nAll methods takes **build_info** dict as Input. It contains the full training, validation data, and also all model parameters. Also you can save your own attributes to the build_info. The full structure of build info is presented in the hidden cell below. NOTE: All GPU data attributes are stored as CuPy arrays and CPU data as NumPy arrays.","metadata":{}},{"cell_type":"markdown","source":"To create callback we should inherit Callbak class. There are 4 methods, that could be redefined:\n        - before_train - outputs None\n        - before_iteration - outputs None\n        - after_train - outputs None\n        - after_iteration - outputs bool - if training should be stopped after iteration\n\n    Methods receive build_info - the state dict, that could be accessed and modifier\n\n    Basic build info structure:\n\n    build_info = {\n            'data': {\n                'train': {\n                    'features_cpu': np.ndarray - raw feature matrix,\n                    'features_gpu': cp.ndarray - uint8 quantized feature matrix on GPU,\n                    'target': y - cp.ndarray - processed target variable on GPU,\n                    'sample_weight': cp.ndarray - processed sample_weight on GPU or None,\n                    'ensemble': cp.ndarray - current model prediction (with no postprocessing,\n                        ex. before sigmoid for logloss) on GPU,\n                    'grad': cp.ndarray of gradients on GPU, before first iteration - None,\n                    'hess': cp.ndarray of hessians on GPU, before first iteration - None,\n\n                    'last_tree': {\n                        'leaves': cp.ndarray - nodes indices of the last trained tree,\n                        'preds': cp.ndarray - predictions of the last trained tree,\n                    }\n\n                },\n                'valid': {\n                    'features_cpu' the same as train, but list, each element corresponds each validation sample,\n                    'features_gpu': ...,\n                    'target': ...,\n                    'sample_weight': ...,\n                    'ensemble': ...,\n\n                    'last_tree': {\n                        'leaves': ...,\n                        'preds': ...,\n                    }\n\n                }\n            },\n            'borders': list of np.ndarray - list or quantization borders,\n            'model': GradientBoosting - model, that is trained,\n            'mempool': cp.cuda.MemoryPool - memory pool used for train, could be used to clean memory to prevent OOM,\n            'builder': DepthwiseTreeBuilder - the instance of tree builder, contains training params,\n\n            'num_iter': int, current number of iteration,\n            'iter_scores': list of float - list of metric values for all validation sets for the last iteration,\n        }\n\n","metadata":{"_kg_hide-input":true}},{"cell_type":"markdown","source":"The First thing we want to do is to customize the **Sampler** class. **Sampler** in **Py-Boost** defines the strategy of both columns/rows sampling. Here it will be used as the custom columns sampler. Remember the idea - on **each boosting step we will select the random portion of columns that will be used as Features. Other will be used as Target**. Small modification - let's select columns with larger gradient norm more frequent, because they need more to be updated","metadata":{}},{"cell_type":"code","source":"class ImputeSampler(BaseSampler):\n    \"\"\"\n    Class Sampler will sample nullable columns to be target or feature\n    at the each boosting step\n    \"\"\"\n    def __init__(self, sample=0.8, target_cols=None):\n        \"\"\"\n        \n        Args:\n            sample: float, rate of columns that will be sampled as features\n            target_cols: list of int, indices of columns that could be both features and targets\n            \n        Returns:\n\n        \"\"\"\n        assert sample < 1, 'Sample should be lower than 1'\n        \n        self.target_cols = list(target_cols)\n        super().__init__(sample, axis=1)\n        self._temp = None\n        \n    def before_train(self, build_info):\n        \"\"\"Here we create indexers for the nullable columns, that could be a target\n\n        Args:\n            build_info: dict\n\n        Returns:\n\n        \"\"\"\n        self.length = build_info['data']['train']['features_gpu'].shape[self.axis]\n        indexer = np.arange(self.length)\n        self.nullable = np.asarray(self.target_cols, dtype=np.uint64)\n        self.nout = max(1, int(self.nullable.shape[0] * self.sample))\n        \n    def before_iteration(self, build_info):\n        \"\"\"Shuffle indexers and save to build info to pass it into the other classes\n\n        Args:\n            build_info: dict\n\n        Returns:\n\n        \"\"\"\n        grad = build_info['data']['train']['grad'][:, self.target_cols]\n        # use gradient norm as importance measure\n        grad_norm = (grad ** 2).sum(axis=0) ** .5\n        # norm to get probabilities. Add smooth constant\n        p = grad_norm / grad_norm.sum()\n        # sample target columnns\n        target = np.sort(np.random.choice(self.nullable, size=self.nout, replace=False, p=p.get()))\n        # others are features columns\n        features = np.setdiff1d(np.arange(self.length, dtype=np.uint64), target)\n        \n        build_info['use_as_target'] = cp.asarray(target)\n        self._temp = cp.asarray(features)\n        build_info['use_as_features'] = self._temp\n        \n    def __call__(self):\n        \"\"\"Return the indices of features cols\n\n        Returns:\n\n        \"\"\"\n\n        return self._temp\n    \n    def after_train(self, build_info):\n        \n        del self._temp, self.indexer, self.valid_sl","metadata":{"execution":{"iopub.status.busy":"2022-06-29T10:08:43.667751Z","iopub.execute_input":"2022-06-29T10:08:43.668117Z","iopub.status.idle":"2022-06-29T10:08:43.681265Z","shell.execute_reply.started":"2022-06-29T10:08:43.668087Z","shell.execute_reply":"2022-06-29T10:08:43.68035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The other important thing is to modify the **loss** and the **metric** functions (NOTE: some target values could be NaNs). Here we use **MSE** as loss and **RMSE** as metric with the small modification - they will be able to accept NaN values in the target input. The idea is following - loss function will output zeros in the gradinet and hessian values on the places where it was NaN value in target (if we do not have information we should not update solution). Metric will just ignores the NaN in target while computing the mean error\n\nThe one other thing to implement here is **LossImputer** class. It will be implemented via **Callback**. It will help loss to ignore not only NaNs in loss, but also loss by columns that are selected to be a feature","metadata":{}},{"cell_type":"code","source":"class MSEWithNanLoss(MSELoss):\n    \"\"\"\n    This is custom MSE Loss that accepts NaN values and ignores features\n    \"\"\"\n    def __init__(self, ):\n        \n        self.feats_cols = None\n    \n    def get_grad_hess(self, y_true, y_pred):\n        \"\"\"\n        \n        Args:\n            y_true: cp.ndarray of target values\n            y_pred: cp.ndarray of predicted values\n            \n        Returns:\n\n        \"\"\"\n        mask = ~cp.isnan(y_true)\n        # apply features mask\n        grad = y_pred - cp.where(mask, y_true, 0)\n        hess = mask.astype(cp.float32)\n        grad *= hess\n        # we will ignore not only NaNs but also columns that are used as features !!!\n        if self.feats_cols is not None:\n            hess[:, self.feats_cols] = 0\n            grad *= hess\n        \n        return grad, hess\n\n    def base_score(self, y_true):\n        \"\"\"This method defines how to initialize the ensemble\n        \n        Args:\n            y_true: cp.ndarray of target values\n            \n        Returns:\n\n        \"\"\"\n        return cp.nanmean(y_true, axis=0)\n    \n    \nclass LossImputer(Callback):\n    \"\"\"\n    This is Callback. It modifies the Loss to ignore features columns\n    \"\"\"\n    def before_iteration(self, build_info):\n        \"\"\"\n        \n        Args:\n            build_info: dict\n            \n        Returns:\n        \n        \"\"\"\n        build_info['data']['train']['hess'][:, build_info['use_as_features']] = 0\n        build_info['data']['train']['grad'] *= build_info['data']['train']['hess']\n        \n        build_info['model'].loss.feats_cols = build_info['use_as_features']\n    \n    def after_iteration(self, build_info):\n        \"\"\"\n        \n        Args:\n            build_info: dict\n            \n        Returns:\n        \n        \"\"\"\n        build_info['model'].loss.feats_cols = build_info['use_as_features'] = None\n    \n\n    \nclass RMSEWithNaNMetric(RMSEMetric):\n    \"\"\"\n    This is custom MSE Loss that accepts NaN values and ignores features\n    \"\"\"\n    def __init__(self, target_cols):\n        \"\"\"\n        \n        Args:\n            target_cols: list of int, indices of columns that could be both features and targets\n            \n        Returns:\n\n        \"\"\"\n        self.target_cols = target_cols\n\n    \n    def __call__(self, y_true, y_pred, sample_weight=None):\n        \"\"\"\n        \n        Args:\n            y_true: cp.ndarray of target values\n            y_pred: cp.ndarray of predicted values\n            sample_weight: cp.nndarray of sample weights or None\n            \n        Returns:\n\n        \"\"\"\n        y_true = y_true[:, self.target_cols]\n        y_pred = y_pred[:, self.target_cols]         \n        \n        mask = ~cp.isnan(y_true)\n        \n        err = (cp.where(mask, y_true, 0) - y_pred) ** 2\n        return err[mask].mean() ** .5","metadata":{"execution":{"iopub.status.busy":"2022-06-29T08:57:52.983255Z","iopub.execute_input":"2022-06-29T08:57:52.983614Z","iopub.status.idle":"2022-06-29T08:57:52.996986Z","shell.execute_reply.started":"2022-06-29T08:57:52.983584Z","shell.execute_reply":"2022-06-29T08:57:52.996064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The last thing to modify is the **sketching strategy**. Here I will not depthly explain how it works, but simply it reduces the output dimensions on the tree structure search step. To learn how it speed up the training you can see [multioutput tutorial](https://github.com/sb-ai-lab/Py-Boost/blob/master/tutorials/Tutorial_2_Advanced_multioutput.ipynb) on the Github. The only thing we need to modify in the sketch - just to tell it, which columns are targets on the current iteration","metadata":{}},{"cell_type":"code","source":"class ImputeSketch(RandomProjectionSketch):\n    \"\"\"\n    Multioutput sketch is actually the main feature of Py-Boost\n    that helps it being efficient on multioutput tasks.\n\n    \"\"\"\n    def __init__(self, *args, **kwargs):\n        \"\"\"\n        \n        Args:\n            *args: args of sketch\n            **kwargs: kwargs of sketch\n            \n        Returns:\n        \n        \"\"\"\n        self.target_cols = None  \n        super().__init__(*args, **kwargs)\n        \n    def before_iteration(self, build_info):\n        \"\"\"Before each iteration it saves target indices\n        \n        Args:\n            build_info: dict\n        \n        Returns:\n        \n        \"\"\"\n        self.target_cols = build_info['use_as_target']\n        \n    def __call__(self, grad, hess):\n        \"\"\"Call method just select the target columns and pass it to the original sketch\n        \n        Args:\n            grad: cp.ndarray, gradient\n            hess: cp.ndarray, hessian\n            \n        Returns:\n        \n        \"\"\"\n        # select target\n        grad = grad[:, self.target_cols]\n        \n        # empty hess\n        hess = cp.ones((grad.shape[0], 1), dtype=cp.float32)\n        \n        return super().__call__(grad, hess)","metadata":{"execution":{"iopub.status.busy":"2022-06-29T09:00:41.894344Z","iopub.execute_input":"2022-06-29T09:00:41.894706Z","iopub.status.idle":"2022-06-29T09:00:41.904291Z","shell.execute_reply.started":"2022-06-29T09:00:41.894674Z","shell.execute_reply":"2022-06-29T09:00:41.902377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training Py-Boost\n\nHere we will build it all together and train our models. It takes quite a long time (few hours).","metadata":{}},{"cell_type":"code","source":"%%time\ntest_pred = np.zeros_like(X_test)\n\nN_MOD = 4\n\nfor nnull in range(N_MOD):\n    \n    X_nan = X_train.copy()\n    # add random NaNs into the train\n    sl = np.random.rand(*X_nan.shape).argsort(axis=1) < nnull\n    X_nan[sl] = np.nan\n    \n    # Py-Boost customization components\n    loss = MSEWithNanLoss()\n    loss_imp = LossImputer()\n    metric = RMSEWithNaNMetric(target_index)\n    sketch = ImputeSketch(1)\n    sampler = ImputeSampler(0.5, target_index)\n    \n    # Boosting\n    model = GradientBoosting(\n            loss, metric,\n            ntrees=50000,\n            lr=0.05,\n            max_depth=6,\n            min_data_in_leaf=10,\n            lambda_l2=1,\n            gd_steps=2,\n            quantization='Quantile', \n            colsample=sampler,\n            subsample=1, \n            use_hess=True,\n            multioutput_sketch=sketch,\n            callbacks=[loss_imp, ],\n            verbose=10000, \n        )\n    \n    # fit\n    # NOTE: the same data is used as features and targets\n    model.fit(X_nan, X_nan)\n    \n    # predict on current NaN slice\n    if (nnull + 1) < N_MOD:\n        sl = test_nan == (nnull + 1)\n    else:\n        sl = test_nan >= (nnull + 1)\n\n    test_pred[sl] += model.predict(X_test[sl], batch_size=1000000)","metadata":{"execution":{"iopub.status.busy":"2022-06-29T10:34:42.916342Z","iopub.execute_input":"2022-06-29T10:34:42.916688Z","iopub.status.idle":"2022-06-29T10:36:18.384482Z","shell.execute_reply.started":"2022-06-29T10:34:42.916658Z","shell.execute_reply":"2022-06-29T10:36:18.382741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create submission","metadata":{}},{"cell_type":"code","source":"%%time\nsub = pd.read_csv('../input/tabular-playground-series-jun-2022/sample_submission.csv')\n\nf123 = sub['row-col'] \\\n    .map(lambda x: x.split('-')[1]) \\\n    .map(means).values\n\nnan_sl = np.isnan(X_test)\nrows = np.tile(test_index.values, (X_test.shape[1], 1)).T[nan_sl]\ncols = np.tile(features_cols, (X_test.shape[0], 1))[nan_sl]\nres = Series(test_pred[nan_sl], index=Series(rows).astype(str) + '-' + Series(cols))\n\nf4 = sub['row-col'].map(res).values\n\nsub['value'] = np.where(np.isnan(f123), f4, f123)\n\nsub.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-06-29T10:37:40.797582Z","iopub.execute_input":"2022-06-29T10:37:40.797932Z","iopub.status.idle":"2022-06-29T10:37:46.184574Z","shell.execute_reply.started":"2022-06-29T10:37:40.797903Z","shell.execute_reply":"2022-06-29T10:37:46.183532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Despite the fact, in **TPS** competitions neural network approaches are more likely to be a better tool, this solution seems to be the best of boosing family from public kernels and can be competitive on the real world datasets.\n\nHope you like my approach and the **Py-Boost** framework:) And don't forget to upvote this kernel and star us on the GitHub :) Good luck!","metadata":{}},{"cell_type":"code","source":"s = '<iframe src=\"https://ghbtns.com/github-btn.html?user=sb-ai-lab&repo=Py-Boost&type=star&count=true&size=large\" frameborder=\"0\" scrolling=\"0\" width=\"170\" height=\"30\" title=\"Py-Boost GitHub\"></iframe>'\nHTML(s)","metadata":{"execution":{"iopub.status.busy":"2022-06-29T10:22:15.813281Z","iopub.execute_input":"2022-06-29T10:22:15.813619Z","iopub.status.idle":"2022-06-29T10:22:15.819927Z","shell.execute_reply.started":"2022-06-29T10:22:15.813591Z","shell.execute_reply":"2022-06-29T10:22:15.818855Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]}]}