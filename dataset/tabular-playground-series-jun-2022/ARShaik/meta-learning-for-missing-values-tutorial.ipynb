{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <div style=\"padding:20px;color:white;margin:0;font-size:200%;text-align:center;display:fill;border-radius:5px;background-color:#AF601A;overflow:hidden;font-weight:500\">TPS June 2022\n</div>\n\n\n","metadata":{}},{"cell_type":"markdown","source":" ### If you are a beginner, see my other notebook for imputation tutorial [notebook](https://www.kaggle.com/code/abdulravoofshaik/quick-eda-and-missing-values-tutorial). This work would not have been possible without the great insights from the following notebooks. Please consider upvoting the original work also\n ### The following cartoon depicts the overall framework for applying ensemble technique for imputation. I have used two neural network models (one with high Epoch, one with low Epoch), two LGBM models, one catboost model to train on the training dataset. Later I had applied Linear regression model for meta learning purpose. \n \n ###  EDA is not the focus of this notebook. Please see this excellent [notebook](https://www.kaggle.com/code/masatomurakawamm/tps-jun22-application-of-masked-language-model) by Masato Murakawa if you are intersted in EDA.\n\n <div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:verdana;\">\n    ðŸ“ŒImportant note: I have applied meta learning on the training dataset, as I wanted to find the best meta model which fits our dataset.\n</div>\n \n","metadata":{}},{"cell_type":"markdown","source":"<img src=\"https://cdn-images-1.medium.com/max/946/1*TcGmOuMTARvshOY5bDN81Q.png\">\n\nsource: https://towardsai.net/p/l/machine-learning-model-stacking-in-python","metadata":{}},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#E59866;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:black;\"><b>1.0 | Load data and Preprocessing</b></p>\n</div>","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom tqdm import tqdm\nimport missingno as msno\npd.set_option('display.max_columns', None)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\nimport xgboost as xgb\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import cross_val_score, KFold\nfrom sklearn.metrics import mean_squared_error\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\nfrom lightgbm import LGBMRegressor\nimport matplotlib.pyplot as plt ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-26T13:51:32.417494Z","iopub.execute_input":"2022-06-26T13:51:32.418037Z","iopub.status.idle":"2022-06-26T13:51:33.621234Z","shell.execute_reply.started":"2022-06-26T13:51:32.417925Z","shell.execute_reply":"2022-06-26T13:51:33.620044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv(\"../input/tabular-playground-series-jun-2022/data.csv\")\nTarget = pd.read_csv(\"../input/tabular-playground-series-jun-2022/sample_submission.csv\", index_col='row-col')","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-26T13:51:33.622567Z","iopub.execute_input":"2022-06-26T13:51:33.622944Z","iopub.status.idle":"2022-06-26T13:51:47.206277Z","shell.execute_reply.started":"2022-06-26T13:51:33.622912Z","shell.execute_reply":"2022-06-26T13:51:47.205416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Below two csv files contains predict of the trained neural network on the whole dataset. For full details on the training can be found in this public [notebook](https://www.kaggle.com/code/alexryzhkov/lightautoml-nn-imputer-tps-june-22).\n","metadata":{}},{"cell_type":"code","source":"train_pred_lowcnn= pd.read_csv(\"../input/metalearn-june2022/low_epoch_autoML_Pred_train_Test.csv\") # low Epoch\ntrain_pred_highcnn= pd.read_csv(\"../input/metalearn-june2022/highEpoch_autoML_Pred_train_Test.csv\") # high Epoch","metadata":{"execution":{"iopub.status.busy":"2022-06-26T13:51:47.207359Z","iopub.execute_input":"2022-06-26T13:51:47.207918Z","iopub.status.idle":"2022-06-26T13:51:51.977156Z","shell.execute_reply.started":"2022-06-26T13:51:47.207881Z","shell.execute_reply":"2022-06-26T13:51:51.975928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Below, four csv files contain predictions of the trained model on the whole dataset. I have used XGB, Catboost and LGBM techniques for this purpose. You can get an intuition about the training procedure in my other public [notebook](https://www.kaggle.com/code/abdulravoofshaik/may-the-best-man-model-learn-ensemble4imputation).","metadata":{}},{"cell_type":"code","source":"train_pred_xgb= pd.read_csv(\"../input/metalearn-june2022/Merged_train_Subsets_xgb.csv\")\ntrain_pred_cat= pd.read_csv(\"../input/metalearn-june2022/Merged_train_Subsets_catboost.csv\")\ntrain_pred_lowlgbm= pd.read_csv(\"../input/metalearn-june2022/Merged_train_Subsets_low_lgbm.csv\")\ntrain_pred_highlgbm= pd.read_csv(\"../input/metalearn-june2022/Merged_train_Subsets_20kLGBM.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-06-26T13:51:51.979573Z","iopub.execute_input":"2022-06-26T13:51:51.980007Z","iopub.status.idle":"2022-06-26T13:53:13.664411Z","shell.execute_reply.started":"2022-06-26T13:51:51.979971Z","shell.execute_reply":"2022-06-26T13:53:13.663033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lets take a look at the data that we have imported\ndisplay(train_pred_xgb.head(), train_pred_cat.head(), train_pred_highlgbm.head(), train_pred_lowlgbm.head(),data.head())","metadata":{"execution":{"iopub.status.busy":"2022-06-26T13:53:13.66622Z","iopub.execute_input":"2022-06-26T13:53:13.667185Z","iopub.status.idle":"2022-06-26T13:53:13.910348Z","shell.execute_reply.started":"2022-06-26T13:53:13.66714Z","shell.execute_reply":"2022-06-26T13:53:13.909544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Optmize memory\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose:\n        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df\n\ndata = reduce_mem_usage(data)\nTarget = reduce_mem_usage(Target)\ntrain_pred_xgb = reduce_mem_usage(train_pred_xgb)\ntrain_pred_cat = reduce_mem_usage(train_pred_cat)\ntrain_pred_highlgbm = reduce_mem_usage(train_pred_highlgbm)\ntrain_pred_lowlgbm = reduce_mem_usage(train_pred_lowlgbm)\ntrain_pred_lowcnn = reduce_mem_usage(train_pred_lowcnn)\ntrain_pred_highcnn = reduce_mem_usage(train_pred_highcnn)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-26T13:53:13.911856Z","iopub.execute_input":"2022-06-26T13:53:13.912446Z","iopub.status.idle":"2022-06-26T13:53:54.105281Z","shell.execute_reply.started":"2022-06-26T13:53:13.912409Z","shell.execute_reply":"2022-06-26T13:53:54.104132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#E59866;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:black;\"><b>2.0 | Preprocessing</b></p>\n</div>\nLets divide the data into four subsets.","metadata":{}},{"cell_type":"code","source":"# this code snippet is taken from https://www.kaggle.com/code/martynovandrey/tps-jun-22-splitted-dataset-24x-faster. Consider upvoting the original author also\nfeatures = list(data.columns)\nfeatures_1, features_2, features_3, features_4 = [], [], [], []\nF = [[], [], [], [], []]\nfor feature in features:\n    for i in [1, 2, 3, 4]:\n        if feature.split('_')[1] == str(i):\n            F[i].append(feature)\ndf = [[], [], [], [], []]\n\nfor i in [1, 2, 3, 4]:\n    df[i] = data[F[i]].copy()","metadata":{"execution":{"iopub.status.busy":"2022-06-26T13:53:54.106686Z","iopub.execute_input":"2022-06-26T13:53:54.107065Z","iopub.status.idle":"2022-06-26T13:53:54.65599Z","shell.execute_reply.started":"2022-06-26T13:53:54.107029Z","shell.execute_reply":"2022-06-26T13:53:54.65478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#removing f1,f2,f3 from the dataframes, as we will focus on columns starting with F_4_. \nF_4_list = [col for col in data.columns if col.startswith('F_4_')]\n# adding xgb data\ntrain_pred_xgb=train_pred_xgb[F_4_list]\ntrain_pred_cat=train_pred_cat[F_4_list]\ntrain_pred_highlgbm=train_pred_highlgbm[F_4_list]\ntrain_pred_lowlgbm=train_pred_lowlgbm[F_4_list]\ntrain_pred_highcnn=train_pred_highcnn[F_4_list]\ntrain_pred_lowcnn=train_pred_lowcnn[F_4_list]\n","metadata":{"execution":{"iopub.status.busy":"2022-06-26T13:53:54.657726Z","iopub.execute_input":"2022-06-26T13:53:54.658588Z","iopub.status.idle":"2022-06-26T13:53:55.874633Z","shell.execute_reply.started":"2022-06-26T13:53:54.658536Z","shell.execute_reply":"2022-06-26T13:53:55.873419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lets add a suffix \ntrain_pred_xgb = train_pred_xgb.add_suffix('_xgb')\ntrain_pred_cat = train_pred_cat.add_suffix('_cat')\ntrain_pred_highlgbm = train_pred_highlgbm.add_suffix('_highlgbm')\ntrain_pred_lowlgbm = train_pred_lowlgbm.add_suffix('_lowlgbm')\ntrain_pred_highcnn = train_pred_highcnn.add_suffix('_highcnn')\ntrain_pred_lowcnn = train_pred_lowcnn.add_suffix('_lowcnn')","metadata":{"execution":{"iopub.status.busy":"2022-06-26T13:53:55.876035Z","iopub.execute_input":"2022-06-26T13:53:55.876487Z","iopub.status.idle":"2022-06-26T13:53:55.945255Z","shell.execute_reply.started":"2022-06-26T13:53:55.876447Z","shell.execute_reply":"2022-06-26T13:53:55.944167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#merging all the F4 subsets into one dataframe\nMerged_F4Subsets = pd.concat([train_pred_xgb, train_pred_cat, train_pred_highlgbm,train_pred_lowlgbm,train_pred_highcnn,train_pred_lowcnn, df[4]], axis=1)\nMerged_F4Subsets.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-26T13:53:55.946957Z","iopub.execute_input":"2022-06-26T13:53:55.947676Z","iopub.status.idle":"2022-06-26T13:53:56.215243Z","shell.execute_reply.started":"2022-06-26T13:53:55.947635Z","shell.execute_reply":"2022-06-26T13:53:56.214067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pd.set_option('display.max_rows', 25)\nprint(Merged_F4Subsets.isnull().sum())","metadata":{"execution":{"iopub.status.busy":"2022-06-26T13:53:56.216853Z","iopub.execute_input":"2022-06-26T13:53:56.21723Z","iopub.status.idle":"2022-06-26T13:53:56.416853Z","shell.execute_reply.started":"2022-06-26T13:53:56.2172Z","shell.execute_reply":"2022-06-26T13:53:56.415495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#E59866;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:black;\"><b>3.0 | Ensembling</b></p>\n</div>","metadata":{}},{"cell_type":"code","source":"dummy_df2=pd.DataFrame(0, index=np.arange(len(df[4])), columns=df[4].columns) # create zero filled dataframe\ndummy_df2=df[4].copy()\nfor column in df[4].columns: \n    print('Processing Colunm Name : ', column)\n    dummy_df=pd.DataFrame()\n    \n    if df[4][column].isnull().sum() == 0:\n        print(df[4][column].isnull().sum())\n        continue    # continue as no NaN values found in this column\n    #collect all the columns start with selected column\n    col_list = [col for col in Merged_F4Subsets.columns if col.startswith(column+str('_'))]\n    print(col_list)\n    dummy_df= pd.concat([Merged_F4Subsets[col_list],Merged_F4Subsets[column]],axis=1)\n    col_nan_ix = dummy_df[dummy_df[column].isnull()].index  # identify the rows which has NaN in column F_1_0\n    train_df = dummy_df.drop(col_nan_ix, axis = 0)  # this is the df which has no NaN\n    test_df = dummy_df[dummy_df.index.isin(col_nan_ix)]\n    print('R2 score for high CNN',r2_score(train_df[column+str('_highcnn')].values, train_df[column].values))\n    print('R2 score for low CNN',r2_score(train_df[column+str('_lowcnn')].values, train_df[column].values))\n    print('R2 score for highlgbm',r2_score(train_df[column+str('_highlgbm')].values, train_df[column].values))\n    print('R2 score for lowlgbm',r2_score(train_df[column+str('_lowlgbm')].values, train_df[column].values))\n    print('R2 score for xgb',r2_score(train_df[column+str('_xgb')].values, train_df[column].values))\n    print('R2 score for cat',r2_score(train_df[column+str('_cat')].values, train_df[column].values))\n    X= train_df[col_list].values\n    y= train_df[column].values\n\n    # Use KFold CV to avoid overfitting\n    test_preds = None\n    rmse_list = []\n    fold_num = 1\n    for train_index, val_index in KFold(n_splits=5, shuffle=True).split(X, y):\n        X_train, y_train = X[train_index], y[train_index]\n        X_val, y_val = X[val_index], y[val_index]     \n        model = LinearRegression()\n        model.fit(X_train,y_train)\n        val_predictions = model.predict(X_val)\n        r2_val = r2_score(y_val, val_predictions)\n        print('Fold {fold_num} R2: {rmse:.4f}'.format(fold_num=fold_num, rmse=r2_val))\n        rmse_list.append(r2_val)\n        fold_num += 1\n        # Use each fold's model to predict test values and add them to test_preds\n        if test_preds is None:\n            test_preds = model.predict(test_df.drop([column],axis=1))\n        else:\n            test_preds += model.predict(test_df.drop([column],axis=1))\n\n    # Get average of predictions from KFold CV for submission\n    test_preds /= fold_num\n    dummy_df2[column][col_nan_ix] = model.predict(test_df.drop([column],axis=1))\n    print('Average KFold rmse: {avg_rmse:.4f}'.format(avg_rmse = np.mean(np.array(rmse_list))))","metadata":{"execution":{"iopub.status.busy":"2022-06-26T13:53:56.419018Z","iopub.execute_input":"2022-06-26T13:53:56.419624Z","iopub.status.idle":"2022-06-26T13:54:17.169598Z","shell.execute_reply.started":"2022-06-26T13:53:56.419569Z","shell.execute_reply":"2022-06-26T13:54:17.165996Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### lets take a close the results. For example, for the column 'F_4_0'\n### R2 score when CNN based model with high Epochs used is 0.9799020843221459\n### R2 score when CNN based model with low Epochs used is  0.9362032002345319\n### R2 score when LGBM based model with high number of n_estimators used is 0.8955968491804145\n### R2 score when LGBM based model with low number of n_estimators used is 0.2907488927788481\n### R2 score when XGB based model used is 0.5172610850741659\n### R2 score when Catboost based model is 0.49500719294973083\n### now the best part, the linear regressino model was able to give R2 score of 0.9899 for all the folds. We can see the same trend for all the other columns in this subset.","metadata":{}},{"cell_type":"code","source":"#Now we use simple meanimputer for subset-1 and subset-3. Note that subset-2 has no missing values\nfrom sklearn.impute import SimpleImputer\nimp = SimpleImputer(\n        missing_values=np.nan,\n        strategy='mean') \nfor i in [1,3]:    \n    df[i][:] = imp.fit_transform(df[i])","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-26T13:54:17.173557Z","iopub.execute_input":"2022-06-26T13:54:17.174148Z","iopub.status.idle":"2022-06-26T13:54:18.092621Z","shell.execute_reply.started":"2022-06-26T13:54:17.174096Z","shell.execute_reply":"2022-06-26T13:54:18.091432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Merged_Subsets = pd.concat([df[1], df[2], df[3], dummy_df2], axis=1)\nsubmission = pd.read_csv('../input/tabular-playground-series-jun-2022/sample_submission.csv', index_col='row-col')\nfor i in tqdm(submission.index):\n    row = int(i.split('-')[0])\n    col = i.split('-')[1]\n    submission.loc[i, 'value'] = Merged_Subsets.loc[row, col]\n\nsubmission.to_csv('meta_lr_submission2.csv')","metadata":{"execution":{"iopub.status.busy":"2022-06-26T13:54:18.094177Z","iopub.execute_input":"2022-06-26T13:54:18.094677Z","iopub.status.idle":"2022-06-26T13:55:52.417383Z","shell.execute_reply.started":"2022-06-26T13:54:18.094634Z","shell.execute_reply":"2022-06-26T13:55:52.415423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:verdana;\">\n    ðŸ“Œ Now your turn. I have used Linear regression for meta learning, but you can use some other techniques such as XGB, Catboost. Make sure you do not overfit.\n</div>","metadata":{}},{"cell_type":"markdown","source":"### The Lb score is 0.87 which is lower. This could be two things:\n1) The model is overfitted <br>\n2) The public LB score is misleading. <br>\n\nI belive the number two is true. Because, we have applied Kfold and all the folds are shown very good fit with training data. We have seen similar LB in previous competitions. \nWay forward, fine tune Neural networks model\n\n\n## Work in progress\n<img src=\"https://pbs.twimg.com/media/FPBbO_zWQAokwwc?format=jpg&name=small\">\n","metadata":{}},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#E59866;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:black;\"><b>3.0 | References</b></p>\n</div>","metadata":{}},{"cell_type":"markdown","source":"https://towardsdatascience.com/using-the-missingno-python-library-to-identify-and-visualise-missing-data-prior-to-machine-learning-34c8c5b5f009 <br>\nhttps://towardsdatascience.com/how-to-handle-missing-data-8646b18db0d4 <br>\nhttps://www.kaggle.com/code/parulpandey/a-guide-to-handling-missing-values-in-python/notebook <br>\nhttps://www.kaggle.com/code/residentmario/using-missingno-to-diagnose-data-sparsity/notebook <br>\nhttps://www.analyticsvidhya.com/blog/2021/05/dealing-with-missing-values-in-python-a-complete-guide/ <br>\nhttps://www.kaggle.com/code/calebreigada/getting-started-eda-preprocessing <br>\nhttps://medium.com/swlh/impute-missing-values-the-right-way-c63735fccccd <br>\n\n","metadata":{}}]}