{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <div style=\"padding:20px;color:white;margin:0;font-size:200%;text-align:center;display:fill;border-radius:5px;background-color:#AF7AC5;overflow:hidden;font-weight:500\">TPS JUNE 2022\n</div>","metadata":{}},{"cell_type":"markdown","source":" ### Most of the public notebooks in this competition has assumed that single type of algorithum (either XGB, Pytorch or LGBM) is applicable for all the columns. I would like to propose a different way of handling where each column can is treated individually and based on the training metric best algorithum is choosen for a given column.\n ### If you are a beginner, see my other notebook for imputation tutorial [notebook](https://www.kaggle.com/code/abdulravoofshaik/quick-eda-and-missing-values-tutorial). \n ### This notebook assumes that you are familer with EDA and know how to use single algorithum. If you are not, please see these notebooks [one](https://www.kaggle.com/code/dwin183287/tps-june-2022-eda) and [two](https://www.kaggle.com/code/abdulravoofshaik/top-3-solution-lgbm-mean).\n \n ### PS: This notebook is computationally expensive and takes longtime to run.  \n\n","metadata":{}},{"cell_type":"markdown","source":"\n\n","metadata":{}},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#D7BDE2;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:black;\"><b>1.0 | Load data</b></p>\n</div>","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom tqdm import tqdm\nimport missingno as msno\npd.set_option('display.max_columns', None)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.impute import SimpleImputer\nimport matplotlib.pyplot as plt\nimport matplotlib.colors\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom plotly.offline import init_notebook_mode\nimport seaborn as sns\ndata = pd.read_csv(\"../input/tabular-playground-series-jun-2022/data.csv\")\nTarget = pd.read_csv(\"../input/tabular-playground-series-jun-2022/sample_submission.csv\", index_col='row-col')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#D7BDE2;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:black;\"><b>2.0 | Preprocessing</b></p>\n</div>","metadata":{}},{"cell_type":"markdown","source":"### Lets start with column F_1_0 and try to replace the NaN values. First we need to find out where are the missing values located in this column. As shown below figure, we need to split our data into two sets. \n### Training set: It consists of known values for F_1_0 column, which means all the rows with non-NaN value in F_1_0 column. \n### Test set: It consists of Unknown values for F_1_0 column, which means all the rows with  NaN value in F_1_0 column.\n### Here we apply three different algorithums (LGBM, XGB and Catboost) to train on each column. Based on the error analysis we choose the best model to predict NaN values.\n### It has been well established in the previous notebook xxx that there are no missing values for columns starting with F_2. Columns starting with F_4 are correlated with each other. \n","metadata":{}},{"cell_type":"code","source":"## we apply the same concept to individual column and develop 80 individual models. As we have noticed earlier, dataset has four different subsets. Lets divide them.\n# this code snippet is taken from https://www.kaggle.com/code/martynovandrey/tps-jun-22-splitted-dataset-24x-faster. Consider upvoting the original author also\nfeatures = list(data.columns)\nfeatures_1, features_2, features_3, features_4 = [], [], [], []\nF = [[], [], [], [], []]\nfor feature in features:\n    for i in [1, 2, 3, 4]:\n        if feature.split('_')[1] == str(i):\n            F[i].append(feature)\ndf = [[], [], [], [], []]\n\nfor i in [1, 2, 3, 4]:\n    df[i] = data[F[i]]","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#D7BDE2;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:black;\"><b>3.0 | Modeling</b></p>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:verdana;\">\n    ðŸ“Œ Important note:  The strategy is to use LGBM regression, XGB regression and Catboost Regression and choose the best model for each column for F4. For F1 and F3 I will use mean imputer.\n</div>","metadata":{}},{"cell_type":"code","source":"%%time\n# Ensemble regression\nfrom sklearn.model_selection import KFold, StratifiedKFold, cross_val_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nimport xgboost as xgb\nfrom catboost import CatBoostRegressor\nimport statistics\nfrom sklearn.metrics import r2_score\nEnsemble_df = pd.DataFrame()\nxgb.set_config(verbosity=0)\n\nkfold = KFold(n_splits = 3, shuffle=True, random_state = 0) # you can increase the n_splits value to 10, to minimize the runtime I have used 3\n\nlgbm_params = {'random_state': 22,\n#           'device' : 'gpu',   #uncomment it if you want to use GPU\n          'n_estimators': 4000, # you can increase the n_splits value to >10000, to minimize the runtime I have used 4000\n          'learning_rate' : 0.1,\n          'metric' : 'r2'}\n\nxgb_params = {'random_state': 22,\n#           'tree_method' : 'gpu_hist',   #uncomment it if you want to use GPU\n          'n_estimators': 20,  # you can increase the n_splits value to >1000, to minimize the runtime I have used 20\n          'metric' : 'r2'} \n\ncat_params = {'random_state': 123,\n#           'task_type' : 'GPU',   #uncomment it if you want to use GPU\n          'iterations': 500,  # you can increase the n_splits value to >2000, to minimize the runtime I have used 100\n         'loss_function': 'RMSE',}\n\n\nfor i in [4]:\n    dummy_df = pd.DataFrame()\n    LGBM_df = pd.DataFrame()\n    XGB_df = pd.DataFrame()\n    CAT_df = pd.DataFrame()\n    ENSEMBLE_df = pd.DataFrame()\n    \n    col_train = pd.DataFrame()\n    col_test = pd.DataFrame()\n    \n    dummy_df=df[i].copy()\n    LGBM_df=df[i].copy()\n    XGB_df = df[i].copy()\n    CAT_df= df[i].copy()\n    ENSEMBLE_df= df[i].copy()\n    \n    for column in dummy_df.columns: \n        print('----------------------------------------Processing Colunm Name : ', column)\n        lgbm_feature_imp, y_pred_list_lgbm, y_true_list, lgbm_R2_list  = [],[],[],[]\n        xgb_feature_imp, y_pred_list_xgb, xgb_R2_list  = [],[],[]\n        cat_feature_imp, y_pred_list_cat, cat_R2_list  = [],[],[]\n        if dummy_df[column].isnull().sum() == 0:\n            print(dummy_df[column].isnull().sum())\n            continue    # continue as no NaN values found in this column\n        col_nan_ix = dummy_df[dummy_df[column].isnull()].index  # identify the rows which has NaN in column \n        col_train = dummy_df.drop(col_nan_ix, axis = 0)  #training set which has fixed value but other columns might have NaN values\n        col_test = dummy_df[dummy_df.index.isin(col_nan_ix)] # test set which has NaN value in the test set\n        lgbm_model = LGBMRegressor(**lgbm_params)\n        xgb_model = XGBRegressor(**xgb_params)\n        cat_model = CatBoostRegressor(**cat_params)\n        X = col_train.drop([column],axis=1).reset_index(drop=True)\n        y = col_train[column].reset_index(drop=True)\n        for fold, (train_index, val_index) in enumerate(kfold.split(X, y)): # split the trainset to train and valid set\n            print(\"==fold==\", fold)\n            X_train = X.loc[train_index]\n            X_val = X.loc[val_index]\n            y_train = y.loc[train_index]\n            y_val = y.loc[val_index]\n            print(\"Fitting CAT for the column\",column)\n            cat_model.fit(X_train,y_train,logging_level='Silent')\n            print(\"Fitting LGBM for the column\",column)\n            lgbm_model.fit(X_train,y_train)\n            print(\"Fitting XGB for the column\",column)\n            xgb_model.fit(X_train,y_train)\n\n            y_pred_lgbm = lgbm_model.predict(X_val)\n            y_pred_xgb = xgb_model.predict(X_val)\n            y_pred_cat = cat_model.predict(X_val)\n            y_pred_list_lgbm = np.append(y_pred_list_lgbm, y_pred_lgbm)\n            y_pred_list_xgb= np.append(y_pred_list_xgb, y_pred_xgb)\n            y_pred_list_cat= np.append(y_pred_list_cat, y_pred_cat)\n            y_true_list = np.append(y_true_list, y_val)\n            lgbm_R2_list.append(r2_score(y_val,y_pred_lgbm)) # Collecting the LGBM R2 score for each fold\n            xgb_R2_list.append(r2_score(y_val,y_pred_xgb))   # Collecting the XGB R2 score for each fold\n            cat_R2_list.append(r2_score(y_val,y_pred_cat))   # Collecting the CAT R2 score for each fold\n        LGBM_df[column][col_nan_ix] = lgbm_model.predict(col_test.drop([column],axis=1))\n        XGB_df[column][col_nan_ix] = xgb_model.predict(col_test.drop([column],axis=1))\n        CAT_df[column][col_nan_ix] = cat_model.predict(col_test.drop([column],axis=1))\n        print(\"R2_lgbm\",statistics.mean(lgbm_R2_list))\n        print(\"XGB_lgbm\",statistics.mean(xgb_R2_list))\n        print(\"CAT_lgbm\",statistics.mean(cat_R2_list))\n        Ensebmle_error = {'LGBM' : statistics.mean(lgbm_R2_list),  # calculating mean R2 of all the folds\n        'XGB' : statistics.mean(xgb_R2_list),\n        'CAT' : statistics.mean(cat_R2_list)}\n        best_model= max(Ensebmle_error, key=Ensebmle_error.get)  # identifying best model by comparing R2 values\n        if best_model=='LGBM':\n          print(\"best model is LGBM for the column\",column)\n          ENSEMBLE_df[column][col_nan_ix] = lgbm_model.predict(col_test.drop([column],axis=1))\n        elif best_model=='XGB':\n          print(\"best model is XGB for the column\",column)\n          ENSEMBLE_df[column][col_nan_ix] = xgb_model.predict(col_test.drop([column],axis=1))\n        else:\n          print(\"best model is CAT for the column\",column)\n          ENSEMBLE_df[column][col_nan_ix] = cat_model.predict(col_test.drop([column],axis=1))\ndf[4]=ENSEMBLE_df.copy()","metadata":{"_kg_hide-output":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Now we use simple meanimputer for subset-1 and subset-3. Note that subset-2 has no missing values","metadata":{}},{"cell_type":"code","source":"from sklearn.impute import SimpleImputer\nimp = SimpleImputer(\n        missing_values=np.nan,\n        strategy='mean') \nfor i in [1,3]:    \n    df[i][:] = imp.fit_transform(df[i])","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Merged_Subsets = pd.concat([df[1], df[2], df[3], df[4]], axis=1)\nsubmission = pd.read_csv('../input/tabular-playground-series-jun-2022/sample_submission.csv', index_col='row-col')\nfor i in tqdm(submission.index):\n    row = int(i.split('-')[0])\n    col = i.split('-')[1]\n    submission.loc[i, 'value'] = Merged_Subsets.loc[row, col]\n\n# submission.to_csv('submission.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###  We need to optimize the model by chaning the algorithums and optimzing the hyperparameters. So far, after tuning, my results are as follows. I will update the code after finalizing the model. You can also try.","metadata":{}},{"cell_type":"code","source":"submission= pd.read_csv(\"../input/tps-june2022-bestofthree/submission_best_v4.csv\")\nsubmission.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Work in Progress.....","metadata":{}},{"cell_type":"markdown","source":"<img src=\"https://media1.giphy.com/media/5nvQ7fBWhPVXXOcfRI/giphy.gif?cid=6c09b952a8132b3cf6fd8d9503e91775e810cf6d3d504bc4&rid=giphy.gif&ct=g\">","metadata":{}},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#E59866;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:black;\"><b>3.0 | References</b></p>\n</div>","metadata":{}},{"cell_type":"markdown","source":"https://towardsdatascience.com/using-the-missingno-python-library-to-identify-and-visualise-missing-data-prior-to-machine-learning-34c8c5b5f009 <br>\nhttps://towardsdatascience.com/how-to-handle-missing-data-8646b18db0d4 <br>\nhttps://www.kaggle.com/code/parulpandey/a-guide-to-handling-missing-values-in-python/notebook <br>\nhttps://www.kaggle.com/code/residentmario/using-missingno-to-diagnose-data-sparsity/notebook <br>\nhttps://www.analyticsvidhya.com/blog/2021/05/dealing-with-missing-values-in-python-a-complete-guide/ <br>\nhttps://www.kaggle.com/code/calebreigada/getting-started-eda-preprocessing <br>\nhttps://medium.com/swlh/impute-missing-values-the-right-way-c63735fccccd <br>\n\n","metadata":{}}]}