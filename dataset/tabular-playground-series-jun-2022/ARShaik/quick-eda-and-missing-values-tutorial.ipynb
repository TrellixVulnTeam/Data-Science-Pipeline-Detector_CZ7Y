{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div style=\"padding:20px;color:white;margin:0;font-size:200%;text-align:center;display:fill;border-radius:5px;background-color:#196F3D;overflow:hidden;font-weight:500\">TPS JUNE 2022</div>\n<br>\n<br>\n<br>\n<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#52BE80;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:black;\"><b>1.0 | Introduction</b></p>\n</div>\n\nData Cleaning is the process of finding and correcting the inaccurate/incorrect data that are present in the dataset. One such process needed is to do something about the values that are missing in the dataset. In real life, many datasets will have many missing values, so dealing with them is an important step.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom tqdm import tqdm\nimport missingno as msno\nfrom sklearn.impute import SimpleImputer","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-14T15:22:13.115351Z","iopub.execute_input":"2022-06-14T15:22:13.116502Z","iopub.status.idle":"2022-06-14T15:22:14.031079Z","shell.execute_reply.started":"2022-06-14T15:22:13.116439Z","shell.execute_reply":"2022-06-14T15:22:14.030267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#52BE80;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:black;\"><b>2.0 | Load data and basic checks</b></p>\n</div>\n","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv(\"../input/tabular-playground-series-jun-2022/data.csv\")\nprint(\"Data Shape: There are {:,.0f} rows and {:,.0f} columns.\\nMissing values = {}, Duplicates = {}.\\n\".\n      format(data.shape[0], data.shape[1],data.isna().sum().sum(), data.duplicated().sum()))\ndf=data.describe()\ndisplay(df.style.format('{:,.3f}')\n        .background_gradient(subset=(df.index[1:],df.columns[:]), cmap='GnBu'))\n","metadata":{"execution":{"iopub.status.busy":"2022-06-14T15:22:14.032744Z","iopub.execute_input":"2022-06-14T15:22:14.033813Z","iopub.status.idle":"2022-06-14T15:22:23.292003Z","shell.execute_reply.started":"2022-06-14T15:22:14.033755Z","shell.execute_reply":"2022-06-14T15:22:23.290377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get the number of missing data points per column\nmissing_values_count = data.isnull().sum()\n\n# look at the # of missing points in the first ten columns\nmissing_values_count[0:30]","metadata":{"execution":{"iopub.status.busy":"2022-06-14T15:22:23.292748Z","iopub.status.idle":"2022-06-14T15:22:23.293083Z","shell.execute_reply.started":"2022-06-14T15:22:23.292932Z","shell.execute_reply":"2022-06-14T15:22:23.292948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Quick insight:\nAs we can see above, some of the columns have no missing values, which is a good sign. We need to see if any of the columns are correlated.","metadata":{}},{"cell_type":"code","source":"# how many total missing values do we have?\ntotal_cells = np.product(data.shape)\ntotal_missing = missing_values_count.sum()\n\n# percent of data that is missing\n(total_missing/total_cells) * 100","metadata":{"execution":{"iopub.status.busy":"2022-06-14T15:22:23.294312Z","iopub.status.idle":"2022-06-14T15:22:23.294805Z","shell.execute_reply.started":"2022-06-14T15:22:23.294631Z","shell.execute_reply":"2022-06-14T15:22:23.29465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Lets visulize first 500 rows for missing values","metadata":{}},{"cell_type":"code","source":"import missingno as msno\nmsno.matrix(data.sample(500))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-14T15:22:23.295715Z","iopub.status.idle":"2022-06-14T15:22:23.296024Z","shell.execute_reply.started":"2022-06-14T15:22:23.295879Z","shell.execute_reply":"2022-06-14T15:22:23.295894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Quick insights:\n1) The missing data location is vary random. We need to identify is there any pattern to it.","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport matplotlib.colors\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom plotly.offline import init_notebook_mode\nimport seaborn as sns\nprimary_bgcolor = \"#f4f0ea\"\nprimary_blue = \"#496595\"\nprimary_blue2 = \"#85a1c1\"\nprimary_blue3 = \"#3f4d63\"\nprimary_grey = \"#c6ccd8\"\nprimary_black = \"#202022\"\ncorr = data.corr().abs()\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\n\nfig, ax = plt.subplots(figsize=(90, 50), facecolor=primary_bgcolor)\n# ax.text(-1.1, 0.16, 'Correlation between the Continuous Features', fontsize=10, fontweight='bold', fontfamily='serif')\nax.text(-1.1, 0.3, 'There is no features that pass more than 0.32 correlation within each other', fontsize=13, fontweight='light', fontfamily='serif')\n\n\n# plot heatmap\nres=sns.heatmap(corr, mask=mask, annot=True, fmt=\".2f\", cmap='coolwarm',annot_kws={\"size\": 25},\n            cbar_kws={\"shrink\": .2}, vmin=0, vmax=1)\nres.set_xticklabels(res.get_xmajorticklabels(), fontsize = 28)\nres.set_yticklabels(res.get_ymajorticklabels(), fontsize = 28)\n# yticks\nplt.yticks(rotation=0)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-14T15:22:23.296803Z","iopub.status.idle":"2022-06-14T15:22:23.297109Z","shell.execute_reply.started":"2022-06-14T15:22:23.296966Z","shell.execute_reply":"2022-06-14T15:22:23.296981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Quick insight: As we can see above some of the columns are highly correlated. We can use this informaiton to replace the missing data.","metadata":{}},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#52BE80;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:black;\"><b>3.0 | Imputation techniques</b></p>\n</div>\nTypically replacing the missing data with new data is called imputation. There are various techniques for this task depeding on the type of the data that we have. Typically imputation can be divided into following types\n\n1) Constant value imputation: As name suggests all the missing values will be replaced with constant value.  <br> \n2) Basic statiscal value imputation: As name suggests missing valeus will be treated as the mean, median or most frequent value of that column  <br>\n3) Advanced techniques:  <br>\nWhen basic techniques does not address the problem, machine learning algorithms can be used to identify the substitute value for the missing values. \n","metadata":{}},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#52BE80;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:black;\"><b>3.1 | Basic statistics- Mean value</b></p>\n</div>","metadata":{}},{"cell_type":"code","source":"Target = pd.read_csv(\"../input/tabular-playground-series-jun-2022/sample_submission.csv\", index_col='row-col')\nTarget.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-14T15:22:23.298135Z","iopub.status.idle":"2022-06-14T15:22:23.29845Z","shell.execute_reply.started":"2022-06-14T15:22:23.298308Z","shell.execute_reply":"2022-06-14T15:22:23.298322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# data = data.sample(frac =.25)","metadata":{"execution":{"iopub.status.busy":"2022-06-14T15:22:23.299419Z","iopub.status.idle":"2022-06-14T15:22:23.299766Z","shell.execute_reply.started":"2022-06-14T15:22:23.299615Z","shell.execute_reply":"2022-06-14T15:22:23.299631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# reference notebook\nimp = SimpleImputer(\n        missing_values=np.nan,\n        strategy='mean')\ndata_mean=data.copy()\ndata_mean[:] = imp.fit_transform(data)\nfor i in tqdm(Target.index):\n    row = int(i.split('-')[0])\n    col = i.split('-')[1]\n    Target.loc[i, 'value'] = data_mean.loc[row, col]","metadata":{"execution":{"iopub.status.busy":"2022-06-14T15:22:23.301005Z","iopub.status.idle":"2022-06-14T15:22:23.301318Z","shell.execute_reply.started":"2022-06-14T15:22:23.301165Z","shell.execute_reply":"2022-06-14T15:22:23.30118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#52BE80;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:black;\"><b>3.2 | Basic statistics- Constant value</b></p>\n</div>","metadata":{}},{"cell_type":"code","source":"# imputing with a constant\nimp = SimpleImputer(\n        missing_values=np.nan,\n        strategy='constant')\ndata_constant=data.copy()\ndata_constant[:] = imp.fit_transform(data)\nfor i in tqdm(Target.index):\n    row = int(i.split('-')[0])\n    col = i.split('-')[1]\n    Target.loc[i, 'value'] = data_constant.loc[row, col]","metadata":{"execution":{"iopub.status.busy":"2022-06-14T15:22:23.302194Z","iopub.status.idle":"2022-06-14T15:22:23.302535Z","shell.execute_reply.started":"2022-06-14T15:22:23.302362Z","shell.execute_reply":"2022-06-14T15:22:23.302376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#52BE80;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:black;\"><b>3.3 | Basic statistics- Most frequent</b></p>\n</div>","metadata":{}},{"cell_type":"code","source":"imp = SimpleImputer(\n        missing_values=np.nan,\n        strategy='most_frequent')\ndata_frequent=data.copy()\ndata_frequent[:] = imp.fit_transform(data)\nfor i in tqdm(Target.index):\n    row = int(i.split('-')[0])\n    col = i.split('-')[1]\n    Target.loc[i, 'value'] = data_frequent.loc[row, col]","metadata":{"execution":{"iopub.status.busy":"2022-06-14T15:22:23.30342Z","iopub.status.idle":"2022-06-14T15:22:23.303767Z","shell.execute_reply.started":"2022-06-14T15:22:23.303619Z","shell.execute_reply":"2022-06-14T15:22:23.303635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#52BE80;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:black;\"><b>4.1 | Advanced- K-Nearest Neighbor Imputation</b></p>\n</div>","metadata":{}},{"cell_type":"code","source":"# from sklearn.impute import KNNImputer\n# data_knn = data.copy(deep=True)\n\n# knn_imputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\n# #tranforming individual column seperately\n# for column in data_knn:   \n#     data_knn[column] = knn_imputer.fit_transform(data_knn[[column]])\n# for i in tqdm(Target.index):\n#     row = int(i.split('-')[0])\n#     col = i.split('-')[1]\n#     Target.loc[i, 'value'] = data_knn.loc[row, col]","metadata":{"execution":{"iopub.status.busy":"2022-06-14T15:22:23.304768Z","iopub.status.idle":"2022-06-14T15:22:23.305073Z","shell.execute_reply.started":"2022-06-14T15:22:23.30493Z","shell.execute_reply":"2022-06-14T15:22:23.304944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Notes: For quick EDA I have used uniform weights. We should try various options to see which one fit the best. This technique takes extremly longtime for large datasets. I have commented it out to minimze the runtime.","metadata":{}},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#52BE80;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:black;\"><b>4.2 | Advanced- MICE</b></p>\n</div>\nHere Round-robin technique is used where the missing values are replaced as a functio nof other features. Mutiple regression is performed over random sample of the column data and then regression values are used to replace the missing values.","metadata":{}},{"cell_type":"code","source":"from sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\n\ndata_MICE = data.copy(deep=True)\nmice_imputer = IterativeImputer()\n#tranforming individual column seperately\nfor column in data_MICE:   \n    data_MICE[column] = mice_imputer.fit_transform(data_MICE[[column]])\nfor i in tqdm(Target.index):\n    row = int(i.split('-')[0])\n    col = i.split('-')[1]\n    Target.loc[i, 'value'] = data_MICE.loc[row, col]","metadata":{"execution":{"iopub.status.busy":"2022-06-14T15:22:23.306079Z","iopub.status.idle":"2022-06-14T15:22:23.306406Z","shell.execute_reply.started":"2022-06-14T15:22:23.306259Z","shell.execute_reply":"2022-06-14T15:22:23.306274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#52BE80;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:black;\"><b>4.3 | Advanced- Regression</b></p>\n</div>\n \n","metadata":{}},{"cell_type":"markdown","source":"An efficient way to handle missing values is predicting them using high performance regression techniques. This is a much more accurate solution to the problem. In this technique, one column is treated as a target at a time and try to predict the column's value using other columns. If the missing values are present only in the numerical columns, high performance regression models cand do the missing value predictions. For each column, we can take all the rows where the column value is present as the training dataset and the missing column values as the test dataset.","metadata":{}},{"cell_type":"markdown","source":"<img src=\"https://i.postimg.cc/90BdtTjq/imputer.gif\">\n\n","metadata":{}},{"cell_type":"markdown","source":"### Here I have applied aforementioned regression technique to current data.","metadata":{}},{"cell_type":"code","source":"from distutils.dir_util import copy_tree\n\nfrom_dir = '../input/tpsjune2022-tabnet/'\nto_dir = './'\ncopy_tree(from_dir, to_dir)\nimport os\nos.rename(r'./submission_pytorch.csv',r'./submission.csv')\n\n","metadata":{"execution":{"iopub.status.busy":"2022-06-14T15:22:23.307434Z","iopub.status.idle":"2022-06-14T15:22:23.307784Z","shell.execute_reply.started":"2022-06-14T15:22:23.307636Z","shell.execute_reply":"2022-06-14T15:22:23.307652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Full implimentation of above technique is shown in this public [notebook](https://www.kaggle.com/code/abdulravoofshaik/top-3-solution-lgbm-mean) which has resulted in very good score when compared with other techniques. Note that I have used LGBM, but you can try other techniques such as XGboost, RandomForest, Catboost....etc.","metadata":{}},{"cell_type":"markdown","source":"### Well, there is no single best way to handle missing values. One needs to experiment with different methods and then decide which method is best for a particular problem. ","metadata":{}},{"cell_type":"markdown","source":"## Work in progress","metadata":{}},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#52BE80;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:black;\"><b>5.0 | References</b></p>\n</div>","metadata":{}},{"cell_type":"markdown","source":"https://towardsdatascience.com/using-the-missingno-python-library-to-identify-and-visualise-missing-data-prior-to-machine-learning-34c8c5b5f009 <br>\nhttps://towardsdatascience.com/how-to-handle-missing-data-8646b18db0d4 <br>\nhttps://www.kaggle.com/code/parulpandey/a-guide-to-handling-missing-values-in-python/notebook <br>\nhttps://www.kaggle.com/code/residentmario/using-missingno-to-diagnose-data-sparsity/notebook <br>\nhttps://www.analyticsvidhya.com/blog/2021/05/dealing-with-missing-values-in-python-a-complete-guide/ <br>\nhttps://www.kaggle.com/competitions/tabular-playground-series-jun-2022/discussion/328369 <br>\n\n","metadata":{}}]}