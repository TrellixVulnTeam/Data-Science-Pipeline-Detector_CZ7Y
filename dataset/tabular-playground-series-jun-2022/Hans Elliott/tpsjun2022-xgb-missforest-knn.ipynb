{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Tabular Playground Series - June 2022\nHans Elliott\n\n**Task: Imputation on large dataset**  \n**Criterion: RMSE**    \nModels:\n- KNN (parallelized with Pool)\n- \"Missing Forest\", i.e. iterative imputer with RF regressor (parallelized with Pool)  \n- XGBoost with GPU acceleration\n- XGBoost + mean imputation - score: 0.94892 rmse","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\n# for knn\nfrom sklearn.impute import KNNImputer\n# for missing forest\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.ensemble import RandomForestRegressor\n# XG Boost\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_squared_error as MSE\n# Neural Net\nimport tensorflow as tf\n# misc\nfrom tqdm import tqdm\nimport multiprocessing\nfrom multiprocessing import Pool","metadata":{"execution":{"iopub.status.busy":"2022-06-30T19:58:26.73057Z","iopub.execute_input":"2022-06-30T19:58:26.731443Z","iopub.status.idle":"2022-06-30T19:58:26.736768Z","shell.execute_reply.started":"2022-06-30T19:58:26.731412Z","shell.execute_reply":"2022-06-30T19:58:26.735888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"multiprocessing.cpu_count() ##using GPUs","metadata":{"execution":{"iopub.status.busy":"2022-06-30T19:58:30.091011Z","iopub.execute_input":"2022-06-30T19:58:30.091463Z","iopub.status.idle":"2022-06-30T19:58:30.09972Z","shell.execute_reply.started":"2022-06-30T19:58:30.091413Z","shell.execute_reply":"2022-06-30T19:58:30.098883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Competition: https://www.kaggle.com/competitions/tabular-playground-series-jun-2022  \nEDA/Simple Baseline Notebook: https://www.kaggle.com/code/hanselliott/tabularjun2022-eda-simplerulebaseline?scriptVersionId=99652696 ","metadata":{}},{"cell_type":"code","source":"raw_data = pd.read_csv(\"../input/tabular-playground-series-jun-2022/data.csv\")\nsample_sub = pd.read_csv(\"../input/tabular-playground-series-jun-2022/sample_submission.csv\")\nsample_sub.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T19:58:31.645263Z","iopub.execute_input":"2022-06-30T19:58:31.645767Z","iopub.status.idle":"2022-06-30T19:58:49.236055Z","shell.execute_reply.started":"2022-06-30T19:58:31.64573Z","shell.execute_reply":"2022-06-30T19:58:49.235154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"raw_data.shape","metadata":{"execution":{"iopub.status.busy":"2022-06-30T18:24:39.730212Z","iopub.execute_input":"2022-06-30T18:24:39.731017Z","iopub.status.idle":"2022-06-30T18:24:39.736443Z","shell.execute_reply.started":"2022-06-30T18:24:39.730982Z","shell.execute_reply":"2022-06-30T18:24:39.735547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Imputing missing values, all numeric variables**","metadata":{}},{"cell_type":"code","source":"raw_data.describe()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T18:24:42.434496Z","iopub.execute_input":"2022-06-30T18:24:42.434871Z","iopub.status.idle":"2022-06-30T18:24:45.736545Z","shell.execute_reply.started":"2022-06-30T18:24:42.434841Z","shell.execute_reply":"2022-06-30T18:24:45.735638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = raw_data.drop(labels='row_id',axis=1)\ntrain.shape","metadata":{"execution":{"iopub.status.busy":"2022-06-30T19:59:04.073143Z","iopub.execute_input":"2022-06-30T19:59:04.074078Z","iopub.status.idle":"2022-06-30T19:59:04.24654Z","shell.execute_reply.started":"2022-06-30T19:59:04.074037Z","shell.execute_reply":"2022-06-30T19:59:04.24546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Total missing vals: \", train.isna().sum().sum() )","metadata":{"execution":{"iopub.status.busy":"2022-06-30T18:24:55.50043Z","iopub.execute_input":"2022-06-30T18:24:55.500876Z","iopub.status.idle":"2022-06-30T18:24:55.661764Z","shell.execute_reply.started":"2022-06-30T18:24:55.500835Z","shell.execute_reply":"2022-06-30T18:24:55.660944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# NO F2s are missing\ntrain.info()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T18:25:13.605424Z","iopub.execute_input":"2022-06-30T18:25:13.606226Z","iopub.status.idle":"2022-06-30T18:25:13.756431Z","shell.execute_reply.started":"2022-06-30T18:25:13.606189Z","shell.execute_reply":"2022-06-30T18:25:13.755388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Parallel K = 5 Nearest Neighbors Imputation Function","metadata":{}},{"cell_type":"code","source":"def knn_impute(df):\n    imputer = KNNImputer(n_neighbors=5, weights=\"distance\")  ##sklearn\n    imputed_df = imputer.fit_transform(df)\n    return imputed_df\n\ndef multiprocess_knn(df_list):\n    p = Pool(processes=4)  ##multiprocessing\n    data = p.map(knn_impute, [df for df in df_list])\n    p.close()\n    return data","metadata":{"execution":{"iopub.status.busy":"2022-06-30T07:41:39.105828Z","iopub.execute_input":"2022-06-30T07:41:39.106171Z","iopub.status.idle":"2022-06-30T07:41:39.112881Z","shell.execute_reply.started":"2022-06-30T07:41:39.106141Z","shell.execute_reply":"2022-06-30T07:41:39.111958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Parallel Missing Forest Imputation Function","metadata":{}},{"cell_type":"code","source":"# define missing forest imputation fns based on sklearn's iterative imputer\nrf_estimator = RandomForestRegressor(n_estimators=50,\n                                    criterion=\"squared_error\", #closest option to RMSE (the challenge's criterion)\n                                    n_jobs=1)\n\ndef miss_forest(df):\n    imputer = IterativeImputer(estimator=rf_estimator,\n                              max_iter=3,\n                              min_value = -15, max_value=15, ##based on EDA, values shouldnt exceed this range\n                              verbose=2)\n    imputed_df = imputer.fit_transform(df)\n    return imputed_df\n\n\n# function to parallelize it across sub-dfs\ndef multi_missforest(df_list):\n    p = Pool(processes=4)\n    data = p.map(miss_forest, [df for df in df_list])\n    p.close()\n    return data","metadata":{"execution":{"iopub.status.busy":"2022-06-30T06:51:09.887482Z","iopub.execute_input":"2022-06-30T06:51:09.887982Z","iopub.status.idle":"2022-06-30T06:51:09.900508Z","shell.execute_reply.started":"2022-06-30T06:51:09.88794Z","shell.execute_reply":"2022-06-30T06:51:09.899662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# XGBoost Imputation Function","metadata":{}},{"cell_type":"code","source":"def xgboost_impute(df):\n    \"\"\"\n    Iterates through columns with missing values. Finds samples that are missing in current column col and removes them into a\n    \"test\" subset. The non-missing samples in col are the y_train labels. The missing samples are the y_test labels (what we want to impute.)\n    The remaining columns (all other than col) at the test indices are the X_test subset.\n    The remaining columns (all other than col) not at the test indices are the X_train subset.\n    An XGBoost model is fit on X_train and y_train, and then predicted onto X_test to impute y_test.\n    \"\"\"\n    df = df.copy()\n    cols_w_miss = df.filter(regex = \"F_1|F_3|F_4\").columns ##no F2 features are missing cols, and fn will fail if x_test ends up empty\n    for i, col in enumerate(tqdm(cols_w_miss)):\n        #Data\n        y_test = df[df[col].isnull()][col]       ##the samples in col w missing vals\n        y_train = df[~df[col].isnull()][col]     ##the samples in col w/out missing vals\n        x_train = df[~df[col].isnull()].drop(col, axis=1)  ##the training subset without the col being imputed\n        x_test = df[df[col].isnull()].drop(col, axis=1)     ##the testing subset without the col being imputed \n        #Model\n        xgb = XGBRegressor(tree_method='gpu_hist',\n                           predictor= \"gpu_predictor\", \n                           eta = 0.3,\n                           max_depth = 3,\n                           n_estimators = 750)\n        xgb.fit(x_train, y_train)\n        #RMSE Score:\n        train_pred = xgb.predict(x_train)\n        rmse = np.sqrt(MSE(y_train, train_pred))\n        print(\"Column: \", col, \"| Train RMSE: \", rmse)\n        #Replace the missing vals with predictions\n        df.loc[y_test.index, col] = xgb.predict(x_test)\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-06-30T19:17:44.095489Z","iopub.execute_input":"2022-06-30T19:17:44.096388Z","iopub.status.idle":"2022-06-30T19:17:44.106231Z","shell.execute_reply.started":"2022-06-30T19:17:44.096346Z","shell.execute_reply":"2022-06-30T19:17:44.105452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Imputation\n### Create sub-dataframes for parallel imputation","metadata":{}},{"cell_type":"code","source":"## Break data into subsets to allow for multiprocessing (faster imputation)\nsub1 = train[0:100000].copy()\nsub2 = train[100000:200000].copy()\nsub3 = train[200000:300000].copy()\nsub4 = train[300000:400000].copy()\nsub5 = train[400000:500000].copy()\nsub6 = train[500000:600000].copy()\nsub7 = train[600000:700000].copy()\nsub8 = train[700000:800000].copy()\nsub9 = train[800000:900000].copy()\nsub10 = train[900000:1000000].copy()\ndf_list = [sub1, sub2, sub3, sub4, sub5, sub6, sub7, sub8, sub9, sub10]","metadata":{"execution":{"iopub.status.busy":"2022-06-30T02:06:34.173678Z","iopub.execute_input":"2022-06-30T02:06:34.174154Z","iopub.status.idle":"2022-06-30T02:06:35.343098Z","shell.execute_reply.started":"2022-06-30T02:06:34.174118Z","shell.execute_reply":"2022-06-30T02:06:35.342337Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def split_df(n, df):\n    \"\"\"\n    Splits df into n sub-dfs and adds them to a list. Stored sequentially (in order of index)\n    \"\"\"\n    start_i = 0\n    end_i = len(df)//n\n    \n    df_list = []\n    for i in range(0, n):\n        sub_i = df[start_i:end_i].copy()\n        df_list.append(sub_i)\n        start_i = end_i\n        end_i = end_i + len(df)//n\n    \n    return df_list","metadata":{"execution":{"iopub.status.busy":"2022-06-30T19:40:39.940042Z","iopub.execute_input":"2022-06-30T19:40:39.940435Z","iopub.status.idle":"2022-06-30T19:40:39.945812Z","shell.execute_reply.started":"2022-06-30T19:40:39.940404Z","shell.execute_reply":"2022-06-30T19:40:39.945065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_list = split_df(10, train)\nlen(df_list)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T19:40:41.956982Z","iopub.execute_input":"2022-06-30T19:40:41.957882Z","iopub.status.idle":"2022-06-30T19:40:42.493318Z","shell.execute_reply.started":"2022-06-30T19:40:41.957837Z","shell.execute_reply":"2022-06-30T19:40:42.492645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## KNN Imputation","metadata":{}},{"cell_type":"code","source":"# Execute KNN Algorithm (in parallel)\nresults_list = multiprocess_knn(df_list)\n            # results_list = [df.pipe(knn_impute) for df in df_list] ##for non parallel","metadata":{"execution":{"iopub.status.busy":"2022-06-30T02:06:44.48022Z","iopub.execute_input":"2022-06-30T02:06:44.480697Z","iopub.status.idle":"2022-06-30T02:06:44.486462Z","shell.execute_reply.started":"2022-06-30T02:06:44.48066Z","shell.execute_reply":"2022-06-30T02:06:44.48508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Public Score: 1.31252**","metadata":{}},{"cell_type":"markdown","source":"## Missing Forest Imputation","metadata":{}},{"cell_type":"code","source":"# Execute MissForest Algorithm (in parallel)\nresults_list = multi_missforest(df_list)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T02:06:44.614797Z","iopub.execute_input":"2022-06-30T02:06:44.615524Z","iopub.status.idle":"2022-06-30T02:07:06.846472Z","shell.execute_reply.started":"2022-06-30T02:06:44.615486Z","shell.execute_reply":"2022-06-30T02:07:06.842467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## XGBoost Imputation\nDon't want to split up the df here since XGBoost automatically computes in parallel and I am using gpu_hist/predict to take advantage of the GPU accelerator.","metadata":{}},{"cell_type":"code","source":"# Execute XGBoost on full df\nresults = xgboost_impute(train)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T19:39:40.807485Z","iopub.execute_input":"2022-06-30T19:39:40.808006Z","iopub.status.idle":"2022-06-30T19:39:40.814777Z","shell.execute_reply.started":"2022-06-30T19:39:40.807966Z","shell.execute_reply":"2022-06-30T19:39:40.814006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#save results to csv for backup\nresults.to_csv(\"xgb_results.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T09:02:02.401841Z","iopub.execute_input":"2022-06-30T09:02:02.402601Z","iopub.status.idle":"2022-06-30T09:03:30.973127Z","shell.execute_reply.started":"2022-06-30T09:02:02.402555Z","shell.execute_reply":"2022-06-30T09:03:30.971912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Takes ~1hr 30 mins to impute using the XGBoost methods (w GPUs) so I stored the results externally as backup\nresults = pd.read_csv(\"../input/xgboost-results1/xgb_results.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-06-30T18:32:14.323692Z","iopub.execute_input":"2022-06-30T18:32:14.324072Z","iopub.status.idle":"2022-06-30T18:32:28.949988Z","shell.execute_reply.started":"2022-06-30T18:32:14.324035Z","shell.execute_reply":"2022-06-30T18:32:28.949197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Missing values remaining: \", results.isna().sum().sum())\nprint(\"results.shape: \", results.shape)\nresults.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T18:32:51.627235Z","iopub.execute_input":"2022-06-30T18:32:51.627908Z","iopub.status.idle":"2022-06-30T18:32:51.78361Z","shell.execute_reply.started":"2022-06-30T18:32:51.62787Z","shell.execute_reply":"2022-06-30T18:32:51.782746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Public Score: 1.05612**","metadata":{}},{"cell_type":"markdown","source":"## Format/Prep Results\n**(Ignore for XGBoost)**","metadata":{}},{"cell_type":"code","source":"print(\"Is len(results_list) 10?: \", len(results_list))\nresults_list[0]","metadata":{"execution":{"iopub.status.busy":"2022-06-30T02:07:06.84989Z","iopub.status.idle":"2022-06-30T02:07:06.856002Z","shell.execute_reply.started":"2022-06-30T02:07:06.855594Z","shell.execute_reply":"2022-06-30T02:07:06.855657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Convert imputer output to DataFrame since sklearn imputer outputs np array\ncolnames = train.columns.tolist()\nfor i in range(0, len(results_list)):\n    results_list[i] = pd.DataFrame(results_list[i])\n    results_list[i].columns = colnames","metadata":{"execution":{"iopub.status.busy":"2022-06-30T02:07:06.861533Z","iopub.status.idle":"2022-06-30T02:07:06.862041Z","shell.execute_reply.started":"2022-06-30T02:07:06.861812Z","shell.execute_reply":"2022-06-30T02:07:06.861834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results = pd.concat(results_list) ##concatenate list of dfs into one df\nresults = results.reset_index()   ##reset the indices","metadata":{"execution":{"iopub.status.busy":"2022-06-30T02:07:06.86334Z","iopub.status.idle":"2022-06-30T02:07:06.863765Z","shell.execute_reply.started":"2022-06-30T02:07:06.863558Z","shell.execute_reply":"2022-06-30T02:07:06.863579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Missing values remaining: \", results.isna().sum().sum())","metadata":{"execution":{"iopub.status.busy":"2022-06-30T09:04:20.289761Z","iopub.execute_input":"2022-06-30T09:04:20.290466Z","iopub.status.idle":"2022-06-30T09:04:20.480546Z","shell.execute_reply.started":"2022-06-30T09:04:20.29042Z","shell.execute_reply":"2022-06-30T09:04:20.4794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Extract Imputations (\"predictions\") for Submission  \nThis method is somewhat time consuming given the size of the data. Could potentially use the `split_df` fn to parallelize the replacement though. ","metadata":{}},{"cell_type":"code","source":"submission = sample_sub.copy()\nfor i, idx in tqdm(enumerate(submission['row-col'])):    \n    row = int(idx.split('-')[0])\n    col = idx.split('-')[1]\n    submission.iloc[i, 1] = results.loc[row, col]\nsubmission","metadata":{"execution":{"iopub.status.busy":"2022-06-30T18:33:09.470956Z","iopub.execute_input":"2022-06-30T18:33:09.471337Z","iopub.status.idle":"2022-06-30T19:11:58.507126Z","shell.execute_reply.started":"2022-06-30T18:33:09.471307Z","shell.execute_reply":"2022-06-30T19:11:58.506353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T19:12:27.058822Z","iopub.execute_input":"2022-06-30T19:12:27.059175Z","iopub.status.idle":"2022-06-30T19:12:30.160351Z","shell.execute_reply.started":"2022-06-30T19:12:27.059145Z","shell.execute_reply":"2022-06-30T19:12:30.159507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# Another attempt\nFollowing the methodology of some popular notebooks, I'm going to use mean imputations for the F_1 and F_3 columns. I'll keep the XGBoost imputation for the F_4 columns since it seemed to perform well on those.  \nMy EDA showed that F_1 and F_3 features rigidly follow normal distributions (mean 0, sd 1, min -5, max 5).  \nThe F_4 features are less uniform. ","metadata":{}},{"cell_type":"code","source":"##XGB results from above (Saved externally to avoid reimputing)\nxgbresults = pd.read_csv(\"../input/xgboost-results1/xgb_results.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-06-30T20:11:23.72849Z","iopub.execute_input":"2022-06-30T20:11:23.728893Z","iopub.status.idle":"2022-06-30T20:11:39.456117Z","shell.execute_reply.started":"2022-06-30T20:11:23.728862Z","shell.execute_reply":"2022-06-30T20:11:39.455158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Column groups:\nf1col = [x for x in train.columns if x.startswith('F_1')] ##mean impute\nf2col = [x for x in train.columns if x.startswith('F_2')] ##no missing vals\nf3col = [x for x in train.columns if x.startswith('F_3')] ##mean impute\nf4col = [x for x in train.columns if x.startswith('F_4')] ##use xgboost imputes","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##Mean impute F_1, F_3\nmeans = train.copy()\nmeans[f1col+f3col] = means[f1col+f3col].fillna(means[f1col+f3col].mean())\nprint(\"NAs left in df to be imputed: \", means.isna().sum().sum() )\nprint(\"NAs left in F_1, F_3: \", means[f1col+f3col].isna().sum().sum())","metadata":{"execution":{"iopub.status.busy":"2022-06-30T20:17:55.273073Z","iopub.execute_input":"2022-06-30T20:17:55.273491Z","iopub.status.idle":"2022-06-30T20:17:56.593563Z","shell.execute_reply.started":"2022-06-30T20:17:55.273453Z","shell.execute_reply":"2022-06-30T20:17:56.59168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Index imputations into submission df\nsubmission2 = sample_sub.copy()\nfor i, idx in tqdm(enumerate(submission2['row-col'])):    \n    row = int(idx.split('-')[0])\n    col = idx.split('-')[1]\n    if col not in f4col:\n        submission2.iloc[i,1] = means.loc[row, col] ##use the mean imputation for f1, f3 cols\n    else:\n        submission2.iloc[i, 1] = xgbresults.loc[row, col] ##use the xgboost imputation for f4 cols\nsubmission2","metadata":{"execution":{"iopub.status.busy":"2022-06-30T20:21:22.654043Z","iopub.execute_input":"2022-06-30T20:21:22.654475Z","iopub.status.idle":"2022-06-30T21:01:15.143355Z","shell.execute_reply.started":"2022-06-30T20:21:22.654441Z","shell.execute_reply":"2022-06-30T21:01:15.142338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission2.to_csv('submission2.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T21:01:35.267362Z","iopub.execute_input":"2022-06-30T21:01:35.268213Z","iopub.status.idle":"2022-06-30T21:01:38.708021Z","shell.execute_reply.started":"2022-06-30T21:01:35.268173Z","shell.execute_reply":"2022-06-30T21:01:38.707186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Public Score: 0.94892**","metadata":{}},{"cell_type":"markdown","source":"# Notes\nNeither imputer method is fast on data of this size. KNN is slightly faster than Iterative RF but increasing K might improve performance and would slow down the algo.  \nIterative imputation with RF is quite slow on this size of data.  \nThe non-parametric methods are definitely useful considering the features (F_1,...,F_4) are generated from different distributions (shown in EDA).    \nImplementing them in parallel helps a bit with speed but also reduces the amount of data used to impute in each sub-df.\nImplementing the XGBoost algorithm is advantageous for many reasons.\n- For one, XGBoost is known for performing highly on tabular data.\n- Also, it can easily be implemented in parallel (it automatically uses all available threads) and can capitalize on Kaggle's GPU access.\n- For `tree_method`, setting `'gpu_hist'` is recommended for higher performance on large datasets. `'gpu_hist'` is a GPU implementation of the hist algorithm, which aims to speed up training by binning input values into buckets (like a histogram). Basically, it reduces the number of unique values for each feature. And since XGBoost offers GPU support and Kaggle offers GPU access, it's super easy to implement. \n- `'gpu_predictor'` is used when `tree_method` is set to gpu_hist, per the docs.","metadata":{}},{"cell_type":"markdown","source":"---\nNN concept","metadata":{}},{"cell_type":"code","source":"kernel_reg = tf.keras.regularizers.L1L2(l1=0.01, l2=0.01)\n\nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Input(shape=79), ##minus the col being imputed\n    tf.keras.layers.Normalization(axis=-1),\n    #tf.keras.layers.Dense(750, activation='relu', kernel_regularizer=kernel_reg),\n    #tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=kernel_reg),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=kernel_reg),\n    tf.keras.layers.Dense(1) #activation='linear')\n])\nmodel.summary()\n\noptim = tf.keras.optimizers.Adam(learning_rate=0.001, clipnorm=1)\n\nmodel.compile(optimizer=optim,\n             loss='mse',\n             metrics=[tf.keras.metrics.RootMeanSquaredError()]\n             )\n\n#fn for resetting params to randomly initialized\nreset_mod = lambda model, weights: model.set_weights(weights)\nmod_weights = model.get_weights()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T20:06:37.104625Z","iopub.execute_input":"2022-06-30T20:06:37.10535Z","iopub.status.idle":"2022-06-30T20:06:37.160254Z","shell.execute_reply.started":"2022-06-30T20:06:37.105306Z","shell.execute_reply":"2022-06-30T20:06:37.159323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def nn_impute(df):\n    \"\"\"\n    Iterates through columns with missing values. Finds samples that are missing in current column col and removes them into a\n    \"test\" subset. The non-missing samples in col are the y_train labels. The missing samples are the y_test labels (what we want to impute.)\n    The remaining columns (all other than col) at the test indices are the X_test subset.\n    The remaining columns (all other than col) not at the test indices are the X_train subset.\n    A neural net model is fit on X_train and y_train, and then predicted onto X_test to impute y_test.\n    \"\"\"\n    df = df.copy()\n    reset_mod(model, mod_weights)\n    #Impute certain cols\n    cols_w_miss = df.filter(regex = \"F_3|F_4\").columns ##no F2 features are missing cols, and fn will fail if x_test ends up empty\n    for i, col in enumerate(tqdm(cols_w_miss)):\n        # Data\n        y_test = df[df[col].isnull()][col]                 ##the samples in col w missing vals\n        y_train = df[~df[col].isnull()][col]               ##the samples in col w/out missing vals\n        x_train = df[~df[col].isnull()].drop(col, axis=1)  ##the training subset without the col being imputed\n        x_test = df[df[col].isnull()].drop(col, axis=1)    ##the testing subset without the col being imputed \n        \n        # Model\n        model.fit(x_train, y_train, epochs=5)\n        # RMSE Score:\n        #train_pred = xgb.predict(x_train)\n        #rmse = np.sqrt(MSE(y_train, train_pred))\n        #print(\"Column: \", col, \"| Train RMSE: \", rmse)\n        # Replace the missing vals with predictions\n        df.loc[y_test.index, col] = model.predict(x_test)\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-06-30T20:06:39.413649Z","iopub.execute_input":"2022-06-30T20:06:39.414203Z","iopub.status.idle":"2022-06-30T20:06:39.428951Z","shell.execute_reply.started":"2022-06-30T20:06:39.414169Z","shell.execute_reply":"2022-06-30T20:06:39.427802Z"},"trusted":true},"execution_count":null,"outputs":[]}]}