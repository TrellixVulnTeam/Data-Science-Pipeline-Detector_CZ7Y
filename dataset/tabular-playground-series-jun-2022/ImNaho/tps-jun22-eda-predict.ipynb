{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\nfrom sklearn.experimental import enable_iterative_imputer \nfrom sklearn.impute import IterativeImputer\nfrom xgboost import XGBRegressor\nfrom sklearn.linear_model import OrthogonalMatchingPursuit\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn import linear_model\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-30T13:58:58.759831Z","iopub.execute_input":"2022-06-30T13:58:58.760767Z","iopub.status.idle":"2022-06-30T13:59:01.621568Z","shell.execute_reply.started":"2022-06-30T13:58:58.760669Z","shell.execute_reply":"2022-06-30T13:59:01.620114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# read data","metadata":{}},{"cell_type":"code","source":"train=pd.read_csv(\"/kaggle/input/tabular-playground-series-jun-2022/data.csv\")\nsubmission=pd.read_csv(\"/kaggle/input/tabular-playground-series-jun-2022/sample_submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-06-30T13:59:01.623901Z","iopub.execute_input":"2022-06-30T13:59:01.624935Z","iopub.status.idle":"2022-06-30T13:59:19.68956Z","shell.execute_reply.started":"2022-06-30T13:59:01.624873Z","shell.execute_reply":"2022-06-30T13:59:19.688587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# check data","metadata":{}},{"cell_type":"code","source":"train.shape","metadata":{"execution":{"iopub.status.busy":"2022-06-30T13:59:19.69085Z","iopub.execute_input":"2022-06-30T13:59:19.691408Z","iopub.status.idle":"2022-06-30T13:59:19.699981Z","shell.execute_reply.started":"2022-06-30T13:59:19.69136Z","shell.execute_reply":"2022-06-30T13:59:19.699004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T13:59:19.702472Z","iopub.execute_input":"2022-06-30T13:59:19.703212Z","iopub.status.idle":"2022-06-30T13:59:19.744512Z","shell.execute_reply.started":"2022-06-30T13:59:19.703163Z","shell.execute_reply":"2022-06-30T13:59:19.743495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.dtypes","metadata":{"execution":{"iopub.status.busy":"2022-06-30T13:59:19.746091Z","iopub.execute_input":"2022-06-30T13:59:19.746654Z","iopub.status.idle":"2022-06-30T13:59:19.754656Z","shell.execute_reply.started":"2022-06-30T13:59:19.746621Z","shell.execute_reply":"2022-06-30T13:59:19.753423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.dtypes","metadata":{"execution":{"iopub.status.busy":"2022-06-30T13:59:19.755993Z","iopub.execute_input":"2022-06-30T13:59:19.756323Z","iopub.status.idle":"2022-06-30T13:59:19.767909Z","shell.execute_reply.started":"2022-06-30T13:59:19.756272Z","shell.execute_reply":"2022-06-30T13:59:19.76683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"s = (train.dtypes != 'float64')\nobject_cols = list(s[s].index)\n\nprint(\"train Categorical variables:\")\nprint(object_cols)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T13:59:19.769223Z","iopub.execute_input":"2022-06-30T13:59:19.77002Z","iopub.status.idle":"2022-06-30T13:59:19.779303Z","shell.execute_reply.started":"2022-06-30T13:59:19.769977Z","shell.execute_reply":"2022-06-30T13:59:19.778478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T13:59:19.780241Z","iopub.execute_input":"2022-06-30T13:59:19.780954Z","iopub.status.idle":"2022-06-30T13:59:19.795224Z","shell.execute_reply.started":"2022-06-30T13:59:19.780919Z","shell.execute_reply":"2022-06-30T13:59:19.794509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"markdown","source":"**how many columns have null?**","metadata":{}},{"cell_type":"code","source":"cols_with_missing = [col for col in train.columns if train[col].isnull().any()]\nprint('number of column which have null is : ',len(cols_with_missing))","metadata":{"execution":{"iopub.status.busy":"2022-06-30T13:59:19.796247Z","iopub.execute_input":"2022-06-30T13:59:19.796749Z","iopub.status.idle":"2022-06-30T13:59:19.876487Z","shell.execute_reply.started":"2022-06-30T13:59:19.796717Z","shell.execute_reply":"2022-06-30T13:59:19.875063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*columns that have null cell is 55  \n26 column have no null cell  \n(but 1 of 26 columns is row_id)*","metadata":{}},{"cell_type":"markdown","source":"**how many null cell each column have?**","metadata":{}},{"cell_type":"code","source":"count_list=[]\nfor col in cols_with_missing:\n    count=train[col].isnull().tolist().count(True)\n    count_list.append(count)\ndf=pd.DataFrame(columns=['missing_col', 'count'])\ndf['missing_col']=cols_with_missing\ndf['count']=count_list\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T13:59:19.878881Z","iopub.execute_input":"2022-06-30T13:59:19.87922Z","iopub.status.idle":"2022-06-30T13:59:21.227578Z","shell.execute_reply.started":"2022-06-30T13:59:19.879191Z","shell.execute_reply":"2022-06-30T13:59:21.226684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T13:59:21.228739Z","iopub.execute_input":"2022-06-30T13:59:21.229297Z","iopub.status.idle":"2022-06-30T13:59:21.248051Z","shell.execute_reply.started":"2022-06-30T13:59:21.229244Z","shell.execute_reply":"2022-06-30T13:59:21.247096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.histplot(data=df['count'])","metadata":{"execution":{"iopub.status.busy":"2022-06-30T13:59:21.248971Z","iopub.execute_input":"2022-06-30T13:59:21.250026Z","iopub.status.idle":"2022-06-30T13:59:21.433589Z","shell.execute_reply.started":"2022-06-30T13:59:21.24998Z","shell.execute_reply":"2022-06-30T13:59:21.43251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*55 columns have null cell almost same number*","metadata":{}},{"cell_type":"markdown","source":"**how many rows have null cell?**","metadata":{}},{"cell_type":"code","source":"row_with_missing = [row for row in range(train.shape[0]) if train.loc[row,:].isnull().any()]\nprint('number of row which have null is : ',len(row_with_missing))","metadata":{"execution":{"iopub.status.busy":"2022-06-30T13:59:21.434849Z","iopub.execute_input":"2022-06-30T13:59:21.435474Z","iopub.status.idle":"2022-06-30T14:00:33.325084Z","shell.execute_reply.started":"2022-06-30T13:59:21.43543Z","shell.execute_reply":"2022-06-30T14:00:33.322736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"count_list=[]\nfor row in row_with_missing:\n    count=train.loc[row,:].isnull().tolist().count(True)\n    count_list.append(count)\ndf=pd.DataFrame(count_list)\ndf['count_col']=1","metadata":{"execution":{"iopub.status.busy":"2022-06-30T14:00:33.326748Z","iopub.status.idle":"2022-06-30T14:00:33.327173Z","shell.execute_reply.started":"2022-06-30T14:00:33.326995Z","shell.execute_reply":"2022-06-30T14:00:33.327015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df.groupby(0).sum().T)\nsns.histplot(data=df[0])","metadata":{"execution":{"iopub.status.busy":"2022-06-30T14:00:33.32943Z","iopub.status.idle":"2022-06-30T14:00:33.329803Z","shell.execute_reply.started":"2022-06-30T14:00:33.329637Z","shell.execute_reply":"2022-06-30T14:00:33.329654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*about 64% rows have null  \nalmost rows at that have null,  have 1 or 2 cells that are null  \nand some of them have 9 cells that are null*","metadata":{}},{"cell_type":"markdown","source":"**which column have no null?**","metadata":{}},{"cell_type":"code","source":"full_col_df=train.drop(cols_with_missing,axis=1)\nfull_col_df=full_col_df.drop('row_id',axis=1)\nfull_col_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T14:00:36.719186Z","iopub.execute_input":"2022-06-30T14:00:36.720703Z","iopub.status.idle":"2022-06-30T14:00:36.934528Z","shell.execute_reply.started":"2022-06-30T14:00:36.720598Z","shell.execute_reply":"2022-06-30T14:00:36.933701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*column F_2_0 ～ F_2_24 have no null value*","metadata":{}},{"cell_type":"code","source":"full_col_df.describe().T","metadata":{"execution":{"iopub.status.busy":"2022-06-30T14:00:33.332963Z","iopub.status.idle":"2022-06-30T14:00:33.333544Z","shell.execute_reply.started":"2022-06-30T14:00:33.333241Z","shell.execute_reply":"2022-06-30T14:00:33.333269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**check heatmap**","metadata":{}},{"cell_type":"code","source":"sns.set(rc = {'figure.figsize':(15,8)})\ndf_corr=train.corr()\nsns.heatmap(df_corr, vmax=1, vmin=-1, center=0)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T14:00:33.335011Z","iopub.status.idle":"2022-06-30T14:00:33.33558Z","shell.execute_reply.started":"2022-06-30T14:00:33.335314Z","shell.execute_reply":"2022-06-30T14:00:33.335341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*column F_2_0 ～ F_2_24 looks have correlation  \nand F_4_0 ～ F_4_14 also have. it is interesting*","metadata":{}},{"cell_type":"markdown","source":"**check F_2_0 ～ F_2_24 column data histplot** ","metadata":{}},{"cell_type":"code","source":"col_list=full_col_df.columns\nncols = 3\nnrows = 8\n\nfig, axes = plt.subplots(nrows, ncols, figsize=(40,50), facecolor='#EAEAF2')\n\nfor r in range(nrows):\n    for c in range(ncols):\n        col = col_list[r*ncols+c+1]\n        sns.histplot(x=full_col_df[col], ax=axes[r, c])\n        axes[r, c].set_ylabel('')\n        axes[r, c].set_xlabel(col, fontsize=15, fontweight='bold')\n        axes[r, c].tick_params(labelsize=10, width=0.5)\n        axes[r, c].xaxis.offsetText.set_fontsize(20)\n        axes[r, c].yaxis.offsetText.set_fontsize(20)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T14:00:33.337002Z","iopub.status.idle":"2022-06-30T14:00:33.337594Z","shell.execute_reply.started":"2022-06-30T14:00:33.337335Z","shell.execute_reply":"2022-06-30T14:00:33.33737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**check other than F_2_0 ～ F_2_24 column data histplot** ","metadata":{}},{"cell_type":"code","source":"col_list=train[cols_with_missing].columns\nncols = 5\nnrows = 11\n\nfig, axes = plt.subplots(nrows, ncols, figsize=(30,45), facecolor='#EAEAF2')\n\nfor r in range(nrows):\n    for c in range(ncols):\n        col = col_list[r*ncols+c]\n        sns.histplot(x=train[col], ax=axes[r, c])\n        axes[r, c].set_ylabel('')\n        axes[r, c].set_xlabel(col, fontsize=15, fontweight='bold')\n        axes[r, c].tick_params(labelsize=10, width=0.5)\n        axes[r, c].xaxis.offsetText.set_fontsize(20)\n        axes[r, c].yaxis.offsetText.set_fontsize(20)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T14:00:33.340208Z","iopub.status.idle":"2022-06-30T14:00:33.341547Z","shell.execute_reply.started":"2022-06-30T14:00:33.341241Z","shell.execute_reply":"2022-06-30T14:00:33.341272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# prepare train data with Imputer","metadata":{}},{"cell_type":"markdown","source":"**I'll use SimpleImputer and IterativeImputer from scikit-learn**   \nhttps://scikit-learn.org/stable/modules/classes.html#module-sklearn.impute","metadata":{}},{"cell_type":"code","source":"simple_imputer = SimpleImputer(missing_values=np.nan, fill_value=None,strategy='median')\nsimple_inputed=pd.DataFrame(simple_imputer.fit_transform(train.iloc[:,1:]))\nsimple_inputed.columns=train.iloc[:,1:].columns","metadata":{"execution":{"iopub.status.busy":"2022-06-30T14:00:42.911206Z","iopub.execute_input":"2022-06-30T14:00:42.911662Z","iopub.status.idle":"2022-06-30T14:01:02.505428Z","shell.execute_reply.started":"2022-06-30T14:00:42.911622Z","shell.execute_reply":"2022-06-30T14:01:02.504102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# estimator = XGBRegressor(n_estimators=1000, \n#                          learning_rate=0.2, \n#                          max_depth=3, \n#                          random_state=42, \n#                          tree_method='gpu_hist')\n# iterative_imputer = IterativeImputer(estimator=estimator, missing_values=np.nan, max_iter=10, random_state=42)\n\n# iterative_inputed=pd.DataFrame(iterative_imputer.fit_transform(train.iloc[:,1:]))\n# iterative_inputed.columns=train.iloc[:,1:].columns\n# iterative_inputed.to_csv('iterative_inputed.csv')\n\niterative_inputed=pd.read_csv(\"../input/tps-jun22-eda-predict/iterative_inputed.csv\")\niterative_inputed.drop('Unnamed: 0',axis=1,inplace=True)\niterative_inputed['null_count']=train.isnull().sum(axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T14:01:02.507268Z","iopub.execute_input":"2022-06-30T14:01:02.507643Z","iopub.status.idle":"2022-06-30T14:01:20.133027Z","shell.execute_reply.started":"2022-06-30T14:01:02.507607Z","shell.execute_reply":"2022-06-30T14:01:20.132159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#output for next ver\niterative_inputed.to_csv('iterative_inputed.csv')","metadata":{"execution":{"iopub.status.busy":"2022-06-30T14:00:33.347616Z","iopub.status.idle":"2022-06-30T14:00:33.348145Z","shell.execute_reply.started":"2022-06-30T14:00:33.347893Z","shell.execute_reply":"2022-06-30T14:00:33.347918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# train model","metadata":{}},{"cell_type":"code","source":"df_score=pd.DataFrame()\n\nfor i in range(55):\n    \n    df_train = train.dropna(subset=[cols_with_missing[i]])\n    df_iterative=iterative_inputed.loc[df_train.index,:]\n    \n    X=df_iterative.drop(cols_with_missing[i],axis=1)\n    y=df_iterative[cols_with_missing[i]]\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y,random_state=0,test_size=0.2)\n    \n    score_list=[]\n\n    #use LGBMRegressor\n    #from this my notebook ver29,I found LGBMRegressor perform really good for columns F_4_1~F_4_14\n    #So I'll skip other columns fitting because it takes long time to fit\n    if i >39:\n        X=X.iloc[:,65:-1]\n        X_train, X_valid, y_train, y_valid = train_test_split(X, y,random_state=0,test_size=0.2)\n        rg_model = lgb.LGBMRegressor(num_leaves=200,\n                                 subsample_freq=3,\n                                 n_estimators=2000,\n                                 metric='root_mean_squared_error')\n        rg_model.fit(X_train, y_train)\n        y_pred=rg_model.predict(X_valid)\n        #print('LGBMRegressor',np.sqrt(mean_squared_error(y_valid,y_pred)))\n        score_list.append(np.sqrt(mean_squared_error(y_valid,y_pred)))\n        \n    else:\n        score_list.append(3)\n        \n    \n    #use OrthogonalMatchingPursuit\n    rg_model = OrthogonalMatchingPursuit(normalize=False)\n    rg_model.fit(X_train, y_train)\n    y_pred=rg_model.predict(X_valid)\n    #print('OrthogonalMatchingPursuit',np.sqrt(mean_squared_error(y_valid,y_pred)))\n    score_list.append(np.sqrt(mean_squared_error(y_valid,y_pred)))\n    \n    #use ElasticNet\n    rg_model = ElasticNet()\n    rg_model.fit(X_train, y_train)\n    y_pred=rg_model.predict(X_valid)\n    #print('ElasticNet',np.sqrt(mean_squared_error(y_valid,y_pred)))\n    score_list.append(np.sqrt(mean_squared_error(y_valid,y_pred)))\n\n    #use BayesianRidge\n    rg_model = linear_model.BayesianRidge()\n    rg_model.fit(X_train, y_train)\n    y_pred=rg_model.predict(X_valid)\n    #print('BayesianRidge',np.sqrt(mean_squared_error(y_valid,y_pred)))\n    score_list.append(np.sqrt(mean_squared_error(y_valid,y_pred)))    \n    \n    df_score[cols_with_missing[i]]=score_list\n\ndf_score.loc['std']=train[cols_with_missing].std()\ndf_score","metadata":{"execution":{"iopub.status.busy":"2022-06-30T14:00:33.349839Z","iopub.status.idle":"2022-06-30T14:00:33.350374Z","shell.execute_reply.started":"2022-06-30T14:00:33.350101Z","shell.execute_reply":"2022-06-30T14:00:33.350127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# compare score and search best one","metadata":{}},{"cell_type":"markdown","source":"**check best model at each columns**","metadata":{}},{"cell_type":"code","source":"a=pd.concat([df_score.idxmin(),df_score.min()],axis=1)\na['diff_std']=np.abs(a[1]-df_score.loc['std'].T)\na","metadata":{"execution":{"iopub.status.busy":"2022-06-30T14:00:33.352533Z","iopub.status.idle":"2022-06-30T14:00:33.353086Z","shell.execute_reply.started":"2022-06-30T14:00:33.352828Z","shell.execute_reply":"2022-06-30T14:00:33.352855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**it looks almost same score, OrthogonalMatchingPursuit & BayesianRidge & ElasticNet & just std**","metadata":{}},{"cell_type":"code","source":"row_list=a[0].tolist()\ndiff_list=a['diff_std'].tolist()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T14:00:33.354841Z","iopub.status.idle":"2022-06-30T14:00:33.355394Z","shell.execute_reply.started":"2022-06-30T14:00:33.35511Z","shell.execute_reply":"2022-06-30T14:00:33.355136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# train model with all data","metadata":{}},{"cell_type":"code","source":"df_score_submit=train.copy()\n\nfor i in range(55):\n    \n    y=train.dropna(subset=[cols_with_missing[i]])\n    y=y[cols_with_missing[i]]\n    X_train=iterative_inputed.loc[y.index,:]\n    X_train=X_train.drop(cols_with_missing[i],axis=1)\n    X_valid=iterative_inputed.drop(cols_with_missing[i],axis=1)\n    \n    if row_list[i]==0:\n        rg_model = lgb.LGBMRegressor(num_leaves=300,\n                                     subsample_freq=10,\n                                     n_estimators=3000,\n                                     metric='root_mean_squared_error')\n        X_train=X_train.iloc[:,65:-1]\n        X_valid=X_valid.iloc[:,65:-1]\n        rg_model.fit(X_train, y)\n        df_score_submit[cols_with_missing[i]]=rg_model.predict(X_valid)\n    else:\n        rg_model = OrthogonalMatchingPursuit(normalize=False)\n        rg_model_E = ElasticNet(alpha=0.3, l1_ratio = 0.8, max_iter=300)\n        rg_model_B = linear_model.BayesianRidge(tol = 0.0003,n_iter=300)\n        \n        rg_model.fit(X_train, y)\n        rg_model_E.fit(X_train, y)\n        rg_model_B.fit(X_train, y)\n        \n        if diff_list[i]<0.0001:\n            df_score_submit[cols_with_missing[i]]=(rg_model.predict(X_valid)\n                                                   +rg_model_E.predict(X_valid)\n                                                   +rg_model_B.predict(X_valid)\n                                                   +simple_inputed[cols_with_missing[i]])/4\n        else:\n            df_score_submit[cols_with_missing[i]]=(rg_model.predict(X_valid)\n                                                   +rg_model_E.predict(X_valid)\n                                                   +rg_model_B.predict(X_valid))/3","metadata":{"execution":{"iopub.status.busy":"2022-06-30T14:00:33.356961Z","iopub.status.idle":"2022-06-30T14:00:33.357326Z","shell.execute_reply.started":"2022-06-30T14:00:33.35715Z","shell.execute_reply":"2022-06-30T14:00:33.357166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# make prediction for submit","metadata":{}},{"cell_type":"code","source":"submission[['row','col']]=submission['row-col'].str.split('-', expand=True)\nsubmission_list=[]\nfor i in range(1000000):\n    submission_list.append(df_score_submit.loc[int(submission.loc[i,'row']),submission.loc[i,'col']])\nsubmission['value']=submission_list\nsubmission=submission.drop(['row','col'],axis=1)\nsubmission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T14:00:33.358716Z","iopub.status.idle":"2022-06-30T14:00:33.359099Z","shell.execute_reply.started":"2022-06-30T14:00:33.358906Z","shell.execute_reply":"2022-06-30T14:00:33.35893Z"},"trusted":true},"execution_count":null,"outputs":[]}]}