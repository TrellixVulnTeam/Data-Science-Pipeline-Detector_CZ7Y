{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"---\n# [Tabular Playground Series - Jun 2022][1]\n\n- The June edition of the 2022 Tabular Playground series is all about data imputation. The dataset has similarities to the [May 2022 Tabular Playground](https://www.kaggle.com/competitions/tabular-playground-series-may-2022/overview), except that there are no targets. Rather, there are missing data values in the dataset, and your task is to predict what these values should be.\n\n---\n#### **The aim of this notebook is to impute the missing values with Transformer model, which is trained on the task of application of Masked Language Modeling (MLM).**\n- **1. Conduct exploratory data analysis (EDA).**\n- **2. Converting numerical features into categorical features by binning.**\n- **3. Build a TabTransformer model.**\n- **4. Train the TabTransformer model on the MLM-like training task.**\n\n---\n**References:** Thanks to previous great lectures, codes and notebooks.\n- [ðŸ”¥ðŸ”¥[TensorFlow]TabTransformerðŸ”¥ðŸ”¥][2]\n- [Structured data learning with TabTransformer][3]\n- [Sachin's Blog Tensorflow Learning Rate Finder][4]\n- [End-to-end Masked Language Modeling with BERT][7]\n- [Customize what happens in Model.fit][8]\n- [Fine-tuning a masked language model][9]\n\n**My Previous Notebooks:**\n- [SpaceshipTitanic: EDA + TabTransformer[TensorFlow]][5]\n\n---\n### **If you find this notebook useful, please do give me an upvote. It helps me keep up my motivation.**\n#### **Also, I would appreciate it if you find any mistakes and help me correct them.**\n\n---\n[1]: https://www.kaggle.com/competitions/tabular-playground-series-jun-2022/overview\n[2]: https://www.kaggle.com/code/usharengaraju/tensorflow-tabtransformer\n[3]: https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/structured_data/ipynb/tabtransformer.ipynb\n[4]: https://sachinruk.github.io/blog/tensorflow/learning%20rate/2021/02/15/Tensorflow-Learning-Rate-Finder.html\n[5]: https://www.kaggle.com/code/masatomurakawamm/spaceshiptitanic-eda-tabtransformer-tensorflow\n\n[7]: https://keras.io/examples/nlp/masked_language_modeling/\n[8]: https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit\n[9]: https://huggingface.co/course/chapter7/3?fw=tf","metadata":{}},{"cell_type":"markdown","source":"### **Masked Language Modeling (MLM)**\n\nMLM is famous for a pre-training tasks of BERT model: transformer-based language model for Natural Language Processing (NLP). In MLM training, a part of input words is replaced with the `MASK` token. And the model predicts the masked inputs based on the contextual information from un-masked input words.\n\n<img src=\"https://ainow.ai/wp-content/uploads/2019/04/BERT-image1.png\" width=\"500\"/>\n\n---\nIn this competition, I applied MLM to missing value imputation task. \nThe methods of training a Transformer-based model (TabTransformer) on MLM-like task is as follows.\n1. The data which does not include any missing values is used for training data.\n2. A part of input data is rondomly replaced with mask token, and the original values of masked positions are target values.\n3. The model outputs each values corresponding to each input features. For the MSE loss calculation, outputs corresponding to masked inputs are considered.\n4. After training, I will use the trained model as the imputer. Missing values in all data are replaced with mask token, and the model fills them based on contextualized input feature information.","metadata":{}},{"cell_type":"markdown","source":"<h1 style=\"background:#05445E; border:0; border-radius: 12px; color:#D3D3D3\"><center>0. TABLE OF CONTENTS</center></h1>\n\n<ul class=\"list-group\" style=\"list-style-type:none;\">\n    <li><a href=\"#1\" class=\"list-group-item list-group-item-action\">1. Settings</a></li>\n    <li><a href=\"#2\" class=\"list-group-item list-group-item-action\">2. Data Loading</a></li>\n    <li><a href=\"#3\" class=\"list-group-item list-group-item-action\">3. Exploratory Data Analysis</a>\n        <ul class=\"list-group\" style=\"list-style-type:none;\">\n            <li><a href=\"#3.1\" class=\"list-group-item list-group-item-action\">3.1 Numerical Features</a>\n                <ul class=\"list-group\" style=\"list-style-type:none;\">\n                    <li><a href=\"#3.1.1\" class=\"list-group-item list-group-item-action\">3.1.1 Statistics of Numerical Features</a></li>\n                    <li><a href=\"#3.1.2\" class=\"list-group-item list-group-item-action\">3.1.2 Binning for Numerical Features</a></li>\n                </ul>\n            </li>\n            <li><a href=\"#3.2\" class=\"list-group-item list-group-item-action\">3.2 Categorical Feature</a></li>\n        </ul>\n    </li>\n    <li><a href=\"#4\" class=\"list-group-item list-group-item-action\">4. Model and Dataset</a>\n        <ul class=\"list-group\" style=\"list-style-type:none;\">\n            <li><a href=\"#4.1\" class=\"list-group-item list-group-item-action\">4.1 Dataset for MLM-like training</a></li>\n            <li><a href=\"#4.2\" class=\"list-group-item list-group-item-action\">4.2 Preprocessing Model</a></li>\n            <li><a href=\"#4.3\" class=\"list-group-item list-group-item-action\">4.3 Training Model</a></li>\n        </ul>\n    </li>\n    <li><a href=\"#5\" class=\"list-group-item list-group-item-action\">5. Model Training</a>\n        <ul class=\"list-group\" style=\"list-style-type:none;\">\n            <li><a href=\"#5.1\" class=\"list-group-item list-group-item-action\">5.1 Learning Rate Finder</a></li>\n            <li><a href=\"#5.2\" class=\"list-group-item list-group-item-action\">5.2 Model Training</a></li>\n        </ul>\n    </li>\n    <li><a href=\"#6\" class=\"list-group-item list-group-item-action\">6. Missing Value Imputation</a></li>\n</ul>\n","metadata":{}},{"cell_type":"markdown","source":"<a id =\"1\"></a><h1 style=\"background:#05445E; border:0; border-radius: 12px; color:#D3D3D3\"><center>1. Settings</center></h1>","metadata":{}},{"cell_type":"code","source":"## Import dependencies \nimport numpy as np\nimport pandas as pd\nimport scipy as sp\n\nimport matplotlib.pyplot as plt \n%matplotlib inline\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\n\nimport os\nimport pathlib\nimport gc\nimport sys\nimport re\nimport math \nimport random\nimport functools\nimport time \nimport datetime as dt\nfrom tqdm import tqdm \n\nimport sklearn\nfrom sklearn.model_selection import KFold, StratifiedKFold\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport tensorflow_addons as tfa\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint('import done!')","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-11T23:52:52.463977Z","iopub.execute_input":"2022-06-11T23:52:52.464695Z","iopub.status.idle":"2022-06-11T23:52:52.477058Z","shell.execute_reply.started":"2022-06-11T23:52:52.464658Z","shell.execute_reply":"2022-06-11T23:52:52.476118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Parameters\ndata_config = {'data_csv_path': '../input/tabular-playground-series-jun-2022/data.csv',\n               'sample_submission_path': '../input/tabular-playground-series-jun-2022/sample_submission.csv',\n              }\n\nexp_config = {'gpu': True,\n              'tf_memory_limit': True,\n              'n_bins': 64,\n              'mask_ratio': 0.2,\n              'mask_token': -100,\n              'batch_size': 512,\n              'val_size': 5_000,\n              'learning_rate': 5e-3,\n              'weight_decay': 0.0001,\n              'train_epochs': 8,\n              'checkpoint_filepath': './tmp/model/exp.ckpt',\n             }\n\nmodel_config = {'embedding_dim': 24,\n                'num_transformer_blocks': 6,\n                'num_heads': 3,\n                'tf_dropout_rates': [0., 0., 0., 0., 0., 0.,],\n                'ff_dropout_rates': [0., 0., 0., 0., 0., 0.,],\n                'mlp_dropout_rates': [0., 0.1],\n                'mlp_hidden_units_factors': [4, 4],\n               }\n\nprint('Parameters setted!')","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-11T23:52:52.47856Z","iopub.execute_input":"2022-06-11T23:52:52.479202Z","iopub.status.idle":"2022-06-11T23:52:52.491245Z","shell.execute_reply.started":"2022-06-11T23:52:52.479165Z","shell.execute_reply":"2022-06-11T23:52:52.489967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## For reproducible results    \ndef seed_all(s):\n    random.seed(s)\n    np.random.seed(s)\n    tf.random.set_seed(s)\n    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n    os.environ['PYTHONHASHSEED'] = str(s) \n    print('Seeds setted!')\n    \nglobal_seed = 42\nseed_all(global_seed)\n\n\n## Limit GPU Memory in TensorFlow\n## Because TensorFlow, by default, allocates the full amount of available GPU memory when it is launched. \nif exp_config['tf_memory_limit']:\n    physical_devices = tf.config.list_physical_devices('GPU')\n    if len(physical_devices) > 0:\n        for device in physical_devices:\n            tf.config.experimental.set_memory_growth(device, True)\n            print('{} memory growth: {}'.format(device, tf.config.experimental.get_memory_growth(device)))\n        else:\n            print(\"Not enough GPU hardware devices available\")","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-11T23:52:52.492733Z","iopub.execute_input":"2022-06-11T23:52:52.494441Z","iopub.status.idle":"2022-06-11T23:52:52.551906Z","shell.execute_reply.started":"2022-06-11T23:52:52.49441Z","shell.execute_reply":"2022-06-11T23:52:52.550862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id =\"2\"></a><h1 style=\"background:#05445E; border:0; border-radius: 12px; color:#D3D3D3\"><center>2. Data Loading</center></h1>","metadata":{}},{"cell_type":"markdown","source":"---\n### [File and Data Field Descriptions](https://www.kaggle.com/competitions/tabular-playground-series-jun-2022/data)\n\n- **data.csv** - the file includes normalized continuous data and categorical data; your task is to predict the values of the missing data.\n\n- **sample_submission.csv** - a sample submission file in the correct format; the row-col indicator corresponds to the row and column of each missing value in data.csv\n\n---\n### [Submission & Evaluation](https://www.kaggle.com/competitions/tabular-playground-series-jun-2022/overview/evaluation)\n\n- Submissions are scored on the root mean squared error (RMSE).\n\n---","metadata":{}},{"cell_type":"code","source":"## Data Loading\ndata_df = pd.read_csv(data_config['data_csv_path'])\nsubmission_df = pd.read_csv(data_config['sample_submission_path'])\n\nprint(f'train_length: {len(data_df)}')\nprint(f'submission_length: {len(submission_df)}')","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-11T23:52:52.555916Z","iopub.execute_input":"2022-06-11T23:52:52.556361Z","iopub.status.idle":"2022-06-11T23:53:05.192136Z","shell.execute_reply.started":"2022-06-11T23:52:52.556332Z","shell.execute_reply":"2022-06-11T23:53:05.191258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Null Value Check\nprint('data_df.info()'); print(data_df.info(), '\\n')\n\n## train_df Check\ndata_df.head()","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-11T23:53:05.193346Z","iopub.execute_input":"2022-06-11T23:53:05.194357Z","iopub.status.idle":"2022-06-11T23:53:05.363075Z","shell.execute_reply.started":"2022-06-11T23:53:05.194318Z","shell.execute_reply":"2022-06-11T23:53:05.362158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df.head()","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-11T23:53:05.365429Z","iopub.execute_input":"2022-06-11T23:53:05.365788Z","iopub.status.idle":"2022-06-11T23:53:05.3749Z","shell.execute_reply.started":"2022-06-11T23:53:05.36575Z","shell.execute_reply":"2022-06-11T23:53:05.373979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id =\"3\"></a><h1 style=\"background:#05445E; border:0; border-radius: 12px; color:#D3D3D3\"><center>3. Exploratory Data Analysis</center></h1>","metadata":{}},{"cell_type":"code","source":"## Duplicates check\nfeatures = list(data_df.columns)\nfeatures.remove('row_id')\n#print(f'features: {features}')\n\ndata = data_df.duplicated(subset=features)\nprint(f'Number of duplicates: {data.values.sum()}')\n\ndata_df = data_df.drop_duplicates(subset=features)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-11T23:53:05.376315Z","iopub.execute_input":"2022-06-11T23:53:05.376725Z","iopub.status.idle":"2022-06-11T23:53:08.724139Z","shell.execute_reply.started":"2022-06-11T23:53:05.376688Z","shell.execute_reply":"2022-06-11T23:53:08.721624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Check the number of NaN values per sample.\nn_null = data_df.isnull().sum(axis=1)\nplot = sns.histplot(data=n_null, bins=20, stat=\"percent\")\nplot.set_xlabel('The number of NaN values per sample')\nprint(f'The max number of NaN values per sample is {n_null.max()}.')","metadata":{"execution":{"iopub.status.busy":"2022-06-11T23:53:08.725399Z","iopub.status.idle":"2022-06-11T23:53:08.725888Z","shell.execute_reply.started":"2022-06-11T23:53:08.725644Z","shell.execute_reply":"2022-06-11T23:53:08.725667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Heatmap of correlation matrix\nfig = px.imshow(data_df.corr(),\n                color_continuous_scale='RdBu_r',\n                color_continuous_midpoint=0, \n                aspect='auto')\nfig.update_layout(height=1000, \n                  width=1000,\n                  title = \"Heatmap\",                  \n                  showlegend=False)\nfig.show()","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-11T23:53:08.727421Z","iopub.status.idle":"2022-06-11T23:53:08.728053Z","shell.execute_reply.started":"2022-06-11T23:53:08.727799Z","shell.execute_reply":"2022-06-11T23:53:08.727824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"numerical_features = ['F_1_0', 'F_1_1', 'F_1_2', 'F_1_3', 'F_1_4', 'F_1_5', 'F_1_6',\n       'F_1_7', 'F_1_8', 'F_1_9', 'F_1_10', 'F_1_11', 'F_1_12', 'F_1_13',\n       'F_1_14', 'F_3_0', 'F_3_1', 'F_3_2',\n       'F_3_3', 'F_3_4', 'F_3_5', 'F_3_6', 'F_3_7', 'F_3_8', 'F_3_9', 'F_3_10',\n       'F_3_11', 'F_3_12', 'F_3_13', 'F_3_14', 'F_3_15', 'F_3_16', 'F_3_17',\n       'F_3_18', 'F_3_19', 'F_3_20', 'F_3_21', 'F_3_22', 'F_3_23', 'F_3_24',\n       'F_4_0', 'F_4_1', 'F_4_2', 'F_4_3', 'F_4_4', 'F_4_5', 'F_4_6', 'F_4_7',\n       'F_4_8', 'F_4_9', 'F_4_10', 'F_4_11', 'F_4_12', 'F_4_13', 'F_4_14',]\n\ncategorical_features = ['F_2_0', 'F_2_1', 'F_2_2', 'F_2_3', 'F_2_4', 'F_2_5', 'F_2_6',\n       'F_2_7', 'F_2_8', 'F_2_9', 'F_2_10', 'F_2_11', 'F_2_12', 'F_2_13',\n       'F_2_14', 'F_2_15', 'F_2_16', 'F_2_17', 'F_2_18', 'F_2_19', 'F_2_20',\n       'F_2_21', 'F_2_22', 'F_2_23', 'F_2_24', ]","metadata":{"execution":{"iopub.status.busy":"2022-06-11T23:53:08.730456Z","iopub.status.idle":"2022-06-11T23:53:08.731135Z","shell.execute_reply.started":"2022-06-11T23:53:08.730874Z","shell.execute_reply":"2022-06-11T23:53:08.730897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id =\"3.1\"></a><h2 style=\"background:#75E6DA; border:0; border-radius: 12px; color:black\"><center>3.1 Numerical Features</center></h2>","metadata":{}},{"cell_type":"markdown","source":"<a id =\"3.1.1\"></a><h2 style=\"background:#D4F1F4; border:0; border-radius: 12px; color:black\"><center>3.1.1 Statistics of Numerical Features </center></h2>","metadata":{}},{"cell_type":"code","source":"data_df[features].describe().T.style.bar(subset=['mean'],)\\\n                        .background_gradient(subset=['std'], cmap='coolwarm')\\\n                        .background_gradient(subset=['50%'], cmap='coolwarm')\\\n                        .background_gradient(subset=['max'], cmap='coolwarm')\\","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-11T23:53:08.732194Z","iopub.status.idle":"2022-06-11T23:53:08.732853Z","shell.execute_reply.started":"2022-06-11T23:53:08.732612Z","shell.execute_reply":"2022-06-11T23:53:08.732635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Numerical feature distributions\nn_cols = 3\nn_rows = int(np.ceil(len(numerical_features) / n_cols))\n\nfig, axes = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(15, 60))\n\nbins = exp_config['n_bins']\nfor i, column in enumerate(numerical_features):\n    q, mod = divmod(i, n_cols)\n    sns.histplot(x=column, data=data_df, ax=axes[q][mod], bins=bins, stat=\"percent\", legend=True)\n    axes[q][mod].set_title(f'Distribution of {numerical_features[i]}',size=15)\n    axes[q][mod].set(xlabel=None)\n    \n#fig.suptitle('Numerical feature distributions', fontsize=20)\nfig.tight_layout()\nplt.show()","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-11T23:53:08.73415Z","iopub.status.idle":"2022-06-11T23:53:08.734731Z","shell.execute_reply.started":"2022-06-11T23:53:08.73449Z","shell.execute_reply":"2022-06-11T23:53:08.734515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id =\"3.1.2\"></a><h2 style=\"background:#D4F1F4; border:0; border-radius: 12px; color:black\"><center>3.1.2 Binning for Numerical Features </center></h2>","metadata":{}},{"cell_type":"code","source":"def binning_numerical(dataframe, numerical, n_bins=20):\n    df = dataframe.copy()\n    for nc in numerical:\n        df[nc] = pd.cut(df[nc], bins=n_bins, labels=False)\n    return df\n\nn_bins = exp_config['n_bins']\ndata_bin_df = binning_numerical(data_df, numerical_features, n_bins=n_bins)","metadata":{"execution":{"iopub.status.busy":"2022-06-11T23:53:08.736132Z","iopub.status.idle":"2022-06-11T23:53:08.737052Z","shell.execute_reply.started":"2022-06-11T23:53:08.736784Z","shell.execute_reply":"2022-06-11T23:53:08.736807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Numerical feature distributions after binning\nn_cols = 3\nn_rows = int(np.ceil(len(numerical_features) / n_cols))\n\nfig, axes = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(15, 60))\n\nbins = exp_config['n_bins']\nfor i, column in enumerate(numerical_features):\n    q, mod = divmod(i, n_cols)\n    sns.histplot(x=column, data=data_bin_df, ax=axes[q][mod], bins=bins, stat=\"percent\", legend=True)\n    axes[q][mod].set_title(f'Distribution of {numerical_features[i]}',size=15)\n    axes[q][mod].set(xlabel=None)\n    \n#fig.suptitle('Numerical feature distributions after binning', fontsize=20)\nfig.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-11T23:53:08.738155Z","iopub.status.idle":"2022-06-11T23:53:08.73903Z","shell.execute_reply.started":"2022-06-11T23:53:08.738777Z","shell.execute_reply":"2022-06-11T23:53:08.7388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id =\"3.2\"></a><h2 style=\"background:#75E6DA; border:0; border-radius: 12px; color:black\"><center>3.2 Categorical Features</center></h2>","metadata":{}},{"cell_type":"code","source":"## The number of unique values in each categorical features\ndata_df[categorical_features].nunique()","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-11T23:53:08.740096Z","iopub.status.idle":"2022-06-11T23:53:08.74096Z","shell.execute_reply.started":"2022-06-11T23:53:08.740674Z","shell.execute_reply":"2022-06-11T23:53:08.740699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Categorical feature distributions\nn_cols = 3\nn_rows = int(np.ceil(len(categorical_features) / n_cols))\n\nfig, axes = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(15, 30))\n\nbins = 60\nfor i, column in enumerate(categorical_features):\n    q, mod = divmod(i, n_cols)\n    sns.histplot(x=column, data=data_bin_df, ax=axes[q][mod], bins=bins, stat=\"percent\", legend=True)\n    axes[q][mod].set_title(f'Distribution of {categorical_features[i]}',size=15)\n    axes[q][mod].set(xlabel=None)\n    \n#fig.suptitle('Categorical feature distributions', fontsize=20)\nfig.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-11T23:53:08.742288Z","iopub.status.idle":"2022-06-11T23:53:08.743353Z","shell.execute_reply.started":"2022-06-11T23:53:08.743086Z","shell.execute_reply":"2022-06-11T23:53:08.743113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id =\"4\"></a><h1 style=\"background:#05445E; border:0; border-radius: 12px; color:#D3D3D3\"><center>4. Model and Dataset</center></h1>","metadata":{}},{"cell_type":"markdown","source":"<a id =\"4.1\"></a><h2 style=\"background:#75E6DA; border:0; border-radius: 12px; color:black\"><center>4.1 Dataset for MLM-like training </center></h2>","metadata":{}},{"cell_type":"code","source":"## After binning, all features are categorical.\ncategorical_features = ['F_1_0', 'F_1_1', 'F_1_2', 'F_1_3', 'F_1_4', 'F_1_5', 'F_1_6',\n       'F_1_7', 'F_1_8', 'F_1_9', 'F_1_10', 'F_1_11', 'F_1_12', 'F_1_13',\n       'F_1_14', 'F_2_0', 'F_2_1', 'F_2_2', 'F_2_3', 'F_2_4', 'F_2_5', 'F_2_6',\n       'F_2_7', 'F_2_8', 'F_2_9', 'F_2_10', 'F_2_11', 'F_2_12', 'F_2_13',\n       'F_2_14', 'F_2_15', 'F_2_16', 'F_2_17', 'F_2_18', 'F_2_19', 'F_2_20',\n       'F_2_21', 'F_2_22', 'F_2_23', 'F_2_24', 'F_3_0', 'F_3_1', 'F_3_2',\n       'F_3_3', 'F_3_4', 'F_3_5', 'F_3_6', 'F_3_7', 'F_3_8', 'F_3_9', 'F_3_10',\n       'F_3_11', 'F_3_12', 'F_3_13', 'F_3_14', 'F_3_15', 'F_3_16', 'F_3_17',\n       'F_3_18', 'F_3_19', 'F_3_20', 'F_3_21', 'F_3_22', 'F_3_23', 'F_3_24',\n       'F_4_0', 'F_4_1', 'F_4_2', 'F_4_3', 'F_4_4', 'F_4_5', 'F_4_6', 'F_4_7',\n       'F_4_8', 'F_4_9', 'F_4_10', 'F_4_11', 'F_4_12', 'F_4_13', 'F_4_14']\nprint(len(categorical_features))","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-11T23:53:08.744484Z","iopub.status.idle":"2022-06-11T23:53:08.745389Z","shell.execute_reply.started":"2022-06-11T23:53:08.745158Z","shell.execute_reply":"2022-06-11T23:53:08.745181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Drop NaN-containing data\nnonull_data = data_df.dropna(how='any', axis=0)\nprint(nonull_data.shape)\n\nnonull_bin_data = data_bin_df.dropna(how='any', axis=0)\nnonull_bin_data = nonull_bin_data.astype('int32')\nprint(nonull_bin_data.shape)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-11T23:53:08.746445Z","iopub.status.idle":"2022-06-11T23:53:08.74734Z","shell.execute_reply.started":"2022-06-11T23:53:08.747107Z","shell.execute_reply":"2022-06-11T23:53:08.74713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nonull_data[features].describe().T.style.bar(subset=['mean'],)\\\n                        .background_gradient(subset=['std'], cmap='coolwarm')\\\n                        .background_gradient(subset=['50%'], cmap='coolwarm')\\\n                        .background_gradient(subset=['max'], cmap='coolwarm')\\","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-11T23:53:08.748394Z","iopub.status.idle":"2022-06-11T23:53:08.749277Z","shell.execute_reply.started":"2022-06-11T23:53:08.749039Z","shell.execute_reply":"2022-06-11T23:53:08.749062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Train valid split (Hold-out)\nval_size = exp_config['val_size']\n\nnonull_valid_data = nonull_bin_data[:val_size]\nnonull_train_data = nonull_bin_data[val_size:]\n\nnonull_valid_target = nonull_data[:val_size]\nnonull_train_target = nonull_data[val_size:]","metadata":{"execution":{"iopub.status.busy":"2022-06-11T23:53:08.750602Z","iopub.status.idle":"2022-06-11T23:53:08.751509Z","shell.execute_reply.started":"2022-06-11T23:53:08.75122Z","shell.execute_reply":"2022-06-11T23:53:08.751247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_train_dataset(data_df,\n                         target_df,\n                         categorical_features,\n                         mask_ratio=0.1,\n                         mask_token=-100,\n                         batch_size=128, \n                         shuffle=True,\n                         repeat=False,\n                         drop_remainder=False,\n                         return_sample_weight=True):\n    \n    data_df = data_df.drop(['row_id'], axis=1)\n    data_values = data_df.values\n    \n    target_df = target_df.drop(['row_id'], axis=1)\n    target = target_df.values\n    \n    ## mask array has '1' elements by the percentage of `mask_ratio`.\n    mask = np.random.random_sample(size=target.shape) \n    mask = np.where(mask < mask_ratio, 1, 0) ## Masked parts are 1s, and others are 0s. \n    \n    ## Masked parts on input data are replaced with the mask_token.\n    train_data = np.where(mask==1, mask_token, data_values)  \n    \n    train_data = train_data.astype(np.int32)\n    target = target.astype(np.float32)\n    \n    data = {}\n    for i, cf in enumerate(categorical_features):\n        data[cf] = train_data[:, i]\n        \n    if return_sample_weight:\n        sample_weight = mask.astype(np.float32) ## Only masked parts are considered in loss calculation.\n        sample_weight = np.expand_dims(sample_weight, -1)\n        ds = tf.data.Dataset.from_tensor_slices((data, target, sample_weight))\n    else:\n        ds = tf.data.Dataset.from_tensor_slices((data, target))\n    \n    if shuffle:\n        if len(data_df) < 100000:\n            buffer_size = len(data_df)\n        else:\n            buffer_size = 100000\n        ds = ds.shuffle(buffer_size=buffer_size)\n    if repeat:\n        ds = ds.repeat()\n    ds = ds.batch(batch_size, drop_remainder=drop_remainder)\n    ds = ds.prefetch(batch_size)\n    \n    return ds","metadata":{"execution":{"iopub.status.busy":"2022-06-11T23:53:08.752878Z","iopub.status.idle":"2022-06-11T23:53:08.753816Z","shell.execute_reply.started":"2022-06-11T23:53:08.753582Z","shell.execute_reply":"2022-06-11T23:53:08.753605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Create datasets\nmask_ratio = exp_config['mask_ratio']\nbatch_size = exp_config['batch_size']\nmask_token = exp_config['mask_token']\n\ntrain_ds = create_train_dataset(nonull_train_data, \n                                nonull_train_target,\n                                categorical_features,\n                                mask_ratio=mask_ratio,\n                                mask_token=mask_token,\n                                batch_size=batch_size,\n                                shuffle=True,\n                                repeat=False,\n                                drop_remainder=True,\n                                return_sample_weight=True)\n\nvalid_ds = create_train_dataset(nonull_valid_data,\n                                nonull_valid_target,\n                                categorical_features,\n                                mask_ratio=mask_ratio,\n                                mask_token=mask_token,\n                                batch_size=batch_size,\n                                shuffle=False,\n                                repeat=False,\n                                drop_remainder=True,\n                                return_sample_weight=True)\n\nexample_data, example_labels, example_sample_weight = next(train_ds.take(1).as_numpy_iterator())\n\nfor key in example_data:\n    print(f'{key}, shape:{example_data[key].shape}, {example_data[key].dtype}')\n    \nprint(f'labels shape: {example_labels.shape}')\nprint(f'sample_wegihts shape: {example_sample_weight.shape}')","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-11T23:53:08.754911Z","iopub.status.idle":"2022-06-11T23:53:08.755825Z","shell.execute_reply.started":"2022-06-11T23:53:08.755594Z","shell.execute_reply":"2022-06-11T23:53:08.755617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id =\"4.2\"></a><h2 style=\"background:#75E6DA; border:0; border-radius: 12px; color:black\"><center>4.2 Preprocessing Model </center></h2>","metadata":{}},{"cell_type":"code","source":"def create_preprocessing_model(categorical, df, mask_token=-100):\n    ## Create input layers\n    preprocess_inputs = {}\n    for c in categorical:\n        preprocess_inputs[c] = layers.Input(shape=(1,),\n                                             dtype=np.int32)\n    \n    ## Create preprocess layers        \n    lookup_layers = {}\n    for c in categorical:\n        lookup_layers[c] = layers.IntegerLookup(vocabulary=df[c].unique(),\n                                            mask_token=mask_token,\n                                            output_mode='int')\n                \n    ## Create outputs\n    preprocess_outputs = {}\n    for c in categorical:\n        preprocess_outputs[c] = lookup_layers[c](preprocess_inputs[c])\n            \n    ## Create model\n    preprocessing_model = tf.keras.Model(preprocess_inputs,\n                                         preprocess_outputs)\n    \n    return preprocessing_model, lookup_layers","metadata":{"execution":{"iopub.status.busy":"2022-06-11T23:53:08.756949Z","iopub.status.idle":"2022-06-11T23:53:08.758109Z","shell.execute_reply.started":"2022-06-11T23:53:08.757844Z","shell.execute_reply":"2022-06-11T23:53:08.757869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Create preprocessing model\npreprocessing_model, lookup_layers = create_preprocessing_model(\n    categorical_features, nonull_train_data, mask_token=mask_token)\n\n## Apply the preprocessing model in tf.data.Dataset.map\ntrain_ds = train_ds.map(lambda x, y, sw: (preprocessing_model(x), y, sw),\n                        num_parallel_calls=tf.data.AUTOTUNE)\nvalid_ds = valid_ds.map(lambda x, y, sw: (preprocessing_model(x), y, sw),\n                        num_parallel_calls=tf.data.AUTOTUNE)\n\n## Display a preprocessed input sample\nexample_data = next(train_ds.take(1).as_numpy_iterator())[0]\nfor key in example_data:\n    print(f'{key}, shape: {example_data[key].shape}, {example_data[key].dtype}')","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-11T23:53:08.759367Z","iopub.status.idle":"2022-06-11T23:53:08.760134Z","shell.execute_reply.started":"2022-06-11T23:53:08.759887Z","shell.execute_reply":"2022-06-11T23:53:08.759911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id =\"4.3\"></a><h2 style=\"background:#75E6DA; border:0; border-radius: 12px; color:black\"><center>4.3 Training Model </center></h2>","metadata":{}},{"cell_type":"markdown","source":"### Tab Transformer\n\nThe TabTransformer architecture works as follows:\n\n- All the categorical features are encoded as embeddings, using the same embedding_dims. This means that each value in each categorical feature will have its own embedding vector.\n\n- A column embedding, one embedding vector for each categorical feature, is added (point-wise) to the categorical feature embedding (*Note*: We will prepare each embedding layers for each categorical features, thus, we will not use the column embeddings this time).\n\n- The embedded categorical features are fed into a stack of Transformer blocks. Each Transformer block consists of a multi-head self-attention layer followed by a feed-forward layer.\n\n- The outputs of the final Transformer layer, which are the contextual embeddings of the categorical features, are concatenated with the input numerical features, and fed into a final MLP block. (*Note*: This time, there are no numerical features. And I don't concatenate the contextualized features because I need each outputs corresponding to each inputs.)\n\n<img src=\"https://raw.githubusercontent.com/keras-team/keras-io/master/examples/structured_data/img/tabtransformer/tabtransformer.png\" width=\"500\"/>\n","metadata":{}},{"cell_type":"code","source":"def create_training_model(categorical, lookup_layers, \n                          tf_dropout_rates, \n                          ff_dropout_rates,\n                          mlp_hidden_units_factors=[2, 1],\n                          mlp_dropout_rates=[0., 0.],\n                          embedding_dim=12,\n                          num_transformer_blocks=6, \n                          num_heads=3,):\n    \n    ## Create input layers\n    model_inputs = {key: layers.Input(shape=(1,),\n                                      dtype='int64') for key in categorical}\n    \n    ## Create Embeddings for categorical features\n    encoded_feature_list = []\n    for key in model_inputs:\n        embedding = layers.Embedding(input_dim=lookup_layers[key].vocabulary_size(),\n                                     output_dim=embedding_dim)\n        encoded_feature = embedding(model_inputs[key])\n        encoded_feature_list.append(encoded_feature)\n    encoded_features = tf.concat(encoded_feature_list, axis=1)\n    \n    for block_idx in range(num_transformer_blocks):\n        ## Create a multi-head attention layer\n        attention_output = layers.MultiHeadAttention(\n            num_heads=num_heads,\n            key_dim=embedding_dim,\n            dropout=tf_dropout_rates[block_idx],\n            name=f'multi-head_attention_{block_idx}'\n        )(encoded_features, encoded_features)\n        ## Skip connection 1\n        x = layers.Add(\n            name=f'skip_connection1_{block_idx}'\n        )([attention_output, encoded_features])\n        ## Layer normalization 1\n        x = layers.LayerNormalization(\n            name=f'layer_norm1_{block_idx}', \n            epsilon=1e-6\n        )(x)\n        ## Feedforward\n        feedforward_output = keras.Sequential([\n            layers.Dense(embedding_dim, activation=keras.activations.gelu),\n            layers.Dropout(ff_dropout_rates[block_idx]),\n        ], name=f'feedforward_{block_idx}'\n        )(x)\n        ## Skip_connection 2\n        x = layers.Add(\n            name=f'skip_connection2_{block_idx}'\n        )([feedforward_output, x])\n        ## Layer normalization 2\n        encoded_features = layers.LayerNormalization(\n            name=f'layer_norm2_{block_idx}', \n            epsilon=1e-6\n        )(x)\n        \n    mlp_layers = []\n    mlp_hidden_units = [\n        int(factor * encoded_features.shape[-1]) for factor in mlp_hidden_units_factors]\n    \n    for i, units in enumerate(mlp_hidden_units):\n        mlp_layers.append(layers.BatchNormalization())\n        mlp_layers.append(layers.Dense(units,\n                                       activation=keras.activations.selu))\n        mlp_layers.append(layers.Dropout(mlp_dropout_rates[i]))\n    mlp_layers.append(layers.Dense(1, activation=None))\n    model_outputs = keras.Sequential(mlp_layers, name='MLP')(encoded_features)\n    model_outputs = tf.squeeze(model_outputs)\n    \n    ## Create model\n    training_model = keras.Model(inputs=model_inputs,\n                                 outputs=model_outputs)\n    \n    return training_model","metadata":{"execution":{"iopub.status.busy":"2022-06-11T23:53:08.761579Z","iopub.status.idle":"2022-06-11T23:53:08.762226Z","shell.execute_reply.started":"2022-06-11T23:53:08.761992Z","shell.execute_reply":"2022-06-11T23:53:08.762015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Settings for TabTransformer\ntf_dropout_rates = model_config['tf_dropout_rates']\nff_dropout_rates = model_config['ff_dropout_rates']\nembedding_dim = model_config['embedding_dim']\nnum_transformer_blocks = model_config['num_transformer_blocks']\nnum_heads = model_config['num_heads']\nmlp_dropout_rates = model_config['mlp_dropout_rates']\nmlp_hidden_units_factors = model_config['mlp_hidden_units_factors']\n\n## Create TabTransformer\ntraining_model = create_training_model(categorical_features,\n                                       lookup_layers,\n                                       tf_dropout_rates=tf_dropout_rates, \n                                       ff_dropout_rates=ff_dropout_rates,\n                                       mlp_hidden_units_factors=mlp_hidden_units_factors,\n                                       mlp_dropout_rates=mlp_dropout_rates,\n                                       embedding_dim=embedding_dim,\n                                       num_transformer_blocks=num_transformer_blocks,\n                                       num_heads=num_heads,\n                                       )","metadata":{"execution":{"iopub.status.busy":"2022-06-11T23:53:08.76358Z","iopub.status.idle":"2022-06-11T23:53:08.764486Z","shell.execute_reply.started":"2022-06-11T23:53:08.764251Z","shell.execute_reply":"2022-06-11T23:53:08.764274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## model compile and build\n\n## Settings for Training\nepochs = exp_config['train_epochs']\nbatch_size = exp_config['batch_size']\nsteps_per_epoch = len(nonull_train_data)//batch_size \n\n## Model compile\nlearning_rate = exp_config['learning_rate']\nweight_decay = exp_config['weight_decay']\n\nlearning_schedule = tf.keras.optimizers.schedules.CosineDecay(\n    initial_learning_rate=learning_rate,\n    decay_steps=epochs*steps_per_epoch, \n    alpha=0.0)\n\noptimizer = tfa.optimizers.AdamW(\n    learning_rate=learning_schedule,\n    weight_decay=weight_decay)\n\nloss_fn = keras.losses.MeanSquaredError()\n\ntraining_model.compile(optimizer=optimizer,\n                       loss=loss_fn,\n                       metrics=[keras.metrics.RootMeanSquaredError(),\n                                keras.metrics.MeanAbsoluteError()])\n\ntraining_model.summary()","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-11T23:53:08.765606Z","iopub.status.idle":"2022-06-11T23:53:08.766366Z","shell.execute_reply.started":"2022-06-11T23:53:08.766127Z","shell.execute_reply":"2022-06-11T23:53:08.766153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MaskeImputationModel(tf.keras.Model):\n    def train_step(self, inputs):\n        if len(inputs) == 3:\n            features, labels, sample_weight = inputs\n        else:\n            features, labels = inputs\n            sample_weight = None\n            \n        with tf.GradientTape() as tape:\n            predictions = self(features, training=True)\n            loss = self.compiled_loss(labels, predictions, regularization_losses=self.losses,\n                                      sample_weight=sample_weight) ## the loss function is configured in `compile()`\n            \n        ## Compute gradients\n        trainable_vars = self.trainable_variables\n        gradients = tape.gradient(loss, trainable_vars)\n        \n        ## Update weights\n        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n        \n        ## Update metrics\n        self.compiled_metrics.update_state(labels, predictions)\n        \n        return {m.name: m.result() for m in self.metrics}","metadata":{"execution":{"iopub.status.busy":"2022-06-11T23:53:08.76882Z","iopub.status.idle":"2022-06-11T23:53:08.769498Z","shell.execute_reply.started":"2022-06-11T23:53:08.769264Z","shell.execute_reply":"2022-06-11T23:53:08.769287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Create the training model which can receive `sample_weight` argument.\nmasked_imputation_inputs = training_model.input\nmasked_imputation_outputs = training_model(masked_imputation_inputs)\n\ntraining_model = MaskeImputationModel(inputs=masked_imputation_inputs,\n                                      outputs=masked_imputation_outputs)","metadata":{"execution":{"iopub.status.busy":"2022-06-11T23:53:08.770755Z","iopub.status.idle":"2022-06-11T23:53:08.771429Z","shell.execute_reply.started":"2022-06-11T23:53:08.771193Z","shell.execute_reply":"2022-06-11T23:53:08.771215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id =\"5\"></a><h1 style=\"background:#05445E; border:0; border-radius: 12px; color:#D3D3D3\"><center>5. Model Training</center></h1>","metadata":{}},{"cell_type":"markdown","source":"<a id =\"5.1\"></a><h2 style=\"background:#75E6DA; border:0; border-radius: 12px; color:black\"><center>5.1 Learning Rate Finder </center></h2>","metadata":{}},{"cell_type":"code","source":"class LRFind(tf.keras.callbacks.Callback):\n    def __init__(self, min_lr, max_lr, n_rounds):\n        self.min_lr = min_lr\n        self.max_lr = max_lr\n        self.step_up = tf.constant((max_lr / min_lr) ** (1 / n_rounds))\n        self.lrs = []\n        self.losses = []\n        \n    def on_train_begin(self, logs=None):\n        self.weights = self.model.get_weights()\n        self.model.optimizer.lr = self.min_lr\n        \n    def on_train_batch_end(self, batch, logs=None):\n        self.lrs.append(self.model.optimizer.lr.numpy())\n        self.losses.append(logs['loss'])\n        self.model.optimizer.lr = self.model.optimizer.lr * self.step_up\n        if self.model.optimizer.lr > self.max_lr:\n            self.model.stop_training = True \n    \n    def on_train_end(self, logs=None):\n        self.model.set_weights(self.weights)","metadata":{"execution":{"iopub.status.busy":"2022-06-11T23:53:08.772671Z","iopub.status.idle":"2022-06-11T23:53:08.773367Z","shell.execute_reply.started":"2022-06-11T23:53:08.77313Z","shell.execute_reply":"2022-06-11T23:53:08.773154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"min_lr = 1e-6\nmax_lr = 1e-1\nlr_find_epochs = 1\nlr_find_steps = 100\nlr_find_batch_size = 512\n\nlr_find = LRFind(min_lr, max_lr, lr_find_steps)\n\n## Model compile\nlearning_rate = exp_config['learning_rate']\nweight_decay = exp_config['weight_decay']\n\nlearning_schedule = tf.keras.optimizers.schedules.CosineDecay(\n    initial_learning_rate=learning_rate,\n    decay_steps=epochs*steps_per_epoch, \n    alpha=0.0)\n\noptimizer = tfa.optimizers.AdamW(\n    learning_rate=learning_schedule,\n    weight_decay=weight_decay)\n\nloss_fn = keras.losses.MeanSquaredError()\n\n## LR Find\ntraining_model.compile(optimizer=optimizer,\n                       loss=loss_fn,\n                       metrics=[keras.metrics.RootMeanSquaredError(),\n                                keras.metrics.MeanAbsoluteError()])\n\ntraining_model.fit(train_ds,\n                   steps_per_epoch=lr_find_steps,\n                   epochs=lr_find_epochs,\n                   callbacks=[lr_find])\n\nplt.plot(lr_find.lrs, lr_find.losses)\nplt.xscale('log')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-11T23:53:08.774605Z","iopub.status.idle":"2022-06-11T23:53:08.775277Z","shell.execute_reply.started":"2022-06-11T23:53:08.775042Z","shell.execute_reply":"2022-06-11T23:53:08.775065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id =\"5.2\"></a><h2 style=\"background:#75E6DA; border:0; border-radius: 12px; color:black\"><center>5.2 Model Training </center></h2>","metadata":{}},{"cell_type":"code","source":"def model_training(training_model, preprocessing_model, \n                   data_df, target_df, epochs, \n                   val_size=10000, batch_size=256):\n    train_losses = []\n    valid_losses = []\n    for _ in range(epochs):\n        ## Create train and valid datasets\n        random_idx = np.random.permutation(np.arange(len(nonull_data)))\n        \n        valid_data = data_df.iloc[random_idx[:val_size]]\n        train_data = data_df.iloc[random_idx[val_size:]]\n        \n        valid_target = target_df.iloc[random_idx[:val_size]]\n        train_target = target_df.iloc[random_idx[val_size:]]\n        \n        train_ds = create_train_dataset(train_data, \n                                        train_target,\n                                        categorical_features,\n                                        mask_ratio=mask_ratio,\n                                        mask_token=mask_token,\n                                        batch_size=batch_size,\n                                        shuffle=False,\n                                        repeat=False,\n                                        drop_remainder=False,\n                                        return_sample_weight=True)\n        \n        train_ds = train_ds.map(lambda x, y, sw: (preprocessing_model(x), y, sw),\n                                num_parallel_calls=tf.data.AUTOTUNE)\n        \n        valid_ds = create_train_dataset(valid_data, \n                                        valid_target,\n                                        categorical_features,\n                                        mask_ratio=mask_ratio,\n                                        mask_token=mask_token,\n                                        batch_size=batch_size,\n                                        shuffle=False,\n                                        repeat=False,\n                                        drop_remainder=False,\n                                        return_sample_weight=True)\n        \n        valid_ds = valid_ds.map(lambda x, y, sw: (preprocessing_model(x), y, sw),\n                                num_parallel_calls=tf.data.AUTOTUNE)\n        \n        ## Model training\n        history = training_model.fit(train_ds,\n                                     epochs=1,\n                                     shuffle=False,\n                                     validation_data=valid_ds,\n                                    )\n        \n        train_losses.append(history.history['loss'])\n        valid_losses.append(history.history['val_loss'])\n        \n        gc.collect()\n        \n    return train_losses, valid_losses","metadata":{"execution":{"iopub.status.busy":"2022-06-11T23:53:08.776575Z","iopub.status.idle":"2022-06-11T23:53:08.77727Z","shell.execute_reply.started":"2022-06-11T23:53:08.777033Z","shell.execute_reply":"2022-06-11T23:53:08.777057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Settings for Training\nepochs = exp_config['train_epochs']\nval_size = exp_config['val_size']\nbatch_size = exp_config['batch_size']\nsteps_per_epoch = len(nonull_train_data)//batch_size \n\n## Re-construct the model\ntraining_model_config = training_model.get_config()\ntraining_model = tf.keras.Model.from_config(training_model_config)\n\n## Model compile\nlearning_rate = exp_config['learning_rate']\nweight_decay = exp_config['weight_decay']\n\nlearning_schedule = tf.keras.optimizers.schedules.CosineDecay(\n    initial_learning_rate=learning_rate,\n    decay_steps=epochs*steps_per_epoch, \n    alpha=0.0)\n\noptimizer = tfa.optimizers.AdamW(\n    learning_rate=learning_schedule,\n    weight_decay=weight_decay)\n\nloss_fn = keras.losses.MeanSquaredError()\n\ntraining_model.compile(optimizer=optimizer,\n                       loss=loss_fn,\n                       metrics=[keras.metrics.RootMeanSquaredError(),\n                                keras.metrics.MeanAbsoluteError()])\n\n## Model training\ntrain_losses, valid_losses = model_training(training_model, \n                                            preprocessing_model, \n                                            data_df=nonull_bin_data,\n                                            target_df=nonull_data,\n                                            epochs=epochs, \n                                            val_size=val_size,\n                                            batch_size=batch_size)\n\n## Plot the train and valid losses\nplt.figure(figsize=(7, 5))\nplt.plot(np.arange(epochs), train_losses, label='Train Loss')\nplt.plot(np.arange(epochs), valid_losses, label='Valid Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-11T23:53:08.778494Z","iopub.status.idle":"2022-06-11T23:53:08.779181Z","shell.execute_reply.started":"2022-06-11T23:53:08.778945Z","shell.execute_reply":"2022-06-11T23:53:08.778968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id =\"6\"></a><h1 style=\"background:#05445E; border:0; border-radius: 12px; color:#D3D3D3\"><center>6. Missing Value Imputation</center></h1>","metadata":{}},{"cell_type":"code","source":"## Inference_model = preprocessing_model + training_model\nimpute_inputs = preprocessing_model.input\nimpute_outputs = training_model(preprocessing_model(impute_inputs))\nimpute_model = tf.keras.Model(inputs=impute_inputs,\n                              outputs=impute_outputs)","metadata":{"execution":{"iopub.status.busy":"2022-06-11T23:53:08.780397Z","iopub.status.idle":"2022-06-11T23:53:08.781101Z","shell.execute_reply.started":"2022-06-11T23:53:08.780848Z","shell.execute_reply":"2022-06-11T23:53:08.780872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_test_dataset(df,\n                        categorical_features,\n                        mask_token=-100,\n                        batch_size=256, \n                        drop_remainder=False):\n    \n    df = df.drop(['row_id'], axis=1)\n    df = df.fillna(mask_token)\n    df = df.astype(np.int32)\n    \n    train_data = df.values\n    \n    data = {}\n    for i, cf in enumerate(categorical_features):\n        data[cf] = train_data[:, i]\n    \n    ds = tf.data.Dataset.from_tensor_slices(data)\n    ds = ds.batch(batch_size, drop_remainder=drop_remainder)\n    ds = ds.prefetch(batch_size)\n    \n    return ds","metadata":{"execution":{"iopub.status.busy":"2022-06-11T23:53:08.782307Z","iopub.status.idle":"2022-06-11T23:53:08.782994Z","shell.execute_reply.started":"2022-06-11T23:53:08.782739Z","shell.execute_reply":"2022-06-11T23:53:08.782762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Test Dataset\ntest_ds = create_test_dataset(data_bin_df,\n                              categorical_features,\n                              batch_size=batch_size,\n                              drop_remainder=False)","metadata":{"execution":{"iopub.status.busy":"2022-06-11T23:53:08.784211Z","iopub.status.idle":"2022-06-11T23:53:08.784941Z","shell.execute_reply.started":"2022-06-11T23:53:08.784677Z","shell.execute_reply":"2022-06-11T23:53:08.784702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Inference and submission\npreds = impute_model.predict(test_ds)\n\ndef imputation_extract(row_col, categorical, preds):\n    row = int(row_col.split('-')[0])\n    col = categorical_features.index(row_col.split('-')[1])\n    return preds[row, col]\n\nimputation_extract = functools.partial(imputation_extract,\n                                       categorical=categorical_features,\n                                       preds=preds)\n    \nsubmission_df['value'] = submission_df['row-col'].map(imputation_extract)\nsubmission_df.to_csv('submission.csv', index=False)\nsubmission_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-11T23:53:08.786154Z","iopub.status.idle":"2022-06-11T23:53:08.78682Z","shell.execute_reply.started":"2022-06-11T23:53:08.786583Z","shell.execute_reply":"2022-06-11T23:53:08.786606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n**I think this MLM-lile imputation pre-training task could also be useful for other tabular regression or classification problems.**\n\n---","metadata":{}}]}