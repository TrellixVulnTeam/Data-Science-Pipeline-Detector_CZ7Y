{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import MaxNLocator\nfrom matplotlib.colors import ListedColormap\nimport seaborn as sns\nfrom cycler import cycler\nfrom IPython.display import display\nimport datetime\nimport math\nimport random\nfrom pathlib import Path\nfrom tqdm import tqdm\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, LearningRateScheduler, EarlyStopping\nfrom tensorflow.keras.layers import Dense, Input, InputLayer, Add, Concatenate, Dropout, BatchNormalization\nfrom tensorflow.keras.utils import plot_model\n\nplt.rcParams['axes.prop_cycle'] = cycler(color=['#ffd700'] +\n                                         plt.rcParams['axes.prop_cycle'].by_key()['color'][1:])","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-29T11:53:08.649168Z","iopub.execute_input":"2022-06-29T11:53:08.650322Z","iopub.status.idle":"2022-06-29T11:53:16.36256Z","shell.execute_reply.started":"2022-06-29T11:53:08.650178Z","shell.execute_reply":"2022-06-29T11:53:16.361571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot training history\ndef plot_history(history, *, n_epochs=None, plot_lr=False, title=None, bottom=None, top=None):\n    \"\"\"Plot (the last n_epochs epochs of) the training history\n    \n    Plots loss and optionally val_loss and lr.\"\"\"\n    plt.figure(figsize=(15, 6))\n    from_epoch = 0 if n_epochs is None else max(len(history['loss']) - n_epochs, 0)\n    \n    # Plot training and validation losses\n    plt.plot(np.arange(from_epoch, len(history['loss'])), history['loss'][from_epoch:], label='Training loss')\n    try:\n        plt.plot(np.arange(from_epoch, len(history['loss'])), history['val_loss'][from_epoch:], label='Validation loss')\n        best_epoch = np.argmin(np.array(history['val_loss']))\n        best_val_loss = history['val_loss'][best_epoch]\n        if best_epoch >= from_epoch:\n            plt.scatter([best_epoch], [best_val_loss], c='r', label=f'Best val_loss = {best_val_loss:.5f}')\n        if best_epoch > 0:\n            almost_epoch = np.argmin(np.array(history['val_loss'])[:best_epoch])\n            almost_val_loss = history['val_loss'][almost_epoch]\n            if almost_epoch >= from_epoch:\n                plt.scatter([almost_epoch], [almost_val_loss], c='orange', label='Second best val_loss')\n    except KeyError:\n        pass\n    if bottom is not None: plt.ylim(bottom=bottom)\n    if top is not None: plt.ylim(top=top)\n    plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend(loc='lower left')\n    if title is not None: plt.title(title)\n        \n    # Plot learning rate\n    if plot_lr and 'lr' in history:\n        ax2 = plt.gca().twinx()\n        ax2.plot(np.arange(from_epoch, len(history['lr'])), np.array(history['lr'][from_epoch:]), color='g', label='Learning rate')\n        ax2.set_ylabel('Learning rate')\n        ax2.legend(loc='upper right')\n        \n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-29T11:53:16.364358Z","iopub.execute_input":"2022-06-29T11:53:16.365258Z","iopub.status.idle":"2022-06-29T11:53:16.381682Z","shell.execute_reply.started":"2022-06-29T11:53:16.365226Z","shell.execute_reply":"2022-06-29T11:53:16.380852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_path = Path('/kaggle/input/tabular-playground-series-jun-2022/')\n\ndata = pd.read_csv(input_path / 'data.csv', index_col='row_id')\nsubmission = pd.read_csv(input_path / 'sample_submission.csv', index_col='row-col')\ndata_premuted = pd.read_csv('../input/lastlastlast/premutedRound2TwoNansWhat.csv', index_col='row_id')","metadata":{"execution":{"iopub.status.busy":"2022-06-29T11:53:16.383956Z","iopub.execute_input":"2022-06-29T11:53:16.384539Z","iopub.status.idle":"2022-06-29T11:53:51.265794Z","shell.execute_reply.started":"2022-06-29T11:53:16.384502Z","shell.execute_reply":"2022-06-29T11:53:51.264024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f4NaNs = data.iloc[:,65:].isna().sum(axis=1)\nf4_col_4NaNs = {}\nsum = 0\nfor col1 in data.iloc[:,65:].columns:\n    for col2 in data.iloc[:,66:].columns:\n        for col3 in data.iloc[:,67:].columns:\n            for col4 in data.iloc[:,68:].columns:\n                if ( int(col1[4:]) >= int(col2[4:]) or int(col2[4:]) >= int(col3[4:]) or int(col3[4:]) >= int(col4[4:]) ):\n                    continue\n                key = str(col1)+str(col2)+str(col3)+str(col4)\n                f4_col1 = data.iloc[:,65:].isna()[col1]\n                f4_col2 = data.iloc[:,65:].isna()[col2]\n                f4_col3 = data.iloc[:,65:].isna()[col3]\n                f4_col4 = data.iloc[:,65:].isna()[col4]\n                f4_col_4NaNs[key] = sorted(list(set(f4NaNs[f4NaNs == 4].index) & set(f4_col1[f4_col1 == 1].index) & set(f4_col2[f4_col2 == 1].index) & set(f4_col3[f4_col3 == 1].index) & set(f4_col4[f4_col4 == 1].index)))\n                sum += len(f4_col_4NaNs[key])\nprint(sum)\nprint(len(f4_col_4NaNs))","metadata":{"execution":{"iopub.status.busy":"2022-06-29T11:54:00.428578Z","iopub.execute_input":"2022-06-29T11:54:00.429115Z","iopub.status.idle":"2022-06-29T11:58:46.150978Z","shell.execute_reply.started":"2022-06-29T11:54:00.429076Z","shell.execute_reply":"2022-06-29T11:58:46.149866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"index_list = data.columns.to_list()\n\nfor index1 in index_list[65:]:\n    for index2 in index_list[66:]:\n        for index3 in index_list[67:]:\n            for index4 in index_list[68:]:\n                if ( int(index1[4:]) >= int(index2[4:]) or int(index2[4:]) >= int(index3[4:]) or int(index3[4:]) >= int(index4[4:]) ):\n                    continue\n                key = str(index1)+str(index2)+str(index3)+str(index4)\n                #if len(f4_col_4NaNs[key]) == 0:\n                #    continue\n                if (index1 != 'F_4_0'):\n                    continue\n                if (index2 != 'F_4_1'):\n                    continue\n                if (index3 != 'F_4_3'):\n                    continue\n                if (index4 != 'F_4_5'):\n                    continue\n                \n                missing = f4_col_4NaNs[key]\n                no_missing = f4NaNs[f4NaNs == 0].index\n                train = data_premuted.iloc[:,65:].iloc[no_missing,]\n                test = data_premuted.iloc[:,65:].iloc[missing,]\n                X = train.drop([index1, index2, index3, index4],axis=1)\n                y = train[[index1, index2, index3, index4]]\n                X_test = test.drop([index1, index2, index3, index4],axis=1)\n                print(index1, 'and', index2, 'and', index3, 'and', index4)\nprint(X.shape, X_test.shape)\nprint(y.shape)","metadata":{"execution":{"iopub.status.busy":"2022-06-29T11:58:51.491309Z","iopub.execute_input":"2022-06-29T11:58:51.491667Z","iopub.status.idle":"2022-06-29T11:58:51.662618Z","shell.execute_reply.started":"2022-06-29T11:58:51.491635Z","shell.execute_reply":"2022-06-29T11:58:51.661625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.layers import Activation\nfrom tensorflow.keras.utils import get_custom_objects\n\nclass Mish(Activation):\n\n    def __init__(self, activation, **kwargs):\n        super(Mish, self).__init__(activation, **kwargs)\n        self.__name__ = 'Mish'\n\n\ndef mish(inputs):\n    return inputs * tf.math.tanh(tf.math.softplus(inputs))\n\nget_custom_objects().update({'Mish': Mish(mish)})","metadata":{"execution":{"iopub.status.busy":"2022-06-29T11:59:02.453487Z","iopub.execute_input":"2022-06-29T11:59:02.454012Z","iopub.status.idle":"2022-06-29T11:59:02.490412Z","shell.execute_reply.started":"2022-06-29T11:59:02.45398Z","shell.execute_reply":"2022-06-29T11:59:02.489389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def my_model():\n    \"\"\"Simple sequential neural network with four hidden layers.\n    \n    Returns a (not yet compiled) instance of tensorflow.keras.models.Model.\n    \"\"\"\n    activation = 'Mish'\n    inputs = Input(shape=(len(X.columns),))\n    \n    xa1 = Dense(32, kernel_regularizer=tf.keras.regularizers.l2(35e-6),\n              activation=activation,\n             )(inputs)\n    xa2 = Dense(32, kernel_regularizer=tf.keras.regularizers.l2(35e-6),\n              activation=activation,\n             )(xa1)\n    \n    xa3 = Concatenate()([xa1, xa2])\n    x = BatchNormalization()(xa3)\n    \n    x = Dense(16, kernel_regularizer=tf.keras.regularizers.l2(35e-6),\n              activation=activation,\n             )(x)\n    #x = BatchNormalization()(x)\n    x = Dense(16, kernel_regularizer=tf.keras.regularizers.l2(35e-6),\n              activation=activation,\n             )(x)\n    #x = BatchNormalization()(x)\n    x = Dense(16, kernel_regularizer=tf.keras.regularizers.l2(35e-6),\n              activation=activation,\n             )(x)\n    x = Dense(16, kernel_regularizer=tf.keras.regularizers.l2(35e-6),\n              activation=activation,\n             )(x)\n    x = Dense(8, kernel_regularizer=tf.keras.regularizers.l2(35e-6),\n              activation=activation,\n             )(x)\n    x = Dense(4,     # for 4 NaNs\n              activation='linear',\n             )(x)\n    model = Model(inputs, x)\n    return model\n\nplot_model(my_model(), show_layer_names=False, show_shapes=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-29T11:59:04.263384Z","iopub.execute_input":"2022-06-29T11:59:04.26373Z","iopub.status.idle":"2022-06-29T11:59:08.440869Z","shell.execute_reply.started":"2022-06-29T11:59:04.2637Z","shell.execute_reply":"2022-06-29T11:59:08.439546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#### %%time\n# Cross-validation of the regressor\n\nEPOCHS = 200\nEPOCHS_COSINEDECAY = 150\nCYCLES = 1\nVERBOSE = 0 # set to 0 for less output, or to 2 for more output\nDIAGRAMS = True\nUSE_PLATEAU = True\nBATCH_SIZE = 2048\nONLY_FIRST_FOLD = True\n\n# see https://keras.io/getting_started/faq/#how-can-i-obtain-reproducible-results-using-keras-during-development\nnp.random.seed(1)\nrandom.seed(1)\ntf.random.set_seed(1)\n\ndef fit_model(X_tr, y_tr, X_va=None, y_va=None, run=0):\n    \"\"\"Scale the data, fit a model, plot the training history and optionally validate the model\n    \n    Returns a trained instance of tensorflow.keras.models.Model.\n    \n    As a side effect, updates y_va_pred, history_list and score_list.\n    \"\"\"\n    global y_va_pred\n    start_time = datetime.datetime.now()\n    \n    scaler = StandardScaler()\n    y_scaler = StandardScaler()\n    X_tr = scaler.fit_transform(X_tr)\n    y_tr_scaled = y_scaler.fit_transform(y_tr)\n    \n    if X_va is not None:\n        X_va = scaler.transform(X_va)\n        y_va_scaled = y_scaler.transform(y_va)\n        validation_data = (X_va, y_va_scaled)\n    else:\n        validation_data = None\n\n    # Define the learning rate schedule and EarlyStopping\n    lr_start=0.01\n    if USE_PLATEAU and X_va is not None: # use early stopping\n        epochs = EPOCHS\n        lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.7, \n                               patience=4, verbose=VERBOSE)\n        es = EarlyStopping(monitor=\"val_loss\",\n                           patience=12, \n                           verbose=1,\n                           mode=\"min\", \n                           restore_best_weights=True)\n        callbacks = [lr, es, tf.keras.callbacks.TerminateOnNaN()]\n\n    else: # use cosine learning rate decay rather than early stopping\n        epochs = EPOCHS_COSINEDECAY\n        lr_end = 0.0002\n        def cosine_decay(epoch):\n            # w decays from 1 to 0 in every cycle\n            # epoch == 0                  -> w = 1 (first epoch of cycle)\n            # epoch == epochs_per_cycle-1 -> w = 0 (last epoch of cycle)\n            epochs_per_cycle = epochs // CYCLES\n            epoch_in_cycle = epoch % epochs_per_cycle\n            if epochs_per_cycle > 1:\n                w = (1 + math.cos(epoch_in_cycle / (epochs_per_cycle-1) * math.pi)) / 2\n            else:\n                w = 1\n            return w * lr_start + (1 - w) * lr_end\n\n        lr = LearningRateScheduler(cosine_decay, verbose=0)\n        callbacks = [lr, tf.keras.callbacks.TerminateOnNaN()]\n        \n    # Construct and compile the model\n    model = my_model()\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_start),\n                  metrics=tf.keras.metrics.RootMeanSquaredError(),\n                  loss=tf.keras.losses.MeanSquaredError())\n\n    # Train the model\n    history = model.fit(X_tr, y_tr_scaled, \n                        validation_data=validation_data, \n                        epochs=epochs,\n                        verbose=VERBOSE,\n                        batch_size=BATCH_SIZE,\n                        shuffle=True,\n                        callbacks=callbacks)\n\n    history_list.append(history.history)\n    callbacks, es, lr, history = None, None, None, None\n    \n    if X_va is None:\n        print(f\"Training rmse: {history_list[-1]['root_mean_squared_error'][-1]:.4f}\")\n    else:\n        lastloss = f\"Training rmse: {history_list[-1]['root_mean_squared_error'][-1]:.4f} | Val rmse: {history_list[-1]['val_root_mean_squared_error'][-1]:.4f}\"\n        \n        # Inference for validation\n        y_va_pred = model.predict(X_va, batch_size=len(X_va), verbose=0)\n        \n        # Evaluation: Execution time, loss and RMSE\n        score = mean_squared_error(y_va_scaled, y_va_pred, squared=False)\n        \n        y_va_pred_unscaled = y_scaler.inverse_transform(y_va_pred)\n        score_unscaled = mean_squared_error(y_va, y_va_pred_unscaled, squared=False)\n        print(f\"Fold {run}.{fold} | {str(datetime.datetime.now() - start_time)[-12:-7]}\"\n              f\" | {lastloss} | RMSE_scaled: {score:.5f}\"\n              f\" | RMSE_unscaled: {score_unscaled:.5f}\")\n        score_list.append(score)\n        score_list_unscaled.append(score_unscaled)\n        \n        if DIAGRAMS and fold == 0 and run == 0:\n            # Plot training history\n            plot_history(history_list[-1], \n                         title=f\"Learning curve (validation RMSE_scaled = {score:.5f})\",\n                         plot_lr=True)\n\n    return model, scaler, y_scaler\n\n\nprint(f\"{len(X.columns)} features\")\nhistory_list = []\nscore_list = []\nscore_list_unscaled = []\nkf = KFold(n_splits=5)\nfor fold, (idx_tr, idx_va) in enumerate(kf.split(X)):\n    X_tr = X.iloc[idx_tr]\n    X_va = X.iloc[idx_va]\n    y_tr = y.iloc[idx_tr]\n    y_va = y.iloc[idx_va]\n    \n    fit_model(X_tr, y_tr, X_va, y_va)\n    if ONLY_FIRST_FOLD: break # we only need the first fold\n\nprint(f\"RMSE_scaled:   {np.mean(score_list):.5f}\")\nprint(f\"RMSE_unscaled: {np.mean(score_list_unscaled):.5f}\")","metadata":{"execution":{"iopub.status.busy":"2022-06-29T11:59:17.951066Z","iopub.execute_input":"2022-06-29T11:59:17.951539Z","iopub.status.idle":"2022-06-29T12:03:41.979958Z","shell.execute_reply.started":"2022-06-29T11:59:17.9515Z","shell.execute_reply":"2022-06-29T12:03:41.979017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"index_list = data.columns.to_list()\n#summing = 0\n#flag = True\nfor index1 in index_list[65:]:\n    for index2 in index_list[66:]:\n        for index3 in index_list[67:]:\n            for index4 in index_list[68:]:\n                if ( int(index1[4:]) >= int(index2[4:]) or int(index2[4:]) >= int(index3[4:]) or int(index3[4:]) >= int(index4[4:]) ):\n                    continue\n                key = str(index1)+str(index2)+str(index3)+str(index4)\n                if len(f4_col_4NaNs[key]) == 0:\n                    continue\n                #if (index1 == 'F_4_0' and index2 == 'F_4_2' and index3 == 'F_4_4' and index4 == 'F_4_12'):\n                #    flag = False\n                #    continue\n                #if flag:\n                #    continue\n                print()\n                print(index1, 'and', index2, 'and', index3, 'and', index4)\n                #if summing >= 36:\n                #    break\n                #summing += 1\n                missing = f4_col_4NaNs[key]\n                no_missing = f4NaNs[f4NaNs == 0].index\n                train = data_premuted.iloc[:,65:].iloc[no_missing,]\n                test = data_premuted.iloc[:,65:].iloc[missing,]\n                X = train.drop([index1, index2, index3, index4],axis=1)\n                y = train[[index1, index2, index3, index4]]\n                X_test = test.drop([index1, index2, index3, index4],axis=1)\n                print(X.shape, X_test.shape)\n                print(y.shape)\n\n                X_tr = X\n                y_tr = y\n                #pred_list = []\n                for seed in range(2, 3):\n                    np.random.seed(seed)\n                    random.seed(seed)\n                    tf.random.set_seed(seed)\n                    model, scaler, y_scaler = fit_model(X_tr, y_tr, run=seed)\n                    preds = y_scaler.inverse_transform(model.predict(scaler.transform(X_test), \n                                                                              batch_size=len(X_test), verbose=0))\n                    print(f\"{seed:2}\", preds.shape)\n\n                #preds = np.array(pred_list).mean(axis=0)\n                data_completed = pd.DataFrame()\n                data_all = data_premuted.iloc[:,65:][[index1, index2, index3, index4]]\n                data_all.iloc[missing,] = preds\n                data_completed = pd.concat([data_completed, data_all],axis=1)\n                data_premuted.loc[:,[index1, index2, index3, index4]] = data_completed","metadata":{"execution":{"iopub.status.busy":"2022-06-29T12:04:17.494013Z","iopub.execute_input":"2022-06-29T12:04:17.494381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in tqdm(submission.index):\n    row = int(i.split('-')[0])\n    col = i.split('-')[1]\n    submission.loc[i, 'value'] = data_premuted.loc[row, col]\n\nsubmission","metadata":{"execution":{"iopub.status.busy":"2022-06-26T14:57:45.275641Z","iopub.execute_input":"2022-06-26T14:57:45.275995Z","iopub.status.idle":"2022-06-26T14:59:15.48173Z","shell.execute_reply.started":"2022-06-26T14:57:45.275954Z","shell.execute_reply":"2022-06-26T14:59:15.480718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_premuted.to_csv('premutedAfterFourNans_final3.csv')","metadata":{"execution":{"iopub.status.busy":"2022-06-26T14:59:18.299493Z","iopub.execute_input":"2022-06-26T14:59:18.299865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('submission.csv')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}