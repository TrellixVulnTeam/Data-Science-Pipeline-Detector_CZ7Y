{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Idia\n\n**important**  \nIterativeImputer(LGB, XGB, etc.)  \nIncrease the number of num_boost_round (LGB)  \n: The more you increase, the better the accuracy. I don't know what the upper limit is.  \n  \n**not important**  \nIterativeImputer(BayesianRidge(default))  \nIncrease the number of max_iter (IterativeImputer)","metadata":{}},{"cell_type":"code","source":"# base\nfrom pathlib import Path\nimport pandas as pd\nimport numpy as np\nimport random\nimport os\nimport tensorflow as tf\n\n# CV\nfrom sklearn.model_selection import KFold, StratifiedKFold\n\n# Imputer\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\n\n# linear\nfrom sklearn.linear_model import LinearRegression\n\n# LGB\nimport lightgbm as lgb\nfrom lightgbm import LGBMRegressor\n\n# metrics\nfrom sklearn.metrics import mean_squared_error\n\n# plot\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as mno\nfrom tqdm import tqdm\n\nimport warnings\nwarnings.filterwarnings('ignore')\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\ntf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)","metadata":{"execution":{"iopub.status.busy":"2022-06-02T12:17:21.161412Z","iopub.execute_input":"2022-06-02T12:17:21.161971Z","iopub.status.idle":"2022-06-02T12:17:29.553594Z","shell.execute_reply.started":"2022-06-02T12:17:21.161866Z","shell.execute_reply":"2022-06-02T12:17:29.552522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def set_seed(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    \nseed=2022\nset_seed(seed)","metadata":{"execution":{"iopub.status.busy":"2022-06-02T12:17:33.928954Z","iopub.execute_input":"2022-06-02T12:17:33.929351Z","iopub.status.idle":"2022-06-02T12:17:33.936245Z","shell.execute_reply.started":"2022-06-02T12:17:33.929321Z","shell.execute_reply":"2022-06-02T12:17:33.934818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/tabular-playground-series-jun-2022/data.csv')\nsub = pd.read_csv('../input/tabular-playground-series-jun-2022/sample_submission.csv')\nsubmission = pd.read_csv('../input/tabular-playground-series-jun-2022/sample_submission.csv', index_col='row-col')","metadata":{"execution":{"iopub.status.busy":"2022-06-02T12:17:37.771302Z","iopub.execute_input":"2022-06-02T12:17:37.772805Z","iopub.status.idle":"2022-06-02T12:17:58.03823Z","shell.execute_reply.started":"2022-06-02T12:17:37.772732Z","shell.execute_reply":"2022-06-02T12:17:58.03684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mno.matrix(df, figsize = (20, 5))","metadata":{"execution":{"iopub.status.busy":"2022-06-02T12:17:58.040396Z","iopub.execute_input":"2022-06-02T12:17:58.040826Z","iopub.status.idle":"2022-06-02T12:18:17.926099Z","shell.execute_reply.started":"2022-06-02T12:17:58.040793Z","shell.execute_reply":"2022-06-02T12:18:17.924711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"missing_columns = [c for c in df.columns if 'F_2' not in c and c != 'row_id']","metadata":{"execution":{"iopub.status.busy":"2022-06-02T12:18:17.928261Z","iopub.execute_input":"2022-06-02T12:18:17.928886Z","iopub.status.idle":"2022-06-02T12:18:17.93524Z","shell.execute_reply.started":"2022-06-02T12:18:17.928831Z","shell.execute_reply":"2022-06-02T12:18:17.9336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Iterative Imputation (LightGBM)\nReference from  \nhttps://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html  \nhttps://scikit-learn.org/stable/modules/impute.html#iterative-imputer ","metadata":{}},{"cell_type":"code","source":"base_model = LGBMRegressor(num_boost_round=1000, random_state = seed)\n\nlgb_iterative_imp = IterativeImputer(\n                       estimator=base_model,\n                       max_iter=20, \n                       initial_strategy='mean', # {'mean', 'median', 'most_frequent', 'constant'}\n                       imputation_order='ascending', # {‘ascending’, ‘descending’, ‘roman’, ‘arabic’, ‘random’}\n                       verbose=1,\n                       random_state=seed)\n\nlgb_iterative_imp_df = pd.DataFrame(lgb_iterative_imp.fit_transform(df), columns=df.columns)","metadata":{"_kg_hide-output":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mno.matrix(lgb_iterative_imp_df, figsize = (20,5))","metadata":{"execution":{"iopub.status.busy":"2022-06-02T04:43:35.725459Z","iopub.execute_input":"2022-06-02T04:43:35.726753Z","iopub.status.idle":"2022-06-02T04:43:54.509382Z","shell.execute_reply.started":"2022-06-02T04:43:35.726642Z","shell.execute_reply":"2022-06-02T04:43:54.508185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# View the prediction error for each feature\nView RMSE when each feature is used as the objective variable","metadata":{}},{"cell_type":"code","source":"# lightgbm\nclass ModelLgb:\n\n    def __init__(self):\n        self.model = None\n\n    def fit(self, tr_x, tr_y, va_x, va_y):\n        params = {\n        'objective':'regression',\n        'metric':'rmse',\n        'seed': seed,\n        'verbosity':-1,\n        'learning_rate':0.1,\n        }\n        \n        num_round = 1000\n        early_stopping_rounds=10\n        \n        lgb_train = lgb.Dataset(tr_x, tr_y)\n        lgb_eval = lgb.Dataset(va_x, va_y)\n        \n        self.model = lgb.train(params, lgb_train, valid_sets=lgb_eval, \n                               num_boost_round=num_round, early_stopping_rounds=early_stopping_rounds,\n                               verbose_eval=-1\n                              )\n        \n#         lgb.plot_importance(self.model, figsize=(20,30))\n        \n    def predict(self, x):\n        pred = self.model.predict(x, num_iteration=self.model.best_iteration)\n        return pred","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# run model & make prediction feature\ndef mk_predict(model, train_x, train_y):\n    \n    set_seed(seed)\n    \n    va_preds = []\n    va_idxes = []\n    \n    rmses = []\n    \n    kf = KFold(n_splits=5, shuffle=True, random_state=seed)\n    for i, (tr_idx, va_idx) in tqdm(enumerate(kf.split(train_x))):\n        \n        print('='*15 + f'fold{i+1}' + '='*15)\n\n        tr_x, va_x = train_x.iloc[tr_idx], train_x.iloc[va_idx]\n        tr_y, va_y = train_y.iloc[tr_idx], train_y.iloc[va_idx]\n        \n        model.fit(tr_x, tr_y, va_x, va_y)\n\n        # valid predict & index\n        va_pred = model.predict(va_x)\n        va_preds.append(va_pred)\n        va_idxes.append(va_idx)\n        \n        # valid loss\n        va_rmse = np.sqrt(mean_squared_error(va_pred, va_y))\n        print(f'RMSE : {va_rmse}')\n        rmses.append(va_rmse)\n        \n    # sort valid pred    \n    va_idxes = np.concatenate(va_idxes)\n    va_preds = np.concatenate(va_preds, axis=0)\n    order = np.argsort(va_idxes)\n    train_preds = va_preds[order]\n    \n    # mean RMSE\n    mean_rmse = np.mean(rmses)\n    print(f'Mean RMSE : {mean_rmse}')\n         \n    return train_preds, mean_rmse","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\npred_df = lgb_iterative_imp_df.copy()\nrmse_dict = {}\nfor target_col in tqdm(lgb_iterative_imp_df.columns[1:]):\n    print('#'*15 + target_col + '#'*15)\n    train_x = lgb_iterative_imp_df.drop(['row_id', target_col], axis=1)\n    train_y = lgb_iterative_imp_df[target_col]\n    model_lgb = ModelLgb()\n    pred_df[target_col], rmse = mk_predict(model_lgb, train_x, train_y)\n    rmse_dict[target_col] = rmse","metadata":{"_kg_hide-output":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# View RMSE when each feature is used as the objective variable\ndata = rmse_dict\nnames = list(data.keys())\nvalues = list(data.values())\nfig = plt.figure(figsize = (60, 20))\n\nplt.bar(range(len(data)), values, tick_label=names)\nplt.title('RMSE when each feature is used as the objective variable')\nplt.xlabel('Features')\nplt.ylabel('RMSE')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submit","metadata":{}},{"cell_type":"code","source":"# Iterative Imputation (LightGBM)\ndata = lgb_iterative_imp_df.set_index('row_id')\n\nfor i in tqdm(submission.index):\n    row = int(i.split('-')[0])\n    col = i.split('-')[1]\n    submission.loc[i, 'value'] = data.loc[row, col]\n\nsub.value= submission.value.values\nsub.to_csv('Submission_lgb_iterative_imp.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Iterative Imputation (LightGBM) & Predict again\ndata = pred_df.set_index('row_id')\n\nfor i in tqdm(submission.index):\n    row = int(i.split('-')[0])\n    col = i.split('-')[1]\n    submission.loc[i, 'value'] = data.loc[row, col]\n\nsub.value= submission.value.values\nsub.to_csv('Submission_lgb_pred.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]}]}