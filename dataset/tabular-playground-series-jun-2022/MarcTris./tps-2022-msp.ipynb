{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# Libarary import\n\nimport numpy as np                  # data arrays\nimport pandas as pd                 # data reading\nfrom tqdm import tqdm               # processing percentage bar\nfrom lightgbm import LGBMRegressor  # gradient boosting regressor\n\n# Data reading\n\ndata = pd.read_csv(\"../input/tabular-playground-series-jun-2022/data.csv\")\nsubmission = pd.read_csv('../input/tabular-playground-series-jun-2022/sample_submission.csv', index_col='row-col')\n\n# Data handling\n\n    ## Feature distribution\n\nfeatures = list(data.columns)         \nfeatures_1, features_2, features_3, features_4 = [], [], [], []\nF = [[], [], [], [], []]\nfor feature in features:                     # Iterating features\n    for i in [1, 2, 3, 4]:                   # Iterating data subsets\n        if feature.split('_')[1] == str(i):  # Split a string for every underscore present 'F_4_11' = 3 splits\n            F[i].append(feature)             # Appending feature to array\ndf = [[], [], [], [], []]\n\nfor i in [1, 2, 3, 4]:\n    df[i] = data[F[i]]  # Iterating data subsets\n    \n    ## Plot the number of null values per sample\n\nfeatures = list(data.columns)\nmissing_values_count = pd.DataFrame(data[features].isnull().sum())\nmissing_values_count.reset_index()\nmissing_values_count.plot()\n\n# Parameters\n\nparameters = {'random_state': 22, 'n_estimators': 5000, 'learning_rate' : 0.1}     \n    # random_state = sets a seed to to the random generator, meaning the same results will be produced as long as the random state is consistent\n    # n_splits = >10000, reduce to minimize runtime. n_estimators = number of trees or number of samples on which the algoritm will work\n    # learning_rate = defines step_size at each iteration or how fast the model learns from the training set. \n    # High learning rates can cause model to learn too quickly and arrive at a non-optimal set of parameters \n\n# Modelling\n\nfor i in [1,3,4]:\n    dataframe = pd.DataFrame()     # DataFrame = a 2d data structure, with this we can perform basic operations on rows/columns\n    train_column = pd.DataFrame()\n    test_column = pd.DataFrame()\n    dataframe=df[i].copy()\n    for column in dataframe.columns: \n        if dataframe[column].isnull().sum() == 0:\n            continue                                                 # continue if no null values are present\n        indexed_null = dataframe[dataframe[column].isnull()].index   # indexing rows with null values\n        \n        train_column = dataframe.drop(indexed_null, axis = 0)        # tranining set: at least one column has no null values  \n        test_column = dataframe[dataframe.index.isin(indexed_null)]  \n        \n        X = train_column.drop([column],axis=1)\n        y = train_column[column]\n        \n        model = LGBMRegressor(**parameters)\n        model.fit(X,y)\n        \n        dataframe[column][indexed_null] = model.predict(test_column.drop([column],axis=1))\n    df[i]=dataframe.copy()                                         \n          \n# Submission\n    \nmerged_data = pd.concat([df[1], df[2], df[3], df[4]], axis=1)\n\nfor i in tqdm(submission.index):\n    row = int(i.split('-')[0])\n    col = i.split('-')[1]\n    submission.loc[i, 'value'] = merged_data.loc[row, col]\n\nsubmission.to_csv('submission.csv')\n\nprint(submission)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T12:16:39.510012Z","iopub.execute_input":"2022-06-27T12:16:39.510795Z","iopub.status.idle":"2022-06-27T12:25:53.62697Z","shell.execute_reply.started":"2022-06-27T12:16:39.510698Z","shell.execute_reply":"2022-06-27T12:25:53.625714Z"},"trusted":true},"execution_count":null,"outputs":[]}]}