{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-22T08:51:26.438752Z","iopub.execute_input":"2022-06-22T08:51:26.439346Z","iopub.status.idle":"2022-06-22T08:51:26.456659Z","shell.execute_reply.started":"2022-06-22T08:51:26.439309Z","shell.execute_reply":"2022-06-22T08:51:26.455251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loading packages\n    # pandas\nimport pandas as pd\n    # data_split\nfrom sklearn.model_selection import train_test_split\n    # regressor\nfrom sklearn.tree import DecisionTreeRegressor\n    # mean_absolute_error\nfrom sklearn.metrics import mean_absolute_error\n    # numpy\nimport numpy as np\n\n# Data Processing\n    # reading DataFrame, MUST be split first\ndf_train = pd.read_csv('../input/tabular-playground-series-jun-2022/data.csv')\ndf_test = pd.read_csv('../input/tabular-playground-series-jun-2022/sample_submission.csv')\n    # describing data\ndf_test.head()\ndf_train.describe\n\n\n\n    # spltiting data\nfeatures = [f'cont{x}'for x in range(1,15)]\nX = df_train.drop(['row_id'], axis=1)\ny = df_train.target(['features'])\nX_train, X_test, y_train, y_test=train_test_split(X, y, test_size=0.20, random_state=33)\n        # note: test_size defines percent of data getting sorted into validation, so 80% is assinged to training.\n        #       random_state defines the spliting of data into sets, if not specified can cause spliting of data \n        #       to be done differently everytime the model is trained\n    # test function\ndf_testing = pd.read_csv('../input/tabular-playground-series-jun-2022/sample_submission.csv')\n    # dependent variable = missing values\ny = ['value']\n    # independent variable\nX = df_train[features]\n    # verify no null values\nprint(df.train.isnull().sum())\nprint(df.test.isnull().sum())\n    # dropping null/irrelevant values e.g. ID column\n\n\n\n# Modeling \n    # fitting\ntree_model = DecisionTreeRegressor(random_state=33)\ntree_model.fit(X,y)\n    # predictions\npredictions_tree_model = tree_model.predict(X_test)\nMAE_tree_model = mean_absolute_error(y_test, predictions_tree_model)\nprint(MAE_tree_model)\n\n# Optimizing model\n    # max_leaf_nodes\n    # outliers using box plot / replacing outliers\n    # multiple decision trees / combine --> random forests\n    \n# Submission\n\nfor i in tqdm(sample_sub.index):\n    row = int(i.split('-')[0])\n    col = i.split('-')[1]\n    sample_sub.loc[i, 'value'] = df.loc[row, col]\n\nsample_sub.to_csv('submission.csv')","metadata":{"execution":{"iopub.status.busy":"2022-06-22T09:32:30.032363Z","iopub.execute_input":"2022-06-22T09:32:30.032923Z","iopub.status.idle":"2022-06-22T09:32:45.840439Z","shell.execute_reply.started":"2022-06-22T09:32:30.032875Z","shell.execute_reply":"2022-06-22T09:32:45.838641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\n%matplotlib inline\n\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, \\\n                                    train_test_split\nfrom sklearn.metrics import mean_squared_error as mse\n# loading data\ndf = pd.read_csv('../input/tabular-playground-series-jun-2022/data.csv',index_col=1)\ny = pd.DataFrame(df['row_id'])\ntrain = df.drop(['row_id'],axis=1)\n\ntest = pd.read_csv('../input/tabular-playground-series-jun-2022/sample_submission.csv',index_col=1)\n\nnewdf = df.fillna(0)\n\ntrain.head()\n\n\nimport lightgbm as lgb\nLGB = lgb.LGBMRegressor(random_state=33, n_estimators=4800, min_data_per_group=5, boosting_type='gbdt',\n num_leaves=246, max_dept=-1, learning_rate=0.005, subsample_for_bin=200000,\n lambda_l1= 1.074622455507616e-05, lambda_l2= 2.0521330798729704e-06, n_jobs=-1, cat_smooth=1.0, \n importance_type='split', metric='rmse', min_child_samples=20, min_gain_to_split=0.0, feature_fraction=0.5, \n bagging_freq=6, min_sum_hessian_in_leaf=0.001, min_data_in_leaf=100, bagging_fraction=0.82063411)\n\nparameters = {'depth': [4,6,8,10,12,14,18,20],\n              'learning_rate' : [0.005, 0.01, 0.035, 0.05, 0.1, 0.15, 0.2],\n              'iterations'    : [300, 800, 1000, 1800, 3000, 4100, 5000]}\n\ngrid = GridSearchCV(estimator=LGB, param_grid = parameters, cv = 3, n_jobs=-1)\n\ngrid.fit(train, y)\n\nLGB.fit(train, y)\n\nprint(\" Results from Grid Search \" )\nprint(\"\\n The best estimator across ALL searched params:\\n\", grid.best_estimator_)\nprint(\"\\n The best score across ALL searched params:\\n\", grid.best_score_)\nprint(\"\\n The best parameters across ALL searched params:\\n\", grid.best_params_)\n\ntree_preds = pd.DataFrame(LGB.predict(test),index=test.index)\ntree_preds.columns=['row_id']\n\ntree_preds.to_csv('predictions/decisiontree.csv')\nprint(tree_preds)\n\n###from sklearn.tree import DecisionTreeRegressor\n\n# hyperparameter tuning, with cross-validation using GridSearchCV\n###tree = DecisionTreeRegressor(random_state=42,criterion='mse')\n\n###tree_params = {'max_depth':range(1,11),\n               ###'max_features':range(4,15)}\n              \n###tree_grid = GridSearchCV(tree,tree_params, cv=5,n_jobs=-1)\n\n###tree_grid.fit(train,y)\n###GridSearchCV(cv=5, estimator=DecisionTreeRegressor(random_state=42), n_jobs=-1,\n             ###param_grid={'max_depth': range(1, 11),\n                         ###'max_features': range(4, 15)})\n# best parameters\n###tree_grid.best_params_\n###{'max_depth': 8, 'max_features': 6}\n# making prediction with tree using above parameters\n###tree = DecisionTreeRegressor(random_state=42,max_depth=8,max_features=6)\n###tree.fit(train,y)\n###tree_preds = pd.DataFrame(tree.predict(test),index=test.index)\n###tree_preds.columns=['row_id']\n\n###tree_preds.to_csv('predictions/decisiontree.csv')\n###print(tree_preds)","metadata":{"execution":{"iopub.status.busy":"2022-06-22T12:55:37.731228Z","iopub.execute_input":"2022-06-22T12:55:37.732528Z"},"trusted":true},"execution_count":null,"outputs":[]}]}