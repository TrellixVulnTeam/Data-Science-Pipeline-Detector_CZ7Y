{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd \nimport numpy as np \nimport os\ndir_data = \"../input/tabular-playground-series-jun-2022/data.csv\"\ndata_size = os.path.getsize(dir_data)/1e9\nprint(f\"The file size is {data_size:.4f} GB\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-21T07:04:41.568883Z","iopub.execute_input":"2022-06-21T07:04:41.569409Z","iopub.status.idle":"2022-06-21T07:04:41.59519Z","shell.execute_reply.started":"2022-06-21T07:04:41.569336Z","shell.execute_reply":"2022-06-21T07:04:41.594373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load Data using Pandas\n%time\ndata = pd.read_csv(dir_data)","metadata":{"execution":{"iopub.status.busy":"2022-06-21T07:05:26.945825Z","iopub.execute_input":"2022-06-21T07:05:26.946167Z","iopub.status.idle":"2022-06-21T07:05:43.486852Z","shell.execute_reply.started":"2022-06-21T07:05:26.946137Z","shell.execute_reply":"2022-06-21T07:05:43.486035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.info()","metadata":{"execution":{"iopub.status.busy":"2022-06-21T07:05:44.609751Z","iopub.execute_input":"2022-06-21T07:05:44.610118Z","iopub.status.idle":"2022-06-21T07:05:44.775308Z","shell.execute_reply.started":"2022-06-21T07:05:44.610081Z","shell.execute_reply":"2022-06-21T07:05:44.774126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load Data using Datatable\n%time\nimport datatable as dt \ndt_df = dt.fread(dir_data)\npd_df = dt_df.to_pandas()\n","metadata":{"execution":{"iopub.status.busy":"2022-06-21T07:08:43.331888Z","iopub.execute_input":"2022-06-21T07:08:43.332705Z","iopub.status.idle":"2022-06-21T07:08:45.709694Z","shell.execute_reply.started":"2022-06-21T07:08:43.332661Z","shell.execute_reply":"2022-06-21T07:08:45.708886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import dask.dataframe as dd","metadata":{"execution":{"iopub.status.busy":"2022-06-21T07:08:47.431241Z","iopub.execute_input":"2022-06-21T07:08:47.43159Z","iopub.status.idle":"2022-06-21T07:08:49.760499Z","shell.execute_reply.started":"2022-06-21T07:08:47.431558Z","shell.execute_reply":"2022-06-21T07:08:49.759753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load Data using Dask\n%time\ndf_dask = dd.read_csv(dir_data)\n","metadata":{"execution":{"iopub.status.busy":"2022-06-21T07:08:53.492307Z","iopub.execute_input":"2022-06-21T07:08:53.492675Z","iopub.status.idle":"2022-06-21T07:08:53.516058Z","shell.execute_reply.started":"2022-06-21T07:08:53.492639Z","shell.execute_reply":"2022-06-21T07:08:53.51525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make sure to activate your GPU on yur kenrnel when using cudf\nimport cudf\n%time\ndf_cudf = cudf.read_csv(dir_data)","metadata":{"execution":{"iopub.status.busy":"2022-06-21T07:08:54.041273Z","iopub.execute_input":"2022-06-21T07:08:54.041565Z","iopub.status.idle":"2022-06-21T07:08:58.001536Z","shell.execute_reply.started":"2022-06-21T07:08:54.041537Z","shell.execute_reply":"2022-06-21T07:08:58.000597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Voila! You can use datatable or cudF to load your data faster later in your next competition. It is faster compared to loading data using pandas. You see the result on this experiment by looking at the Wall time. There is also one trick that you can use by compressing the data type to reduce memory usage. You can see the following code below.","metadata":{}},{"cell_type":"code","source":"# https://www.kaggle.com/code/gemartin/load-data-reduce-memory-usage/notebook\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df\n\n\ndef import_data(file):\n    \"\"\"create a dataframe and optimize its memory usage\"\"\"\n    df = pd.read_csv(file, parse_dates=True, keep_date_col=True)\n    df = reduce_mem_usage(df)\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-06-21T07:05:11.684653Z","iopub.execute_input":"2022-06-21T07:05:11.68512Z","iopub.status.idle":"2022-06-21T07:05:11.711391Z","shell.execute_reply.started":"2022-06-21T07:05:11.68508Z","shell.execute_reply":"2022-06-21T07:05:11.710404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('-' * 80)\nprint('data')\ntrain = import_data(dir_data)","metadata":{"execution":{"iopub.status.busy":"2022-06-21T07:07:15.189399Z","iopub.execute_input":"2022-06-21T07:07:15.18987Z","iopub.status.idle":"2022-06-21T07:07:31.379544Z","shell.execute_reply.started":"2022-06-21T07:07:15.189827Z","shell.execute_reply":"2022-06-21T07:07:31.378658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"you can see we compressed the size of the file from 617+ MB to 132.56 MB. it decreased around 78%. Then you can load as usual using cudf,dask, or even pandas. ","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}