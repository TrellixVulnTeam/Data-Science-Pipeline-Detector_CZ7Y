{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Tabular Playground Series- June 2022 Part 1\n\nObjective:- \nThis notebook is used to participate in the TPS June2022 edition. We aim to impute missing values in the provided dataset using descriptive statistics and other column entries in the table\n\n**In this component, we perform EDA and impute feature missing values for groups 1,3. Features in group 4 are imputed in the next part, using models (with accelerators)**","metadata":{}},{"cell_type":"code","source":"!pip install swifter\nfrom IPython.display import clear_output;\nclear_output();","metadata":{"execution":{"iopub.status.busy":"2022-06-28T10:51:47.622302Z","iopub.execute_input":"2022-06-28T10:51:47.622789Z","iopub.status.idle":"2022-06-28T10:51:59.540686Z","shell.execute_reply.started":"2022-06-28T10:51:47.622754Z","shell.execute_reply":"2022-06-28T10:51:59.539252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Performing general package imports:-\nimport numpy as np;\nimport pandas as pd;\nimport swifter;\nfrom termcolor import colored;\nimport matplotlib.pyplot as plt;\n%matplotlib inline\nimport seaborn as sns;\nfrom scipy.stats import skew, iqr;\n\nfrom warnings import filterwarnings;\nfilterwarnings(action= 'ignore');\nfrom gc import collect;\nfrom tqdm.notebook import tqdm;\n\nnp.random.seed(10);","metadata":{"execution":{"iopub.status.busy":"2022-06-28T10:51:59.543633Z","iopub.execute_input":"2022-06-28T10:51:59.544053Z","iopub.status.idle":"2022-06-28T10:51:59.55847Z","shell.execute_reply.started":"2022-06-28T10:51:59.544012Z","shell.execute_reply":"2022-06-28T10:51:59.557018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Performing model specific imports:-\n# Using the sklearnex.patch_sklearn to improve the speed of sklearn library:-\nfrom sklearnex import patch_sklearn; patch_sklearn()\nfrom sklearn.model_selection import KFold;\nfrom sklearn.metrics import mean_squared_error;","metadata":{"execution":{"iopub.status.busy":"2022-06-28T10:51:59.560243Z","iopub.execute_input":"2022-06-28T10:51:59.560626Z","iopub.status.idle":"2022-06-28T10:51:59.577769Z","shell.execute_reply.started":"2022-06-28T10:51:59.560591Z","shell.execute_reply":"2022-06-28T10:51:59.575934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1. Training data import","metadata":{}},{"cell_type":"code","source":"xytrain = pd.read_csv('../input/tabular-playground-series-jun-2022/data.csv', \n                      encoding = 'utf8', index_col= 'row_id');\nsub_fl = pd.read_csv('../input/tabular-playground-series-jun-2022/sample_submission.csv', \n                     encoding= 'utf8');\n\nprint(colored(f\"\\nTraining data\\n\", color = 'blue', attrs= ['bold']));\ndisplay(xytrain.head(5).style.format(precision=2));\n\nprint(colored(f\"\\nSample submission data\\n\", color = 'blue', attrs= ['bold']));\ndisplay(sub_fl.head(5).style.format(precision= 2));","metadata":{"execution":{"iopub.status.busy":"2022-06-28T10:51:59.580206Z","iopub.execute_input":"2022-06-28T10:51:59.580737Z","iopub.status.idle":"2022-06-28T10:52:14.493136Z","shell.execute_reply.started":"2022-06-28T10:51:59.580695Z","shell.execute_reply":"2022-06-28T10:52:14.491931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Basic information check:-\nprint(colored(f\"\\nTraining data information\\n\", color = 'blue', attrs= ['bold']));\ndisplay(xytrain.info());\n\nprint(colored(f\"\\nSubmission file information\\n\", color = 'blue', attrs= ['bold']));\ndisplay(sub_fl.info());","metadata":{"execution":{"iopub.status.busy":"2022-06-28T10:52:14.494908Z","iopub.execute_input":"2022-06-28T10:52:14.496099Z","iopub.status.idle":"2022-06-28T10:52:14.825511Z","shell.execute_reply.started":"2022-06-28T10:52:14.496058Z","shell.execute_reply":"2022-06-28T10:52:14.824402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Memory reduction\n\nThe dataframes needlessly assign float64 to most columns and int64 to all integer columns.\nThis can be managed by re-assigning datatypes based on min-max column values. Memory usage is thus reduced substantially.\n","metadata":{}},{"cell_type":"code","source":"# Reducing dataframe memory usage :-\ndef ReduceMemory(df: pd.DataFrame):\n    \"\"\"\n    This function reduces the associated dataframe's memory usage.\n    It reassigns the data-types of columns according to their min-max values.\n    It also displays the dataframe information after memory reduction.\n    \"\"\";\n    \n    # Reducing float column memory usage:-\n    for col in tqdm(df.iloc[0:2, 1:].select_dtypes('float').columns):\n        col_min = np.amin(df[col].dropna());\n        col_max = np.amax(df[col].dropna());\n        \n        if col_min >= np.finfo(np.float16).min and col_max <= np.finfo(np.float16).max: \n            df[col] = df[col].astype(np.float16)\n        elif col_min >= np.finfo(np.float32).min and col_max <= np.finfo(np.float32).max : \n            df[col] = df[col].astype(np.float32)\n        else: pass;\n\n    # Reducing integer column memory usage:-\n    for col in tqdm(df.iloc[0:2, 1:].select_dtypes('int').columns):\n        col_min = df[col].min(); \n        col_max = df[col].max();\n        \n        if col_min >= np.iinfo(np.int8).min and col_max <= np.iinfo(np.int8).max:\n            df[col] = df[col].astype(np.int8);\n        elif col_min >= np.iinfo(np.int16).min and col_max <= np.iinfo(np.int16).max:\n            df[col] = df[col].astype(np.int16);\n        elif col_min >= np.iinfo(np.int32).min & col_max <= np.iinfo(np.int32).max:\n            df[col] = df[col].astype(np.int32);\n        else: pass;\n        \n    print(colored(f\"\\nDataframe information after memory reduction\\n\", \n                  color = 'blue', attrs= ['bold']));\n    display(df.info()); \n    \n    return df;","metadata":{"execution":{"iopub.status.busy":"2022-06-28T10:52:14.82703Z","iopub.execute_input":"2022-06-28T10:52:14.827467Z","iopub.status.idle":"2022-06-28T10:52:14.844024Z","shell.execute_reply.started":"2022-06-28T10:52:14.82743Z","shell.execute_reply":"2022-06-28T10:52:14.842461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Implementing memory reduction on the existing data-sets:-\nxytrain = ReduceMemory(df= xytrain);\nsub_fl = ReduceMemory(df= sub_fl);\ncollect();","metadata":{"execution":{"iopub.status.busy":"2022-06-28T10:52:14.845771Z","iopub.execute_input":"2022-06-28T10:52:14.846252Z","iopub.status.idle":"2022-06-28T10:52:25.368883Z","shell.execute_reply.started":"2022-06-28T10:52:14.846194Z","shell.execute_reply":"2022-06-28T10:52:25.367578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**We have successfully reduced the memory usage of the input tables substantially from 651Mb to approximately 140Mb. This translates to an appriximately 78% memory reduction without major information loss**","metadata":{}},{"cell_type":"markdown","source":"## 3. Exploratory Data Analysis:-\n\nEDA analysis consists of- \n1. Data-type and distribution details\n2. Null values per column in the data table\n3. Correlation plots and inferences\n4. Other association metrics (mutual information)- if needed\n5. Data visualization as deemed necessary","metadata":{}},{"cell_type":"code","source":"# Model data description:-\nprint(colored(f\"\\nModel data description with null values per column\\n\", \n              color = 'blue', attrs= ['bold']));\n\nxytrain_desc_sum = \\\npd.concat((xytrain.describe(percentiles= np.arange(0.10,1.0,0.10)).transpose(),\n           xytrain.isna().sum(axis=0),\n           xytrain.nunique(axis=0),\n           pd.DataFrame(data= skew(xytrain.dropna(), axis=0),\n                        index= xytrain.columns, columns= ['skewness'])), \n                  axis=1).\\\nrename({0: 'nb_null_values',1: 'nb_unique_values'}, axis=1);\n\ndisplay(xytrain_desc_sum.style.format('{:,.2f}'));","metadata":{"execution":{"iopub.status.busy":"2022-06-28T10:52:25.370529Z","iopub.execute_input":"2022-06-28T10:52:25.370985Z","iopub.status.idle":"2022-06-28T10:52:38.894269Z","shell.execute_reply.started":"2022-06-28T10:52:25.370947Z","shell.execute_reply":"2022-06-28T10:52:38.893085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Grouping the table columns into 4 groups based on the F_{nb} values:-\n# Creating input column list:-\nxytrain_col = xytrain.columns;\n\n# Populating the feature groups with dictionary comprehension:-\nFtre_Grp_Dict = {i: xytrain_col[xytrain_col.str[0:3] == f\"F_{i}\"] for i in range(1,5,1)};\nprint(colored(f\"\\nModel data feature groups\\n\", color = 'blue', attrs= ['bold']));\ndisplay(Ftre_Grp_Dict);","metadata":{"execution":{"iopub.status.busy":"2022-06-28T10:52:38.895929Z","iopub.execute_input":"2022-06-28T10:52:38.896972Z","iopub.status.idle":"2022-06-28T10:52:38.910185Z","shell.execute_reply.started":"2022-06-28T10:52:38.896929Z","shell.execute_reply":"2022-06-28T10:52:38.908949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Calculation of nulls per column in the data-set:-\n\nThe number of nulls per column could be of use in model development, especially for columns in group 4 where a model is developed.","metadata":{}},{"cell_type":"code","source":"_ = pd.DataFrame(xytrain.isna().sum(axis=0), columns = ['Nb_Nulls']);\n_['Null_Rate'] = _/ len(xytrain.index);\n\nfig, ax = plt.subplots(2,1, figsize = (25,16), sharex= True);\n_.loc[_['Nb_Nulls'] > 0, ['Nb_Nulls']].plot.bar(ax= ax[0], color = 'tab:blue');\nax[0].set_title(f\"Null instances per column in the data\\n\", color = 'tab:blue', fontsize= 12);\nax[0].set_yticks(range(0,19001,1000));\n\n_.loc[_['Nb_Nulls'] > 0, ['Null_Rate']].plot.bar(ax= ax[1], color = 'tab:blue');\nax[1].set_title(f\"Null-rate per column in the data\\n\", color = 'tab:blue', fontsize= 12);\nax[1].set_yticks(np.arange(0.0, 0.022, 0.002));\n\nplt.tight_layout();\nplt.show();\n\ndel _;","metadata":{"execution":{"iopub.status.busy":"2022-06-28T10:52:38.914487Z","iopub.execute_input":"2022-06-28T10:52:38.915375Z","iopub.status.idle":"2022-06-28T10:52:40.718266Z","shell.execute_reply.started":"2022-06-28T10:52:38.915334Z","shell.execute_reply":"2022-06-28T10:52:40.717132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Calculation of nulls per row in the dataset:-\n\nThe number of nulls per row could be of use in model development, especially for columns in group 4 where a model is developed. We aim to elicit the nulls per row and then the column names where nulls are present per row.","metadata":{}},{"cell_type":"code","source":"# Analyzing the nulls per row only in feature group 4:-\ndef Locate_NullRows(df:pd.DataFrame, title: str):\n    \"\"\"\n    This function locates the nulls across rows in the dataframe (or a subset) and plots it.\n    Input:- \n    df (pd.DataFrame)- Analysis dataframe\n    title (string)- Title for plots\n    \"\"\";\n    \n    df['Nb_Nulls'] = df.isna().sum(axis=1);\n    _ = pd.DataFrame(df.query(\"Nb_Nulls > 0\").groupby(['Nb_Nulls']).size(),\n                     columns = ['Nb_Nulls']);\n\n    print(colored(f\"\\n{title}\\n\", color = 'blue', attrs= ['bold', 'dark']));\n    display(_.style.format('{:,.0f}'));\n\n    fig, ax= plt.subplots(1,1, figsize= (12,6));\n    _.plot.bar(ax=ax);\n    ax.set_title(f\"\\n{title}\\n\", color = 'tab:blue', fontsize= 12);\n    ax.grid(visible= True, which = 'both', color = 'grey', linewidth= 0.75, linestyle= '--');\n    ax.set_xlabel('Number of nulls', fontsize= 9);\n    \n    plt.tight_layout();\n    plt.xticks(rotation= 0);\n    plt.show();\n\n    del _;\n    df = df.drop(['Nb_Nulls'], axis=1);","metadata":{"execution":{"iopub.status.busy":"2022-06-28T10:52:40.720287Z","iopub.execute_input":"2022-06-28T10:52:40.720699Z","iopub.status.idle":"2022-06-28T10:52:40.732338Z","shell.execute_reply.started":"2022-06-28T10:52:40.720664Z","shell.execute_reply":"2022-06-28T10:52:40.731235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Locating nulls across table components:-\nLocate_NullRows(df= xytrain, title = 'Full table- nulls');\nLocate_NullRows(df= xytrain.loc[:, Ftre_Grp_Dict.get(4)],title = 'Feature group4- nulls');\ncollect();","metadata":{"execution":{"iopub.status.busy":"2022-06-28T10:52:40.733773Z","iopub.execute_input":"2022-06-28T10:52:40.734168Z","iopub.status.idle":"2022-06-28T10:52:42.890023Z","shell.execute_reply.started":"2022-06-28T10:52:40.734111Z","shell.execute_reply":"2022-06-28T10:52:42.888841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Delving into group4 nulls to elicit model specifics:-\n_ = xytrain.loc[:, list(Ftre_Grp_Dict.get(4))];\n_['Nb_Nulls'] = _.isna().sum(axis=1);\n_['Nb_Nulls'].astype(np.int8);\n\ndef EnlistNullCol(row):\n    \"This function forms a list of all columns in a row that contain null values\";\n    null_col_comb = [];\n    for i in row.items(): \n        if np.isnan(i[1], casting= 'safe'): null_col_comb.append(i[0]);\n    return ' '.join(null_col_comb);\n\n_['Null_Col'] = _.swifter.apply(EnlistNullCol, axis=1);\n\nprint(colored(f\"\\nNull columns in feature group4-\\n\", color= 'blue', attrs= ['dark', 'bold']));\ndisplay(_.loc[_.Null_Col != '', ['Null_Col']].value_counts());\ncollect();","metadata":{"execution":{"iopub.status.busy":"2022-06-28T11:09:24.176252Z","iopub.execute_input":"2022-06-28T11:09:24.177349Z","iopub.status.idle":"2022-06-28T11:10:16.666804Z","shell.execute_reply.started":"2022-06-28T11:09:24.177291Z","shell.execute_reply":"2022-06-28T11:10:16.665363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting correlation plots for various sections in group4 with nulls between 1-5:-\nfor nb_nulls in tqdm(range(1,6,1)):\n    fig, ax= plt.subplots(1,1, figsize= (20,10));\n    _corr = _.loc[_.Nb_Nulls == nb_nulls].drop(['Nb_Nulls', 'Null_Col'], axis=1).corr();\n    sns.heatmap(_corr, annot= True, fmt= '.0%', linewidth= 0.35, center= True, cmap= 'icefire',\n                linecolor= 'black', mask = np.triu(np.ones_like(_corr)),ax= ax);\n    ax.set_title(f\"\\nCorrelation plot for nulls = {nb_nulls}\\n\", color= 'tab:blue');\n    plt.tight_layout();\n    plt.yticks(rotation= 0);\n    plt.xticks(rotation= 90);\n    plt.show();\n    del _corr;\n    collect();\n    \ndel _;\ncollect();","metadata":{"execution":{"iopub.status.busy":"2022-06-28T11:10:44.710681Z","iopub.execute_input":"2022-06-28T11:10:44.711298Z","iopub.status.idle":"2022-06-28T11:10:53.298114Z","shell.execute_reply.started":"2022-06-28T11:10:44.711254Z","shell.execute_reply":"2022-06-28T11:10:53.29676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Data distribution analysis for all features:-","metadata":{}},{"cell_type":"code","source":"# Analyzing the data distributions per column group with continuous columns:-\nfor ftre_grp_nb in tqdm([1,3,4]):\n    fig, axes = plt.subplots(1,1, figsize= (30,30));\n\n    nplots= len(Ftre_Grp_Dict[ftre_grp_nb]);\n    ncols= 4;\n    nrows= int(np.ceil(nplots/ ncols));\n    \n    print(colored(f\"\\nDistributions for feature group {ftre_grp_nb}\\n\", \n                  color= 'blue', attrs= ['bold']));\n    for i , col in tqdm(enumerate(Ftre_Grp_Dict[ftre_grp_nb])):\n        plt.subplot(nrows, ncols, i+1);\n        sns.histplot(x=xytrain[col].values, bins=100, color = 'tab:blue');\n    plt.show();","metadata":{"execution":{"iopub.status.busy":"2022-06-21T14:12:14.087413Z","iopub.execute_input":"2022-06-21T14:12:14.087747Z","iopub.status.idle":"2022-06-21T14:12:37.938438Z","shell.execute_reply.started":"2022-06-21T14:12:14.087717Z","shell.execute_reply":"2022-06-21T14:12:37.937507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig,ax= plt.subplots(1,1, figsize= (16,6));\nxytrain_desc_sum.loc[(np.abs(xytrain_desc_sum.skewness) >= 0.10) \n                     & (xytrain_desc_sum.index.str[0:3] != 'F_2'), 'skewness'].plot.bar(ax=ax);\nax.set_title(f\"Non-normally distributed columns in groups F_1, F_3 and F_4\\n\", \n             color= 'tab:blue', fontsize= 12);\nax.grid(visible= True, which= 'both', color= 'grey', linestyle = '--', linewidth= 0.75);\nax.set_yticks(np.arange(-1.0, 0.50, 0.1), fontsize= 8);\n\nplt.tight_layout();\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2022-06-21T14:12:37.939758Z","iopub.execute_input":"2022-06-21T14:12:37.940699Z","iopub.status.idle":"2022-06-21T14:12:38.290764Z","shell.execute_reply.started":"2022-06-21T14:12:37.940654Z","shell.execute_reply":"2022-06-21T14:12:38.289905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Analyzing discrete column distributions in group 2:-\nprint(colored(f\"Value-counts for feature group 2\\n\", color= 'blue', attrs= ['bold']));\nfig, axes = plt.subplots(5,5, figsize= (31,31));\n\nfor i, col in tqdm(enumerate(Ftre_Grp_Dict.get(2))):\n    xytrain[col].value_counts().sort_index(ascending= True).plot.bar(ax=axes[i//5, i%5]);\n    axes[i//5, i%5].grid(visible= True, which= 'both', color= 'lightgrey', \n                         linewidth = 0.75, linestyle= '--');\n    axes[i//5, i%5].set(xlabel= '', ylabel='');\nplt.tight_layout();\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2022-06-21T14:12:38.292162Z","iopub.execute_input":"2022-06-21T14:12:38.292473Z","iopub.status.idle":"2022-06-21T14:12:42.391213Z","shell.execute_reply.started":"2022-06-21T14:12:38.292446Z","shell.execute_reply":"2022-06-21T14:12:42.390538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting the positions of nulls in the data-set:-\nfor i in tqdm([1,3,4]):\n    fig, ax= plt.subplots(1,1, figsize= (25,20));\n    sns.heatmap(data= xytrain[Ftre_Grp_Dict[i]].isna(), \n                cmap= 'binary', cbar_kws={'label': 'Missing Data'},ax= ax);\n    ax.set_title(f\"\\nMissing value heatmap for group {i}\\n\", fontsize= 12, color= 'tab:blue');\n    ax.set(ylabel='');\n    plt.xticks(rotation = 45, fontsize= 7);\n    plt.show();","metadata":{"execution":{"iopub.status.busy":"2022-06-21T14:12:42.392251Z","iopub.execute_input":"2022-06-21T14:12:42.392975Z","iopub.status.idle":"2022-06-21T14:13:34.918393Z","shell.execute_reply.started":"2022-06-21T14:12:42.392943Z","shell.execute_reply":"2022-06-21T14:13:34.917611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Feature line-plots creation to exhibit possible periodicity patterns:-","metadata":{}},{"cell_type":"code","source":"# Plotting line-plots for the features in the given group for potential time-series data/ periodicity detection:-\ndef Plot_Ftre(grp_nb: np.int8, color: str= 'blue', sample_size: list= [100,1000,10000]):\n    \"\"\"\n    This function plots the features specified in the group to elicit patterns/periodicity.\n    This is used to check for potential time series data/ null value filling algorithm choice\n    \"\"\";\n    \n    print(colored(f\"Feature group {grp_nb} lineplots\\n\", color = 'green', attrs= ['dark', 'bold']));\n\n    for chunk_size in tqdm(sample_size):\n        print(colored(f\"\\nCurrent chunk size = {chunk_size}\\n\", color = 'red', attrs= ['dark', 'bold']));\n\n        for ftre in tqdm(Ftre_Grp_Dict.get(4)):\n            y= xytrain['F_4_0'].dropna()[0:chunk_size];\n            fig, ax= plt.subplots(1,1, figsize= (25, 7.5));\n            sns.lineplot(y= y, x= y.index, color = color, linestyle= '-', linewidth = 1.0, ax= ax);\n            ax.set_title(f\"\\nSample lineplot for {ftre}\\n\", color= color, fontsize= 12);\n            ax.grid(visible= True, which= 'both', color= 'grey', linestyle= '--', linewidth= 0.50);\n            ax.set(ylabel= '', xlabel= '');\n            ax.set_xticks(range(0,chunk_size + 1, int(chunk_size/10)), fontsize= 8);\n\n            plt.xticks(rotation= 45);\n            plt.tight_layout();\n            plt.show();\n            del y;\n            collect();\n        collect();\n    collect();","metadata":{"execution":{"iopub.status.busy":"2022-06-21T14:10:21.668736Z","iopub.status.idle":"2022-06-21T14:10:21.669099Z","shell.execute_reply.started":"2022-06-21T14:10:21.668935Z","shell.execute_reply":"2022-06-21T14:10:21.668954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Plot_Ftre(grp_nb=1, color= 'crimson');\ncollect();","metadata":{"execution":{"iopub.status.busy":"2022-06-21T14:10:21.670195Z","iopub.status.idle":"2022-06-21T14:10:21.670547Z","shell.execute_reply.started":"2022-06-21T14:10:21.670393Z","shell.execute_reply":"2022-06-21T14:10:21.67041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Plot_Ftre(grp_nb=3, color = 'indigo');\ncollect();","metadata":{"execution":{"iopub.status.busy":"2022-06-21T14:10:21.672073Z","iopub.status.idle":"2022-06-21T14:10:21.672427Z","shell.execute_reply.started":"2022-06-21T14:10:21.672258Z","shell.execute_reply":"2022-06-21T14:10:21.672275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Plot_Ftre(grp_nb=4, color = 'teal');\ncollect();","metadata":{"execution":{"iopub.status.busy":"2022-06-21T14:10:21.673481Z","iopub.status.idle":"2022-06-21T14:10:21.673824Z","shell.execute_reply.started":"2022-06-21T14:10:21.673669Z","shell.execute_reply":"2022-06-21T14:10:21.673686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Correlation heatmaps for potential linear relations between all features and within a group:-","metadata":{}},{"cell_type":"code","source":"# Plotting correlation matrices across all feature groups:-\nfig, ax= plt.subplots(1,1, figsize= (30,20));\nsns.heatmap(xytrain.drop('Nb_Nulls', axis=1).\\\n            corr(method = 'pearson'),linewidths= 0.50, linecolor=\"black\",\n            square= True, cmap= 'Blues', annot= False, ax= ax);   \nax.set_title(f\"\\n{method.capitalize()} correlation heatmap- full data\\n\", \n         fontsize= 12, color= 'tab:blue');  \nplt.xticks(rotation= 45);\nplt.yticks(rotation= 0);\nplt.tight_layout();\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2022-06-21T14:13:43.954097Z","iopub.execute_input":"2022-06-21T14:13:43.954524Z","iopub.status.idle":"2022-06-21T14:14:04.217591Z","shell.execute_reply.started":"2022-06-21T14:13:43.95449Z","shell.execute_reply":"2022-06-21T14:14:04.216644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting correlation matrices across feature groups:-\nfor method in tqdm(['pearson', 'spearman', 'kendall']):\n    _ = xytrain[Ftre_Grp_Dict[4]].corr(method= method);\n    fig, ax= plt.subplots(1,1, figsize= (42,16));\n    sns.heatmap(_,fmt=\".1%\", mask= np.triu(np.ones_like(_)),linewidths= 0.75,\n                linecolor=\"black\", square= True,cmap= 'Spectral', annot= True, ax= ax);   \n    ax.set_title(f\"\\n{method.capitalize()} correlation heatmap-group 4\\n\", \n                 fontsize= 12, color= 'black');  \n    plt.xticks(rotation= 45);\n    plt.yticks(rotation= 0);\n    plt.show();\n    del _;\n\ncollect();","metadata":{"execution":{"iopub.status.busy":"2022-06-21T14:20:57.560926Z","iopub.execute_input":"2022-06-21T14:20:57.561552Z","iopub.status.idle":"2022-06-21T14:21:49.72232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Key inferences from EDA:- \n\n1. F_2 columns are integer encoded columns for categorical features. They don't have any null features\n2. F_1, F_3 and F_4 features are float columns, we have reduced the memory size of these columns without any major information loss. \n3. F_1 and F_3 columns are not correlated with themselves or with other table columns. Imputing their column missing values with descriptive statistics values (mean, median, IQR, constant (like 0)) could be a good idea here. We will try various approaches for each column herewith and evaluate the best option thereby. **These columns could be considered as 'missing completely at random'**\n4. F_4 set of columns exhibit some level of correlation among themselves. Model development for each F_4 set columns (eg. F_4_0) as a function of other F_4 group columns could be an option. Missing values across each column (index) is the test-set for the regression model. Standard machine learning models like ensemble trees/ neural networks could be used. **According to the literature for null values, these columns could be considered as 'missing at random'**. In total, 703 combinations of null valued columns exist in feature group 4.\n5. Location of nulls throughout the table is checked and no specific pattern is elicited. Nulls are scattered throughout the table, except for the columns in feature group 2\n6. *Columns in feature group F_2 behave strangely. They don't have nulls, are categorical encoded values and are of the integer type versus float otherwise. Do they hold an unravelled mystery? Am I searching incorrectly? Am I missing out on something? Thoughts? Comments? Why are these columns even included in the table if they offer a nondescript contribution to the model?*","metadata":{}},{"cell_type":"markdown","source":"## 4. Imputation for feature groups 1 and 3:-\n\nIn this section, we will impute the nulls across the feature groups 1,3 using the descriptive statistics measures and elicit the best method for each column in these groups","metadata":{}},{"cell_type":"code","source":"# Creating output data-set with all relavant columns, options and relavant MSE:-\ndesc_stat_sum = np.zeros((0,6), dtype= np.float32);\n\n# Creating groups 1,3 features:-\ngrp13_ftre = list(Ftre_Grp_Dict.get(1)) + list(Ftre_Grp_Dict.get(3));","metadata":{"execution":{"iopub.status.busy":"2022-06-21T14:10:21.677793Z","iopub.status.idle":"2022-06-21T14:10:21.678137Z","shell.execute_reply.started":"2022-06-21T14:10:21.67798Z","shell.execute_reply":"2022-06-21T14:10:21.677997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for ftre in tqdm(grp13_ftre):\n    print(colored(f\"Current feature = {ftre}\", color = 'blue', attrs= ['dark', 'bold']));\n    y = xytrain[ftre].dropna();\n    \n    for fold_nb, (train_idx, dev_idx) in enumerate(\n        KFold(n_splits=5, shuffle= True, random_state= 10).split(y)):\n\n        print(colored(f\"Current fold = {fold_nb}\", color = 'red' , attrs= ['dark']));\n        \n        ytrain, ydev = y.values[train_idx], y.values[dev_idx];\n        mse_mean = mean_squared_error(ydev, np.full(len(ydev), np.mean(ytrain)));\n        mse_median = mean_squared_error(ydev, np.full(len(ydev), np.median(ytrain)));\n        mse_iqr = mean_squared_error(ydev, np.full(len(ydev), iqr(ytrain)));\n        mse_zero = mean_squared_error(ydev, np.zeros(len(ydev)));\n        \n        desc_stat_sum= np.vstack((desc_stat_sum,\n                                 np.array([ftre, fold_nb, mse_mean, mse_median, mse_iqr, mse_zero])));\n        del mse_mean, mse_median, mse_iqr, mse_zero;  ","metadata":{"execution":{"iopub.status.busy":"2022-06-21T14:10:21.679147Z","iopub.status.idle":"2022-06-21T14:10:21.679499Z","shell.execute_reply.started":"2022-06-21T14:10:21.679341Z","shell.execute_reply":"2022-06-21T14:10:21.679359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Converting the array to a dataframe and displaying results with best imputation method:-\ndesc_stat_sum = pd.DataFrame(desc_stat_sum, \n                             columns = ['ftre_nm', 'fold_nb', 'mean', 'median', 'iqr', 'zero']);\ndesc_stat_sum[['mean', 'median', 'iqr', 'zero']] = \\\ndesc_stat_sum[['mean', 'median', 'iqr', 'zero']].astype(np.float32);\ndesc_stat_sum['imp_mthd_lbl'] = desc_stat_sum[['mean', 'median', 'iqr', 'zero']].idxmin(axis=1);\n\nimp_mthd_sum = \\\ndesc_stat_sum[['ftre_nm', 'imp_mthd_lbl']].groupby('ftre_nm').agg({'imp_mthd_lbl': lambda x: x.mode()});\n\n# plotting the best imputation by feature:-\nfig, ax= plt.subplots(1,1, figsize= (5,5));\nimp_mthd_sum['imp_mthd_lbl'].value_counts().plot.bar(ax= ax);\nax.set_title(f\"Imputation best option per feature\\n\", fontsize= 12, color= 'tab:blue');\nax.grid(visible= True, which= 'both', color= 'grey', linestyle= '--', linewidth= 0.75);\nplt.tight_layout();\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2022-06-21T14:10:21.680609Z","iopub.status.idle":"2022-06-21T14:10:21.680949Z","shell.execute_reply.started":"2022-06-21T14:10:21.680797Z","shell.execute_reply":"2022-06-21T14:10:21.680815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Implementation steps:- \n\n1. Collate the dataset sample with null records from the reference column. This will become the index for the submission file\n2. Impute the values of the null with the chosen strategy. This depends on the results from the previous step\n3. Align the imputed data to the master submission table","metadata":{}},{"cell_type":"code","source":"# Creating output data-set to store the imputed values:-\nSubmission = pd.DataFrame(data= None, columns= sub_fl.iloc[0:5,:].columns);\n\n# Imputing the respective columns with the chosen method:-\nprint(colored(f\"Missing value imputation for groups 1,3\\n\", \n              color = 'blue', attrs= ['dark', 'bold']));\n\nfor ftre in tqdm(grp13_ftre):\n    print(colored(f\"Current feature = {ftre}\", color = 'red', attrs= ['dark']));\n    _ = xytrain[[ftre]].loc[xytrain[ftre].isna()];\n    _['row-col'] = _.index.map(str) + '-' + ftre;\n    _['value']= \\\n    np.select(\\\n    [(imp_mthd_sum.loc[ftre] == 'zero'), (imp_mthd_sum.loc[ftre] == 'mean'), \n     (imp_mthd_sum.loc[ftre] == 'median')],\n    [np.zeros(len(_)),  \n     np.full(len(_), np.mean(xytrain[ftre].dropna())), \n     np.full(len(_), np.median(xytrain[ftre].dropna()))]\n    );\n\n    Submission = \\\n    pd.concat((Submission, _.drop([ftre], axis=1)), axis=0, ignore_index= True);\n    \nprint(colored(f\"\\nTotal missing values imputed = {len(Submission):,.0f}\\n\", \n              color = 'blue', attrs= ['dark', 'bold']));\n\ncollect();","metadata":{"execution":{"iopub.status.busy":"2022-06-21T14:10:21.682008Z","iopub.status.idle":"2022-06-21T14:10:21.68237Z","shell.execute_reply.started":"2022-06-21T14:10:21.682187Z","shell.execute_reply":"2022-06-21T14:10:21.682204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Submission.to_csv(\"Submission_Grp13.csv\", index= False);","metadata":{"execution":{"iopub.status.busy":"2022-06-21T14:10:21.683442Z","iopub.status.idle":"2022-06-21T14:10:21.683778Z","shell.execute_reply.started":"2022-06-21T14:10:21.683618Z","shell.execute_reply":"2022-06-21T14:10:21.683635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The next notebook in this series will develop a model for the F_4_ group using the other F_4_ columns. We will explore a variety of ML models and a neural network for the same.**","metadata":{}}]}