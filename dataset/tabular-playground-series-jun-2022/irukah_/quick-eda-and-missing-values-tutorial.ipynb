{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div style=\"padding:20px;color:white;margin:0;font-size:200%;text-align:center;display:fill;border-radius:5px;background-color:#196F3D;overflow:hidden;font-weight:500\">TPS May 2022</div>\n<br>\n<br>\n<br>\n<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#52BE80;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:black;\"><b>1.0 | Introduction</b></p>\n</div>\n\nThank you for sharing a nice notebook! >>> https://www.kaggle.com/code/abdulravoofshaik/quick-eda-and-missing-values-tutorial  \nTranslate notebook into Japanese.\n\nData Cleaning is the process of finding and correcting the inaccurate/incorrect data that are present in the dataset. One such process needed is to do something about the values that are missing in the dataset. In real life, many datasets will have many missing values, so dealing with them is an important step.\n\nデータクリーニングとはデータセットに存在する不正確なデータや間違ったデータを見つけ出し、修正するプロセスである。  \n今回のプロセスはデータセット内の欠損値を適切な値にすること -> 代入(imputation)することである。","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom tqdm import tqdm\nimport missingno as msno # 欠損値の出現パターンを簡単に可視化するライブラリ\nfrom sklearn.impute import SimpleImputer # 欠損値を埋める機械学習ライブラリ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-12T10:48:54.892538Z","iopub.execute_input":"2022-06-12T10:48:54.893272Z","iopub.status.idle":"2022-06-12T10:48:56.417556Z","shell.execute_reply.started":"2022-06-12T10:48:54.893175Z","shell.execute_reply":"2022-06-12T10:48:56.416453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#52BE80;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:black;\"><b>2.0 | Load data and basic checks</b></p>\n</div>\n\nデータの読み込みと基本チェック","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv(\"../input/tabular-playground-series-jun-2022/data.csv\")\nprint(\"Data Shape: There are {:,.0f} rows and {:,.0f} columns.\\nMissing values = {}, Duplicates = {}.\\n\".\n      format(data.shape[0], data.shape[1],data.isna().sum().sum(), data.duplicated().sum()))\nprint('データ形式: 1000000行81列\\n欠損値がある行1000000, 重複がある行0')\n\ndf=data.describe()\ndisplay(df.style.format('{:,.3f}')\n        .background_gradient(subset=(df.index[1:],df.columns[:]), cmap='GnBu'))","metadata":{"execution":{"iopub.status.busy":"2022-06-12T10:48:56.419446Z","iopub.execute_input":"2022-06-12T10:48:56.41991Z","iopub.status.idle":"2022-06-12T10:49:26.552327Z","shell.execute_reply.started":"2022-06-12T10:48:56.419868Z","shell.execute_reply":"2022-06-12T10:49:26.551562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get the number of missing data points per column\n# 列ごとの欠損値データを取得する\nmissing_values_count = data.isnull().sum()\n\n# look at the # of missing points in the first ten columns\n# 最初の30カラムをみてみる\nmissing_values_count[0:30]","metadata":{"execution":{"iopub.status.busy":"2022-06-12T10:49:26.553494Z","iopub.execute_input":"2022-06-12T10:49:26.554276Z","iopub.status.idle":"2022-06-12T10:49:26.695093Z","shell.execute_reply.started":"2022-06-12T10:49:26.554245Z","shell.execute_reply":"2022-06-12T10:49:26.694151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Quick insight:\nAs we can see above, some of the columns have no missing values, which is a good sign. We need to see if any of the columns are correlated.\n\n考察： 上記のようにいくつかの列には欠損値がなく、これは良い兆候にある。列のどれが相関しているか確認する必要がある。","metadata":{}},{"cell_type":"code","source":"# how many total missing values do we have?\n# 欠損値は合計いくつあるか\ntotal_cells = np.product(data.shape)\ntotal_missing = missing_values_count.sum()\n\n# percent of data that is missing\n# 欠損値をパーセンテージで表示する\n(total_missing/total_cells) * 100","metadata":{"execution":{"iopub.status.busy":"2022-06-12T10:49:26.697185Z","iopub.execute_input":"2022-06-12T10:49:26.697686Z","iopub.status.idle":"2022-06-12T10:49:26.704957Z","shell.execute_reply.started":"2022-06-12T10:49:26.697646Z","shell.execute_reply":"2022-06-12T10:49:26.704119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Lets visulize first 500 rows for missing values\n## 最初の500行の欠損値を可視化する","metadata":{}},{"cell_type":"code","source":"import missingno as msno # seaborn.heatmapと同様\n# ランダムに抽出した欠損値の出現パターンを可視化\nmsno.matrix(data.sample(500))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-12T10:49:26.706336Z","iopub.execute_input":"2022-06-12T10:49:26.706888Z","iopub.status.idle":"2022-06-12T10:49:27.162775Z","shell.execute_reply.started":"2022-06-12T10:49:26.706848Z","shell.execute_reply":"2022-06-12T10:49:27.162083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Quick insights:\n1) The missing data location is vary random. We need to identify is there any pattern to it.\n\n考察：欠損データの位置はランダムに変化する。この欠損データにパターンがあるかどうかを調べる必要がある。","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport matplotlib.colors\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom plotly.offline import init_notebook_mode\nimport seaborn as sns\nprimary_bgcolor = \"#f4f0ea\"\nprimary_blue = \"#496595\"\nprimary_blue2 = \"#85a1c1\"\nprimary_blue3 = \"#3f4d63\"\nprimary_grey = \"#c6ccd8\"\nprimary_black = \"#202022\"\ncorr = data.corr().abs() # 相関係数を取得する\n# np.triu -> NumPy配列ndarrayから上三角行列を取得する\n# np.one_like -> 既存配列から要素が1の初期化配列を生成する方法\nmask = np.triu(np.ones_like(corr, dtype=np.bool)) \n\nfig, ax = plt.subplots(figsize=(90, 50), facecolor=primary_bgcolor)\n# ax.text(-1.1, 0.16, 'Correlation between the Continuous Features', fontsize=10, fontweight='bold', fontfamily='serif')\n# 連続的な特徴の相関を表示\n\nax.text(-1.1, 0.3, 'There is no features that pass more than 0.32 correlation within each other', fontsize=13, fontweight='light', fontfamily='serif')\n# 相互の相関が0.32を超える特徴は存在しない\n\n# plot heatmap\nres=sns.heatmap(corr, mask=mask, annot=True, fmt=\".2f\", cmap='coolwarm',annot_kws={\"size\": 25},\n            cbar_kws={\"shrink\": .2}, vmin=0, vmax=1)\nres.set_xticklabels(res.get_xmajorticklabels(), fontsize = 28)\nres.set_yticklabels(res.get_ymajorticklabels(), fontsize = 28)\n# yticks\nplt.yticks(rotation=0)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-12T10:49:27.164095Z","iopub.execute_input":"2022-06-12T10:49:27.164873Z","iopub.status.idle":"2022-06-12T10:49:58.747266Z","shell.execute_reply.started":"2022-06-12T10:49:27.16484Z","shell.execute_reply":"2022-06-12T10:49:58.746314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Quick insight: As we can see above some of the columns are highly correlated. We can use this informaiton to replace the missing data.\n\n考察：　いくつかの列は非常に相関がある。この情報を使って欠損地のデータを置き換えることができる。","metadata":{}},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#52BE80;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:black;\"><b>3.0 | Imputation techniques</b></p>\n</div>\nTypically replacing the missing data with new data is called imputation. There are various techniques for this task depeding on the type of the data that we have. Typically imputation can be divided into following types\n\n1) Constant value imputation: As name suggests all the missing values will be replaced with constant value.  <br> \n2) Basic statiscal value imputation: As name suggests missing valeus will be treated as the mean, median or most frequent value of that column  <br>\n3) Advanced techniques:  <br>\nWhen basic techniques does not address the problem, machine learning algorithms can be used to identify the substitute value for the missing values. \n\n一般的に欠損値を新しいデータで置換することをインピュテーションと呼ぶ。この実装方法はデータの種類に応じて様々な手法がある。  \nインピュテーションは次のタイプに分かれる。\n\n1) 定数の代入：　全ての欠損値を定数で置換する  \n2) 統計値の代入：　欠損値をその列の平均値・中央値・最頻値として扱う  \n3) その他高度な代入  \n-> 基本的な手法で対応できない場合は機械学習アルゴリズムを用いて欠損値の代入値を決める。","metadata":{}},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#52BE80;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:black;\"><b>3.1 | Basic statistics- Mean value</b></p>\n</div>\n\n基本的な統計 - 平均値","metadata":{}},{"cell_type":"code","source":"Target = pd.read_csv(\"../input/tabular-playground-series-jun-2022/sample_submission.csv\", index_col='row-col')\nTarget.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-12T10:49:58.748498Z","iopub.execute_input":"2022-06-12T10:49:58.748848Z","iopub.status.idle":"2022-06-12T10:49:59.816026Z","shell.execute_reply.started":"2022-06-12T10:49:58.748817Z","shell.execute_reply":"2022-06-12T10:49:59.81508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# data = data.sample(frac =.25)","metadata":{"execution":{"iopub.status.busy":"2022-06-12T10:49:59.817417Z","iopub.execute_input":"2022-06-12T10:49:59.818132Z","iopub.status.idle":"2022-06-12T10:49:59.824143Z","shell.execute_reply.started":"2022-06-12T10:49:59.818076Z","shell.execute_reply":"2022-06-12T10:49:59.822756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# reference notebook\n# Simpleimputer -> 欠損地の補完をする。 補完方法(mean, median, most_frequent, constant)\n    # missing_values：置き換える値\n    # strategy：統計値の種類\n    # fill_value：strategyがconstantになっている場合に、整数や文字列を指定して置き換える\nimp = SimpleImputer(\n        missing_values=np.nan,\n        strategy='mean')\ndata_mean=data.copy()\ndata_mean[:] = imp.fit_transform(data)\n# tqdm -> アラビア語（taqadum）で「進歩」を意味するらしい\n    # プログレスバーを表示できるので時間がかかる処理に有用\nfor i in tqdm(Target.index):\n    row = int(i.split('-')[0])\n    col = i.split('-')[1]\n    Target.loc[i, 'value'] = data_mean.loc[row, col]","metadata":{"execution":{"iopub.status.busy":"2022-06-12T10:49:59.825484Z","iopub.execute_input":"2022-06-12T10:49:59.825851Z","iopub.status.idle":"2022-06-12T10:51:42.489634Z","shell.execute_reply.started":"2022-06-12T10:49:59.82582Z","shell.execute_reply":"2022-06-12T10:51:42.488618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#52BE80;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:black;\"><b>3.2 | Basic statistics- Constant value</b></p>\n</div>\n\n基本的な統計 - 定数値","metadata":{}},{"cell_type":"code","source":"# imputing with a constant\nimp = SimpleImputer(\n        missing_values=np.nan,\n        strategy='constant')\ndata_constant=data.copy()\ndata_constant[:] = imp.fit_transform(data)\nfor i in tqdm(Target.index):\n    row = int(i.split('-')[0])\n    col = i.split('-')[1]\n    Target.loc[i, 'value'] = data_constant.loc[row, col]","metadata":{"execution":{"iopub.status.busy":"2022-06-12T10:51:42.492359Z","iopub.execute_input":"2022-06-12T10:51:42.492712Z","iopub.status.idle":"2022-06-12T10:53:23.500883Z","shell.execute_reply.started":"2022-06-12T10:51:42.492666Z","shell.execute_reply":"2022-06-12T10:53:23.499856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#52BE80;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:black;\"><b>3.3 | Basic statistics- Most frequent</b></p>\n</div>\n\n基本的な統計 - 最頻値","metadata":{}},{"cell_type":"code","source":"imp = SimpleImputer(\n        missing_values=np.nan,\n        strategy='most_frequent')\ndata_frequent=data.copy()\ndata_frequent[:] = imp.fit_transform(data)\nfor i in tqdm(Target.index):\n    row = int(i.split('-')[0])\n    col = i.split('-')[1]\n    Target.loc[i, 'value'] = data_frequent.loc[row, col]","metadata":{"execution":{"iopub.status.busy":"2022-06-12T10:53:23.507105Z","iopub.execute_input":"2022-06-12T10:53:23.507384Z","iopub.status.idle":"2022-06-12T10:55:14.169592Z","shell.execute_reply.started":"2022-06-12T10:53:23.507357Z","shell.execute_reply":"2022-06-12T10:55:14.168765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#52BE80;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:black;\"><b>4.1 | Advanced- K-Nearest Neighbor Imputation</b></p>\n</div>\n\n高度な代入 : k近傍法で代入","metadata":{}},{"cell_type":"code","source":"# from sklearn.impute import KNNImputer\n# data_knn = data.copy(deep=True)\n\n# knn_imputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\n# #tranforming individual column seperately\n# for column in data_knn:   \n#     data_knn[column] = knn_imputer.fit_transform(data_knn[[column]])\n# for i in tqdm(Target.index):\n#     row = int(i.split('-')[0])\n#     col = i.split('-')[1]\n#     Target.loc[i, 'value'] = data_knn.loc[row, col]","metadata":{"execution":{"iopub.status.busy":"2022-06-12T10:55:14.173094Z","iopub.execute_input":"2022-06-12T10:55:14.173394Z","iopub.status.idle":"2022-06-12T10:55:14.177884Z","shell.execute_reply.started":"2022-06-12T10:55:14.173366Z","shell.execute_reply":"2022-06-12T10:55:14.176953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Notes: For quick EDA I have used uniform weights. We should try various options to see which one fit the best. This technique takes extremly longtime for large datasets. I have commented it out to minimze the runtime.\n\nEDA処理を早くするために均一なウェイトを使用した。様々なオプションを試してどれが一番合うか確認する必要がある。  \nこの手法は大きなデータセットでは時間がかかるのでコメントアウトしている。","metadata":{}},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#52BE80;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:black;\"><b>4.2 | Advanced- MICE</b></p>\n</div>\n\n* MICE: 多重補完法(Multivariate Imputation by Chained Equations)\n\nHere Round-robin technique is used where the missing values are replaced as a functio nof other features. Mutiple regression is performed over random sample of the column data and then regression values are used to replace the missing values.\n\n\nラウンドロビン法を用いて欠損値を他の特徴量の関数として置き換える。列データのランダムなサンプルに対して多重回帰を行い、回帰値を用いて欠損値を置換する。","metadata":{}},{"cell_type":"code","source":"from sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\n\ndata_MICE = data.copy(deep=True)\nmice_imputer = IterativeImputer()\n#tranforming individual column seperately\nfor column in data_MICE:   \n    data_MICE[column] = mice_imputer.fit_transform(data_MICE[[column]])\nfor i in tqdm(Target.index):\n    row = int(i.split('-')[0])\n    col = i.split('-')[1]\n    Target.loc[i, 'value'] = data_MICE.loc[row, col]","metadata":{"execution":{"iopub.status.busy":"2022-06-12T10:55:14.179124Z","iopub.execute_input":"2022-06-12T10:55:14.179468Z","iopub.status.idle":"2022-06-12T10:56:53.380033Z","shell.execute_reply.started":"2022-06-12T10:55:14.17944Z","shell.execute_reply":"2022-06-12T10:56:53.379022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#52BE80;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:black;\"><b>4.3 | Advanced- Regression</b></p>\n</div>\n \n* 回帰","metadata":{}},{"cell_type":"markdown","source":"An efficient way to handle missing values is predicting them using high performance regression techniques. This is a much more accurate solution to the problem. In this technique, one column is treated as a target at a time and try to predict the column's value using other columns. If the missing values are present only in the numerical columns, high performance regression models cand do the missing value predictions. For each column, we can take all the rows where the column value is present as the training dataset and the missing column values as the test dataset.\n\n\n欠損値を扱う効率的な方法は、高性能な回帰法を使った予測である。この手法では1度に1つの列をターゲット(目的変数)として扱い、他の列(説明変数)を使ってその列の予測をする。  \n欠損値が数値列のみの場合、この回帰モデルは欠損値予測を行うことができる。\n\n各列について、その列の値が存在する全ての行を学習データセットとして扱い、欠損値をテストデータセットとして表すことができる。","metadata":{}},{"cell_type":"markdown","source":"<img src=\"https://i.postimg.cc/90BdtTjq/imputer.gif\">\n\n","metadata":{}},{"cell_type":"markdown","source":"### Here I have applied aforementioned regression technique to current data.\n\nここでは前述の回帰手法を現在のデータに適用している。","metadata":{}},{"cell_type":"code","source":"# from distutils.dir_util import copy_tree\n\n# from_dir = '../input/tpsjune2022advanced-imputation'\n# to_dir = './'\n\n# copy_tree(from_dir, to_dir)","metadata":{"execution":{"iopub.status.busy":"2022-06-12T10:57:24.976664Z","iopub.execute_input":"2022-06-12T10:57:24.97706Z","iopub.status.idle":"2022-06-12T10:57:24.982188Z","shell.execute_reply.started":"2022-06-12T10:57:24.97703Z","shell.execute_reply":"2022-06-12T10:57:24.981412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Full implimentation of above technique is shown in this public [notebook](https://www.kaggle.com/code/abdulravoofshaik/top-3-solution-lgbm-mean) which has resulted in very good score when compared with other techniques. Note that I have used LGBM, but you can try other techniques such as XGboost, RandomForest, Catboost....etc.\n\nこのノートブックでは上記の手法を完全に実装しており、他の手法と比較して非常に良いスコアを得られている。  \nLGBMを使っているがXGBoost, RandomForest, Catboostなどの手法も試すことができる。","metadata":{}},{"cell_type":"markdown","source":"### Well, there is no single best way to handle missing values. One needs to experiment with different methods and then decide which method is best for a particular problem. \n\n### 欠損値を処理する最良の方法は1つではない。 いろいろな方法を試してみて、特定の問題に対してどの方法が一番良いかを決めることが大事である。","metadata":{}},{"cell_type":"markdown","source":"## Work in progress","metadata":{}},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#52BE80;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:black;\"><b>5.0 | References</b></p>\n</div>","metadata":{}},{"cell_type":"markdown","source":"https://towardsdatascience.com/using-the-missingno-python-library-to-identify-and-visualise-missing-data-prior-to-machine-learning-34c8c5b5f009 <br>\nhttps://towardsdatascience.com/how-to-handle-missing-data-8646b18db0d4 <br>\nhttps://www.kaggle.com/code/parulpandey/a-guide-to-handling-missing-values-in-python/notebook <br>\nhttps://www.kaggle.com/code/residentmario/using-missingno-to-diagnose-data-sparsity/notebook <br>\nhttps://www.analyticsvidhya.com/blog/2021/05/dealing-with-missing-values-in-python-a-complete-guide/ <br>\nhttps://www.kaggle.com/competitions/tabular-playground-series-jun-2022/discussion/328369 <br>\n\n","metadata":{}}]}