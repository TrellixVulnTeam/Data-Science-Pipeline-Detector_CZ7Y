{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<font color=\"lightseagreen\" size=+3.5><b> JUNE 2022 TPS: Missing Value Imputation Challenge</b></font>\n\n---\n---\n<a id=\"1\"></a>\n<font color=\"lightseagreen\" size=+2.0><b> Introduction</b></font>\n\nIn this month's Tabular Playground series we are tasked with data imputation challeng. According to the hosts, the dataset has similarities to the May 2022 Tabular Playground, except that there are no targets; instead, there are missing data values in the dataset, and our task is to predict what these values should be.\n\n**Evaluation Metric** : Submissions are scored on the root mean squared error (RMSE).\n\n---\n\n<font color=\"lightseagreen\" size=+2.0><b> Table of Contents</b></font>\n    \n* [1. Introduction](#1)\n* [2. Data Overview](#2)\n* [3. Missing Values](#3)\n* [4. Imputing Missing Values](#4)\n* [5. Reference](#5)\n\n---\n---","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd \n\nfrom tqdm.notebook import tqdm\nimport lightgbm\nimport xgboost\nimport catboost\n\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.figure_factory as ff\nimport plotly.graph_objects as go\nfrom matplotlib.ticker import FormatStrFormatter\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-17T23:00:22.440699Z","iopub.execute_input":"2022-06-17T23:00:22.441216Z","iopub.status.idle":"2022-06-17T23:00:27.558759Z","shell.execute_reply.started":"2022-06-17T23:00:22.441114Z","shell.execute_reply":"2022-06-17T23:00:27.557692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub =  pd.read_csv('../input/tabular-playground-series-jun-2022/sample_submission.csv', index_col='row-col')\ndf = pd.read_csv('../input/tabular-playground-series-jun-2022/data.csv', index_col='row_id')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-17T23:00:27.561239Z","iopub.execute_input":"2022-06-17T23:00:27.562094Z","iopub.status.idle":"2022-06-17T23:00:31.764492Z","shell.execute_reply.started":"2022-06-17T23:00:27.562047Z","shell.execute_reply":"2022-06-17T23:00:31.761594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2\"></a>\n<font color=\"lightseagreen\" size=+2.5><b>2. Data Overview</b></font>\n\n- The dataset has `1 million rows` and `80 columns` excluding the `row_id` column.\n- All columns have numerical values. Features starting with `F_2` are on **int type**. The rest are of **float type**.\n- The max (31.23) and min (-26.28) values of the dataset occur in the same columns, `F_4_11`\n","metadata":{}},{"cell_type":"code","source":"df.info()","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-17T23:00:31.765332Z","iopub.status.idle":"2022-06-17T23:00:31.765775Z","shell.execute_reply.started":"2022-06-17T23:00:31.765572Z","shell.execute_reply":"2022-06-17T23:00:31.765593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(df.shape)\ndisplay(df.head())","metadata":{"execution":{"iopub.status.busy":"2022-06-17T23:00:31.767673Z","iopub.status.idle":"2022-06-17T23:00:31.768521Z","shell.execute_reply.started":"2022-06-17T23:00:31.768045Z","shell.execute_reply":"2022-06-17T23:00:31.768078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe().T.sort_values(by='mean' , ascending = False)\\\n.style.background_gradient(cmap='Greys')\\\n.bar(subset=[\"mean\",], color='#6495ED')\\\n.bar(subset=[\"max\"], color='#ff355d')","metadata":{"execution":{"iopub.status.busy":"2022-06-17T23:00:31.770566Z","iopub.status.idle":"2022-06-17T23:00:31.771332Z","shell.execute_reply.started":"2022-06-17T23:00:31.770949Z","shell.execute_reply":"2022-06-17T23:00:31.770982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"3\"></a>\n<font color=\"lightseagreen\" size=+2.5><b>3. Missing Values</b></font>\n\n- The challenge of this competition is about filling the missing values. But not all the columns have missing values.\n- Features starting with `F_1, F_3 and F_4` do have missing values, but features starting with `F_2` (a total of 25 features) do not.\n- The percentage of missing values is just around 1.8% for the columns which have missing values in them.\n","metadata":{}},{"cell_type":"code","source":"def null_value_df(data):    \n    null_values_df = []    \n    for col in data.columns[:]:\n        pct_na = np.round((100 * (data[col].isna().sum())/len(data)), 2)\n        avg = data[col].mean()\n        max_ = data[col].max()\n        min_ = data[col].min()\n\n        dict1 ={\n            'Features' : col,\n            'NA (count)': data[col].isna().sum(),\n            'NA (%)': (pct_na),\n            'avg' : avg,\n            'min' : min_,\n            'max' : max_\n        }\n        null_values_df.append(dict1)\n    return pd.DataFrame(null_values_df, index=None)\n\nDF1 = null_value_df(df)\n\nfig = go.Figure(data=[go.Scatter(x=DF1['Features'],\n                             y=DF1[\"NA (%)\"],                              \n                             name='train', mode='markers', marker_color='lightseagreen'),\n                      ])\nfig.add_vrect(\n    x0=15, x1=39,\n    annotation_text=\"No Missing Values Area\", annotation_position=\"top\",\n    fillcolor=\"lightgray\", opacity=0.5,\n    layer=\"below\", line_width=0,\n),\nfig.update_layout(title_text='<b> Percentage of missing values <b>',\n                  font_family=\"San Serif\",\n                  template='simple_white',\n                  width=850, height=400,\n                  xaxis_title='Features', \n                  yaxis_title='Missing Values (%)',\n                  titlefont={'color':'black', 'size': 24, 'family': 'San-Serif'})\nfig.update_yaxes(showgrid=False, showline=False, showticklabels=True)\nfig.update_xaxes(showgrid=False, showline=True, showticklabels=True)\nfig.show()\n\nfig = go.Figure(data=[go.Scatter(x=DF1['Features'],\n                             y=DF1[\"avg\"],                              \n                             name='Avg', mode='markers', marker_color='lightseagreen'),\n                      go.Scatter(x=DF1['Features'],\n                             y=DF1[\"min\"],                              \n                             name='Min', mode='markers', marker_color='salmon'),\n                      go.Scatter(x=DF1['Features'],\n                             y=DF1[\"max\"],                              \n                             name='Max', mode='markers', marker_color='gold'),\n                      \n                      ])\nfig.add_vrect(\n    x0=15, x1=39,\n    annotation_text=\"No Missing Values Area\", annotation_position=\"top\",\n    fillcolor=\"lightgray\", opacity=0.5,\n    layer=\"below\", line_width=0,\n),\nfig.add_vrect(\n    x0=65, x1=80,\n    annotation_text=\"High variance region\", annotation_position=\"top\",\n    fillcolor=\"lightgray\", opacity=0.1,\n    layer=\"below\", line_width=0,\n),\n\nfig.update_layout(title_text='<b> Column-wise Avg/Max/Min <b>',\n                  font_family=\"San Serif\",\n                  template='simple_white',\n                  showlegend =True,\n                  width=850, height=400,\n                  xaxis_title='Features', \n                  yaxis_title='Column Stats (mean, min, max)',\n                  titlefont={'color':'black', 'size': 24, 'family': 'San-Serif'})\nfig.update_yaxes(showgrid=False, showline=False, showticklabels=True)\nfig.update_xaxes(showgrid=False, showline=True, showticklabels=True)\nfig.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-17T23:00:31.773853Z","iopub.status.idle":"2022-06-17T23:00:31.774663Z","shell.execute_reply.started":"2022-06-17T23:00:31.774335Z","shell.execute_reply":"2022-06-17T23:00:31.774368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n<font color=\"lightseagreen\" size=+1.5><b>Observations</b></font>\n\n- We see a kind of symmetry with the max and min values of columns with missing values which we do not see when columns have no missing value in them. We also see that the min values for all columsn with no missing values is ZERO, but the other group (cols with missing values) have negative values in them. Finding an imputation strategy which can reverse this symmetry could be helpful if anyone can find it.\n- We also observe that the variance in features which starts with `F_4_x` as compared to the other missing-value features such as `F_1_x` and `F_3_x`\n","metadata":{}},{"cell_type":"code","source":"def density_plot(df, title):     \n       \n    fig, ax = plt.subplots(8, 10, figsize=(24, 16), sharey=False, facecolor='#dddddd')\n    \n    fig.subplots_adjust(top=0.90)\n    i = 1\n    for feature in df.columns:\n        plt.subplot(8, 10, i)\n        ax = sns.kdeplot(df[feature], shade=True,  color='#6495ED',  alpha=0.85, label='train')\n        ax.yaxis.set_major_formatter(FormatStrFormatter('%.0f'))\n        ax.xaxis.set_label_position('top')\n        \n        if feature.startswith('F_2') :           \n            ax.set_facecolor('lightsalmon')\n            \n        ax.set_ylabel('')\n        ax.set_yticks([])        \n        ax.set_xticks([])\n        i += 1\n\n    plt.suptitle(title, fontsize=20)\n    plt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-17T23:00:31.77615Z","iopub.status.idle":"2022-06-17T23:00:31.776943Z","shell.execute_reply.started":"2022-06-17T23:00:31.776617Z","shell.execute_reply":"2022-06-17T23:00:31.776667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"density_plot(df.sample(frac=0.05), title='Density Plot: All Features')","metadata":{"execution":{"iopub.status.busy":"2022-06-17T23:00:31.778356Z","iopub.status.idle":"2022-06-17T23:00:31.779174Z","shell.execute_reply.started":"2022-06-17T23:00:31.778854Z","shell.execute_reply":"2022-06-17T23:00:31.778897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font color=\"lightseagreen\" size=+1.5><b>Observations</b></font>\n\n- The distribution of values in columns with missing values is close to normal. Whereas those columns with No missing values have right-skewed distribution. This could again be important in choosing imputing strategies.\n- The spiky distribustions of `F_2_x` features suggests that these features might be of categorical type. Let's check the unique values in each of those coloumns. \n- We see that the unique values of each `F_2_x` feature is between 11 and 18 confirming that these are categorical features.","metadata":{}},{"cell_type":"code","source":"def df_unique(df):\n    df_unique = []\n    for feature in df.columns:\n        if feature.startswith('F_2') : \n\n            dict1 ={\n                'Features' : feature,\n                'Unique': df[feature].nunique(),            \n            }\n            df_unique.append(dict1)\n    return pd.DataFrame(df_unique, index=None)\n\nDF1 = df_unique(df)\n\nfig = go.Figure(data=[go.Bar(x=DF1['Features'],\n                             y=DF1['Unique'],                              \n                             name='train', marker_color='lightseagreen'),\n                      ])\nfig.update_layout(title_text='<b> Unique values of Categorical Features (F_2_*)<b>',\n                  font_family=\"San Serif\",\n                  template='simple_white',\n                  width=850, height=400,\n                  xaxis_title='Features', \n                  yaxis_title='Number of unique values',\n                  titlefont={'color':'black', 'size': 24, 'family': 'San-Serif'})#.update_xaxes(categoryorder='total descending')\n\nfig.update_yaxes(showgrid=False, showline=False, showticklabels=True)\nfig.update_xaxes(showgrid=False, showline=True, showticklabels=True)\nfig.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-17T23:00:31.78068Z","iopub.status.idle":"2022-06-17T23:00:31.781545Z","shell.execute_reply.started":"2022-06-17T23:00:31.781215Z","shell.execute_reply":"2022-06-17T23:00:31.781246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(5, 5, figsize=(24, 16), sharey=False, facecolor='#dddddd')    \nfig.subplots_adjust(top=0.93)\ni = 1\nfor feature in df.columns:\n    if feature.startswith('F_2'):\n        plt.subplot(5, 5, i)\n        ax = sns.countplot(data=df.sample(frac=0.01), x=feature, color='#42ddd4')\n        ax.set_facecolor('white')\n        i += 1\nplt.suptitle('Categorical Features Count Plot (F_2_x)', fontsize=24)\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-17T23:00:31.783093Z","iopub.status.idle":"2022-06-17T23:00:31.783963Z","shell.execute_reply.started":"2022-06-17T23:00:31.783607Z","shell.execute_reply":"2022-06-17T23:00:31.78364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font color=\"lightseagreen\" size=+1.5><b>Creating New Features (stats)</b></font>\n\nLet's create additional features from simple stats, such as row-wise average, max/min ration, NA count.","metadata":{}},{"cell_type":"code","source":"df['average'] = df.mean(axis=1)\ndf['abs_min_max_ratio'] = df.max(axis=1) / np.abs(df.min(axis=1))\ndf['na_count'] = df.isnull().sum(axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-06-17T23:00:31.785472Z","iopub.status.idle":"2022-06-17T23:00:31.786293Z","shell.execute_reply.started":"2022-06-17T23:00:31.78597Z","shell.execute_reply":"2022-06-17T23:00:31.786002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font color=\"lightseagreen\" size=+1.5><b>Creating New Features (from F_2_x features)</b></font>\n\nHere I will try creating new features by combining the `F_2_x` features based on unique category  similarities. This may not make any sense at all as my basis for selecting interaction is the shape of distributions, nevertheless I will do so and get feedback from the LB score.","metadata":{}},{"cell_type":"code","source":"# additional features tried in version 4 and 5\ndf['f1318']= df['F_2_1'] + df['F_2_3'] - df['F_2_18'] #feats with 15 cats\ndf['f5617']= df['F_2_5'] + df['F_2_6'] - df['F_2_17'] #features with 13 cats\ndf['f1024']= df['F_2_10'] - df['F_2_24'] #features with 18 cats\ndf['f47']= df['F_2_4'] - df['F_2_7'] #features with 17 cats\ndf['f2923']= df['F_2_2'] + df['F_2_9'] - df['F_2_23'] #features with 12 and 11 cats\ndf['f141516']= df['F_2_14'] + df['F_2_15'] - df['F_2_16'] #features with 14 cats\ndf['f81119']= df['F_2_8'] + df['F_2_11'] - df['F_2_19'] #features with 14 cats\ndf['f012132122']= df['F_2_0'] + df['F_2_12'] + df['F_2_13'] - df['F_2_21'] - df['F_2_22']#features with 16 cats","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-17T23:00:31.787767Z","iopub.status.idle":"2022-06-17T23:00:31.788611Z","shell.execute_reply.started":"2022-06-17T23:00:31.788251Z","shell.execute_reply":"2022-06-17T23:00:31.788283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<!-- feats_2 = [feat for feat in df.columns if feat.startswith('f') | feat.startswith('F_2')]\nDF_ = df[feats_2]\n\ncorr = DF_.corr()\nmask = np.triu(np.ones_like(corr, dtype=bool))\nf, ax = plt.subplots(figsize=(16, 12))\n#cmap = sns.diverging_palette(230, 0, as_cmap=True)\nsns.heatmap(corr, mask=mask, cmap='coolwarm', vmax=1.0, vmin=-.1, center=0, annot=False,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": 0.75}); -->","metadata":{"execution":{"iopub.status.busy":"2022-06-07T11:48:19.87341Z","iopub.execute_input":"2022-06-07T11:48:19.873881Z","iopub.status.idle":"2022-06-07T11:48:24.513347Z","shell.execute_reply.started":"2022-06-07T11:48:19.87385Z","shell.execute_reply":"2022-06-07T11:48:24.512185Z"},"_kg_hide-input":true}},{"cell_type":"code","source":"fig, ax = plt.subplots(3, 3, figsize=(18, 10), sharey=False, facecolor='#dddddd')    \nfig.subplots_adjust(top=0.90)\ni = 1\nfor feature in df.columns:\n    if feature.startswith('f'):\n        plt.subplot(3, 3, i)\n        ax = sns.countplot(data=df.sample(frac=0.1), x=feature, color='#42ddd4')\n        ax.set_facecolor('white')\n        \n        ax.yaxis.set_major_formatter(FormatStrFormatter('%.0f'))\n        ax.xaxis.set_label_position('top')\n        \n        ax.set_ylabel('')\n        ax.set_yticks([])        \n        ax.set_xticks([])\n        i += 1\nplt.suptitle('Newly Created Cat. Features (from F_2_x features)', fontsize=20)\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-17T23:00:31.790124Z","iopub.status.idle":"2022-06-17T23:00:31.790959Z","shell.execute_reply.started":"2022-06-17T23:00:31.790607Z","shell.execute_reply":"2022-06-17T23:00:31.790672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"4\"></a>\n<font color=\"lightseagreen\" size=+2.5><b>4. Imputing Missing Values</b></font>\n\nThe challenge in this competition is finding the right imputation technique for the missing values. There are several out-of-the box imputing algorithms available. SimpueImputer, IterativeImputer, KNNImputer, and few others. We will see which one works better.\n\n\"For various reasons, many real world datasets contain missing values, often encoded as blanks, NaNs or other placeholders. Such datasets however are incompatible with scikit-learn estimators which assume that all values in an array are numerical, and that all have and hold meaning. A basic strategy to use incomplete datasets is to discard entire rows and/or columns containing missing values. However, this comes at the price of losing data which may be valuable (even though incomplete). A better strategy is to impute the missing values, i.e., to infer them from the known part of the data. \n\n\"One type of imputation algorithm is `univariate`, which imputes values in the `i-th` feature dimension using only non-missing values in that feature dimension (e.g. `impute.SimpleImputer`). By contrast, `multivariate` imputation algorithms use the `entire set of available feature` dimensions to estimate the missing values (e.g. `impute.IterativeImputer`)\" [[Scikit-learn](https://scikit-learn.org/stable/modules/impute.html#iterative-imputer)]\n\n\n<font color=\"lightseagreen\" size=+1.5 ><b>Iterative Imputation</b></font>","metadata":{}},{"cell_type":"code","source":"from tqdm.notebook import tqdm\n\nimport lightgbm\nimport xgboost\nimport catboost\n\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\n\nSEED = 622","metadata":{"execution":{"iopub.status.busy":"2022-06-17T23:00:31.792421Z","iopub.status.idle":"2022-06-17T23:00:31.793284Z","shell.execute_reply.started":"2022-06-17T23:00:31.792952Z","shell.execute_reply":"2022-06-17T23:00:31.792982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# code from: https://www.kaggle.com/code/hiro5299834/tps-jun-2022-iterativeimputer-baseline\n\nxgb = xgboost.XGBRegressor(\n        n_estimators=500,\n        random_state=SEED,\n        tree_method='gpu_hist',\n    )\nimp = IterativeImputer(\n    estimator=xgb,\n    missing_values=np.nan,\n    max_iter=7,\n    initial_strategy='mean',\n    imputation_order='ascending',\n    verbose=2,\n    random_state=SEED\n)\n\ndf[:] = imp.fit_transform(df)","metadata":{"execution":{"iopub.status.busy":"2022-06-17T23:00:31.794815Z","iopub.status.idle":"2022-06-17T23:00:31.795667Z","shell.execute_reply.started":"2022-06-17T23:00:31.795359Z","shell.execute_reply":"2022-06-17T23:00:31.795393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in tqdm(sub.index):\n    row = int(i.split('-')[0])\n    col = i.split('-')[1]\n    sub.loc[i, 'value'] = df.loc[row, col]\n\nsub.to_csv(\"submission_xgb_03.csv\")\nsub","metadata":{"execution":{"iopub.status.busy":"2022-06-17T23:00:31.797156Z","iopub.status.idle":"2022-06-17T23:00:31.797966Z","shell.execute_reply.started":"2022-06-17T23:00:31.797628Z","shell.execute_reply":"2022-06-17T23:00:31.79768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Score history**:\n- Original features , 7 iterations (LB = 0.92441)\n- Original + additional features, 7 iterations (LB = 0.92401)\n- Original + additional feature, 10 iterations (LB = 0.92552)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"5\"></a>\n<font color=\"lightseagreen\" size=+2.5><b>5. Reference</b></font>\n\n1. https://www.kaggle.com/competitions/tabular-playground-series-jun-2022/overview/evaluation\n\n2. https://scikit-learn.org/stable/modules/impute.html#iterative-imputer\n\n3. https://www.kaggle.com/code/hiro5299834/tps-jun-2022-iterativeimputer-baseline","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}