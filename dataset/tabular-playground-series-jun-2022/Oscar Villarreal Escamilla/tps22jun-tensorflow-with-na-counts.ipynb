{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Thank you @ehekatlact for sharing the idea of using the na count of each record, and @hiro5299834 and @edrickkesuma for the great improvements, please upvote the following notebooks:\n* https://www.kaggle.com/code/ehekatlact/tps2206-the-na-count-of-each-record-is-critical\n* https://www.kaggle.com/code/hiro5299834/tps-jun-2022-pytorch-lightning-with-na-counts\n* https://www.kaggle.com/code/edrickkesuma/np-random-top-public-notebook\n\nSince the notebooks above were made using PyTorch, I thought it would be very interesting and fun to create a TensorFlow implementation of this idea. The only modifications I made besides the translation to TensorFlow were:\n* Using weight normalization in the dense layers.\n* Pre-calculating a matrix of negative ones whose rows are randomly shuffled through \"tf.keras.utils.Sequence\" at each epoch.\n\nI tried to make it as simple as possible. I hope you find it as interesting as I did. Thank you for reading!","metadata":{}},{"cell_type":"code","source":"import os, random\nimport numpy as np, pandas as pd\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\n# Tensorflow:\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras import layers as L\nfrom tensorflow_addons.activations import mish\nfrom tensorflow_addons.layers import WeightNormalization\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau,EarlyStopping,ModelCheckpoint","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Input:\ndata = pd.read_csv('../input/tabular-playground-series-jun-2022/data.csv',index_col=0)\nsample_submission = pd.read_csv('../input/tabular-playground-series-jun-2022/sample_submission.csv',index_col=0)\n\n# Column groups:\nf1col = [x for x in data.columns if x.startswith('F_1')]\nf2col = [x for x in data.columns if x.startswith('F_2')]\nf3col = [x for x in data.columns if x.startswith('F_3')]\nf4col = [x for x in data.columns if x.startswith('F_4')]\n\n# Mean imputation for F1 and F3 columns:\ndata[f1col+f3col] = data[f1col+f3col].fillna(data[f1col+f3col].mean())\n\n# Minus one imputation of F4 columns in the testing set:\ndata_test = data.copy()\ndata_test[f4col] = data_test[f4col].fillna(-1)\n\n# Row-wise NaN counts of F4 columns:\ndata[f4col].isna().sum(axis=1).value_counts()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_model(hidden_size=256):\n    \n    # Input:\n    inputF4 = L.Input(shape=len(f4col)-1)\n    \n    # Network:\n    output = WeightNormalization(L.Dense(units=hidden_size*4,activation=mish))(inputF4)\n    output = L.BatchNormalization()(output)\n    output = WeightNormalization(L.Dense(units=hidden_size*4,activation=mish))(output)\n    output = WeightNormalization(L.Dense(units=hidden_size*2,activation=mish))(output)\n    output = WeightNormalization(L.Dense(units=hidden_size,activation=mish))(output)\n    output = L.Dense(units=1, activation='linear')(output)\n    \n    # Output:\n    model = tf.keras.Model(inputF4, output)\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n        loss='mse', metrics=[tf.keras.metrics.RootMeanSquaredError()]\n    )\n    return(model)\n\nclass CustomDataset(tf.keras.utils.Sequence):\n    \n    def __init__(self, X, y, noise, batch_size=256, random_state=42):\n        '''initialize dataset and noise positions'''\n        np.random.seed(random_state)\n        tf.random.set_seed(random_state)\n        self.X, self.y, self.noise = X, y, noise\n        self.batch_size = batch_size\n        self.on_epoch_end()\n        \n    def __len__(self):\n        '''number of batches per epoch'''\n        return self.X.shape[0] // self.batch_size\n    \n    def __getitem__(self, index):\n        '''generate batch and Randomly add -1 to the training and validation sets\n        in the same amount as expected in the testing set'''\n        batch_indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n        batch_noises = self.noiseindexes[index*self.batch_size:(index+1)*self.batch_size]\n        X_batch, y_batch = self.X[batch_indexes], self.y[batch_indexes]\n        for i in range(self.noise.shape[1]):\n            X_batch[range(self.batch_size),self.noise[batch_noises,i]] = -1  \n        return X_batch, y_batch\n    \n    def on_epoch_end(self):\n        '''shuffle the training set as well as the -1 positions'''\n        self.indexes = np.arange(len(self.X))\n        self.noiseindexes = np.arange(len(self.noise))\n        np.random.shuffle(self.indexes)\n        np.random.shuffle(self.noiseindexes)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Change the line below to \"True\" to create the models instead of loading from the dataset. I created the models in my personal computer and uploaded them afterwards since I almost run out of GPU time.","metadata":{}},{"cell_type":"code","source":"train = False","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = data.copy()\n\nfor cnt in range(5):\n    \n    # Prepare values which will be set to -1 during training:\n    random.seed(cnt)\n    np.random.seed(cnt)\n    tf.random.set_seed(cnt)\n    os.environ['PYTHONHASHSEED'] = str(cnt)\n    noise = np.array([np.random.choice(\n        len(f4col)-1, size=cnt, replace=False\n    ) for _ in range(data.shape[0])])\n    \n    # Find rows with zero-nan and with \"cnt+1\" nans:\n    rownan0 = data[f4col].isna().sum(axis=1)==0\n    rownans = data[f4col].isna().sum(axis=1)==cnt+1\n    \n    for col in f4col:\n        \n        # Get testing set from rows with the amount of nan given by the current 'cnt' value:\n        colna = data[col].isna()\n        X_test = data_test.loc[rownans&colna,f4col].drop(columns=col).values\n        \n        # If there are NaNs in the testing set:\n        if(X_test.shape[0]):\n        \n            # Get training and validation sets from zero-nan rows:\n            X = data.loc[rownan0,f4col]\n            y = X.pop(col)\n            X_train, X_valid, y_train, y_valid, noise_train, noise_valid = train_test_split(\n                X.values, y.values, noise[rownan0], train_size=0.8, random_state=cnt\n            )\n\n            # Training:\n            model = get_model()\n            if train:\n                history = model.fit(\n                    CustomDataset(X_train, y_train, noise_train), \n                    validation_data = CustomDataset(X_valid, y_valid, noise_valid),\n                    epochs=300, verbose=0, #use_multiprocessing=True, workers=4, \n                    callbacks=[\n                        ReduceLROnPlateau(monitor='val_loss',mode='min',\n                            verbose=0,factor=0.5,patience=3),\n                        EarlyStopping(mode='min',restore_best_weights=True,\n                            verbose=0,min_delta=1e-4,patience=10),\n                        ModelCheckpoint(f'model_{cnt}_{col}.hdf5',monitor='val_loss',mode='min',\n                            verbose=0,save_best_only=True,save_weights_only=True),\n                    ]\n                )\n                model.load_weights(f'model_{cnt}_{col}.hdf5')\n            else:\n                model.load_weights(f'../input/tps22juntf/model_{cnt}_{col}.hdf5')\n\n            # Performance:\n            for i in range(cnt):\n                X_valid[range(len(X_valid)),noise_valid[:,i]] = -1  \n            y_pred = model.predict(X_valid)\n            print(f'FFNN: cnt={cnt+1}, col={col}, RMSE={mean_squared_error(y_valid,y_pred,squared=False)}')        \n\n            # Inference:\n            result.loc[rownans&colna,col] = model.predict(X_test)\n            K.clear_session()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Submission:\nfor i in sample_submission.index:\n    row, col = i.split('-')\n    sample_submission.loc[i,'value'] = result.loc[int(row),col]\n    \nsample_submission.to_csv('submission.csv')\nsample_submission.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Thank you for reading! Please let me know if you have any questions or suggestions.","metadata":{}}]}