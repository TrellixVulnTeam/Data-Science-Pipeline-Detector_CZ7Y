{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# How random are the NaNs?\n\nI wanted to take a look at the distribution of the missing values to see if they are completely random.\n\nPrimarily I'm interested in the feasibility of synthesizing subsets of test data that would evaluate comparably to the LB metric. So I will investigate:\n\n- Is the distribution of NaNs in the data random?\n- If so, how sensitive is the metric to our synthesized test data?\n- Are eval metrics on the test subsets comparable to the LB metric?\n\nTLDR: \n- NaNs seem distributed completely at random.\n- using a subset of the complete records and randomly generating missing values seems to produce evaluation results that align well with leaderboard results.\n","metadata":{}},{"cell_type":"code","source":"from pathlib import Path\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\nfrom scipy.stats import binom\nimport matplotlib.pyplot as plt\n\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer\n\n# does this speed imputation up?\nfrom sklearnex import patch_sklearn\npatch_sklearn()\n\nRANDOM_STATE=42\nINPUT_PATH = Path('../input/tabular-playground-series-jun-2022')\n\npd.set_option('display.max_columns', None)\npd.set_option('display.float_format', '{:.3f}'.format)\n\nnp.random.seed(RANDOM_STATE)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-18T18:51:16.683839Z","iopub.execute_input":"2022-06-18T18:51:16.684218Z","iopub.status.idle":"2022-06-18T18:51:18.189291Z","shell.execute_reply.started":"2022-06-18T18:51:16.684142Z","shell.execute_reply":"2022-06-18T18:51:18.188111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dtypes = {'row_id' : 'int',\n          'F_2_0' : 'int', 'F_2_1' : 'int', 'F_2_2' : 'int',\n          'F_2_3' : 'int', 'F_2_4' : 'int', 'F_2_5' : 'int', \n          'F_2_6' : 'int', 'F_2_7' : 'int', 'F_2_8' : 'int',\n          'F_2_9' : 'int', 'F_2_10' : 'int', 'F_2_11' : 'int',\n          'F_2_12' : 'int', 'F_2_13' : 'int', 'F_2_14' : 'int',\n          'F_2_15' : 'int', 'F_2_16' : 'int', 'F_2_17' : 'int',\n          'F_2_18' : 'int', 'F_2_19' : 'int', 'F_2_20' : 'int',\n          'F_2_21' : 'int', 'F_2_22' : 'int', 'F_2_23' : 'int',\n          'F_2_24' : 'int'}\n\ndata = pd.read_csv(INPUT_PATH / 'data.csv', \n                   index_col='row_id',\n                   dtype = dtypes)\n\nsubmission = pd.read_csv(INPUT_PATH / 'sample_submission.csv', \n                         index_col='row-col')","metadata":{"execution":{"iopub.status.busy":"2022-06-18T18:51:18.190456Z","iopub.execute_input":"2022-06-18T18:51:18.191122Z","iopub.status.idle":"2022-06-18T18:51:36.62742Z","shell.execute_reply.started":"2022-06-18T18:51:18.191085Z","shell.execute_reply":"2022-06-18T18:51:36.626334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def cols_by_prefix(columns, prefix):\n    return [x for x in columns if x.startswith(prefix)]\n\ncols_f1 = cols_by_prefix(data.columns, 'F_1')\ncols_f2 = cols_by_prefix(data.columns, 'F_2')\ncols_f3 = cols_by_prefix(data.columns, 'F_3')\ncols_f4 = cols_by_prefix(data.columns, 'F_4')\ncols_f134 = cols_f1 + cols_f3 + cols_f4\n\ndata_f134 = data[cols_f134]\ndata_f1 = data[cols_f1]\ndata_f2 = data[cols_f2]\ndata_f3 = data[cols_f3]\ndata_f4 = data[cols_f4]","metadata":{"execution":{"iopub.status.busy":"2022-06-18T18:51:36.62903Z","iopub.execute_input":"2022-06-18T18:51:36.629427Z","iopub.status.idle":"2022-06-18T18:51:37.113652Z","shell.execute_reply.started":"2022-06-18T18:51:36.629389Z","shell.execute_reply":"2022-06-18T18:51:37.112646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# What's Missing\n\nLet's examine the data and the column groups to get a sense for what's missing.\n\n- We have a million rows. \n- Column groups F_1, F_3, F_4 are all floating point and contain missing values, and every column has some missing values. \n- Column group F_2 appear to be ordinals or categoricals of some kind, and have no missing values.","metadata":{}},{"cell_type":"code","source":"data_f1.describe()","metadata":{"execution":{"iopub.status.busy":"2022-06-18T18:51:37.117038Z","iopub.execute_input":"2022-06-18T18:51:37.117493Z","iopub.status.idle":"2022-06-18T18:51:38.052042Z","shell.execute_reply.started":"2022-06-18T18:51:37.117458Z","shell.execute_reply":"2022-06-18T18:51:38.05127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_f2.describe()","metadata":{"execution":{"iopub.status.busy":"2022-06-18T18:51:38.05328Z","iopub.execute_input":"2022-06-18T18:51:38.053845Z","iopub.status.idle":"2022-06-18T18:51:38.90178Z","shell.execute_reply.started":"2022-06-18T18:51:38.05381Z","shell.execute_reply":"2022-06-18T18:51:38.900849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_f3.describe()","metadata":{"execution":{"iopub.status.busy":"2022-06-18T18:51:38.903318Z","iopub.execute_input":"2022-06-18T18:51:38.903895Z","iopub.status.idle":"2022-06-18T18:51:40.376527Z","shell.execute_reply.started":"2022-06-18T18:51:38.903861Z","shell.execute_reply":"2022-06-18T18:51:40.375492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_f4.describe()","metadata":{"execution":{"iopub.status.busy":"2022-06-18T18:51:40.377787Z","iopub.execute_input":"2022-06-18T18:51:40.378478Z","iopub.status.idle":"2022-06-18T18:51:41.242811Z","shell.execute_reply.started":"2022-06-18T18:51:40.378443Z","shell.execute_reply":"2022-06-18T18:51:41.24168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# Hypothesis - values are missing at random","metadata":{}},{"cell_type":"code","source":"print(f'total missing values: {data.isna().sum().sum()}')","metadata":{"execution":{"iopub.status.busy":"2022-06-18T18:51:41.244159Z","iopub.execute_input":"2022-06-18T18:51:41.244527Z","iopub.status.idle":"2022-06-18T18:51:41.417885Z","shell.execute_reply.started":"2022-06-18T18:51:41.244477Z","shell.execute_reply":"2022-06-18T18:51:41.416682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have 1,000,000 NaNs - 1 per row. So if there are 55 columns with NaNs, our hypothesis is that cells are missing completely at random, with probability $1/55$.\n\nLet's look at the correlation between features that have missing values - here we are just looking at correlating missingness, so we replace the values of the features with a missing indicator.\n\nLet's also look at correlation of missing values with the values in the F_2 columns, to determine if particular values of those columns induce missingness in the other columns.\n\nConclusion: Missing values don't appear to be correlated with each other, or with the values in the F_2 columns.","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(1, 1, figsize=(10,8))\n\ntemp = pd.concat([data_f134.isnull().astype(int), data[cols_f2]], axis=1)\ncorr = temp.corr()\nsns.heatmap(corr, vmin=-1, vmax=1,  cmap='BrBG')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-18T18:51:41.419046Z","iopub.execute_input":"2022-06-18T18:51:41.419343Z","iopub.status.idle":"2022-06-18T18:52:02.088277Z","shell.execute_reply.started":"2022-06-18T18:51:41.419314Z","shell.execute_reply":"2022-06-18T18:52:02.086715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Examine the column distributions\n\nIf every cell is randomly NaN with probability $1/55$, then we can view each column as a binomial random variable, distributed as $Binom(1000000, 1/55)$\n\nPer the binomial distribution, this gives:\n$$\nn = 1000000, \\, p=1/55 \\\\\n\\mu = np = 18181.81818181 \\\\\n\\sigma^2 = np(1-p) = 17851.23966942 \\\\\n\\sigma = \\sqrt{\\sigma^2} = 133.60853142\n$$\n\nWhat do our sample mean and sample variance look like?\n- obviously the mean matches exactly \n- the variance/standard deviation seem reasonably close to our hypothesis\n\n","metadata":{}},{"cell_type":"code","source":"data_f134.isna().sum().agg(['mean', 'var', 'std'])","metadata":{"execution":{"iopub.status.busy":"2022-06-18T18:52:02.09091Z","iopub.execute_input":"2022-06-18T18:52:02.091777Z","iopub.status.idle":"2022-06-18T18:52:02.233335Z","shell.execute_reply.started":"2022-06-18T18:52:02.091742Z","shell.execute_reply":"2022-06-18T18:52:02.232164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"What does this binomial distribution look like?","metadata":{}},{"cell_type":"code","source":"p = 1/55\nn = 1E6\n\nrv = binom(n,p)\nmean, var = rv.stats()\nmean = mean[()]\nvar = var[()]\nstd = var**.5\n\nfig, ax = plt.subplots(1, 1, figsize=(10,8))\n\nx = np.arange(binom.ppf(0.005, n, p),\n              binom.ppf(0.995, n, p))\n\nax.plot(x, binom.pmf(x, n, p), 'bo', ms=5, label='pmf')\nax.legend(loc='best', frameon=False)\nplt.title(f'binom(1E6, 1/55) [mean:{mean:.2f} std:{std:.2f}]')\n\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-06-18T18:52:02.235069Z","iopub.execute_input":"2022-06-18T18:52:02.235727Z","iopub.status.idle":"2022-06-18T18:52:02.501553Z","shell.execute_reply.started":"2022-06-18T18:52:02.235651Z","shell.execute_reply":"2022-06-18T18:52:02.500416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note this looks normal! When $n$ is large, we can approximate a binomial distribution with a normal distribution.\n\nSo in our case, we have 55 RVs, which we hypothesize are distributed $N(18181.8181, 17851.2396)$ and standard deviation $133.608$ \n\nWe expect to see the count of missing values by column clustered around the mean, with a spread of a few hundred (since our standard deviation is $\\approx 133$)\n\nLet's plot the missing value counts by column, and also look at a histogram. \n- We see the values are indeed clustered around the mean.\n- The histogram doesn't look entirely normal, but remember our sample size here is only 55.","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(1, 2, figsize=(14,6))\n\nax[0].set_title(\"NaN column counts\")\ndata_f134.isna().sum().plot(ax=ax[0], ylim=(0,20000))\n\n\nax[1].set_title(\"NaN column counts - histogram\")\nna_col_counts = data_f134.isna().sum()\nna_col_counts.hist(ax=ax[1])\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-18T18:52:02.502848Z","iopub.execute_input":"2022-06-18T18:52:02.503198Z","iopub.status.idle":"2022-06-18T18:52:03.148231Z","shell.execute_reply.started":"2022-06-18T18:52:02.503166Z","shell.execute_reply":"2022-06-18T18:52:03.147382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"What else can we do to confirm each column is indeed pulled from a normal distribution?\n\nWell, if it's normal, we expect ~68% of values to fall within 1-sigma, ~95% within 2-sigma, and ~99.7% within 3-sigma. \n\nLooks pretty close!\n","metadata":{}},{"cell_type":"code","source":"na_row_counts = data_f134.isna().sum(axis=0)\n\nfor i in range(1,4):\n    sigma_lb = mean - i * std\n    sigma_ub = mean + i * std\n    pct = na_row_counts.between(sigma_lb, sigma_ub).sum() / na_row_counts.size\n    print(f\"{i}-sigma: {pct:.2f}\")\n","metadata":{"execution":{"iopub.status.busy":"2022-06-18T18:52:03.151267Z","iopub.execute_input":"2022-06-18T18:52:03.151867Z","iopub.status.idle":"2022-06-18T18:52:03.290374Z","shell.execute_reply.started":"2022-06-18T18:52:03.15183Z","shell.execute_reply":"2022-06-18T18:52:03.289574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"_Conclusion_: the column distributions do indeed appear drawn from the same binomial distribution.\n\n_Notes_: \n- I think we could do a t-test on each of the columns vs the population mean.\n- I think we could do a hypothesis test on the sample variance since the binomial distribution is normal for large $n$\n\n# Examine Row Distributions\nIf we examine row distributions, we expect missing values to be distributed $ binom{55, 1/55) $. Note our sample size is now 1000000.\n\nFor small n, the binomial distribution doesn't look anything like a normal distribution. But since we have such a large sample, we can compare our proportions with the distribution.\n\n","metadata":{}},{"cell_type":"code","source":"n = 55\n\nfig, ax = plt.subplots(1, 2, figsize=(14,6))\n\nx = np.arange(binom.ppf(0.000000001, n, p),\n              binom.ppf(0.999999999, n, p))\n\n# plot the PMF\nax[0].plot(x, binom.pmf(x, n, p), 'bo', ms=8)\nax[0].set_title(\"PMF - binom(55,1/55)\")\n\n# plot the sample proportions\nproportions = data.isnull().sum(axis=1).value_counts() / 1E6\nax[1].plot(proportions, 'r+', ms=8)\nax[1].set_title(\"Missing value proportions\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-18T18:52:03.291727Z","iopub.execute_input":"2022-06-18T18:52:03.292258Z","iopub.status.idle":"2022-06-18T18:52:03.834794Z","shell.execute_reply.started":"2022-06-18T18:52:03.292225Z","shell.execute_reply":"2022-06-18T18:52:03.833572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looks like a perfect match!\n\nConclusions:\n- Missing values appear uncorrelated with each other\n- Column distributions match expectations\n- Row distributions match expections\n- Values are missing completely at random in F1,F3,F4 column groups, with probability $1/55$","metadata":{}},{"cell_type":"markdown","source":"# Why does this matter? Testing!\n\nSo why does this matter? Well, if values are missing completely at random, perhaps we can use the rows with no missing values to synthesis our own test data for imputation. \n\nAnd since we know what the values actually are, we can use the test metric to evaluate our imputation without having to submit to the competition.\n\nAlso, if using a subset of the data is predictive of the LB score, then our training/evaluation can go faster.\n\nSo the basic idea is:\n- use the ~365000 rows with no missing values\n- randomly generate a set of missing values \n- run a couple of imputation techniques on this synthetic data\n- is the delta in performance reflective of the delta in performance on the full dataset / leaderboard?\n","metadata":{}},{"cell_type":"code","source":"def make_training(df, n, p, random_state):\n    # first find all rows with *no* NaN; sample n rows\n    df = df[~df.isnull().any(axis=1)]\n    if n > 0:\n        df = df.sample(n=n, random_state=random_state)\n    \n    # random mask of NaN locations; only cols F_1*, F_3*, F_4*\n    mask = np.random.random(df[cols_f134].shape) < p\n    df_na = df[cols_f134].mask(mask)\n\n    # put it back together with F_2*\n    df_na = pd.concat([df_na[cols_f1], df[cols_f2], df_na[cols_f3], df_na[cols_f4]], axis=1)\n    return df, df_na, df_na.isna().sum().sum()\n\ndef rmse(df1, df2, n):\n    sse = (df1.to_numpy() - df2.to_numpy())**2\n    return (sse.sum()/n)**0.5\n","metadata":{"execution":{"iopub.status.busy":"2022-06-18T18:52:03.836273Z","iopub.execute_input":"2022-06-18T18:52:03.836666Z","iopub.status.idle":"2022-06-18T18:52:03.845529Z","shell.execute_reply.started":"2022-06-18T18:52:03.83662Z","shell.execute_reply":"2022-06-18T18:52:03.844678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Mean Imputer\n\nHere we run a sklearn SimpleImputer. I've tested this in the competition with the full dataset\n- __LB=1.41613__\n\nRMSE on our synthetic test set is close!","metadata":{}},{"cell_type":"code","source":"%%time\n\ntrain, train_na, na_count = make_training(data, -1, p, RANDOM_STATE)\nimputer = SimpleImputer(strategy=\"mean\")\ntrain_na[:] = imputer.fit_transform(train_na)\nprint(f'RMSE={rmse(train, train_na, na_count)}')","metadata":{"execution":{"iopub.status.busy":"2022-06-18T18:52:03.847142Z","iopub.execute_input":"2022-06-18T18:52:03.847812Z","iopub.status.idle":"2022-06-18T18:52:14.406594Z","shell.execute_reply.started":"2022-06-18T18:52:03.847779Z","shell.execute_reply":"2022-06-18T18:52:14.405248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Iterative Imputer\n\nHere we run sklearn IterativeImputer, restricting to 3 iterations. I've also tested this on the leaderboard:\n- __LB=0.99623__\n\nOnce again, RMSE on the synthetic test data is close!","metadata":{}},{"cell_type":"code","source":"%%time\n\ntrain, train_na, na_count = make_training(data, -1, p, RANDOM_STATE)\nimputer = IterativeImputer(verbose=2, max_iter=3, random_state=RANDOM_STATE)\ntrain_na[:] = imputer.fit_transform(train_na)\nprint(f'RMSE={rmse(train, train_na, na_count)}')","metadata":{"execution":{"iopub.status.busy":"2022-06-18T18:52:14.408438Z","iopub.execute_input":"2022-06-18T18:52:14.408985Z","iopub.status.idle":"2022-06-18T19:12:53.526301Z","shell.execute_reply.started":"2022-06-18T18:52:14.408938Z","shell.execute_reply":"2022-06-18T19:12:53.524965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Variability\n\nPerhaps we just got lucky. Here we'll take 3 folds of 200,000 records from the available data, and compare RMSE of all 3.\n\nResults seem pretty pretty consistent. Seems promising.","metadata":{}},{"cell_type":"code","source":"imputer = IterativeImputer(verbose=2, max_iter=3, random_state=RANDOM_STATE)\nrmses = []\nfor i in range(3):\n    train, train_na, na_count = make_training(data, 200000, p, RANDOM_STATE)\n    train_na[:] = imputer.fit_transform(train_na)\n    rmses.append(rmse(train, train_na, na_count))\n    print(f'RMSE={rmses[i]}')\nprint(f'RMSE mean over folds: {np.mean(rmses)}')","metadata":{"execution":{"iopub.status.busy":"2022-06-18T19:47:15.5742Z","iopub.execute_input":"2022-06-18T19:47:15.574769Z","iopub.status.idle":"2022-06-18T19:47:27.050068Z","shell.execute_reply.started":"2022-06-18T19:47:15.574734Z","shell.execute_reply":"2022-06-18T19:47:27.048602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusions\n\n- Values are missing completely at random in column groups F1, F3, F4, with probability $1/55$\n- We can create synthetic test data by randomizing missing values in the complete records of the full dataset\n- This synthetic test data produces evaluation results that seem a good proxy for the full dataset results on the leaderboard\n\n","metadata":{}}]}