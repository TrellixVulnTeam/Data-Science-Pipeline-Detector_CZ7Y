{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('darkgrid')\nfrom tqdm import tqdm\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport xgboost as xgb\nimport lightgbm as lgb\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport warnings\nwarnings.filterwarnings('ignore')\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-11T14:40:02.854004Z","iopub.execute_input":"2022-06-11T14:40:02.854409Z","iopub.status.idle":"2022-06-11T14:40:03.828944Z","shell.execute_reply.started":"2022-06-11T14:40:02.854371Z","shell.execute_reply":"2022-06-11T14:40:03.827811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = r'../input/tabular-playground-series-jun-2022'\n\ndf = pd.read_csv(path + '/data.csv', index_col='row_id')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-11T14:38:32.024181Z","iopub.execute_input":"2022-06-11T14:38:32.025067Z","iopub.status.idle":"2022-06-11T14:38:49.604126Z","shell.execute_reply.started":"2022-06-11T14:38:32.025013Z","shell.execute_reply":"2022-06-11T14:38:49.603103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_sample = pd.read_csv(path + '/sample_submission.csv', index_col='row-col')\ndf_sample.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-11T14:38:49.605816Z","iopub.execute_input":"2022-06-11T14:38:49.606338Z","iopub.status.idle":"2022-06-11T14:38:50.649063Z","shell.execute_reply.started":"2022-06-11T14:38:49.60629Z","shell.execute_reply":"2022-06-11T14:38:50.647951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check the missing value% in each feature vector\n\nmissing_value_percent = []\n\nfor i in tqdm(range(df.shape[1])):\n    missing_value_percent.append(df.iloc[:, i].isnull().sum())\n\nmissing_value_percent = [i/df.shape[0] * 100 for i in missing_value_percent]\n\nfig, ax = plt.subplots(figsize=(20, 4))\nax.bar(x=df.columns, height=missing_value_percent)\nax.set_xticklabels(df.columns, rotation=45, ha='right')\n\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-11T14:40:07.893596Z","iopub.execute_input":"2022-06-11T14:40:07.894213Z","iopub.status.idle":"2022-06-11T14:40:10.594006Z","shell.execute_reply.started":"2022-06-11T14:40:07.894162Z","shell.execute_reply":"2022-06-11T14:40:10.59277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### not all of them contains missing values;F_2_xx series do not contain missing values\n### for those vector containing missing values, the nan% is around 1.75%","metadata":{}},{"cell_type":"raw","source":"# check the distribution of each feature vector\n\nfor i in tqdm(range(df.shape[1])):\n    fig, ax2 = plt.subplots(figsize=(20, 1))\n    ax2.plot(df.index, df.iloc[:, i])\n    ax2.set_title(df.iloc[:, i].name)","metadata":{"execution":{"iopub.status.busy":"2022-06-08T14:32:16.421985Z","iopub.execute_input":"2022-06-08T14:32:16.422509Z","iopub.status.idle":"2022-06-08T14:32:56.228654Z","shell.execute_reply.started":"2022-06-08T14:32:16.422461Z","shell.execute_reply":"2022-06-08T14:32:56.22728Z"},"jupyter":{"outputs_hidden":true}}},{"cell_type":"markdown","source":"### some of these vectors are centered around zero, ranging either (-5, 5) or (-10, 10);\n### some contain outliers, pushing the distribution either upwards (0, 5) or downwards (-10 ,0);\n### F_2_xx series are positive only (0, 10);","metadata":{}},{"cell_type":"raw","source":"# check correlation amongst each feature vectors\n\nfig, ax3 = plt.subplots(figsize=(20, 18))\nmask = np.triu(df.corr())\nsns.heatmap(df.corr(), mask=mask, cmap='coolwarm', linewidth=0.5)","metadata":{"execution":{"iopub.status.busy":"2022-06-08T14:32:56.230298Z","iopub.execute_input":"2022-06-08T14:32:56.23094Z","iopub.status.idle":"2022-06-08T14:33:35.261279Z","shell.execute_reply.started":"2022-06-08T14:32:56.230897Z","shell.execute_reply":"2022-06-08T14:33:35.260065Z"}}},{"cell_type":"markdown","source":"### some feature vectors, such as F_2_xx and F_4_xx, are correlated with other vectors in the same series;\n### but others are not","metadata":{}},{"cell_type":"code","source":"# divide vectors into four series F_#_xx\n\ndf_f_1_cols = []\ndf_f_2_cols = []\ndf_f_3_cols = []\ndf_f_4_cols = []\n\nfor col in df.columns:\n    if col[:3] == 'F_1':\n        df_f_1_cols.append(col)\n    elif col[:3] == 'F_2':\n        df_f_2_cols.append(col)\n    elif col[:3] == 'F_3':\n        df_f_3_cols.append(col)\n    elif col[:3] == 'F_4':\n        df_f_4_cols.append(col)\n    else:\n        continue","metadata":{"execution":{"iopub.status.busy":"2022-06-11T14:41:33.040182Z","iopub.execute_input":"2022-06-11T14:41:33.040573Z","iopub.status.idle":"2022-06-11T14:41:33.047854Z","shell.execute_reply.started":"2022-06-11T14:41:33.040541Z","shell.execute_reply":"2022-06-11T14:41:33.047122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# take a closer look at F_2_xx series\n\ndf_f_2 = df[df_f_2_cols]\n\nfig, ax3 = plt.subplots(figsize=(20, 18))\nmask = np.triu(df_f_2.corr())\nsns.heatmap(df_f_2.corr(), mask=mask, cmap='coolwarm', linewidth=0.5, annot = True, fmt = '.2f')","metadata":{"execution":{"iopub.status.busy":"2022-06-11T14:41:37.500205Z","iopub.execute_input":"2022-06-11T14:41:37.500628Z","iopub.status.idle":"2022-06-11T14:41:42.894651Z","shell.execute_reply.started":"2022-06-11T14:41:37.500595Z","shell.execute_reply":"2022-06-11T14:41:42.892986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_f_2.plot(kind='box', figsize = (16,10))","metadata":{"execution":{"iopub.status.busy":"2022-06-11T14:41:59.203549Z","iopub.execute_input":"2022-06-11T14:41:59.204063Z","iopub.status.idle":"2022-06-11T14:42:01.330203Z","shell.execute_reply.started":"2022-06-11T14:41:59.204012Z","shell.execute_reply":"2022-06-11T14:42:01.329418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# take a closer look at F_4_xx series\n\ndf_f_4 = df[df_f_4_cols]\n\nfig, ax3 = plt.subplots(figsize=(16, 12))\nmask = np.triu(df_f_4.corr())\nsns.heatmap(df_f_4.corr(), mask=mask, cmap='coolwarm', linewidth=0.5, annot = True, fmt = '.2f')","metadata":{"execution":{"iopub.status.busy":"2022-06-11T14:42:12.201659Z","iopub.execute_input":"2022-06-11T14:42:12.202114Z","iopub.status.idle":"2022-06-11T14:42:14.48495Z","shell.execute_reply.started":"2022-06-11T14:42:12.202079Z","shell.execute_reply":"2022-06-11T14:42:14.48396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_f_4.plot(kind='box', figsize = (12,8))","metadata":{"execution":{"iopub.status.busy":"2022-06-11T14:42:24.729407Z","iopub.execute_input":"2022-06-11T14:42:24.729857Z","iopub.status.idle":"2022-06-11T14:42:25.992844Z","shell.execute_reply.started":"2022-06-11T14:42:24.729823Z","shell.execute_reply":"2022-06-11T14:42:25.99177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check F_1 and F_3 as well\n\ndf_f_1 = df[df_f_1_cols]\ndf_f_3 = df[df_f_3_cols]","metadata":{"execution":{"iopub.status.busy":"2022-06-11T14:42:41.508945Z","iopub.execute_input":"2022-06-11T14:42:41.509402Z","iopub.status.idle":"2022-06-11T14:42:41.636333Z","shell.execute_reply.started":"2022-06-11T14:42:41.509367Z","shell.execute_reply":"2022-06-11T14:42:41.635432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_f_1.plot(kind='box', figsize = (12,8))","metadata":{"execution":{"iopub.status.busy":"2022-06-11T14:42:42.152046Z","iopub.execute_input":"2022-06-11T14:42:42.152681Z","iopub.status.idle":"2022-06-11T14:42:43.57129Z","shell.execute_reply.started":"2022-06-11T14:42:42.152644Z","shell.execute_reply":"2022-06-11T14:42:43.570186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_f_3.plot(kind='box', figsize = (20,8))","metadata":{"execution":{"iopub.status.busy":"2022-06-11T14:42:43.573395Z","iopub.execute_input":"2022-06-11T14:42:43.574091Z","iopub.status.idle":"2022-06-11T14:42:45.534467Z","shell.execute_reply.started":"2022-06-11T14:42:43.57404Z","shell.execute_reply":"2022-06-11T14:42:45.533398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# as the entire F_2 contains no missing values;\n# they will be used as training data to impute other vectors;\n\n# here are a few ideas:\n# Strategy # 1: use F_2 to do lgb regression to impute other vectors, F_1, 3, and 4 >> 1.41652\n# Strategy # 2: mean imputation for F_1 and F_3, leave F_2 alone, \n\n\n# imputation technique can be lightgbm regression as it is swift\n# other candidates: xgboost, or neural networks","metadata":{"execution":{"iopub.status.busy":"2022-06-11T14:43:13.089826Z","iopub.execute_input":"2022-06-11T14:43:13.090318Z","iopub.status.idle":"2022-06-11T14:43:13.094883Z","shell.execute_reply.started":"2022-06-11T14:43:13.090285Z","shell.execute_reply":"2022-06-11T14:43:13.09386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Strategy # 1\n# starting with the first strategie: keep using F_2 to impute other columns\n# iterate through the F_1, F_3, and F_4 series\n\ndf_imputation = df.copy()\n\nfor vector in tqdm([df_f_1_cols, df_f_3_cols, df_f_4_cols]):\n    for col in vector:\n        df_f_2_merge_temp = df_f_2.merge(df_imputation[col], on ='row_id')\n\n        train = df_f_2_merge_temp[df_f_2_merge_temp[col].isna() == False]\n        test = df_f_2_merge_temp[df_f_2_merge_temp[col].isna() == True]\n\n        X_train = train.iloc[:,:-1]\n        y_train = train.iloc[:,-1]\n\n        X_test = test.iloc[:,:-1]\n\n        model = lgb.LGBMRegressor()\n        model.fit(X_train, y_train)\n\n        pred = model.predict(X_test)\n\n        concat_temp = pd.concat([X_test, pd.Series(pred, index=X_test.index, name = col)], axis = 1)\n        impute_temp = pd.concat([train, concat_temp])\n        impute_temp.sort_index(inplace=True)\n\n        df_imputation[col] = impute_temp[col]    ","metadata":{"execution":{"iopub.status.busy":"2022-06-11T14:43:24.651948Z","iopub.execute_input":"2022-06-11T14:43:24.652403Z","iopub.status.idle":"2022-06-11T14:43:24.674982Z","shell.execute_reply.started":"2022-06-11T14:43:24.652361Z","shell.execute_reply":"2022-06-11T14:43:24.673626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Strategy # 2\n\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"# iterate through the F_1\n\nfor col in df_f_1_cols:\n    df_f_2_merge_temp = df_f_2.merge(df_f_1[col], on ='row_id')\n    \n    train = df_f_2_merge_temp[df_f_2_merge_temp[col].isna() == False]\n    test = df_f_2_merge_temp[df_f_2_merge_temp[col].isna() == True]\n\n    X_train = train.iloc[:,:-1]\n    y_train = train.iloc[:,-1]\n\n    X_test = test.iloc[:,:-1]\n    \n    model = lgb.LGBMRegressor()\n    model.fit(X_train, y_train)\n\n    pred = model.predict(X_test)\n    \n    concat_temp = pd.concat([X_test, pd.Series(pred, index=X_test.index, name = col)], axis = 1)\n    impute_temp = pd.concat([train, concat_temp])\n    impute_temp.sort_index(inplace=True)\n    \n    df_f_1[col] = impute_temp[col]\n    \ndf_f_1.to_csv('df_f_1.csv', index = False)    ","metadata":{"execution":{"iopub.status.busy":"2022-06-10T12:45:18.737761Z","iopub.execute_input":"2022-06-10T12:45:18.738457Z","iopub.status.idle":"2022-06-10T13:09:07.271966Z","shell.execute_reply.started":"2022-06-10T12:45:18.738419Z","shell.execute_reply":"2022-06-10T13:09:07.270167Z"}}},{"cell_type":"code","source":"for i in tqdm(df_sample.index):\n    row = int(i.split('-')[0])\n    col = i.split('-')[1]\n    df_sample.loc[i, 'value'] = df_imputation.loc[row, col]\n\ndf_sample.to_csv('submission.csv')","metadata":{"execution":{"iopub.status.busy":"2022-06-11T15:06:04.186342Z","iopub.execute_input":"2022-06-11T15:06:04.186758Z","iopub.status.idle":"2022-06-11T15:08:14.084214Z","shell.execute_reply.started":"2022-06-11T15:06:04.186725Z","shell.execute_reply":"2022-06-11T15:08:14.082636Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# keep updating","metadata":{},"execution_count":null,"outputs":[]}]}