{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom pathlib import Path\nfrom tqdm import tqdm\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-26T04:28:42.471055Z","iopub.execute_input":"2022-06-26T04:28:42.471514Z","iopub.status.idle":"2022-06-26T04:28:43.564529Z","shell.execute_reply.started":"2022-06-26T04:28:42.471433Z","shell.execute_reply":"2022-06-26T04:28:43.563559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Problem Statement\n\nFor this challenge, we are given (simulated) manufacturing control data that contains missing values due to electronic errors. \nWe need to predict the values of all missing data in this dataset. ","metadata":{}},{"cell_type":"code","source":"input_path = Path('/kaggle/input/tabular-playground-series-jun-2022/')\ndata = pd.read_csv(input_path / 'data.csv', index_col='row_id')\nsubmission = pd.read_csv(input_path / 'sample_submission.csv', index_col='row-col')","metadata":{"execution":{"iopub.status.busy":"2022-06-26T04:28:48.421353Z","iopub.execute_input":"2022-06-26T04:28:48.421795Z","iopub.status.idle":"2022-06-26T04:29:06.246809Z","shell.execute_reply.started":"2022-06-26T04:28:48.421755Z","shell.execute_reply":"2022-06-26T04:29:06.245728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Exploration","metadata":{}},{"cell_type":"markdown","source":"### Which columns have missing values?\n\nEverything except the F_2_* columns (int64 columns)","metadata":{}},{"cell_type":"code","source":"data.columns","metadata":{"execution":{"iopub.status.busy":"2022-06-26T04:34:50.249834Z","iopub.execute_input":"2022-06-26T04:34:50.250485Z","iopub.status.idle":"2022-06-26T04:34:50.26142Z","shell.execute_reply.started":"2022-06-26T04:34:50.250445Z","shell.execute_reply":"2022-06-26T04:34:50.260497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.info()","metadata":{"execution":{"iopub.status.busy":"2022-06-26T04:29:48.448339Z","iopub.execute_input":"2022-06-26T04:29:48.449021Z","iopub.status.idle":"2022-06-26T04:29:48.646144Z","shell.execute_reply.started":"2022-06-26T04:29:48.448974Z","shell.execute_reply":"2022-06-26T04:29:48.645184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Frequency plot of missing values across rows","metadata":{}},{"cell_type":"code","source":"pd.DataFrame(data.isnull().sum(axis=1), columns = ['Missing value count']).reset_index().groupby('Missing value count').row_id.count().plot(kind='barh')","metadata":{"execution":{"iopub.status.busy":"2022-06-26T04:29:50.628793Z","iopub.execute_input":"2022-06-26T04:29:50.629242Z","iopub.status.idle":"2022-06-26T04:29:51.144646Z","shell.execute_reply.started":"2022-06-26T04:29:50.6292Z","shell.execute_reply":"2022-06-26T04:29:51.143647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Approximately 1-2% of data missing in each column","metadata":{}},{"cell_type":"code","source":"pd.options.display.float_format = '{:,.2f} %'.format\n(data.isnull().sum()/len(data))*100","metadata":{"execution":{"iopub.status.busy":"2022-06-26T04:30:06.277742Z","iopub.execute_input":"2022-06-26T04:30:06.278087Z","iopub.status.idle":"2022-06-26T04:30:06.48899Z","shell.execute_reply.started":"2022-06-26T04:30:06.278056Z","shell.execute_reply":"2022-06-26T04:30:06.487636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.options.display.float_format = '{:,.2f}'.format","metadata":{"execution":{"iopub.status.busy":"2022-06-26T04:30:13.245944Z","iopub.execute_input":"2022-06-26T04:30:13.246291Z","iopub.status.idle":"2022-06-26T04:30:13.251836Z","shell.execute_reply.started":"2022-06-26T04:30:13.246261Z","shell.execute_reply":"2022-06-26T04:30:13.250883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Which features are correlated with each other?\n\n1. F_2 features are only correlated with F_2 features, so we can't use them to predict any other features. Since they don't have missing values as well, the assumption is to ignore them.\n2. F_4 features are correlated with F_4 features , so we can use them to find missing values in F_4 using regression\n3. F_1 and F_3 have 0 correlation with any other features, so we need to resort to mean imputation","metadata":{}},{"cell_type":"code","source":"data_corr = data.corr()\nplt.subplots(figsize=(25,20))\nsns.heatmap(data_corr, annot= True, cmap=\"YlOrRd\", fmt = '0.1f', vmin=-0.6, vmax=0.6);","metadata":{"execution":{"iopub.status.busy":"2022-06-26T04:30:15.966355Z","iopub.execute_input":"2022-06-26T04:30:15.96697Z","iopub.status.idle":"2022-06-26T04:30:55.081069Z","shell.execute_reply.started":"2022-06-26T04:30:15.966932Z","shell.execute_reply":"2022-06-26T04:30:55.080169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data distribution of the continuous variables - using boxplot\n\nAcknowledgement : https://www.kaggle.com/code/dhirajkumar612/iterative-imputer-linear-regression","metadata":{}},{"cell_type":"code","source":"columns_with_correlations = ['F_4_0', 'F_4_1', 'F_4_2', 'F_4_3',\n       'F_4_4', 'F_4_5', 'F_4_6', 'F_4_7', 'F_4_8', 'F_4_9', 'F_4_10',\n       'F_4_11', 'F_4_12', 'F_4_13', 'F_4_14']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(18, 18))\nfor i, col in enumerate(columns_with_correlations): \n    ax = plt.subplot(11,5, i+1) \n    sns.boxplot(data=data[columns_with_correlations],x=col,ax=ax)\nplt.suptitle('Data distribution of continuous variables')\nplt.tight_layout()                    ","metadata":{"execution":{"iopub.status.busy":"2022-06-26T04:45:20.337044Z","iopub.execute_input":"2022-06-26T04:45:20.337512Z","iopub.status.idle":"2022-06-26T04:45:22.950917Z","shell.execute_reply.started":"2022-06-26T04:45:20.337478Z","shell.execute_reply":"2022-06-26T04:45:22.948688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Power transformation\n\nWe see that some data is skewed to the left with outliers from the box plot above. Power transformation can help. A power transform will make the probability distribution of a variable more Gaussian.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import PowerTransformer\npower = PowerTransformer(method='yeo-johnson', standardize=False)\nf4_data = pd.DataFrame( power.fit_transform(data[columns_with_correlations]),columns = columns_with_correlations)","metadata":{"execution":{"iopub.status.busy":"2022-06-26T04:43:05.616342Z","iopub.execute_input":"2022-06-26T04:43:05.616926Z","iopub.status.idle":"2022-06-26T04:43:23.587951Z","shell.execute_reply.started":"2022-06-26T04:43:05.61689Z","shell.execute_reply":"2022-06-26T04:43:23.587002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(18, 18))\nfor i, col in enumerate(columns_with_correlations): \n    ax = plt.subplot(11,5, i+1) \n    sns.boxplot(data=f4_data,x=col,ax=ax)\nplt.suptitle('Data distribution of continuous variables')\nplt.tight_layout()                    ","metadata":{"execution":{"iopub.status.busy":"2022-06-26T04:46:03.987751Z","iopub.execute_input":"2022-06-26T04:46:03.988647Z","iopub.status.idle":"2022-06-26T04:46:05.804161Z","shell.execute_reply.started":"2022-06-26T04:46:03.98861Z","shell.execute_reply":"2022-06-26T04:46:05.80323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Strategy to identify missing values\n\nLet's use mean imputation for F_1 and F_3 features and XgBoost based regression imputer for F_4 features. ","metadata":{}},{"cell_type":"markdown","source":"### Mean Imputation","metadata":{}},{"cell_type":"code","source":"columns_to_mean_impute = ['F_1_0', 'F_1_1', 'F_1_2', 'F_1_3', 'F_1_4', 'F_1_5', 'F_1_6', 'F_1_7',\n       'F_1_8', 'F_1_9', 'F_1_10', 'F_1_11', 'F_1_12', 'F_1_13', 'F_1_14',\n       'F_3_0', 'F_3_1', 'F_3_2', 'F_3_3', 'F_3_4', 'F_3_5', 'F_3_6', 'F_3_7',\n       'F_3_8', 'F_3_9', 'F_3_10', 'F_3_11', 'F_3_12', 'F_3_13', 'F_3_14',\n       'F_3_15', 'F_3_16', 'F_3_17', 'F_3_18', 'F_3_19', 'F_3_20', 'F_3_21',\n       'F_3_22', 'F_3_23', 'F_3_24']","metadata":{"execution":{"iopub.status.busy":"2022-06-26T04:46:29.609501Z","iopub.execute_input":"2022-06-26T04:46:29.610099Z","iopub.status.idle":"2022-06-26T04:46:29.616272Z","shell.execute_reply.started":"2022-06-26T04:46:29.610061Z","shell.execute_reply":"2022-06-26T04:46:29.615098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.impute import SimpleImputer\nimp = SimpleImputer(\n        missing_values=np.nan,\n        strategy='mean')\nmean_imputed_data = pd.DataFrame( imp.fit_transform(data[columns_to_mean_impute]),columns = columns_to_mean_impute)","metadata":{"execution":{"iopub.status.busy":"2022-06-26T04:46:31.61357Z","iopub.execute_input":"2022-06-26T04:46:31.614546Z","iopub.status.idle":"2022-06-26T04:46:32.637561Z","shell.execute_reply.started":"2022-06-26T04:46:31.614496Z","shell.execute_reply":"2022-06-26T04:46:32.636569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### MICE (Multivariate feature imputation) Regression based imputation\n\nA strategy for imputing missing values by modeling each feature with missing values as a function of other features in a round-robin fashion. It performns multiple regressions over random sample ofthe data, then takes the average ofthe multiple regression values and uses that value to impute the missing value. In sklearn, it is implemented as follows:","metadata":{}},{"cell_type":"code","source":"from sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nimport xgboost","metadata":{"execution":{"iopub.status.busy":"2022-06-26T04:47:58.95707Z","iopub.execute_input":"2022-06-26T04:47:58.95743Z","iopub.status.idle":"2022-06-26T04:47:58.962608Z","shell.execute_reply.started":"2022-06-26T04:47:58.9574Z","shell.execute_reply":"2022-06-26T04:47:58.961671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reg_imputer = IterativeImputer(estimator=\n                               xgboost.XGBRegressor(n_estimators=1000, \n                                       tree_method='gpu_hist', \n                                       predictor=\"gpu_predictor\",\n                                       eval_metric=mean_squared_error),\n                               verbose=2,\n                               max_iter=20)\nreg_imputed_data = pd.DataFrame( reg_imputer.fit_transform(f4_data),columns = f4_data.columns)","metadata":{"execution":{"iopub.status.busy":"2022-06-26T04:48:00.739027Z","iopub.execute_input":"2022-06-26T04:48:00.739394Z","iopub.status.idle":"2022-06-26T04:57:20.395975Z","shell.execute_reply.started":"2022-06-26T04:48:00.739362Z","shell.execute_reply":"2022-06-26T04:57:20.394819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Join both imputations","metadata":{}},{"cell_type":"code","source":"final_imputed_data = pd.concat([mean_imputed_data, reg_imputed_data], axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-06-26T04:57:20.398267Z","iopub.execute_input":"2022-06-26T04:57:20.398956Z","iopub.status.idle":"2022-06-26T04:57:20.566606Z","shell.execute_reply.started":"2022-06-26T04:57:20.398918Z","shell.execute_reply":"2022-06-26T04:57:20.565552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_imputed_data.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2022-06-26T04:57:20.568001Z","iopub.execute_input":"2022-06-26T04:57:20.568345Z","iopub.status.idle":"2022-06-26T04:57:20.676655Z","shell.execute_reply.started":"2022-06-26T04:57:20.568296Z","shell.execute_reply":"2022-06-26T04:57:20.675608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Generate Submission File","metadata":{}},{"cell_type":"code","source":"for i in tqdm(submission.index):\n    row = int(i.split('-')[0])\n    col = i.split('-')[1]\n    submission.loc[i,'value']= final_imputed_data.loc[row, col]\n    \nsubmission.to_csv('mean_and_regression_imputer_power_transform_xgboost.csv')","metadata":{"execution":{"iopub.status.busy":"2022-06-26T04:57:20.678988Z","iopub.execute_input":"2022-06-26T04:57:20.679396Z","iopub.status.idle":"2022-06-26T04:58:55.90883Z","shell.execute_reply.started":"2022-06-26T04:57:20.679358Z","shell.execute_reply":"2022-06-26T04:58:55.907864Z"},"trusted":true},"execution_count":null,"outputs":[]}]}