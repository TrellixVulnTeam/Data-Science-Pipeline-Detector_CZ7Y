{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%HTML\n\n<style type=\"text/css\">\n\ndiv.h2 {\n    background-color: #665191; \n    color: white; \n    padding: 5px; \n    padding-right: 300px; \n    font-size: 25px;  \n    margin-top: 2px;\n    margin-bottom: 10px;\n}\n\ndiv.h3 {\n    background-color: white; \n    color: #fe0000; \n    padding: 5px; \n    padding-right: 300px; \n    font-size: 20px; \n    margin-top: 2px;\n    margin-bottom: 10px;\n}\n</style>","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-10T12:33:18.131925Z","iopub.execute_input":"2022-06-10T12:33:18.132413Z","iopub.status.idle":"2022-06-10T12:33:18.140772Z","shell.execute_reply.started":"2022-06-10T12:33:18.13238Z","shell.execute_reply":"2022-06-10T12:33:18.139485Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip uninstall matplotlib -y\n# !pip install matplotlib==3.1.3\n\nimport matplotlib\nprint(matplotlib.__version__)\n\n!pip install mplcyberpunk\nimport mplcyberpunk\n\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport matplotlib \nfrom matplotlib import gridspec\nimport seaborn as sns\n\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nimport shap\nimport xgboost as xgb\nfrom lightgbm import LGBMClassifier\nfrom sklearn.metrics import roc_auc_score, roc_curve\n\nimport joblib\nimport warnings\nwarnings.filterwarnings(\"ignore\")\npd.set_option('display.max_columns', None)\nplt.style.use(\"cyberpunk\")\n\n# runtime configuration of matplotlib\nplt.rc(\"figure\", \n    autolayout=False, \n    figsize=(20, 10),\n)\nplt.rc(\n    \"axes\",\n    labelweight=\"bold\",\n    labelsize=\"large\",\n    titleweight=\"bold\",\n    titlesize=20,\n    titlepad=10,\n)","metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n\n### <div class=\"h2\">Introduction</div>\n\nThis competition is all about imputation of missing values and there is no target. The task is to predict the missing values. There are different appraches for handling missing values and I will treat this is a regression approach where every column with missing values is the target where the model will be trained on the non-missing values inorder to predict the missing values. [SRK](https://www.kaggle.com/competitions/tabular-playground-series-jun-2022/discussion/328369) has a discussion post about this approach.\n\nMy main aim in this notebook is to use different approaches like *SHAP* and *PDP* to create insights into how a ML model like XGBoost works. A model is like a detective that searches for interaction and patterns in the data and uses these findings to make predictions. Since most of these models are by design not interpretable it is wise to use the above mentioned apporaches to check whether the model has learned the right patterns. \n\nSince the data for this competition is anonymus it is hard to check if the model has learned the right things. Nonetheless the technique itself can be useful in analysing the feature space for feature engineering.\n\n\n   <a id=\"toc\"></a>\n   \n1. [Data Overview](#1)\n2. [Modelling approach](#2)\n3. [Model inspection with SHAP](#3)\n4. [Model inspection with PDP](#4)\n5. [References](#5)\n\nI will keep adding content; its work in progress.","metadata":{}},{"cell_type":"markdown","source":"---\n<a id=\"1\"></a>\n### <div class=\"h2\">1. Data Overview</div>","metadata":{}},{"cell_type":"code","source":"data_raw = pd.read_csv('../input/tabular-playground-series-jun-2022/data.csv', index_col=\"row_id\")","metadata":{"execution":{"iopub.status.busy":"2022-06-10T11:55:29.896037Z","iopub.execute_input":"2022-06-10T11:55:29.896554Z","iopub.status.idle":"2022-06-10T11:55:46.338034Z","shell.execute_reply.started":"2022-06-10T11:55:29.896513Z","shell.execute_reply":"2022-06-10T11:55:46.336966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr = data_raw.loc[:, :].corr().abs()\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\n\nfig = plt.figure(figsize=(35, 20), facecolor='#002637', edgecolor='r')\nax = fig.add_subplot()\n\nsns.heatmap(corr, mask=mask, cmap='coolwarm', annot=True, fmt='.2f', cbar=False, ax=ax)\nax.tick_params(axis='x', colors='w', labelsize=12, rotation=90)\nax.tick_params(axis='y', colors='w', labelsize=12)\n\nplt.suptitle(\"Pearson correlation coefficient\", color=\"white\", fontsize=35, y=0.92)\nplt.yticks(rotation=0) \nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-10T15:29:42.715911Z","iopub.execute_input":"2022-06-10T15:29:42.716417Z","iopub.status.idle":"2022-06-10T15:30:12.375477Z","shell.execute_reply.started":"2022-06-10T15:29:42.716379Z","shell.execute_reply":"2022-06-10T15:30:12.374778Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"F_4 = []\nfor i in range(15):\n    string = \"F_4_\" + str(i)\n    F_4.append(string)\n\ncorr = data_raw.loc[:, F_4].corr().abs()\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\n\nfig = plt.figure(tight_layout=True, figsize=(20,10))\nspec = gridspec.GridSpec(ncols=2, nrows=1, figure=fig)\n\n\nax0 = fig.add_subplot(spec[0, 0])\nsns.heatmap(corr, mask=mask, cmap='coolwarm', annot=True, fmt='.2f', cbar=False, ax=ax0)\nax0.tick_params(axis='x', colors='w', labelsize=12, rotation=90)\nax0.tick_params(axis='y', colors='w', labelsize=12, rotation=0)\nax0.set_title(\"Pearson correlation coefficient\", fontsize=14, fontweight ='bold', y=1.01)\n\n\nax0 = fig.add_subplot(spec[0, 1])\n\nnull = pd.DataFrame(data_raw.loc[:, F_4].isnull().sum())\ndtypes = pd.DataFrame(data_raw.loc[:, F_4].dtypes)\ndata_info = dtypes.merge(null, left_index=True, right_index=True)\ndata_info.columns = [\"type\", \"nulls\"]\ndata_info.sort_values(by=[\"nulls\"], ascending=False, inplace=True)\ndata_info[\"% missing\"] = 100*data_info[\"nulls\"]/data_raw.shape[0]\n\nsns.scatterplot(x=data_info.index, y=\"% missing\", data=data_info, ax=ax0)\nax0.set_title(\"Percentage missing values\", fontsize=14, fontweight ='bold', y=1.01)\nax0.tick_params(axis='x', colors='w', labelsize=12, rotation=90)\n    \nplt.suptitle(\"F_4\", ha=\"center\", y=1.03, fontweight ='bold', fontsize=24) \nplt.show()\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-10T15:47:55.541895Z","iopub.execute_input":"2022-06-10T15:47:55.542305Z","iopub.status.idle":"2022-06-10T15:47:57.505356Z","shell.execute_reply.started":"2022-06-10T15:47:55.542271Z","shell.execute_reply":"2022-06-10T15:47:57.504387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ðŸ’¡ **INSIGHTS**\n- From the first plot one can conclude that there is correlation between `F_2` features and correlation between `F_4` features. Otherwise there is no correlation among other features.\n- In the second I have zoomed in on the feature space `F_4`. Here we can see that `F_4_11` correlates the highest with `F_4_8`. Overall there is some sort of correlation between features. \n- From the third plot one can conclude that all features of `F_4` has missing values with `F_4_2` the most.","metadata":{}},{"cell_type":"markdown","source":"---\n<a id=\"2\"></a>\n### <div class=\"h2\">2. Modelling approach</div>\n\nTo use the SHAP package we first need to train a model since SHAP is a model agnostic approach designed to explain any given black-box model. If you are applying SHAP to a real-world problem you should follow best practices. Specifically, you should ensure your model performs well on both a training and validation set. The better your model the more reliable your results will be. \n\nFor the sake of demonstration I have chosen `F_4_8` as target and the rest of `F_4` features as predictors and applied XGBoost to train a model. As a quick check on this model, I have calculated the `R-square` of the validation set which about`90%`. The model should be fine to demonstrate the SHAP package. The code below can be easly modified to train a different model or maybe more than one model.\n","metadata":{}},{"cell_type":"code","source":"F_4 = []\nfor i in range(15):\n    string = \"F_4_\" + str(i)\n    F_4.append(string)\n\n# Substitute [\"F_4_8\"] with F_4 if you want to train a model for all F_14 features.\nfor col in [\"F_4_8\"]:\n    target = col\n    features = [col for col in data_raw.loc[:, F_4].columns if col != target]\n    \n    train = data_raw[~data_raw[target].isnull()]\n    test = data_raw[data_raw[target].isnull()]\n\n    X = train.loc[:, features]\n    y = train.loc[:, target]\n\n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=1307)\n\n    model = xgb.XGBRegressor(learning_rate=0.2, max_depth=8, eval_metric=\"rmse\", n_jobs=-1, n_estimators=100)\n    model.fit(X_train, y_train)\n\n    # save the model to disk\n    filename = col\n    joblib.dump(model, filename)\n\n    #predict for the validation set\n    result = model.score(X_val, y_val)\n    print('R-square value for column: ' + col + ' is ' + result.astype(str))\n\n    predicted = model.predict(X_val)\n    rmse = np.sqrt(((predicted - y_val) ** 2).mean())\n    print('RMSE value for column: ' + col + ' is ' + rmse.astype(str) + \"\\n\")\n\n    # Using a random sample of the dataframe for better time computation\n    X_sampled = X_val.sample(20000, random_state=1307)\n    joblib.dump(X_sampled, filename + \"_X_val_sampled\")\n\n    # explain the model's predictions using SHAP values\n    explainer = shap.TreeExplainer(model)\n    shap_values = explainer.shap_values(X_sampled)\n    joblib.dump(shap_values, filename + \"_shap_values\")\n\n    #Get SHAP interaction values\n    shap_interaction = explainer.shap_interaction_values(X_sampled)\n    joblib.dump(shap_interaction, filename + \"_shap_interaction\")","metadata":{"execution":{"iopub.status.busy":"2022-06-10T13:21:24.561238Z","iopub.execute_input":"2022-06-10T13:21:24.561754Z","iopub.status.idle":"2022-06-10T13:27:14.268283Z","shell.execute_reply.started":"2022-06-10T13:21:24.561703Z","shell.execute_reply":"2022-06-10T13:27:14.267305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n<a id=\"3\"></a>\n### <div class=\"h2\">3. Model inspection with SHAP</div>\n\nThe focus will be on applying the SHAP package and interpreting the results. `SHAP values` are used to explain individual predictions made by a model. It does this by giving the contributions of each factor to the final prediction. `SHAP interaction` values extend on this by breaking down the contributions into their main and interaction effects. We can use these to highlight and visualise interactions in data. It can also be a useful tool to understand how your model makes predictions.\n\nSpecifically, we start with a plot of the features with the most impact on the predictions. These features are the most important for the model to make predictions. What is missing in this plot is how the feature influences the predictions. This is where the second plot comes in handy. The second plot is a combination of feature importance with feature interaction with respect to the target.\n\nThe third plot is the aboslute mean of SHAP interaction effects. The diagonal values are the main effects of the features and off-diagonal values are the interaction effects. The off-diagonal values can be used to check if there are hidden interactions in the dataset.","metadata":{}},{"cell_type":"code","source":"X_sampled = joblib.load('../input/files-f-4-8/F_4_8_X_val_sampled')\nshap_values = joblib.load('../input/files-f-4-8/F_4_8_shap_values')\nshap_interaction = joblib.load('../input/files-f-4-8/F_4_8_shap_interaction')\n\ndef plot_shap(X_sampled, shap_values, shap_interaction, i: str):\n\n    fig = plt.figure(tight_layout=True, figsize=(15,50))\n    spec = gridspec.GridSpec(ncols=2, nrows=2, figure=fig)\n\n\n    ax0 = fig.add_subplot(spec[0, 0])\n    shap.summary_plot(shap_values, features=X_sampled, feature_names=X_sampled.columns, max_display=10, plot_type=\"bar\", \n    plot_size=[15,10], axis_color=\"white\", show=False, cmap='green', color_bar=False)\n    ax0.set_title(f'Top Influencers\\n\\nFeatures with most impact on the model output', fontsize=12, y=1.03, fontweight ='bold')\n    ax0.tick_params(axis='both', colors='white', labelsize=10)    \n    ax0.set_xlabel('mean(|SHAP value|)', fontsize=10)\n\n    ax1 = fig.add_subplot(spec[0, 1])\n    shap.summary_plot(shap_values, X_sampled, axis_color=\"white\", max_display=10, plot_size=[15,10], show=False)\n    ax1.set_title(f'Directionality\\n\\nShows how values of a feature influences the models output', fontsize=12, y=1.03, fontweight ='bold')\n    ax1.tick_params(axis='both', colors='white', labelsize=10)  \n    ax1.set_xlabel('SHAP value', fontsize=10)\n\n\n    # Get absolute mean of matrices\n    mean_shap = np.abs(shap_interaction).mean(0)\n    df = pd.DataFrame(mean_shap, index=X_sampled.columns, columns=X_sampled.columns)\n\n    # times off diagonal by 2\n    df.where(df.values == np.diagonal(df),df.values*2, inplace=True)\n\n    #fig = plt.figure(figsize=(35, 20), facecolor='#002637', edgecolor='r')\n    ax2 = fig.add_subplot(spec[1, :])\n    sns.heatmap(df.round(decimals=3), cmap='coolwarm', annot=True, fmt='.6g', cbar=False, ax=ax2)\n    ax2.tick_params(axis='x', colors='w', labelsize=9, rotation=90)\n    ax2.tick_params(axis='y', colors='w', labelsize=9, rotation=0)\n    ax2.set_title(\"SHAP interaction values\", color=\"white\", fontsize=12, y=1.01, fontweight ='bold')\n\n    \n    plt.suptitle(i, ha=\"center\", y=1.0, fontweight ='bold', fontsize=14)\n    plt.show()  \n  \nplot_shap(X_sampled, shap_values, shap_interaction, \"F_4_8\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-10T12:51:52.277381Z","iopub.execute_input":"2022-06-10T12:51:52.277894Z","iopub.status.idle":"2022-06-10T12:52:00.125917Z","shell.execute_reply.started":"2022-06-10T12:51:52.277856Z","shell.execute_reply":"2022-06-10T12:52:00.124793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ðŸ’¡ **INSIGHTS**\n\n- From the first plot one can conclude that feature `F_4_11` is the most influential feature of the model and `F_4_7` is the least influential.\n- The second plot suggests the higher the value of `F_4_11` the lower the impact on the predictions. The opposite is true for `F_4_4`.\n- From the third plot one can observe that the main effect is large for features `F_4_11` (`0.497`) and `F_4_13` (`0.256`). This tells us that these features tend to have large positive or negative main effects. In other words, these features tend to have a significant impact on the modelâ€™s predictions. \n- Similarly, we can see that interaction effects for (`F_4_11`, `F_4_13`) --> (`0.114`) is the highest. So it is intersting to check how this interaction looks at local level. This is demonstrated in the figure below.","metadata":{}},{"cell_type":"code","source":"# F_4 = []\n# for i in range(0, 15):\n#     string = \"F_4_\" + str(i)\n#     F_4.append(string)\n    \n# i=8\n# model = joblib.load(\"F_4_\"+str(i))\n# X_sampled = joblib.load(\"F_4_\"+str(i)+\"_X_val_sampled\")\n# shap_values = joblib.load(\"F_4_\"+str(i)+\"_shap_values\")\n# shap_interaction = joblib.load(\"F_4_\"+str(i)+\"_shap_interaction\")\n# F_4.remove(\"F_4_\"+str(i))\n\nfig = plt.figure(tight_layout=True, figsize=(15,10))\nspec = gridspec.GridSpec(ncols=2, nrows=2, figure=fig)\n\n\nax0 = fig.add_subplot(spec[0, 0])\nshap.dependence_plot(\"F_4_11\", shap_values, X_sampled, axis_color='w', show=False, ax=ax0)\nax0.set_title(f'SHAP effect', fontsize=10)\n\n\nax0 = fig.add_subplot(spec[0, 1])\nf1=\"F_4_11\"\nf2=\"F_4_13\"\nshap.dependence_plot((f1, f2), shap_interaction, X_sampled, axis_color=\"w\", show=False, ax=ax0)\nax0.yaxis.label.set_color('white')          \nax0.xaxis.label.set_color('white')          \nax0.tick_params(axis='both', colors='white')    \nax0.set_title(f'SHAP Interaction effect', fontsize=10)\n\nax2 = fig.add_subplot(spec[1, :])\npoints = ax2.scatter(data_raw.F_4_11, data_raw.F_4_13, c=data_raw.F_4_8, cmap=\"jet\", lw=0)\nplt.colorbar(points)\nax2.tick_params(axis='both', colors='white')    #setting up X-axis tick color to white\nax2.set_xlabel(\"F_4_11\", fontsize=12, fontweight=\"bold\", color=\"white\")\nax2.set_ylabel(\"F_4_13\", fontsize=12, fontweight=\"bold\", color=\"white\")\nax2.set_title(f'Scatterplot', fontsize=10)\nax2.text(-5, 0, \"1\", fontsize=18, fontweight=\"bold\", verticalalignment='top', rotation=\"horizontal\", color=\"k\")\nax2.text(15, 0, \"2\", fontsize=18, fontweight=\"bold\", verticalalignment='top', rotation=\"horizontal\", color=\"k\")\nax2.text(41, 1, \"Target\", fontsize=18, verticalalignment='top', rotation=\"horizontal\", color=\"w\")\n\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-10T15:25:03.842339Z","iopub.execute_input":"2022-06-10T15:25:03.842857Z","iopub.status.idle":"2022-06-10T15:25:21.671162Z","shell.execute_reply.started":"2022-06-10T15:25:03.842818Z","shell.execute_reply":"2022-06-10T15:25:21.670156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ðŸ’¡ **INSIGHTS**\n- From the first plot one can conclude that as the value of `F_4_11` increases it tends to have a negative impact on the predictions.\n- Plot of the SHAP interaction value between `F_4_11` and `F_4_13` shows that high `F_4_11` values has a negative impact on the predictions confered by high values of `F_4_13`.\n- The scatter plot implies that `F_4_11` and `F_4_13` have a weak negative correlation and that there is also interaction between these two features with respect to the target. That is region `1` has higher target values compared to region `2`. ","metadata":{}},{"cell_type":"markdown","source":"---\n<a id=\"4\"></a>\n### <div class=\"h2\">4. Model inspection with PDP</div>\n\n\nComing soon","metadata":{}},{"cell_type":"markdown","source":"---\n<a id=\"5\"></a>\n### <div class=\"h2\">5. References</div>\n\n\nDomain Knowledge References\n\n1.https://github.com/slundberg/shap/blob/master/notebooks/tabular_examples/tree_based_models/Census%20income%20classification%20with%20LightGBM.ipynb\n2.https://towardsdatascience.com/analysing-interactions-with-shap-8c4a2bc11c2a#:~:text=SHAP%20values%20are%20used%20to,their%20main%20and%20interaction%20effects\n","metadata":{}}]}