{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install scikit-learn-intelex --progress-bar off >> /tmp/pip_sklearnex.log","metadata":{"execution":{"iopub.status.busy":"2022-06-03T07:43:32.770744Z","iopub.execute_input":"2022-06-03T07:43:32.771197Z","iopub.status.idle":"2022-06-03T07:43:45.693106Z","shell.execute_reply.started":"2022-06-03T07:43:32.771103Z","shell.execute_reply":"2022-06-03T07:43:45.69198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom catboost import CatBoostRegressor\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\nfrom tqdm.notebook import tqdm\nfrom sklearnex import patch_sklearn, config_context\npatch_sklearn()\n\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.impute import SimpleImputer","metadata":{"execution":{"iopub.status.busy":"2022-06-03T07:43:45.695242Z","iopub.execute_input":"2022-06-03T07:43:45.695628Z","iopub.status.idle":"2022-06-03T07:43:49.359137Z","shell.execute_reply.started":"2022-06-03T07:43:45.695589Z","shell.execute_reply":"2022-06-03T07:43:49.358112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.core.display import display, HTML, Javascript\n\ncolor_map = ['#FFFFFF','#FF5733']\n\nprompt = color_map[-1]\nmain_color = color_map[0]\nstrong_main_color = color_map[1]\ncustom_colors = [strong_main_color, main_color]\n\ncss_file = '''\ndiv #notebook {\nbackground-color: white;\nline-height: 20px;\n}\n\n#notebook-container {\n%s\nmargin-top: 2em;\npadding-top: 2em;\nborder-top: 4px solid %s;\n-webkit-box-shadow: 0px 0px 8px 2px rgba(224, 212, 226, 0.5);\n    box-shadow: 0px 0px 8px 2px rgba(224, 212, 226, 0.5);\n}\n\ndiv .input {\nmargin-bottom: 1em;\n}\n\n.rendered_html h1, .rendered_html h2, .rendered_html h3, .rendered_html h4, .rendered_html h5, .rendered_html h6 {\ncolor: %s;\nfont-weight: 600;\n}\n\ndiv.input_area {\nborder: none;\n    background-color: %s;\n    border-top: 2px solid %s;\n}\n\ndiv.input_prompt {\ncolor: %s;\n}\n\ndiv.output_prompt {\ncolor: %s; \n}\n\ndiv.cell.selected:before, div.cell.selected.jupyter-soft-selected:before {\nbackground: %s;\n}\n\ndiv.cell.selected, div.cell.selected.jupyter-soft-selected {\n    border-color: %s;\n}\n\n.edit_mode div.cell.selected:before {\nbackground: %s;\n}\n\n.edit_mode div.cell.selected {\nborder-color: %s;\n\n}\n'''\n\ndef to_rgb(h): \n    return tuple(int(h[i:i+2], 16) for i in [0, 2, 4])\n\nmain_color_rgba = 'rgba(%s, %s, %s, 0.1)' % (to_rgb(main_color[1:]))\nopen('notebook.css', 'w').write(css_file % ('width: 95%;', main_color, main_color, main_color_rgba, \n                                            main_color,  main_color, prompt, main_color, main_color, \n                                            main_color, main_color))\n\ndef nb(): \n    return HTML(\"<style>\" + open(\"notebook.css\", \"r\").read() + \"</style>\")\nnb()","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-03T07:43:49.364045Z","iopub.execute_input":"2022-06-03T07:43:49.364457Z","iopub.status.idle":"2022-06-03T07:43:49.390405Z","shell.execute_reply.started":"2022-06-03T07:43:49.364415Z","shell.execute_reply":"2022-06-03T07:43:49.389156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"![](https://i.imgur.com/wNuFkP1.png)\n\n# <div style=\"padding: 30px;color:white;margin:10;font-size:80%;text-align:left;display:fill;border-radius:10px;background-color:#F1C40F;overflow:hidden;background-image: url(https://i.imgur.com/f1SNYqc.png)\"><b><span style='color:white'>1 | Introduction </span></b> </div>\n\n## **<span style='color:#370FA9'>NOTEBOOK AIM - DATA IMPUTATION</span>**\n\n- As the **[dataset](https://www.kaggle.com/competitions/tabular-playground-series-jun-2022)** suggests, the data comes with missing data & we have impute missing data, in one form or another\n- There are a number of ways we can treat missing data in a dataset & there are plenty of notebooks showing **simple imputation methods**\n- My favourite approach for data imputation is an **<span style='color:#CB61CE'>ensemble model approach</span>**, as it offers a wide range of of variability and allows us to experiment with hyperparameters\n- In this notebook, we'll take a look at a **<span style='color:#CB61CE'>model based approach to imputation</span>**, including **<span style='color:#CB61CE'>ensembling</span>** of different models, \n- We'll be using very contrasting models from **unsupervised** & **supervised learning** methods\n\n## **<span style='color:#370FA9'>DATA IMPUTATION TYPES</span>**\n\n- I will not go into too much depth, as can be seen in a **[discussion post](https://www.kaggle.com/competitions/tabular-playground-series-jun-2022/discussion/328568)**, there is a no shortage of literature on this topic\n- There are lots of notebooks utilising **[SimpleImputer](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html)** as well, which is quite a **quick method**, so it has its merits\n- Focusing on Model based imputation methods here:\n\n> - Model based approached are slightly more costly compared to more simpler methods such as **[SimpleImputer](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html)**:\n> - While some imputation methods are deemed appropriate for a specific type of data, e.g. normally distributed data, MCAR missingness, etc., \n> - These methods are criticized mostly for biasing our estimates and models. Some, therefore, believed that deletion methods are safer in some circumstances.\n> - Fortunately for us, newer categories of imputation methods address these weaknesses of the simple imputation and the deletion methods.\n> - These are **model-based** and **multiple imputation** methods (**ensembles**)\n> - An example of a  **<span style='color:#CB61CE'>model based imputer</span>** can be found in **[IterativeImputer](https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html)** & in **[notebook](https://www.kaggle.com/code/reymaster/0-93002-iterative-imputer-baseline)**\n\n## **<span style='color:#370FA9'>GPU ACCELERATION</span>**\n- One clear downside of a model based approach (even more an **<span style='color:#CB61CE'>ensemble</span>** approach) is the **high cost of imputation** compared to simpler methods\n- However, there are some neat tools we can utilise in order to accelerate the entire process:\n> - **<mark style=\"background-color:#FFC300;color:white;border-radius:5px;opacity:1.0\">Catboost</mark>** itself, allows models to be trained on GPUs, as shown in **[reference](https://catboost.ai/en/docs/features/training-on-gpu)**\n> - For **<mark style=\"background-color:#393939;color:white;border-radius:5px;opacity:1.0\">kNR</mark>**, it's a little more trickier, the base library doesn't use GPU by default, however we can utilise the extension **<span style='color:#CB61CE'>sklearnex</span>**, as shown in **[reference](https://github.com/intel/scikit-learn-intelex)**\n- The effectiveness of GPU depends on **how much data we use** to train a model, as there is a cost of moving data to the GPU\n- Smaller datasets don't benefit from GPU usage, however as the **[Tabular Data Competition](https://www.kaggle.com/competitions/tabular-playground-series-jun-2022)** dataset contains a million rows,\n- So there should be a clear difference between using & not using a GPU to impute missing data when using the **<span style='color:#CB61CE'>model based approach for imputation</span>** ","metadata":{}},{"cell_type":"markdown","source":"# <div style=\"padding: 30px;color:white;margin:10;font-size:80%;text-align:left;display:fill;border-radius:10px;background-color:#F1C40F;overflow:hidden;background-image: url(https://i.imgur.com/f1SNYqc.png)\"><b><span style='color:white'>2 | Imputation Class </span></b> </div>\n\n## **<span style='color:#370FA9'>CLASS ATTRIBUTES & METHODS</span>**\n\n**<mark style=\"background-color:#393939;color:white;border-radius:5px;opacity:1.0\">ATTRIBUTES</mark>**\n- <code>df</code> - Each **instance** requires an input dataframe (which we want to impute)\n- <code>idf</code> - Created upon calling **<mark style=\"background-color:#393939;color:white;border-radius:5px;opacity:1.0\">methods</mark>** (**impute_model**,**impute_simple**), othewise is **None**\n- <code>na_cols</code> - List of features which have missing/NaN data\n- <code>coeffCB</code> - Ensemble Coefficient for the **<mark style=\"background-color:#FFC300;color:white;border-radius:5px;opacity:1.0\">Catboost</mark>** (gradient boosting regressor)\n- <code>coeffkN</code> - Ensemble Coefficient for the **<mark style=\"background-color:#393939;color:white;border-radius:5px;opacity:1.0\">kNR</mark>** (regressor)\n- <code>knr_neigh</code> - Number of nearest neighbours in **<mark style=\"background-color:#393939;color:white;border-radius:5px;opacity:1.0\">kNR</mark>**\n- <code>gpu</code> - Utilise **GPU** during data imputation\n- <code>cat_only</code> - Impute data using **<mark style=\"background-color:#FFC300;color:white;border-radius:5px;opacity:1.0\">Catboost</mark>** only (reduces the imputation time)\n\n**<mark style=\"background-color:#393939;color:white;border-radius:5px;opacity:1.0\">METHODS</mark>**\n- <code>plot_na</code> - Visualise missing data before imputation (**option**='percentage'/'count',**show_id**='original'/'imputed')\n- <code>stats</code> - Visualise the feature/column univariate distribution\n- <code>impute_model</code> - **Ensemble Model Imputation Approach**\n- <code>impute_simple</code> - Simple imputation, by **mean** or **median** value in a column, the **most frequent value** or some **constant value**","metadata":{}},{"cell_type":"code","source":"''' DATA IMPUTATION '''\n# class is used for data imputation & visualisation\n\n# [1] Input dataframe upon instantiation, set ldf\n# [2] Visualise missing data % or count in each column using plot_na method\n# [3] Impute imported dataframe using impute_model, impute_simple\n# [4] Once imputed, data is stored in the idf attribute\n# If the sklearnex library is loaded, scikit-learn-intelex extension will be used\n\nclass imputer:\n    \n    def __init__(self,ldf=None,gpu=False,cat_only=False):\n        self.df = ldf            # imported dataframe \n        self.idf = None          # imputed data\n        self.na_cols = None      # columns with missing data\n        \n        # Check if there is any missing data in the dataframe\n        na_sum = ldf.isna().sum(axis=0) # series\n        na_cols = list(na_sum[na_sum>0].index)\n        if(len(na_cols) > 0):\n            print(f'[note] {len(na_cols)} columns with missing data found')\n            self.na_cols = na_cols\n        \n        # Model Imputation attributes\n        \n        # parameters we can set before calling impute_model\n        self.knr_neigh = 2       # Number of Neighbours in kNN model\n        self.coeffCB = 0.5       # Ensemble Coefficient for CB\n        self.coeffkN = 0.5       # Ensemble Coefficient for knn\n        self.gpu = gpu           # GPU option for Catboost\n        self.cat_only = cat_only # CatBoost Imputation only (no kNR) \n        \n        # adjust the ensemble coefficient if only one model\n        if(self.cat_only):\n            self.coeffCB = 1.0\n        \n        # Constant Imputer attributes\n        \n        self.si_const_val = 1.0  # impute NaN with constant value\n        \n    ''' Function to plot missing data '''\n\n    # arguments:\n    # option - display missing data as percentage ('percentage') or count ('count')\n    # show_id - identifier to show which dataframe to show missing data\n\n    def plot_na(self,option='percentage',show_id='original'):\n        \n        # choose which dataframe to use\n        if(show_id == 'original'):\n            ldf = self.df\n        else:\n            ldf = self.idf\n    \n        # Find missing data in columns    \n        if(option == 'percentage'):\n            leng = len(ldf)\n            na_sum = (ldf.isnull().sum()/leng*100).sort_values(ascending=False)\n        else:\n            na_sum = ldf.isna().sum(axis=0)\n    \n        fig = px.bar(na_sum,color=na_sum.values)\n    \n        # Plot aesthetics\n        fig.update_layout(height=300,template='plotly_white',\n                          title='Missing Data in Columns',\n                      margin=dict(l=50,r=80,t=80,b=40),\n                      font=dict(family='sans-serif',size=12),\n                      xaxis_title=\"Feature Names\",\n                      yaxis_title=\"Number of Missing Rows\",\n                      showlegend=False)\n    \n        if(option == 'perc'):\n            fig.update_layout(yaxis_title=\"% Missing Data\")\n        else:\n            fig.update_layout(yaxis_title=\"Number of Missing Rows\")\n        \n        fig.update_traces(width=.9)          \n        fig.update_coloraxes(showscale=False) # remove colourbar\n        fig.show()\n        \n    ''' Plot Univariate Statistics of Missing Data Columns '''\n    # visualise univariate statistics data of missing columns\n    \n    # to_plot - (str) argument option to change to histogram/boxplot/violin plot\n    # n_cols - (int)number of columns in subplot\n    # height - (int) height of the figure\n    # sample - (int) number of samples only\n\n    def stats(self,n_cols=5,height=400,sample=None,bins=50,barmode='overlay'):\n\n        # select only int or floats (assumed 64 bit)\n        ldf = self.df.select_dtypes(include=['float64','int64'])\n        if(self.idf is not None):\n            ildf = self.idf.select_dtypes(include=['float64','int64']) \n\n        # If there are any columss with missing data\n        if(len(self.na_cols)>0):\n            \n            if(sample is not None):\n                show_columns = self.na_cols[:sample]\n            else:\n                show_columns = self.na_cols\n\n            n_rows = -(-len(show_columns) // n_cols)  # math.ceil in a fast way, without import\n            row_pos, col_pos = 1, 0\n            \n            fig = make_subplots(rows=n_rows, cols=n_cols,subplot_titles=show_columns)\n\n            for col in show_columns:\n                if(self.idf is not None):                    \n                    itrace = go.Histogram(x=ildf[col],nbinsx=bins,marker_color='#283747',name='imputed')\n                    trace = go.Histogram(x=ldf[col],nbinsx=bins,marker_color='#C7C7C7',name='before')\n                else:\n                    trace = go.Histogram(x=ldf[col],nbinsx=bins,marker_color='#283747')\n\n                if(col_pos == n_cols): \n                    row_pos += 1\n                col_pos = col_pos + 1 if (col_pos < n_cols) else 1\n                \n                if(self.idf is not None):\n                    fig.add_trace(itrace, row=row_pos, col=col_pos)\n                    fig.add_trace(trace, row=row_pos, col=col_pos)\n                else:\n                    fig.add_trace(trace, row=row_pos, col=col_pos)\n\n            fig.update_layout(template='plotly_white',\n                              title=f'Univariate Feature Distribution',\n                              font=dict(family='sans-serif',size=12))\n\n            fig.update_traces(marker=dict(line=dict(width=0.5, color='white')),\n                              opacity=0.75)\n                \n            # If we have already imputed & can make comparison\n            if(self.idf is not None):\n                fig.update_layout(barmode=barmode)\n                \n            fig.update(layout_showlegend=False)\n            fig.update_layout(height=height);fig.show()\n        \n    ''' Model basaed imputation '''\n    # ensemble based model imputation kNR & CatBoost\n    # if method argument cols is not given, all columns are imputed\n    # cols - list of strings (column names)\n    \n    def impute_model(self,cols=None):\n\n        # separate dataframe into numerical/categorical\n        \n        # select numerical columns in df\n        ldf = self.df.select_dtypes(include=[np.number])           \n        #  select categorical columns in df\n        ldf_putaside = self.df.select_dtypes(exclude=[np.number])  \n\n        # define columns w/ and w/o missing data\n        \n        # list of features w/ missing data \n        cols_nan = ldf.columns[ldf.isna().any()].tolist()       \n        # get all colun data w/o missing data\n        cols_no_nan = ldf.columns.difference(cols_nan).values     \n\n        if(cols is not None):\n            cols_nan = cols\n            df1 = ldf[cols_nan].describe()\n\n        ''' Cycle through all columns '''\n        # that have Nan Data and impute data using modeling\n            \n        fill_id = -1\n        for col in tqdm(cols_nan):  \n            \n            fill_id+=1\n            \n            # indicies which have missing data will become our test set\n            imp_test = ldf[ldf[col].isna()]\n            \n            # all indicies which which have no missing data\n            imp_train = ldf.dropna()\n            \n            # instantiate Catboost Regressor Supervised Approach\n            if(self.gpu):\n                cbmodel = CatBoostRegressor(verbose=False,\n                                            task_type=\"GPU\")\n            else:\n                cbmodel = CatBoostRegressor(verbose=False)\n                \n            # instantiate KNR Unsupervised Approach\n            if(self.gpu):\n                with config_context(target_offload=\"gpu:0\"):\n                    knmodel = KNeighborsRegressor(n_neighbors=self.knr_neigh)\n            else:\n                knmodel = KNeighborsRegressor(n_neighbors=self.knr_neigh)\n            \n            # fit models on no-nan data\n            if(self.cat_only):\n                cbm = cbmodel.fit(imp_train[cols_no_nan], imp_train[col])\n                cbP = cbm.predict(imp_test[cols_no_nan])\n                pred = self.coeffCB*cbP\n                \n            else:\n                knm = knmodel.fit(imp_train[cols_no_nan], imp_train[col])\n                cbm = cbmodel.fit(imp_train[cols_no_nan], imp_train[col])\n                knrP = knm.predict(imp_test[cols_no_nan])\n                cbP = cbm.predict(imp_test[cols_no_nan])\n                \n                pred = self.coeffkN*knrP + self.coeffCB*cbP # Simple Model Ensemble\n\n            # add imputation information\n            ldf.loc[self.df[col].isna(), col] = pred              \n            ldf.loc[self.df[col].isna(),'fill_id'] = fill_id # Add imp. \n\n        df2 = ldf[cols_nan].describe()\n\n        self.idf =  pd.concat([ldf,ldf_putaside],axis=1)\n        print(f'[note] imputation complete')\n        \n       \n    ''' Simple Imputer Function '''\n    # simple imputation using SimpleImputer\n\n    def impute_simple(self,option='mean'):\n        \n        if(option == 'mean'):\n\n            # by default simple imputer uses mean\n            imputed_mean = SimpleImputer()\n            self.idf = imputed_mean.fit_transform(self.df)\n\n        elif(option == 'median'):\n\n            # impute with column median\n            imputed_median = SimpleImputer(strategy='median')\n            self.idf = imputed_median.fit_transform(self.df)\n\n        elif(option == 'most_freq'):\n\n            # impute using most frequent value in each column\n            imputed_freq = SimpleImputer(strategy='most_frequent')\n            self.idf = imputed_freq.fit_transform(self.df)\n\n        elif(option == 'constant'):\n\n            # impute using most frequent value in each column\n            imputed_const = SimpleImputer(strategy='constant',\n                                          fill_value=self.si_const_val)\n            self.idf = imputed_const.fit_transform(self.df)","metadata":{"execution":{"iopub.status.busy":"2022-06-03T09:21:29.337583Z","iopub.execute_input":"2022-06-03T09:21:29.338015Z","iopub.status.idle":"2022-06-03T09:21:29.375914Z","shell.execute_reply.started":"2022-06-03T09:21:29.337981Z","shell.execute_reply":"2022-06-03T09:21:29.375176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"''' Simple Test - Class Usage '''\n# Testing our class\n\n# Dataset Usage Examples\nfrom sklearn import datasets\n\n# convert sklearn dataset to pandas DataFrame\ndef sklearn_to_df(sklearn_dataset):\n    df = pd.DataFrame(sklearn_dataset.data, columns=sklearn_dataset.feature_names)\n    df['target'] = pd.Series(sklearn_dataset.target)\n    return df\n\n''' Load DataFrame w/o missing data '''\ndf_cali = sklearn_to_df(datasets.fetch_california_housing())\ndf_cali.head()\n\n''' Remove random percentage of data in two columns '''\n# Randomly remove some data in columns\n\ndef create_nan(ldf,column,p):\n    ldf[column] = ldf[column].apply(lambda x: np.nan if np.random.rand() < p else x)\n    return ldf\n    \ndf_mod = create_nan(df_cali,'HouseAge',2/10)\ndf_mod = create_nan(df_mod,'AveBedrms',2/4)\n\n# Instantiate Imputer Class\ncali_impute = imputer(ldf=df_cali)\n\n# Model based imputation of two columns\ncali_impute.impute_model(['HouseAge','AveBedrms'])\n\n# Dataframe contaning imputed df & imputer info column (fill_id)\ncali_impute.idf.head()\n# cali_impute.stats(sample=2,barmode='relative')","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-03T09:24:25.547256Z","iopub.execute_input":"2022-06-03T09:24:25.547872Z","iopub.status.idle":"2022-06-03T09:24:31.364902Z","shell.execute_reply.started":"2022-06-03T09:24:25.547832Z","shell.execute_reply":"2022-06-03T09:24:31.364069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <div style=\"padding: 30px;color:white;margin:10;font-size:80%;text-align:left;display:fill;border-radius:10px;background-color:#F1C40F;overflow:hidden;background-image: url(https://i.imgur.com/f1SNYqc.png)\"><b><span style='color:white'>3 | Visualising Missing Data </span></b> </div>\n\n## **<span style='color:#370FA9'>LOADING DATA</span>**\n- Let's read the data & view the some basic information about the dataset using **<mark style=\"background-color:#393939;color:white;border-radius:5px;opacity:1.0\">info</mark>**\n- We'll create two subsets, one for **<span style='color:#CB61CE'>CPU imputation</span>** (2000 entries) & one for **<span style='color:#CB61CE'>GPU imputation</span>** (10000 entries), using **<mark style=\"background-color:#393939;color:white;border-radius:5px;opacity:1.0\">sample</mark>** method","metadata":{}},{"cell_type":"code","source":"# Import data\ndata = pd.read_csv('/kaggle/input/tabular-playground-series-jun-2022/data.csv')\nprint(f'[note] data loaded {data.shape}')\n\n# Create a small subset\ndata_small = data.sample(2000)\nprint(f'[note] selected subset {data_small.shape}')\n\n# Create a large dataset (used for GPU imputation)\ndata_large = data.sample(100000)\nprint(f'[note] selected subset {data_large.shape}')","metadata":{"execution":{"iopub.status.busy":"2022-06-03T09:16:56.773965Z","iopub.execute_input":"2022-06-03T09:16:56.774609Z","iopub.status.idle":"2022-06-03T09:17:13.21498Z","shell.execute_reply.started":"2022-06-03T09:16:56.774575Z","shell.execute_reply":"2022-06-03T09:17:13.213985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- We have two types of **<span style='color:#CB61CE'>dtypes</span>**, both of which are **<span style='color:#CB61CE'>numerical</span>**, so we can use a model approach (**regressor**)\n- Without having to do any modification to column data before imputation, 'int64' suggests there are discretised numeric features","metadata":{}},{"cell_type":"code","source":"# Check dtype information\ndata_small.dtypes.unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- If we had **<span style='color:#CB61CE'>categorical features</span>**, **[@ijcrook](https://www.kaggle.com/ijcrook)** has uploaded some useful functions for data imputation using **<span style='color:#CB61CE'>model based imputation</span>** in functions **<mark style=\"background-color:#393939;color:white;border-radius:5px;opacity:1.0\">knn_impute</mark>**, **<mark style=\"background-color:#393939;color:white;border-radius:5px;opacity:1.0\">ML_impute</mark>**","metadata":{}},{"cell_type":"markdown","source":"## **<span style='color:#370FA9'>CHECKING FOR NAN VALUES</span>**\n- Aside from imputation, class <code>impute</code> contains methods for visualising missing data as well\n- We can call method **<mark style=\"background-color:#393939;color:white;border-radius:5px;opacity:1.0\">plot_na</mark>**, which by default will show the percentage of missing data in each column (**count** or **percentage** are available)","metadata":{}},{"cell_type":"code","source":"check_data = imputer(data_small) # create instance\ncheck_data.plot_na()       # visualise missing data % in each column","metadata":{"execution":{"iopub.status.busy":"2022-06-03T09:17:13.216735Z","iopub.execute_input":"2022-06-03T09:17:13.217206Z","iopub.status.idle":"2022-06-03T09:17:13.337728Z","shell.execute_reply.started":"2022-06-03T09:17:13.217168Z","shell.execute_reply":"2022-06-03T09:17:13.336972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <div style=\"padding: 30px;color:white;margin:10;font-size:80%;text-align:left;display:fill;border-radius:10px;background-color:#F1C40F;overflow:hidden;background-image: url(https://i.imgur.com/f1SNYqc.png)\"><b><span style='color:white'>4 | Data Imputation </span></b> </div>\n\n## **[CPU] <span style='color:#370FA9'>ENSEMBLE IMPUTATION OF MISSING DATA</span>**\n- We import the desired **dataframe** containing missing data by first creating an instance of class <code>imputer</code> & setting argument **ldf**\n- Setting parameters such as <code>coeffCB</code>, <code>coeffkN</code> ..., the full list of class attributes are summarised in **<mark style=\"background-color:#393939;color:white;border-radius:5px;opacity:1.0\">Section 2</mark>**\n- Then we fill in the missing data in the dataframe by calling the **<mark style=\"background-color:#393939;color:white;border-radius:5px;opacity:1.0\">impute_model</mark>** method","metadata":{}},{"cell_type":"code","source":"# impute model on CPU\ndata_impute_small = imputer(ldf=data_small)\ndata_impute_small.impute_model()","metadata":{"execution":{"iopub.status.busy":"2022-06-03T09:21:35.164349Z","iopub.execute_input":"2022-06-03T09:21:35.165238Z","iopub.status.idle":"2022-06-03T09:22:28.598861Z","shell.execute_reply.started":"2022-06-03T09:21:35.1652Z","shell.execute_reply":"2022-06-03T09:22:28.597868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **[GPU] <span style='color:#370FA9'>ENSEMBLE IMPUTATION OF MISSING DATA</span>**\n- For larger datasets, we can utilise **GPU** acceleration for **<mark style=\"background-color:#FFC300;color:white;border-radius:5px;opacity:1.0\">Catboost</mark>** & **<mark style=\"background-color:#393939;color:white;border-radius:5px;opacity:1.0\">kNR</mark>** (via the scikit-learn-intelex extension), to save some time training the model, setting <code>gpu</code> to **True**\n- However there is little use to utilise the **GPU** on small datasets, but for the entire dataset (**1M rows**) this should give some speedup for imputation","metadata":{}},{"cell_type":"code","source":"# impute missing data using gpu\ndata_impute_large = imputer(ldf=data_large,gpu=True)\ndata_impute_large.impute_model()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **<span style='color:#370FA9'>VISUALISE MISSING DATA POST IMPUTATION</span>**\n- To visualise the imputed dataframe, we can set <code>show_id</code> to **imputed** in the **<mark style=\"background-color:#393939;color:white;border-radius:5px;opacity:1.0\">plot_na</mark>** method","metadata":{}},{"cell_type":"code","source":"# check if data has been imputed correctly \ndata_impute_small.plot_na(show_id='imputed')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **<span style='color:#370FA9'>CHECKING UNIVARIATE DISTRIBUTIONS</span>**\n- After imputation, we probably want to **check the data distribution** of **<span style='color:#CB61CE'>before</span>** and **<span style='color:#CB61CE'>after</span>**, to make sure the data hasn't been altered significantly\n- We can plot the features/columns that had missing data and compare the **<span style='color:#CB61CE'>univariate feature distributions</span>** using the **<mark style=\"background-color:#393939;color:white;border-radius:5px;opacity:1.0\">stats</mark>** method","metadata":{}},{"cell_type":"code","source":"# compare univariate distributions before/after\ndata_impute_small.stats(n_cols=3,height=900,sample=9,barmode='relative')","metadata":{"execution":{"iopub.status.busy":"2022-06-03T09:23:24.929045Z","iopub.execute_input":"2022-06-03T09:23:24.929541Z","iopub.status.idle":"2022-06-03T09:23:25.141471Z","shell.execute_reply.started":"2022-06-03T09:23:24.929497Z","shell.execute_reply":"2022-06-03T09:23:25.14078Z"},"trusted":true},"execution_count":null,"outputs":[]}]}