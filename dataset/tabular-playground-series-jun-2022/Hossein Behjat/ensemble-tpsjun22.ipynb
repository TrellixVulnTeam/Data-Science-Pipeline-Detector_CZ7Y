{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a id=\"load-libraries\"></a>\n# Load the libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\n\nimport lightgbm as lgbm\n\nimport optuna\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n\nfrom tqdm import tqdm\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\npd.set_option('display.max_columns', None)\npd.set_option('display.float_format', '{:.2f}'.format)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-14T12:17:27.598905Z","iopub.execute_input":"2022-06-14T12:17:27.599451Z","iopub.status.idle":"2022-06-14T12:17:27.606531Z","shell.execute_reply.started":"2022-06-14T12:17:27.599415Z","shell.execute_reply":"2022-06-14T12:17:27.605528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"read-data\"></a>\n# Read the data","metadata":{}},{"cell_type":"code","source":"data_df = pd.read_csv('../input/tabular-playground-series-jun-2022/data.csv')\nsubmission_df = pd.read_csv('../input/tabular-playground-series-jun-2022/sample_submission.csv')\nprint(data_df.shape)\ndata_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-14T11:54:20.739022Z","iopub.execute_input":"2022-06-14T11:54:20.73941Z","iopub.status.idle":"2022-06-14T11:54:33.735469Z","shell.execute_reply.started":"2022-06-14T11:54:20.739371Z","shell.execute_reply":"2022-06-14T11:54:33.734243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"reduce-memory\"></a>\n# Reducing the memory usage by data","metadata":{}},{"cell_type":"code","source":"def reduce_memory_usage(df, verbose=True):\n    numerics = [\"int8\", \"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\n    start_mem = df.memory_usage().sum() / 1024 ** 2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if (\n                    c_min > np.finfo(np.float16).min\n                    and c_max < np.finfo(np.float16).max\n                ):\n                    df[col] = df[col].astype(np.float16)\n                elif (\n                    c_min > np.finfo(np.float32).min\n                    and c_max < np.finfo(np.float32).max\n                ):\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() / 1024 ** 2\n    if verbose:\n        print(\n            \"Mem. usage decreased to {:.2f} Mb ({:.1f}% reduction)\".format(\n                end_mem, 100 * (start_mem - end_mem) / start_mem\n            )\n        )\n    return df\n\ndata_df = reduce_memory_usage(data_df, verbose=True)\nsubmission_df = reduce_memory_usage(submission_df, verbose=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-14T11:54:33.737066Z","iopub.execute_input":"2022-06-14T11:54:33.737577Z","iopub.status.idle":"2022-06-14T11:54:39.196097Z","shell.execute_reply.started":"2022-06-14T11:54:33.737534Z","shell.execute_reply":"2022-06-14T11:54:39.195234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"ann-modeling\"></a>\n# Define the ANN model","metadata":{}},{"cell_type":"code","source":"def ann_model(train_data, test_data):\n    \n    global submission\n    \n    X = train_data.drop('row_id', axis=1)\n    target = X.pop(column)\n    test_data.pop(column)\n    row_id = test_data.pop('row_id')\n    X = scaler.fit_transform(X)\n    X = imputer.fit_transform(X)\n    test_data = scaler.transform(test_data)\n    test_data = imputer.transform(test_data)\n    \n    X_train, X_valid, y_train, y_valid = train_test_split(X, target)\n    \n        \n    lr_start = 0.01\n    lr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.7, patience = 4, verbose = 1)\n    es = EarlyStopping(monitor = 'val_loss',patience = 12, verbose = 1, mode = 'min', restore_best_weights = True)\n    callbacks = [lr, es]\n    \n    model = keras.Sequential([\n        layers.Dense(128, kernel_regularizer=keras.regularizers.l2(40e-6), activation='swish', input_shape=(X.shape[1],)),\n        layers.BatchNormalization(axis=1),\n        layers.Dense(64, kernel_regularizer=keras.regularizers.l2(40e-6), activation='swish'),\n        layers.BatchNormalization(axis=1),\n        layers.Dense(32, kernel_regularizer=keras.regularizers.l2(40e-6), activation='swish'),\n        layers.BatchNormalization(axis=1),\n        layers.Dense(1, activation='linear'),\n    ])\n    \n    optimizer_func = keras.optimizers.Adam(learning_rate = lr_start)\n    loss_func = keras.losses.MeanSquaredError()\n    \n    model.compile(optimizer = optimizer_func, loss = loss_func, metrics=[keras.metrics.RootMeanSquaredError()])\n    \n    validation_data = (X_valid, y_valid)\n    \n    model.fit(X_train, \n              y_train, \n              validation_data = validation_data, \n              epochs          = 32,\n              verbose         = 2,\n              batch_size      = 2048,\n              shuffle         = True,\n              callbacks       = callbacks\n            )\n    \n    callbacks, es, lr = None, None, None\n    \n    \n    y_val_pred = model.predict(X_valid, batch_size = 2048, verbose = 2)\n    score = mean_absolute_error(y_valid, y_val_pred) ** 0.5\n    \n    best_scores[column] = [score]\n    \n    test_preds = model.predict(test_data)\n    submission = submission.append(pd.DataFrame({'row-col': row_id.astype(str) + '-' + column, 'value': test_preds[:, 0]}))","metadata":{"execution":{"iopub.status.busy":"2022-06-14T11:54:39.197825Z","iopub.execute_input":"2022-06-14T11:54:39.198397Z","iopub.status.idle":"2022-06-14T11:54:39.211521Z","shell.execute_reply.started":"2022-06-14T11:54:39.198363Z","shell.execute_reply":"2022-06-14T11:54:39.21041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"lgbm-modeling\"></a>\n# Define the LGBM model","metadata":{}},{"cell_type":"code","source":"def lgbm_model(train_data, test_data):\n    \n    global submission\n    \n    X = train_data.drop('row_id', axis=1)\n    target = X.pop(column)\n    test_data.pop(column)\n    row_id = test_data.pop('row_id')\n    \n    X_train, X_valid, y_train, y_valid = train_test_split(X, target)\n    \n    lgbm_train = lgbm.Dataset(X_train, label=y_train)\n    lgbm_eval = lgbm.Dataset(X_valid, y_valid, reference=lgbm_train)\n    \n    def objective(trial, lgbm_train, lgbm_eval):\n    \n        params = {\n#          \"device_type\": trial.suggest_categorical(\"device_type\", ['gpu']),\n         'boosting_type': trial.suggest_categorical('boosting_type',['gbdt']),\n         \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 0.3, log=True),\n         \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 200, step=10),\n         \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n         \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 0.01, 100, log=True),\n         \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 0.01, 100, log=True),\n         \"bagging_fraction\": trial.suggest_float(\n             \"bagging_fraction\", 0.5, 0.95, step=0.05\n         ),\n         \"bagging_freq\": trial.suggest_categorical(\"bagging_freq\", [1]),\n         \"feature_fraction\": trial.suggest_float(\n             \"feature_fraction\", 0.5, 0.95, step=0.05\n         ),\n\n         'task': trial.suggest_categorical('task', ['prediction',]),\n         'objective': trial.suggest_categorical('objective', ['regression',]),\n         'metric': trial.suggest_categorical('metric', ['rmse',]),\n         'verbosity': trial.suggest_categorical('verbosity', [-1]),\n             }\n\n        model = lgbm.train(\n                        params,\n                        lgbm_train,\n                        2000,\n                        callbacks=[\n                                    lgbm.early_stopping(stopping_rounds=10),\n                                    lgbm.log_evaluation(2000),\n                                   ],\n                        valid_sets=[lgbm_eval],\n             )\n\n        return model.best_score['valid_0']['rmse']\n    \n    study = optuna.create_study(direction='minimize', study_name='LGBM')\n    func = lambda trial: objective(trial, lgbm_train, lgbm_eval)\n    study.optimize(func, n_trials=15)\n    \n    best_params = study.best_params\n\n    best_model = lgbm.train(\n                       best_params,\n                       lgbm_train,\n                       5000,\n                       callbacks=[\n                                   lgbm.early_stopping(stopping_rounds=100),\n                                   lgbm.log_evaluation(5000),\n                                  ],\n                       valid_sets=[lgbm_eval],\n                      )\n    \n    best_scores[column] = [best_model.best_score['valid_0']['rmse']]\n    \n    test_preds = best_model.predict(test_data)\n    submission = submission.append(pd.DataFrame({'row-col': row_id.astype(str) + '-' + column,\n                                   'value': test_preds}))","metadata":{"execution":{"iopub.status.busy":"2022-06-14T12:17:52.050613Z","iopub.execute_input":"2022-06-14T12:17:52.051067Z","iopub.status.idle":"2022-06-14T12:17:52.067009Z","shell.execute_reply.started":"2022-06-14T12:17:52.05103Z","shell.execute_reply":"2022-06-14T12:17:52.065945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"ensemble-modeling\"></a>\n# Train Ensembles","metadata":{}},{"cell_type":"code","source":"%%time\n\nmiss_cols = data_df.columns[data_df.isnull().any()]\nsubmission = pd.DataFrame(columns=['row-col', 'value'])\nscaler = StandardScaler()\nimputer = SimpleImputer(strategy='mean')\nbest_scores   = {}\n\nfor column in tqdm(miss_cols):\n    \n    ann_cols = ['F_4_5', 'F_4_1', 'F_4_7', 'F_4_0', 'F_4_6']\n    lgbm_cols = ['F_4_8', 'F_1_13', 'F_4_12', 'F_3_19', 'F_1_7', 'F_3_21', 'F_4_11', 'F_1_12', 'F_4_13', 'F_4_9', 'F_4_3', 'F_4_2', 'F_4_10', 'F_4_14', 'F_4_4']\n#     fillna_cols = ['F_3_22', 'F_3_2', 'F_3_20', 'F_3_9', 'F_3_1', 'F_3_15', 'F_3_24', 'F_3_7', 'F_1_1', 'F_1_4', 'F_1_9', 'F_3_12', 'F_1_0', 'F_3_8', 'F_3_13', 'F_1_5', 'F_1_14', 'F_3_0', 'F_1_8', 'F_1_3', 'F_3_5', 'F_3_18', 'F_3_4', 'F_1_2', 'F_3_16', 'F_3_6', 'F_3_11', 'F_3_3', 'F_3_14', 'F_3_17', 'F_1_10', 'F_1_11', 'F_3_10', 'F_1_6', 'F_3_23']\n    \n    train_data = data_df.loc[(data_df[column].notnull())]\n    test_data = data_df.loc[(data_df[column].isnull())]\n        \n    if column in ann_cols:\n        ann_model(train_data, test_data)\n        \n    elif column in lgbm_cols:\n        lgbm_model(train_data, test_data)\n        \n    else:\n        X_train = train_data[column]\n        X_valid = test_data[column]\n        y_valid = X_train.sample(n=int(X_valid.shape[0]))\n        row_id = test_data.pop('row_id')\n\n        X_train_mean = X_train.mean()\n        X_valid.fillna(X_train_mean, inplace=True)\n        rmse_score = mean_squared_error(y_valid, X_valid)**0.5\n        best_scores[column] = [rmse_score]\n        submission = submission.append(pd.DataFrame({'row-col': row_id.astype(str) + '-' + column,\n                                   'value': X_valid}))","metadata":{"execution":{"iopub.status.busy":"2022-06-14T12:20:26.075301Z","iopub.execute_input":"2022-06-14T12:20:26.075802Z","iopub.status.idle":"2022-06-14T12:22:52.877117Z","shell.execute_reply.started":"2022-06-14T12:20:26.075764Z","shell.execute_reply":"2022-06-14T12:22:52.876156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"ploting\"></a>\n# Plot the results","metadata":{}},{"cell_type":"code","source":"scores_df = pd.DataFrame(best_scores)\nscores_df = scores_df.sort_values(by=0, axis=1)\ndisplay(scores_df)\n\n_, ax = plt.subplots(figsize=(20, 8))\nsns.barplot(data=scores_df, ax=ax)\nplt.xticks(rotation = 90)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-14T12:23:02.198482Z","iopub.execute_input":"2022-06-14T12:23:02.198886Z","iopub.status.idle":"2022-06-14T12:23:02.377476Z","shell.execute_reply.started":"2022-06-14T12:23:02.198853Z","shell.execute_reply":"2022-06-14T12:23:02.376405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"submission\"></a>\n# Make a submission","metadata":{}},{"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)\nsubmission","metadata":{"execution":{"iopub.status.busy":"2022-06-14T12:23:09.287807Z","iopub.execute_input":"2022-06-14T12:23:09.288684Z","iopub.status.idle":"2022-06-14T12:23:09.37663Z","shell.execute_reply.started":"2022-06-14T12:23:09.288639Z","shell.execute_reply":"2022-06-14T12:23:09.375569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}