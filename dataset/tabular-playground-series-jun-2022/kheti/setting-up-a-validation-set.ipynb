{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-10T05:54:32.48757Z","iopub.execute_input":"2022-06-10T05:54:32.488064Z","iopub.status.idle":"2022-06-10T05:54:32.52007Z","shell.execute_reply.started":"2022-06-10T05:54:32.487945Z","shell.execute_reply":"2022-06-10T05:54:32.519029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## In this competition, we have been given only one dataset with some missing values and our goal is to predict for those missing values. \n\n## The aim of this notebook is to show how to create training and validation sets from the entire data and use that to improve the model's performance.\n\n## Some functions to generate plots have been taken from [this](https://www.kaggle.com/code/robikscube/handling-with-missing-data-youtube-stream) notebook.","metadata":{}},{"cell_type":"markdown","source":"## Update\n\n#### 1. The actual % of missing values per column is 1.8% and not 18%. This has been rectified.\n#### 2. A Cross validation loop has been included in place of simple train_test_split.\n#### 3. The metric has been changed to RMSE instead of MSE.","metadata":{}},{"cell_type":"code","source":"import matplotlib.pylab as plt\ncolor_pal = plt.rcParams['axes.prop_cycle'].by_key()['color']\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nplt.style.use('ggplot')\n\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2022-06-10T05:54:44.635125Z","iopub.execute_input":"2022-06-10T05:54:44.635632Z","iopub.status.idle":"2022-06-10T05:54:45.281217Z","shell.execute_reply.started":"2022-06-10T05:54:44.635579Z","shell.execute_reply":"2022-06-10T05:54:45.280228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(\"../input/tabular-playground-series-jun-2022/data.csv\")\nss = pd.read_csv(\"../input/tabular-playground-series-jun-2022/sample_submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-06-10T05:54:45.634854Z","iopub.execute_input":"2022-06-10T05:54:45.63582Z","iopub.status.idle":"2022-06-10T05:55:05.48847Z","shell.execute_reply.started":"2022-06-10T05:54:45.635768Z","shell.execute_reply":"2022-06-10T05:55:05.487693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.shape","metadata":{"execution":{"iopub.status.busy":"2022-06-10T05:55:05.489844Z","iopub.execute_input":"2022-06-10T05:55:05.490267Z","iopub.status.idle":"2022-06-10T05:55:05.497187Z","shell.execute_reply.started":"2022-06-10T05:55:05.490238Z","shell.execute_reply":"2022-06-10T05:55:05.496599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Function to plot % of missing values","metadata":{}},{"cell_type":"code","source":"def show_perc_values_missing(df):\n    ncounts = pd.DataFrame([df.isna().mean()]).T\n    ncounts = ncounts.rename(columns={0: \"train_missing\"})\n\n    ncounts.query(\"train_missing > 0\").plot(\n        kind=\"barh\", figsize=(8, 15), title=\"% of Values Missing\"\n    )\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-10T05:55:05.49815Z","iopub.execute_input":"2022-06-10T05:55:05.498535Z","iopub.status.idle":"2022-06-10T05:55:05.507546Z","shell.execute_reply.started":"2022-06-10T05:55:05.498509Z","shell.execute_reply":"2022-06-10T05:55:05.506791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_perc_values_missing(train)","metadata":{"execution":{"iopub.status.busy":"2022-06-10T05:55:05.509268Z","iopub.execute_input":"2022-06-10T05:55:05.509559Z","iopub.status.idle":"2022-06-10T05:55:06.487607Z","shell.execute_reply.started":"2022-06-10T05:55:05.509533Z","shell.execute_reply":"2022-06-10T05:55:06.486388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ~1.8% of values is missing in each column","metadata":{}},{"cell_type":"code","source":"#List of all columns with missing values\nnacols = ['F_1_0', 'F_1_1', 'F_1_2', 'F_1_3', 'F_1_4', 'F_1_5', 'F_1_6',\n       'F_1_7', 'F_1_8', 'F_1_9', 'F_1_10', 'F_1_11', 'F_1_12', 'F_1_13',\n       'F_1_14', 'F_3_0', 'F_3_1', 'F_3_2',\n       'F_3_3', 'F_3_4', 'F_3_5', 'F_3_6', 'F_3_7', 'F_3_8', 'F_3_9', 'F_3_10',\n       'F_3_11', 'F_3_12', 'F_3_13', 'F_3_14', 'F_3_15', 'F_3_16', 'F_3_17',\n       'F_3_18', 'F_3_19', 'F_3_20', 'F_3_21', 'F_3_22', 'F_3_23', 'F_3_24',\n       'F_4_0', 'F_4_1', 'F_4_2', 'F_4_3', 'F_4_4', 'F_4_5', 'F_4_6', 'F_4_7',\n       'F_4_8', 'F_4_9', 'F_4_10', 'F_4_11', 'F_4_12', 'F_4_13', 'F_4_14']","metadata":{"execution":{"iopub.status.busy":"2022-06-10T05:55:06.48875Z","iopub.execute_input":"2022-06-10T05:55:06.48932Z","iopub.status.idle":"2022-06-10T05:55:06.495645Z","shell.execute_reply.started":"2022-06-10T05:55:06.489287Z","shell.execute_reply":"2022-06-10T05:55:06.494701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Generate a feature to track the number of missing values in a row.\ntrain[\"n_missing\"] = train[nacols].isna().sum(axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-06-10T05:55:06.496758Z","iopub.execute_input":"2022-06-10T05:55:06.497117Z","iopub.status.idle":"2022-06-10T05:55:06.733849Z","shell.execute_reply.started":"2022-06-10T05:55:06.49709Z","shell.execute_reply":"2022-06-10T05:55:06.732988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[\"n_missing\"].value_counts().plot(\n    kind=\"bar\", title=\"Number of Missing Values per Sample\"\n)","metadata":{"execution":{"iopub.status.busy":"2022-06-10T05:55:06.734923Z","iopub.execute_input":"2022-06-10T05:55:06.735214Z","iopub.status.idle":"2022-06-10T05:55:06.952844Z","shell.execute_reply.started":"2022-06-10T05:55:06.735183Z","shell.execute_reply":"2022-06-10T05:55:06.952218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rows_with_no_missing = train.query(\"n_missing == 0\")\nrows_with_no_missing","metadata":{"execution":{"iopub.status.busy":"2022-06-10T05:55:06.953932Z","iopub.execute_input":"2022-06-10T05:55:06.954391Z","iopub.status.idle":"2022-06-10T05:55:07.263147Z","shell.execute_reply.started":"2022-06-10T05:55:06.954358Z","shell.execute_reply":"2022-06-10T05:55:07.262488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ~36% of the rows have no missing values. ","metadata":{}},{"cell_type":"markdown","source":"## We can use these 36% of the rows to create training and validation sets. But we have to keep certain things in mind to ensure that the train data we create resembles the original dataset as closely as possible. This is done by the below steps.\n\n* Introduce missing values **\"randomly\"** in the 36% of the rows with no missing data.\n* Make sure to keep \"F_2_*\" columns as non-missing.\n* Ensure around 1.8% of the data is misssing in each column.\n\n## Since we know the ground truth values for these 36% of the rows, we can use the training and validation sets thus created to compare models and improve performance.","metadata":{}},{"cell_type":"code","source":"rows_with_no_missing.drop(['row_id', 'n_missing'], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-10T05:55:19.017511Z","iopub.execute_input":"2022-06-10T05:55:19.017876Z","iopub.status.idle":"2022-06-10T05:55:19.08428Z","shell.execute_reply.started":"2022-06-10T05:55:19.017848Z","shell.execute_reply":"2022-06-10T05:55:19.083183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create a copy of the dataframe containing rows with no missing values and then randomly introduce ~1.8% of missing values in each column","metadata":{}},{"cell_type":"code","source":"sub_train_ground_truth = rows_with_no_missing.copy()","metadata":{"execution":{"iopub.status.busy":"2022-06-10T05:55:20.599271Z","iopub.execute_input":"2022-06-10T05:55:20.599682Z","iopub.status.idle":"2022-06-10T05:55:20.651134Z","shell.execute_reply.started":"2022-06-10T05:55:20.599648Z","shell.execute_reply":"2022-06-10T05:55:20.650461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in nacols:\n    vals_to_nan = rows_with_no_missing[col].sample(frac=0.018).index\n    rows_with_no_missing.loc[vals_to_nan, col] = np.NaN","metadata":{"execution":{"iopub.status.busy":"2022-06-10T05:55:24.132602Z","iopub.execute_input":"2022-06-10T05:55:24.133479Z","iopub.status.idle":"2022-06-10T05:55:25.453697Z","shell.execute_reply.started":"2022-06-10T05:55:24.133422Z","shell.execute_reply":"2022-06-10T05:55:25.452786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check % of missing values in newly created dataframe\nshow_perc_values_missing(rows_with_no_missing)","metadata":{"execution":{"iopub.status.busy":"2022-06-10T05:55:25.455384Z","iopub.execute_input":"2022-06-10T05:55:25.45642Z","iopub.status.idle":"2022-06-10T05:55:26.314165Z","shell.execute_reply.started":"2022-06-10T05:55:25.45637Z","shell.execute_reply":"2022-06-10T05:55:26.313219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Exactly 1.8% of the values is now missing in our data. Now we will split it into training and validation sets, try an Imputer on both sets and check its performance.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold","metadata":{"execution":{"iopub.status.busy":"2022-06-10T05:58:06.149111Z","iopub.execute_input":"2022-06-10T05:58:06.149633Z","iopub.status.idle":"2022-06-10T05:58:06.282322Z","shell.execute_reply.started":"2022-06-10T05:58:06.149592Z","shell.execute_reply":"2022-06-10T05:58:06.281167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Simple Imputer","metadata":{}},{"cell_type":"code","source":"from sklearn.impute import SimpleImputer\nimptr = SimpleImputer(strategy=\"mean\", add_indicator=False)","metadata":{"execution":{"iopub.status.busy":"2022-06-10T05:58:11.36337Z","iopub.execute_input":"2022-06-10T05:58:11.363872Z","iopub.status.idle":"2022-06-10T05:58:11.502754Z","shell.execute_reply.started":"2022-06-10T05:58:11.363833Z","shell.execute_reply":"2022-06-10T05:58:11.501562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kf = KFold(n_splits=5)\n\nmse_train=[]\nmse_valid=[]\n\nfor i, (train_idx, valid_idx) in enumerate(kf.split(rows_with_no_missing)):\n    print(\"CV fold:\" + str(i+1))\n    \n    train_imputed = imptr.fit_transform(rows_with_no_missing.iloc[train_idx])\n    train_imputed = pd.DataFrame(train_imputed, columns=rows_with_no_missing.columns)\n    \n    valid_imputed = imptr.transform(rows_with_no_missing.iloc[valid_idx])\n    valid_imputed = pd.DataFrame(valid_imputed, columns=rows_with_no_missing.columns)\n    \n    y_train = sub_train_ground_truth.iloc[train_idx]\n    y_valid = sub_train_ground_truth.iloc[valid_idx]\n    \n    mse_train.append(np.sqrt(mean_squared_error(y_train, train_imputed)))\n    mse_valid.append(np.sqrt(mean_squared_error(y_valid, valid_imputed)))\n    \n    print(\"Training error:\" + str(np.sqrt(mean_squared_error(y_train, train_imputed))))\n    print(\"Validation error:\" + str(np.sqrt(mean_squared_error(y_valid, valid_imputed))))","metadata":{"execution":{"iopub.status.busy":"2022-06-10T05:58:41.816126Z","iopub.execute_input":"2022-06-10T05:58:41.816631Z","iopub.status.idle":"2022-06-10T05:58:51.877231Z","shell.execute_reply.started":"2022-06-10T05:58:41.816593Z","shell.execute_reply":"2022-06-10T05:58:51.876026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"The average training error is: \" + str(sum(mse_train)/len(mse_train)))\nprint(\"The average CV error is: \" + str(sum(mse_valid)/len(mse_valid)))","metadata":{"execution":{"iopub.status.busy":"2022-06-10T06:30:53.958255Z","iopub.execute_input":"2022-06-10T06:30:53.958816Z","iopub.status.idle":"2022-06-10T06:30:53.966024Z","shell.execute_reply.started":"2022-06-10T06:30:53.958765Z","shell.execute_reply":"2022-06-10T06:30:53.96522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## LGBM Imputer","metadata":{}},{"cell_type":"code","source":"# !rm -r kuma_utils\n!git clone https://github.com/analokmaus/kuma_utils.git","metadata":{"execution":{"iopub.status.busy":"2022-06-10T06:00:06.067327Z","iopub.execute_input":"2022-06-10T06:00:06.068645Z","iopub.status.idle":"2022-06-10T06:00:07.924492Z","shell.execute_reply.started":"2022-06-10T06:00:06.068597Z","shell.execute_reply":"2022-06-10T06:00:07.923101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\nsys.path.append(\"kuma_utils/\")\nfrom kuma_utils.preprocessing.imputer import LGBMImputer","metadata":{"execution":{"iopub.status.busy":"2022-06-10T06:00:07.9275Z","iopub.execute_input":"2022-06-10T06:00:07.928022Z","iopub.status.idle":"2022-06-10T06:00:09.165323Z","shell.execute_reply.started":"2022-06-10T06:00:07.927967Z","shell.execute_reply":"2022-06-10T06:00:09.164227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgbm_imtr = LGBMImputer(n_iter=100, verbose=True)\n\nmse_train_lgbm=[]\nmse_valid_lgbm=[]\n\nfor i, (train_idx, valid_idx) in enumerate(kf.split(rows_with_no_missing)):\n    print(\"CV fold:\" + str(i+1))\n    \n    train_imputed = lgbm_imtr.fit_transform(rows_with_no_missing.iloc[train_idx])\n    train_imputed = pd.DataFrame(train_imputed, columns=rows_with_no_missing.columns)\n    \n    valid_imputed = lgbm_imtr.transform(rows_with_no_missing.iloc[valid_idx])\n    valid_imputed = pd.DataFrame(valid_imputed, columns=rows_with_no_missing.columns)\n    \n    y_train = sub_train_ground_truth.iloc[train_idx]\n    y_valid = sub_train_ground_truth.iloc[valid_idx]\n    \n    mse_train_lgbm.append(np.sqrt(mean_squared_error(y_train, train_imputed)))\n    mse_valid_lgbm.append(np.sqrt(mean_squared_error(y_valid, valid_imputed)))\n    \n    print(\"Training error:\" + str(np.sqrt(mean_squared_error(y_train, train_imputed))))\n    print(\"Validation error:\" + str(np.sqrt(mean_squared_error(y_valid, valid_imputed))))","metadata":{"execution":{"iopub.status.busy":"2022-06-10T06:03:08.715038Z","iopub.execute_input":"2022-06-10T06:03:08.717412Z","iopub.status.idle":"2022-06-10T06:30:53.565424Z","shell.execute_reply.started":"2022-06-10T06:03:08.717335Z","shell.execute_reply":"2022-06-10T06:30:53.563044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"The average training error is: \" + str(sum(mse_train_lgbm)/len(mse_train_lgbm)))\nprint(\"The average CV error is: \" + str(sum(mse_valid_lgbm)/len(mse_valid_lgbm)))","metadata":{"execution":{"iopub.status.busy":"2022-06-10T06:31:22.959177Z","iopub.execute_input":"2022-06-10T06:31:22.959807Z","iopub.status.idle":"2022-06-10T06:31:22.966775Z","shell.execute_reply.started":"2022-06-10T06:31:22.959745Z","shell.execute_reply":"2022-06-10T06:31:22.965803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = [['Simple_Imputer','0.1578772878904608', '0.15787718194045347'], ['LGBM_Imputer', '0.11630687954281457', '0.11633002301138315' ]]\ndf = pd.DataFrame(data, columns=['Model','Training_Error', 'Cross Validation_Error'])\n \ndf","metadata":{"execution":{"iopub.status.busy":"2022-06-04T13:00:29.62487Z","iopub.execute_input":"2022-06-04T13:00:29.625309Z","iopub.status.idle":"2022-06-04T13:00:29.64001Z","shell.execute_reply.started":"2022-06-04T13:00:29.625272Z","shell.execute_reply":"2022-06-04T13:00:29.638894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Will add and compare more models that have been used in this competition so far.\n\n# Thanks for reading!!","metadata":{}}]}