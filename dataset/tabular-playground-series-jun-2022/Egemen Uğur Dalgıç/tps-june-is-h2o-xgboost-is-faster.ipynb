{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from pathlib import Path\nimport h2o\nfrom h2o.estimators import H2OXGBoostEstimator\nfrom tqdm import tqdm\nimport xgboost as xgb\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nimport numpy as np \nimport pandas as pd \n\nimport warnings\nwarnings.filterwarnings('ignore')\n\ninput_path = Path('/kaggle/input/tabular-playground-series-jun-2022/')\ndataset = pd.read_csv(input_path / 'data.csv')\nsample_submission = pd.read_csv('../input/tabular-playground-series-jun-2022/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2022-06-08T08:08:01.242805Z","iopub.execute_input":"2022-06-08T08:08:01.243203Z","iopub.status.idle":"2022-06-08T08:08:20.587462Z","shell.execute_reply.started":"2022-06-08T08:08:01.243126Z","shell.execute_reply":"2022-06-08T08:08:20.586591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"h2o.init()","metadata":{"execution":{"iopub.status.busy":"2022-06-08T08:08:20.58912Z","iopub.execute_input":"2022-06-08T08:08:20.58962Z","iopub.status.idle":"2022-06-08T08:08:28.865511Z","shell.execute_reply.started":"2022-06-08T08:08:20.589577Z","shell.execute_reply":"2022-06-08T08:08:28.864625Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Goal of This Notebook\n\nXGBoost is a great, robust tool for predicting tabular data. However due to its convergence speed sometimes it is not preferred (at least at first glance). Yet there are a couple of ways to speed up the convergence speed. One of them is to enable using histograms like in the LGBM algorithm.\n\nIterative Imputer is a great way to handle missing data. However, the convergence speed is very low when it is used with any tree algorithms. We could use linear models if the features are linearly correlated but from the Pearson correlations, we see that it is not the case. \n\nOne idea is using Iterative Imputer with XGBoost algorithm while enabling gpu_hist. In this [notebook](https://www.kaggle.com/code/hiro5299834/tps-jun-2022-iterativeimputer-baseline), you can see how it is applied.\n\nAnother idea is using h2o.xgboost instead of the regular one. The claim is that h2o.xgboost is significantly faster both in CPU and GPU (for the full experiment and conclusion you can visit [here](https://sefiks.com/2019/11/07/why-you-should-build-xgboost-models-within-h2o/#:~:text=To%20sum%20up%2C%20h2o%20distribution,for%20a%20large%20data%20set.)).\n\n* \"Besides, training lasts 204 seconds in h2o when GPU is enabled whereas regular XGBoost cannot handle memory if GPU is enabled and this causes the kernel to die.\"\n\n* \"To sum up, h2o distribution is 1.6 times faster than the regular xgboost on CPU. Besides, building a model on a GPU can be run on just h2o for a large data set.\"\n\nRegular XGBoost supports GPU under the tree method *gpu_hist*.\n\nH2O supports GPU under several tree methods. For the large datasets it uses *approx* tree method which creates bins in every iterations. Optionally *hist* tree method can be used.\n\nIn this notebook, I tested the second claim to see whether it is true or not. Hope you'll enjoy it!","metadata":{}},{"cell_type":"markdown","source":"## 1. h2o.XGBoost (tree_method = approx) ","metadata":{}},{"cell_type":"code","source":"dataset_h2o = h2o.H2OFrame(dataset)\n\nparams = {\n    'ntrees': 250,\n    'booster': 'gbtree',\n    'backend': 'gpu',\n    'reg_lambda': 0.5013016642587416,\n    'reg_alpha': 0.48576060322334563,\n    'colsample_bytree': 0.9,\n    'subsample': 1.0,\n    'learn_rate': 0.1,\n    'max_depth': 9,\n    'min_child_weight': 3,\n    'stopping_metric': 'RMSE'}","metadata":{"execution":{"iopub.status.busy":"2022-06-08T08:08:28.86729Z","iopub.execute_input":"2022-06-08T08:08:28.867916Z","iopub.status.idle":"2022-06-08T08:10:17.410068Z","shell.execute_reply.started":"2022-06-08T08:08:28.867876Z","shell.execute_reply":"2022-06-08T08:10:17.40922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\npred_list = {}\n\nfeatures = dataset_h2o.columns[1:]\nfor feat in tqdm(features):\n    if dataset_h2o[feat].isna().any():\n        missing_rows = dataset_h2o[feat].isna()\n        train_data = dataset_h2o.drop([\"row_id\"],axis = 1)[~missing_rows,:]\n        test_data = dataset_h2o.drop([\"row_id\", feat], axis = 1)[missing_rows,:]\n        model = H2OXGBoostEstimator(**params)\n        model.train(y = feat, training_frame = train_data)\n        preds = model.predict(test_data).as_data_frame().values.ravel()\n        pred_list[feat] = preds","metadata":{"execution":{"iopub.status.busy":"2022-06-08T08:10:17.412041Z","iopub.execute_input":"2022-06-08T08:10:17.412512Z","iopub.status.idle":"2022-06-08T09:14:50.118269Z","shell.execute_reply.started":"2022-06-08T08:10:17.412474Z","shell.execute_reply":"2022-06-08T09:14:50.117429Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_h2o = dataset_h2o.as_data_frame()\nfor col,val in pred_list.items():\n    dataset_h2o.loc[dataset_h2o[col].isnull(),col] = val","metadata":{"execution":{"iopub.status.busy":"2022-06-08T09:14:50.119583Z","iopub.execute_input":"2022-06-08T09:14:50.120123Z","iopub.status.idle":"2022-06-08T09:15:31.556414Z","shell.execute_reply.started":"2022-06-08T09:14:50.120085Z","shell.execute_reply":"2022-06-08T09:15:31.555578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"row_cols = sample_submission[\"row-col\"].str.split(\"-\", expand = True)\nrelevant_rows = row_cols[0].astype(np.int32).values\nrelevant_columns = row_cols[1].values\npredictions = []\nfor row, col in zip(relevant_rows,relevant_columns):\n    predictions.append(dataset_h2o.loc[row,col])\nsubmission = pd.DataFrame()\nsubmission['row-col'] = sample_submission[\"row-col\"]\nsubmission[\"value\"] = predictions\nsubmission.to_csv(\"submission.csv\", index = False)\n\n# Score: 0.93","metadata":{"execution":{"iopub.status.busy":"2022-06-08T09:19:57.274599Z","iopub.execute_input":"2022-06-08T09:19:57.27522Z","iopub.status.idle":"2022-06-08T09:21:34.562234Z","shell.execute_reply.started":"2022-06-08T09:19:57.275166Z","shell.execute_reply":"2022-06-08T09:21:34.561393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Regular XGBoost (gpu-hist)","metadata":{}},{"cell_type":"code","source":"dataset_xgb = dataset.copy()","metadata":{"execution":{"iopub.status.busy":"2022-06-08T09:28:02.415376Z","iopub.execute_input":"2022-06-08T09:28:02.415963Z","iopub.status.idle":"2022-06-08T09:28:02.650369Z","shell.execute_reply.started":"2022-06-08T09:28:02.415926Z","shell.execute_reply":"2022-06-08T09:28:02.649528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params = {\n    'n_estimators': 250,\n    'booster': 'gbtree',\n    'reg_lambda': 0.5013016642587416,\n    'reg_alpha': 0.48576060322334563,\n    'colsample_bytree': 0.9,\n    'missing_values': np.nan,\n    'subsample': 1.0,\n    'learn_rate': 0.1,\n    'max_depth': 9,\n    'min_child_weight': 3,\n    'stopping_metric': 'RMSE',\n    'tree_method': 'gpu_hist',\n    'verbosity' : 0 }","metadata":{"execution":{"iopub.status.busy":"2022-06-08T09:39:22.517996Z","iopub.execute_input":"2022-06-08T09:39:22.518365Z","iopub.status.idle":"2022-06-08T09:39:22.524359Z","shell.execute_reply.started":"2022-06-08T09:39:22.518333Z","shell.execute_reply":"2022-06-08T09:39:22.523485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\npred_list = {}\nfor feat in tqdm(features):\n    if dataset_xgb[feat].isnull().any():\n        missing_rows = np.where(dataset_xgb.loc[:,feat].isnull())[0]\n        non_missing_rows = np.where(dataset_xgb.loc[:,feat].notnull())[0]\n        train_data = dataset_xgb.drop([\"row_id\"],axis = 1).iloc[non_missing_rows,:].reset_index(drop = True)\n        test_data = dataset_xgb.drop([\"row_id\", feat], axis = 1).iloc[missing_rows,:].reset_index(drop = True)\n        X = train_data.drop(feat, axis = 1)\n        y = train_data[feat]\n        model = xgb.XGBRegressor(**params)\n        model.fit(X,y)\n        preds = model.predict(test_data)\n        pred_list[feat] = preds","metadata":{"execution":{"iopub.status.busy":"2022-06-08T10:10:44.82304Z","iopub.execute_input":"2022-06-08T10:10:44.823416Z","iopub.status.idle":"2022-06-08T10:28:36.540566Z","shell.execute_reply.started":"2022-06-08T10:10:44.823382Z","shell.execute_reply":"2022-06-08T10:28:36.539891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col,val in pred_list.items():\n    dataset_xgb.loc[dataset_xgb[col].isnull(),col] = val","metadata":{"execution":{"iopub.status.busy":"2022-06-08T10:10:36.496587Z","iopub.status.idle":"2022-06-08T10:10:36.497547Z","shell.execute_reply.started":"2022-06-08T10:10:36.49729Z","shell.execute_reply":"2022-06-08T10:10:36.497316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. h2o.XGBoost (tree_method = hist)","metadata":{}},{"cell_type":"code","source":"dataset_h2o = h2o.H2OFrame(dataset)\n\nparams = {\n    'ntrees': 250,\n    'booster': 'gbtree',\n    'backend': 'gpu',\n    'reg_lambda': 0.5013016642587416,\n    'reg_alpha': 0.48576060322334563,\n    'colsample_bytree': 0.9,\n    'tree_method': 'hist',\n    'subsample': 1.0,\n    'learn_rate': 0.1,\n    'max_depth': 9,\n    'min_child_weight': 3,\n    'stopping_metric': 'RMSE'}","metadata":{"execution":{"iopub.status.busy":"2022-06-08T10:38:21.358592Z","iopub.execute_input":"2022-06-08T10:38:21.359625Z","iopub.status.idle":"2022-06-08T10:39:57.136799Z","shell.execute_reply.started":"2022-06-08T10:38:21.359564Z","shell.execute_reply":"2022-06-08T10:39:57.135861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\npred_list = {}\n\nfeatures = dataset_h2o.columns[1:]\nfor feat in tqdm(features):\n    if dataset_h2o[feat].isna().any():\n        missing_rows = dataset_h2o[feat].isna()\n        train_data = dataset_h2o.drop([\"row_id\"],axis = 1)[~missing_rows,:]\n        test_data = dataset_h2o.drop([\"row_id\", feat], axis = 1)[missing_rows,:]\n        model = H2OXGBoostEstimator(**params)\n        model.train(y = feat, training_frame = train_data)\n        preds = model.predict(test_data).as_data_frame().values.ravel()\n        pred_list[feat] = preds","metadata":{"execution":{"iopub.status.busy":"2022-06-08T10:39:57.138618Z","iopub.execute_input":"2022-06-08T10:39:57.139061Z","iopub.status.idle":"2022-06-08T11:45:53.906628Z","shell.execute_reply.started":"2022-06-08T10:39:57.139008Z","shell.execute_reply":"2022-06-08T11:45:53.904976Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataset Conversion Time","metadata":{}},{"cell_type":"code","source":"#%%time\n#a = dataset_h2o.as_data_frame()","metadata":{"execution":{"iopub.status.busy":"2022-06-08T11:54:21.001673Z","iopub.execute_input":"2022-06-08T11:54:21.002545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Results\n\n* The total run time of h2o.XGBoost is 1.04 and 1.05 hours for approx and hist tree methods respectively whereas regular XGBoost with same hyperparameters took 17 min. There is a big difference between regular XGBoost and the other. \n\n* One thing that I realized during training was converting Pandas DataFrame to H2OFrame takes a long time. Because of that, I suspected that some of the difference might be caused due to converting H2OFrame to Pandas DataFrame so I decided to calculate the total time of this process. The conversion time is approximately 14 seconds. There are 55 columns having missing values so total conversion time was approximately 13 min. The difference between two algorithms can not be explained with conversion time. \n\n## Other Inspirational Kernels\n\n* https://www.kaggle.com/code/cv13j0/tps-jun22-nn-multivariate-feature-imputation/notebook\n* https://www.kaggle.com/code/hiro5299834/tps-jun-2022-iterativeimputer-baseline\n\n## References\n\n* https://sefiks.com/2019/11/07/why-you-should-build-xgboost-models-within-h2o/#:~:text=To%20sum%20up%2C%20h2o%20distribution,for%20a%20large%20data%20set.\n* https://xgboost.readthedocs.io/en/stable/gpu/index.html\n* https://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/xgboost.html\n","metadata":{}},{"cell_type":"markdown","source":"Thanks...","metadata":{}}]}