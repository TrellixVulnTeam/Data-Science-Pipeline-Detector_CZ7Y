{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1>TPS Jun 2022: Missing data imputation</h1>\n\n<h2> Data description </h2>\n\nFor this challenge, we are given (simulated) manufacturing control data that contains missing values due to electronic errors. \n\n<h3>Goal </h3>\n\n* Predict the values of all missing data in this dataset. (Note, while there are continuous and categorical features, only the continuous features have missing values.)\n    \n<h3> Files </h3>\n\n* data.csv - the file includes normalized continuous data and categorical data.\n\n* sample_submission.csv - a sample submission file in the correct format; the row-col indicator corresponds to the row and column of each missing value in data.csv\n\n    \n<h3> Evaluation  </h3>\n\n* Submissions are scored on the root mean squared error (RMSE) ","metadata":{}},{"cell_type":"markdown","source":"# Import libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler\nfrom tqdm import tqdm\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' ## hide tf warnings\nimport tensorflow as tf\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.layers import Dense, Input, InputLayer, Add, BatchNormalization, Dropout\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, LearningRateScheduler, EarlyStopping\n","metadata":{"execution":{"iopub.status.busy":"2022-06-20T07:09:58.071331Z","iopub.execute_input":"2022-06-20T07:09:58.071814Z","iopub.status.idle":"2022-06-20T07:10:04.718434Z","shell.execute_reply.started":"2022-06-20T07:09:58.071769Z","shell.execute_reply":"2022-06-20T07:10:04.717569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading data","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv('../input/tabular-playground-series-jun-2022/data.csv')\nsubmission = pd.read_csv('../input/tabular-playground-series-jun-2022/sample_submission.csv',\n                         index_col='row-col')","metadata":{"execution":{"iopub.status.busy":"2022-06-20T07:10:04.720058Z","iopub.execute_input":"2022-06-20T07:10:04.720785Z","iopub.status.idle":"2022-06-20T07:10:21.259885Z","shell.execute_reply.started":"2022-06-20T07:10:04.720742Z","shell.execute_reply":"2022-06-20T07:10:21.259026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Info","metadata":{}},{"cell_type":"code","source":"data.info()","metadata":{"execution":{"iopub.status.busy":"2022-06-18T21:19:26.765521Z","iopub.execute_input":"2022-06-18T21:19:26.766399Z","iopub.status.idle":"2022-06-18T21:19:26.91255Z","shell.execute_reply.started":"2022-06-18T21:19:26.766361Z","shell.execute_reply":"2022-06-18T21:19:26.910901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### ðŸ”µ We can see only float-type features contain Nan values","metadata":{}},{"cell_type":"markdown","source":"# The percentage of missing values","metadata":{}},{"cell_type":"code","source":"def get_missings(df):\n    labels,values = list(),list()\n    for column in df.columns:\n           if df[column].isnull().sum():\n            labels.append(column)\n            values.append((df[column].isnull().sum() / len(df[column]))*100)\n            missings=pd.DataFrame({'Column':labels,\n                                   'Missing(Percent)':values}).sort_values(by='Missing(Percent)',\n                                                                           ascending=False)\n    return missings\n    \nget_missings(data).head(20)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T07:10:21.261232Z","iopub.execute_input":"2022-06-20T07:10:21.261716Z","iopub.status.idle":"2022-06-20T07:10:21.580721Z","shell.execute_reply.started":"2022-06-20T07:10:21.261661Z","shell.execute_reply":"2022-06-20T07:10:21.579628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### ðŸ”µ The percentage of missing values of features is almost the same\n","metadata":{}},{"cell_type":"markdown","source":"## Make a list of feature names containing Nan","metadata":{}},{"cell_type":"code","source":"features_with_Nan = get_missings(data).Column.to_list()\n\n# Number of features containing Nan values\nprint('There are ',len(features_with_Nan), 'features with Nan values')","metadata":{"execution":{"iopub.status.busy":"2022-06-20T07:10:21.58317Z","iopub.execute_input":"2022-06-20T07:10:21.583689Z","iopub.status.idle":"2022-06-20T07:10:21.881573Z","shell.execute_reply.started":"2022-06-20T07:10:21.583632Z","shell.execute_reply":"2022-06-20T07:10:21.880411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating a function for feature selection\nThis function takes a column name and returns 30 features that are most correlated with it, Later I will use this function to select features to train our model in a large loop, This will reduce the runtime and since we are choosing related features it may improve the score.","metadata":{}},{"cell_type":"code","source":"def high_correlated(col):\n    \n    return data.corrwith(data[col]).abs().sort_values(ascending=False)[1:33].index.to_list()","metadata":{"execution":{"iopub.status.busy":"2022-06-20T07:10:21.883417Z","iopub.execute_input":"2022-06-20T07:10:21.883876Z","iopub.status.idle":"2022-06-20T07:10:21.889159Z","shell.execute_reply.started":"2022-06-20T07:10:21.883827Z","shell.execute_reply":"2022-06-20T07:10:21.888189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modeling","metadata":{}},{"cell_type":"markdown","source":"## Define RMSE loss function","metadata":{}},{"cell_type":"code","source":"def rmse(y_true, y_pred):\n    return K.sqrt(K.mean(K.square(y_pred - y_true)))","metadata":{"execution":{"iopub.status.busy":"2022-06-20T07:10:21.890775Z","iopub.execute_input":"2022-06-20T07:10:21.891447Z","iopub.status.idle":"2022-06-20T07:10:21.9005Z","shell.execute_reply.started":"2022-06-20T07:10:21.891407Z","shell.execute_reply":"2022-06-20T07:10:21.899623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Implementing NN Model\nI will run this function for each column using a loop","metadata":{}},{"cell_type":"code","source":"def nn_model(X_train,y_train,X_val,y_val,X_test):\n    \n    # Random seed\n    tf.random.set_seed(42)\n    \n    # Create a sequential model\n    model= Sequential([\n    tf.keras.layers.Input(shape = X_train.shape[1:]),\n    Dense(512, activation='swish'),\n    BatchNormalization(),\n    Dense(256, activation='swish'),\n    BatchNormalization(),\n    Dense(128, activation='swish'),\n    BatchNormalization(),\n    Dense(64, activation='swish'),\n    BatchNormalization(),\n    Dense(32, activation='swish'),\n    BatchNormalization(),\n    Dense(16, activation='swish'),\n    BatchNormalization(),\n    Dense(1,   activation = 'linear')\n    ])\n    \n    # Compile the model\n    model.compile(\n    loss=rmse,\n    optimizer=Adam(learning_rate = 0.01),\n    metrics=[rmse]\n    )\n        \n    # Define callbacks\n    lr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.5, patience = 3, verbose = 0)\n    es = EarlyStopping(monitor = 'val_loss',patience = 12, verbose = 0, mode = 'min', restore_best_weights = True)\n    callbacks=[lr,es]\n    \n    # Fit the model\n    history = model.fit(X_train, y_train,\n                       epochs=33,\n                       validation_data=(X_val, y_val),\n                       batch_size= 4094,\n                       shuffle = True,\n                       callbacks = callbacks,\n                       verbose=0)\n    \n    return model,history","metadata":{"execution":{"iopub.status.busy":"2022-06-20T07:10:21.902197Z","iopub.execute_input":"2022-06-20T07:10:21.90276Z","iopub.status.idle":"2022-06-20T07:10:21.914484Z","shell.execute_reply.started":"2022-06-20T07:10:21.902719Z","shell.execute_reply":"2022-06-20T07:10:21.913178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Iterate over columns and run the model\n\n* The following outer loop goes only through 55 columns that have missing values.\n* To impute a column only the most correlated features (35 features) with the target column will be selected to train the model .\n* The inner loop is for splitting train data to 4 splits then training model through cross validation and making out-of-fold predictions.\n* The mean of oof predictions would be the final predicted values for current column.\n* We can also save validation loss of each column in a list for later analysis.","metadata":{}},{"cell_type":"code","source":"data_imputed = pd.DataFrame() \nloss_per_feature={}\n\n# Iterate over the columns that contain Nan values\nfor col in tqdm(data[features_with_Nan].columns):\n    \n    predictions=[]\n    validation_loss=[]\n    \n    # Mask to access not_null part of the current column\n    not_null = ~data[col].isnull()\n    \n    # Train dataset (includes non-null part of current column)\n    train = data.loc[not_null]\n    \n    # Test dataset (includes null part of current column)\n    test = data.loc[~not_null]\n    \n    # Feature selection\n    selected_features=[n for n in high_correlated(col) if n not in ['row_id', col]]\n    # Cross validation type\n    kf = KFold(n_splits = 5)\n    \n    # Splitting data to train and validation\n    for fold, (train_idx, val_idx) in enumerate(kf.split(train[selected_features])):\n\n        X_train, X_val = train.iloc[train_idx].drop(col,axis = 1), train.iloc[val_idx].drop(col, axis = 1)\n        y_train, y_val = train.iloc[train_idx][col], train.iloc[val_idx][col]\n        X_test = test.drop(col, axis = 1)\n        \n        # Fillna with the mean\n        X_train, X_val = X_train.fillna(X_train.median()), X_val.fillna(X_val.median())\n        X_test = X_test.fillna(X_test.median())\n        \n        # Standard Scaling \n        scaler = StandardScaler()\n        X_train=scaler.fit_transform(X_train)\n        X_val=scaler.transform(X_val)\n        X_test=scaler.transform(X_test)\n        \n        # Running NN model \n        model = nn_model(X_train,y_train,X_val,y_val,X_test)\n        \n        # Make an out-of-fold prediction\n        y_preds = model[0].predict(X_test)\n        \n        # Add y_preds to a list\n        predictions.append(y_preds)\n        \n        # Save loss for current fold\n        validation_loss.append(model[1].history[\"val_loss\"][-1])\n\n    # Caluculate the mean of oof predictions\n    mean_values = np.array(predictions).mean(axis = 0)\n    \n    # Save mean-loss for current feature\n    loss_per_feature[col] = np.mean(validation_loss)\n    \n    # Specifying column to impute\n    imputed_feature = data[col].copy()\n    \n    # Filling missing values\n    imputed_feature.loc[~not_null] =  mean_values.ravel()\n    \n    # Concatenate imputed columns\n    data_imputed = pd.concat([data_imputed, imputed_feature],axis = 1)\n\n# Replace columns with imputed columns \ndata[features_with_Nan] = data_imputed","metadata":{"execution":{"iopub.status.busy":"2022-06-20T07:11:17.836525Z","iopub.execute_input":"2022-06-20T07:11:17.836949Z","iopub.status.idle":"2022-06-20T07:11:56.225005Z","shell.execute_reply.started":"2022-06-20T07:11:17.836915Z","shell.execute_reply":"2022-06-20T07:11:56.223713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Validation loss per column\n\nKnowing which columns have been imputed with low accuracy, we can look for another solution for them, but not now... maybe in the next update ðŸ¤”","metadata":{}},{"cell_type":"code","source":"loss_df=pd.DataFrame(loss_per_feature,index=['Validation_RMSE']).T.sort_values(by='Validation_RMSE',\n                                                                               ascending=False)\n\nloss_df","metadata":{"execution":{"iopub.status.busy":"2022-06-19T07:40:30.172948Z","iopub.execute_input":"2022-06-19T07:40:30.17355Z","iopub.status.idle":"2022-06-19T07:40:30.463278Z","shell.execute_reply.started":"2022-06-19T07:40:30.17351Z","shell.execute_reply":"2022-06-19T07:40:30.46143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Making submission","metadata":{}},{"cell_type":"code","source":"for i in submission.index: \n    row = int(i.split('-')[0])\n    col = i.split('-')[1]\n    submission.loc[i, 'value'] = data.loc[row, col]\n\nsubmission.to_csv(\"submission.csv\")\nsubmission","metadata":{"execution":{"iopub.status.busy":"2022-06-14T11:11:11.798468Z","iopub.execute_input":"2022-06-14T11:11:11.799288Z","iopub.status.idle":"2022-06-14T11:12:32.672801Z","shell.execute_reply.started":"2022-06-14T11:11:11.799244Z","shell.execute_reply":"2022-06-14T11:12:32.672044Z"},"trusted":true},"execution_count":null,"outputs":[]}]}