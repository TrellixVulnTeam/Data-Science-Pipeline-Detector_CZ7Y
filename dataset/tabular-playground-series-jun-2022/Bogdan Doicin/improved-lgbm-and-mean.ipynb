{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-10T05:17:05.387856Z","iopub.execute_input":"2022-06-10T05:17:05.388272Z","iopub.status.idle":"2022-06-10T05:17:05.396167Z","shell.execute_reply.started":"2022-06-10T05:17:05.388237Z","shell.execute_reply":"2022-06-10T05:17:05.395423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. Beginning, inspiration and improvement\n\nThis notebook is an improvement from another notebook, called: \"Top 3% solution: LGBM+Mean\". It can be found [here](https://www.kaggle.com/code/abdulravoofshaik/top-3-solution-lgbm-mean) so please check it out and give the man a vote, because he deserves it. I surely did.\n\nBasically, he divides the entire dataset into four parts, delimited by the *F_x_y* features. The *F_4_x* features are the most correlated of all, thus it makes sense to focus our imputing on that part, the other three parts being imputed using the *mean* strategy of sklearn's *SimpleImputer* class. The *F_4_x* features were imputed using the *LGBMRegressor* class, with 20k estimators.\n\nThis gave me *inspiration* for an *improvement* to the method. The improvement comes from the fact that *SimpleImputer* offers appaling results for this dataset. How do I know? Well:\n1. I tried the method;\n1. If it were that easy, everybody would have done it.\n\nSo, why not apply the *LGBMRegressor* to the other three parts as well? If the idea offers superior results to the fourth part, why not apply it to the other three?\n\nLet's see what happens.\n\n# 2. Importing the needed libraries.\n\nHere we import other libraries that we need, except for *numpy* and *pandas*, which are imported by default.","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom lightgbm import LGBMRegressor\nfrom sklearn.impute import SimpleImputer\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2022-06-10T05:17:10.963621Z","iopub.execute_input":"2022-06-10T05:17:10.964019Z","iopub.status.idle":"2022-06-10T05:17:12.671668Z","shell.execute_reply.started":"2022-06-10T05:17:10.963988Z","shell.execute_reply":"2022-06-10T05:17:12.670723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Defining imputing function\n\nBelow the imputing function is defined. It will be used to all the imputing that will be done.","metadata":{}},{"cell_type":"code","source":"def imputing(data,data_final,n_estimators=500):\n    for column in tqdm(data.columns):\n        if data[column].isnull().any():\n            missing_list = list(np.where(data[column].isnull())[0])\n            no_missing_list = list(np.where(data[column].isnull() == False)[0])\n            train = data.iloc[no_missing_list,]\n            test = data.iloc[missing_list,]\n            X = train.drop([column], axis=1)\n            y = train[column]\n            X_test = test.drop([column], axis=1)\n            X_cols = X.columns\n            X_test_cols = X_test.columns\n            model = LGBMRegressor(n_estimators=n_estimators, metric='r2',boosting_type='dart')\n            model.fit(X=X, y=y)\n            y_predict = model.predict(X_test)\n            print(model.score(X=X, y=y))\n            data_all = data[column]\n            data_all.iloc[missing_list,] = y_predict\n            data_final = pd.concat([data_final, data_all], axis=1)\n        else:\n            data_final = pd.concat([data_final, data[column]], axis=1)\n    return data_final","metadata":{"execution":{"iopub.status.busy":"2022-06-10T05:32:05.351707Z","iopub.status.idle":"2022-06-10T05:32:05.352456Z","shell.execute_reply.started":"2022-06-10T05:32:05.352263Z","shell.execute_reply":"2022-06-10T05:32:05.35229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The significance of the protoype parameters are:\n1. **data** = the Data Frame that will be imputed;\n1. **data_final** = the Data Frame that stores the values of the imputed columns, including the columns' imputed values;\n1. **n_estimators** = the number of estimators for *LGBMRegressor*. The default value of 500 was chosen arbitrary.\n\n# 4. Reading inputs\n\nHere we read the *.csv* file content, store the column list for further use, then we display different properties of the Data Frame. In the last two lines, we create an empty Data Frame called *data_final*, which will store the values of the imputed columns, as I said above. The *submission* variable will store the final data ready for submission. We read the content of the *sample_submission* file because it's easier to modify that Data Frame than to create a new one.","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv('../input/tabular-playground-series-jun-2022/data.csv')\ndata_columns_list = data.columns.tolist()\nprint(data.head())\nprint(data.describe())\nprint(data.info())\ndata_final = pd.DataFrame()\n\nsubmission = pd.read_csv('../input/tabular-playground-series-jun-2022/sample_submission.csv', index_col='row-col')","metadata":{"execution":{"iopub.status.busy":"2022-06-10T05:17:24.065399Z","iopub.execute_input":"2022-06-10T05:17:24.06612Z","iopub.status.idle":"2022-06-10T05:17:48.286919Z","shell.execute_reply.started":"2022-06-10T05:17:24.066077Z","shell.execute_reply":"2022-06-10T05:17:48.285999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. EDA of features.\n\nThis plot is taken from the notebook that I linked above and presents the EDA of features.","metadata":{}},{"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (25, 20)\nfig, ax = plt.subplots(9, 9)\nfig.text(0.35, 1, 'EDA of Features', {'size': 24})\ni = j = 0\nfor col in data.columns:\n    if col not in ['row_id']:\n        ax[j, i].hist(data[col], bins=100)\n        ax[j, i].set_title(col, {'size': '14', 'weight': 'bold'})\n        if i == 8:\n            i = 0\n            j += 1\n        else:\n            i += 1\nplt.rcParams.update({'axes.facecolor': 'lightgreen'})\nplt.figure(facecolor='red')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-10T05:18:18.147182Z","iopub.execute_input":"2022-06-10T05:18:18.147637Z","iopub.status.idle":"2022-06-10T05:18:40.888181Z","shell.execute_reply.started":"2022-06-10T05:18:18.147601Z","shell.execute_reply":"2022-06-10T05:18:40.887125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. Correlation plots.\n\nAgain, these plots are taken from the notebook that I linked above. It represents the correlations between each *F_x_y* features. As the author said, the *F_4_x* part is the most correlated of all.","metadata":{}},{"cell_type":"code","source":"features = list(data.columns)\nfeatures_1, features_2, features_3, features_4 = [], [], [], []\nF = [[], [], [], [], []]\nfor feature in features:\n    for i in [1, 2, 3, 4]:\n        if feature.split('_')[1] == str(i):\n            F[i].append(feature)\ndf = [[], [], [], [], []]\nfig, axs = plt.subplots(nrows=4, ncols=1, figsize=(18, 30))\nfor i in [1, 2, 3, 4]:\n    df[i] = data[F[i]]\n    corr = df[i].corr()\n    sns.set(font_scale=0.7)\n    sns.heatmap(corr, ax=axs[i - 1], annot=True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-10T05:19:02.577434Z","iopub.execute_input":"2022-06-10T05:19:02.577858Z","iopub.status.idle":"2022-06-10T05:19:18.762769Z","shell.execute_reply.started":"2022-06-10T05:19:02.577824Z","shell.execute_reply":"2022-06-10T05:19:18.7618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 7. Parts estimation\n\nHere we use the imputing function to impute the missing values of the four parts. The only difference is the number of estimators, which can be read from the code.","metadata":{}},{"cell_type":"code","source":"print('First part: ')\ndf[1]=imputing(df[1],data_final,n_estimators=50)\nprint('Second part: ')\ndf[2]=imputing(df[2],data_final,n_estimators=1)\nprint('Third part: ')\ndf[3]=imputing(df[3],data_final,n_estimators=50)\nprint('Fourth part: ')\ndf[4]=imputing(df[4],data_final,n_estimators=20000)","metadata":{"execution":{"iopub.status.busy":"2022-06-10T05:32:23.442831Z","iopub.execute_input":"2022-06-10T05:32:23.443244Z","iopub.status.idle":"2022-06-10T05:33:34.654821Z","shell.execute_reply.started":"2022-06-10T05:32:23.44321Z","shell.execute_reply":"2022-06-10T05:33:34.653173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 8. Assembling the result\n\nHere we assemble the four parts into the final, imputed Data Frame","metadata":{}},{"cell_type":"code","source":"data = pd.concat([df[1], df[2], df[3], df[4]], axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-06-09T13:21:57.530011Z","iopub.execute_input":"2022-06-09T13:21:57.530454Z","iopub.status.idle":"2022-06-09T13:23:50.228722Z","shell.execute_reply.started":"2022-06-09T13:21:57.530414Z","shell.execute_reply":"2022-06-09T13:23:50.22778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 9. Filling the submission Data Frame\n\nThe title is self-describing.","metadata":{}},{"cell_type":"code","source":"for i in tqdm(submission.index):\n    row = int(i.split('-')[0])\n    col = i.split('-')[1]\n    submission.loc[i, 'value'] = data.loc[row, col]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 10. Building the csv file\n\nAgain, the title is self-describing.","metadata":{}},{"cell_type":"code","source":"submission.to_csv('submission.csv')","metadata":{"execution":{"iopub.status.busy":"2022-06-09T13:23:50.229795Z","iopub.execute_input":"2022-06-09T13:23:50.230141Z","iopub.status.idle":"2022-06-09T13:23:53.384259Z","shell.execute_reply.started":"2022-06-09T13:23:50.230113Z","shell.execute_reply":"2022-06-09T13:23:53.383014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 11. Conclusions\n\nAs one can see, this solution yielded better results. The original dataframe had a score of 0.87724, while this one has a score of 0.87456. However, a few aspects need to be mentioned:\n\n1. The running time of this notebook is longer than the running time of the original notebook. The original notebook ended its execution in about 7 hours, this one took about 9 hours.\n1. I took the number of estimators by trial and error. At first, I thought that a higher number of estimators (10k or so) for the first three parts will compensate the poor correlation inside them. Decreasing the number of estimators lead to a score improvement, so I conclude that such a high number actually lead to overfitting. I stopped iterating at these values because I saw that the score improvements are more than marginal (0.0001) to justify about 9 hours of waiting;\n1. Does the score gain worth 2 more computing hours? I don't know. Depends on the final purpose. For me it means 10 places or so, but for others, it may worth it.\n\nThis is my improved solution. Thank you for reading so far. I am looking forward to your comments and questions and don't forget to vote for it, if you liked it.","metadata":{}}]}