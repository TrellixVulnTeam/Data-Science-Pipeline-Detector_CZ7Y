{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-29T13:02:40.424303Z","iopub.execute_input":"2022-06-29T13:02:40.425479Z","iopub.status.idle":"2022-06-29T13:02:40.437725Z","shell.execute_reply.started":"2022-06-29T13:02:40.425414Z","shell.execute_reply":"2022-06-29T13:02:40.435984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)","metadata":{"execution":{"iopub.status.busy":"2022-06-29T13:02:40.439364Z","iopub.execute_input":"2022-06-29T13:02:40.440547Z","iopub.status.idle":"2022-06-29T13:02:40.450187Z","shell.execute_reply.started":"2022-06-29T13:02:40.440495Z","shell.execute_reply":"2022-06-29T13:02:40.448832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/tabular-playground-series-jun-2022/data.csv')","metadata":{"execution":{"iopub.status.busy":"2022-06-29T13:02:40.45241Z","iopub.execute_input":"2022-06-29T13:02:40.453451Z","iopub.status.idle":"2022-06-29T13:03:16.703191Z","shell.execute_reply.started":"2022-06-29T13:02:40.4534Z","shell.execute_reply":"2022-06-29T13:03:16.701533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isna().sum(axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-06-29T13:03:16.704656Z","iopub.execute_input":"2022-06-29T13:03:16.705536Z","iopub.status.idle":"2022-06-29T13:03:16.890134Z","shell.execute_reply.started":"2022-06-29T13:03:16.705493Z","shell.execute_reply":"2022-06-29T13:03:16.889008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"arr = df.fillna(0).corr()","metadata":{"execution":{"iopub.status.busy":"2022-06-29T13:03:16.892671Z","iopub.execute_input":"2022-06-29T13:03:16.893476Z","iopub.status.idle":"2022-06-29T13:03:37.986191Z","shell.execute_reply.started":"2022-06-29T13:03:16.893428Z","shell.execute_reply":"2022-06-29T13:03:37.984061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"high_corr = np.argwhere(arr.values > 0.05)\nhigh_corr = high_corr[high_corr[:,0] != high_corr[:,1],:]\nhigh_corr","metadata":{"execution":{"iopub.status.busy":"2022-06-29T13:03:37.988849Z","iopub.execute_input":"2022-06-29T13:03:37.989536Z","iopub.status.idle":"2022-06-29T13:03:38.008199Z","shell.execute_reply.started":"2022-06-29T13:03:37.98948Z","shell.execute_reply":"2022-06-29T13:03:38.006371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can observe that only elements in F_2 and F_4 are correlated. F_2 is not a target so we focus on F_4.","metadata":{}},{"cell_type":"code","source":"df.isna().sum(axis=1).value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-06-29T13:03:38.01068Z","iopub.execute_input":"2022-06-29T13:03:38.01146Z","iopub.status.idle":"2022-06-29T13:03:38.292048Z","shell.execute_reply.started":"2022-06-29T13:03:38.011416Z","shell.execute_reply":"2022-06-29T13:03:38.290643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are varying number of NA entries in each row. To accomodate this, we take training/validation data from the rows with no NA entries, and develop models with one target variable (and the rest predictor variables). We impute the remaining NA values by mean when making the prediction.","metadata":{}},{"cell_type":"code","source":"df_train = df[df.isna().sum(axis=1) == 0].drop('row_id', axis=1)\ndf_train.shape","metadata":{"execution":{"iopub.status.busy":"2022-06-29T13:03:38.294342Z","iopub.execute_input":"2022-06-29T13:03:38.294911Z","iopub.status.idle":"2022-06-29T13:03:38.926727Z","shell.execute_reply.started":"2022-06-29T13:03:38.294857Z","shell.execute_reply":"2022-06-29T13:03:38.924482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.iloc[:,1:].std()","metadata":{"execution":{"iopub.status.busy":"2022-06-29T13:03:38.928866Z","iopub.execute_input":"2022-06-29T13:03:38.929767Z","iopub.status.idle":"2022-06-29T13:03:40.985712Z","shell.execute_reply.started":"2022-06-29T13:03:38.929705Z","shell.execute_reply":"2022-06-29T13:03:40.984082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#F_1\n#df_train_f1 = df_train.iloc[:,:15]\n#This gives MSE by using the mean.\n#df_train_f1.var(axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-06-28T10:21:15.990407Z","iopub.execute_input":"2022-06-28T10:21:15.990854Z","iopub.status.idle":"2022-06-28T10:21:15.996752Z","shell.execute_reply.started":"2022-06-28T10:21:15.990817Z","shell.execute_reply":"2022-06-28T10:21:15.995036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#from sklearn.model_selection import train_test_split\n#X = df_train_f1.drop('F_1_0', axis=1)\n#y = df_train_f1['F_1_0']\n#X_train, X_test, y_train, y_test = train_test_split(X, y)","metadata":{"execution":{"iopub.status.busy":"2022-06-28T10:21:15.999163Z","iopub.execute_input":"2022-06-28T10:21:15.999538Z","iopub.status.idle":"2022-06-28T10:21:16.01012Z","shell.execute_reply.started":"2022-06-28T10:21:15.999506Z","shell.execute_reply":"2022-06-28T10:21:16.008848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#from sklearn.neighbors import KNeighborsRegressor\n#knn = KNeighborsRegressor()\n#knn.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-06-28T10:21:16.015783Z","iopub.execute_input":"2022-06-28T10:21:16.016642Z","iopub.status.idle":"2022-06-28T10:21:16.024657Z","shell.execute_reply.started":"2022-06-28T10:21:16.016582Z","shell.execute_reply":"2022-06-28T10:21:16.02368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#preds = knn.predict(X_test.iloc[0:1000,:])\n#print(np.mean(np.square(preds - y_test.values[:1000])))","metadata":{"execution":{"iopub.status.busy":"2022-06-28T10:21:16.026256Z","iopub.execute_input":"2022-06-28T10:21:16.026909Z","iopub.status.idle":"2022-06-28T10:21:16.03988Z","shell.execute_reply.started":"2022-06-28T10:21:16.026868Z","shell.execute_reply":"2022-06-28T10:21:16.038662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It seems that knn is even worse than just using mean. Let's try this on other columns.","metadata":{}},{"cell_type":"code","source":"#X = df_train.drop('F_1_0', axis=1)\n#y = df_train['F_1_0']\n#X_train, X_test, y_train, y_test = train_test_split(X, y)\n#knn = KNeighborsRegressor()\n#knn.fit(X_train, y_train)\n#preds = knn.predict(X_test.iloc[0:1000,:])\n#print(np.mean(np.square(preds - y_test.values[:1000])))","metadata":{"execution":{"iopub.status.busy":"2022-06-28T10:21:16.042136Z","iopub.execute_input":"2022-06-28T10:21:16.043269Z","iopub.status.idle":"2022-06-28T10:21:16.053488Z","shell.execute_reply.started":"2022-06-28T10:21:16.043223Z","shell.execute_reply":"2022-06-28T10:21:16.052456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def knn(col, df):\n    X = df.drop(col, axis=1)\n    y = df[col]\n    X_train, X_test, y_train, y_test = train_test_split(X, y)\n    knn = KNeighborsRegressor()\n    knn.fit(X_train, y_train)\n    preds = knn.predict(X_test.iloc[0:1000,:])\n    print(np.mean(np.square(preds - y_test.values[:1000])))","metadata":{"execution":{"iopub.status.busy":"2022-06-28T10:21:16.054812Z","iopub.execute_input":"2022-06-28T10:21:16.055372Z","iopub.status.idle":"2022-06-28T10:21:16.072124Z","shell.execute_reply.started":"2022-06-28T10:21:16.055339Z","shell.execute_reply":"2022-06-28T10:21:16.070813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#for j in range(1,15):\n#    col = 'F_1_'+str(j)\n#    knn(col, df_train)\n#    knn(col, df_train_f1)","metadata":{"execution":{"iopub.status.busy":"2022-06-28T10:21:16.074543Z","iopub.execute_input":"2022-06-28T10:21:16.076195Z","iopub.status.idle":"2022-06-28T10:21:16.089986Z","shell.execute_reply.started":"2022-06-28T10:21:16.076141Z","shell.execute_reply":"2022-06-28T10:21:16.088694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see that this is in general worse than using mean.","metadata":{}},{"cell_type":"code","source":"#F_4\ndf_train_f4 = df_train.iloc[:,15+25+25:]\ndf_train_f4.corr()","metadata":{"execution":{"iopub.status.busy":"2022-06-29T13:03:40.989809Z","iopub.execute_input":"2022-06-29T13:03:40.9903Z","iopub.status.idle":"2022-06-29T13:03:41.310934Z","shell.execute_reply.started":"2022-06-29T13:03:40.99026Z","shell.execute_reply":"2022-06-29T13:03:41.309661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nimport xgboost as xgb\nfrom sklearn.metrics import mean_squared_error","metadata":{"execution":{"iopub.status.busy":"2022-06-29T13:03:41.313364Z","iopub.execute_input":"2022-06-29T13:03:41.314316Z","iopub.status.idle":"2022-06-29T13:03:42.803721Z","shell.execute_reply.started":"2022-06-29T13:03:41.314253Z","shell.execute_reply":"2022-06-29T13:03:42.802051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def model_linear(col, df, num_round, l=0, mask=False):\n    X = df.drop(col, axis=1)\n    y = df[col]\n    X_train, X_test, y_train, y_test = train_test_split(X, y)\n    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, train_size=0.8)\n    param = {'objective': 'reg:squarederror', 'booster': 'gblinear', 'eval_metric': 'rmse', 'lambda': l}\n    dtrain = xgb.DMatrix(data=X_train, label=y_train)\n    dtest = xgb.DMatrix(data=X_val, label=y_val)\n    evallist = [(dtest, 'eval'), (dtrain, 'train')]\n    bst = xgb.train(param, dtrain, num_round, evallist, verbose_eval=False)\n    #Randomly mask some values\n    if mask:\n        masks = np.random.choice(14, X_test.shape[0])\n        temp = X_test.values\n        temp[np.arange(X_test.shape[0]),masks] = 0\n        preds = bst.predict(xgb.DMatrix(pd.DataFrame(temp, columns = X_test.columns)))\n    else:\n        preds = bst.predict(xgb.DMatrix(X_test))\n    print(col, np.sqrt(mean_squared_error(y_test, preds)), df[col].std())\n    #xgb.plot_importance(bst)\n    return bst","metadata":{"execution":{"iopub.status.busy":"2022-06-29T13:03:42.805703Z","iopub.execute_input":"2022-06-29T13:03:42.806163Z","iopub.status.idle":"2022-06-29T13:03:42.820902Z","shell.execute_reply.started":"2022-06-29T13:03:42.80612Z","shell.execute_reply":"2022-06-29T13:03:42.819435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def model_tree(col, df, num_round, l=1, mask=False):\n    X = df.drop(col, axis=1)\n    y = df[col]\n    X_train, X_test, y_train, y_test = train_test_split(X, y)\n    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, train_size=0.8)\n    param = {'objective': 'reg:squarederror', 'booster': 'gbtree', 'eval_metric': 'rmse', 'lambda': l}\n    dtrain = xgb.DMatrix(data=X_train, label=y_train)\n    dtest = xgb.DMatrix(data=X_val, label=y_val)\n    evallist = [(dtest, 'eval'), (dtrain, 'train')]\n    bst = xgb.train(param, dtrain, num_round, evallist, verbose_eval=False)\n    if mask:\n        #Randomly mask some values\n        masks = np.random.choice(14, X_test.shape[0])\n        temp = X_test.values\n        temp[np.arange(X_test.shape[0]),masks] = 0\n        preds = bst.predict(xgb.DMatrix(pd.DataFrame(temp, columns = X_test.columns)))\n    else:\n        preds = bst.predict(xgb.DMatrix(X_test))\n    print(col, np.sqrt(mean_squared_error(y_test, preds)), df[col].std())\n    #xgb.plot_importance(bst)\n    return bst","metadata":{"execution":{"iopub.status.busy":"2022-06-29T13:03:42.823192Z","iopub.execute_input":"2022-06-29T13:03:42.824166Z","iopub.status.idle":"2022-06-29T13:03:42.838888Z","shell.execute_reply.started":"2022-06-29T13:03:42.824112Z","shell.execute_reply":"2022-06-29T13:03:42.837778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We first try the linear method (seeing the relatively high correlations).","metadata":{}},{"cell_type":"code","source":"#for i in df_train_f4.columns:\n#    model_linear(i, df_train_f4, 100)","metadata":{"execution":{"iopub.status.busy":"2022-06-28T10:21:17.299678Z","iopub.execute_input":"2022-06-28T10:21:17.300856Z","iopub.status.idle":"2022-06-28T10:21:17.317292Z","shell.execute_reply.started":"2022-06-28T10:21:17.300781Z","shell.execute_reply":"2022-06-28T10:21:17.315864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This does less well on columns 0, 3, 6, 9. We may have to try a different method. (trees)\nAlso note this method does not do well if there are NA values in the features.","metadata":{}},{"cell_type":"code","source":"#for i in ['F_4_0', 'F_4_3', 'F_4_6', 'F_4_9']:\n#    model_tree(i, df_train_f4, 100)","metadata":{"execution":{"iopub.status.busy":"2022-06-28T10:21:17.318385Z","iopub.execute_input":"2022-06-28T10:21:17.318756Z","iopub.status.idle":"2022-06-28T10:21:17.330368Z","shell.execute_reply.started":"2022-06-28T10:21:17.318713Z","shell.execute_reply":"2022-06-28T10:21:17.329134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This seems to do a slightly better job. We stick with these models.","metadata":{}},{"cell_type":"code","source":"models = []\nfor i in df_train_f4.columns:\n    model = []\n    #Normal model\n    if i in ['F_4_0', 'F_4_3', 'F_4_6', 'F_4_9']:\n        model.append(model_tree(i, df_train_f4, 100, 0.5, False))\n    else:\n        model.append(model_linear(i, df_train_f4, 200, 0, False))\n    #Model for entries with one or more NA values\n    model.append(model_tree(i, df_train_f4, 50, 1, True))\n    models.append(model)","metadata":{"execution":{"iopub.status.busy":"2022-06-29T13:03:42.840592Z","iopub.execute_input":"2022-06-29T13:03:42.841675Z","iopub.status.idle":"2022-06-29T13:14:46.272777Z","shell.execute_reply.started":"2022-06-29T13:03:42.841621Z","shell.execute_reply":"2022-06-29T13:14:46.2714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = []\nfor i in range(15):\n    arr = df[df[df.columns[15+25+25+i+1]].isna()].iloc[:,1+15+25+25:]\n    arr = arr.drop([df.columns[15+25+25+i+1]], axis=1)\n    #Use linear/tree model\n    normal = xgb.DMatrix(arr[arr.isna().sum(axis=1) == 0])\n    more = xgb.DMatrix(arr[arr.isna().sum(axis=1) > 0])\n    preds.append([models[i][0].predict(normal), models[i][1].predict(more)])","metadata":{"execution":{"iopub.status.busy":"2022-06-29T13:14:46.276718Z","iopub.execute_input":"2022-06-29T13:14:46.27715Z","iopub.status.idle":"2022-06-29T13:14:47.322163Z","shell.execute_reply.started":"2022-06-29T13:14:46.277113Z","shell.execute_reply":"2022-06-29T13:14:47.321062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in preds:\n    print(i[0].shape, i[1].shape)","metadata":{"execution":{"iopub.status.busy":"2022-06-29T13:14:47.323981Z","iopub.execute_input":"2022-06-29T13:14:47.324825Z","iopub.status.idle":"2022-06-29T13:14:47.335685Z","shell.execute_reply.started":"2022-06-29T13:14:47.324775Z","shell.execute_reply":"2022-06-29T13:14:47.333567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = pd.read_csv('/kaggle/input/tabular-playground-series-jun-2022/sample_submission.csv')\nmissing_cols = np.argwhere(df.isna().values)\nprint(missing_cols)\nmeans = df.mean(axis=0).values\nprint(means)","metadata":{"execution":{"iopub.status.busy":"2022-06-29T13:14:47.337792Z","iopub.execute_input":"2022-06-29T13:14:47.338628Z","iopub.status.idle":"2022-06-29T13:14:49.512886Z","shell.execute_reply.started":"2022-06-29T13:14:47.338581Z","shell.execute_reply":"2022-06-29T13:14:49.511193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Is there a way to make this faster?\ncounts = []\nfor i in range(15):\n    counts.append([0,0])\nresult = []\nfor i in range(missing_cols.shape[0]):\n    ind = missing_cols[i][1]\n    if ind < 15+25+25+1:\n        result.append(means[ind])\n    else:\n        feats = df.iloc[missing_cols[i][0],:].drop(df.columns[ind]).iloc[15+25+25+1:]\n        na = 1 if feats.isna().sum() > 0 else 0\n        try:\n            result.append(preds[ind-15-25-25-1][na][counts[ind-15-25-25-1][na]])\n        except:\n            print(ind, feats, na, counts)\n        counts[ind-15-25-25-1][na] += 1\nsub['value'] = np.array(result)","metadata":{"execution":{"iopub.status.busy":"2022-06-29T13:15:52.384432Z","iopub.execute_input":"2022-06-29T13:15:52.384919Z","iopub.status.idle":"2022-06-29T13:20:07.303499Z","shell.execute_reply.started":"2022-06-29T13:15:52.384883Z","shell.execute_reply":"2022-06-29T13:20:07.302233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-29T13:20:18.511278Z","iopub.execute_input":"2022-06-29T13:20:18.511805Z","iopub.status.idle":"2022-06-29T13:20:18.526313Z","shell.execute_reply.started":"2022-06-29T13:20:18.511766Z","shell.execute_reply":"2022-06-29T13:20:18.52518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-06-29T13:20:23.500582Z","iopub.execute_input":"2022-06-29T13:20:23.501108Z","iopub.status.idle":"2022-06-29T13:20:27.477171Z","shell.execute_reply.started":"2022-06-29T13:20:23.501053Z","shell.execute_reply":"2022-06-29T13:20:27.475609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}