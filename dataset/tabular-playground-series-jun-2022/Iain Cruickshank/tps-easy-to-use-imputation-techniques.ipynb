{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# set up LightGBM to use GPUs. See https://www.kaggle.com/code/dromosys/gpu-accelerated-lightgbm-full/notebook\n\n!rm -r /opt/conda/lib/python3.6/site-packages/lightgbm\n!git clone --recursive https://github.com/Microsoft/LightGBM","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-28T14:33:08.10875Z","iopub.status.idle":"2022-06-28T14:33:08.109082Z","shell.execute_reply.started":"2022-06-28T14:33:08.108936Z","shell.execute_reply":"2022-06-28T14:33:08.108951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!apt-get install -y -qq libboost-all-dev","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-28T14:33:08.110324Z","iopub.status.idle":"2022-06-28T14:33:08.111484Z","shell.execute_reply.started":"2022-06-28T14:33:08.111246Z","shell.execute_reply":"2022-06-28T14:33:08.111269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%bash\ncd LightGBM\nrm -r build\nmkdir build\ncd build\ncmake -DUSE_GPU=1 -DOpenCL_LIBRARY=/usr/local/cuda/lib64/libOpenCL.so -DOpenCL_INCLUDE_DIR=/usr/local/cuda/include/ ..\nmake -j$(nproc)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-28T14:33:08.112477Z","iopub.status.idle":"2022-06-28T14:33:08.113345Z","shell.execute_reply.started":"2022-06-28T14:33:08.113123Z","shell.execute_reply":"2022-06-28T14:33:08.113147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cd LightGBM/python-package/;python3 setup.py install --precompile","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-28T14:33:08.114489Z","iopub.status.idle":"2022-06-28T14:33:08.114938Z","shell.execute_reply.started":"2022-06-28T14:33:08.114723Z","shell.execute_reply":"2022-06-28T14:33:08.114744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir -p /etc/OpenCL/vendors && echo \"libnvidia-opencl.so.1\" > /etc/OpenCL/vendors/nvidia.icd\n!rm -r LightGBM","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-28T14:33:08.122953Z","iopub.execute_input":"2022-06-28T14:33:08.123495Z","iopub.status.idle":"2022-06-28T14:33:09.454032Z","shell.execute_reply.started":"2022-06-28T14:33:08.123461Z","shell.execute_reply":"2022-06-28T14:33:09.453086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## In this notebook, I will demonstrate various techniques for imputing data. While the results are for the TPS June 2022 challenge, the various functions and concepts can be used on any data science project. Please feel free to use whatever code or idedas help and give this notebook and upvote!","metadata":{}},{"cell_type":"code","source":"import os, random, time\nimport numpy as np\nimport pandas as pd\nfrom pandas.api.types import CategoricalDtype\nfrom scipy.stats import mode\n\nfrom sklearn.preprocessing import OrdinalEncoder, MinMaxScaler, OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer, SimpleImputer, KNNImputer\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, ExtraTreesRegressor, ExtraTreesClassifier\nfrom sklearn.compose import ColumnTransformer\nfrom category_encoders import MEstimateEncoder\nfrom lightgbm import LGBMClassifier, LGBMRegressor, early_stopping\n\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nplt.style.use(\"seaborn-whitegrid\")\nplt.rc(\"figure\", autolayout=True)\nplt.rc(\n    \"axes\",\n    labelweight=\"bold\",\n    labelsize=\"large\",\n    titleweight=\"bold\",\n    titlesize=14,\n    titlepad=10,\n)","metadata":{"execution":{"iopub.status.busy":"2022-06-28T14:33:09.455839Z","iopub.execute_input":"2022-06-28T14:33:09.456686Z","iopub.status.idle":"2022-06-28T14:33:13.911637Z","shell.execute_reply.started":"2022-06-28T14:33:09.456644Z","shell.execute_reply":"2022-06-28T14:33:13.910857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. Exploratory Data Analysis\n- look at the data set, and in particular, what data types are present and what gaps exist\n- get counts of missing data\n- look for any patterns in the missing data (see [here](https://www.kaggle.com/code/abdulravoofshaik/top-3-solution-lgbm-mean/notebook?scriptVersionId=97501106) for code and ideas used in this analysis)","metadata":{"papermill":{"duration":0.02091,"end_time":"2022-04-24T16:44:22.141877","exception":false,"start_time":"2022-04-24T16:44:22.120967","status":"completed"},"tags":[]}},{"cell_type":"code","source":"df = pd.read_csv(\"../input/tabular-playground-series-jun-2022/data.csv\", index_col='row_id')","metadata":{"papermill":{"duration":5.607504,"end_time":"2022-04-24T16:44:27.769593","exception":false,"start_time":"2022-04-24T16:44:22.162089","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-28T14:33:13.912791Z","iopub.execute_input":"2022-06-28T14:33:13.914906Z","iopub.status.idle":"2022-06-28T14:33:29.95164Z","shell.execute_reply.started":"2022-06-28T14:33:13.914869Z","shell.execute_reply":"2022-06-28T14:33:29.950778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"papermill":{"duration":0.050641,"end_time":"2022-04-24T16:44:27.842607","exception":false,"start_time":"2022-04-24T16:44:27.791966","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-28T14:33:29.953789Z","iopub.execute_input":"2022-06-28T14:33:29.954188Z","iopub.status.idle":"2022-06-28T14:33:29.986944Z","shell.execute_reply.started":"2022-06-28T14:33:29.95415Z","shell.execute_reply":"2022-06-28T14:33:29.98626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20,8))\nnulls = df.isnull().sum()\nsns.barplot(x= nulls.index, y=nulls.values)\nplt.title(\"Counts of Missing Values per Column\")","metadata":{"execution":{"iopub.status.busy":"2022-06-28T14:33:29.987842Z","iopub.execute_input":"2022-06-28T14:33:29.988214Z","iopub.status.idle":"2022-06-28T14:33:31.226853Z","shell.execute_reply.started":"2022-06-28T14:33:29.98818Z","shell.execute_reply":"2022-06-28T14:33:31.225946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20,8))\nsns.heatmap(df.isnull(), yticklabels=False, cbar=False)\nplt.title(\"Locations of Null Values in the Data\")","metadata":{"execution":{"iopub.status.busy":"2022-06-28T14:33:31.228424Z","iopub.execute_input":"2022-06-28T14:33:31.2288Z","iopub.status.idle":"2022-06-28T14:34:34.655705Z","shell.execute_reply.started":"2022-06-28T14:33:31.228764Z","shell.execute_reply":"2022-06-28T14:34:34.654929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# look at counts of missing values for records\n\nrecord_nulls = df.isnull().sum(axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-06-28T14:34:34.657094Z","iopub.execute_input":"2022-06-28T14:34:34.658165Z","iopub.status.idle":"2022-06-28T14:34:34.811228Z","shell.execute_reply.started":"2022-06-28T14:34:34.658098Z","shell.execute_reply":"2022-06-28T14:34:34.810326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.unique(record_nulls, return_counts=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-28T14:34:34.812474Z","iopub.execute_input":"2022-06-28T14:34:34.812932Z","iopub.status.idle":"2022-06-28T14:34:34.847919Z","shell.execute_reply.started":"2022-06-28T14:34:34.812895Z","shell.execute_reply":"2022-06-28T14:34:34.846974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# look at just those in 'F_4' category\n\nf_4_df = df[df.columns[df.columns.isin([i for i in list(df.columns) if i.split('_')[1] in ['4']])]]\nf_4_record_nulls = f_4_df.isnull().sum(axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-06-28T14:34:34.849304Z","iopub.execute_input":"2022-06-28T14:34:34.849751Z","iopub.status.idle":"2022-06-28T14:34:34.903965Z","shell.execute_reply.started":"2022-06-28T14:34:34.849715Z","shell.execute_reply":"2022-06-28T14:34:34.90322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.unique(f_4_record_nulls, return_counts=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-28T14:34:34.907173Z","iopub.execute_input":"2022-06-28T14:34:34.907475Z","iopub.status.idle":"2022-06-28T14:34:34.936812Z","shell.execute_reply.started":"2022-06-28T14:34:34.907451Z","shell.execute_reply":"2022-06-28T14:34:34.935973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- We have about 20-18% missing values within each column, expcet for the 'F_2' columns\n- Each of variables appear to be numeric, so model-based imputation will be regression based\n- The columns with 'F_4' variable type appear to have some correlations, as do te 'F_2' columns","metadata":{"papermill":{"duration":0.266248,"end_time":"2022-04-24T16:45:22.752945","exception":false,"start_time":"2022-04-24T16:45:22.486697","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# 2. Specify Helper Functions\n- function for making sure categorical columns are labeled as such\n- function for encoding categorical columns as a label encoder, with a function for reversing the encoding","metadata":{"papermill":{"duration":0.25343,"end_time":"2022-04-24T16:45:23.264654","exception":false,"start_time":"2022-04-24T16:45:23.011224","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def specify_categoricals(df):\n    # Nominal categories\n    for name in df.select_dtypes(\"object\"):\n        df[name] = df[name].astype(\"category\")\n        # Add a None category for missing values\n        if \"None\" not in df[name].cat.categories:\n            df[name].cat.add_categories(\"None\", inplace=True)\n    return df","metadata":{"papermill":{"duration":0.260681,"end_time":"2022-04-24T16:45:25.851697","exception":false,"start_time":"2022-04-24T16:45:25.591016","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-28T14:34:34.937961Z","iopub.execute_input":"2022-06-28T14:34:34.938406Z","iopub.status.idle":"2022-06-28T14:34:34.943996Z","shell.execute_reply.started":"2022-06-28T14:34:34.93837Z","shell.execute_reply":"2022-06-28T14:34:34.942987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def label_encode(df):\n    X = df.copy()\n    X_cat = X.select_dtypes([\"category\"])\n    columns = X_cat.columns\n    enc = OrdinalEncoder()\n    X_cat = enc.fit_transform(X_cat)\n    X[columns] = X_cat\n    return X, enc, columns","metadata":{"papermill":{"duration":0.26982,"end_time":"2022-04-24T16:45:24.823501","exception":false,"start_time":"2022-04-24T16:45:24.553681","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-28T14:34:34.945496Z","iopub.execute_input":"2022-06-28T14:34:34.946129Z","iopub.status.idle":"2022-06-28T14:34:34.956957Z","shell.execute_reply.started":"2022-06-28T14:34:34.946083Z","shell.execute_reply":"2022-06-28T14:34:34.956054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Specify Different Imputers\n- Base imputer using some measure like mean or median for numerical, and mode for categorical\n- kNN imputer\n- Imputation by machine learning model (in this case, LightGBM)","metadata":{}},{"cell_type":"code","source":"def simple_impute(df):\n    '''\n    Impute the numerical columns by the median value for each column and\n    impute the categorical columns by the most frequent, or mode, for each column\n    Note: one can easily switch in different imputers for each of the data types to something like kNN or iterative\n    '''\n    \n    # Impute missing values for numerical data\n    # imp_num = IterativeImputer(estimator=ExtraTreesRegressor(), initial_strategy='median', max_iter=20)\n    imp_num = SimpleImputer(strategy='mean')\n    numerical_df = df.select_dtypes(\"number\")\n    numerical_df = pd.DataFrame(data=imp_num.fit_transform(numerical_df), index=numerical_df.index, columns =numerical_df.columns)\n    \n    if df.select_dtypes(\"category\").shape[1] >0:\n        # Imput missing values for categorical data\n        # imp_cat = IterativeImputer(estimator=ExtraTreesClassifier(), initial_strategy='most_frequent', max_iter=20)\n        imp_cat = SimpleImputer(strategy='most_frequent')\n        categorical_df = df.select_dtypes(\"category\")\n        enc = OrdinalEncoder()\n        categorical_df = pd.DataFrame(data=enc.fit_transform(categorical_df), columns=categorical_df.columns)\n        categorical_imputations = enc.inverse_transform(imp_cat.fit_transform(categorical_df))\n        categorical_df = pd.DataFrame(data=categorical_imputations, index=categorical_df.index, columns =categorical_df.columns, dtype=\"category\")\n        return categorical_df.join(numerical_df).reindex(columns= df.columns)\n    else:\n        return numerical_df","metadata":{"papermill":{"duration":0.271085,"end_time":"2022-04-24T16:45:23.789939","exception":false,"start_time":"2022-04-24T16:45:23.518854","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-28T14:34:34.958154Z","iopub.execute_input":"2022-06-28T14:34:34.959064Z","iopub.status.idle":"2022-06-28T14:34:34.969934Z","shell.execute_reply.started":"2022-06-28T14:34:34.959012Z","shell.execute_reply":"2022-06-28T14:34:34.969023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def knn_impute(df):\n    '''\n    Impute the numerical columns by the k-Nearest Neighbors for each column and\n    impute the categorical columns by the most frequent, or mode, for each column.\n    Note: For the numerical impute, one-hot encode the categorical variables first.\n    Then, for the categorical variables just use the categorical variables\n    '''\n    \n    numerical_df = df.select_dtypes(\"number\")\n    if df.select_dtypes(\"category\").shape[1] >0:\n        categorical_df = df.select_dtypes(\"category\")\n        enc = OneHotEncoder(sparse=False)\n        one_hot_df = pd.DataFrame(data=enc.fit_transform(categorical_df), index=categorical_df.index, columns =enc.get_feature_names_out(categorical_df.columns))\n\n    # Begin with imputing the numerical data by kNN first\n    scaler = MinMaxScaler()\n    numerical_df = pd.DataFrame(data=scaler.fit_transform(numerical_df), columns=numerical_df.columns)\n    if df.select_dtypes(\"category\").shape[1] >0:\n        numerical_df = numerical_df.join(one_hot_df) # add in categorical features, if they exist\n    numerical_imputer = KNNImputer(n_neighbors=5)\n    numerical_imputations = scaler.inverse_transform(numerical_imputer.fit_transform(numerical_df)) # impute the data and return it back to original scales\n    if df.select_dtypes(\"category\").shape[1] >0:\n        numerical_df = pd.DataFrame(data = numerical_imputations[:,:-one_hot_df.shape[1]], index = numerical_df.index, columns =numerical_df.columns[:-one_hot_df.shape[1]])\n    else:\n        numerical_df = pd.DataFrame(data = numerical_imputations, index = numerical_df.index, columns =numerical_df.columns)\n        \n    # Now impute the categorical variables\n    if df.select_dtypes(\"category\").shape[1] >0:\n        # Imput missing values for categorical data\n        # imp_cat = IterativeImputer(estimator=ExtraTreesClassifier(), initial_strategy='most_frequent', max_iter=20)\n        imp_cat = SimpleImputer(strategy='most_frequent')\n        categorical_df = df.select_dtypes(\"category\")\n        enc = OrdinalEncoder()\n        categorical_df = pd.DataFrame(data=enc.fit_transform(categorical_df), columns=categorical_df.columns)\n        categorical_imputations = enc.inverse_transform(imp_cat.fit_transform(categorical_df))\n        categorical_df = pd.DataFrame(data= categorical_imputations, index=categorical_df.index, columns =categorical_df.columns, dtype=\"category\")\n        return categorical_df.join(numerical_df).reindex(columns= df.columns)\n    else:\n        return numerical_df","metadata":{"execution":{"iopub.status.busy":"2022-06-28T14:34:34.972829Z","iopub.execute_input":"2022-06-28T14:34:34.973557Z","iopub.status.idle":"2022-06-28T14:34:34.989869Z","shell.execute_reply.started":"2022-06-28T14:34:34.973524Z","shell.execute_reply":"2022-06-28T14:34:34.989042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def ML_impute(df, params = {}):\n    '''\n    Impute missing values by treating the imputational as a machine learning problem. For numerical\n    columns, we can treat the problem as a regression problem, and for categorical, a classification problem.\n    For this method, we'll iterate through all of the columns with one column being the target variable\n    and the others as being predictor variables\n    '''\n    \n    df = df.copy()\n    \n    # label encode categorical variables\n    columns = df.columns.to_list()\n    cat_cols = df.select_dtypes(\"category\").columns.to_list()\n    enc = OrdinalEncoder()\n    df[cat_cols] = enc.fit_transform(df[cat_cols])\n    \n    # Randomized column selection\n    for i in random.sample(range(len(df.columns)), len(df.columns)):\n    \n    # Starting with most null values to least\n    # for i in np.argsort(-df.isnull().sum().values):\n        column = columns[i]\n        # Check to make sure there are null values that need to be imputed\n        if not df[column].isnull().any():\n            continue\n        \n        print(\"Imputing Column: {}\".format(column))\n            \n        # Create train, test, and validation data using the null values of the column of interest\n        X_train = df.loc[df[column].notnull()]\n        y_train = X_train.pop(column)\n        \n        X_test = df.loc[df[column].isnull()]\n        _ = X_test.pop(column)\n        \n        # If we have more data, we use more estimators for the imputation model\n        n_estimators = min(5000, int(len(X_train) / 10))\n\n        if column in cat_cols:\n            model = LGBMClassifier(**params, n_estimators=n_estimators, device='gpu')\n        else:\n            model = LGBMRegressor(**params, n_estimators=n_estimators, device='gpu')\n        \n        model.fit(X_train, y_train)\n        print(\"Score of Column {} is {}\".format(column, model.score(X_train,y_train))) \n        preds =model.predict(X_test)\n        m = df[column].isna()\n        df.loc[m, column]  = preds.flatten()\n        \n    # Recode the categorical variables to their original values\n    if len(cat_cols) >0:\n        df[cat_cols] = enc.inverse_transform(df[cat_cols])\n        \n    return df","metadata":{"execution":{"iopub.status.busy":"2022-06-28T14:34:34.993378Z","iopub.execute_input":"2022-06-28T14:34:34.993969Z","iopub.status.idle":"2022-06-28T14:34:35.01007Z","shell.execute_reply.started":"2022-06-28T14:34:34.993915Z","shell.execute_reply":"2022-06-28T14:34:35.009291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def ensemble_impute(df, num_runs=5, params = {}):\n    '''\n    Since the order of columns can mattter in a model-based, iterative imputation, with this function\n    we create an ensemble of model based imputations - with different column orderings - to get more\n    robust imputation results\n    '''\n    \n    final_df = df.copy()\n    original_df = df.copy()\n    \n    #encode the categorical variables\n    cat_cols = df.select_dtypes(\"category\").columns.to_list()\n    enc = OrdinalEncoder()\n    df[cat_cols] = enc.fit_transform(df[cat_cols])\n    \n    imputed_dfs = []\n    for run in range(num_runs):\n        start_time = time.time()\n        df = ML_impute(original_df, params)\n        imputed_dfs.append(df)\n        print(\"--------- Run {} Complete ----------\".format(run))\n        print(\"---- Run {} Complete in time {:.3f} -----\".format(run, time.time()-start_time))\n    \n    # Use mode across the categorical columns and recode back to original variables formats\n    if len(cat_cols) >0:\n        final_df[cat_cols] = np.squeeze(mode(np.array([df[cat_cols].values for df in imputed_dfs]), axis=0)[0], axis=0)\n        final_df[cat_cols] = enc.inverse_transform(final_df[cat_cols])\n        \n    #Use mean across the numerical columns\n    final_df[final_df.columns[~final_df.columns.isin(cat_cols)]] = np.mean(np.array([df[df.columns[~df.columns.isin(cat_cols)]].values for df in imputed_dfs]), axis=0)\n\n    return final_df","metadata":{"execution":{"iopub.status.busy":"2022-06-28T14:34:35.011306Z","iopub.execute_input":"2022-06-28T14:34:35.011966Z","iopub.status.idle":"2022-06-28T14:34:35.023578Z","shell.execute_reply.started":"2022-06-28T14:34:35.011929Z","shell.execute_reply":"2022-06-28T14:34:35.022699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Now, lets do some data imputations\n- Do any preprocessesing of the dataframes, to identify different column dtypes\n- specify a function for saving out the imputed data\n- Based on previous notebooks, we will do two imputations\n    - simple imputation for columns in the 'F_1' and 'F_3' blocks\n    - model-based imputation for columns in the 'F_4' block","metadata":{"papermill":{"duration":0.248054,"end_time":"2022-04-24T16:45:26.345911","exception":false,"start_time":"2022-04-24T16:45:26.097857","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def make_submission(df, results):\n\n    #df is the original df\n    #results is the df of the imputation with column names\n    #file str is the file name (minus the .csv)\n    \n    df_num = df.select_dtypes(include='number')\n    df_num_col = df_num.columns\n\n    submission_df = pd.DataFrame(columns=['row-col', 'value'])\n    sub_dict = {}\n    d_index = 0\n    for col in df.columns:    \n        null_index = df[df[col].isnull()].index.tolist()\n        for i in null_index:\n            cell_id = '-'.join([str(i), col])\n            value = results[col][i]\n            sub_dict[d_index] = {'row-col': cell_id, 'value': value}\n            d_index+=1\n\n    submission_df = pd.DataFrame.from_dict(sub_dict, orient='index')\n\n    return submission_df","metadata":{"papermill":{"duration":0.266132,"end_time":"2022-04-24T16:56:24.378","exception":false,"start_time":"2022-04-24T16:56:24.111868","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-28T14:34:35.024999Z","iopub.execute_input":"2022-06-28T14:34:35.025631Z","iopub.status.idle":"2022-06-28T14:34:35.034495Z","shell.execute_reply.started":"2022-06-28T14:34:35.025591Z","shell.execute_reply":"2022-06-28T14:34:35.033745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = specify_categoricals(df)","metadata":{"papermill":{"duration":0.898686,"end_time":"2022-04-24T16:45:27.699548","exception":false,"start_time":"2022-04-24T16:45:26.800862","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-28T14:34:35.037021Z","iopub.execute_input":"2022-06-28T14:34:35.037616Z","iopub.status.idle":"2022-06-28T14:34:35.047484Z","shell.execute_reply.started":"2022-06-28T14:34:35.037578Z","shell.execute_reply":"2022-06-28T14:34:35.046665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4(a) Using each of the Different Imputers","metadata":{}},{"cell_type":"markdown","source":"### Simple Impute\n------------------------\nSimple Impute results: ~1.41708","metadata":{}},{"cell_type":"code","source":"simple_impute_df = simple_impute(df)","metadata":{"execution":{"iopub.status.busy":"2022-06-28T14:34:35.04873Z","iopub.execute_input":"2022-06-28T14:34:35.049656Z","iopub.status.idle":"2022-06-28T14:34:37.107871Z","shell.execute_reply.started":"2022-06-28T14:34:35.049618Z","shell.execute_reply":"2022-06-28T14:34:37.107023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"simple_df_results = make_submission(df, simple_impute_df)\nsimple_df_results.to_csv(\"simple_impute_submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-06-28T14:34:37.109168Z","iopub.execute_input":"2022-06-28T14:34:37.109617Z","iopub.status.idle":"2022-06-28T14:35:02.774997Z","shell.execute_reply.started":"2022-06-28T14:34:37.109579Z","shell.execute_reply":"2022-06-28T14:35:02.773905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### k-NN Impute\n-----------------------\n***note, this one can take a long time to run***. You can also easily craft and IterativeImputer from this with Sci-kit learn's imputers.","metadata":{}},{"cell_type":"code","source":"#knn_impute_df = knn_impute(df)","metadata":{"execution":{"iopub.status.busy":"2022-06-28T14:35:02.776311Z","iopub.execute_input":"2022-06-28T14:35:02.77688Z","iopub.status.idle":"2022-06-28T14:35:02.782532Z","shell.execute_reply.started":"2022-06-28T14:35:02.776843Z","shell.execute_reply":"2022-06-28T14:35:02.781057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#knn_impute_df_results = make_submission(df, knn_impute_df)\n#knn_impute_df_results.to_csv(\"knn_impute_submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-06-28T14:35:02.784474Z","iopub.execute_input":"2022-06-28T14:35:02.785572Z","iopub.status.idle":"2022-06-28T14:35:02.791704Z","shell.execute_reply.started":"2022-06-28T14:35:02.7855Z","shell.execute_reply":"2022-06-28T14:35:02.790627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Imputation by Machine Learning Model\n------------------------------\nLighGBM Boost Impute with standard settings (~0.91645)","metadata":{}},{"cell_type":"code","source":"#boost_impute_df = ML_impute(df)","metadata":{"execution":{"iopub.status.busy":"2022-06-28T14:35:02.793364Z","iopub.execute_input":"2022-06-28T14:35:02.794337Z","iopub.status.idle":"2022-06-28T14:35:02.801319Z","shell.execute_reply.started":"2022-06-28T14:35:02.794295Z","shell.execute_reply":"2022-06-28T14:35:02.800278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#boost_df_results = make_submission(df, boost_impute_df)\n#boost_df_results.to_csv(\"basic_lgbm_impute_submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-06-28T14:35:02.803034Z","iopub.execute_input":"2022-06-28T14:35:02.804187Z","iopub.status.idle":"2022-06-28T14:35:02.809317Z","shell.execute_reply.started":"2022-06-28T14:35:02.804147Z","shell.execute_reply":"2022-06-28T14:35:02.808168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4(b) Using a combination of imputers on different columns\n\nWe will use this code to create the submission the usae of a combination of imputers was inspired by [this notebook](https://www.kaggle.com/code/djustin/mean-and-lgb/notebook?scriptVersionId=97493929), [this notebook](https://www.kaggle.com/code/mirenaborisova/tps-june-22-simpleimputer-lgbm-lb-0-87540/notebook?scriptVersionId=97680406), and [this notebook](https://www.kaggle.com/code/abdulravoofshaik/top-3-solution-lgbm-mean/notebook?scriptVersionId=97501106). Please consider giving their notebooks a look (and an upvote).","metadata":{}},{"cell_type":"code","source":"# Create a working df with the imputations and just carry forward the 'F_2' block, since no imputation is needed there\nF_2_cols = [i for i in list(df.columns) if i.split('_')[1] in ['2']]\nworking_df = df[df.columns[df.columns.isin(F_2_cols)]]","metadata":{"execution":{"iopub.status.busy":"2022-06-28T14:35:02.810931Z","iopub.execute_input":"2022-06-28T14:35:02.811995Z","iopub.status.idle":"2022-06-28T14:35:02.895504Z","shell.execute_reply.started":"2022-06-28T14:35:02.811953Z","shell.execute_reply":"2022-06-28T14:35:02.894607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Simple Impute Step\nSimple imputations for those columns in F_1 and F_3 blocks","metadata":{}},{"cell_type":"code","source":"F_1_and_3_cols = [i for i in list(df.columns) if i.split('_')[1] in ['1', '3']]\nsimple_impute_df = simple_impute(df[df.columns[df.columns.isin(F_1_and_3_cols)]])","metadata":{"papermill":{"duration":654.446863,"end_time":"2022-04-24T16:56:22.404626","exception":false,"start_time":"2022-04-24T16:45:27.957763","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-28T14:35:02.896851Z","iopub.execute_input":"2022-06-28T14:35:02.897433Z","iopub.status.idle":"2022-06-28T14:35:03.902852Z","shell.execute_reply.started":"2022-06-28T14:35:02.897397Z","shell.execute_reply":"2022-06-28T14:35:03.902021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Add in the simple imputations from just the F_1 and F_3 blocks to the working df\nworking_df = working_df.merge(simple_impute_df, left_index=True, right_index=True)","metadata":{"papermill":{"duration":0.468943,"end_time":"2022-04-24T16:56:23.32944","exception":false,"start_time":"2022-04-24T16:56:22.860497","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-28T14:35:03.904244Z","iopub.execute_input":"2022-06-28T14:35:03.904657Z","iopub.status.idle":"2022-06-28T14:35:04.106574Z","shell.execute_reply.started":"2022-06-28T14:35:03.904619Z","shell.execute_reply":"2022-06-28T14:35:04.105722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model-based (Ensemble) Impute Step\nModel-based imputations for columns in the F_4 block","metadata":{"papermill":{"duration":0.256622,"end_time":"2022-04-24T16:57:03.17781","exception":false,"start_time":"2022-04-24T16:57:02.921188","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Add in the F_4 columns to the working_df that already has imputations for all of the other blocks, so that we can use previous imputations in determing F_4's imputations\n\nF_4_cols = [i for i in list(df.columns) if i.split('_')[1] in ['4']]\nworking_df = working_df.merge(df[df.columns[df.columns.isin(F_4_cols)]], left_index=True, right_index=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-28T14:35:04.110472Z","iopub.execute_input":"2022-06-28T14:35:04.110772Z","iopub.status.idle":"2022-06-28T14:35:04.389864Z","shell.execute_reply.started":"2022-06-28T14:35:04.110746Z","shell.execute_reply":"2022-06-28T14:35:04.389009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Add in the number of missing cells per record as a feature\n\nworking_df['total_na_counts'] = df.isnull().sum(axis=1)\nf_4_df = df[df.columns[df.columns.isin([i for i in list(df.columns) if i.split('_')[1] in ['4']])]]\nworking_df['F_4_na_counts']= f_4_df.isnull().sum(axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-06-28T14:35:04.391329Z","iopub.execute_input":"2022-06-28T14:35:04.391708Z","iopub.status.idle":"2022-06-28T14:35:04.637954Z","shell.execute_reply.started":"2022-06-28T14:35:04.391671Z","shell.execute_reply":"2022-06-28T14:35:04.63714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgb_params = {\n    'lambda_l1': 1,\n    'lambda_l2': 1,\n    'bagging_freq': 1,\n    'bagging_fraction': 0.7,\n    'verbose':-1\n}","metadata":{"execution":{"iopub.status.busy":"2022-06-28T14:35:04.639498Z","iopub.execute_input":"2022-06-28T14:35:04.639885Z","iopub.status.idle":"2022-06-28T14:35:04.64473Z","shell.execute_reply.started":"2022-06-28T14:35:04.639848Z","shell.execute_reply":"2022-06-28T14:35:04.644017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"working_df = ensemble_impute(working_df, params=lgb_params)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_df = partial_working_df[partial_working_df.columns[partial_working_df.columns.isin(df.columns)]]","metadata":{"execution":{"iopub.status.busy":"2022-06-28T14:10:05.034133Z","iopub.status.idle":"2022-06-28T14:10:05.034853Z","shell.execute_reply.started":"2022-06-28T14:10:05.034466Z","shell.execute_reply":"2022-06-28T14:10:05.034541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Finally, submit results of the imputation\n\ndf_results = make_submission(df, final_df)\ndf_results.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-06-28T14:10:05.036925Z","iopub.status.idle":"2022-06-28T14:10:05.037793Z","shell.execute_reply.started":"2022-06-28T14:10:05.037443Z","shell.execute_reply":"2022-06-28T14:10:05.037474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you can see form the results, a ML-model based imputer tends to give the best results for this type of missing data situation. However, also including simple imputation for some of the columns will give better results. There are some future avenues to investigate based on this:\n- changing the order in which the ML-model imputer sees the columns\n- changing the ML model in the imputer, or its settings\n- trying some feature engineering or imputation across the 'F' block of features\n\n***Please don't hestitate to post any comments or questions, and consider upvoting***","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}