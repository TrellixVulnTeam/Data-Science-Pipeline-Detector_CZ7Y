{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# TPS-JUN22, Neuronal Net, Multivariate Feature Imputation üßä\n\n\n\n## <span style=\"color:orange\"> A Tensorflow Model Using Keras API</span>.\nHello Kaggle! \n\nIn this Notebook I developed a Neuronal Network Imputer usinf the Multivariate Feature Imputation technique and examples of this techniques applied to GDBT Models; The idea it's quite simple but powerrful.\nBasically we train a model in all the features - 1 in wich the minus 1 feature is the column we are trying to impute or estimate, we complete an iteration for all the features availables that are missing values. \n\n<img src='https://drive.google.com/uc?id=1x03IDYdr80DYOFsdKY54vpGif7eLsjPT' width = 500>\n\n\n\n\nMultivariate imputer that estimates each feature from all the others. A strategy for imputing missing values by modeling each feature with missing values as a function of other features in a round-robin fashion.\n\nA more sophisticated approach is to use the IterativeImputer class, which models each feature with missing values as a function of other features, and uses that estimate for imputation. It does so in an iterated round-robin fashion: at each step, a feature column is designated as output y and the other feature columns are treated as inputs X. A regressor is fit on (X, y) for known y. Then, the regressor is used to predict the missing values of y. This is done for each feature in an iterative fashion, and then is repeated for max_iter imputation rounds. The results of the final imputation round are returned.\n\n\n### ‚ú® Strategy...\nBuild a NN Model that train in the N-1 Features and predict on Feature -1, the training takes a lot of time...\n\n\n### üìÖ Updates...\n**06/04/2022**\n* Developed the initial Notebook using XGBoost\n* Adapated the XGBoost Function to train a NN Model\n* Generated Submission for the Leaderboard \n\n**06/05/2022**\n* Maybe try some features\n\n\n### üñ•Ô∏è Credits...\nI based my initial work on this Notebooks, thanks to the Authours;\n* https://www.kaggle.com/code/reymaster/0-93002-iterative-imputer-baseline\n* https://www.kaggle.com/code/djustin/lgb-cpu/data\n* ...\n\n\n### üíΩ Data Description\nFor this challenge, you are given (simulated) manufacturing control data that contains missing values due to electronic errors. Your task is to predict the values of all missing data in this dataset. (Note, while there are continuous and categorical features, only the continuous features have missing values.)\n\nHere's a notebook that you can use to get started.\n\nGood luck!\n\n**Files**\n* **data.csv** - the file includes normalized continuous data and categorical data; your task is to predict the values of the missing data.\n* **sample_submission.csv** - a sample submission file in the correct format; the row-col indicator corresponds to the row and column of each missing value in data.csv\n\n\n### üìñ Table of Content...\n* **Importing Python Libraries for the NotebookNotebook Configuration**\n* **Notebook Configuration**\n* **Loading the Datasets into a Pandas DataFrame**\n* **Multivariate Feature Imputation GDBT**\n* Model Parameters\n* Model Training XGBoost or LBGM Loop, For all the Features\n* Model Training XGBoost or LBGM Loop, For all the Features    \n* **Multivariare Feature Imputation Using Tensorflow / Keras**\n* Defining Model NN Libraries\n* Defining a NN Model Function; Simple Network Architecture\n* Defining the Model Fit Function\n* Training the Model in a Cross Validation Loop for all the Features\n* **Model Submission**\n","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# Importing Python Libraries for the Notebook\nPlaceholder, explanations of this sections...","metadata":{}},{"cell_type":"code","source":"%%time\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Loading more libraries for the model...\nfrom pathlib import Path\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nfrom tqdm import tqdm\nimport datetime","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# Notebook Configuration\nPlaceholder, explanations of this sections...","metadata":{}},{"cell_type":"code","source":"%%time\n# I like to disable my Notebook Warnings.\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Notebook Configuration...\n\n# Amount of data we want to load into the Model...\nDATA_ROWS = None\n# Dataframe, the amount of rows and cols to visualize...\nNROWS = 50\nNCOLS = 15\n# Main data location path...\nBASE_PATH = '...'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Configure notebook display settings to only use 2 decimal places, tables look nicer.\npd.options.display.float_format = '{:,.5f}'.format\npd.set_option('display.max_columns', NCOLS) \npd.set_option('display.max_rows', NROWS)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# Loading the Datasets into a Pandas DataFrame\nPlaceholder, explanations of this sections...","metadata":{}},{"cell_type":"code","source":"%%time\n# Load the CSV information into a Pandas DataFrame...\ninput_path = Path('/kaggle/input/tabular-playground-series-jun-2022/')\n\ndataset = pd.read_csv(input_path / 'data.csv')\nsubmission = pd.read_csv(input_path / 'sample_submission.csv', index_col='row-col')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ndataset.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# Feature Engineering\nPlaceholder, explanations of this sections...","metadata":{}},{"cell_type":"code","source":"%%time\ndataset['num_nans'] = dataset.isnull().sum(axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ndataset.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ndataset.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Multivariate Feature Imputation GDBT\nPlaceholder, explanations of this sections...","metadata":{}},{"cell_type":"markdown","source":"## Model Parameters\nPlaceholder, explanations of this sections...","metadata":{}},{"cell_type":"code","source":"%%script false --no-raise-error\n%%time\nSEED = 22\nESTIMATORS = 128\n\nparams = {'n_estimators': ESTIMATORS,\n          'random_state': SEED,\n          'tree_method' : 'gpu_hist'}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"## Model Training XGBoost or LBGM Loop, For all the Features\nPlaceholder, explanations of this sections...","metadata":{}},{"cell_type":"code","source":"%%script false --no-raise-error\n%%time \n# Creates a loop, train in all -1 columns use -1 column as a target...\n\nfeatures = dataset.columns.to_list()\ndata_completed = pd.DataFrame()\n\n\nfor feat in tqdm(features):\n    if dataset[feat].isnull().any():\n        \n        # Identify missing values...\n        missing_values = list(np.where(dataset[feat].isnull())[0])\n        not_missing_values = list(np.where(dataset[feat].isnull() == False)[0])\n        \n        trn_data = dataset.iloc[not_missing_values,]\n        tst_data = dataset.iloc[missing_values,]\n        \n        X = trn_data.drop([feat,'row_id'],axis = 1)\n        y = trn_data[feat]\n        X_test = tst_data.drop([feat,'row_id'],axis = 1)\n            \n        model = XGBRegressor(**params)\n        #model = LGBMRegressor(n_estimators = 10,metric = 'rmse')\n        model.fit(X,y)\n        \n        y_predict = model.predict(X_test)\n        imputed_data = dataset[feat]\n        imputed_data.iloc[missing_values,] = y_predict\n        \n        data_completed = pd.concat([data_completed, imputed_data],axis = 1)\n        \n    else:\n        data_completed = pd.concat([data_completed, dataset[feat]],axis = 1)\n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%script false --no-raise-error\n%%time\nfor i in tqdm(submission.index):\n    row = int(i.split('-')[0])\n    col = i.split('-')[1]\n    submission.loc[i, 'value'] = dataset.loc[row, col]\n\nsubmission.to_csv(\"gdbt_submission.csv\")\nsubmission","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# Multivariare Feature Imputation Using Tensorflow / Keras\nPlaceholder, explanations of this sections...","metadata":{}},{"cell_type":"markdown","source":"## Defining Model NN Libraries\nPlaceholder, explanations of this sections...","metadata":{}},{"cell_type":"code","source":"%%time\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, LearningRateScheduler, EarlyStopping\nfrom tensorflow.keras.layers import Dense, Input, InputLayer, Add, BatchNormalization, Dropout, Concatenate\n\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_absolute_error\n\nimport random\nimport math","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"## Defining Model NN Parameters\nPlaceholder, explanations of this sections...","metadata":{}},{"cell_type":"code","source":"%%time\n# Defining model parameters...\nBATCH_SIZE = 2048\nEPOCHS     = 32 # For Testing Purposes, Increase for Final Submission...\nVERBOSE    = 0 \nNUM_FOLDS  = 3  # For Testing Purposes, Increase for Final Submission...","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"## Defining a NN Model Function; Simple Network Architecture\nPlaceholder, explanations of this sections...","metadata":{}},{"cell_type":"code","source":"%%time\ndef nn_model_one():\n    \n    '''\n    Function to define the Neuronal Network architecture...\n    '''\n    \n    L2 = 65e-6\n    activation_func = 'swish'\n    inputs = Input(shape = (len(features) - 2)) # Remove the Id and the durrent feature that's imputed\n    \n    x = Dense(128, \n              #use_bias  = True, \n              kernel_regularizer = tf.keras.regularizers.l2(L2), \n              activation = activation_func)(inputs)\n    \n    x = BatchNormalization()(x)\n    \n    x = Dense(64, \n          #use_bias  = True, \n          kernel_regularizer = tf.keras.regularizers.l2(L2), \n          activation = activation_func)(x)\n    \n    x = BatchNormalization()(x)\n    \n    x = Dense(32, \n              #use_bias  = True, \n              kernel_regularizer = tf.keras.regularizers.l2(L2), \n              activation = activation_func)(x)\n    \n    x = BatchNormalization()(x)\n\n    x = Dense(1 , \n              #use_bias  = True, \n              #kernel_regularizer = tf.keras.regularizers.l2(L2),\n              activation = 'linear')(x)\n    \n    model = Model(inputs, x)\n    \n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ndef nn_model_two():\n    \n    '''\n    Function to define the Neuronal Network architecture...\n    '''\n    \n    L2 = 65e-6\n    activation_func = 'swish'\n    inputs = Input(shape = (len(features) - 2)) # Remove the Id and the durrent feature that's imputed\n    \n    x0 = Dense(256, kernel_regularizer = tf.keras.regularizers.l2(L2), activation = activation_func)(inputs)\n    x1 = Dense(64,  kernel_regularizer = tf.keras.regularizers.l2(L2), activation = activation_func)(x0)\n    x1 = Dense(64,  kernel_regularizer = tf.keras.regularizers.l2(L2), activation = activation_func)(x1)\n    x1 = Concatenate()([x1, x0])\n    x1 = Dropout(0.1)(x1)\n    x1 = BatchNormalization()(x1)\n    x1 = Dense(16, kernel_regularizer = tf.keras.regularizers.l2(L2), activation = activation_func)(x1)\n    x1 = Dense(1,  kernel_regularizer = tf.keras.regularizers.l2(4e-4), activation = 'sigmoid')(x1)\n    model = Model(inputs, x1)\n    \n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Baseline Network = [64,32,16,1] With 16 Epochs...  *\n# Test Two Network = [128,64,32,1] With 16 Epochs... **\n# Test Three Network = [128,64,32,1] With 32 Epochs... ???","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"## Defining the Model Fit Function\nPlaceholder, explanations of this sections...","metadata":{}},{"cell_type":"code","source":" %%time\n# Defining model training function...\ndef fit_model(X_train, y_train, X_val, y_val, X_test, run = 0):\n    '''\n    '''\n    lr_start = 0.01\n    start_time = datetime.datetime.now()\n    \n    scaler = StandardScaler()\n    X_train = scaler.fit_transform(X_train)\n\n    epochs = EPOCHS    \n    lr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.7, patience = 4, verbose = VERBOSE)\n    es = EarlyStopping(monitor = 'val_loss',patience = 12, verbose = 1, mode = 'min', restore_best_weights = True)\n    callbacks = [lr, es]\n    \n    model = nn_model_two() # Selection of the Second Model.\n    optimizer_func = tf.keras.optimizers.Adam(learning_rate = lr_start)\n    loss_func = tf.keras.losses.MeanSquaredError()\n    \n    model.compile(optimizer = optimizer_func, loss = loss_func)\n    \n    X_val = scaler.transform(X_val)\n    validation_data = (X_val, y_val)\n    \n    history = model.fit(X_train, \n                        y_train, \n                        validation_data = validation_data, \n                        epochs          = epochs,\n                        verbose         = VERBOSE,\n                        batch_size      = BATCH_SIZE,\n                        shuffle         = True,\n                        callbacks       = callbacks\n                       )\n    \n    history_list.append(history.history)\n    #print(f'Training Loss:{history_list[-1][\"loss\"][-1]:.5f}')\n    callbacks, es, lr, history = None, None, None, None\n    \n    \n    y_val_pred = model.predict(X_val, batch_size = BATCH_SIZE, verbose = VERBOSE)\n    score = mean_absolute_error(y_val, y_val_pred)\n    \n    #print(f'Fold {run}.{fold} | {str(datetime.datetime.now() - start_time)[-12:-7]}'\n    #      f'| MSE: {score:.5f}')\n    \n    score_list.append(score)\n    \n    tst_data_scaled = scaler.transform(X_test)\n    tst_pred = model.predict(tst_data_scaled)\n    predictions.append(tst_pred)\n    \n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"## Training the Model in a Cross Validation Loop for all the Features\nPlaceholder, explanations of this sections...","metadata":{}},{"cell_type":"code","source":"%%time\n# Create empty lists to store NN training metrics and predictions\n\n\n# Creates a loop, train in all -1 columns use -1 column as a target...\nfeatures = dataset.columns.to_list()\ndata_completed = pd.DataFrame()\n\n\nfor feat in tqdm(features):\n    # Create empty lists to store NN training metrics and predictions\n    history_list = []\n    score_list   = []\n    predictions  = []\n    \n    if dataset[feat].isnull().any():\n        #print('Training Model For: ',feat)\n        \n        # Identify missing values...\n        missing_values = list(np.where(dataset[feat].isnull())[0])\n        not_missing_values = list(np.where(dataset[feat].isnull() == False)[0])\n        \n        \n        trn_data = dataset.iloc[not_missing_values,]\n        tst_data = dataset.iloc[missing_values,]\n        \n        # Define kfolds for training purposes...\n        kf = KFold(n_splits = NUM_FOLDS)\n\n        for fold, (trn_idx, val_idx) in enumerate(kf.split(trn_data)):\n            #print(f' Training fold: {fold}...')\n            X_train, X_val = trn_data.iloc[trn_idx].drop([feat,'row_id'],axis = 1), trn_data.iloc[val_idx].drop([feat,'row_id'], axis = 1)\n            y_train, y_val = trn_data.iloc[trn_idx][feat], trn_data.iloc[val_idx][feat]\n            X_test = tst_data.drop([feat,'row_id'], axis = 1)\n            \n            X_train, X_val = X_train.fillna(X_train.mean()), X_val.fillna(X_val.mean())\n            X_test = X_test.fillna(X_test.mean())\n            \n            fit_model(X_train, y_train, X_val, y_val, X_test)\n        \n        mean_values = np.array(predictions).mean(axis = 0)\n        imputed_data = dataset[feat]\n        imputed_data.iloc[missing_values] = mean_values.ravel()\n        data_completed = pd.concat([data_completed, imputed_data],axis = 1)\n    \n    else:\n        data_completed = pd.concat([data_completed, dataset[feat]],axis = 1)        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# Model Submission\nPlaceholder, explanations of this sections...","metadata":{}},{"cell_type":"code","source":"nn_submission = pd.read_csv(input_path / 'sample_submission.csv', index_col='row-col')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ndata_completed.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nfor i in tqdm(nn_submission.index):\n    row = int(i.split('-')[0])\n    col = i.split('-')[1]\n    nn_submission.loc[i, 'value'] = data_completed.loc[row, col]\n\nnn_submission.to_csv(\"nn_submission.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nn_submission","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}}]}