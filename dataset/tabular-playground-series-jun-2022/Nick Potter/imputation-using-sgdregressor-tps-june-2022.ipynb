{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Imputation of missing data using an SGD regression approach","metadata":{}},{"cell_type":"code","source":"# Library imports\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd \nimport random as rd\nimport seaborn as sns\n\n# Read in the competition data\nsample = pd.read_csv('/kaggle/input/tabular-playground-series-jun-2022/sample_submission.csv')\ndata = pd.read_csv('/kaggle/input/tabular-playground-series-jun-2022/data.csv', index_col='row_id')\n\nTTESTS = True\n# Take a sample of the data just for notebook development purposes:\n#MAXROWS = 100000\nMAXROWS = data.shape[0]\n#TCOLS = [f'F_1_{x}' for x in range(3)] + \\\n#    [f'F_2_{x}' for x in range(4)] + \\\n#    [f'F_3_{x}' for x in range(5)] + \\\n#    [f'F_4_{x}' for x in range(6)]\nTCOLS = data.columns\ndata = data.head(MAXROWS)\ndata = data[TCOLS]","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-27T23:07:12.210553Z","iopub.execute_input":"2022-06-27T23:07:12.211416Z","iopub.status.idle":"2022-06-27T23:07:30.804098Z","shell.execute_reply.started":"2022-06-27T23:07:12.211304Z","shell.execute_reply":"2022-06-27T23:07:30.802866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Introduction \n\nThis month's TPS is slightly different from the previous TPS competitions in that there is no target variable. Instead we are asked to impute (infill) missing data. I tried mean (and median) infilling, using `IterativeImputer`, and a clustering idea, which didn't work as well as I had hoped. My next approach to this problem is to treat each column as a target (predictor) variable, fit a model to the rest of the columns, and use this model to impute the missing values. The main difficulty with applying this methodology is that we are unable to select an appropriate model as there is no training data available. As such, the first step is to create a training dataset using complete rows of the provided data.\n\n* Identify the missing mechanism\n* Create a training dataset\n* Fit models on each column of the training dataset\n* Use model parameters for each column to impute data \n\n## Missing mechanism\n\nIt is generally accepted that there are three different types of missingness:\n\n* Missing Completely at Random (MCAR): the occurrence of missing values is basically a random process. In particular, missingness in a particular column doesn't depend on values in other columns or in the column itself.\n* Missing at Random (MAR): the occurrence of missing values depends on values in a different column.\n* Missing Not at Random (MNAR): the occurrence of missing values depends on (unseen) values in the column itself.\n\nIdentifying the type of missingness is important as it will guide the choice of imputation method. Unfortunately it is not really easy to determine the type of missingness. In particular, it can be hard, if not impossible, to identify MNAR as we need to know the values of the missing data (or at least have some meta-understanding of why missing data is present). Differentiating between MCAR and MAR is a bit easier, but it is like identifying outliers - different statistical tests gives us evidence but there is generally no conclusive way to identify whether data is MAR or MCAR.\n\n# Creating a dataset to test ideas\n\n## Feature engineering\n\nJust create summary columns of standard deviation of values across `Fx` variables. Also `F_4` variables do better after a power transform to make them more normal.","metadata":{}},{"cell_type":"code","source":"\nF1cols = [x for x in data.columns if 'F_1' in x]\nF2cols = [x for x in data.columns if 'F_2' in x]\nF3cols = [x for x in data.columns if 'F_3' in x]\nF4cols = [x for x in data.columns if 'F_4' in x]\ndata['F1_sd'] = data.loc[:,F1cols].std(axis=1)\ndata['F2_sd'] = data.loc[:,F2cols].std(axis=1)\ndata['F3_sd'] = data.loc[:,F3cols].std(axis=1)\ndata['F4_sd'] = data.loc[:,F4cols].std(axis=1)\n\nfrom sklearn.preprocessing import PowerTransformer\n\nyjpt = PowerTransformer(method='yeo-johnson', standardize=False)\ndata[F4cols] = yjpt.fit_transform(data[F4cols])","metadata":{"execution":{"iopub.status.busy":"2022-06-27T23:07:30.806695Z","iopub.execute_input":"2022-06-27T23:07:30.807464Z","iopub.status.idle":"2022-06-27T23:07:30.826221Z","shell.execute_reply.started":"2022-06-27T23:07:30.807428Z","shell.execute_reply":"2022-06-27T23:07:30.825118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n\n## Identifying the missingness mechanism (MAR vs. MCAR)\n\n\nNormally we have a training dataset provided, which can be split using one of the [scikit-learn cross-validation splitters](https://scikit-learn.org/stable/modules/cross_validation.html). Unfortunately we weren't given a training data set this month, so let's make one using the subset of the complete data. First, we need to work out what kind of missingness we have.\n\nAs discussed above, there are generally three kinds of missingness considered: MNAR, MAR and MCAR. It is difficult to identify MNAR, but here we consider whether the data is MAR or MCAR. Two tests are applied: relating the occurrence of missingness to values in other columns, and relating missingness to the occurrence of missingness in other columns.\n\n### Is missing data related to values in other columns?\n\nIterating through each column with missing data, we can check if missingness is related to values in all the other columns in turn by performing a t-test for the difference in means of the variable for missing data and non-missing data in the response column. \n\n","metadata":{}},{"cell_type":"code","source":"%%time \nif TTESTS:\n\n    from statsmodels.stats.weightstats import ttest_ind\n    results = pd.DataFrame(columns = ['missing_column','regressor_column','p_value'])\n    results_missing_indicator = pd.DataFrame(columns = ['missing_column','regressor_column','p_value'])\n    row = 0\n    for j, y_column in enumerate(data.columns):\n        #print('-----')\n        #print(f'Column {y_column}')\n        dlc = data.loc[:,y_column].isna().value_counts()\n        #print(dlc)\n        if len(dlc) == 1:\n            #print(f'Column {y_column} has no (or all) missing data, skipping')\n            for x_column in data.columns:\n                results.loc[row] = [y_column, x_column, 1]\n                results_missing_indicator.loc[row] = [y_column, x_column, 1]\n                row +=1 \n\n            continue\n        Bvals = [x for x in data.groupby(data.loc[:,y_column].isna())[y_column].groups.keys()]\n        x0=[list(x) for x in data.groupby(data.loc[:,y_column].isna())[y_column].groups.values()]\n\n        for i, x_column in enumerate(data.columns):\n            if sum(data[x_column].isna()) == 0:\n                results.loc[row] = [y_column, x_column, 1]\n                results_missing_indicator.loc[row] = [y_column, x_column, 1]\n                row +=1 \n                continue\n            if x_column == y_column:\n                continue\n            ttest_res = ttest_ind(data.loc[x0[0], x_column].dropna(), data.loc[x0[1], x_column].dropna())\n            results.loc[row] = [y_column, x_column, ttest_res[1]]\n            ttest_res_missing = ttest_ind(data.loc[x0[0], x_column].isna(), data.loc[x0[1], x_column].isna())\n            if ttest_res_missing[1] == np.nan:\n                raise Exception\n            #print('          ',ttest_res_missing[1])\n            #print('xxx',ttest_res_missing)\n            results_missing_indicator.loc[row] = [y_column, x_column, ttest_res_missing[1]]\n            row += 1\n            #print(f'Missing data column: {y_column} against {x_column}, t-test p-value = {ttest_res[1]:.3f}')\n    results_missing_indicator\n\n\n    plt.figure(figsize=(9,9))\n    sns.heatmap(results.pivot(index='missing_column', columns='regressor_column', values='p_value'))\n\n    results","metadata":{"execution":{"iopub.status.busy":"2022-06-27T23:07:30.829956Z","iopub.execute_input":"2022-06-27T23:07:30.830294Z","iopub.status.idle":"2022-06-27T23:08:09.481805Z","shell.execute_reply.started":"2022-06-27T23:07:30.830264Z","shell.execute_reply":"2022-06-27T23:08:09.480545Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Another way missing data could be related to other columns is if data in the response column is more (or less) likely to be missing if data in another column is missing. We can check this using a t-test for the difference in missingness (say as an indicator variable) in each other column.","metadata":{}},{"cell_type":"code","source":"if TTESTS:\n    sns.heatmap(results_missing_indicator.pivot(index='missing_column', columns='regressor_column', values='p_value'))\n    results_missing_indicator","metadata":{"execution":{"iopub.status.busy":"2022-06-27T23:08:09.48429Z","iopub.execute_input":"2022-06-27T23:08:09.484606Z","iopub.status.idle":"2022-06-27T23:08:09.890526Z","shell.execute_reply.started":"2022-06-27T23:08:09.48458Z","shell.execute_reply":"2022-06-27T23:08:09.888111Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Based on these graphs, there doesn't appear to be any pattern in the missingness. [eduus710](https://www.kaggle.com/eduus710) came to a similar conclusion with a more complete [missingness analysis](https://www.kaggle.com/code/eduus710/tps-jun2022-how-random-are-the-nans). So, to create a dataset for method testing, we can create missing values randomly (and independently) in each column based on the proportions of missingness from the full dataset.\n\n### Caveats\n\n1. I have not considered interactions between variables here. If missingness is related to the interaction of two variables, this won't come up in this test.\n2. The middle variables are categorical (ordinal) variables and this methodology doesn't check this either. For example, if a particular column is more likely to be missing if, say variable `F_2_1` is equal to 2, a t-test will not necessarily pick this up.\n\n## Proportion of missing values in each column\n\nLet's have a look at the proportion of missing values in each column. We can use this information to make the missing columns in the testing data similar to the problem data.","metadata":{}},{"cell_type":"code","source":"\n%%time \nplt.figure(figsize=(18,6))\nmissing_pcts = [x/data.shape[0] for x in np.sum(data.isna())]\nplt.bar(list(data.columns), missing_pcts)\n#plt.hist(missing_pcts)\nnzero_missing_pcts = [x for x in missing_pcts if x > 0]\n# Get mean and sd for use in generating number of missing rows for each column:\nnorm_params = (np.mean(nzero_missing_pcts), \n               np.sqrt(np.var(nzero_missing_pcts)))\nax = plt.gca()\n_=ax.set(ylabel='Proportion of values missing',\n         xlabel='Column')\n_=plt.xticks(rotation=90)\n","metadata":{"execution":{"iopub.status.busy":"2022-06-27T23:08:09.892085Z","iopub.execute_input":"2022-06-27T23:08:09.892401Z","iopub.status.idle":"2022-06-27T23:08:10.795835Z","shell.execute_reply.started":"2022-06-27T23:08:09.892374Z","shell.execute_reply":"2022-06-27T23:08:10.794691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's put together the testing dataset. The dataframe `incomplete_data` will be like the `data` dataset from the competition, and `missing` is like `sample_submission`, i.e. with one row per missing value. `complete_data` contains all rows of `data` with no missing values, and we have the missing values themselves in the `missing` data frame in the `answers` column.","metadata":{}},{"cell_type":"code","source":"%%time \n\nmissing_cols = np.sum(data.isna(),0)\nmissing_col_ixes = np.nonzero(list(missing_cols>0))[0]\nincomplete_rows = np.sum(data.isna(), 1)\ncomplete_row_ixes = np.nonzero(list(incomplete_rows==0))[0]\n\ncomplete_data = data.iloc[complete_row_ixes,:]\ncomplete_data.reset_index(drop=True, inplace=True)\ncomplete_data.index.name = 'row_id'\n#complete_data\nincomplete_data = complete_data.copy()\nncomplete_rows,_ = complete_data.shape\n\nrd.seed(1)\nmissing = pd.DataFrame(columns = ['row','col','answer'])\nrow = 0\nfor col_ix in missing_col_ixes:\n    nmissing_col = int(np.round(rd.normalvariate(*norm_params) * len(complete_row_ixes)))\n    missing_ixes_col = [int(x) for x in rd.sample(range(ncomplete_rows), nmissing_col)]\n    missing = pd.concat([missing,\n                         pd.DataFrame(np.column_stack((missing_ixes_col, [col_ix,] * nmissing_col, [np.nan,] * nmissing_col)),\n                                      columns=['row','col','answer'])])\n    #print(f'Column {complete_data.columns[col_ix]}')\n    for j in range(nmissing_col):\n        x = complete_data.iloc[missing_ixes_col[j], col_ix]\n        #print(x)\n        missing.iloc[row + j,2] = x\n        incomplete_data.iloc[missing_ixes_col[j], col_ix] = np.nan\n    row += nmissing_col\n\n\nmissing = missing.astype({'row': int, 'col': int, 'answer': float})\nmissing['row-col'] = [f'{int(x[\"row\"])}-{complete_data.columns[int(x[\"col\"])]}' for i,x in missing.iterrows()]\nmissing.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-27T23:08:10.797378Z","iopub.execute_input":"2022-06-27T23:08:10.797784Z","iopub.status.idle":"2022-06-27T23:08:10.981399Z","shell.execute_reply.started":"2022-06-27T23:08:10.797751Z","shell.execute_reply":"2022-06-27T23:08:10.980197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Save the data for possible use elsewhere:","metadata":{}},{"cell_type":"code","source":"missing.to_csv('missing.csv', index=False)\nincomplete_data.to_csv('incomplete_data.csv', index=False)\ncomplete_data.to_csv('complete_data.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T23:08:10.982655Z","iopub.execute_input":"2022-06-27T23:08:10.983565Z","iopub.status.idle":"2022-06-27T23:08:11.040261Z","shell.execute_reply.started":"2022-06-27T23:08:10.983521Z","shell.execute_reply":"2022-06-27T23:08:11.038965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model development and model selection\n\nNow that we have a training dataset with an appropriate (hopefully!) missingness structure, models can be developed for the data by using the constructed training data. We iterate through each column separately, so let's suppose we have fixed a response column. The data will have missingness randomly distributed through each column:","metadata":{}},{"cell_type":"code","source":"# I adapted these diagrams from https://scikit-learn.org/stable/auto_examples/model_selection/plot_cv_indices.html\ncmap_data = plt.cm.Paired\nfig, ax = plt.subplots()\nncols = 5\nnpts = 100\nthresh = 0.85\ngroups = np.reshape((np.random.rand(ncols*npts) < thresh)*1, (npts,ncols))\nfor col in range(ncols):\n    _=ax.scatter(\n        range(len(groups[:,col])),\n        [col] * len(groups[:,col]),\n        c=groups[:,col],\n        marker=\"_\",\n        lw=15,\n        cmap=cmap_data,\n    )\n_=ax.set(\n    ylim=[-1, 5],\n    yticks=range(5),\n    yticklabels=['Response (missing col)'] + [f'Variable {x}' for x in [4,3,2,1]],\n    xlabel=\"Row index\",\n)\n\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-27T23:08:11.042149Z","iopub.execute_input":"2022-06-27T23:08:11.0425Z","iopub.status.idle":"2022-06-27T23:08:11.188258Z","shell.execute_reply.started":"2022-06-27T23:08:11.042466Z","shell.execute_reply":"2022-06-27T23:08:11.187001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A train/test split on this data can then be set up based on missingness in the response column:","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots()\norder = np.argsort(groups[:,0])[::-1]\ngroups_sorted = groups[order, :]\nfor col in range(ncols):\n    _=ax.scatter(\n        range(len(groups_sorted[:,col])),\n        [col] * len(groups_sorted[:,col]),\n        c=groups_sorted[:,col],\n        marker=\"_\",\n        lw=15,\n        cmap=cmap_data,\n    )\n_=ax.set(\n    ylim=[-1.5, 5],\n    yticks=range(5),\n    yticklabels=['Response (missing col)'] + [f'Variable {x}' for x in [4,3,2,1]],\n    xlabel=\"Row index\",\n)\n\nobs_prop_missing = sum(groups_sorted[:,0])/len(groups_sorted[:,0])\n_=ax.plot([-1,-1],[-1.25,4.15],color='black')\n_=ax.plot([obs_prop_missing*npts-.75,]*2,[-1.25,4.15],color='black')\n_=ax.plot([npts,]*2,[-1.25,4.15],color='black')\n_=ax.text(30,-1.255, 'Training\\n data', fontsize=14)\n_=ax.text(obs_prop_missing*npts + 2,-1.255, 'Test\\ndata', fontsize=14)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-27T23:08:11.190045Z","iopub.execute_input":"2022-06-27T23:08:11.191332Z","iopub.status.idle":"2022-06-27T23:08:11.389939Z","shell.execute_reply.started":"2022-06-27T23:08:11.191276Z","shell.execute_reply":"2022-06-27T23:08:11.389155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As can be seen, we have missing data in the other columns throughout, but no missing data in the training data split of our manufactured dataset. The test data split has all missing data for the response column, but of course we know the correct values for this as the `answer` column in the `missing` dataframe.\n\nMy approach is to fit multiple linear regressions to each response column separately, which seems reasonable given the [more or less normal distribution](https://www.kaggle.com/code/matthewszhang/tps-june-interactive-eda-sklearn-imputer) of all continuous variables. This approach could prove problematic for the discrete (ordinal) variables, `F_2_xx`. The main issue here is that we don't want to fit linear regressions to all of the other columns as we will end up with an overfit model. Some kind of variable selection is needed, such as stepwise regression variable selection. Of course, nobody does stepwise regression anymore as it is way too slow. \n\nI recently read the [Lasso chapter](https://www.cambridge.org/core/books/computer-age-statistical-inference/sparse-modeling-and-the-lasso/F840511B2F6A5A756FDDF1EA91BBA9DE) of Comuter Age Statistical Inference by Efron and Hastie (good book by the way). The fundamental idea here is that we can use a regularisation parameter (optimised with cross-validation) to prevent overfitting of a regression model. [Ridge regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) is essentially multiple linear regression using a squared (L2) regularisation function, whereas [Lasso regresion](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html) uses a absolute value (L1) regularisation function. As the L1-regularisation of Lasso has sharp edges as opposed to the curved boundary of the L2-regularisation function, lasso regression can have many coefficients set to zero, which gives us variable selection almost for free (or at least much cheaper than using stepwise regression). \n\nI tried a few different linear models (both ridge regression and the lasso), as well as the usual suspects (random forests, xgboost, etc.) but ended up settling on [SGD Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html). This is a very flexible linear model methodology which can encompass logistic regression, linear support vector machines (SVM), ridge regression and lasso regression through the choice of model parameters. The SGD in SGDRegressor stands for stochastic gradient descent and refers to the parameter optimisation method under the hood. \n\nI use `GridSearchCV` to choose the optimal regularisation parameter (`alpha`) and `l1_ratio` parameter. According to the function API, `l1_ratio` equal to 1 corresponds to lasso regression, and `l1_ratio` equal to 0 corresponds to ridge regression. `SGDRegressor` thus allows both models and a kind of ridge-lasso hybrid regression model if the parameter is between 0 and 1.\n\nThe preprocessing and modelling pipeline consists of three steps:\n1. Infill missing values in the predictor columns using mean values, or similar,\n2. Standardise predictor columns (this is important for `SGDRegressor`, but not important for something like `RandomForestRegressor`),\n3. Fit regression model\n\nI have implemented this in a scikit-learn type estimator using `BaseEstimator` (to inherit a few things like the `set_params` method) and `fit` and `predict` methods. This way it works directly with `GridSearchCV`.","metadata":{}},{"cell_type":"code","source":"%%time \n\nfrom sklearn.base import BaseEstimator\n\nclass Model(BaseEstimator):\n    def __init__(self, alpha=1, l1_ratio=0):#, alpha=1):\n        self.alpha = alpha\n        self.l1_ratio = l1_ratio\n    def predict(self, X):\n        #print('predicting...')\n        Xtr = self.si.transform(X)\n        Xtr2 = self.ss.transform(Xtr)\n        #print('  infilling zeros...')\n        ypred = self.mod.predict(Xtr2)\n        return ypred\n    def fit(self, X, y):\n        from sklearn.impute import SimpleImputer\n        from sklearn.preprocessing import StandardScaler\n        #print('infilling...')\n        self.si = SimpleImputer()\n        Xtr = self.si.fit_transform(X)\n        self.ss = StandardScaler()\n        #Xtr2 = self.ss.fit_transform(Xtr)\n        Xtr2 = self.ss.fit_transform(Xtr)\n        #print('fitting model...')\n        from sklearn.linear_model import SGDRegressor\n        self.mod = SGDRegressor(alpha=self.alpha, \n                                l1_ratio=self.l1_ratio,\n                                penalty='elasticnet', \n                                random_state = 1)\n\n        self.mod.fit(Xtr2,y)\ndef RMSE(y_pred, y_true):\n    from sklearn.metrics import mean_squared_error\n    return mean_squared_error(y_pred, y_true, squared=False)\n","metadata":{"execution":{"iopub.status.busy":"2022-06-27T23:08:11.392168Z","iopub.execute_input":"2022-06-27T23:08:11.392869Z","iopub.status.idle":"2022-06-27T23:08:11.433822Z","shell.execute_reply.started":"2022-06-27T23:08:11.392833Z","shell.execute_reply":"2022-06-27T23:08:11.432682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, perform the hyperparameter tuning on a range of values of `alpha` and `l1_ratio`. The `PredefinedSplit` cross validator allows us to specify the training/test data split directly (i.e. using the missingness of the response column) for use in `GridSearchCV`:","metadata":{}},{"cell_type":"code","source":"alphas = [pow(10,x) for x in range(-8,1)]\nl1_ratios = [0,0.25,0.5,0.75,1]\n\n\n\nfrom sklearn.model_selection import GridSearchCV, PredefinedSplit\nfrom sklearn.metrics import make_scorer\nplt.figure(figsize=[16,16])\npanel = 1\nbest_params = {}\n\nfor response_column in complete_data.columns[np.unique(missing['col'])]:\n    \n    print(response_column, end=' ')\n\n    # PredefinedSplit uses the following coding:\n    # -1: train\n    #  0: test\n    pds_indices = (-1*(1-incomplete_data[response_column].isna()*1))\n\n    ixes = list(PredefinedSplit(pds_indices).split())\n    train_ix, test_ix = ixes[0]\n\n    gscv = GridSearchCV(estimator = Model(),\n                        cv = PredefinedSplit(pds_indices),\n                        param_grid = {'alpha': alphas,\n                                      'l1_ratio': l1_ratios},\n                        scoring = make_scorer(RMSE,\n                                             greater_is_better=False),\n                        verbose=1,refit=True)\n\n    gscv.fit(incomplete_data.drop(response_column, axis=1), \n             complete_data[response_column])\n    \n    cv_results = pd.DataFrame(gscv.cv_results_['params'])\n    cv_results['score'] = gscv.cv_results_['mean_test_score']\n    results_for_plotting = cv_results.pivot(index='alpha',columns='l1_ratio', values='score')\n    plty = np.array(results_for_plotting)\n    plt.subplot(7,8,panel)\n    plt.semilogx(alphas,-plty)\n    plt.plot(alphas, \n             [RMSE(complete_data.iloc[test_ix,:][response_column], \n                   [np.mean(complete_data.iloc[test_ix,:][response_column]),]*complete_data.iloc[test_ix,:].shape[0]),]*len(alphas),\n             'k:')\n    ax3 = plt.gca()\n    plt.title(response_column)\n    ax3.set_yticklabels([])\n    ax3.set_xticklabels([])\n    ix = gscv.cv_results_['params'].index(gscv.best_params_)\n    plt.plot(gscv.best_params_['alpha'], -gscv.cv_results_['mean_test_score'][ix],'ro')\n    \n    \n    ypred_train = gscv.best_estimator_.predict(incomplete_data.iloc[train_ix,:].drop(response_column, axis=1))\n    RMSE_train = RMSE(ypred_train, complete_data.iloc[train_ix,:][response_column])\n    ypred_test = gscv.best_estimator_.predict(incomplete_data.iloc[test_ix,:].drop(response_column, axis=1))\n    RMSE_test = RMSE(ypred_test, complete_data.iloc[test_ix,:][response_column])\n\n    best_params[response_column] = dict(gscv.best_params_, **{'RMSE_train': RMSE_train,\n                                                              'RMSE_test': RMSE_test}) # concatenate dictionaries\n    panel += 1\n    #print('-------------------------------------')\n    \nprint()\n# Plot final panel as legend\nplt.subplot(7,8,panel)\n_=plt.plot([0,1],np.array(((1,1),(2,2),(3,3),(4,4),(5,5))).T)\nax = plt.gca()\nax.set_xlim(0,1.3)\nfor i in range(1,6):\n    _=ax.text(1.05, i, 'l1-ratio = '+str(l1_ratios[i-1]),size=12)\n_=ax.axis('off')\n\nplt.suptitle('Test RMSE (y-axis) vs. regularisation parameter (x-axis)')\n# Save results\nres_df = pd.DataFrame.from_dict(best_params, orient='index')\nres_df\nres_df.to_csv('cv_results.csv')","metadata":{"execution":{"iopub.status.busy":"2022-06-27T23:24:06.311026Z","iopub.execute_input":"2022-06-27T23:24:06.311914Z","iopub.status.idle":"2022-06-27T23:25:04.272119Z","shell.execute_reply.started":"2022-06-27T23:24:06.311868Z","shell.execute_reply":"2022-06-27T23:25:04.271314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Solid lines in each panel show the test-set RMSE for different values of `alpha` (x-axis) and the `l1-ratio` (line colour). The best model based on test-set RMSE occurs at the red dot. In almost all cases, the `GridSearchCV` is picking out a minimal-RMSE model. \n\nThe dotted line shows the RMSE of the test data from the mean-infilled baseline model, i.e. $\\sqrt{1/n\\sum(y_i-\\bar{y})^2}$. For some variables, the model is doing better than the mean-infilled baseline model, whereas in other cases the mean model fits the test data better. \n\n\n# Model prediction\n\nNext, we fit the best models to the competition data. Data is filled in left-to-right, and so imputed columns will be used for predictions for later columns.","metadata":{}},{"cell_type":"code","source":"%%time \n\nfor response_column in data.columns[list(np.nonzero(np.array(np.sum(data.isna())))[0])]:\n\n    print(response_column, end=' ')\n    missing_ixes = data[response_column].isna()\n    train_i = data.loc[~missing_ixes]\n    test_i = data.loc[missing_ixes]\n    \n\n    #Xte = si.transform(test_i.drop(response_column, axis=1))\n    mod_i = Model(alpha = res_df.loc[response_column,'alpha'],\n                  l1_ratio = res_df.loc[response_column,'l1_ratio'])\n    mod_i.fit(train_i.drop(response_column, axis=1), \n              train_i[response_column])\n    ypred = mod_i.predict(test_i.drop(response_column, axis=1))\n    data.iloc[test_i.index, np.nonzero(data.columns == response_column)[0][0]] = ypred\nprint()\n\n# Untransform the F_4 variables\n\ndata[F4cols] = yjpt.inverse_transform(data[F4cols])","metadata":{"execution":{"iopub.status.busy":"2022-06-24T03:52:20.913305Z","iopub.status.idle":"2022-06-24T03:52:20.915072Z","shell.execute_reply.started":"2022-06-24T03:52:20.914631Z","shell.execute_reply":"2022-06-24T03:52:20.914696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nrc = [x.split('-') for x in sample['row-col']]\nrow,col = zip(*rc)\nrow = [int(x) for x in row]\nsample['row'] = row\nsample['col'] = col\nsample_ix = np.bitwise_and(np.array(row) < MAXROWS, [x in TCOLS for x in col])\nsample = sample.iloc[sample_ix,:]\n\nsample['col_ix'] = [list(TCOLS).index(c) for c in sample['col']]\n\nsample.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-24T03:52:20.917023Z","iopub.status.idle":"2022-06-24T03:52:20.917742Z","shell.execute_reply.started":"2022-06-24T03:52:20.917397Z","shell.execute_reply":"2022-06-24T03:52:20.917427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Populate the submission data frame","metadata":{}},{"cell_type":"code","source":"%%time \n\nsubmission = sample.copy()\nvalues = [np.nan,] * submission.shape[0]\ni = 0\nfor _,x in submission.iterrows():\n    #if i % (int(submission.shape[0]/10)) == 0:\n    #    print(i)\n    values[i] = data.iloc[submission.iloc[i,:]['row'], submission.iloc[i,:]['col_ix']]\n    i += 1\nsubmission['value'] = values\n\nsubmission[['row-col','value']].head()\n\nsubmission[['row-col','value']].to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-06-24T03:52:20.920241Z","iopub.status.idle":"2022-06-24T03:52:20.920725Z","shell.execute_reply.started":"2022-06-24T03:52:20.920502Z","shell.execute_reply":"2022-06-24T03:52:20.920522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Error estimate\n\nAn estimate of the RMSE on the competition data can be obtained from the cross-validation model fitting - which is of course likely to be an underestimation for the usual reasons. ","metadata":{}},{"cell_type":"code","source":"np.sqrt(np.mean([x**2 for x in res_df['RMSE_test']]))","metadata":{"execution":{"iopub.status.busy":"2022-06-24T03:52:20.923894Z","iopub.status.idle":"2022-06-24T03:52:20.924454Z","shell.execute_reply.started":"2022-06-24T03:52:20.924231Z","shell.execute_reply":"2022-06-24T03:52:20.924253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusions\n\nSchafer and Graham (2002) suggest that: \"with or without missing data, the goal of a statistical procedure should be to make valid and efficient inferences about a population of interest — not to estimate, predict, or recover missing observations nor to obtain the same results that we would have seen with complete data.\" The goals of this month's TPS competition oppose this somewhat, in that our goal is explicitly to recover missing observations as well as possible. \n\nThe column-by-column SGDRegression approach I developed here definitely does a better job than mean-infilled imputation. However, several of the variables end up with mean-infilled models anyway. I ended up getting better results in this competition playing around with parameters from `IterativeImputer`.\n\nApart from the simple mean-infilled type of imputation, imputation seems to take forever. The `IterativeImputer` approach from sklearn runs for a similar order of magnitude to the approach I developed here, and comments in the discussion suggest that lots of people struggled with developing a non-trivial approach that ran relatively quickly.\n \nFaced with imputation methods that might end up taking longer than the actual modelling itself, mean infilling looks attractive. However, the main issue with simple approaches like this is underestimation of variance. If we need to use the imputed data column for subsequent modelling, infilling with the mean could end up biasing model parameters, variances, and estimates of predictive uncertainty, etc. As Schafer and Graham (2002) write: \"The average of the variable is preserved, but other aspects of its distribution - variance, quantiles, and so forth - are altered with potentially serious ramifications.\" In this case, resampling (\"hot deck\") type approaches start looking attractive.\n\nFinally, given missing data problems are not (or should not) really be prediction problems, perhaps a better result could be achieved using unsupervised models. I spent some time early on this month investigating this kind of approach. The basic idea behind this approach would be to cluster the complete data into 'similar' groups and use the groups as analogues to records with missing values. After identifying clusters, we'd identify which cluster each missing row belongs to and infill using the mean, or perhaps a more sophisticated estimate, of the variable for the group. If there was more time left I'd probably spend some of it exploring this kind of approach in more depth.\n\n# References\n\nI found the following (classic) papers helpful:\n\nRubin, D. B. (1976). Inference and missing data. Biometrika, 63, 581–592.\n\nSchafer, J. & Graham, J. (2002). Missing Data: Our View of the State of the Art. Psychological Methods. 7. 147-177. 10.1037/1082-989X.7.2.147. \n\n","metadata":{}}]}