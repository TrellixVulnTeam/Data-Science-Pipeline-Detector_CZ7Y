{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-22T01:51:54.281595Z","iopub.execute_input":"2022-06-22T01:51:54.282379Z","iopub.status.idle":"2022-06-22T01:51:54.312798Z","shell.execute_reply.started":"2022-06-22T01:51:54.282296Z","shell.execute_reply":"2022-06-22T01:51:54.312042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\n\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler","metadata":{"execution":{"iopub.status.busy":"2022-06-22T01:51:54.314417Z","iopub.execute_input":"2022-06-22T01:51:54.315021Z","iopub.status.idle":"2022-06-22T01:51:58.196705Z","shell.execute_reply.started":"2022-06-22T01:51:54.314974Z","shell.execute_reply":"2022-06-22T01:51:58.189429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers","metadata":{"execution":{"iopub.status.busy":"2022-06-22T01:51:58.198379Z","iopub.execute_input":"2022-06-22T01:51:58.198875Z","iopub.status.idle":"2022-06-22T01:52:04.042572Z","shell.execute_reply.started":"2022-06-22T01:51:58.198839Z","shell.execute_reply":"2022-06-22T01:52:04.041533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Read data","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/tabular-playground-series-jun-2022/data.csv')","metadata":{"execution":{"iopub.status.busy":"2022-06-22T01:52:04.047672Z","iopub.execute_input":"2022-06-22T01:52:04.048559Z","iopub.status.idle":"2022-06-22T01:52:22.403196Z","shell.execute_reply.started":"2022-06-22T01:52:04.048514Z","shell.execute_reply":"2022-06-22T01:52:22.402298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"F_4_cols = data.columns[data.columns.str.contains('F_4_')]\nF_3_cols = data.columns[data.columns.str.contains('F_3_')]\nF_2_cols = data.columns[data.columns.str.contains('F_2_')]\nF_1_cols = data.columns[data.columns.str.contains('F_1_')]\n\nnumeric_col = data.columns[data.dtypes == 'float64']\ncategorical_col = data.columns[data.dtypes == 'int64'].drop('row_id')","metadata":{"execution":{"iopub.status.busy":"2022-06-22T01:52:22.404726Z","iopub.execute_input":"2022-06-22T01:52:22.405326Z","iopub.status.idle":"2022-06-22T01:52:22.421517Z","shell.execute_reply.started":"2022-06-22T01:52:22.405288Z","shell.execute_reply":"2022-06-22T01:52:22.420843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axs = plt.subplots(nrows=4, ncols=1, figsize=(18, 30))\n\nfor i, col in enumerate([F_1_cols, F_2_cols, F_3_cols, F_4_cols]):\n    temp = data[col]\n    corr = temp.corr()\n    sns.heatmap(corr, ax=axs[i-1], annot=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-22T01:52:22.423711Z","iopub.execute_input":"2022-06-22T01:52:22.424272Z","iopub.status.idle":"2022-06-22T01:52:36.439446Z","shell.execute_reply.started":"2022-06-22T01:52:22.424236Z","shell.execute_reply":"2022-06-22T01:52:36.438734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Simple imputation with regressor for F_4 columns","metadata":{}},{"cell_type":"code","source":"missing_data_cols = np.concatenate([F_1_cols, F_3_cols, F_4_cols])","metadata":{"execution":{"iopub.status.busy":"2022-06-22T01:52:36.440369Z","iopub.execute_input":"2022-06-22T01:52:36.440692Z","iopub.status.idle":"2022-06-22T01:52:36.445745Z","shell.execute_reply.started":"2022-06-22T01:52:36.440648Z","shell.execute_reply":"2022-06-22T01:52:36.444821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Check the % missing values in each columns","metadata":{}},{"cell_type":"code","source":"pd.DataFrame([data[missing_data_cols].isna().mean()]).T.plot(kind='barh', figsize=(15, 10))","metadata":{"execution":{"iopub.status.busy":"2022-06-22T01:52:36.447222Z","iopub.execute_input":"2022-06-22T01:52:36.447657Z","iopub.status.idle":"2022-06-22T01:52:37.331661Z","shell.execute_reply.started":"2022-06-22T01:52:36.447622Z","shell.execute_reply":"2022-06-22T01:52:37.330732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Approx 1.8% values is missing in each columns","metadata":{}},{"cell_type":"markdown","source":"### Create validation set from non-missing data to evaluate imputation approaches. We will try to sample 1.8% of values in each column as validation data\n- For each approach, experiment we can test their performances by using this validation set\n- Impute F4 columns with regressor, first we try to experiment with different configs such as input features, and model parameters and compare them with validation data","metadata":{}},{"cell_type":"code","source":"validation_index_f4 = dict()  # indexes use for validation, we will use regressor to impute F_4 columns only\nfor col in F_4_cols:\n    selected_index = data[~data[col].isnull()].sample(frac=0.018, random_state=123).index.values\n    validation_index_f4[col] = selected_index","metadata":{"execution":{"iopub.status.busy":"2022-06-22T01:52:37.333157Z","iopub.execute_input":"2022-06-22T01:52:37.333519Z","iopub.status.idle":"2022-06-22T01:52:41.001914Z","shell.execute_reply.started":"2022-06-22T01:52:37.333481Z","shell.execute_reply":"2022-06-22T01:52:41.001063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Lightgbm regressor","metadata":{}},{"cell_type":"code","source":"def baseline_regressor_scoring(data, numeric_col, validation_index, fillna=None):\n    scoring_dict = dict()\n    for col_name, index in validation_index.items():\n        print('Start f: ', col_name) \n        learning_rate = 0.1  # try different lr for experiment\n        \n        # train data\n        col_data = data[~data[col].isnull()]\n        train_data = col_data[~col_data.index.isin(index)]\n        X_train = train_data[numeric_col.drop(col_name)].copy()\n        y_train = train_data[col_name]\n\n        # validation data on non-missing values\n        val_data = data.iloc[index]\n        X_val = val_data[numeric_col.drop(col_name)].copy()\n        y_val = val_data[col_name]\n        \n        if fillna is not None:  # fillna with a value for LGBMRegressor \n            X_train.fillna(-999, inplace=True)\n            X_val.fillna(-999, inplace=True)\n\n        # fit model\n        reg = lgb.LGBMRegressor(n_estimators=2000, device='gpu', learning_rate=learning_rate, objective='mean_squared_error', n_jobs=4)\n        reg.fit(X_train, y_train)\n        \n        scoring_dict[col_name] = [reg.score(X_val, y_val)]\n    return scoring_dict","metadata":{"execution":{"iopub.status.busy":"2022-06-22T01:52:41.003308Z","iopub.execute_input":"2022-06-22T01:52:41.004044Z","iopub.status.idle":"2022-06-22T01:52:41.013586Z","shell.execute_reply.started":"2022-06-22T01:52:41.004003Z","shell.execute_reply":"2022-06-22T01:52:41.012726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# use only F_4 features to predict F_4 columns\nlightgbm_regressor = baseline_regressor_scoring(data, F_4_cols, validation_index_f4)\nlightgbm_regressor = pd.DataFrame(lightgbm_regressor)","metadata":{"execution":{"iopub.status.busy":"2022-06-22T01:52:41.015944Z","iopub.execute_input":"2022-06-22T01:52:41.016515Z","iopub.status.idle":"2022-06-22T01:54:09.498341Z","shell.execute_reply.started":"2022-06-22T01:52:41.016479Z","shell.execute_reply":"2022-06-22T01:54:09.495275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### NN regressor","metadata":{}},{"cell_type":"code","source":"def create_nn_model(input_shape):\n    model = keras.Sequential([keras.layers.Input(input_shape),\n                          keras.layers.Dense(64, activation='relu'),\n                          keras.layers.BatchNormalization(),\n                          keras.layers.Dense(256, activation='relu'),\n                          keras.layers.BatchNormalization(),\n                          keras.layers.Dense(256, activation='relu'),\n                          keras.layers.BatchNormalization(),\n                          keras.layers.Dense(64, activation='relu'),\n                          keras.layers.BatchNormalization(),\n                          keras.layers.Dense(1, activation='linear')])\n\n    model.compile(loss=keras.losses.MeanSquaredError(),\n                optimizer=tf.keras.optimizers.Adam(0.001))\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-06-22T01:54:13.217926Z","iopub.execute_input":"2022-06-22T01:54:13.218299Z","iopub.status.idle":"2022-06-22T01:54:13.224991Z","shell.execute_reply.started":"2022-06-22T01:54:13.218267Z","shell.execute_reply":"2022-06-22T01:54:13.224152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_nn_reg(data, numeric_col, validation_index, scaler, dropna=True, fill_value=0):\n    scoring_dict = dict()\n    for col_name, index in validation_index.items():\n        print('Start f: ', col_name)\n        \n        # train data\n        col_data = data[~data[col_name].isnull()]\n        train_data = col_data[~col_data.index.isin(index)].copy()\n        \n        # validation data on non-missing values\n        val_data = data.iloc[index].copy()\n        if dropna:\n            train_data.dropna(inplace=True)\n            val_data.dropna(inplace=True)\n            \n        X_train = train_data[numeric_col.drop(col_name)].copy()\n        y_train = train_data[col_name]\n\n        X_val = val_data[numeric_col.drop(col_name)].copy()\n        y_val = val_data[col_name]\n    \n        if not dropna:\n            X_train.fillna(fill_value, inplace=True)\n            X_val.fillna(fill_value, inplace=True)\n        \n        X_train = X_train.values\n        X_val = X_val.values\n        if scaler is not None:\n            scaler = scaler.fit(X_train)\n            X_train = scaler.transform(X_train)\n            X_val = scaler.transform(X_val)\n        \n        model = create_nn_model(X_train.shape[-1])\n        early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min',\n                                              patience=5, restore_best_weights=True)\n \n        model.fit(X_train, y_train, batch_size=2048, epochs=20, shuffle=True, verbose=0, \n                  validation_split=0.1, callbacks=[early_stop])\n        scoring_dict[col_name] = [r2_score(y_val, model.predict(X_val).flatten())]\n        print(scoring_dict[col_name])\n    print('-'*40)\n    return scoring_dict","metadata":{"execution":{"iopub.status.busy":"2022-06-22T01:54:14.318082Z","iopub.execute_input":"2022-06-22T01:54:14.318638Z","iopub.status.idle":"2022-06-22T01:54:14.331728Z","shell.execute_reply.started":"2022-06-22T01:54:14.318605Z","shell.execute_reply":"2022-06-22T01:54:14.33073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nn_baseline = train_nn_reg(data, F_1_cols.append([F_2_cols, F_3_cols, F_4_cols]),\n                           validation_index_f4, scaler=None, dropna=True)\nnn_baseline = pd.DataFrame(nn_baseline)","metadata":{"execution":{"iopub.status.busy":"2022-06-22T01:54:15.556506Z","iopub.execute_input":"2022-06-22T01:54:15.557157Z","iopub.status.idle":"2022-06-22T01:54:42.435716Z","shell.execute_reply.started":"2022-06-22T01:54:15.557119Z","shell.execute_reply":"2022-06-22T01:54:42.434523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nn_baseline_norm = train_nn_reg(data, F_1_cols.append([F_2_cols, F_3_cols, F_4_cols]),\n                           validation_index_f4, scaler=StandardScaler(), dropna=True)\nnn_baseline_norm = pd.DataFrame(nn_baseline_norm)","metadata":{"execution":{"iopub.status.busy":"2022-06-22T01:54:09.50498Z","iopub.status.idle":"2022-06-22T01:54:09.505814Z","shell.execute_reply.started":"2022-06-22T01:54:09.505568Z","shell.execute_reply":"2022-06-22T01:54:09.505591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nn_baseline_no_scale_keepna = train_nn_reg(data, F_1_cols.append([F_2_cols, F_3_cols, F_4_cols]),\n                           validation_index_f4, scaler=None, dropna=False, fill_value=0)\nnn_baseline_no_scale_keepna = pd.DataFrame(nn_baseline_no_scale_keepna)","metadata":{"execution":{"iopub.status.busy":"2022-06-22T01:54:09.50685Z","iopub.status.idle":"2022-06-22T01:54:09.507632Z","shell.execute_reply.started":"2022-06-22T01:54:09.507397Z","shell.execute_reply":"2022-06-22T01:54:09.507419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Different methods result can be visualized here","metadata":{}},{"cell_type":"code","source":"scoring = lightgbm_regressor[F_4_cols].append(nn_baseline)\nscoring = scoring.append(nn_baseline_norm)\nscoring = scoring.append(nn_baseline_no_scale_keepna)\nscoring.reset_index(drop=True, inplace=True)\nscoring['label'] = ['lightgbm', 'nn baseline', 'nn baseline norm', 'nn baseline keepna']\nscoring = scoring.melt(['label'], var_name='col', value_name='score')","metadata":{"execution":{"iopub.status.busy":"2022-06-21T02:27:34.941253Z","iopub.execute_input":"2022-06-21T02:27:34.941734Z","iopub.status.idle":"2022-06-21T02:27:34.960398Z","shell.execute_reply.started":"2022-06-21T02:27:34.941691Z","shell.execute_reply":"2022-06-21T02:27:34.959209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"F4_impute_configs = scoring.loc[scoring[scoring.label.isin(['nn_baseline_no_scale_keepna','lightgbm'])].groupby('col')['score'].idxmax()]","metadata":{"execution":{"iopub.status.busy":"2022-06-21T02:27:36.760066Z","iopub.execute_input":"2022-06-21T02:27:36.760472Z","iopub.status.idle":"2022-06-21T02:27:36.771072Z","shell.execute_reply.started":"2022-06-21T02:27:36.76044Z","shell.execute_reply":"2022-06-21T02:27:36.770239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.catplot(data=scoring, kind=\"bar\", x=\"col\", y=\"score\", palette='dark', hue=\"label\", alpha=.6, height=5, aspect=2)","metadata":{"execution":{"iopub.status.busy":"2022-06-21T02:27:38.492493Z","iopub.execute_input":"2022-06-21T02:27:38.4934Z","iopub.status.idle":"2022-06-21T02:27:38.912608Z","shell.execute_reply.started":"2022-06-21T02:27:38.493364Z","shell.execute_reply":"2022-06-21T02:27:38.911526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# impute F4 columns with configs found above\ndef train_reg(data, numeric_col, impute_cols):\n    imputed_data = data.copy()\n    \n    for col in impute_cols:    \n        print('Start training: ', col)\n        # get train_data by column and cluster\n        temp_data = data[~data[col].isnull()]        \n        pred_data = data[data[col].isnull()]\n        \n        X = temp_data[numeric_col.drop(col)]\n        y = temp_data[col]\n        \n        reg = lgb.LGBMRegressor(n_estimators=50000, device='gpu', metric='rmse', n_jobs=-1)\n        reg.fit(X, y)\n        \n        imputed_data.loc[pred_data.index, col] = reg.predict(pred_data[numeric_col.drop(col)])\n\n        print('Training score {}: '.format(col), reg.score(X, y))\n        print('-'*40)\n    return imputed_data[impute_cols]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"impute_cols_lightgbm = F4_impute_configs[F4_impute_configs.label == 'lightgbm']['col'].values\nimputed_f4_data_lightgbm = train_reg(data, numeric_col=F_4_cols, impute_cols=impute_cols_lightgbm)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_nn_reg_pred(data, numeric_col, impute_cols):\n    imputed_data = data.copy()\n    \n    for col in impute_cols:       \n        print('Start training: ', col)\n        # get train_data by column and cluster\n        temp_data = data[~data[col].isnull()]        \n        pred_data = data[data[col].isnull()]\n        \n        X = temp_data[numeric_col.drop(col)].fillna(0)\n        y = temp_data[col]\n        \n        model = create_nn_model(X.shape[-1])\n        early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min',\n                                              patience=5, restore_best_weights=True)\n                                              \n        folds = KFold(n_splits=5, shuffle=True, random_state=123).split(X, y)                                              \n        for j, (train_idx, val_idx) in enumerate(folds):\n            print('\\nFold ',j)\n\n            x_fold_train = X.values[train_idx]\n            x_fold_valid = X.values[val_idx]\n            y_fold_train = y.values[train_idx]\n            y_fold_valid = y.values[val_idx]\n            model.fit(x_fold_train, y_fold_train, batch_size=2048, epochs=30, shuffle=True, verbose=0, \n                      validation_data=(x_fold_valid, y_fold_valid), callbacks=[early_stop])\n            \n        print('Training score {}: '.format(col), r2_score(y, model.predict(X).flatten()))\n        \n        imputed_data.loc[pred_data.index, col] = model.predict(pred_data[numeric_col.drop(col)].fillna(0)).flatten()\n        \n    return imputed_data[impute_cols]","metadata":{"execution":{"iopub.status.busy":"2022-06-21T02:29:42.685571Z","iopub.execute_input":"2022-06-21T02:29:42.686109Z","iopub.status.idle":"2022-06-21T02:29:42.700118Z","shell.execute_reply.started":"2022-06-21T02:29:42.686073Z","shell.execute_reply":"2022-06-21T02:29:42.698912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"impute_cols_nn = F4_impute_configs[F4_impute_configs.label == 'nn_baseline_no_scale_keepna']['col'].values\nimputed_f4_data_nn = train_nn_reg_pred(data, numeric_col=pd.Index(np.concatenate([F_4_cols, F_1_cols, F_3_cols, F_2_cols])), impute_cols=F_4_cols)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imputed_data = data.copy()\nimputed_data.loc[:, impute_cols_lightgbm] = imputed_f4_data_lightgbm\nimputed_data.loc[:, F_4_cols] = imputed_f4_data_nn","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" - Impute F1, F3 columns with basics methods","metadata":{}},{"cell_type":"code","source":"validation_index_F1_F3 = dict()\nfor col in np.concatenate([F_1_cols, F_3_cols]):\n    selected_index = data[~data[col].isnull()].sample(frac=0.018, random_state=123).index.values\n    validation_index_F1_F3[col] = selected_index","metadata":{"execution":{"iopub.status.busy":"2022-06-15T10:10:52.783953Z","iopub.execute_input":"2022-06-15T10:10:52.784287Z","iopub.status.idle":"2022-06-15T10:11:05.56231Z","shell.execute_reply.started":"2022-06-15T10:10:52.784251Z","shell.execute_reply":"2022-06-15T10:11:05.561281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# find the best methods can be used for imputation F_1, F_3 columns\ndef basic_impute_scoring(data, validation_index):\n    scoring_dict = dict()\n    for col_name, index in validation_index.items():\n        val_data = data.iloc[index][col_name]        \n        median_score = np.sqrt(mean_squared_error(val_data, [data[col_name].median()]*len(val_data)))\n        mean_score = np.sqrt(mean_squared_error(val_data, [data[col_name].mean()]*len(val_data)))\n        zero_score = np.sqrt(mean_squared_error(val_data, [0]*len(val_data)))\n        \n        scoring_dict[col_name] = [median_score, mean_score, zero_score]\n    return scoring_dict","metadata":{"execution":{"iopub.status.busy":"2022-06-15T10:11:05.563685Z","iopub.execute_input":"2022-06-15T10:11:05.564153Z","iopub.status.idle":"2022-06-15T10:11:05.573009Z","shell.execute_reply.started":"2022-06-15T10:11:05.564109Z","shell.execute_reply":"2022-06-15T10:11:05.57202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"basic_impute = basic_impute_scoring(data, validation_index_F1_F3)\nbasic_impute = pd.DataFrame(basic_impute)\nbasic_impute['label'] = ['median', 'mean', 'zeros']\nbasic_impute = basic_impute.melt('label', var_name='col')\nbasic_impute","metadata":{"execution":{"iopub.status.busy":"2022-06-15T10:11:05.575794Z","iopub.execute_input":"2022-06-15T10:11:05.576286Z","iopub.status.idle":"2022-06-15T10:11:07.794296Z","shell.execute_reply.started":"2022-06-15T10:11:05.576239Z","shell.execute_reply":"2022-06-15T10:11:07.793312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"impute_configs = basic_impute.loc[basic_impute.groupby('col')['value'].idxmin()][['label', 'col']].values\nimpute_configs","metadata":{"execution":{"iopub.status.busy":"2022-06-15T10:11:07.795253Z","iopub.execute_input":"2022-06-15T10:11:07.795577Z","iopub.status.idle":"2022-06-15T10:11:07.813487Z","shell.execute_reply.started":"2022-06-15T10:11:07.79555Z","shell.execute_reply":"2022-06-15T10:11:07.812794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for method, col in impute_configs:\n    if method == 'zeros':\n        imputed_data.loc[imputed_data[col].isnull(), col] = 0\n    elif method == 'median':\n        imputed_data.loc[imputed_data[col].isnull(), col] = imputed_data[col].median()\n    elif method == 'mean':\n        imputed_data.loc[imputed_data[col].isnull(), col] = imputed_data[col].mean()","metadata":{"execution":{"iopub.status.busy":"2022-06-15T10:11:45.64529Z","iopub.execute_input":"2022-06-15T10:11:45.646001Z","iopub.status.idle":"2022-06-15T10:11:46.333719Z","shell.execute_reply.started":"2022-06-15T10:11:45.645944Z","shell.execute_reply":"2022-06-15T10:11:46.332906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Generate submission file\n- Generate function is taken from [here](https://www.kaggle.com/code/ygorana/tps-june-2022-ultra-fast-submissions-50s)","metadata":{}},{"cell_type":"code","source":"def generate_submission(source_df: pd.DataFrame, output_df: pd.DataFrame) -> pd.DataFrame:\n    # Melt source dataframe filtered on NaN values to form [row_id, col, isNull] ...\n    # ... with MultiIndex on (row_id, col)\n    nan_only = (source_df.isna().melt(ignore_index=False, var_name='col', value_name='isNull')\n                .query('isNull == True')\n                .set_index(['col'], append=True))\n\n    # Melt output dataframe to form [row_id, col, value] with MultiIndex on (row_id, col)\n    out = (output_df.melt(ignore_index=False, var_name='col').set_index(['col'], append=True))\n\n    # Filter output's MultiIndex on nan_only's MultiIndex\n    out = (out.loc[nan_only.index].sort_index())\n    \n    # Flatten MultiIndex to Index & rename to desired column\n    out.index = [f'{r}-{c}' for r, c in out.index]\n    out.index.name = 'row-col'\n    return out","metadata":{"execution":{"iopub.status.busy":"2022-06-15T10:11:50.280918Z","iopub.execute_input":"2022-06-15T10:11:50.281626Z","iopub.status.idle":"2022-06-15T10:11:50.291688Z","shell.execute_reply.started":"2022-06-15T10:11:50.281563Z","shell.execute_reply":"2022-06-15T10:11:50.290771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = generate_submission(data, imputed_data).reset_index()","metadata":{"execution":{"iopub.status.busy":"2022-06-15T10:11:51.931166Z","iopub.execute_input":"2022-06-15T10:11:51.931752Z","iopub.status.idle":"2022-06-15T10:13:02.852472Z","shell.execute_reply.started":"2022-06-15T10:11:51.931714Z","shell.execute_reply":"2022-06-15T10:13:02.851326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T10:13:02.865559Z","iopub.execute_input":"2022-06-15T10:13:02.865987Z","iopub.status.idle":"2022-06-15T10:13:06.059784Z","shell.execute_reply.started":"2022-06-15T10:13:02.865951Z","shell.execute_reply":"2022-06-15T10:13:06.058667Z"},"trusted":true},"execution_count":null,"outputs":[]}]}