{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-25T00:23:16.456123Z","iopub.execute_input":"2022-06-25T00:23:16.457566Z","iopub.status.idle":"2022-06-25T00:23:16.494963Z","shell.execute_reply.started":"2022-06-25T00:23:16.457316Z","shell.execute_reply":"2022-06-25T00:23:16.493745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_submission = pd.read_csv('/kaggle/input/tabular-playground-series-jun-2022/sample_submission.csv', index_col='row-col')\ndf_submission","metadata":{"execution":{"iopub.status.busy":"2022-06-25T00:23:42.532729Z","iopub.execute_input":"2022-06-25T00:23:42.533103Z","iopub.status.idle":"2022-06-25T00:23:43.789268Z","shell.execute_reply.started":"2022-06-25T00:23:42.533072Z","shell.execute_reply":"2022-06-25T00:23:43.788217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.set_option('display.max_columns', 100)\ndf = pd.read_csv('/kaggle/input/tabular-playground-series-jun-2022/data.csv', index_col='row_id')\ndf","metadata":{"execution":{"iopub.status.busy":"2022-06-25T00:24:11.525713Z","iopub.execute_input":"2022-06-25T00:24:11.52618Z","iopub.status.idle":"2022-06-25T00:24:25.931283Z","shell.execute_reply.started":"2022-06-25T00:24:11.526126Z","shell.execute_reply":"2022-06-25T00:24:25.930087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Check the correlation","metadata":{}},{"cell_type":"code","source":"from matplotlib import pylab as plt\nimport seaborn as sns\n\nfig, ax = plt.subplots(figsize=(40, 40))\nsns.heatmap(df.corr(), annot=True, fmt='.1f')","metadata":{"execution":{"iopub.status.busy":"2022-06-25T00:25:00.85808Z","iopub.execute_input":"2022-06-25T00:25:00.859788Z","iopub.status.idle":"2022-06-25T00:25:42.312774Z","shell.execute_reply.started":"2022-06-25T00:25:00.859731Z","shell.execute_reply":"2022-06-25T00:25:42.311808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* It is better to use the average to replace the missing valuess in F_1 and F_3.\n* F_4 has no correlation with F_1, F_2, and F_3","metadata":{}},{"cell_type":"markdown","source":"# Modeling with F_4 function only","metadata":{}},{"cell_type":"markdown","source":"I took an approach like the MLM model.  \nI only need a single model, but to get a high score, the model needs a lot of parameters.","metadata":{}},{"cell_type":"code","source":"import itertools\n\ndef data_generator(df, batch_size, is_train=True):\n    df = df.copy()\n    while True:\n        if is_train:\n            df.sample(frac=1)\n        x_float = df.loc[:, df.columns.str.startswith('F_4_')]\n        for i in range((len(df)-1)//batch_size + 1):\n            _x_float = x_float[i*batch_size:(i+1)*batch_size].values\n            if is_train:\n                # mask = (np.random.rand(*_x_float.shape) > 0.3).astype(float)\n                # mask = (np.random.rand(*_x_float.shape) > 0.2).astype(float)\n                mask = (np.random.rand(*_x_float.shape) > 0.15).astype(float)\n            else:\n                mask = np.ones(_x_float.shape)\n            flg_nan = np.isnan(_x_float).astype(float)\n            mask = mask * (1-flg_nan)  # if flg_nan is 1 then mask is 0.\n            if not is_train:\n                flg_nan = np.zeros(_x_float.shape)\n            x = [np.nan_to_num(_x_float), mask, flg_nan]\n            yield x, np.nan_to_num(_x_float)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\ntf.random.set_seed(42)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"float_dim = sum(df.columns.str.startswith('F_4_'))\n\ndropout_p = 0\nn_d = 16  # 2048\nn_mul_growth = 1\nn_add_growth = n_d\nn_depth = 2  # 6\nn_bottleneck = 16  # 512\nis_concat = True\noptimizer = 'nadam'  \nreduce_lr_patience = 3  # 15\nearly_stopping_patience = 5  # 30\n\n\ninputs = []\n\n\nfloat_input = tf.keras.layers.Input(\n    shape=(float_dim,), name='float_input'\n)\ninputs.append(float_input)\n\nmask_input = tf.keras.layers.Input(\n    shape=(float_dim,), name='mask_input'\n)\ninputs.append(mask_input)\nembeds_output = tf.keras.layers.Multiply()([float_input, mask_input])\n\nflg_nan_input = tf.keras.layers.Input(\n    shape=(float_dim,), name='flg_nan_input'\n)\ninputs.append(flg_nan_input)\n\n\nxs = [embeds_output]\nx = embeds_output\n\n# Main Network\nfor i in range(n_depth):\n    if is_concat:\n        x = tf.keras.layers.Concatenate()(xs)\n    if n_bottleneck:\n        x = tf.keras.layers.Dense(n_bottleneck, activation='relu')(x)\n    x = tf.keras.layers.Dense(n_d * (n_mul_growth ** i) + (n_add_growth * i), activation='relu')(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    if dropout_p:\n        x = tf.keras.layers.Dropout(dropout_p)(x)\n    xs.append(x)\n\nif is_concat:\n    x = tf.keras.layers.Concatenate()(xs)\nx = tf.keras.layers.Dense(float_dim)(x)\nx = tf.keras.layers.Multiply()([x, tf.ones((float_dim,))-flg_nan_input])  # if missing value, then output is 0. (Also input is 0.)\na1 = tf.keras.layers.Multiply()([float_input, mask_input])  # if mask is 1, then output == input.\na2 = tf.keras.layers.Multiply()([x, tf.ones((float_dim,))-mask_input])  # if mask is 0, then output == x.\noutput = tf.keras.layers.Add()([a1, a2])\n\nmodel = tf.keras.models.Model(inputs=inputs, outputs=output)\n\nmodel.compile(optimizer=optimizer, loss='mean_squared_error', metrics=tf.keras.metrics.RootMeanSquaredError())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ndf_train, df_test = train_test_split(df, test_size=0.2, random_state=42)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_root_mean_squared_error\", patience=early_stopping_patience, restore_best_weights=True)\nreduce_lr_on_plateau = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_root_mean_squared_error', factor=0.5, patience=reduce_lr_patience)\nmodel_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n    filepath='weight.hdf5', \n    monitor='val_root_mean_squared_error',\n    verbose=1,\n    save_best_only=True,\n    save_weights_only=True,\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.random.seed(42)\nbatch_size = 4096\ngen_train = data_generator(df_train, batch_size=batch_size)\ngen_test = data_generator(df_test, batch_size=batch_size)\nmodel.fit_generator(\n    gen_train, \n    steps_per_epoch=(len(df_train)-1)//batch_size + 1, \n    epochs=800,\n    validation_data=gen_test,\n    validation_steps=5 * ((len(df_test)-1)//batch_size + 1),\n    callbacks = [early_stopping, reduce_lr_on_plateau, model_checkpoint]\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predict F_4 missing values","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\ntf.random.set_seed(42)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del df_train\ndel df_test\ndel gen_train\ndel gen_test","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\ngc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.random.seed(42)\nbatch_size = 4096\ngen_pred = data_generator(df, batch_size=batch_size, is_train=False)\npred = model.predict(gen_pred, steps=(len(df)-1)//batch_size + 1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Impute F_1, F_3, F_4","metadata":{}},{"cell_type":"code","source":"_df_sub = df.select_dtypes('float').copy()\n_df_sub.loc[:, _df_sub.columns.str.startswith('F_4_')] = pred\n_df_sub","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.impute import SimpleImputer\n\nimp = SimpleImputer(\n        missing_values=np.nan,\n        strategy='mean')\n\n_df_sub[:] = imp.fit_transform(_df_sub)\n_df_sub","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm \n\nfor i in tqdm(df_submission.index):\n    row = int(i.split('-')[0])\n    col = i.split('-')[1]\n    df_submission.loc[i, 'value'] = _df_sub.loc[row, col]\n\ndf_submission.to_csv('submisson_neural.csv')","metadata":{},"execution_count":null,"outputs":[]}]}