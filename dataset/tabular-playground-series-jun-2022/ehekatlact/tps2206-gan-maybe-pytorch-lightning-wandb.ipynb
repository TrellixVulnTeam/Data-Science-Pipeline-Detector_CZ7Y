{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"I just wanted to practice implementing GANs, I don't know if GANs are superior or if GANs are implemented correctly.\n\nPlease don't feel bad.\n\nIf there are any oddities in this notebook, I would appreciate your pointing them out.","metadata":{}},{"cell_type":"code","source":"# For Google Colab\n\"\"\"\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\n# Install kaggle packages\n!pip install -q kaggle\n!pip install -q kaggle-cli\n\n# Lib\nfrom google.colab import files\n\n# Please Upload `kaggle.json` file\nuploaded = files.upload()\n\n# Then copy kaggle.json into the folder where the API expects to find it.\n!mkdir -p ~/.kaggle\n!cp kaggle.json ~/.kaggle/\n!chmod 600 ~/.kaggle/kaggle.json\n!ls ~/.kaggle\n\n!kaggle competitions download -c tabular-playground-series-jun-2022\n!unzip -o tabular-playground-series-jun-2022.zip -d tabular-playground-series-jun-2022\n# !kaggle kernels output ehekatlact/tps2206-pytorch-lightning-fine-tuning2 -p ./DataSet\n\"\"\"","metadata":{"id":"08ede1b7","outputId":"7b664d8b-d19c-44d6-8a2f-6090643c699a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%capture\n!pip install wandb\n!pip install pytorch_lightning","metadata":{"id":"95312a76"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import wandb\ntry:\n    # add-ons -> secrets -> set your wandb api key\n    from kaggle_secrets import UserSecretsClient\n    user_secrets = UserSecretsClient()\n    secret_value_0 = user_secrets.get_secret(\"wandb_api\")\n    wandb.login(key=secret_value_0)\n    anony = None\nexcept:\n    anony = \"must\"\n    print('If you want to use your W&B account, go to Add-ons -> Secrets and provide your W&B access token. Use the Label name as wandb_api. \\nGet your W&B access token from here: https://wandb.ai/authorize')","metadata":{"id":"e50babc2","outputId":"67f11291-1b60-4502-a780-cbe80f2e31dc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Lib","metadata":{"id":"74ff08e2"}},{"cell_type":"code","source":"# common\nimport os\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nimport time, gc, string, math\nfrom tqdm.notebook import tqdm\nimport warnings\nimport shutil\nfrom collections import defaultdict\nimport heapq\nimport datetime\nimport random\nfrom collections import OrderedDict\nimport glob\nimport copy\n\n# sklearn\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import LabelEncoder\n\n# pytorch\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch import optim\nfrom torch.optim import lr_scheduler\n\n# pytorch lightning\nimport pytorch_lightning as pl\nfrom pytorch_lightning.loggers import WandbLogger\n","metadata":{"id":"baeddd0d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.set_option('display.max_columns', 200)\npd.set_option('display.max_rows', 200)","metadata":{"id":"HmNa5M7LXKOV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for google colab\n# os.chdir(\"/content/drive/MyDrive/colab_data/TPS2206\")","metadata":{"id":"yWDSyU66tnEj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nos.makedirs('model', exist_ok=True)\nshutil.rmtree('./model/')\nos.makedirs('model', exist_ok=True)\n\"\"\"","metadata":{"id":"dcec1181","outputId":"e18e7985-cf37-439a-c2b3-4de3909b0ac6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Read DF","metadata":{"id":"d671d1ab"}},{"cell_type":"code","source":"data = pd.read_csv('../input/tabular-playground-series-jun-2022/data.csv', index_col='row_id')\nsub = pd.read_csv('../input/tabular-playground-series-jun-2022/sample_submission.csv', index_col='row-col')","metadata":{"id":"50249fa9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in data.columns:\n    if \"F_4\" not in col:\n        data[col] = data[col].fillna(data[col].mean())","metadata":{"id":"BfU09bCzohwR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"na_col_list = []\nfor col in data.columns:\n    if data[col].isna().sum() != 0:\n        na_col_list.append(col)","metadata":{"id":"DPfpFrvH04oq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"na_index_of = {}\nfor col in na_col_list:\n    na_index = list(np.where(data[col].isnull())[0])\n    na_index_of[col] = na_index","metadata":{"id":"ac89d698"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_col_list = []\nval_col_list = []\nle = LabelEncoder()\nfor col in data.columns:\n    if data[col].nunique() < 32:\n        cat_col_list.append(col)\n        data[col] = le.fit_transform(data[col])\n    else:\n        val_col_list.append(col)","metadata":{"id":"ztIxH1EnJc_Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.fillna(data.mean(), inplace=True)","metadata":{"id":"UNzk5EUz_jBW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Pytorch","metadata":{"id":"eb1655fa"}},{"cell_type":"markdown","source":"### DataSet and DataLoader","metadata":{"id":"b1554fbf"}},{"cell_type":"code","source":"class TrainDataset(Dataset):\n    def __init__(self, X_val, X_cat, X_na, na_col_list=None):\n        self.X_val = X_val\n        self.X_cat = X_cat\n        self.X_na = X_na\n        self.na_col_list = na_col_list\n\n    def __len__(self):\n        return len(self.X_na)\n\n    def __getitem__(self, item):\n        # pre_val_inputs = copy.deepcopy(self.X_val[item])\n        pre_val_inputs = self.X_val[item].copy()\n        i = 0\n        if self.na_col_list is not None:\n            # val_inputsのうちいずれか1つを乱数で埋める\n            # x = -1\n            x = np.random.randn()\n            i = random.randrange(0, len(na_col_list))\n            pre_val_inputs[self.na_col_list[i]] = x\n        val_inputs = torch.tensor(pre_val_inputs, dtype=torch.float32)\n        cat_inputs = torch.tensor(self.X_cat[item], dtype=torch.int32)\n        outputs = torch.tensor(self.X_na[item], dtype=torch.float32)\n        zeros = torch.zeros(1, dtype=torch.float32)\n        ones = torch.ones(1, dtype=torch.float32)\n\n        return val_inputs, cat_inputs, outputs, zeros, ones, i","metadata":{"id":"850b174a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_val = data.loc[:, val_col_list].values\ndata_cat = data.loc[:, cat_col_list].values\ndata_na = data.loc[:, na_col_list].values\nds = TrainDataset(data_val, data_cat, data_na, na_col_list)\nfor v, c, o, zero, ones, i in ds:\n    break\ndel data_val, data_cat, data_na\ngc.collect()","metadata":{"id":"wH1GVcQIg0LL","outputId":"231530c6-3bd0-4a83-9f8b-352de57b0bad"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DataModule(pl.LightningDataModule):\n    # train, val, testの3つのDataLoaderを定義する\n    # trainerにこれを渡すと、train, val, testのそれぞれのステップでこれを渡してくれる\n    def __init__(self, train, valid, cat_col_list, val_col_list, na_col_list, batch_size):\n        self.train_val = train.loc[:, val_col_list].values\n        self.valid_val = valid.loc[:, val_col_list].values\n        self.train_cat = train.loc[:, cat_col_list].values\n        self.valid_cat = valid.loc[:, cat_col_list].values\n        self.train_na = train.loc[:, na_col_list].values\n        self.valid_na = valid.loc[:, na_col_list].values\n        self.batch_size = batch_size\n        self.na_col_list = [val_col_list.index(col) for col in na_col_list]\n        self._log_hyperparams = None  # ナニコレ・・・\n\n    def train_dataloader(self):\n        ds = TrainDataset(self.train_val, self.train_cat, self.train_na, self.na_col_list)\n        dl = DataLoader(ds, batch_size=self.batch_size, shuffle=True, pin_memory=True, drop_last=True, num_workers=CFG.num_workers, persistent_workers=True)\n        return dl\n\n    def val_dataloader(self):\n        ds = TrainDataset(self.valid_val, self.valid_cat, self.valid_na)\n        dl = DataLoader(ds, batch_size=self.batch_size, shuffle=False, pin_memory=True, drop_last=False, num_workers=CFG.num_workers, persistent_workers=True)\n        return dl\n\n    def predict_dataloader(self):\n        ds = TrainDataset(self.valid_val, self.valid_cat, self.valid_na)\n        dl = DataLoader(ds, batch_size=self.batch_size, shuffle=False, pin_memory=True, drop_last=False, num_workers=CFG.num_workers, persistent_workers=True)\n        return dl\n\n    def prepare_data_per_node(self):\n        # TODO 本来要らないはずなんだけど・・・\n        pass\n\n    def teardown(self, stage=None):\n        torch.cuda.empty_cache()  # TODO: これであってるのか不明　何も出てこないんだよね\n        gc.collect()","metadata":{"id":"d1741d8e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Pytorch Model","metadata":{"id":"0050d402"}},{"cell_type":"code","source":"class Generator(nn.Module):\n    def __init__(self, val_input_size, cat_input_size, output_size):\n        super().__init__()\n        hidden_size = 100\n        emb_dim = 8\n        self.cat_input_size = cat_input_size\n        _emb_list = [nn.Embedding(32, emb_dim) for _ in range(cat_input_size)]\n        self.emb_list = nn.ModuleList(_emb_list)\n        self.fc1 = nn.Linear(val_input_size+emb_dim*cat_input_size, hidden_size*4)\n        self.bn1 = nn.BatchNorm1d(hidden_size*4)\n        self.fc2 = nn.Linear(hidden_size*4, hidden_size*4)\n        self.fc3 = nn.Linear(hidden_size*4, hidden_size*2)\n        self.fc4 = nn.Linear(hidden_size*2, hidden_size)\n        self.fc5 = nn.Linear(hidden_size, output_size)\n    \n    def forward(self, val_x, cat_x):\n        # dropoutとbnの併用禁止\n        # bnは活性化関数の前に\n        embbed_list = []\n        for i in range(self.cat_input_size):\n            emb = self.emb_list[i]\n            embbed_list.append(emb(cat_x[:, i]))\n        embbed = torch.cat(embbed_list, dim=1)\n        x = torch.cat([val_x, embbed], dim=1)\n        x = F.silu(self.bn1((self.fc1(x))))\n        x = F.silu(self.fc2(x))\n        x = F.silu(self.fc3(x))\n        x = F.silu(self.fc4(x))\n        x = self.fc5(x)\n        return x","metadata":{"id":"48355fe7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Discriminator(nn.Module):\n    def __init__(self, input_size):\n        super().__init__()\n        hidden_size = 50\n        self.fc1 = nn.Linear(input_size, hidden_size*8)\n        self.bn1 = nn.BatchNorm1d(hidden_size*8)\n        self.fc2 = nn.Linear(hidden_size*8, hidden_size*4)\n        self.fc3 = nn.Linear(hidden_size*4, hidden_size*2)\n        self.fc4 = nn.Linear(hidden_size*2, hidden_size)\n        self.fc5 = nn.Linear(hidden_size, 1)\n    \n    def forward(self, x):\n        # dropoutとbnの併用禁止\n        # bnは活性化関数の前に\n        x = F.silu(self.bn1((self.fc1(x))))\n        x = F.silu(self.fc2(x))\n        x = F.silu(self.fc3(x))\n        x = F.silu(self.fc4(x))\n        x = torch.sigmoid(self.fc5(x))\n        return x","metadata":{"id":"XsahzecJOpIj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class RMSELoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.mse = nn.MSELoss()\n        \n    def forward(self, yhat, y):\n        return torch.sqrt(self.mse(yhat,y))","metadata":{"id":"41717aa5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class NNModel(pl.LightningModule):\n    # https://pytorch-lightning.readthedocs.io/en/stable/notebooks/lightning_examples/basic-gan.html\n    def __init__(self, generator: nn.Module, discriminator: nn.Module):\n        super().__init__()\n        self.generator = generator\n        self.discriminator = discriminator\n        self.criterion_gen = RMSELoss()\n        self.criterion_dis = nn.BCELoss()\n        self.lr = CFG.lr\n\n    def forward(self, val_x, cat_x) -> torch.Tensor:\n        gen = self.generator(val_x, cat_x)\n        return gen\n\n    # Setup Optimizer and Scheduler\n    def configure_optimizers(self):\n        gen_model_params = [p for n, p in self.generator.named_parameters()]\n        dis_model_params = [p for n, p in self.discriminator.named_parameters()]\n        gen_optimizer_params = [\n            {\"params\":  gen_model_params,\n             \"weight_decay\": CFG.weight_decay,\n             \"lr\": 1e-3\n            },\n        ]\n        dis_optimizer_params = [\n            {\"params\":  dis_model_params,\n             \"weight_decay\": CFG.weight_decay,\n             \"lr\": 1e-2\n            },\n        ]\n\n        gen_optimizer = optim.Adam(gen_optimizer_params)\n        dis_optimizer = optim.Adam(dis_optimizer_params)\n\n        gen_scheduler = lr_scheduler.CosineAnnealingLR(dis_optimizer,\n                                                T_max=2000,\n                                                eta_min=1e-5,\n                                                )\n        interval = \"step\"\n\n        dis_scheduler = lr_scheduler.CosineAnnealingLR(gen_optimizer,\n                                                T_max=3000,\n                                                eta_min=1e-5,\n                                                )\n        interval = \"step\"\n\n        return [gen_optimizer, dis_optimizer], [{\"scheduler\": gen_scheduler, \"interval\": interval}, {\"scheduler\": dis_scheduler, \"interval\": interval}]\n\n    # training valid test steps\n    def training_step(self, batch_data, batch_idx, optimizer_idx):\n        # batch_data: DataModuleで定義したtrain_dataloaderの結果\n        # 戻値: lossであることが必須(裏でoptimizerに渡すため)\n        X_val, X_cat, X_na, zeros, ones, i = batch_data\n        # train generator\n        if optimizer_idx == 0:\n            # 生成したデータが本物=zeroと判定されるほど良い\n            gen = self(X_val, X_cat)\n            dis = self.discriminator(gen)\n            g_loss = self.criterion_dis(dis, zeros)\n            rmse = self.criterion_gen(X_na[:, i], gen[:, i])\n            tqdm_dict = {\"g_loss\": g_loss, \"rmse\": rmse}\n            output = OrderedDict({\"loss\": g_loss+rmse, \"progress_bar\": tqdm_dict, \"log\": tqdm_dict})\n        \n        # train discriminator \n        else:  # optimizer_idx == 1:\n            # 生成したデータが偽物=onesと判定されるほど良い\n            # 本物のデータが本物=zerosと判定されるほど良い\n            gen = self(X_val, X_cat)\n            fake_dis = self.discriminator(gen)\n            fake_loss = self.criterion_dis(fake_dis, ones)\n            real_dis = self.discriminator(X_na)\n            real_loss = self.criterion_dis(real_dis, zeros)\n            loss = fake_loss+real_loss\n            tqdm_dict = {\"fake_loss\": fake_loss, \"real_loss\": real_loss}\n            output = OrderedDict({\"loss\": loss, \"progress_bar\": tqdm_dict, \"log\": tqdm_dict})\n\n        return output\n\n    def training_epoch_end(self, outputs):\n        # 1epoch分の処理(全バッチの処理)のreturn値をlistで受け取る\n        rmse_list = [x[0]['log']['rmse'] for x in outputs]   # genとdicの2つが返される\n        g_loss_list = [x[0]['log']['g_loss'] for x in outputs]\n        fake_loss_list = [x[1]['log']['fake_loss'] for x in outputs]\n        real_loss_list = [x[1]['log']['real_loss'] for x in outputs]\n        rmse = torch.stack(rmse_list).mean()\n        g_loss = torch.stack(g_loss_list).mean()\n        fake_loss = torch.stack(fake_loss_list).mean()\n        real_loss = torch.stack(real_loss_list).mean()\n        self.log('rmse', rmse, prog_bar=True)\n        self.log('g_loss', g_loss, prog_bar=True)\n        self.log('fake_loss', fake_loss, prog_bar=True)\n        self.log('real_loss', real_loss, prog_bar=True)\n        if (self.current_epoch+1) % CFG.print_epoch_freq == 0:\n            print(\"epoch:\", self.current_epoch, \"rmse:\", rmse.item(), \"g_loss\", g_loss.item(), \"fake_loss\", fake_loss.item(), \"real_loss\", real_loss.item())\n\n    \"\"\"\n    def validation_step(self, batch_data, batch_idx):\n        # 戻値: 任意の辞書\n        pass\n\n    def validation_epoch_end(self, outputs):\n        pass\n    \"\"\"\n\n    def predict_step(self, batch_data, batch_idx):\n        # 実際に予測させるときに使う\n        X_val, X_cat, X_na, _, _, _ = batch_data\n        outputs = self(X_val, X_cat)\n        outputs = outputs.squeeze()\n        # criterionがwithLogit系の場合は、sigmoidを追加する。\n        # outputs = torch.sigmoid(outputs)\n        return outputs","metadata":{"id":"5e923f02"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CFG:\n    num_workers = 2  # colabは4, kaggleは2\n    weight_decay=0\n    scheduler_type=\"ReduceLR\"\n    print_epoch_freq=1\n    max_epochs=5\n    batch_size=1000\n    lr = 1e-3\n    min_lr = 1e-6\n    loop_end = 20\n    debug = False\n\nif CFG.debug:\n    CFG.max_epochs=1","metadata":{"id":"5893d5c7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoint_path_of = defaultdict(str)","metadata":{"id":"HNWeULNYuVZL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_name_prefix = datetime.datetime.now().strftime('%m%d%H%M%S')\n\nfor loop in range(CFG.loop_end):\n    train = data\n    valid = data\n    result_data = copy.deepcopy(data)\n\n    dm = DataModule(train, valid, cat_col_list, val_col_list, na_col_list, CFG.batch_size)\n\n    # create model\n    pre_model_name = \"model\"+model_name_prefix + \"_\" +str(loop-1)\n    model_name = \"model\"+model_name_prefix + \"_\" +str(loop)\n    dirpath = \"./model/\"\n    gen = Generator(len(val_col_list), len(cat_col_list), len(na_col_list))\n    dis = Discriminator(len(na_col_list))\n    model = NNModel(gen, dis)\n    if checkpoint_path_of[pre_model_name] != \"\":\n        print('read model')\n        checkpint_path = checkpoint_path_of[pre_model_name]\n        model.load_from_checkpoint(checkpoint_path, generator=gen, discriminator=dis)\n        checkpoint = torch.load(checkpoint_path)\n\n    # train\n    logger = WandbLogger()\n    logger.log_hyperparams(CFG.__dict__)\n    callbacks = [\n                # pl.callbacks.EarlyStopping('valid_avg_loss', patience=5),  # validation_epoch_endの戻値が10ターン改善がなかったら打ち止め\n                pl.callbacks.ModelCheckpoint(dirpath=\"./model/\", filename=model_name, save_last=True, save_weights_only=False),  # model保存の設定\n                pl.callbacks.LearningRateMonitor(),  # ログに学習率を吐き出す設定\n    ]\n    trainer = pl.Trainer(accelerator=\"auto\", devices=\"auto\", max_epochs=CFG.max_epochs, logger=logger, callbacks=callbacks, enable_progress_bar=True)\n    trainer.fit(model, datamodule=dm)\n    wandb.finish()\n\n    # load_best_model\n    checkpoint_path = glob.glob(dirpath+model_name+\"*.ckpt\")[0]\n    model.load_from_checkpoint(checkpoint_path, generator=gen, discriminator=dis)\n    checkpoint = torch.load(checkpoint_path)\n    checkpoint_path_of[model_name] = dirpath+model_name+\".ckpt\"\n\n    # predict\n    dm = DataModule(train, valid, cat_col_list, val_col_list, na_col_list, CFG.batch_size)\n    results = trainer.predict(model=model, datamodule=dm)\n    preds = []\n    for batch in results:\n        preds.append(batch)\n    outputs = torch.cat(preds, dim=0)\n\n    # result_data.loc[valid_index, na_col_list] = outputs.tolist()\n    result_data.loc[:, na_col_list] = outputs.tolist()\n\n    # fill_na\n    for col in na_col_list:\n        na_index = na_index_of[col]\n        data.loc[na_index, col] = result_data.loc[na_index, col]\n\n    gc.collect()\n","metadata":{"id":"670c0a6b","outputId":"28c93897-915c-4519-e984-aa9863958e94"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result_data.describe()","metadata":{"id":"iFDKHkYBepIr","outputId":"4cbd4f0e-8770-4d45-e286-4649be4a777c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result_data","metadata":{"id":"ID-HO_iNHh0u","outputId":"e7fde19b-82a5-489a-b457-fff626ed8979"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data","metadata":{"id":"XvLZ1p45kVqD","outputId":"f9cca339-68ec-44d2-d2d1-79693d4413a3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ind_list = []\nval_list = []\nfor i in tqdm(sub.index):\n    row = int(i.split('-')[0])\n    col = i.split('-')[1]\n    val = data[col][row]\n    ind_list.append(i)\n    val_list.append(val)","metadata":{"id":"4c531db5","outputId":"64ea65e5-80c3-4589-a3fd-2a85fbfd9d6a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub['value'].loc[ind_list] = val_list","metadata":{"id":"43c08dc7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub.to_csv(\"submission.csv\", index=True)\nsub","metadata":{"id":"a2c00839","outputId":"6e02e4d6-681a-41d6-b21f-7fceb8048ca8"},"execution_count":null,"outputs":[]}]}