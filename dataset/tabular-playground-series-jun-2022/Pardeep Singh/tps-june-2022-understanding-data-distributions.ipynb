{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import MaxNLocator\nimport warnings\nfrom tqdm import tqdm\nfrom IPython.display import display\nfrom cycler import cycler\n\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\nfrom sklearn.linear_model import LinearRegression, SGDRegressor, LogisticRegression, SGDClassifier\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.model_selection import cross_val_score, train_test_split, cross_val_score, StratifiedKFold\nfrom sklearn.metrics import mean_squared_error\n\nfrom mlxtend.regressor import StackingRegressor\nfrom mlxtend.classifier import StackingClassifier\n\nfrom catboost import CatBoostRegressor, CatBoostClassifier\nfrom xgboost import XGBRegressor, XGBClassifier\n\nsns.set()\n%matplotlib inline\nwarnings.filterwarnings(action='ignore')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-16T05:23:04.274911Z","iopub.execute_input":"2022-06-16T05:23:04.275379Z","iopub.status.idle":"2022-06-16T05:23:06.364523Z","shell.execute_reply.started":"2022-06-16T05:23:04.275286Z","shell.execute_reply":"2022-06-16T05:23:06.363528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def read_data(dir='../input/tabular-playground-series-jun-2022'):\n    return (\n        pd.read_csv(f'{dir}/data.csv'),\n        pd.read_csv(f'{dir}/sample_submission.csv'),\n    )\n\ndef create_submission_file(df, cols=['id', 'target'], file_name='submission.csv'):\n    df[cols].to_csv(f'submissions/{file_name}', index=False)    \n    \ndata_df, submission_df = read_data()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-16T05:23:06.366348Z","iopub.execute_input":"2022-06-16T05:23:06.366769Z","iopub.status.idle":"2022-06-16T05:23:25.924396Z","shell.execute_reply.started":"2022-06-16T05:23:06.366731Z","shell.execute_reply":"2022-06-16T05:23:25.923359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Introduction","metadata":{}},{"cell_type":"markdown","source":"This notebook walks through the dataset and perform high level exploration of different features. Motivation behind this notebook is to understand the dataset at a high level before starting to work on the modeling part.\n\nThis notebook does the followings:\n\n- Explores the different features and their distributions.\n- Explore the correlation between different features.\n- Check the number of null values for each features and at row level.","metadata":{}},{"cell_type":"markdown","source":"## Bird Eye View of the Dataset","metadata":{}},{"cell_type":"code","source":"print(data_df.shape)\nprint(submission_df.shape)\ndisplay(data_df.head(10))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-16T05:23:25.925738Z","iopub.execute_input":"2022-06-16T05:23:25.928924Z","iopub.status.idle":"2022-06-16T05:23:25.970435Z","shell.execute_reply.started":"2022-06-16T05:23:25.928884Z","shell.execute_reply":"2022-06-16T05:23:25.969514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_df.info()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-16T05:23:25.971517Z","iopub.execute_input":"2022-06-16T05:23:25.971954Z","iopub.status.idle":"2022-06-16T05:23:26.152479Z","shell.execute_reply.started":"2022-06-16T05:23:25.971917Z","shell.execute_reply":"2022-06-16T05:23:26.151239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.set_option('display.max_columns', 500)\ndisplay(data_df[data_df.columns[1:]].agg(['mean', 'min', 'max', 'std']))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-16T05:23:26.155408Z","iopub.execute_input":"2022-06-16T05:23:26.155979Z","iopub.status.idle":"2022-06-16T05:23:28.10005Z","shell.execute_reply.started":"2022-06-16T05:23:26.155934Z","shell.execute_reply":"2022-06-16T05:23:28.098946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.set_option('display.max_rows', 500)\ndisplay(data_df[data_df.columns[1:]].agg(['mean', 'min', 'max', 'std']).transpose())","metadata":{"execution":{"iopub.status.busy":"2022-06-16T05:23:28.101773Z","iopub.execute_input":"2022-06-16T05:23:28.102224Z","iopub.status.idle":"2022-06-16T05:23:30.02906Z","shell.execute_reply.started":"2022-06-16T05:23:28.102181Z","shell.execute_reply":"2022-06-16T05:23:30.028358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Level Data Analysis","metadata":{}},{"cell_type":"markdown","source":"### Number of rows with null values for each columns","metadata":{}},{"cell_type":"code","source":"cols_with_null_values = []\ncols_without_null_values = []\nfor col in data_df.columns:\n    null_rows = data_df[data_df[col].isna()].shape[0]\n    if null_rows > 0:\n        cols_with_null_values.append(col)\n        print(f\"{col} column has {null_rows} null rows which is {null_rows * 100 /data_df.shape[0]}% of the data.\")\n    else:\n        cols_without_null_values.append(col)        ","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-16T05:23:30.030091Z","iopub.execute_input":"2022-06-16T05:23:30.031029Z","iopub.status.idle":"2022-06-16T05:23:31.117314Z","shell.execute_reply.started":"2022-06-16T05:23:30.030991Z","shell.execute_reply":"2022-06-16T05:23:31.11642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Out of {data_df.shape[1] - 1} columns, {len(cols_with_null_values)} columns has null values.\\n\")\nprint(\"Columns without null values:\\n\\n\", cols_without_null_values, \"\\n\")\nprint(\"Columns with null values:\\n\\n\", cols_with_null_values, \"\\n\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-16T05:23:31.118668Z","iopub.execute_input":"2022-06-16T05:23:31.118983Z","iopub.status.idle":"2022-06-16T05:23:31.124339Z","shell.execute_reply.started":"2022-06-16T05:23:31.118954Z","shell.execute_reply":"2022-06-16T05:23:31.123526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 55 float features","metadata":{}},{"cell_type":"markdown","source":"#### Features starting with `F_1_*`","metadata":{}},{"cell_type":"code","source":"sns.set_palette('RdBu')\nfeature_f_1_columns = [\n    f\n    for f in data_df.columns\n    if data_df[f].dtype == 'float64' and f.startswith('F_1_')\n]\n\nprint(f\"{len(feature_f_1_columns)} columns are float columns starting with F_1_*.\")\n\nfig, axs = plt.subplots(\n    5, 3, figsize=(20, 12)\n)\n\nfor f, ax in zip(feature_f_1_columns, axs.ravel()):\n    ax.hist(\n        data_df[f], density=True, bins=100,\n    )\n    ax.set_title(\n        f\"{f}: mean={data_df[f].mean():.1f}, std={data_df[f].std():.1f}\"\n    )\nplt.tight_layout()\nplt.suptitle(\n    \"Histograms of the float features starting with F_1_*\",\n    y=1.02,\n    fontsize=20\n)\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-16T05:23:31.125809Z","iopub.execute_input":"2022-06-16T05:23:31.126118Z","iopub.status.idle":"2022-06-16T05:23:37.262025Z","shell.execute_reply.started":"2022-06-16T05:23:31.126089Z","shell.execute_reply":"2022-06-16T05:23:37.261273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12, 8))\nsns.heatmap(\n    data_df[\n        feature_f_1_columns\n    ].corr(),\n    cmap='coolwarm',\n    center=0,\n    annot=True,\n    fmt='.2f'\n)\nplt.suptitle(\n    'Correlation in F_1_* features',\n    y=1.02,\n    fontsize=20\n)\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-16T05:23:37.262875Z","iopub.execute_input":"2022-06-16T05:23:37.263228Z","iopub.status.idle":"2022-06-16T05:23:39.200342Z","shell.execute_reply.started":"2022-06-16T05:23:37.263198Z","shell.execute_reply":"2022-06-16T05:23:39.199521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations**\n\n- All features apart from `F_1_7, F_1_12, F_1_13` are normally distributed, are centered around 0(`Mean = 0`) and have `Standard Deviation = 1`.\n- `F_1_7, F_1_12, F_1_13` features have `Standard Deviation = .7` and `Mean = -0.1`.","metadata":{}},{"cell_type":"markdown","source":"#### Features starting with `F_3_*`","metadata":{}},{"cell_type":"code","source":"sns.set_palette('RdBu')\n\nfeature_f_3_columns = [\n    f\n    for f in data_df.columns\n    if data_df[f].dtype == 'float64' and f.startswith('F_3_')\n]\n\nprint(f\"{len(feature_f_3_columns)} columns are float columns starting with F_3_*.\")\n\nfig, axs = plt.subplots(\n    5, 5, figsize=(30, 16)\n)\n\nfor f, ax in zip(feature_f_3_columns, axs.ravel()):\n    ax.hist(\n        data_df[f], density=True, bins=100\n    )\n    ax.set_title(\n        f\"{f}: mean={data_df[f].mean():.1f}, std={data_df[f].std():.1f}\"\n    )\nplt.suptitle(\n    \"Histograms of the float features starting with F_3_*\",\n    y=1.02,\n    fontsize=20\n)\nplt.tight_layout()\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-16T05:23:39.20186Z","iopub.execute_input":"2022-06-16T05:23:39.202273Z","iopub.status.idle":"2022-06-16T05:23:49.28089Z","shell.execute_reply.started":"2022-06-16T05:23:39.202244Z","shell.execute_reply":"2022-06-16T05:23:49.280108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20, 8))\nsns.heatmap(\n    data_df[\n        feature_f_3_columns\n    ].corr(),\n    cmap='coolwarm',\n    center=0,\n    annot=True,\n    fmt='.2f'\n)\nplt.suptitle(\n    'Correlation in F_3_* features',\n    y=1.02,\n    fontsize=20\n)\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-16T05:23:49.281921Z","iopub.execute_input":"2022-06-16T05:23:49.282497Z","iopub.status.idle":"2022-06-16T05:23:54.161401Z","shell.execute_reply.started":"2022-06-16T05:23:49.282443Z","shell.execute_reply":"2022-06-16T05:23:54.160174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations**\n\n- All features apart from `F_3_19, F_3_21` are normally distributed, are centered around 0(`Mean = 0`) and have `Standard Deviation = 1`.\n- `F_3_19, F_3_21` features have `Standard Deviation = 0.7` and `Mean = -0.1`.","metadata":{}},{"cell_type":"markdown","source":"#### Features starting with `F_4_*`","metadata":{}},{"cell_type":"code","source":"sns.set_palette('RdBu')\n\nfeature_f_4_columns = [\n    f\n    for f in data_df.columns\n    if data_df[f].dtype == 'float64' and f.startswith('F_4_')\n]\n\nprint(f\"{len(feature_f_4_columns)} columns are float columns starting with F_4_*.\")\n\nfig, axs = plt.subplots(\n    5, 3, figsize=(20, 12)\n)\n\nfor f, ax in zip(feature_f_4_columns, axs.ravel()):\n    ax.hist(\n        data_df[f], density=True, bins=100\n    )\n    ax.set_title(\n        f\"{f}: mean={data_df[f].mean():.1f}, std={data_df[f].std():.1f}\"\n    )\nplt.tight_layout()\nplt.suptitle(\n    \"Histograms of the float features starting with F_4_*\",\n    y=1.02,\n    fontsize=20\n)\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-16T05:23:54.162749Z","iopub.execute_input":"2022-06-16T05:23:54.163169Z","iopub.status.idle":"2022-06-16T05:24:00.30995Z","shell.execute_reply.started":"2022-06-16T05:23:54.163135Z","shell.execute_reply":"2022-06-16T05:24:00.308863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12, 8))\nsns.heatmap(\n    data_df[\n        feature_f_4_columns\n    ].corr(),\n    cmap='coolwarm',\n    center=0,\n    annot=True,\n    fmt='.2f'\n)\nplt.suptitle('Correlation in F_4_* features')\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-16T05:24:00.313511Z","iopub.execute_input":"2022-06-16T05:24:00.314493Z","iopub.status.idle":"2022-06-16T05:24:02.258034Z","shell.execute_reply.started":"2022-06-16T05:24:00.314426Z","shell.execute_reply":"2022-06-16T05:24:02.25697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations**\n\n- Features with `F_4_*` has the most variation.\n- Features `F_4_1, F_4_4, F_4_5, F_4_7, F_4_12, F_4_13` has `Mean = 0.3 or -0.3` and `Standard Deviation=2.4`.\n- Other features has `Mean as -0.2, -0.1, 0.0, 0.3, 0.6` and `Standard Deviation as 0.7, 0.8, 2.3, 5.0`","metadata":{}},{"cell_type":"markdown","source":"### Int Feature `F_2_*`","metadata":{}},{"cell_type":"code","source":"feature_f_2_columns = [\n    f\n    for f in data_df\n    if data_df[f].dtype == 'int64' and f.startswith('F_2_')\n]\nprint(f\"{len(feature_f_2_columns)} columns are int columns.\")\n\nfigure = plt.figure(figsize=(30, 16))\nfor i, f in enumerate(feature_f_2_columns):\n    plt.subplot(5, 5, i + 1)\n    ax = plt.gca()\n    vc = data_df[f].value_counts()\n    ax.bar(vc.index, vc)\n    ax.set_xlabel(f'{f}: mean={data_df[f].mean():.1f}, std={data_df[f].std():.1f}')\n    ax.xaxis.set_major_locator(\n        MaxNLocator(integer=True)\n    )\nplt.suptitle(\n    'Histograms of the float features starting with F_2_*',\n    y=1.02,\n    fontsize=20\n)\nfigure.tight_layout()\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-16T05:24:02.259311Z","iopub.execute_input":"2022-06-16T05:24:02.259668Z","iopub.status.idle":"2022-06-16T05:24:07.965598Z","shell.execute_reply.started":"2022-06-16T05:24:02.259635Z","shell.execute_reply":"2022-06-16T05:24:07.96493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20, 8))\nsns.heatmap(\n    data_df[\n        feature_f_2_columns\n    ].corr(),\n    cmap='coolwarm',\n    center=0,\n    annot=True,\n    fmt='.2f'\n)\nplt.suptitle('Correlation in F_2_* features')\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-16T05:24:07.96671Z","iopub.execute_input":"2022-06-16T05:24:07.967121Z","iopub.status.idle":"2022-06-16T05:24:12.845397Z","shell.execute_reply.started":"2022-06-16T05:24:07.967083Z","shell.execute_reply":"2022-06-16T05:24:12.844572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations**\n\n- `Mean` is in `[0.7, 3.3] range`.\n- `Standard Deviation` is in `[1.0, 2.0] range`.","metadata":{}},{"cell_type":"markdown","source":"## Features with Null Values\n\n- Find out rows where only 1 feature is null\n- Find out rows where more than 1 features are null","metadata":{}},{"cell_type":"code","source":"float_features = [\n    f\n    for f in data_df\n    if data_df[f].dtype == 'float64' and f != 'row_id'\n]\ndisplay(data_df[data_df.isnull().any(axis=1)][float_features].isnull().sum(axis=1).value_counts())","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-16T05:24:12.846428Z","iopub.execute_input":"2022-06-16T05:24:12.847236Z","iopub.status.idle":"2022-06-16T05:24:13.233033Z","shell.execute_reply.started":"2022-06-16T05:24:12.8472Z","shell.execute_reply":"2022-06-16T05:24:13.232056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set()\n\nfig = plt.figure(figsize=(24, 12))\n\nsns.set_style(\"darkgrid\")\n\nax = sns.countplot(\n    x=data_df[data_df.isnull().any(axis=1)][float_features].isnull().sum(axis=1), \n    palette='RdBu'\n)\nax.bar_label(\n    ax.containers[0],\n    padding=2\n)\nfig.suptitle(\n    'Number of null values per row',\n    horizontalalignment='center', verticalalignment='bottom', fontsize=15,\n    y=.9\n);","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-16T05:24:13.234509Z","iopub.execute_input":"2022-06-16T05:24:13.234885Z","iopub.status.idle":"2022-06-16T05:24:13.991835Z","shell.execute_reply.started":"2022-06-16T05:24:13.234854Z","shell.execute_reply":"2022-06-16T05:24:13.990859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations**\n\n- There are `635226` rows with null values out of `1000000` total rows.\n- Most of the rows either has 1 or 2 null values. 2 rows has 9 null values as well.","metadata":{}},{"cell_type":"markdown","source":"## Credits","metadata":{}},{"cell_type":"markdown","source":"- This notebook would have been possible without all the great people who have shared amazing notebooks in the previous Playground Series competition.\n- I have learned alot reading and reproducing [@ambrosm](https://www.kaggle.com/ambrosm) notebooks from this year playground series competitions. Float and Integer values exploration is based on @ambrosm [May Playground EDA Notbook](https://www.kaggle.com/code/ambrosm/tpsmay22-eda-which-makes-sense) notebook, do check that out for more intresting techniques to explore this type of dataset.\n\n\n**Thanks for reading this notebook, feedbacks and comments are welcome. Enjoy this competition and Keep on Learing...**","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}}]}