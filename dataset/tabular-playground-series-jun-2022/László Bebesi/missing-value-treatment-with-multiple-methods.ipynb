{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nfrom sklearn.impute import KNNImputer\nfrom sklearn.preprocessing import StandardScaler\nimport gc\nfrom multiprocessing import Pool\nfrom functools import partial\nimport seaborn as sns\nimport matplotlib.pyplot as plt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-10T08:38:43.079069Z","iopub.execute_input":"2022-06-10T08:38:43.081143Z","iopub.status.idle":"2022-06-10T08:38:44.021743Z","shell.execute_reply.started":"2022-06-10T08:38:43.081038Z","shell.execute_reply":"2022-06-10T08:38:44.02076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 0: Helper functions","metadata":{}},{"cell_type":"code","source":"def set_missing_values(df, incomplete_variables, missing_ratio,random_state=42):\n    \"\"\"\n    To be used for cross validation, parameter tunning etc.\n    \n    Inputs:\n        df : pd.DataFrame\n            Dataframe to be enriched with missing values\n        incomplete_variables : list\n            List of variables subject to missing value generation \n        missing_ratio : float\n            Ratio of missing variables e.g.: 0.0187 translates to 1.87% per col\n        random_state : int\n            Random state value in integer for reproducibility\n    Returns:\n        pd.DataFrame\n            Dataframe with missing col values\n    \"\"\"\n    for col in incomplete_variables:\n        df.loc[df.sample(frac=missing_ratio,random_state=random_state).index, col] = np.nan\n        # I introduce this to give some perturbation to my random states to avoid cyclicality\n        # While I still maintain reproducibility\n        random_state=random_state * 6397 % 7919\n    return df\n\ndef generate_missing_ids(df,incomplete_variables):\n    \"\"\"\n    To be used to determine missing col-row values\n    \n    Inputs:\n        df : pd.DataFrame\n            Dataframe to be enriched with missing values\n        incomplete_variables : list\n            List of variables subject to missing value generation \n    Returns:\n        pd.DataFrame\n            Dataframe with missing row-col values\n    \"\"\"\n    missing_values=list()\n    for col in incomplete_variables:\n        missing_values=missing_values+[str(l) + \"_\" + col for l in list(df[df[col].isnull()].index)]\n    return pd.DataFrame(missing_values,columns=[\"row-col\"])\n\ndef grab_values(df, missing_ids, incomplete_variables, variable_name=\"predict\"):\n    \"\"\"\n    To be used to get missing col-row values from tables that contain the prediction\n    \n    Inputs:\n        df : pd.DataFrame\n            Dataframe that has the predictions\n        missing_ids: pd.DataFrame\n            Dataframe that contains the list of missing row-col pairs\n        incomplete_variables : list\n            List of variables subject to missing value generation \n        variable_name : str\n            Variable name for predictions\n    Returns:\n        pd.DataFrame\n            Dataframe with the fill missed fields\n    \"\"\"\n    missing_values=list(missing_ids[\"row-col\"])\n    summary_df=pd.DataFrame()\n    for col in incomplete_variables:\n        condition=(df.index+\"_\"+col).isin(missing_values)\n        tdf=df.loc[condition]\n        summary_df=summary_df.append(pd.DataFrame({\n            \"row-col\": tdf.index+\"_\"+col,\n            variable_name: tdf[col],\n        }))\n    return summary_df\n\ndef fill_miss_means(df,incomplete_variables):\n    \"\"\"\n    To be used to get missing col-row values from tables that contain the prediction\n    \n    Inputs:\n        df : pd.DataFrame\n            Dataframe that has the predictions\n        incomplete_variables : list\n            List of variables subject to missing value generation \n    Returns:\n        No return, the original dataframe is modified and enriched with predictions\n    \"\"\"\n    for col in incomplete_variables:\n        mean_temp=np.nanmean(df[col])\n        df.loc[df[col].isnull()]=mean_temp\n        \ndef get_rmse(all_together, v1=\"realized\", v2=\"forecast\", to_print=False):\n    \"\"\"\n    Calculates RMSE for standard output\n    \n    Input:\n        all_together: pd.DataFrame\n            Df containing the forecast\n        v1: str\n            String for the name of variable e.g. realized\n        v2: str\n            String for the name of variable e.g. predict or forecast\n    \n    Returns:\n        rmse: float\n            Returns the float containign the RMSE\n    \"\"\"\n    all_together[\"diff\"]=(all_together[v1]-all_together[v2])**2\n    rmse=np.sqrt(np.nansum((all_together[v1]-all_together[v2])**2)/len(all_together))\n    if to_print:\n        print(\"RMSE : \" + str(round(rmse,2)))\n    return rmse, all_together[[\"row-col\",\"diff\"]]\n\ndef plot_rmse(rmse_list, title=\"\"):\n    plt.hist(rmse_list,color=\"#BA55D3\");\n    plt.title(\"RMSE values in cross validation\"+title);\n    print(\"Mean: \"+str(round(np.mean(rmse_list),4)))\n    print(\"Std: \"+str(round(np.std(rmse_list),4)))\n    print(\"Min: \"+str(round(np.min(rmse_list),4)))\n    print(\"Max: \"+str(round(np.max(rmse_list),4)))\n    print(\"Folds: \"+str(len(rmse_list)))","metadata":{"execution":{"iopub.status.busy":"2022-06-10T10:34:01.506399Z","iopub.execute_input":"2022-06-10T10:34:01.506904Z","iopub.status.idle":"2022-06-10T10:34:01.530912Z","shell.execute_reply.started":"2022-06-10T10:34:01.506863Z","shell.execute_reply":"2022-06-10T10:34:01.529712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def single_perform_cross_validation(random_state,\n                                    data_complete,\n                                    incomplete_variables,\n                                    missing_ratio,\n                                    all_variables,\n                                    fill_miss_engine=None,\n                                    subsample=0.1,\n                                    **kwargs):\n        random_state=random_state*100\n        # 0. Use complete data and generate test data\n        if subsample is not None:\n            data_complete_small=data_complete.sample(frac=subsample, random_state=1)\n        else:\n            data_complete_small=data_complete.copy()\n\n        data_pipeline=data_complete_small.copy()\n        data_pipeline=set_missing_values(data_pipeline,\n                                          incomplete_variables,\n                                          missing_ratio,\n                                          random_state=random_state)\n        # 1. get missing ids\n        missing_ids=generate_missing_ids(data_pipeline,incomplete_variables)\n\n        # 2. get realized data\n        value_realized=grab_values(df=data_complete_small,\n                               missing_ids=missing_ids,\n                               incomplete_variables=incomplete_variables,variable_name=\"realized\")\n        # 3. perform prediction\n        fill_miss_engine(data_pipeline, incomplete_variables, **kwargs)\n\n        # 4. get predicted data in format\n        value_forc=grab_values(df=data_pipeline,\n                               missing_ids=missing_ids,\n                               incomplete_variables=incomplete_variables,variable_name=\"forecast\")\n        # 5. evaluate\n        all_together=value_forc.merge(value_realized,how=\"outer\",on=\"row-col\")\n        return get_rmse(all_together)\n\ndef perform_cross_validation_parallel(data_complete,\n                                         incomplete_variables,\n                                         missing_ratio,\n                                         all_variables,\n                                         fill_miss_engine=None,\n                                         subsample=0.1,\n                                         repeat=3,\n                                         processes=1,\n                                         **kwargs,\n                                        ):\n    \"\"\"\n    Performs cross validation using a prespecified engine\n    \n    Inputs:\n        data_complete : pd.DataFrame\n            Dataframe to be enriched with missing values\n        incomplete_variables : list\n            List of variables subject to missing value generation \n        missing_ratio : float\n            Ratio of missing variables e.g.: 0.0187 translates to 1.87% per col\n        all_variables : list\n            List of ALL variables in the dataframe \n        subsample : scalar\n            Set subsample to use only a certain ratio of the data e.g. 0.1=10%\n        repeat : int\n            Number of repeats with different random_state\n        fill_miss_engine : function\n            The engine expects the data_complete and the all_variables as the input\n        processes : int\n            Number of processes to be used in pool\n    Returns:\n        list\n            Dataframe containing cross validation rmse values\n    \"\"\"   \n    \n    with Pool(processes=processes) as pool: \n        rmse_list=pool.map(\n                          partial(single_perform_cross_validation, \n                                 data_complete=data_complete,\n                                 incomplete_variables=incomplete_variables,\n                                 missing_ratio=missing_ratio,\n                                 all_variables=all_variables,\n                                 fill_miss_engine=fill_miss_engine,\n                                 subsample=subsample,\n                                 **kwargs\n                                 ), range(1,repeat+1) )\n    \n    rmse_values=list()\n    df_list=list()\n    for elem in rmse_list:\n        rmse_values.append(elem[0])\n        df_list.append(elem[1])\n    df_final=pd.concat(df_list,axis=0)\n    return rmse_values, df_final","metadata":{"execution":{"iopub.status.busy":"2022-06-10T08:38:44.045643Z","iopub.execute_input":"2022-06-10T08:38:44.046009Z","iopub.status.idle":"2022-06-10T08:38:44.063096Z","shell.execute_reply.started":"2022-06-10T08:38:44.045978Z","shell.execute_reply":"2022-06-10T08:38:44.062245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 1: Read data and explore","metadata":{}},{"cell_type":"code","source":"# display sample submission to see the result format:\nsample_submit=pd.read_csv(\"/kaggle/input/tabular-playground-series-jun-2022/sample_submission.csv\")\ndisplay(sample_submit.head())","metadata":{"execution":{"iopub.status.busy":"2022-06-10T08:38:44.064893Z","iopub.execute_input":"2022-06-10T08:38:44.065316Z","iopub.status.idle":"2022-06-10T08:38:45.039894Z","shell.execute_reply.started":"2022-06-10T08:38:44.065286Z","shell.execute_reply":"2022-06-10T08:38:45.038693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data0=pd.read_csv(\"/kaggle/input/tabular-playground-series-jun-2022/data.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-06-10T08:38:45.041102Z","iopub.execute_input":"2022-06-10T08:38:45.041869Z","iopub.status.idle":"2022-06-10T08:39:04.987064Z","shell.execute_reply.started":"2022-06-10T08:38:45.041771Z","shell.execute_reply":"2022-06-10T08:39:04.985987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(data0.head())","metadata":{"execution":{"iopub.status.busy":"2022-06-10T08:39:04.988751Z","iopub.execute_input":"2022-06-10T08:39:04.989203Z","iopub.status.idle":"2022-06-10T08:39:05.020263Z","shell.execute_reply.started":"2022-06-10T08:39:04.989159Z","shell.execute_reply":"2022-06-10T08:39:05.019207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(data0.describe().T)","metadata":{"execution":{"iopub.status.busy":"2022-06-10T08:39:05.021802Z","iopub.execute_input":"2022-06-10T08:39:05.022233Z","iopub.status.idle":"2022-06-10T08:39:10.330917Z","shell.execute_reply.started":"2022-06-10T08:39:05.022198Z","shell.execute_reply":"2022-06-10T08:39:10.329707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(data0.info())","metadata":{"execution":{"iopub.status.busy":"2022-06-10T08:39:10.332277Z","iopub.execute_input":"2022-06-10T08:39:10.333257Z","iopub.status.idle":"2022-06-10T08:39:10.578255Z","shell.execute_reply.started":"2022-06-10T08:39:10.33322Z","shell.execute_reply":"2022-06-10T08:39:10.577276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### -> Around 2 % of the records are mising in each column","metadata":{}},{"cell_type":"code","source":"all_variables=list(data0.columns[1:])\ncomplete_variables=[var for var in all_variables if var.startswith(\"F_2\")]\nincomplete_variables=[var for var in all_variables if not var.startswith(\"F_2\")]\n# Save a complete table without any missing\ndata_complete=data0.copy().dropna(axis=\"rows\")\nprint(\"Rows without any missing values: \"+str(len(data_complete)))\nmissing_ratio=1-0.9815","metadata":{"execution":{"iopub.status.busy":"2022-06-10T08:39:10.579471Z","iopub.execute_input":"2022-06-10T08:39:10.579858Z","iopub.status.idle":"2022-06-10T08:39:11.633288Z","shell.execute_reply.started":"2022-06-10T08:39:10.579827Z","shell.execute_reply":"2022-06-10T08:39:11.632305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_complete[\"row_id\"]=data_complete[\"row_id\"].astype(str)\ndata_complete.set_index(keys=[\"row_id\"],inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-10T08:39:11.638692Z","iopub.execute_input":"2022-06-10T08:39:11.639061Z","iopub.status.idle":"2022-06-10T08:39:12.130982Z","shell.execute_reply.started":"2022-06-10T08:39:11.639029Z","shell.execute_reply":"2022-06-10T08:39:12.129987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### -> Non missing observations are around 364 thousand (36,4% of total)\n* <font size=\"4\"> We have 55 columns with missing values, each column have approximately 2% missing\")</font>\n* <font size=\"4\"> The probability that a row has no missing at all is (1-2%)^55 which is more or less around 30-40%\")</font>\n* <font size=\"4\"> So at first glance no correlation between the missing values of the observations</font>\n* <font size=\"4\"> We might change our view later once we see more interaction between variables</font>\n","metadata":{"execution":{"iopub.status.busy":"2022-06-06T18:57:06.156288Z","iopub.execute_input":"2022-06-06T18:57:06.157072Z","iopub.status.idle":"2022-06-06T18:57:06.161718Z","shell.execute_reply.started":"2022-06-06T18:57:06.15703Z","shell.execute_reply":"2022-06-06T18:57:06.160843Z"}}},{"cell_type":"markdown","source":"# Step 2: Naive approach with simple means","metadata":{}},{"cell_type":"code","source":"%%time\nrmse_list_means, mse_df_means=perform_cross_validation_parallel(data_complete=data_complete,\n                                                                 incomplete_variables=incomplete_variables,\n                                                                 missing_ratio=missing_ratio,\n                                                                 all_variables=all_variables,\n                                                                 fill_miss_engine=fill_miss_means,\n                                                                 subsample=0.1,  \n                                                                 repeat=24,\n                                                                 processes=4             \n                                                                 )","metadata":{"execution":{"iopub.status.busy":"2022-06-10T08:39:12.132268Z","iopub.execute_input":"2022-06-10T08:39:12.132639Z","iopub.status.idle":"2022-06-10T08:40:23.081221Z","shell.execute_reply.started":"2022-06-10T08:39:12.13259Z","shell.execute_reply":"2022-06-10T08:40:23.079998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_rmse(rmse_list_means, title=\" (Simple Means)\")","metadata":{"execution":{"iopub.status.busy":"2022-06-10T10:34:06.749432Z","iopub.execute_input":"2022-06-10T10:34:06.749858Z","iopub.status.idle":"2022-06-10T10:34:06.941196Z","shell.execute_reply.started":"2022-06-10T10:34:06.749825Z","shell.execute_reply":"2022-06-10T10:34:06.940409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font size=\"4\"> Notes: the scores observed here are around the mean benchmark which is 1.41613 </font>","metadata":{}},{"cell_type":"markdown","source":"# Step 3: KNN imputer with the wrapper function","metadata":{}},{"cell_type":"markdown","source":"<font size=\"4\"><b> Explanation in steps </font></b><br>\n<font size=\"3\">1. We going to create distances which are sensitive to scaling, hence we standardize </font><br>\n<font size=\"3\">2. KNNImputer identifies the \"n_neighbors\" number of nearest neighbors </font><br>\n<font size=\"3\">3. The imputed value will be the weighted average of the \"n_neighbors\" points feature values, these neighbors have non missing feature exposures </font><br>\n<font size=\"3\">4. We can have multiple weighting schemas, the default is 'uniform' (simple average), the 'distance' option sets the weights inversely proportional to the distance, we can also add our own wieghting function </font><br>\n<font size=\"3\">5. Finally we need to scale back our data to the original scale </font>\n\n<a href=\"https://scikit-learn.org/stable/modules/impute.html#knnimpute\">Please read more in KNNImputer documentation</a>","metadata":{"execution":{"iopub.status.busy":"2022-06-09T18:30:57.018284Z","iopub.execute_input":"2022-06-09T18:30:57.018788Z","iopub.status.idle":"2022-06-09T18:30:57.027266Z","shell.execute_reply.started":"2022-06-09T18:30:57.018748Z","shell.execute_reply":"2022-06-09T18:30:57.025523Z"}}},{"cell_type":"code","source":"def KNN_engine(data_pipeline, all_variables, **kwargs):\n    scaler = StandardScaler()\n    data_pipeline.loc[:,all_variables]=scaler.fit_transform(data_pipeline.loc[:,all_variables])\n    imputer = KNNImputer(**kwargs)\n    data_pipeline.loc[:,all_variables] = imputer.fit_transform(data_pipeline.loc[:,all_variables])\n    data_pipeline.loc[:,all_variables] = scaler.inverse_transform(data_pipeline.loc[:,all_variables])\n    gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-06-10T08:40:23.348194Z","iopub.execute_input":"2022-06-10T08:40:23.348672Z","iopub.status.idle":"2022-06-10T08:40:23.356272Z","shell.execute_reply.started":"2022-06-10T08:40:23.348611Z","shell.execute_reply":"2022-06-10T08:40:23.355084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nknn1_rmse_list,_ =perform_cross_validation_parallel(data_complete=data_complete,\n                                 incomplete_variables=incomplete_variables,\n                                 missing_ratio=missing_ratio,\n                                 all_variables=all_variables,\n                                 fill_miss_engine=KNN_engine,\n                                 subsample=0.04,  \n                                 repeat=12,\n                                 processes=4,               \n                                 n_neighbors=20,\n                                 weights = 'distance')","metadata":{"execution":{"iopub.status.busy":"2022-06-10T08:40:23.357869Z","iopub.execute_input":"2022-06-10T08:40:23.358343Z","iopub.status.idle":"2022-06-10T08:45:23.190227Z","shell.execute_reply.started":"2022-06-10T08:40:23.358299Z","shell.execute_reply":"2022-06-10T08:45:23.187915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_rmse(knn1_rmse_list, title=\" (KNN distance weighted)\")","metadata":{"execution":{"iopub.status.busy":"2022-06-10T10:34:11.169065Z","iopub.execute_input":"2022-06-10T10:34:11.169504Z","iopub.status.idle":"2022-06-10T10:34:11.393562Z","shell.execute_reply.started":"2022-06-10T10:34:11.169468Z","shell.execute_reply":"2022-06-10T10:34:11.392559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nknn2_rmse_list,_ =perform_cross_validation_parallel(data_complete=data_complete,\n                                 incomplete_variables=incomplete_variables,\n                                 missing_ratio=missing_ratio,\n                                 all_variables=all_variables,\n                                 fill_miss_engine=KNN_engine,\n                                 subsample=0.04,  \n                                 repeat=12,\n                                 processes=4,               \n                                 n_neighbors=20,\n                                 weights = 'uniform')","metadata":{"execution":{"iopub.status.busy":"2022-06-10T08:45:23.422263Z","iopub.execute_input":"2022-06-10T08:45:23.422701Z","iopub.status.idle":"2022-06-10T08:50:33.677653Z","shell.execute_reply.started":"2022-06-10T08:45:23.422671Z","shell.execute_reply":"2022-06-10T08:50:33.676362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_rmse(knn2_rmse_list, title=\" (KNN uniform weighted)\")","metadata":{"execution":{"iopub.status.busy":"2022-06-10T10:34:16.232507Z","iopub.execute_input":"2022-06-10T10:34:16.233679Z","iopub.status.idle":"2022-06-10T10:34:16.454662Z","shell.execute_reply.started":"2022-06-10T10:34:16.233632Z","shell.execute_reply":"2022-06-10T10:34:16.453682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 4: Iterative imputer with Simple OLS","metadata":{}},{"cell_type":"markdown","source":"<font size=\"4\"><b> Explanation in steps </font></b><br>\n<font size=\"3\">0. The framework if built in a flexible way, so it can use multiple estimators</font><br>\n<font size=\"3\">1. We going to standardize the data with a scaler, which will be handy if regularization is used</font><br>\n<font size=\"3\">2. IterativeImputer estimates the missing values for a given feature by using the other features as explanatory variables </font><br>\n<font size=\"3\">3. When a model for a feature is estimated the missing values are estimated based on that model and the iteration proceeds to the next feature </font><br>\n<font size=\"3\">4.  When a new iteration starts (all features were fill missed at least once) the estimation is performed using the newly perfomed fill missed data as well, and the estimates are refined for the missing values using the new estimates </font><br>\n<font size=\"3\">5. The iteration goes on, the default \"max_iter\" param, hence the number of iterations is 10. </font><br>\n<font size=\"3\">6. Finally we need to scale back our data to the original scale </font>\n\n<a href=\"https://scikit-learn.org/stable/modules/impute.html#iterative-imputer\">Please read more in IterativeImputer documentation</a>","metadata":{}},{"cell_type":"code","source":"from sklearn.experimental import enable_iterative_imputer\n#We need to add the above import so the iterativeimputer can work\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom lightgbm import LGBMRegressor","metadata":{"execution":{"iopub.status.busy":"2022-06-10T08:50:33.908374Z","iopub.execute_input":"2022-06-10T08:50:33.908821Z","iopub.status.idle":"2022-06-10T08:50:35.009963Z","shell.execute_reply.started":"2022-06-10T08:50:33.908783Z","shell.execute_reply":"2022-06-10T08:50:35.00886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def IterativeImputerBase(data_pipeline, all_variables, **kwargs):\n    scaler = StandardScaler()\n    data_pipeline.loc[:,all_variables]=scaler.fit_transform(data_pipeline.loc[:,all_variables])\n    imputer = IterativeImputer(random_state=42,**kwargs)\n    data_pipeline.loc[:,all_variables] = imputer.fit_transform(data_pipeline.loc[:,all_variables])\n    data_pipeline.loc[:,all_variables] = scaler.inverse_transform(data_pipeline.loc[:,all_variables])\n    gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-06-10T08:50:35.011527Z","iopub.execute_input":"2022-06-10T08:50:35.011957Z","iopub.status.idle":"2022-06-10T08:50:35.019238Z","shell.execute_reply.started":"2022-06-10T08:50:35.01192Z","shell.execute_reply":"2022-06-10T08:50:35.018114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nols_rmse_list, ols_rmse_df =perform_cross_validation_parallel(data_complete=data_complete,\n                                 incomplete_variables=incomplete_variables,\n                                 missing_ratio=missing_ratio,\n                                 all_variables=all_variables,\n                                 fill_miss_engine=IterativeImputerBase,\n                                 subsample=0.02,  \n                                 repeat=12,\n                                 processes=4,\n                                 estimator=LinearRegression(),\n                                 max_iter=30,\n                                 )","metadata":{"execution":{"iopub.status.busy":"2022-06-10T09:03:54.029878Z","iopub.execute_input":"2022-06-10T09:03:54.030279Z","iopub.status.idle":"2022-06-10T09:15:55.327144Z","shell.execute_reply.started":"2022-06-10T09:03:54.030248Z","shell.execute_reply":"2022-06-10T09:15:55.325873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_rmse(ols_rmse_list, title=\" (Iterative OLS)\")","metadata":{"execution":{"iopub.status.busy":"2022-06-10T10:34:21.140756Z","iopub.execute_input":"2022-06-10T10:34:21.141225Z","iopub.status.idle":"2022-06-10T10:34:21.344026Z","shell.execute_reply.started":"2022-06-10T10:34:21.141182Z","shell.execute_reply":"2022-06-10T10:34:21.342849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 5: Iterative imputer with DecisionTreeRegressor and LGBMRegressor","metadata":{}},{"cell_type":"code","source":"%%time\ncb_rmse_list, cb_rmse_df =perform_cross_validation_parallel(data_complete=data_complete,\n                                 incomplete_variables=incomplete_variables,\n                                 missing_ratio=missing_ratio,\n                                 all_variables=all_variables,\n                                 fill_miss_engine=IterativeImputerBase,\n                                 subsample=0.02,  \n                                 repeat=12,\n                                 processes=4,\n                                 estimator=DecisionTreeRegressor(random_state=11),\n                                 max_iter=10,\n                                 )","metadata":{"execution":{"iopub.status.busy":"2022-06-10T09:15:55.537859Z","iopub.execute_input":"2022-06-10T09:15:55.538192Z","iopub.status.idle":"2022-06-10T09:46:32.186407Z","shell.execute_reply.started":"2022-06-10T09:15:55.538162Z","shell.execute_reply":"2022-06-10T09:46:32.185174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_rmse(cb_rmse_list, title=\" (Iterative RegressionTree)\")","metadata":{"execution":{"iopub.status.busy":"2022-06-10T10:34:26.639345Z","iopub.execute_input":"2022-06-10T10:34:26.640233Z","iopub.status.idle":"2022-06-10T10:34:26.811754Z","shell.execute_reply.started":"2022-06-10T10:34:26.640174Z","shell.execute_reply":"2022-06-10T10:34:26.810811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nlgbm_rmse_list, lgbm_rmse_df =perform_cross_validation_parallel(data_complete=data_complete,\n                                 incomplete_variables=incomplete_variables,\n                                 missing_ratio=missing_ratio,\n                                 all_variables=all_variables,\n                                 fill_miss_engine=IterativeImputerBase,\n                                 subsample=0.02,  \n                                 repeat=12,\n                                 processes=2,\n                                 estimator=LGBMRegressor(random_state=11,n_estimators=10),\n                                 max_iter=2,\n                                 )","metadata":{"execution":{"iopub.status.busy":"2022-06-10T10:54:01.243543Z","iopub.execute_input":"2022-06-10T10:54:01.244018Z","iopub.status.idle":"2022-06-10T11:42:30.783281Z","shell.execute_reply.started":"2022-06-10T10:54:01.24398Z","shell.execute_reply":"2022-06-10T11:42:30.780355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_rmse(lgbm_rmse_list, title=\" (Iterative LGMB regressor)\")","metadata":{"execution":{"iopub.status.busy":"2022-06-10T11:42:30.788402Z","iopub.execute_input":"2022-06-10T11:42:30.788959Z","iopub.status.idle":"2022-06-10T11:42:31.032226Z","shell.execute_reply.started":"2022-06-10T11:42:30.788912Z","shell.execute_reply":"2022-06-10T11:42:31.031103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Note 1: The \"more\" sophisticated methods take a lot of time to finish unfortunately\n### Note 2: We are close to leaderboard benchmarks even with these small subsamples","metadata":{}}]}