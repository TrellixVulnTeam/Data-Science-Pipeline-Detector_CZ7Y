{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# TPS Jan 22\n**Table of content**\n* Data analisys of TS\n    * Basic EDA\n    * Distribution target\n    * Growth Rate\n    * Heatmap of seasons\n    * Exponential weighted moving\n* Stationarity\n    * Dickey-Fuller Test of original target\n    * Dickey-Fuller Test of log target\n    * Decomposition TS\n    * Dickey-Fuller Test of decomposition residual log target\n* Basic models\n    * Model Autoregressive [AR]\n    * Model Moving Average [MA]\n    * Model AR + MA + difference(y_t, I)[ARIMA]\n    * Prophet\n* Machine learning\n    * CatBoost\n* Prophet\n-------------------\n_May be soon_\n* Machine learning\n    * XGBoost\n    * LightGBM\n* Deep Learnig\n    * LSTM\n    * GRU","metadata":{}},{"cell_type":"code","source":"#================== TPS Jan 2022 =======================#\n#----------------  import packages  -------------------#\nimport numpy as np \nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom cycler import cycler\nfrom statsmodels.tsa.arima_model import ARIMA\nimport statsmodels.api as sm\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport scipy.stats as scs\nfrom statsmodels.tsa.stattools import adfuller\nfrom fbprophet import Prophet\nimport math\n#------------------  load data   ---------------------#\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\ntrain = pd.read_csv('../input/tabular-playground-series-jan-2022/train.csv')\ntest = pd.read_csv('../input/tabular-playground-series-jan-2022/test.csv')\nssub = pd.read_csv('../input/tabular-playground-series-jan-2022/sample_submission.csv')\n \nholiday_data = pd.read_csv('../input/public-and-unofficial-holidays-nor-fin-swe-201519/holidays.csv')\n\ngdp_per_capita = pd.read_csv('../input/gdp-per-capita-finland-norway-sweden-201519/GDP_per_capita_2015_to_2019_Finland_Norway_Sweden.csv', index_col='year')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-01-15T18:38:23.615591Z","iopub.execute_input":"2022-01-15T18:38:23.615844Z","iopub.status.idle":"2022-01-15T18:38:23.67024Z","shell.execute_reply.started":"2022-01-15T18:38:23.615815Z","shell.execute_reply":"2022-01-15T18:38:23.669489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!tree ../input/","metadata":{"execution":{"iopub.status.busy":"2022-01-15T18:38:23.86458Z","iopub.execute_input":"2022-01-15T18:38:23.8648Z","iopub.status.idle":"2022-01-15T18:38:24.571428Z","shell.execute_reply.started":"2022-01-15T18:38:23.864775Z","shell.execute_reply":"2022-01-15T18:38:24.570614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preparate_df(df):\n    df['date'] = pd.to_datetime(df['date'])\n    df['year'] = df['date'].dt.year\n    df['month'] = df['date'].dt.month\n    df['day'] = df['date'].dt.day\n    df['dayofweek'] = df['date'].dt.dayofweek\n    df['dayofmonth'] = df['date'].dt.days_in_month\n    df['dayofyear'] = df['date'].dt.dayofyear\n    df['weekday'] = df['date'].dt.weekday\n    df['weekofyear'] = df['date'].dt.weekofyear\n    if 'num_sold' in df.columns:\n        df['log_num_sold'] = np.log(df['num_sold'])\n    return df\n\ntrain = preparate_df(train)\ntest = preparate_df(test)\n\n\ndef holiday_features(holiday_df, df):\n    \n    fin_holiday = holiday_df.loc[holiday_df.country == 'Finland']\n    swe_holiday = holiday_df.loc[holiday_df.country == 'Sweden']\n    nor_holiday = holiday_df.loc[holiday_df.country == 'Norway']\n    \n    df['fin holiday'] = df.date.isin(fin_holiday.date).astype(int)\n    df['swe holiday'] = df.date.isin(swe_holiday.date).astype(int)\n    df['nor holiday'] = df.date.isin(nor_holiday.date).astype(int)\n    \n    df['holiday'] = np.zeros(df.shape[0]).astype(int)\n    \n    df.loc[df.country == 'Finland', 'holiday'] = df.loc[df.country == 'Finland', 'fin holiday']\n    df.loc[df.country == 'Sweden', 'holiday'] = df.loc[df.country == 'Sweden', 'swe holiday']\n    df.loc[df.country == 'Norway', 'holiday'] = df.loc[df.country == 'Norway', 'nor holiday']\n    \n    df.drop(['fin holiday', 'swe holiday', 'nor holiday'], axis=1, inplace=True)\n    \n    return df\n\nholiday_features(holiday_data, train)\nholiday_features(holiday_data, test)\n\ngdp_dict = gdp_per_capita.unstack().to_dict()\n\n# Create new 'gdp_per_capita' column\ntrain['gdp_per_capita'] = train.set_index(['country', 'year']).index.map(gdp_dict.get)\ntest['gdp_per_capita']  = test.set_index(['country', 'year']).index.map(gdp_dict.get)\n\ndef fourier_features(df):\n    # One-hot encoding (no need to encode the last categories)\n    for country in ['Finland', 'Norway']:\n        df[country] = df.country == country\n        \n    df['KaggleRama'] = df.store == 'KaggleRama'\n    \n    for product in ['Kaggle Mug', 'Kaggle Hat']:\n        df[product] = df['product'] == product\n    \n    # Seasonal variations (Fourier series)\n    # The three products have different seasonal patterns\n    dayofyear = df.date.dt.dayofyear\n    \n    for k in range(1, 3):\n        df[f'sin{k}'] = np.sin(dayofyear / 365 * 2 * math.pi * k)\n        df[f'cos{k}'] = np.cos(dayofyear / 365 * 2 * math.pi * k)\n        df[f'mug_sin{k}'] = df[f'sin{k}'] * df['Kaggle Mug']\n        df[f'mug_cos{k}'] = df[f'cos{k}'] * df['Kaggle Mug']\n        df[f'hat_sin{k}'] = df[f'sin{k}'] * df['Kaggle Hat']\n        df[f'hat_cos{k}'] = df[f'cos{k}'] * df['Kaggle Hat']\n        \n    return df\n\nfourier_features(train)\nfourier_features(test)\n\ndef smape(y_true, y_pred):\n    return 1 / len(y_true) * np.sum(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred)) * 100)\n","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-15T18:38:24.741669Z","iopub.execute_input":"2022-01-15T18:38:24.742231Z","iopub.status.idle":"2022-01-15T18:38:25.209095Z","shell.execute_reply.started":"2022-01-15T18:38:24.742193Z","shell.execute_reply":"2022-01-15T18:38:25.20838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-15T17:31:11.552281Z","iopub.execute_input":"2022-01-15T17:31:11.55463Z","iopub.status.idle":"2022-01-15T17:31:11.60606Z","shell.execute_reply.started":"2022-01-15T17:31:11.554591Z","shell.execute_reply":"2022-01-15T17:31:11.604854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data analisys of TS\n### Basic EDA","metadata":{}},{"cell_type":"code","source":"\n\nplt.rcParams['figure.dpi'] = 600\nplt.set_cmap('jet')\nfig, axs = plt.subplots(3, 3, figsize=(24, 12), facecolor='#f6f5f5')\nfig.subplots_adjust(hspace=0.75, wspace=0.2)\n\ncolormap = ['#1DBA94','#1C5ED2', '#FFC300', '#C70039']\nplt.rc('axes', prop_cycle=(cycler('color', colormap)))\nbackground_color = '#f6f5f5'\nmonth_teg = {0: 'Jan', 1: 'Feb', 2: 'Mar', 3: 'Apr', 4: 'May', 5: 'Jun', 6: 'Jul', 7: 'Aug', 8: 'Sen', 9: 'Oct', 10: 'Nov', 11: 'Dec'}\nmonth_labels = [month_teg[x] for x in range(0, 12)]\n\n#---------- FINLAND ----------\nplt.text(12500, 10600, 'Finland', fontsize=16, weight='heavy')\n_ = pd.DataFrame(pd.pivot_table(train[train.country == 'Finland'], index='year', values=['num_sold'], columns=['month'])['num_sold'].values, index=list(range(2015, 2019)))\n# Plot [0, 0]\ncolors = plt.cm.jet(np.linspace(0, 1, 10))\naxs[0, 0].plot(_.T)\naxs[0, 0].legend(_.T.columns, loc='upper left')\naxs[0, 0].set_title('Seasonality`s numbers of sales by Finland', fontdict={'fontsize': 12, 'fontweight': 'bold'})\naxs[0, 0].set_xticklabels(month_labels)\naxs[0, 0].xaxis.set_tick_params(rotation=45)\n# Plot [0, 1]\nsns.lineplot(data=train[(train.country == 'Finland') & (train.store == 'KaggleMart')], y='num_sold', x='date', hue='product', ax=axs[0, 1])\naxs[0, 1].set_title('Number of sales in KaggleMart by Finland', fontdict={'fontsize': 12, 'fontweight': 'bold'})\naxs[0, 1].xaxis.set_tick_params(rotation=45)\n# Plot [0, 2]\nsns.lineplot(data=train[(train.country == 'Finland') & (train.store == 'KaggleRama')], y='num_sold', x='date', hue='product', ax=axs[0, 2])\naxs[0, 2].set_title('Number of sales in KaggleRama by Finland', fontdict={'fontsize': 12, 'fontweight': 'bold'})\naxs[0, 2].xaxis.set_tick_params(rotation=45)\n\n#---------- NORWAY ----------\nplt.text(12500, 6700, 'Norway', fontsize=16, weight='heavy')\n_ = pd.DataFrame(pd.pivot_table(train[train.country == 'Norway'], index='year', values=['num_sold'], columns=['month'])['num_sold'].values, index=list(range(2015, 2019)))\n# Plot [1, 0]\naxs[1, 0].plot(_.T)\naxs[1, 0].legend(_.T.columns, loc='upper left')\naxs[1, 0].set_title('Seasonality`s numbers of sales by Norway', fontdict={'fontsize': 12, 'fontweight': 'bold'})\naxs[1, 0].set_xticklabels(month_labels)\naxs[1, 0].xaxis.set_tick_params(rotation=45)\n# Plot [1, 1]\nsns.lineplot(data=train[(train.country == 'Norway') & (train.store == 'KaggleMart')], y='num_sold', x='date', hue='product', ax=axs[1, 1])\naxs[1, 1].set_title('Number of sales in KaggleMart by Norway', fontdict={'fontsize': 12, 'fontweight': 'bold'})\naxs[1, 1].xaxis.set_tick_params(rotation=45)\n# Plot [1, 2]\nsns.lineplot(data=train[(train.country == 'Norway') & (train.store == 'KaggleRama')], y='num_sold', x='date', hue='product', ax=axs[1, 2])\naxs[1, 2].set_title('Number of sales in KaggleRama by Norway', fontdict={'fontsize': 12, 'fontweight': 'bold'})\naxs[1, 2].xaxis.set_tick_params(rotation=45)\n\n#---------- SWEDEN ----------\nplt.text(12500, 2800, 'Sweden', fontsize=16, weight='heavy')\n_ = pd.DataFrame(pd.pivot_table(train[train.country == 'Sweden'], index='year', values=['num_sold'], columns=['month'])['num_sold'].values, index=list(range(2015, 2019)))\n# Plot [2, 0]\naxs[2, 0].plot(_.T)\naxs[2, 0].legend(_.T.columns, loc='upper left')\naxs[2, 0].set_title('Seasonality`s numbers of sales by Sweden', fontdict={'fontsize': 12, 'fontweight': 'bold'})\naxs[2, 0].set_xticklabels(month_labels)\naxs[2, 0].xaxis.set_tick_params(rotation=45)\n# Plot [2, 1]\nsns.lineplot(data=train[(train.country == 'Sweden') & (train.store == 'KaggleMart')], y='num_sold', x='date', hue='product', ax=axs[2, 1])\naxs[2, 1].set_title('Number of sales in KaggleMart by Sweden', fontdict={'fontsize': 12, 'fontweight': 'bold'})\naxs[2, 1].xaxis.set_tick_params(rotation=45)\n# Plot [2, 2]\nsns.lineplot(data=train[(train.country == 'Sweden') & (train.store == 'KaggleRama')], y='num_sold', x='date', hue='product', ax=axs[2, 2])\naxs[2, 2].set_title('Number of sales in KaggleRama by Sweden', fontdict={'fontsize': 12, 'fontweight': 'bold'})\naxs[2, 2].xaxis.set_tick_params(rotation=45)\n\nfor i in range(3):\n    for j in range(3):\n        for s in [\"top\",\"right\"]:\n                axs[i, j].spines[s].set_visible(False)\n                axs[i, j].set_facecolor(background_color)\n                axs[i, j].grid(which='major', axis='x', zorder=1, color='#EEEEEE', linewidth=0.4)\n                axs[i, j].xaxis.offsetText.set_fontsize(4)\n                axs[i, j].yaxis.offsetText.set_fontsize(4)\n                axs[i, j].set_ylabel('')\n                axs[i, j].set_xlabel('')\n                axs[i, j].tick_params(labelsize=8, width=1)\n                if j == 0:\n                    axs[i, j].legend(list(range(2015, 2019)), ncol=4, facecolor=background_color, edgecolor=background_color, loc='upper center')\n                else:\n                    axs[i, j].legend(train['product'].unique(), ncol=3, facecolor=background_color, edgecolor=background_color, loc='upper center')\n\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2022-01-15T17:31:11.607993Z","iopub.execute_input":"2022-01-15T17:31:11.608423Z","iopub.status.idle":"2022-01-15T17:31:19.955549Z","shell.execute_reply.started":"2022-01-15T17:31:11.608385Z","shell.execute_reply":"2022-01-15T17:31:19.954788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Seasonality is clearly visible in the first row of graphs\n* Different products have different sales variation\n* There are also different sales in different markets\n* Products are equally preferred for all countries\n* Test period: 4 years\n* Peak at the end of the year","metadata":{}},{"cell_type":"markdown","source":"### Distribution target","metadata":{}},{"cell_type":"code","source":"\nplt.rcParams['figure.dpi'] = 300\nfig, axs = plt.subplots(3, 1, figsize=(12, 10), facecolor='#f6f5f5')\n\ncolormap1 = ['#1DBA94','#1C5ED2', '#FFC300']\ncolormap2 = ['#A3E4D7', '#82E0AA', '#45B39D']\ncolormap3= ['#7FB3D5', '#F1948A']\nfig.subplots_adjust(hspace=0.5, wspace=0.3)\nplt.rc('axes', prop_cycle=(cycler('color', colormap1)))\nbackground_color = '#f6f5f5'\n\n# ax[0, 0]\n#ax.grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.4)\nsns.kdeplot(train['num_sold'], data=train, ax=axs[0], color=colormap1, fill=True, hue='product')\naxs[0].set_title('Distribution numbers of sales by product', fontdict={'fontsize': 12, 'fontweight': 'bold'})\naxs[0].legend(train['product'].unique(), ncol=3, facecolor=background_color, edgecolor=background_color, loc='upper center')\n\nplt.rc('axes', prop_cycle=(cycler('color', colormap3)))\nsns.kdeplot(train['num_sold'], data=train, ax=axs[1], color=colormap3, fill=True, hue='store')\naxs[1].set_title('Distribution numbers of sales by store', fontdict={'fontsize': 12, 'fontweight': 'bold'})\naxs[1].legend(train['store'].unique(), ncol=2, facecolor=background_color, edgecolor=background_color, loc='upper center')\n\nplt.rc('axes', prop_cycle=(cycler('color', colormap2)))\nsns.kdeplot(train['num_sold'], data=train, ax=axs[2], color=colormap2, fill=True, hue='country')\naxs[2].set_title('Distribution numbers of sales by country', fontdict={'fontsize': 12, 'fontweight': 'bold'})\naxs[2].legend(train['country'].unique(), ncol=3, facecolor=background_color, edgecolor=background_color, loc='upper center')\n\nfor i in range(3):\n    for s in [\"top\",\"right\"]:\n        axs[i].spines[s].set_visible(False)\n    axs[i].set_facecolor(background_color)\n    #ax.grid(which='major', axis='x', zorder=-2, color='#EEEEEE', linewidth=0.4)\n    axs[i].xaxis.offsetText.set_fontsize(4)\n    axs[i].yaxis.offsetText.set_fontsize(4)\n    axs[i].set_ylabel('')\n    axs[i].set_xlabel('')\n    axs[i].tick_params(labelsize=8, width=1)\n    \nplt.show();","metadata":{"execution":{"iopub.status.busy":"2022-01-15T17:31:19.956695Z","iopub.execute_input":"2022-01-15T17:31:19.956982Z","iopub.status.idle":"2022-01-15T17:31:21.620394Z","shell.execute_reply.started":"2022-01-15T17:31:19.956924Z","shell.execute_reply":"2022-01-15T17:31:21.617128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* The distributions are noticeably skewed\n* The first chart look loke Chi-Square Distribution ","metadata":{}},{"cell_type":"markdown","source":"### Growth Rate","metadata":{}},{"cell_type":"code","source":"growth_rate = np.exp(np.diff(np.log(train['num_sold']))) - 1\n\nseasons = {'summer': [6, 7, 8], 'autumn': [9,10,11], 'winter': [12, 1, 2], 'spring': [3, 4, 5]}\nprint('======================================================================================')\nprint(f'      Season     |     Kaggle Mugs     |     Kaggle Hats     |    Kaggle Stickers    ')\nfor year in range(2015, 2019):\n    print('------------------------------------------------------------------------------------')\n    for name in seasons:\n        _1 = train[(train['month'].isin(seasons[name])) & (train['year'] == year) & (train['product'] == 'Kaggle Mug')]['num_sold']\n        _2 = train[(train['month'].isin(seasons[name])) & (train['year'] == year) & (train['product'] == 'Kaggle Hat')]['num_sold']\n        _3 = train[(train['month'].isin(seasons[name])) & (train['year'] == year) & (train['product'] == 'Kaggle Sticker')]['num_sold']\n        growth_rate_mug = np.exp(np.diff(np.log(_1))) - 1\n        growth_rate_hat = np.exp(np.diff(np.log(_2))) - 1\n        growth_rate_st = np.exp(np.diff(np.log(_3))) - 1\n        np.mean(growth_rate)\n        print(f'  {name} of {year} | growth_rate : {np.mean(growth_rate_mug)* 100:.1f}% | growth_rate : {np.mean(growth_rate_hat)* 100:.1f}% | growth_rate : {np.mean(growth_rate_st)* 100:.1f}%')\nprint('------------------------------------------------------------------------------------')\nprint('======================================================================================')","metadata":{"execution":{"iopub.status.busy":"2022-01-15T17:31:21.621603Z","iopub.execute_input":"2022-01-15T17:31:21.622235Z","iopub.status.idle":"2022-01-15T17:31:21.872402Z","shell.execute_reply.started":"2022-01-15T17:31:21.622198Z","shell.execute_reply":"2022-01-15T17:31:21.871646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Despite the fact that the products have slightly different distribution parameters and variations, their growth rate are the same\n* But its not look like True, some seasons are supposed to be negative, may be some issue in my code","metadata":{}},{"cell_type":"markdown","source":"### Heatmap of seasons","metadata":{}},{"cell_type":"code","source":"fig, axs = plt.subplots(1, figsize=(12, 10), facecolor='#f6f5f5')\nplt.rc('axes', prop_cycle=(cycler('color', colormap1)))\nbackground_color = '#f6f5f5'\n#sns.color_palette(['#1DBA94','#1C5ED2', '#FFC300', '#C70039'], as_cmap=True)\n#cmap = matplotlib.colors.LinearSegmentedColormap.from_list(\"\", colormap1)\nsns.heatmap(pd.pivot_table(train, index='month', columns='year', values='num_sold'), annot=True, ax=axs, fmt=\".2f\", cmap='YlGnBu')\naxs.set_title('Heatmap of seasons', fontdict={'fontsize': 12, 'fontweight': 'bold'})\naxs.set_facecolor(background_color)","metadata":{"execution":{"iopub.status.busy":"2022-01-15T17:31:21.873843Z","iopub.execute_input":"2022-01-15T17:31:21.874117Z","iopub.status.idle":"2022-01-15T17:31:23.107648Z","shell.execute_reply.started":"2022-01-15T17:31:21.874083Z","shell.execute_reply":"2022-01-15T17:31:23.107015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* More about seasons, customers prefer buying products while season is winter\n* We have some possitive trend and seasonality","metadata":{}},{"cell_type":"markdown","source":"### Exponential weighted moving","metadata":{}},{"cell_type":"code","source":"plt.rcParams['figure.dpi'] = 300\nfig, axs = plt.subplots(3, 1, figsize=(12, 10), facecolor='#f6f5f5')\nfig.subplots_adjust(hspace=0.5, wspace=0.3)\n\nbackground_color = '#f6f5f5'\ncolormap3 = ['#232267', '#12724B', '#BB3E21']\ncolormap2 = ['#20379C', '#138D75', '#E07C12']\ncolormap1 = ['#1C5ED2', '#1DBA94', '#FFC300']\n\nhalflife_15 = 8\nhalflife_30 = 15\n\n_11 = train[train['product'] == 'Kaggle Hat'][['num_sold', 'date']].set_index('date')\n_12 = _11.ewm(halflife=halflife_15).mean()\n_13 = _11.ewm(halflife=halflife_30).mean()\n_21 = train[train['product'] == 'Kaggle Mug'][['num_sold', 'date']].set_index('date')\n_22 = _21.ewm(halflife=halflife_15).mean()\n_23 = _21.ewm(halflife=halflife_30).mean()\n_31 = train[train['product'] == 'Kaggle Sticker'][['num_sold', 'date']].set_index('date')\n_32 = _31.ewm(halflife=halflife_15).mean()\n_33 = _31.ewm(halflife=halflife_30).mean()\naxs[0].plot(_11, color=colormap1[0])\naxs[0].plot(_12, color=colormap2[0], linewidth=2)\naxs[0].plot(_13, color=colormap3[0], linewidth=0.5)\naxs[0].set_title('Exponential weighted moving by Kaggle Hats', fontdict={'fontsize': 12, 'fontweight': 'bold'})\naxs[0].legend(['Original', 'EWM.8', 'EWM.15'], ncol=3, facecolor=background_color, edgecolor=background_color, loc='upper center')\n\naxs[1].plot(_21, color=colormap1[1])\naxs[1].plot(_22, color=colormap2[1], linewidth=2)\naxs[1].plot(_23, color=colormap3[1], linewidth=0.5)\naxs[1].set_title('Exponential weighted moving by Kaggle Mugs', fontdict={'fontsize': 12, 'fontweight': 'bold'})\naxs[1].legend(['Original', 'EWM.8', 'EWM.15'], ncol=3, facecolor=background_color, edgecolor=background_color, loc='upper center')\n\naxs[2].plot(_31, color=colormap1[2])\naxs[2].plot(_32, color=colormap2[2], linewidth=2)\naxs[2].plot(_33, color=colormap3[2], linewidth=0.5)\naxs[2].set_title('Exponential weighted moving by Kaggle Stickers', fontdict={'fontsize': 12, 'fontweight': 'bold'})\naxs[2].legend(['Original', 'EWM.8', 'EWM.15'], ncol=3, facecolor=background_color, edgecolor=background_color, loc='upper center')\n\nfor i in range(3):\n    for s in [\"top\",\"right\"]:\n        axs[i].spines[s].set_visible(False)\n    axs[i].set_facecolor(background_color)\n    #ax.grid(which='major', axis='x', zorder=-2, color='#EEEEEE', linewidth=0.4)\n    axs[i].xaxis.offsetText.set_fontsize(4)\n    axs[i].yaxis.offsetText.set_fontsize(4)\n    axs[i].set_ylabel('')\n    axs[i].set_xlabel('')\n    axs[i].tick_params(labelsize=8, width=1)\n    \nplt.show();","metadata":{"execution":{"iopub.status.busy":"2022-01-15T17:31:23.108901Z","iopub.execute_input":"2022-01-15T17:31:23.109275Z","iopub.status.idle":"2022-01-15T17:31:24.905868Z","shell.execute_reply.started":"2022-01-15T17:31:23.109241Z","shell.execute_reply":"2022-01-15T17:31:24.905247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Exponential weighted moving is good tool in the fight against large variation or noise","metadata":{}},{"cell_type":"markdown","source":"## Stationarity\n### Dickey-Fuller Test of original target","metadata":{}},{"cell_type":"code","source":"\ndef test_stationarity(ts, window=8):\n    #.rolling() \n    #Determing rolling statistics\n    rolmean = ts.rolling(window=window).mean()\n    rolstd = ts.rolling(window=window).std()\n    plt.rcParams['figure.dpi'] = 300\n    fig, axs = plt.subplots(1, figsize=(18, 6), facecolor='#f6f5f5')\n    fig.subplots_adjust(hspace=0.3, wspace=0.3)\n    axs.plot(ts)\n    axs.plot(rolmean, linewidth=1)\n    axs.plot(rolstd, linewidth=1)\n    for s in [\"top\",\"right\"]:\n        axs.spines[s].set_visible(False)\n    axs.set_facecolor(background_color)\n    #ax.grid(which='major', axis='x', zorder=-2, color='#EEEEEE', linewidth=0.4)\n    axs.xaxis.offsetText.set_fontsize(4)\n    axs.yaxis.offsetText.set_fontsize(4)\n    axs.set_ylabel('')\n    axs.set_xlabel('')\n    axs.tick_params(labelsize=8, width=1)\n    axs.set_title('Dickey-Fuller Test', fontdict={'fontsize': 16, 'fontweight': 'bold'})\n    axs.legend(['Original', 'Rolling Mean', 'Rolling Std'], ncol=3, facecolor=background_color, edgecolor=background_color, loc='upper center')\n    \n    print('Results of Dickey-Fuller Test:')\n    dftest = adfuller(ts, autolag='AIC')\n    #print(dftest)\n    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic', 'p-value', 'Lags Used', 'Number of Observations Used'])\n    for key, value in dftest[4].items():\n        dfoutput['Critical Values (%s)'%key] = value\n    print(dfoutput)\n_ = train[train['product'] == 'Kaggle Hat'][['num_sold', 'date']].set_index('date')\ntest_stationarity(_)","metadata":{"execution":{"iopub.status.busy":"2022-01-15T17:31:24.908702Z","iopub.execute_input":"2022-01-15T17:31:24.909138Z","iopub.status.idle":"2022-01-15T17:31:26.805342Z","shell.execute_reply.started":"2022-01-15T17:31:24.909067Z","shell.execute_reply":"2022-01-15T17:31:26.803845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* This dataset can be considered almost stationary, it has a low p-value, you only need to deal with Critical Values","metadata":{}},{"cell_type":"markdown","source":"### Dickey-Fuller Test of log target","metadata":{}},{"cell_type":"code","source":"_ = train[train['product'] == 'Kaggle Hat'][['log_num_sold', 'date']].set_index('date')\n\ntest_stationarity(_)","metadata":{"execution":{"iopub.status.busy":"2022-01-15T17:31:26.806603Z","iopub.execute_input":"2022-01-15T17:31:26.807167Z","iopub.status.idle":"2022-01-15T17:31:28.285402Z","shell.execute_reply.started":"2022-01-15T17:31:26.80713Z","shell.execute_reply":"2022-01-15T17:31:28.281207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* logarithm is bad idea","metadata":{}},{"cell_type":"markdown","source":"### Decomposition TS","metadata":{}},{"cell_type":"code","source":"from statsmodels.tsa.seasonal import seasonal_decompose\ndef get_decomposition(ts, period):\n    colormap = ['#1DBA94','#1C5ED2', '#FFC300', '#C70039']\n    \n    decomposition = seasonal_decompose(ts, period=period)\n    trend = decomposition.trend\n    seasonal = decomposition.seasonal\n    resid = decomposition.resid\n    \n    plt.rcParams['figure.dpi'] = 300\n    fig, axs = plt.subplots(4, figsize=(18, 12), facecolor='#f6f5f5')\n    fig.subplots_adjust(hspace=0.6, wspace=0.3)\n    axs[0].plot(ts, label='Original', color = colormap[0])\n    axs[1].plot(trend, label='Trend', color = colormap[1])\n    axs[2].plot(seasonal, label='Seasonal', color = colormap[2])\n    axs[3].plot(resid, label='Resid', color = colormap[3])\n    \n    for i in range(4):\n        for s in [\"top\",\"right\"]:\n            axs[i].spines[s].set_visible(False)\n        axs[i].set_facecolor(background_color)\n        #ax.grid(which='major', axis='x', zorder=-2, color='#EEEEEE', linewidth=0.4)\n        axs[i].xaxis.offsetText.set_fontsize(4)\n        axs[i].yaxis.offsetText.set_fontsize(4)\n        axs[i].set_ylabel('')\n        axs[i].set_xlabel('')\n        axs[i].tick_params(labelsize=8, width=1)\n        \n    axs[0].set_title('Original', fontdict={'fontsize': 12, 'fontweight': 'bold'})\n    axs[1].set_title('Trend', fontdict={'fontsize': 12, 'fontweight': 'bold'})\n    axs[2].set_title('Seasonal', fontdict={'fontsize': 12, 'fontweight': 'bold'})\n    axs[3].set_title('Resid', fontdict={'fontsize': 12, 'fontweight': 'bold'})\n    #axs.legend(['Original', 'Rolling Mean', 'Rolling Std'], ncol=3, facecolor=background_color, edgecolor=background_color, loc='upper center')\n    \n    return resid\n_ = train[(train['product'] == 'Kaggle Hat') & (train['year'] == 2016)][['log_num_sold', 'date']].set_index('date')\nresidual = get_decomposition(_, 366)","metadata":{"execution":{"iopub.status.busy":"2022-01-15T17:31:28.286861Z","iopub.execute_input":"2022-01-15T17:31:28.287371Z","iopub.status.idle":"2022-01-15T17:31:29.939379Z","shell.execute_reply.started":"2022-01-15T17:31:28.287332Z","shell.execute_reply":"2022-01-15T17:31:29.935919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Seasonal decomposition confirms some our hypotheses","metadata":{}},{"cell_type":"markdown","source":"### Dickey-Fuller Test of decomposition residual log target","metadata":{}},{"cell_type":"code","source":"ts_log_decompose = residual\nts_log_decompose.dropna(inplace=True)\ntest_stationarity(ts_log_decompose)","metadata":{"execution":{"iopub.status.busy":"2022-01-15T17:31:29.940853Z","iopub.execute_input":"2022-01-15T17:31:29.941271Z","iopub.status.idle":"2022-01-15T17:31:30.865769Z","shell.execute_reply.started":"2022-01-15T17:31:29.941236Z","shell.execute_reply":"2022-01-15T17:31:30.864968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* This better one","metadata":{}},{"cell_type":"markdown","source":"## Basic models\n### Model Autoregressive [AR]","metadata":{}},{"cell_type":"code","source":"X = train[(train['product'] == 'Kaggle Hat')][['log_num_sold', 'date']].set_index('date')\n\ndef get_ar(df, order=(2,0,1), name='ARMA(2,1)'):\n    \n    background_color = '#f6f5f5'\n    model = ARIMA(df, order=order)\n    res = model.fit(disp=-1)\n    pred = res.fittedvalues\n    print(res.summary())\n    \n    print('==================================')\n    print('----------- Metrics --------------')\n    rss = np.sum((pred.values-df.values)**2)\n    smape_val = smape(df.values, pred.values)\n    print(f'RSS | {rss:.4f}')\n    print(f'SMAPE | {smape_val:.4f}')\n    print('----------------------------------')\n    #plt.plot(res.fittedvalues, alpha=.7)\n    #plt.plot(df, alpha = 0.7)\n\n    plt.rcParams['figure.dpi'] = 300\n    fig, axs = plt.subplots(3, figsize=(18, 12), facecolor='#f6f5f5')\n    fig.subplots_adjust(hspace=0.6, wspace=0.3)\n    axs[0].plot(df, color = colormap[3], alpha = 0.7)\n    axs[0].plot(pred, color = colormap[1], alpha = 0.7)\n    axs[0].legend(['Original', 'Model'], ncol=2, facecolor=background_color, edgecolor=background_color, loc='upper center')\n    sm.graphics.tsa.plot_acf(pred, lags=12*4, ax=axs[1])\n    sm.graphics.tsa.plot_pacf(pred, lags=12*4, ax=axs[2])\n    for i in range(3):\n        for s in [\"top\",\"right\"]:\n            axs[i].spines[s].set_visible(False)\n        axs[i].set_facecolor(background_color)\n        #ax.grid(which='major', axis='x', zorder=-2, color='#EEEEEE', linewidth=0.4)\n        axs[i].xaxis.offsetText.set_fontsize(4)\n        axs[i].yaxis.offsetText.set_fontsize(4)\n        axs[i].set_ylabel('')\n        axs[i].set_xlabel('')\n        axs[i].tick_params(labelsize=8, width=1)\n        \n    axs[0].set_title(f'Model {name}', fontdict={'fontsize': 12, 'fontweight': 'bold'})\n    axs[1].set_title('Autocorrelation', fontdict={'fontsize': 12, 'fontweight': 'bold'})\n    axs[2].set_title('Partial Autocorrelation', fontdict={'fontsize': 12, 'fontweight': 'bold'})\n    \n    layout = (2, 2)\n    fig = plt.figure(figsize=(18, 12), facecolor='#f6f5f5')\n    qq_ax = plt.subplot2grid(layout, (0, 0))\n    pp_ax = plt.subplot2grid(layout, (0, 1))\n    sm.qqplot(pred, line='s', ax=qq_ax)\n    scs.probplot(pred, sparams=(pred.mean(), pred.std()), plot=pp_ax)\n    for i in [qq_ax, pp_ax]:\n        for s in [\"top\",\"right\"]:\n            i.spines[s].set_visible(False)\n        i.set_facecolor(background_color)\n        #ax.grid(which='major', axis='x', zorder=-2, color='#EEEEEE', linewidth=0.4)\n        i.xaxis.offsetText.set_fontsize(4)\n        i.yaxis.offsetText.set_fontsize(4)\n        i.set_ylabel('')\n        i.set_xlabel('T-Quantilies')\n        i.tick_params(labelsize=8, width=1)\n    qq_ax.set_title('QQ-Plot', fontdict={'fontsize': 12, 'fontweight': 'bold'})\n    pp_ax.set_title('Probability Plot', fontdict={'fontsize': 12, 'fontweight': 'bold'})\n    \nget_ar(X)","metadata":{"execution":{"iopub.status.busy":"2022-01-15T17:31:30.867129Z","iopub.execute_input":"2022-01-15T17:31:30.867548Z","iopub.status.idle":"2022-01-15T17:31:41.691228Z","shell.execute_reply.started":"2022-01-15T17:31:30.867513Z","shell.execute_reply":"2022-01-15T17:31:41.69045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* My AR model is really shit...","metadata":{}},{"cell_type":"markdown","source":"### Model Mean Average [MA]","metadata":{}},{"cell_type":"code","source":"get_ar(X, (1,0,2), 'Arma(1,0,2)')","metadata":{"execution":{"iopub.status.busy":"2022-01-15T17:31:41.692681Z","iopub.execute_input":"2022-01-15T17:31:41.692918Z","iopub.status.idle":"2022-01-15T17:31:48.172441Z","shell.execute_reply.started":"2022-01-15T17:31:41.692888Z","shell.execute_reply":"2022-01-15T17:31:48.17183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* This better one, but...","metadata":{}},{"cell_type":"code","source":"X = train[(train['product'] == 'Kaggle Hat')][['log_num_sold', 'date']].set_index('date')\n\ndef get_arima(df, order=(1,1,2), name='ARMA(1,1,2)'):\n    \n    background_color = '#f6f5f5'\n    model = sm.tsa.ARIMA(df, order=order)\n    res = model.fit()\n    pred = res.fittedvalues\n    print(res.summary())\n    \n    print('==================================')\n    print('----------- Metrics --------------')\n    rss = np.sum((pred.values-df.values)**2)\n    smape_val = smape(df.values, pred.values)\n    print(f'RSS | {rss:.4f}')\n    print(f'SMAPE | {smape_val:.4f}')\n    print('----------------------------------')\n    #plt.plot(res.fittedvalues, alpha=.7)\n    #plt.plot(df, alpha = 0.7)\n\n    plt.rcParams['figure.dpi'] = 300\n    fig, axs = plt.subplots(3, figsize=(18, 12), facecolor='#f6f5f5')\n    fig.subplots_adjust(hspace=0.6, wspace=0.3)\n    axs[0].plot(df, color = colormap[3], alpha = 0.7)\n    axs[0].plot(pred, color = colormap[1], alpha = 0.7)\n    axs[0].legend([ 'Original', 'Model'], ncol=2, facecolor=background_color, edgecolor=background_color, loc='upper center')\n    sm.graphics.tsa.plot_acf(pred, lags=12*4, ax=axs[1])\n    sm.graphics.tsa.plot_pacf(pred, lags=12*4, ax=axs[2])\n    for i in range(3):\n        for s in [\"top\",\"right\"]:\n            axs[i].spines[s].set_visible(False)\n        axs[i].set_facecolor(background_color)\n        #ax.grid(which='major', axis='x', zorder=-2, color='#EEEEEE', linewidth=0.4)\n        axs[i].xaxis.offsetText.set_fontsize(4)\n        axs[i].yaxis.offsetText.set_fontsize(4)\n        axs[i].set_ylabel('')\n        axs[i].set_xlabel('')\n        axs[i].tick_params(labelsize=8, width=1)\n        \n    axs[0].set_title(f'Model {name}', fontdict={'fontsize': 12, 'fontweight': 'bold'})\n    axs[1].set_title('Autocorrelation', fontdict={'fontsize': 12, 'fontweight': 'bold'})\n    axs[2].set_title('Partial Autocorrelation', fontdict={'fontsize': 12, 'fontweight': 'bold'})\n    \n    layout = (2, 2)\n    fig = plt.figure(figsize=(18, 12), facecolor='#f6f5f5')\n    qq_ax = plt.subplot2grid(layout, (0, 0))\n    pp_ax = plt.subplot2grid(layout, (0, 1))\n    sm.qqplot(pred, line='s', ax=qq_ax)\n    scs.probplot(pred, sparams=(pred.mean(), pred.std()), plot=pp_ax)\n    for i in [qq_ax, pp_ax]:\n        for s in [\"top\",\"right\"]:\n            i.spines[s].set_visible(False)\n        i.set_facecolor(background_color)\n        #ax.grid(which='major', axis='x', zorder=-2, color='#EEEEEE', linewidth=0.4)\n        i.xaxis.offsetText.set_fontsize(4)\n        i.yaxis.offsetText.set_fontsize(4)\n        i.set_ylabel('')\n        i.set_xlabel('T-Quantilies')\n        i.tick_params(labelsize=8, width=1)\n    qq_ax.set_title('QQ-Plot', fontdict={'fontsize': 12, 'fontweight': 'bold'})\n    pp_ax.set_title('Probability Plot', fontdict={'fontsize': 12, 'fontweight': 'bold'})\n    \nget_arima(X, (1,1,2))","metadata":{"execution":{"iopub.status.busy":"2022-01-15T17:31:48.173764Z","iopub.execute_input":"2022-01-15T17:31:48.174169Z","iopub.status.idle":"2022-01-15T17:31:57.744456Z","shell.execute_reply.started":"2022-01-15T17:31:48.174129Z","shell.execute_reply":"2022-01-15T17:31:57.743799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Okey my skill in basic tss models not well, may be someone tell me whats problem do i have in comments","metadata":{}},{"cell_type":"markdown","source":"## Machine Learning\n### CatBoost\n* 5.4 min score\n* 6.14 mean score 10folds\n* 5.32 without cross-valid [5k iter]\n\n**updates will be asap**","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom catboost import CatBoostRegressor\nimport optuna\ndef transform(df):\n    le = LabelEncoder()\n    for col in ['country', 'product', 'store']:\n        df[col] = le.fit_transform(df[col])\n    return df\ntransform(train)\ntransform(test)\n\nFEATURES = [col for col in train.columns if col not in ['row_id', 'date', 'num_sold', 'log_num_sold']]\n\nparam = {'iterations': 62250, \n         'od_wait': 3366, \n         'learning_rate': 0.03248025377961145, \n         'reg_lambda': 0.3260692020520345, \n         'subsample': 0.855134852487254, \n         'random_strength': 13.37112038825282, \n         'depth': 12, 'min_data_in_leaf': 40, \n         'leaf_estimation_iterations': 13,\n         'eval_metric':'SMAPE',\n         'loss_function':'MAE',\n         'task_type':\"GPU\",\n         'bootstrap_type':'Poisson'\n        }\n\ny = train['num_sold']\ntrain2 = train[FEATURES]\ntest2 = test[FEATURES]\n\nkfold = TimeSeriesSplit(10)\n\ntest_pred = []\nfor fold, (train_id, test_id) in enumerate(kfold.split(train2)):\n    print('<------- fold', fold+1, '------->')\n    x_train, y_train = train2.iloc[train_id], y.iloc[train_id]\n    x_valid, y_valid = train2.iloc[test_id], y.iloc[test_id]\n    \n    cat = CatBoostRegressor(**param)\n    cat.fit(x_train, y_train, eval_set = (x_valid, y_valid), verbose = 1000, early_stopping_rounds = 1500)\n    test_pred.append(cat.predict(test2))\n    \n    train_pred = cat.predict(x_train)\n    train_score = smape(y_train, np.ceil(train_pred))\n    valid_pred = cat.predict(x_valid)\n    valid_score = smape(y_valid, np.ceil(valid_pred))\n    print(f'Train SMAPE: {valid_score}')\n    print(f'Valid SMAPE: {valid_score}')\n    #scores.append(valid_score)\n    \nsold = np.mean(test_pred, axis = 0)\nssub['num_sold'] = sold\nssub.to_csv('submission_catboost.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2022-01-15T17:42:02.06595Z","iopub.execute_input":"2022-01-15T17:42:02.066599Z","iopub.status.idle":"2022-01-15T17:46:42.666084Z","shell.execute_reply.started":"2022-01-15T17:42:02.066561Z","shell.execute_reply":"2022-01-15T17:46:42.664948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nparam = {'iterations': 4000, \n         'od_wait': 3366, \n         'learning_rate': 0.03248025377961145, \n         'reg_lambda': 0.3260692020520345, \n         'subsample': 0.855134852487254, \n         'random_strength': 13.37112038825282, \n         'depth': 12, 'min_data_in_leaf': 40, \n         'leaf_estimation_iterations': 13,\n         'eval_metric':'SMAPE',\n         'loss_function':'MAE',\n         'task_type':\"GPU\",\n         'bootstrap_type':'Poisson'\n        }\n\ncat = CatBoostRegressor(**param)\ny = train['num_sold']\ntrain2 = train[FEATURES]\ncat.fit(train2, y, verbose = 1000, early_stopping_rounds = 1500)","metadata":{"execution":{"iopub.status.busy":"2022-01-15T18:11:53.497873Z","iopub.execute_input":"2022-01-15T18:11:53.498222Z","iopub.status.idle":"2022-01-15T18:21:51.450529Z","shell.execute_reply.started":"2022-01-15T18:11:53.498182Z","shell.execute_reply":"2022-01-15T18:21:51.449789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_pred = cat.predict(train2)\ntrain_score = smape(y, np.ceil(train_pred))\ntrain_score","metadata":{"execution":{"iopub.status.busy":"2022-01-15T18:23:20.468185Z","iopub.execute_input":"2022-01-15T18:23:20.468434Z","iopub.status.idle":"2022-01-15T18:23:20.925601Z","shell.execute_reply.started":"2022-01-15T18:23:20.468405Z","shell.execute_reply":"2022-01-15T18:23:20.924852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.rcParams['figure.dpi'] = 300\nfig, axs = plt.subplots(1, figsize=(18, 12), facecolor='#f6f5f5')\nfig.subplots_adjust(hspace=0.6, wspace=0.3)\naxs.plot(train_pred, color = 'red', alpha = 0.5)\naxs.plot(y, color = 'blue', alpha = 0.5)","metadata":{"execution":{"iopub.status.busy":"2022-01-15T18:23:46.476048Z","iopub.execute_input":"2022-01-15T18:23:46.476376Z","iopub.status.idle":"2022-01-15T18:23:57.329556Z","shell.execute_reply.started":"2022-01-15T18:23:46.476335Z","shell.execute_reply":"2022-01-15T18:23:57.328926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.histplot(data=train_pred)\nsns.histplot(data=y, color='red')\n","metadata":{"execution":{"iopub.status.busy":"2022-01-15T18:24:54.584767Z","iopub.execute_input":"2022-01-15T18:24:54.587793Z","iopub.status.idle":"2022-01-15T18:24:55.585099Z","shell.execute_reply.started":"2022-01-15T18:24:54.587738Z","shell.execute_reply":"2022-01-15T18:24:55.584415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat.get_feature_importance()","metadata":{"execution":{"iopub.status.busy":"2022-01-15T18:27:17.257534Z","iopub.execute_input":"2022-01-15T18:27:17.257789Z","iopub.status.idle":"2022-01-15T18:27:18.093403Z","shell.execute_reply.started":"2022-01-15T18:27:17.257759Z","shell.execute_reply":"2022-01-15T18:27:18.092365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_feature_importance(importance,names,model_type):\n    \n    #Create arrays from feature importance and feature names\n    feature_importance = np.array(importance)\n    feature_names = np.array(names)\n    \n    #Create a DataFrame using a Dictionary\n    data={'feature_names':feature_names,'feature_importance':feature_importance}\n    fi_df = pd.DataFrame(data)\n    \n    #Sort the DataFrame in order decreasing feature importance\n    fi_df.sort_values(by=['feature_importance'], ascending=False,inplace=True)\n    \n    #Define size of bar plot\n    plt.figure(figsize=(10,8))\n    #Plot Searborn bar chart\n    sns.barplot(x=fi_df['feature_importance'], y=fi_df['feature_names'])\n    #Add chart labels\n    plt.title(model_type + 'FEATURE IMPORTANCE')\n    plt.xlabel('FEATURE IMPORTANCE')\n    plt.ylabel('FEATURE NAMES')","metadata":{"execution":{"iopub.status.busy":"2022-01-15T18:29:07.934091Z","iopub.execute_input":"2022-01-15T18:29:07.934549Z","iopub.status.idle":"2022-01-15T18:29:07.942192Z","shell.execute_reply.started":"2022-01-15T18:29:07.934512Z","shell.execute_reply":"2022-01-15T18:29:07.940945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_feature_importance(cat.get_feature_importance(),train2.columns,'CATBOOST')","metadata":{"execution":{"iopub.status.busy":"2022-01-15T18:29:28.045076Z","iopub.execute_input":"2022-01-15T18:29:28.045593Z","iopub.status.idle":"2022-01-15T18:29:29.400486Z","shell.execute_reply.started":"2022-01-15T18:29:28.045551Z","shell.execute_reply":"2022-01-15T18:29:29.399801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#sold = np.mean(test_pred, axis = 0)\nssub['num_sold'] = cat.predict(test2)\nssub.to_csv('submission_catboost_mean.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2022-01-15T18:26:00.970742Z","iopub.execute_input":"2022-01-15T18:26:00.97101Z","iopub.status.idle":"2022-01-15T18:26:01.112495Z","shell.execute_reply.started":"2022-01-15T18:26:00.970977Z","shell.execute_reply":"2022-01-15T18:26:01.11178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* better without ts split validation","metadata":{}},{"cell_type":"code","source":"#sold = test_pred[9]\n#ssub['num_sold'] = sold\n#ssub.to_csv('submission_catboost_9fold_[5dot50].csv', index = False)\n#\n#sold = test_pred[3]\n#ssub['num_sold'] = sold\n#ssub.to_csv('submission_catboost_9fold_[5dot53].csv', index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#from sklearn.model_selection import train_test_split\n#def objective(trial):\n#    train_x, valid_x, train_y, valid_y = train_test_split(train2, y, test_size=0.3)\n#\n#    param = {'iterations':trial.suggest_int(\"iterations\", 1000, 100000),\n#              'od_wait':trial.suggest_int('od_wait', 500, 5000),\n#              'task_type':\"GPU\",\n#              'learning_rate' : trial.suggest_uniform('learning_rate', 0.02 , 0.06),\n#              'reg_lambda': trial.suggest_loguniform('reg_lambda', 0.30 , 0.33),\n#              'subsample': trial.suggest_uniform('subsample',0.8,1.0),\n#              'random_strength': trial.suggest_uniform('random_strength',10,50),\n#              'depth': trial.suggest_int('depth',1,15),\n#              'min_data_in_leaf': trial.suggest_int('min_data_in_leaf',1,50),\n#              'leaf_estimation_iterations': trial.suggest_int('leaf_estimation_iterations',1,15),\n#              'bootstrap_type':'Poisson',\n#              'eval_metric':'SMAPE',\n#              'loss_function':'MAE'\n#               }\n#    \n#    gbm = CatBoostRegressor(**param)\n#\n#    gbm.fit(train_x, train_y, eval_set=[(valid_x, valid_y)], verbose=0, early_stopping_rounds=100)\n#\n#    preds = gbm.predict(valid_x)\n#    pred_labels = np.rint(preds)\n#    accuracy = smape(valid_y, pred_labels)\n#    return accuracy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#study = optuna.create_study(direction='minimize')\n#study.optimize(objective, n_trials=8, timeout=600)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Prophet","metadata":{}},{"cell_type":"code","source":"# Training period is between 2015-01-01 and 2018-01-01\n# Validation period is between 2018-01-01 and 2019-01-01\n# https://www.kaggle.com/gunesevitan/tabular-playground-series-jan-2022-prophet#3.-Holidays\nnew_year = pd.DataFrame({\n  'holiday': 'new_year',\n  'ds': pd.to_datetime(['2015-01-01', '2016-01-01', '2017-01-01', '2018-01-01', '2019-01-01']),\n  'lower_window': -1,\n  'upper_window': 0,\n})\n\neaster = pd.DataFrame({\n  'holiday': 'easter',\n  'ds': pd.to_datetime(['2015-04-05', '2016-03-27', '2017-04-16', '2018-04-01', '2019-04-21']),\n  'lower_window': 0,\n  'upper_window': 7,\n})\n\nholidays = pd.concat((new_year, easter))\n#holidays\n\nfolds = [\n    ('2015-01-01', '2018-01-01'),\n    ('2018-01-01', '2019-01-01'),\n]\n\ndf_train = train.copy()\ndf_test = test.copy()\n\ncountries = df_train['country'].unique()\nstores = df_train['store'].unique()\nproducts = df_train['product'].unique()\n\nfor country in countries:\n    for store in stores:\n        for product in products:\n            for fold, (start, end) in enumerate(folds):\n                # Skip iteration if it's the last fold\n                if fold == len(folds) - 1:\n                    continue\n                    \n                train_idx = (df_train['date'] >= start) &\\\n                            (df_train['date'] < end) &\\\n                            (df_train['country'] == country) &\\\n                            (df_train['store'] == store) &\\\n                            (df_train['product'] == product)\n                train = df_train.loc[train_idx, ['date', 'num_sold']].reset_index(drop=True)\n                train = train.rename(columns={'date': 'ds', 'num_sold': 'y'})\n                val_idx = (df_train['date'] >= folds[fold + 1][0]) &\\\n                          (df_train['date'] < folds[fold + 1][1]) &\\\n                          (df_train['country'] == country) &\\\n                          (df_train['store'] == store) &\\\n                          (df_train['product'] == product)\n                val = df_train.loc[val_idx, ['date', 'num_sold']].reset_index(drop=True)\n                val = val.rename(columns={'date': 'ds', 'num_sold': 'y'})\n                \n                model = Prophet(\n                    growth='linear',\n                    holidays=holidays,\n                    n_changepoints=10,\n                    changepoint_range=0.4,\n                    yearly_seasonality=True,\n                    weekly_seasonality=True,\n                    daily_seasonality=False,\n                    seasonality_mode='additive',\n                    seasonality_prior_scale=25,\n                    holidays_prior_scale=100,\n                    changepoint_prior_scale=0.01,\n                    interval_width=0.5,\n                    uncertainty_samples=False\n                )\n                model.fit(train)\n                \n                train_predictions = model.predict(train[['ds']])['yhat']\n                val_predictions = model.predict(val[['ds']])['yhat']\n                df_train.loc[val_idx, 'prophet_forecast'] =  val_predictions.values\n\n                train_score = smape(train['y'].values, train_predictions.values)\n                val_score = smape(val['y'].values, val_predictions.values)\n                print(f'\\nTraining Range [{start}, {end}) - {country} - {store} - {product} - Train SMAPE: {train_score:4f}')\n                print(f'Validation Range [{folds[fold + 1][0]}, {folds[fold + 1][1]}) - {country} - {store} - {product} - Validation SMAPE: {val_score:4f}\\n')\n                \n                test_idx = (df_test['country'] == country) &\\\n                           (df_test['store'] == store) &\\\n                           (df_test['product'] == product)\n                test = df_test.loc[test_idx, ['date']].reset_index(drop=True)\n                test = test.rename(columns={'date': 'ds'})\n                test_predictions = model.predict(test[['ds']])['yhat']\n                df_test.loc[test_idx, 'prophet_forecast'] = test_predictions.values\n                ","metadata":{"execution":{"iopub.status.busy":"2022-01-15T18:38:32.69017Z","iopub.execute_input":"2022-01-15T18:38:32.690453Z","iopub.status.idle":"2022-01-15T18:38:43.742227Z","shell.execute_reply.started":"2022-01-15T18:38:32.690422Z","shell.execute_reply":"2022-01-15T18:38:43.741293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install pystan==2.19.1.1\n!pip install prophet\n!pip install neuralprophet[live]\nfrom neuralprophet import NeuralProphet\nfolds = [\n    ('2015-01-01', '2018-01-01'),\n    ('2018-01-01', '2019-01-01'),\n]\n\n# Neural Prophet requires holidays to be in one-hot encoded format on all timesteps\nevents = pd.concat((holidays['ds'], pd.get_dummies(holidays['holiday'])), axis=1)\n\nfor country in countries:\n    for store in stores:\n        for product in products:\n            for fold, (start, end) in enumerate(folds):\n                # Skip iteration if it's the last fold\n                if fold == len(folds) - 1:\n                    continue\n                    \n                train_idx = (df_train['date'] >= start) &\\\n                            (df_train['date'] < end) &\\\n                            (df_train['country'] == country) &\\\n                            (df_train['store'] == store) &\\\n                            (df_train['product'] == product)\n                train = df_train.loc[train_idx, ['date', 'num_sold']].reset_index(drop=True)\n                train = train.rename(columns={'date': 'ds', 'num_sold': 'y'})\n                train = train.merge(events, on='ds', how='left').fillna(0)\n                train['easter'] = train['easter'].astype(np.uint8)\n                train['new_year'] = train['new_year'].astype(np.uint8)\n                val_idx = (df_train['date'] >= folds[fold + 1][0]) &\\\n                          (df_train['date'] < folds[fold + 1][1]) &\\\n                          (df_train['country'] == country) &\\\n                          (df_train['store'] == store) &\\\n                          (df_train['product'] == product)\n                val = df_train.loc[val_idx, ['date', 'num_sold']].reset_index(drop=True)\n                val = val.rename(columns={'date': 'ds', 'num_sold': 'y'})\n                val = val.merge(events, on='ds', how='left').fillna(0)\n                val['easter'] = val['easter'].astype(np.uint8)\n                val['new_year'] = val['new_year'].astype(np.uint8)\n                \n                model = NeuralProphet(\n                    growth='linear',\n                    n_changepoints=10,\n                    changepoints_range=0.4,\n                    trend_reg=1,\n                    trend_reg_threshold=False,\n                    yearly_seasonality=True,\n                    weekly_seasonality=True,\n                    daily_seasonality=False,\n                    seasonality_mode='additive',\n                    seasonality_reg=1,\n                    n_forecasts=365,\n                    normalize='off'\n                )\n                model = model.add_events(['new_year'], mode='multiplicative', lower_window=-1)\n                model = model.add_events(['easter'], mode='additive', upper_window=7)\n                model.fit(train, freq='D')\n                \n                train_predictions = model.predict(train)['yhat1']\n                val_predictions = model.predict(val)['yhat1']\n                df_train.loc[val_idx, 'neural_prophet_forecast'] =  val_predictions.values\n\n                train_score = smape(train['y'].values, train_predictions.values)\n                val_score = smape(val['y'].values, val_predictions.values)\n                print(f'\\nTraining Range [{start}, {end}) - {country} - {store} - {product} - Train SMAPE: {train_score:4f}')\n                print(f'Validation Range [{folds[fold + 1][0]}, {folds[fold + 1][1]}) - {country} - {store} - {product} - Validation SMAPE: {val_score:4f}\\n')\n                \n                test_idx = (df_test['country'] == country) &\\\n                           (df_test['store'] == store) &\\\n                           (df_test['product'] == product)\n                test = df_test.loc[test_idx, ['date']].reset_index(drop=True)\n                test = test.rename(columns={'date': 'ds'})\n                test['y'] = np.nan\n                test = test.merge(events, on='ds', how='left').fillna(0)\n                test['easter'] = test['easter'].astype(np.uint8)\n                test['new_year'] = test['new_year'].astype(np.uint8)\n                test_predictions = model.predict(test)['yhat1']\n                df_test.loc[test_idx, 'neural_prophet_forecast'] = test_predictions.values","metadata":{"execution":{"iopub.status.busy":"2022-01-15T18:38:43.747374Z","iopub.execute_input":"2022-01-15T18:38:43.747671Z","iopub.status.idle":"2022-01-15T18:43:40.292683Z","shell.execute_reply.started":"2022-01-15T18:38:43.747634Z","shell.execute_reply":"2022-01-15T18:43:40.291804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_idx = (df_train['date'] >= '2018-01-01') & (df_train['date'] < '2019-01-01')\nprophet_score = smape(df_train.loc[val_idx, 'num_sold'], df_train.loc[val_idx, 'prophet_forecast'])\nneural_prophet_score = smape(df_train.loc[val_idx, 'num_sold'], df_train.loc[val_idx, 'neural_prophet_forecast'])\nprint(f'Prophet - Validation SMAPE: {prophet_score:6f}')\nprint(f'Neural Prophet - Validation SMAPE: {neural_prophet_score:6f}')","metadata":{"execution":{"iopub.status.busy":"2022-01-15T18:43:40.294184Z","iopub.execute_input":"2022-01-15T18:43:40.294447Z","iopub.status.idle":"2022-01-15T18:43:40.30803Z","shell.execute_reply.started":"2022-01-15T18:43:40.294412Z","shell.execute_reply":"2022-01-15T18:43:40.307211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_all = pd.concat((df_train, df_test), axis=0, ignore_index=True)\ntest_idx = (df_all['date'] >= '2019-01-01') & (df_all['date'] < '2020-01-01')\ndf_submission = df_all.loc[test_idx, ['row_id', 'prophet_forecast', 'neural_prophet_forecast']].reset_index(drop=True)\ndf_submission['num_sold'] = (df_submission['prophet_forecast'] + df_submission['neural_prophet_forecast']) / 2\ndf_submission[['row_id', 'num_sold']].to_csv('submission_prophet_neuprophet.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-15T18:43:40.310356Z","iopub.execute_input":"2022-01-15T18:43:40.310607Z","iopub.status.idle":"2022-01-15T18:43:40.356331Z","shell.execute_reply.started":"2022-01-15T18:43:40.310574Z","shell.execute_reply":"2022-01-15T18:43:40.355634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#df_train['ds'] = df_train['date']\n#predss = model.predict(df_train)[['ds', 'yhat']].set_index('ds')","metadata":{"execution":{"iopub.status.busy":"2022-01-15T18:44:43.020645Z","iopub.execute_input":"2022-01-15T18:44:43.021128Z","iopub.status.idle":"2022-01-15T18:44:43.025314Z","shell.execute_reply.started":"2022-01-15T18:44:43.021086Z","shell.execute_reply":"2022-01-15T18:44:43.023774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df_all[19728:26298][['num_sold']]\npred = df_all[19728:26298][['prophet_forecast']]","metadata":{"execution":{"iopub.status.busy":"2022-01-15T18:44:45.08152Z","iopub.execute_input":"2022-01-15T18:44:45.082086Z","iopub.status.idle":"2022-01-15T18:44:45.088257Z","shell.execute_reply.started":"2022-01-15T18:44:45.082041Z","shell.execute_reply":"2022-01-15T18:44:45.087452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_plot_prophet(pred, df):\n    print('==================================')\n    print('----------- Metrics --------------')\n    rss = np.sum((pred.values-df.values)**2)\n    smape_val = smape(df.values, pred.values)\n    print(f'RSS | {rss:.4f}')\n    print(f'SMAPE | {smape_val:.4f}')\n    print('----------------------------------')\n    #plt.plot(res.fittedvalues, alpha=.7)\n    #plt.plot(df, alpha = 0.7)\n    colormap = ['#1DBA94','#1C5ED2', '#FFC300', '#C70039']\n    background_color='#f6f5f5'\n    plt.rcParams['figure.dpi'] = 300\n    fig, axs = plt.subplots(3, figsize=(18, 12), facecolor='#f6f5f5')\n    fig.subplots_adjust(hspace=0.6, wspace=0.3)\n    axs[0].plot(df, color = colormap[3], alpha = 0.7)\n    axs[0].plot(pred, color = colormap[1], alpha = 0.7)\n    axs[0].legend(['Original', 'Model'], ncol=2, facecolor=background_color, edgecolor=background_color, loc='upper center')\n    sm.graphics.tsa.plot_acf(pred, lags=12*4, ax=axs[1])\n    sm.graphics.tsa.plot_pacf(pred, lags=12*4, ax=axs[2])\n    for i in range(3):\n        for s in [\"top\",\"right\"]:\n            axs[i].spines[s].set_visible(False)\n        axs[i].set_facecolor(background_color)\n        #ax.grid(which='major', axis='x', zorder=-2, color='#EEEEEE', linewidth=0.4)\n        axs[i].xaxis.offsetText.set_fontsize(4)\n        axs[i].yaxis.offsetText.set_fontsize(4)\n        axs[i].set_ylabel('')\n        axs[i].set_xlabel('')\n        axs[i].tick_params(labelsize=8, width=1)\n        \n    axs[0].set_title(f'Model', fontdict={'fontsize': 12, 'fontweight': 'bold'})\n    axs[1].set_title('Autocorrelation', fontdict={'fontsize': 12, 'fontweight': 'bold'})\n    axs[2].set_title('Partial Autocorrelation', fontdict={'fontsize': 12, 'fontweight': 'bold'})\n    \n    layout = (2, 2)\n    fig = plt.figure(figsize=(18, 12), facecolor='#f6f5f5')\n    qq_ax = plt.subplot2grid(layout, (0, 0))\n    pp_ax = plt.subplot2grid(layout, (0, 1))\n    sm.qqplot(pred, line='s', ax=qq_ax)\n    #scs.probplot(pred, sparams=(pred.mean(), pred.std()), plot=pp_ax)\n    for i in [qq_ax, pp_ax]:\n        for s in [\"top\",\"right\"]:\n            i.spines[s].set_visible(False)\n        i.set_facecolor(background_color)\n        #ax.grid(which='major', axis='x', zorder=-2, color='#EEEEEE', linewidth=0.4)\n        i.xaxis.offsetText.set_fontsize(4)\n        i.yaxis.offsetText.set_fontsize(4)\n        i.set_ylabel('')\n        i.set_xlabel('T-Quantilies')\n        i.tick_params(labelsize=8, width=1)\n    qq_ax.set_title('QQ-Plot', fontdict={'fontsize': 12, 'fontweight': 'bold'})\n    pp_ax.set_title('Probability Plot', fontdict={'fontsize': 12, 'fontweight': 'bold'})","metadata":{"execution":{"iopub.status.busy":"2022-01-15T18:44:50.7617Z","iopub.execute_input":"2022-01-15T18:44:50.761974Z","iopub.status.idle":"2022-01-15T18:44:50.779627Z","shell.execute_reply.started":"2022-01-15T18:44:50.761929Z","shell.execute_reply":"2022-01-15T18:44:50.778775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_plot_prophet(pred, df)","metadata":{"execution":{"iopub.status.busy":"2022-01-15T18:44:52.435172Z","iopub.execute_input":"2022-01-15T18:44:52.435998Z","iopub.status.idle":"2022-01-15T18:44:55.292451Z","shell.execute_reply.started":"2022-01-15T18:44:52.435935Z","shell.execute_reply":"2022-01-15T18:44:55.291631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df_all[19728:26298][['num_sold']]\npred = df_all[19728:26298][['neural_prophet_forecast']]\nget_plot_prophet(pred, df)","metadata":{"execution":{"iopub.status.busy":"2022-01-15T18:44:55.294072Z","iopub.execute_input":"2022-01-15T18:44:55.294374Z","iopub.status.idle":"2022-01-15T18:44:57.926851Z","shell.execute_reply.started":"2022-01-15T18:44:55.29434Z","shell.execute_reply":"2022-01-15T18:44:57.926024Z"},"trusted":true},"execution_count":null,"outputs":[]}]}