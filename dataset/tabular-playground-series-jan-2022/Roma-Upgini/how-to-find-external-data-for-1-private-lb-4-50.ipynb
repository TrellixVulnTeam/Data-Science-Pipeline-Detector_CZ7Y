{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# üîé How to find relevant external features & data for kaggle competitions in 10 minutes [1/3]\n### Part #1 - Improve accuracy of Kaggle TOP1 leaderboard notebook in 10 minutes\n##### [Part #2 Link](https://www.kaggle.com/code/romaupgini/zero-feature-engineering-with-upgini-pycaret)\n##### [Part #3 Link](https://www.kaggle.com/code/romaupgini/external-data-features-for-multivariate-ts)\n______________________________\n*updated 2022-05-27 [@roma-upgini](https://www.kaggle.com/romaupgini)*\n\n**‚ùì Before reading the notebook, what will you learn from it?**\n\n1. How external data & features might help on Kaggle: two scenarios\n2. How to find relevant external features in less than 10 minutes and save time on feature engineering \n3. How to calculate metrics and uplifts from new external features\n4. What external data sources might help you on Kaggle competitions\n\nüó£ Share this notebook: [Shareable Link](https://www.kaggle.com/code/romaupgini/guide-how-to-find-relevant-external-features-1)\n______________________________\n### Table of contents\n* [Intro](#Intro)\n\n* [How external data & features might help on Kaggle?](#How-external-data-&-features-might-help-on-Kaggle?)\n\n* [Packages and functions](#Packages-and-functions)\n\n* [Final improvement of polished kernel](#Final-improvement-of-polished-kernel)\n\n    - [1Ô∏è‚É£ Let's take existing TOP-1 winning solution with external data as a baseline](#1%EF%B8%8F%E2%83%A3-Let's-take-existing-TOP-1-winning-solution-with-external-data-as-a-baseline)\n    - [2Ô∏è‚É£ Find relevant external features](#2Ô∏è‚É£-Find-relevant-external-features)\n    \n* [External data sources & features](#%F0%9F%8C%8E-External-data-sources-&-features)\n* [References](#References)  \n\n## Intro\n**Competition**: [TPS January 2022](https://www.kaggle.com/competitions/tabular-playground-series-jan-2022), SMAPE as a target metric  \n**Special thanks**: [@ambrosm](https://www.kaggle.com/ambrosm) for the 1st place [notebook](https://www.kaggle.com/code/ambrosm/tpsjan22-10-advanced-linear-model-with-cci/notebook) and great disscussion on external data [here](https://www.kaggle.com/competitions/tabular-playground-series-jan-2022/discussion/302694)  \nüìö In this notebook we'll use:\n* [Upgini](https://github.com/upgini/upgini#readme) - Low-code Feature search and enrichment library for supervised machine learning applications.   \n<a href=\"https://github.com/upgini/upgini\">\n    <img src=\"https://img.shields.io/badge/GitHub-100000?style=for-the-badge&logo=github&logoColor=white\"  align='center'>\n</a>  \n\n**Baseline model** in this notebook is based on *@ambrosm* notebook (first place) with some minor changes:\n\n* Feature engineering part was slightly changed, so we can prepare main features and external features separately;\n* SimpleImputer was added to dataprep pipeline to deal with missing values while adding new external features;\n* Constant scaling factor for the test predictions was removed.","metadata":{}},{"cell_type":"markdown","source":"## How external data & features might help on Kaggle?\nKaggle is always about learning and leader board progress (hopefully from learning, not cheating ;-))  \nAnd every Kaggler wants to progress as fast as possible, so time saving tips & tricks is a big deal as well.  \nThat's why low-code tools is adopted among kagglers.\n\nSo, there are **two major scenarios** of external features & data introduction in competitions on Kaggle:\n\n1. **Final improvement of a polished kernel**  \nIn this scenario you want **to improve already polished kernel** (optimized features, model architecture and hyperparams) with new external features.  \nBefore that, most of the juice already has been \"squeezed\" from competition data by significant efforts in feature engineering.  \nAnd you want to answer the simple question - *Is there any external data sources and features which might boost accuracy a bit more?*  \nHowever, there is a caveat to this approach: current model architecture & hyperparameters might be suboptimal for the new feature set, after introduction even single new var.  \nSo extra step back for model tuning might be needed.\n\n2. **Low-code initial feature engineering - add relevant external features @start**  \nHere you want to **save time on feature search and engineering**. If there are some ready-to-use external features and data, let's use it to speed up the overall progress.  \nIn this scenario always make sense to check that new external features have optimal representation for specific task and target model architecture. Example - category features for linear regression models should be one-hot-encoded.\nThis type of feature preparation should be done manually in any case.  \nSame as scenario #1, there is a caveat to this approach: a lot of features not always a good thing - they might lead to dimensionality increase and model overfitting.  \nSo you have to check model accuracy improvement metrics after enrichment with the new features and ALWAYS with appropriate cross-validation strategy.\n \nIn this Guide we'll go with **Scenario #1**. Also you can check out guide for the [**Scenario #2**](https://www.kaggle.com/code/romaupgini/zero-feature-engineering-with-upgini-pycaret).","metadata":{}},{"cell_type":"markdown","source":"## Packages and functions","metadata":{}},{"cell_type":"code","source":"%pip install -Uq upgini\n\nimport pandas as pd\nimport numpy as np\nimport pickle\nimport itertools\nimport gc\nimport math\nimport matplotlib.pyplot as plt\nimport dateutil.easter as easter\nfrom datetime import datetime, date, timedelta\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import KFold, GroupKFold, TimeSeriesSplit\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import make_pipeline\nimport lightgbm as lgb\nimport scipy.stats\nimport os\n\ndef smape_loss(y_true, y_pred):\n    return np.mean(np.abs(y_true - y_pred) / (y_true + np.abs(y_pred))) * 200\n\ndef read_main_data(input_data_path):\n    train_df = pd.read_csv(f'{input_data_path}/tabular-playground-series-jan-2022/train.csv')\n    test_df = pd.read_csv(f'{input_data_path}/tabular-playground-series-jan-2022/test.csv')\n    train_df[\"segment\"], test_df[\"segment\"] = \"train\", \"test\"\n    kaggle_rama_sold = train_df[train_df.store == \"KaggleRama\"].num_sold.values\n    kaggle_mart_sold = train_df[train_df.store == \"KaggleMart\"].num_sold.values\n    kaggle_rama_ratio = np.mean(kaggle_rama_sold/kaggle_mart_sold)\n    \n    df = pd.concat([train_df, test_df]).reset_index(drop=True)\n    df['date'] = pd.to_datetime(df.date)\n    \n    return df, kaggle_rama_ratio\n\ndef read_additional_data():\n    gdp_df = pd.read_csv(\n        f'{input_data_path}/gdp-20152019-finland-norway-and-sweden/GDP_data_2015_to_2019_Finland_Norway_Sweden.csv'\n    )\n    gdp_df.set_index('year', inplace=True)\n\n    cci_df = pd.read_csv(f'{input_data_path}/oecd-consumer-confidence-index/DP_LIVE_21012022073653464.csv')\n    cci_df.set_index(['LOCATION', 'TIME'], inplace=True)\n    \n    return gdp_df, cci_df","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-22T14:43:31.928169Z","iopub.execute_input":"2022-05-22T14:43:31.92859Z","iopub.status.idle":"2022-05-22T14:43:44.253679Z","shell.execute_reply.started":"2022-05-22T14:43:31.928545Z","shell.execute_reply":"2022-05-22T14:43:44.252457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_main_features(df):\n    new_df = df[[\"row_id\", \"date\", \"country\", \"segment\", \"num_sold\"]].copy()\n    \n    ## one-hot encoding\n    new_df['KaggleRama'] = df.store == 'KaggleRama'\n    for country in ['Finland', 'Norway']:\n        new_df[country] = df.country == country\n    for product in ['Kaggle Mug', 'Kaggle Hat']:\n        new_df[product] = df['product'] == product\n           \n    ## datetime features\n    new_df['wd4'] = np.where(df.date.dt.weekday == 4, 1, 0)\n    new_df['wd56'] = np.where(df.date.dt.weekday >= 5, 1, 0)\n    \n    dayofyear = df.date.dt.dayofyear\n    for k in range(1, 3):\n        sink = np.sin(dayofyear / 365 * 2 * math.pi * k)\n        cosk = np.cos(dayofyear / 365 * 2 * math.pi * k)\n        new_df[f'mug_sin{k}'] = sink * new_df['Kaggle Mug']\n        new_df[f'mug_cos{k}'] = cosk * new_df['Kaggle Mug']\n        new_df[f'hat_sin{k}'] = sink * new_df['Kaggle Hat']\n        new_df[f'hat_cos{k}'] = cosk * new_df['Kaggle Hat']\n    new_df.drop(columns=['mug_sin1'], inplace=True)\n    new_df.drop(columns=['mug_sin2'], inplace=True)\n        \n    # special days\n    new_df = pd.concat([\n        new_df,\n        pd.DataFrame({f\"dec{d}\":(df.date.dt.month == 12) & (df.date.dt.day == d) for d in range(24, 32)}),\n        pd.DataFrame({\n            f\"n-dec{d}\": (df.date.dt.month == 12) & (df.date.dt.day == d) & (df.country == 'Norway') \n            for d in range(25, 32)\n        }),\n        pd.DataFrame({\n            f\"f-jan{d}\": (df.date.dt.month == 1) & (df.date.dt.day == d) & (df.country == 'Finland')\n            for d in range(1, 15)\n        }),\n        pd.DataFrame({\n            f\"n-jan{d}\": (df.date.dt.month == 1) & (df.date.dt.day == d) & (df.country == 'Norway')\n            for d in range(1, 10)\n        }),\n        pd.DataFrame({\n            f\"s-jan{d}\": (df.date.dt.month == 1) & (df.date.dt.day == d) & (df.country == 'Sweden')\n            for d in range(1, 15)\n        })\n    ], axis=1)\n    \n    # May and June\n    new_df = pd.concat([\n        new_df,\n        pd.DataFrame({\n            f\"may{d}\": (df.date.dt.month == 5) & (df.date.dt.day == d) \n            for d in list(range(1, 10))\n        }),\n        pd.DataFrame({\n            f\"may{d}\": (df.date.dt.month == 5) & (df.date.dt.day == d) & (df.country == 'Norway')\n            for d in list(range(18, 26)) + [27]\n        }),\n        pd.DataFrame({\n            f\"june{d}\": (df.date.dt.month == 6) & (df.date.dt.day == d) & (df.country == 'Sweden')\n            for d in list(range(8, 15))\n        })\n    ], axis=1)\n    \n    # Last Wednesday of June\n    wed_june_map = {\n        2015: pd.Timestamp(('2015-06-24')),\n        2016: pd.Timestamp(('2016-06-29')),\n        2017: pd.Timestamp(('2017-06-28')),\n        2018: pd.Timestamp(('2018-06-27')),\n        2019: pd.Timestamp(('2019-06-26'))\n    }\n    wed_june_date = df.date.dt.year.map(wed_june_map)\n    new_df = pd.concat([\n        new_df,\n        pd.DataFrame({\n            f\"wed_june{d}\": (df.date - wed_june_date == np.timedelta64(d, \"D\")) & (df.country != 'Norway')\n            for d in list(range(-4, 5))\n        })\n    ], axis=1)\n    \n    # First Sunday of November\n    sun_nov_map = {\n        2015: pd.Timestamp(('2015-11-1')),\n        2016: pd.Timestamp(('2016-11-6')),\n        2017: pd.Timestamp(('2017-11-5')),\n        2018: pd.Timestamp(('2018-11-4')),\n        2019: pd.Timestamp(('2019-11-3'))\n    }\n    sun_nov_date = df.date.dt.year.map(sun_nov_map)\n    new_df = pd.concat([\n        new_df,\n        pd.DataFrame({\n            f\"sun_nov{d}\": (df.date - sun_nov_date == np.timedelta64(d, \"D\")) & (df.country != 'Norway')\n            for d in list(range(0, 9))\n        })\n    ], axis=1)\n    \n    # First half of December (Independence Day of Finland, 6th of December)\n    new_df = pd.concat([\n        new_df,\n        pd.DataFrame({\n            f\"dec{d}\": (df.date.dt.month == 12) & (df.date.dt.day == d) & (df.country == 'Finland')\n            for d in list(range(6, 15))\n        }\n    )], axis=1)\n\n    # Easter\n    easter_date = df.date.apply(lambda date: pd.Timestamp(easter.easter(date.year)))\n    new_df = pd.concat([\n        new_df,\n        pd.DataFrame({\n            f\"easter{d}\": (df.date - easter_date == np.timedelta64(d, \"D\"))\n            for d in list(range(-2, 11)) + list(range(40, 48)) + list(range(51, 58))\n        }),\n        pd.DataFrame({\n            f\"n_easter{d}\": (df.date - easter_date == np.timedelta64(d, \"D\")) & (df.country == 'Norway')\n            for d in list(range(-3, 8)) + list(range(50, 61))\n        })\n    ], axis=1)\n    \n    features_list = [\n        f for f in new_df.columns \n        if f not in [\"row_id\", \"date\", \"segment\", \"country\", \"num_sold\", \"KaggleRama\"]\n    ]\n    return new_df, features_list\n\ndef get_gdp(row):\n    country = 'GDP_' + row.country\n    return gdp_df.loc[row.date.year, country]\n        \ndef get_cci(row):\n    country = row.country\n    time = f\"{row.date.year}-{row.date.month:02d}\"\n    if country == 'Norway': country = 'Finland'\n    return cci_df.loc[country[:3].upper(), time].Value\n\ndef generate_extra_features(df, features_list):\n    df['gdp'] = np.log(df.apply(get_gdp, axis=1))\n    df['cci'] = df.apply(get_cci, axis=1)\n    features_list_upd = features_list + [\"gdp\", \"cci\"]\n    \n    return df, features_list_upd","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-22T14:43:44.256385Z","iopub.execute_input":"2022-05-22T14:43:44.256783Z","iopub.status.idle":"2022-05-22T14:43:44.30393Z","shell.execute_reply.started":"2022-05-22T14:43:44.256732Z","shell.execute_reply":"2022-05-22T14:43:44.303204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fit_model(model, df, features_list, kaggle_rama_ratio):\n    cols_to_scale = [\n        'wd4', 'wd56', 'Finland', 'Norway', 'Kaggle Mug', 'Kaggle Hat',\n        'mug_cos1', 'hat_sin1', 'hat_cos1', 'mug_cos2','hat_sin2', 'hat_cos2',\n        'gdp'\n    ]\n    cols_to_scale = [f for f in cols_to_scale if f in features_list]\n    stages = [('general', MinMaxScaler(), cols_to_scale)]\n    if \"cci\" in features_list:\n        stages.append(('cci', MinMaxScaler((0, 0.06)), ['cci']))\n    column_tr = ColumnTransformer(stages, remainder=MinMaxScaler((0, 2.8)))\n    dataprep_ppl = make_pipeline(column_tr, SimpleImputer(), StandardScaler(with_std=False))\n    X_train = dataprep_ppl.fit_transform(df[features_list])\n    y_train = df.num_sold.values.reshape(-1, 1).copy()\n    y_train[df.KaggleRama.values > 0] = y_train[df.KaggleRama.values > 0] / kaggle_rama_ratio\n    y_train = np.log(y_train).ravel()\n    fitted_model = model.fit(X_train, y_train)\n    model_coef = (\n        pd.DataFrame({\"name\": features_list, \"coef\": np.abs(model.coef_)})\n        .sort_values(\"coef\", ascending=False)\n        .reset_index(drop=True)\n    )\n        \n    return dataprep_ppl, fitted_model, model_coef\n\ndef predict(dataprep_ppl, fitted_model, df, features_list, kaggle_rama_ratio):\n    X_pred = dataprep_ppl.transform(df[features_list])\n    y_pred = fitted_model.predict(X_pred)\n    y_pred = np.exp(y_pred).reshape(-1, 1)\n    y_pred[df.KaggleRama.values > 0] = y_pred[df.KaggleRama.values > 0] * kaggle_rama_ratio\n    return y_pred\n\ndef cross_validate(model, df, features_list, kaggle_rama_ratio, cv=None):\n    np.random.seed(0)\n    scores_list = []\n    df_train = df.query(\"segment == 'train'\").reset_index(drop=True)\n    model_coef = pd.DataFrame({\"name\": []})\n    for fold, (train_idx, val_idx) in enumerate(cv.split(df_train)):\n        df_tr, df_val = df_train.iloc[train_idx], df_train.iloc[val_idx]\n        y_val = df_val.num_sold.values.reshape(-1, 1)\n        \n        dataprep_ppl, fitted_model, model_coef_ = fit_model(model, df_tr, features_list, kaggle_rama_ratio)\n        model_coef = (\n            model_coef\n            .merge(\n                model_coef_.rename(columns={\"coef\": f\"coef_{fold}\"}), \n                on=\"name\", how=\"outer\"\n            )\n        )\n        y_val_pred = predict(dataprep_ppl, fitted_model, df_val, features_list, kaggle_rama_ratio)\n        score = round(smape_loss(y_val, y_val_pred), 3)\n        scores_list.append(score)\n        \n    return scores_list, model_coef\n\ndef make_submission(model, df, features_list, kaggle_rama_ratio, submission_path=\"\"):\n    df_train, df_test = df[df.segment == \"train\"].copy(), df[df.segment == \"test\"].copy()\n    dataprep_ppl, fitted_model, _ = fit_model(model, df_train, features_list, kaggle_rama_ratio)\n    df_sub = df_test[['row_id']].copy()\n    df_sub[\"num_sold\"] = predict(dataprep_ppl, fitted_model, df_test, features_list, kaggle_rama_ratio)\n    df_sub[\"num_sold\"] = np.round(df_sub[\"num_sold\"])\n    df_sub.to_csv(submission_path, index=False)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-22T14:43:44.306811Z","iopub.execute_input":"2022-05-22T14:43:44.307219Z","iopub.status.idle":"2022-05-22T14:43:44.338563Z","shell.execute_reply.started":"2022-05-22T14:43:44.307175Z","shell.execute_reply":"2022-05-22T14:43:44.336711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Final improvement of polished kernel\n## 1Ô∏è‚É£ Let's take existing TOP-1 winning solution with external data as a baseline","metadata":{}},{"cell_type":"markdown","source":"There is already an external data in this solution, so improvement shouldn't be an easy walk ;-):\n\n1) *GDP statistics per year/country* (\"gdp-20152019-finland-norway-and-sweden\" dataset);  \n2) *Consumer Confidence Index* per year/month/country (Value field of \"oecd-consumer-confidence-index\" dataset).\n\nAnd we want to improve winning kernel by finding new/better external features.  \n\nThere is no changes in feature engineering from original [notebook](https://www.kaggle.com/code/ambrosm/tpsjan22-10-advanced-linear-model-with-cci/notebook) by [@ambrosm](https://www.kaggle.com/ambrosm).</br>\nSo let's calculate metrics for baseline solution.  \nFirst, read train/test data from csv, combine them in one dataframe and generate features:\n","metadata":{}},{"cell_type":"code","source":"input_data_path = \"/kaggle/input\"\ndf, kaggle_rama_ratio = read_main_data(input_data_path)\n\n# a lot of calendar based features\ndf, baseline_features = generate_main_features(df)\nprint(df.shape)\nprint(\"Number of features:\", len(baseline_features))\ndf.segment.value_counts()\n\n# features from GDP and CCI\ngdp_df, cci_df = read_additional_data()\ndf, top_solution_features = generate_extra_features(df, baseline_features)\nprint(df.shape)\nset(top_solution_features) - set(baseline_features)","metadata":{"execution":{"iopub.status.busy":"2022-05-22T14:43:44.340035Z","iopub.execute_input":"2022-05-22T14:43:44.34077Z","iopub.status.idle":"2022-05-22T14:43:58.789752Z","shell.execute_reply.started":"2022-05-22T14:43:44.340722Z","shell.execute_reply":"2022-05-22T14:43:58.788253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Define model, cross-validation split and apply cross-validation to estimate model accuracy:","metadata":{}},{"cell_type":"code","source":"model = Ridge(alpha=0.2, tol=0.00001, max_iter=10000)\ncv = KFold(n_splits=5)\n\ntop_solution_scores, model_coef = cross_validate(model, df, top_solution_features, kaggle_rama_ratio, cv=cv)\nprint(\"Top solution SMAPE by folds:\", top_solution_scores)\nprint(\"Top solution avg SMAPE:\", sum(top_solution_scores)/len(top_solution_scores))","metadata":{"execution":{"iopub.status.busy":"2022-05-22T14:43:58.791245Z","iopub.execute_input":"2022-05-22T14:43:58.791507Z","iopub.status.idle":"2022-05-22T14:44:01.018558Z","shell.execute_reply.started":"2022-05-22T14:43:58.791477Z","shell.execute_reply":"2022-05-22T14:44:01.017236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, make submission file:","metadata":{}},{"cell_type":"code","source":"submission_path = 'submission_top_solution.csv'\nmake_submission(model, df, top_solution_features, kaggle_rama_ratio, submission_path)","metadata":{"execution":{"iopub.status.busy":"2022-05-22T14:44:01.022122Z","iopub.execute_input":"2022-05-22T14:44:01.023094Z","iopub.status.idle":"2022-05-22T14:44:01.606164Z","shell.execute_reply.started":"2022-05-22T14:44:01.023005Z","shell.execute_reply":"2022-05-22T14:44:01.604863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Submission has score of **4.13** on public LB and **4.55** on private LB (**first place in the competition**). \nThis is our baseline.  \nCan we improve the first place solution even more? Let's find out!","metadata":{}},{"cell_type":"markdown","source":"## 2Ô∏è‚É£ Find relevant external features","metadata":{}},{"cell_type":"markdown","source":"To find new features we'll use [Upgini Feature search and enrichment library for supervised machine learning applications](https://github.com/upgini/upgini#readme)  \nTo initiate search with Upgini library, you need to define so called [*search keys*](https://github.com/upgini/upgini#-search-key-types-we-support-more-is-coming) - a set of columns to join external data sources. In this competition we can use the following keys:\n\n1. Column **date** should be used as **SearchKey.DATE**.;  \n2. Column **country** (after conversion to ISO-3166 country code) should be used as **SearchKey.COUNTRY**.\n    \nWith this set of search keys, our dataset will be matched with [different time-specific features (such as weather data, calendar data, financial data, etc)](https://github.com/upgini/upgini#-connected-data-sources-and-coverage), taking into account the country where sales happened. Than relevant selection and ranking will be done.  \nAs a result, we'll add new, only relevant features with additional information about specific dates and countries.","metadata":{}},{"cell_type":"code","source":"from upgini import SearchKey\n\n# here we simply map each country to its ISO-3166 code\ncountry_iso_map = {\n    \"Finland\": \"FI\",\n    \"Norway\": \"NO\",\n    \"Sweden\": \"SE\"\n}\ndf[\"country_iso\"] = df.country.map(country_iso_map)\n\n## define search keys\nsearch_keys = {\n    \"date\": SearchKey.DATE, \n    \"country_iso\": SearchKey.COUNTRY\n}","metadata":{"execution":{"iopub.status.busy":"2022-05-22T14:44:01.608192Z","iopub.execute_input":"2022-05-22T14:44:01.608914Z","iopub.status.idle":"2022-05-22T14:44:02.612113Z","shell.execute_reply.started":"2022-05-22T14:44:01.608852Z","shell.execute_reply":"2022-05-22T14:44:02.611296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To start the search, we need to initiate *scikit-learn* compartible `FeaturesEnricher` transformer with appropriate **search** parameters. After that, we can call the **fit** method of `features_enricher` to start the search.\n> The ratio between KaggleRama and KaggleMart sales is a constant, so we'll use only KaggleMart sales for feature search and model training","metadata":{}},{"cell_type":"code","source":"%%time\nfrom upgini import FeaturesEnricher\nfrom upgini.metadata import CVType, RuntimeParameters\n\n## define X_train / y_train, remove KaggleMart\ncondition = (df.segment == \"train\") & (df.KaggleRama == False)\nX_train, y_train = df.loc[condition, list(search_keys.keys()) + top_solution_features], df.loc[condition, \"num_sold\"]\n\n## define Features Enricher\nfeatures_enricher = FeaturesEnricher(\n    search_keys = search_keys\n)","metadata":{"execution":{"iopub.status.busy":"2022-05-22T14:44:44.587813Z","iopub.execute_input":"2022-05-22T14:44:44.588241Z","iopub.status.idle":"2022-05-22T14:44:50.34658Z","shell.execute_reply.started":"2022-05-22T14:44:44.588197Z","shell.execute_reply":"2022-05-22T14:44:50.345324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`FeaturesEnricher.fit()` has a flag `calculate_metrics` for the quick estimation of quality improvement on cross-validation and eval sets. This step is quite similar to [sklearn.model_selection.cross_val_score](https://scikit-learn.org/stable/modules/cross_validation.html#computing-cross-validated-metrics), so you can pass exact metric with `scoring` parameter:\n\n1. Built-in scoring [functions](https://github.com/upgini/upgini/blob/main/README.md#-accuracy-and-uplift-metrics-calculations);\n2. Custom scorer (in this case - scorer based on SMAPE loss).    \n\nAnd we pass final Ridge model estimator with parameter `estimator`, for correct metric calculation, right in search results.  \nNotice that you should pass **X_train** as the first argument and **y_train** as the second argument for `FeaturesEnricher.fit()`, just like in scikit-learn.  \n\n*Step will take around 3.5 minutes*","metadata":{}},{"cell_type":"code","source":"%%time\nfrom sklearn.metrics import make_scorer\n\n## define SMAPE custom scoring function\nscorer = make_scorer(smape_loss, greater_is_better=False)\nscorer.__name__ = \"SMAPE\"\n\n## launch fit\nfeatures_enricher.fit(X_train, y_train,\n                      calculate_metrics = True,\n                      scoring = scorer,\n                      estimator = model)","metadata":{"execution":{"iopub.status.busy":"2022-05-22T14:45:06.488319Z","iopub.execute_input":"2022-05-22T14:45:06.488632Z","iopub.status.idle":"2022-05-22T14:49:32.119723Z","shell.execute_reply.started":"2022-05-22T14:45:06.488602Z","shell.execute_reply":"2022-05-22T14:49:32.118495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We've got **60+ relevant features**, which might improve accuracy of the model. Ranked by [SHAP values](https://en.wikipedia.org/wiki/Shapley_value).  \n\nInitial features from search dataset will be checked for relevancy as well, so you don't need an extra feature selection step.\n\nSMAPE uplift after enrichment with all of the new external features is *negative*  - as it doesn't make sense to use ALL of them for linear Ridge model.  \nLet's enrich initial feature space with only **TOP-3** most important features.\n\n>Generally it's a bad idea to put a lot of features with unknown structure (and possibly high pairwise correlation) into a linear model, like Ridge or Lasso without careful selection and pre-processing.\n\n*Step will take around 2 minutes*","metadata":{}},{"cell_type":"code","source":"%%time\n\n## call transform and enrich dataset with TOP-3 features only\ndf_enriched = features_enricher.transform(df, max_features=3, keep_input = True)\n\n## put top-3 new external features names into selected features list\nenricher_features = [\n    f for f in features_enricher.get_features_info().feature_name.values\n    if f not in list(search_keys.keys()) + top_solution_features\n]\nbest_enricher_features = enricher_features[:3]","metadata":{"execution":{"iopub.status.busy":"2022-05-22T14:49:32.122725Z","iopub.execute_input":"2022-05-22T14:49:32.123843Z","iopub.status.idle":"2022-05-22T14:51:26.436978Z","shell.execute_reply.started":"2022-05-22T14:49:32.123778Z","shell.execute_reply":"2022-05-22T14:51:26.435813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Top-3 of found features:\")\nbest_enricher_features","metadata":{"execution":{"iopub.status.busy":"2022-05-22T14:51:26.439501Z","iopub.execute_input":"2022-05-22T14:51:26.440117Z","iopub.status.idle":"2022-05-22T14:51:26.447598Z","shell.execute_reply.started":"2022-05-22T14:51:26.440048Z","shell.execute_reply":"2022-05-22T14:51:26.446789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3Ô∏è‚É£ Submit and calculate final leaderbord progress\nLet's estimate model quality and make a submission file:","metadata":{}},{"cell_type":"code","source":"#same cross-validation split and model estimator as for baseline notebook in #1 Part\nupgini_scores, model_coef = cross_validate(\n    model, df_enriched, \n    top_solution_features + best_enricher_features, \n    kaggle_rama_ratio, cv=cv\n)\nprint(\"Top solution SMAPE by folds:\", top_solution_scores)\nprint(\"Upgini SMAPE by folds:\", upgini_scores)\nprint(\"Top solution avg SMAPE:\", sum(top_solution_scores)/len(top_solution_scores))\nprint(\"Upgini avg SMAPE:\", sum(upgini_scores)/len(upgini_scores))","metadata":{"execution":{"iopub.status.busy":"2022-05-22T14:51:26.449458Z","iopub.execute_input":"2022-05-22T14:51:26.45015Z","iopub.status.idle":"2022-05-22T14:51:34.334994Z","shell.execute_reply.started":"2022-05-22T14:51:26.449893Z","shell.execute_reply":"2022-05-22T14:51:34.329411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_path = 'submission.csv'\nmake_submission(\n    model, df_enriched, \n    top_solution_features + best_enricher_features, \n    kaggle_rama_ratio, submission_path\n)","metadata":{"execution":{"iopub.status.busy":"2022-05-22T14:51:34.337196Z","iopub.execute_input":"2022-05-22T14:51:34.337938Z","iopub.status.idle":"2022-05-22T14:51:36.293445Z","shell.execute_reply.started":"2022-05-22T14:51:34.337882Z","shell.execute_reply":"2022-05-22T14:51:36.291324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This submission has score of **4.095** on public LB and **4.50** on private LB.  \nJust to remider - baseline TOP-1 solution had **4.13** on public LB and **4.55** on private LB (*with 2 external data sources already*).   \n**We've got a consistent improvement both on public and private parts of LB!**","metadata":{}},{"cell_type":"markdown","source":"## üåé Relevant external features & data sources\nLeader board accuracy improved from enrichment with 3 new external features:\n\n* **f_cci_1y_shift_0fa85f6f** - Consumer Confidence Index with 1 year lag. It's a Consumer Confidence Index value derivative for Finland and Sweden (CCI not available for Norway). CCI as feature already has been introduced in baseline notebook, but as a raw CCI index value with scaling on data prep step.\n\n* **f_pcpiham_wt_531b4347** -  Consumer Price index for Health group of products & services. In general, Consumer Price indexes are index numbers that measure changes in the prices of goods and services purchased or otherwise acquired by households, which households use directly, or indirectly, to satisfy their own needs and wants.  \nSo it has a lot of information about inflation in specific country and for specific type of services and goods.  \nIt's been updated by the [Organisation for Economic Cooperation and Development (OECD)](https://data.oecd.org/price/inflation-cpi.htm) on a monthly basis.\n\n* **f_cci_6m_shift_653a5999** - Consumer Confidence Index with 6 months lag.\n\n## References\n* [How to calculate the SMAPE score](https://www.kaggle.com/c/tabular-playground-series-jan-2022/discussion/298201) by [@carlmcbrideellis](https://www.kaggle.com/carlmcbrideellis);\n* [Approximating SMAPE](https://www.kaggle.com/c/tabular-playground-series-jan-2022/discussion/298473) by [@ambrosm](https://www.kaggle.com/ambrosm).\n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}