{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# üîé How to find relevant external features & data for kaggle competitions in 10 minutes [2/3]\n### Part #2 - Zero feature engineering with low-code libraries: Upgini + PyCaret\n##### [Part #1 Link](https://www.kaggle.com/code/romaupgini/guide-how-to-find-relevant-external-features-1)\n##### [Part #3 Link](https://www.kaggle.com/code/romaupgini/external-data-features-for-multivariate-ts)\n______________________________\n*updated 26.05.22 [@roma-upgini](https://www.kaggle.com/romaupgini)*\n\n**‚ùì Before reading the notebook, what will you learn from it?**\n\n1. How external data & features might help on Kaggle: two scenarios\n2. How to find relevant external features in less than 10 minutes and save time on feature engineering \n3. How to calculate metrics and uplifts from new external features\n4. What external data sources might help you on Kaggle competitions\n\nüó£ Share this notebook: [Shareable Link](https://www.kaggle.com/code/romaupgini/zero-feature-engineering-with-upgini-pycaret)\n_______________________________\n### Table of contents\n* [Intro](#Intro)\n\n* [How external data & features might help on Kaggle?](#How-external-data-&-features-might-help-on-Kaggle?)\n\n* [Packages and functions](#Packages-and-functions)\n\n* [Low-code initial feature engineering - add relevant external features @start](#Low-code-initial-feature-engineering---add-relevant-external-features-@start-and-without-manual-effort)\n\n    - [1Ô∏è‚É£ Read train & test data](#1%EF%B8%8F%E2%83%A3-Read-train-&-test-data)\n    - [2Ô∏è‚É£ Find relevant external features](#2Ô∏è‚É£-Find-relevant-external-features)\n    - [3Ô∏è‚É£ Build model using PyCaret](#3Ô∏è‚É£-Build-model-using-PyCaret)\n    \n    \n* [External data sources & features](#%F0%9F%8C%8E-External-data-sources-&-features)   \n\n## Intro\n**Competition**: [TPS January 2022](https://www.kaggle.com/competitions/tabular-playground-series-jan-2022), SMAPE as a target metric.\n\n**Special thanks**: [@maxencefzr](https://www.kaggle.com/maxencefzr) for the [notebook](https://www.kaggle.com/code/maxencefzr/tps-jan22-catboost-using-pycaret/notebook) with PyCaret usage example.\n\nüìö In this notebook we'll use: \n\n* [Upgini]( https://github.com/upgini/upgini) - Low-code Feature search and enrichment library for supervised machine learning applications;\n* [PyCaret](https://pycaret.org/) - Low-code machine learning library in Python that automates machine learning workflows.  \n\n<a href=\"https://github.com/upgini/upgini\">\n    <img src=\"https://img.shields.io/badge/GitHub-100000?style=for-the-badge&logo=github&logoColor=white\"  align='center'>\n</a>","metadata":{"execution":{"iopub.status.busy":"2022-05-25T18:30:41.556425Z","iopub.execute_input":"2022-05-25T18:30:41.556974Z","iopub.status.idle":"2022-05-25T18:30:41.565943Z","shell.execute_reply.started":"2022-05-25T18:30:41.556923Z","shell.execute_reply":"2022-05-25T18:30:41.564986Z"}}},{"cell_type":"markdown","source":"## How external data & features might help on Kaggle?\nKaggle is always about learning and leader board progress (hopefully from learning, not cheating ;-))  \nAnd every Kaggler wants to progress as fast as possible, so time saving tips & tricks is a big deal as well.  \nThat's why low-code tools is adopted among kagglers.\n\nSo, there are **two major scenarios** of external features & data introduction in competitions on Kaggle:\n\n1. **Final improvement of a polished kernel**  \nIn this scenario you want **to improve already polished kernel** (optimized features, model architecture and hyperparams) with new external features.  \nBefore that, most of the juice already has been \"squeezed\" from competition data by significant efforts in feature engineering.  \nAnd you want to answer the simple question - *Is there any external data sources and features which might boost accuracy a bit more?*  \nHowever, there is a caveat to this approach: current model architecture & hyperparameters might be suboptimal for the new feature set, after introduction even single new var.  \nSo extra step back for model tuning might be needed.\n\n2. **Low-code initial feature engineering - add relevant external features @start**  \nHere you want to **save time on feature search and engineering**. If there are some ready-to-use external features and data, let's use it to speed up the overall progress.  \nIn this scenario always make sense to check that new external features have optimal representation for specific task and target model architecture. Example - category features for linear regression models should be one-hot-encoded.\nThis type of feature preparation should be done manually in any case.  \nSame as scenario #1, there is a caveat to this approach: a lot of features not always a good thing - they might lead to dimensionality increase and model overfitting.  \nSo you have to check model accuracy improvement metrics after enrichment with the new features and ALWAYS with appropriate cross-validation strategy.\n \nIn this notebook, we'll go with **Scenario #2**. Also you can check out guide for the [**Scenario #1**](https://www.kaggle.com/code/romaupgini/guide-how-to-find-relevant-external-features-1).","metadata":{}},{"cell_type":"markdown","source":"## Packages and functions","metadata":{}},{"cell_type":"code","source":"%pip install -Uq upgini\n%pip install pycaret\n\nimport logging\nlogging.getLogger(\"logs\").setLevel(\"DEBUG\")\n\nimport pandas as pd\nimport numpy as np\nfrom pycaret.regression import *\n\ndef smape(actual, predicted):\n    numerator = np.abs(predicted - actual)\n    denominator = (np.abs(actual) + np.abs(predicted)) / 2\n    \n    return np.mean(numerator / denominator)*100\n\ndef read_main_data(input_data_path):\n    train_df = pd.read_csv(f'{input_data_path}/tabular-playground-series-jan-2022/train.csv')\n    test_df = pd.read_csv(f'{input_data_path}/tabular-playground-series-jan-2022/test.csv')\n    train_df[\"segment\"], test_df[\"segment\"] = \"train\", \"test\"\n    df = pd.concat([train_df, test_df]).reset_index(drop=True)\n    df['date'] = pd.to_datetime(df.date)\n    \n    return df","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-03T22:26:49.321826Z","iopub.execute_input":"2022-06-03T22:26:49.322168Z","iopub.status.idle":"2022-06-03T22:27:17.844803Z","shell.execute_reply.started":"2022-06-03T22:26:49.322127Z","shell.execute_reply":"2022-06-03T22:27:17.843648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Low-code feature engineering - add relevant external features @start and without manual effort\nThe main idea of this notebook is to build baseline solution using only low-code Machine Learning tools. Namely, we search, generate and select relevant features with Upgini, then we prepare data and build final model with PyCaret. \n\nThe entire code of data preparation, feature engineering and modelling takes only a few lines, so you don't have to spend a lot of time doing all these operations manually.","metadata":{}},{"cell_type":"markdown","source":"## 1Ô∏è‚É£ Read train & test data\nRead train/test data from csv and combine them in one dataframe:","metadata":{}},{"cell_type":"code","source":"input_data_path = \"/kaggle/input\"\ndf = read_main_data(input_data_path)\n\nprint(\"Train + test dataframe size:\", df.shape)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-03T22:27:17.847564Z","iopub.execute_input":"2022-06-03T22:27:17.847931Z","iopub.status.idle":"2022-06-03T22:27:17.925833Z","shell.execute_reply.started":"2022-06-03T22:27:17.847886Z","shell.execute_reply":"2022-06-03T22:27:17.924905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's define list of baseline features (we'll simply use \"country\", \"store\" and \"product\" as is). As you can see, there is no feature engineering at all - all three columns are presented in the initial train and test datasets:","metadata":{}},{"cell_type":"code","source":"baseline_features = [\n    \"country\", \"store\", \"product\"\n]","metadata":{"execution":{"iopub.status.busy":"2022-06-03T22:27:17.927068Z","iopub.execute_input":"2022-06-03T22:27:17.927358Z","iopub.status.idle":"2022-06-03T22:27:17.931636Z","shell.execute_reply.started":"2022-06-03T22:27:17.927326Z","shell.execute_reply":"2022-06-03T22:27:17.930634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's also predict logarithm of the \"num_sold\" field, instead of predicting \"num_sold\" field directly:","metadata":{}},{"cell_type":"code","source":"df[\"num_sold_log\"] = np.log1p(df[\"num_sold\"])","metadata":{"execution":{"iopub.status.busy":"2022-06-03T22:27:17.933532Z","iopub.execute_input":"2022-06-03T22:27:17.933779Z","iopub.status.idle":"2022-06-03T22:27:17.950555Z","shell.execute_reply.started":"2022-06-03T22:27:17.933751Z","shell.execute_reply":"2022-06-03T22:27:17.949591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2Ô∏è‚É£ Find relevant external features\nTo find new features we'll use [Upgini Feature search and enrichment library for supervised machine learning applications](https://github.com/upgini/upgini#readme)  \nTo initiate search with Upgini library, you need to define so called [*search keys*](https://github.com/upgini/upgini#-search-key-types-we-support-more-is-coming) - a set of columns to join external data sources. In this competition we can use the following keys:\n\n1. Column **date** should be used as **SearchKey.DATE**.;  \n2. Column **country** (after conversion to ISO-3166 country code) should be used as **SearchKey.COUNTRY**.\n    \nWith this set of search keys, our dataset will be matched with [different time-specific features (such as weather data, calendar data, financial data, etc)](https://github.com/upgini/upgini#-connected-data-sources-and-coverage), taking into account the country where sales happened. Than relevant selection and ranking will be done.  \nAs a result, we'll add new, only relevant features with additional information about specific dates and countries.","metadata":{}},{"cell_type":"code","source":"from upgini import SearchKey \n\n# here we simply map each country to its ISO-3166 code\ncountry_iso_map = {\n    \"Finland\": \"FI\",\n    \"Norway\": \"NO\",\n    \"Sweden\": \"SE\"\n}\ndf[\"country_iso\"] = df.country.map(country_iso_map)\ndf.country_iso.value_counts()\n\n## define search keys\nsearch_keys = {\n    \"date\": SearchKey.DATE, \n    \"country_iso\": SearchKey.COUNTRY\n}","metadata":{"execution":{"iopub.status.busy":"2022-06-03T22:27:17.951955Z","iopub.execute_input":"2022-06-03T22:27:17.952811Z","iopub.status.idle":"2022-06-03T22:27:17.973886Z","shell.execute_reply.started":"2022-06-03T22:27:17.952752Z","shell.execute_reply":"2022-06-03T22:27:17.972711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To start the search, we need to initiate *scikit-learn* compartible `FeaturesEnricher` transformer with appropriate **search** parameters and cross-validation type (here we use [TimeSeries](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html) CV, because our target variable strongly depends on time, ie we have TS prediction task).  \nAfter that, we can call the **fit** method of `features_enricher` to start the search.","metadata":{}},{"cell_type":"code","source":"from upgini import FeaturesEnricher\nfrom upgini.metadata import CVType, RuntimeParameters\n\n## define X_train & y_train\nX_train = df.loc[df.segment == \"train\", [\"date\", \"country_iso\"] + baseline_features]\ny_train = df.loc[df.segment == \"train\", \"num_sold_log\"]\n\n## define FeaturesEnricher\nfeatures_enricher = FeaturesEnricher(\n    search_keys=search_keys, \n    cv=CVType.time_series\n)","metadata":{"execution":{"iopub.status.busy":"2022-06-03T22:27:17.975388Z","iopub.execute_input":"2022-06-03T22:27:17.975676Z","iopub.status.idle":"2022-06-03T22:27:18.007033Z","shell.execute_reply.started":"2022-06-03T22:27:17.975645Z","shell.execute_reply":"2022-06-03T22:27:18.005999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`FeaturesEnricher.fit()` has a flag `calculate_metrics` for the quick estimation of quality improvement on cross-validation and eval sets. This step is quite similar to [sklearn.model_selection.cross_val_score](https://scikit-learn.org/stable/modules/cross_validation.html#computing-cross-validated-metrics), so you can pass exact metric with `scoring` parameter:\n\n1. Built-in scoring [functions](https://github.com/upgini/upgini/blob/main/README.md#-accuracy-and-uplift-metrics-calculations) (in this case - scorer based on Mean Squared Error);\n2. Custom scorer.    \n\nNotice that you should pass **X_train** as the first argument and **y_train** as the second argument for `FeaturesEnricher.fit()`, just like in scikit-learn.  \n\n*Step will take around 3.5 minutes*","metadata":{}},{"cell_type":"code","source":"%%time\n\nfeatures_enricher.fit(X_train, y_train, calculate_metrics = True)","metadata":{"execution":{"iopub.status.busy":"2022-06-03T22:17:34.584925Z","iopub.execute_input":"2022-06-03T22:17:34.585188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We've got **70+ relevant features**, which might improve accuracy of the model, ranked by [SHAP values](https://en.wikipedia.org/wiki/Shapley_value).  \n\nUplift after enrichment with all of the new external features is *positive* - so, the features from search actually contain some useful information about our target variable. Let's enrich initial feature space with found features.\n\n*Step will take around 2 minutes*","metadata":{}},{"cell_type":"code","source":"## call transform and enrich dataset\ndf_enriched = features_enricher.transform(df, keep_input=True)\n\n## define list of found features\nenricher_features = [\n    f for f in features_enricher.get_features_info().feature_name.values\n    if f not in [\"date\", \"country_iso\"] + baseline_features\n]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3Ô∏è‚É£ Build model using PyCaret","metadata":{}},{"cell_type":"markdown","source":"Now it's time to build a model using PyCaret. At first, we need to configurate **setup** for model training. \n\nNotice that we use TimeSeries cross-validation with 5 folds, just like we did during feature search:","metadata":{}},{"cell_type":"code","source":"_ = setup(\n    data = df_enriched[df_enriched.segment == \"train\"],\n    target = \"num_sold_log\",\n    numeric_features = enricher_features,\n    categorical_features = baseline_features,\n    ignore_features=[\"date\", \"country_iso\", \"segment\", \"num_sold\"],\n    fold_strategy = 'timeseries',\n    fold = 5,\n    data_split_shuffle = False, \n    silent = True\n)","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's fit model based on CatBoost regressor with **create_model** function. Notice that we don't need to apply additional data preparation (for example, encoding of categorical features) - PyCaret does it automatically:","metadata":{}},{"cell_type":"code","source":"model = create_model('catboost', random_state=0)\nmodel","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"That's it, our model is ready! Let's plot feature importances for the selected features:","metadata":{}},{"cell_type":"code","source":"plot_model(model, 'feature')","metadata":{"execution":{"iopub.status.busy":"2022-06-03T22:27:54.193128Z","iopub.execute_input":"2022-06-03T22:27:54.193671Z","iopub.status.idle":"2022-06-03T22:27:54.687015Z","shell.execute_reply.started":"2022-06-03T22:27:54.193633Z","shell.execute_reply":"2022-06-03T22:27:54.686161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can also interpret results of the model using SHAP summary plot:","metadata":{}},{"cell_type":"code","source":"interpret_model(model)","metadata":{"execution":{"iopub.status.busy":"2022-06-03T22:27:49.098914Z","iopub.execute_input":"2022-06-03T22:27:49.099817Z","iopub.status.idle":"2022-06-03T22:27:54.191651Z","shell.execute_reply.started":"2022-06-03T22:27:49.099776Z","shell.execute_reply":"2022-06-03T22:27:54.190905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally, let's make predictions for the test part of the dataset and submit them:","metadata":{}},{"cell_type":"code","source":"submission = pd.read_csv('../input/tabular-playground-series-jan-2022/sample_submission.csv')\ny_pred = predict_model(model, data=df_enriched[df_enriched.segment == \"test\"])['Label']\nsubmission[\"num_sold\"] = np.round(np.expm1(y_pred.values))\nsubmission.to_csv('submission.csv', index=False)","metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As a result, we build quite a good solution for the competition **without manual data preparation, feature engineering and modelling** thanks to the low-code instruments! ","metadata":{}},{"cell_type":"markdown","source":"## üåé Relevant external features & data sources","metadata":{}},{"cell_type":"markdown","source":"Here is the description for the **TOP-2** most important features from Upgini enrichment:\n\n* **f_NID_NGDP_f8ff00f6** - Total investment in percents of country's GDP. \n\n* **f_pcpiham_wt_531b4347** -  Consumer Price index for Health group of products & services. In general, Consumer Price indexes are index numbers that measure changes in the prices of goods and services purchased or otherwise acquired by households, which households use directly, or indirectly, to satisfy their own needs and wants.  \nSo it has a lot of information about inflation in specific country and for specific type of services and goods.  \nIt's been updated by the [Organisation for Economic Cooperation and Development (OECD)](https://data.oecd.org/price/inflation-cpi.htm) on a monthly basis.","metadata":{}}]}