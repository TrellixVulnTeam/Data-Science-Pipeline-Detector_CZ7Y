{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#### üìöPN : 2\nüìö[PN : 1](https://www.kaggle.com/sarabhian/gbr-extremely-beginner-level-guide-1)","metadata":{}},{"cell_type":"markdown","source":"# üçÄWELCOME To My Notebook \nThis is my 2022's first Notebook... \n\n**HAPPY NEW YEAR TO ALL** üéâüéâüéä\n\n\nI will try to explore and explain the notebook as friendly as I can, I can guarantee  you that it will be very interesting  to read this notebook üòÇ","metadata":{}},{"cell_type":"markdown","source":"# üîÖ Basic EDA","metadata":{}},{"cell_type":"markdown","source":"**Understanding The Problem** all of us have read the description of problem statement and it says ,\n>There are two (fictitious) independent store chains selling Kaggle merchandise that want to become the official outlet for all things Kaggle. We've decided to see if the Kaggle community could help us figure out which of the store chains would have the best sales going forward. So, we've collected some data and are asking you to build `forecasting models` to help us decide.","metadata":{}},{"cell_type":"markdown","source":"As soon as you hear the name `time series` or`forecasting model` then change your attitude towards that problem , because this problem need to be handled in very unique manner, we can't apply basic supervised learning models directly to predict the future,\n\nlet's understand what time series means :\n\n**Time Series Prediction**  \n* The data which follows the periodicity with respect to time  is `Time Series Data` ,\n* Generally Data looks like `Sine waves` or like `stock market curves` but repeating the curves after some interval\n* We want to understant how the trends of predictor `y`  ( in our case Y is no. of sales ) changes with respect to time \n* and we want to predict the future trends of predictor `y` ( in simple we want to predict the future sales , future stock prices , future product evaluations and many more ...)\n\n\nI tried to discuss few simple , easily understandable + effective techniques to solve this problem\nlet's dive into the data science of the future prediction üèä‚Äç‚ôÄÔ∏èüèä‚Äç‚ôÄÔ∏è\n\n","metadata":{}},{"cell_type":"markdown","source":"## üí† Importing libraries","metadata":{}},{"cell_type":"code","source":"## basic libraries\nimport os\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport seaborn as sns\n%matplotlib inline\n\n## libraries for feature engineering\nfrom sklearn.preprocessing import LabelEncoder\n","metadata":{"execution":{"iopub.status.busy":"2022-01-26T10:46:02.836141Z","iopub.execute_input":"2022-01-26T10:46:02.836612Z","iopub.status.idle":"2022-01-26T10:46:04.182721Z","shell.execute_reply.started":"2022-01-26T10:46:02.836454Z","shell.execute_reply":"2022-01-26T10:46:04.181638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## üí† importing DATA","metadata":{}},{"cell_type":"code","source":"path = \"../input/tabular-playground-series-jan-2022\"\ntdf = pd.read_csv(path + \"/train.csv\")\nTdf = pd.read_csv(path + \"/test.csv\")\nsdf = pd.read_csv(path +\"/sample_submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-01-26T10:46:04.184696Z","iopub.execute_input":"2022-01-26T10:46:04.184962Z","iopub.status.idle":"2022-01-26T10:46:04.283169Z","shell.execute_reply.started":"2022-01-26T10:46:04.184933Z","shell.execute_reply":"2022-01-26T10:46:04.282315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"train:{tdf.shape} \\ntest:{Tdf.shape}\\ntest-train ratio: {(Tdf.shape[0])/(tdf.shape[0])}\")","metadata":{"execution":{"iopub.status.busy":"2022-01-26T10:46:04.284592Z","iopub.execute_input":"2022-01-26T10:46:04.285026Z","iopub.status.idle":"2022-01-26T10:46:04.289981Z","shell.execute_reply.started":"2022-01-26T10:46:04.284993Z","shell.execute_reply":"2022-01-26T10:46:04.289339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ok we have very less data , which is always a bad thing for us to create models ü§£","metadata":{}},{"cell_type":"markdown","source":"## üí†Determining the predictor Y and it's behaviour\nlet's find the label `Y` from the data and  see what exactly we have to predict","metadata":{}},{"cell_type":"code","source":"tdf.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-26T10:46:04.291329Z","iopub.execute_input":"2022-01-26T10:46:04.291851Z","iopub.status.idle":"2022-01-26T10:46:04.321633Z","shell.execute_reply.started":"2022-01-26T10:46:04.291811Z","shell.execute_reply":"2022-01-26T10:46:04.320901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Tdf.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-26T10:46:04.323593Z","iopub.execute_input":"2022-01-26T10:46:04.324212Z","iopub.status.idle":"2022-01-26T10:46:04.335686Z","shell.execute_reply.started":"2022-01-26T10:46:04.324175Z","shell.execute_reply":"2022-01-26T10:46:04.33481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So, here the feature named `num_sold` is the predictor `y` for this problem, let's see what this `num_sold` has inside it","metadata":{}},{"cell_type":"code","source":"tdf.num_sold.describe()","metadata":{"execution":{"iopub.status.busy":"2022-01-26T10:46:04.337282Z","iopub.execute_input":"2022-01-26T10:46:04.337808Z","iopub.status.idle":"2022-01-26T10:46:04.360098Z","shell.execute_reply.started":"2022-01-26T10:46:04.337761Z","shell.execute_reply":"2022-01-26T10:46:04.358963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"total predictions :\", tdf.num_sold.count())\nprint(\"\\ntotal unique predictions:\",tdf.num_sold.nunique())\nprint(\"\\nnull_predictions: \" ,tdf.num_sold.isnull().sum())","metadata":{"execution":{"iopub.status.busy":"2022-01-26T10:46:04.361416Z","iopub.execute_input":"2022-01-26T10:46:04.362209Z","iopub.status.idle":"2022-01-26T10:46:04.374279Z","shell.execute_reply.started":"2022-01-26T10:46:04.362161Z","shell.execute_reply":"2022-01-26T10:46:04.372912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.lineplot(y=tdf['num_sold'],x=tdf['date'] ) # liquidity \nplt.show()\nsns.histplot(data=tdf ,x= \"num_sold\" ) # distribution \nplt.show()\nsns.boxplot(data=tdf, x= \"num_sold\") # variance","metadata":{"execution":{"iopub.status.busy":"2022-01-26T10:46:04.375254Z","iopub.execute_input":"2022-01-26T10:46:04.37548Z","iopub.status.idle":"2022-01-26T10:47:00.895108Z","shell.execute_reply.started":"2022-01-26T10:46:04.375454Z","shell.execute_reply":"2022-01-26T10:47:00.894158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`fig 1` this fig is computationally heavy to plot as we have not seperated `date` with respect to specific `year` , `month` and `day` but for first glance we can see our predictor `y` is following periodicity which clearly indicates this could be the problem of time series prediction even if it is not specified in compitition\n\n`fig2` üòÆ chi-squared curve or skewed normal distribution... looks great curve, <br>\n`Note:`  predictor `y` is distributed under chi_square distribution, which clearly indicates under normal conditions  we should be using categorical ML Models like multiclass  classification , clustering or softmax or svm  .....`but` this is not a supervised learning problem it's problem of future prediction which comes under `time series predictions`  so our entire methodology to solve the problem changes ...<br>\n\n`fig3` it shows that predictor `y` is having some outliers which are indications of high sales on some festival or some special ocassion","metadata":{}},{"cell_type":"markdown","source":"# üîÖ  Feature Engineering For Time Series Data ","metadata":{}},{"cell_type":"code","source":"tdf.head(3)","metadata":{"execution":{"iopub.status.busy":"2022-01-26T10:47:00.896491Z","iopub.execute_input":"2022-01-26T10:47:00.896758Z","iopub.status.idle":"2022-01-26T10:47:00.909805Z","shell.execute_reply.started":"2022-01-26T10:47:00.896718Z","shell.execute_reply":"2022-01-26T10:47:00.908549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tdf.info()","metadata":{"execution":{"iopub.status.busy":"2022-01-26T10:47:00.911947Z","iopub.execute_input":"2022-01-26T10:47:00.912233Z","iopub.status.idle":"2022-01-26T10:47:00.946675Z","shell.execute_reply.started":"2022-01-26T10:47:00.912203Z","shell.execute_reply":"2022-01-26T10:47:00.945836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"soka üòÆ ,as we can see there are `4 features` which are `objects`and `2` are `int64` , and if we see tdf.head output then we can see 3 of the 4 objects are `categorical:`  `country` , `store` ,`product` and 1 of them is `time` , time could be categorical , ordinal, continuous .. anything [refer here for understanding time data type](https://statisticalanalysisconsulting.com/is-time-nominal-ordinal-interval-or-ratio-is-it-categorical-or-continuous/) most of the time we drop the date in the datasets but in this problem  `date` is the main feature ","metadata":{}},{"cell_type":"markdown","source":"## ‚≠ï creating new time features\n\nhere we try to break the timeline into different parts  like `year , month , week , days , weekdays , quarter , Hours , Minutes , seconds` etc to understand how the predictor `y` ( no. of sales) is changing with respect to these different timelines\n","metadata":{}},{"cell_type":"code","source":"def time_splitter(df,date): #here df = data and date= given time feature \n    df[date]         = pd.to_datetime(df[date])\n    df['year']       = df[date].dt.year\n    df['month']      = df[date].dt.month\n    df['day']        = df[date].dt.day\n    df['week']       = df[date].dt.isocalendar().week\n    df['weekday']    = df[date].dt.weekday\n    df['weekend']    = (df[date].dt.weekday>4).astype(int)\n    df['day_of_yr']  = df[date].dt.dayofyear\n    df['quarter']    = df[date].dt.quarter\n    return df\n\n# time splitting for training data\ntdf=time_splitter(tdf,'date')\ntdf.loc[300:310]","metadata":{"execution":{"iopub.status.busy":"2022-01-26T10:47:00.948142Z","iopub.execute_input":"2022-01-26T10:47:00.948404Z","iopub.status.idle":"2022-01-26T10:47:01.027132Z","shell.execute_reply.started":"2022-01-26T10:47:00.948358Z","shell.execute_reply":"2022-01-26T10:47:01.02621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- - - - - -","metadata":{}},{"cell_type":"code","source":"# time splitting for test data\nTdf=time_splitter(Tdf,'date')\nTdf.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-26T10:47:01.028162Z","iopub.execute_input":"2022-01-26T10:47:01.028395Z","iopub.status.idle":"2022-01-26T10:47:01.06374Z","shell.execute_reply.started":"2022-01-26T10:47:01.028353Z","shell.execute_reply":"2022-01-26T10:47:01.062824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tdf=tdf.drop(['row_id','date'],axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-01-26T10:47:01.064978Z","iopub.execute_input":"2022-01-26T10:47:01.065204Z","iopub.status.idle":"2022-01-26T10:47:01.076669Z","shell.execute_reply.started":"2022-01-26T10:47:01.065177Z","shell.execute_reply":"2022-01-26T10:47:01.075599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ncol=list(tdf.columns)  \ncol","metadata":{"execution":{"iopub.status.busy":"2022-01-26T10:47:01.081784Z","iopub.execute_input":"2022-01-26T10:47:01.082062Z","iopub.status.idle":"2022-01-26T10:47:01.08917Z","shell.execute_reply.started":"2022-01-26T10:47:01.082033Z","shell.execute_reply":"2022-01-26T10:47:01.088274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### üí¢null value analysis ","metadata":{}},{"cell_type":"code","source":"print(f\"values with None:\\n{tdf[tdf.values==None].count()} \\n\\nvalue with null or empty:\\\n     \\n{tdf.isnull().nunique()}\")","metadata":{"execution":{"iopub.status.busy":"2022-01-26T10:47:01.091Z","iopub.execute_input":"2022-01-26T10:47:01.091905Z","iopub.status.idle":"2022-01-26T10:47:01.14711Z","shell.execute_reply.started":"2022-01-26T10:47:01.091866Z","shell.execute_reply":"2022-01-26T10:47:01.146045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"for beginners above line of code may look difficult to understand üòÖ \nbut it's simple \n1. first I used `f\" {} {}\"`  to write code in just one print function \n2. then to break the lines I used `\\` at end of line after **value with null or empty:`\\`** üëà\n3. inside {} I used two functions \n   * `tdf[tdf.values==None].count()` comapres values of data with `None` data type since nunique can't detect this data type as null value \n   * `tdf.nunique(axis=0 , dropna =False)`  compares null values like `[] ,\" \" , NaN ` etc \n\n> outputs for values with none ==0 => there is no None value<br>\n> outputs for null values ==1 => isnull() writes true or false as nunique of isnull is 1 which means either all isnull values are True (entire data is Null)  or all isnull values are false ( there is no null value in the data)","metadata":{}},{"cell_type":"markdown","source":"so conclusion is there is no null value in this dataset everthing looks great and easyüòÇ","metadata":{}},{"cell_type":"code","source":"sns.set_style('darkgrid')\nsns.pairplot(data=tdf ,height=5 , corner=True ,diag_kind='kde')","metadata":{"execution":{"iopub.status.busy":"2022-01-26T10:47:01.14964Z","iopub.execute_input":"2022-01-26T10:47:01.150177Z","iopub.status.idle":"2022-01-26T10:47:18.510947Z","shell.execute_reply.started":"2022-01-26T10:47:01.15013Z","shell.execute_reply":"2022-01-26T10:47:18.510015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"this above plot looks scary isn't it ? üëπüëπ most of the people consider pairplot as nonsense plot because people won't understand itü§£ü§£, but let me tell you the insights that I am getting from this plot\ndouble click of graph and see each graph carefully to check the below given insights<br>\n( considering `num_sold` =`ns` for decreasing tyoing load üòÅ)<br>\n1. first column of plot:\n\n * `quarter` vs `ns` : in 2nd and 4th quarter we had to more sales\n * `day_of_year` vs `ns` : for days 50:150 and for days 350:365 we had high sales\n * `weekend` vs `ns` : we had little high sales on weekends (as expected)\n * `weekday` vs `ns` : --||--\n * `day` vs `ns` : at end of month we have high sales\n * `month` vs `ns` : 4th ,5th and 12th month had more sales\n * `year` vs `ns` : 2017 and 2018 had more sales ( I will say sales are increasing each year)\n * `index` vs `ns` : indicates num_sold is chi-square distributed which is expected curve for sales \n <br><br>\n similarly I can tell each plots meaning but I am tired of writing so you guys decide on your own üòÇ","metadata":{}},{"cell_type":"code","source":"cols = ['country','store','product']\n[print(f\"\\nTotal Sales by {i}  :\\n{tdf[i].value_counts()}\") for i in cols]","metadata":{"execution":{"iopub.status.busy":"2022-01-26T10:47:18.512137Z","iopub.execute_input":"2022-01-26T10:47:18.51244Z","iopub.status.idle":"2022-01-26T10:47:18.535248Z","shell.execute_reply.started":"2022-01-26T10:47:18.512347Z","shell.execute_reply":"2022-01-26T10:47:18.534271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"total data is symmetric ... this is doing of the team who arranged this competition , we don't get symmetric data in generalüòâ","metadata":{}},{"cell_type":"markdown","source":"##  üí† Handling Categorical features","metadata":{}},{"cell_type":"markdown","source":"since our data has huge classes we will use label encoding ","metadata":{}},{"cell_type":"code","source":"from collections import defaultdict\nd= defaultdict(LabelEncoder)         # trick to do encoding and decoding vary easily üòÅ\n \nx = tdf[['country' ,'store','product','year']]\n\nx=x.apply(lambda x: d[x.name].fit_transform(x))\nx.head()\ntdf[['country' ,'store','product','year']]=x\ntdf","metadata":{"execution":{"iopub.status.busy":"2022-01-26T10:47:18.5369Z","iopub.execute_input":"2022-01-26T10:47:18.537192Z","iopub.status.idle":"2022-01-26T10:47:18.595803Z","shell.execute_reply.started":"2022-01-26T10:47:18.537156Z","shell.execute_reply":"2022-01-26T10:47:18.59489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for inverting the x to it's original data if needed\nx=x.apply(lambda x: d[x.name].inverse_transform(x))\nx.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-26T10:47:18.597207Z","iopub.execute_input":"2022-01-26T10:47:18.597493Z","iopub.status.idle":"2022-01-26T10:47:18.613647Z","shell.execute_reply.started":"2022-01-26T10:47:18.59744Z","shell.execute_reply":"2022-01-26T10:47:18.611825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"references [lambda](https://towardsdatascience.com/lambda-functions-with-practical-examples-in-python-45934f3653a8)\n[LabelEncoder](https://scikit-learn.org/stable/modules/preprocessing_targets.html) [trick to use encoding](https://stackoverflow.com/questions/24458645/label-encoding-across-multiple-columns-in-scikit-learn)","metadata":{}},{"cell_type":"markdown","source":"ok now we got data all in numerical form let's do some more visualizations and analysis","metadata":{}},{"cell_type":"markdown","source":"### üí¢outlier analysis","metadata":{}},{"cell_type":"code","source":"  tdf.describe()","metadata":{"execution":{"iopub.status.busy":"2022-01-26T10:47:18.614728Z","iopub.execute_input":"2022-01-26T10:47:18.61549Z","iopub.status.idle":"2022-01-26T10:47:18.694586Z","shell.execute_reply.started":"2022-01-26T10:47:18.615443Z","shell.execute_reply":"2022-01-26T10:47:18.693649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"the only outlier here is `num_sold`  which we are not goint to fix as it will help us to underfit the data","metadata":{}},{"cell_type":"markdown","source":"let's do some interesting outlier analysis for this new year üòÇüí•","metadata":{}},{"cell_type":"markdown","source":"#### 1.Eliptic Envelope Algorithm for outlier detection","metadata":{}},{"cell_type":"markdown","source":"suppose we want to see how much % of data is outside the regionr as per our recommended outlier region we can use boxplot but there is something more good for it as follows\n\nlet's see we keep data with 98% inlier and 2 % outlier ==>  contamination = 0.02","metadata":{}},{"cell_type":"code","source":"from sklearn.covariance import EllipticEnvelope\nX= tdf[\"num_sold\"].values\nX=X.reshape((13149,2)) # 13149 = len(X)/2 since X need to be 2D array\n# print(X)\nenvlp = EllipticEnvelope(contamination = 0.02 ,random_state=10)  # contamination == % of data outside ellipse\npred = envlp.fit_predict(X)\n# Extract outliers\noutlier_index = np.where(pred<=-1)\noutlier_values = X[outlier_index]\n\n# Plot the data\nsns.set(rc={'figure.figsize':(20,12)})\nsns.set_style('darkgrid')\nsns.scatterplot(x=X[:,0], y=X[:,1] )\nsns.scatterplot(x=outlier_values[:,0], \n                y=outlier_values[:,1], color='r')\nplt.title(\"Elliptic Envelope Outlier Detection\", fontsize=15, pad=15)\n\n#plt.savefig(\"Elliptic Envelope Detection.png\", dpi=80)\n\n# outlier count \nprint(\"total outliers\" ,outlier_values.size)","metadata":{"execution":{"iopub.status.busy":"2022-01-26T10:47:18.695798Z","iopub.execute_input":"2022-01-26T10:47:18.696037Z","iopub.status.idle":"2022-01-26T10:47:22.003332Z","shell.execute_reply.started":"2022-01-26T10:47:18.696009Z","shell.execute_reply":"2022-01-26T10:47:22.002334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tdf['num_sold'].describe()","metadata":{"execution":{"iopub.status.busy":"2022-01-26T10:47:22.00498Z","iopub.execute_input":"2022-01-26T10:47:22.005531Z","iopub.status.idle":"2022-01-26T10:47:22.018894Z","shell.execute_reply.started":"2022-01-26T10:47:22.005487Z","shell.execute_reply":"2022-01-26T10:47:22.017269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# üîÖ MODEL ","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nimport lightgbm as lgb\nimport xgboost as xgb\nimport catboost as cb\nfrom sklearn.model_selection import train_test_split , cross_val_score\nfrom sklearn.preprocessing import LabelEncoder","metadata":{"execution":{"iopub.status.busy":"2022-01-26T10:47:22.020702Z","iopub.execute_input":"2022-01-26T10:47:22.021046Z","iopub.status.idle":"2022-01-26T10:47:23.473218Z","shell.execute_reply.started":"2022-01-26T10:47:22.021002Z","shell.execute_reply":"2022-01-26T10:47:23.47179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nX=tdf.drop(['num_sold'],axis=1)\ny=tdf[\"num_sold\"]\nX['week']=X['week'].astype('int')","metadata":{"execution":{"iopub.status.busy":"2022-01-26T10:47:23.474771Z","iopub.execute_input":"2022-01-26T10:47:23.475398Z","iopub.status.idle":"2022-01-26T10:47:23.483509Z","shell.execute_reply.started":"2022-01-26T10:47:23.47532Z","shell.execute_reply":"2022-01-26T10:47:23.482586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train , X_val ,y_train , y_val = train_test_split(X,y,test_size=0.24983 , random_state =47)","metadata":{"execution":{"iopub.status.busy":"2022-01-26T10:47:23.484747Z","iopub.execute_input":"2022-01-26T10:47:23.485323Z","iopub.status.idle":"2022-01-26T10:47:23.506578Z","shell.execute_reply.started":"2022-01-26T10:47:23.485274Z","shell.execute_reply":"2022-01-26T10:47:23.505629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def SMAPE(y_true,y_pred):\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    smape=0\n    if(len(y_true)==len(y_pred)):\n        smape = (100/len(y_true)) * np.sum(2* np.abs(y_pred-y_true)/(np.abs(y_true)+np.abs(y_pred)))\n    else:\n        return\n    return(smape)","metadata":{"execution":{"iopub.status.busy":"2022-01-26T10:47:23.508192Z","iopub.execute_input":"2022-01-26T10:47:23.508459Z","iopub.status.idle":"2022-01-26T10:47:23.514589Z","shell.execute_reply.started":"2022-01-26T10:47:23.508418Z","shell.execute_reply":"2022-01-26T10:47:23.513583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dic_model = {\"RandomForest\":RandomForestRegressor(n_estimators=100),\n             'GradientBoosting':GradientBoostingRegressor(n_estimators=100),\n            'XGradientBoosting':xgb.XGBRegressor(n_estimators=100),\n            'CatBoostRegressor':cb.CatBoostRegressor(n_estimators=100),\n            'LightGBM': lgb.LGBMRegressor(n_estimators=100)}\n\nfor i in dic_model:\n    #Training\n    print(\"Training with \",i+\" Algorithm....\")\n    print()\n    model = dic_model[i].fit(X_train,y_train)\n    \n    #Predicting\n    print(\"Predicting with \",i+\" Model....\")\n    print()\n    prediction = model.predict(X_val)\n    \n    # Using SMAPE for predicting models\n    print(\"SMAPE of \",i+\" Model is \",SMAPE(y_val,prediction))\n    print(\"------------------------------------------------------------------\")","metadata":{"execution":{"iopub.status.busy":"2022-01-26T10:47:23.537353Z","iopub.execute_input":"2022-01-26T10:47:23.537587Z","iopub.status.idle":"2022-01-26T10:47:34.633443Z","shell.execute_reply.started":"2022-01-26T10:47:23.53756Z","shell.execute_reply":"2022-01-26T10:47:34.63244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model.compile(tf.keras.optimizers.Adam(learning_rate=0.0001,beta_1=0.99),\n#              loss='hinge',\n#              metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2022-01-26T10:47:34.637788Z","iopub.execute_input":"2022-01-26T10:47:34.638165Z","iopub.status.idle":"2022-01-26T10:47:34.645436Z","shell.execute_reply.started":"2022-01-26T10:47:34.638125Z","shell.execute_reply":"2022-01-26T10:47:34.644736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" # 'min_data_in_leaf': 186 'l2_leaf_reg': 0.018327921341491783,'rsm': 0.7531284161762606, \nparams = { 'max_bin': 376, \n          'learning_rate': 1.299687, 'n_estimators': 4560, 'max_depth': 500, \n          'random_state': 47}\nmodel_cat = lgb.LGBMRegressor(**params)\nmodel_cat.fit(X,y,verbose=20)","metadata":{"execution":{"iopub.status.busy":"2022-01-26T10:47:34.647018Z","iopub.execute_input":"2022-01-26T10:47:34.648256Z","iopub.status.idle":"2022-01-26T10:47:42.785544Z","shell.execute_reply.started":"2022-01-26T10:47:34.64821Z","shell.execute_reply":"2022-01-26T10:47:42.784558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" prediction = model_cat.predict(X_val)\n    \n    # Using SMAPE for predicting models\nprint(\"SMAPE of \",i+\" Model is \",SMAPE(y_val,prediction))\nprint(\"------------------------------------------------------------------\")","metadata":{"execution":{"iopub.status.busy":"2022-01-26T10:47:42.789088Z","iopub.execute_input":"2022-01-26T10:47:42.789598Z","iopub.status.idle":"2022-01-26T10:47:43.627244Z","shell.execute_reply.started":"2022-01-26T10:47:42.789558Z","shell.execute_reply":"2022-01-26T10:47:43.626565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from collections import defaultdict\nc= defaultdict(LabelEncoder)         # trick to do encoding and decoding vary easily üòÅ\n \nx = Tdf[['country' ,'store','product' ,'year']]\n\nx=x.apply(lambda x: c[x.name].fit_transform(x))\nx.head()\nTdf[['country' ,'store','product' ,'year']]=x\nTdf","metadata":{"execution":{"iopub.status.busy":"2022-01-26T10:47:43.631267Z","iopub.execute_input":"2022-01-26T10:47:43.631889Z","iopub.status.idle":"2022-01-26T10:47:43.668709Z","shell.execute_reply.started":"2022-01-26T10:47:43.631843Z","shell.execute_reply":"2022-01-26T10:47:43.668051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test =Tdf.drop(['row_id','date'],axis=1)\nX_test['week']=X_test['week'].astype('int')","metadata":{"execution":{"iopub.status.busy":"2022-01-26T10:47:43.670108Z","iopub.execute_input":"2022-01-26T10:47:43.670622Z","iopub.status.idle":"2022-01-26T10:47:43.677284Z","shell.execute_reply.started":"2022-01-26T10:47:43.670583Z","shell.execute_reply":"2022-01-26T10:47:43.676358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred=model_cat.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-01-26T10:47:43.6785Z","iopub.execute_input":"2022-01-26T10:47:43.678743Z","iopub.status.idle":"2022-01-26T10:47:44.522858Z","shell.execute_reply.started":"2022-01-26T10:47:43.678713Z","shell.execute_reply":"2022-01-26T10:47:44.522103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred.shape","metadata":{"execution":{"iopub.status.busy":"2022-01-26T10:47:44.524356Z","iopub.execute_input":"2022-01-26T10:47:44.524889Z","iopub.status.idle":"2022-01-26T10:47:44.531494Z","shell.execute_reply.started":"2022-01-26T10:47:44.524851Z","shell.execute_reply":"2022-01-26T10:47:44.530641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub=pd.DataFrame()\nsub[\"row_id\"]=Tdf[\"row_id\"]\nsub[\"num_sold\"]=np.ceil(pred)","metadata":{"execution":{"iopub.status.busy":"2022-01-26T10:47:44.532674Z","iopub.execute_input":"2022-01-26T10:47:44.537156Z","iopub.status.idle":"2022-01-26T10:47:44.547235Z","shell.execute_reply.started":"2022-01-26T10:47:44.537103Z","shell.execute_reply":"2022-01-26T10:47:44.546442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub","metadata":{"execution":{"iopub.status.busy":"2022-01-26T10:47:44.548489Z","iopub.execute_input":"2022-01-26T10:47:44.549006Z","iopub.status.idle":"2022-01-26T10:47:44.569632Z","shell.execute_reply.started":"2022-01-26T10:47:44.548966Z","shell.execute_reply":"2022-01-26T10:47:44.568721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub.to_csv('submission.csv' , index=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-26T10:47:44.57077Z","iopub.execute_input":"2022-01-26T10:47:44.571067Z","iopub.status.idle":"2022-01-26T10:47:44.596168Z","shell.execute_reply.started":"2022-01-26T10:47:44.571039Z","shell.execute_reply":"2022-01-26T10:47:44.595467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}