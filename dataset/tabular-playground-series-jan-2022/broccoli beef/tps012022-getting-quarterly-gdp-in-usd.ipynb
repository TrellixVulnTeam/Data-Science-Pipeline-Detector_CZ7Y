{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-01-25T01:14:13.490822Z","iopub.execute_input":"2022-01-25T01:14:13.491073Z","iopub.status.idle":"2022-01-25T01:14:13.531813Z","shell.execute_reply.started":"2022-01-25T01:14:13.491004Z","shell.execute_reply":"2022-01-25T01:14:13.530951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Version history\n* v.1: Started\n* v.2: Replaced cosine similarity by MAD. Introduced a projective transformation to model the relationship between quarterly GDP and quarterly sales.\n* v.3: Changed optimization algorithm from L-BFGS-B (default) to Nelder-Mead because the former sometimes terminates abnormally. Added assertion to make sure the optimization terminates successfully.","metadata":{}},{"cell_type":"markdown","source":"# Introduction\n\nA lot of contestants in this competition use external GDP data to model and forecast the annual sales in 2019. Most noted that nominal GDP in USD is a good fit to the training data. Now your modeling may lead you to a more detailed breakdown, such as quarterly sales or even monthly sales in 2019. It seems that quarterly nominal GDP in USD is not easy to find, especially you stick with reputable sources. In the following, I combine annual GDP data in USD from [World Bank](https://data.worldbank.org) and quarterly data in local currency from [IMF](https://data.imf.org) to produce quarterly GDP data in USD.","metadata":{}},{"cell_type":"markdown","source":"# Loading data\n\nI have already extracted the quarterly data from IMF into a CSV file.","metadata":{}},{"cell_type":"code","source":"gdp_quarterly_lcu = pd.read_csv('../input/gdp-fin-nor-swe-20152019-quarterly-imf/GDP_FIN_NOR_SWE_2015-2019_Quarterly_IMF.csv')\ngdp_quarterly_lcu","metadata":{"execution":{"iopub.status.busy":"2022-01-25T01:14:13.53284Z","iopub.execute_input":"2022-01-25T01:14:13.533066Z","iopub.status.idle":"2022-01-25T01:14:13.569605Z","shell.execute_reply.started":"2022-01-25T01:14:13.533036Z","shell.execute_reply":"2022-01-25T01:14:13.568993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Then I load the data from World Bank, specifically the nominal GDP data in USD.","metadata":{}},{"cell_type":"code","source":"df_gdp = pd.read_csv('../input/gdp-fin-nor-swe-20152019-multiple-sources/GDP_FIN_NOR_SWE_2015-2019_Multiple_Sources.csv')\ngdp_annual_usd = df_gdp[(df_gdp['Measure']=='Current prices, current exchange rates')&(df_gdp['Data Source']=='World Bank')].copy()\ngdp_annual_usd","metadata":{"execution":{"iopub.status.busy":"2022-01-25T01:14:13.570835Z","iopub.execute_input":"2022-01-25T01:14:13.571341Z","iopub.status.idle":"2022-01-25T01:14:13.601657Z","shell.execute_reply.started":"2022-01-25T01:14:13.571294Z","shell.execute_reply":"2022-01-25T01:14:13.601043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The only difference between the data in local currency and the data in USD is the exchange rate, which varies from year to year. But we can figure out the quarterly breakdown from the data in local currency and apply it to the data in USD. \n\nFirst step is to calculate the relative contribution of each quarter for each year.","metadata":{}},{"cell_type":"code","source":"gdp_quarterly_lcu['annual']=gdp_quarterly_lcu[['Q1','Q2','Q3','Q4']].to_numpy().sum(axis=1)\nfor q in range(1,5):\n    gdp_quarterly_lcu[F'Q{q}']=gdp_quarterly_lcu[F'Q{q}']/gdp_quarterly_lcu['annual']\ngdp_quarterly_lcu[['Year','Country','Q1','Q2','Q3','Q4']]","metadata":{"execution":{"iopub.status.busy":"2022-01-25T01:14:13.603924Z","iopub.execute_input":"2022-01-25T01:14:13.604131Z","iopub.status.idle":"2022-01-25T01:14:13.630062Z","shell.execute_reply.started":"2022-01-25T01:14:13.604105Z","shell.execute_reply":"2022-01-25T01:14:13.629273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, split the annual USD amount according to the quarterly contributions, and we are done!","metadata":{}},{"cell_type":"code","source":"gdp_quarterly_usd=gdp_quarterly_lcu[['Year','Country','Q1','Q2','Q3','Q4']].join(gdp_annual_usd.set_index(['Year','Country'])['Value'],on=['Year','Country'])\nfor q in range(1,5):\n    gdp_quarterly_usd[F'Q{q}']=gdp_quarterly_usd[F'Q{q}']*gdp_quarterly_usd['Value']\ngdp_quarterly_usd = gdp_quarterly_usd.rename(columns={'Value':'Annual'})\ngdp_quarterly_usd","metadata":{"execution":{"iopub.status.busy":"2022-01-25T01:14:13.631261Z","iopub.execute_input":"2022-01-25T01:14:13.631497Z","iopub.status.idle":"2022-01-25T01:14:13.669579Z","shell.execute_reply.started":"2022-01-25T01:14:13.631468Z","shell.execute_reply":"2022-01-25T01:14:13.668957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Validation\n\nUltimately, we are doing this so that we can forecast 2019 sales for the Kaggle shops using the GDP data. Does the quarterly GDP data we just derived work for that purpose? Let's investigate.\n\nWe first load the training data and add two helper columns `Year` and `Quarter`.","metadata":{}},{"cell_type":"code","source":"train_data = pd.read_csv('../input/tabular-playground-series-jan-2022/train.csv',parse_dates=['date'])\ntrain_data['Year'] = train_data.date.apply(lambda x:x.year)\ntrain_data['Quarter'] = train_data.date.apply(lambda x: (x.month-1)//3+1)\ntrain_data","metadata":{"execution":{"iopub.status.busy":"2022-01-25T01:14:13.670604Z","iopub.execute_input":"2022-01-25T01:14:13.670831Z","iopub.status.idle":"2022-01-25T01:14:14.16669Z","shell.execute_reply.started":"2022-01-25T01:14:13.670794Z","shell.execute_reply":"2022-01-25T01:14:14.165704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For each country-store-product-quarter combination, we calculate the correlation between the quarterly sales and the quarterly GDP from 2015 to 2018. We also calculate the annual GDP correlation for comparison.","metadata":{}},{"cell_type":"markdown","source":"### QoQ and YoY correlation","metadata":{}},{"cell_type":"code","source":"corr = []\nfor country in ['Finland','Norway','Sweden']:\n    for store in ['KaggleMart','KaggleRama']:\n        for product in ['Kaggle Mug','Kaggle Hat','Kaggle Sticker']:\n                row = [country,store,product]\n                df = train_data[(train_data['country']==country)&(train_data['store']==store)&(train_data['product']==product)]\n                for q in range(1,5):\n                    df_q = df[df['Quarter']==q]\n                    sales = [df_q[df_q['Year']==yr]['num_sold'].sum() for yr in range(2015,2019)]\n                    gdp = gdp_quarterly_usd[(gdp_quarterly_usd['Country']==country)&(gdp_quarterly_usd['Year']<2019)][F'Q{q}'].to_numpy()\n                    row.append(pd.Series(sales).corr(pd.Series(gdp)))\n\n                sales = [df[df['Year']==yr]['num_sold'].sum() for yr in range(2015,2019)]\n                gdp = gdp_quarterly_usd[(gdp_quarterly_usd['Country']==country)&(gdp_quarterly_usd['Year']<2019)]['Annual'].to_numpy()\n                row.append(pd.Series(sales).corr(pd.Series(gdp)))\n                corr.append(row)\npd.DataFrame(corr,columns=['country','store','product','Q1','Q2','Q3','Q4','Annual'])","metadata":{"execution":{"iopub.status.busy":"2022-01-25T01:14:14.167816Z","iopub.execute_input":"2022-01-25T01:14:14.168032Z","iopub.status.idle":"2022-01-25T01:14:14.808129Z","shell.execute_reply.started":"2022-01-25T01:14:14.168004Z","shell.execute_reply":"2022-01-25T01:14:14.807421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The year-on-year (linear) correlation is very high, as most contestants already know. The quarter-on-quarter correlation is not as high, especially for Norway. It seems that using the quarterly GDP data to project quarterly sales is not such a good idea.","metadata":{}},{"cell_type":"markdown","source":"What about predicting the quarterly breakdown within a given year? For a given year, the quarterly sales form a distribution. The quarterly GDP data form another distribution. For simplicity, we use the mean absolute deviation (MAD) to compare them, which is valid as long as we normalize the quarterly GDP data and quarterly sales data so that within each year they sum to 1.","metadata":{}},{"cell_type":"markdown","source":"### Predicting quarterly contributions","metadata":{}},{"cell_type":"code","source":"l_1 = []\nfor country in ['Finland','Norway','Sweden']:\n    for store in ['KaggleMart','KaggleRama']:\n        for product in ['Kaggle Mug','Kaggle Hat','Kaggle Sticker']:\n            row = [country,store,product]\n            for yr in range(2015,2019):\n                df = train_data[(train_data['country']==country)&(train_data['store']==store)&\n                                (train_data['product']==product)&(train_data['Year']==yr)]\n                sales = np.array([df[df['Quarter']==q]['num_sold'].sum() for q in range(1,5)])\n                gdp = gdp_quarterly_usd[(gdp_quarterly_usd['Country']==country)&(gdp_quarterly_usd['Year']==yr)][['Q1','Q2','Q3','Q4']].to_numpy().flatten()\n                row.append(np.abs(sales/np.sum(sales)-gdp/np.sum(gdp)).mean())\n            l_1.append(row)\n                           \npd.DataFrame(l_1,columns=['country','store','product','2015','2016','2017','2018'])","metadata":{"execution":{"iopub.status.busy":"2022-01-25T01:14:14.809392Z","iopub.execute_input":"2022-01-25T01:14:14.809778Z","iopub.status.idle":"2022-01-25T01:14:16.104445Z","shell.execute_reply.started":"2022-01-25T01:14:14.809744Z","shell.execute_reply":"2022-01-25T01:14:16.103533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So the MAD's are kind of small, but not good enough. Can we do better? Technically, we have the quarterly GDP's expressed as proportions \\\\((g_0:g_1:g_2:g_3)\\\\) in projective space \\\\(\\mathbb{P}^3\\\\) where \\\\(g_0+g_1+g_2+g_3=1\\\\), and we want to find a model, i.e., a projective transformation, that would give us the quarterly sales proportions \\\\((q_0:q_1:q_2:q_3)\\\\) where \\\\(q_0+q_1+q_2+q_3=1\\\\). We are going to try the simplest projective transformation\n$$\n(g_1:g_2:g_3:g_4)\\mapsto(\\alpha_0g_0:\\alpha_1g_1:\\alpha_2g_2:\\alpha_3g_3)\n$$\nwhere, without loss of generality, we can assume \\\\(\\alpha_0=1\\\\). This innocent looking transformation is more sophisticated than it looks, because it predicts the quarterly sales by a rational transformation:\n$$\nq_i=\\frac{\\alpha_ig_i}{\\alpha_0g_0+\\alpha_1g_1+\\alpha_2g_2+\\alpha_3g_3}\n$$\nFinding the parameters \\\\(\\alpha_1\\\\), \\\\(\\alpha_2\\\\) and \\\\(\\alpha_3\\\\) requires nonlinear optimization with the \\\\(\\ell^1\\\\) objective, and this is done for every country-store-product combination with 4 data points from 2015 to 2018.","metadata":{}},{"cell_type":"code","source":"def obj_fn(x,gdp_quarterly_array,quarterly_sales_array):\n    alpha = np.array([1,x[0],x[1],x[2]]).reshape((1,4))\n    y=alpha*gdp_quarterly_array\n    y=y/np.sum(y,axis=1,keepdims=True)\n    return np.abs(y-quarterly_sales_array).mean()","metadata":{"execution":{"iopub.status.busy":"2022-01-25T01:14:16.105933Z","iopub.execute_input":"2022-01-25T01:14:16.106254Z","iopub.status.idle":"2022-01-25T01:14:16.11354Z","shell.execute_reply.started":"2022-01-25T01:14:16.106211Z","shell.execute_reply":"2022-01-25T01:14:16.112517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy.optimize import minimize\n\nl_1 = []\nsales_est_cache = {}\nfor country in ['Finland','Norway','Sweden']:\n    for store in ['KaggleMart','KaggleRama']:\n        for product in ['Kaggle Mug','Kaggle Hat','Kaggle Sticker']:\n            row = [country,store,product]\n            df = train_data[(train_data['country']==country)&(train_data['store']==store)&\n                                (train_data['product']==product)]\n            quarterly_sales_array = np.array([df[(df['Year']==yr)&(df['Quarter']==q)]['num_sold'].sum() for yr in range(2015,2019) for q in range(1,5)])\n            quarterly_sales_array = quarterly_sales_array.reshape((-1,4))\n            quarterly_sales_array = quarterly_sales_array/np.sum(quarterly_sales_array,axis=1,keepdims=True)\n            gdp_quarterly_array = gdp_quarterly_usd[gdp_quarterly_usd.Country==country][['Q1','Q2','Q3','Q4']].to_numpy().astype(np.float)[:-1,:]\n            gdp_quarterly_array = gdp_quarterly_array/np.sum(gdp_quarterly_array,axis=1,keepdims=True)\n            result = minimize(lambda x: obj_fn(x,gdp_quarterly_array,quarterly_sales_array), (1,1,1), bounds=[(0,None)]*3,\n                             method='Nelder-Mead')\n            assert result.success\n            x1,x2,x3 = result.x\n            alpha = np.array([1,x1,x2,x3]).reshape((1,4))\n            sales_est = alpha*gdp_quarterly_array\n            sales_est=sales_est/np.sum(sales_est,axis=1,keepdims=True)\n            for i in range(4):\n                row.append(np.abs(sales_est[i,:]-quarterly_sales_array[i,:]).mean())\n            l_1.append(row)\n            sales_est_cache[(country,store,product)] = [quarterly_sales_array,gdp_quarterly_array,sales_est]\n    \n                           \npd.DataFrame(l_1,columns=['country','store','product','2015','2016','2017','2018'])","metadata":{"execution":{"iopub.status.busy":"2022-01-25T01:14:16.115078Z","iopub.execute_input":"2022-01-25T01:14:16.115384Z","iopub.status.idle":"2022-01-25T01:14:17.29632Z","shell.execute_reply.started":"2022-01-25T01:14:16.115342Z","shell.execute_reply":"2022-01-25T01:14:17.295447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"MAD's are reduced, in some cases by an order of magnitude. Let's check the extreme results, starting with the worst result, which is Norway-KaggleRama-Mug in 2016 with a MAD of 0.009285. Let's visualize the distributions.","metadata":{}},{"cell_type":"code","source":"quarterly_sales_array,gdp_quarterly_array,sales_est=sales_est_cache[('Norway','KaggleRama','Kaggle Mug')]\nnp.abs(sales_est[1,:]-quarterly_sales_array[1,:]).mean()\nfrom matplotlib import pyplot as plt\nfig = plt.figure(figsize=(8, 8))\nplt.bar(np.arange(4),quarterly_sales_array[1,:],width=0.25)\nplt.bar(np.arange(4)+0.25,sales_est[1,:],width=0.25)\nplt.bar(np.arange(4)+0.5,gdp_quarterly_array[1,:],width=0.25)\nplt.xticks(np.arange(4)+0.25,['Q1','Q2','Q3','Q4'])\nplt.legend(['Actual Sales','Predicted Sales','GDP'])\nplt.title('2016 Quarterly Distributions (Nor-Rama-Mug)')\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-25T01:14:17.297584Z","iopub.execute_input":"2022-01-25T01:14:17.297843Z","iopub.status.idle":"2022-01-25T01:14:17.565569Z","shell.execute_reply.started":"2022-01-25T01:14:17.297812Z","shell.execute_reply":"2022-01-25T01:14:17.564929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now visualize the best result, which is Sweden-KaggleRama-Hat in 2018 with a MAD of 0.000234.","metadata":{}},{"cell_type":"code","source":"quarterly_sales_array,gdp_quarterly_array,sales_est=sales_est_cache[('Sweden','KaggleRama','Kaggle Hat')]\nnp.abs(sales_est[1,:]-quarterly_sales_array[1,:]).mean()\nfrom matplotlib import pyplot as plt\nfig = plt.figure(figsize=(8, 8))\nplt.bar(np.arange(4),quarterly_sales_array[1,:],width=0.25)\nplt.bar(np.arange(4)+0.25,sales_est[1,:],width=0.25)\nplt.bar(np.arange(4)+0.5,gdp_quarterly_array[1,:],width=0.25)\nplt.xticks(np.arange(4)+0.25,['Q1','Q2','Q3','Q4'])\nplt.legend(['Actual Sales','Predicted Sales','GDP'])\nplt.title('2018 Quarterly Distributions (Swe-Rama-Hat)')\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-25T01:14:17.566887Z","iopub.execute_input":"2022-01-25T01:14:17.567189Z","iopub.status.idle":"2022-01-25T01:14:17.812228Z","shell.execute_reply.started":"2022-01-25T01:14:17.56715Z","shell.execute_reply":"2022-01-25T01:14:17.81161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If we had used the quarterly GDP directly for prediction, we would have predicted a best Q4, whereas the actual Q4 was mediocre.","metadata":{}},{"cell_type":"markdown","source":"# Conclusion\n\nIt seems that a plausible strategy is to forecast the 2019 annual sales using the annual GDP data, and then predict the quarterly breakdown using the quarterly data. A simple projective transformation can be used to model the relationship between quarterly GDP distributions and quarterly sales distributions. Since we are using only the relative contributions of the quarters, we could have just used the quarterly GDP data in local currency.","metadata":{}}]}