{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-01-04T04:12:48.418829Z","iopub.execute_input":"2022-01-04T04:12:48.419655Z","iopub.status.idle":"2022-01-04T04:12:48.454202Z","shell.execute_reply.started":"2022-01-04T04:12:48.41951Z","shell.execute_reply":"2022-01-04T04:12:48.453024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Introduction\n\nIn this notebook, we'll investigate long term trend in a time series. For the dataset at hand, this means the trend in the 365-day moving average. There are seasonal variations as well, but we'll focus for the moment on averages over 365 days. We would need to remove this trend before tree models can predict seasonal variations within a year.","metadata":{}},{"cell_type":"code","source":"from matplotlib import pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2022-01-04T04:12:48.456003Z","iopub.execute_input":"2022-01-04T04:12:48.456255Z","iopub.status.idle":"2022-01-04T04:12:48.460907Z","shell.execute_reply.started":"2022-01-04T04:12:48.456226Z","shell.execute_reply":"2022-01-04T04:12:48.459937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data =  pd.read_csv('../input/tabular-playground-series-jan-2022/train.csv',parse_dates=['date'])\ntest_data = pd.read_csv('../input/tabular-playground-series-jan-2022/test.csv',parse_dates=['date'])","metadata":{"execution":{"iopub.status.busy":"2022-01-04T04:12:48.462847Z","iopub.execute_input":"2022-01-04T04:12:48.463502Z","iopub.status.idle":"2022-01-04T04:12:48.560628Z","shell.execute_reply.started":"2022-01-04T04:12:48.463453Z","shell.execute_reply":"2022-01-04T04:12:48.559536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"assert np.all(train_data['num_sold']>0)","metadata":{"execution":{"iopub.status.busy":"2022-01-04T04:12:48.564868Z","iopub.execute_input":"2022-01-04T04:12:48.565757Z","iopub.status.idle":"2022-01-04T04:12:48.574112Z","shell.execute_reply.started":"2022-01-04T04:12:48.565695Z","shell.execute_reply":"2022-01-04T04:12:48.572998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from statsmodels.tsa.stattools import adfuller, kpss\n\ndef adf_test(timeseries, significance=0.05):\n    print(\"Results of Dickey-Fuller Test:\")\n    dftest = adfuller(timeseries, autolag=\"AIC\", regression='ctt')\n    dfoutput = pd.Series(\n        dftest[0:4],\n        index=[\n            \"Test Statistic\",\n            \"p-value\",\n            \"#Lags Used\",\n            \"Number of Observations Used\",\n        ],\n    )\n    for key, value in dftest[4].items():\n        dfoutput[\"Critical Value (%s)\" % key] = value\n\n    print(dfoutput)\n    if dfoutput['p-value']<significance:\n        print(f'At {significance} significance level, the time series is trend-stationary')\n    else:\n        print(f'At {significance} significance level, the time series is not trend-stationary')\n    print()\n        \n\ndef kpss_test(timeseries,significance=0.05):\n    print(\"Results of KPSS Test:\")\n    kpsstest = kpss(timeseries, regression=\"ct\", nlags=\"auto\")\n    kpss_output = pd.Series(\n        kpsstest[0:3], index=[\"Test Statistic\", \"p-value\", \"Lags Used\"]\n    )\n    for key, value in kpsstest[3].items():\n        kpss_output[\"Critical Value (%s)\" % key] = value\n\n    print(kpss_output)\n    if kpss_output['p-value']<significance:\n        print(f'At {significance} significance level, the time series is not trend-stationary')\n    else:\n        print(f'At {significance} significance level, the time series is trend-stationary')\n    print() \n    \ndef test_for_stationarity(df,col,significance=0.05):\n    print(F'Column to be tested: {col}')\n    print()\n    adf_test(pd.Series(df[col].to_numpy(),index=df['date']),significance=significance)\n    kpss_test(pd.Series(df[col].to_numpy(),index=df['date']),significance=significance)","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-04T04:12:48.575751Z","iopub.execute_input":"2022-01-04T04:12:48.576819Z","iopub.status.idle":"2022-01-04T04:12:49.850348Z","shell.execute_reply.started":"2022-01-04T04:12:48.57677Z","shell.execute_reply":"2022-01-04T04:12:49.84938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_df(df,country,store,product):\n    return df[(df['country']==country) & (df['store']==store) & (df['product']==product)].copy()","metadata":{"execution":{"iopub.status.busy":"2022-01-04T04:12:49.851762Z","iopub.execute_input":"2022-01-04T04:12:49.852417Z","iopub.status.idle":"2022-01-04T04:12:49.85776Z","shell.execute_reply.started":"2022-01-04T04:12:49.852374Z","shell.execute_reply":"2022-01-04T04:12:49.856731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We'll consider the following combination of country, store and product. That's why I call this a case study. It is reasonble to expect that the trend would be different for different combinations.","metadata":{}},{"cell_type":"code","source":"country,store,product = 'Finland','KaggleMart','Kaggle Mug'","metadata":{"execution":{"iopub.status.busy":"2022-01-04T04:12:49.859376Z","iopub.execute_input":"2022-01-04T04:12:49.860724Z","iopub.status.idle":"2022-01-04T04:12:49.870254Z","shell.execute_reply.started":"2022-01-04T04:12:49.860674Z","shell.execute_reply":"2022-01-04T04:12:49.869194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = get_df(train_data,country,store,product)\ndf_test = get_df(test_data,country,store,product)","metadata":{"execution":{"iopub.status.busy":"2022-01-04T04:12:49.871494Z","iopub.execute_input":"2022-01-04T04:12:49.8718Z","iopub.status.idle":"2022-01-04T04:12:49.898915Z","shell.execute_reply.started":"2022-01-04T04:12:49.871764Z","shell.execute_reply":"2022-01-04T04:12:49.898015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Working in log-space\n\nNow we add new columns into the dataframe that will be useful later. `t` is the number of days since January 1, 2015. `log_num_sold` is where we'll be working on. It is noteworthy that if we remove a trend additively in log-space, it corresponds to removing a trend multiplicatively in the original space. More specifically, if we remove a trend \\\\(\\eta_t\\\\) from \\\\(y_t\\\\) and \\\\(y_t'\\\\) so that \\\\(y_t=\\eta_t\\bar{y}_t\\\\), \\\\(y_t'=\\eta_t\\bar{y}_t'\\\\), the SMAPE term is the same, so that we can work exclusively in the log-space without worrying about the trend that has been removed. If we remove a trend additively in the original space, this would not be possible.\n$$\n2\\frac{|y_t-y_t'|}{|y_t|+|y_t'|}=2\\frac{|\\bar{y}_t-\\bar{y}_t'|}{|\\bar{y}_t|+|\\bar{y}_t'|}\n$$","metadata":{}},{"cell_type":"code","source":"t0 = df.date.iloc[0]\ndf['t'] = (df.date-t0).astype('timedelta64[D]').astype(np.int).to_numpy()\ndf_test['t'] = (df_test.date-t0).astype('timedelta64[D]').to_numpy()\ndf['log_num_sold'] = np.log(df['num_sold'])\ndf = df.set_index('t')\ndf_test = df_test.set_index('t')","metadata":{"execution":{"iopub.status.busy":"2022-01-04T04:12:49.900277Z","iopub.execute_input":"2022-01-04T04:12:49.900528Z","iopub.status.idle":"2022-01-04T04:12:49.91774Z","shell.execute_reply.started":"2022-01-04T04:12:49.9005Z","shell.execute_reply":"2022-01-04T04:12:49.916805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's make some plots to get some insight.","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(figsize=(18, 6))\nfig.add_subplot(131)\nplt.plot(df.date,df.log_num_sold)\nplt.title('log_num_sold: raw plot')\nfig.add_subplot(132)\nplt.plot(df.date,df.log_num_sold.rolling(365).mean())\nplt.title('log_num_sold: 365-day moving average')\nfig.add_subplot(133)\nplt.plot(df.date,df.log_num_sold.rolling(183).mean())\nplt.title('log_num_sold: 183-day moving average')\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-04T04:12:49.921846Z","iopub.execute_input":"2022-01-04T04:12:49.922467Z","iopub.status.idle":"2022-01-04T04:12:50.588906Z","shell.execute_reply.started":"2022-01-04T04:12:49.922412Z","shell.execute_reply":"2022-01-04T04:12:50.587884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From these plots, it is clear that there is a year-on-year up trend, while there is a seasonal variation within a year.","metadata":{}},{"cell_type":"markdown","source":"# Stationarity?\n\nIt is common to use some statistical tests to test for stationarity of a time series. We will use the augmented Dickey-Fuller Test and the KPSS Test.","metadata":{}},{"cell_type":"code","source":"test_for_stationarity(df,'log_num_sold')","metadata":{"execution":{"iopub.status.busy":"2022-01-04T04:12:50.590298Z","iopub.execute_input":"2022-01-04T04:12:50.590533Z","iopub.status.idle":"2022-01-04T04:12:50.691535Z","shell.execute_reply.started":"2022-01-04T04:12:50.590505Z","shell.execute_reply":"2022-01-04T04:12:50.690661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Not suprisingly, the answer is no. The KPSS test cannot reject the null hypothesis that the time series is trend-stationary, while the Dickey-Fuller test cannot reject the null hypothesis that it is *not* trend-stationary. We can tell from the plots already that it is not stationary. ","metadata":{}},{"cell_type":"markdown","source":"# Spline modeling\n\nLooking at the 365-day moving average plot, it seems that we'd better go for a non-parametric model. We go with the PCHIP spline interpolater for its local monotonicity that tends to overshoot less. For the control points (knots) we use 5 evenly spaced time points over the 4-year period.","metadata":{}},{"cell_type":"code","source":"x0=[0,365,731,1096,1460] \nz0=np.full(len(x0),df.log_num_sold.mean()) # initial solution","metadata":{"execution":{"iopub.status.busy":"2022-01-04T04:12:50.693233Z","iopub.execute_input":"2022-01-04T04:12:50.693769Z","iopub.status.idle":"2022-01-04T04:12:50.701999Z","shell.execute_reply.started":"2022-01-04T04:12:50.693725Z","shell.execute_reply":"2022-01-04T04:12:50.700961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Instead of fitting the spline to the data points directly (which vary wildly as a stochastic process with a lot of outliers), we chose to fit the spline such that its 365-day moving average matches the 365-day moving average of the data. Otherwise, it is a pretty straightword least squares fit.","metadata":{}},{"cell_type":"code","source":"window=365","metadata":{"execution":{"iopub.status.busy":"2022-01-04T04:12:50.704123Z","iopub.execute_input":"2022-01-04T04:12:50.705591Z","iopub.status.idle":"2022-01-04T04:12:50.713124Z","shell.execute_reply.started":"2022-01-04T04:12:50.705547Z","shell.execute_reply":"2022-01-04T04:12:50.712272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from numba import njit\n\n@njit(fastmath=True)\ndef moving_average(x,window):\n    s = x[:len(x)-window+1].copy()\n    for i in range(1,window):\n        s += x[i:len(x)-window+1+i]\n    return s/window        ","metadata":{"execution":{"iopub.status.busy":"2022-01-04T04:12:50.714726Z","iopub.execute_input":"2022-01-04T04:12:50.715699Z","iopub.status.idle":"2022-01-04T04:12:51.500164Z","shell.execute_reply.started":"2022-01-04T04:12:50.715657Z","shell.execute_reply":"2022-01-04T04:12:51.499381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy.interpolate import PchipInterpolator\ndef obj_fn(z,x0,x,y_ma,window):\n    y_pred = moving_average(PchipInterpolator(x0, z)(x),window)\n    return y_pred-y_ma","metadata":{"execution":{"iopub.status.busy":"2022-01-04T04:12:51.501558Z","iopub.execute_input":"2022-01-04T04:12:51.501949Z","iopub.status.idle":"2022-01-04T04:12:51.507614Z","shell.execute_reply.started":"2022-01-04T04:12:51.501892Z","shell.execute_reply":"2022-01-04T04:12:51.506535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy.optimize import least_squares\ny_ma = moving_average(df.log_num_sold.to_numpy(),window)\nresult = least_squares(lambda z: obj_fn(z,x0,df.index,y_ma,window),z0)","metadata":{"execution":{"iopub.status.busy":"2022-01-04T04:12:51.508816Z","iopub.execute_input":"2022-01-04T04:12:51.509647Z","iopub.status.idle":"2022-01-04T04:12:52.300555Z","shell.execute_reply.started":"2022-01-04T04:12:51.509599Z","shell.execute_reply":"2022-01-04T04:12:52.299664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now add columns `year_trend` and `log_num_sold_1` which is `log_num_sold` with the year trend removed.","metadata":{}},{"cell_type":"code","source":"cs = PchipInterpolator(x0, result.x)\ndf['year_trend'] = cs(df.index)\ndf['log_num_sold_1'] = df['log_num_sold']-df['year_trend']\ndf_test['year_trend'] = cs(df_test.index)\ndf","metadata":{"execution":{"iopub.status.busy":"2022-01-04T04:12:52.301769Z","iopub.execute_input":"2022-01-04T04:12:52.302013Z","iopub.status.idle":"2022-01-04T04:12:52.329876Z","shell.execute_reply.started":"2022-01-04T04:12:52.301985Z","shell.execute_reply":"2022-01-04T04:12:52.329294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's make a plot of the 365-day moving averages to see how well the fit went.","metadata":{}},{"cell_type":"code","source":"plt.plot(df.date,df.log_num_sold.rolling(window).mean())\nplt.plot(df.date,df.year_trend.rolling(window).mean())\nplt.legend(['log_num_sold', 'year_trend'], loc='upper left')\nplt.title('365-day moving averages')\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-04T04:12:52.330883Z","iopub.execute_input":"2022-01-04T04:12:52.331226Z","iopub.status.idle":"2022-01-04T04:12:52.567427Z","shell.execute_reply.started":"2022-01-04T04:12:52.331192Z","shell.execute_reply":"2022-01-04T04:12:52.566884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Is the remainder `log_num_sold_1` stationary?","metadata":{}},{"cell_type":"code","source":"test_for_stationarity(df,'log_num_sold_1')","metadata":{"execution":{"iopub.status.busy":"2022-01-04T04:12:52.568376Z","iopub.execute_input":"2022-01-04T04:12:52.568659Z","iopub.status.idle":"2022-01-04T04:12:52.655284Z","shell.execute_reply.started":"2022-01-04T04:12:52.568635Z","shell.execute_reply":"2022-01-04T04:12:52.654362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Not surprisingly, the answer is still no. We haven't removed the seasonal trend within a year yet. Let's make some plots to confirm that.","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(figsize=(18, 6))\nfig.add_subplot(131)\nplt.plot(df.date,df.log_num_sold_1)\nplt.title('log_num_sold_1: raw plot')\nfig.add_subplot(132)\nplt.plot(df.date,df.log_num_sold_1.rolling(365).mean())\nplt.title('log_num_sold_1: 365-day moving average')\nfig.add_subplot(133)\nplt.plot(df.date,df.log_num_sold_1.rolling(183).mean())\nplt.title('log_num_sold_1: 183-day moving average')\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-04T04:12:52.657149Z","iopub.execute_input":"2022-01-04T04:12:52.657784Z","iopub.status.idle":"2022-01-04T04:12:53.253338Z","shell.execute_reply.started":"2022-01-04T04:12:52.657736Z","shell.execute_reply":"2022-01-04T04:12:53.252693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So we see from the 365-day moving average that over a 365-day period, `log_num_sold_1` is mostly noise. But the cyclical trend is obvious in the 183-day moving average.\n\nLet's visualize the year trend in the original `num_sold` space","metadata":{}},{"cell_type":"code","source":"plt.plot(df.date,np.exp(df.year_trend),'b-')\nplt.plot(df_test.date,np.exp(df_test.year_trend), 'g-')\nplt.title('num_sold year trend')\nplt.legend(['num_sold 2015 - 2018 (fitted)', 'num_sold 2019 (extrapolated)'], loc='upper left')\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-04T04:12:53.254522Z","iopub.execute_input":"2022-01-04T04:12:53.254891Z","iopub.status.idle":"2022-01-04T04:12:53.462316Z","shell.execute_reply.started":"2022-01-04T04:12:53.254863Z","shell.execute_reply":"2022-01-04T04:12:53.461374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally, we can also estimate annual sales for year 2019 using the trend. ","metadata":{}},{"cell_type":"code","source":"xx = [np.datetime64(F'{year}') for year in range(2015,2020)]\nyy_pred = []\nyy_actual = []\ndf0 = pd.concat([df,df_test])\nfor i in range(2015,2020):\n    yy_pred.append(np.sum(np.exp(df0.year_trend[(df0.date>=np.datetime64(F'{i}-01-01'))&(df0.date<=np.datetime64(F'{i}-12-31'))])))\n    yy_actual.append(np.sum(df.num_sold[(df.date>=np.datetime64(F'{i}-01-01'))&(df.date<=np.datetime64(F'{i}-12-31'))]))\n    \nplt.plot(xx[:-1],yy_actual[:-1],'cx-')\nplt.plot(xx,yy_pred,'bo-')\nplt.legend(['actual', 'fitted/extrapolated'], loc='upper left')\nplt.title('Annual sales')\n\nfrom matplotlib.dates import YearLocator, MonthLocator, DateFormatter\nyears = YearLocator()   \nyearsFmt = DateFormatter('%Y')\nax = plt.gca()\nax.xaxis.set_major_locator(years)\nax.xaxis.set_major_formatter(yearsFmt)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-04T04:12:53.46358Z","iopub.execute_input":"2022-01-04T04:12:53.463797Z","iopub.status.idle":"2022-01-04T04:12:53.657781Z","shell.execute_reply.started":"2022-01-04T04:12:53.46377Z","shell.execute_reply":"2022-01-04T04:12:53.656912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# What's Next?\n\nIn principle, the time series `log_num_sold_1` can be fitted already with your favorite tree model. It is not a stationary time series yet but over-the-years trend has been removed, so the tree model should be able to predict the seasonal cycles. For completeness sake, I plan to detrend the time series completely in a future notebook by removing the cyclical trend. Hope to see you then!","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}