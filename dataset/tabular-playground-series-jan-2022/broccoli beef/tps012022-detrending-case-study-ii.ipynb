{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-01-24T05:07:54.129122Z","iopub.execute_input":"2022-01-24T05:07:54.130245Z","iopub.status.idle":"2022-01-24T05:07:54.167717Z","shell.execute_reply.started":"2022-01-24T05:07:54.13Z","shell.execute_reply":"2022-01-24T05:07:54.16703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from matplotlib import pyplot as plt","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-01-24T05:07:54.171331Z","iopub.execute_input":"2022-01-24T05:07:54.17169Z","iopub.status.idle":"2022-01-24T05:07:54.17581Z","shell.execute_reply.started":"2022-01-24T05:07:54.171652Z","shell.execute_reply":"2022-01-24T05:07:54.174961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Introduction\n\nIn this notebook, I will continue the effort started in [Detrending Case Study I](https://www.kaggle.com/siukeitin/tps012022-detrending-case-study-i), in which I model a long term trend (over the years) using a spline. One of the reasons for using a spline is so that I can extrapolate the trend to 2019. Of course, extrapolation with splines is risky, as a purely mathematical construct need not predict \"reality\", even if that \"reality\" is fabricated. Since then, many contestants use external GDP data to model a more plausible 2019 annual trend. I also think that that's more reliable than purely mathematical spline extrapolation.\n\nIn the following, I'll describe a different approach to detrending that is more data-driven. Finding the trend and removing it from the training data turns out to be relatively trivial. The problem is how to estimate the trend for the test data. We will describe an attempt using external annual and quarterly GDP data.","metadata":{}},{"cell_type":"code","source":"from statsmodels.tsa.stattools import adfuller, kpss\n\ndef adf_test(timeseries, significance=0.05):\n    print(\"Results of Dickey-Fuller Test:\")\n    dftest = adfuller(timeseries, autolag=\"AIC\", regression='ctt')\n    dfoutput = pd.Series(\n        dftest[0:4],\n        index=[\n            \"Test Statistic\",\n            \"p-value\",\n            \"#Lags Used\",\n            \"Number of Observations Used\",\n        ],\n    )\n    for key, value in dftest[4].items():\n        dfoutput[\"Critical Value (%s)\" % key] = value\n\n    print(dfoutput)\n    if dfoutput['p-value']<significance:\n        print(f'At {significance} significance level, the time series is trend-stationary')\n    else:\n        print(f'At {significance} significance level, the time series is not trend-stationary')\n    print()\n        \n\ndef kpss_test(timeseries,significance=0.05):\n    print(\"Results of KPSS Test:\")\n    kpsstest = kpss(timeseries, regression=\"ct\", nlags=\"auto\")\n    kpss_output = pd.Series(\n        kpsstest[0:3], index=[\"Test Statistic\", \"p-value\", \"Lags Used\"]\n    )\n    for key, value in kpsstest[3].items():\n        kpss_output[\"Critical Value (%s)\" % key] = value\n\n    print(kpss_output)\n    if kpss_output['p-value']<significance:\n        print(f'At {significance} significance level, the time series is not trend-stationary')\n    else:\n        print(f'At {significance} significance level, the time series is trend-stationary')\n    print() \n    \ndef test_for_stationarity(timeseries,significance=0.05):\n    adf_test(timeseries,significance=significance)\n    kpss_test(timeseries,significance=significance)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-01-24T05:07:54.177248Z","iopub.execute_input":"2022-01-24T05:07:54.177903Z","iopub.status.idle":"2022-01-24T05:07:55.329213Z","shell.execute_reply.started":"2022-01-24T05:07:54.177852Z","shell.execute_reply":"2022-01-24T05:07:55.328306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_df(df,country,store,product):\n    return df[(df['country']==country) & (df['store']==store) & (df['product']==product)].copy()","metadata":{"execution":{"iopub.status.busy":"2022-01-24T05:07:55.331742Z","iopub.execute_input":"2022-01-24T05:07:55.332028Z","iopub.status.idle":"2022-01-24T05:07:55.337539Z","shell.execute_reply.started":"2022-01-24T05:07:55.331991Z","shell.execute_reply":"2022-01-24T05:07:55.336389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data=pd.read_csv('../input/tabular-playground-series-jan-2022/train.csv',parse_dates=['date'])\ntest_data=pd.read_csv('../input/tabular-playground-series-jan-2022/test.csv',parse_dates=['date'])","metadata":{"execution":{"iopub.status.busy":"2022-01-24T05:07:55.338563Z","iopub.execute_input":"2022-01-24T05:07:55.338837Z","iopub.status.idle":"2022-01-24T05:07:55.444942Z","shell.execute_reply.started":"2022-01-24T05:07:55.338807Z","shell.execute_reply":"2022-01-24T05:07:55.444044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"country,store,product = 'Finland','KaggleMart','Kaggle Mug'","metadata":{"execution":{"iopub.status.busy":"2022-01-24T05:07:55.446161Z","iopub.execute_input":"2022-01-24T05:07:55.446746Z","iopub.status.idle":"2022-01-24T05:07:55.450745Z","shell.execute_reply.started":"2022-01-24T05:07:55.446706Z","shell.execute_reply":"2022-01-24T05:07:55.449726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = get_df(train_data,country,store,product)\ndf_test = get_df(test_data,country,store,product)","metadata":{"execution":{"iopub.status.busy":"2022-01-24T05:07:55.452211Z","iopub.execute_input":"2022-01-24T05:07:55.452622Z","iopub.status.idle":"2022-01-24T05:07:55.489332Z","shell.execute_reply.started":"2022-01-24T05:07:55.452582Z","shell.execute_reply":"2022-01-24T05:07:55.488345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Motivation for doing the detrending from scratch\n\nBut perhaps I should explain the reason for this detrending effort, when there are existing tools out there to perform detrending. Typically, it is taught that a time series can be decomposed into a trend component, a seasonal component and a residual component. To demonstrate that such approach may not be desirable for the current dataset, let's try the well-known [STL decomposition](http://bit.ly/stl1990). We will use the implementation from [statsmodels](https://www.statsmodels.org). Since we would like to decompose the time series multiplicatively, we take logarithm before passing in the data.","metadata":{}},{"cell_type":"code","source":"from statsmodels.tsa.seasonal import STL\n\nresult = STL(np.log(df.num_sold), period= 365, seasonal=183).fit()\nfig = plt.figure(figsize=(30, 6))\nfig.add_subplot(311)\nplt.plot(np.exp(result.trend))\nplt.title('trend')\nfig.add_subplot(312)\nplt.plot(np.exp(result.seasonal))\nplt.title('seasonal')\nfig.add_subplot(313)\nplt.plot(np.exp(result.resid))\nplt.title('residual')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-24T05:07:55.490542Z","iopub.execute_input":"2022-01-24T05:07:55.491441Z","iopub.status.idle":"2022-01-24T05:07:56.667583Z","shell.execute_reply.started":"2022-01-24T05:07:55.491405Z","shell.execute_reply":"2022-01-24T05:07:56.666729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"While the trend component looks reasonably smooth, the seasonal component seems to include a lot of high frequencies and looks much less smooth than the [textbook example](https://www.statsmodels.org/dev/examples/notebooks/generated/stl_decomposition.html) for the \\\\(\\mbox{CO}_2\\\\) data. One big concern is that removing these high frequencies from the residual component might actually cause damage to the data when we eventually apply machine learning to the residual series. Another big question is, if we add the trend and seasonal component as features to the training data, how do we do that for the test data? There is no obvious way to do that. Training on the residual component would be pointless if we don't have the trend and seasonal component for the test data.","metadata":{}},{"cell_type":"markdown","source":"# Detrending the training data by monthly sales","metadata":{}},{"cell_type":"markdown","source":"We begin by adding some helper columns.","metadata":{}},{"cell_type":"code","source":"df['year'] = df.date.apply(lambda x:x.year)\ndf['month'] = df.date.apply(lambda x:x.month)\ndf['quarter'] = df.date.apply(lambda x: (x.month-1)//3+1)","metadata":{"execution":{"iopub.status.busy":"2022-01-24T05:07:56.668957Z","iopub.execute_input":"2022-01-24T05:07:56.669662Z","iopub.status.idle":"2022-01-24T05:07:56.704174Z","shell.execute_reply.started":"2022-01-24T05:07:56.669609Z","shell.execute_reply":"2022-01-24T05:07:56.703494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2022-01-24T05:07:56.705158Z","iopub.execute_input":"2022-01-24T05:07:56.705561Z","iopub.status.idle":"2022-01-24T05:07:56.729966Z","shell.execute_reply.started":"2022-01-24T05:07:56.705526Z","shell.execute_reply":"2022-01-24T05:07:56.728906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now compute the quarterly sales (to be used later) and the monthly sales.","metadata":{}},{"cell_type":"code","source":"quarterly_sales = np.zeros((4,4))\nfor year in range(2015,2019):\n    for quarter in range(1,5):\n        quarterly_sales[year-2015,quarter-1] = df[(df.year==year)&(df.quarter==quarter)]['num_sold'].sum()\n\nquarterly_sales = quarterly_sales/np.sum(quarterly_sales,axis=1,keepdims=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-24T05:07:56.731734Z","iopub.execute_input":"2022-01-24T05:07:56.732079Z","iopub.status.idle":"2022-01-24T05:07:56.761487Z","shell.execute_reply.started":"2022-01-24T05:07:56.732034Z","shell.execute_reply":"2022-01-24T05:07:56.760693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"monthly_sales = []\nfor year in range(2015,2019):\n    for month in range(1,13):\n        monthly_sales.append([year,month,df[(df.year==year)&(df.month==month)]['num_sold'].sum()])\n        \nmonthly_sales_df = pd.DataFrame(monthly_sales,columns=['year','month','monthly_sales'])","metadata":{"execution":{"iopub.status.busy":"2022-01-24T05:07:56.762802Z","iopub.execute_input":"2022-01-24T05:07:56.763308Z","iopub.status.idle":"2022-01-24T05:07:56.81194Z","shell.execute_reply.started":"2022-01-24T05:07:56.763276Z","shell.execute_reply":"2022-01-24T05:07:56.811106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.join(monthly_sales_df.set_index(['year','month']),on=['year','month'])","metadata":{"execution":{"iopub.status.busy":"2022-01-24T05:07:56.816221Z","iopub.execute_input":"2022-01-24T05:07:56.816534Z","iopub.status.idle":"2022-01-24T05:07:56.836978Z","shell.execute_reply.started":"2022-01-24T05:07:56.816502Z","shell.execute_reply":"2022-01-24T05:07:56.835836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now simply remove the monthly sales from `num_sold`.","metadata":{}},{"cell_type":"code","source":"df['num_sold_resid'] = df['num_sold']/df['monthly_sales']*30\ndf","metadata":{"execution":{"iopub.status.busy":"2022-01-24T05:07:56.838294Z","iopub.execute_input":"2022-01-24T05:07:56.838648Z","iopub.status.idle":"2022-01-24T05:07:56.861411Z","shell.execute_reply.started":"2022-01-24T05:07:56.838604Z","shell.execute_reply":"2022-01-24T05:07:56.860413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I claim that `num_sold_resid` is already detrend. Let's use statistical tests to ascertain that.","metadata":{}},{"cell_type":"code","source":"test_for_stationarity(df['num_sold_resid'])","metadata":{"execution":{"iopub.status.busy":"2022-01-24T05:07:56.86277Z","iopub.execute_input":"2022-01-24T05:07:56.863046Z","iopub.status.idle":"2022-01-24T05:07:56.982212Z","shell.execute_reply.started":"2022-01-24T05:07:56.863006Z","shell.execute_reply":"2022-01-24T05:07:56.981264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The p-value for the Dickey-Fuller test is actually very small (much smaller than the 0.05 threshold). Let's visualize what's left after removing the monthly sales.","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(figsize=(30, 3))\nplt.plot(df.date,df.num_sold_resid)\nplt.title('num_sold_resid')\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-24T05:07:56.984007Z","iopub.execute_input":"2022-01-24T05:07:56.984645Z","iopub.status.idle":"2022-01-24T05:07:57.319302Z","shell.execute_reply.started":"2022-01-24T05:07:56.984581Z","shell.execute_reply":"2022-01-24T05:07:57.31826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"OK, looks pretty trendless. And what does the trend that we just removed look like?","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(figsize=(30, 3))\nplt.plot(df.date,df.monthly_sales)\nplt.title('monthly_sales')\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-24T05:07:57.320509Z","iopub.execute_input":"2022-01-24T05:07:57.32079Z","iopub.status.idle":"2022-01-24T05:07:57.595168Z","shell.execute_reply.started":"2022-01-24T05:07:57.32076Z","shell.execute_reply":"2022-01-24T05:07:57.594145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This trend is piecewise constant which may not look like the kind of smooth trends shown in textbooks, but we can be sure that no high frequency information has been  removed from the time series. For example, a spike due to a holiday is still in the data, to be learned by our regressor model.","metadata":{}},{"cell_type":"markdown","source":"# Feature engineering for the test data","metadata":{}},{"cell_type":"markdown","source":"So that was easy, detrending the training data. But we added a feature `monthly_sales` to the training data. We could remove that feature when we train on `num_sold_resid`, but we still need the feature for the test data because we need to combine the predictions from our regressor model with the monthly sales for the  test data to produce our final predictions. To produce `monthly_sales` column for the test data, we'll follow the following steps.\n\n1. Estimate the annual sales for 2019 from the annual GDP data\n2. Estimate the quarterly sales breakdown for 2019 from the quarterly GDP data\n3. Estimate the monthly sales breakdown for 2019 from the quarterly sales breakdown from step 2 and past history (2015 - 2018) of monthly sales breakdown\n4. Combine results from step 1 and step 3 to produce the monthly sales estimate in units sold","metadata":{}},{"cell_type":"markdown","source":"### Estimating the annual sales for 2019","metadata":{}},{"cell_type":"markdown","source":"The annual GDP data is highly (linearly) correlated with the annual sales data, so we just use linear regression fit and interpolate (since 2019 GDP actually drops).","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n\ndf_gdp = pd.read_csv('../input/gdp-fin-nor-swe-20152019-multiple-sources/GDP_FIN_NOR_SWE_2015-2019_Multiple_Sources.csv')\ngdp_annual = df_gdp[(df_gdp['Measure']=='Current prices, current exchange rates')&(df_gdp['Data Source']=='World Bank')&\n                   (df_gdp['Country']==country)].copy()\nannual_sales=df.groupby('year')['num_sold'].sum()\n\ndef project_sales(gdp,annual_sales,plot=True):\n    x=gdp['Value'].to_numpy()[:-1].reshape((-1,1))\n    y=annual_sales.to_numpy()\n    linreg = LinearRegression().fit(x,y)\n    x_test=[[gdp['Value'].iloc[-1]]]\n    y_test = linreg.predict(x_test)[0]\n    y_pred = linreg.predict(x)\n    residues = y_pred-y\n    mu = np.mean(residues)\n    sigma = np.std(residues)\n    if plot:\n        plt.plot(x,y,'x')\n        plt.errorbar(x_test,[y_test],fmt='o',capsize=3,yerr=sigma)\n        plot_xrange = np.linspace(min(x.flatten()),max(x.flatten()),100)\n        plt.plot(plot_xrange,linreg.predict(plot_xrange.reshape((-1,1))))\n        plt.legend(['GDP/annual sales data', 'Fitted line','Projected 2019 annual sales'], loc='upper left')\n    return y_test, sigma\n\nannual_sales_2019,_ = project_sales(gdp_annual,annual_sales)","metadata":{"execution":{"iopub.status.busy":"2022-01-24T05:07:57.596368Z","iopub.execute_input":"2022-01-24T05:07:57.596808Z","iopub.status.idle":"2022-01-24T05:07:58.124391Z","shell.execute_reply.started":"2022-01-24T05:07:57.596773Z","shell.execute_reply":"2022-01-24T05:07:58.123718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Estimating quarterly sales breakdown","metadata":{}},{"cell_type":"markdown","source":"The quarterly GDP data does not correlate so well with the quarterly sales so we cannot estimate the quarterly sales using linear regression as was done with the annual data. However, the quarterly sales breakdown (proportions of the annual GDP) can be predicted from the quarterly GDP data. For more details, please see [my other notebook](https://www.kaggle.com/siukeitin/tps012022-getting-quarterly-gdp-in-usd). In short, it involves determining the parameters for a projective transformation that maps quarterly GDP breakdown to quarterly sales breakdown.","metadata":{}},{"cell_type":"code","source":"gdp_quarterly_df = pd.read_csv('../input/gdp-fin-nor-swe-20152019-quarterly-imf/GDP_FIN_NOR_SWE_2015-2019_Quarterly_IMF.csv')\ngdp_quarterly = gdp_quarterly_df[gdp_quarterly_df.Country==country][['Q1','Q2','Q3','Q4']].to_numpy().astype(np.float)\ngdp_quarterly = gdp_quarterly/np.sum(gdp_quarterly,axis=1,keepdims=True)\ngdp_quarterly","metadata":{"execution":{"iopub.status.busy":"2022-01-24T05:07:58.126287Z","iopub.execute_input":"2022-01-24T05:07:58.126667Z","iopub.status.idle":"2022-01-24T05:07:58.145516Z","shell.execute_reply.started":"2022-01-24T05:07:58.126605Z","shell.execute_reply":"2022-01-24T05:07:58.144854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def obj_fn_q(x,gdp_quarterly,quarterly_sales):\n    alpha = np.array([1,x[0],x[1],x[2]]).reshape((1,4))\n    y=alpha*gdp_quarterly\n    y=y/np.sum(y,axis=1,keepdims=True)\n    return np.abs(y-quarterly_sales).mean()","metadata":{"execution":{"iopub.status.busy":"2022-01-24T05:07:58.146748Z","iopub.execute_input":"2022-01-24T05:07:58.147433Z","iopub.status.idle":"2022-01-24T05:07:58.153089Z","shell.execute_reply.started":"2022-01-24T05:07:58.147398Z","shell.execute_reply":"2022-01-24T05:07:58.152465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy.optimize import minimize\nresult = minimize(lambda x: obj_fn_q(x,gdp_quarterly[:-1,:],quarterly_sales), (1,1,1), bounds=[(0,None)]*3)\nx1,x2,x3 = result.x\nquarterly_sales_2019 = np.array([1,x1,x2,x3])*gdp_quarterly[-1,:]\nquarterly_sales_2019 = quarterly_sales_2019/np.sum(quarterly_sales_2019)\nquarterly_sales_2019 # This is just the breakdown, not the actual sales","metadata":{"execution":{"iopub.status.busy":"2022-01-24T05:07:58.154132Z","iopub.execute_input":"2022-01-24T05:07:58.154598Z","iopub.status.idle":"2022-01-24T05:07:58.197384Z","shell.execute_reply.started":"2022-01-24T05:07:58.154548Z","shell.execute_reply":"2022-01-24T05:07:58.196693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Estimating monthly sales breakdown","metadata":{}},{"cell_type":"markdown","source":"Now we have the quarterly sales breakdown, but we need the monthly sales breakdown. On the other hand, we have 4 years of monthly sales examples from 2015 to 2018. We don't want to make up monthly sales patterns, but we are comfortable with convex combinations of them. In other words, a candidate monthly sales breakdown for 2019 is\n$$\ns = w_1\\cdot s_{2015}+w_2\\cdot s_{2016}+w_3\\cdot s_{2017}+w_4\\cdot s_{2018}\n$$\nwhere \\\\(w_1\\\\), \\\\(w_2\\\\), \\\\(w_3\\\\), \\\\(w_4\\\\) are parameters such that \\\\(w_1+w_2+w_3+w_4=1\\\\). Note that this modeling is more general than the commonly used exponential smoothing that has only one parameter (\\\\(\\alpha\\\\)).\n\nWith the monthly breakdown \\\\(s\\\\) we can derive the corresponding quarterly breakdown \\\\(s_q\\\\) by combining every 3 months. We require \\\\(s_q\\\\) to match the quarterly sales breakdown from step 2 as much as possible, in the \\\\(\\ell^1\\\\) sense. Needless to say, the parameters \\\\(w_1\\\\), \\\\(w_2\\\\), \\\\(w_3\\\\), \\\\(w_4\\\\) are determined by constrained optimization.","metadata":{}},{"cell_type":"code","source":"from numba import njit\n\n@njit(fastmath=True)\ndef obj_fn_m(w,data,sales_est):\n    sales_m = (data@w).reshape((-1,))\n    sales_q = np.array([sales_m[:3].sum(),sales_m[3:6].sum(),sales_m[6:9].sum(),sales_m[9:].sum()])\n    return np.power(sales_q-sales_est,2).sum()","metadata":{"execution":{"iopub.status.busy":"2022-01-24T05:07:58.19851Z","iopub.execute_input":"2022-01-24T05:07:58.198853Z","iopub.status.idle":"2022-01-24T05:07:59.108374Z","shell.execute_reply.started":"2022-01-24T05:07:58.198824Z","shell.execute_reply":"2022-01-24T05:07:59.1074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data=monthly_sales_df['monthly_sales'].to_numpy().reshape((-1,12)).T\ndata=data/data.sum(axis=0,keepdims=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-24T05:07:59.109871Z","iopub.execute_input":"2022-01-24T05:07:59.110217Z","iopub.status.idle":"2022-01-24T05:07:59.116667Z","shell.execute_reply.started":"2022-01-24T05:07:59.110173Z","shell.execute_reply":"2022-01-24T05:07:59.115389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy.optimize import minimize, LinearConstraint\n\nresult = minimize(lambda w: obj_fn_m(w,data,quarterly_sales_2019),(0.25,0.25,0.25,0.25),bounds=[(0,1)]*4,\n                  constraints=LinearConstraint(np.ones((1,4)), 1, 1),method='trust-constr')\n\nassert result.success\nw=result.x\nw","metadata":{"execution":{"iopub.status.busy":"2022-01-24T05:07:59.118453Z","iopub.execute_input":"2022-01-24T05:07:59.119471Z","iopub.status.idle":"2022-01-24T05:08:00.838945Z","shell.execute_reply.started":"2022-01-24T05:07:59.119409Z","shell.execute_reply":"2022-01-24T05:08:00.838071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The result of the optimization says that we should combine mostly the monthly sales breakdown from 2018 and 2015.","metadata":{}},{"cell_type":"markdown","source":"### Combining step1 and step 3","metadata":{}},{"cell_type":"code","source":"monthly_sales_2019 = (data@w).reshape((-1,))*annual_sales_2019\nmonthly_sales_2019","metadata":{"execution":{"iopub.status.busy":"2022-01-24T05:08:00.840596Z","iopub.execute_input":"2022-01-24T05:08:00.841108Z","iopub.status.idle":"2022-01-24T05:08:00.849715Z","shell.execute_reply.started":"2022-01-24T05:08:00.841064Z","shell.execute_reply":"2022-01-24T05:08:00.84871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's visualize how well `monthly_sales_2019` matches `quarterly_sales_2019` when combining the months in each quarter.","metadata":{}},{"cell_type":"code","source":"from matplotlib import pyplot as plt\nfrom matplotlib.ticker import FormatStrFormatter\n\nfig = plt.figure(figsize=(8, 8))\nplt.bar(np.arange(4),quarterly_sales_2019,width=0.25)\nplt.bar(np.arange(4)+0.25,monthly_sales_2019.reshape((-1,3)).sum(axis=1)/annual_sales_2019,width=0.25)\nplt.xticks(np.arange(4)+0.25,['Q1','Q2','Q3','Q4'])\nplt.legend(['Target (from GDP)','Matching'])\nplt.title('2019 Quarterly Sales Distributions')\nplt.gca().set_yscale('log',base=0.1,subs=range(2,10))\nplt.tick_params(axis='y', which='minor')\nplt.gca().yaxis.set_minor_formatter(FormatStrFormatter(\"%.2f\"))\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-24T05:08:00.852076Z","iopub.execute_input":"2022-01-24T05:08:00.852443Z","iopub.status.idle":"2022-01-24T05:08:01.128744Z","shell.execute_reply.started":"2022-01-24T05:08:00.852387Z","shell.execute_reply":"2022-01-24T05:08:01.127802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now add feature `monthly_sales` to the test data!","metadata":{}},{"cell_type":"code","source":"df_test['monthly_sales'] = test_data.date.apply(lambda x: monthly_sales_2019[x.month-1])\ndf_test","metadata":{"execution":{"iopub.status.busy":"2022-01-24T05:08:01.129851Z","iopub.execute_input":"2022-01-24T05:08:01.130112Z","iopub.status.idle":"2022-01-24T05:08:01.188477Z","shell.execute_reply.started":"2022-01-24T05:08:01.130049Z","shell.execute_reply":"2022-01-24T05:08:01.187775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(30, 3))\nplt.plot(df.date,df.monthly_sales,'b-')\nplt.plot([df.date.iloc[-1]]+list(df_test.date),[df.monthly_sales.iloc[-1]]+list(df_test.monthly_sales),'g-')\nplt.title('monthly_sales (aggregated (2015-2018) and estimated (2019))')\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-24T05:08:01.189867Z","iopub.execute_input":"2022-01-24T05:08:01.190178Z","iopub.status.idle":"2022-01-24T05:08:01.440044Z","shell.execute_reply.started":"2022-01-24T05:08:01.190147Z","shell.execute_reply":"2022-01-24T05:08:01.439353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusions\n\nThis concludes the effort to detrend the time series. The estimated monthly sales for 2019 need to be tested. A regressor model needs to be trained on `num_sold_resid`, its predictions combined with `monthly_sales` to produce predictions for `num_sold`. That would be the ultimate test. It would be my next task. If you are trying a similar approach, feel free to leave a comment.","metadata":{}}]}