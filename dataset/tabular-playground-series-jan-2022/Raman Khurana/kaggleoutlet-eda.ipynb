{"nbformat_minor":4,"nbformat":4,"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"cells":[{"cell_type":"markdown","source":"# Table of Content \n1. [Introduction](#Intrduction)\n2. [Data Loading And Utilities](#Utils)\n3. [EDA](#EDA)\n4. [Features Extraction](#FE)\n5. [Prediction Methods](#PM)\n\n    5.1 [Mean](#mean)\n    \n    5.2 [Random Forest Regressor](#RandomForest)\n    \n    5.3 [LSTM](#LSTM)\n\n    5.4 [ARIMA](#ARIMA)\n    \n    5.5 [SARIMAX](#SARIMAX)\n\n\n\n\n\n\n\n\n","metadata":{}},{"cell_type":"markdown","source":"\n # Introduction <a class=\"anchor\" id=\"Introduction\"></a>\n * This notebook aims to perform a EDA of the provided dataset find out the features in the data provided to make a decision about the next Kaggle store. \n* If you find it useful, please upvote to keep me motivated for more additions to it. \n\n\n","metadata":{}},{"cell_type":"markdown","source":"# DATA access and other utilities<a class=\"anchor\" id=\"Utils\"></a>","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\ndf_trn = pd.read_csv(\"/kaggle/input/tabular-playground-series-jan-2022/train.csv\")\ndf_tst = pd.read_csv(\"/kaggle/input/tabular-playground-series-jan-2022/test.csv\")\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-25T09:07:10.909416Z","iopub.execute_input":"2022-01-25T09:07:10.910457Z","iopub.status.idle":"2022-01-25T09:07:10.963029Z","shell.execute_reply.started":"2022-01-25T09:07:10.910414Z","shell.execute_reply":"2022-01-25T09:07:10.961992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's try to understand the dataset provided for this competetion. The test dataset here has ~26K entries and 6 features. \n* date: give the date on which a given product is sold. \n* country: in which country this product was sold. \n* store: which store sold a given product. \n* product: what exactly is the product. \n* num_sold: how many pieces of a given product are sold on a given day, in a given country by a given store. ","metadata":{}},{"cell_type":"code","source":"df_trn","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-25T09:07:10.965343Z","iopub.execute_input":"2022-01-25T09:07:10.966239Z","iopub.status.idle":"2022-01-25T09:07:10.985858Z","shell.execute_reply.started":"2022-01-25T09:07:10.966187Z","shell.execute_reply":"2022-01-25T09:07:10.984965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"## The test dataset is as follows:\n* We have 6569 entries in the test dataset. \n* If you notice the num_sold column is missing, this is the number we need to predict. \n\n### Plan of action:\n* I am going to use a couple of basic models in the begining to mark as a reference and then use the advanced more to see how much I manage to improve in terms of prediction of the total sales. ","metadata":{}},{"cell_type":"code","source":"df_tst","metadata":{"execution":{"iopub.status.busy":"2022-01-25T09:07:10.987628Z","iopub.execute_input":"2022-01-25T09:07:10.988243Z","iopub.status.idle":"2022-01-25T09:07:11.007873Z","shell.execute_reply.started":"2022-01-25T09:07:10.988195Z","shell.execute_reply":"2022-01-25T09:07:11.007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA<a class=\"anchor\" id=\"EDA\"></a>\n\n","metadata":{}},{"cell_type":"markdown","source":"### Let's explore the training dataset, \n1. There are three unique product, ['Kaggle Mug', 'Kaggle Hat', 'Kaggle Sticker']\n2. As said, there are two ","metadata":{}},{"cell_type":"code","source":"print (df_trn[\"product\"].unique())\nprint (df_trn[\"country\"].unique())","metadata":{"execution":{"iopub.status.busy":"2022-01-25T09:07:11.009752Z","iopub.execute_input":"2022-01-25T09:07:11.010392Z","iopub.status.idle":"2022-01-25T09:07:11.02653Z","shell.execute_reply.started":"2022-01-25T09:07:11.010342Z","shell.execute_reply":"2022-01-25T09:07:11.02564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"df_trn[\"country\"].unique()","metadata":{"execution":{"iopub.status.busy":"2022-01-25T09:07:11.02953Z","iopub.execute_input":"2022-01-25T09:07:11.029871Z","iopub.status.idle":"2022-01-25T09:07:11.039799Z","shell.execute_reply.started":"2022-01-25T09:07:11.029819Z","shell.execute_reply":"2022-01-25T09:07:11.038982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(df_trn)","metadata":{"execution":{"iopub.status.busy":"2022-01-25T09:07:11.041116Z","iopub.execute_input":"2022-01-25T09:07:11.04176Z","iopub.status.idle":"2022-01-25T09:07:11.050982Z","shell.execute_reply.started":"2022-01-25T09:07:11.041717Z","shell.execute_reply":"2022-01-25T09:07:11.050069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"store_list=df_trn[\"store\"].unique()\nproduct_list=df_trn[\"product\"].unique()\ncountry_list=df_trn[\"country\"].unique()\n","metadata":{"execution":{"iopub.status.busy":"2022-01-25T09:07:11.052598Z","iopub.execute_input":"2022-01-25T09:07:11.053415Z","iopub.status.idle":"2022-01-25T09:07:11.070644Z","shell.execute_reply.started":"2022-01-25T09:07:11.05337Z","shell.execute_reply":"2022-01-25T09:07:11.069883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_trn.groupby(\"country\").num_sold.sum()\n","metadata":{"execution":{"iopub.status.busy":"2022-01-25T09:07:11.072375Z","iopub.execute_input":"2022-01-25T09:07:11.073002Z","iopub.status.idle":"2022-01-25T09:07:11.086508Z","shell.execute_reply.started":"2022-01-25T09:07:11.072955Z","shell.execute_reply":"2022-01-25T09:07:11.085897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* This function group the dataframe based on the columns passes as an argument and return a dictionary. The dictionary contain the value of column as \"key\" and the dataframe as the dictionary value. ","metadata":{}},{"cell_type":"code","source":"def getGroupedDataFrames(df, columnName):\n    grouped_df=list(df.groupby(columnName))\n    df_dictionary={}\n    for i in range(len(grouped_df)):\n        df_dictionary[grouped_df[i][0]] = grouped_df[i][1]\n    return df_dictionary","metadata":{"execution":{"iopub.status.busy":"2022-01-25T09:07:11.08795Z","iopub.execute_input":"2022-01-25T09:07:11.088858Z","iopub.status.idle":"2022-01-25T09:07:11.095646Z","shell.execute_reply.started":"2022-01-25T09:07:11.08881Z","shell.execute_reply":"2022-01-25T09:07:11.094685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Let's seprate the dataframe for each store and each country, so in total we now have 2x3=6 dataframes, saved in a single object. ","metadata":{}},{"cell_type":"code","source":"df_grp_store = getGroupedDataFrames(df_trn,\"store\")\ndf_country={}\nfor istore in store_list:\n    df_country[istore] = getGroupedDataFrames(df_grp_store[istore],\"country\")\n\n","metadata":{"execution":{"iopub.status.busy":"2022-01-25T09:07:11.099043Z","iopub.execute_input":"2022-01-25T09:07:11.099444Z","iopub.status.idle":"2022-01-25T09:07:11.121502Z","shell.execute_reply.started":"2022-01-25T09:07:11.0994Z","shell.execute_reply":"2022-01-25T09:07:11.120776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Let's check the overall sale from each store in each country for each of the product. \n* this is not very helpful to see the evolution but do tell which store sold more product and in which category. ","metadata":{}},{"cell_type":"code","source":"for istore in store_list:\n    for icountry in country_list:\n        print (istore, icountry)\n        print (df_country[istore][icountry].groupby(\"product\").num_sold.sum())\n        print (\"------\")","metadata":{"execution":{"iopub.status.busy":"2022-01-25T09:07:11.122974Z","iopub.execute_input":"2022-01-25T09:07:11.123392Z","iopub.status.idle":"2022-01-25T09:07:11.147423Z","shell.execute_reply.started":"2022-01-25T09:07:11.123352Z","shell.execute_reply":"2022-01-25T09:07:11.146548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* In order to see better the evolution of sales of these products in each store in each country, we need to see the time evolution plots/time series distribution of these these sales. \n* Let's check them for each of them using seaborn. ","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nsns.set(rc = {'figure.figsize':(20,8)})","metadata":{"execution":{"iopub.status.busy":"2022-01-25T09:07:11.148901Z","iopub.execute_input":"2022-01-25T09:07:11.14918Z","iopub.status.idle":"2022-01-25T09:07:11.154026Z","shell.execute_reply.started":"2022-01-25T09:07:11.149146Z","shell.execute_reply":"2022-01-25T09:07:11.153323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#sns.lineplot(x=\"date\", y=\"num_sold\",hue=\"product\",\n#             data=df_country[\"KaggleMart\"][\"Sweden\"],\n#            ci=None)","metadata":{"execution":{"iopub.status.busy":"2022-01-25T09:07:11.155337Z","iopub.execute_input":"2022-01-25T09:07:11.155575Z","iopub.status.idle":"2022-01-25T09:07:11.169984Z","shell.execute_reply.started":"2022-01-25T09:07:11.155546Z","shell.execute_reply":"2022-01-25T09:07:11.169141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#sns.lineplot(x=\"date\", y=\"num_sold\",hue=\"product\",\n#             data=df_country[\"KaggleMart\"][\"Finland\"],ci=None)","metadata":{"execution":{"iopub.status.busy":"2022-01-25T09:07:11.171138Z","iopub.execute_input":"2022-01-25T09:07:11.17136Z","iopub.status.idle":"2022-01-25T09:07:11.181941Z","shell.execute_reply.started":"2022-01-25T09:07:11.171331Z","shell.execute_reply":"2022-01-25T09:07:11.181192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#sns.lineplot(x=\"date\", y=\"num_sold\",hue=\"product\",\n#             data=df_country[\"KaggleMart\"][\"Norway\"], ci=None)","metadata":{"execution":{"iopub.status.busy":"2022-01-25T09:07:11.183395Z","iopub.execute_input":"2022-01-25T09:07:11.183616Z","iopub.status.idle":"2022-01-25T09:07:11.192926Z","shell.execute_reply.started":"2022-01-25T09:07:11.183588Z","shell.execute_reply":"2022-01-25T09:07:11.192067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#sns.lineplot(x=\"date\", y=\"num_sold\",hue=\"product\",\n#             data=df_country[\"KaggleRama\"][\"Sweden\"],ci=None)","metadata":{"execution":{"iopub.status.busy":"2022-01-25T09:07:11.194466Z","iopub.execute_input":"2022-01-25T09:07:11.194713Z","iopub.status.idle":"2022-01-25T09:07:11.204105Z","shell.execute_reply.started":"2022-01-25T09:07:11.194682Z","shell.execute_reply":"2022-01-25T09:07:11.202886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#sns.lineplot(x=\"date\", y=\"num_sold\",hue=\"product\",\n#             data=df_country[\"KaggleRama\"][\"Finland\"],ci=None)","metadata":{"execution":{"iopub.status.busy":"2022-01-25T09:07:11.205526Z","iopub.execute_input":"2022-01-25T09:07:11.205878Z","iopub.status.idle":"2022-01-25T09:07:11.215244Z","shell.execute_reply.started":"2022-01-25T09:07:11.205846Z","shell.execute_reply":"2022-01-25T09:07:11.214298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#sns.lineplot(x=\"date\", y=\"num_sold\",hue=\"product\",\n#             data=df_country[\"KaggleRama\"][\"Norway\"],ci=None)","metadata":{"execution":{"iopub.status.busy":"2022-01-25T09:07:11.216483Z","iopub.execute_input":"2022-01-25T09:07:11.216703Z","iopub.status.idle":"2022-01-25T09:07:11.225794Z","shell.execute_reply.started":"2022-01-25T09:07:11.216675Z","shell.execute_reply":"2022-01-25T09:07:11.224937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#sns.lineplot(x=\"date\", y=\"num_sold\",hue=\"product\",\n#             data=df_grp_store[\"KaggleRama\"],ci=None)\n","metadata":{"execution":{"iopub.status.busy":"2022-01-25T09:07:11.227093Z","iopub.execute_input":"2022-01-25T09:07:11.227323Z","iopub.status.idle":"2022-01-25T09:07:11.236307Z","shell.execute_reply.started":"2022-01-25T09:07:11.227296Z","shell.execute_reply":"2022-01-25T09:07:11.235309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### As a next step let's check the distribution of these sales per year and their comparision per product, per country, per store. \n* This will give insight about the sesonality in the data. \n* Basically I plot the data in the seasonal form. As can be seen from previous timing plots that there are overall trend is similar in each year. ","metadata":{}},{"cell_type":"code","source":"df=df_country[\"KaggleRama\"][\"Sweden\"]\ndf[\"year\"] = pd.to_datetime(df.date).dt.year\ndf[\"day\"] = pd.to_datetime(df.date).dt.day_of_year\ndf[\"day\"]\n\n#df[\"year\"]\n","metadata":{"execution":{"iopub.status.busy":"2022-01-25T09:07:11.237394Z","iopub.execute_input":"2022-01-25T09:07:11.237638Z","iopub.status.idle":"2022-01-25T09:07:11.262808Z","shell.execute_reply.started":"2022-01-25T09:07:11.237607Z","shell.execute_reply":"2022-01-25T09:07:11.261848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Features in the seasonal plot: \nAs can be seen in timing data plot, for 4 years, that there is some seasonality present in the data. We can clearly see those features from seasonal plot. Note that the distribution here is only for KaggleRama::Sweden\n1. There are TWO major periods (and one minor/long range) when sales of the product have increased, or should say exploded. \n - First is around new year, \n - Second, somewhere in the April, that is the time close to Easter, celebrated in the EU. \n - Third, A wide increase in sales from May-June, this is likely due the begining of the summer and summer vacation. \n2. In addition to the festival sales peak there is one more feature to be seen in sales of each product.\n - There features are visible by small speaks occuring after regular interval, from first look this seems to be weekly. The number of peaks are roughly 50-55, so they seems to correspond to weekend. And why not, people do enjoy shopping with family over the weekend. \n \n \n When we want to predict the sales of these product, we need to keep in mind following information: \n 1. End of year / New year sales explosion \n 2. Easter blast \n 3. Summer leisure \n 4. Weekend fun with family and friends \n 5. Three products have completely different sales in each country, and rise in sales is also different. This implies we must treat them as independent when making prediction. And same is true for each country. \n  - One all the prediction for each product and country is made, we must sum them to predict the total number, instead of predicting everything in one i.e. total sales. \n  \n ","metadata":{}},{"cell_type":"code","source":"sns.lineplot(data=df, \n             x='day', \n             y='num_sold', \n             hue='year', \n             style=\"product\",\n             legend='full',ci=None)\n","metadata":{"execution":{"iopub.status.busy":"2022-01-25T09:07:11.264439Z","iopub.execute_input":"2022-01-25T09:07:11.264689Z","iopub.status.idle":"2022-01-25T09:07:12.046323Z","shell.execute_reply.started":"2022-01-25T09:07:11.264658Z","shell.execute_reply":"2022-01-25T09:07:12.045342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Rolling Average \nLet's check the rolling average using dataframe.rolling function. I am trying rolling average with various window, say 5,7 and 8 days to see the feature in the time series data. Let's start with the Kaggle hat data.\n\n* The 5 day and 8 day rolling average does not show any smoothening properties, \n* However 7 day rolling average does indicate that the distribution is now much more smooth and the weekly fluctuations are now gone. \n* Similar situation for remaining product and countries and stores. \n\nLet's see how can we use all these information. ","metadata":{}},{"cell_type":"code","source":"## get the kaggle Hat dataframe instead of all 3 product. \ndf_kaggle_hat = df[(df[\"product\"]==\"Kaggle Hat\")]\ndf_kaggle_hat","metadata":{"execution":{"iopub.status.busy":"2022-01-25T09:07:12.04776Z","iopub.execute_input":"2022-01-25T09:07:12.048045Z","iopub.status.idle":"2022-01-25T09:07:12.071509Z","shell.execute_reply.started":"2022-01-25T09:07:12.048012Z","shell.execute_reply":"2022-01-25T09:07:12.070544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"df_kaggle_hat[\"avg5\"] = df_kaggle_hat.num_sold.rolling(5).mean()\ndf_kaggle_hat[\"avg7\"] = df_kaggle_hat.num_sold.rolling(7).mean()\ndf_kaggle_hat[\"avg8\"] = df_kaggle_hat.num_sold.rolling(8).mean()\ndf_kaggle_hat","metadata":{"execution":{"iopub.status.busy":"2022-01-25T09:07:12.075764Z","iopub.execute_input":"2022-01-25T09:07:12.076334Z","iopub.status.idle":"2022-01-25T09:07:12.10765Z","shell.execute_reply.started":"2022-01-25T09:07:12.076298Z","shell.execute_reply":"2022-01-25T09:07:12.106798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.lineplot(data=df_kaggle_hat, \n             x='day', \n             y='avg7', \n             hue='year', \n             legend='full',ci=None)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-01-25T09:07:12.108902Z","iopub.execute_input":"2022-01-25T09:07:12.10917Z","iopub.status.idle":"2022-01-25T09:07:12.505294Z","shell.execute_reply.started":"2022-01-25T09:07:12.109139Z","shell.execute_reply":"2022-01-25T09:07:12.504338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.lineplot(data=df_kaggle_hat, \n             x='day', \n             y='avg5', \n             hue='year', \n             legend='full',ci=None)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-01-25T09:07:12.506615Z","iopub.execute_input":"2022-01-25T09:07:12.506866Z","iopub.status.idle":"2022-01-25T09:07:12.907385Z","shell.execute_reply.started":"2022-01-25T09:07:12.506834Z","shell.execute_reply":"2022-01-25T09:07:12.906662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.lineplot(data=df_kaggle_hat, \n             x='day', \n             y='avg8', \n             hue='year', \n             legend='full',ci=None)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-01-25T09:07:12.908687Z","iopub.execute_input":"2022-01-25T09:07:12.909403Z","iopub.status.idle":"2022-01-25T09:07:13.298963Z","shell.execute_reply.started":"2022-01-25T09:07:12.909365Z","shell.execute_reply":"2022-01-25T09:07:13.29817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I am going to try some of the trivial methods to forcast the sales for next year and compare them. \n1. The first method being tried is average method: This assumes that the future values of the sales will equal to the average of the collected time series data. Quite straightforward!! ","metadata":{}},{"cell_type":"markdown","source":"# Features Extraction<a class=\"anchor\" id=\"FE\"></a>\n\n","metadata":{}},{"cell_type":"markdown","source":"## Categorical data \nThere are a few features which are given in categorical form, e.g. country, product, store. It will make the like easy if I use some encoding for these variables and make them quantitative variables. ","metadata":{}},{"cell_type":"code","source":"country_map={\"Finland\":0,\n             \"Norway\":1,\n             \"Sweden\":2}\n\nproduct_map={\"Kaggle Mug\":0,\n             \"Kaggle Hat\":1,\n             \"Kaggle Sticker\":2}\nstore_map={\"KaggleRama\":0,\n           \"KaggleMart\":1}\n\ndef CategorialToQuantitative(df):\n    df.replace(country_map,inplace=True)\n    df.replace(store_map,inplace=True)\n    df.replace(product_map,inplace=True)\n    return df\n","metadata":{"execution":{"iopub.status.busy":"2022-01-25T09:07:13.300221Z","iopub.execute_input":"2022-01-25T09:07:13.300424Z","iopub.status.idle":"2022-01-25T09:07:13.306685Z","shell.execute_reply.started":"2022-01-25T09:07:13.300397Z","shell.execute_reply":"2022-01-25T09:07:13.305594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_trn = CategorialToQuantitative(df_trn)\ndf_tst = CategorialToQuantitative(df_tst)","metadata":{"execution":{"iopub.status.busy":"2022-01-25T09:07:13.307757Z","iopub.execute_input":"2022-01-25T09:07:13.307986Z","iopub.status.idle":"2022-01-25T09:07:13.436369Z","shell.execute_reply.started":"2022-01-25T09:07:13.307958Z","shell.execute_reply":"2022-01-25T09:07:13.435744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Extract time information from provided date ","metadata":{}},{"cell_type":"code","source":"def gettimeFeatures(df):\n    df[\"day_of_year\"]=pd.to_datetime(df.date).dt.day_of_year\n    df[\"day_of_month\"] = pd.to_datetime(df.date).dt.day\n    df[\"week\"]=pd.to_datetime(df.date).dt.isocalendar().week\n    df[\"quarter\"]=pd.to_datetime(df.date).dt.quarter\n    df[\"month\"]=pd.to_datetime(df.date).dt.month\n    df[\"year\"]=pd.to_datetime(df.date).dt.year\n    df[\"weekd\"]=pd.to_datetime(df.date).dt.weekday\n    df[\"weekend\"]=(pd.to_datetime(df.date).dt.weekday>4).astype(int) ## weekday range from 0 to 6. \n    \n    ## Easter \n    #2015: 5 April: 95\n    #2016: 27 March: 88\n    #2017: 16 April: 106\n    #2018: 1 April: 91\n    #2019: 21 April: 111 \n    ## I still look for an easier way to do this, instead of hardcoding it \n    df.loc[df.month>-1,\"easter\"]=0 ## setting default \n    df.loc[ ( ( (df.year==2018) & ( abs(df.day_of_year-91)<10) ) | \n              ( (df.year==2017) & ( abs(df.day_of_year-106)<10)) | \n              ( (df.year==2016) & ( abs(df.day_of_year-88)<10) ) |\n              ( (df.year==2018) & ( abs(df.day_of_year-95)<10) ) |\n              ( (df.year==2019) & ( abs(df.day_of_year-111)<10) )\n            ),\"easter\"]=1\n\n    ## Year End \n    df.loc[df.month>0,\"year_end\"]=0 ## setting default\n    df.loc[ ( (  (df.month==12) & (df.day_of_month>22) ) |\n                 (  (df.month==1) &  (df.day_of_month <8) ) ),\"year_end\"]=1\n\n    ## Summer \n    df.loc[df.day_of_year>-1,\"summer\"]=0 ## ## setting default\n    df.loc[ ( (df.day_of_year>125) & (df.day_of_year<190) ),\"summer\"] =1\n\n\n\n    ## we don't need date anymore \n    df.drop([\"date\"],axis=1,inplace=True)\n    return df ","metadata":{"execution":{"iopub.status.busy":"2022-01-25T09:07:13.437417Z","iopub.execute_input":"2022-01-25T09:07:13.438028Z","iopub.status.idle":"2022-01-25T09:07:13.453694Z","shell.execute_reply.started":"2022-01-25T09:07:13.437995Z","shell.execute_reply":"2022-01-25T09:07:13.452846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_trn = gettimeFeatures(df_trn)\ndf_tst = gettimeFeatures(df_tst)\ndf_tst","metadata":{"execution":{"iopub.status.busy":"2022-01-25T09:07:13.454999Z","iopub.execute_input":"2022-01-25T09:07:13.455219Z","iopub.status.idle":"2022-01-25T09:07:13.63288Z","shell.execute_reply.started":"2022-01-25T09:07:13.455193Z","shell.execute_reply":"2022-01-25T09:07:13.632045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Metric for submission ","metadata":{}},{"cell_type":"code","source":"def SMAPE(y_true, y_pred):\n    diff = np.abs(y_true - y_pred) / (y_true + np.abs(y_pred)) * 200\n    return diff.mean()\n","metadata":{"execution":{"iopub.status.busy":"2022-01-25T09:07:13.634248Z","iopub.execute_input":"2022-01-25T09:07:13.6349Z","iopub.status.idle":"2022-01-25T09:07:13.639571Z","shell.execute_reply.started":"2022-01-25T09:07:13.634854Z","shell.execute_reply":"2022-01-25T09:07:13.638633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prediction Algorithms<a class=\"anchor\" id=\"PM\"></a>\n## Mean <a class=\"anchor\" id=\"mean\"></a>\n\n","metadata":{}},{"cell_type":"code","source":"df_grp_store = getGroupedDataFrames(df_trn,\"store\")\ndf_country={}\nfor istore in df_trn.store.unique():\n    df_country[istore] = getGroupedDataFrames(df_grp_store[istore],\"country\")\n\ndf_mean = pd.DataFrame()\ndf_mean_tmp = pd.DataFrame()\nfor istore in range (0,2):\n    for icountry in range (0,3):\n        df_mean_tmp = pd.DataFrame((df_country[istore][icountry].groupby(\"product\").mean()[\"num_sold\"]))\n        df_mean_tmp[\"country\"]=icountry\n        df_mean_tmp[\"store\"]=istore\n        #print (df_mean_tmp)\n        df_mean = pd.concat([df_mean,df_mean_tmp])\n\ndf_mean\ndf_mean.reset_index().rename({'product':'product'}, axis = 'columns')\ndf_out = df_tst.merge(df_mean,on=['product','country','store'])\ndf_out.drop([\"country\",\"store\",\"product\", \"day_of_year\", \"week\", \"quarter\", \"month\", \"year\", \"weekd\", \"weekend\"],axis=1,inplace=True)\n#df_out.to_csv(\"submission.csv\",index=False)\n#df_out.shape\n\n","metadata":{"execution":{"iopub.status.busy":"2022-01-25T09:15:31.379563Z","iopub.execute_input":"2022-01-25T09:15:31.379871Z","iopub.status.idle":"2022-01-25T09:15:31.432697Z","shell.execute_reply.started":"2022-01-25T09:15:31.379837Z","shell.execute_reply":"2022-01-25T09:15:31.431608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Result with mean: \n\n* When sales set to the mean of past data,  per store, per product and country the scrore omn test data is: 13.04\n* I will skip the naive, snaive and other trivial methods and try to use simple methods to compare the results. \nLet's try to use the engeniered variables in a random forest Regressor and XGBoose to see the improvement,  see the response. ","metadata":{}},{"cell_type":"markdown","source":"## Random Forest <a class=\"anchor\" id=\"RandomForest\"></a>\n\n","metadata":{}},{"cell_type":"code","source":"df_tst.columns","metadata":{"execution":{"iopub.status.busy":"2022-01-25T09:07:13.729781Z","iopub.execute_input":"2022-01-25T09:07:13.730011Z","iopub.status.idle":"2022-01-25T09:07:13.742839Z","shell.execute_reply.started":"2022-01-25T09:07:13.729986Z","shell.execute_reply":"2022-01-25T09:07:13.741869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nclf = RandomForestRegressor()\nvars=['country', 'store', 'product', 'day_of_year', 'week',\n       'quarter', 'month', 'year', 'weekd', 'weekend', 'day_of_month',\n       'easter', 'year_end', 'summer']\nX = df_trn[vars]\nY = df_trn['num_sold']\nclf.fit(X,Y)\nX_tst = df_tst[vars]\nY_tst=clf.predict(X_tst)\n","metadata":{"execution":{"iopub.status.busy":"2022-01-25T09:07:13.743904Z","iopub.execute_input":"2022-01-25T09:07:13.744276Z","iopub.status.idle":"2022-01-25T09:07:23.955455Z","shell.execute_reply.started":"2022-01-25T09:07:13.744242Z","shell.execute_reply":"2022-01-25T09:07:23.954373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_tst[\"num_sold\"] = Y_tst\ndf_out = df_tst.drop([\"country\",\"store\",\"product\", \"day_of_year\", \"week\", \"quarter\", \"month\", \"year\", \"weekd\", \"weekend\",'day_of_month', 'easter', 'year_end', 'summer'],axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-01-25T09:07:23.957139Z","iopub.execute_input":"2022-01-25T09:07:23.957382Z","iopub.status.idle":"2022-01-25T09:07:23.965601Z","shell.execute_reply.started":"2022-01-25T09:07:23.957351Z","shell.execute_reply":"2022-01-25T09:07:23.964395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_out.to_csv(\"submission.csv\",index=False)\ndf_out.shape\n\n","metadata":{"execution":{"iopub.status.busy":"2022-01-25T09:07:23.96719Z","iopub.execute_input":"2022-01-25T09:07:23.967454Z","iopub.status.idle":"2022-01-25T09:07:24.007664Z","shell.execute_reply.started":"2022-01-25T09:07:23.967424Z","shell.execute_reply":"2022-01-25T09:07:24.006969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Result of Random Forest Regressor \n* The first iteration of result with RFR give a huge improvement w.r.t mean, score is 7.249 ","metadata":{}},{"cell_type":"markdown","source":"## Splitting the data \n* Let's seprate train data into two parts, first 3 years as training and remaining one year as test, so that  I can get the score without submitting the prediction. ","metadata":{}},{"cell_type":"code","source":"df_train = df_trn[df_trn.year<2018]\ndf_test  = df_trn[df_trn.year==2018]\n\nX_Train, Y_Train = df_train[vars], df_train[\"num_sold\"]\nX_Test, Y_True          = df_test[vars], df_test[\"num_sold\"]\n\nclf.fit(X_Train,Y_Train)\nY_Test=clf.predict(X_Test)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-01-25T09:07:24.008932Z","iopub.execute_input":"2022-01-25T09:07:24.009309Z","iopub.status.idle":"2022-01-25T09:07:31.480056Z","shell.execute_reply.started":"2022-01-25T09:07:24.009277Z","shell.execute_reply":"2022-01-25T09:07:31.479117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SMAPE(Y_True,Y_Test)","metadata":{"execution":{"iopub.status.busy":"2022-01-25T09:07:31.481514Z","iopub.execute_input":"2022-01-25T09:07:31.481937Z","iopub.status.idle":"2022-01-25T09:07:31.490474Z","shell.execute_reply.started":"2022-01-25T09:07:31.481868Z","shell.execute_reply":"2022-01-25T09:07:31.489592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LSTM to predict th## LSTM <a class=\"anchor\" id=\"LSTM\"></a>\n\ne sales ","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import keras\nimport matplotlib.pyplot as plt\nfrom keras.layers import Dense,Dropout,LSTM\nfrom keras.models import Sequential\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import StandardScaler \n\n","metadata":{"execution":{"iopub.status.busy":"2022-01-25T09:07:31.49172Z","iopub.execute_input":"2022-01-25T09:07:31.492039Z","iopub.status.idle":"2022-01-25T09:07:37.454128Z","shell.execute_reply.started":"2022-01-25T09:07:31.492007Z","shell.execute_reply":"2022-01-25T09:07:37.453089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_Train = X_Train.astype(float)\nX_Test = X_Test.astype(float)\nY_Train = Y_Train.astype(float)\nY_True = Y_True.astype(float)\n\n#scaler  = StandardScaler()\n#scaler = scaler.fit(X_Train)\n#X_Train_scaled = scaler.transform(X_Train)\n#X_Train_scaled.shape, type(X_Train_scaled)\n\n#scaler1  = StandardScaler()\n#scaler1 = scaler1.fit(X_Test)\n#X_Test_scaled = scaler1.transform(X_Test)\n#X_Test_scaled.shape\n\n","metadata":{"execution":{"iopub.status.busy":"2022-01-25T09:07:37.45539Z","iopub.execute_input":"2022-01-25T09:07:37.455604Z","iopub.status.idle":"2022-01-25T09:07:37.464858Z","shell.execute_reply.started":"2022-01-25T09:07:37.455575Z","shell.execute_reply":"2022-01-25T09:07:37.46414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_Train_scaled = np.array(X_Train)\nX_Test_scaled = np.array(X_Test)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-01-25T09:07:37.465973Z","iopub.execute_input":"2022-01-25T09:07:37.466172Z","iopub.status.idle":"2022-01-25T09:07:37.484512Z","shell.execute_reply.started":"2022-01-25T09:07:37.466147Z","shell.execute_reply":"2022-01-25T09:07:37.483523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nX_Train_LSTM = []\nY_Train_LSTM = [] \nX_Test_LSTM = []\nY_True_LSTM = [] \n\n\nn_future = 1 \nn_past = 365\nfor i in range (n_past, len(X_Train_scaled)-n_future+1):\n    X_Train_LSTM.append(X_Train_scaled[i-n_past:i, 0:X_Train_scaled.shape[1] ])\n    Y_Train_LSTM.append(X_Train_scaled[i+n_future-1:i+n_future,0])\n\nfor i in range (n_past, len(X_Test_scaled)-n_future+1):\n    X_Test_LSTM.append(X_Test_scaled[i-n_past:i, 0:X_Test_scaled.shape[1] ])\n    Y_True_LSTM.append(X_Test_scaled[i+n_future-1:i+n_future,0])\n\n\nX_Train_LSTM, Y_Train_LSTM = np.array(X_Train_LSTM), np.array(Y_Train_LSTM)\nX_Test_LSTM, Y_True_LSTM = np.array(X_Test_LSTM), np.array(Y_True_LSTM)\n'''","metadata":{"execution":{"iopub.status.busy":"2022-01-25T09:07:37.486061Z","iopub.execute_input":"2022-01-25T09:07:37.486288Z","iopub.status.idle":"2022-01-25T09:07:37.498874Z","shell.execute_reply.started":"2022-01-25T09:07:37.48626Z","shell.execute_reply":"2022-01-25T09:07:37.497979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_Train_LSTM = X_Train_scaled.reshape(X_Train_scaled.shape[0],X_Train_scaled.shape[1],1)\nY_Train_LSTM = np.array(Y_Train)\nX_Test_LSTM = X_Test_scaled.reshape(X_Test_scaled.shape[0],X_Test_scaled.shape[1],1)\n#Y_True_LSTM = \n#X_Test_LSTM.shape\nY_Train_LSTM.shape, X_Train_LSTM.shape, Y_Train_LSTM","metadata":{"execution":{"iopub.status.busy":"2022-01-25T09:07:37.501551Z","iopub.execute_input":"2022-01-25T09:07:37.501904Z","iopub.status.idle":"2022-01-25T09:07:37.514689Z","shell.execute_reply.started":"2022-01-25T09:07:37.50186Z","shell.execute_reply":"2022-01-25T09:07:37.513813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Sequential()\nmodel.add(LSTM(units = 50 , return_sequences=True , input_shape = (X_Train_LSTM.shape[1], 1 )))\n\nmodel.add(Dropout(0.2))\n# Second LSTM layer\nmodel.add(LSTM(units=50, return_sequences=True))\nmodel.add(Dropout(0.2))\n# Third LSTM layer\nmodel.add(LSTM(units=50, return_sequences=True))\nmodel.add(Dropout(0.2))\n# Fourth LSTM layer\nmodel.add(LSTM(units=50))\nmodel.add(Dropout(0.2))\n# The output layer\nmodel.add(Dense(units=1))\nmodel.compile(optimizer='adam',loss='mean_squared_error')\n","metadata":{"execution":{"iopub.status.busy":"2022-01-25T09:07:37.516593Z","iopub.execute_input":"2022-01-25T09:07:37.517219Z","iopub.status.idle":"2022-01-25T09:07:38.916097Z","shell.execute_reply.started":"2022-01-25T09:07:37.517173Z","shell.execute_reply":"2022-01-25T09:07:38.91523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-01-25T09:07:38.918115Z","iopub.execute_input":"2022-01-25T09:07:38.918345Z","iopub.status.idle":"2022-01-25T09:07:38.929315Z","shell.execute_reply.started":"2022-01-25T09:07:38.918316Z","shell.execute_reply":"2022-01-25T09:07:38.92776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"history = model.fit(X_Train_LSTM,Y_Train_LSTM,batch_size=32 , epochs=5)","metadata":{"execution":{"iopub.status.busy":"2022-01-25T09:07:38.930684Z","iopub.execute_input":"2022-01-25T09:07:38.930934Z","iopub.status.idle":"2022-01-25T09:09:50.079608Z","shell.execute_reply.started":"2022-01-25T09:07:38.930875Z","shell.execute_reply":"2022-01-25T09:09:50.078748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"forecast = model.predict(X_Test_LSTM)\nforecast = pd.Series(forecast.reshape(forecast.shape[0]))","metadata":{"execution":{"iopub.status.busy":"2022-01-25T09:09:50.080983Z","iopub.execute_input":"2022-01-25T09:09:50.081258Z","iopub.status.idle":"2022-01-25T09:09:54.316613Z","shell.execute_reply.started":"2022-01-25T09:09:50.081228Z","shell.execute_reply":"2022-01-25T09:09:54.315873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_true = np.array(Y_True)\ny_true.reshape(y_true.shape[0])\nSMAPE(y_true,forecast)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-01-25T09:09:54.317866Z","iopub.execute_input":"2022-01-25T09:09:54.318118Z","iopub.status.idle":"2022-01-25T09:09:54.330039Z","shell.execute_reply.started":"2022-01-25T09:09:54.318089Z","shell.execute_reply":"2022-01-25T09:09:54.329202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Results of LSTM \n* As can be seen the primitive LSTM method does not work well. The score is 53, However the Random Forest gives much better results. ","metadata":{}},{"cell_type":"markdown","source":"1. mean \n2. random forest regressor \n3. LSTM \n\nFrom initial investigations RFR seems to be the best with the present set of features, \n\nIn the next version I plan to test, ARIMA, SARIMA, and SARIMAX and a hybrid more with Prophet. ","metadata":{}},{"cell_type":"markdown","source":"## ARIMA <a class=\"anchor\" id=\"ARIMA\"></a>\n* ARIMA stands for Auto Regression Integrated Moving Average. It was fun reading this book to understand about the TSA and prediction, https://otexts.com/fpp3/arima.html \n* ","metadata":{}},{"cell_type":"code","source":"from pandas.plotting import autocorrelation_plot, lag_plot\ndf_sel = df_train[ ( (df_train.country==0) & (df_train.store==0) & (df_train[\"product\"]==0) )]\nseries  = df_sel.num_sold\nlag_plot(series, lag=1)\n","metadata":{"execution":{"iopub.status.busy":"2022-01-25T09:52:39.210068Z","iopub.execute_input":"2022-01-25T09:52:39.210648Z","iopub.status.idle":"2022-01-25T09:52:39.537228Z","shell.execute_reply.started":"2022-01-25T09:52:39.210612Z","shell.execute_reply":"2022-01-25T09:52:39.536372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"autocorrelation_plot(series)","metadata":{"execution":{"iopub.status.busy":"2022-01-25T09:52:40.092977Z","iopub.execute_input":"2022-01-25T09:52:40.09402Z","iopub.status.idle":"2022-01-25T09:52:40.434246Z","shell.execute_reply.started":"2022-01-25T09:52:40.093979Z","shell.execute_reply":"2022-01-25T09:52:40.433287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nimport matplotlib.pyplot as plt\nplt.rcParams.update({'figure.figsize':(9,7), 'figure.dpi':120})\nfig, axes = plt.subplots(3, 2, sharex=True)\n\naxes[0, 0].plot(series); axes[0, 0].set_title('Original Series')\nplot_acf(series, ax=axes[0, 1])\n\n# 1st Differencing\naxes[1, 0].plot(series.diff()); axes[1, 0].set_title('1st Order Differencing')\nplot_acf(series.diff().dropna(), ax=axes[1, 1])\n\n# 2nd Differencing\naxes[2, 0].plot(series.diff().diff()); axes[2, 0].set_title('2nd Order Differencing')\nplot_acf(series.diff().diff().dropna(), ax=axes[2, 1])\n'''\n","metadata":{"execution":{"iopub.status.busy":"2022-01-25T09:52:42.428548Z","iopub.execute_input":"2022-01-25T09:52:42.428812Z","iopub.status.idle":"2022-01-25T09:52:42.436777Z","shell.execute_reply.started":"2022-01-25T09:52:42.428785Z","shell.execute_reply":"2022-01-25T09:52:42.435805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from statsmodels.tsa.arima.model import ARIMA\nfrom matplotlib import pyplot\nmodel = ARIMA(series, order=(5,1,1))\nmodel_fit = model.fit()\nresiduals = pd.DataFrame(model_fit.resid)\nresiduals.plot()\npyplot.show()\n# density plot of residuals\nresiduals.plot(kind='kde')\npyplot.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-01-25T09:52:53.676831Z","iopub.execute_input":"2022-01-25T09:52:53.677155Z","iopub.status.idle":"2022-01-25T09:52:55.060988Z","shell.execute_reply.started":"2022-01-25T09:52:53.677125Z","shell.execute_reply":"2022-01-25T09:52:55.059883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* The residual error plot shows that there are features in the data which are not picked well by the model with defined parameter set (5,1,0). \n* The density plot of the erros show that the errors are almost normal / Gaussian distributed, but the mean is slightly shifted to lower values, i.e. slightly smaller than 0, with an width of about 200. Recall this is not a proper gaussian, it has a slow fall on the right. ","metadata":{}},{"cell_type":"markdown","source":"### For forecasting ","metadata":{}},{"cell_type":"code","source":"series_test = df_test[ ( (df_test.country==0) & (df_test.store==0) & (df_test[\"product\"]==0) )].num_sold\nseries_out = model_fit.forecast(365, alpha=0.05)  # 95% conf","metadata":{"execution":{"iopub.status.busy":"2022-01-25T10:00:01.708431Z","iopub.execute_input":"2022-01-25T10:00:01.709184Z","iopub.status.idle":"2022-01-25T10:00:01.734115Z","shell.execute_reply.started":"2022-01-25T10:00:01.709144Z","shell.execute_reply":"2022-01-25T10:00:01.733169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"series_out = series_out.reset_index().drop(\"index\",axis=1)\nseries_test = series_test.reset_index().drop(\"index\",axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-01-25T10:01:48.48821Z","iopub.execute_input":"2022-01-25T10:01:48.489195Z","iopub.status.idle":"2022-01-25T10:01:48.497598Z","shell.execute_reply.started":"2022-01-25T10:01:48.48915Z","shell.execute_reply":"2022-01-25T10:01:48.496979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SMAPE(series_test.num_sold,series_out.predicted_mean","metadata":{"execution":{"iopub.status.busy":"2022-01-25T10:04:17.038288Z","iopub.execute_input":"2022-01-25T10:04:17.038717Z","iopub.status.idle":"2022-01-25T10:04:17.047196Z","shell.execute_reply.started":"2022-01-25T10:04:17.038682Z","shell.execute_reply":"2022-01-25T10:04:17.046569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* It can be seen clearly that ARIMA is not able to pick the seasonality of data, therefore, It will make sense to use SARIMA instead of ARIMA. In addition to just the series, I would like to use other information available. So the natural choice is SARIMAX. Let's take a look. \n","metadata":{}},{"cell_type":"markdown","source":"## SARIMAX <a class=\"anchor\" id=\"SARIMAX\"></a>\nComing soon\n\n\n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}