{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# The January 2022 competition with Fastai v2","metadata":{}},{"cell_type":"markdown","source":"This notebook is a quick demonstration, who to use the Fastai v2 library for a Kaggle tabular competition. Fastai v2 is based on pytorch and allows you, to build a decent machine learning application. For more information please visit the Fastai documentation: https://docs.fast.ai/. I will link to \"Chapter 9, Tabular Modelling Deep Dive\" and the notebook \"09_tabular.ipynb\"","metadata":{}},{"cell_type":"code","source":"from fastai.tabular.all import * \nfrom fastai.test_utils import show_install\n\nshow_install()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-26T17:00:03.14957Z","iopub.execute_input":"2022-01-26T17:00:03.149884Z","iopub.status.idle":"2022-01-26T17:00:04.059392Z","shell.execute_reply.started":"2022-01-26T17:00:03.14983Z","shell.execute_reply":"2022-01-26T17:00:04.058533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.random.seed(41)\ntorch.manual_seed(41)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2022-01-26T17:00:04.066208Z","iopub.execute_input":"2022-01-26T17:00:04.066459Z","iopub.status.idle":"2022-01-26T17:00:04.072935Z","shell.execute_reply.started":"2022-01-26T17:00:04.066421Z","shell.execute_reply":"2022-01-26T17:00:04.072113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The data set is located in the following directory","metadata":{}},{"cell_type":"code","source":"path = Path('../input/tabular-playground-series-jan-2022')\nnordic_path = Path('../input/festivities-in-finland-norway-sweden-tsp-0122')\nPath.BASE_PATH = path\npath.ls(), nordic_path.ls()","metadata":{"execution":{"iopub.status.busy":"2022-01-26T17:00:04.074185Z","iopub.execute_input":"2022-01-26T17:00:04.074436Z","iopub.status.idle":"2022-01-26T17:00:04.08921Z","shell.execute_reply.started":"2022-01-26T17:00:04.074401Z","shell.execute_reply":"2022-01-26T17:00:04.088214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I use Pandas to import them and to verify, where null values are there or some values are missing. The result shows, that the data set is complete, so that no additional data completion is needed. That's a goog result!","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv(os.path.join(path, 'train.csv'))\ntest_df = pd.read_csv(os.path.join(path, 'test.csv'))\nsample_submission = pd.read_csv(os.path.join(path, 'sample_submission.csv'))\nnordic_holidays = pd.read_csv(os.path.join(nordic_path, 'nordic_holidays.csv'),\n                              index_col=0,\n                              header=0,\n                              names=['holiday_date', 'holiday', 'holiday_country'])\n        \ntrain_df.isna().sum().sum(), test_df.isna().sum().sum(), train_df.isnull().sum().sum(), test_df.isnull().sum().sum()","metadata":{"execution":{"iopub.status.busy":"2022-01-26T17:00:04.091846Z","iopub.execute_input":"2022-01-26T17:00:04.09214Z","iopub.status.idle":"2022-01-26T17:00:04.174434Z","shell.execute_reply.started":"2022-01-26T17:00:04.092105Z","shell.execute_reply":"2022-01-26T17:00:04.173692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Info about the nordic holidays are published in this dataset: https://www.kaggle.com/lucamassaron/festivities-in-finland-norway-sweden-tsp-0122\nThanks to the author!","metadata":{}},{"cell_type":"markdown","source":"Let's display our data.","metadata":{}},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-26T17:00:04.175627Z","iopub.execute_input":"2022-01-26T17:00:04.176027Z","iopub.status.idle":"2022-01-26T17:00:04.190392Z","shell.execute_reply.started":"2022-01-26T17:00:04.175992Z","shell.execute_reply":"2022-01-26T17:00:04.189571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The column 'num_sold' is our depended variable. For numerical reasons the logarithmic value is usefull. The value range of the depended variables is needed to build the tabular learner later on. Setting the range will add a sigmoid function at the last output. ","metadata":{}},{"cell_type":"code","source":"dep_var = ['num_sold']\ntrain_df[dep_var] = np.log(train_df[dep_var])\nmax_dep_value = np.max(train_df[dep_var].max()) * 1.05\nmin_dep_value = np.min(train_df[dep_var].min()) * 0.95\ndep_value_range = torch.tensor([min_dep_value, max_dep_value], device=device)\ndep_value_range, dep_var","metadata":{"execution":{"iopub.status.busy":"2022-01-26T17:00:04.191669Z","iopub.execute_input":"2022-01-26T17:00:04.192001Z","iopub.status.idle":"2022-01-26T17:00:05.89785Z","shell.execute_reply.started":"2022-01-26T17:00:04.191965Z","shell.execute_reply":"2022-01-26T17:00:05.897142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's find out how the data in the object columns 'country', 'store' and 'product' are distributed. The good news is that they are all equal distributed!","metadata":{}},{"cell_type":"code","source":"np.unique(train_df['country'], return_counts=True), np.unique(train_df['store'], return_counts=True), np.unique(train_df['product'], return_counts=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-26T17:00:05.89927Z","iopub.execute_input":"2022-01-26T17:00:05.899539Z","iopub.status.idle":"2022-01-26T17:00:05.965396Z","shell.execute_reply.started":"2022-01-26T17:00:05.899503Z","shell.execute_reply":"2022-01-26T17:00:05.964499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.unique(test_df['country'], return_counts=True), np.unique(test_df['store'], return_counts=True), np.unique(test_df['product'], return_counts=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-26T17:00:05.966975Z","iopub.execute_input":"2022-01-26T17:00:05.967277Z","iopub.status.idle":"2022-01-26T17:00:05.990377Z","shell.execute_reply.started":"2022-01-26T17:00:05.967239Z","shell.execute_reply":"2022-01-26T17:00:05.989606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The provided data contain a date value and i will see, whether the dates in the training and in the test data are overlapping. They don't!","metadata":{}},{"cell_type":"code","source":"train_df['date'].min(), train_df['date'].max(), test_df['date'].min(), test_df['date'].max(), len(test_df), len(train_df)- len(test_df)","metadata":{"execution":{"iopub.status.busy":"2022-01-26T17:00:05.99288Z","iopub.execute_input":"2022-01-26T17:00:05.994362Z","iopub.status.idle":"2022-01-26T17:00:06.012041Z","shell.execute_reply.started":"2022-01-26T17:00:05.994334Z","shell.execute_reply":"2022-01-26T17:00:06.011262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For the training process of the used neural network, i must split the values in the data frame train_df into a training \nand a validation part. But how should i select them? The task of this competion is to predict values, \nwhich are located in future. I will use the same amount of validation data as the amount of test data i have.\nTherefore i will use values with the row index (0-19727) for the training data and rows (19728, 26297) for the \nvalidation data. ","metadata":{}},{"cell_type":"code","source":"cut = train_df['date'][(train_df['date'] == train_df['date'][len(test_df)])].index.max()\ntrain_idx = range(len(train_df)-cut-1)\nvalid_idx = range(len(train_df)-cut, len(train_df)-1)\nsplits = (list(train_idx),list(valid_idx))\ntrain_idx, valid_idx","metadata":{"execution":{"iopub.status.busy":"2022-01-26T17:00:06.01465Z","iopub.execute_input":"2022-01-26T17:00:06.015237Z","iopub.status.idle":"2022-01-26T17:00:06.028736Z","shell.execute_reply.started":"2022-01-26T17:00:06.015201Z","shell.execute_reply":"2022-01-26T17:00:06.028015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's try add more features about the nordic holidays to the data frame. Can we improve the overall score with these additional datas? I will define a flag to control this feature. ","metadata":{}},{"cell_type":"code","source":"add_nordic_holidays=True","metadata":{"execution":{"iopub.status.busy":"2022-01-26T17:00:06.030175Z","iopub.execute_input":"2022-01-26T17:00:06.030645Z","iopub.status.idle":"2022-01-26T17:00:06.034297Z","shell.execute_reply.started":"2022-01-26T17:00:06.030603Z","shell.execute_reply":"2022-01-26T17:00:06.033501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nordic_holidays.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-26T17:00:06.035664Z","iopub.execute_input":"2022-01-26T17:00:06.036209Z","iopub.status.idle":"2022-01-26T17:00:06.047892Z","shell.execute_reply.started":"2022-01-26T17:00:06.036115Z","shell.execute_reply":"2022-01-26T17:00:06.047107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The following function calculates for each row in df the time distance between the current date and the last occurred holiday date and adds these values to the passed dataframe. The new column name is created by field and prefix name.","metadata":{}},{"cell_type":"code","source":"def get_elapsed(df, fld, pre):\n    day1 = np.timedelta64(1, 'D')\n    last_date = np.datetime64()\n    res = []\n\n    for c,c_h,h_d,d in zip(df.country.values, df.holiday_country.values, df[fld].values, df.date.values):\n        if c == c_h :\n            last_date = h_d\n        res.append(((d-last_date).astype('timedelta64[D]') / day1))\n    \n    df[pre+fld] = res\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-01-26T17:00:06.051721Z","iopub.execute_input":"2022-01-26T17:00:06.051962Z","iopub.status.idle":"2022-01-26T17:00:06.058301Z","shell.execute_reply.started":"2022-01-26T17:00:06.051937Z","shell.execute_reply":"2022-01-26T17:00:06.057409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Create a new data frame with the time difference before and after the last occurred holiday.","metadata":{}},{"cell_type":"code","source":"def get_distance_to_holiday_date(df, field='holiday_date'):\n    \n    \n    columns = ['row_id', 'date', 'country','holiday', 'store', 'holiday_date', 'holiday_country']\n    \n    dist_df = df[columns].copy()\n    dist_df = dist_df.sort_values(['store', 'date'])\n    dist_df = get_elapsed(dist_df, field, 'After_')\n    \n    dist_df = dist_df.sort_values(['store', 'date'], ascending=[True, False])\n    dist_df = get_elapsed(dist_df, field, 'Before_')\n    \n    dist_df['After_holiday_date'] = dist_df['After_holiday_date'].fillna(0).astype(int)\n    dist_df['Before_holiday_date'] = dist_df['Before_holiday_date'].fillna(0).astype(int)\n    \n    dist_df.drop(['country', 'holiday_date', 'holiday', 'holiday_country'],  axis=1,  inplace=True)\n    return dist_df","metadata":{"execution":{"iopub.status.busy":"2022-01-26T17:00:06.05965Z","iopub.execute_input":"2022-01-26T17:00:06.059913Z","iopub.status.idle":"2022-01-26T17:00:06.068533Z","shell.execute_reply.started":"2022-01-26T17:00:06.05988Z","shell.execute_reply":"2022-01-26T17:00:06.06776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def add_nordic_holidays(df):\n    df = pd.merge(df, nordic_holidays, left_on=['date', 'country'], right_on=['holiday_date', 'holiday_country'], how='left')\n    \n    # calculate the distance before and after the holidays\n  #  dist_df = get_distance_to_holiday_date(df)\n   # df = pd.merge(df, dist_df, left_on=['row_id', 'date', 'store'], right_on=['row_id', 'date', 'store'], how='left')\n    \n    df['holiday'] = df['holiday'].astype('category')\n    \n    # the values of 'holiday_date' aren't needed anymore, let's drop the column\n    df.drop(['holiday_date', 'holiday_country'], axis=1, inplace=True)\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-01-26T17:00:06.069853Z","iopub.execute_input":"2022-01-26T17:00:06.070106Z","iopub.status.idle":"2022-01-26T17:00:06.077659Z","shell.execute_reply.started":"2022-01-26T17:00:06.070074Z","shell.execute_reply":"2022-01-26T17:00:06.076875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if add_nordic_holidays:\n    print(\"We will add info about the nordic holidays to the data frames\")\n    \n    train_df = add_nordic_holidays(train_df)\n    test_df = add_nordic_holidays(test_df)\n    \n    print(\"Done.\")\nelse:\n    print(\"No holiday infos added\")\n    \n# set the index \ntrain_df.set_index('row_id', inplace=True)\ntest_df.set_index('row_id', inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-26T17:00:06.078854Z","iopub.execute_input":"2022-01-26T17:00:06.079524Z","iopub.status.idle":"2022-01-26T17:00:06.118209Z","shell.execute_reply.started":"2022-01-26T17:00:06.079486Z","shell.execute_reply":"2022-01-26T17:00:06.117439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To process time series as in this competion, it's a clever way to extract more metadata from a date value like number of week, number of day in current month or year and so forth. The fastai library offers the function 'add_datepart' to execute this extraction. You specify the column, you want to extract. The parameter drop specifies, whether this column is droped or not, which is the default. I will do so","metadata":{}},{"cell_type":"code","source":"train_df = add_datepart(train_df, 'date', drop=True)\ntest_df = add_datepart(test_df, 'date', drop=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-26T17:00:06.119244Z","iopub.execute_input":"2022-01-26T17:00:06.119945Z","iopub.status.idle":"2022-01-26T17:00:06.199585Z","shell.execute_reply.started":"2022-01-26T17:00:06.119905Z","shell.execute_reply":"2022-01-26T17:00:06.198902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's see the modified dataframe and the new added values.","metadata":{}},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-26T17:00:06.201071Z","iopub.execute_input":"2022-01-26T17:00:06.201345Z","iopub.status.idle":"2022-01-26T17:00:06.220217Z","shell.execute_reply.started":"2022-01-26T17:00:06.201307Z","shell.execute_reply":"2022-01-26T17:00:06.219263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I need a list of the column names, which are candidates for category variables and which are no candidates, also called continous variables. The Fastai library offers the function 'cont_cat_split' to do this for us. ","metadata":{}},{"cell_type":"code","source":"cont_vars, cat_vars = cont_cat_split(train_df, dep_var= dep_var,  max_card=12)\ncat_vars, cont_vars","metadata":{"execution":{"iopub.status.busy":"2022-01-26T17:00:06.222087Z","iopub.execute_input":"2022-01-26T17:00:06.222661Z","iopub.status.idle":"2022-01-26T17:00:06.235328Z","shell.execute_reply.started":"2022-01-26T17:00:06.222621Z","shell.execute_reply":"2022-01-26T17:00:06.23407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(cat_vars), len(cont_vars)","metadata":{"execution":{"iopub.status.busy":"2022-01-26T17:00:06.236306Z","iopub.execute_input":"2022-01-26T17:00:06.237917Z","iopub.status.idle":"2022-01-26T17:00:06.244679Z","shell.execute_reply.started":"2022-01-26T17:00:06.237876Z","shell.execute_reply":"2022-01-26T17:00:06.243708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The next step is to create a data loader. The Fastai library offers a powerful helper called 'TabularPandas'. It needs the data frame, list of the category and continous variables, the depened variable and a splitter. The splitter divides the data set into two parts: one for the training and one for the validation and for internal optimization step in each epoch. The batch size is set to 64.\n","metadata":{}},{"cell_type":"code","source":"procs=[Categorify, FillMissing, Normalize]\nto_train = TabularPandas(train_df, \n                         procs=procs, \n                         cat_names=cat_vars, \n                         cont_names=cont_vars, \n                         splits=splits, \n                         device=device,\n                         y_names=dep_var,\n                         y_block=RegressionBlock())","metadata":{"execution":{"iopub.status.busy":"2022-01-26T17:00:06.246056Z","iopub.execute_input":"2022-01-26T17:00:06.246846Z","iopub.status.idle":"2022-01-26T17:00:06.567208Z","shell.execute_reply.started":"2022-01-26T17:00:06.2468Z","shell.execute_reply":"2022-01-26T17:00:06.56653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dls = to_train.dataloaders(bs=128)\nlen(dls.train),len(dls.valid), type(dls.train), dls.train.device","metadata":{"execution":{"iopub.status.busy":"2022-01-26T17:00:06.570235Z","iopub.execute_input":"2022-01-26T17:00:06.570648Z","iopub.status.idle":"2022-01-26T17:00:06.58813Z","shell.execute_reply.started":"2022-01-26T17:00:06.570618Z","shell.execute_reply":"2022-01-26T17:00:06.587487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We must define the SMAPE function based on Wikipedia. The function isn't part of pytorch or fastai currently.","metadata":{}},{"cell_type":"code","source":"def smape(y_pred, target):\n    return torch.mean(2*torch.abs(y_pred - target)/(torch.abs(target) + torch.abs(y_pred)))","metadata":{"execution":{"iopub.status.busy":"2022-01-26T17:00:06.589561Z","iopub.execute_input":"2022-01-26T17:00:06.590244Z","iopub.status.idle":"2022-01-26T17:00:06.595061Z","shell.execute_reply.started":"2022-01-26T17:00:06.590207Z","shell.execute_reply":"2022-01-26T17:00:06.5943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MyLoss(nn.Module):\n    def __init__(self, diff_weight=0.5):\n        super().__init__()\n        self.diff_weight = diff_weight\n        \n    def forward(self,y_pred, target):\n        num_loss =  (1-self.diff_weight) * smape(y_pred[:,0], target[:,0])\n        diff_loss = self.diff_weight * smape(y_pred[:,1], target[:,1])\n        return num_loss + diff_loss    ","metadata":{"execution":{"iopub.status.busy":"2022-01-26T17:00:06.596114Z","iopub.execute_input":"2022-01-26T17:00:06.596813Z","iopub.status.idle":"2022-01-26T17:00:06.604513Z","shell.execute_reply.started":"2022-01-26T17:00:06.596762Z","shell.execute_reply":"2022-01-26T17:00:06.603809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"At least i create a learner pasing the dataloader into it. The default settings are two hidden layers with 200 and 100 elements. But i use more hidden layers to increase the number of trainable parameters, as you can see in the reported summary. Increasing the number of parameters in the neural network will improve the accuarcy and score, hopefully.","metadata":{}},{"cell_type":"code","source":"my_config = tabular_config(ps=.15, embed_p=0.15, use_bn=True, y_range=dep_value_range)\n\nlearn = tabular_learner(dls, \n                        n_out = dls.c,\n                        config = my_config,\n                        layers = [64,256,1024,1024,256,64,16],\n                        metrics = [smape, exp_rmspe]) \n                       \nlearn.summary()","metadata":{"execution":{"iopub.status.busy":"2022-01-26T17:00:06.606059Z","iopub.execute_input":"2022-01-26T17:00:06.606318Z","iopub.status.idle":"2022-01-26T17:00:07.071297Z","shell.execute_reply.started":"2022-01-26T17:00:06.606284Z","shell.execute_reply":"2022-01-26T17:00:07.070498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn.lr_find()","metadata":{"execution":{"iopub.status.busy":"2022-01-26T17:00:07.072638Z","iopub.execute_input":"2022-01-26T17:00:07.07298Z","iopub.status.idle":"2022-01-26T17:00:09.442246Z","shell.execute_reply.started":"2022-01-26T17:00:07.072939Z","shell.execute_reply":"2022-01-26T17:00:09.44146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"/home/egbert/tmp/submission_best.csvI will use a maximum learning rate of 3e-3. Starting the learning process is quite easy, i will run for 250 epochs, we have small data set and we can process this data in a few seconds per epoch. I will save the model with the best, with the lowest validation lost value. The Fastai library offers the SaveModelCallback callback. You must specify the file name only. The option with_opt=True stores the values of the optimizer also. You will find the new file under models/kaggle_tps_jan_2022.pth","metadata":{}},{"cell_type":"code","source":"learn.fit_one_cycle(300, 3e-3, cbs=SaveModelCallback(fname='kaggle_tps_jan_2022', with_opt=True))","metadata":{"execution":{"iopub.status.busy":"2022-01-26T17:00:09.443411Z","iopub.execute_input":"2022-01-26T17:00:09.443646Z","iopub.status.idle":"2022-01-26T17:10:22.980232Z","shell.execute_reply.started":"2022-01-26T17:00:09.443611Z","shell.execute_reply":"2022-01-26T17:10:22.979538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn.show_results(shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-26T17:10:22.981385Z","iopub.execute_input":"2022-01-26T17:10:22.981711Z","iopub.status.idle":"2022-01-26T17:10:23.076642Z","shell.execute_reply.started":"2022-01-26T17:10:22.981673Z","shell.execute_reply":"2022-01-26T17:10:23.075813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To calculate the predictions for this competition, i will load the best model from the training process. Best model means the model where the validation loss has the lowest value.","metadata":{}},{"cell_type":"code","source":"learn.load('kaggle_tps_jan_2022')","metadata":{"execution":{"iopub.status.busy":"2022-01-26T17:10:23.078105Z","iopub.execute_input":"2022-01-26T17:10:23.078361Z","iopub.status.idle":"2022-01-26T17:10:23.113298Z","shell.execute_reply.started":"2022-01-26T17:10:23.078326Z","shell.execute_reply":"2022-01-26T17:10:23.112529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dlt = learn.dls.test_dl(test_df, bs=64) \nnn_preds, _ = learn.get_preds(dl=dlt) \nnn_preds.min(), nn_preds.max()","metadata":{"execution":{"iopub.status.busy":"2022-01-26T17:10:23.114578Z","iopub.execute_input":"2022-01-26T17:10:23.114844Z","iopub.status.idle":"2022-01-26T17:10:23.686428Z","shell.execute_reply.started":"2022-01-26T17:10:23.114809Z","shell.execute_reply":"2022-01-26T17:10:23.685707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission[\"num_sold\"] = np.exp(nn_preds)\nsample_submission.to_csv(\"submission.csv\", index=False)\nsample_submission.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-26T17:10:23.690358Z","iopub.execute_input":"2022-01-26T17:10:23.692549Z","iopub.status.idle":"2022-01-26T17:10:23.736032Z","shell.execute_reply.started":"2022-01-26T17:10:23.692512Z","shell.execute_reply":"2022-01-26T17:10:23.735395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls -la","metadata":{"execution":{"iopub.status.busy":"2022-01-26T17:10:23.739511Z","iopub.execute_input":"2022-01-26T17:10:23.741667Z","iopub.status.idle":"2022-01-26T17:10:24.520371Z","shell.execute_reply.started":"2022-01-26T17:10:23.74163Z","shell.execute_reply":"2022-01-26T17:10:24.519583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"That's it for the begining..","metadata":{}}]}