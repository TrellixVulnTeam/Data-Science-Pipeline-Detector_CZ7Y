{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Intro","metadata":{}},{"cell_type":"markdown","source":"Hi All, This is my very first Kaggle Kernel, so hopefully is useful for a lot of people.\nI've been working as a DS since a long time and finally I decided to enter a Kaggle competition, even if it is a Playground one. \n\nDuring this Competition I have learned a lot from different other kernels so I will try to do my best to give proper credit to everyone!!\n\n**Disclaimer:** Despite taking a lot of ideas from other kernels I tend to reimplementing to learn what is hapenning in there.\n\nThis Kernel will focus only in my Scripts to get my best submission so far.\n\n* I'm using **Feature Engine** for all the Preprocessing Steps. \n* **Scikit-Learn** Pipelines to make everything more organized.\n* **Catboost** as my main Model.\n\nIf yout think this helps you with your learning, please **UPVOTE**.","metadata":{}},{"cell_type":"markdown","source":"**COMMENT: I know I'm not using seeds for reproducibility, but for some reason the results in my machine and in the Kernel were exactly the same.**\n\nFor some reason, once I tried a new learning rate the results in my Local Validation Scheme changed, instead of giving me 7.01 started ginving me 7.05 with no change. So I started tuning HPs until I got a 7.00. I submitted and I got a new LB Score of 4.89 just by regularizing Catboost.","metadata":{}},{"cell_type":"markdown","source":"# Installing/Importing Libraries","metadata":{"execution":{"iopub.status.busy":"2022-01-08T14:28:39.906775Z","iopub.execute_input":"2022-01-08T14:28:39.907127Z","iopub.status.idle":"2022-01-08T14:28:39.912257Z","shell.execute_reply.started":"2022-01-08T14:28:39.907094Z","shell.execute_reply":"2022-01-08T14:28:39.910987Z"}}},{"cell_type":"code","source":"!pip install feature_engine","metadata":{"execution":{"iopub.status.busy":"2022-01-08T17:15:46.094278Z","iopub.execute_input":"2022-01-08T17:15:46.095115Z","iopub.status.idle":"2022-01-08T17:15:57.22986Z","shell.execute_reply.started":"2022-01-08T17:15:46.094948Z","shell.execute_reply":"2022-01-08T17:15:57.228907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import holidays\nimport joblib\nimport numpy as np\nimport pandas as pd\nfrom catboost import CatBoostRegressor\nfrom feature_engine.creation import CombineWithReferenceFeature\nfrom feature_engine.creation import CyclicalTransformer\nfrom feature_engine.encoding import OneHotEncoder\nfrom feature_engine.imputation import CategoricalImputer\nfrom feature_engine.selection import DropFeatures\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import TimeSeriesSplit","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-08T17:15:57.232849Z","iopub.execute_input":"2022-01-08T17:15:57.23362Z","iopub.status.idle":"2022-01-08T17:15:58.833663Z","shell.execute_reply.started":"2022-01-08T17:15:57.233579Z","shell.execute_reply":"2022-01-08T17:15:58.832739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Utilities\n\nThese are Utilities I just created/used to make my code more organized.\n\n* I changed SMAPE implementations several times, so I think I took it from this [Kernel](https://www.kaggle.com/maxencefzr/tps-jan22-eda-simple-catboost). Thanks a lot!\n\n* I also hacked the CyclicalTransformer from Feature Engine. The main reason to do this was that if I tried to create 2 different Cyclycal variables with the same Base variables they were overwritten. This is because by default Cyclical Transformer assigns _cos and _sin as suffixes. Thanks [Soledad Galli](https://www.kaggle.com/solegalli) for the Library. \n\nFinally I created Variables from Dates using Pandas. I made it by myself but then I noticed a lot of people where using the same. So thanks to all of them.","metadata":{}},{"cell_type":"code","source":"# Smape Calculation to test model.\ndef smape(y_true, y_pred):\n    numerator = np.abs(y_pred - y_true)\n    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n    \n    return np.mean(numerator / denominator)*100\n\n# This is a modified version of Cyclical Transformers of FEature Engine.\n# This modification allows me to make multiple different Cyclycal Transformers \n# to same variable with no overwriting.\nclass CyclicalTransformerV2(CyclicalTransformer):\n    def __init__(self, suffix = None, **kwargs):\n            super().__init__(**kwargs)\n            self.suffix = suffix\n        \n    def transform(self, X):\n        X = super().transform(X)\n        if self.suffix is not None:\n            transformed_names = X.filter(regex = r'sin$|cos$').columns\n            new_names = {name: name + self.suffix for name in transformed_names}\n            X.rename(columns=new_names, inplace=True)\n        return X\n\n\n# Utility to create Date Features \ndef create_date_features(df):\n    df['day_of_year'] = df.date.dt.day_of_year\n    df['day_of_month'] = df.date.dt.day\n    df['day_of_week'] = df.date.dt.weekday\n    df['month'] = df.date.dt.month\n    df['quarter'] = df.date.dt.quarter\n    df['year'] = df.date.dt.year\n    df['period'] = df.date.dt.to_period('M')\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-01-08T17:15:58.835057Z","iopub.execute_input":"2022-01-08T17:15:58.835316Z","iopub.status.idle":"2022-01-08T17:15:58.845468Z","shell.execute_reply.started":"2022-01-08T17:15:58.835283Z","shell.execute_reply":"2022-01-08T17:15:58.844429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Importing Data","metadata":{}},{"cell_type":"markdown","source":"So I just Imported the data making sure to parse dates correctly.","metadata":{"execution":{"iopub.status.busy":"2022-01-08T14:46:14.912368Z","iopub.execute_input":"2022-01-08T14:46:14.912726Z","iopub.status.idle":"2022-01-08T14:46:14.920237Z","shell.execute_reply.started":"2022-01-08T14:46:14.912693Z","shell.execute_reply":"2022-01-08T14:46:14.918498Z"}}},{"cell_type":"code","source":"model_name = 'MAE_15000_cyc2_es_cv'\ndf = pd.read_csv('../input/tabular-playground-series-jan-2022/train.csv', \n                                 index_col=0, parse_dates=['date'])\ndf = create_date_features(df)","metadata":{"execution":{"iopub.status.busy":"2022-01-08T17:15:58.847545Z","iopub.execute_input":"2022-01-08T17:15:58.847772Z","iopub.status.idle":"2022-01-08T17:15:58.961788Z","shell.execute_reply.started":"2022-01-08T17:15:58.847742Z","shell.execute_reply":"2022-01-08T17:15:58.960946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Then I took an Idea from [here](https://www.kaggle.com/maxencefzr/tps-jan22-eda-simple-catboost) too about using Holidays. It turns out I didn't know at all the holidays library. So Inspired by that I just added the holidays for every Country to capture the effect and merge it to the DataFrame.","metadata":{}},{"cell_type":"code","source":"\nfinland = pd.DataFrame([dict(date = date, finland_holiday = event, country= 'Finland') \n                        for date, event in holidays.Finland(years=[2015, 2016, 2017, 2018, 2019]).items()])\nfinland['date'] = finland['date'].astype(\"datetime64\")\n\nnorway = pd.DataFrame([dict(date = date, norway_holiday = event, country= 'Norway') \n                       for date, event in holidays.Norway(years=[2015, 2016, 2017, 2018, 2019]).items()])\nnorway['date'] = norway['date'].astype(\"datetime64\")\n\nsweden = pd.DataFrame([dict(date = date, sweden_holiday = event.replace(\", Söndag\", \"\"), country= 'Sweden') \n    for date, event in holidays.Sweden(years=[2015, 2016, 2017, 2018, 2019]).items() if event != 'Söndag'])\nsweden['date'] = sweden['date'].astype(\"datetime64\")\n\ndf = (df.merge(finland, on = ['date', 'country'], how = 'left')\n      .merge(norway, on = ['date', 'country'], how = 'left')\n      .merge(sweden, on = ['date', 'country'], how = 'left'))\n\nprint('Columns Before Training:', df.columns)\ndf","metadata":{"execution":{"iopub.status.busy":"2022-01-08T17:15:58.963116Z","iopub.execute_input":"2022-01-08T17:15:58.963376Z","iopub.status.idle":"2022-01-08T17:15:59.068725Z","shell.execute_reply.started":"2022-01-08T17:15:58.963345Z","shell.execute_reply":"2022-01-08T17:15:59.067362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"markdown","source":"Here is where the modeling part starts.\n\n* I started learning a lot from AmbroSM [Kernel](https://www.kaggle.com/ambrosm/tpsjan22-03-linear-model#Residuals-of-the-simple-model). And in this kernel I found how to create Fourier Series and Interaction with some of the variables. \n\n* By using the Hacked Cyclical Transformer I was able to quicly create Fourier Terms for n=1, 2 at the day level. I just used these because adding more terms started to be detrimental for the model. I'm guessing I was adding too much noise.\n\nThe Fourier Terms using Cyclical Transformer are as follows: \n\n$$var\\_sin = sin\\left(\\frac{2\\pi}{max\\_value}t\\right)$$\n$$var\\_cos = cos\\left(\\frac{2\\pi}{max\\_value}t\\right)$$\n\n$$var\\_sin = sin\\left(\\frac{2n\\pi}{max\\_value}t\\right) = sin\\left(\\frac{2\\pi}{\\frac{max\\_value}{n}}t\\right)$$\n$$var\\_cos = cos\\left(\\frac{2n\\pi}{max\\_value}t\\right) = cos\\left(\\frac{2\\pi}{\\frac{max\\_value}{n}}t\\right)$$\n\n* Then using `CombineWithReferenceFeature` from feature engine I could quickly create interactions between the products (OneHotEncoded) and the Fourier Terms.\n\n* Finally I dropped the features I was not using.\n\nOne thing that could be a bit weird is why I'm doing my preprocessing separately. This is because I'm using Early Stopping with my Catboost Model, so in order to have the same transformations in the Training and Validation sets I decided to do this stage by separate.","metadata":{}},{"cell_type":"code","source":"# Fourier Terms of n = 1\ncyc_1 = CyclicalTransformerV2(variables = ['day_of_year','day_of_month','day_of_week'], \n                max_values = {'day_of_year':365,'day_of_month': 30,'day_of_week': 7}, \n                suffix = '_1')\n# Fourier Terms of n = 2\ncyc_2 = CyclicalTransformerV2(variables = ['day_of_year','day_of_month','day_of_week'], \n                max_values = {'day_of_year':365/2,'day_of_month': 30/2,'day_of_week': 7/2}, \n                suffix = '_2')\n\nprep = Pipeline(steps  =[\n        ('cat_imputation', CategoricalImputer()),        \n        ('ohe',OneHotEncoder()),\n        ('cyc', cyc_1),\n        ('cyc2', cyc_2),\n        # ('cyc3', cyc_3),\n        ('combo', CombineWithReferenceFeature(\n        variables_to_combine=['day_of_year_sin_1', 'day_of_year_cos_1','day_of_month_sin_1', \n                            'day_of_month_cos_1', 'day_of_week_sin_1','day_of_week_cos_1', \n                            'day_of_year_sin_2', 'day_of_year_cos_2','day_of_month_sin_2', \n                            'day_of_month_cos_2', 'day_of_week_sin_2','day_of_week_cos_2', \n                            ], \n        reference_variables=['product_Kaggle Mug', 'product_Kaggle Hat','product_Kaggle Sticker'],\n        operations=['mul'])),\n        ('drop',DropFeatures(features_to_drop=['date','period'])),\n        ])\n\n\n    \nX = prep.fit_transform(df.drop(columns=['num_sold']))\ny = df.num_sold\n\njoblib.dump(prep, f'./prep_{model_name}.joblib')","metadata":{"execution":{"iopub.status.busy":"2022-01-08T17:15:59.070275Z","iopub.execute_input":"2022-01-08T17:15:59.070768Z","iopub.status.idle":"2022-01-08T17:15:59.535148Z","shell.execute_reply.started":"2022-01-08T17:15:59.070733Z","shell.execute_reply":"2022-01-08T17:15:59.534269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CV Strategy","metadata":{}},{"cell_type":"markdown","source":"* I'm just using TimeSeries Split using Four Folds. I'm not 100% percent sure, but since we have almost the same number of records in every year (probably different in 2016 since it was a leap year), this *should* be equivalent to train by year.\n\nAs I said before, training Catboost with MAE as a loss function and checking overfitting with SMAPE directly since Catboost includes it. Also I'm adding 1000 early stopping rounds to make training faster since it cannot be done in GPU (SMAPE is not available as a metric in GPU).\n\n* One thing to point out is that I'm saving every model trained (In the different folds) as regularization strategy. I learned this in this [Abishek's Video](https://www.youtube.com/watch?v=zcqgj-Udcqs), so thanks.\n\n* Got a ~7.01 in my Validation Scheme.\n\n","metadata":{}},{"cell_type":"code","source":"folds = TimeSeriesSplit(n_splits=4)\nscore = []\nfor fold, (train_idx, val_idx) in enumerate(folds.split(X)):\n    \n    X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n    X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]\n    \n    model = CatBoostRegressor(iterations=15000,\n                            learning_rate=0.03,\n                            bootstrap_type='Bayesian',\n                            boosting_type='Plain',\n                            loss_function='MAE',\n                            l2_leaf_reg = 5, # Added as Regularization\n                            use_best_model = True, \n                            # loss_function='Huber:delta=0.5',\n                            eval_metric='SMAPE',\n                            # random_seed = 123,\n                            # task_type=\"GPU\",\n                            # devices='0:1'\n                            )\n\n    model.fit(X_train, y_train, \n            eval_set = (X_val, y_val),\n            early_stopping_rounds = 1000\n            )\n\n    y_pred = model.predict(X_val)\n    \n    \n    joblib.dump(model, f'./{model_name}_fold_{fold}.joblib')\n    score.append(smape(y_val, y_pred))\n\nprint('Score', score)\nprint('Mean Score', np.mean(score))","metadata":{"execution":{"iopub.status.busy":"2022-01-08T17:15:59.53679Z","iopub.execute_input":"2022-01-08T17:15:59.537104Z","iopub.status.idle":"2022-01-08T17:16:53.694917Z","shell.execute_reply.started":"2022-01-08T17:15:59.537061Z","shell.execute_reply":"2022-01-08T17:16:53.694056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference","metadata":{"execution":{"iopub.status.busy":"2022-01-08T14:17:46.881671Z","iopub.execute_input":"2022-01-08T14:17:46.881975Z","iopub.status.idle":"2022-01-08T14:17:46.887104Z","shell.execute_reply.started":"2022-01-08T14:17:46.881945Z","shell.execute_reply":"2022-01-08T14:17:46.885985Z"}}},{"cell_type":"code","source":"df_test = pd.read_csv('../input/tabular-playground-series-jan-2022/test.csv', index_col=0, parse_dates=['date'])\nid = df_test.index\ndf_test = create_date_features(df_test)\n\ndf_test = df_test.merge(finland, on = ['date', 'country'], how = 'left').merge(norway, on = ['date', 'country'], how = 'left').merge(sweden, on = ['date', 'country'], how = 'left')\nprep = joblib.load(f'./prep_{model_name}.joblib')\n\nX = prep.fit_transform(df_test)","metadata":{"execution":{"iopub.status.busy":"2022-01-08T17:16:53.69637Z","iopub.execute_input":"2022-01-08T17:16:53.697236Z","iopub.status.idle":"2022-01-08T17:16:53.904141Z","shell.execute_reply.started":"2022-01-08T17:16:53.697191Z","shell.execute_reply":"2022-01-08T17:16:53.903236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* For Inference I just repeated the process for training, making sure to Blend all the Models of the different Folds by using different weights. I applied this since the good results obtained by Luca Massaron in a Linear Model implementing sample weights [here](https://www.kaggle.com/lucamassaron/kaggle-merchandise-eda-with-baseline-linear-model). \n\n* I also applied rounding to the Predictions seen [here](https://www.kaggle.com/xinyangkabuda/baseline-xgboost-lightgbm-stacking-v5-round) and actually I got a little boost in my score.\n\nOne thing I don't want to do is adding the **GDP data** (but I feel very pressured to do it). I've seen a lot of kernels doing this but in my opinion this is just exploiting a Leakage. Probably it is a good idea for this competition, but not sure if it could be applied in a real model. I tried to wait as far as I can before add those features.","metadata":{}},{"cell_type":"code","source":"preds_dict = {}\nfor fold in range(4):\n    pipe = joblib.load(f'./{model_name}_fold_{fold}.joblib')\n    preds = pipe.predict(X)\n    \n    preds_dict[fold] = preds\n    \nfinal_preds = pd.DataFrame(preds_dict)#.mean(axis = 1).apply(np.round).astype(\"int\")\n\nfinal_preds = (0.1 * final_preds.iloc[:,0] + 0.2 * final_preds.iloc[:,1] + 0.3 * final_preds.iloc[:,2] + 0.4 * final_preds.iloc[:,3]).apply(np.round).astype(\"int\")\nfinal_preds.index = id\nfinal_preds.name = 'num_sold'\nprint(final_preds)\n\nfinal_preds.to_csv(f'submission.csv')\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-01-08T17:16:53.905489Z","iopub.execute_input":"2022-01-08T17:16:53.906004Z","iopub.status.idle":"2022-01-08T17:16:54.034635Z","shell.execute_reply.started":"2022-01-08T17:16:53.905935Z","shell.execute_reply":"2022-01-08T17:16:54.033718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion","metadata":{}},{"cell_type":"markdown","source":"Thanks a lot to every one here helping me become a better data scientist. And if you like the kernel, please **UPVOTE**, it would be really nice to see I can help others too!!\n\nCheers!!","metadata":{}},{"cell_type":"markdown","source":"# This is my Local Submission","metadata":{}},{"cell_type":"code","source":"import pandas as pd\ndf = pd.read_csv('../input/local-submission-filecsv/MAE_15000_cyc2_es_cv_l2_5_sub_weighting.csv')\ndf.to_csv('submission_local.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2022-01-08T18:36:55.304817Z","iopub.execute_input":"2022-01-08T18:36:55.305838Z","iopub.status.idle":"2022-01-08T18:36:55.327109Z","shell.execute_reply.started":"2022-01-08T18:36:55.305774Z","shell.execute_reply":"2022-01-08T18:36:55.326471Z"},"trusted":true},"execution_count":null,"outputs":[]}]}