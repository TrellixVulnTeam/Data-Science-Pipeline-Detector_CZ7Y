{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#### In this notebook I start first from building a quick EDA revealing patters in the data, then I use EDA insights to build a simple linear model with only essential features (based on a out of time approach).","metadata":{}},{"cell_type":"markdown","source":"##### Since we will use Scikit-learn models, let's accelerate them first by using Intel Extension.","metadata":{}},{"cell_type":"markdown","source":"* vers. 20 - introduced selective rounding\n* vers. 22 - adding sin/cos processing of day of the year at different time lags","metadata":{}},{"cell_type":"code","source":"!pip install scikit-learn -U","metadata":{"execution":{"iopub.status.busy":"2022-01-16T23:43:06.535673Z","iopub.execute_input":"2022-01-16T23:43:06.536157Z","iopub.status.idle":"2022-01-16T23:43:23.776753Z","shell.execute_reply.started":"2022-01-16T23:43:06.536031Z","shell.execute_reply":"2022-01-16T23:43:23.775733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# IntelÂ® Extension for Scikit-learn installation:\n!pip install scikit-learn-intelex","metadata":{"execution":{"iopub.status.busy":"2022-01-16T23:43:23.779267Z","iopub.execute_input":"2022-01-16T23:43:23.779536Z","iopub.status.idle":"2022-01-16T23:43:57.868256Z","shell.execute_reply.started":"2022-01-16T23:43:23.779507Z","shell.execute_reply":"2022-01-16T23:43:57.867307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearnex import patch_sklearn\npatch_sklearn()","metadata":{"execution":{"iopub.status.busy":"2022-01-16T23:43:57.869977Z","iopub.execute_input":"2022-01-16T23:43:57.87036Z","iopub.status.idle":"2022-01-16T23:43:59.118257Z","shell.execute_reply.started":"2022-01-16T23:43:57.870313Z","shell.execute_reply":"2022-01-16T23:43:59.117306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### We then load the data, it is necessary paying attention to convert the date into datetime","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn","metadata":{"execution":{"iopub.status.busy":"2022-01-17T00:00:42.616946Z","iopub.execute_input":"2022-01-17T00:00:42.617597Z","iopub.status.idle":"2022-01-17T00:00:42.622952Z","shell.execute_reply.started":"2022-01-17T00:00:42.617558Z","shell.execute_reply":"2022-01-17T00:00:42.622241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loading train and test data\ntrain = pd.read_csv(\"../input/tabular-playground-series-jan-2022/train.csv\", parse_dates=['date'])\ntest = pd.read_csv(\"../input/tabular-playground-series-jan-2022/test.csv\", parse_dates=['date'])","metadata":{"execution":{"iopub.status.busy":"2022-01-16T23:43:59.264047Z","iopub.execute_input":"2022-01-16T23:43:59.265056Z","iopub.status.idle":"2022-01-16T23:43:59.355486Z","shell.execute_reply.started":"2022-01-16T23:43:59.264984Z","shell.execute_reply":"2022-01-16T23:43:59.354625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.dtypes","metadata":{"execution":{"iopub.status.busy":"2022-01-16T23:43:59.356964Z","iopub.execute_input":"2022-01-16T23:43:59.357541Z","iopub.status.idle":"2022-01-16T23:43:59.369014Z","shell.execute_reply.started":"2022-01-16T23:43:59.357493Z","shell.execute_reply":"2022-01-16T23:43:59.368022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Before starting with EDA, it is important to check about the data structure. Apparently we have a combination of time series based on countries, stores and products. Let's first check if all the combinations appear in train and test.","metadata":{}},{"cell_type":"code","source":"# figuring out the theoretically possible level combination\ntime_series = ['country', 'store', 'product']\ncombinations = 1\nfor feat in time_series:\n    combinations *= train[feat].nunique()\n    \nprint(f\"There are {combinations} possible combinations\")","metadata":{"execution":{"iopub.status.busy":"2022-01-16T23:43:59.370929Z","iopub.execute_input":"2022-01-16T23:43:59.371301Z","iopub.status.idle":"2022-01-16T23:43:59.391496Z","shell.execute_reply.started":"2022-01-16T23:43:59.371254Z","shell.execute_reply":"2022-01-16T23:43:59.390255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"time_series = ['country', 'store', 'product']\ncountry_store_product_train = train[time_series].drop_duplicates().sort_values(time_series)\ncountry_store_product_test =test[time_series].drop_duplicates().sort_values(time_series)\n\ncond_1 = len(country_store_product_train) == combinations\nprint(f\"Are all theoretical combinations present in train: {cond_1}\")\ncond_2 = (country_store_product_train == country_store_product_test).all().all()\nprint(f\"Are combinations the same in train and test: {cond_2}\")","metadata":{"execution":{"iopub.status.busy":"2022-01-16T23:43:59.392876Z","iopub.execute_input":"2022-01-16T23:43:59.393119Z","iopub.status.idle":"2022-01-16T23:43:59.427044Z","shell.execute_reply.started":"2022-01-16T23:43:59.393089Z","shell.execute_reply":"2022-01-16T23:43:59.426164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### As a second step let's visualize how time is split between train and test.","metadata":{}},{"cell_type":"code","source":"train_dates = train.date.drop_duplicates().sort_values()\ntest_dates = test.date.drop_duplicates().sort_values()\n\nfig, ax = plt.subplots(1, 1, figsize = (11, 7))\ncmap_cv = plt.cm.coolwarm\n\ncolor_index = np.array([1] * len(train_dates) + [0] * len(test_dates))\n\nax.scatter(range(len(train_dates)), [.5] * len(train_dates),\n           c=color_index[:len(train_dates)], marker='_', lw=15, cmap=cmap_cv,\n           label='train', vmin=-.2, vmax=1.2)\n\nax.scatter(range(len(train_dates), len(train_dates) + len(test_dates)), [.55] * len(test_dates),\n           c=color_index[len(train_dates):], marker='_', lw=15, cmap=cmap_cv,\n           label='test', vmin=-.2, vmax=1.2)\n\ntick_locations = np.cumsum([0, 365, 366, 365, 365, 365])\nfor i in (tick_locations):\n    ax.vlines(i, 0, 2,linestyles='dotted', colors = 'grey')\n    \nax.set_xticks(tick_locations)\nax.set_xticklabels([2015, 2016, 2017, 2018, 2019, 2020], rotation = 0)\nax.set_yticklabels(labels=[])\nplt.ylim([0.45, 0.60])\nax.legend(loc=\"upper left\", title=\"data\")\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-16T23:43:59.428283Z","iopub.execute_input":"2022-01-16T23:43:59.428513Z","iopub.status.idle":"2022-01-16T23:43:59.713024Z","shell.execute_reply.started":"2022-01-16T23:43:59.428484Z","shell.execute_reply":"2022-01-16T23:43:59.71221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Having four complete years available allows various types of testing and modelling. In this EDA we will limit to use the last year available (2018) as an hold-out, setting our baseline model to be able to forecast an entire year in the future.","metadata":{}},{"cell_type":"markdown","source":"##### As a last check we verify that no date is missing from train and test:","metadata":{}},{"cell_type":"code","source":"missing_train = pd.date_range(start=train_dates.min(), end=train_dates.max()).difference(train_dates)\nmissing_test = pd.date_range(start=test_dates.min(), end=test_dates.max()).difference(test_dates)\nprint(f\"missing dates in train: {len(missing_train)} and in test: {len(missing_test)}\")","metadata":{"execution":{"iopub.status.busy":"2022-01-16T23:43:59.715712Z","iopub.execute_input":"2022-01-16T23:43:59.715934Z","iopub.status.idle":"2022-01-16T23:43:59.726259Z","shell.execute_reply.started":"2022-01-16T23:43:59.715905Z","shell.execute_reply":"2022-01-16T23:43:59.725369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Having completed the checks, we process the datetime information, extracting it informative elements at different time granularities:","metadata":{}},{"cell_type":"code","source":"# We create different time granularity\n\ndef process_time(df):\n    df['year'] = df['date'].dt.year\n    df['month'] = df['date'].dt.month\n    df['week'] = df['date'].dt.isocalendar().week\n    df['week'][df['week']>52] = 52\n    df['day'] = df['date'].dt.day\n    df['dayofweek'] = df['date'].dt.dayofweek\n    df['quarter'] = df['date'].dt.quarter\n    df['dayofyear'] = df['date'].dt.dayofyear\n    return df\n\ntrain = process_time(train)\ntest = process_time(test)","metadata":{"execution":{"iopub.status.busy":"2022-01-16T23:43:59.727628Z","iopub.execute_input":"2022-01-16T23:43:59.727858Z","iopub.status.idle":"2022-01-16T23:43:59.793901Z","shell.execute_reply.started":"2022-01-16T23:43:59.727828Z","shell.execute_reply":"2022-01-16T23:43:59.793025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### We are ready to explore the data. In order to highlight the time series characteristics, we create panels of products x countries x shops. We start by aggregating at a year level.","metadata":{}},{"cell_type":"code","source":"for product in ['Kaggle Mug', 'Kaggle Hat', 'Kaggle Sticker']:\n    print(f\"\\n--- {product} ---\\n\")\n    fig = plt.figure(figsize=(20, 10), dpi=100)\n    fig.subplots_adjust(hspace=0.25)\n    for i, store in enumerate(['KaggleMart', 'KaggleRama']):\n        for j, country in enumerate(['Finland', 'Norway', 'Sweden']):\n            ax = fig.add_subplot(2, 3, (i*3+j+1))\n            selection = (train['country']==country)&(train['store']==store)&(train['product']==product)\n            selected = train[selection]\n            selected.set_index('date').groupby('year')['num_sold'].mean().plot(ax=ax)\n            ax.set_title(f\"{country}:{store}\")\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-16T23:43:59.795552Z","iopub.execute_input":"2022-01-16T23:43:59.796219Z","iopub.status.idle":"2022-01-16T23:44:03.474889Z","shell.execute_reply.started":"2022-01-16T23:43:59.796151Z","shell.execute_reply":"2022-01-16T23:44:03.473065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### The first series of panels points out that the country effect is kind of indipendent from store and product. There is an underlying country dynamic that replicates the same no matter the shop or the product sold by it. We also notice that shops differentiate only for the level of sales.","metadata":{}},{"cell_type":"markdown","source":"##### Our next panels will explore seaasonality based on months:","metadata":{}},{"cell_type":"code","source":"for product in ['Kaggle Mug', 'Kaggle Hat', 'Kaggle Sticker']:\n    fig = plt.figure(figsize=(20, 10), dpi=100)\n    fig.subplots_adjust(hspace=0.25)\n    for i, store in enumerate(['KaggleMart', 'KaggleRama']):\n        for j, country in enumerate(['Finland', 'Norway', 'Sweden']):\n            ax = fig.add_subplot(2, 3, (i*3+j+1))\n            selection = (train['country']==country)&(train['store']==store)&(train['product']==product)\n            selected = train[selection]\n            for year in [2015, 2016, 2017, 2018]:\n                selected[selected.year==year].set_index('date').groupby('month')['num_sold'].mean().plot(ax=ax, label=year)\n            ax.set_title(f\"{product} | {country}:{store}\")\n            ax.legend()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-16T23:44:03.476642Z","iopub.execute_input":"2022-01-16T23:44:03.476941Z","iopub.status.idle":"2022-01-16T23:44:08.11498Z","shell.execute_reply.started":"2022-01-16T23:44:03.476905Z","shell.execute_reply":"2022-01-16T23:44:08.114327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Here we notice two important elements: seanality curves are different for each product and they also differ from year to year. Averaging the curves probably is safe bet for the future, as well as considering more relevant the recent years (thus weighting more the year 2018 for instance). For the sticker product, year 2017 seems particularly different from others.","metadata":{}},{"cell_type":"markdown","source":"##### We now proceed to examine seasonality even more in detail at a week level:","metadata":{}},{"cell_type":"code","source":"for product in ['Kaggle Mug', 'Kaggle Hat', 'Kaggle Sticker']:\n    print(f\"\\n--- {product} ---\\n\")\n    fig = plt.figure(figsize=(20, 10), dpi=100)\n    fig.subplots_adjust(hspace=0.25)\n    for i, store in enumerate(['KaggleMart', 'KaggleRama']):\n        for j, country in enumerate(['Finland', 'Norway', 'Sweden']):\n            ax = fig.add_subplot(2, 3, (i*3+j+1))\n            selection = (train['country']==country)&(train['store']==store)&(train['product']==product)\n            selected = train[selection]\n            for year in [2015, 2016, 2017, 2018]:\n                selected[selected.year==year].set_index('date').groupby('week')['num_sold'].mean().plot(ax=ax, label=year)\n            ax.set_title(f\"{country}:{store}\")\n            ax.legend()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-16T23:44:08.116214Z","iopub.execute_input":"2022-01-16T23:44:08.116886Z","iopub.status.idle":"2022-01-16T23:44:12.402955Z","shell.execute_reply.started":"2022-01-16T23:44:08.116843Z","shell.execute_reply":"2022-01-16T23:44:12.402046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### At a week level we see that differences are due to peaks. Peaks seem different in Spring. Probably is is Easter effect.","metadata":{}},{"cell_type":"markdown","source":"##### We now start obeserving recurrences at a monthly level:","metadata":{}},{"cell_type":"code","source":"for product in ['Kaggle Mug', 'Kaggle Hat', 'Kaggle Sticker']:\n    print(f\"\\n--- {product} ---\\n\")\n    fig = plt.figure(figsize=(20, 10), dpi=100)\n    fig.subplots_adjust(hspace=0.25)\n    for i, store in enumerate(['KaggleMart', 'KaggleRama']):\n        for j, country in enumerate(['Finland', 'Norway', 'Sweden']):\n            ax = fig.add_subplot(2, 3, (i*3+j+1))\n            selection = (train['country']==country)&(train['store']==store)&(train['product']==product)\n            selected = train[selection]\n            for year in [2015, 2016, 2017, 2018]:\n                selected[selected.year==year].set_index('date').groupby('day')['num_sold'].mean().plot(ax=ax, label=year)\n            ax.set_title(f\"{country}:{store}\")\n            ax.legend()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-16T23:44:12.404343Z","iopub.execute_input":"2022-01-16T23:44:12.404588Z","iopub.status.idle":"2022-01-16T23:44:16.963372Z","shell.execute_reply.started":"2022-01-16T23:44:12.404557Z","shell.execute_reply":"2022-01-16T23:44:16.962431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### The middle of the month usually presents less sales. The peak at the end may be influenced by seasonal peaks (end of year).","metadata":{}},{"cell_type":"markdown","source":"##### And we completed by inspecting at a day of the week level:","metadata":{}},{"cell_type":"code","source":"for product in ['Kaggle Mug', 'Kaggle Hat', 'Kaggle Sticker']:\n    print(f\"\\n--- {product} ---\\n\")\n    fig = plt.figure(figsize=(20, 10), dpi=100)\n    fig.subplots_adjust(hspace=0.25)\n    for i, store in enumerate(['KaggleMart', 'KaggleRama']):\n        for j, country in enumerate(['Finland', 'Norway', 'Sweden']):\n            ax = fig.add_subplot(2, 3, (i*3+j+1))\n            selection = (train['country']==country)&(train['store']==store)&(train['product']==product)\n            selected = train[selection]\n            for year in [2015, 2016, 2017, 2018]:\n                selected[selected.year==year].set_index('date').groupby('dayofweek')['num_sold'].sum().plot(ax=ax, label=year)\n            ax.set_title(f\"{country}:{store}\")\n            ax.legend()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-16T23:44:16.964847Z","iopub.execute_input":"2022-01-16T23:44:16.965105Z","iopub.status.idle":"2022-01-16T23:44:21.453314Z","shell.execute_reply.started":"2022-01-16T23:44:16.965067Z","shell.execute_reply":"2022-01-16T23:44:21.45227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Friday and the week-end are the best days, but Sundays are not always at the same level as Saturdays (it depends on the year - why?).","metadata":{}},{"cell_type":"markdown","source":"##### Based on the information we got we now proceed to feature engineering and to enrich the data (using festivities and GDP data).","metadata":{}},{"cell_type":"code","source":"festivities = pd.read_csv(\"../input/festivities-in-finland-norway-sweden-tsp-0122/nordic_holidays.csv\",\n                          parse_dates=['date'],\n                          usecols=['date', 'country', 'holiday'])","metadata":{"execution":{"iopub.status.busy":"2022-01-16T23:44:21.454824Z","iopub.execute_input":"2022-01-16T23:44:21.455193Z","iopub.status.idle":"2022-01-16T23:44:21.473025Z","shell.execute_reply.started":"2022-01-16T23:44:21.45514Z","shell.execute_reply":"2022-01-16T23:44:21.472271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gdp = pd.read_csv(\"../input/gdp-20152019-finland-norway-and-sweden/GDP_data_2015_to_2019_Finland_Norway_Sweden.csv\")\ngdp = np.concatenate([gdp[['year', 'GDP_Finland']].values, \n                      gdp[['year', 'GDP_Norway']].values, \n                      gdp[['year', 'GDP_Sweden']].values])\ngdp = pd.DataFrame(gdp, columns=['year', 'gdp'])\ngdp['country'] = ['Finland']*5 + ['Norway']*5 +['Sweden']*5","metadata":{"execution":{"iopub.status.busy":"2022-01-16T23:44:21.474359Z","iopub.execute_input":"2022-01-16T23:44:21.47461Z","iopub.status.idle":"2022-01-16T23:44:21.493326Z","shell.execute_reply.started":"2022-01-16T23:44:21.47458Z","shell.execute_reply":"2022-01-16T23:44:21.492488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### We now process the data and scale it. Since EDA revealed how the different characteristics of the series are mostly main effects (country and store), we focus on finding the way to model the interaction between products and time.","metadata":{}},{"cell_type":"code","source":"train['product'].unique(), train['store'].unique()","metadata":{"execution":{"iopub.status.busy":"2022-01-16T23:44:21.495759Z","iopub.execute_input":"2022-01-16T23:44:21.496035Z","iopub.status.idle":"2022-01-16T23:44:21.507805Z","shell.execute_reply.started":"2022-01-16T23:44:21.496001Z","shell.execute_reply":"2022-01-16T23:44:21.506886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n\ndef process_data(df):\n    \n    processed = dict()\n    processed['row_id'] = df['row_id']\n    \n    print(\"creating dummies for main effects of time, country, store and product\")\n    to_dummies = ['country', 'store', 'product', 'month', 'week', 'day', 'dayofweek']\n    for feat in to_dummies:\n        tmp = pd.get_dummies(df[feat])\n        for col in tmp.columns:\n            processed[feat+'_'+str(col)] = tmp[col]\n    \n    print(\"creating dummies with 7 gg halo effect for Nordic holidays\")\n    tmp = pd.get_dummies(\n        df.merge(festivities, on=['date', 'country'], how='left').sort_values('row_id')['holiday'])\n    for col in tmp.columns:\n            peak = tmp[col].values + tmp[col].rolling(7).mean().fillna(0).values\n            processed['holiday_'+str(col)] =  peak\n    \n    print(\"creating interactions\")\n    high_lvl_interactions = [\n        ['country', 'product', 'month'],\n        ['country', 'product', 'week'],\n        ['country', 'store', 'week'],\n        ['country', 'product', 'month', 'day'],\n        ['country', 'product', 'month', 'dayofweek'],\n    ]\n    for sel in high_lvl_interactions:\n        tmp = pd.get_dummies(df[sel].apply(lambda row: '_'.join(row.values.astype(str)), axis=1))\n        for col in tmp.columns:\n            processed[col] = tmp[col]\n            \n    print(\"modelling time as continuous per each country\")\n    for country in ['Finland', 'Norway', 'Sweden']:\n        processed[country + '_prog'] = ((df.row_id // 18) + 1) * (df['country']==country).astype(int)\n        processed[country + '_prog^2'] = (processed[country + '_prog']**2)\n        processed[country + '_prog^3'] = (processed[country + '_prog']**3)\n    \n    print(\"adding sin/cos of day of year\")\n    dayofyear = df.date.dt.dayofyear\n    steps = 4\n    for k in range(1, 32, steps):\n        processed[f'sin{k}'] = np.sin(dayofyear / 365 * 2 * np.pi * k)\n        processed[f'cos{k}'] = np.cos(dayofyear / 365 * 2 * np.pi * k)\n    \n    print(\"adding log(gdp)\")\n    gdp_countries = np.log1p(df.merge(gdp, on=['country', 'year'], how='left')['gdp'].values)\n    for country in ['Finland', 'Norway', 'Sweden']:\n        processed['gdp_'+ country] = gdp_countries * (df['country']==country).astype(int)\n            \n    print(f\"completed processing {len(processed)-1} features\")\n    \n    values = list()\n    columns = list()\n    for key, value in processed.items():\n        values.append(np.array(value))\n        columns.append(key)\n        \n    values = np.array(values).T        \n    processed = pd.DataFrame(values, columns=columns)\n    \n    print(\"resorting row ids\")\n    processed = processed.sort_values('row_id').set_index('row_id')\n    return processed\n\ndef process_target(df):\n    target = pd.DataFrame({'row_id':df['row_id'], 'num_sold':df['num_sold']})\n    target = target.sort_values('row_id').set_index('row_id')\n    return target\n\ntrain_test = process_data(train.append(test))\n\nprocessed_train = train_test.iloc[:len(train)].copy()\nprocessed_test = train_test.iloc[len(train):].copy()\n\ntarget = np.ravel(process_target(train))","metadata":{"execution":{"iopub.status.busy":"2022-01-17T00:04:09.834204Z","iopub.execute_input":"2022-01-17T00:04:09.834532Z","iopub.status.idle":"2022-01-17T00:04:18.118626Z","shell.execute_reply.started":"2022-01-17T00:04:09.834499Z","shell.execute_reply":"2022-01-17T00:04:18.117601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Since EDA demonstrated that there is some kind of yearly change, we overweight more recent observations.","metadata":{}},{"cell_type":"code","source":"def weighting(df, weights):\n    return df.year.replace(weights).values\n    \nweights = weighting(train, {2015:0.125, 2016:0.25, 2017:0.5, 2018:1})","metadata":{"execution":{"iopub.status.busy":"2022-01-07T10:43:20.229601Z","iopub.execute_input":"2022-01-07T10:43:20.229861Z","iopub.status.idle":"2022-01-07T10:43:20.237054Z","shell.execute_reply.started":"2022-01-07T10:43:20.229809Z","shell.execute_reply":"2022-01-07T10:43:20.236268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"processed_train.shape, train.shape, processed_test.shape, test.shape","metadata":{"execution":{"iopub.status.busy":"2022-01-07T10:43:20.238353Z","iopub.execute_input":"2022-01-07T10:43:20.238591Z","iopub.status.idle":"2022-01-07T10:43:20.795721Z","shell.execute_reply.started":"2022-01-07T10:43:20.238562Z","shell.execute_reply":"2022-01-07T10:43:20.794588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### We prepare all the evaluation measures, both at an aggregate level, with exp transformation and at an individual cases level (for error analysis)","metadata":{}},{"cell_type":"code","source":"def SMAPE(y_true, y_pred):\n    # From https://www.kaggle.com/cpmpml/smape-weirdness\n    denominator = (y_true + np.abs(y_pred)) / 200.0\n    diff = np.abs(y_true - y_pred) / denominator\n    diff[denominator == 0] = 0.0\n    return np.mean(diff)\n\ndef SMAPE_exp(y_true, y_pred):\n    y_true = np.exp(y_true)\n    y_pred = np.exp(y_pred)\n    denominator = (y_true + np.abs(y_pred)) / 200.0\n    diff = np.abs(y_true - y_pred) / denominator\n    diff[denominator == 0] = 0.0\n    return np.mean(diff)\n\ndef SMAPE_err(y_true, y_pred):\n    # From https://www.kaggle.com/cpmpml/smape-weirdness\n    denominator = (y_true + np.abs(y_pred)) / 200.0\n    diff = np.abs(y_true - y_pred) / denominator\n    diff[denominator == 0] = 0.0\n    return diff","metadata":{"execution":{"iopub.status.busy":"2022-01-07T10:43:20.797641Z","iopub.execute_input":"2022-01-07T10:43:20.798045Z","iopub.status.idle":"2022-01-07T10:43:20.80829Z","shell.execute_reply.started":"2022-01-07T10:43:20.797996Z","shell.execute_reply":"2022-01-07T10:43:20.807241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### We try modelling both an additive and a multiplicative predictor (for multiplicative you just need a log transoformation of the target because exponentiation of the sum of logs of the terms correspond to multiplications of the terms)","metadata":{}},{"cell_type":"code","source":"# due to float calculations the computation is approximated\na = 5\nb = 7\nprint(a * b) # pure multiplicative\nprint(np.exp(np.log(a) + np.log(b))) # multiplicative made additive by log","metadata":{"execution":{"iopub.status.busy":"2022-01-07T10:43:20.812212Z","iopub.execute_input":"2022-01-07T10:43:20.81267Z","iopub.status.idle":"2022-01-07T10:43:20.821804Z","shell.execute_reply.started":"2022-01-07T10:43:20.812632Z","shell.execute_reply":"2022-01-07T10:43:20.821132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n\naddictive_model = LinearRegression().fit(processed_train, target, sample_weight=weights)\naddictive_fit = addictive_model.predict(processed_train.values)\nprint(f\"addictive fit: {SMAPE(y_true=target, y_pred=addictive_fit): 0.3f}\")","metadata":{"execution":{"iopub.status.busy":"2022-01-07T10:43:20.823152Z","iopub.execute_input":"2022-01-07T10:43:20.823399Z","iopub.status.idle":"2022-01-07T10:44:20.359798Z","shell.execute_reply.started":"2022-01-07T10:43:20.823368Z","shell.execute_reply":"2022-01-07T10:44:20.358737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"multiplicative_model = LinearRegression().fit(processed_train, np.log(target), sample_weight=weights)\nmultiplicative_fit = np.exp(multiplicative_model.predict(processed_train.values))\nprint(f\"multiplicative fit: {SMAPE(y_true=target, y_pred=multiplicative_fit):0.3f}\")","metadata":{"execution":{"iopub.status.busy":"2022-01-07T10:44:20.368717Z","iopub.execute_input":"2022-01-07T10:44:20.376621Z","iopub.status.idle":"2022-01-07T10:45:17.07193Z","shell.execute_reply.started":"2022-01-07T10:44:20.376532Z","shell.execute_reply":"2022-01-07T10:45:17.064489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### It seems that the multiplicative model is better. We now use it to predict on the test set.","metadata":{}},{"cell_type":"code","source":"def selective_rounding(preds, lower=0.3, upper=0.7):\n    # selective rounding\n    dec = preds % 1\n    to_round = (dec<=lower)|(dec>=upper)\n    preds[to_round] = np.round(preds[to_round])\n    return preds","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = LinearRegression().fit(processed_train, np.log(target), sample_weight=weights)\nsubmission = pd.read_csv(\"../input/tabular-playground-series-jan-2022/sample_submission.csv\")\npreds = np.exp(model.predict(processed_test.values))\n\n# rounding\npreds = selective_rounding(preds, lower=0.3, upper=0.7)\n\nsubmission.num_sold = preds\nsubmission.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-07T10:45:17.073493Z","iopub.execute_input":"2022-01-07T10:45:17.07422Z","iopub.status.idle":"2022-01-07T10:46:12.248793Z","shell.execute_reply.started":"2022-01-07T10:45:17.074171Z","shell.execute_reply":"2022-01-07T10:46:12.247673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### For testing purposes we refit the model only on years 2015-2017 using 2018 as an hold-out test.","metadata":{}},{"cell_type":"code","source":"log_target = np.log(target)\ntrain_set = list(train.index[train.date<'2018-01-01'])\nval_set = list(train.index[train.date>='2018-01-01'])\nmodel = LinearRegression()\n\nmodel.fit(processed_train.iloc[train_set], log_target[train_set], sample_weight=weights[train_set])\npreds = np.exp(model.predict(processed_train.iloc[val_set].values))\n\n# rounding\npreds = selective_rounding(preds, lower=0.3, upper=0.7)\n\nsmape = SMAPE(y_true=target[val_set], y_pred=preds)\nprint(f\"hold-out smape: {smape}\")","metadata":{"execution":{"iopub.status.busy":"2022-01-07T10:46:12.250506Z","iopub.execute_input":"2022-01-07T10:46:12.260094Z","iopub.status.idle":"2022-01-07T10:46:59.222818Z","shell.execute_reply.started":"2022-01-07T10:46:12.260026Z","shell.execute_reply":"2022-01-07T10:46:59.213859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### If you liked the notebook, consider to upvote, thank you and happy Kaggling!","metadata":{}}]}