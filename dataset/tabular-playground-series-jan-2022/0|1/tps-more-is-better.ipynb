{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Introduction\n\n\nData preprocessing and most part of code is copied from https://www.kaggle.com/ambrosm/tpsjan22-03-linear-model/notebook\n\nThis notebbok\n* Explores and compares several models in sklean library\n* Creates ensemble that has no overfitting risk since test data is only used for predictions\n\nUpdates\n* V2: models' hyper-paramters are tuned to get the largest validation score","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport pickle\nimport itertools\nimport gc\nimport math\nimport matplotlib.pyplot as plt\nimport dateutil.easter as easter\nfrom matplotlib.ticker import MaxNLocator, FormatStrFormatter, PercentFormatter\nfrom datetime import datetime, date, timedelta\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GroupKFold, train_test_split\nfrom sklearn.linear_model import LinearRegression, HuberRegressor, Ridge, Lasso, ElasticNet, BayesianRidge, PassiveAggressiveRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.neural_network import MLPRegressor\nnp.random.seed(42)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T16:28:08.105682Z","iopub.execute_input":"2022-01-13T16:28:08.106299Z","iopub.status.idle":"2022-01-13T16:28:09.331446Z","shell.execute_reply.started":"2022-01-13T16:28:08.106198Z","shell.execute_reply":"2022-01-13T16:28:09.330781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Preparation","metadata":{}},{"cell_type":"code","source":"original_train_df = pd.read_csv('../input/tabular-playground-series-jan-2022/train.csv')\noriginal_test_df = pd.read_csv('../input/tabular-playground-series-jan-2022/test.csv')\ngdp_df = pd.read_csv('../input/gdp-20152019-finland-norway-and-sweden/GDP_data_2015_to_2019_Finland_Norway_Sweden.csv')\n\ngdp_df.set_index('year', inplace=True)\n\n# The dates are read as strings and must be converted\nfor df in [original_train_df, original_test_df]:\n    df['date'] = pd.to_datetime(df.date)\noriginal_train_df.head(2)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T16:28:09.333237Z","iopub.execute_input":"2022-01-13T16:28:09.333723Z","iopub.status.idle":"2022-01-13T16:28:09.439171Z","shell.execute_reply.started":"2022-01-13T16:28:09.333676Z","shell.execute_reply":"2022-01-13T16:28:09.438597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Feature engineering\ndef engineer(df):\n    \"\"\"Return a new dataframe with the engineered features\"\"\"\n    \n    def get_gdp(row):\n        country = 'GDP_' + row.country\n        return gdp_df.loc[row.date.year, country]\n        \n    new_df = pd.DataFrame({'gdp': np.log(df.apply(get_gdp, axis=1)),\n                           'wd4': df.date.dt.weekday == 4, # Friday\n                           'wd56': df.date.dt.weekday >= 5, # Saturday and Sunday\n                          })\n\n    # One-hot encoding (no need to encode the last categories)\n    for country in ['Finland', 'Norway']:\n        new_df[country] = df.country == country\n    new_df['KaggleRama'] = df.store == 'KaggleRama'\n    for product in ['Kaggle Mug', 'Kaggle Hat']:\n        new_df[product] = df['product'] == product\n        \n    # Seasonal variations (Fourier series)\n    # The three products have different seasonal patterns\n    dayofyear = df.date.dt.dayofyear\n    for k in range(1, 3):\n        new_df[f'sin{k}'] = np.sin(dayofyear / 365 * 2 * math.pi * k)\n        new_df[f'cos{k}'] = np.cos(dayofyear / 365 * 2 * math.pi * k)\n        new_df[f'mug_sin{k}'] = new_df[f'sin{k}'] * new_df['Kaggle Mug']\n        new_df[f'mug_cos{k}'] = new_df[f'cos{k}'] * new_df['Kaggle Mug']\n        new_df[f'hat_sin{k}'] = new_df[f'sin{k}'] * new_df['Kaggle Hat']\n        new_df[f'hat_cos{k}'] = new_df[f'cos{k}'] * new_df['Kaggle Hat']\n\n    return new_df\n\ntrain_df = engineer(original_train_df)\ntrain_df['date'] = original_train_df.date\ntrain_df['num_sold'] = original_train_df.num_sold.astype(np.float32)\ntest_df = engineer(original_test_df)\n\nfeatures = test_df.columns\n\nfor df in [train_df, test_df]:\n    df[features] = df[features].astype(np.float32)\nprint(list(features))","metadata":{"execution":{"iopub.status.busy":"2022-01-13T16:28:09.440205Z","iopub.execute_input":"2022-01-13T16:28:09.440509Z","iopub.status.idle":"2022-01-13T16:28:10.664375Z","shell.execute_reply.started":"2022-01-13T16:28:09.440481Z","shell.execute_reply":"2022-01-13T16:28:10.663689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Feature engineering for holidays\ndef engineer_more(df):\n    \"\"\"Return a new dataframe with more engineered features\"\"\"\n    new_df = engineer(df)\n\n    # End of year\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({f\"dec{d}\":\n                                      (df.date.dt.month == 12) & (df.date.dt.day == d)\n                                      for d in range(24, 32)}),\n                        pd.DataFrame({f\"n-dec{d}\":\n                                      (df.date.dt.month == 12) & (df.date.dt.day == d) & (df.country == 'Norway')\n                                      for d in range(24, 32)}),\n                        pd.DataFrame({f\"f-jan{d}\":\n                                      (df.date.dt.month == 1) & (df.date.dt.day == d) & (df.country == 'Finland')\n                                      for d in range(1, 14)}),\n                        pd.DataFrame({f\"jan{d}\":\n                                      (df.date.dt.month == 1) & (df.date.dt.day == d) & (df.country == 'Norway')\n                                      for d in range(1, 10)}),\n                        pd.DataFrame({f\"s-jan{d}\":\n                                      (df.date.dt.month == 1) & (df.date.dt.day == d) & (df.country == 'Sweden')\n                                      for d in range(1, 15)})],\n                       axis=1)\n    \n    # May\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({f\"may{d}\":\n                                      (df.date.dt.month == 5) & (df.date.dt.day == d) \n                                      for d in list(range(1, 10))}), #  + list(range(17, 25))\n                        pd.DataFrame({f\"may{d}\":\n                                      (df.date.dt.month == 5) & (df.date.dt.day == d) & (df.country == 'Norway')\n                                      for d in list(range(19, 26))})],\n                       axis=1)\n    \n    # June and July\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({f\"june{d}\":\n                                      (df.date.dt.month == 6) & (df.date.dt.day == d) & (df.country == 'Sweden')\n                                      for d in list(range(8, 14))}),\n                        #pd.DataFrame({f\"june{d}\":\n                        #              (df.date.dt.month == 6) & (df.date.dt.day == d) & (df.country == 'Norway')\n                        #              for d in list(range(22, 31))}),\n                        #pd.DataFrame({f\"july{d}\":\n                        #              (df.date.dt.month == 7) & (df.date.dt.day == d) & (df.country == 'Norway')\n                        #              for d in list(range(1, 3))})],\n                       ],\n                       axis=1)\n    \n    # Last Wednesday of June\n    wed_june_date = df.date.dt.year.map({2015: pd.Timestamp(('2015-06-24')),\n                                         2016: pd.Timestamp(('2016-06-29')),\n                                         2017: pd.Timestamp(('2017-06-28')),\n                                         2018: pd.Timestamp(('2018-06-27')),\n                                         2019: pd.Timestamp(('2019-06-26'))})\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({f\"wed_june{d}\": \n                                      (df.date - wed_june_date == np.timedelta64(d, \"D\")) & (df.country != 'Norway')\n                                      for d in list(range(-4, 6))})],\n                       axis=1)\n    \n    # First Sunday of November\n    sun_nov_date = df.date.dt.year.map({2015: pd.Timestamp(('2015-11-1')),\n                                         2016: pd.Timestamp(('2016-11-6')),\n                                         2017: pd.Timestamp(('2017-11-5')),\n                                         2018: pd.Timestamp(('2018-11-4')),\n                                         2019: pd.Timestamp(('2019-11-3'))})\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({f\"sun_nov{d}\": \n                                      (df.date - sun_nov_date == np.timedelta64(d, \"D\")) & (df.country != 'Norway')\n                                      for d in list(range(0, 9))})],\n                       axis=1)\n    \n    # First half of December (Independence Day of Finland, 6th of December)\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({f\"dec{d}\":\n                                      (df.date.dt.month == 12) & (df.date.dt.day == d) & (df.country == 'Finland')\n                                      for d in list(range(6, 14))})],\n                       axis=1)\n\n    # Easter\n    easter_date = df.date.apply(lambda date: pd.Timestamp(easter.easter(date.year)))\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({f\"easter{d}\": \n                                      (df.date - easter_date == np.timedelta64(d, \"D\"))\n                                      for d in list(range(-2, 11)) + list(range(40, 48)) + list(range(50, 59))})],\n                       axis=1)\n    \n    return new_df.astype(np.float32)\n\ntrain_df = engineer_more(original_train_df)\ntrain_df['date'] = original_train_df.date\ntrain_df['num_sold'] = original_train_df.num_sold.astype(np.float32)\ntest_df = engineer_more(original_test_df)\n\nfeatures = list(test_df.columns)\nprint(list(features))\n\n# prepare single scaler\nscaler = StandardScaler()\nX = scaler.fit_transform(train_df[features])","metadata":{"execution":{"iopub.status.busy":"2022-01-13T16:28:10.666735Z","iopub.execute_input":"2022-01-13T16:28:10.667072Z","iopub.status.idle":"2022-01-13T16:28:13.266894Z","shell.execute_reply.started":"2022-01-13T16:28:10.667028Z","shell.execute_reply":"2022-01-13T16:28:13.265985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's take 10% of train dataset that will be used for ensemble","metadata":{}},{"cell_type":"code","source":"mask = np.random.rand(len(train_df)) < 0.9\nvalid_df = train_df[~mask]\ntrain_df = train_df[mask]\nvalid_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-13T16:28:13.268235Z","iopub.execute_input":"2022-01-13T16:28:13.268454Z","iopub.status.idle":"2022-01-13T16:28:13.309093Z","shell.execute_reply.started":"2022-01-13T16:28:13.268419Z","shell.execute_reply":"2022-01-13T16:28:13.308327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len((valid_df.columns))","metadata":{"execution":{"iopub.status.busy":"2022-01-13T16:28:13.310206Z","iopub.execute_input":"2022-01-13T16:28:13.310431Z","iopub.status.idle":"2022-01-13T16:28:13.316288Z","shell.execute_reply.started":"2022-01-13T16:28:13.310403Z","shell.execute_reply":"2022-01-13T16:28:13.315556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Models\n\nI pick morels from sklearn libraries that solve linear regression tasks. \nYou can find more information about them at https://scikit-learn.org/stable/supervised_learning.html\n\nNote that I don't tune hyperparameters, so models perform better if more time was spent on them","metadata":{}},{"cell_type":"code","source":"linear = LinearRegression()                                              # simple one\nhuber = HuberRegressor()                                                 # uses more complicated loss that makes it robust \nridge = Ridge()                                                          # avoids overfitting because of weight addition into loss function. So weights are kept small\nlasso = Lasso(max_iter=200, alpha=0.2)                                   # has more complicated reguilarization\nelastic_net = ElasticNet(max_iter=200)                                   # combination of lasso and ridge\nbayesian = BayesianRidge()                                               # statistical analysis is understaken in this model. It is actually very interesting one\n\nperceptron = MLPRegressor(hidden_layer_sizes=(128, 32, 16),              # several fully connected layers with hidden function. It may not be that useful in linearized data\n                    max_iter=200,\n                    activation='tanh', # tanh performs better than relu\n                    solver='adam')\n# someting more complex\npar = PassiveAggressiveRegressor()                                       \ngbr = GradientBoostingRegressor()","metadata":{"execution":{"iopub.status.busy":"2022-01-13T16:28:13.317416Z","iopub.execute_input":"2022-01-13T16:28:13.317716Z","iopub.status.idle":"2022-01-13T16:28:13.327747Z","shell.execute_reply.started":"2022-01-13T16:28:13.317655Z","shell.execute_reply":"2022-01-13T16:28:13.327033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train\n\nTrain all of them independently on whole train ataset and see its score on the validation data","metadata":{}},{"cell_type":"code","source":"def smape_loss(y_true, y_pred):\n    \"\"\"SMAPE Loss\"\"\"\n    return np.abs(y_true - y_pred) / (y_true + np.abs(y_pred)) * 200\n\ndef train_model(model, scaler, X_tr, X_va=None):\n    start_time = datetime.now()\n    \n    X = X_tr[features]\n    y = X_tr.num_sold.values.reshape(-1, 1)\n    \n    X = scaler.transform(X)\n\n    model.fit(X, np.log(y).ravel())\n    \n    if X_va is not None:\n        X_v = X_va[features]\n        X_v = scaler.transform(X_v)\n        y_v = X_va.num_sold.values.reshape(-1, 1)\n\n\n        y_v_pred = np.exp(model.predict(X_v)).reshape(-1, 1)\n        \n        smape_before_correction = np.mean(smape_loss(y_v, y_v_pred))\n\n        smape = np.mean(smape_loss(y_v, y_v_pred))\n        print(f\"Model {model} | {str(datetime.now() - start_time)[-12:-7]}\"\n              f\" | SMAPE: {smape:.5f}\")\n        \n        plt.figure(figsize=(10, 10))\n        plt.scatter(y_v, y_v_pred, s=1, color='r')\n        plt.plot([plt.xlim()[0], plt.xlim()[1]], [plt.xlim()[0], plt.xlim()[1]], '--', color='k')\n        plt.gca().set_aspect('equal')\n        plt.xlabel('y_true')\n        plt.ylabel('y_pred')\n        plt.title('OOF Predictions')\n        plt.show()\n        \n    return model","metadata":{"execution":{"iopub.status.busy":"2022-01-13T16:28:13.328941Z","iopub.execute_input":"2022-01-13T16:28:13.329618Z","iopub.status.idle":"2022-01-13T16:28:13.344477Z","shell.execute_reply.started":"2022-01-13T16:28:13.329576Z","shell.execute_reply":"2022-01-13T16:28:13.343606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"linear = train_model(linear, scaler, train_df, valid_df)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T16:28:13.34593Z","iopub.execute_input":"2022-01-13T16:28:13.346367Z","iopub.status.idle":"2022-01-13T16:28:13.910616Z","shell.execute_reply.started":"2022-01-13T16:28:13.346334Z","shell.execute_reply":"2022-01-13T16:28:13.909768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"huber = train_model(huber, scaler, train_df, valid_df)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T16:28:13.912571Z","iopub.execute_input":"2022-01-13T16:28:13.912817Z","iopub.status.idle":"2022-01-13T16:28:16.619028Z","shell.execute_reply.started":"2022-01-13T16:28:13.912781Z","shell.execute_reply":"2022-01-13T16:28:16.61745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ridge = train_model(ridge, scaler, train_df, valid_df)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T16:28:16.620515Z","iopub.execute_input":"2022-01-13T16:28:16.621008Z","iopub.status.idle":"2022-01-13T16:28:16.946361Z","shell.execute_reply.started":"2022-01-13T16:28:16.620964Z","shell.execute_reply":"2022-01-13T16:28:16.944092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lasso = Lasso(max_iter=200, alpha=0.000002)\nlasso = train_model(lasso, scaler, train_df, valid_df)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T16:28:16.947894Z","iopub.execute_input":"2022-01-13T16:28:16.948216Z","iopub.status.idle":"2022-01-13T16:28:17.910408Z","shell.execute_reply.started":"2022-01-13T16:28:16.948179Z","shell.execute_reply":"2022-01-13T16:28:17.909568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"elastic_net = ElasticNet(max_iter=200, alpha=0.0000002)\nelastic_net = train_model(elastic_net, scaler, train_df, valid_df)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T16:28:17.911882Z","iopub.execute_input":"2022-01-13T16:28:17.91213Z","iopub.status.idle":"2022-01-13T16:28:18.810959Z","shell.execute_reply.started":"2022-01-13T16:28:17.9121Z","shell.execute_reply":"2022-01-13T16:28:18.809951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bayesian = train_model(bayesian, scaler, train_df, valid_df)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T16:28:18.81216Z","iopub.execute_input":"2022-01-13T16:28:18.812406Z","iopub.status.idle":"2022-01-13T16:28:19.461268Z","shell.execute_reply.started":"2022-01-13T16:28:18.812375Z","shell.execute_reply":"2022-01-13T16:28:19.46007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"perceptron = MLPRegressor(hidden_layer_sizes=(64, 16, 8),              # several fully connected layers with hidden function. It may not be that useful in linearized data\n                    max_iter=100,\n                    activation='tanh', \n                    solver='adam')\nperceptron = train_model(perceptron, scaler, train_df, valid_df)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T16:28:19.462775Z","iopub.execute_input":"2022-01-13T16:28:19.463175Z","iopub.status.idle":"2022-01-13T16:28:44.212073Z","shell.execute_reply.started":"2022-01-13T16:28:19.463131Z","shell.execute_reply":"2022-01-13T16:28:44.210096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"par = PassiveAggressiveRegressor(early_stopping=True)                                       \n\npar = train_model(par, scaler, train_df, valid_df)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T16:28:44.213068Z","iopub.execute_input":"2022-01-13T16:28:44.213267Z","iopub.status.idle":"2022-01-13T16:28:44.717052Z","shell.execute_reply.started":"2022-01-13T16:28:44.213241Z","shell.execute_reply":"2022-01-13T16:28:44.71561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gbr = GradientBoostingRegressor(max_depth=7)\ngbr = train_model(gbr, scaler, train_df, valid_df)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T16:28:44.71834Z","iopub.execute_input":"2022-01-13T16:28:44.719166Z","iopub.status.idle":"2022-01-13T16:29:02.280027Z","shell.execute_reply.started":"2022-01-13T16:28:44.71912Z","shell.execute_reply":"2022-01-13T16:29:02.279031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Observations\nSimple regressions perform well while multilayer perceptron and other complicated models have lower score. \nComplexity of this problem is too low for complex models to work. Probably it is better to break the problem by using more feature engineering or autoencoder to make data complex\nInteresting that Lasso and Elastic Net failed completely. May be they are required to be tuned or the problem is that they want to acees the inter weights of model. I don't know\n\nUpdate: Hyperparameters needed to be tuned well to achieve good results","metadata":{}},{"cell_type":"markdown","source":"## Ensemble\nNow let's each model predict on validation dataset and find the corresponding weights on the final encemble using this data. \nInstead of manual set, I will use lasso regression to fiind the weighs. Restrictions are that regression is zero biased and weights are positive. Lasso type is used bevause only this model in library has the parameter of only positive weights","metadata":{}},{"cell_type":"code","source":"def predict(model, scaler, df):\n    pred_list = []\n    pred_list.append(np.exp(model.predict(scaler.transform(df[features]))))\n    return pred_list","metadata":{"execution":{"iopub.status.busy":"2022-01-13T13:44:17.321828Z","iopub.execute_input":"2022-01-13T13:44:17.32242Z","iopub.status.idle":"2022-01-13T13:44:17.327897Z","shell.execute_reply.started":"2022-01-13T13:44:17.322372Z","shell.execute_reply":"2022-01-13T13:44:17.327132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"linear_valid = predict(linear, scaler, valid_df)\nhuber_valid = predict(huber, scaler, valid_df)\nridge_valid = predict(ridge, scaler, valid_df)\nlasso_valid = predict(lasso, scaler, valid_df)\nelastic_net_valid = predict(elastic_net, scaler, valid_df)\nbayesian_valid = predict(bayesian, scaler, valid_df)\nperceptron_valid = predict(perceptron, scaler, valid_df)\npar_valid = predict(par, scaler, valid_df)\ngbr_valid = predict(gbr, scaler, valid_df)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T13:44:19.795749Z","iopub.execute_input":"2022-01-13T13:44:19.796065Z","iopub.status.idle":"2022-01-13T13:44:19.952503Z","shell.execute_reply.started":"2022-01-13T13:44:19.796034Z","shell.execute_reply":"2022-01-13T13:44:19.951523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"names = ['linear', 'huber', 'ridge', 'lasso', 'elastic_net', 'bayesian', 'perceptron', 'par', 'gbr', 'actual']\n#names = ['linear', 'huber', 'ridge', 'lasso', 'elastic_net', 'bayesian', 'actual']\nactual = valid_df['num_sold'].values\ndata = pd.DataFrame(list(zip(linear_valid[0], huber_valid[0], ridge_valid[0], lasso_valid[0], elastic_net_valid[0], bayesian_valid[0], perceptron_valid[0], par_valid[0], gbr_valid[0], actual)), columns=names)\n#data = pd.DataFrame(list(zip(linear_valid[0], huber_valid[0], ridge_valid[0], lasso_valid[0], elastic_net_valid[0], bayesian_valid[0], actual)), columns=names)\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-13T13:55:23.132569Z","iopub.execute_input":"2022-01-13T13:55:23.133156Z","iopub.status.idle":"2022-01-13T13:55:23.162481Z","shell.execute_reply.started":"2022-01-13T13:55:23.133113Z","shell.execute_reply":"2022-01-13T13:55:23.161915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train ensemble\nensemble = Lasso(fit_intercept=False, positive=True, alpha=0)\nensemble.fit(data[names[:-1]], data['actual'])\nprint('Weight of ensemble are', ensemble.coef_)\nensemble.coef_.sum()","metadata":{"execution":{"iopub.status.busy":"2022-01-13T13:55:28.280891Z","iopub.execute_input":"2022-01-13T13:55:28.281358Z","iopub.status.idle":"2022-01-13T13:55:28.298714Z","shell.execute_reply.started":"2022-01-13T13:55:28.281326Z","shell.execute_reply":"2022-01-13T13:55:28.297635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"PassiveAggressiveRegressor has nonzero weight. Means it is still good even if too complex","metadata":{}},{"cell_type":"code","source":"linear_test = predict(linear, scaler, test_df)\nhuber_test = predict(huber, scaler, test_df)\nridge_test = predict(ridge, scaler, test_df)\nlasso_test = predict(lasso, scaler, test_df)\nelastic_net_test = predict(elastic_net, scaler, test_df)\nbayesian_test = predict(bayesian, scaler, test_df)\nperceptron_test = predict(perceptron, scaler, test_df)\npar_test = predict(par, scaler, test_df)\ngbr_test = predict(gbr, scaler, test_df)\n\n\ndata = pd.DataFrame(list(zip(linear_test[0], huber_test[0], ridge_test[0], lasso_test[0], elastic_net_test[0], bayesian_test[0], perceptron_test[0], par_test[0], gbr_test[0])), columns=names[:-1])\n#data = pd.DataFrame(list(zip(linear_test[0], huber_test[0], ridge_test[0], lasso_test[0], elastic_net_test[0], bayesian_test[0])), columns=names[:-1])\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-13T13:55:47.316421Z","iopub.execute_input":"2022-01-13T13:55:47.31678Z","iopub.status.idle":"2022-01-13T13:55:47.656388Z","shell.execute_reply.started":"2022-01-13T13:55:47.316743Z","shell.execute_reply":"2022-01-13T13:55:47.655555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predict\ntest_prediction = ensemble.predict(data)\n\nsub = original_test_df[['row_id']].copy()\nsub['num_sold'] = test_prediction\nsub.to_csv('submission.csv', index=False)\nsub.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-13T13:56:05.836433Z","iopub.execute_input":"2022-01-13T13:56:05.836735Z","iopub.status.idle":"2022-01-13T13:56:05.906941Z","shell.execute_reply.started":"2022-01-13T13:56:05.836698Z","shell.execute_reply":"2022-01-13T13:56:05.905958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the distribution of the test predictions\nplt.figure(figsize=(16,3))\nplt.hist(train_df['num_sold'], bins=np.linspace(0, 3000, 201),\n         density=True, label='Training')\nplt.hist(sub['num_sold'], bins=np.linspace(0, 3000, 201),\n         density=True, rwidth=0.5, label='Test predictions')\nplt.xlabel('num_sold')\nplt.ylabel('Frequency')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-13T13:56:07.403482Z","iopub.execute_input":"2022-01-13T13:56:07.403763Z","iopub.status.idle":"2022-01-13T13:56:08.704545Z","shell.execute_reply.started":"2022-01-13T13:56:07.403734Z","shell.execute_reply":"2022-01-13T13:56:08.703671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_rounded = sub.copy()\nsub_rounded['num_sold'] = (sub_rounded['num_sold']).round() # cheating\nsub_rounded.to_csv('submission_rounded.csv', index=False)\nsub_rounded.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-13T13:56:15.362802Z","iopub.execute_input":"2022-01-13T13:56:15.36377Z","iopub.status.idle":"2022-01-13T13:56:15.395092Z","shell.execute_reply.started":"2022-01-13T13:56:15.363729Z","shell.execute_reply":"2022-01-13T13:56:15.394211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}