{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport dateutil.easter as easter # Used to get Easter date each year, which is a significant holiday in Nordic Countries","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Load all of the datasets that will be used\ntrain = pd.read_csv('../input/tabular-playground-series-jan-2022/train.csv')\ntest = pd.read_csv('../input/tabular-playground-series-jan-2022/test.csv')\ngdp_df = pd.read_csv('../input/consumer-price-index-20152019-nordic-countries/Best_CPI.csv')\ngdp_pc_df = pd.read_csv('../input/gdp-per-capita-finland-norway-sweden-201519/GDP_per_capita_2015_to_2019_Finland_Norway_Sweden.csv')\nmacro_df = pd.read_csv('../input/macroeconomic-composite-finland-norway-sweden/macro_economic_idx.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to process the date column, including getting holidays\ndef dateProcess1(df, gdp_df):\n    # Make a bunch of columns for the dates\n    day_mon_list = []\n    mon_list = []\n    year_list = []\n\n    for k in range(len(df['date'])):\n        splt = df.iloc[k]['date'].split('-')\n        day_mon_list.append(int(splt[2]))\n        mon_list.append(int(splt[1]))\n        year_list.append(int(splt[0]) - 2015)\n    df['day_of_month'] = day_mon_list\n    df['month'] = mon_list\n    df['year'] = year_list\n    \n    # Add GDP Data\n    gdp_list = []\n    gdp_pc_list = []\n    for i in range(len(df['year'])):\n        if(df.iloc[i]['country'] == 'Finland'):\n            gdp_list.append(gdp_df.iloc[(3*df.iloc[i]['year'])]['GDP'])\n            gdp_pc_list.append(gdp_pc_df.iloc[df.iloc[i]['year']]['Finland'])\n        elif(df.iloc[i]['country'] == 'Norway'):\n            gdp_list.append(gdp_df.iloc[(3*df.iloc[i]['year']) + 1]['GDP'])\n            gdp_pc_list.append(gdp_pc_df.iloc[df.iloc[i]['year']]['Norway'])\n        elif(df.iloc[i]['country'] == 'Sweden'):\n            gdp_list.append(gdp_df.iloc[(3*df.iloc[i]['year']) + 2]['GDP'])\n            gdp_pc_list.append(gdp_pc_df.iloc[df.iloc[i]['year']]['Sweden'])\n    df['gdp_list'] = gdp_list\n    df['gdp_per_capita'] = gdp_pc_list\n   \n    # Add macro data\n    macro_list = []\n    for j in range(len(df['year'])):\n        if(df.iloc[j]['country'] == 'Finland'): \n            if(df.iloc[j]['product'] == 'Kaggle Hat'): \n                macro_list.append(macro_df.iloc[df.iloc[j]['year']]['macro_comp'])\n            elif(df.iloc[j]['product'] == 'Kaggle Mug'): \n                macro_list.append(macro_df.iloc[df.iloc[j]['year'] + 5]['macro_comp'])\n            elif(df.iloc[j]['product'] == 'Kaggle Sticker'): \n                macro_list.append(macro_df.iloc[df.iloc[j]['year'] + 10]['macro_comp'])\n        elif(df.iloc[j]['country'] == 'Sweden'): \n            if(df.iloc[j]['product'] == 'Kaggle Hat'): \n                macro_list.append(macro_df.iloc[df.iloc[j]['year'] + 15]['macro_comp'])\n            elif(df.iloc[j]['product'] == 'Kaggle Mug'): \n                macro_list.append(macro_df.iloc[df.iloc[j]['year'] + 20]['macro_comp'])\n            elif(df.iloc[j]['product'] == 'Kaggle Sticker'): \n                macro_list.append(macro_df.iloc[df.iloc[j]['year'] + 25]['macro_comp'])\n        elif(df.iloc[j]['country'] == 'Norway'): \n            if(df.iloc[j]['product'] == 'Kaggle Hat'): \n                macro_list.append(macro_df.iloc[df.iloc[j]['year'] + 30]['macro_comp'])\n            elif(df.iloc[j]['product'] == 'Kaggle Mug'): \n                macro_list.append(macro_df.iloc[df.iloc[j]['year'] + 35]['macro_comp'])\n            elif(df.iloc[j]['product'] == 'Kaggle Sticker'): \n                macro_list.append(macro_df.iloc[df.iloc[j]['year'] + 40]['macro_comp'])\n    df['macro_data'] = macro_list      \n    \n    \n    df['date'] = pd.to_datetime(df['date'])\n    df['weekend'] = df.date.dt.weekday >= 5 # Saturday and Sunday\n    df['friday'] = df.date.dt.weekday == 4 # Friday\n    df['day_of_year'] = df.date.dt.dayofyear\n    \n    # Christmas\n    xmas_date = df.date.dt.year.apply(lambda year: pd.Timestamp(str(year)+'-12-25'))\n    df['xmas_adjust'] = (df.date - xmas_date).dt.days.clip(lower=-20,upper=16).astype(float)\n          \n    # Easter\n    easter_date = df.date.apply(lambda date: pd.Timestamp(easter.easter(date.year)))\n    df['easter_adj']= (df.date - easter_date).dt.days.clip(lower =-3,upper = 60).astype(float)\n    df.loc[df['easter_adj'].isin(range(12, 39)), 'easter_adj'] = 12 \n    \n    # Black Friday\n    black_fri_date = df.date.dt.year.map({2015: pd.Timestamp(('2015-11-27')),\n                                         2016: pd.Timestamp(('2016-11-25')),\n                                         2017: pd.Timestamp(('2017-11-24')),\n                                         2018: pd.Timestamp(('2018-11-23')),\n                                         2019: pd.Timestamp(('2019-11-29'))})\n    df['days_from_black_friday'] = (df.date - black_fri_date).dt.days.clip(-5, 5)\n    \n    # Last Wednesday of June\n    wed_june_date = df.date.dt.year.map({2015: pd.Timestamp(('2015-06-24')),\n                                         2016: pd.Timestamp(('2016-06-29')),\n                                         2017: pd.Timestamp(('2017-06-28')),\n                                         2018: pd.Timestamp(('2018-06-27')),\n                                         2019: pd.Timestamp(('2019-06-26'))})\n    df['days_from_wed_jun'] = (df.date - wed_june_date).dt.days.clip(-5, 5)\n    \n    #First Sunday of November (second Sunday is Father's Day)\n    sun_nov_date = df.date.dt.year.map({2015: pd.Timestamp(('2015-11-1')),\n                                         2016: pd.Timestamp(('2016-11-6')),\n                                         2017: pd.Timestamp(('2017-11-5')),\n                                         2018: pd.Timestamp(('2018-11-4')),\n                                         2019: pd.Timestamp(('2019-11-3'))})\n    df['days_from_sun_nov'] = (df.date - sun_nov_date).dt.days.clip(-1, 9)\n    \n    print(df['date'])\n    df.drop(columns=['date'],inplace=True)\n\ndateProcess1(train, gdp_df)\ndateProcess1(test, gdp_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\nimport category_encoders as ce\n\n# Nearly all of our data is categorical, and we do not know a clear correlation between categories and num_sold, so we will hot encode using scikit-learn's OneHotEnocder\ndef dataProcess(x):\n    one_hot = ce.OneHotEncoder(cols = ['country'])\n    x = one_hot.fit_transform(x)\n\n    one_hot1 = ce.OneHotEncoder(cols = ['store']) # Creating a new hot encoder for each column may not be the most efficient, feel free to optimize this\n    x = one_hot1.fit_transform(x)\n\n    one_hot2 = ce.OneHotEncoder(cols = ['product'])\n    x = one_hot2.fit_transform(x)\n    return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfeatures = ['country', 'store', 'product', 'day_of_month', 'month', 'year', 'day_of_year', 'weekend', 'friday', 'xmas_adjust', 'easter_adj', 'days_from_black_friday', 'days_from_wed_jun', 'days_from_sun_nov', 'gdp_list', 'macro_data']\nlabels = ['num_sold']\nx_train = train[features]\ny_train = train[labels]\ny_train = np.ravel(y_train) # Scikit-learn didn't like my y-column unless I used this .ravel() method from numpy\nx_test = test[features]\n\n\nx_train = dataProcess(x_train)\nx_test = dataProcess(x_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import OrdinalEncoder\n\n# Canvert True/False values to numerical data\nobject_cols = ['weekend', 'friday']\n\nordinal_encoder = OrdinalEncoder()\nx_train[object_cols] = ordinal_encoder.fit_transform(x_train[object_cols])\nx_train[object_cols] = ordinal_encoder.transform(x_train[object_cols])\n\nx_test[object_cols] = ordinal_encoder.fit_transform(x_test[object_cols])\nx_test[object_cols] = ordinal_encoder.transform(x_test[object_cols])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nx_train_train, x_train_test, y_train_train, y_train_test = train_test_split(x_train, y_train, test_size=0.3) # split the data so we can get an idea of our model's performance","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nrf_regressor = RandomForestRegressor(n_estimators = 2400)\n\n# X-treme gradient boost\nfrom xgboost import XGBRegressor\nxgb_regressor = XGBRegressor(n_estimators = 824, learning_rate = 0.20677789662324045, max_depth=3)\n\n# CatBoost\nimport catboost\nfrom catboost import CatBoostRegressor\ncat_boost = CatBoostRegressor(learning_rate=0.25, max_depth = 5, n_estimators = 824)\n\nfrom sklearn.ensemble import AdaBoostRegressor\nfinal = AdaBoostRegressor()\n# Light Gradient Boosting Machine\nfrom lightgbm import LGBMRegressor\nlg_boost = LGBMRegressor(learning_rate=0.09, max_depth = 5, n_estimators = 824)\n\nfrom sklearn.ensemble import StackingRegressor\n\nstack_regressor = StackingRegressor([('cat', cat_boost), ('xg', xgb_regressor), ('lgbm', lg_boost), ('rf', rf_regressor)])\nstack_regressor.fit(x_train_train, y_train_train)\ny_pred = stack_regressor.predict(x_train_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This is the way that the competition will grade our predictions\ndef SMAPE(y_true, y_pred):\n    diff = np.abs(y_true - y_pred) / (y_true + np.abs(y_pred)) * 200\n    return diff.mean()\nprint(SMAPE(y_train_test, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This kernel contains the code to actually create a .csv predictions file to submit to the competition. This time, we train the model with all the training \n# data and then predict the testing data\n\nstack_regressor.fit(x_train, y_train)\ny_pred_test = stack_regressor.predict(x_test)\n\nrow_id = [] # Create a row_id column\nfor y in range(26298, 32868):\n    row_id.append(y)\n\nfor j in range(len(y_pred_test)): # Rounding has been found to make results slightly better\n    y_pred_test[j] = round(float(y_pred_test[j]))\n    \ny_pred_df = pd.DataFrame(row_id, columns=['row_id'])\ny_pred_df['num_sold'] = y_pred_test\ny_pred_df.to_csv('submission.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}