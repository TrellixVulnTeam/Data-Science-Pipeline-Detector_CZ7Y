{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction","metadata":{}},{"cell_type":"markdown","source":"See my previous notebook for EDA and a LGBM model: [tps-jan-22-eda-modelling](https://www.kaggle.com/samuelcortinhas/tps-jan-22-eda-modelling). This notebook improves on my previous one and attempts a hybrid model. \n\n**Acknowledgements:**\n* Kaggle's [time series course](https://www.kaggle.com/learn/time-series).\n* This [notebook](https://www.kaggle.com/teckmengwong/tps2201-hybrid-time-series/notebook#Data/Feature-Engineering) by [Teck Meng Wong](https://www.kaggle.com/teckmengwong).\n* Many of [AmbrosM's](https://www.kaggle.com/ambrosm) great notebooks.\n* This [notebook](https://www.kaggle.com/lucamassaron/kaggle-merchandise-eda-with-baseline-linear-model/notebook) by [Luca Massaron](https://www.kaggle.com/lucamassaron).","metadata":{}},{"cell_type":"markdown","source":"# Libraries","metadata":{}},{"cell_type":"code","source":"# Core\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nsns.set(style='darkgrid', font_scale=1.4)\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom itertools import combinations\nimport math\nimport statistics\nimport scipy.stats\nfrom scipy.stats import pearsonr\nimport time\nfrom datetime import datetime\nimport matplotlib.dates as mdates\nimport dateutil.easter as easter\n\n# Sklearn\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, TimeSeriesSplit\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LinearRegression, Ridge\n\n# Models\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\nfrom lightgbm import LGBMRegressor\n\n# Tensorflow\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import callbacks","metadata":{"execution":{"iopub.status.busy":"2022-01-27T17:26:00.860253Z","iopub.execute_input":"2022-01-27T17:26:00.860755Z","iopub.status.idle":"2022-01-27T17:26:00.877029Z","shell.execute_reply.started":"2022-01-27T17:26:00.860704Z","shell.execute_reply":"2022-01-27T17:26:00.8763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data","metadata":{}},{"cell_type":"markdown","source":"**Load data**","metadata":{}},{"cell_type":"code","source":"# Save to df\ntrain_data=pd.read_csv('../input/tabular-playground-series-jan-2022/train.csv', index_col='row_id')\ntest_data=pd.read_csv('../input/tabular-playground-series-jan-2022/test.csv', index_col='row_id')\n\n# Shape and preview\nprint('Training data df shape:',train_data.shape)\nprint('Test data df shape:',test_data.shape)\ntrain_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-27T17:26:00.878977Z","iopub.execute_input":"2022-01-27T17:26:00.880016Z","iopub.status.idle":"2022-01-27T17:26:00.95654Z","shell.execute_reply.started":"2022-01-27T17:26:00.879972Z","shell.execute_reply":"2022-01-27T17:26:00.955373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Datetime and drop 29th Feb**","metadata":{}},{"cell_type":"code","source":"# Convert date to datetime\ntrain_data.date=pd.to_datetime(train_data.date)\ntest_data.date=pd.to_datetime(test_data.date)\n\n# drop 29th Feb\ntrain_data.drop(train_data[(train_data.date.dt.month==2) & (train_data.date.dt.day==29)].index, axis=0, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-27T17:26:00.95807Z","iopub.execute_input":"2022-01-27T17:26:00.958305Z","iopub.status.idle":"2022-01-27T17:26:00.987229Z","shell.execute_reply.started":"2022-01-27T17:26:00.958277Z","shell.execute_reply":"2022-01-27T17:26:00.986556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Quick EDA","metadata":{}},{"cell_type":"markdown","source":"**num_sold by store**","metadata":{}},{"cell_type":"code","source":"# Figure\nplt.figure(figsize=(12,5))\n\n# Groupby\naa=train_data.groupby(['date','store']).agg(num_sold=('num_sold','sum'))\n\n# Lineplot\nsns.lineplot(data=aa, x='date', y='num_sold', hue='store')\n\n# Aesthetics\nplt.title('num_sold by store')","metadata":{"execution":{"iopub.status.busy":"2022-01-27T17:26:00.988381Z","iopub.execute_input":"2022-01-27T17:26:00.989281Z","iopub.status.idle":"2022-01-27T17:26:01.920041Z","shell.execute_reply.started":"2022-01-27T17:26:00.989247Z","shell.execute_reply":"2022-01-27T17:26:01.919017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* KaggleRama consistently sells more products than KaggleMart. \n* There are big spikes towards the end of each year.","metadata":{}},{"cell_type":"markdown","source":"**num_sold by product**","metadata":{}},{"cell_type":"code","source":"# Subplots\nfig, axes = plt.subplots(2, 1, figsize=(12, 10))\n\n# Groupby\nKR=train_data[train_data.store=='KaggleRama']\nKM=train_data[train_data.store=='KaggleMart']\nbb=KR.groupby(['date','product']).agg(num_sold=('num_sold','sum'))\ncc=KM.groupby(['date','product']).agg(num_sold=('num_sold','sum'))\n\n# Lineplots\nax1=sns.lineplot(ax=axes[0], data=bb, x='date', y='num_sold', hue='product')\nax2=sns.lineplot(ax=axes[1], data=cc, x='date', y='num_sold', hue='product')\n\n# Aesthetics\nax1.title.set_text('KaggleRama')\nax2.title.set_text('KaggleMart')","metadata":{"execution":{"iopub.status.busy":"2022-01-27T17:26:01.922463Z","iopub.execute_input":"2022-01-27T17:26:01.922868Z","iopub.status.idle":"2022-01-27T17:26:03.945518Z","shell.execute_reply.started":"2022-01-27T17:26:01.922832Z","shell.execute_reply":"2022-01-27T17:26:03.944544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* The Hat and Mug show strong yearly seasonal trends whereas the Sticker remains fairly constant. We can use Fourier Features to model these trends.\n* Hats sells the most, then Mugs and finally Stickers.","metadata":{}},{"cell_type":"markdown","source":"**num_sold by country**","metadata":{}},{"cell_type":"code","source":"# Subplots\nfig, axes = plt.subplots(2, 1, figsize=(12, 10))\n\n# Groupby\ndd=KR.groupby(['date','country']).agg(num_sold=('num_sold','sum'))\nee=KM.groupby(['date','country']).agg(num_sold=('num_sold','sum'))\n\n# Lineplots\nax1=sns.lineplot(ax=axes[0], data=dd, x='date', y='num_sold', hue='country')\nax2=sns.lineplot(ax=axes[1], data=ee, x='date', y='num_sold', hue='country')\n\n# Aesthetics\nax1.title.set_text('KaggleRama')\nax2.title.set_text('KaggleMart')","metadata":{"execution":{"iopub.status.busy":"2022-01-27T17:26:03.94688Z","iopub.execute_input":"2022-01-27T17:26:03.947135Z","iopub.status.idle":"2022-01-27T17:26:06.419203Z","shell.execute_reply.started":"2022-01-27T17:26:03.947105Z","shell.execute_reply":"2022-01-27T17:26:06.418413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Most products are sold in Norway, then Sweden followed by Finland.\n* Some spikes accur at different times for each country, perhaps because of different holidays.","metadata":{}},{"cell_type":"markdown","source":"# Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"**Labels and features**","metadata":{}},{"cell_type":"code","source":"# Labels\ny=train_data.num_sold\n\n# Features\nX=train_data.drop('num_sold', axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-01-27T17:26:06.420833Z","iopub.execute_input":"2022-01-27T17:26:06.421664Z","iopub.status.idle":"2022-01-27T17:26:06.429728Z","shell.execute_reply.started":"2022-01-27T17:26:06.421619Z","shell.execute_reply":"2022-01-27T17:26:06.428762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Public holidays (including unofficial ones)**","metadata":{}},{"cell_type":"code","source":"# From https://www.kaggle.com/c/tabular-playground-series-jan-2022/discussion/298990\ndef unofficial_hol(df):\n    countries = {'Finland': 1, 'Norway': 2, 'Sweden': 3}\n    stores = {'KaggleMart': 1, 'KaggleRama': 2}\n    products = {'Kaggle Mug': 1,'Kaggle Hat': 2, 'Kaggle Sticker': 3}\n    \n    # load holiday info.\n    hol_path = '../input/public-and-unofficial-holidays-nor-fin-swe-201519/holidays.csv'\n    holiday = pd.read_csv(hol_path)\n    \n    fin_holiday = holiday.loc[holiday.country == 'Finland']\n    swe_holiday = holiday.loc[holiday.country == 'Sweden']\n    nor_holiday = holiday.loc[holiday.country == 'Norway']\n    df['fin holiday'] = df.date.isin(fin_holiday.date).astype(int)\n    df['swe holiday'] = df.date.isin(swe_holiday.date).astype(int)\n    df['nor holiday'] = df.date.isin(nor_holiday.date).astype(int)\n    df['holiday'] = np.zeros(df.shape[0]).astype(int)\n    df.loc[df.country == 'Finland', 'holiday'] = df.loc[df.country == 'Finland', 'fin holiday']\n    df.loc[df.country == 'Sweden', 'holiday'] = df.loc[df.country == 'Sweden', 'swe holiday']\n    df.loc[df.country == 'Norway', 'holiday'] = df.loc[df.country == 'Norway', 'nor holiday']\n    df.drop(['fin holiday', 'swe holiday', 'nor holiday'], axis=1, inplace=True)\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2022-01-27T17:26:06.431318Z","iopub.execute_input":"2022-01-27T17:26:06.431824Z","iopub.status.idle":"2022-01-27T17:26:06.445188Z","shell.execute_reply.started":"2022-01-27T17:26:06.431779Z","shell.execute_reply":"2022-01-27T17:26:06.443991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Holidays (from AmbrosM)**","metadata":{}},{"cell_type":"code","source":"def get_holidays(df):\n    # End of year\n    df = pd.concat([df, pd.DataFrame({f\"dec{d}\":\n                      (df.date.dt.month == 12) & (df.date.dt.day == d)\n                      for d in range(24, 32)}),\n        pd.DataFrame({f\"n-dec{d}\":\n                      (df.date.dt.month == 12) & (df.date.dt.day == d) & (df.country == 'Norway')\n                      for d in range(24, 32)}),\n        pd.DataFrame({f\"f-jan{d}\":\n                      (df.date.dt.month == 1) & (df.date.dt.day == d) & (df.country == 'Finland')\n                      for d in range(1, 14)}),\n        pd.DataFrame({f\"jan{d}\":\n                      (df.date.dt.month == 1) & (df.date.dt.day == d) & (df.country == 'Norway')\n                      for d in range(1, 10)}),\n        pd.DataFrame({f\"s-jan{d}\":\n                      (df.date.dt.month == 1) & (df.date.dt.day == d) & (df.country == 'Sweden')\n                      for d in range(1, 15)})], axis=1)\n    \n    # May\n    df = pd.concat([df, pd.DataFrame({f\"may{d}\":\n                      (df.date.dt.month == 5) & (df.date.dt.day == d) \n                      for d in list(range(1, 10))}),\n        pd.DataFrame({f\"may{d}\":\n                      (df.date.dt.month == 5) & (df.date.dt.day == d) & (df.country == 'Norway')\n                      for d in list(range(19, 26))})], axis=1)\n    \n    # June and July\n    df = pd.concat([df, pd.DataFrame({f\"june{d}\":\n                   (df.date.dt.month == 6) & (df.date.dt.day == d) & (df.country == 'Sweden')\n                   for d in list(range(8, 14))})], axis=1)\n    \n    #Swedish Rock Concert\n    #Jun 3, 2015 – Jun 6, 2015\n    #Jun 8, 2016 – Jun 11, 2016\n    #Jun 7, 2017 – Jun 10, 2017\n    #Jun 6, 2018 – Jun 10, 2018\n    #Jun 5, 2019 – Jun 8, 2019\n    swed_rock_fest  = df.date.dt.year.map({2015: pd.Timestamp(('2015-06-6')),\n                                         2016: pd.Timestamp(('2016-06-11')),\n                                         2017: pd.Timestamp(('2017-06-10')),\n                                         2018: pd.Timestamp(('2018-06-10')),\n                                         2019: pd.Timestamp(('2019-06-8'))})\n\n    df = pd.concat([df, pd.DataFrame({f\"swed_rock_fest{d}\":\n                                      (df.date - swed_rock_fest == np.timedelta64(d, \"D\")) & (df.country == 'Sweden')\n                                      for d in list(range(-3, 3))})], axis=1)\n    \n    # Last Wednesday of June\n    wed_june_date = df.date.dt.year.map({2015: pd.Timestamp(('2015-06-24')),\n                                         2016: pd.Timestamp(('2016-06-29')),\n                                         2017: pd.Timestamp(('2017-06-28')),\n                                         2018: pd.Timestamp(('2018-06-27')),\n                                         2019: pd.Timestamp(('2019-06-26'))})\n    \n    df = pd.concat([df, pd.DataFrame({f\"wed_june{d}\": \n                   (df.date - wed_june_date == np.timedelta64(d, \"D\")) & (df.country != 'Norway')\n                   for d in list(range(-4, 6))})], axis=1)\n    \n    # First Sunday of November\n    sun_nov_date = df.date.dt.year.map({2015: pd.Timestamp(('2015-11-1')),\n                                         2016: pd.Timestamp(('2016-11-6')),\n                                         2017: pd.Timestamp(('2017-11-5')),\n                                         2018: pd.Timestamp(('2018-11-4')),\n                                         2019: pd.Timestamp(('2019-11-3'))})\n    \n    df = pd.concat([df, pd.DataFrame({f\"sun_nov{d}\": \n                   (df.date - sun_nov_date == np.timedelta64(d, \"D\")) & (df.country != 'Norway')\n                   for d in list(range(0, 9))})], axis=1)\n    \n    # First half of December (Independence Day of Finland, 6th of December)\n    df = pd.concat([df, pd.DataFrame({f\"dec{d}\":\n                   (df.date.dt.month == 12) & (df.date.dt.day == d) & (df.country == 'Finland')\n                   for d in list(range(6, 14))})], axis=1)\n\n    # Easter\n    easter_date = df.date.apply(lambda date: pd.Timestamp(easter.easter(date.year)))\n    df = pd.concat([df, pd.DataFrame({f\"easter{d}\":\n                   (df.date - easter_date == np.timedelta64(d, \"D\"))\n                   for d in list(range(-2, 11)) + list(range(40, 48)) + list(range(50, 59))})], axis=1)\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2022-01-27T17:26:06.446684Z","iopub.execute_input":"2022-01-27T17:26:06.447335Z","iopub.status.idle":"2022-01-27T17:26:06.477559Z","shell.execute_reply.started":"2022-01-27T17:26:06.447297Z","shell.execute_reply":"2022-01-27T17:26:06.47682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Include day of week, month, year etc**","metadata":{}},{"cell_type":"code","source":"def date_feat_eng_X1(df):\n    df['year']=df['date'].dt.year                   # 2015 to 2019\n    return df\n\ndef date_feat_eng_X2(df):\n    df['day_of_week']=df['date'].dt.dayofweek       # 0 to 6\n    df['day_of_month']=df['date'].dt.day            # 1 to 31\n    df['dayofyear'] = df['date'].dt.dayofyear       # 1 to 366\n    df.loc[(df.date.dt.year==2016) & (df.dayofyear>60), 'dayofyear'] -= 1   # 1 to 365\n    df['week']=df['date'].dt.isocalendar().week     # 1 to 53\n    df['week']=df['week'].astype('int')             # int64\n    df['month']=df['date'].dt.month                 # 1 to 12\n    return df","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-01-27T17:26:06.47869Z","iopub.execute_input":"2022-01-27T17:26:06.479521Z","iopub.status.idle":"2022-01-27T17:26:06.496624Z","shell.execute_reply.started":"2022-01-27T17:26:06.479487Z","shell.execute_reply":"2022-01-27T17:26:06.495826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**GDP**","metadata":{}},{"cell_type":"code","source":"def get_GDP(df):\n\n    # Load data\n    GDP_data = pd.read_csv(\"../input/gdp-20152019-finland-norway-and-sweden/GDP_data_2015_to_2019_Finland_Norway_Sweden.csv\",index_col=\"year\")\n\n    # Rename the columns in GDP df \n    GDP_data.columns = ['Finland', 'Norway', 'Sweden']\n\n    # Create a dictionary\n    GDP_dictionary = GDP_data.unstack().to_dict()\n    \n    # Create GDP column\n    df['GDP']=df.set_index(['country', 'year']).index.map(GDP_dictionary.get)\n    \n    # Log transform (only if the target is log-transformed too)\n    df['GDP']=np.log(df['GDP'])\n    \n    # Split GDP by country (for linear model)\n    df['GDP_Finland']=df['GDP'] * (df['country']=='Finland')\n    df['GDP_Norway']=df['GDP'] * (df['country']=='Norway')\n    df['GDP_Sweden']=df['GDP'] * (df['country']=='Sweden')\n    \n    # Drop column\n    df=df.drop(['GDP','year'],axis=1)\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2022-01-27T17:26:06.497954Z","iopub.execute_input":"2022-01-27T17:26:06.498353Z","iopub.status.idle":"2022-01-27T17:26:06.515532Z","shell.execute_reply.started":"2022-01-27T17:26:06.498299Z","shell.execute_reply":"2022-01-27T17:26:06.514766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**GDP per capita**","metadata":{}},{"cell_type":"code","source":"def GDP_PC(df):\n    # Load data\n    GDP_PC_data = pd.read_csv(\"../input/gdp-per-capita-finland-norway-sweden-201519/GDP_per_capita_2015_to_2019_Finland_Norway_Sweden.csv\",index_col=\"year\")\n    \n    # Create a dictionary\n    GDP_PC_dictionary = GDP_PC_data.unstack().to_dict()\n\n    # Create new GDP_PC column\n    df['GDP_PC'] = df.set_index(['country', 'year']).index.map(GDP_PC_dictionary.get)\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2022-01-27T17:26:06.516841Z","iopub.execute_input":"2022-01-27T17:26:06.517219Z","iopub.status.idle":"2022-01-27T17:26:06.528735Z","shell.execute_reply.started":"2022-01-27T17:26:06.517178Z","shell.execute_reply":"2022-01-27T17:26:06.52806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**GDP vs GDP per capita**","metadata":{}},{"cell_type":"code","source":"def GDP_corr(df):\n    \n    # Load data\n    GDP_data = pd.read_csv(\"../input/gdp-20152019-finland-norway-and-sweden/GDP_data_2015_to_2019_Finland_Norway_Sweden.csv\",index_col=\"year\")\n    GDP_PC_data = pd.read_csv(\"../input/gdp-per-capita-finland-norway-sweden-201519/GDP_per_capita_2015_to_2019_Finland_Norway_Sweden.csv\",index_col=\"year\")\n\n    # Rename the columns\n    GDP_data.columns = ['Finland', 'Norway', 'Sweden']\n    \n    # Create dictionary\n    GDP_dictionary = GDP_data.unstack().to_dict()\n    GDP_PC_dictionary = GDP_PC_data.unstack().to_dict()\n    \n    # Add year column\n    df['year']=df.date.dt.year\n    \n    # Make new column\n    df['GDP']=df.set_index(['country', 'year']).index.map(GDP_dictionary.get)\n    df['GDP_PC'] = df.set_index(['country', 'year']).index.map(GDP_PC_dictionary.get)\n\n    # Initialise output\n    feat_corr=[]\n    \n    # Compute pairwise correlations\n    for SS in ['KaggleMart', 'KaggleRama']:\n        for CC in ['Finland', 'Norway', 'Sweden']:\n            for PP in ['Kaggle Mug', 'Kaggle Hat', 'Kaggle Sticker']:\n                subset=df[(df.store==SS)&(df.country==CC)&(df['product']==PP)].groupby(['year']).agg(num_sold=('num_sold','sum'), GDP=('GDP','mean'), GDP_PC=('GDP_PC','mean'))\n                v1=subset.num_sold\n                v2=subset.GDP\n                v3=subset.GDP_PC\n                \n                r1, _ = pearsonr(v1,v2)\n                r2, _ = pearsonr(v1,v3)\n                \n                feat_corr.append([f'{SS}, {CC}, {PP}', r1, r2])\n\n    return pd.DataFrame(feat_corr, columns=['Features', 'GDP_corr', 'GDP_PC_corr'])\n    \ncorr_df=GDP_corr(train_data)\ncorr_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-27T17:26:06.530024Z","iopub.execute_input":"2022-01-27T17:26:06.530437Z","iopub.status.idle":"2022-01-27T17:26:07.019193Z","shell.execute_reply.started":"2022-01-27T17:26:06.530387Z","shell.execute_reply":"2022-01-27T17:26:07.01807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* In general, both GDP and GDP_PC are very highly correlated to the num_sold aggregate each year. \n* GDP tends to have a slightly higher correlation than GDP_PC.","metadata":{}},{"cell_type":"markdown","source":"**Fourier features**","metadata":{}},{"cell_type":"code","source":"# From https://www.kaggle.com/ambrosm/tpsjan22-03-linear-model#Simple-feature-engineering-(without-holidays)\ndef FourierFeatures(df):\n    # temporary one hot encoding\n    for product in ['Kaggle Mug', 'Kaggle Hat']:\n        df[product] = df['product'] == product\n    \n    # The three products have different seasonal patterns\n    dayofyear = df.date.dt.dayofyear\n    for k in range(1, 2):\n        df[f'sin{k}'] = np.sin(dayofyear / 365 * 2 * math.pi * k)\n        df[f'cos{k}'] = np.cos(dayofyear / 365 * 2 * math.pi * k)\n        df[f'mug_sin{k}'] = df[f'sin{k}'] * df['Kaggle Mug']\n        df[f'mug_cos{k}'] = df[f'cos{k}'] * df['Kaggle Mug']\n        df[f'hat_sin{k}'] = df[f'sin{k}'] * df['Kaggle Hat']\n        df[f'hat_cos{k}'] = df[f'cos{k}'] * df['Kaggle Hat']\n        df=df.drop([f'sin{k}', f'cos{k}'], axis=1)\n    \n    # drop temporary one hot encoding\n    df=df.drop(['Kaggle Mug', 'Kaggle Hat'], axis=1)\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2022-01-27T17:26:07.022158Z","iopub.execute_input":"2022-01-27T17:26:07.022448Z","iopub.status.idle":"2022-01-27T17:26:07.033496Z","shell.execute_reply.started":"2022-01-27T17:26:07.022417Z","shell.execute_reply":"2022-01-27T17:26:07.032532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Interactions**","metadata":{}},{"cell_type":"code","source":"# Help linear model find the right height of trends for each combination of features\ndef get_interactions(df):\n    df['KR_Sweden_Mug']=(df.country=='Sweden')*(df['product']=='Kaggle Mug')*(df.store=='KaggleRama')\n    df['KR_Sweden_Hat']=(df.country=='Sweden')*(df['product']=='Kaggle Hat')*(df.store=='KaggleRama')\n    df['KR_Sweden_Sticker']=(df.country=='Sweden')*(df['product']=='Kaggle Sticker')*(df.store=='KaggleRama')\n    df['KR_Norway_Mug']=(df.country=='Norway')*(df['product']=='Kaggle Mug')*(df.store=='KaggleRama')\n    df['KR_Norway_Hat']=(df.country=='Norway')*(df['product']=='Kaggle Hat')*(df.store=='KaggleRama')\n    df['KR_Norway_Sticker']=(df.country=='Norway')*(df['product']=='Kaggle Sticker')*(df.store=='KaggleRama')\n    df['KR_Finland_Mug']=(df.country=='Finland')*(df['product']=='Kaggle Mug')*(df.store=='KaggleRama')\n    df['KR_Finland_Hat']=(df.country=='Finland')*(df['product']=='Kaggle Hat')*(df.store=='KaggleRama')\n    df['KR_Finland_Sticker']=(df.country=='Finland')*(df['product']=='Kaggle Sticker')*(df.store=='KaggleRama')\n    \n    df['KM_Sweden_Mug']=(df.country=='Sweden')*(df['product']=='Kaggle Mug')*(df.store=='KaggleMart')\n    df['KM_Sweden_Hat']=(df.country=='Sweden')*(df['product']=='Kaggle Hat')*(df.store=='KaggleMart')\n    df['KM_Sweden_Sticker']=(df.country=='Sweden')*(df['product']=='Kaggle Sticker')*(df.store=='KaggleMart')\n    df['KM_Norway_Mug']=(df.country=='Norway')*(df['product']=='Kaggle Mug')*(df.store=='KaggleMart')\n    df['KM_Norway_Hat']=(df.country=='Norway')*(df['product']=='Kaggle Hat')*(df.store=='KaggleMart')\n    df['KM_Norway_Sticker']=(df.country=='Norway')*(df['product']=='Kaggle Sticker')*(df.store=='KaggleMart')\n    df['KM_Finland_Mug']=(df.country=='Finland')*(df['product']=='Kaggle Mug')*(df.store=='KaggleMart')\n    df['KM_Finland_Hat']=(df.country=='Finland')*(df['product']=='Kaggle Hat')*(df.store=='KaggleMart')\n    df['KM_Finland_Sticker']=(df.country=='Finland')*(df['product']=='Kaggle Sticker')*(df.store=='KaggleMart')\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2022-01-27T17:26:07.036075Z","iopub.execute_input":"2022-01-27T17:26:07.036366Z","iopub.status.idle":"2022-01-27T17:26:07.054957Z","shell.execute_reply.started":"2022-01-27T17:26:07.036335Z","shell.execute_reply":"2022-01-27T17:26:07.053803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Drop date and one hot encoding**","metadata":{}},{"cell_type":"code","source":"def dropdate(df):\n    df=df.drop('date',axis=1)\n    return df\n\ndef onehot(df,columns):\n    df=pd.get_dummies(df, columns)\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-01-27T17:26:07.056605Z","iopub.execute_input":"2022-01-27T17:26:07.056875Z","iopub.status.idle":"2022-01-27T17:26:07.07389Z","shell.execute_reply.started":"2022-01-27T17:26:07.056843Z","shell.execute_reply":"2022-01-27T17:26:07.072745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Put pieces together**","metadata":{}},{"cell_type":"code","source":"# Feature set for trend model\ndef FeatEng_X1(df):\n    df=date_feat_eng_X1(df)\n    df=get_GDP(df)\n    df=FourierFeatures(df)\n    df=get_interactions(df)\n    df=dropdate(df)\n    df=onehot(df,['store', 'product', 'country'])\n    return df\n\n# Feature set for interactions model\ndef FeatEng_X2(df):\n    df=date_feat_eng_X2(df)\n    df=unofficial_hol(df)\n    df=get_holidays(df)\n    df=dropdate(df)\n    df=onehot(df,['store', 'product', 'country'])\n    return df\n\n# Apply feature engineering\nX_train_1=FeatEng_X1(X)\nX_train_2=FeatEng_X2(X)\nX_test_1=FeatEng_X1(test_data)\nX_test_2=FeatEng_X2(test_data)","metadata":{"execution":{"iopub.status.busy":"2022-01-27T17:26:07.075343Z","iopub.execute_input":"2022-01-27T17:26:07.075632Z","iopub.status.idle":"2022-01-27T17:26:09.679245Z","shell.execute_reply.started":"2022-01-27T17:26:07.075599Z","shell.execute_reply":"2022-01-27T17:26:09.678317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Hybrid model","metadata":{}},{"cell_type":"markdown","source":"The idea is as follows. Linear interpolation is good at extrapolating trends but poor at learning interactions. Conversely, decision tree algorithms like XGBoost are very good at learning interactions but can't extrapolate trends. A hybrid model tries to take the best of both worlds by first learning the trend with linear interpolation and then learning the interactions on the detrended time series.","metadata":{}},{"cell_type":"code","source":"# A class is a collection of properties and methods (like models from Sklearn)\nclass HybridModel:\n    def __init__(self, model_1, model_2, grid=None):\n        self.model_1 = model_1\n        self.model_2 = model_2\n        self.grid=grid\n        \n    def fit(self, X_train_1, X_train_2, y):\n        # Train model 1\n        self.model_1.fit(X_train_1, y)\n        \n        # Predictions from model 1 (trend)\n        y_trend = self.model_1.predict(X_train_1)\n\n        if self.grid:\n            # Grid search\n            tscv = TimeSeriesSplit(n_splits=3)\n            grid_model = GridSearchCV(estimator=self.model_2, cv=tscv, param_grid=self.grid)\n        \n            # Train model 2 on detrended series\n            grid_model.fit(X_train_2, y-y_trend)\n            \n            # Model 2 preditions (for residual analysis)\n            y_resid = grid_model.predict(X_train_2)\n            \n            # Save model\n            self.grid_model=grid_model\n        else:\n            # Train model 2 on residuals\n            self.model_2.fit(X_train_2, y-y_trend)\n            \n            # Model 2 preditions (for residual analysis)\n            y_resid = self.model_2.predict(X_train_2)\n        \n        # Save data\n        self.y_train_trend = y_trend\n        self.y_train_resid = y_resid\n        \n    def predict(self, X_test_1, X_test_2):\n        # Predict trend using model 1\n        y_trend = self.model_1.predict(X_test_1)\n        \n        if self.grid:\n            # Grid model predictions\n            y_resid = self.grid_model.predict(X_test_2)\n        else:\n            # Model 2 predictions\n            y_resid = self.model_2.predict(X_test_2)\n        \n        # Add predictions together\n        y_pred = y_trend + y_resid\n        \n        # Save data\n        self.y_test_trend = y_trend\n        self.y_test_resid = y_resid\n        \n        return y_pred","metadata":{"execution":{"iopub.status.busy":"2022-01-27T17:26:09.681118Z","iopub.execute_input":"2022-01-27T17:26:09.68145Z","iopub.status.idle":"2022-01-27T17:26:09.694774Z","shell.execute_reply.started":"2022-01-27T17:26:09.681409Z","shell.execute_reply":"2022-01-27T17:26:09.693701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prediction","metadata":{}},{"cell_type":"markdown","source":"**Ensembling**","metadata":{}},{"cell_type":"code","source":"# Choose models\nmodel_1=LinearRegression()\nmodels_2=[LGBMRegressor(random_state=0), CatBoostRegressor(random_state=0, verbose=False), XGBRegressor(random_state=0)]\n\n# Parameter grid\nparam_grid = {'n_estimators': [100, 150, 200, 225, 250, 275, 300],\n        'max_depth': [4, 5, 6, 7],\n        'learning_rate': [0.1, 0.12, 0.13, 0.14, 0.15]}\n\n# Initialise output vectors\ny_pred=np.zeros(len(test_data))\ntrain_preds=np.zeros(len(y))\n\n# Ensemble predictions\nfor model_2 in models_2:\n    # Start timer\n    start = time.time()\n    \n    # Construct hybrid model\n    model = HybridModel(model_1, model_2, grid=param_grid)\n\n    # Train model\n    model.fit(X_train_1, X_train_2, np.log(y))\n\n    # Save predictions\n    y_pred += np.exp(model.predict(X_test_1,X_test_2))\n    \n    # Training set predictions (for residual analysis)\n    train_preds += np.exp(model.y_train_trend+model.y_train_resid)\n    \n    # Stop timer\n    stop = time.time()\n    \n    print(f'Model_2:{model_2} -- time:{round((stop-start)/60,2)} mins')\n    \n    if model.grid:\n        print('Best parameters:',model.grid_model.best_params_,'\\n')\n    \n# Scale\ny_pred = y_pred/len(models_2)\ntrain_preds = train_preds/len(models_2)","metadata":{"_kg_hide-input":false,"_kg_hide-output":false,"execution":{"iopub.status.busy":"2022-01-27T17:26:09.696055Z","iopub.execute_input":"2022-01-27T17:26:09.696269Z","iopub.status.idle":"2022-01-27T17:26:31.294111Z","shell.execute_reply.started":"2022-01-27T17:26:09.696243Z","shell.execute_reply":"2022-01-27T17:26:31.293356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Post-processing**","metadata":{}},{"cell_type":"code","source":"# From https://www.kaggle.com/fergusfindley/ensembling-and-rounding-techniques-comparison\ndef geometric_round(arr):\n    result_array = arr\n    result_array = np.where(result_array < np.sqrt(np.floor(arr)*np.ceil(arr)), np.floor(arr), result_array)\n    result_array = np.where(result_array >= np.sqrt(np.floor(arr)*np.ceil(arr)), np.ceil(arr), result_array)\n    return result_array\n\ny_pred=geometric_round(y_pred)\n\n# Save predictions to file\noutput = pd.DataFrame({'row_id': test_data.index, 'num_sold': y_pred})\n\n# Check format\noutput.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-27T17:26:31.295588Z","iopub.execute_input":"2022-01-27T17:26:31.296061Z","iopub.status.idle":"2022-01-27T17:26:31.312677Z","shell.execute_reply.started":"2022-01-27T17:26:31.296018Z","shell.execute_reply":"2022-01-27T17:26:31.311843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-27T17:26:31.314076Z","iopub.execute_input":"2022-01-27T17:26:31.314305Z","iopub.status.idle":"2022-01-27T17:26:31.33874Z","shell.execute_reply.started":"2022-01-27T17:26:31.314276Z","shell.execute_reply":"2022-01-27T17:26:31.337912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Plot predictions","metadata":{}},{"cell_type":"code","source":"def plot_predictions(SS, CC, PP, series=output):\n    '''\n    SS=store\n    CC=country\n    PP=product\n    '''\n    \n    # uncomment if your dataframes have different names\n    #train_data=train_df\n    #test_data=test_df\n    \n    # Training set target\n    train_subset=train_data[(train_data.store==SS)&(train_data.country==CC)&(train_data['product']==PP)]\n    \n    # Predictions\n    plot_index=test_data[(test_data.store==SS)&(test_data.country==CC)&(test_data['product']==PP)].index\n    pred_subset=series[series.row_id.isin(plot_index)].reset_index(drop=True)\n    \n    # Plot\n    plt.figure(figsize=(12,5))\n    n1=len(train_subset['num_sold'])\n    n2=len(pred_subset['num_sold'])\n    plt.plot(np.arange(n1),train_subset['num_sold'], label='Training')\n    plt.plot(np.arange(n1,n1+n2),pred_subset['num_sold'], label='Predictions')\n    plt.title('\\n'+f'Store:{SS}, Country:{CC}, Product:{PP}')\n    plt.legend()\n    plt.xlabel('Days since 2015-01-01')\n    plt.ylabel('num_sold')","metadata":{"execution":{"iopub.status.busy":"2022-01-27T17:26:31.341175Z","iopub.execute_input":"2022-01-27T17:26:31.341463Z","iopub.status.idle":"2022-01-27T17:26:31.351206Z","shell.execute_reply.started":"2022-01-27T17:26:31.341432Z","shell.execute_reply":"2022-01-27T17:26:31.35026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Plot trends**","metadata":{}},{"cell_type":"code","source":"# Put into dataframes\ny_trend=pd.DataFrame({'row_id': test_data.index, 'num_sold': np.exp(model.y_test_trend)})\ny_resid=pd.DataFrame({'row_id': test_data.index, 'num_sold': np.exp(model.y_test_resid)})\ny_pred=pd.DataFrame({'row_id': test_data.index, 'num_sold': np.exp(model.y_test_trend+model.y_test_resid)})\n\n# Choose parameters\nSS='KaggleMart'\nCC='Norway'\n\n# Plot trends (model 1 predictions)\nplot_predictions(SS, CC, 'Kaggle Hat', series=y_trend)\nplot_predictions(SS, CC, 'Kaggle Mug', series=y_trend)\nplot_predictions(SS, CC, 'Kaggle Sticker', series=y_trend)","metadata":{"execution":{"iopub.status.busy":"2022-01-27T17:26:31.352601Z","iopub.execute_input":"2022-01-27T17:26:31.353046Z","iopub.status.idle":"2022-01-27T17:26:32.485656Z","shell.execute_reply.started":"2022-01-27T17:26:31.353013Z","shell.execute_reply":"2022-01-27T17:26:32.4847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**All predictions**","metadata":{}},{"cell_type":"code","source":"for SS in ['KaggleMart','KaggleRama']:\n    for CC in ['Finland', 'Norway', 'Sweden']:\n        for PP in ['Kaggle Mug', 'Kaggle Hat', 'Kaggle Sticker']:\n            plot_predictions(SS, CC, PP)","metadata":{"execution":{"iopub.status.busy":"2022-01-27T17:26:32.487234Z","iopub.execute_input":"2022-01-27T17:26:32.48767Z","iopub.status.idle":"2022-01-27T17:26:39.45221Z","shell.execute_reply.started":"2022-01-27T17:26:32.487636Z","shell.execute_reply":"2022-01-27T17:26:39.451077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Residual Analysis","metadata":{}},{"cell_type":"markdown","source":"**Plot residuals**","metadata":{}},{"cell_type":"code","source":"# need to ensemble\ntrain_preds = np.exp(model.y_train_trend+model.y_train_resid)\n\n# Residuals on training set (SMAPE)\nresiduals = 200 * (train_preds - y) / (train_preds + y)\n\n# Plot residuals\nplt.figure(figsize=(12,4))\nplt.scatter(np.arange(len(residuals)),residuals, s=1)\nplt.hlines([0], 0, residuals.index.max(), color='k')\nplt.title('Residuals on training set')\nplt.xlabel('Sample')\nplt.ylabel('SMAPE')","metadata":{"execution":{"iopub.status.busy":"2022-01-27T17:26:39.453778Z","iopub.execute_input":"2022-01-27T17:26:39.454174Z","iopub.status.idle":"2022-01-27T17:26:39.765681Z","shell.execute_reply.started":"2022-01-27T17:26:39.454141Z","shell.execute_reply":"2022-01-27T17:26:39.765066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Plot histogram of residuals**","metadata":{}},{"cell_type":"code","source":"mu, std = scipy.stats.norm.fit(residuals)\n\nplt.figure(figsize=(12,4))\nplt.hist(residuals, bins=100, density=True)\nx = np.linspace(plt.xlim()[0], plt.xlim()[1], 200)\nplt.plot(x, scipy.stats.norm.pdf(x, mu, std), 'r', linewidth=2)\nplt.title(f'Histogram of residuals; mean = {residuals.mean():.4f}, '\n          f'$\\sigma = {residuals.std():.1f}$, SMAPE = {residuals.abs().mean():.5f}')\nplt.xlabel('Residual (percent)')\nplt.ylabel('Density')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-27T17:26:39.766689Z","iopub.execute_input":"2022-01-27T17:26:39.767437Z","iopub.status.idle":"2022-01-27T17:26:40.179974Z","shell.execute_reply.started":"2022-01-27T17:26:39.767399Z","shell.execute_reply":"2022-01-27T17:26:40.179117Z"},"trusted":true},"execution_count":null,"outputs":[]}]}