{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction","metadata":{}},{"cell_type":"markdown","source":"In TPS Jan 2022, the aim of this competition is to predict the number of items sold by two different companies. The features that are available to us are date, country, store and product. My initialy impression is that this is a very small dataset (features and samples) and so a gradient boosted model like XGBoost could do well here.\n\n**Note** that I made the first version of this notebook before I looked at other peoples solutions. After this I build on other peoples ideas to improve my own score and learn new techniques.\n\n**Ver. 2 Acknowledgments:**\n* [Rounding up predictions](https://www.kaggle.com/c/tabular-playground-series-jan-2022/discussion/298201#1642988) by [Carl McBride Ellis](https://www.kaggle.com/carlmcbrideellis).\n* [LGBM](https://www.kaggle.com/ambrosm/tpsjan22-06-lightgbm-quickstart/notebook) by [Ambros M](https://www.kaggle.com/ambrosm).\n* [Holidays](https://www.kaggle.com/mfedeli/tabular-playground-series-jan-2022) by [Matteo Fedeli](https://www.kaggle.com/mfedeli).\n\n**Ver. 3 Acknowledgments:**\n* [GDP](https://www.kaggle.com/carlmcbrideellis/gdp-of-finland-norway-and-sweden-2015-2019/data) by [Carl McBride Ellis](https://www.kaggle.com/carlmcbrideellis).\n\n**Ver. 5 Acknowledgments:**\n* [Date feat. eng.](https://www.kaggle.com/lucamassaron/kaggle-merchandise-eda-with-baseline-linear-model) by [Luca Massaron](https://www.kaggle.com/lucamassaron).\n\n**Ver. 6 Acknowledgments:**\n* [TimesSeriesSplit + weekend feature](https://www.kaggle.com/adamwurdits/tps-01-2022-catboost-w-optuna-seed-averaging?scriptVersionId=84848139) by [Adam Wurdits](https://www.kaggle.com/adamwurdits).\n* [Unofficial holidays](https://www.kaggle.com/c/tabular-playground-series-jan-2022/discussion/298990) by [Vincent Pallares](https://www.kaggle.com/vpallares).\n\n**Ver. 8 Acknowledgments:**\n* [CPI](https://www.kaggle.com/sardorabdirayimov/consumer-price-index-20152019-nordic-countries) by [Sardor Abdirayimov](https://www.kaggle.com/sardorabdirayimov).\n* [Fourier features](https://www.kaggle.com/ryanholbrook/seasonality) from the [kaggle time series course](https://www.kaggle.com/learn/time-series).\n\n**Ver. 9 Acknowledgments:**\n* [Geometric rounding](https://www.kaggle.com/fergusfindley/ensembling-and-rounding-techniques-comparison) by [Fergus Findley](https://www.kaggle.com/fergusfindley).\n\n**Ver. 10 Acknowledgments:**\n* [Fourier Features](https://www.kaggle.com/c/tabular-playground-series-jan-2022/discussion/301629) by [AmbrosM](https://www.kaggle.com/ambrosm).","metadata":{}},{"cell_type":"markdown","source":"# Libraries","metadata":{}},{"cell_type":"code","source":"# Core\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nsns.set(style='darkgrid', font_scale=1.4)\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom itertools import combinations\nimport statistics\nimport time\nfrom datetime import datetime\nimport matplotlib.dates as mdates\n\n# Sklearn\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LinearRegression\n\n# Models\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\nfrom lightgbm import LGBMRegressor\n\n# Tensorflow\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import callbacks","metadata":{"execution":{"iopub.status.busy":"2022-01-19T19:39:34.87994Z","iopub.execute_input":"2022-01-19T19:39:34.881017Z","iopub.status.idle":"2022-01-19T19:39:34.897067Z","shell.execute_reply.started":"2022-01-19T19:39:34.880988Z","shell.execute_reply":"2022-01-19T19:39:34.896355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data","metadata":{}},{"cell_type":"markdown","source":"**Load data**","metadata":{}},{"cell_type":"code","source":"# Save to df\ntrain_data=pd.read_csv('../input/tabular-playground-series-jan-2022/train.csv', index_col='row_id')\ntest_data=pd.read_csv('../input/tabular-playground-series-jan-2022/test.csv', index_col='row_id')\n\n# Shape and preview\nprint('Training data df shape:',train_data.shape)\nprint('Test data df shape:',test_data.shape)\ntrain_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-19T19:39:35.574212Z","iopub.execute_input":"2022-01-19T19:39:35.574794Z","iopub.status.idle":"2022-01-19T19:39:35.623645Z","shell.execute_reply.started":"2022-01-19T19:39:35.574767Z","shell.execute_reply":"2022-01-19T19:39:35.6232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Initial thoughts:*\n* Date data can be tricky to deal with but luckily here it is already converted to international date time format. This means entries have an ordering, which makes it easier to sort and plot.\n* Country, store and product features are not currently numeric. The first pre-processing step could be encoding these to categorical numeric using a one-hot scheme. We'll check below for missing values and number of unique entries to see if we can do this. ","metadata":{}},{"cell_type":"markdown","source":"**Missing values**","metadata":{}},{"cell_type":"code","source":"print('Number of missing values in training set:',train_data.isna().sum().sum())\nprint('')\nprint('Number of missing values in test set:',test_data.isna().sum().sum())","metadata":{"execution":{"iopub.status.busy":"2022-01-19T19:39:36.568218Z","iopub.execute_input":"2022-01-19T19:39:36.569544Z","iopub.status.idle":"2022-01-19T19:39:36.582674Z","shell.execute_reply.started":"2022-01-19T19:39:36.569512Z","shell.execute_reply":"2022-01-19T19:39:36.582172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are no missing values. That's very nice of kaggle.","metadata":{}},{"cell_type":"markdown","source":"**Cardinality of features**","metadata":{}},{"cell_type":"code","source":"print('Training cardinalities: \\n', train_data.nunique())\nprint('')\nprint('Test cardinalities: \\n', test_data.nunique())","metadata":{"execution":{"iopub.status.busy":"2022-01-19T19:39:37.121513Z","iopub.execute_input":"2022-01-19T19:39:37.122055Z","iopub.status.idle":"2022-01-19T19:39:37.138787Z","shell.execute_reply.started":"2022-01-19T19:39:37.122027Z","shell.execute_reply":"2022-01-19T19:39:37.138078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The cardinalities of country, store and product are very small and so one-hot encoding this categorical data is justified. It would now be good to see what the range of date values is to understand the timescale we are dealing with.","metadata":{}},{"cell_type":"markdown","source":"**Timeframe**","metadata":{}},{"cell_type":"code","source":"print('Training data:')\nprint('Min date', train_data['date'].min())\nprint('Max date', train_data['date'].max())\nprint('')\nprint('Test data:')\nprint('Min date', test_data['date'].min())\nprint('Max date', test_data['date'].max())","metadata":{"execution":{"iopub.status.busy":"2022-01-19T19:39:37.621585Z","iopub.execute_input":"2022-01-19T19:39:37.622848Z","iopub.status.idle":"2022-01-19T19:39:37.636667Z","shell.execute_reply.started":"2022-01-19T19:39:37.622814Z","shell.execute_reply":"2022-01-19T19:39:37.635989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ok, so this tells us that the training data spans 3 years from 2015 to 2018 and the test data picks up from where we left off and spans a further 1 year, i.e. 2019. ","metadata":{}},{"cell_type":"code","source":"# Convert date to datetime\ntrain_data.date=pd.to_datetime(train_data.date)\ntest_data.date=pd.to_datetime(test_data.date)","metadata":{"execution":{"iopub.status.busy":"2022-01-19T19:39:37.963569Z","iopub.execute_input":"2022-01-19T19:39:37.964769Z","iopub.status.idle":"2022-01-19T19:39:37.979676Z","shell.execute_reply.started":"2022-01-19T19:39:37.964731Z","shell.execute_reply":"2022-01-19T19:39:37.978972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"markdown","source":"**Store sales**","metadata":{}},{"cell_type":"markdown","source":"Let's begin by plotting the target data.  ","metadata":{}},{"cell_type":"code","source":"# Figure\nplt.figure(figsize=(12,5))\n\n# Groupby\naa=train_data.groupby(['date','store']).agg(num_sold=('num_sold','sum'))\n\n# Lineplot\nsns.lineplot(data=aa, x='date', y='num_sold', hue='store')\n\n# Aesthetics\nplt.title('num_sold by store')","metadata":{"execution":{"iopub.status.busy":"2022-01-19T19:39:38.66569Z","iopub.execute_input":"2022-01-19T19:39:38.665909Z","iopub.status.idle":"2022-01-19T19:39:39.323908Z","shell.execute_reply.started":"2022-01-19T19:39:38.665885Z","shell.execute_reply":"2022-01-19T19:39:39.323239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Observations:*\n* Kaggle Rama is consistently selling more products than Kaggle Mart. \n* The number of products sold for both companies oscillates depending on the time of year (season) and fluctuates rapidly (this is probably due to weekday vs weekend sales).\n* There are big spikes towards the end of each year (likely due to christmas) and also some other smaller seasonal spikes (perhaps easter holidays etc).","metadata":{}},{"cell_type":"markdown","source":"**Store sales by country**","metadata":{}},{"cell_type":"markdown","source":"This is to look at whether the stores sell more products in certain countries or not.","metadata":{}},{"cell_type":"code","source":"# Subplots\nfig, axes = plt.subplots(2, 1, figsize=(12, 10))\n\n# Groupby\nKR=train_data[train_data.store=='KaggleRama']\nKM=train_data[train_data.store=='KaggleMart']\nbb=KR.groupby(['date','country']).agg(num_sold=('num_sold','sum'))\ncc=KM.groupby(['date','country']).agg(num_sold=('num_sold','sum'))\n\n# Lineplots\nax1=sns.lineplot(ax=axes[0], data=bb, x='date', y='num_sold', hue='country')\nax2=sns.lineplot(ax=axes[1], data=cc, x='date', y='num_sold', hue='country')\n\n# Aesthetics\nax1.title.set_text('KaggleRama')\nax2.title.set_text('KaggleMart')","metadata":{"execution":{"iopub.status.busy":"2022-01-19T19:39:39.503604Z","iopub.execute_input":"2022-01-19T19:39:39.503955Z","iopub.status.idle":"2022-01-19T19:39:40.991753Z","shell.execute_reply.started":"2022-01-19T19:39:39.503931Z","shell.execute_reply":"2022-01-19T19:39:40.99089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Observations:*\n* We see that both stores sell more products in Norway than the other two countries. \n* Finland and Sweden perform very similarly but maybe Sweden has a slight edge in general. ","metadata":{}},{"cell_type":"markdown","source":"**Store sales by product type**","metadata":{}},{"cell_type":"code","source":"# Subplots\nfig, axes = plt.subplots(2, 1, figsize=(12, 10))\n\n# Groupby\ndd=KR.groupby(['date','product']).agg(num_sold=('num_sold','sum'))\nee=KM.groupby(['date','product']).agg(num_sold=('num_sold','sum'))\n\n# Lineplots\nax1=sns.lineplot(ax=axes[0], data=dd, x='date', y='num_sold', hue='product')\nax2=sns.lineplot(ax=axes[1], data=ee, x='date', y='num_sold', hue='product')\n\n# Aesthetics\nax1.title.set_text('KaggleRama')\nax2.title.set_text('KaggleMart')","metadata":{"execution":{"iopub.status.busy":"2022-01-19T19:39:40.993218Z","iopub.execute_input":"2022-01-19T19:39:40.993466Z","iopub.status.idle":"2022-01-19T19:39:42.68678Z","shell.execute_reply.started":"2022-01-19T19:39:40.993435Z","shell.execute_reply":"2022-01-19T19:39:42.68521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Observations:*\n* We see that both stores sell Hats the most, then Mugs and finally Stickers the least. \n* Sales of stickers is fairly constant throughout the year, whereas hat (especially) and mug sales is more affected by seasonality. ","metadata":{}},{"cell_type":"markdown","source":"# Pre-processing & Feat. Eng.","metadata":{}},{"cell_type":"markdown","source":"**Labels and features**","metadata":{}},{"cell_type":"code","source":"# Labels\ny=train_data.num_sold\n\n# Features\nX=train_data.drop('num_sold', axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-01-19T19:39:45.824334Z","iopub.execute_input":"2022-01-19T19:39:45.82459Z","iopub.status.idle":"2022-01-19T19:39:45.829926Z","shell.execute_reply.started":"2022-01-19T19:39:45.824561Z","shell.execute_reply":"2022-01-19T19:39:45.829205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Public holidays**","metadata":{}},{"cell_type":"code","source":"holiday_path = '../input/holidays-finland-norway-sweden-20152019/Holidays_Finland_Norway_Sweden_2015-2019.csv'\n\ndef GetHoliday(holiday_path, df):\n    \"\"\"\n    Get a boolean feature of whether the current row is a holiday sale\n    \"\"\"\n    \n    holiday = pd.read_csv(holiday_path)\n    fin_holiday = holiday.loc[holiday.Country == 'Finland']\n    swe_holiday = holiday.loc[holiday.Country == 'Sweden']\n    nor_holiday = holiday.loc[holiday.Country == 'Norway']\n    df['fin holiday'] = df.date.isin(fin_holiday.Date).astype(int)\n    df['swe holiday'] = df.date.isin(swe_holiday.Date).astype(int)\n    df['nor holiday'] = df.date.isin(nor_holiday.Date).astype(int)\n    \n    df['holiday'] = np.zeros(df.shape[0]).astype(int)\n    df.loc[df.country == 'Finland', 'holiday'] = df.loc[df.country == 'Finland', 'fin holiday']\n    df.loc[df.country == 'Sweden', 'holiday'] = df.loc[df.country == 'Sweden', 'swe holiday']\n    df.loc[df.country == 'Norway', 'holiday'] = df.loc[df.country == 'Norway', 'nor holiday']\n    df.drop(['fin holiday', 'swe holiday', 'nor holiday'], axis=1, inplace=True)\n    return df\n\n#X = GetHoliday(holiday_path, X)\n#test_data = GetHoliday(holiday_path, test_data)","metadata":{"execution":{"iopub.status.busy":"2022-01-19T19:39:46.320281Z","iopub.execute_input":"2022-01-19T19:39:46.32079Z","iopub.status.idle":"2022-01-19T19:39:46.3277Z","shell.execute_reply.started":"2022-01-19T19:39:46.320764Z","shell.execute_reply":"2022-01-19T19:39:46.327165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**All holidays (inc. unofficial)**","metadata":{}},{"cell_type":"code","source":"hol_path = '../input/public-and-unofficial-holidays-nor-fin-swe-201519/holidays.csv'\n\ndef unofficial_hol(hol_path, df):\n    countries = {'Finland': 1, 'Norway': 2, 'Sweden': 3}\n    stores = {'KaggleMart': 1, 'KaggleRama': 2}\n    products = {'Kaggle Mug': 1,'Kaggle Hat': 2, 'Kaggle Sticker': 3}\n    \n    # load holiday info.\n    holiday = pd.read_csv(hol_path)\n    \n    fin_holiday = holiday.loc[holiday.country == 'Finland']\n    swe_holiday = holiday.loc[holiday.country == 'Sweden']\n    nor_holiday = holiday.loc[holiday.country == 'Norway']\n    df['fin holiday'] = df.date.isin(fin_holiday.date).astype(int)\n    df['swe holiday'] = df.date.isin(swe_holiday.date).astype(int)\n    df['nor holiday'] = df.date.isin(nor_holiday.date).astype(int)\n    df['holiday'] = np.zeros(df.shape[0]).astype(int)\n    df.loc[df.country == 'Finland', 'holiday'] = df.loc[df.country == 'Finland', 'fin holiday']\n    df.loc[df.country == 'Sweden', 'holiday'] = df.loc[df.country == 'Sweden', 'swe holiday']\n    df.loc[df.country == 'Norway', 'holiday'] = df.loc[df.country == 'Norway', 'nor holiday']\n    df.drop(['fin holiday', 'swe holiday', 'nor holiday'], axis=1, inplace=True)\n    \n    return df\n\nX = unofficial_hol(hol_path, X)\ntest = unofficial_hol(hol_path, test_data)","metadata":{"execution":{"iopub.status.busy":"2022-01-19T19:39:46.81467Z","iopub.execute_input":"2022-01-19T19:39:46.815467Z","iopub.status.idle":"2022-01-19T19:39:46.860151Z","shell.execute_reply.started":"2022-01-19T19:39:46.815441Z","shell.execute_reply":"2022-01-19T19:39:46.859684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Include day of week, month, year etc**","metadata":{}},{"cell_type":"code","source":"def date_feat_eng(df):\n    df['day_of_week']=df['date'].dt.dayofweek       # 0 to 6\n    df['day_of_month']=df['date'].dt.day            # 1 to 31\n    df['weekend']=(df['day_of_week']//5 == 1)       # 0 or 1\n    df['weekend']=df['weekend'].astype('int')       # int64\n    df['week']=df['date'].dt.isocalendar().week     # 1 to 53\n    df['week'][df['week']>52]=52                    # 1 to 52\n    df['week']=df['week'].astype('int')             # int64\n    df['month']=df['date'].dt.month                 # 1 to 12\n    df['quarter']=df['date'].dt.quarter             # 1 to 4\n    df['year']=df['date'].dt.year                   # 2015 to 2019\n    return df\n\nX= date_feat_eng(X)\ntest=date_feat_eng(test)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-01-19T19:39:47.529568Z","iopub.execute_input":"2022-01-19T19:39:47.530425Z","iopub.status.idle":"2022-01-19T19:39:47.565272Z","shell.execute_reply.started":"2022-01-19T19:39:47.530394Z","shell.execute_reply":"2022-01-19T19:39:47.564785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Drop 29th Feb**","metadata":{}},{"cell_type":"markdown","source":"This date isn't useful for prediction because it doesn't appear in the test set. (2019 is not a leap year) It will also make the Fourier analysis easier later.","metadata":{}},{"cell_type":"code","source":"# drop 29th Feb\n#y.drop(X[(X.month==2) & (X.day_of_month==29)].index, axis=0, inplace=True)\n#X.drop(X[(X.month==2) & (X.day_of_month==29)].index, axis=0, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-19T19:39:48.672824Z","iopub.execute_input":"2022-01-19T19:39:48.673048Z","iopub.status.idle":"2022-01-19T19:39:48.676152Z","shell.execute_reply.started":"2022-01-19T19:39:48.673024Z","shell.execute_reply":"2022-01-19T19:39:48.675559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Gross Domestic Product (GDP)**","metadata":{}},{"cell_type":"code","source":"# Load data\nGDP_data = pd.read_csv(\"../input/gdp-20152019-finland-norway-and-sweden/GDP_data_2015_to_2019_Finland_Norway_Sweden.csv\",index_col=\"year\")\n\n# Rename the columns in GDP df \nGDP_data.columns = ['Finland', 'Norway', 'Sweden']\n\n# Plot data\nplt.figure(figsize=(8,5))\n\n# Heatmap with annotations\nsns.heatmap(GDP_data, annot=True, fmt='g', cmap='Blues')\n\n# Aesthetics\nplt.title('Heatmap of GDP in nordic countries')","metadata":{"execution":{"iopub.status.busy":"2022-01-19T19:39:49.164227Z","iopub.execute_input":"2022-01-19T19:39:49.164506Z","iopub.status.idle":"2022-01-19T19:39:49.410169Z","shell.execute_reply.started":"2022-01-19T19:39:49.164478Z","shell.execute_reply":"2022-01-19T19:39:49.409352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see that the GDP in 2019 is lower than in 2018 so this could suggest sales will actually decrease in the test set.","metadata":{}},{"cell_type":"markdown","source":"Now for some absolute [wizardry](https://www.kaggle.com/carlmcbrideellis/gdp-of-finland-norway-and-sweden-2015-2019/comments) with the help of Carl...","metadata":{}},{"cell_type":"code","source":"# Create a dictionary\nGDP_dictionary = GDP_data.unstack().to_dict()\n\n# Create new GDP column\n#X['GDP'] = X.set_index(['country', 'year']).index.map(GDP_dictionary.get)\n#test['GDP'] = test.set_index(['country', 'year']).index.map(GDP_dictionary.get)","metadata":{"execution":{"iopub.status.busy":"2022-01-19T19:39:50.387682Z","iopub.execute_input":"2022-01-19T19:39:50.387908Z","iopub.status.idle":"2022-01-19T19:39:50.393299Z","shell.execute_reply.started":"2022-01-19T19:39:50.387884Z","shell.execute_reply":"2022-01-19T19:39:50.392366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**GDP per capita**","metadata":{}},{"cell_type":"code","source":"# Load data\nGDP_PC=pd.read_csv('../input/gdp-per-capita-finland-norway-sweden-201519/GDP_per_capita_2015_to_2019_Finland_Norway_Sweden.csv',index_col=\"year\")\n\n# Create a dictionary\nGDP_PC_dictionary = GDP_PC.unstack().to_dict()\n\n# Create new GDP_PC column\nX['GDP_PC'] = X.set_index(['country', 'year']).index.map(GDP_PC_dictionary.get)\ntest['GDP_PC'] = test.set_index(['country', 'year']).index.map(GDP_PC_dictionary.get)\n\n# Preview df\nX.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-19T19:39:51.152211Z","iopub.execute_input":"2022-01-19T19:39:51.152468Z","iopub.status.idle":"2022-01-19T19:39:51.214983Z","shell.execute_reply.started":"2022-01-19T19:39:51.152444Z","shell.execute_reply":"2022-01-19T19:39:51.214437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**CPI (inflation)**","metadata":{}},{"cell_type":"code","source":"# Does not improve score\n'''\n# load data\nCPI_data = pd.read_csv('../input/consumer-price-index-20152019-nordic-countries/consumer_price_index.csv')\n\n# format data\nCPI_data=CPI_data.T.iloc[2:,:]\nCPI_data.columns = ['Finland', 'Norway', 'Sweden']\nCPI_data.index=[2015,2016,2017,2018,2019]\nCPI_data.index.name='year'\n\n# Round to 2 d.p\nCPI_data=CPI_data.astype(float).round(2)\n\n# Create a dictionary\nCPI_dictionary = CPI_data.unstack().to_dict()\n\n# Create new CPI column\nX['CPI'] = X.set_index(['country', 'year']).index.map(CPI_dictionary.get)\ntest['CPI'] = test.set_index(['country', 'year']).index.map(CPI_dictionary.get)\n\n# Preview df\nX.head()\n'''","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-01-19T19:39:51.797784Z","iopub.execute_input":"2022-01-19T19:39:51.79914Z","iopub.status.idle":"2022-01-19T19:39:51.806706Z","shell.execute_reply.started":"2022-01-19T19:39:51.799098Z","shell.execute_reply":"2022-01-19T19:39:51.806012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Fourier features","metadata":{}},{"cell_type":"markdown","source":"These work best for linear models. Check out my other [notebook](https://www.kaggle.com/samuelcortinhas/tps-jan-22-hybrid-model) that uses these with a hybrid model. ","metadata":{}},{"cell_type":"code","source":"# From https://www.kaggle.com/ambrosm/tpsjan22-03-linear-model#Simple-feature-engineering-(without-holidays)\ndef FourierFeatures(df):\n    # temporary one hot encoding\n    for product in ['Kaggle Mug', 'Kaggle Hat']:\n        df[product] = df['product'] == product\n    \n    # The three products have different seasonal patterns\n    dayofyear = df.date.dt.dayofyear\n    for k in range(1, 3):\n        df[f'sin{k}'] = np.sin(dayofyear / 365 * 2 * math.pi * k)\n        df[f'cos{k}'] = np.cos(dayofyear / 365 * 2 * math.pi * k)\n        df[f'mug_sin{k}'] = df[f'sin{k}'] * df['Kaggle Mug']\n        df[f'mug_cos{k}'] = df[f'cos{k}'] * df['Kaggle Mug']\n        df[f'hat_sin{k}'] = df[f'sin{k}'] * df['Kaggle Hat']\n        df[f'hat_cos{k}'] = df[f'cos{k}'] * df['Kaggle Hat']\n        df=df.drop([f'sin{k}', f'cos{k}'], axis=1)\n    \n    # drop temporary one hot encoding\n    df=df.drop(['Kaggle Mug','Kaggle Hat'], axis=1)\n    \n    return df\n\n# add fourier features\n#X=fourier_features(X)\n#test=fourier_features(test)","metadata":{"execution":{"iopub.status.busy":"2022-01-19T19:39:54.262081Z","iopub.execute_input":"2022-01-19T19:39:54.263326Z","iopub.status.idle":"2022-01-19T19:39:54.275515Z","shell.execute_reply.started":"2022-01-19T19:39:54.263267Z","shell.execute_reply":"2022-01-19T19:39:54.27474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Drop date**","metadata":{}},{"cell_type":"code","source":"X.drop('date',axis=1, inplace=True)\ntest.drop('date',axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-19T19:39:56.409638Z","iopub.execute_input":"2022-01-19T19:39:56.410482Z","iopub.status.idle":"2022-01-19T19:39:56.417276Z","shell.execute_reply.started":"2022-01-19T19:39:56.410442Z","shell.execute_reply":"2022-01-19T19:39:56.416844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Encode categorical variables**","metadata":{}},{"cell_type":"code","source":"X=pd.get_dummies(X, columns=['store', 'country', 'product'])\ntest=pd.get_dummies(test, columns=['store', 'country', 'product'])","metadata":{"execution":{"iopub.status.busy":"2022-01-19T19:39:58.815455Z","iopub.execute_input":"2022-01-19T19:39:58.815719Z","iopub.status.idle":"2022-01-19T19:39:58.848572Z","shell.execute_reply.started":"2022-01-19T19:39:58.81569Z","shell.execute_reply":"2022-01-19T19:39:58.847799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modelling","metadata":{}},{"cell_type":"markdown","source":"Base model for exploration and evaluation of new ideas","metadata":{}},{"cell_type":"code","source":"'''\n# Break off a validation set (in time-series-split style)\nX_train=X.iloc[:3*len(X)//4,:]\nX_valid=X.iloc[3*len(X)//4:,:]\ny_train=y.iloc[:3*len(X)//4]\ny_valid=y.iloc[3*len(X)//4:]\n\n# Base model\nmodel=LGBMRegressor(random_state=0, n_estimators=200, max_depth=6)\n\n# Train model\nmodel.fit(X_train,y_train)\n\n# Predict\npreds = model.predict(X_valid)\n\n# Calcaculate smape\ndef smape(A, F):\n    return 100/len(A) * np.sum(2 * np.abs(F - A) / (np.abs(A) + np.abs(F)))\n\n# Evaluate smape\nsmape(preds,y_valid)\n'''","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-01-19T19:26:50.074532Z","iopub.execute_input":"2022-01-19T19:26:50.07479Z","iopub.status.idle":"2022-01-19T19:26:50.080415Z","shell.execute_reply.started":"2022-01-19T19:26:50.074752Z","shell.execute_reply":"2022-01-19T19:26:50.079642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Store results from experiments\nsmape_results=pd.DataFrame.from_dict({'Method':['base','include holidays','date feat. eng. (FE)', 'holidays + date FE', \n                                                'prev. row + GDP (model A)', 'model A + weekend', 'model A + day dummy',\n                                                'model A + unofficial holidays', 'prev. row + GDP per capita', 'GDP per capita instead of GDP (Model B)',\n                                                'Model B + CPI', 'Model B + drop feb 29', 'Model B + drop feb 29 + Fourier feats.'],\n                                      'SMAPE': [16.52,16.46,9.06, 8.94, 9.02, 9.02, 21.97, 9.00, 8.97, 7.82, 7.83, 7.93, 7.93]})\nsmape_results","metadata":{"execution":{"iopub.status.busy":"2022-01-19T19:26:50.081519Z","iopub.execute_input":"2022-01-19T19:26:50.081772Z","iopub.status.idle":"2022-01-19T19:26:50.098017Z","shell.execute_reply.started":"2022-01-19T19:26:50.081737Z","shell.execute_reply":"2022-01-19T19:26:50.097219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Parameter grid\ngrid = {'n_estimators': [50, 75, 100, 125, 150, 175, 200, 225, 250],\n        'max_depth': [2, 4, 6, 8, 10, 12],\n        'learning_rate': [0.01, 0.025, 0.05, 0.075, 0.1, 0.125, 0.15]}\n\n# XGBoost model\nmodel=LGBMRegressor(random_state=0)\n\n# Grid Search with n-fold cross validation\ngrid_model = GridSearchCV(model,grid,cv=5)\n\n# Train classifier with optimal parameters\ngrid_model.fit(X,y)","metadata":{"_kg_hide-output":false,"execution":{"iopub.status.busy":"2022-01-19T19:26:50.099555Z","iopub.execute_input":"2022-01-19T19:26:50.099901Z","iopub.status.idle":"2022-01-19T19:34:28.867236Z","shell.execute_reply.started":"2022-01-19T19:26:50.099864Z","shell.execute_reply":"2022-01-19T19:34:28.866451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Results from Grid Search**","metadata":{}},{"cell_type":"code","source":"print(\"The best parameters across ALL searched params:\\n\",grid_model.best_params_)\nprint(\"\\n The best score across ALL searched params:\\n\",grid_model.best_score_) # r^2 score","metadata":{"execution":{"iopub.status.busy":"2022-01-19T19:34:28.868045Z","iopub.execute_input":"2022-01-19T19:34:28.868201Z","iopub.status.idle":"2022-01-19T19:34:28.872848Z","shell.execute_reply.started":"2022-01-19T19:34:28.86818Z","shell.execute_reply":"2022-01-19T19:34:28.872299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prediction","metadata":{}},{"cell_type":"code","source":"# from https://www.kaggle.com/fergusfindley/ensembling-and-rounding-techniques-comparison\ndef geometric_round(arr):\n    result_array = arr\n    result_array = np.where(result_array < np.sqrt(np.floor(arr)*np.ceil(arr)), np.floor(arr), result_array)\n    result_array = np.where(result_array >= np.sqrt(np.floor(arr)*np.ceil(arr)), np.ceil(arr), result_array)\n    return result_array\n\n# Make predictions\npreds_test = geometric_round(grid_model.predict(test))\n\n# Save predictions to file\noutput = pd.DataFrame({'row_id': test.index,\n                       'num_sold': preds_test})\n\n# Check format\noutput.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-19T19:40:04.399145Z","iopub.execute_input":"2022-01-19T19:40:04.399417Z","iopub.status.idle":"2022-01-19T19:40:04.455639Z","shell.execute_reply.started":"2022-01-19T19:40:04.399387Z","shell.execute_reply":"2022-01-19T19:40:04.455204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-19T19:40:06.97779Z","iopub.execute_input":"2022-01-19T19:40:06.978917Z","iopub.status.idle":"2022-01-19T19:40:06.997126Z","shell.execute_reply.started":"2022-01-19T19:40:06.978878Z","shell.execute_reply":"2022-01-19T19:40:06.996461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Plot predictions","metadata":{}},{"cell_type":"code","source":"def plot_predictions(SS, CC, PP):\n    '''\n    SS=store\n    CC=country\n    PP=product\n    '''\n    \n    # uncomment if your dataframes have different names\n    #train_data=train_df\n    #test_data=test_df\n    #output=preds\n    \n    # Training set target\n    train_subset=train_data[(train_data.store==SS)&(train_data.country==CC)&(train_data['product']==PP)]\n\n    # Predictions\n    plot_index=test_data[(test_data.store==SS)&(test_data.country==CC)&(test_data['product']==PP)].index\n    pred_subset=output[output.row_id.isin(plot_index)].reset_index(drop=True)\n\n    # Plot\n    plt.figure(figsize=(12,5))\n    n1=len(train_subset['num_sold'])\n    n2=len(pred_subset['num_sold'])\n    plt.plot(np.arange(n1),train_subset['num_sold'], label='Training')\n    plt.plot(np.arange(n1,n1+n2),pred_subset['num_sold'], label='Predictions')\n    plt.title('\\n'+f'Store:{SS}, Country:{CC}, Product:{PP}')\n    plt.legend()\n    plt.xlabel('Days since 2015-01-01')\n    plt.ylabel('num_sold')","metadata":{"execution":{"iopub.status.busy":"2022-01-19T19:40:09.90363Z","iopub.execute_input":"2022-01-19T19:40:09.904003Z","iopub.status.idle":"2022-01-19T19:40:09.912814Z","shell.execute_reply.started":"2022-01-19T19:40:09.903977Z","shell.execute_reply":"2022-01-19T19:40:09.911847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for SS in ['KaggleMart','KaggleRama']:\n    for CC in ['Finland', 'Norway', 'Sweden']:\n        for PP in ['Kaggle Mug', 'Kaggle Hat', 'Kaggle Sticker']:\n            plot_predictions(SS, CC, PP)","metadata":{"execution":{"iopub.status.busy":"2022-01-19T19:40:10.944938Z","iopub.execute_input":"2022-01-19T19:40:10.94516Z","iopub.status.idle":"2022-01-19T19:40:15.724932Z","shell.execute_reply.started":"2022-01-19T19:40:10.945138Z","shell.execute_reply":"2022-01-19T19:40:15.724247Z"},"trusted":true},"execution_count":null,"outputs":[]}]}