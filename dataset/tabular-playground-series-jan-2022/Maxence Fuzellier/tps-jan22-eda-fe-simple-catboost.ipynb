{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# TPS-Jan22 üéâ | EDA + FE + Simple CatBoost","metadata":{}},{"cell_type":"markdown","source":"### Happy New Year!\nMay this year bring good health, happiness and success at whatever you choose to accomplish!\n\n___\n\n# üìå Introduction\n\n>Hello Kagglers,\n>\n>This notebook is a simple implementation of a CatBoost Regressor, as well as some EDA, feature engineering, cross-validation explanation and a SMAPE function.\n>\n>As a beginner and newcomer, making this first notebook as public is a milestone for me. I believe that there is no better way to improve than to share and report on your knowledge, investigations and achievements.\n>\n>May it be interesting and useful to you. Do not hesitate to provide feedback!\n\n\n# üìù Agenda\n>1. [üìö Loading libraries and files](#Loading)\n>2. [üîç Exploratory Data Analysis](#EDA)\n>3. [‚öôÔ∏è Feature Engineering](#FeatureEngineering)\n>4. [‚úÖ Cross-validation Method](#Validation)\n>5. [üèãÔ∏è Model Training & Inference](#TrainingInference)","metadata":{}},{"cell_type":"markdown","source":"___\n# <a name=\"Loading\">üìö Loading libraries and files</a>","metadata":{}},{"cell_type":"code","source":"import os\nimport warnings\n\nimport numpy as np  # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport math\nfrom pathlib import Path\n\n# Mute warnings\nwarnings.filterwarnings(\"ignore\")\n\n%matplotlib inline","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":false,"execution":{"iopub.status.busy":"2022-01-19T10:21:13.155073Z","iopub.execute_input":"2022-01-19T10:21:13.155413Z","iopub.status.idle":"2022-01-19T10:21:14.081873Z","shell.execute_reply.started":"2022-01-19T10:21:13.155322Z","shell.execute_reply":"2022-01-19T10:21:14.081006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!tree ../input/","metadata":{"execution":{"iopub.status.busy":"2022-01-19T10:21:14.083543Z","iopub.execute_input":"2022-01-19T10:21:14.083866Z","iopub.status.idle":"2022-01-19T10:21:14.85686Z","shell.execute_reply.started":"2022-01-19T10:21:14.083822Z","shell.execute_reply":"2022-01-19T10:21:14.855782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_dir = Path('../input/tabular-playground-series-jan-2022')\nholiday_dir = Path('../input/public-and-unofficial-holidays-nor-fin-swe-201519')\ngdp_dir = Path('../input/gdp-20152019-finland-norway-and-sweden')\n\ntrain = pd.read_csv(\n    data_dir / 'train.csv',\n    dtype={\n        'country': 'category',\n        'store': 'category',\n        'product': 'category',\n        'num_sold': 'float32',\n    },\n    index_col='row_id'\n)\n\ntest = pd.read_csv(\n    data_dir / \"test.csv\",\n    dtype={\n        'country': 'category',\n        'store': 'category',\n        'product': 'category',\n    },\n    index_col='row_id'\n)\n\ntarget_col = train.columns.difference(test.columns)[0]\n\nholiday_data = pd.read_csv(holiday_dir / 'holidays.csv')\n\ngdp = pd.read_csv(\n    gdp_dir / 'GDP_data_2015_to_2019_Finland_Norway_Sweden.csv', index_col='year')","metadata":{"execution":{"iopub.status.busy":"2022-01-19T10:21:14.858714Z","iopub.execute_input":"2022-01-19T10:21:14.858991Z","iopub.status.idle":"2022-01-19T10:21:14.963309Z","shell.execute_reply.started":"2022-01-19T10:21:14.858962Z","shell.execute_reply":"2022-01-19T10:21:14.962495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"___\n# <a name=\"EDA\">üîç Exploratory Data Analysis</a>","metadata":{}},{"cell_type":"markdown","source":"### <a name=\"FeatureAnalysis\">Feature Analysis</a>","metadata":{}},{"cell_type":"markdown","source":"First, let's have a glance at some basic information about our data.","metadata":{}},{"cell_type":"code","source":"train.info()","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2022-01-19T10:21:14.966094Z","iopub.execute_input":"2022-01-19T10:21:14.966509Z","iopub.status.idle":"2022-01-19T10:21:14.986371Z","shell.execute_reply.started":"2022-01-19T10:21:14.966472Z","shell.execute_reply":"2022-01-19T10:21:14.985794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note that the <code>date</code> feature is originally <code>str</code>-typed, so we will convert it to <code>datetime</code> to make any further process easier with *pandas*.\n\nHowever, before converting <code>date</code> values, let's see if all of the values are, ideally, following the same <code>month/day/four-digit year</code> format. We can get an idea of how widespread this issue is by checking the length of each entry in the <code>date</code> column.","metadata":{}},{"cell_type":"code","source":"def len_data_count(column):\n    return column.str.len().value_counts()\n\nprint(len_data_count(train.date))\nprint(len_data_count(test.date))","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2022-01-19T10:21:14.987279Z","iopub.execute_input":"2022-01-19T10:21:14.987816Z","iopub.status.idle":"2022-01-19T10:21:15.020755Z","shell.execute_reply.started":"2022-01-19T10:21:14.987785Z","shell.execute_reply":"2022-01-19T10:21:15.020159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It looks like all values are 10-characters long, which is good news. We can now convert our column.","metadata":{}},{"cell_type":"code","source":"train['date'] = pd.to_datetime(train['date'])\ntest['date']  = pd.to_datetime(test['date'])","metadata":{"execution":{"iopub.status.busy":"2022-01-19T10:21:15.021803Z","iopub.execute_input":"2022-01-19T10:21:15.022125Z","iopub.status.idle":"2022-01-19T10:21:15.039211Z","shell.execute_reply.started":"2022-01-19T10:21:15.022097Z","shell.execute_reply":"2022-01-19T10:21:15.038262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Moreover, it looks like there is no missing values in any field.\n\nWant to make sure about it? Alright.","metadata":{}},{"cell_type":"code","source":"display(train.isnull().sum(), test.isnull().sum())","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2022-01-19T10:21:15.040507Z","iopub.execute_input":"2022-01-19T10:21:15.040779Z","iopub.status.idle":"2022-01-19T10:21:15.056984Z","shell.execute_reply.started":"2022-01-19T10:21:15.040743Z","shell.execute_reply":"2022-01-19T10:21:15.056194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We now have the confirmation.\n\nAfterwards, let's look at the **cardinality** of each column.","metadata":{}},{"cell_type":"code","source":"display(train.iloc[:,1:-1].nunique(), test.iloc[:,1:-1].nunique())","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2022-01-19T10:21:15.058265Z","iopub.execute_input":"2022-01-19T10:21:15.058468Z","iopub.status.idle":"2022-01-19T10:21:15.072872Z","shell.execute_reply.started":"2022-01-19T10:21:15.058443Z","shell.execute_reply":"2022-01-19T10:21:15.071863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Then, it would be relevant to count each of these values' occurrences.<br />\nAt least for the categorical features, since the <code>date</code> column will be the subject of a later treatment.","metadata":{}},{"cell_type":"code","source":"# Count for each unique values\ncategorical_cols = train.select_dtypes('category').columns.tolist()\n\nfor col in categorical_cols:\n    display(pd.DataFrame(train[col].value_counts()))","metadata":{"execution":{"iopub.status.busy":"2022-01-19T10:21:15.074054Z","iopub.execute_input":"2022-01-19T10:21:15.074352Z","iopub.status.idle":"2022-01-19T10:21:15.101512Z","shell.execute_reply.started":"2022-01-19T10:21:15.074316Z","shell.execute_reply":"2022-01-19T10:21:15.100771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Well, the least that can be said is that **all features are balanced!**","metadata":{}},{"cell_type":"markdown","source":"___\n# <a name=\"FeatureEngineering\">‚öôÔ∏è Feature Engineering</a>","metadata":{}},{"cell_type":"markdown","source":"üìå This part has been updated and largely inspired by these two notebooks:\n> * [TPSJAN22-03 Linear Model](https://www.kaggle.com/ambrosm/tpsjan22-03-linear-model) & [TPSJAN22-06 LightGBM Quickstart](https://www.kaggle.com/ambrosm/tpsjan22-06-lightgbm-quickstart) by [AmbrosM](https://www.kaggle.com/ambrosm)<br />\n> * [TPS Jan 22 - EDA + modelling](https://www.kaggle.com/samuelcortinhas/tps-jan-22-eda-modelling) by [Samuel Cortinhas](https://www.kaggle.com/samuelcortinhas)","metadata":{}},{"cell_type":"markdown","source":"We are dealing with time-series data, therefore it is relevant to consider the impact of holidays, which naturally play a large role in business activities.","metadata":{}},{"cell_type":"code","source":"import dateutil.easter as easter\n\ndef holiday_features(holiday_df, df):\n    \n    fin_holiday = holiday_df.loc[holiday_df.country == 'Finland']\n    swe_holiday = holiday_df.loc[holiday_df.country == 'Sweden']\n    nor_holiday = holiday_df.loc[holiday_df.country == 'Norway']\n    \n    df['fin holiday'] = df.date.isin(fin_holiday.date).astype(int)\n    df['swe holiday'] = df.date.isin(swe_holiday.date).astype(int)\n    df['nor holiday'] = df.date.isin(nor_holiday.date).astype(int)\n    \n    df['holiday'] = np.zeros(df.shape[0]).astype(int)\n    \n    df.loc[df.country == 'Finland', 'holiday'] = df.loc[df.country == 'Finland', 'fin holiday']\n    df.loc[df.country == 'Sweden', 'holiday'] = df.loc[df.country == 'Sweden', 'swe holiday']\n    df.loc[df.country == 'Norway', 'holiday'] = df.loc[df.country == 'Norway', 'nor holiday']\n    \n    df.drop(['fin holiday', 'swe holiday', 'nor holiday'], axis=1, inplace=True)\n    \n    gdp_exponent = 1.2121103201489674\n    # c.f https://www.kaggle.com/ambrosm/tpsjan22-03-linear-model\n    \n    # GDP features\n    def get_gdp(row):\n        \"\"\"Return the GDP based on row.country and row.date.year\"\"\"\n        country = 'GDP_' + row.country\n        \n        return gdp.loc[row.date.year, country] ** gdp_exponent\n    \n    df['gdp'] = pd.DataFrame(df.apply(get_gdp, axis=1))\n    \n    \n    # Easter\n    easter_date = df.date.apply(lambda date: pd.Timestamp(easter.easter(date.year)))\n    df['days_from_easter'] = (df.date - easter_date).dt.days.clip(-5, 65)\n    \n    # Last Sunday of May (Mother's Day)\n    sun_may_date = df.date.dt.year.map({\n        2015: pd.Timestamp(('2015-5-31')),\n        2016: pd.Timestamp(('2016-5-29')),\n        2017: pd.Timestamp(('2017-5-28')),\n        2018: pd.Timestamp(('2018-5-27')),\n        2019: pd.Timestamp(('2019-5-26'))\n    })\n    #new_df['days_from_sun_may'] = (df.date - sun_may_date).dt.days.clip(-1, 9)\n    \n    # Last Wednesday of June\n    wed_june_date = df.date.dt.year.map({\n        2015: pd.Timestamp(('2015-06-24')),\n        2016: pd.Timestamp(('2016-06-29')),\n        2017: pd.Timestamp(('2017-06-28')),\n        2018: pd.Timestamp(('2018-06-27')),\n        2019: pd.Timestamp(('2019-06-26'))\n    })\n    df['days_from_wed_jun'] = (df.date - wed_june_date).dt.days.clip(-5, 5)\n    \n    # First Sunday of November (second Sunday is Father's Day)\n    sun_nov_date = df.date.dt.year.map({\n        2015: pd.Timestamp(('2015-11-1')),\n        2016: pd.Timestamp(('2016-11-6')),\n        2017: pd.Timestamp(('2017-11-5')),\n        2018: pd.Timestamp(('2018-11-4')),\n        2019: pd.Timestamp(('2019-11-3'))\n    })\n    df['days_from_sun_nov'] = (df.date - sun_nov_date).dt.days.clip(-1, 9)\n    \n    return df\n\ntrain = holiday_features(holiday_data, train)\ntest  = holiday_features(holiday_data, test)","metadata":{"execution":{"iopub.status.busy":"2022-01-19T10:21:15.104145Z","iopub.execute_input":"2022-01-19T10:21:15.104355Z","iopub.status.idle":"2022-01-19T10:21:17.353889Z","shell.execute_reply.started":"2022-01-19T10:21:15.104329Z","shell.execute_reply":"2022-01-19T10:21:17.352937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, the cardinality of each categorical feature is quite low, and that we do not want to impose an ordinal order, **one-hot encoding** may be a good way to encode our categorical features.","metadata":{}},{"cell_type":"code","source":"train = pd.get_dummies(train, columns=categorical_cols)\ntest  = pd.get_dummies(test, columns=categorical_cols)","metadata":{"execution":{"iopub.status.busy":"2022-01-19T10:21:17.357606Z","iopub.execute_input":"2022-01-19T10:21:17.357903Z","iopub.status.idle":"2022-01-19T10:21:17.374497Z","shell.execute_reply.started":"2022-01-19T10:21:17.357868Z","shell.execute_reply":"2022-01-19T10:21:17.373786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since we have a <code>date</code>-typed feature here, and models are rarely able to use dates and times as they are, we would benefit from encoding it as categorical variables as this can often yield useful information about temporal patterns.\n\nFurthermore, time-series data (such as product sales) often have distributions that differs from week days to week-ends for example, it is likely that using the day of the week as a new feature is a relevant option we have.","metadata":{}},{"cell_type":"code","source":"# Nothing to see here!\n# Copy 'date' feature for further visualization/explanation\ndate_copy = train.date","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-01-19T10:21:17.375684Z","iopub.execute_input":"2022-01-19T10:21:17.376469Z","iopub.status.idle":"2022-01-19T10:21:17.381527Z","shell.execute_reply.started":"2022-01-19T10:21:17.376432Z","shell.execute_reply":"2022-01-19T10:21:17.380669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def new_date_features(df):\n    df['year'] = df.date.dt.year \n    df['quarter'] = df.date.dt.quarter\n    df['month'] = df.date.dt.month  \n    df['week'] = df.date.dt.week \n    df['day'] = df.date.dt.day  \n    df['weekday'] = df.date.dt.weekday\n    df['day_of_week'] = df.date.dt.dayofweek  \n    df['day_of_year'] = df.date.dt.dayofyear  \n    df['week_of_year'] = df.date.dt.weekofyear\n    df['day_of_month'] = df.date.dt.days_in_month  \n    df['is_weekend'] = np.where((df['weekday'] == 5) | (df['weekday'] == 6), 1, 0)\n    df['is_friday'] = np.where((df['weekday'] == 4), 1, 0)\n    \n    df.drop('date', axis=1, inplace=True)\n    \n    return df\n    \ntrain = new_date_features(train)\ntest  = new_date_features(test)","metadata":{"execution":{"iopub.status.busy":"2022-01-19T10:21:17.382598Z","iopub.execute_input":"2022-01-19T10:21:17.383233Z","iopub.status.idle":"2022-01-19T10:21:17.484571Z","shell.execute_reply.started":"2022-01-19T10:21:17.383193Z","shell.execute_reply":"2022-01-19T10:21:17.483496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally, here are our datasets, before moving to the cross-validation step.","metadata":{}},{"cell_type":"code","source":"# Target transformation\ny = np.log1p(train[target_col] / train.gdp)\n\ntrain.drop(target_col, axis=1, inplace=True)\ntrain","metadata":{"execution":{"iopub.status.busy":"2022-01-19T10:21:17.485787Z","iopub.execute_input":"2022-01-19T10:21:17.486011Z","iopub.status.idle":"2022-01-19T10:21:17.521805Z","shell.execute_reply.started":"2022-01-19T10:21:17.485984Z","shell.execute_reply":"2022-01-19T10:21:17.520939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"___\n# <a name=\"Validation\">‚úÖ Cross-validation method</a>","metadata":{}},{"cell_type":"markdown","source":"As afore-mentionned, we are dealing with time-series data.<br />\nThus, we do not want to use information about the future to train our model. We will therefore opt for <code>TimeSeriesSplit</code> as a our **cross-validation** technique.\n\nüìå According to the *scikit-learn* documentation:\n>[TimeSeriesSplit](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html) provides train/test indices to split time series data samples that are observed at fixed time intervals, in train/test sets. In each split, test indices must be higher than before, and thus shuffling in cross validator is inappropriate.","metadata":{}},{"cell_type":"code","source":"# Function modified from:\n# https://scikit-learn.org/stable/auto_examples/model_selection/plot_cv_indices.html\n# Inspired by https://www.kaggle.com/tomwarrens/timeseriessplit-how-to-use-it/notebook\n\nfrom matplotlib.patches import Patch\n\ndef plot_cv_indices(cv, X, y, n_splits, date_col=None):\n    \"\"\"Create a sample plot for indices of a cross-validation object.\"\"\"\n    \n    fig, ax = plt.subplots(1, 1, figsize = (12, 8))\n\n    # Generate the training/testing visualizations for each CV split\n    for ii, (tr, tt) in enumerate(cv.split(X=X, y=y)):\n        # Fill in indices with the training/test groups\n        indices = np.array([np.nan] * len(X))\n        indices[tt] = 1\n        indices[tr] = 0\n\n        # Visualize the results\n        ax.scatter(\n            range(len(indices)),\n            [ii + 0.5] * len(indices),\n            c=indices,\n            marker=\"_\",\n            lw=10,\n            cmap=cmap_cv,\n            vmin=-0.2,\n            vmax=1.2,\n            zorder=2\n        )\n\n    # Formatting\n    yticklabels = list(range(n_splits))\n    \n    if date_col is not None:\n        tick_locations  = ax.get_xticks()\n        tick_dates = [\" \"] + date_col.iloc[list(tick_locations[1:-1])].astype(str).tolist() + [\" \"]\n\n        tick_locations_str = [str(int(i)) for i in tick_locations]\n        new_labels = ['\\n\\n'.join(x) for x in zip(list(tick_locations_str), tick_dates)]\n        \n        ax.set_xticks(tick_locations)\n        ax.set_xticklabels(new_labels)\n    \n    # Custom visualization\n    ax.set_facecolor('#fcfcfc')\n    ax.grid(alpha=0.7, linewidth=1, zorder=0)\n    \n    ax.set_yticks(np.arange(n_splits) + .5)\n    ax.set_yticklabels(yticklabels)\n    ax.set_ylabel('CV iteration', fontsize=15, labelpad=10)\n    ax.set_ylim([n_splits+0.2, -.2])\n    ax.yaxis.set_tick_params(labelsize=12, pad=10, length=0)\n    \n    ax.set_xlabel('Sample index', fontsize=15, labelpad=10)\n    ax.xaxis.set_tick_params(labelsize=12, pad=10, length=0)\n    \n    ax.legend(\n        [\n            Patch(color=cmap_cv(.8)), \n            Patch(color=cmap_cv(.02))\n        ],\n        [\n            'Testing set', \n            'Training set'\n        ],\n        fontsize=12,\n        loc=(1.02, .8)\n    )\n    \n    ax.set_title(\n        '{}'.format(type(cv).__name__),\n        loc=\"left\", \n        color=\"#000\", \n        fontsize=20, \n        pad=5, \n        y=1, \n        zorder=3\n    )\n    \n    return ax","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-19T10:21:17.523089Z","iopub.execute_input":"2022-01-19T10:21:17.523301Z","iopub.status.idle":"2022-01-19T10:21:17.540015Z","shell.execute_reply.started":"2022-01-19T10:21:17.523274Z","shell.execute_reply":"2022-01-19T10:21:17.539224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import TimeSeriesSplit\n\nfolds = TimeSeriesSplit(n_splits=4)\n\n# Visualization\ncmap_cv = plt.cm.bwr\nplot_cv_indices(folds, train, y, folds.n_splits, date_col=date_copy);","metadata":{"execution":{"iopub.status.busy":"2022-01-19T10:21:17.541393Z","iopub.execute_input":"2022-01-19T10:21:17.542128Z","iopub.status.idle":"2022-01-19T10:21:19.279783Z","shell.execute_reply.started":"2022-01-19T10:21:17.542081Z","shell.execute_reply":"2022-01-19T10:21:19.278607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"___\n# <a name=\"TrainingInference\">üèãÔ∏è Model Training & Inference</a>","metadata":{}},{"cell_type":"markdown","source":"Submissions are evaluated on SMAPE between forecasts and actual values.","metadata":{}},{"cell_type":"markdown","source":"![SMAPE formula](https://media.geeksforgeeks.org/wp-content/uploads/20211120224204/smapeformula.png)","metadata":{}},{"cell_type":"code","source":"def smape(actual, predicted):\n    numerator = np.abs(predicted - actual)\n    denominator = (np.abs(actual) + np.abs(predicted)) / 2\n    \n    return np.mean(numerator / denominator)*100","metadata":{"execution":{"iopub.status.busy":"2022-01-19T10:21:19.282833Z","iopub.execute_input":"2022-01-19T10:21:19.283099Z","iopub.status.idle":"2022-01-19T10:21:19.288481Z","shell.execute_reply.started":"2022-01-19T10:21:19.283069Z","shell.execute_reply":"2022-01-19T10:21:19.287523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training phase","metadata":{}},{"cell_type":"markdown","source":"**Tip:** Since the SMAPE evaluation metric is asymmetric. In this case, underestimated values are much more penalized than overestimated values. Then, feel free to round your predictions **up** to the nearest value.<br />\n<br />\nüìå You will find more by having a glance to these awesome notebooks: \n> * [SMAPE Weirdness](https://www.kaggle.com/cpmpml/smape-weirdness) by [CPMP](https://www.kaggle.com/cpmpml)\n> * [TPS Jan 2022: A simple average model (no ML)](https://www.kaggle.com/carlmcbrideellis/tps-jan-2022-a-simple-average-model-no-ml) by [Carl McBride Ellis](https://www.kaggle.com/carlmcbrideellis).\n>\n>The last one being related to this very Tabular Playground.","metadata":{}},{"cell_type":"code","source":"from catboost import CatBoostRegressor\n\ny_pred = np.zeros(len(test))\nscores = []\n\nfor fold, (train_id, test_id) in enumerate(folds.split(train, groups=date_copy.dt.year)):\n    print(\"Fold: \", fold)\n    \n    # Splitting\n    X_train, y_train = train.iloc[train_id], y.iloc[train_id]\n    X_valid, y_valid = train.iloc[test_id], y.iloc[test_id]\n    \n    # Model with parameters\n    params = {\n        'iterations': 10000,\n        'depth': 5, \n        'l2_leaf_reg': 12.06,\n        'bootstrap_type': 'Bayesian',\n        'boosting_type': 'Plain',\n        'loss_function': 'MAE',\n        'eval_metric': 'SMAPE',\n        'od_type': 'Iter',       # type of overfitting detector\n        'od_wait': 40,\n        'has_time': True         # use the order of the data (ts), do not permute\n    }\n    \n    model = CatBoostRegressor(**params)\n\n    # Training\n    model.fit(\n        X_train, y_train, \n        eval_set=(X_valid, y_valid),\n        early_stopping_rounds=1000,\n        verbose=1000\n    )\n    \n    print('\\n')\n    \n    # Evaluation\n    valid_pred = model.predict(X_valid)\n    \n    valid_score = smape(\n        np.expm1(y_valid) * X_valid.gdp.values, \n        np.ceil(np.expm1(valid_pred) * X_valid.gdp.values)\n    )\n    \n    scores.append(valid_score)\n    \n    # Prediction for submission\n    y_pred += (np.expm1(model.predict(test)) * test.gdp.values) / folds.n_splits","metadata":{"execution":{"iopub.status.busy":"2022-01-19T10:21:19.289683Z","iopub.execute_input":"2022-01-19T10:21:19.289949Z","iopub.status.idle":"2022-01-19T10:22:05.98624Z","shell.execute_reply.started":"2022-01-19T10:21:19.289912Z","shell.execute_reply":"2022-01-19T10:22:05.985398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Evaluation","metadata":{}},{"cell_type":"markdown","source":"Next, we can evaluate the model thanks to our custom SMAPE function.","metadata":{}},{"cell_type":"code","source":"score = np.array(scores).mean()\nprint('Mean SMAPE score: ', score)","metadata":{"execution":{"iopub.status.busy":"2022-01-19T10:22:05.987307Z","iopub.execute_input":"2022-01-19T10:22:05.98754Z","iopub.status.idle":"2022-01-19T10:22:05.992779Z","shell.execute_reply.started":"2022-01-19T10:22:05.987511Z","shell.execute_reply":"2022-01-19T10:22:05.991697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Submission","metadata":{}},{"cell_type":"code","source":"submission = pd.read_csv('../input/tabular-playground-series-jan-2022/sample_submission.csv')\nsubmission.num_sold = np.ceil(y_pred) # rounding up\nsubmission","metadata":{"execution":{"iopub.status.busy":"2022-01-19T10:22:05.994105Z","iopub.execute_input":"2022-01-19T10:22:05.994392Z","iopub.status.idle":"2022-01-19T10:22:06.02587Z","shell.execute_reply.started":"2022-01-19T10:22:05.994352Z","shell.execute_reply":"2022-01-19T10:22:06.024979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-19T10:22:06.027185Z","iopub.execute_input":"2022-01-19T10:22:06.027482Z","iopub.status.idle":"2022-01-19T10:22:06.054426Z","shell.execute_reply.started":"2022-01-19T10:22:06.027449Z","shell.execute_reply":"2022-01-19T10:22:06.053414Z"},"trusted":true},"execution_count":null,"outputs":[]}]}