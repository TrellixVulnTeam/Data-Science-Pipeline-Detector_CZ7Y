{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h3>Our teacher (Ambrosm) gave us an homework after his exciting lesson :-)\n    \n    \nhttps://www.kaggle.com/ambrosm/tpsjan22-06-lightgbm-quickstart\n    \nPlease upvote, I would like my degree  :-)","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport lightgbm\nimport xgboost as xgb\nfrom catboost import CatBoostRegressor\nimport pickle\nfrom datetime import datetime\nfrom matplotlib import pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.model_selection import KFold\nimport dateutil.easter as easter","metadata":{"execution":{"iopub.status.busy":"2022-01-09T08:35:07.964633Z","iopub.execute_input":"2022-01-09T08:35:07.964918Z","iopub.status.idle":"2022-01-09T08:35:07.970449Z","shell.execute_reply.started":"2022-01-09T08:35:07.96489Z","shell.execute_reply":"2022-01-09T08:35:07.969296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2> Exactly the same Features engineering from AmbrosM","metadata":{}},{"cell_type":"code","source":"original_train_df = pd.read_csv('../input/tabular-playground-series-jan-2022/train.csv', parse_dates=['date'])\noriginal_test_df = pd.read_csv('../input/tabular-playground-series-jan-2022/test.csv', parse_dates=['date'])\ngdp_df = pd.read_csv('../input/tps-2022-1-gdp/GDP_data_2015_to_2019_Finland_Norway_Sweden.csv')","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-01-09T08:35:07.972967Z","iopub.execute_input":"2022-01-09T08:35:07.973727Z","iopub.status.idle":"2022-01-09T08:35:08.036502Z","shell.execute_reply.started":"2022-01-09T08:35:07.973677Z","shell.execute_reply":"2022-01-09T08:35:08.035461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gdp_df.index=gdp_df['year']\ngdp_df.drop('year',axis=1,inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-09T08:35:08.038039Z","iopub.execute_input":"2022-01-09T08:35:08.038267Z","iopub.status.idle":"2022-01-09T08:35:08.043628Z","shell.execute_reply.started":"2022-01-09T08:35:08.038241Z","shell.execute_reply":"2022-01-09T08:35:08.042656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def smape_loss(y_true, y_pred):\n    \"\"\"SMAPE Loss\"\"\"\n    return np.abs(y_true - y_pred) / (y_true + np.abs(y_pred)) * 200","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-01-09T08:35:08.045258Z","iopub.execute_input":"2022-01-09T08:35:08.045787Z","iopub.status.idle":"2022-01-09T08:35:08.059051Z","shell.execute_reply.started":"2022-01-09T08:35:08.045742Z","shell.execute_reply":"2022-01-09T08:35:08.058335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2> Feature engineering","metadata":{}},{"cell_type":"code","source":"def get_gdp(row):\n    \"\"\"Return the GDP based on row.country and row.date.year\"\"\"\n    country = 'GDP_' + row.country\n    return gdp_df.loc[row.date.year, country]\n\nle_dict = {feature: LabelEncoder().fit(original_train_df[feature]) for feature in ['country', 'product', 'store']}\n\ndef engineer(df):\n    \"\"\"Return a new dataframe with the engineered features\"\"\"\n    \n    new_df = pd.DataFrame({'gdp': df.apply(get_gdp, axis=1),\n                           'dayofyear': df.date.dt.dayofyear,\n                           'wd4': df.date.dt.weekday == 4, # Friday\n                           'wd56': df.date.dt.weekday >= 5, # Saturday and Sunday\n                          })\n\n    new_df.loc[(df.date.dt.year != 2016) & (df.date.dt.month >=3), 'dayofyear'] += 1 # fix for leap years\n    \n    for feature in ['country', 'product', 'store']:\n        new_df[feature] = le_dict[feature].transform(df[feature])\n        \n    # Easter\n    easter_date = df.date.apply(lambda date: pd.Timestamp(easter.easter(date.year)))\n    new_df['days_from_easter'] = (df.date - easter_date).dt.days.clip(-5, 65)\n    \n    # Last Sunday of May (Mother's Day)\n    sun_may_date = df.date.dt.year.map({2015: pd.Timestamp(('2015-5-31')),\n                                         2016: pd.Timestamp(('2016-5-29')),\n                                         2017: pd.Timestamp(('2017-5-28')),\n                                         2018: pd.Timestamp(('2018-5-27')),\n                                         2019: pd.Timestamp(('2019-5-26'))})\n    #new_df['days_from_sun_may'] = (df.date - sun_may_date).dt.days.clip(-1, 9)\n    \n    # Last Wednesday of June\n    wed_june_date = df.date.dt.year.map({2015: pd.Timestamp(('2015-06-24')),\n                                         2016: pd.Timestamp(('2016-06-29')),\n                                         2017: pd.Timestamp(('2017-06-28')),\n                                         2018: pd.Timestamp(('2018-06-27')),\n                                         2019: pd.Timestamp(('2019-06-26'))})\n    new_df['days_from_wed_jun'] = (df.date - wed_june_date).dt.days.clip(-5, 5)\n    \n    # First Sunday of November (second Sunday is Father's Day)\n    sun_nov_date = df.date.dt.year.map({2015: pd.Timestamp(('2015-11-1')),\n                                         2016: pd.Timestamp(('2016-11-6')),\n                                         2017: pd.Timestamp(('2017-11-5')),\n                                         2018: pd.Timestamp(('2018-11-4')),\n                                         2019: pd.Timestamp(('2019-11-3'))})\n    new_df['days_from_sun_nov'] = (df.date - sun_nov_date).dt.days.clip(-1, 9)\n    \n    return new_df\n\ntrain_df = engineer(original_train_df)\ntrain_df['date'] = original_train_df.date # used in GroupKFold\ntrain_df['num_sold'] = original_train_df.num_sold.astype(np.float32)\ntrain_df['target'] = np.log(train_df['num_sold'] / train_df['gdp'])\ntest_df = engineer(original_test_df)\n\nfeatures = test_df.columns.difference(['gdp'])","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-01-09T08:35:08.061354Z","iopub.execute_input":"2022-01-09T08:35:08.061596Z","iopub.status.idle":"2022-01-09T08:35:09.657088Z","shell.execute_reply.started":"2022-01-09T08:35:08.061566Z","shell.execute_reply":"2022-01-09T08:35:09.656238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2> Parameters from Optuna tunings","metadata":{}},{"cell_type":"code","source":"lgb_params = {\n        'objective': 'regression',\n        'force_row_wise': True,\n        'verbosity': -1,\n        'seed': 1,\n        'learning_rate': 0.03,\n        'lambda_l1': 5e-05,\n        'lambda_l2': 1e-06,\n        'num_leaves': 20,\n        'feature_fraction': 0.6,\n        'bagging_fraction': 0.43,\n        'bagging_freq': 5,\n        'min_child_samples': 17,\n        }                        \n\ncat_params = {\n        'eval_metric': 'SMAPE', \n        'use_best_model': True,\n        'learning_rate': 0.04421730001498909,\n        'depth': 6,\n        'l2_leaf_reg': 0.24960109471113703,\n        'random_strength': 2.1314060037536735,\n        'grow_policy': 'SymmetricTree',\n        'max_bin': 406,\n        'min_data_in_leaf': 77,\n        'bootstrap_type': 'Bayesian',\n        'bagging_temperature': 0.7392707417524894}\n\nxgb_params = {\n        'tree_method': 'hist',\n        'grow_policy' : 'lossguide',\n        'learning_rate': 0.03399878704233446,\n        'max_depth': 5,\n        'reg_alpha': 0.7814373604498039,\n        'reg_lambda': 0.00018093104956619317,\n        'max_delta_step': 2,\n        'min_child_weight': 14,\n        'colsample_bytree': 0.6489299778623602,\n        'subsample': 0.6033298718112065,\n        'max_leaves': 187,  \n        }","metadata":{"tags":[],"execution":{"iopub.status.busy":"2022-01-09T08:35:09.658391Z","iopub.execute_input":"2022-01-09T08:35:09.658636Z","iopub.status.idle":"2022-01-09T08:35:09.667279Z","shell.execute_reply.started":"2022-01-09T08:35:09.658604Z","shell.execute_reply":"2022-01-09T08:35:09.666577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2> Training and predictions","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\ndef training_prediction() :\n    \n    run = 5 # for seeds blending\n    test_pred_list = []\n    cat_test_pred_list = []\n    all_score_list = []\n    lgb_score_list = []\n    cat_score_list = []\n    xgb_score_list = []\n\n    xgb_test = xgb.DMatrix(test_df[features])\n\n    kf = GroupKFold(n_splits=4)\n\n    for i in range(run):\n\n        lgb_params['seed'] = i\n        cat_params['random_seed'] = i\n        xgb_params['seed'] = i\n\n        print(25*'-',\"RUN\",i,25*'-')\n\n        for fold, (train_idx, val_idx) in enumerate(\n            kf.split(train_df,\n            groups = train_df.date.dt.year)):\n\n            X_tr = train_df.iloc[train_idx]\n            X_va = train_df.iloc[val_idx]\n\n            # Preprocess the train data\n            X_tr_f = X_tr[features]\n            y_tr = X_tr.target.values\n\n            lgb_data_tr = lightgbm.Dataset(\n                        X_tr[features],\n                        label = y_tr,\n                        categorical_feature = ['country',\n                                             'product',\n                                             'store'])\n\n            xgb_data_tr = xgb.DMatrix(\n                        X_tr[features],\n                        label=y_tr)\n\n            # Preprocess the validation data\n            X_va_f = X_va[features]\n            y_va = X_va.target.values\n\n            lgb_data_va = lightgbm.Dataset(\n                        X_va[features], \n                        label = y_va)\n\n            xgb_data_va = xgb.DMatrix(\n                        X_va[features],\n                        label = y_va)\n            evallist = [(xgb_data_va, 'eval'), \n                        (xgb_data_tr, 'train')]\n\n            # Training  \n            lgb_model = lightgbm.train(\n                        lgb_params,\n                        lgb_data_tr,\n                        num_boost_round=2000,\n                        categorical_feature =['country',\n                                             'product',\n                                             'store'])\n\n            cat_model = CatBoostRegressor(**cat_params) \n            cat_model.fit(\n                        X_tr_f,\n                        y_tr,eval_set =[( X_va_f,y_va)],\n                        verbose = 0,\n                        early_stopping_rounds = 200)\n\n            xgb_model = xgb.train(\n                        xgb_params, \n                        xgb_data_tr,\n                        num_boost_round=2000, \n                        evals = evallist,\n                        verbose_eval = 0,\n                        early_stopping_rounds = 200)\n\n            # Predictions\n            lgb_y_va_pred = np.exp(lgb_model.predict(X_va_f)) * X_va['gdp']\n            test_pred_list.append(np.exp(lgb_model.predict(test_df[features])) * test_df['gdp'].values)\n\n            cat_y_va_pred = np.exp(cat_model.predict(X_va_f)) * X_va['gdp']\n            test_pred_list.append(np.exp(cat_model.predict(test_df[features])) * test_df['gdp'].values)\n            cat_test_pred_list.append(np.exp(cat_model.predict(test_df[features])) * test_df['gdp'].values)\n\n            xgb_y_va_pred = np.exp(xgb_model.predict(xgb_data_va)) * X_va['gdp']\n            test_pred_list.append(np.exp(xgb_model.predict(xgb_test)) * test_df['gdp'].values)\n\n            del  xgb_data_tr, xgb_data_va, lgb_data_va\n\n            # Score list for each algo\n            lgb_smape = np.round(np.mean(smape_loss(X_va.num_sold, lgb_y_va_pred)),4)\n            lgb_score_list.append(lgb_smape)\n\n            cat_smape = np.round(np.mean(smape_loss(X_va.num_sold, cat_y_va_pred)),4)\n            cat_score_list.append(cat_smape)\n\n            xgb_smape = np.round(np.mean(smape_loss(X_va.num_sold, xgb_y_va_pred)),4)\n            xgb_score_list.append(xgb_smape)\n\n            # list for total average score (mean)\n            all_score_list += lgb_score_list + cat_score_list + xgb_score_list\n\n        print('RUN', i,\"Cumulative Average SMAPE\", np.round(sum(all_score_list) / len(all_score_list),4))\n    print(40*'*')   \n    print(\"TOTAL Average SMAPE   :\", np.round(sum(all_score_list) / len(all_score_list),4))\n    print(40*'*') \n    print(\"\\nOf which :\")\n    print(\"LGB Average SMAPE   :\", np.round(sum(lgb_score_list) / len(lgb_score_list),4))\n    print(\"CAT Average SMAPE   :\", np.round(sum(cat_score_list) / len(cat_score_list),4))\n    print(\"XGB Average SMAPE   :\", np.round(sum(xgb_score_list) / len(xgb_score_list),4),'\\n\\n')\n\n\n    # Training scores visualization\n    plt.figure(figsize = (12,7))\n    plt.plot(lgb_score_list,label ='LightGBM')\n    plt.plot(cat_score_list,label ='CatBoost')\n    plt.plot(xgb_score_list,label ='XgBoost')\n\n    for i in range(run-1):\n        plt.axvline(x = (i+1) * 4, \n                    label = 'RUN' +str(i+1),\n                    linewidth = 2, \n                    color ='black',\n                    linestyle = 'dotted')\n    plt.axvline(x = 19, \n                    label ='RUN' +'4',\n                    linewidth = 2, \n                    color ='black',\n                    linestyle = 'dotted')\n\n    plt.ylabel('SMAPE')\n    plt.xlabel('RUN x FOLDS')\n    plt.title('Comparison between runs and Algo',fontsize = 20)\n    plt.legend()\n    plt.show()\n    \n    return lgb_score_list, cat_score_list, xgb_score_list, all_score_list, test_pred_list, cat_test_pred_list","metadata":{"execution":{"iopub.status.busy":"2022-01-09T08:35:09.668531Z","iopub.execute_input":"2022-01-09T08:35:09.669355Z","iopub.status.idle":"2022-01-09T08:35:09.693105Z","shell.execute_reply.started":"2022-01-09T08:35:09.669298Z","shell.execute_reply":"2022-01-09T08:35:09.692231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2> First training","metadata":{}},{"cell_type":"code","source":"lgb_score_list, cat_score_list, xgb_score_list, all_score_list, test_pred_list, cat_test_pred_list = training_prediction()","metadata":{"execution":{"iopub.status.busy":"2022-01-09T08:35:09.694525Z","iopub.execute_input":"2022-01-09T08:35:09.694777Z","iopub.status.idle":"2022-01-09T08:42:31.575461Z","shell.execute_reply.started":"2022-01-09T08:35:09.694746Z","shell.execute_reply":"2022-01-09T08:42:31.57448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h4> We can see that SEED does not provide very different results between each run.\nCatboost provides the best result and xgboost the worse, some work has to be done for xgb tuning ...","metadata":{}},{"cell_type":"markdown","source":"<h2> Second training with pseudo labelling","metadata":{}},{"cell_type":"code","source":"#https://www.kaggle.com/andrej0marinchenko/tps-jan-2022-automated-ensembling :\npseudo = pd.read_csv('../input/best-submission/submission_best.csv')\npseudo_test = original_test_df.copy()\npseudo_test['num_sold'] = pseudo['num_sold']\npseudo_df = engineer(pseudo_test)\npseudo_df['date'] = pseudo_test.date \npseudo_df['num_sold'] = pseudo_test.num_sold.astype(np.float32)\npseudo_df['target'] = np.log(pseudo_df['num_sold'] / pseudo_df['gdp'])\n\ntrain_df = pd.concat([train_df,pseudo_df],axis=0)\ntrain_df = train_df.reset_index(drop = True)","metadata":{"execution":{"iopub.status.busy":"2022-01-09T08:42:31.576893Z","iopub.execute_input":"2022-01-09T08:42:31.577153Z","iopub.status.idle":"2022-01-09T08:42:31.90115Z","shell.execute_reply.started":"2022-01-09T08:42:31.577105Z","shell.execute_reply":"2022-01-09T08:42:31.900313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"p_lgb_score_list, p_cat_score_list, p_xgb_score_list, p_all_score_list, p_test_pred_list, p_cat_test_pred_list = training_prediction()","metadata":{"execution":{"iopub.status.busy":"2022-01-09T08:42:31.902563Z","iopub.execute_input":"2022-01-09T08:42:31.903157Z","iopub.status.idle":"2022-01-09T08:50:16.145318Z","shell.execute_reply.started":"2022-01-09T08:42:31.903109Z","shell.execute_reply":"2022-01-09T08:50:16.144498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h4> We can see that algo are crushed by pseudo labels...no benefits","metadata":{}},{"cell_type":"markdown","source":"<h2> Submissions","metadata":{}},{"cell_type":"code","source":"sub = pd.read_csv('../input/tabular-playground-series-jan-2022/sample_submission.csv')\nsub['num_sold'] = sum(p_test_pred_list) / len(p_test_pred_list)\nsub.to_csv('submission_cat_lgb_xgb.csv', index = False)\npd.read_csv('submission_cat_lgb_xgb.csv').head(2)","metadata":{"execution":{"iopub.status.busy":"2022-01-09T08:50:16.224093Z","iopub.execute_input":"2022-01-09T08:50:16.22586Z","iopub.status.idle":"2022-01-09T08:50:16.265273Z","shell.execute_reply.started":"2022-01-09T08:50:16.225816Z","shell.execute_reply":"2022-01-09T08:50:16.264404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub['num_sold'] = sum(p_cat_test_pred_list) / len(p_cat_test_pred_list)\nsub.to_csv('submission_cat.csv', index = False)\npd.read_csv('submission_cat.csv').head(2)","metadata":{"execution":{"iopub.status.busy":"2022-01-09T08:50:16.267351Z","iopub.execute_input":"2022-01-09T08:50:16.267562Z","iopub.status.idle":"2022-01-09T08:50:16.298619Z","shell.execute_reply.started":"2022-01-09T08:50:16.267536Z","shell.execute_reply":"2022-01-09T08:50:16.297852Z"},"trusted":true},"execution_count":null,"outputs":[]}]}