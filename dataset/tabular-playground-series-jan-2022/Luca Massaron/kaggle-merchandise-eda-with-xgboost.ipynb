{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#### In this notebook I extend my previous EDA and linear model to an XGBoost model and I optimize it for SMAPE using Optuna.","metadata":{}},{"cell_type":"markdown","source":"##### We first load the data, it is necessary paying attention to convert the date into datetime","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn\nimport dateutil.easter as easter\nfrom xgboost import XGBRegressor\nimport optuna\nfrom optuna.integration import XGBoostPruningCallback","metadata":{"execution":{"iopub.status.busy":"2022-01-18T16:32:40.031906Z","iopub.execute_input":"2022-01-18T16:32:40.032237Z","iopub.status.idle":"2022-01-18T16:32:40.037141Z","shell.execute_reply.started":"2022-01-18T16:32:40.032197Z","shell.execute_reply":"2022-01-18T16:32:40.036207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loading train and test data\ntrain = pd.read_csv(\"../input/tabular-playground-series-jan-2022/train.csv\", parse_dates=['date'])\ntest = pd.read_csv(\"../input/tabular-playground-series-jan-2022/test.csv\", parse_dates=['date'])","metadata":{"execution":{"iopub.status.busy":"2022-01-18T16:32:40.03854Z","iopub.execute_input":"2022-01-18T16:32:40.039143Z","iopub.status.idle":"2022-01-18T16:32:40.086694Z","shell.execute_reply.started":"2022-01-18T16:32:40.039101Z","shell.execute_reply":"2022-01-18T16:32:40.086009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.dtypes","metadata":{"execution":{"iopub.status.busy":"2022-01-18T16:32:40.088197Z","iopub.execute_input":"2022-01-18T16:32:40.088499Z","iopub.status.idle":"2022-01-18T16:32:40.096504Z","shell.execute_reply.started":"2022-01-18T16:32:40.08846Z","shell.execute_reply":"2022-01-18T16:32:40.095682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Before starting with EDA, it is important to check about the data structure. Apparently we have a combination of time series based on countries, stores and products. Let's first check if all the combinations appear in train and test.","metadata":{}},{"cell_type":"code","source":"# figuring out the theoretically possible level combination\ntime_series = ['country', 'store', 'product']\ncombinations = 1\nfor feat in time_series:\n    combinations *= train[feat].nunique()\n    \nprint(f\"There are {combinations} possible combinations\")","metadata":{"execution":{"iopub.status.busy":"2022-01-18T16:32:40.098316Z","iopub.execute_input":"2022-01-18T16:32:40.098848Z","iopub.status.idle":"2022-01-18T16:32:40.114079Z","shell.execute_reply.started":"2022-01-18T16:32:40.098809Z","shell.execute_reply":"2022-01-18T16:32:40.113277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"time_series = ['country', 'store', 'product']\ncountry_store_product_train = train[time_series].drop_duplicates().sort_values(time_series)\ncountry_store_product_test =test[time_series].drop_duplicates().sort_values(time_series)\n\ncond_1 = len(country_store_product_train) == combinations\nprint(f\"Are all theoretical combinations present in train: {cond_1}\")\ncond_2 = (country_store_product_train == country_store_product_test).all().all()\nprint(f\"Are combinations the same in train and test: {cond_2}\")","metadata":{"execution":{"iopub.status.busy":"2022-01-18T16:32:40.115539Z","iopub.execute_input":"2022-01-18T16:32:40.115834Z","iopub.status.idle":"2022-01-18T16:32:40.14163Z","shell.execute_reply.started":"2022-01-18T16:32:40.115788Z","shell.execute_reply":"2022-01-18T16:32:40.140973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### As a second step let's visualize how time is split between train and test.","metadata":{}},{"cell_type":"code","source":"train_dates = train.date.drop_duplicates().sort_values()\ntest_dates = test.date.drop_duplicates().sort_values()\n\nfig, ax = plt.subplots(1, 1, figsize = (11, 7))\ncmap_cv = plt.cm.coolwarm\n\ncolor_index = np.array([1] * len(train_dates) + [0] * len(test_dates))\n\nax.scatter(range(len(train_dates)), [.5] * len(train_dates),\n           c=color_index[:len(train_dates)], marker='_', lw=15, cmap=cmap_cv,\n           label='train', vmin=-.2, vmax=1.2)\n\nax.scatter(range(len(train_dates), len(train_dates) + len(test_dates)), [.55] * len(test_dates),\n           c=color_index[len(train_dates):], marker='_', lw=15, cmap=cmap_cv,\n           label='test', vmin=-.2, vmax=1.2)\n\ntick_locations = np.cumsum([0, 365, 366, 365, 365, 365])\nfor i in (tick_locations):\n    ax.vlines(i, 0, 2,linestyles='dotted', colors = 'grey')\n    \nax.set_xticks(tick_locations)\nax.set_xticklabels([2015, 2016, 2017, 2018, 2019, 2020], rotation = 0)\nax.set_yticklabels(labels=[])\nplt.ylim([0.45, 0.60])\nax.legend(loc=\"upper left\", title=\"data\")\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-18T16:32:40.142902Z","iopub.execute_input":"2022-01-18T16:32:40.143186Z","iopub.status.idle":"2022-01-18T16:32:40.357968Z","shell.execute_reply.started":"2022-01-18T16:32:40.143126Z","shell.execute_reply":"2022-01-18T16:32:40.357321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Having four complete years available allows various types of testing and modelling. In this EDA we will limit to use the last year available (2018) as an hold-out, setting our baseline model to be able to forecast an entire year in the future.","metadata":{}},{"cell_type":"markdown","source":"##### As a last check we verify that no date is missing from train and test:","metadata":{}},{"cell_type":"code","source":"missing_train = pd.date_range(start=train_dates.min(), end=train_dates.max()).difference(train_dates)\nmissing_test = pd.date_range(start=test_dates.min(), end=test_dates.max()).difference(test_dates)\nprint(f\"missing dates in train: {len(missing_train)} and in test: {len(missing_test)}\")","metadata":{"execution":{"iopub.status.busy":"2022-01-18T16:32:40.359049Z","iopub.execute_input":"2022-01-18T16:32:40.359288Z","iopub.status.idle":"2022-01-18T16:32:40.367873Z","shell.execute_reply.started":"2022-01-18T16:32:40.359254Z","shell.execute_reply":"2022-01-18T16:32:40.367098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Having completed the checks, we process the datetime information, extracting it informative elements at different time granularities:","metadata":{}},{"cell_type":"code","source":"# We create different time granularity\n\ndef process_time(df):\n    df['year'] = df['date'].dt.year\n    df['month'] = df['date'].dt.month\n    df['week'] = df['date'].dt.isocalendar().week\n    df['week'][df['week']>52] = 52\n    df['day'] = df['date'].dt.day\n    df['dayofweek'] = df['date'].dt.dayofweek\n    df['quarter'] = df['date'].dt.quarter\n    df['dayofyear'] = df['date'].dt.dayofyear\n    # leap year correction\n    df.loc[(df.date.dt.is_leap_year) & (df.dayofyear >= 60),'dayofyear'] -= 1\n    return df\n\ntrain = process_time(train)\ntest = process_time(test)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T16:32:40.369617Z","iopub.execute_input":"2022-01-18T16:32:40.370122Z","iopub.status.idle":"2022-01-18T16:32:40.431846Z","shell.execute_reply.started":"2022-01-18T16:32:40.370052Z","shell.execute_reply":"2022-01-18T16:32:40.431163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### We are ready to explore the data. In order to highlight the time series characteristics, we create panels of products x countries x shops. We start by aggregating at a year level.","metadata":{}},{"cell_type":"code","source":"for product in ['Kaggle Mug', 'Kaggle Hat', 'Kaggle Sticker']:\n    print(f\"\\n--- {product} ---\\n\")\n    fig = plt.figure(figsize=(20, 10), dpi=100)\n    fig.subplots_adjust(hspace=0.25)\n    for i, store in enumerate(['KaggleMart', 'KaggleRama']):\n        for j, country in enumerate(['Finland', 'Norway', 'Sweden']):\n            ax = fig.add_subplot(2, 3, (i*3+j+1))\n            selection = (train['country']==country)&(train['store']==store)&(train['product']==product)\n            selected = train[selection]\n            selected.set_index('date').groupby('year')['num_sold'].mean().plot(ax=ax)\n            ax.set_title(f\"{country}:{store}\")\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-18T16:32:40.43292Z","iopub.execute_input":"2022-01-18T16:32:40.433463Z","iopub.status.idle":"2022-01-18T16:32:43.139915Z","shell.execute_reply.started":"2022-01-18T16:32:40.433428Z","shell.execute_reply":"2022-01-18T16:32:43.139239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### The first series of panels points out that the country effect is kind of indipendent from store and product. There is an underlying country dynamic that replicates the same no matter the shop or the product sold by it. We also notice that shops differentiate only for the level of sales.","metadata":{}},{"cell_type":"markdown","source":"##### Our next panels will explore seaasonality based on months:","metadata":{}},{"cell_type":"code","source":"for product in ['Kaggle Mug', 'Kaggle Hat', 'Kaggle Sticker']:\n    fig = plt.figure(figsize=(20, 10), dpi=100)\n    fig.subplots_adjust(hspace=0.25)\n    for i, store in enumerate(['KaggleMart', 'KaggleRama']):\n        for j, country in enumerate(['Finland', 'Norway', 'Sweden']):\n            ax = fig.add_subplot(2, 3, (i*3+j+1))\n            selection = (train['country']==country)&(train['store']==store)&(train['product']==product)\n            selected = train[selection]\n            for year in [2015, 2016, 2017, 2018]:\n                selected[selected.year==year].set_index('date').groupby('month')['num_sold'].mean().plot(ax=ax, label=year)\n            ax.set_title(f\"{product} | {country}:{store}\")\n            ax.legend()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-18T16:32:43.141142Z","iopub.execute_input":"2022-01-18T16:32:43.14182Z","iopub.status.idle":"2022-01-18T16:32:46.683851Z","shell.execute_reply.started":"2022-01-18T16:32:43.14178Z","shell.execute_reply":"2022-01-18T16:32:46.683231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Here we notice two important elements: seanality curves are different for each product and they also differ from year to year. Averaging the curves probably is safe bet for the future, as well as considering more relevant the recent years (thus weighting more the year 2018 for instance). For the sticker product, year 2017 seems particularly different from others.","metadata":{}},{"cell_type":"markdown","source":"##### We now proceed to examine seasonality even more in detail at a week level:","metadata":{}},{"cell_type":"code","source":"for product in ['Kaggle Mug', 'Kaggle Hat', 'Kaggle Sticker']:\n    print(f\"\\n--- {product} ---\\n\")\n    fig = plt.figure(figsize=(20, 10), dpi=100)\n    fig.subplots_adjust(hspace=0.25)\n    for i, store in enumerate(['KaggleMart', 'KaggleRama']):\n        for j, country in enumerate(['Finland', 'Norway', 'Sweden']):\n            ax = fig.add_subplot(2, 3, (i*3+j+1))\n            selection = (train['country']==country)&(train['store']==store)&(train['product']==product)\n            selected = train[selection]\n            for year in [2015, 2016, 2017, 2018]:\n                selected[selected.year==year].set_index('date').groupby('week')['num_sold'].mean().plot(ax=ax, label=year)\n            ax.set_title(f\"{country}:{store}\")\n            ax.legend()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-18T16:32:46.685101Z","iopub.execute_input":"2022-01-18T16:32:46.685744Z","iopub.status.idle":"2022-01-18T16:32:50.316991Z","shell.execute_reply.started":"2022-01-18T16:32:46.685706Z","shell.execute_reply":"2022-01-18T16:32:50.316338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### At a week level we see that differences are due to peaks. Peaks seem different in Spring. Probably is is Easter effect.","metadata":{}},{"cell_type":"markdown","source":"##### We now start obeserving recurrences at a monthly level:","metadata":{}},{"cell_type":"code","source":"for product in ['Kaggle Mug', 'Kaggle Hat', 'Kaggle Sticker']:\n    print(f\"\\n--- {product} ---\\n\")\n    fig = plt.figure(figsize=(20, 10), dpi=100)\n    fig.subplots_adjust(hspace=0.25)\n    for i, store in enumerate(['KaggleMart', 'KaggleRama']):\n        for j, country in enumerate(['Finland', 'Norway', 'Sweden']):\n            ax = fig.add_subplot(2, 3, (i*3+j+1))\n            selection = (train['country']==country)&(train['store']==store)&(train['product']==product)\n            selected = train[selection]\n            for year in [2015, 2016, 2017, 2018]:\n                selected[selected.year==year].set_index('date').groupby('day')['num_sold'].mean().plot(ax=ax, label=year)\n            ax.set_title(f\"{country}:{store}\")\n            ax.legend()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-18T16:32:50.320847Z","iopub.execute_input":"2022-01-18T16:32:50.321532Z","iopub.status.idle":"2022-01-18T16:32:53.954875Z","shell.execute_reply.started":"2022-01-18T16:32:50.321496Z","shell.execute_reply":"2022-01-18T16:32:53.954273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### The middle of the month usually presents less sales. The peak at the end may be influenced by seasonal peaks (end of year).","metadata":{}},{"cell_type":"markdown","source":"##### And we completed by inspecting at a day of the week level:","metadata":{}},{"cell_type":"code","source":"for product in ['Kaggle Mug', 'Kaggle Hat', 'Kaggle Sticker']:\n    print(f\"\\n--- {product} ---\\n\")\n    fig = plt.figure(figsize=(20, 10), dpi=100)\n    fig.subplots_adjust(hspace=0.25)\n    for i, store in enumerate(['KaggleMart', 'KaggleRama']):\n        for j, country in enumerate(['Finland', 'Norway', 'Sweden']):\n            ax = fig.add_subplot(2, 3, (i*3+j+1))\n            selection = (train['country']==country)&(train['store']==store)&(train['product']==product)\n            selected = train[selection]\n            for year in [2015, 2016, 2017, 2018]:\n                selected[selected.year==year].set_index('date').groupby('dayofweek')['num_sold'].sum().plot(ax=ax, label=year)\n            ax.set_title(f\"{country}:{store}\")\n            ax.legend()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-18T16:32:53.956153Z","iopub.execute_input":"2022-01-18T16:32:53.956631Z","iopub.status.idle":"2022-01-18T16:32:57.641901Z","shell.execute_reply.started":"2022-01-18T16:32:53.956595Z","shell.execute_reply":"2022-01-18T16:32:57.637761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Friday and the week-end are the best days, but Sundays are not always at the same level as Saturdays (it depends on the year - why?).","metadata":{}},{"cell_type":"markdown","source":"##### Based on the information we got we now proceed to feature engineering and to enrich the data (using festivities and GDP data).","metadata":{}},{"cell_type":"code","source":"festivities = pd.read_csv(\"../input/festivities-in-finland-norway-sweden-tsp-0122/nordic_holidays.csv\",\n                          parse_dates=['date'],\n                          usecols=['date', 'country', 'holiday'])","metadata":{"execution":{"iopub.status.busy":"2022-01-18T16:32:57.64305Z","iopub.execute_input":"2022-01-18T16:32:57.644578Z","iopub.status.idle":"2022-01-18T16:32:57.653973Z","shell.execute_reply.started":"2022-01-18T16:32:57.644538Z","shell.execute_reply":"2022-01-18T16:32:57.653097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gdp = pd.read_csv(\"../input/gdp-20152019-finland-norway-and-sweden/GDP_data_2015_to_2019_Finland_Norway_Sweden.csv\")\ngdp = np.concatenate([gdp[['year', 'GDP_Finland']].values, \n                      gdp[['year', 'GDP_Norway']].values, \n                      gdp[['year', 'GDP_Sweden']].values])\ngdp = pd.DataFrame(gdp, columns=['year', 'gdp'])\ngdp['country'] = ['Finland']*5 + ['Norway']*5 +['Sweden']*5","metadata":{"execution":{"iopub.status.busy":"2022-01-18T16:32:57.655551Z","iopub.execute_input":"2022-01-18T16:32:57.655849Z","iopub.status.idle":"2022-01-18T16:32:57.668213Z","shell.execute_reply.started":"2022-01-18T16:32:57.655809Z","shell.execute_reply":"2022-01-18T16:32:57.667325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### We now process the data and scale it. Since EDA revealed how the different characteristics of the series are mostly main effects (country and store), we focus on finding the way to model the interaction between products and time.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n\ndef process_data(df):\n    \n    processed = dict()\n    processed['row_id'] = df['row_id']\n    \n    print(\"creating dummies for main effects of time, country, store and product\")\n    to_dummies = ['country', 'store', 'product']\n    for feat in to_dummies:\n        tmp = pd.get_dummies(df[feat])\n        for col in tmp.columns:\n            processed[feat+'_'+str(col)] = tmp[col]\n    processed['wd4'] = (df.date.dt.weekday == 4).astype(int)\n    processed['wd56'] = (df.date.dt.weekday >= 5).astype(int)\n    \n    print(\"modelling time as continuous\")\n    processed['prog'] = ((df.row_id // 18) + 1)\n    \n    print(\"modelling time as cyclic\")\n    for time_measure in ['quarter', 'month', 'week', 'day', 'dayofyear', 'dayofweek']:\n        processed[time_measure] = df[time_measure]\n    \n    print(\"adding country gdp\")\n    gdp_exponent = 1.2121103201489674 # see https://www.kaggle.com/ambrosm/tpsjan22-03-linear-model for an explanation\n    gdp_countries = df.merge(gdp, on=['country', 'year'], how='left')['gdp'].values\n    processed['gdp'] = gdp_countries * gdp_exponent\n    \n    print(\"creating dummies and halo effect for Nordic holidays\")\n    tmp = pd.get_dummies(\n            df.merge(festivities, on=['date', 'country'], how='left').sort_values('row_id')['holiday'])\n    for col in tmp.columns:\n        processed['holiday_' + str(col)] =  tmp[col].values\n        halo = np.zeros(len(df))\n        dates = df[(tmp[col]==1).values].date.unique()\n        for date in dates:\n            year = date.astype('datetime64[Y]').astype(int) + 1970\n            halo[df.year.values==year] += (df.date[df.year==year] - date).dt.days.clip(lower=-14, upper=14).values \n        processed['holiday_halo_' + str(col)] = halo\n    \n    # Christmas\n    xmas_date = df.date.dt.year.apply(lambda year: pd.Timestamp(str(year)+'-12-25'))\n    processed['xmas_adjust'] = (df.date - xmas_date).dt.days.clip(lower=-20,upper=6)\n        \n    # New Year \n    processed['newyear_adjust1'] = df.dayofyear.clip(lower=0,upper=10)\n    processed['newyear_adjust2'] = df.dayofyear.clip(lower=0,upper=2)\n    \n    # Easter\n    easter_date = df.date.apply(lambda date: pd.Timestamp(easter.easter(date.year)))\n    processed['easter_adj'] = (df.date - easter_date).dt.days.clip(lower =-3,upper = 12)\n    \n    # Last Wednesday of June\n    wed_june_date = df.date.dt.year.map({2015: pd.Timestamp(('2015-06-24')),\n                                         2016: pd.Timestamp(('2016-06-29')),\n                                         2017: pd.Timestamp(('2017-06-28')),\n                                         2018: pd.Timestamp(('2018-06-27')),\n                                         2019: pd.Timestamp(('2019-06-26'))})\n    processed['days_from_wed_jun'] = (df.date - wed_june_date).dt.days.clip(-5, 5)\n    \n    #First Sunday of November (second Sunday is Father's Day)\n    sun_nov_date = df.date.dt.year.map({2015: pd.Timestamp(('2015-11-1')),\n                                         2016: pd.Timestamp(('2016-11-6')),\n                                         2017: pd.Timestamp(('2017-11-5')),\n                                         2018: pd.Timestamp(('2018-11-4')),\n                                         2019: pd.Timestamp(('2019-11-3'))})\n    processed['days_from_sun_nov'] = (df.date - sun_nov_date).dt.days.clip(-1, 9)\n    \n    print(f\"completed processing {len(processed)-1} features\")\n\n    values = list()\n    columns = list()\n    for key, value in processed.items():\n        value = np.array(value).astype(np.float32)\n        values.append(value)\n        columns.append(key)\n        \n    values = np.array(values).T\n    print(values.shape, values[1].shape, len(columns))\n    \n    processed = pd.DataFrame(values, columns=columns)\n    \n    print(\"resorting row ids\")\n    processed = processed.sort_values('row_id').set_index('row_id')\n    return processed\n\ndef process_target(df):\n    target = pd.DataFrame({'row_id':df['row_id'], 'num_sold':df['num_sold']})\n    target = target.sort_values('row_id').set_index('row_id')\n    return target\n\ntrain_test = process_data(train.append(test))\n\nprocessed_train = train_test.iloc[:len(train)].copy()\nprocessed_test = train_test.iloc[len(train):].copy()\n\ntarget = np.ravel(process_target(train))","metadata":{"execution":{"iopub.status.busy":"2022-01-18T16:41:50.85759Z","iopub.execute_input":"2022-01-18T16:41:50.857838Z","iopub.status.idle":"2022-01-18T16:41:52.175211Z","shell.execute_reply.started":"2022-01-18T16:41:50.85781Z","shell.execute_reply":"2022-01-18T16:41:52.173714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"processed_train.shape, train.shape, processed_test.shape, test.shape","metadata":{"execution":{"iopub.status.busy":"2022-01-18T16:42:18.926531Z","iopub.execute_input":"2022-01-18T16:42:18.927388Z","iopub.status.idle":"2022-01-18T16:42:18.93387Z","shell.execute_reply.started":"2022-01-18T16:42:18.927332Z","shell.execute_reply":"2022-01-18T16:42:18.933069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### We prepare all the evaluation measures, both at an aggregate level, with exp transformation and at an individual cases level (for error analysis)","metadata":{}},{"cell_type":"code","source":"def SMAPE(y_true, y_pred):\n    # From https://www.kaggle.com/cpmpml/smape-weirdness\n    denominator = (y_true + np.abs(y_pred)) / 200.0\n    diff = np.abs(y_true - y_pred) / denominator\n    diff[denominator == 0] = 0.0\n    return np.mean(diff)\n\ndef SMAPE_exp(y_true, y_pred):\n    y_true = np.exp(y_true)\n    y_pred = np.exp(y_pred)\n    denominator = (y_true + np.abs(y_pred)) / 200.0\n    diff = np.abs(y_true - y_pred) / denominator\n    diff[denominator == 0] = 0.0\n    return np.mean(diff)\n\ndef SMAPE_err(y_true, y_pred):\n    # From https://www.kaggle.com/cpmpml/smape-weirdness\n    denominator = (y_true + np.abs(y_pred)) / 200.0\n    diff = np.abs(y_true - y_pred) / denominator\n    diff[denominator == 0] = 0.0\n    return diff\n\ndef selective_rounding(preds, lower=0.3, upper=0.7):\n    # selective rounding\n    dec = preds % 1\n    to_round = (dec<=lower)|(dec>=upper)\n    preds[to_round] = np.round(preds[to_round])\n    return preds\n\ndef weighting(df, weights):\n    return df.year.replace(weights).values\n    \nweights = weighting(train, {2015:0.125, 2016:0.25, 2017:0.5, 2018:1})","metadata":{"execution":{"iopub.status.busy":"2022-01-18T16:42:28.186816Z","iopub.execute_input":"2022-01-18T16:42:28.188611Z","iopub.status.idle":"2022-01-18T16:42:28.199709Z","shell.execute_reply.started":"2022-01-18T16:42:28.18856Z","shell.execute_reply":"2022-01-18T16:42:28.198995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def objective(trial):\n    \n    params = {\n            'learning_rate': trial.suggest_float(\"learning_rate\", 1e-4, 1.0, log=True),\n            'reg_lambda': trial.suggest_loguniform(\"reg_lambda\", 1e-8, 100.0),\n            'reg_alpha': trial.suggest_loguniform(\"reg_alpha\", 1e-8, 100.0),\n            'subsample': trial.suggest_float(\"subsample\", 0.1, 1.0),\n            'colsample_bytree': trial.suggest_float(\"colsample_bytree\", 0.1, 1.0),\n            'max_depth': trial.suggest_int(\"max_depth\", 1, 9),\n            'min_child_weight': trial.suggest_int(\"min_child_weight\", 1, 25),\n            'gamma': trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True),\n            'grow_policy': trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"]),\n    }\n    \n    model = XGBRegressor(\n        random_state=0,\n        tree_method=\"gpu_hist\",\n        predictor=\"gpu_predictor\",\n        n_estimators=10_000,\n        **params,\n    )\n    \n    smape = list()\n    \n    for year in train.year.unique():\n        \n        train_set = list(train.row_id[train.year!=year])\n        val_set = list(train.row_id[train.year==year])\n\n        x = processed_train.iloc[train_set]\n        x_test = processed_train.iloc[val_set]\n        y = target[train_set]\n        y_test = target[val_set]\n    \n        model.fit(x, y, verbose=100)\n\n        preds = model.predict(x_test)\n        smape.append(SMAPE(y_test, preds))\n    \n    return np.mean(smape)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:16:02.580278Z","iopub.execute_input":"2022-01-18T17:16:02.580805Z","iopub.status.idle":"2022-01-18T17:16:02.590951Z","shell.execute_reply.started":"2022-01-18T17:16:02.580767Z","shell.execute_reply":"2022-01-18T17:16:02.590268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"study = optuna.create_study(direction=\"minimize\")\nstudy.optimize(objective, n_trials=60)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:16:08.744168Z","iopub.execute_input":"2022-01-18T17:16:08.744701Z","iopub.status.idle":"2022-01-18T17:37:59.269895Z","shell.execute_reply.started":"2022-01-18T17:16:08.744662Z","shell.execute_reply":"2022-01-18T17:37:59.269085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Best SMAPE on holdout data: {study.best_value:0.5f}\")","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:37:59.271573Z","iopub.execute_input":"2022-01-18T17:37:59.272308Z","iopub.status.idle":"2022-01-18T17:37:59.278372Z","shell.execute_reply.started":"2022-01-18T17:37:59.272266Z","shell.execute_reply":"2022-01-18T17:37:59.277619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"best parameters:\")\nprint(study.best_params)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:37:59.279771Z","iopub.execute_input":"2022-01-18T17:37:59.281346Z","iopub.status.idle":"2022-01-18T17:37:59.294884Z","shell.execute_reply.started":"2022-01-18T17:37:59.281301Z","shell.execute_reply":"2022-01-18T17:37:59.293883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### We now use the XGBRegressor with its best parameters to train on various yearly subsets of all data and to predict on the test set.","metadata":{}},{"cell_type":"code","source":"params = study.best_params\ntest_preds = np.zeros(len(processed_test))\nyears = train.year.unique()\n\n# train on train yearly subsets\nfor year in years:\n    train_set = list(train.row_id[train.year!=year])\n    val_set = list(train.row_id[train.year==year])\n\n    x = processed_train.iloc[train_set]\n    x_test = processed_train.iloc[val_set]\n    y = target[train_set]\n    y_test = target[val_set]\n    \n    model = XGBRegressor(random_state=0,\n                     tree_method=\"gpu_hist\",\n                     predictor=\"gpu_predictor\",\n                     n_estimators=10_000,\n                     **params,)\n\n    model.fit(x, y)\n    preds = model.predict(x_test)\n    smape = SMAPE(y_test, preds)\n    \n    print(f\"SMAPE year=={year} : {smape:0.5f}\")\n    \n    test_preds += model.predict(processed_test) / len(years)\n    \n# train on all the data\nmodel = XGBRegressor(random_state=0,\n                     tree_method=\"gpu_hist\",\n                     predictor=\"gpu_predictor\",\n                     n_estimators=10_000,\n                     **params,)\n\nmodel.fit(processed_train, target)\n\n# blending\ntest_preds = test_preds * 0.5 + model.predict(processed_test) * 0.5","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:37:59.297062Z","iopub.execute_input":"2022-01-18T17:37:59.297609Z","iopub.status.idle":"2022-01-18T17:43:52.645152Z","shell.execute_reply.started":"2022-01-18T17:37:59.297571Z","shell.execute_reply":"2022-01-18T17:43:52.644438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.read_csv(\"../input/tabular-playground-series-jan-2022/sample_submission.csv\")\n\n# rounding\npreds = selective_rounding(test_preds, lower=0.3, upper=0.7)\n\nsubmission.num_sold = preds\nsubmission.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:43:52.646296Z","iopub.execute_input":"2022-01-18T17:43:52.646551Z","iopub.status.idle":"2022-01-18T17:43:52.67869Z","shell.execute_reply.started":"2022-01-18T17:43:52.646518Z","shell.execute_reply":"2022-01-18T17:43:52.678071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-18T17:43:52.679685Z","iopub.execute_input":"2022-01-18T17:43:52.679947Z","iopub.status.idle":"2022-01-18T17:43:52.689682Z","shell.execute_reply.started":"2022-01-18T17:43:52.679894Z","shell.execute_reply":"2022-01-18T17:43:52.688786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### If you liked the notebook, consider to upvote, thank you and happy Kaggling!","metadata":{}}]}