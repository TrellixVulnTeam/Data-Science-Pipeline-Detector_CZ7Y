{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# IMPORTS","metadata":{}},{"cell_type":"code","source":"from sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import make_scorer\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn import preprocessing\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-01-22T22:13:43.514326Z","iopub.status.busy":"2022-01-22T22:13:43.513914Z","iopub.status.idle":"2022-01-22T22:13:44.704453Z","shell.execute_reply":"2022-01-22T22:13:44.703716Z","shell.execute_reply.started":"2022-01-22T22:13:43.514283Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CONSTANTS","metadata":{}},{"cell_type":"code","source":"INPUT_TRAIN = \"../input/tabular-playground-series-jan-2022/train.csv\"\nINPUT_TEST = \"../input/tabular-playground-series-jan-2022/test.csv\"\nSUBMISSION = \"../input/tabular-playground-series-jan-2022/sample_submission.csv\"","metadata":{"execution":{"iopub.execute_input":"2022-01-22T22:13:49.532057Z","iopub.status.busy":"2022-01-22T22:13:49.53178Z","iopub.status.idle":"2022-01-22T22:13:49.536042Z","shell.execute_reply":"2022-01-22T22:13:49.535255Z","shell.execute_reply.started":"2022-01-22T22:13:49.53203Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# FUNCTIONS","metadata":{}},{"cell_type":"code","source":"def check_and_plot_nan_percentage(df=None, x_offset=0, y_offset=0, print_values=True):\n    \"\"\"\n    Plots the percentage of missing values on each columns of an input dataframe\n\n            Parameters:\n                    df (DataFrame): A pandas Dataframe\n                    x_offset (float): x_offset on each bar value\n                    y_offset (float): y_offset on each bar value\n                    print_values (boolean): Set it to True to display percentage on bars\n\n    \"\"\"\n    if df is None:\n        print(\"Input dataframe is None : exit\")\n        return\n    else:\n        values = []\n        for c in df.columns:\n            values.append(100*df[c].isna().sum() / df.shape[0])\n        plt.figure(figsize=(18, 12))\n        plt.title(\"NaN percentage per column\",\n                  fontsize=16,\n                  fontweight='bold',\n                  pad=20\n                  )\n        plt.bar(range(0, len(df.columns)), values, edgecolor='black')\n        plt.xticks(range(0, len(df.columns)), df.columns, rotation=90)\n        xlocs, xlabs = plt.xticks()\n        if print_values:\n            for i, v in enumerate(values):\n                if v > 0:\n                    if i % 2 == 0:\n                        plt.text(xlocs[i] + x_offset, v +\n                                 y_offset, str(round(v, 1)))\n                    else:\n                        plt.text(xlocs[i] + x_offset, v +\n                                 y_offset, str(round(v, 1)))\n        plt.show()","metadata":{"execution":{"iopub.execute_input":"2022-01-22T22:13:52.325467Z","iopub.status.busy":"2022-01-22T22:13:52.325171Z","iopub.status.idle":"2022-01-22T22:13:52.336577Z","shell.execute_reply":"2022-01-22T22:13:52.335832Z","shell.execute_reply.started":"2022-01-22T22:13:52.325435Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. DATA LOADING","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv(INPUT_TRAIN)\ndf_test = pd.read_csv(INPUT_TEST)","metadata":{"execution":{"iopub.execute_input":"2022-01-22T22:13:54.814203Z","iopub.status.busy":"2022-01-22T22:13:54.813681Z","iopub.status.idle":"2022-01-22T22:13:54.887924Z","shell.execute_reply":"2022-01-22T22:13:54.887005Z","shell.execute_reply.started":"2022-01-22T22:13:54.814169Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. FIRST DATA LOOK AROUND","metadata":{}},{"cell_type":"markdown","source":"### 2.1 Check column names","metadata":{}},{"cell_type":"code","source":"df_train.info()","metadata":{"execution":{"iopub.execute_input":"2022-01-22T22:13:56.952507Z","iopub.status.busy":"2022-01-22T22:13:56.952197Z","iopub.status.idle":"2022-01-22T22:13:56.99175Z","shell.execute_reply":"2022-01-22T22:13:56.990918Z","shell.execute_reply.started":"2022-01-22T22:13:56.952472Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.info()","metadata":{"execution":{"iopub.execute_input":"2022-01-22T22:13:59.835022Z","iopub.status.busy":"2022-01-22T22:13:59.834726Z","iopub.status.idle":"2022-01-22T22:13:59.852251Z","shell.execute_reply":"2022-01-22T22:13:59.851122Z","shell.execute_reply.started":"2022-01-22T22:13:59.834989Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Are test columns in train columns ?\",\n      df_test.columns.isin(df_train.columns).all())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.2 Check missing values on each column","metadata":{}},{"cell_type":"code","source":"check_and_plot_nan_percentage(\n    df=df_train, x_offset=0, y_offset=0, print_values=True\n)","metadata":{"execution":{"iopub.execute_input":"2022-01-22T22:14:01.942682Z","iopub.status.busy":"2022-01-22T22:14:01.942314Z","iopub.status.idle":"2022-01-22T22:14:02.215003Z","shell.execute_reply":"2022-01-22T22:14:02.214148Z","shell.execute_reply.started":"2022-01-22T22:14:01.942641Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"check_and_plot_nan_percentage(\n    df=df_test, x_offset=0, y_offset=0, print_values=True\n)","metadata":{"execution":{"iopub.execute_input":"2022-01-22T22:14:05.114059Z","iopub.status.busy":"2022-01-22T22:14:05.113758Z","iopub.status.idle":"2022-01-22T22:14:05.330579Z","shell.execute_reply":"2022-01-22T22:14:05.329756Z","shell.execute_reply.started":"2022-01-22T22:14:05.114027Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head()","metadata":{"execution":{"iopub.execute_input":"2022-01-22T22:14:08.263713Z","iopub.status.busy":"2022-01-22T22:14:08.263388Z","iopub.status.idle":"2022-01-22T22:14:08.280718Z","shell.execute_reply":"2022-01-22T22:14:08.279945Z","shell.execute_reply.started":"2022-01-22T22:14:08.263667Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.head()","metadata":{"execution":{"iopub.execute_input":"2022-01-22T22:14:10.274635Z","iopub.status.busy":"2022-01-22T22:14:10.273856Z","iopub.status.idle":"2022-01-22T22:14:10.286097Z","shell.execute_reply":"2022-01-22T22:14:10.285169Z","shell.execute_reply.started":"2022-01-22T22:14:10.274601Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.3 Store categorcial columns and check unique values between train and test set","metadata":{}},{"cell_type":"code","source":"CATEGORICAL_COLUMNS = [\"country\", \"store\", \"product\"]","metadata":{"execution":{"iopub.execute_input":"2022-01-22T22:14:12.725478Z","iopub.status.busy":"2022-01-22T22:14:12.724618Z","iopub.status.idle":"2022-01-22T22:14:12.729477Z","shell.execute_reply":"2022-01-22T22:14:12.728759Z","shell.execute_reply.started":"2022-01-22T22:14:12.725436Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for c in CATEGORICAL_COLUMNS:\n    print(\"Train:\", df_train[c].unique())\n    print(\"Test :\", df_test[c].unique())\n    print(\"Are train and test values the same ?\",\n          (df_test[c].unique() == df_train[c].unique()).all())","metadata":{"execution":{"iopub.execute_input":"2022-01-22T22:14:15.794757Z","iopub.status.busy":"2022-01-22T22:14:15.793885Z","iopub.status.idle":"2022-01-22T22:14:15.810385Z","shell.execute_reply":"2022-01-22T22:14:15.809692Z","shell.execute_reply.started":"2022-01-22T22:14:15.794716Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.4 Convert date to datetime object with the right format","metadata":{}},{"cell_type":"code","source":"format = '%Y/%m/%d'\ndf_train['date'] = pd.to_datetime(df_train['date'], format=format)\ndf_test['date'] = pd.to_datetime(df_test['date'], format=format)","metadata":{"execution":{"iopub.execute_input":"2022-01-22T22:14:18.024816Z","iopub.status.busy":"2022-01-22T22:14:18.024116Z","iopub.status.idle":"2022-01-22T22:14:18.042326Z","shell.execute_reply":"2022-01-22T22:14:18.041639Z","shell.execute_reply.started":"2022-01-22T22:14:18.024775Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.5 Describe numerical columns","metadata":{}},{"cell_type":"code","source":"for c in df_train.columns:\n    if c not in CATEGORICAL_COLUMNS:\n        print(df_train[c].describe())\n        print((\"\\n\"))","metadata":{"execution":{"iopub.execute_input":"2022-01-22T22:14:20.655518Z","iopub.status.busy":"2022-01-22T22:14:20.654737Z","iopub.status.idle":"2022-01-22T22:14:20.674617Z","shell.execute_reply":"2022-01-22T22:14:20.674046Z","shell.execute_reply.started":"2022-01-22T22:14:20.655478Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['num_sold'].hist()\nplt.show()","metadata":{"execution":{"iopub.execute_input":"2022-01-22T22:14:23.899011Z","iopub.status.busy":"2022-01-22T22:14:23.898686Z","iopub.status.idle":"2022-01-22T22:14:24.152909Z","shell.execute_reply":"2022-01-22T22:14:24.15189Z","shell.execute_reply.started":"2022-01-22T22:14:23.89898Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. CLEANING","metadata":{}},{"cell_type":"code","source":"df_train = df_train.drop(columns=[\"row_id\"])","metadata":{"execution":{"iopub.execute_input":"2022-01-22T22:14:27.272177Z","iopub.status.busy":"2022-01-22T22:14:27.271885Z","iopub.status.idle":"2022-01-22T22:14:27.279753Z","shell.execute_reply":"2022-01-22T22:14:27.278845Z","shell.execute_reply.started":"2022-01-22T22:14:27.272142Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. FEATURE ENGINEERING","metadata":{}},{"cell_type":"code","source":"df_train[\"weekday\"] = df_train[\"date\"].dt.dayofweek\ndf_train[\"month\"] = df_train[\"date\"].dt.month\ndf_train[\"year\"] = df_train[\"date\"].dt.year\ndf_train['is_weekend'] = (df_train['date'].dt.weekday >= 5).astype(int)\n\ndf_test[\"weekday\"] = df_test[\"date\"].dt.dayofweek\ndf_test[\"month\"] = df_test[\"date\"].dt.month\ndf_test[\"year\"] = df_test[\"date\"].dt.year\ndf_test['is_weekend'] = (df_test['date'].dt.weekday >= 5).astype(int)","metadata":{"execution":{"iopub.execute_input":"2022-01-22T22:14:29.676166Z","iopub.status.busy":"2022-01-22T22:14:29.675764Z","iopub.status.idle":"2022-01-22T22:14:29.702953Z","shell.execute_reply":"2022-01-22T22:14:29.7021Z","shell.execute_reply.started":"2022-01-22T22:14:29.67613Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = df_train.drop(columns=[\"date\"])\ndf_test = df_test.drop(columns=[\"date\"])","metadata":{"execution":{"iopub.execute_input":"2022-01-22T22:14:31.965583Z","iopub.status.busy":"2022-01-22T22:14:31.965203Z","iopub.status.idle":"2022-01-22T22:14:31.974443Z","shell.execute_reply":"2022-01-22T22:14:31.973769Z","shell.execute_reply.started":"2022-01-22T22:14:31.965538Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I drop date column on each dataset after doing the feature engineering because the intertia will be coupled otherwise.","metadata":{}},{"cell_type":"markdown","source":"# 5. DATA EXPLORATION","metadata":{}},{"cell_type":"code","source":"years = list(df_train[\"year\"].unique())\nmonths = list(df_train[\"month\"].unique())\ndays = list(df_train[\"weekday\"].unique())\ncountries = list(df_train[\"country\"].unique())\nstores = list(df_train[\"store\"].unique())\nproducts = list(df_train[\"product\"].unique())","metadata":{"execution":{"iopub.execute_input":"2022-01-22T22:14:35.545455Z","iopub.status.busy":"2022-01-22T22:14:35.545188Z","iopub.status.idle":"2022-01-22T22:14:35.560256Z","shell.execute_reply":"2022-01-22T22:14:35.559581Z","shell.execute_reply.started":"2022-01-22T22:14:35.545426Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ax = df_train[\"is_weekend\"].value_counts(normalize=True).plot(kind=\"bar\",\n                                                              title=\"Sales distribution over weekends\",\n                                                              ylabel=\"Sales percentage\"\n                                                              )\nax.set_xticklabels([\"During week\", \"During weekend\"])\nplt.show()","metadata":{"execution":{"iopub.execute_input":"2022-01-22T22:14:37.683239Z","iopub.status.busy":"2022-01-22T22:14:37.682967Z","iopub.status.idle":"2022-01-22T22:14:37.870445Z","shell.execute_reply":"2022-01-22T22:14:37.86954Z","shell.execute_reply.started":"2022-01-22T22:14:37.68321Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_sales_per_year = []\ntotal_sales_per_month_per_year = []\ntotal_sales_per_day_over_year = []\n\nfor i in range(0, len(years)):\n    total_sales_per_month_per_year.append([])\n    total_sales_per_day_over_year.append([])\n    total_sales_per_year.append(\n        df_train[df_train[\"year\"] == years[i]][\"num_sold\"].sum())\n    sub_df = df_train[df_train[\"year\"] == years[i]].copy(deep=True)\n    for month in months:\n        total_sales_per_month_per_year[i].append(\n            sub_df[sub_df[\"month\"] == month][\"num_sold\"].sum())\n    for day in days:\n        total_sales_per_day_over_year[i].append(\n            sub_df[sub_df[\"weekday\"] == day][\"num_sold\"].sum())\n\ntotal_sales_per_year /= sum(total_sales_per_year)\n\nplt.bar(years, total_sales_per_year)\nplt.xticks(years)\nplt.title(\"Total sales over years\")\nplt.ylabel(\"Percentage\")\nplt.show()\n\nfig, axs = plt.subplots(1, len(years), figsize=[16, 6], sharey=True)\nfor i in range(0, len(years)):\n    axs[i].bar(months, total_sales_per_month_per_year[i])\n    axs[i].set_title(str(\"Year: \" + str(years[i])))\n    axs[i].set_xticks(months)\n    axs[i].set_xlabel(\"Month\")\nplt.show()\n\nfig, axs = plt.subplots(1, len(years), figsize=[16, 6], sharey=True)\nfor i in range(0, len(years)):\n    axs[i].bar(days, total_sales_per_day_over_year[i])\n    axs[i].set_title(str(\"Year: \" + str(years[i])))\n    axs[i].set_xticks(days)\n    axs[i].set_xlabel(\"Weekday\")\nplt.show()","metadata":{"execution":{"iopub.execute_input":"2022-01-22T22:14:40.365091Z","iopub.status.busy":"2022-01-22T22:14:40.364792Z","iopub.status.idle":"2022-01-22T22:14:41.762506Z","shell.execute_reply":"2022-01-22T22:14:41.761865Z","shell.execute_reply.started":"2022-01-22T22:14:40.365057Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Sales growth from year to year\n* A seasonality is observable over the years: Sales increase in December and January, decrease in February, increase again in March, April and May, then decrease until August and increase again from September\n* Weekend concentrate sales","metadata":{}},{"cell_type":"code","source":"total_sales_per_country = []\ntotal_sales_per_country_over_years = []\nfor i in range(0, len(countries)):\n    total_sales_per_country_over_years.append([])\n    total_sales_per_country.append(\n        df_train[df_train[\"country\"] == countries[i]][\"num_sold\"].sum())\n    sub_df = df_train[df_train[\"country\"] == countries[i]].copy(deep=True)\n    for year in years:\n        total_sales_per_country_over_years[i].append(\n            sub_df[sub_df[\"year\"] == year][\"num_sold\"].sum())\n\ntotal_sales_per_country /= sum(total_sales_per_country)\nplt.bar(countries, total_sales_per_country)\nplt.xticks(countries)\nplt.title(\"Total sales over countries\")\nplt.ylabel(\"Sales percentage\")\nplt.show()\n\nfig, axs = plt.subplots(1, len(countries), figsize=[16, 6], sharey=True)\nfor i in range(0, len(countries)):\n    axs[i].bar(years, total_sales_per_country_over_years[i])\n    axs[i].set_xticks(years)\n    axs[i].set_title(str(\"Country: \" + str(countries[i])))\n    axs[i].set_xlabel(\"Year\")\nplt.show()","metadata":{"execution":{"iopub.execute_input":"2022-01-22T22:14:45.515674Z","iopub.status.busy":"2022-01-22T22:14:45.51526Z","iopub.status.idle":"2022-01-22T22:14:46.012782Z","shell.execute_reply":"2022-01-22T22:14:46.011887Z","shell.execute_reply.started":"2022-01-22T22:14:45.515643Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_sales_per_store = []\ntotal_sales_per_store_over_years = []\ntotal_sales_per_store_per_country = []\ntotal_sales_per_store_per_product = []\n\nfor i in range(0, len(stores)):\n    total_sales_per_store_over_years.append([])\n    total_sales_per_store_per_country.append([])\n    total_sales_per_store_per_product.append([])\n    total_sales_per_store.append(\n        df_train[df_train[\"store\"] == stores[i]][\"num_sold\"].sum())\n    sub_df = df_train[df_train[\"store\"] == stores[i]].copy(deep=True)\n    for year in years:\n        total_sales_per_store_over_years[i].append(\n            sub_df[sub_df[\"year\"] == year][\"num_sold\"].sum())\n    for country in countries:\n        total_sales_per_store_per_country[i].append(\n            sub_df[sub_df[\"country\"] == country][\"num_sold\"].sum())\n    for product in products:\n        total_sales_per_store_per_product[i].append(\n            sub_df[sub_df[\"product\"] == product][\"num_sold\"].sum())\n\ntotal_sales_per_store /= sum(total_sales_per_store)\nplt.bar(stores, total_sales_per_store)\nplt.xticks(stores)\nplt.title(\"Total sales per store over the years\")\nplt.ylabel(\"Sales percentage\")\nplt.show()\n\nfig, axs = plt.subplots(1, len(stores), figsize=[16, 6], sharey=True)\nfor i in range(0, len(stores)):\n    axs[i].bar(years, total_sales_per_store_over_years[i])\n    axs[i].set_xticks(years)\n    axs[i].set_title(str(\"Store: \" + str(stores[i])))\n    axs[i].set_xlabel(\"Year\")\nplt.show()\n\nfig, axs = plt.subplots(1, len(stores), figsize=[16, 6], sharey=True)\nfor i in range(0, len(stores)):\n    axs[i].bar(countries, total_sales_per_store_per_country[i])\n    axs[i].set_xticks(countries)\n    axs[i].set_title(str(\"Store: \" + str(stores[i])))\n    axs[i].set_xlabel(\"Country\")\nplt.show()\n\nfig, axs = plt.subplots(1, len(stores), figsize=[16, 6], sharey=True)\nfor i in range(0, len(stores)):\n    axs[i].bar(products, total_sales_per_store_per_product[i])\n    axs[i].set_xticks(products)\n    axs[i].set_title(str(\"Store: \" + str(stores[i])))\n    axs[i].set_xlabel(\"Product\")\nplt.show()","metadata":{"execution":{"iopub.execute_input":"2022-01-22T22:14:49.279568Z","iopub.status.busy":"2022-01-22T22:14:49.279267Z","iopub.status.idle":"2022-01-22T22:14:50.090227Z","shell.execute_reply":"2022-01-22T22:14:50.08928Z","shell.execute_reply.started":"2022-01-22T22:14:49.279534Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_sales_per_product = []\ntotal_sales_per_product_over_years = []\ntotal_sales_per_product_per_country = []\n\nfor i in range(0, len(products)):\n    total_sales_per_product_over_years.append([])\n    total_sales_per_product_per_country.append([])\n    total_sales_per_product.append(\n        df_train[df_train[\"product\"] == products[i]][\"num_sold\"].sum())\n    sub_df = df_train[df_train[\"product\"] == products[i]].copy(deep=True)\n    for year in years:\n        total_sales_per_product_over_years[i].append(\n            sub_df[sub_df[\"year\"] == year][\"num_sold\"].sum())\n    for country in countries:\n        total_sales_per_product_per_country[i].append(\n            sub_df[sub_df[\"country\"] == country][\"num_sold\"].sum())\n\ntotal_sales_per_product /= sum(total_sales_per_product)\nplt.bar(products, total_sales_per_product)\nplt.xticks(products)\nplt.title(\"Total sales per product over the years\")\nplt.ylabel(\"Sales percentage\")\nplt.show()\n\nfig, axs = plt.subplots(1, len(products), figsize=[16, 6], sharey=True)\nfor i in range(0, len(products)):\n    axs[i].bar(years, total_sales_per_product_over_years[i])\n    axs[i].set_xticks(years)\n    axs[i].set_title(str(\"Products: \" + str(products[i])))\n    axs[i].set_xlabel(\"Year\")\nplt.show()\n\nfig, axs = plt.subplots(1, len(products), figsize=[16, 6], sharey=True)\nfor i in range(0, len(products)):\n    axs[i].bar(countries, total_sales_per_product_per_country[i])\n    axs[i].set_xticks(countries)\n    axs[i].set_title(str(\"Product: \" + str(products[i])))\n    axs[i].set_xlabel(\"Country\")\nplt.show()","metadata":{"execution":{"iopub.execute_input":"2022-01-22T22:14:53.955987Z","iopub.status.busy":"2022-01-22T22:14:53.955445Z","iopub.status.idle":"2022-01-22T22:14:54.715207Z","shell.execute_reply":"2022-01-22T22:14:54.714264Z","shell.execute_reply.started":"2022-01-22T22:14:53.955954Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6.PREPROCESSING","metadata":{}},{"cell_type":"markdown","source":"### 6.1 Encode categorical columns","metadata":{}},{"cell_type":"code","source":"le = preprocessing.LabelEncoder()\n\nfor c in CATEGORICAL_COLUMNS:\n    df_train[c] = le.fit_transform(df_train[c])\n    df_test[c] = le.transform(df_test[c])","metadata":{"execution":{"iopub.execute_input":"2022-01-22T22:15:01.104225Z","iopub.status.busy":"2022-01-22T22:15:01.103929Z","iopub.status.idle":"2022-01-22T22:15:01.146418Z","shell.execute_reply":"2022-01-22T22:15:01.145782Z","shell.execute_reply.started":"2022-01-22T22:15:01.104192Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 6.2 Construct X and y arrays","metadata":{}},{"cell_type":"code","source":"y = np.array(df_train[\"num_sold\"])\ny = y.reshape(-1, 1)","metadata":{"execution":{"iopub.execute_input":"2022-01-22T22:15:03.524841Z","iopub.status.busy":"2022-01-22T22:15:03.52409Z","iopub.status.idle":"2022-01-22T22:15:03.529662Z","shell.execute_reply":"2022-01-22T22:15:03.528755Z","shell.execute_reply.started":"2022-01-22T22:15:03.524801Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = np.array(df_train.drop(columns=\"num_sold\"))\nX_to_pred = np.array(df_test.drop(columns=[\"row_id\"]))","metadata":{"execution":{"iopub.execute_input":"2022-01-22T22:15:05.834653Z","iopub.status.busy":"2022-01-22T22:15:05.834093Z","iopub.status.idle":"2022-01-22T22:15:05.846238Z","shell.execute_reply":"2022-01-22T22:15:05.845545Z","shell.execute_reply.started":"2022-01-22T22:15:05.834615Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 6.3 Split train dataframe into train and test set","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.execute_input":"2022-01-22T22:15:10.584933Z","iopub.status.busy":"2022-01-22T22:15:10.584622Z","iopub.status.idle":"2022-01-22T22:15:10.593393Z","shell.execute_reply":"2022-01-22T22:15:10.592209Z","shell.execute_reply.started":"2022-01-22T22:15:10.5849Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 6.4 Create the appropriate score","metadata":{}},{"cell_type":"code","source":"def smape(y_true, y_pred):\n    return (100/y_true.shape[0]) * np.sum(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred)))\n\n\n# SMAPE must be lowered to increase performances\nsmape_score = make_scorer(score_func=smape, greater_is_better=False)","metadata":{"execution":{"iopub.execute_input":"2022-01-22T22:15:13.135886Z","iopub.status.busy":"2022-01-22T22:15:13.135393Z","iopub.status.idle":"2022-01-22T22:15:13.141903Z","shell.execute_reply":"2022-01-22T22:15:13.140901Z","shell.execute_reply.started":"2022-01-22T22:15:13.135855Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 7. Training","metadata":{}},{"cell_type":"markdown","source":"For training, i will train Random forest regressor + Gradient boosting model on the train set. The training will be performed on a grid to optimize hyperparameters and validate them with a cross validation on 5-folds. \n\nSo, GridSearchCV will be used.","metadata":{}},{"cell_type":"markdown","source":"### 7.1 Random forest regressor","metadata":{}},{"cell_type":"code","source":"rfr = RandomForestRegressor()\nparameters = {'n_estimators': (100, 200, 300, 500, 1000, 1500, 2000),\n              'max_depth': [None, 3, 5],\n              'bootstrap': [True, False]\n              }\nres = GridSearchCV(estimator=rfr, param_grid=parameters,\n                   scoring=smape_score, cv=5).fit(X_train, y_train.ravel())\n\nprint(\"Best params found through the gridsearch: \", res.best_params_)\n\nopti_rfr = res.best_estimator_","metadata":{"execution":{"iopub.execute_input":"2022-01-22T22:15:18.041064Z","iopub.status.busy":"2022-01-22T22:15:18.040048Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = np.round(opti_rfr.predict(X_test)).reshape(-1, 1).astype(int)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b> Evaluate the performances with the SMAPE metric between predictions and true values.","metadata":{}},{"cell_type":"code","source":"smp = smape(y_true=y_test, y_pred=y_pred)\nprint(\"SMAPE on test set =\", smp)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b> Evaluate the impact of introducing a biais on the predictions","metadata":{}},{"cell_type":"code","source":"biais = [i for i in range(-30, 30)]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"smps = []\nfor i in biais:\n    smp = smape(y_true=y_test, y_pred=y_pred+i)\n    smps.append(smp)\n\nprint(\"Best SMAPE on test =\", min(smps),\n      \" biais =\", biais[smps.index(min(smps))])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(biais, smps)\nplt.title(\"Impact of biais on predictions\")\nplt.ylabel(\"SMAPE Score\")\nplt.xlabel(\"Biais value\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 7.2 GradientBoosting","metadata":{}},{"cell_type":"code","source":"gboost = GradientBoostingRegressor(random_state=0)\nparameters = {'n_estimators': (50, 75, 100, 150, 200, 250, 300, 500),\n              'learning_rate': (0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.08, 0.1, 0.15, 0.2),\n              'max_depth': (3, 5, 8), \"max_features\": [\"auto\", \"log2\"]\n              }\nres = GridSearchCV(estimator=gboost, param_grid=parameters,\n                   scoring=smape_score, cv=5).fit(X_train, y_train.ravel())\n\nprint(\"Best params found through the gridsearch= \", res.best_params_)\n\nopti_gboost = res.best_estimator_","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = np.round(opti_gboost.predict(X_test)).reshape(-1, 1).astype(int)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b> Evaluate the performances with the SMAPE metric between predictions and true values.","metadata":{}},{"cell_type":"code","source":"smp = smape(y_true=y_test, y_pred=y_pred)\nprint(\"SMAPE on test set =\", smp)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b> Evaluate the impact of introducing a biais on the predictions","metadata":{}},{"cell_type":"code","source":"smps = []\nfor i in biais:\n    smp = smape(y_true=y_test, y_pred=y_pred+i)\n    smps.append(smp)\n\nprint(\"Best SMAPE on test =\", min(smps),\n      \" biais =\", biais[smps.index(min(smps))])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(biais, smps)\nplt.title(\"Impact of biais on predictions\")\nplt.ylabel(\"SMAPE Score\")\nplt.xlabel(\"Biais value\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 8. Impact of replacing outliers by mean","metadata":{}},{"cell_type":"code","source":"y = np.array(df_train[\"num_sold\"])\nlimit = round(np.percentile(y, 96), 2)\noutliers = y > limit\ny[outliers] = np.mean(y)\ny = y.reshape(-1, 1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gboost = GradientBoostingRegressor(random_state=0)\nparameters = {'n_estimators': (50, 75, 100, 150, 200, 250, 300, 500),\n              'learning_rate': (0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.08, 0.1, 0.15, 0.2),\n              'max_depth': (3, 5, 8), \"max_features\": [\"auto\", \"log2\"]\n              }\nres = GridSearchCV(estimator=gboost, param_grid=parameters,\n                   scoring=smape_score, cv=5).fit(X_train, y_train.ravel())\n\nprint(\"Best params found through the gridsearch= \", res.best_params_)\n\nopti_gboost2 = res.best_estimator_","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = np.round(opti_gboost2.predict(X_test)).reshape(-1, 1).astype(int)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b> Evaluate the performances with the SMAPE metric between predictions and true values.","metadata":{}},{"cell_type":"code","source":"smp = smape(y_true=y_test, y_pred=y_pred)\nprint(\"SMAPE on test set =\", smp)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b> Evaluate the impact of introducing a biais on the predictions","metadata":{}},{"cell_type":"code","source":"smps = []\nfor i in biais:\n    smp = smape(y_true=y_test, y_pred=y_pred+i)\n    smps.append(smp)\n\nprint(\"Best SMAPE on test =\", min(smps),\n      \" biais =\", biais[smps.index(min(smps))])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(biais, smps)\nplt.title(\"Impact of biais on predictions\")\nplt.ylabel(\"SMAPE Score\")\nplt.xlabel(\"Biais value\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 9. Impact of replacing outliers by limit","metadata":{}},{"cell_type":"code","source":"y = np.array(df_train[\"num_sold\"])\nlimit = round(np.percentile(y, 96), 2)\noutliers = y > limit\ny[outliers] = limit\ny = y.reshape(-1, 1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gboost = GradientBoostingRegressor(random_state=0)\nparameters = {'n_estimators': (50, 75, 100, 150, 200, 250, 300, 500),\n              'learning_rate': (0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.08, 0.1, 0.15, 0.2),\n              'max_depth': (3, 5, 8), \"max_features\": [\"auto\", \"log2\"]\n              }\nres = GridSearchCV(estimator=gboost, param_grid=parameters,\n                   scoring=smape_score, cv=5).fit(X_train, y_train.ravel())\n\nprint(\"Best params found through the gridsearch= \", res.best_params_)\n\nopti_gboost3 = res.best_estimator_","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = np.round(opti_gboost3.predict(X_test)).reshape(-1, 1).astype(int)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b> Evaluate the performances with the SMAPE metric between predictions and true values.","metadata":{}},{"cell_type":"code","source":"smp = smape(y_true=y_test, y_pred=y_pred)\nprint(\"SMAPE on test set =\", smp)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b> Evaluate the impact of introducing a biais on the predictions","metadata":{}},{"cell_type":"code","source":"smps = []\nfor i in biais:\n    smp = smape(y_true=y_test, y_pred=y_pred+i)\n    smps.append(smp)\n\nprint(\"Best SMAPE on test =\", min(smps),\n      \" biais =\", biais[smps.index(min(smps))])\n\nplt.plot(biais, smps)\nplt.title(\"Impact of biais on predictions\")\nplt.ylabel(\"SMAPE Score\")\nplt.xlabel(\"Biais value\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 10. Forecast the number of sales for the coming years (Test dataframe)","metadata":{}},{"cell_type":"code","source":"best_biais = -1","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_sold = np.round(opti_gboost.predict(X_to_pred)).reshape(-1, 1).astype(int) + best_biais","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(SUBMISSION)\ndf[\"num_sold\"] = num_sold\ndf.to_csv(\"submission.csv\", index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note : using a y_train filtered or clamped decrease performance on the test dataframe (8.91 with replacement by limit vs. 8.71 without any processing)","metadata":{}},{"cell_type":"markdown","source":"# REFERENCES","metadata":{}},{"cell_type":"markdown","source":"* Tensor girl: https://www.kaggle.com/usharengaraju/tensorflow-tf-data-keraspreprocessinglayers-w-b for the cheat code to compute the feature is_weekend\n\n* SMAPE formula : https://en.wikipedia.org/wiki/Symmetric_mean_absolute_percentage_error","metadata":{}}]}