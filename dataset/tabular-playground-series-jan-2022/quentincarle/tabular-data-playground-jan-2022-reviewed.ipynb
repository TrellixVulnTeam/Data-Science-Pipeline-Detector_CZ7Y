{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import make_scorer\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-29T19:58:12.690991Z","iopub.execute_input":"2022-01-29T19:58:12.691276Z","iopub.status.idle":"2022-01-29T19:58:12.697023Z","shell.execute_reply.started":"2022-01-29T19:58:12.691245Z","shell.execute_reply":"2022-01-29T19:58:12.696302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CONSTANTS","metadata":{}},{"cell_type":"code","source":"INPUT_TRAIN = \"../input/tabular-playground-series-jan-2022/train.csv\"\nINPUT_TEST = \"../input/tabular-playground-series-jan-2022/test.csv\"\nSUBMISSION = \"../input/tabular-playground-series-jan-2022/sample_submission.csv\"","metadata":{"execution":{"iopub.status.busy":"2022-01-29T19:58:12.698398Z","iopub.execute_input":"2022-01-29T19:58:12.699035Z","iopub.status.idle":"2022-01-29T19:58:12.709179Z","shell.execute_reply.started":"2022-01-29T19:58:12.699001Z","shell.execute_reply":"2022-01-29T19:58:12.708456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# FUNCTIONS","metadata":{}},{"cell_type":"code","source":"def check_and_plot_nan_percentage(df=None, x_offset=0, y_offset=0, print_values=True):\n    \"\"\"\n    Plots the percentage of missing values on each columns of an input dataframe\n\n            Parameters:\n                    df (DataFrame): A pandas Dataframe\n                    x_offset (float): x_offset on each bar value\n                    y_offset (float): y_offset on each bar value\n                    print_values (boolean): Set it to True to display percentage on bars\n\n    \"\"\"\n    if df is None:\n        print(\"Input dataframe is None : exit\")\n        return\n    else:\n        values = []\n        for c in df.columns:\n            values.append(100*df[c].isna().sum() / df.shape[0])\n        plt.figure(figsize=(9, 6))\n        plt.title(\"NaN percentage per column\",\n                  fontsize=16,\n                  fontweight='bold',\n                  pad=20\n                  )\n        plt.bar(range(0, len(df.columns)), values, edgecolor='black')\n        plt.xticks(range(0, len(df.columns)), df.columns, rotation=90)\n        xlocs, xlabs = plt.xticks()\n        if print_values:\n            for i, v in enumerate(values):\n                if v > 0:\n                    if i % 2 == 0:\n                        plt.text(xlocs[i] + x_offset, v +\n                                 y_offset, str(round(v, 1)))\n                    else:\n                        plt.text(xlocs[i] + x_offset, v +\n                                 y_offset, str(round(v, 1)))\n        plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-29T19:58:12.710629Z","iopub.execute_input":"2022-01-29T19:58:12.710967Z","iopub.status.idle":"2022-01-29T19:58:12.723061Z","shell.execute_reply.started":"2022-01-29T19:58:12.71094Z","shell.execute_reply":"2022-01-29T19:58:12.722249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate_smape_model(model=None, X_test=None, y_test=None, biais_values=None):\n    \"\"\"\n    Makes predictions on X_test then, evaluate the metric SMAPE between predictions and y_test\n    and then evaluate SMAPE on predictions + biais values.\n\n            Parameters:\n                    model : A sklearn or tensorflow model\n                    X_test (array): x_offset on each bar value\n                    y_test (array): y_offset on each bar value\n                    biais_values (List): List of biais to add on predictions to evaluate SMAPE\n\n    \"\"\"\n    if model is None:\n        print(\"Input model is None\")\n        return\n    elif X_test is None:\n        print(\"X_test is None\")\n        return\n    elif y_test is None:\n        print(\"y_test is None\")\n        return\n    else:\n        y_pred = np.round(model.predict(X_test)).reshape(-1, 1).astype(int)\n        smp = smape(y_true=y_test, y_pred=y_pred)\n        print(\"SMAPE on test set =\", smp)\n        print(\"#\" * 10)\n\n        if biais_values is not None and len(biais_values) > 0:\n            smps = []\n            for i in biais_values:\n                smp = smape(y_true=y_test, y_pred=y_pred+i)\n                smps.append(smp)\n\n            print(\"Best SMAPE on test =\", min(smps),\n                \" biais =\", biais[smps.index(min(smps))])\n            print(\"#\" * 10)\n\n            plt.plot(biais_values, smps)\n            plt.title(\"Impact of biais on predictions\")\n            plt.ylabel(\"SMAPE Score\")\n            plt.xlabel(\"Biais value\")\n            plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-29T19:58:12.724195Z","iopub.execute_input":"2022-01-29T19:58:12.724452Z","iopub.status.idle":"2022-01-29T19:58:12.739068Z","shell.execute_reply.started":"2022-01-29T19:58:12.724423Z","shell.execute_reply":"2022-01-29T19:58:12.738439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_train_test_set():\n    \"\"\"\n    Checks if train and test set exist and returns them into a list.\n\n            Return:\n                    X_train (Array): Set use for training in fit method\n                    X_test (Array): Set use to predict\n                    y_test (Array): Set use to compare predictions and true values\n                    y_train (Array): Set use for training in fit method\n\n    \"\"\"\n    if not os.path.exists(\"data/x_train.npy\"):\n        print(\"X_train has not been created or has been deleted : cannot continue\")\n        return [None, None, None, None]\n    elif not os.path.exists(\"data/x_test.npy\"):\n        print(\"X_test has not been created or has been deleted : cannot continue\")\n        return [None, None, None, None]\n    elif not os.path.exists(\"data/y_train.npy\"):\n        print(\"y_train has not been created or has been deleted : cannot continue\")\n        return [None, None, None, None]\n    elif not os.path.exists(\"data/y_test.npy\"):\n        print(\"y_test has not been created or has been deleted : cannot continue\")\n        return [None, None, None, None]\n    else:\n        X_train = np.load(\"data/x_train.npy\")\n        X_test = np.load(\"data/x_test.npy\")\n        y_train = np.load(\"data/y_train.npy\")\n        y_test = np.load(\"data/y_test.npy\")\n        return [X_train, X_test, y_train, y_test]","metadata":{"execution":{"iopub.status.busy":"2022-01-29T19:58:12.74064Z","iopub.execute_input":"2022-01-29T19:58:12.74098Z","iopub.status.idle":"2022-01-29T19:58:12.757751Z","shell.execute_reply.started":"2022-01-29T19:58:12.740947Z","shell.execute_reply":"2022-01-29T19:58:12.756975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. DATA LOADING","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv(INPUT_TRAIN)\ndf_test = pd.read_csv(INPUT_TEST)","metadata":{"execution":{"iopub.status.busy":"2022-01-29T19:58:12.759353Z","iopub.execute_input":"2022-01-29T19:58:12.75981Z","iopub.status.idle":"2022-01-29T19:58:12.816397Z","shell.execute_reply.started":"2022-01-29T19:58:12.759694Z","shell.execute_reply":"2022-01-29T19:58:12.815736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. FIRST DATA LOOK AROUND","metadata":{}},{"cell_type":"code","source":"df_train.info()","metadata":{"execution":{"iopub.status.busy":"2022-01-29T19:58:12.818136Z","iopub.execute_input":"2022-01-29T19:58:12.81885Z","iopub.status.idle":"2022-01-29T19:58:12.834217Z","shell.execute_reply.started":"2022-01-29T19:58:12.818814Z","shell.execute_reply":"2022-01-29T19:58:12.833639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.info()","metadata":{"execution":{"iopub.status.busy":"2022-01-29T19:58:12.942058Z","iopub.execute_input":"2022-01-29T19:58:12.942993Z","iopub.status.idle":"2022-01-29T19:58:12.956378Z","shell.execute_reply.started":"2022-01-29T19:58:12.942945Z","shell.execute_reply":"2022-01-29T19:58:12.955541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Are test columns in train columns ?\",\n      df_test.columns.isin(df_train.columns).all())","metadata":{"execution":{"iopub.status.busy":"2022-01-29T19:58:12.958139Z","iopub.execute_input":"2022-01-29T19:58:12.958952Z","iopub.status.idle":"2022-01-29T19:58:12.964249Z","shell.execute_reply.started":"2022-01-29T19:58:12.958911Z","shell.execute_reply":"2022-01-29T19:58:12.963432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.2 Check missing values on each column","metadata":{}},{"cell_type":"code","source":"check_and_plot_nan_percentage(\n    df=df_train, x_offset=0, y_offset=0, print_values=True\n)","metadata":{"execution":{"iopub.status.busy":"2022-01-29T19:58:12.965602Z","iopub.execute_input":"2022-01-29T19:58:12.966301Z","iopub.status.idle":"2022-01-29T19:58:13.152056Z","shell.execute_reply.started":"2022-01-29T19:58:12.966255Z","shell.execute_reply":"2022-01-29T19:58:13.151107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"check_and_plot_nan_percentage(\n    df=df_test, x_offset=0, y_offset=0, print_values=True\n)","metadata":{"execution":{"iopub.status.busy":"2022-01-29T19:58:13.153272Z","iopub.execute_input":"2022-01-29T19:58:13.153536Z","iopub.status.idle":"2022-01-29T19:58:13.322709Z","shell.execute_reply.started":"2022-01-29T19:58:13.153505Z","shell.execute_reply":"2022-01-29T19:58:13.321865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-29T19:58:13.324894Z","iopub.execute_input":"2022-01-29T19:58:13.325136Z","iopub.status.idle":"2022-01-29T19:58:13.337401Z","shell.execute_reply.started":"2022-01-29T19:58:13.325107Z","shell.execute_reply":"2022-01-29T19:58:13.336574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-29T19:58:13.33917Z","iopub.execute_input":"2022-01-29T19:58:13.339737Z","iopub.status.idle":"2022-01-29T19:58:13.354698Z","shell.execute_reply.started":"2022-01-29T19:58:13.339689Z","shell.execute_reply":"2022-01-29T19:58:13.353835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.3 Store categorcial columns and check unique values between train and test set","metadata":{}},{"cell_type":"code","source":"CATEGORICAL_COLUMNS = [\"country\", \"store\", \"product\"]","metadata":{"execution":{"iopub.status.busy":"2022-01-29T19:58:13.355819Z","iopub.execute_input":"2022-01-29T19:58:13.356419Z","iopub.status.idle":"2022-01-29T19:58:13.360705Z","shell.execute_reply.started":"2022-01-29T19:58:13.356364Z","shell.execute_reply":"2022-01-29T19:58:13.359902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for c in CATEGORICAL_COLUMNS:\n    print(\"Train:\", df_train[c].unique())\n    print(\"Test :\", df_test[c].unique())\n    print(\"Are train and test values the same ?\",\n          (df_test[c].unique() == df_train[c].unique()).all())","metadata":{"execution":{"iopub.status.busy":"2022-01-29T19:58:13.362016Z","iopub.execute_input":"2022-01-29T19:58:13.362214Z","iopub.status.idle":"2022-01-29T19:58:13.387997Z","shell.execute_reply.started":"2022-01-29T19:58:13.362191Z","shell.execute_reply":"2022-01-29T19:58:13.387434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.4 Convert date to datetime object with the right format","metadata":{}},{"cell_type":"code","source":"format = '%Y/%m/%d'\ndf_train['date'] = pd.to_datetime(df_train['date'], format=format)\ndf_test['date'] = pd.to_datetime(df_test['date'], format=format)","metadata":{"execution":{"iopub.status.busy":"2022-01-29T19:58:13.388907Z","iopub.execute_input":"2022-01-29T19:58:13.389465Z","iopub.status.idle":"2022-01-29T19:58:13.402951Z","shell.execute_reply.started":"2022-01-29T19:58:13.389433Z","shell.execute_reply":"2022-01-29T19:58:13.402094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.5 Describe numerical columns","metadata":{}},{"cell_type":"code","source":"for c in df_train.columns:\n    if c not in CATEGORICAL_COLUMNS:\n        print(df_train[c].describe())\n        print((\"\\n\"))","metadata":{"execution":{"iopub.status.busy":"2022-01-29T19:58:13.404311Z","iopub.execute_input":"2022-01-29T19:58:13.404647Z","iopub.status.idle":"2022-01-29T19:58:13.421598Z","shell.execute_reply.started":"2022-01-29T19:58:13.40462Z","shell.execute_reply":"2022-01-29T19:58:13.420723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['num_sold'].hist()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-29T19:58:13.422776Z","iopub.execute_input":"2022-01-29T19:58:13.423085Z","iopub.status.idle":"2022-01-29T19:58:13.619748Z","shell.execute_reply.started":"2022-01-29T19:58:13.423052Z","shell.execute_reply":"2022-01-29T19:58:13.618777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. CLEANING","metadata":{}},{"cell_type":"code","source":"df_train = df_train.drop(columns=[\"row_id\"])","metadata":{"execution":{"iopub.status.busy":"2022-01-29T19:58:13.621144Z","iopub.execute_input":"2022-01-29T19:58:13.621486Z","iopub.status.idle":"2022-01-29T19:58:13.628538Z","shell.execute_reply.started":"2022-01-29T19:58:13.621441Z","shell.execute_reply":"2022-01-29T19:58:13.627723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. FEATURE ENGINEERING","metadata":{}},{"cell_type":"code","source":"df_train[\"weekday\"] = df_train[\"date\"].dt.dayofweek\ndf_train[\"month\"] = df_train[\"date\"].dt.month\ndf_train[\"year\"] = df_train[\"date\"].dt.year\ndf_train['is_weekend'] = (df_train['date'].dt.weekday >= 5).astype(int)\n\ndf_test[\"weekday\"] = df_test[\"date\"].dt.dayofweek\ndf_test[\"month\"] = df_test[\"date\"].dt.month\ndf_test[\"year\"] = df_test[\"date\"].dt.year\ndf_test['is_weekend'] = (df_test['date'].dt.weekday >= 5).astype(int)","metadata":{"execution":{"iopub.status.busy":"2022-01-29T19:58:13.629861Z","iopub.execute_input":"2022-01-29T19:58:13.630158Z","iopub.status.idle":"2022-01-29T19:58:13.660301Z","shell.execute_reply.started":"2022-01-29T19:58:13.630129Z","shell.execute_reply":"2022-01-29T19:58:13.659445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = df_train.drop(columns=[\"date\"])\ndf_test = df_test.drop(columns=[\"date\"])","metadata":{"execution":{"iopub.status.busy":"2022-01-29T19:58:13.663078Z","iopub.execute_input":"2022-01-29T19:58:13.663308Z","iopub.status.idle":"2022-01-29T19:58:13.673502Z","shell.execute_reply.started":"2022-01-29T19:58:13.663281Z","shell.execute_reply":"2022-01-29T19:58:13.672657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I drop date column on each dataset after doing the feature engineering because the intertia will be coupled otherwise.","metadata":{}},{"cell_type":"markdown","source":"# 5. DATA EXPLORATION","metadata":{}},{"cell_type":"code","source":"years = list(df_train[\"year\"].unique())\nmonths = list(df_train[\"month\"].unique())\ndays = list(df_train[\"weekday\"].unique())\ncountries = list(df_train[\"country\"].unique())\nstores = list(df_train[\"store\"].unique())\nproducts = list(df_train[\"product\"].unique())","metadata":{"execution":{"iopub.status.busy":"2022-01-29T19:58:13.674692Z","iopub.execute_input":"2022-01-29T19:58:13.675255Z","iopub.status.idle":"2022-01-29T19:58:13.687821Z","shell.execute_reply.started":"2022-01-29T19:58:13.675214Z","shell.execute_reply":"2022-01-29T19:58:13.686994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ax = df_train[\"is_weekend\"].value_counts(normalize=True).plot(kind=\"bar\",\n                                                              title=\"Sales distribution over weekends\",\n                                                              ylabel=\"Sales percentage\"\n                                                              )\nax.set_xticklabels([\"During week\", \"During weekend\"])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-29T19:58:13.689153Z","iopub.execute_input":"2022-01-29T19:58:13.689532Z","iopub.status.idle":"2022-01-29T19:58:14.038888Z","shell.execute_reply.started":"2022-01-29T19:58:13.689499Z","shell.execute_reply":"2022-01-29T19:58:14.037895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_sales_per_year = []\ntotal_sales_per_month_per_year = []\ntotal_sales_per_day_over_year = []\n\nfor i in range(0, len(years)):\n    total_sales_per_month_per_year.append([])\n    total_sales_per_day_over_year.append([])\n    total_sales_per_year.append(\n        df_train[df_train[\"year\"] == years[i]][\"num_sold\"].sum())\n    sub_df = df_train[df_train[\"year\"] == years[i]].copy(deep=True)\n    for month in months:\n        total_sales_per_month_per_year[i].append(\n            sub_df[sub_df[\"month\"] == month][\"num_sold\"].sum())\n    for day in days:\n        total_sales_per_day_over_year[i].append(\n            sub_df[sub_df[\"weekday\"] == day][\"num_sold\"].sum())\n\ntotal_sales_per_year /= sum(total_sales_per_year)\n\nplt.bar(years, total_sales_per_year)\nplt.xticks(years)\nplt.title(\"Total sales over years\")\nplt.ylabel(\"Percentage\")\nplt.show()\n\nfig, axs = plt.subplots(1, len(years), figsize=[16, 6], sharey=True)\nfor i in range(0, len(years)):\n    axs[i].bar(months, total_sales_per_month_per_year[i])\n    axs[i].set_title(str(\"Year: \" + str(years[i])))\n    axs[i].set_xticks(months)\n    axs[i].set_xlabel(\"Month\")\nplt.show()\n\nfig, axs = plt.subplots(1, len(years), figsize=[16, 6], sharey=True)\nfor i in range(0, len(years)):\n    axs[i].bar(days, total_sales_per_day_over_year[i])\n    axs[i].set_title(str(\"Year: \" + str(years[i])))\n    axs[i].set_xticks(days)\n    axs[i].set_xlabel(\"Weekday\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-29T19:58:14.039908Z","iopub.execute_input":"2022-01-29T19:58:14.040256Z","iopub.status.idle":"2022-01-29T19:58:15.193764Z","shell.execute_reply.started":"2022-01-29T19:58:14.040223Z","shell.execute_reply":"2022-01-29T19:58:15.192915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Sales growth from year to year\n* A seasonality is observable over the years: Sales increase in December and January, decrease in February, increase again in March, April and May, then decrease until August and increase again from September\n* Weekend concentrate sales","metadata":{}},{"cell_type":"code","source":"total_sales_per_country = []\ntotal_sales_per_country_over_years = []\nfor i in range(0, len(countries)):\n    total_sales_per_country_over_years.append([])\n    total_sales_per_country.append(\n        df_train[df_train[\"country\"] == countries[i]][\"num_sold\"].sum())\n    sub_df = df_train[df_train[\"country\"] == countries[i]].copy(deep=True)\n    for year in years:\n        total_sales_per_country_over_years[i].append(\n            sub_df[sub_df[\"year\"] == year][\"num_sold\"].sum())\n\ntotal_sales_per_country /= sum(total_sales_per_country)\nplt.bar(countries, total_sales_per_country)\nplt.xticks(countries)\nplt.title(\"Total sales over countries\")\nplt.ylabel(\"Sales percentage\")\nplt.show()\n\nfig, axs = plt.subplots(1, len(countries), figsize=[16, 6], sharey=True)\nfor i in range(0, len(countries)):\n    axs[i].bar(years, total_sales_per_country_over_years[i])\n    axs[i].set_xticks(years)\n    axs[i].set_title(str(\"Country: \" + str(countries[i])))\n    axs[i].set_xlabel(\"Year\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-29T19:58:15.195144Z","iopub.execute_input":"2022-01-29T19:58:15.19544Z","iopub.status.idle":"2022-01-29T19:58:15.690409Z","shell.execute_reply.started":"2022-01-29T19:58:15.195407Z","shell.execute_reply":"2022-01-29T19:58:15.689584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_sales_per_store = []\ntotal_sales_per_store_over_years = []\ntotal_sales_per_store_per_country = []\ntotal_sales_per_store_per_product = []\n\nfor i in range(0, len(stores)):\n    total_sales_per_store_over_years.append([])\n    total_sales_per_store_per_country.append([])\n    total_sales_per_store_per_product.append([])\n    total_sales_per_store.append(\n        df_train[df_train[\"store\"] == stores[i]][\"num_sold\"].sum())\n    sub_df = df_train[df_train[\"store\"] == stores[i]].copy(deep=True)\n    for year in years:\n        total_sales_per_store_over_years[i].append(\n            sub_df[sub_df[\"year\"] == year][\"num_sold\"].sum())\n    for country in countries:\n        total_sales_per_store_per_country[i].append(\n            sub_df[sub_df[\"country\"] == country][\"num_sold\"].sum())\n    for product in products:\n        total_sales_per_store_per_product[i].append(\n            sub_df[sub_df[\"product\"] == product][\"num_sold\"].sum())\n\ntotal_sales_per_store /= sum(total_sales_per_store)\nplt.bar(stores, total_sales_per_store)\nplt.xticks(stores)\nplt.title(\"Total sales per store over the years\")\nplt.ylabel(\"Sales percentage\")\nplt.show()\n\nfig, axs = plt.subplots(1, len(stores), figsize=[16, 6], sharey=True)\nfor i in range(0, len(stores)):\n    axs[i].bar(years, total_sales_per_store_over_years[i])\n    axs[i].set_xticks(years)\n    axs[i].set_title(str(\"Store: \" + str(stores[i])))\n    axs[i].set_xlabel(\"Year\")\nplt.show()\n\nfig, axs = plt.subplots(1, len(stores), figsize=[16, 6], sharey=True)\nfor i in range(0, len(stores)):\n    axs[i].bar(countries, total_sales_per_store_per_country[i])\n    axs[i].set_xticks(countries)\n    axs[i].set_title(str(\"Store: \" + str(stores[i])))\n    axs[i].set_xlabel(\"Country\")\nplt.show()\n\nfig, axs = plt.subplots(1, len(stores), figsize=[16, 6], sharey=True)\nfor i in range(0, len(stores)):\n    axs[i].bar(products, total_sales_per_store_per_product[i])\n    axs[i].set_xticks(products)\n    axs[i].set_title(str(\"Store: \" + str(stores[i])))\n    axs[i].set_xlabel(\"Product\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-29T19:58:15.691779Z","iopub.execute_input":"2022-01-29T19:58:15.692018Z","iopub.status.idle":"2022-01-29T19:58:16.605132Z","shell.execute_reply.started":"2022-01-29T19:58:15.691989Z","shell.execute_reply":"2022-01-29T19:58:16.60424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_sales_per_product = []\ntotal_sales_per_product_over_years = []\ntotal_sales_per_product_per_country = []\n\nfor i in range(0, len(products)):\n    total_sales_per_product_over_years.append([])\n    total_sales_per_product_per_country.append([])\n    total_sales_per_product.append(\n        df_train[df_train[\"product\"] == products[i]][\"num_sold\"].sum())\n    sub_df = df_train[df_train[\"product\"] == products[i]].copy(deep=True)\n    for year in years:\n        total_sales_per_product_over_years[i].append(\n            sub_df[sub_df[\"year\"] == year][\"num_sold\"].sum())\n    for country in countries:\n        total_sales_per_product_per_country[i].append(\n            sub_df[sub_df[\"country\"] == country][\"num_sold\"].sum())\n\ntotal_sales_per_product /= sum(total_sales_per_product)\nplt.bar(products, total_sales_per_product)\nplt.xticks(products)\nplt.title(\"Total sales per product over the years\")\nplt.ylabel(\"Sales percentage\")\nplt.show()\n\nfig, axs = plt.subplots(1, len(products), figsize=[16, 6], sharey=True)\nfor i in range(0, len(products)):\n    axs[i].bar(years, total_sales_per_product_over_years[i])\n    axs[i].set_xticks(years)\n    axs[i].set_title(str(\"Products: \" + str(products[i])))\n    axs[i].set_xlabel(\"Year\")\nplt.show()\n\nfig, axs = plt.subplots(1, len(products), figsize=[16, 6], sharey=True)\nfor i in range(0, len(products)):\n    axs[i].bar(countries, total_sales_per_product_per_country[i])\n    axs[i].set_xticks(countries)\n    axs[i].set_title(str(\"Product: \" + str(products[i])))\n    axs[i].set_xlabel(\"Country\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-29T19:58:16.60659Z","iopub.execute_input":"2022-01-29T19:58:16.607323Z","iopub.status.idle":"2022-01-29T19:58:17.571192Z","shell.execute_reply.started":"2022-01-29T19:58:16.607278Z","shell.execute_reply":"2022-01-29T19:58:17.570408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6.PREPROCESSING","metadata":{}},{"cell_type":"markdown","source":"### 6.1 Encode categorical columns","metadata":{}},{"cell_type":"code","source":"le = preprocessing.LabelEncoder()\n\nfor c in CATEGORICAL_COLUMNS:\n    df_train[c] = le.fit_transform(df_train[c])\n    df_test[c] = le.transform(df_test[c])","metadata":{"execution":{"iopub.status.busy":"2022-01-29T19:58:17.572325Z","iopub.execute_input":"2022-01-29T19:58:17.57268Z","iopub.status.idle":"2022-01-29T19:58:17.596027Z","shell.execute_reply.started":"2022-01-29T19:58:17.572648Z","shell.execute_reply":"2022-01-29T19:58:17.59544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 6.2 Construct X and y arrays","metadata":{}},{"cell_type":"code","source":"y = np.array(df_train[\"num_sold\"])\ny = y.reshape(-1, 1)","metadata":{"execution":{"iopub.status.busy":"2022-01-29T19:58:17.596916Z","iopub.execute_input":"2022-01-29T19:58:17.597703Z","iopub.status.idle":"2022-01-29T19:58:17.601356Z","shell.execute_reply.started":"2022-01-29T19:58:17.597669Z","shell.execute_reply":"2022-01-29T19:58:17.600816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = np.array(df_train.drop(columns=\"num_sold\"))\nX_to_pred = np.array(df_test.drop(columns=[\"row_id\"]))","metadata":{"execution":{"iopub.status.busy":"2022-01-29T19:58:17.60217Z","iopub.execute_input":"2022-01-29T19:58:17.602735Z","iopub.status.idle":"2022-01-29T19:58:17.617044Z","shell.execute_reply.started":"2022-01-29T19:58:17.602701Z","shell.execute_reply":"2022-01-29T19:58:17.616427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 6.3 Split train dataframe into train and test set","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Save set to use it in different branches\nif not os.path.exists(\"data/\"):\n    os.mkdir(\"data/\")\nnp.save(\"data/x_train.npy\", X_train)\nnp.save(\"data/x_test.npy\", X_test)\nnp.save(\"data/y_train.npy\", y_train)\nnp.save(\"data/y_test.npy\", y_test)","metadata":{"execution":{"iopub.status.busy":"2022-01-29T19:58:17.617939Z","iopub.execute_input":"2022-01-29T19:58:17.618473Z","iopub.status.idle":"2022-01-29T19:58:17.635763Z","shell.execute_reply.started":"2022-01-29T19:58:17.618443Z","shell.execute_reply":"2022-01-29T19:58:17.63513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 6.4 Create the appropriate score","metadata":{}},{"cell_type":"code","source":"def smape(y_true, y_pred):\n    return (100/y_true.shape[0]) * np.sum(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred)))\n\n# SMAPE must be lowered to increase performances\nsmape_score = make_scorer(score_func=smape, greater_is_better=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-29T19:58:17.637254Z","iopub.execute_input":"2022-01-29T19:58:17.637563Z","iopub.status.idle":"2022-01-29T19:58:17.644222Z","shell.execute_reply.started":"2022-01-29T19:58:17.637524Z","shell.execute_reply":"2022-01-29T19:58:17.643588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 7. Training","metadata":{}},{"cell_type":"markdown","source":"I will train Gradient boosting + random forest regressor on the train set. The training will be performed on a grid to optimize hyperparameters and validate them through a cross validation on 5-folds. \n\nGridSearchCV will be used to achieve it.\n\nFirst, load train and test set from data folder to make sure to share the same set between all branches / try / models.","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = load_train_test_set()","metadata":{"execution":{"iopub.status.busy":"2022-01-29T19:58:17.647174Z","iopub.execute_input":"2022-01-29T19:58:17.647593Z","iopub.status.idle":"2022-01-29T19:58:17.659263Z","shell.execute_reply.started":"2022-01-29T19:58:17.647562Z","shell.execute_reply":"2022-01-29T19:58:17.658444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gboost = GradientBoostingRegressor(random_state=0)\nparameters = {'n_estimators': (50, 75, 100, 150, 200, 250, 300, 500),\n              'learning_rate': (0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.08, 0.1, 0.15, 0.2),\n              'max_depth': (3, 5, 8), \"max_features\": [\"auto\", \"log2\"]\n              }\nres = GridSearchCV(estimator=gboost, param_grid=parameters,\n                   scoring=smape_score, cv=5).fit(X_train, y_train.ravel())\n\nprint(\"Best params found through the gridsearch= \", res.best_params_)\n\nopti_gboost = res.best_estimator_","metadata":{"execution":{"iopub.status.busy":"2022-01-29T19:58:17.660301Z","iopub.execute_input":"2022-01-29T19:58:17.66259Z","iopub.status.idle":"2022-01-29T21:13:23.287065Z","shell.execute_reply.started":"2022-01-29T19:58:17.662545Z","shell.execute_reply":"2022-01-29T21:13:23.286194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Below I define a list of values that will be added to the output predictions in order to check if the global minimum has been reached.","metadata":{}},{"cell_type":"code","source":"biais = [i for i in range(-30, 30)]","metadata":{"execution":{"iopub.status.busy":"2022-01-29T21:13:23.288486Z","iopub.execute_input":"2022-01-29T21:13:23.288717Z","iopub.status.idle":"2022-01-29T21:13:23.294864Z","shell.execute_reply.started":"2022-01-29T21:13:23.288689Z","shell.execute_reply":"2022-01-29T21:13:23.2943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate_smape_model(model=opti_gboost, X_test=X_test, y_test=y_test, biais_values=biais)","metadata":{"execution":{"iopub.status.busy":"2022-01-29T21:13:23.295684Z","iopub.execute_input":"2022-01-29T21:13:23.29647Z","iopub.status.idle":"2022-01-29T21:13:23.576066Z","shell.execute_reply.started":"2022-01-29T21:13:23.296439Z","shell.execute_reply":"2022-01-29T21:13:23.575243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 8. Impact of replacing outliers by mean","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = load_train_test_set()","metadata":{"execution":{"iopub.status.busy":"2022-01-29T21:13:23.577326Z","iopub.execute_input":"2022-01-29T21:13:23.578207Z","iopub.status.idle":"2022-01-29T21:13:23.585582Z","shell.execute_reply.started":"2022-01-29T21:13:23.578163Z","shell.execute_reply":"2022-01-29T21:13:23.58472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = np.array(df_train[\"num_sold\"])\nlimit = round(np.percentile(y, 96), 2)\noutliers = y_train > limit\ny_train[outliers] = np.mean(y)\noutliers = y_test > limit\ny_test[outliers] = np.mean(y)","metadata":{"execution":{"iopub.status.busy":"2022-01-29T21:13:23.587341Z","iopub.execute_input":"2022-01-29T21:13:23.587877Z","iopub.status.idle":"2022-01-29T21:13:23.599931Z","shell.execute_reply.started":"2022-01-29T21:13:23.587843Z","shell.execute_reply":"2022-01-29T21:13:23.599075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"res = GridSearchCV(estimator=gboost, param_grid=parameters,\n                   scoring=smape_score, cv=5).fit(X_train, y_train.ravel())\n\nprint(\"Best params found through the gridsearch= \", res.best_params_)\n\nopti_gboost2 = res.best_estimator_","metadata":{"execution":{"iopub.status.busy":"2022-01-29T21:13:23.601118Z","iopub.execute_input":"2022-01-29T21:13:23.601782Z","iopub.status.idle":"2022-01-29T21:39:53.010707Z","shell.execute_reply.started":"2022-01-29T21:13:23.601748Z","shell.execute_reply":"2022-01-29T21:39:53.009303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate_smape_model(model=opti_gboost2, X_test=X_test, y_test=y_test, biais_values=biais)","metadata":{"execution":{"iopub.status.busy":"2022-01-29T21:39:53.011969Z","iopub.status.idle":"2022-01-29T21:39:53.012721Z","shell.execute_reply.started":"2022-01-29T21:39:53.012423Z","shell.execute_reply":"2022-01-29T21:39:53.012456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 9. Impact of replacing outliers by limit (96th percentile)","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = load_train_test_set()","metadata":{"execution":{"iopub.status.busy":"2022-01-29T21:39:53.014263Z","iopub.status.idle":"2022-01-29T21:39:53.014739Z","shell.execute_reply.started":"2022-01-29T21:39:53.014493Z","shell.execute_reply":"2022-01-29T21:39:53.014518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = np.array(df_train[\"num_sold\"])\nlimit = round(np.percentile(y, 96), 2)\noutliers = y_train > limit\ny_train[outliers] = limit\noutliers = y_test > limit\ny_test[outliers] = limit","metadata":{"execution":{"iopub.status.busy":"2022-01-29T21:39:53.01687Z","iopub.status.idle":"2022-01-29T21:39:53.017489Z","shell.execute_reply.started":"2022-01-29T21:39:53.017218Z","shell.execute_reply":"2022-01-29T21:39:53.017244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"res = GridSearchCV(estimator=gboost, param_grid=parameters,\n                   scoring=smape_score, cv=5).fit(X_train, y_train.ravel())\n\nprint(\"Best params found through the gridsearch= \", res.best_params_)\n\nopti_gboost3 = res.best_estimator_","metadata":{"execution":{"iopub.status.busy":"2022-01-29T21:39:53.01855Z","iopub.status.idle":"2022-01-29T21:39:53.019177Z","shell.execute_reply.started":"2022-01-29T21:39:53.018907Z","shell.execute_reply":"2022-01-29T21:39:53.018935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate_smape_model(model=opti_gboost3, X_test=X_test, y_test=y_test, biais_values=biais)","metadata":{"execution":{"iopub.status.busy":"2022-01-29T21:39:53.020744Z","iopub.status.idle":"2022-01-29T21:39:53.021528Z","shell.execute_reply.started":"2022-01-29T21:39:53.021242Z","shell.execute_reply":"2022-01-29T21:39:53.021271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 10. Train the random forest regressor as the gradient boosting","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = load_train_test_set()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rfr = RandomForestRegressor()\nparameters = {'n_estimators': (100, 200, 300, 500, 1000, 1500, 2000),\n              'max_depth': [None, 3, 5],\n              'bootstrap': [True, False]\n              }\nres = GridSearchCV(estimator=rfr, param_grid=parameters,\n                   scoring=smape_score, cv=5).fit(X_train, y_train.ravel())\n\nprint(\"Best params found through the gridsearch: \", res.best_params_)\n\nopti_rfr = res.best_estimator_","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"High number of estimators with random forest regressor avoid overfitting but consume more computation time.","metadata":{}},{"cell_type":"code","source":"evaluate_smape_model(model=opti_rfr, X_test=X_test, y_test=y_test, biais_values=biais)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 11. Impact of replacing outliers by mean","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = load_train_test_set()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = np.array(df_train[\"num_sold\"])\nlimit = round(np.percentile(y, 96), 2)\noutliers = y_train > limit\ny_train[outliers] = np.mean(y)\noutliers = y_test > limit\ny_test[outliers] = np.mean(y)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"res = GridSearchCV(estimator=rfr, param_grid=parameters,\n                   scoring=smape_score, cv=5).fit(X_train, y_train.ravel())\n\nprint(\"Best params found through the gridsearch: \", res.best_params_)\n\nopti_rfr2 = res.best_estimator_","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate_smape_model(model=opti_rfr2, X_test=X_test, y_test=y_test, biais_values=biais)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 12. Impact of replacing outliers by limit","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = load_train_test_set()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = np.array(df_train[\"num_sold\"])\nlimit = round(np.percentile(y, 96), 2)\noutliers = y_train > limit\ny_train[outliers] = limit\noutliers = y_test > limit\ny_test[outliers] = limit","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"res = GridSearchCV(estimator=rfr, param_grid=parameters,\n                   scoring=smape_score, cv=5).fit(X_train, y_train.ravel())\n\nprint(\"Best params found through the gridsearch: \", res.best_params_)\n\nopti_rfr3 = res.best_estimator_","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate_smape_model(model=opti_rfr3, X_test=X_test, y_test=y_test, biais_values=biais)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 13. Forecast the number of sales for the coming years (Test dataframe)","metadata":{}},{"cell_type":"markdown","source":"Here the gradient boosting model with a biais of -2 and without preprocessing on num_sold is selected despite the SMAPE on test set is not the lower. Indeed, clamping the value of num_sold based on a limit defined from the past could be a good idea only if the sale behaviour will be same as the one in past. But the data exploration shows that there is a stable growth market year from year to year so clamping the values could not represent this growth.","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = load_train_test_set()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_biais = -2","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_sold = np.round(opti_gboost.predict(X_to_pred)).reshape(-1, 1).astype(int) + best_biais","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(SUBMISSION)\ndf[\"num_sold\"] = num_sold\ndf.to_csv(\"submission.csv\", index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 14. Sales predictions per store","metadata":{}},{"cell_type":"code","source":"df_train_original = pd.read_csv(INPUT_TRAIN)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for c in CATEGORICAL_COLUMNS:\n    df_train_original[c] = le.fit_transform(df_train_original[c])\n    df_test[c] = le.inverse_transform(df_test[c])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test[\"num_sold\"] = num_sold","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kagglemart_sales = df_test[df_test[\"store\"] == \"KaggleMart\"][\"num_sold\"].sum()\nkagglerama_sales = df_test[df_test[\"store\"] == \"KaggleRama\"][\"num_sold\"].sum()\nprint(\"Sales predictions for KaggleMart\", kagglemart_sales)\nprint(\"Sales predictions for KaggleRama\", kagglerama_sales)\nprint(\"Checksum valid ?\", num_sold.sum() == kagglemart_sales + kagglerama_sales)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b> Conclusion:</b> KaggleRama will have the higher number of sales for 2019.","metadata":{}},{"cell_type":"markdown","source":"# REFERENCES","metadata":{}},{"cell_type":"markdown","source":"* Tensor girl: https://www.kaggle.com/usharengaraju/tensorflow-tf-data-keraspreprocessinglayers-w-b for the cheat code to compute the feature is_weekend in one line of code\n\n* SMAPE formula : https://en.wikipedia.org/wiki/Symmetric_mean_absolute_percentage_error","metadata":{}}]}