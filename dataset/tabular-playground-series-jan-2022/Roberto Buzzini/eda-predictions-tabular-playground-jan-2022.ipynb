{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:31:34.120269Z","iopub.execute_input":"2022-01-30T15:31:34.120682Z","iopub.status.idle":"2022-01-30T15:31:34.131744Z","shell.execute_reply.started":"2022-01-30T15:31:34.120638Z","shell.execute_reply":"2022-01-30T15:31:34.130629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# importing useful packages\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme()\npd.options.display.float_format = '{:,}'.format","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:31:34.133896Z","iopub.execute_input":"2022-01-30T15:31:34.134533Z","iopub.status.idle":"2022-01-30T15:31:34.142722Z","shell.execute_reply.started":"2022-01-30T15:31:34.134492Z","shell.execute_reply":"2022-01-30T15:31:34.141709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Importing Datasets:**","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(r'../input/tabular-playground-series-jan-2022/train.csv')\ntest = pd.read_csv(r'../input/tabular-playground-series-jan-2022/test.csv')","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:31:34.144444Z","iopub.execute_input":"2022-01-30T15:31:34.144764Z","iopub.status.idle":"2022-01-30T15:31:34.196612Z","shell.execute_reply.started":"2022-01-30T15:31:34.144734Z","shell.execute_reply":"2022-01-30T15:31:34.195668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#holidays = pd.read_csv(r'../input/gdp-of-finland-norway-and-sweden-2015-2019/GDP_data_2015_to_2019_Finland_Norway_Sweden.csv', parse_dates=['date'])\n#holidays.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:31:34.198107Z","iopub.execute_input":"2022-01-30T15:31:34.198369Z","iopub.status.idle":"2022-01-30T15:31:34.202792Z","shell.execute_reply.started":"2022-01-30T15:31:34.198335Z","shell.execute_reply":"2022-01-30T15:31:34.201475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## EDA","metadata":{}},{"cell_type":"markdown","source":"**First overview of the datasets:**","metadata":{}},{"cell_type":"code","source":"print(f'train shape: {train.shape}', f'test shape: {test.shape}', f'train rows / (train + test rows): {train.shape[0]/(train.shape[0]+test.shape[0])}', sep='\\n')","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:31:34.206075Z","iopub.execute_input":"2022-01-30T15:31:34.207222Z","iopub.status.idle":"2022-01-30T15:31:34.218063Z","shell.execute_reply.started":"2022-01-30T15:31:34.207166Z","shell.execute_reply":"2022-01-30T15:31:34.216688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Which columns compose the datasets?","metadata":{}},{"cell_type":"code","source":"print(f'train columns: {train.columns}', f'test columns: {test.columns}', sep='\\n')","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:31:34.219759Z","iopub.execute_input":"2022-01-30T15:31:34.221268Z","iopub.status.idle":"2022-01-30T15:31:34.230422Z","shell.execute_reply.started":"2022-01-30T15:31:34.221204Z","shell.execute_reply":"2022-01-30T15:31:34.229651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see, we have to predict the sales occured in each store in order to predict are best ones going forward.","metadata":{}},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:31:34.231533Z","iopub.execute_input":"2022-01-30T15:31:34.232134Z","iopub.status.idle":"2022-01-30T15:31:34.256402Z","shell.execute_reply.started":"2022-01-30T15:31:34.232095Z","shell.execute_reply":"2022-01-30T15:31:34.255532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['date'] = pd.to_datetime(train['date'])\ntest['date'] = pd.to_datetime(test['date'])","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:31:34.257889Z","iopub.execute_input":"2022-01-30T15:31:34.258302Z","iopub.status.idle":"2022-01-30T15:31:34.275877Z","shell.execute_reply.started":"2022-01-30T15:31:34.258265Z","shell.execute_reply":"2022-01-30T15:31:34.275167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"How many missing values are there in training and test sets?","metadata":{}},{"cell_type":"code","source":"train.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:31:34.277087Z","iopub.execute_input":"2022-01-30T15:31:34.277426Z","iopub.status.idle":"2022-01-30T15:31:34.294707Z","shell.execute_reply.started":"2022-01-30T15:31:34.277398Z","shell.execute_reply":"2022-01-30T15:31:34.29401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:31:34.296077Z","iopub.execute_input":"2022-01-30T15:31:34.29634Z","iopub.status.idle":"2022-01-30T15:31:34.311823Z","shell.execute_reply.started":"2022-01-30T15:31:34.296308Z","shell.execute_reply":"2022-01-30T15:31:34.311167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are no NA's, great news.","metadata":{}},{"cell_type":"markdown","source":"- How many / Which countries contains a Kaggle store? \n- How many Kaggle stores are there in the dataset? H\n- How many / Which products are there? \n- Which countries/stores sell more?","metadata":{}},{"cell_type":"code","source":"train['country'].unique()","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:31:34.313569Z","iopub.execute_input":"2022-01-30T15:31:34.314105Z","iopub.status.idle":"2022-01-30T15:31:34.327621Z","shell.execute_reply.started":"2022-01-30T15:31:34.314061Z","shell.execute_reply":"2022-01-30T15:31:34.326881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Kaggle stores are situated only in northern Europe.","metadata":{}},{"cell_type":"code","source":"train['store'].unique()","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:31:34.329108Z","iopub.execute_input":"2022-01-30T15:31:34.32945Z","iopub.status.idle":"2022-01-30T15:31:34.341121Z","shell.execute_reply.started":"2022-01-30T15:31:34.3294Z","shell.execute_reply":"2022-01-30T15:31:34.339835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are two different stores.","metadata":{}},{"cell_type":"code","source":"train['product'].unique()","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:31:34.344026Z","iopub.execute_input":"2022-01-30T15:31:34.344446Z","iopub.status.idle":"2022-01-30T15:31:34.353913Z","shell.execute_reply.started":"2022-01-30T15:31:34.344313Z","shell.execute_reply":"2022-01-30T15:31:34.353038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As We can expected, it seems that stores contain only nerd products :)","metadata":{}},{"cell_type":"code","source":"country_sold = train.groupby('country')['num_sold'].sum().sort_values(ascending=False)\ncountry_sold.map('{:,}'.format)","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:31:34.357659Z","iopub.execute_input":"2022-01-30T15:31:34.358143Z","iopub.status.idle":"2022-01-30T15:31:34.371417Z","shell.execute_reply.started":"2022-01-30T15:31:34.358108Z","shell.execute_reply":"2022-01-30T15:31:34.370756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.pie(country_sold.values, labels=country_sold.index, autopct='%0.1f%%')\nplt.title('Sales share per country')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:31:34.372949Z","iopub.execute_input":"2022-01-30T15:31:34.374111Z","iopub.status.idle":"2022-01-30T15:31:34.489043Z","shell.execute_reply.started":"2022-01-30T15:31:34.374067Z","shell.execute_reply":"2022-01-30T15:31:34.488004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"products_mean = train.groupby(['country', 'store', 'product']).agg(\n    {'product': 'count', 'num_sold': 'mean'})\nproducts_mean['product'].map('{:,}'.format)\nproducts_mean['num_sold'].map('{:,}'.format)\nproducts_mean","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:31:34.491011Z","iopub.execute_input":"2022-01-30T15:31:34.491545Z","iopub.status.idle":"2022-01-30T15:31:34.539418Z","shell.execute_reply.started":"2022-01-30T15:31:34.491492Z","shell.execute_reply":"2022-01-30T15:31:34.538464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Countries, stores and products sold are equally distributed in train set, but some places sell more in quantity. I would say that every product and every store are registered for every day between 2015 and 2018\n- Norway through the years sold more than 4 millions of pieces, for about 43% of all sales\n- Finland in the country with the worst results\n- It seems that the Kaggle Hat is the best product everywhere, followed by the Kaggle Mug.\n\nLet's see which store sells more between KaggleRama and KaggleMart in each country: \n","metadata":{}},{"cell_type":"code","source":"store_sold_average = train.groupby(['country', 'store']).agg(\n    {'num_sold': 'mean'})\nstore_sold_average","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:31:34.541314Z","iopub.execute_input":"2022-01-30T15:31:34.541839Z","iopub.status.idle":"2022-01-30T15:31:34.569901Z","shell.execute_reply.started":"2022-01-30T15:31:34.541783Z","shell.execute_reply":"2022-01-30T15:31:34.568941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"store_sold_total = train.groupby(['store'])['num_sold'].sum()\nplt.pie(store_sold_total.values, labels=store_sold_total.index, autopct='%0.1f%%')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:31:34.571753Z","iopub.execute_input":"2022-01-30T15:31:34.572093Z","iopub.status.idle":"2022-01-30T15:31:34.70878Z","shell.execute_reply.started":"2022-01-30T15:31:34.57205Z","shell.execute_reply":"2022-01-30T15:31:34.70791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- KaggleRama seems to bee the best store, on average and on total sales, counting for the 63.5% of total amount\n\nLet's see the sales distribution for each country and store.","metadata":{}},{"cell_type":"code","source":"sns.kdeplot(x=train['num_sold'], hue=train['country'])\nplt.title('Sales distribution')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:31:34.710659Z","iopub.execute_input":"2022-01-30T15:31:34.711213Z","iopub.status.idle":"2022-01-30T15:31:35.167531Z","shell.execute_reply.started":"2022-01-30T15:31:34.711168Z","shell.execute_reply":"2022-01-30T15:31:35.166688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.kdeplot(x=train['num_sold'], hue=train['store'])\nplt.title('Sales distribution')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:31:35.169288Z","iopub.execute_input":"2022-01-30T15:31:35.169827Z","iopub.status.idle":"2022-01-30T15:31:35.736031Z","shell.execute_reply.started":"2022-01-30T15:31:35.169783Z","shell.execute_reply":"2022-01-30T15:31:35.734991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Sales distribution is right skewed in all countries and stores","metadata":{}},{"cell_type":"markdown","source":"The df is a timeseries, so I'll transform date column into the index and we can then add year, month and weekday name columns:","metadata":{}},{"cell_type":"code","source":"train.set_index(train['date'], inplace=True)\ntest.set_index(test['date'], inplace=True)\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:31:35.73731Z","iopub.execute_input":"2022-01-30T15:31:35.737539Z","iopub.status.idle":"2022-01-30T15:31:35.752Z","shell.execute_reply.started":"2022-01-30T15:31:35.737511Z","shell.execute_reply":"2022-01-30T15:31:35.751017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['Year'] = train.index.year\ntrain['Month'] = train.index.month\ntrain['Weekday'] = train.index.day_name()\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:31:35.753222Z","iopub.execute_input":"2022-01-30T15:31:35.753435Z","iopub.status.idle":"2022-01-30T15:31:35.78911Z","shell.execute_reply.started":"2022-01-30T15:31:35.753408Z","shell.execute_reply":"2022-01-30T15:31:35.788283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's visualize something possible time patterns:","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=[12, 6])\ntrain['num_sold'].plot(linewidth=0.5)","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:31:35.790245Z","iopub.execute_input":"2022-01-30T15:31:35.790474Z","iopub.status.idle":"2022-01-30T15:31:36.567254Z","shell.execute_reply.started":"2022-01-30T15:31:35.790446Z","shell.execute_reply":"2022-01-30T15:31:36.566424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Sales are too variables, but we can instantly see some patterns:\n- there is always a peak in sales around December/January then they go immediately down\n- after sales increase until May/June/July and they go down until October/November\n- the cycle is repeated\n\nWe can say that there are regular seasonality and cyclical trends.\n\nLet's see if the trend is the same for each country and store:","metadata":{}},{"cell_type":"code","source":"for country in train['country'].unique():\n    temp_df = train.copy()\n    temp_df.loc[temp_df['country'] == country, 'num_sold'].plot(linewidth=0.5)\n    plt.title(country)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:31:36.568391Z","iopub.execute_input":"2022-01-30T15:31:36.56862Z","iopub.status.idle":"2022-01-30T15:31:38.532368Z","shell.execute_reply.started":"2022-01-30T15:31:36.568592Z","shell.execute_reply":"2022-01-30T15:31:38.531585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- The trend is exactly the same for each country, and what about the stores?\n","metadata":{}},{"cell_type":"code","source":"for country in train['country'].unique():\n    temp_df = train.copy()\n    for store in temp_df['store'].unique():\n        temp_df.loc[temp_df['store'] == store, 'num_sold'].plot(linewidth=0.5)\n        plt.title([country, store])\n        plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:31:38.533557Z","iopub.execute_input":"2022-01-30T15:31:38.533795Z","iopub.status.idle":"2022-01-30T15:31:42.906196Z","shell.execute_reply.started":"2022-01-30T15:31:38.533749Z","shell.execute_reply":"2022-01-30T15:31:42.905458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The same for the stores.","metadata":{}},{"cell_type":"markdown","source":"Let's see if kaggle sales are going up through the years as it seems and which are the best months and days of the week to sell:","metadata":{}},{"cell_type":"code","source":"train['num_sold'].resample('Y').sum().map('{:,}'.format)","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:31:42.907769Z","iopub.execute_input":"2022-01-30T15:31:42.908017Z","iopub.status.idle":"2022-01-30T15:31:42.920029Z","shell.execute_reply.started":"2022-01-30T15:31:42.907986Z","shell.execute_reply":"2022-01-30T15:31:42.919098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Kaggle sales are going up through last years.","metadata":{}},{"cell_type":"code","source":"year_sales = train['num_sold'].resample('Y').sum()\nyear_sales.pct_change()","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:31:42.921282Z","iopub.execute_input":"2022-01-30T15:31:42.921548Z","iopub.status.idle":"2022-01-30T15:31:42.936094Z","shell.execute_reply.started":"2022-01-30T15:31:42.921514Z","shell.execute_reply":"2022-01-30T15:31:42.935406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Sales increased for about 7% from 2017 to 2018!","metadata":{}},{"cell_type":"code","source":"year_month_group = train.groupby(['Year', 'Month']).agg(\n    {'num_sold': 'sum'}).sort_values(by=['Year', 'num_sold'], ascending=False)\nyear_month_group['num_sold'] = year_month_group['num_sold'].map('{:,}'.format)\nyear_month_group","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:31:42.937605Z","iopub.execute_input":"2022-01-30T15:31:42.938204Z","iopub.status.idle":"2022-01-30T15:31:42.965101Z","shell.execute_reply.started":"2022-01-30T15:31:42.938161Z","shell.execute_reply":"2022-01-30T15:31:42.964044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"month_group = train.groupby(['Month']).agg(\n    {'num_sold': 'sum'}).sort_values(by=['num_sold'], ascending=False)\nmonth_group['num_sold'] = month_group['num_sold'].map('{:,}'.format)\nmonth_group","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:31:42.966846Z","iopub.execute_input":"2022-01-30T15:31:42.967486Z","iopub.status.idle":"2022-01-30T15:31:42.985672Z","shell.execute_reply.started":"2022-01-30T15:31:42.96743Z","shell.execute_reply":"2022-01-30T15:31:42.98501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- December is the month with most sales, probably for Christmas\n- The The first part of the year from January to May is a good time for kaggle pockets","metadata":{}},{"cell_type":"code","source":"weekday_sales = train.groupby(['Year', 'Weekday']).agg(\n    {'num_sold': 'sum'}).sort_values(by=['Year', 'num_sold'], ascending=False)\nweekday_sales['num_sold'] = weekday_sales['num_sold'].map('{:,}'.format)\nweekday_sales","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:31:42.986926Z","iopub.execute_input":"2022-01-30T15:31:42.987317Z","iopub.status.idle":"2022-01-30T15:31:43.011216Z","shell.execute_reply.started":"2022-01-30T15:31:42.987286Z","shell.execute_reply":"2022-01-30T15:31:43.010268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Most of sales are concentrated in the weekend.\n\n\nLet's see the variability in each year and month.","metadata":{}},{"cell_type":"code","source":"sns.boxplot(data=train, x='Year', y='num_sold')\nplt.ylabel('num_sold')\nplt.title('Sales by Year')","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:31:43.012996Z","iopub.execute_input":"2022-01-30T15:31:43.013497Z","iopub.status.idle":"2022-01-30T15:31:43.343237Z","shell.execute_reply.started":"2022-01-30T15:31:43.013462Z","shell.execute_reply":"2022-01-30T15:31:43.342358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Variability through each year is pretty much the same.\n\n- There are many outliers. We'll to take care of that in training model section","metadata":{}},{"cell_type":"code","source":"sns.boxplot(data=train, x='Month', y='num_sold')\nplt.ylabel('num_sold')\nplt.title('Sales by Year')","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:31:43.344685Z","iopub.execute_input":"2022-01-30T15:31:43.347243Z","iopub.status.idle":"2022-01-30T15:31:43.840371Z","shell.execute_reply.started":"2022-01-30T15:31:43.347191Z","shell.execute_reply":"2022-01-30T15:31:43.83954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Last plots didn't tell much more then what we saw earlier.","metadata":{}},{"cell_type":"markdown","source":"Let's explore Sales average per month trend","metadata":{}},{"cell_type":"code","source":"df = train.groupby(['country','Year','Month']).num_sold.mean().reset_index()\nfig = plt.figure(figsize  = (20,13)) \nyear = 2015\nfor i in range(4):\n    ax = fig.add_subplot(2,2,i+1)\n    ax.plot(df[(df['Year']==year) & (df['country']=='Norway')]['Month'], df[(df['Year']==year) & (df['country']=='Norway')]['num_sold'], label = 'Norway')\n    ax.plot(df[(df['Year']==year) & (df['country']=='Sweden')]['Month'], df[(df['Year']==year) & (df['country']=='Sweden')]['num_sold'], label = 'Sweden')\n    ax.plot(df[(df['Year']==year) & (df['country']=='Finland')]['Month'], df[(df['Year']==year) & (df['country']=='Finland')]['num_sold'], label = 'Finland')\n    ax.title.set_text(f'Avg Monthly Sales Trend in {year}')\n    ax.set_ylabel('Average Sales')\n    ax.set_xlabel('Month')\n    ax.legend()\n    year+=1","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:31:43.842129Z","iopub.execute_input":"2022-01-30T15:31:43.842696Z","iopub.status.idle":"2022-01-30T15:31:45.217106Z","shell.execute_reply.started":"2022-01-30T15:31:43.842639Z","shell.execute_reply":"2022-01-30T15:31:45.216063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Trends are pretty much the same, lines are almost the same line translated, especially for Sweden and Finland\n\nWhat about the stores?","metadata":{}},{"cell_type":"code","source":"df = train.groupby(['store','Year','Month']).num_sold.mean().reset_index()\nfig = plt.figure(figsize  = (20,13)) \nyear = 2015\nfor i in range(4):\n    ax = fig.add_subplot(2,2,i+1)\n    ax.plot(df[(df['Year']==year) & (df['store']=='KaggleRama')]['Month'], df[(df['Year']==year) & (df['store']=='KaggleRama')]['num_sold'], label = 'KaggleRama')\n    ax.plot(df[(df['Year']==year) & (df['store']=='KaggleMart')]['Month'], df[(df['Year']==year) & (df['store']=='KaggleMart')]['num_sold'], label = 'KaggleMart')\n    ax.title.set_text(f'Avg Monthly Sales Trend in {year}')\n    ax.set_ylabel('Average Sales')\n    ax.set_xlabel('Month')\n    ax.legend()\n    year+=1","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:31:45.218592Z","iopub.execute_input":"2022-01-30T15:31:45.2189Z","iopub.status.idle":"2022-01-30T15:31:46.293433Z","shell.execute_reply.started":"2022-01-30T15:31:45.218865Z","shell.execute_reply":"2022-01-30T15:31:46.292244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Same insights from the stores\n\nAnd what about the products?","metadata":{}},{"cell_type":"code","source":"train['product'].unique()","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:31:46.295006Z","iopub.execute_input":"2022-01-30T15:31:46.295342Z","iopub.status.idle":"2022-01-30T15:31:46.305672Z","shell.execute_reply.started":"2022-01-30T15:31:46.295302Z","shell.execute_reply":"2022-01-30T15:31:46.30459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = train.groupby(['product','Year','Month']).num_sold.mean().reset_index()\nfig = plt.figure(figsize  = (20,13)) \nyear = 2015\nfor i in range(4):\n    ax = fig.add_subplot(2,2,i+1)\n    ax.plot(df[(df['Year']==year) & (df['product']=='Kaggle Mug')]['Month'], df[(df['Year']==year) & (df['product']=='Kaggle Mug')]['num_sold'], label = 'Kaggle Mug')\n    ax.plot(df[(df['Year']==year) & (df['product']=='Kaggle Hat')]['Month'], df[(df['Year']==year) & (df['product']=='Kaggle Hat')]['num_sold'], label = 'Kaggle Hat')\n    ax.plot(df[(df['Year']==year) & (df['product']=='Kaggle Sticker')]['Month'], df[(df['Year']==year) & (df['product']=='Kaggle Sticker')]['num_sold'], label = 'Kaggle Sticker')\n    ax.title.set_text(f'Avg Monthly Sales Trend in {year}')\n    ax.set_ylabel('Average Sales')\n    ax.set_xlabel('Month')\n    ax.legend()\n    year+=1","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:31:46.307532Z","iopub.execute_input":"2022-01-30T15:31:46.307859Z","iopub.status.idle":"2022-01-30T15:31:47.465616Z","shell.execute_reply.started":"2022-01-30T15:31:46.307817Z","shell.execute_reply":"2022-01-30T15:31:47.465023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Like for the countries and stores, the trend for the products is similar for every year.\n- Hats peak in April and December and has minmum sales in September-October.\n- Mugs peak in December and the sales dips in July-August.\n- Stickers follow almost same sales through out the Year irrespective of country, store etc.","metadata":{}},{"cell_type":"markdown","source":"## Model building","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nimport math","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:31:47.466741Z","iopub.execute_input":"2022-01-30T15:31:47.467157Z","iopub.status.idle":"2022-01-30T15:31:47.471586Z","shell.execute_reply.started":"2022-01-30T15:31:47.467126Z","shell.execute_reply":"2022-01-30T15:31:47.470342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**1. Preparing data for training**","metadata":{}},{"cell_type":"markdown","source":"I'll do a one hot encoding for categorical variables, except for weekdays. For weekdays I will distinguish between days, Fridays and other days of the week I will keep only month and weekday for the time fields, as we saw that the year seems to not influence the sales.","metadata":{}},{"cell_type":"code","source":"train_encoded = pd.concat([train, pd.get_dummies(train[['country', 'store', 'product']])], axis=1).drop(\n    columns=['row_id', 'date', 'country', 'store', 'product', 'country_Finland', 'store_KaggleMart', 'product_Kaggle Sticker'])\nweekday_dict = {\n    'Monday': 1,\n    'Tuesday': 2,\n    'Wednesday': 3,\n    'Thursday': 4,\n    'Friday': 5,\n    'Saturday': 6,\n    'Sunday': 7,\n}\ntrain_encoded['Weekday'] = train_encoded['Weekday'].map(weekday_dict)\ntrain_encoded.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:31:47.473669Z","iopub.execute_input":"2022-01-30T15:31:47.474322Z","iopub.status.idle":"2022-01-30T15:31:47.524763Z","shell.execute_reply.started":"2022-01-30T15:31:47.474214Z","shell.execute_reply":"2022-01-30T15:31:47.523935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`num_sold` column is on another scale in comparison with other variables, but it is the target variable so we don't have to scale it.","metadata":{}},{"cell_type":"markdown","source":"Dividing the predictors from the target column:","metadata":{}},{"cell_type":"code","source":"X = train_encoded.iloc[:, 1:]\ny = train_encoded['num_sold']","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:31:47.526007Z","iopub.execute_input":"2022-01-30T15:31:47.526262Z","iopub.status.idle":"2022-01-30T15:31:47.532215Z","shell.execute_reply.started":"2022-01-30T15:31:47.526232Z","shell.execute_reply":"2022-01-30T15:31:47.531075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"linear_regression = LinearRegression()\nlinear_regression_fit = linear_regression.fit(X, y)","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:31:47.533286Z","iopub.execute_input":"2022-01-30T15:31:47.533887Z","iopub.status.idle":"2022-01-30T15:31:47.552151Z","shell.execute_reply.started":"2022-01-30T15:31:47.533852Z","shell.execute_reply":"2022-01-30T15:31:47.551399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = linear_regression_fit.predict(X)\nprint(f'Linear regression RMSE: {math.sqrt(mean_squared_error(y, y_pred))}')","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:31:47.557797Z","iopub.execute_input":"2022-01-30T15:31:47.558327Z","iopub.status.idle":"2022-01-30T15:31:47.568853Z","shell.execute_reply.started":"2022-01-30T15:31:47.558281Z","shell.execute_reply":"2022-01-30T15:31:47.567953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We are very far from having a good result.\nIn any case, let's see model coefficients:","metadata":{}},{"cell_type":"code","source":"pd.DataFrame(linear_regression_fit.coef_, X.columns, columns=['Coefficients'])","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:31:47.570815Z","iopub.execute_input":"2022-01-30T15:31:47.571504Z","iopub.status.idle":"2022-01-30T15:31:47.585748Z","shell.execute_reply.started":"2022-01-30T15:31:47.57145Z","shell.execute_reply":"2022-01-30T15:31:47.585185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Month has a negative coefficient but we say earlier that december is the best month every year. Maybe it's a good idea to classify eache month by past results\n- Kaggle Hat has a much higher coefficient than Kaggle Mug, and it's a good sign that they are both positive. In fact Kaggle Sticker is the worst product\n- Same insights for countries field coefficients\n- Weekday has a positive coefficient and it's correct, in fact we say tha sales go up from Monday to Friday","metadata":{}},{"cell_type":"markdown","source":"**I think that a linear model is not a good solution for this problem.**","metadata":{}},{"cell_type":"code","source":"month_dict = {\n    1: 3,\n    2: 2,\n    3: 3,\n    4: 3,\n    5: 3,\n    6: 2,\n    7: 1,\n    8: 1,\n    9: 1,\n    10: 1,\n    11: 1,\n    12: 3,\n}\n\nweekend_dict = {\n    1: 0,\n    2: 0,\n    3: 0,\n    4: 0,\n    5: 0,\n    6: 1,\n    7: 1}\ntrain_encoded['IsWeekend'] = train_encoded['Weekday'].map(weekend_dict)\ntrain_encoded['month_class'] = train_encoded['Month'].map(month_dict)\ntrain_encoded.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:31:47.586877Z","iopub.execute_input":"2022-01-30T15:31:47.587363Z","iopub.status.idle":"2022-01-30T15:31:47.623407Z","shell.execute_reply.started":"2022-01-30T15:31:47.58731Z","shell.execute_reply":"2022-01-30T15:31:47.62247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Train correlations:","metadata":{}},{"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (20, 10)\nsns.heatmap(train_encoded.corr(), annot=True, cmap=\"coolwarm\")","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:31:47.625109Z","iopub.execute_input":"2022-01-30T15:31:47.625652Z","iopub.status.idle":"2022-01-30T15:31:48.763679Z","shell.execute_reply.started":"2022-01-30T15:31:47.625602Z","shell.execute_reply":"2022-01-30T15:31:48.76281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Target column deductions:**\n- It seems to be negatively correlated with Month column, but as we saw the best month for sales is december. I think I will only keep month class column in model training\n- Other columns interact with `num_sold` columns like we saw in EDA section: the day of the week influence the number of sales, the hat is the most sold product, Norway is the country with more sales and KaggleRama store sells more than KaggleMart\n\n**Interactions between predictors:**\n- `Weekday` column is highly correlated with `IsWeekend` column. I can think about excluding one of those.","metadata":{}},{"cell_type":"code","source":"X2 = train_encoded.iloc[:,1:]\nX2.drop(columns=['Month', 'Weekday'], inplace=True)\nlinear_regression_fit = linear_regression.fit(X2, y)\ny_pred2 = linear_regression_fit.predict(X2)\nprint(f'Linear regression RMSE: {math.sqrt(mean_squared_error(y, y_pred2))}')","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:31:48.765265Z","iopub.execute_input":"2022-01-30T15:31:48.765503Z","iopub.status.idle":"2022-01-30T15:31:48.788904Z","shell.execute_reply.started":"2022-01-30T15:31:48.765473Z","shell.execute_reply":"2022-01-30T15:31:48.787851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The root mean square error is a little bit lower. It seems that new features worked!\n\nLet's see new coefficients.","metadata":{}},{"cell_type":"code","source":"pd.DataFrame(linear_regression_fit.coef_, X2.columns, columns=['Coefficients'])","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:31:48.790816Z","iopub.execute_input":"2022-01-30T15:31:48.792132Z","iopub.status.idle":"2022-01-30T15:31:48.810738Z","shell.execute_reply.started":"2022-01-30T15:31:48.792073Z","shell.execute_reply":"2022-01-30T15:31:48.809635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's try some ensembles to boost our predictions, let's begin with a Random Forest","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:31:48.812648Z","iopub.execute_input":"2022-01-30T15:31:48.813596Z","iopub.status.idle":"2022-01-30T15:31:48.819774Z","shell.execute_reply.started":"2022-01-30T15:31:48.813539Z","shell.execute_reply":"2022-01-30T15:31:48.818457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(\n     X2, y, test_size=0.25, random_state=123)","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:31:48.822118Z","iopub.execute_input":"2022-01-30T15:31:48.822777Z","iopub.status.idle":"2022-01-30T15:31:48.843803Z","shell.execute_reply.started":"2022-01-30T15:31:48.822721Z","shell.execute_reply":"2022-01-30T15:31:48.842794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"random_forest = RandomForestRegressor()\nrandom_forest_fit = random_forest.fit(X_train, y_train)\ntest_rf_pred = random_forest_fit.predict(X_test)\nprint(f'Random forest RMSE: {math.sqrt(mean_squared_error(y_test, test_rf_pred))}')","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:31:48.845415Z","iopub.execute_input":"2022-01-30T15:31:48.845903Z","iopub.status.idle":"2022-01-30T15:31:49.983298Z","shell.execute_reply.started":"2022-01-30T15:31:48.84586Z","shell.execute_reply":"2022-01-30T15:31:49.982418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Random Forest works much better, but I think tha the hey to improve more is working on feature engineering.","metadata":{}},{"cell_type":"code","source":"importances = random_forest.feature_importances_\nfeature_importance = pd.DataFrame(importances, X2.columns, columns=['Feature importance'])\nfeature_importance.sort_values(by='Feature importance', ascending=False).plot.bar(legend=None, title='Feature importance')","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:31:49.984751Z","iopub.execute_input":"2022-01-30T15:31:49.985231Z","iopub.status.idle":"2022-01-30T15:31:50.307695Z","shell.execute_reply.started":"2022-01-30T15:31:49.98519Z","shell.execute_reply":"2022-01-30T15:31:50.306714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- It seems that features importance values are aligned with what we have seen earlier","metadata":{}},{"cell_type":"code","source":"import xgboost as xgb\n\nxgb_reg = xgb.XGBRegressor(objective ='reg:linear', colsample_bytree = 0.3, learning_rate = 0.1,\n                max_depth = 5, alpha = 10, n_estimators = 10)\nxgb_fit = xgb_reg.fit(X_train, y_train)\ntest_xgb_pred = xgb_fit.predict(X_test)\nprint(f'XGBoost RMSE: {math.sqrt(mean_squared_error(y_test, test_xgb_pred))}')","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:31:50.309347Z","iopub.execute_input":"2022-01-30T15:31:50.309572Z","iopub.status.idle":"2022-01-30T15:31:50.391918Z","shell.execute_reply.started":"2022-01-30T15:31:50.309547Z","shell.execute_reply":"2022-01-30T15:31:50.391265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:31:50.395557Z","iopub.execute_input":"2022-01-30T15:31:50.397381Z","iopub.status.idle":"2022-01-30T15:31:50.413929Z","shell.execute_reply.started":"2022-01-30T15:31:50.397341Z","shell.execute_reply":"2022-01-30T15:31:50.413308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.columns","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:31:50.415166Z","iopub.execute_input":"2022-01-30T15:31:50.415598Z","iopub.status.idle":"2022-01-30T15:31:50.423857Z","shell.execute_reply.started":"2022-01-30T15:31:50.415558Z","shell.execute_reply":"2022-01-30T15:31:50.422707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"avg_sales_2018 = train[train['Year'] == 2018].groupby(['country', 'store', 'product', 'Month', 'Weekday'], as_index=False)['num_sold'].mean()\navg_sales_2018['num_sold_2019'] = avg_sales_2018['num_sold'] + avg_sales_2018['num_sold'] * 0.08 \navg_sales_2018.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:31:50.425245Z","iopub.execute_input":"2022-01-30T15:31:50.425558Z","iopub.status.idle":"2022-01-30T15:31:50.459736Z","shell.execute_reply.started":"2022-01-30T15:31:50.425523Z","shell.execute_reply":"2022-01-30T15:31:50.458694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:32:13.639932Z","iopub.execute_input":"2022-01-30T15:32:13.640244Z","iopub.status.idle":"2022-01-30T15:32:13.65555Z","shell.execute_reply.started":"2022-01-30T15:32:13.640214Z","shell.execute_reply":"2022-01-30T15:32:13.654592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test['Year'].unique() \ntest2 = test.copy()\ntest2['Year'] = pd.to_datetime(test.index).year\ntest2['Month'] = pd.to_datetime(test.index).month\ntest2['Weekday'] = pd.to_datetime(test.index).day_name()\ntest_pred = test2.merge(avg_sales_2018, \n                       how='inner', \n                       left_on=['country', 'store', 'product', 'Month', 'Weekday'],\n                       right_on=['country', 'store', 'product', 'Month', 'Weekday'])\ntest_pred.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:36:13.496491Z","iopub.execute_input":"2022-01-30T15:36:13.497173Z","iopub.status.idle":"2022-01-30T15:36:13.560093Z","shell.execute_reply.started":"2022-01-30T15:36:13.497132Z","shell.execute_reply":"2022-01-30T15:36:13.559162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#submission_df = pd.DataFrame({'row_id': test_pred['row_id'],'num_sold': test_pred['num_sold_2019']})\n#submission_df.to_csv('avg_predictions.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:36:19.583467Z","iopub.execute_input":"2022-01-30T15:36:19.58377Z","iopub.status.idle":"2022-01-30T15:36:19.611914Z","shell.execute_reply.started":"2022-01-30T15:36:19.583739Z","shell.execute_reply":"2022-01-30T15:36:19.610797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since there are many outliers, I could try to use the median for the predictions","metadata":{}},{"cell_type":"code","source":"median_sales_2018 = train[train['Year'] == 2018].groupby(['country', 'store', 'product', 'Month', 'Weekday'], as_index=False)['num_sold'].median()\nmedian_sales_2018['num_sold_2019'] = median_sales_2018['num_sold'] + median_sales_2018['num_sold'] * 0.08\nmedian_pred = test2.merge(median_sales_2018, \n                       how='inner', \n                       left_on=['country', 'store', 'product', 'Month', 'Weekday'],\n                       right_on=['country', 'store', 'product', 'Month', 'Weekday'])\nsubmission_df = pd.DataFrame({'row_id': median_pred['row_id'],'num_sold': median_pred['num_sold_2019']})\nsubmission_df.to_csv('median_predictions.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:43:34.681332Z","iopub.execute_input":"2022-01-30T15:43:34.681592Z","iopub.status.idle":"2022-01-30T15:43:34.748406Z","shell.execute_reply.started":"2022-01-30T15:43:34.681565Z","shell.execute_reply":"2022-01-30T15:43:34.747556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**3. Test preprocessing and predictions**","metadata":{}},{"cell_type":"code","source":"test.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:31:50.502093Z","iopub.status.idle":"2022-01-30T15:31:50.502557Z","shell.execute_reply.started":"2022-01-30T15:31:50.502298Z","shell.execute_reply":"2022-01-30T15:31:50.502325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test['Year'] = test.index.year\ntest['Month'] = test.index.month\ntest['Weekday'] = test.index.day_name()","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:31:50.5043Z","iopub.status.idle":"2022-01-30T15:31:50.504785Z","shell.execute_reply.started":"2022-01-30T15:31:50.504525Z","shell.execute_reply":"2022-01-30T15:31:50.50455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:31:50.506461Z","iopub.status.idle":"2022-01-30T15:31:50.506927Z","shell.execute_reply.started":"2022-01-30T15:31:50.506669Z","shell.execute_reply":"2022-01-30T15:31:50.506695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_encoded = pd.concat([test, pd.get_dummies(test[['country', 'store', 'product']])], axis=1).drop(\n    columns=['row_id', 'date', 'country', 'store', 'product', 'country_Finland', 'store_KaggleMart', 'product_Kaggle Sticker'])\ntest_encoded['Weekday'] = test_encoded['Weekday'].map(weekday_dict)\ntest_encoded.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:31:50.508252Z","iopub.status.idle":"2022-01-30T15:31:50.508719Z","shell.execute_reply.started":"2022-01-30T15:31:50.508467Z","shell.execute_reply":"2022-01-30T15:31:50.508492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_encoded['IsWeekend'] = test_encoded['Weekday'].map(weekend_dict)\ntest_encoded['month_class'] = test_encoded['Month'].map(month_dict)\ntest_encoded.drop(columns=['Month', 'Weekday'], inplace=True)\ntest_encoded.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:31:50.511249Z","iopub.status.idle":"2022-01-30T15:31:50.511764Z","shell.execute_reply.started":"2022-01-30T15:31:50.511486Z","shell.execute_reply":"2022-01-30T15:31:50.511513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Baseline predictions with Random forest regressor:","metadata":{}},{"cell_type":"code","source":"# preds = random_forest_fit.predict(test_encoded)\n# submission_df = pd.DataFrame({'row_id':test['row_id'],'num_sold':preds})\n# submission_df.to_csv('submit_rf.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2022-01-30T15:31:50.513518Z","iopub.status.idle":"2022-01-30T15:31:50.514009Z","shell.execute_reply.started":"2022-01-30T15:31:50.513729Z","shell.execute_reply":"2022-01-30T15:31:50.513754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Credits to other notebooks in the competition**","metadata":{}}]}