{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction \n`V1.0.0`\n### Who am I\nJust a fellow Kaggle learner. I was creating this Notebook as practice and thought it could be useful to some others \n### Who is this for\nThis Notebook is for people that learn from examples. Forget the boring lectures and follow along for some fun/instructive time :)\n### What can I learn here\nYou learn all the basics needed to create a rudimentary XGBoost model with hyperparameter tuning. I go over a multitude of steps with explanations. Hopefully with these building blocks,you can go ahead and build much more complex models.\n\n### Things to remember\n+ Please Upvote/Like the Notebook so other people can learn from it\n+ Feel free to give any recommendations/changes. \n+ I will be continuously updating the notebook. Look forward to many more upcoming changes in the future.\n\n### You can also refer to these notebooks that have helped me as well:\n+ https://www.kaggle.com/cv13j0/tps-jan22-quick-eda-xgboost/notebook","metadata":{}},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\">\n<b>Tip:</b> We will have to run a shell command with the \"!\" mark. We are installing the \"holidays\" library so that we can use it later.\n</div>","metadata":{}},{"cell_type":"code","source":"!pip install holidays","metadata":{"execution":{"iopub.status.busy":"2022-02-10T04:51:19.750444Z","iopub.execute_input":"2022-02-10T04:51:19.750826Z","iopub.status.idle":"2022-02-10T04:51:30.978563Z","shell.execute_reply.started":"2022-02-10T04:51:19.750786Z","shell.execute_reply":"2022-02-10T04:51:30.977264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Computational imports\nimport numpy as np   # Library for n-dimensional arrays\nimport pandas as pd  # Library for dataframes (structured data)\n\n# ML imports\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold, TimeSeriesSplit\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\n\n# Plotting imports\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n\n# I like to disable my Notebook Warnings.\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport holidays\n\n# Set seeds to make the experiment more reproducible.\nfrom numpy.random import seed\nseed(1)\n\n# Allows us to see more information regarding the DataFrame\npd.set_option(\"display.max_rows\", 500)\npd.set_option(\"display.max_columns\", 500)","metadata":{"execution":{"iopub.status.busy":"2022-02-10T04:51:30.980907Z","iopub.execute_input":"2022-02-10T04:51:30.981237Z","iopub.status.idle":"2022-02-10T04:51:32.628775Z","shell.execute_reply.started":"2022-02-10T04:51:30.981204Z","shell.execute_reply":"2022-02-10T04:51:32.627736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Importing Data\n1. Since data is in form of csv file we have to use pandas read_csv to load the data\n2. After loading it is important to check the complete information of data. It is important to get a general feel of the data that we are going to be using.","metadata":{}},{"cell_type":"markdown","source":"Let's get the Path that contains all csv files.","metadata":{}},{"cell_type":"code","source":"PATH_TO_INPUT = '/kaggle/input/tabular-playground-series-jan-2022/'","metadata":{"execution":{"iopub.status.busy":"2022-02-10T04:51:32.631941Z","iopub.execute_input":"2022-02-10T04:51:32.632305Z","iopub.status.idle":"2022-02-10T04:51:32.637537Z","shell.execute_reply.started":"2022-02-10T04:51:32.632255Z","shell.execute_reply":"2022-02-10T04:51:32.636833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Store the data in dataframes using read_csv method.","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv(PATH_TO_INPUT + 'train.csv')\ntest_df = pd.read_csv(PATH_TO_INPUT + 'test.csv')\nsubmission_df = pd.read_csv(PATH_TO_INPUT + 'sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2022-02-10T04:51:32.639668Z","iopub.execute_input":"2022-02-10T04:51:32.640135Z","iopub.status.idle":"2022-02-10T04:51:32.755514Z","shell.execute_reply.started":"2022-02-10T04:51:32.640096Z","shell.execute_reply":"2022-02-10T04:51:32.754314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\">\n<b>Tip:</b> We can use the .head() method to obtain the first 5 rows of the DataFrame.\n</div>","metadata":{}},{"cell_type":"code","source":"train_df.head(5)","metadata":{"execution":{"iopub.status.busy":"2022-02-10T04:51:32.757001Z","iopub.execute_input":"2022-02-10T04:51:32.757358Z","iopub.status.idle":"2022-02-10T04:51:32.782021Z","shell.execute_reply.started":"2022-02-10T04:51:32.757312Z","shell.execute_reply":"2022-02-10T04:51:32.781375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\">\n<b>Tip:</b> We can use the .sample() method to obtain 5 random rows in the DataFrame.\n</div>","metadata":{}},{"cell_type":"code","source":"test_df.sample(5)","metadata":{"execution":{"iopub.status.busy":"2022-02-10T04:51:32.783485Z","iopub.execute_input":"2022-02-10T04:51:32.783916Z","iopub.status.idle":"2022-02-10T04:51:32.801611Z","shell.execute_reply.started":"2022-02-10T04:51:32.783883Z","shell.execute_reply":"2022-02-10T04:51:32.80048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA/Visualizations\nThe goal is to try and gain insights from the data prior to modeling","metadata":{}},{"cell_type":"markdown","source":"## Explorating the Dataframe","metadata":{}},{"cell_type":"markdown","source":"It is useful to use .info() method to quickly have a glance on the general information about the DataFrame. It displays info such as the type of the columnd and also the # of non-null count. In this case there is 26298 entries and for each coloumn we have 26298 non-null count. This means no column has any missing values.","metadata":{}},{"cell_type":"code","source":"train_df.info()","metadata":{"execution":{"iopub.status.busy":"2022-02-10T04:51:32.802844Z","iopub.execute_input":"2022-02-10T04:51:32.803838Z","iopub.status.idle":"2022-02-10T04:51:32.842315Z","shell.execute_reply.started":"2022-02-10T04:51:32.803776Z","shell.execute_reply":"2022-02-10T04:51:32.841413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can also explore unique values for our feature columns using the unique() method.","metadata":{}},{"cell_type":"code","source":"country_list = train_df['country'].unique()\nstore_list = train_df['store'].unique()\nproduct_list = train_df['product'].unique()\n\nprint(f'Country List:{country_list}')\nprint(f'Store List:{store_list}')\nprint(f'Product List:{product_list}')","metadata":{"execution":{"iopub.status.busy":"2022-02-10T04:51:32.844267Z","iopub.execute_input":"2022-02-10T04:51:32.844811Z","iopub.status.idle":"2022-02-10T04:51:32.860243Z","shell.execute_reply.started":"2022-02-10T04:51:32.844763Z","shell.execute_reply":"2022-02-10T04:51:32.859311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The value_counts() method allows us to get unique value counts that exist in a specific column. In this case, we will get the unique values and count of the three feature columns.","metadata":{}},{"cell_type":"code","source":"train_df[\"country\"].value_counts(), train_df[\"store\"].value_counts(), train_df[\"product\"].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-02-10T04:51:34.911148Z","iopub.execute_input":"2022-02-10T04:51:34.91179Z","iopub.status.idle":"2022-02-10T04:51:34.931081Z","shell.execute_reply.started":"2022-02-10T04:51:34.91175Z","shell.execute_reply":"2022-02-10T04:51:34.93037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The describe() method gives a quick summary of the statistical information of the numerical columns. We get descriptions for the mean, standard deviation and max value for example.","metadata":{}},{"cell_type":"code","source":"train_df.describe()","metadata":{"execution":{"iopub.status.busy":"2022-02-10T04:51:36.180595Z","iopub.execute_input":"2022-02-10T04:51:36.180878Z","iopub.status.idle":"2022-02-10T04:51:36.202358Z","shell.execute_reply.started":"2022-02-10T04:51:36.180848Z","shell.execute_reply":"2022-02-10T04:51:36.201465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we are defining a function that returns are categorical, numerical and feature columns. We will be using it consistenly across the notebook.","metadata":{}},{"cell_type":"code","source":"def get_all_cols(df, target, exclude=[]):\n    \n    # Select categorical columns\n    object_cols = [cname for cname in train_df.columns \n                   if train_df[cname].dtype == \"object\"]\n\n    # Select numerical columns\n    num_cols = [cname for cname in train_df.columns \n                if train_df[cname].dtype in ['int64', 'float64', 'uint8']]\n    \n    all_cols = object_cols + num_cols\n    \n    exclude_cols = exclude + [target]\n    \n    feature_cols = [col for col in all_cols if col not in exclude_cols]\n    \n    return object_cols, num_cols, feature_cols","metadata":{"execution":{"iopub.status.busy":"2022-02-10T04:51:37.904868Z","iopub.execute_input":"2022-02-10T04:51:37.905278Z","iopub.status.idle":"2022-02-10T04:51:37.911503Z","shell.execute_reply.started":"2022-02-10T04:51:37.905249Z","shell.execute_reply":"2022-02-10T04:51:37.910725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"object_cols, num_cols, feature_cols = get_all_cols(train_df, 'num_sold', exclude=['row_id', 'date', 'num_sold'])","metadata":{"execution":{"iopub.status.busy":"2022-02-10T04:52:49.455217Z","iopub.execute_input":"2022-02-10T04:52:49.45651Z","iopub.status.idle":"2022-02-10T04:52:49.462496Z","shell.execute_reply.started":"2022-02-10T04:52:49.456447Z","shell.execute_reply":"2022-02-10T04:52:49.46156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-warning\">  \n<b>Note:</b> We exlude row_id, date because we will not be using them spefically as features. We remove\n    num_sold because it is the target.\n</div>","metadata":{}},{"cell_type":"code","source":"object_cols, num_cols, feature_cols","metadata":{"execution":{"iopub.status.busy":"2022-02-10T04:52:50.337991Z","iopub.execute_input":"2022-02-10T04:52:50.338665Z","iopub.status.idle":"2022-02-10T04:52:50.346647Z","shell.execute_reply.started":"2022-02-10T04:52:50.338613Z","shell.execute_reply":"2022-02-10T04:52:50.345639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's define a handy function that quickly gives us the min and max timestep of our dataframe.","metadata":{}},{"cell_type":"code","source":"# Create a simple function to evaluate the time-ranges of the information provided.\n# It will help with the train / validation separations\n\ndef evaluate_time(df):\n    min_date = df['date'].min()\n    max_date = df['date'].max()\n    print(f'Min Date: {min_date} /  Max Date: {max_date}')\n    return None\n\nevaluate_time(train_df)\nevaluate_time(test_df)","metadata":{"execution":{"iopub.status.busy":"2022-02-09T03:45:07.376362Z","iopub.execute_input":"2022-02-09T03:45:07.376752Z","iopub.status.idle":"2022-02-09T03:45:07.400685Z","shell.execute_reply.started":"2022-02-09T03:45:07.376718Z","shell.execute_reply":"2022-02-09T03:45:07.399789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Plotting\nWe will explore various plots that could give us valuable insights.","metadata":{}},{"cell_type":"markdown","source":"### Time-Series Plot\nWe will start by plotting the time-series plot of the number of sell for a specific store. Goal would be to see if there is seasonality in the data (more or less sells during a specific time period) or even a trend (sales inreasing or decreasing over time).","metadata":{}},{"cell_type":"code","source":"km_df = train_df[train_df['store'] == 'KaggleMart']\nkr_df= train_df[train_df['store'] == 'KaggleRama']","metadata":{"execution":{"iopub.status.busy":"2022-02-09T03:45:07.402044Z","iopub.execute_input":"2022-02-09T03:45:07.402308Z","iopub.status.idle":"2022-02-09T03:45:07.42427Z","shell.execute_reply.started":"2022-02-09T03:45:07.402276Z","shell.execute_reply":"2022-02-09T03:45:07.42317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"km_grouped_df = km_df.groupby(['date'])['num_sold'].sum()\nkm_grouped_df.plot(figsize = (10,5))","metadata":{"execution":{"iopub.status.busy":"2022-02-09T03:45:07.426932Z","iopub.execute_input":"2022-02-09T03:45:07.427238Z","iopub.status.idle":"2022-02-09T03:45:07.718235Z","shell.execute_reply.started":"2022-02-09T03:45:07.427186Z","shell.execute_reply":"2022-02-09T03:45:07.717338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kr_grouped_df = kr_df.groupby(['date'])['num_sold'].sum()\nkr_grouped_df.plot(figsize = (10,5))","metadata":{"execution":{"iopub.status.busy":"2022-02-09T03:45:07.719424Z","iopub.execute_input":"2022-02-09T03:45:07.720004Z","iopub.status.idle":"2022-02-09T03:45:08.00555Z","shell.execute_reply.started":"2022-02-09T03:45:07.719961Z","shell.execute_reply":"2022-02-09T03:45:08.004428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We noticed that there is a spike in sales every year around christmas holidays. That is expected since people tend to spend more money around that time of the year.","metadata":{}},{"cell_type":"markdown","source":"## Bar Plot\nHere we will explore the amount of sell per product for a specific store. Goal would be to determine which items has the most sells.","metadata":{}},{"cell_type":"code","source":"km_df = train_df[train_df['store'] == 'KaggleMart']\nkr_df= train_df[train_df['store'] == 'KaggleRama']\n\nkm_grouped_series = km_df.groupby(by = ['product'], as_index=False)['num_sold'].sum()\nkr_grouped_series = kr_df.groupby(by = ['product'], as_index=False)['num_sold'].sum()\n\nkm_grouped_df= km_grouped_series.reset_index()\nkr_grouped_df= kr_grouped_series.reset_index()","metadata":{"execution":{"iopub.status.busy":"2022-02-09T03:45:08.007063Z","iopub.execute_input":"2022-02-09T03:45:08.007342Z","iopub.status.idle":"2022-02-09T03:45:08.039287Z","shell.execute_reply.started":"2022-02-09T03:45:08.00731Z","shell.execute_reply":"2022-02-09T03:45:08.037976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set_style('white') # darkgrid, white grid, dark, white and ticks\ncolors = sns.color_palette('pastel') # Color palette to use\nplt.rc('axes', titlesize=18)     # fontsize of the axes title\nplt.rc('axes', labelsize=14)    # fontsize of the x and y labels\nplt.rc('xtick', labelsize=13)    # fontsize of the tick labels\nplt.rc('ytick', labelsize=13)    # fontsize of the tick labels\nplt.rc('legend', fontsize=13)    # legend fontsize\nplt.rc('font', size=13)          # controls default text sizes\nsns.barplot(data=km_grouped_df, x='product', y= 'num_sold');","metadata":{"execution":{"iopub.status.busy":"2022-02-09T03:45:08.040619Z","iopub.execute_input":"2022-02-09T03:45:08.04121Z","iopub.status.idle":"2022-02-09T03:45:08.250602Z","shell.execute_reply.started":"2022-02-09T03:45:08.041176Z","shell.execute_reply":"2022-02-09T03:45:08.249618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.barplot(data=kr_grouped_df, x='product', y= 'num_sold')","metadata":{"execution":{"iopub.status.busy":"2022-02-09T03:45:08.251836Z","iopub.execute_input":"2022-02-09T03:45:08.252089Z","iopub.status.idle":"2022-02-09T03:45:08.477494Z","shell.execute_reply.started":"2022-02-09T03:45:08.252057Z","shell.execute_reply":"2022-02-09T03:45:08.476555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Categorical Plot\nHere we will explore the amount of sell, mean and distribution per product. We can look at distribution and see if it close to being a Gaussian distribution and also look at if there are any outliers. Goal would be to analyze the statistical distribution of num_sold depending on each product type.","metadata":{}},{"cell_type":"code","source":"# Source vs Price\n\nsns.catplot(y = \"num_sold\", x = \"product\", data = train_df.sort_values(\"num_sold\", ascending = False), kind=\"violin\", height = 4, aspect = 3)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-09T03:45:08.478739Z","iopub.execute_input":"2022-02-09T03:45:08.479001Z","iopub.status.idle":"2022-02-09T03:45:08.957527Z","shell.execute_reply.started":"2022-02-09T03:45:08.47897Z","shell.execute_reply":"2022-02-09T03:45:08.956617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-success\">  \n<b>What we found:</b> From the EDA, we notice that we will have to clean the data before training the model. We notice that there is wrong data types for some columns, missing values and also outliers.\n</div>","metadata":{}},{"cell_type":"markdown","source":"# Feature Engineering\nIn this section, we take the data and preprocess and engineer it so that it is ready to be fed to our model. There are many steps to this.","metadata":{}},{"cell_type":"markdown","source":"## Prepare the Data\nIn this subsection, we look into preparing the feature columns. That can be done by transforming the type of the column to a proper one, creating datetime features from our date column or even adding more valuable feature column (such as holidays) to our dataframe. This is the first step before going to other feature engineering steps.","metadata":{}},{"cell_type":"code","source":"TARGET = 'num_sold'","metadata":{"execution":{"iopub.status.busy":"2022-02-09T03:45:08.958872Z","iopub.execute_input":"2022-02-09T03:45:08.959139Z","iopub.status.idle":"2022-02-09T03:45:08.964369Z","shell.execute_reply.started":"2022-02-09T03:45:08.959106Z","shell.execute_reply":"2022-02-09T03:45:08.963034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we are using the holidays API to get information on the holidays of the three countries.","metadata":{}},{"cell_type":"code","source":"holiday_FI = holidays.CountryHoliday('FI', years=[2015, 2016, 2017, 2018, 2019])\nholiday_NO = holidays.CountryHoliday('NO', years=[2015, 2016, 2017, 2018, 2019])\nholiday_SE = holidays.CountryHoliday('SE', years=[2015, 2016, 2017, 2018, 2019])","metadata":{"execution":{"iopub.status.busy":"2022-02-09T03:45:08.966269Z","iopub.execute_input":"2022-02-09T03:45:08.966614Z","iopub.status.idle":"2022-02-09T03:45:08.988098Z","shell.execute_reply.started":"2022-02-09T03:45:08.966564Z","shell.execute_reply":"2022-02-09T03:45:08.987122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We are creating one dict for all the holidays. Some holidays overlap and we don't want to have repititve entries in our dictionary.","metadata":{}},{"cell_type":"code","source":"holiday_dict = holiday_FI.copy()\nholiday_dict.update(holiday_NO)\nholiday_dict.update(holiday_SE)","metadata":{"execution":{"iopub.status.busy":"2022-02-09T03:45:08.989745Z","iopub.execute_input":"2022-02-09T03:45:08.990111Z","iopub.status.idle":"2022-02-09T03:45:09.007893Z","shell.execute_reply.started":"2022-02-09T03:45:08.990057Z","shell.execute_reply":"2022-02-09T03:45:09.006704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Important step here is to transform our date column to a datetime column. Before this, it was considered as an object column. \n\nWe then use the map() function to map the dict keys to each of our date row values. When the dict key and date row value match, we return the holiday name and it gets stored in our new column named \"holiday_name\"","metadata":{}},{"cell_type":"code","source":"train_df['date'] = pd.to_datetime(train_df['date'])\ntrain_df['holiday_name'] = train_df['date'].map(holiday_dict)\ntrain_df['is_holiday'] = np.where(train_df['holiday_name'].notnull(), 1, 0)\ntrain_df['holiday_name'] = train_df['holiday_name'].fillna('Not Holiday')\n\ntrain_df.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-02-09T03:45:09.00961Z","iopub.execute_input":"2022-02-09T03:45:09.010188Z","iopub.status.idle":"2022-02-09T03:45:09.051011Z","shell.execute_reply.started":"2022-02-09T03:45:09.010137Z","shell.execute_reply":"2022-02-09T03:45:09.050402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-danger\">  \nDon't forget to do the same for our test data!\n</div>","metadata":{}},{"cell_type":"code","source":"test_df['date'] = pd.to_datetime(test_df['date']) # Convert the date to datetime.\ntest_df['holiday_name'] = test_df['date'].map(holiday_dict)\ntest_df['is_holiday'] = np.where(test_df['holiday_name'].notnull(), 1, 0)\ntest_df['holiday_name'] = test_df['holiday_name'].fillna('Not Holiday')\n\ntest_df.sample(10)","metadata":{"execution":{"iopub.status.busy":"2022-02-09T03:45:09.054062Z","iopub.execute_input":"2022-02-09T03:45:09.054744Z","iopub.status.idle":"2022-02-09T03:45:09.081822Z","shell.execute_reply.started":"2022-02-09T03:45:09.054688Z","shell.execute_reply":"2022-02-09T03:45:09.081008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we are defining a function that quickly creates for us time-series features from our date time column. Since we aren't using a RNN Deep Learning model, our model has no knowledge of the sequentiallity of our data points. That is why we have to add many features each representing an information of our date (day, week, month, etc).","metadata":{}},{"cell_type":"code","source":"def create_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Create features base on the date variable, the idea is to extract as much \n    information from the date componets.\n    Args\n        df: Input data to create the features.\n    Returns\n        df: A DataFrame with the new time base features.\n    \"\"\"\n    \n    df['date'] = pd.to_datetime(df['date']) # Convert the date to datetime.\n    \n    # Start the creating future process.\n    df['year'] = df['date'].dt.year\n    df['quarter'] = df['date'].dt.quarter\n    df['month'] = df['date'].dt.month\n    df['day'] = df['date'].dt.day\n    df['dayofweek'] = df['date'].dt.dayofweek\n    df['dayofmonth'] = df['date'].dt.days_in_month\n    df['dayofyear'] = df['date'].dt.dayofyear\n    df['weekofyear'] = df['date'].dt.weekofyear\n    df['weekday'] = df['date'].dt.weekday\n    df['is_weekend'] = np.where((df['weekday'] == 5) | (df['weekday'] == 6), 1, 0)\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2022-02-09T03:45:09.083194Z","iopub.execute_input":"2022-02-09T03:45:09.083994Z","iopub.status.idle":"2022-02-09T03:45:09.095197Z","shell.execute_reply.started":"2022-02-09T03:45:09.083947Z","shell.execute_reply":"2022-02-09T03:45:09.09433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Create the time-series features for both train and test data","metadata":{}},{"cell_type":"code","source":"train_df = create_time_features(train_df)\ntest_df = create_time_features(test_df)\n\ntrain_df.sample(5)","metadata":{"execution":{"iopub.status.busy":"2022-02-09T03:45:09.096639Z","iopub.execute_input":"2022-02-09T03:45:09.097171Z","iopub.status.idle":"2022-02-09T03:45:09.415689Z","shell.execute_reply.started":"2022-02-09T03:45:09.097121Z","shell.execute_reply":"2022-02-09T03:45:09.414859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Handling Missing Values \nIn this subsection, we look if we have any missing data, if so, we take care of it.","metadata":{}},{"cell_type":"code","source":"train_df.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2022-02-09T03:45:09.421362Z","iopub.execute_input":"2022-02-09T03:45:09.421662Z","iopub.status.idle":"2022-02-09T03:45:09.446543Z","shell.execute_reply.started":"2022-02-09T03:45:09.421629Z","shell.execute_reply":"2022-02-09T03:45:09.445571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2022-02-09T03:45:09.449017Z","iopub.execute_input":"2022-02-09T03:45:09.449539Z","iopub.status.idle":"2022-02-09T03:45:09.465749Z","shell.execute_reply.started":"2022-02-09T03:45:09.44949Z","shell.execute_reply":"2022-02-09T03:45:09.464777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lucky for us, there seems to be no missing data for either the training or the test data.","metadata":{}},{"cell_type":"markdown","source":"## Handling imbalanced/Skewed dataset\nIn this subsection, we are going to handle skewed data using log transform. Skewed data can significantly affect the performance of the model. It is especially important to take care of skewed data for models using gradient descent and also models that rely on distances between data points and for example clusters (KNN). A typical regression model using gradient descent performs better with data that resembles more like a gaussian distribution. If the data is not transformed, the model will learn a skewed probability distribution and will not perform well when faced with data closer to one of the extremes of the probability distribution.","metadata":{}},{"cell_type":"markdown","source":"We will use this function to plot the distribution plots.","metadata":{}},{"cell_type":"code","source":"def dist_plots(df):\n    plt.figure(figsize=(10,5))\n    plt.title(\"Distribution Plot\")\n    sns.distplot(df)\n    sns.despine()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-09T03:45:09.466965Z","iopub.execute_input":"2022-02-09T03:45:09.467251Z","iopub.status.idle":"2022-02-09T03:45:09.475856Z","shell.execute_reply.started":"2022-02-09T03:45:09.467219Z","shell.execute_reply":"2022-02-09T03:45:09.474912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_df['num_sold'].skew())\ndist_plots(train_df['num_sold'])","metadata":{"execution":{"iopub.status.busy":"2022-02-09T03:45:09.477216Z","iopub.execute_input":"2022-02-09T03:45:09.477448Z","iopub.status.idle":"2022-02-09T03:45:10.023493Z","shell.execute_reply.started":"2022-02-09T03:45:09.47742Z","shell.execute_reply":"2022-02-09T03:45:10.022854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We notice that the distributiuon is right skewed. Let's try to fix this using log transformation","metadata":{}},{"cell_type":"markdown","source":"Here we are defining a function that takes care of the log transform of a column.","metadata":{}},{"cell_type":"code","source":"def transform_target(df, target):\n    \"\"\"\n    Apply a log transformation to the target for better optimization \n    during training.\n    \"\"\"\n    df[target] = np.log(df[target])\n    return df\n\ntrain_df = transform_target(train_df, TARGET)","metadata":{"execution":{"iopub.status.busy":"2022-02-09T03:45:10.024461Z","iopub.execute_input":"2022-02-09T03:45:10.025148Z","iopub.status.idle":"2022-02-09T03:45:10.03231Z","shell.execute_reply.started":"2022-02-09T03:45:10.025106Z","shell.execute_reply":"2022-02-09T03:45:10.031176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_df['num_sold'].skew())\ndist_plots(train_df['num_sold']);","metadata":{"execution":{"iopub.status.busy":"2022-02-09T03:45:10.033742Z","iopub.execute_input":"2022-02-09T03:45:10.034141Z","iopub.status.idle":"2022-02-09T03:45:10.541985Z","shell.execute_reply.started":"2022-02-09T03:45:10.034095Z","shell.execute_reply":"2022-02-09T03:45:10.541052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The skewness has been reduce by a factor of 10 which is perfect. We must jsut remember to re-transform the output when we predict with the model.","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-warning\">  \n<b>Note:</b> We are not transforming the test data sets since it doesn't contain theoretically the target column.\n</div>","metadata":{}},{"cell_type":"markdown","source":"## Handling Outliers\nIn this subsection, we are going to handle outliers by removing them from our dataset. We have enough samples that removing them shouldn't be an issue. Outliers can significantly affect the performance of the model. It is especially important to take care of outliers in models using gradient descent and also models that rely on distances between data points and clusters (KNN) for example. A typical regression model using gradient descent performs better with data that does not contain many outliers. If the data contains outliers, the model will not perform as well due to factors such as learning a probability distribution that is totally representative of real data (due to the outlier) and also for reasons such as inneficient learning due to adverse effects to the cost functions by the existance of these outliers.\n","metadata":{}},{"cell_type":"markdown","source":"Here we are defining a function that will plot box plots.","metadata":{}},{"cell_type":"code","source":"def box_plots(df):\n    plt.figure(figsize=(10,5))\n    plt.title(\"Box Plot\")\n    sns.boxplot(df)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-09T03:45:10.543316Z","iopub.execute_input":"2022-02-09T03:45:10.543874Z","iopub.status.idle":"2022-02-09T03:45:10.551046Z","shell.execute_reply.started":"2022-02-09T03:45:10.543788Z","shell.execute_reply":"2022-02-09T03:45:10.549883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"box_plots(train_df['num_sold'])","metadata":{"execution":{"iopub.status.busy":"2022-02-09T03:45:10.552493Z","iopub.execute_input":"2022-02-09T03:45:10.553159Z","iopub.status.idle":"2022-02-09T03:45:10.804994Z","shell.execute_reply.started":"2022-02-09T03:45:10.553117Z","shell.execute_reply":"2022-02-09T03:45:10.803974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We notice some outliers outside of our whisker range. Let's remove them by calculating our upper_limit and lower_limit. These are calculated using the interquartile range. ","metadata":{}},{"cell_type":"code","source":"percentile25 = train_df['num_sold'].quantile(0.25)\npercentile75 = train_df['num_sold'].quantile(0.75)\n\niqr = percentile75 - percentile25\n\nupper_limit = percentile75 + 1.5 * iqr\nlower_limit = percentile25 - 1.5 * iqr","metadata":{"execution":{"iopub.status.busy":"2022-02-09T03:45:10.806518Z","iopub.execute_input":"2022-02-09T03:45:10.80688Z","iopub.status.idle":"2022-02-09T03:45:10.817366Z","shell.execute_reply.started":"2022-02-09T03:45:10.80684Z","shell.execute_reply":"2022-02-09T03:45:10.816218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.shape","metadata":{"execution":{"iopub.status.busy":"2022-02-09T03:45:10.819502Z","iopub.execute_input":"2022-02-09T03:45:10.819851Z","iopub.status.idle":"2022-02-09T03:45:10.833649Z","shell.execute_reply.started":"2022-02-09T03:45:10.819788Z","shell.execute_reply":"2022-02-09T03:45:10.832697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = train_df[(train_df['num_sold'] < upper_limit) & (train_df['num_sold'] > lower_limit)]\ntrain_df.shape","metadata":{"execution":{"iopub.status.busy":"2022-02-09T03:45:10.83519Z","iopub.execute_input":"2022-02-09T03:45:10.836173Z","iopub.status.idle":"2022-02-09T03:45:10.855798Z","shell.execute_reply.started":"2022-02-09T03:45:10.83612Z","shell.execute_reply":"2022-02-09T03:45:10.854885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have removed 4 data points.","metadata":{}},{"cell_type":"code","source":"box_plots(train_df['num_sold'])","metadata":{"execution":{"iopub.status.busy":"2022-02-09T03:45:10.857645Z","iopub.execute_input":"2022-02-09T03:45:10.858412Z","iopub.status.idle":"2022-02-09T03:45:11.096337Z","shell.execute_reply.started":"2022-02-09T03:45:10.858358Z","shell.execute_reply":"2022-02-09T03:45:11.09499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After replotting, we have validated that there is no more any outliers :)","metadata":{}},{"cell_type":"markdown","source":"## Handling Categorical Data\nSo that the model can understand categorical data, we must transform them in a numerical form. There is various ways to do that. ","metadata":{}},{"cell_type":"markdown","source":"Some of them categorical data are,\n<div class=\"alert alert-block alert-info\">\n<b>Nominal Data</b> --> data are not in any order --> OneHotEncoder is used in this case\n</div>\n<div class=\"alert alert-block alert-info\">\n<b>Ordinal data </b> --> data are in order --> LabelEncoder is used in this case\n</div>","metadata":{}},{"cell_type":"markdown","source":"Since all our columns are of the nominal type, we are going to use one hot encoding. To do so, we are defining a function that uses get_dummies() method to one hot encode our categorical data.","metadata":{}},{"cell_type":"code","source":"# Convert the Categorical variables to one-hot encoded features...\n# It will help in the training process\ndef create_one_hot(df, categ_colums):\n    \"\"\"\n    Creates one_hot encoded fields for the specified categorical columns...\n    Args\n        df\n        categ_colums\n    Returns\n        df\n    \"\"\"\n    df = pd.get_dummies(df, columns=categ_colums)\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-02-09T03:45:11.097717Z","iopub.execute_input":"2022-02-09T03:45:11.098265Z","iopub.status.idle":"2022-02-09T03:45:11.103677Z","shell.execute_reply.started":"2022-02-09T03:45:11.098226Z","shell.execute_reply":"2022-02-09T03:45:11.102544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's get all numerical and categorical columns","metadata":{}},{"cell_type":"code","source":"def get_all_cols(df, target, exclude=[]):\n    \n    # Select categorical columns\n    object_cols = [cname for cname in train_df.columns \n                   if train_df[cname].dtype == \"object\"]\n\n    # Select numerical columns\n    num_cols = [cname for cname in train_df.columns \n                if train_df[cname].dtype in ['int64', 'float64', 'uint8']]\n    \n    all_cols = object_cols + num_cols\n    \n    exclude_cols = exclude + [target]\n    \n    feature_cols = [col for col in all_cols if col not in exclude_cols]\n    \n    return object_cols, num_cols, feature_cols","metadata":{"execution":{"iopub.status.busy":"2022-02-09T03:45:11.105099Z","iopub.execute_input":"2022-02-09T03:45:11.105351Z","iopub.status.idle":"2022-02-09T03:45:11.122131Z","shell.execute_reply.started":"2022-02-09T03:45:11.10532Z","shell.execute_reply":"2022-02-09T03:45:11.120899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"object_cols, num_cols, feature_cols = get_all_cols(train_df, target=TARGET, exclude=['row_id', 'date', 'num_sold'])","metadata":{"execution":{"iopub.status.busy":"2022-02-09T03:45:11.123745Z","iopub.execute_input":"2022-02-09T03:45:11.124048Z","iopub.status.idle":"2022-02-09T03:45:11.14559Z","shell.execute_reply.started":"2022-02-09T03:45:11.124014Z","shell.execute_reply":"2022-02-09T03:45:11.144561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's one-hot encode by using the function we have defined.","metadata":{}},{"cell_type":"code","source":"train_df = create_one_hot(train_df, object_cols)\ntest_df = create_one_hot(test_df, object_cols)","metadata":{"execution":{"iopub.status.busy":"2022-02-09T03:45:11.147767Z","iopub.execute_input":"2022-02-09T03:45:11.148071Z","iopub.status.idle":"2022-02-09T03:45:11.200569Z","shell.execute_reply.started":"2022-02-09T03:45:11.148038Z","shell.execute_reply":"2022-02-09T03:45:11.198741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Selection\n\nFinding out the best feature which will contribute and have good relation with target variable.\nFollowing are some of the feature selection methods,","metadata":{}},{"cell_type":"markdown","source":"\n<div class=\"alert alert-block alert-info\">\n<b>1. heatmap</b> \n</div>\n<div class=\"alert alert-block alert-info\">\n<b>2. feature_importance_</b> \n</div>\n<div class=\"alert alert-block alert-info\">\n<b>3. SelectKBest</b> \n</div>","metadata":{}},{"cell_type":"markdown","source":"### Correlation \nTo see the correlation between the various features and also with the target value, we will use a heatmap.","metadata":{}},{"cell_type":"code","source":"object_cols, num_cols, feature_cols = get_all_cols(train_df, target=TARGET, exclude=['row_id', 'date'])","metadata":{"execution":{"iopub.status.busy":"2022-02-09T03:45:11.202273Z","iopub.execute_input":"2022-02-09T03:45:11.202859Z","iopub.status.idle":"2022-02-09T03:45:11.21352Z","shell.execute_reply.started":"2022-02-09T03:45:11.202793Z","shell.execute_reply":"2022-02-09T03:45:11.21263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_cols_plus_target = [TARGET] + feature_cols","metadata":{"execution":{"iopub.status.busy":"2022-02-09T03:45:11.216567Z","iopub.execute_input":"2022-02-09T03:45:11.21688Z","iopub.status.idle":"2022-02-09T03:45:11.228999Z","shell.execute_reply.started":"2022-02-09T03:45:11.216847Z","shell.execute_reply":"2022-02-09T03:45:11.228068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-warning\">  \n<b>Note:</b> We are filtering uptil the 11th column since after the 11th column, it is the one hot encoded features. I decided to not plot those since the heatmap will\n    be too dense.\n</div>","metadata":{}},{"cell_type":"code","source":"heatmap_df = train_df[feature_cols_plus_target].iloc[:,:12]","metadata":{"execution":{"iopub.status.busy":"2022-02-09T03:45:11.230534Z","iopub.execute_input":"2022-02-09T03:45:11.231343Z","iopub.status.idle":"2022-02-09T03:45:11.250359Z","shell.execute_reply.started":"2022-02-09T03:45:11.231299Z","shell.execute_reply":"2022-02-09T03:45:11.249391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Finds correlation between Independent and dependent attributes\ntrain_data = pd.read_csv(PATH_TO_INPUT + 'train.csv')\n\nplt.figure(figsize = (18,18))\nsns.heatmap(heatmap_df.corr(), annot = True, cmap = \"RdYlGn\")\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-09T03:45:11.251893Z","iopub.execute_input":"2022-02-09T03:45:11.252151Z","iopub.status.idle":"2022-02-09T03:45:12.322738Z","shell.execute_reply.started":"2022-02-09T03:45:11.252118Z","shell.execute_reply":"2022-02-09T03:45:12.321891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**We notice some features are heavily correlated. We will remove two to reduce the dimensionality of our model:**\n1.  We can remove quarter and keep month since they are heavily correlated.\n2.  We can remove weekofyear and keep month since they are heavily correlated.\n\n**We also notice what has the highest effect on our target variable. We notice:**\n1. The weekend has the highest positive correlation with the target variable which makes sense. People tend to buy more during the weekend (more free time)\n2. The year has lowest correlation with the target variable which also makes sense. The year doesn't really sway someone to buy more or less.","metadata":{}},{"cell_type":"code","source":"object_cols, num_cols, feature_cols = get_all_cols(train_df, target=TARGET, exclude=['row_id', 'date', 'quarter', 'weekofyear'])","metadata":{"execution":{"iopub.status.busy":"2022-02-09T03:45:12.324264Z","iopub.execute_input":"2022-02-09T03:45:12.325094Z","iopub.status.idle":"2022-02-09T03:45:12.33504Z","shell.execute_reply.started":"2022-02-09T03:45:12.325043Z","shell.execute_reply":"2022-02-09T03:45:12.333831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Feature importance (Extra Tree Classifier)\nYou can also use the ExtraTressRegressor from sklearn which will allows you to easily see what are the important features for the Target Price.\n","metadata":{}},{"cell_type":"code","source":"X = train_df[feature_cols]\ny = train_df[TARGET]","metadata":{"execution":{"iopub.status.busy":"2022-02-09T03:45:12.336445Z","iopub.execute_input":"2022-02-09T03:45:12.336689Z","iopub.status.idle":"2022-02-09T03:45:12.352233Z","shell.execute_reply.started":"2022-02-09T03:45:12.336659Z","shell.execute_reply":"2022-02-09T03:45:12.351364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Important feature using ExtraTreesRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor\nselection = ExtraTreesRegressor()\nselection.fit(X, y)","metadata":{"execution":{"iopub.status.busy":"2022-02-09T03:45:12.353478Z","iopub.execute_input":"2022-02-09T03:45:12.353863Z","iopub.status.idle":"2022-02-09T03:45:23.895214Z","shell.execute_reply.started":"2022-02-09T03:45:12.353823Z","shell.execute_reply":"2022-02-09T03:45:23.894214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"You can print it, but it isn't the prettiest.","metadata":{}},{"cell_type":"code","source":"print(selection.feature_importances_)","metadata":{"execution":{"iopub.status.busy":"2022-02-09T03:45:23.897108Z","iopub.execute_input":"2022-02-09T03:45:23.897445Z","iopub.status.idle":"2022-02-09T03:45:23.98351Z","shell.execute_reply.started":"2022-02-09T03:45:23.897402Z","shell.execute_reply":"2022-02-09T03:45:23.98256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (12,8))\nfeat_importances = pd.Series(selection.feature_importances_, index=X.columns)\nfeat_importances.nlargest(20).plot(kind='barh')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-09T03:45:23.985322Z","iopub.execute_input":"2022-02-09T03:45:23.986061Z","iopub.status.idle":"2022-02-09T03:45:24.41223Z","shell.execute_reply.started":"2022-02-09T03:45:23.986014Z","shell.execute_reply":"2022-02-09T03:45:24.411493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the feature importance plot, it is clear that the most important feature here is the product: Kaggle Sticker (Note that we didn't plot the categorical features witht heatmap).\n\nWe also notice that the second most important feature is the store: KaggleRama... Might be a wise decision to buy that store :)","metadata":{}},{"cell_type":"markdown","source":"## Splitting the data\nIn this section, we will split the data in train and test set. Do not confuse test set with our test data. Test set is just a subsample of train_df.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)","metadata":{"execution":{"iopub.status.busy":"2022-02-09T03:45:24.413591Z","iopub.execute_input":"2022-02-09T03:45:24.414296Z","iopub.status.idle":"2022-02-09T03:45:24.433425Z","shell.execute_reply.started":"2022-02-09T03:45:24.414251Z","shell.execute_reply":"2022-02-09T03:45:24.43245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Models\nIn this section, we will explore two models:\n\n1. RandomForestRegressor\n2. XGBRegressor","metadata":{}},{"cell_type":"markdown","source":"## Training\nWe've prepared the food (data), time to... FEED THE MACHINE.","metadata":{}},{"cell_type":"code","source":"from xgboost import XGBRegressor \nfrom sklearn.ensemble import RandomForestRegressor\n\nreg_rf = RandomForestRegressor()\nxgboost_model = XGBRegressor()\n\nhist_reg_rf = reg_rf.fit(X_train, y_train)\nhist_xgboost_model= xgboost_model.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-02-09T03:45:24.434693Z","iopub.execute_input":"2022-02-09T03:45:24.434957Z","iopub.status.idle":"2022-02-09T03:45:37.83884Z","shell.execute_reply.started":"2022-02-09T03:45:24.434927Z","shell.execute_reply":"2022-02-09T03:45:37.837265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This following function is taken from sklearn documentation. It is a concise way of plotting the learning curves.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import learning_curve\n\ndef plot_learning_curve(\n    estimator,\n    title,\n    X,\n    y,\n    axes=None,\n    ylim=None,\n    cv=None,\n    n_jobs=None,\n    train_sizes=np.linspace(0.1, 1.0, 5),\n):\n    \"\"\"\n    Generate 3 plots: the test and training learning curve, the training\n    samples vs fit times curve, the fit times vs score curve.\n\n    Parameters\n    ----------\n    estimator : estimator instance\n        An estimator instance implementing `fit` and `predict` methods which\n        will be cloned for each validation.\n\n    title : str\n        Title for the chart.\n\n    X : array-like of shape (n_samples, n_features)\n        Training vector, where ``n_samples`` is the number of samples and\n        ``n_features`` is the number of features.\n\n    y : array-like of shape (n_samples) or (n_samples, n_features)\n        Target relative to ``X`` for classification or regression;\n        None for unsupervised learning.\n\n    axes : array-like of shape (3,), default=None\n        Axes to use for plotting the curves.\n\n    ylim : tuple of shape (2,), default=None\n        Defines minimum and maximum y-values plotted, e.g. (ymin, ymax).\n\n    cv : int, cross-validation generator or an iterable, default=None\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n\n          - None, to use the default 5-fold cross-validation,\n          - integer, to specify the number of folds.\n          - :term:`CV splitter`,\n          - An iterable yielding (train, test) splits as arrays of indices.\n\n        For integer/None inputs, if ``y`` is binary or multiclass,\n        :class:`StratifiedKFold` used. If the estimator is not a classifier\n        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validators that can be used here.\n\n    n_jobs : int or None, default=None\n        Number of jobs to run in parallel.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    train_sizes : array-like of shape (n_ticks,)\n        Relative or absolute numbers of training examples that will be used to\n        generate the learning curve. If the ``dtype`` is float, it is regarded\n        as a fraction of the maximum size of the training set (that is\n        determined by the selected validation method), i.e. it has to be within\n        (0, 1]. Otherwise it is interpreted as absolute sizes of the training\n        sets. Note that for classification the number of samples usually have\n        to be big enough to contain at least one sample from each class.\n        (default: np.linspace(0.1, 1.0, 5))\n    \"\"\"\n    if axes is None:\n        _, axes = plt.subplots(1, 3, figsize=(20, 5))\n\n    axes[0].set_title(title)\n    if ylim is not None:\n        axes[0].set_ylim(*ylim)\n    axes[0].set_xlabel(\"Training examples\")\n    axes[0].set_ylabel(\"Score\")\n\n    train_sizes, train_scores, test_scores, fit_times, _ = learning_curve(\n        estimator,\n        X,\n        y,\n        cv=cv,\n        n_jobs=n_jobs,\n        train_sizes=train_sizes,\n        return_times=True,\n    )\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    fit_times_mean = np.mean(fit_times, axis=1)\n    fit_times_std = np.std(fit_times, axis=1)\n\n    # Plot learning curve\n    axes[0].grid()\n    axes[0].fill_between(\n        train_sizes,\n        train_scores_mean - train_scores_std,\n        train_scores_mean + train_scores_std,\n        alpha=0.1,\n        color=\"r\",\n    )\n    axes[0].fill_between(\n        train_sizes,\n        test_scores_mean - test_scores_std,\n        test_scores_mean + test_scores_std,\n        alpha=0.1,\n        color=\"g\",\n    )\n    axes[0].plot(\n        train_sizes, train_scores_mean, \"o-\", color=\"r\", label=\"Training score\"\n    )\n    axes[0].plot(\n        train_sizes, test_scores_mean, \"o-\", color=\"g\", label=\"Cross-validation score\"\n    )\n    axes[0].legend(loc=\"best\")\n\n    # Plot n_samples vs fit_times\n    axes[1].grid()\n    axes[1].plot(train_sizes, fit_times_mean, \"o-\")\n    axes[1].fill_between(\n        train_sizes,\n        fit_times_mean - fit_times_std,\n        fit_times_mean + fit_times_std,\n        alpha=0.1,\n    )\n    axes[1].set_xlabel(\"Training examples\")\n    axes[1].set_ylabel(\"fit_times\")\n    axes[1].set_title(\"Scalability of the model\")\n\n    # Plot fit_time vs score\n    fit_time_argsort = fit_times_mean.argsort()\n    fit_time_sorted = fit_times_mean[fit_time_argsort]\n    test_scores_mean_sorted = test_scores_mean[fit_time_argsort]\n    test_scores_std_sorted = test_scores_std[fit_time_argsort]\n    axes[2].grid()\n    axes[2].plot(fit_time_sorted, test_scores_mean_sorted, \"o-\")\n    axes[2].fill_between(\n        fit_time_sorted,\n        test_scores_mean_sorted - test_scores_std_sorted,\n        test_scores_mean_sorted + test_scores_std_sorted,\n        alpha=0.1,\n    )\n    axes[2].set_xlabel(\"fit_times\")\n    axes[2].set_ylabel(\"Score\")\n    axes[2].set_title(\"Performance of the model\")\n\n    return plt","metadata":{"execution":{"iopub.status.busy":"2022-02-09T03:45:37.84084Z","iopub.execute_input":"2022-02-09T03:45:37.841162Z","iopub.status.idle":"2022-02-09T03:45:37.8737Z","shell.execute_reply.started":"2022-02-09T03:45:37.841119Z","shell.execute_reply":"2022-02-09T03:45:37.872838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_learning_curve(\n    xgboost_model,\n    \"XGBoostRegressor Learning Curve\",\n    X_train,\n    y_train,\n    axes=None,\n    ylim=None,\n    cv=5,\n    n_jobs=None,\n    train_sizes=[5000,10000,15000])","metadata":{"execution":{"iopub.status.busy":"2022-02-09T03:45:37.876554Z","iopub.execute_input":"2022-02-09T03:45:37.878235Z","iopub.status.idle":"2022-02-09T03:46:02.436365Z","shell.execute_reply.started":"2022-02-09T03:45:37.87797Z","shell.execute_reply":"2022-02-09T03:46:02.435563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\">\n<b>Plot 1</b> We notice that both our training and validation accuracy are good. Therefore, we are not overfitting to our training data.\n</div>\n<div class=\"alert alert-block alert-info\">\n<b>Plot 2</b> As we increase # training examples, the fit times increases linearly (at 10000 samples, the slope decreases)\n</div>\n<div class=\"alert alert-block alert-info\">\n<b>Plot 3</b> As we fit time increases with the increases of training examples notably, the score increases linearly. \n</div>\n\nConclusion... MORE DATA PLEASE","metadata":{}},{"cell_type":"markdown","source":"## Predicting\nIn this subsection, we will use the basic trained model to predict on our test set (not test data).","metadata":{}},{"cell_type":"code","source":"y_pred_reg_rf = reg_rf.predict(X_test)\ny_pred_xgboost = xgboost_model.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-02-09T03:46:02.43806Z","iopub.execute_input":"2022-02-09T03:46:02.438593Z","iopub.status.idle":"2022-02-09T03:46:02.705481Z","shell.execute_reply.started":"2022-02-09T03:46:02.438546Z","shell.execute_reply":"2022-02-09T03:46:02.704764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(reg_rf.score(X_train, y_train))\nprint(xgboost_model.score(X_train, y_train))","metadata":{"execution":{"iopub.status.busy":"2022-02-09T03:46:02.706963Z","iopub.execute_input":"2022-02-09T03:46:02.707589Z","iopub.status.idle":"2022-02-09T03:46:03.517211Z","shell.execute_reply.started":"2022-02-09T03:46:02.707528Z","shell.execute_reply":"2022-02-09T03:46:03.516357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(reg_rf.score(X_test, y_pred_reg_rf))\nprint(xgboost_model.score(X_test, y_pred_xgboost))","metadata":{"execution":{"iopub.status.busy":"2022-02-09T03:46:03.518645Z","iopub.execute_input":"2022-02-09T03:46:03.519193Z","iopub.status.idle":"2022-02-09T03:46:03.759102Z","shell.execute_reply.started":"2022-02-09T03:46:03.519154Z","shell.execute_reply":"2022-02-09T03:46:03.758366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluating\nIn this subsection, we evaluate using plots and metrics to see if our predictions are good or not.","metadata":{}},{"cell_type":"markdown","source":"### Distribution plots","metadata":{}},{"cell_type":"code","source":"sns.distplot(y_test-y_pred_reg_rf)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-09T03:46:03.76057Z","iopub.execute_input":"2022-02-09T03:46:03.76111Z","iopub.status.idle":"2022-02-09T03:46:04.131278Z","shell.execute_reply.started":"2022-02-09T03:46:03.761068Z","shell.execute_reply":"2022-02-09T03:46:04.129908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.distplot(y_test-y_pred_xgboost)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-09T03:46:04.133006Z","iopub.execute_input":"2022-02-09T03:46:04.133352Z","iopub.status.idle":"2022-02-09T03:46:04.521082Z","shell.execute_reply.started":"2022-02-09T03:46:04.133307Z","shell.execute_reply":"2022-02-09T03:46:04.520341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We notice that the disth plot for the RandomForest seems tigher and a bit less spread. However, notice that the XGBoost graph spreads to only -0.1 to 0.1 while as the RandomForest spreads to -0.2 to 0.2. As well the density at 0 is much higher for the XGBoost than the RandomForest. This means there is more predictions for the XGBoost model that is closer to the expected output.","metadata":{}},{"cell_type":"markdown","source":"### Scatter plot of Target and Predicted","metadata":{}},{"cell_type":"code","source":"plt.scatter(y_test, y_pred_reg_rf, alpha = 0.5)\nplt.xlabel(\"y_test\")\nplt.ylabel(\"y_pred\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-09T03:46:04.522348Z","iopub.execute_input":"2022-02-09T03:46:04.522844Z","iopub.status.idle":"2022-02-09T03:46:04.807735Z","shell.execute_reply.started":"2022-02-09T03:46:04.522796Z","shell.execute_reply":"2022-02-09T03:46:04.806873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.scatter(y_test, y_pred_xgboost, alpha = 0.5)\nplt.xlabel(\"y_test\")\nplt.ylabel(\"y_pred\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-09T03:46:04.809791Z","iopub.execute_input":"2022-02-09T03:46:04.810682Z","iopub.status.idle":"2022-02-09T03:46:05.088007Z","shell.execute_reply.started":"2022-02-09T03:46:04.810616Z","shell.execute_reply":"2022-02-09T03:46:05.08666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We notice that the scatter plot for the XGBoost model is much tighter. The tighter and the more linear this graph is, the more the predicted and expected values are similar. In a perfect model, we would expect this slope to be 1.","metadata":{}},{"cell_type":"markdown","source":"### Metrics to decide on which model to use\nI have chosen a handful of metrics, however the main one used of regression is usually R2 score.","metadata":{}},{"cell_type":"code","source":"from sklearn import metrics","metadata":{"execution":{"iopub.status.busy":"2022-02-09T03:46:05.089899Z","iopub.execute_input":"2022-02-09T03:46:05.090226Z","iopub.status.idle":"2022-02-09T03:46:05.095685Z","shell.execute_reply.started":"2022-02-09T03:46:05.090194Z","shell.execute_reply":"2022-02-09T03:46:05.09444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('RandomForest')\nprint('MAE:', metrics.mean_absolute_error(y_test, y_pred_reg_rf))\nprint('MSE:', metrics.mean_squared_error(y_test, y_pred_reg_rf))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_pred_reg_rf)))\n\nprint('XGBoost')\nprint('MAE:', metrics.mean_absolute_error(y_test, y_pred_xgboost))\nprint('MSE:', metrics.mean_squared_error(y_test, y_pred_xgboost))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_pred_xgboost)))","metadata":{"execution":{"iopub.status.busy":"2022-02-09T03:46:05.097072Z","iopub.execute_input":"2022-02-09T03:46:05.097332Z","iopub.status.idle":"2022-02-09T03:46:05.123847Z","shell.execute_reply.started":"2022-02-09T03:46:05.097303Z","shell.execute_reply":"2022-02-09T03:46:05.122583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metrics.r2_score(y_test, y_pred_reg_rf)","metadata":{"execution":{"iopub.status.busy":"2022-02-09T03:46:05.12569Z","iopub.execute_input":"2022-02-09T03:46:05.125982Z","iopub.status.idle":"2022-02-09T03:46:05.137722Z","shell.execute_reply.started":"2022-02-09T03:46:05.125952Z","shell.execute_reply":"2022-02-09T03:46:05.136929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metrics.r2_score(y_test, y_pred_xgboost)","metadata":{"execution":{"iopub.status.busy":"2022-02-09T03:46:05.138771Z","iopub.execute_input":"2022-02-09T03:46:05.139644Z","iopub.status.idle":"2022-02-09T03:46:05.162893Z","shell.execute_reply.started":"2022-02-09T03:46:05.139603Z","shell.execute_reply":"2022-02-09T03:46:05.1611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"XGBoost outperformed the RandomForest on all three metrics. I decided to proceed with the XGBoost model for the hyperparameter tuning.","metadata":{}},{"cell_type":"markdown","source":"# Hyperparameter Tuning\n\n\n* Choose following method for hyperparameter tuning\n    1. **RandomizedSearchCV** --> Fast\n    2. **GridSearchCV**\n* Assign hyperparameters in form of dictionary\n* Fit the model\n* Check best paramters and best score","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV","metadata":{"execution":{"iopub.status.busy":"2022-02-09T03:46:05.164216Z","iopub.execute_input":"2022-02-09T03:46:05.164597Z","iopub.status.idle":"2022-02-09T03:46:05.176342Z","shell.execute_reply.started":"2022-02-09T03:46:05.164554Z","shell.execute_reply":"2022-02-09T03:46:05.174987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create the random grid for the XGBoost model\n\nparams = {\n \"learning_rate\" : [0.05,0.10,0.15,0.20,0.25,0.30],\n \"max_depth\" : [ 3, 4, 5, 6, 8, 10, 12, 15],\n \"min_child_weight\" : [ 1, 3, 5, 7 ],\n \"gamma\": [ 0.0, 0.1, 0.2 , 0.3, 0.4 ],\n \"colsample_bytree\" : [ 0.3, 0.4, 0.5 , 0.7 ]\n}","metadata":{"execution":{"iopub.status.busy":"2022-02-09T03:46:05.177976Z","iopub.execute_input":"2022-02-09T03:46:05.178825Z","iopub.status.idle":"2022-02-09T03:46:05.193027Z","shell.execute_reply.started":"2022-02-09T03:46:05.178757Z","shell.execute_reply":"2022-02-09T03:46:05.19197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Search for best hyperparameters","metadata":{}},{"cell_type":"code","source":"# Random search of parameters, using 5 fold cross validation, \n# search across 100 different combinations\nxgb_model_tuned = RandomizedSearchCV(estimator = xgboost_model, param_distributions = params, scoring='neg_mean_squared_error', n_iter = 50, cv = 5, verbose=2, random_state=42, n_jobs = 1)","metadata":{"execution":{"iopub.status.busy":"2022-02-09T04:12:50.216448Z","iopub.execute_input":"2022-02-09T04:12:50.217063Z","iopub.status.idle":"2022-02-09T04:12:50.224697Z","shell.execute_reply.started":"2022-02-09T04:12:50.217024Z","shell.execute_reply":"2022-02-09T04:12:50.223679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_model_tuned.fit(X_train,y_train)","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can check the best parameters by accessing the following attribute:","metadata":{}},{"cell_type":"code","source":"xgb_model_tuned.best_params_\n\n# {'min_child_weight': 5,\n#  'max_depth': 8,\n#  'learning_rate': 0.25,\n#  'gamma': 0.0,\n#  'colsample_bytree': 0.5}","metadata":{"execution":{"iopub.status.busy":"2022-02-09T04:26:55.173157Z","iopub.execute_input":"2022-02-09T04:26:55.17345Z","iopub.status.idle":"2022-02-09T04:26:55.181085Z","shell.execute_reply.started":"2022-02-09T04:26:55.173421Z","shell.execute_reply":"2022-02-09T04:26:55.180364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Predicting with tuned model\nLet us used our tuned model to predict the Target price and see if it does better than our untuned model.","metadata":{}},{"cell_type":"code","source":"prediction = xgb_model_tuned.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-02-09T04:27:50.848831Z","iopub.execute_input":"2022-02-09T04:27:50.849226Z","iopub.status.idle":"2022-02-09T04:27:50.879322Z","shell.execute_reply.started":"2022-02-09T04:27:50.849183Z","shell.execute_reply":"2022-02-09T04:27:50.878476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluating tuned model","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (8,8))\nsns.distplot(y_test-prediction)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-09T04:27:51.538701Z","iopub.execute_input":"2022-02-09T04:27:51.539092Z","iopub.status.idle":"2022-02-09T04:27:51.938219Z","shell.execute_reply.started":"2022-02-09T04:27:51.539049Z","shell.execute_reply":"2022-02-09T04:27:51.935866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Already the dist plot looks MUCH better","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (8,8))\nplt.scatter(y_test, prediction, alpha = 0.5)\nplt.xlabel(\"y_test\")\nplt.ylabel(\"y_pred\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-09T04:27:52.540658Z","iopub.execute_input":"2022-02-09T04:27:52.540974Z","iopub.status.idle":"2022-02-09T04:27:52.858579Z","shell.execute_reply.started":"2022-02-09T04:27:52.540936Z","shell.execute_reply":"2022-02-09T04:27:52.857275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Even the scatter plot looks much tighter.","metadata":{}},{"cell_type":"code","source":"# MAE: 0.04518231450992608\n# MSE: 0.003278265867648754\n# RMSE: 0.05725614261936228\n\nprint('MAE:', metrics.mean_absolute_error(y_test, prediction))\nprint('MSE:', metrics.mean_squared_error(y_test, prediction))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, prediction)))","metadata":{"execution":{"iopub.status.busy":"2022-02-09T04:27:54.644057Z","iopub.execute_input":"2022-02-09T04:27:54.64443Z","iopub.status.idle":"2022-02-09T04:27:54.656986Z","shell.execute_reply.started":"2022-02-09T04:27:54.644391Z","shell.execute_reply":"2022-02-09T04:27:54.655543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metrics.r2_score(y_test, prediction)","metadata":{"execution":{"iopub.status.busy":"2022-02-09T04:28:12.292229Z","iopub.execute_input":"2022-02-09T04:28:12.29259Z","iopub.status.idle":"2022-02-09T04:28:12.30193Z","shell.execute_reply.started":"2022-02-09T04:28:12.29255Z","shell.execute_reply":"2022-02-09T04:28:12.300909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally, the MAE and the MSE is lower, but the RMSE is slightly higher.\n\nBy analzing these graphs and metrics, we can actually say that the tuning optimized the model for the better.","metadata":{}},{"cell_type":"markdown","source":"# Save the model to reuse it again","metadata":{}},{"cell_type":"markdown","source":"There's various ways to save the model. We decided to go forward with pickling. It is very easy and straighforward. ","metadata":{}},{"cell_type":"code","source":"import pickle\n# open a file, where you ant to store the data\nfile = open('xgboost_tuned.pkl', 'wb') # wb is write and binary mode\n\n# dump information to that file\npickle.dump(xgb_model_tuned, file)","metadata":{"execution":{"iopub.status.busy":"2022-02-09T04:29:18.687795Z","iopub.execute_input":"2022-02-09T04:29:18.689094Z","iopub.status.idle":"2022-02-09T04:29:18.775156Z","shell.execute_reply.started":"2022-02-09T04:29:18.689027Z","shell.execute_reply":"2022-02-09T04:29:18.774388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = open('xgboost_tuned.pkl','rb')\nxgboost = pickle.load(model)","metadata":{"execution":{"iopub.status.busy":"2022-02-09T04:29:47.902904Z","iopub.execute_input":"2022-02-09T04:29:47.903183Z","iopub.status.idle":"2022-02-09T04:29:47.990327Z","shell.execute_reply.started":"2022-02-09T04:29:47.903155Z","shell.execute_reply":"2022-02-09T04:29:47.989126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_prediction = xgboost.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-02-09T04:29:48.530053Z","iopub.execute_input":"2022-02-09T04:29:48.530377Z","iopub.status.idle":"2022-02-09T04:29:48.561987Z","shell.execute_reply.started":"2022-02-09T04:29:48.530341Z","shell.execute_reply":"2022-02-09T04:29:48.561151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metrics.r2_score(y_test, y_prediction)","metadata":{"execution":{"iopub.status.busy":"2022-02-09T04:29:49.327656Z","iopub.execute_input":"2022-02-09T04:29:49.328043Z","iopub.status.idle":"2022-02-09T04:29:49.338391Z","shell.execute_reply.started":"2022-02-09T04:29:49.328007Z","shell.execute_reply":"2022-02-09T04:29:49.337198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you can see, it is extremely easy to save and load a trained model and to use it for future predictions. No need to traine everytime!","metadata":{}},{"cell_type":"markdown","source":"# Final Remarks\nThank you for going through this notebook. Please feel free to show support and comment on the notebooks with advice or improvements. If you found it useful, please let me know as well :)","metadata":{}}]}