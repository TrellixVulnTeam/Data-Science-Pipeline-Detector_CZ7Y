{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Problem Statement\n\nIn this competition were given a dataset for two fictitous stores and asked to build a forecast predicting which will have better sales going forward. ","metadata":{}},{"cell_type":"markdown","source":"## Loading Libraries and Data","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\nimport seaborn as sns\nimport xgboost as xgb\nfrom sklearn.model_selection import  GridSearchCV, train_test_split\nimport category_encoders as ce\nfrom sklearn.metrics import mean_absolute_error, make_scorer","metadata":{"execution":{"iopub.status.busy":"2022-01-23T18:23:18.633181Z","iopub.execute_input":"2022-01-23T18:23:18.633772Z","iopub.status.idle":"2022-01-23T18:23:20.753591Z","shell.execute_reply.started":"2022-01-23T18:23:18.633645Z","shell.execute_reply":"2022-01-23T18:23:20.752857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"First we'll load our train dataset and define a feature engineering function that we can reuse on our test set. In order to capture the importance of different time series feature's we'll add month, year, day, weekday, and quarter as features.","metadata":{}},{"cell_type":"code","source":"input_df = pd.read_csv(\"../input/tabular-playground-series-jan-2022/train.csv\")\n\ndef initial_engineering(data):\n    \n    '''Defining cleaning function to be used for both the train and test sets'''\n    \n    data['month'] = pd.DatetimeIndex(data['date']).month.astype(object)\n    data['year'] = pd.DatetimeIndex(data['date']).year\n    data['day'] = pd.DatetimeIndex(data['date']).day\n    \n    #Converting existing column types\n    data['country'] = data['country'].astype(object)\n    data['store'] = data['store'].astype(object)\n    data['product'] = data['product'].astype(object)\n    \n    #Adding additional date variables\n    data['date'] = pd.DatetimeIndex(data['date'])\n    data['weekday'] = data['date'].dt.dayofweek.astype(object)\n    data['quarter'] = data['date'].dt.quarter.astype(object)\n    data['month_start'] = data['date'].dt.is_month_start.astype(object)\n    data['month_end'] = data['date'].dt.is_month_end.astype(object)\n    data['quarter_start'] = data['date'].dt.is_quarter_start.astype(object)\n    data['quarter_end'] = data['date'].dt.is_quarter_end.astype(object)\n    return data\n\ndata = initial_engineering(input_df)\n\ndef add_previous_sales(data):\n    \n    '''Adds the previous days sales as a predictor'''\n    \n    first_date = (data.sort_values(by=\"date\"))['date'][0]\n    data_points = len(data[data['date'] == first_date]) #Returning total number of datapoints\n    #print(data_points)\n    \n    data = data.sort_values(by = ['date', 'product_1','product_2', 'product_3', \n                                  'country_1','country_2', 'store_1', 'store_2'])\n    #print(data.drop(data.tail(data_points).index)['num_sold'])\n    data_copy = data.drop(data.tail(data_points).index)['num_sold'].tolist()\n    placeholder = [data['num_sold'].mean() for i in range(data_points)]\n    data_copy = placeholder + data_copy\n    data['previous_sales'] = data_copy\n    return data\n\n","metadata":{"execution":{"iopub.status.busy":"2022-01-23T18:23:20.755347Z","iopub.execute_input":"2022-01-23T18:23:20.755572Z","iopub.status.idle":"2022-01-23T18:23:20.910554Z","shell.execute_reply.started":"2022-01-23T18:23:20.755545Z","shell.execute_reply":"2022-01-23T18:23:20.909939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Visualization\n\nBefore we perform any modeling we want to explore any relationships between our predictor and dependent variables.","metadata":{}},{"cell_type":"code","source":"#Data visualization\ndata.plot(x='date',y='num_sold',figsize=(15,6),linestyle='--', markerfacecolor='r',color='b',markersize=10)\nplt.xlabel('Year-Month')\nplt.ylabel('Number Sold')\n","metadata":{"execution":{"iopub.status.busy":"2022-01-23T18:23:20.91157Z","iopub.execute_input":"2022-01-23T18:23:20.91189Z","iopub.status.idle":"2022-01-23T18:23:21.780427Z","shell.execute_reply.started":"2022-01-23T18:23:20.911864Z","shell.execute_reply":"2022-01-23T18:23:21.779888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Based on the above time series graph we can see a clear seasonal trend with sales peaking in December and January of each year. \n\nNext we'll break our graph down by country, product, and store to determine if there are any differences based on these variables.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10,5))\nchart = sns.relplot(x=\"date\", y=\"num_sold\", hue=\"product\",\n            col=\"country\", row=\"store\", height=3,\n            kind=\"line\", estimator=None, data=data)\nchart.set_xticklabels(rotation=45, horizontalalignment='right')","metadata":{"execution":{"iopub.status.busy":"2022-01-23T18:23:21.782083Z","iopub.execute_input":"2022-01-23T18:23:21.782428Z","iopub.status.idle":"2022-01-23T18:23:24.264943Z","shell.execute_reply.started":"2022-01-23T18:23:21.782387Z","shell.execute_reply":"2022-01-23T18:23:24.263968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Breaking our data down  we can see that all products follow a similar seasonal trend regardless of product type, country, or store. \n\nHowever, while they each exhibit the peaks at the same time there are several differences worth noting:\n* Product sales are impacted by store; this can be seen clearly comparing hat and mug sales in Finland for KaggleRama versus Kagglemart\n* Sales differ across country; can be seen comparing kaggleRama sales for Finland, Norway, and Sweeden\n","metadata":{}},{"cell_type":"code","source":"#Autocorrelation plots\nsm.graphics.tsa.plot_acf(data.num_sold)\nsm.graphics.tsa.plot_pacf(data.num_sold)\n","metadata":{"execution":{"iopub.status.busy":"2022-01-23T18:23:24.266414Z","iopub.execute_input":"2022-01-23T18:23:24.266683Z","iopub.status.idle":"2022-01-23T18:23:25.079464Z","shell.execute_reply.started":"2022-01-23T18:23:24.266652Z","shell.execute_reply":"2022-01-23T18:23:25.078491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Given that we'll be using XGBoost as our model of choice we must first encode our categorical variables to numeric. We'll use OneHotEncoder for this so that our algorithm doesnt mistakenly assume a numeric relationship between different levels of our categorical variables.","metadata":{}},{"cell_type":"code","source":"#Performing initial engineering and adding previous sales as variable  \ndata = initial_engineering(data)\n\n#Encoding categorical variables\nencoder = ce.OneHotEncoder(cols=['weekday', 'quarter', 'product', 'store', 'country', 'month', 'day',\n                                 'month_start', 'month_end', 'quarter_start', 'quarter_end'])\nencoder.fit(data)\ndata = encoder.transform(data)\n#Adding previous days sales as a predictor\ndata = add_previous_sales(data)\n","metadata":{"execution":{"iopub.status.busy":"2022-01-23T18:23:25.080643Z","iopub.execute_input":"2022-01-23T18:23:25.080871Z","iopub.status.idle":"2022-01-23T18:23:25.682299Z","shell.execute_reply.started":"2022-01-23T18:23:25.080844Z","shell.execute_reply":"2022-01-23T18:23:25.681602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Modeling","metadata":{}},{"cell_type":"code","source":"#Modeling\ndata = data.drop([ 'row_id'], axis=1)\nx_train, x_test, y_train, y_test = train_test_split(\n     data.drop(['num_sold'], axis=1), data['num_sold'], test_size=0.2, random_state=0)\n\nx_train = x_train.drop(['date'], axis=1)\nx_test = x_test.drop(['date'], axis=1)\n\n#Defining and fitting initial model\nxgb_model = xgb.XGBRegressor()\nxgb_model.fit(x_train, y_train)\n\n#Defining parameters and performing hyperparameter tuning\nparams = { \n    \"colsample_bytree\": [0.4, 0.5, 0.6],\n    \"gamma\": [0.25],\n    \"learning_rate\": [0.1],\n    \"max_depth\": [3,5,7], \n    \"n_estimators\": [100, 150], \n    \"subsample\": [ 0.3, 0.4, 0.5]\n}\n\ngs = GridSearchCV(\n        estimator=xgb_model,\n        param_grid=params, \n        cv=3, \n        n_jobs=12, \n        scoring=make_scorer(mean_absolute_error),\n        verbose=1\n    )\nfitted_model = gs.fit(x_train, y_train)\n\n\n#Evaluating test accuracy\nprediction = xgb_model.predict(x_test)\nMAE = sum(abs(prediction - y_test))/len(prediction)\nprint(MAE)\n","metadata":{"execution":{"iopub.status.busy":"2022-01-23T18:23:25.683409Z","iopub.execute_input":"2022-01-23T18:23:25.683643Z","iopub.status.idle":"2022-01-23T19:23:58.343369Z","shell.execute_reply.started":"2022-01-23T18:23:25.683606Z","shell.execute_reply":"2022-01-23T19:23:58.341441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Viewing results","metadata":{}},{"cell_type":"code","source":"#Visualizing gridsearch results\ngrid_scores = pd.DataFrame(gs.cv_results_)\n\nchart = sns.relplot(x=\"param_colsample_bytree\", y=\"mean_test_score\",hue = \"param_subsample\",\n                    col=\"param_max_depth\", row = \"param_n_estimators\",height=3,\n                    kind=\"line\", estimator=None, data=grid_scores)\n\nxgb.plot_importance(xgb_model)","metadata":{"execution":{"iopub.status.busy":"2022-01-23T19:23:58.348163Z","iopub.execute_input":"2022-01-23T19:23:58.348774Z","iopub.status.idle":"2022-01-23T19:24:01.868642Z","shell.execute_reply.started":"2022-01-23T19:23:58.34871Z","shell.execute_reply":"2022-01-23T19:24:01.867719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Making Predictions","metadata":{}},{"cell_type":"code","source":"def make_prediction(test, model, data):\n    \n    '''Adds previous days sales and makes predictions'''\n     \n    test = test.sort_values(by = ['date', 'product_1','product_2', 'product_3', \n                                  'country_1','country_2', 'store_1', 'store_2'])\n    dates = (test.sort_values(by=\"date\"))['date'].unique()\n    first_date = dates[0]\n    test_id = test['row_id']\n    test = test.drop(['row_id'], axis = 1)\n    \n    predictions = []\n    previous_sales = []\n    for i in dates:\n        prediction_set = test[test['date'] == i]\n        \n        \n        #If first date in predictions set use last date in train set for previous sales\n        if i == first_date:\n            print(data.tail(len(data[data['date'] == i]))['num_sold'])\n            prediction_set['previous_sales'] = data.tail(len(data[data['date'] == i]))['num_sold']\n            days_prediction = model.predict(prediction_set.drop(['date'], axis=1))\n            predictions = days_prediction\n            previous_sales = days_prediction\n            \n        #else use last prediction as previous sales\n        else:\n            prediction_set['previous_sales'] = previous_sales\n            days_prediction = list(model.predict(prediction_set.drop(['date'], axis=1)))\n            predictions = np.concatenate((predictions, days_prediction), axis = 0)\n            previous_sales = days_prediction\n    print(predictions)\n    print(type(predictions))       \n    frame = { 'row_id': test_id, 'num_sold': predictions }\n    out_df = pd.DataFrame(frame)\n    \n    return out_df\n\n#Loading testdata and performing initial feature engineering\ntest = pd.read_csv(\"../input/tabular-playground-series-jan-2022/test.csv\")\ntest = initial_engineering(test)\n\n#Encoding categorical variables\nencoder = ce.OneHotEncoder(cols=['weekday', 'quarter', 'product', 'store', 'country', 'month', 'day',\n                                 'month_start', 'month_end', 'quarter_start', 'quarter_end'])\nencoder.fit(test)\ntest = encoder.transform(test)\n\n\n#Making prediction and outputting excel file\nout_df= make_prediction(test, xgb_model, data)\nout_df.to_csv('python_out_3.csv', index=False,)","metadata":{"execution":{"iopub.status.busy":"2022-01-23T19:24:01.869935Z","iopub.execute_input":"2022-01-23T19:24:01.870162Z","iopub.status.idle":"2022-01-23T19:24:05.428838Z","shell.execute_reply.started":"2022-01-23T19:24:01.870135Z","shell.execute_reply":"2022-01-23T19:24:05.428105Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]}]}