{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Welcome and have fun learning\n\n#### Linear regression excels at extrapolating trends, but can't learn interactions. XGBoost excels at learning interactions, but can't extrapolate trends. In this lesson, we'll learn how to create \"hybrid\" forecasters that combine complementary learning algorithms and let the strengths of one make up for the weakness of the other. \n\nObjective of this notebook used to be a ~simple~ and robust time series regression for future use.\n\n<blockquote style=\"margin-right:auto; margin-left:auto; padding: 1em; margin:24px;\">\n    <strong>Fork This Notebook!</strong><br>\nCreate your own editable copy of this notebook by clicking on the <strong>Copy and Edit</strong> button in the top right corner.\n</blockquote>\n\n**Notes:**\n\n## Imports and Configuration ##","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nfrom scipy import stats\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import MaxNLocator, FormatStrFormatter, PercentFormatter\nimport seaborn as sns\n\n\nimport ipywidgets as widgets\nfrom learntools.time_series.style import *  # plot style settings\nfrom learntools.time_series.utils import (create_multistep_example,\n                                          load_multistep_data,\n                                          make_lags,\n                                          make_multistep_target,\n                                          plot_multistep)\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom statsmodels.tsa.deterministic import CalendarFourier, DeterministicProcess\nfrom catboost import CatBoostRegressor\nfrom xgboost import XGBRegressor\n\nimport dateutil.easter as easter\n\n# Set Matplotlib defaults\nplt.style.use(\"seaborn-whitegrid\")\nplt.rc(\"figure\", autolayout=True, figsize=(12, 8))\nplt.rc(\n    \"axes\",\n    labelweight=\"bold\",\n    labelsize=\"large\",\n    titleweight=\"bold\",\n    titlesize=16,\n    titlepad=10,\n)\nplot_params = dict(\n    color=\"0.75\",\n    style=\".-\",\n    markeredgecolor=\"0.25\",\n    markerfacecolor=\"0.25\",\n    legend=False,\n)\n%config InlineBackend.figure_format = 'retina'\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport gc\nimport os\nimport math\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-09T09:33:07.75739Z","iopub.execute_input":"2022-01-09T09:33:07.758104Z","iopub.status.idle":"2022-01-09T09:33:09.442253Z","shell.execute_reply.started":"2022-01-09T09:33:07.757964Z","shell.execute_reply":"2022-01-09T09:33:09.441299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Fine tuning","metadata":{}},{"cell_type":"code","source":"# -----------------------------------------------------------------\n# Some parameters to config \nPRODUCTION = False # True: For submission run. False: Fast trial run\n\n# Hyperparameters\nFOLDS = 20 if PRODUCTION else 5   # Only 5 or 10.\nEPOCHS = 68        # Does not matter with Early stopping. Deep network should not take too much epochs to learn\nBATCH_SIZE = 2048   # large enough to fit RAM. If unstable, tuned downward. 4096 2048\nACTIVATION = 'swish' # swish mish relu selu ;swish overfit more cause of narrow global minimun\nKERNEL_INIT = \"glorot_normal\" # Minimal impact, but give your init the right foot forward glorot_uniform lecun_normal\nLEARNING_RATE = 0.000965713 # Not used. Optimal lr is about half the maximum lr \nLR_FACTOR = 0.5   # LEARNING_RATE * LR_FACTOR = New Learning rate on ReduceLROnPlateau. lower down when the LR oscillate\nMIN_DELTA = 0.0000001 # Default 0.0001 0.0000001\nRLRP_PATIENCE = 5 # Learning Rate reduction on ReduceLROnPlateau\nES_PATIENCE = 16  # Early stopping\nDROPOUT = 0.05     # Act like L1 L2 regulator. lower your learning rate in order to overcome the \"boost\" that the dropout probability gives to the learning rate.\nHIDDEN_LAYERS = [320, 288, 64, 32]\n\nOPTIMIZER = 'adam' # adam adamax nadam\nLOSS ='sparse_categorical_crossentropy' # sparse_categorical_crossentropy does not require onehot encoding on labels. categorical_crossentropy\nMETRICS ='accuracy'  # acc accuracy categorical_accuracy sparse_categorical_accuracy\nACC_VAL_METRICS = 'val_accuracy' # 'val_acc' val_accuracy val_sparse_categorical_accuracy\nACC_METRICS = 'accuracy' # acc accuracy 'sparse_categorical_accuracy'\n\n# The dataset is too huge for trial. Sampling it for speed run!\nSAMPLE = 2262087 if PRODUCTION else 11426   # True for FULL run. Max Sample size per category. For quick test: y counts [1468136, 2262087, 195712, 377, 1, 11426, 62261]  # 4000000 total rows\nVALIDATION_SPLIT = 0.15 # Only used to min dataset for quick test\nMAX_TRIAL = 3           # speed trial any% Not used here\nMI_THRESHOLD = 0.001    # Mutual Information threshold value to drop.\n\nRANDOM_STATE = 42\nVERBOSE = 0\n\n# Admin\nID = \"row_id\"            # Id id x X index\nINPUT = \"../input/tabular-playground-series-jan-2022\"\nTPU = False           # True: use TPU.\nBEST_OR_FOLD = False # True: use Best model, False: use KFOLD softvote\nFEATURE_ENGINEERING = True\nPSEUDO_LABEL = True\nBLEND = True\n\n# time series data common new feature  \nDATE = \"date\"\n\nYEAR = \"year\"\nMONTH = \"month\"\nWEEK = \"week\"\nDAY = \"day\"\n\n\nDAYOFYEAR = \"dayofyear\"\nDAYOFMONTH = \"dayofMonth\"\nDAYOFWEEK = \"dayofweek\"\nWEEKDAY = \"weekday\"\n\nassert BATCH_SIZE % 2 == 0, \\\n    \"BATCH_SIZE must be even number.\"","metadata":{"execution":{"iopub.status.busy":"2022-01-09T09:33:09.444376Z","iopub.execute_input":"2022-01-09T09:33:09.444653Z","iopub.status.idle":"2022-01-09T09:33:09.460766Z","shell.execute_reply.started":"2022-01-09T09:33:09.444622Z","shell.execute_reply":"2022-01-09T09:33:09.459976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://www.kaggle.com/c/web-traffic-time-series-forecasting/discussion/36414\ndef smape(y_true, y_pred):\n    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2.0\n    diff = np.abs(y_true - y_pred) / denominator\n    diff[denominator == 0] = 0.0\n    return 100*np.mean(diff)","metadata":{"execution":{"iopub.status.busy":"2022-01-09T09:33:09.462256Z","iopub.execute_input":"2022-01-09T09:33:09.46279Z","iopub.status.idle":"2022-01-09T09:33:09.480719Z","shell.execute_reply.started":"2022-01-09T09:33:09.462585Z","shell.execute_reply":"2022-01-09T09:33:09.478874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_periodogram(ts, detrend='linear', ax=None):\n    from scipy.signal import periodogram\n    fs = pd.Timedelta(\"1Y\") / pd.Timedelta(\"1D\")\n    freqencies, spectrum = periodogram(\n        ts,\n        fs=fs,\n        detrend=detrend,\n        window=\"boxcar\",\n        scaling='spectrum',\n    )\n    if ax is None:\n        _, ax = plt.subplots()\n    ax.step(freqencies, spectrum, color=\"purple\")\n    ax.set_xscale(\"log\")\n    ax.set_xticks([1, 2, 4, 6, 12, 26, 52, 104])\n    ax.set_xticklabels(\n        [\n            \"Annual (1)\",\n            \"Semiannual (2)\",\n            \"Quarterly (4)\",\n            \"Bimonthly (6)\",\n            \"Monthly (12)\",\n            \"Biweekly (26)\",\n            \"Weekly (52)\",\n            \"Semiweekly (104)\",\n        ],\n        rotation=30,\n    )\n    ax.ticklabel_format(axis=\"y\", style=\"sci\", scilimits=(0, 0))\n    ax.set_ylabel(\"Variance\")\n    ax.set_title(\"Periodogram\")\n    return ax","metadata":{"execution":{"iopub.status.busy":"2022-01-09T09:33:09.483316Z","iopub.execute_input":"2022-01-09T09:33:09.484119Z","iopub.status.idle":"2022-01-09T09:33:09.495661Z","shell.execute_reply.started":"2022-01-09T09:33:09.484074Z","shell.execute_reply":"2022-01-09T09:33:09.494582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Preprocessing ##\n\nBefore we can do any feature engineering, we need to *preprocess* the data to get it in a form suitable for analysis. We'll need to:\n- **Load** the data from CSV files\n- **Clean** the data to fix any errors or inconsistencies\n- **Encode** the statistical data type (numeric, categorical)\n- **Impute** any missing values\n\nWe'll wrap all these steps up in a function, which will make easy for you to get a fresh dataframe whenever you need. After reading the CSV file, we'll apply three preprocessing steps, `clean`, `encode`, and `impute`, and then create the data splits: one (`df_train`) for training the model, and one (`df_test`) for making the predictions that you'll submit to the competition for scoring on the leaderboard.","metadata":{}},{"cell_type":"markdown","source":"### Handle Missing Values ###\n\nHandling missing values now will make the feature engineering go more smoothly. We'll impute `0` for missing numeric values and `\"None\"` for missing categorical values. You might like to experiment with other imputation strategies. In particular, you could try creating \"missing value\" indicators: `1` whenever a value was imputed and `0` otherwise.","metadata":{}},{"cell_type":"code","source":"def impute(df):\n    for name in df.select_dtypes(\"number\"):\n        df[name] = df[name].fillna(0)\n    for name in df.select_dtypes(\"category\"):\n        df[name] = df[name].fillna(\"None\")\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-01-09T09:33:09.496749Z","iopub.execute_input":"2022-01-09T09:33:09.497599Z","iopub.status.idle":"2022-01-09T09:33:09.51672Z","shell.execute_reply.started":"2022-01-09T09:33:09.497556Z","shell.execute_reply":"2022-01-09T09:33:09.515938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Reduce Memory usage","metadata":{}},{"cell_type":"code","source":"# for col in df.select_dtypes('int').columns:\n#     df[col] = pd.to_numeric(df[col], downcast = 'integer')\n\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2\n\n    for col in df.columns:\n        col_type = df[col].dtypes\n\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() / 1024**2\n\n    if verbose:\n        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n \n    return df","metadata":{"execution":{"iopub.status.busy":"2022-01-09T09:33:09.518191Z","iopub.execute_input":"2022-01-09T09:33:09.51873Z","iopub.status.idle":"2022-01-09T09:33:09.535162Z","shell.execute_reply.started":"2022-01-09T09:33:09.518682Z","shell.execute_reply":"2022-01-09T09:33:09.533819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data/Feature Engineering","metadata":{}},{"cell_type":"code","source":"def fourier_features(index, freq, order):\n    time = np.arange(len(index), dtype=np.float32)\n    k = 2 * np.pi * (1 / freq) * time\n    features = {}\n    for i in range(1, order + 1):\n        features.update({\n            f\"sin_{freq}_{i}\": np.sin(i * k),\n            f\"cos_{freq}_{i}\": np.cos(i * k),\n        })\n    return pd.DataFrame(features, index=index)\n\n# Compute Fourier features to the 4th order (8 new features) for a\n# series y with daily observations and annual seasonality:\n#\n# fourier_features(y, freq=365.25, order=4)\n\ndef get_basic_ts_features(df):\n    \n    gdp_df = pd.read_csv('../input/gdp-20152019-finland-norway-and-sweden/GDP_data_2015_to_2019_Finland_Norway_Sweden.csv')\n    gdp_df.set_index('year', inplace=True)\n    \n    def get_gdp(row):\n        country = 'GDP_' + row.country\n        return gdp_df.loc[row.date.year, country]\n    \n    df['gdp'] = np.log(df.apply(get_gdp, axis=1))\n    \n    for country in ['Finland', 'Norway']:\n        df[country] = df.country == country\n    for store in ['KaggleMart']:\n        df[store] = df['store'] == store\n    for product in ['Kaggle Mug', 'Kaggle Sticker']:\n        df[product] = df['product'] == product\n    \n#     df[YEAR] = df[DATE].dt.year\n#     df[MONTH] = df[DATE].dt.month\n#     df[WEEK] = df[DATE].dt.week\n#     df[DAY] = df[DATE].dt.day\n#     df[DAYOFYEAR] = df[DATE].dt.dayofyear\n#     df[DAYOFMONTH] = df[DATE].dt.days_in_month\n#     df[DAYOFWEEK] = df[DATE].dt.dayofweek\n#     df[WEEKDAY] = df[DATE].dt.weekday\n    df[WEEKDAY] = df[DATE].dt.weekday == 4\n    df[WEEKDAY] = df[DATE].dt.weekday >= 5\n    # 21 days cyclic for lunar\n    dayofyear = df.date.dt.dayofyear\n    for k in range(1, 22, 1):\n        df[f'sin{k}'] = np.sin(dayofyear / 365 * 2 * math.pi * k)\n        df[f'cos{k}'] = np.cos(dayofyear / 365 * 2 * math.pi * k)\n        df[f'Finland_sin{k}'] = df[f'sin{k}'] * df['Finland']\n        df[f'Finland_cos{k}'] = df[f'cos{k}'] * df['Finland']\n        df[f'Norway_sin{k}'] = df[f'sin{k}'] * df['Norway']\n        df[f'Norway_cos{k}'] = df[f'cos{k}'] * df['Norway']\n        df[f'store_sin{k}'] = df[f'sin{k}'] * df['KaggleMart']\n        df[f'store_cos{k}'] = df[f'cos{k}'] * df['KaggleMart']\n        df[f'mug_sin{k}'] = df[f'sin{k}'] * df['Kaggle Mug']\n        df[f'mug_cos{k}'] = df[f'cos{k}'] * df['Kaggle Mug']\n        df[f'sticker_sin{k}'] = df[f'sin{k}'] * df['Kaggle Sticker']\n        df[f'sticker_cos{k}'] = df[f'cos{k}'] * df['Kaggle Sticker']\n    \n    # End of year\n    for d in range(24, 32):\n        df[f\"dec{d}\"] = (df.date.dt.month == 12) & (df.date.dt.day == d)\n    for d in range(1, 13):\n        df[f\"jan{d}\"] = (df.date.dt.month == 1) & (df.date.dt.day == d)\n    # May\n    for d in list(range(1, 10)) + list(range(17, 25)):\n        df[f\"may{d}\"] = (df.date.dt.month == 5) & (df.date.dt.day == d)\n    # June\n    for d in list(range(6, 14)):\n        df[f\"june{d}\"] = (df.date.dt.month == 6) & (df.date.dt.day == d)\n    \n    # Last Wednesday of June\n    wed_june_date = df.date.dt.year.map({2015: pd.Timestamp(('2015-06-24')),\n                                         2016: pd.Timestamp(('2016-06-29')),\n                                         2017: pd.Timestamp(('2017-06-28')),\n                                         2018: pd.Timestamp(('2018-06-27')),\n                                         2019: pd.Timestamp(('2019-06-26'))})\n    for d in list(range(-5, 6)):\n        df[f\"wed_june{d}\"] = (df.date - wed_june_date == np.timedelta64(d, \"D\"))\n        \n    # First Sunday of November\n    sun_nov_date = df.date.dt.year.map({2015: pd.Timestamp(('2015-11-1')),\n                                         2016: pd.Timestamp(('2016-11-6')),\n                                         2017: pd.Timestamp(('2017-11-5')),\n                                         2018: pd.Timestamp(('2018-11-4')),\n                                         2019: pd.Timestamp(('2019-11-3'))})\n    for d in list(range(0, 10)):\n        df[f\"sun_nov{d}\"] = (df.date - sun_nov_date == np.timedelta64(d, \"D\"))\n        \n    # Easter\n    easter_date = df.date.apply(lambda date: pd.Timestamp(easter.easter(date.year)))\n    for d in list(range(0, 9)) + list(range(50, 60)) + list(range(40, 46)):\n        df[f\"easter{d}\"] = (df.date - easter_date == np.timedelta64(d, \"D\"))\n                                      \n#     df.drop(columns=[DATE], inplace = True)\n    \n    return df  ","metadata":{"execution":{"iopub.status.busy":"2022-01-09T09:33:09.536915Z","iopub.execute_input":"2022-01-09T09:33:09.537309Z","iopub.status.idle":"2022-01-09T09:33:09.574186Z","shell.execute_reply.started":"2022-01-09T09:33:09.537275Z","shell.execute_reply":"2022-01-09T09:33:09.573269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def feature_engineer(df):\n    df = get_basic_ts_features(df)\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-01-09T09:33:09.575549Z","iopub.execute_input":"2022-01-09T09:33:09.576063Z","iopub.status.idle":"2022-01-09T09:33:09.592145Z","shell.execute_reply.started":"2022-01-09T09:33:09.576029Z","shell.execute_reply":"2022-01-09T09:33:09.590711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"from pathlib import Path\n\n\ndef load_data():\n    # Read data\n    data_dir = Path(INPUT)\n    df_train = pd.read_csv(data_dir / \"train.csv\", index_col=ID)\n    df_test = pd.read_csv(data_dir / \"test.csv\", index_col=ID)\n    column_y = df_train.columns.difference(\n        df_test.columns)[0]  # column_y target_col label_col\n    return df_train, df_test, column_y","metadata":{}},{"cell_type":"code","source":"from pathlib import Path\n\n\ndef load_data():\n    # Read data\n    data_dir = Path(INPUT)\n    df_train = pd.read_csv(data_dir / \"train.csv\", parse_dates=[DATE],\n                    usecols=['date', 'country', 'store', 'product', 'num_sold'],\n                    dtype={\n                        'country': 'category',\n                        'store': 'category',\n                        'product': 'category',\n                        'num_sold': 'float32',\n                    },\n                    infer_datetime_format=True,)\n    df_test = pd.read_csv(data_dir / \"test.csv\", index_col=ID, parse_dates=[DATE])\n    column_y = df_train.columns.difference(\n        df_test.columns)[0]  # column_y target_col label_col\n    df_train[DATE] = pd.to_datetime(df_train[DATE])\n    df_test[DATE] = pd.to_datetime(df_test[DATE])\n    return df_train, df_test, column_y\n","metadata":{"execution":{"iopub.status.busy":"2022-01-09T09:33:09.593579Z","iopub.execute_input":"2022-01-09T09:33:09.594383Z","iopub.status.idle":"2022-01-09T09:33:09.606286Z","shell.execute_reply.started":"2022-01-09T09:33:09.594334Z","shell.execute_reply":"2022-01-09T09:33:09.605216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def process_data(df_train, df_test):\n    # Preprocessing\n#     df_train = impute(df_train)\n#     df_test = impute(df_test)\n    \n    if FEATURE_ENGINEERING:\n        df_train = feature_engineer(df_train)\n        df_test = feature_engineer(df_test)\n    \n#     df_train = reduce_mem_usage(df_train)\n#     df_test = reduce_mem_usage(df_test)\n\n    return df_train, df_test","metadata":{"execution":{"iopub.status.busy":"2022-01-09T09:33:09.60918Z","iopub.execute_input":"2022-01-09T09:33:09.609746Z","iopub.status.idle":"2022-01-09T09:33:09.626541Z","shell.execute_reply.started":"2022-01-09T09:33:09.609699Z","shell.execute_reply":"2022-01-09T09:33:09.625473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Data #\n\nAnd now we can call the data loader and get the processed data splits:","metadata":{}},{"cell_type":"code","source":"%%time\ntrain_df, test_df, column_y = load_data()","metadata":{"execution":{"iopub.status.busy":"2022-01-09T09:33:09.62795Z","iopub.execute_input":"2022-01-09T09:33:09.628876Z","iopub.status.idle":"2022-01-09T09:33:09.77327Z","shell.execute_reply.started":"2022-01-09T09:33:09.628823Z","shell.execute_reply":"2022-01-09T09:33:09.772227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Pseudolabeling","metadata":{}},{"cell_type":"code","source":"%%time\ntrain_df, test_df = process_data(train_df, test_df)","metadata":{"execution":{"iopub.status.busy":"2022-01-09T09:33:09.774658Z","iopub.execute_input":"2022-01-09T09:33:09.774891Z","iopub.status.idle":"2022-01-09T09:33:12.91919Z","shell.execute_reply.started":"2022-01-09T09:33:09.774865Z","shell.execute_reply":"2022-01-09T09:33:12.91828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = train_df.copy()\ntrain_data['date'] = train_df.date.dt.to_period('D')\ntest_data = test_df.copy()\ntest_data['date'] = test_df.date.dt.to_period('D')","metadata":{"execution":{"iopub.status.busy":"2022-01-09T09:33:12.92035Z","iopub.execute_input":"2022-01-09T09:33:12.920579Z","iopub.status.idle":"2022-01-09T09:33:13.034383Z","shell.execute_reply.started":"2022-01-09T09:33:12.920552Z","shell.execute_reply":"2022-01-09T09:33:13.033474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = train_data.set_index(['date']).sort_index()\nX_test = test_data.set_index(['date']).sort_index()","metadata":{"execution":{"iopub.status.busy":"2022-01-09T09:33:13.035596Z","iopub.execute_input":"2022-01-09T09:33:13.035858Z","iopub.status.idle":"2022-01-09T09:33:13.148025Z","shell.execute_reply.started":"2022-01-09T09:33:13.035827Z","shell.execute_reply":"2022-01-09T09:33:13.147097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MultiColumnLabelEncoder:\n    def __init__(self,columns = None):\n        self.columns = columns # array of column names to encode\n\n    def fit(self,X,y=None):\n        return self # not relevant here\n\n    def transform(self,X):\n        '''\n        Transforms columns of X specified in self.columns using\n        LabelEncoder(). If no columns specified, transforms all\n        columns in X.\n        '''\n        output = X.copy()\n        if self.columns is not None:\n            for col in self.columns:\n                output[col] = LabelEncoder().fit_transform(output[col])\n        else:\n            for colname,col in output.iteritems():\n                output[colname] = LabelEncoder().fit_transform(col)\n        return output\n\n    def fit_transform(self,X,y=None):\n        return self.fit(X,y).transform(X)","metadata":{"execution":{"iopub.status.busy":"2022-01-09T09:33:13.149224Z","iopub.execute_input":"2022-01-09T09:33:13.149484Z","iopub.status.idle":"2022-01-09T09:33:13.157752Z","shell.execute_reply.started":"2022-01-09T09:33:13.149453Z","shell.execute_reply":"2022-01-09T09:33:13.156965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = MultiColumnLabelEncoder(columns = ['country','store', 'product']).fit_transform(X)\nX_test = MultiColumnLabelEncoder(columns = ['country','store', 'product']).transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-01-09T09:33:13.158889Z","iopub.execute_input":"2022-01-09T09:33:13.159669Z","iopub.status.idle":"2022-01-09T09:33:13.245167Z","shell.execute_reply.started":"2022-01-09T09:33:13.159622Z","shell.execute_reply":"2022-01-09T09:33:13.244111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = train_data.set_index(['date', 'country', 'store', 'product']).sort_index()","metadata":{"execution":{"iopub.status.busy":"2022-01-09T09:33:13.246557Z","iopub.execute_input":"2022-01-09T09:33:13.246797Z","iopub.status.idle":"2022-01-09T09:33:13.316319Z","shell.execute_reply.started":"2022-01-09T09:33:13.246766Z","shell.execute_reply":"2022-01-09T09:33:13.315345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kaggle_sales_2015 = (\n    train_data\n    .groupby(['country', 'store', 'product', 'date'])\n    .mean()\n    .unstack(['country', 'store', 'product'])\n    .loc['2015']\n)","metadata":{"execution":{"iopub.status.busy":"2022-01-09T09:33:13.317891Z","iopub.execute_input":"2022-01-09T09:33:13.318233Z","iopub.status.idle":"2022-01-09T09:33:13.972018Z","shell.execute_reply.started":"2022-01-09T09:33:13.318202Z","shell.execute_reply":"2022-01-09T09:33:13.971012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kaggle_sales_2016 = (\n    train_data\n    .groupby(['country', 'store', 'product', 'date'])\n    .mean()\n    .unstack(['country', 'store', 'product'])\n    .loc['2016']\n)","metadata":{"execution":{"iopub.status.busy":"2022-01-09T09:33:13.973443Z","iopub.execute_input":"2022-01-09T09:33:13.973687Z","iopub.status.idle":"2022-01-09T09:33:14.743614Z","shell.execute_reply.started":"2022-01-09T09:33:13.973656Z","shell.execute_reply":"2022-01-09T09:33:14.742925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kaggle_sales_2017 = (\n    train_data\n    .groupby(['country', 'store', 'product', 'date'])\n    .mean()\n    .unstack(['country', 'store', 'product'])\n    .loc['2017']\n)","metadata":{"execution":{"iopub.status.busy":"2022-01-09T09:33:14.744604Z","iopub.execute_input":"2022-01-09T09:33:14.745199Z","iopub.status.idle":"2022-01-09T09:33:15.38815Z","shell.execute_reply.started":"2022-01-09T09:33:14.745161Z","shell.execute_reply":"2022-01-09T09:33:15.387504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kaggle_sales_2018 = (\n    train_data\n    .groupby(['country', 'store', 'product', 'date'])\n    .mean()\n    .unstack(['country', 'store', 'product'])\n    .loc['2018']\n)","metadata":{"execution":{"iopub.status.busy":"2022-01-09T09:33:15.389389Z","iopub.execute_input":"2022-01-09T09:33:15.39027Z","iopub.status.idle":"2022-01-09T09:33:16.036981Z","shell.execute_reply.started":"2022-01-09T09:33:15.390222Z","shell.execute_reply":"2022-01-09T09:33:16.035608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"frames = [kaggle_sales_2015, kaggle_sales_2016, kaggle_sales_2017, kaggle_sales_2018]\nkaggle_sales = pd.concat(frames)","metadata":{"execution":{"iopub.status.busy":"2022-01-09T09:33:16.038713Z","iopub.execute_input":"2022-01-09T09:33:16.03907Z","iopub.status.idle":"2022-01-09T09:33:16.10517Z","shell.execute_reply.started":"2022-01-09T09:33:16.039027Z","shell.execute_reply":"2022-01-09T09:33:16.104511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kaggle_sales","metadata":{"execution":{"iopub.status.busy":"2022-01-09T09:33:16.106345Z","iopub.execute_input":"2022-01-09T09:33:16.107202Z","iopub.status.idle":"2022-01-09T09:33:16.186161Z","shell.execute_reply.started":"2022-01-09T09:33:16.107152Z","shell.execute_reply":"2022-01-09T09:33:16.185296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# X = train_data.drop(columns=column_y)\n# y = train_data[[column_y]].astype(int)\n\n# X_test = test_data.loc[:,X.columns]\n\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-01-09T09:33:16.187524Z","iopub.execute_input":"2022-01-09T09:33:16.187844Z","iopub.status.idle":"2022-01-09T09:33:16.314214Z","shell.execute_reply.started":"2022-01-09T09:33:16.187799Z","shell.execute_reply":"2022-01-09T09:33:16.313289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check NA\nmissing_val = X.isnull().sum()\nprint(missing_val[missing_val > 0])","metadata":{"execution":{"iopub.status.busy":"2022-01-09T09:33:16.315846Z","iopub.execute_input":"2022-01-09T09:33:16.316371Z","iopub.status.idle":"2022-01-09T09:33:16.353037Z","shell.execute_reply.started":"2022-01-09T09:33:16.316334Z","shell.execute_reply":"2022-01-09T09:33:16.352314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.groupby(column_y).apply(lambda s: s.sample(min(len(s), 5)))","metadata":{"execution":{"iopub.status.busy":"2022-01-09T09:33:16.354143Z","iopub.execute_input":"2022-01-09T09:33:16.354507Z","iopub.status.idle":"2022-01-09T09:33:18.239672Z","shell.execute_reply.started":"2022-01-09T09:33:16.354475Z","shell.execute_reply":"2022-01-09T09:33:18.238719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# y = train_data.loc[:, column_y]\n\n# # YOUR CODE HERE: Make 4 lag features\n# X = make_lags(y, lags=4).dropna()\n\n# # YOUR CODE HERE: Make multistep target\n# y = make_multistep_target(y, steps=16).dropna()\n\n# y, X = y.align(X, join='inner', axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-01-09T09:33:18.241115Z","iopub.execute_input":"2022-01-09T09:33:18.241348Z","iopub.status.idle":"2022-01-09T09:33:18.244992Z","shell.execute_reply.started":"2022-01-09T09:33:18.241321Z","shell.execute_reply":"2022-01-09T09:33:18.244322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig_dims = (50,30)\nax = kaggle_sales.num_sold.plot(title='Sales Trends', figsize=fig_dims)\n_ = ax.set(ylabel=\"Numbers sold\")","metadata":{"execution":{"iopub.status.busy":"2022-01-09T09:33:18.248826Z","iopub.execute_input":"2022-01-09T09:33:18.249536Z","iopub.status.idle":"2022-01-09T09:33:23.42918Z","shell.execute_reply.started":"2022-01-09T09:33:18.249494Z","shell.execute_reply":"2022-01-09T09:33:23.428178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show_me(data) :\n    fig_dims = (20,10)\n    fig, ax = plt.subplots(figsize=fig_dims)\n    sns.set_theme(style=\"whitegrid\")\n    dates = pd.date_range(\"1 1 2015\", periods=365, freq=\"D\")\n    dates = pd.date_range(start='1/1/2015', end='31/12/2016',  freq=\"D\")\n    data.index = dates\n    sns.lineplot(data=data, palette=\"tab10\", linewidth=1)","metadata":{"execution":{"iopub.status.busy":"2022-01-09T09:33:23.430976Z","iopub.execute_input":"2022-01-09T09:33:23.431448Z","iopub.status.idle":"2022-01-09T09:33:23.438128Z","shell.execute_reply.started":"2022-01-09T09:33:23.431355Z","shell.execute_reply":"2022-01-09T09:33:23.436775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# What is Seasonality? #\n\nWe say that a time series exhibits **seasonality** whenever there is a regular, periodic change in the mean of the series. Seasonal changes generally follow the clock and calendar -- repetitions over a day, a week, or a year are common. Seasonality is often driven by the cycles of the natural world over days and years or by conventions of social behavior surrounding dates and times.\n### Choosing Fourier features with the Periodogram\n\nHow many Fourier pairs should we actually include in our feature set? We can answer this question with the periodogram. The **periodogram** tells you the strength of the frequencies in a time series. Specifically, the value on the y-axis of the graph is `(a ** 2 + b ** 2) / 2`, where `a` and `b` are the coefficients of the sine and cosine at that frequency (as in the *Fourier Components* plot above).\n\n<figure style=\"padding: 1em;\">\n<img src=\"https://i.imgur.com/PK6WEe3.png\" width=600, alt=\"\">\n<figcaption style=\"textalign: center; font-style: italic\"><center>Periodogram for the <em>Wiki Trigonometry</em> series.</center></figcaption>\n</figure>\n\nFrom left to right, the periodogram drops off after *Quarterly*, four times a year. That was why we chose four Fourier pairs to model the annual season. The *Weekly* frequency we ignore since it's better modeled with indicators.\n\n### Computing Fourier features (optional)\n\nKnowing how Fourier features are computed isn't essential to using them, but if seeing the details would clarify things, the cell hidden cell below illustrates how a set of Fourier features could be derived from the index of a time series. (We'll use a library function from `statsmodels` for our applications, however.)","metadata":{}},{"cell_type":"code","source":"Rama_swe = [col for col in kaggle_sales.columns if ('KaggleRama' in col) & ('Sweden' in col)]\n# show_me(kaggle_sales[Rama_swe])","metadata":{"execution":{"iopub.status.busy":"2022-01-09T09:33:23.43931Z","iopub.execute_input":"2022-01-09T09:33:23.439971Z","iopub.status.idle":"2022-01-09T09:33:23.459428Z","shell.execute_reply.started":"2022-01-09T09:33:23.439936Z","shell.execute_reply":"2022-01-09T09:33:23.458696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's look at the periodogram:","metadata":{}},{"cell_type":"code","source":"plot_periodogram(X[column_y]);","metadata":{"execution":{"iopub.status.busy":"2022-01-09T09:33:23.460842Z","iopub.execute_input":"2022-01-09T09:33:23.461297Z","iopub.status.idle":"2022-01-09T09:33:24.569775Z","shell.execute_reply.started":"2022-01-09T09:33:23.461264Z","shell.execute_reply":"2022-01-09T09:33:24.569121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The periodogram agrees with the seasonal plots above: a strong semiweekly season and a weaker annual season. The weekly season we'll model with indicators and the annual season with Fourier features. From right to left, the periodogram falls off between Bimonthly (6) and Monthly (12), so let's use 10 Fourier pairs.\n\nWe'll create our seasonal features using DeterministicProcess, the same utility we used in Lesson 2 to create trend features. To use two seasonal periods (weekly and annual), we'll need to instantiate one of them as an \"additional term\":","metadata":{}},{"cell_type":"markdown","source":"# Components and Residuals #\n\nSo that we can design effective hybrids, we need a better understanding of how time series are constructed. We've studied up to now three patterns of dependence: trend, seasons, and cycles. Many time series can be closely described by an additive model of just these three components plus some essentially unpredictable, entirely random *error*:\n\n```\nseries = trend + seasons + cycles + error\n```\n\nEach of the terms in this model we would then call a **component** of the time series.\n\nThe **residuals** of a model are the difference between the target the model was trained on and the predictions the model makes -- the difference between the actual curve and the fitted curve, in other words. Plot the residuals against a feature, and you get the \"left over\" part of the target, or what the model failed to learn about the target from that feature.","metadata":{}},{"cell_type":"code","source":"# You'll add fit and predict methods to this minimal class\nclass BoostedHybrid:\n    def __init__(self, model_1, model_2):\n        self.model_1 = model_1\n        self.model_2 = model_2\n        self.y_columns = None  # store column names from fit method\n    def fit(self, X_1, X_2, y):\n        # Train model_1\n        self.model_1.fit(X_1, y)\n\n        # Make predictions\n        y_fit = pd.DataFrame(\n            self.model_1.predict(X_1), \n            index=X_1.index, columns=y.columns,\n        )\n\n        # Compute residuals\n        y_resid = y - y_fit\n        y_resid = y_resid.unstack() # wide to long\n\n        # Train model_2 on residuals\n        self.model_2.fit(X_2, y_resid)\n\n        # Save column names for predict method\n        self.y_columns = y.columns\n        # Save data for question checking\n        self.y_fit = y_fit\n        self.y_resid = y_resid\n    def predict(self, X_1, X_2):\n        # Predict with model_1\n        y_pred = pd.DataFrame(\n            self.model_1.predict(X_1), \n            index=X_1.index, columns=self.y_columns,\n        )\n        y_pred = y_pred.unstack()  # wide to long\n\n        # Add model_2 predictions to model_1 predictions\n        y_pred += self.model_2.predict(X_2)\n\n        return y_pred.unstack()\n","metadata":{"execution":{"iopub.status.busy":"2022-01-09T09:33:24.571141Z","iopub.execute_input":"2022-01-09T09:33:24.571603Z","iopub.status.idle":"2022-01-09T09:33:24.580954Z","shell.execute_reply.started":"2022-01-09T09:33:24.571568Z","shell.execute_reply":"2022-01-09T09:33:24.579891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"GROUP_INDEX = ['country', 'store', 'product']\n\n# Target series\ny = kaggle_sales.loc[:, column_y]\n\n# X_1: Features for Linear Regression\ndp = DeterministicProcess(index=y.index, order=1)\nX_1 = dp.in_sample()\n\n\n# X_2: Features for XGBoost\nX_2 = X.drop(column_y, axis=1)\n\n# Label encoding for 'family'\n# le = LabelEncoder()  # from sklearn.preprocessing\n# X_2 = X_2.reset_index('date')\n# X_2['date'] = le.fit_transform(X_2['date'])\n\n# Label encoding for seasonality\n# X_2[\"day\"] = X_2.index.dayofyear  # values are day of the month\n\n# X_test[GROUP_INDEX] = X_test[GROUP_INDEX].apply(lambda x: le[x.name].transform(x))","metadata":{"execution":{"iopub.status.busy":"2022-01-09T09:33:24.582684Z","iopub.execute_input":"2022-01-09T09:33:24.583035Z","iopub.status.idle":"2022-01-09T09:33:24.625061Z","shell.execute_reply.started":"2022-01-09T09:33:24.58299Z","shell.execute_reply":"2022-01-09T09:33:24.624053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test_1 = dp.out_of_sample(365)","metadata":{"execution":{"iopub.status.busy":"2022-01-09T09:33:24.626609Z","iopub.execute_input":"2022-01-09T09:33:24.627734Z","iopub.status.idle":"2022-01-09T09:33:24.634149Z","shell.execute_reply.started":"2022-01-09T09:33:24.627682Z","shell.execute_reply":"2022-01-09T09:33:24.633286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Models","metadata":{}},{"cell_type":"code","source":"# Model 1 (trend)\nfrom pyearth import Earth\nfrom sklearn.linear_model import ElasticNet, Lasso, Ridge, HuberRegressor\n\n# Model 2\nfrom sklearn.ensemble import ExtraTreesRegressor, RandomForestRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.neural_network import MLPRegressor\n\n# Boosted Hybrid\n\n# YOUR CODE HERE: Try different combinations of the algorithms above KNeighborsRegressor\nparam1 = {  'loss_function' : 'MultiRMSE',\n            'eval_metric': 'MultiRMSE', \n#             'od_type' : 'Iter',\n#             'od_wait' : 20,\n            'random_state': RANDOM_STATE,\n            'verbose': VERBOSE\n         }\nmodel = BoostedHybrid(\n    model_1 = Ridge(),\n    model_2 = XGBRegressor(objective='reg:squarederror', n_estimators=1000) #CatBoostRegressor(**param1), #XGBRegressor(objective='reg:squarederror', n_estimators=1000)\n)","metadata":{"execution":{"iopub.status.busy":"2022-01-09T09:33:24.635507Z","iopub.execute_input":"2022-01-09T09:33:24.635839Z","iopub.status.idle":"2022-01-09T09:33:24.888384Z","shell.execute_reply.started":"2022-01-09T09:33:24.635797Z","shell.execute_reply":"2022-01-09T09:33:24.887468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train, y_valid = y[:\"2018-12-31\"], y[\"2018-01-01\":\"2018-12-31\"]\nX1_train, X1_valid = X_1[: \"2018-12-31\"], X_1[\"2018-01-01\":\"2018-12-31\"]\nX2_train, X2_valid = X_2.loc[:\"2018-12-31\"], X_2.loc[\"2018-01-01\":\"2018-12-31\"]","metadata":{"execution":{"iopub.status.busy":"2022-01-09T09:33:24.889743Z","iopub.execute_input":"2022-01-09T09:33:24.889982Z","iopub.status.idle":"2022-01-09T09:33:24.902098Z","shell.execute_reply.started":"2022-01-09T09:33:24.889952Z","shell.execute_reply":"2022-01-09T09:33:24.901109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test","metadata":{"execution":{"iopub.status.busy":"2022-01-09T09:33:24.903574Z","iopub.execute_input":"2022-01-09T09:33:24.90392Z","iopub.status.idle":"2022-01-09T09:33:24.943496Z","shell.execute_reply.started":"2022-01-09T09:33:24.903873Z","shell.execute_reply":"2022-01-09T09:33:24.942893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Some of the algorithms above do best with certain kinds of\n# preprocessing on the features (like standardization), but this is\n# just a demo.\nmodel.fit(X1_train, X2_train, y_train)\ny_fit = model.predict(X1_train, X2_train).clip(0.0)\ny_pred = model.predict(X_test_1, X_test).clip(0.0) #X1_valid, X2_valid","metadata":{"execution":{"iopub.status.busy":"2022-01-09T09:33:24.944433Z","iopub.execute_input":"2022-01-09T09:33:24.945263Z","iopub.status.idle":"2022-01-09T09:36:15.01871Z","shell.execute_reply.started":"2022-01-09T09:33:24.945221Z","shell.execute_reply":"2022-01-09T09:36:15.017546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"families = y.columns[0:]","metadata":{"execution":{"iopub.status.busy":"2022-01-09T09:36:15.02103Z","iopub.execute_input":"2022-01-09T09:36:15.021697Z","iopub.status.idle":"2022-01-09T09:36:15.027578Z","shell.execute_reply.started":"2022-01-09T09:36:15.021649Z","shell.execute_reply":"2022-01-09T09:36:15.026525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"axs = y.loc(axis=1)[families].plot(\n    subplots=True, sharex=True, figsize=(30, 80), **plot_params, alpha=0.5,\n)\n_ = y_fit.unstack().unstack().unstack().unstack().unstack().unstack().loc(axis=1)[families].plot(subplots=True, sharex=True, color='C0', ax=axs)\n_ = y_pred.unstack().unstack().unstack().unstack().unstack().unstack().loc(axis=1)[families].plot(subplots=True, sharex=True, color='C3', ax=axs)\nfor ax, family in zip(axs, families):\n    ax.legend(['true','train','forecast'])\n    ax.set_ylabel(family)","metadata":{"execution":{"iopub.status.busy":"2022-01-09T09:36:15.029501Z","iopub.execute_input":"2022-01-09T09:36:15.030075Z","iopub.status.idle":"2022-01-09T09:36:26.530102Z","shell.execute_reply.started":"2022-01-09T09:36:15.030031Z","shell.execute_reply":"2022-01-09T09:36:26.528681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_test_df_by_year = pd.DataFrame(y_pred,columns=y_pred.columns.tolist())","metadata":{"execution":{"iopub.status.busy":"2022-01-09T09:36:26.532029Z","iopub.execute_input":"2022-01-09T09:36:26.53277Z","iopub.status.idle":"2022-01-09T09:36:26.540628Z","shell.execute_reply.started":"2022-01-09T09:36:26.532733Z","shell.execute_reply":"2022-01-09T09:36:26.539501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.y_resid","metadata":{"execution":{"iopub.status.busy":"2022-01-09T09:47:27.591735Z","iopub.execute_input":"2022-01-09T09:47:27.592593Z","iopub.status.idle":"2022-01-09T09:47:27.606Z","shell.execute_reply.started":"2022-01-09T09:47:27.592547Z","shell.execute_reply":"2022-01-09T09:47:27.604698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = pd.read_csv('../input/tabular-playground-series-jan-2022/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2022-01-09T09:36:26.541712Z","iopub.execute_input":"2022-01-09T09:36:26.542433Z","iopub.status.idle":"2022-01-09T09:36:26.56504Z","shell.execute_reply.started":"2022-01-09T09:36:26.542381Z","shell.execute_reply":"2022-01-09T09:36:26.563864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_test_df_by_year","metadata":{"execution":{"iopub.status.busy":"2022-01-09T09:36:26.566444Z","iopub.execute_input":"2022-01-09T09:36:26.566829Z","iopub.status.idle":"2022-01-09T09:36:26.615274Z","shell.execute_reply.started":"2022-01-09T09:36:26.566797Z","shell.execute_reply":"2022-01-09T09:36:26.614352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_submission(df):\n    submission = pd.DataFrame(data=np.zeros((sub.shape[0],2)),index = sub.index.tolist(),columns=['row_id',column_y])\n    INDEX = -1\n    for i in range(365):\n        for j in range (18) :\n            INDEX += 1\n            reorder_index = j\n            if ((j+1) % 3) == 1:\n                reorder_index = j + 1\n            if ((j+1) % 3) == 2:\n                reorder_index = j - 1\n            submission[column_y].loc[INDEX,1]=df.iloc[reorder_index,i]\n    submission['row_id'] = sub['row_id']\n    return submission\n\nsubmission_by_year = make_submission(pred_test_df_by_year)","metadata":{"execution":{"iopub.status.busy":"2022-01-09T09:36:26.616562Z","iopub.execute_input":"2022-01-09T09:36:26.61682Z","iopub.status.idle":"2022-01-09T09:36:29.620934Z","shell.execute_reply.started":"2022-01-09T09:36:26.616789Z","shell.execute_reply":"2022-01-09T09:36:29.61989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(submission_by_year.head(20))","metadata":{"execution":{"iopub.status.busy":"2022-01-09T09:36:29.622153Z","iopub.execute_input":"2022-01-09T09:36:29.62238Z","iopub.status.idle":"2022-01-09T09:36:29.634783Z","shell.execute_reply.started":"2022-01-09T09:36:29.622352Z","shell.execute_reply":"2022-01-09T09:36:29.633595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_by_year.to_csv('submission.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-09T09:36:29.635911Z","iopub.execute_input":"2022-01-09T09:36:29.636506Z","iopub.status.idle":"2022-01-09T09:36:29.676091Z","shell.execute_reply.started":"2022-01-09T09:36:29.636465Z","shell.execute_reply":"2022-01-09T09:36:29.674379Z"},"trusted":true},"execution_count":null,"outputs":[]}]}