{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\nimport pandas as pd\nimport numpy as np\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"_uuid":"13b45f56-28bf-4c5c-b220-c90ec2b271b4","_cell_guid":"00ad6301-b226-4d21-ae44-f1c5d25e5353","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-02-17T06:24:56.250981Z","iopub.execute_input":"2022-02-17T06:24:56.251352Z","iopub.status.idle":"2022-02-17T06:24:57.487442Z","shell.execute_reply.started":"2022-02-17T06:24:56.251262Z","shell.execute_reply":"2022-02-17T06:24:57.486704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Index\n- 1. [Describtive analysis (univariant Analysis)](#descriptive-analysis) \n    - 1.1 [Common Data](#common-data)\n        - 1.1.1 [Data type](#common-data)\n        - 1.1.2 [Missing value](#common-data)\n    - 1.2 [Numerical Data](#numerical-data)\n        - 1.2.1 Quantile statistics\n        - 1.2.2 Descriptive analysis\n        - 1.2.3 [Distribution Analysis](#distribution-analysis)\n    - 1.3 [Categorical Data](#categorical-data)\n        - 1.3.1 [Cordinality](#categorical-data)\n        - 1.3.2 [Unnique count](#categorical-data)\n- 2. [Some basic Understanding about the data](#basic-uderstanding)\n- 3. [Correlation Analysis (bivariant Analysis)](#Correlation-analysis)\n","metadata":{}},{"cell_type":"markdown","source":"# Basic","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv('../input/tabular-playground-series-jan-2022/train.csv')\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-17T06:24:57.488806Z","iopub.execute_input":"2022-02-17T06:24:57.488997Z","iopub.status.idle":"2022-02-17T06:24:57.568881Z","shell.execute_reply.started":"2022-02-17T06:24:57.488969Z","shell.execute_reply":"2022-02-17T06:24:57.568227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"data number of samples---->\",data.shape[0])\nprint(\"data number of columns---->\",data.shape[1])","metadata":{"execution":{"iopub.status.busy":"2022-02-17T06:24:57.570231Z","iopub.execute_input":"2022-02-17T06:24:57.570672Z","iopub.status.idle":"2022-02-17T06:24:57.57831Z","shell.execute_reply.started":"2022-02-17T06:24:57.570633Z","shell.execute_reply":"2022-02-17T06:24:57.577086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Basic feature enginerring. --> it is more meaniful to split the data into day , month and year.\ndata['date'] = pd.to_datetime(data[\"date\"])\n\ndata[\"day\"] = data['date'].dt.day\ndata[\"month\"] = data['date'].dt.month\ndata[\"year\"] = data['date'].dt.year\n\ndata['dayofweek'] = data['date'].dt.dayofweek # day of the week monday = 0 to sunday = 6\ndata['dayofmonth'] = data['date'].dt.days_in_month # day of the month\ndata['dayofyear'] = data['date'].dt.dayofyear # day in the year\ndata['weekofyear'] = data['date'].dt.weekofyear  # week of the year\n\n# data.drop([\"date\"],axis=1,  inplace = True) # chaneg the original data\ndata.head(5)","metadata":{"execution":{"iopub.status.busy":"2022-02-17T06:24:57.580052Z","iopub.execute_input":"2022-02-17T06:24:57.58026Z","iopub.status.idle":"2022-02-17T06:24:57.671655Z","shell.execute_reply.started":"2022-02-17T06:24:57.580236Z","shell.execute_reply":"2022-02-17T06:24:57.670933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"descriptive-analysis\"></a>\n## 1. Describtive analysis (Univariant Analysis)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"common-data\"></a>\n### 1.1 Common Data Analysis","metadata":{}},{"cell_type":"code","source":"def Common_data_analysis(data):\n    print(\"{:=^100}\".format(\" Common data analysis \"))\n    # default settings.\n    column = data.columns\n    total_samples = data.shape[0]\n    value_dict = {}\n    \n    # calculate values.\n    missing_values = data.isnull().sum().values\n    missing_value_percentage = [round((col_missing_count / total_samples) * 100, 2) for col_missing_count in missing_values ]\n    datatype = [data.iloc[:,i].dtype for i in range(data.shape[1])]\n    \n    categorical_data = list(data.loc[:,data.dtypes == 'object'].columns)\n    numerical_data = [d for d  in column if d not in categorical_data]\n    # print the diff datatype and count.\n    print()\n    print(\"Numerical data list {} ---> total {} numerical values\".format(numerical_data, len(numerical_data)))\n    print(\"Categorical data list {} ---> total {} categorical values\".format(categorical_data, len(categorical_data)))\n    print()\n    \n    # organise values.\n    value_dict[\"data type\"] = datatype\n    value_dict[\"Missing Value\"] = missing_values\n    value_dict[\"% of Missing value\"] = missing_value_percentage\n    df = pd.DataFrame(value_dict, columns = value_dict.keys(), index = column)\n    \n    # make a highlight for col has high missing value percentage. (>55% say)\n    # the particular row will be highlighted if it above missing value threshold.\n    def highlight_high_missing_value(sample):\n        threshold = 0.55\n        style = sample.copy()\n        highlight = 'background-color: red;'\n        if sample[2] > threshold:\n            style[:] = highlight\n        else:\n            style[:] = ''\n        return style\n    df = df.style.apply(highlight_high_missing_value, axis = 1)\n    display(df)\n    return (column, categorical_data, numerical_data)\n\ndata_columns, categorical_data, numerical_data = Common_data_analysis(data)\n","metadata":{"execution":{"iopub.status.busy":"2022-02-17T06:24:57.672984Z","iopub.execute_input":"2022-02-17T06:24:57.673437Z","iopub.status.idle":"2022-02-17T06:24:57.788547Z","shell.execute_reply.started":"2022-02-17T06:24:57.6734Z","shell.execute_reply":"2022-02-17T06:24:57.787527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # visualization for missing value.\n# plt.figure(figsize=(15,10))\n# sns.heatmap(data.isnull().transpose(),\n#             cbar_kws={'label': 'Missing Data'})\n# plt.title('Heatmap showing Missing Values ', weight = 'bold', size = 20, color = 'red')\n# plt.xticks(size = 12)\n# plt.yticks(size = 12)\n# plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-17T06:24:57.790021Z","iopub.execute_input":"2022-02-17T06:24:57.790315Z","iopub.status.idle":"2022-02-17T06:24:57.794096Z","shell.execute_reply.started":"2022-02-17T06:24:57.790284Z","shell.execute_reply":"2022-02-17T06:24:57.793374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Observation.\nOnly 3 Categorical value and a happy thing there is no missing value :) ","metadata":{}},{"cell_type":"markdown","source":"<a id=\"numerical-data\"></a>\n### 1.2 Numerical Data","metadata":{}},{"cell_type":"code","source":"def numerical_data_analysis(data, numerical_data):\n    print(\"{:=^100}\".format(\" Numerical data analysis \"))\n\n    column = list(data.columns)\n    column.remove(\"date\")\n\n    min_value = [data[col].min() if col in numerical_data else \"NA\" for col in column]\n    max_value = [data[col].max() if col in numerical_data else \"NA\" for col in column]\n    #mode_value = [data[col].mode() if col in numerical_data else \"NA\" for col in column]\n    mean_value = [data[col].mean() if col in numerical_data else \"NA\" for col in column]\n    std_value = [data[col].std() if col in numerical_data else \"NA\" for col in column]\n    #print(mode_value)\n    skewness_value = [data[col].skew() if col in numerical_data else \"NA\" for col in column]\n    kurtosis_value = [data[col].kurtosis() if col in numerical_data else \"NA\" for col in column]\n\n    q1_value = [data[col].quantile(0.25) if col in numerical_data else \"NA\" for col in column]\n    q2_meadian_value = [data[col].quantile(0.50) if col in numerical_data else \"NA\" for col in column]\n    q3_value = [data[col].quantile(0.75) if col in numerical_data else \"NA\" for col in column]\n\n    # find the range value.\n    def find_range(min_value_list, max_value_list):\n        range_value = [(max_value - min_value)  if min_value != \"NA\" else \"NA\" for max_value, min_value in zip(max_value_list, min_value_list)]\n        return range_value\n\n    # find the inter quartile range. (q3-q1)\n    def iqr(q1_value_list, q3_value_list):\n        range_value = [(q3 - q1) if q1 != \"NA\" else \"NA\" for q3, q1 in zip(q3_value_list, q1_value_list)]\n        return range_value\n\n    range_value = find_range(min_value, max_value)\n    iqr_value = iqr(q1_value, q3_value)\n\n    # organise everything inside a dataframe.\n    df_dict = {}\n    df_dict[\"min\"] = min_value\n    df_dict[\"max\"] = max_value\n    df_dict[\"range(max-min)\"] = range_value\n    #df_dict[\"mode\"] = mode_value\n    df_dict[\"mean/average\"] = mean_value\n    df_dict[\"standard deviation\"] = std_value\n    df_dict[\"Q1\"] = q1_value\n    df_dict[\"meadian/Q2\"] = q2_meadian_value\n    df_dict[\"Q3\"] = q3_value\n    df_dict[\"Inter quantile range\"] = iqr_value\n    df_dict[\"kurtosis\"] = kurtosis_value\n    df_dict[\"Skewness\"] = skewness_value\n\n    df = pd.DataFrame(df_dict, columns = df_dict.keys(), index = column)\n    display(df)\n\nnumerical_data_analysis(data, numerical_data)","metadata":{"execution":{"iopub.status.busy":"2022-02-17T06:24:57.795398Z","iopub.execute_input":"2022-02-17T06:24:57.796015Z","iopub.status.idle":"2022-02-17T06:24:57.864609Z","shell.execute_reply.started":"2022-02-17T06:24:57.795976Z","shell.execute_reply":"2022-02-17T06:24:57.863467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Observation.\nhere skewness is the important measure which gives the information of our dataset. We can say the data follows normal distribution/not by the skewness value. normal distribution value between [-1,+1] . So. all our data is following the normal distribution more or less.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"distribution-analysis\" ></a>\n### 1.2.3 Distribution Analysis","metadata":{}},{"cell_type":"code","source":"# find the distubution of the data. ( visualization would be so good)\ncolumn = data.columns\nplt.figure\nfig, ax = plt.subplots(3,4 , figsize=(15,15))\n# we have 9 numerical values.\ncol, row = 4,3\ncol_count = 0\nfor r in range(row):\n    for c in range(col):\n        sns.histplot(data=data, x=column[col_count], ax=ax[r,c])\n        col_count +=1\n","metadata":{"execution":{"iopub.status.busy":"2022-02-17T06:24:57.866199Z","iopub.execute_input":"2022-02-17T06:24:57.866431Z","iopub.status.idle":"2022-02-17T06:25:00.668463Z","shell.execute_reply.started":"2022-02-17T06:24:57.866408Z","shell.execute_reply":"2022-02-17T06:25:00.667224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Observation.\nMostly all the features are unifomly distributed. \nAll the categorical values are uniformly distributed --> meaning each class in the categorical data has the same number of samples :)\nnum_old has few outliers and little bit skewed to the right. But since it is a target value we don't need to handle that skeness :)\n\n\nData is more clean on Univariant analysis. No more feature engineering like missing value handling, skewness handling don't need to be done.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"categorical-data\"></a>\n### 1.3 Categorical data.\n\nWe found all the categorical data are uniform --> so don't need cardinality analysis anymore.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"#basic-uderstanding\" ></a>\n## 2. Some basic Understanding about the data","metadata":{}},{"cell_type":"markdown","source":"### 1. sales based on country","metadata":{}},{"cell_type":"code","source":"sns.barplot(data=data, x=\"country\" , y = \"num_sold\")","metadata":{"execution":{"iopub.status.busy":"2022-02-17T06:25:00.670225Z","iopub.execute_input":"2022-02-17T06:25:00.670516Z","iopub.status.idle":"2022-02-17T06:25:01.245925Z","shell.execute_reply.started":"2022-02-17T06:25:00.670483Z","shell.execute_reply":"2022-02-17T06:25:01.24535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Norway has the higest sales.","metadata":{}},{"cell_type":"markdown","source":"### 2. sales on every year ","metadata":{}},{"cell_type":"code","source":"sns.barplot(data=data, x=\"country\" , y = \"num_sold\" , hue=\"year\")\n","metadata":{"execution":{"iopub.status.busy":"2022-02-17T06:25:01.248205Z","iopub.execute_input":"2022-02-17T06:25:01.249297Z","iopub.status.idle":"2022-02-17T06:25:02.113013Z","shell.execute_reply.started":"2022-02-17T06:25:01.249254Z","shell.execute_reply":"2022-02-17T06:25:02.111621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"sales normally increased every year on every county. This is normal year after year people love to buy the things and sales will be increased. ","metadata":{}},{"cell_type":"markdown","source":"### 3. yearly sales beaced on stores","metadata":{}},{"cell_type":"code","source":"sns.barplot(data=data, x=\"country\" , y = \"num_sold\" , hue=\"store\")","metadata":{"execution":{"iopub.status.busy":"2022-02-17T06:25:02.114096Z","iopub.execute_input":"2022-02-17T06:25:02.11464Z","iopub.status.idle":"2022-02-17T06:25:02.853051Z","shell.execute_reply.started":"2022-02-17T06:25:02.114556Z","shell.execute_reply":"2022-02-17T06:25:02.85248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"KaggleRama doing better on all the years\nMore or less 40% high sales KaggleRama has than KaggleMart.\nThis can be because of many reasons --\n- KaggleRama may have large store with lots of item (we don't have the information about the stacks)\n- KaggleRama have more branches than KaggleMart (we don't have that information too)\n- Good quality and service also be a reason.","metadata":{}},{"cell_type":"markdown","source":"### 4. Monthly Store sales","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(15,8))\ngroup_by_data=data.groupby(['date','store']).agg(num_sold=('num_sold','sum'))\nsns.lineplot(data=group_by_data, x= \"date\", y=\"num_sold\", hue=\"store\")","metadata":{"execution":{"iopub.status.busy":"2022-02-17T06:25:02.854211Z","iopub.execute_input":"2022-02-17T06:25:02.854627Z","iopub.status.idle":"2022-02-17T06:25:03.476283Z","shell.execute_reply.started":"2022-02-17T06:25:02.854595Z","shell.execute_reply":"2022-02-17T06:25:03.475084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(5,5))\ngroup_by_data=data.groupby(['month','store']).agg(num_sold=('num_sold','sum'))\nsns.lineplot(data=group_by_data, x= \"month\", y=\"num_sold\", hue=\"store\")","metadata":{"execution":{"iopub.status.busy":"2022-02-17T06:25:03.477742Z","iopub.execute_input":"2022-02-17T06:25:03.478006Z","iopub.status.idle":"2022-02-17T06:25:03.721323Z","shell.execute_reply.started":"2022-02-17T06:25:03.477973Z","shell.execute_reply":"2022-02-17T06:25:03.720428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can clearly see the sales is increased on 3-5th and 12 th month every year at both the stores. <br/>\nWe know the reason this is because of some festivals(seasons) <br/>\nIt is started increasing in a high rate at 11th month and so high on 12th month.(this may be because of chrishmas)","metadata":{}},{"cell_type":"markdown","source":"### 5. Product based sales","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(5,5))\ngroup_by_data=data.groupby(['month','product']).agg(num_sold=('num_sold','sum'))\nsns.lineplot(data=group_by_data, x= \"month\", y=\"num_sold\", hue=\"product\")","metadata":{"execution":{"iopub.status.busy":"2022-02-17T06:25:03.722532Z","iopub.execute_input":"2022-02-17T06:25:03.723226Z","iopub.status.idle":"2022-02-17T06:25:03.929683Z","shell.execute_reply.started":"2022-02-17T06:25:03.723192Z","shell.execute_reply":"2022-02-17T06:25:03.928851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The Kaggle hat has the large sales than other 2. <br/>\nKaggle hat sales increased in the month of 3-6 and 12. all other product's sales not so much increased not so much decresed it is normal. <br/>\nAll product sales decreased in the month of Feb that also needs to be focused (but can't correctly tell what may be the reason)","metadata":{}},{"cell_type":"markdown","source":"### 5.2 Product based on each country sales ","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(3,1 , figsize = (10,12))\ncountry = [\"Norway\", \"Sweden\", \"Finland\"]\nfor row in range(3):\n    # plot the Kaglle Hat in a country.\n    ax[row].plot( data[(data[\"country\"]==country[row]) & (data[\"product\"]==\"Kaggle Hat\")][\"month\"],\n                data[(data[\"country\"]==country[row]) & (data[\"product\"]==\"Kaggle Hat\")][\"num_sold\"], label=\"Kaggle Hat\")\n    ax[row].plot(data[(data[\"country\"]==country[row]) & (data[\"product\"]==\"Kaggle Mug\")][\"month\"],\n                 data[(data[\"country\"]==country[row]) & (data[\"product\"]==\"Kaggle Mug\")][\"num_sold\"], label=\"Kaggle Mug\")\n    ax[row].plot(data[(data[\"country\"]==country[row]) & (data[\"product\"]==\"Kaggle Sticker\")][\"month\"],\n                 data[(data[\"country\"]==country[row]) & (data[\"product\"]==\"Kaggle Sticker\")][\"num_sold\"], label=\"Kaggle Sticker\")\n    ax[row].title.set_text(\"Prodcut based sales on country --> {}\".format(country[row]))\n    ax[row].set_ylabel('Sales')\n    ax[row].set_xlabel('Month')\n    ax[row].legend()","metadata":{"execution":{"iopub.status.busy":"2022-02-17T06:25:03.931034Z","iopub.execute_input":"2022-02-17T06:25:03.931294Z","iopub.status.idle":"2022-02-17T06:25:04.509996Z","shell.execute_reply.started":"2022-02-17T06:25:03.93126Z","shell.execute_reply":"2022-02-17T06:25:04.50919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5.3 product based on stores","metadata":{}},{"cell_type":"code","source":"#g = data.groupby([\"store\", \"product\"]).agg(num_sold=('num_sold','sum'))\nsns.barplot(data=data, x= \"store\", y = \"num_sold\", hue=\"product\")","metadata":{"execution":{"iopub.status.busy":"2022-02-17T06:25:04.511062Z","iopub.execute_input":"2022-02-17T06:25:04.511637Z","iopub.status.idle":"2022-02-17T06:25:05.299889Z","shell.execute_reply.started":"2022-02-17T06:25:04.51161Z","shell.execute_reply":"2022-02-17T06:25:05.299186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 6 which days of month has more sales","metadata":{}},{"cell_type":"code","source":"g = data.groupby(\"day\").sum()\nsns.lineplot(data=data, x= \"day\", y = \"num_sold\", hue=\"product\")","metadata":{"execution":{"iopub.status.busy":"2022-02-17T06:25:05.301071Z","iopub.execute_input":"2022-02-17T06:25:05.301456Z","iopub.status.idle":"2022-02-17T06:25:08.389436Z","shell.execute_reply.started":"2022-02-17T06:25:05.301419Z","shell.execute_reply":"2022-02-17T06:25:08.38868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Interesting people buying little high at month ends :) (can't tell the exact reason for this behaviour)","metadata":{}},{"cell_type":"markdown","source":"### 7. Week days based sales","metadata":{}},{"cell_type":"code","source":"g = data.groupby(\"dayofweek\").sum()\nsns.lineplot(data=data, x= \"dayofweek\", y = \"num_sold\", hue=\"product\")","metadata":{"execution":{"iopub.status.busy":"2022-02-17T06:25:08.39087Z","iopub.execute_input":"2022-02-17T06:25:08.391316Z","iopub.status.idle":"2022-02-17T06:25:09.638958Z","shell.execute_reply.started":"2022-02-17T06:25:08.391273Z","shell.execute_reply":"2022-02-17T06:25:09.638203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Not suprising sales high at weekends . lots of reason for this.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"Correlation-analysis\"></a>\n## 3. Correlation Analysis","metadata":{}},{"cell_type":"code","source":"# can be done by heatmap.\nfig = plt.figure(figsize = (18,12))\nsns.heatmap(data=data.corr(), annot=True, vmin=0, vmax=1,)","metadata":{"execution":{"iopub.status.busy":"2022-02-17T06:25:09.640496Z","iopub.execute_input":"2022-02-17T06:25:09.640777Z","iopub.status.idle":"2022-02-17T06:25:10.545433Z","shell.execute_reply.started":"2022-02-17T06:25:09.640739Z","shell.execute_reply":"2022-02-17T06:25:10.544289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Not a single value strongly correlated to target value. We need to do some thing for this in feature engineering stage","metadata":{}},{"cell_type":"code","source":"# Feature engineering loading ....","metadata":{"execution":{"iopub.status.busy":"2022-02-17T06:25:10.546815Z","iopub.execute_input":"2022-02-17T06:25:10.547036Z","iopub.status.idle":"2022-02-17T06:25:10.5511Z","shell.execute_reply.started":"2022-02-17T06:25:10.547008Z","shell.execute_reply":"2022-02-17T06:25:10.550209Z"},"trusted":true},"execution_count":null,"outputs":[]}]}