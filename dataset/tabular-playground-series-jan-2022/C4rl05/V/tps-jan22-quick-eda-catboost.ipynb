{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ðŸ˜¼ TPS-JAN22, Quick EDA + CatBoost\n\nThe following model is a simple implementation using CATBoost Regressor, </br> I saw in some of the public code available that this model architecture was performing better than XGBoost. \nThe objective is to provide a simple framework and foundation as a baseline for more sophisticated implementations using CATBoost.\n\nBelow the table of content of the notebook...\n1. [Installing & Loading Python Libraries.](#1) -- Install and load everything that's nesesary for the model \n2. [Auxiliary FunctionsAuxiliary Functions.](#2) -- I define a few functions that will be used in the model\n3. [Configuring the Notebook.](#3) -- I set some of the decimals and the default amount of cols and rows\n4. [Importing the Information and Creating a DataFrame.](#4) -- Loading the CSV files into a DataFrame\n5. [Exploring the Loaded Data (DataFrames).](#5) -- Review the information loaded to identify everything is right\n6. [Engineering some Features.](#6) -- I will create functions to build features for the model\n7. [Pre-Processing the Features for Training.](#7) -- Encode the features or apply required transformations\n8. [Identifyting Features for Training.](#8) -- Select the features that I'm going to use in the training steps\n9. [Creates a Simple Train / Validation Strategy.](#9) -- Just training a simple model, mosthly as a baseline\n10. [Train a Simple Model (CATBoost Regressor).](#10) -- \n11. [Train a Simple Model (CATBoost Regressor) using a CV Loop.](#11)\n12. [Model Inference (Submission to Kaggle).](#12)\n\n\n**Data Description** </br>\nFor this challenge, you will be predicting a full year worth of sales for three items at two stores located in three different countries. This dataset is completely fictional, but contains many effects you see in real-world data, e.g., weekend and holiday effect, seasonality, etc. The dataset is small enough to allow you to try numerous different modeling approaches.\n\nGood luck!\n\nThe objective of this competition is the following.\n\n**Objective** </br>\nUsing 2015 - 2018, predict the sales by date, country, store, and product for 2019.\n\n**Strategy** </br>\nBecause we are dealing with a time series type of estimation, we need to hide future information from the model; in this simple approach we will use as validation all the data from 2018, so we will train the model with data from 2015-2017\n\n**Update 01/02/2021**\n* Developed a simple Notebook, Quick EDA + Simple Feature Engineering.\n* Cross-Validation strategy based on a fixed date.\n* Cross-Validation strategy base on Time Series Kfold.\n\n**Ideas that I want to implement**\n* Incorporate some type of Lag Features for the Model.","metadata":{}},{"cell_type":"markdown","source":"<a name=\"1\"></a>\n# 1. Installing & Loading Python Libraries. ","metadata":{}},{"cell_type":"code","source":"# Installing a library to utilize the holidays as a feature\n!pip install holidays","metadata":{"execution":{"iopub.status.busy":"2022-01-11T05:58:30.927986Z","iopub.execute_input":"2022-01-11T05:58:30.928382Z","iopub.status.idle":"2022-01-11T05:58:39.528235Z","shell.execute_reply.started":"2022-01-11T05:58:30.92828Z","shell.execute_reply":"2022-01-11T05:58:39.527021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-11T05:58:39.531085Z","iopub.execute_input":"2022-01-11T05:58:39.531461Z","iopub.status.idle":"2022-01-11T05:58:39.542571Z","shell.execute_reply.started":"2022-01-11T05:58:39.531416Z","shell.execute_reply":"2022-01-11T05:58:39.541763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Import LGBM Regressor Model.\n\nfrom catboost import CatBoostRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold, TimeSeriesSplit\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\n\nimport holidays","metadata":{"execution":{"iopub.status.busy":"2022-01-11T05:58:39.544183Z","iopub.execute_input":"2022-01-11T05:58:39.544422Z","iopub.status.idle":"2022-01-11T05:58:40.940122Z","shell.execute_reply.started":"2022-01-11T05:58:39.544395Z","shell.execute_reply":"2022-01-11T05:58:40.939282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a name=\"2\"></a>\n# 2. Auxiliary Functions","metadata":{}},{"cell_type":"code","source":"# Define a function to measure the model performance.\ndef SMAPE(y_true, y_pred):\n    denominator = (y_true + np.abs(y_pred)) / 200.0\n    diff = np.abs(y_true - y_pred) / denominator\n    diff[denominator == 0] = 0.0\n    return np.mean(diff)","metadata":{"execution":{"iopub.status.busy":"2022-01-11T05:58:40.942414Z","iopub.execute_input":"2022-01-11T05:58:40.942882Z","iopub.status.idle":"2022-01-11T05:58:40.948667Z","shell.execute_reply.started":"2022-01-11T05:58:40.942839Z","shell.execute_reply":"2022-01-11T05:58:40.947883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a name=\"3\"></a>\n# 3. Configuring the Notebook.","metadata":{}},{"cell_type":"code","source":"%%time\n# I like to disable my Notebook Warnings.\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2022-01-11T05:58:40.949794Z","iopub.execute_input":"2022-01-11T05:58:40.950148Z","iopub.status.idle":"2022-01-11T05:58:40.962334Z","shell.execute_reply.started":"2022-01-11T05:58:40.950108Z","shell.execute_reply":"2022-01-11T05:58:40.961062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Configure notebook display settings to only use 2 decimal places, tables look nicer.\npd.options.display.float_format = '{:,.2f}'.format\npd.set_option('display.max_columns', 15) \npd.set_option('display.max_rows', 50)","metadata":{"execution":{"iopub.status.busy":"2022-01-11T05:58:40.963586Z","iopub.execute_input":"2022-01-11T05:58:40.96391Z","iopub.status.idle":"2022-01-11T05:58:40.976709Z","shell.execute_reply.started":"2022-01-11T05:58:40.963862Z","shell.execute_reply":"2022-01-11T05:58:40.975942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Configure notebook display settings to only use 2 decimal places, tables look nicer.\npd.options.display.float_format = '{:,.2f}'.format\npd.set_option('display.max_columns', 15) \npd.set_option('display.max_rows', 50)","metadata":{"execution":{"iopub.status.busy":"2022-01-11T05:58:40.978166Z","iopub.execute_input":"2022-01-11T05:58:40.97856Z","iopub.status.idle":"2022-01-11T05:58:40.9905Z","shell.execute_reply.started":"2022-01-11T05:58:40.978518Z","shell.execute_reply":"2022-01-11T05:58:40.989598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a name=\"4\"></a>\n# 4. Importing the Information and Creating a DataFrame.\n...","metadata":{}},{"cell_type":"code","source":"%%time\n# Define the datasets locations...\n\nTRN_PATH = '/kaggle/input/tabular-playground-series-jan-2022/train.csv'\nTST_PATH = '/kaggle/input/tabular-playground-series-jan-2022/test.csv'\nSUB_PATH = '/kaggle/input/tabular-playground-series-jan-2022/sample_submission.csv'","metadata":{"execution":{"iopub.status.busy":"2022-01-11T05:58:40.991731Z","iopub.execute_input":"2022-01-11T05:58:40.991942Z","iopub.status.idle":"2022-01-11T05:58:41.00332Z","shell.execute_reply.started":"2022-01-11T05:58:40.991915Z","shell.execute_reply":"2022-01-11T05:58:41.002428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Read the datasets and create dataframes...\n\ntrain_df = pd.read_csv(TRN_PATH)\ntest_df = pd.read_csv(TST_PATH)\nsubmission_df = pd.read_csv(SUB_PATH)","metadata":{"execution":{"iopub.status.busy":"2022-01-11T05:58:41.006349Z","iopub.execute_input":"2022-01-11T05:58:41.006796Z","iopub.status.idle":"2022-01-11T05:58:41.086673Z","shell.execute_reply.started":"2022-01-11T05:58:41.006766Z","shell.execute_reply":"2022-01-11T05:58:41.086065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a name=\"5\"></a>\n# 5. Exploring the Loaded Data (DataFrames).","metadata":{}},{"cell_type":"code","source":"%%time\n# Explore the size of the dataset loaded...\n\ntrain_df.info()","metadata":{"execution":{"iopub.status.busy":"2022-01-11T05:58:41.088948Z","iopub.execute_input":"2022-01-11T05:58:41.089176Z","iopub.status.idle":"2022-01-11T05:58:41.119398Z","shell.execute_reply.started":"2022-01-11T05:58:41.089147Z","shell.execute_reply":"2022-01-11T05:58:41.118462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Explore the first 5 rows to have an idea what we are dealing with...\n\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-11T05:58:41.120979Z","iopub.execute_input":"2022-01-11T05:58:41.1213Z","iopub.status.idle":"2022-01-11T05:58:41.140804Z","shell.execute_reply.started":"2022-01-11T05:58:41.121256Z","shell.execute_reply":"2022-01-11T05:58:41.140262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Explore the size of the dataset loaded...\n\ntest_df.info()","metadata":{"execution":{"iopub.status.busy":"2022-01-11T05:58:41.142072Z","iopub.execute_input":"2022-01-11T05:58:41.142516Z","iopub.status.idle":"2022-01-11T05:58:41.156034Z","shell.execute_reply.started":"2022-01-11T05:58:41.142484Z","shell.execute_reply":"2022-01-11T05:58:41.15531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Explore the first 5 rows to have an idea what we are dealing with, in this case the Test Set...\n\ntest_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-11T05:58:41.157457Z","iopub.execute_input":"2022-01-11T05:58:41.15777Z","iopub.status.idle":"2022-01-11T05:58:41.170333Z","shell.execute_reply.started":"2022-01-11T05:58:41.157729Z","shell.execute_reply":"2022-01-11T05:58:41.169563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Review some information for the categorical variables...\n\ncountry_list = train_df['country'].unique()\nstore_list = train_df['store'].unique()\nproduct_list = train_df['product'].unique()\n\nprint(f'Country List:{country_list}')\nprint(f'Store List:{store_list}')\nprint(f'Product List:{product_list}')","metadata":{"execution":{"iopub.status.busy":"2022-01-11T05:58:41.171415Z","iopub.execute_input":"2022-01-11T05:58:41.172108Z","iopub.status.idle":"2022-01-11T05:58:41.188627Z","shell.execute_reply.started":"2022-01-11T05:58:41.172073Z","shell.execute_reply":"2022-01-11T05:58:41.187669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Review if there is missing information in the dataset...\n\ntrain_df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-01-11T05:58:41.189831Z","iopub.execute_input":"2022-01-11T05:58:41.190743Z","iopub.status.idle":"2022-01-11T05:58:41.204945Z","shell.execute_reply.started":"2022-01-11T05:58:41.190707Z","shell.execute_reply":"2022-01-11T05:58:41.204162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a simple function to evaluate the time-ranges of the information provided.\n# It will help with the train / validation separations\n\ndef evaluate_time(df):\n    min_date = df['date'].min()\n    max_date = df['date'].max()\n    print(f'Min Date: {min_date} /  Max Date: {max_date}')\n    return None\n\nevaluate_time(train_df)\nevaluate_time(test_df)","metadata":{"execution":{"iopub.status.busy":"2022-01-11T05:58:41.206106Z","iopub.execute_input":"2022-01-11T05:58:41.206669Z","iopub.status.idle":"2022-01-11T05:58:41.219757Z","shell.execute_reply.started":"2022-01-11T05:58:41.206626Z","shell.execute_reply":"2022-01-11T05:58:41.219061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Average Sales / Year (Help me to understand if there is some upward trend.)\ntrain_df['date'] = pd.to_datetime(train_df['date']) # Convert the date to datetime.\ntrain_df['year'] = train_df['date'].dt.year\nsummary = train_df.groupby(['country', 'year'])['num_sold'].mean()\nsummary","metadata":{"execution":{"iopub.status.busy":"2022-01-11T05:58:41.221124Z","iopub.execute_input":"2022-01-11T05:58:41.221791Z","iopub.status.idle":"2022-01-11T05:58:41.254851Z","shell.execute_reply.started":"2022-01-11T05:58:41.221743Z","shell.execute_reply":"2022-01-11T05:58:41.254093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a name=\"6\"></a>\n# 6. Engineering some Features.","metadata":{}},{"cell_type":"code","source":"# Define the model Target for future reference.\nTARGET = 'num_sold'","metadata":{"execution":{"iopub.status.busy":"2022-01-11T05:58:41.255928Z","iopub.execute_input":"2022-01-11T05:58:41.25635Z","iopub.status.idle":"2022-01-11T05:58:41.260543Z","shell.execute_reply.started":"2022-01-11T05:58:41.256317Z","shell.execute_reply":"2022-01-11T05:58:41.259631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Country List:['Finland' 'Norway' 'Sweden']\nholiday_FI = holidays.CountryHoliday('FI', years=[2015, 2016, 2017, 2018, 2019])\nholiday_NO = holidays.CountryHoliday('NO', years=[2015, 2016, 2017, 2018, 2019])\nholiday_SE = holidays.CountryHoliday('SE', years=[2015, 2016, 2017, 2018, 2019])\n\nholiday_dict = holiday_FI.copy()\nholiday_dict.update(holiday_NO)\nholiday_dict.update(holiday_SE)\n\ntrain_df['date'] = pd.to_datetime(train_df['date']) # Convert the date to datetime.\ntrain_df['holiday_name'] = train_df['date'].map(holiday_dict)\ntrain_df['is_holiday'] = np.where(train_df['holiday_name'].notnull(), 1, 0)\ntrain_df['holiday_name'] = train_df['holiday_name'].fillna('Not Holiday')\n\ntest_df['date'] = pd.to_datetime(test_df['date']) # Convert the date to datetime.\ntest_df['holiday_name'] = test_df['date'].map(holiday_dict)\ntest_df['is_holiday'] = np.where(test_df['holiday_name'].notnull(), 1, 0)\ntest_df['holiday_name'] = test_df['holiday_name'].fillna('Not Holiday')","metadata":{"execution":{"iopub.status.busy":"2022-01-11T05:58:41.262246Z","iopub.execute_input":"2022-01-11T05:58:41.262747Z","iopub.status.idle":"2022-01-11T05:58:41.305165Z","shell.execute_reply.started":"2022-01-11T05:58:41.262704Z","shell.execute_reply":"2022-01-11T05:58:41.304494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create some simple features base on the Date field...\n\ndef create_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Create features base on the date variable, the idea is to extract as much \n    information from the date componets.\n    Args\n        df: Input data to create the features.\n    Returns\n        df: A DataFrame with the new time base features.\n    \"\"\"\n    \n    df['date'] = pd.to_datetime(df['date']) # Convert the date to datetime.\n    \n    # Start the creating future process.\n    df['year'] = df['date'].dt.year\n    df['quarter'] = df['date'].dt.quarter\n    df['month'] = df['date'].dt.month\n    df['day'] = df['date'].dt.day\n    df['dayofweek'] = df['date'].dt.dayofweek\n    df['dayofmonth'] = df['date'].dt.days_in_month\n    df['dayofyear'] = df['date'].dt.dayofyear\n    df['weekofyear'] = df['date'].dt.weekofyear\n    df['weekday'] = df['date'].dt.weekday\n    df['is_weekend'] = np.where((df['weekday'] == 5) | (df['weekday'] == 6), 1, 0)\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2022-01-11T05:58:41.306304Z","iopub.execute_input":"2022-01-11T05:58:41.306762Z","iopub.status.idle":"2022-01-11T05:58:41.315363Z","shell.execute_reply.started":"2022-01-11T05:58:41.30672Z","shell.execute_reply":"2022-01-11T05:58:41.314446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Apply the function 'create_time_features' to the dataset...\ntrain_df = create_time_features(train_df)\ntest_df = create_time_features(test_df)","metadata":{"execution":{"iopub.status.busy":"2022-01-11T05:58:41.316698Z","iopub.execute_input":"2022-01-11T05:58:41.317252Z","iopub.status.idle":"2022-01-11T05:58:41.40477Z","shell.execute_reply.started":"2022-01-11T05:58:41.317186Z","shell.execute_reply":"2022-01-11T05:58:41.403954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a name=\"7\"></a>\n# 7. Pre-Processing the Features for Training.","metadata":{}},{"cell_type":"code","source":"# Convert the Categorical variables to one-hoe encoded features...\n# It will help in the training process\n\nCATEGORICAL = ['country', 'store', 'product', 'holiday_name']\ndef create_one_hot(df, categ_colums = CATEGORICAL):\n    \"\"\"\n    Creates one_hot encoded fields for the specified categorical columns...\n    Args\n        df\n        categ_colums\n    Returns\n        df\n    \"\"\"\n    df = pd.get_dummies(df, columns=CATEGORICAL)\n    return df\n\n\ndef encode_categ_features(df, categ_colums = CATEGORICAL):\n    \"\"\"\n    Use the label encoder to encode categorical features...\n    Args\n        df\n        categ_colums\n    Returns\n        df\n    \"\"\"\n    le = LabelEncoder()\n    for col in categ_colums:\n        df['enc_'+col] = le.fit_transform(df[col])\n    return df\n\ntrain_df = encode_categ_features(train_df)\ntest_df = encode_categ_features(test_df)","metadata":{"execution":{"iopub.status.busy":"2022-01-11T05:58:41.405884Z","iopub.execute_input":"2022-01-11T05:58:41.406114Z","iopub.status.idle":"2022-01-11T05:58:41.440477Z","shell.execute_reply.started":"2022-01-11T05:58:41.406087Z","shell.execute_reply":"2022-01-11T05:58:41.439493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_log_target(df, taget = TARGET):\n    \"\"\"\n    Apply a log transformation to the target for better optimization \n    during training.\n    \"\"\"\n    df[TARGET] = np.log(df[TARGET])\n    return df\n\n#train_df = create_log_target(train_df, TARGET)","metadata":{"execution":{"iopub.status.busy":"2022-01-11T05:58:41.441945Z","iopub.execute_input":"2022-01-11T05:58:41.44217Z","iopub.status.idle":"2022-01-11T05:58:41.446377Z","shell.execute_reply.started":"2022-01-11T05:58:41.442143Z","shell.execute_reply":"2022-01-11T05:58:41.445452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['num_sold'].describe()","metadata":{"execution":{"iopub.status.busy":"2022-01-11T05:58:41.447645Z","iopub.execute_input":"2022-01-11T05:58:41.448327Z","iopub.status.idle":"2022-01-11T05:58:41.473516Z","shell.execute_reply.started":"2022-01-11T05:58:41.448281Z","shell.execute_reply":"2022-01-11T05:58:41.472469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a name=\"8\"></a>\n# 8. Identifyting Features for Training.","metadata":{}},{"cell_type":"code","source":"# Extract features and avoid certain columns from the dataframe for training purposes...\navoid = ['row_id', 'date', 'num_sold']\nFEATURES = [feat for feat in train_df.columns if feat not in avoid]\n\n# Print a list of all the features created...\nprint(FEATURES)","metadata":{"execution":{"iopub.status.busy":"2022-01-11T05:58:41.474898Z","iopub.execute_input":"2022-01-11T05:58:41.475273Z","iopub.status.idle":"2022-01-11T05:58:41.482035Z","shell.execute_reply.started":"2022-01-11T05:58:41.475226Z","shell.execute_reply":"2022-01-11T05:58:41.481186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Selecting Features....\nprint(FEATURES)","metadata":{"execution":{"iopub.status.busy":"2022-01-11T05:58:41.483346Z","iopub.execute_input":"2022-01-11T05:58:41.484012Z","iopub.status.idle":"2022-01-11T05:58:41.498595Z","shell.execute_reply.started":"2022-01-11T05:58:41.483966Z","shell.execute_reply":"2022-01-11T05:58:41.497272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FEATURES = [\n            #'country',\n            #'store',\n            #'product',\n            #'holiday_name',\n            'is_holiday',\n            'year',\n            #'quarter',\n            'month',\n            'day',\n            'dayofweek',\n            'dayofmonth',\n            'dayofyear',\n            'weekofyear',\n            'weekday',\n            #'is_weekend',\n            'enc_country',\n            'enc_store',\n            'enc_product',\n            #'enc_holiday_name'\n            ]","metadata":{"execution":{"iopub.status.busy":"2022-01-11T05:58:41.499927Z","iopub.execute_input":"2022-01-11T05:58:41.500616Z","iopub.status.idle":"2022-01-11T05:58:41.510591Z","shell.execute_reply.started":"2022-01-11T05:58:41.500573Z","shell.execute_reply":"2022-01-11T05:58:41.509645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a name=\"9\"></a>\n# 9. Creates a Simple Train / Validation Strategy.","metadata":{}},{"cell_type":"code","source":"# Creates the Train and Validation sets to train the model...\n# Define a cutoff date to split the datasets\nCUTOFF_DATE = '2018-01-01'\n\n# Split the data into train and validation datasets using timestamp best suited for timeseries...\nX_train = train_df[train_df['date'] < CUTOFF_DATE][FEATURES]\ny_train = train_df[train_df['date'] < CUTOFF_DATE][TARGET]\n\nX_val = train_df[train_df['date'] >= CUTOFF_DATE][FEATURES]\ny_val = train_df[train_df['date'] >= CUTOFF_DATE][TARGET]","metadata":{"execution":{"iopub.status.busy":"2022-01-11T05:58:41.5153Z","iopub.execute_input":"2022-01-11T05:58:41.515837Z","iopub.status.idle":"2022-01-11T05:58:41.549146Z","shell.execute_reply.started":"2022-01-11T05:58:41.515774Z","shell.execute_reply":"2022-01-11T05:58:41.548266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a name=\"10\"></a>\n# 10. Train a Simple Model (CATBoost Regressor).","metadata":{}},{"cell_type":"code","source":"# Defines a really simple XGBoost Regressor...\n\ncatboost_params = {'n_estimators': 20_000}\n\n# Create an instance of the XGBRegressor and set the model parameters...\ncbr = CatBoostRegressor(**catboost_params)\n\n# Train the XGBRegressor using the train and validation datasets, \n# Utilizes early_stopping_rounds to control overfitting...\ncbr.fit(X_train,\n        y_train,\n        eval_set=[(X_val, y_val)],\n        early_stopping_rounds = 250,\n        verbose = 500)","metadata":{"execution":{"iopub.status.busy":"2022-01-11T05:58:41.550307Z","iopub.execute_input":"2022-01-11T05:58:41.550528Z","iopub.status.idle":"2022-01-11T05:58:44.73835Z","shell.execute_reply.started":"2022-01-11T05:58:41.550502Z","shell.execute_reply":"2022-01-11T05:58:44.737522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Use the model to predict on the validation set..\nval_pred = cbr.predict(X_val)\n\n# Convert the target back from logarictic transformation.\nval_pred = val_pred\ny_val = y_val\n\n#val_pred = np.exp(val_pred)\n#y_val = np.exp(y_val)\n\nscore = np.sqrt(mean_squared_error(y_val, val_pred))\nprint(f'RMSE: {score} / SMAPE: {SMAPE(y_val, val_pred)}')","metadata":{"execution":{"iopub.status.busy":"2022-01-11T05:58:44.739781Z","iopub.execute_input":"2022-01-11T05:58:44.740052Z","iopub.status.idle":"2022-01-11T05:58:44.761637Z","shell.execute_reply.started":"2022-01-11T05:58:44.740012Z","shell.execute_reply":"2022-01-11T05:58:44.760776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a name=\"11\"></a>\n# 11. Train a Simple Model (CATBoost Regressor) using a CV Loop. ","metadata":{}},{"cell_type":"code","source":"%%time\nN_SPLITS = 20\nEARLY_STOPPING_ROUNDS = 1500 # Will stop training if one metric of one validation data doesnâ€™t improve in last round\nVERBOSE = 0 # Controls the level of information, verbosity","metadata":{"execution":{"iopub.status.busy":"2022-01-11T05:58:44.7627Z","iopub.execute_input":"2022-01-11T05:58:44.762926Z","iopub.status.idle":"2022-01-11T05:58:44.768167Z","shell.execute_reply.started":"2022-01-11T05:58:44.762898Z","shell.execute_reply":"2022-01-11T05:58:44.767391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Cross Validation Loop for the Classifier.\ndef cross_validation_train(train, labels, test, model, model_params, n_folds = 10):\n    \"\"\"\n    The following function is responsable of training a model in a\n    cross validation loop and generate predictions on the specified test set.\n    The function provides the model feature importance list as other variables.\n\n    Args:\n    train  (Dataframe): ...\n    labels (Series): ...\n    test   (Dataframe): ...\n    model  (Model): ...\n    model_params (dict of str: int): ...\n\n    Return:\n    classifier  (Model): ...\n    feat_import (Dataframe): ...\n    test_pred   (Dataframe): ...\n    ...\n\n    \"\"\"\n    # Creates empty place holders for out of fold and test predictions.\n    oof_pred  = np.zeros(len(train)) # We are predicting prob. we need more dimensions.\n    oof_label = np.zeros(len(train))\n    test_pred = np.zeros(len(test)) # We are predicting prob. we need more dimensions\n    test_pred_array = []\n    val_indexes_used = []\n    \n    # Creates empty place holder for the feature importance.\n    feat_import = np.zeros(len(FEATURES))\n    \n    # Creates Stratified Kfold object to be used in the train / validation\n    # phase of the model.\n    Kf = TimeSeriesSplit(n_splits = n_folds)\n    \n    # Start the training and validation loops.\n    for fold, (train_idx, val_idx) in enumerate(Kf.split(train)):\n        # Creates the index for each fold\n        print(f'Fold: {fold+1}')        \n        train_min_date = train_df.iloc[train_idx]['date'].min()\n        train_max_date = train_df.iloc[train_idx]['date'].max()\n        \n        valid_min_date = train_df.iloc[val_idx]['date'].min()\n        valid_max_date = train_df.iloc[val_idx]['date'].max()\n        \n        print(f'Train Min / Max Dates: {train_min_date} / {train_max_date}')\n        print(f'Valid Min / Max Dates: {valid_min_date} / {valid_max_date}')\n\n        print(f'Training on {train_df.iloc[train_idx].shape[0]} Records')\n        print(f'Validating on {train_df.iloc[val_idx].shape[0]} Records')\n        \n        # Generates the Fold. Train and Validation datasets\n        X_trn, y_trn = train.iloc[train_idx], labels.iloc[train_idx]\n        X_val, y_val = train.iloc[val_idx], labels.iloc[val_idx]\n        \n        val_indexes_used = np.concatenate((val_indexes_used, val_idx), axis=None)\n        \n        # Instanciate a classifier based on the model parameters\n        regressor = model(**model_params)\n \n        regressor.fit(X_trn, \n                      y_trn, \n                      eval_set = [(X_val, y_val)], \n                      early_stopping_rounds = EARLY_STOPPING_ROUNDS, \n                      verbose = VERBOSE)\n        \n        # Generate predictions using the trained model\n        val_pred = regressor.predict(X_val)\n        oof_pred[val_idx]  = val_pred # store the predictions for that fold.\n        oof_label[val_idx] = y_val # store the true labels for that fold.\n\n        # Calculate the model error based on the selected metric\n        error =  np.sqrt(mean_squared_error(y_val, val_pred))\n        #error =  np.sqrt(mean_squared_error(np.exp(y_val), np.exp(val_pred)))\n\n        \n        # Print some of the model performance metrics\n        print(f'RMSE: {error}')\n        print(f'SMAPE: {SMAPE(y_val, val_pred)}')\n        #print(f'SMAPE: {SMAPE(np.exp(y_val), np.exp(val_pred))}')\n                        \n        print(\".\"*50)\n\n        # Populate the feature importance matrix\n        feat_import += regressor.feature_importances_\n\n        # Generate predictions for the test set\n        test_pred += (regressor.predict(test)) / n_folds\n        test_pred_array.append(regressor.predict(test))\n                        \n    # Calculate the error across all the folds and print the reuslts\n    val_indexes_used = val_indexes_used.astype(int)\n    global_error = np.sqrt(mean_squared_error(labels.iloc[val_indexes_used], oof_pred[val_indexes_used]))\n    #global_error = np.sqrt(mean_squared_error(np.exp(labels.iloc[val_indexes_used]), np.exp(oof_pred[val_indexes_used])))\n    \n    print('')\n    print(f'RMSE: {global_error}...')\n    print(f'SMAPE: {SMAPE(labels.iloc[val_indexes_used], oof_pred[val_indexes_used])}...')\n    #print(f'SMAPE: {SMAPE(np.exp(labels.iloc[val_indexes_used]), np.exp(oof_pred[val_indexes_used]))}...')\n\n                           \n    return regressor, feat_import, test_pred, oof_label, oof_pred, test_pred_array","metadata":{"execution":{"iopub.status.busy":"2022-01-11T05:58:44.769946Z","iopub.execute_input":"2022-01-11T05:58:44.770321Z","iopub.status.idle":"2022-01-11T05:58:44.78743Z","shell.execute_reply.started":"2022-01-11T05:58:44.770289Z","shell.execute_reply":"2022-01-11T05:58:44.786522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Uses the cross_validation_train to build and train the model with XGBoost\ncbr, ft_imp, pred, oof_label, oof_pred, pred_arr = cross_validation_train(train  = train_df[FEATURES], \n                                                                           labels = train_df[TARGET], \n                                                                           test   = test_df[FEATURES], \n                                                                           model  = CatBoostRegressor, \n                                                                           model_params = catboost_params,\n                                                                           n_folds = N_SPLITS\n                                                                           )","metadata":{"execution":{"iopub.status.busy":"2022-01-11T05:58:44.788973Z","iopub.execute_input":"2022-01-11T05:58:44.789478Z","iopub.status.idle":"2022-01-11T06:00:43.037902Z","shell.execute_reply.started":"2022-01-11T05:58:44.789439Z","shell.execute_reply":"2022-01-11T06:00:43.036296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Plain CATBoost Model </br>\nRMSE: 69.09856619718157...\nSMAPE: 9.879218782588355...\nCPU times: user 6min 1s, sys: 42.9 s, total: 6min 44s\nWall time: 1min 59s\n\nPlain CATBoost Model / Log Target </br>\nRMSE: 73.75406067872883...\nSMAPE: 8.18583587943846...\nCPU times: user 4min 6s, sys: 30.6 s, total: 4min 36s\nWall time: 1min 22s\n\nPlain CATBoost Model, Added is_weekend </br>\nRMSE: 68.89177386350742...\nSMAPE: 9.753530743531984...\nCPU times: user 5min 55s, sys: 41.7 s, total: 6min 37s\nWall time: 1min 57s","metadata":{}},{"cell_type":"markdown","source":"<a name=\"12\"></a>\n# 12. Model Inference (Submission to Kaggle).","metadata":{}},{"cell_type":"code","source":"# Use the created model to predict the sales for 2019...\nsubmission_df['num_sold'] = pred\nsubmission_df['num_sold'] = submission_df['num_sold'].apply(np.ceil)\n#submission_df['num_sold'] = np.exp(pred)\n\n# Creates a submission file for Kaggle...\nsubmission_df.to_csv('submission.csv',index=False)\n\n# Print some of the submisson\nsubmission_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-11T06:00:43.039057Z","iopub.execute_input":"2022-01-11T06:00:43.039277Z","iopub.status.idle":"2022-01-11T06:00:43.065122Z","shell.execute_reply.started":"2022-01-11T06:00:43.039251Z","shell.execute_reply":"2022-01-11T06:00:43.064173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}