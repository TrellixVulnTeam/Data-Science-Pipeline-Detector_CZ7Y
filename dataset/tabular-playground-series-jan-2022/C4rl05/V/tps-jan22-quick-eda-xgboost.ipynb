{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# üéÅ TPS-JAN22, Quick EDA + XGBoost\nThe following model is a simple implementation using XGBoost. The objective is to provide a simple framework and foundation as a baseline for more sophisticated implementations.\nThe objective of this competition is the following.\n\n1. [Loading Python Libraries.](#1)\n2. [Loading CSV and Creating Dataframes.](#2)\n3. [Exploring the Dataframes, (Size, Stats, Nulls and Others).](#3)\n4. [Feature Engineering.](#4)\n5. [Processing the Datasets for Training.](#5)\n6. [Creates a Simple Train / Validation Strategy](#6)\n7. [Train a Simple Model (XGBoost Regressor)](#7)\n8. [Train a Simple Model (XGBoost Regressor) using a CV Loop](#8)\n9. [Model Inference (Submission to Kaggle)](#9)\n\n\n**Data Description** </br>\nFor this challenge, you will be predicting a full year worth of sales for three items at two stores located in three different countries. This dataset is completely fictional, but contains many effects you see in real-world data, e.g., weekend and holiday effect, seasonality, etc. The dataset is small enough to allow you to try numerous different modeling approaches.\n\nGood luck!\n\n\n\n**Objective** </br>\nUsing 2015 - 2018, predict the sales by date, country, store, and product for 2019.\n\n**Strategy** </br>\nBecause we are dealing with a time series type of estimation, we need to hide future information from the model; in this simple approach we will use as validation all the data from 2018, so we will train the model with data from 2015-2017\n\n**Update 12/31/2021**\n* Developed a simple Notebook, Quick EDA + Simple Feature Engineering.\n* Cross-Validation strategy based on a fixed date.\n\n**Update 01/01/2021**\n* Added Cross-Validation loop to the model.\n* Added new features, to identify weekends.\n* Added a proper table of contents.\n* Added features based on Holidays for each of the countries.\n\n**Update 01/02/2021**\n* Improved the CV training function to calculate the SMOTE properly.\n\n**Ideas that I want to implement**\n* New features based on trends.\n\n","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"code","source":"!pip install holidays","metadata":{"execution":{"iopub.status.busy":"2022-01-02T20:24:52.019026Z","iopub.execute_input":"2022-01-02T20:24:52.019663Z","iopub.status.idle":"2022-01-02T20:25:01.208009Z","shell.execute_reply.started":"2022-01-02T20:24:52.019611Z","shell.execute_reply":"2022-01-02T20:25:01.206912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a name=\"1\"></a>\n# Loading Python Libraries. ","metadata":{}},{"cell_type":"code","source":"%%time\n#This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-02T20:25:01.210879Z","iopub.execute_input":"2022-01-02T20:25:01.211242Z","iopub.status.idle":"2022-01-02T20:25:01.222641Z","shell.execute_reply.started":"2022-01-02T20:25:01.2112Z","shell.execute_reply":"2022-01-02T20:25:01.221628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Import LGBM Regressor Model...\n\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold, TimeSeriesSplit\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\n\nimport holidays","metadata":{"execution":{"iopub.status.busy":"2022-01-02T20:25:01.224293Z","iopub.execute_input":"2022-01-02T20:25:01.224642Z","iopub.status.idle":"2022-01-02T20:25:01.742005Z","shell.execute_reply.started":"2022-01-02T20:25:01.224597Z","shell.execute_reply":"2022-01-02T20:25:01.740858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# I like to disable my Notebook Warnings.\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2022-01-02T20:25:01.743587Z","iopub.execute_input":"2022-01-02T20:25:01.744259Z","iopub.status.idle":"2022-01-02T20:25:01.751121Z","shell.execute_reply.started":"2022-01-02T20:25:01.7442Z","shell.execute_reply":"2022-01-02T20:25:01.74983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Configure notebook display settings to only use 2 decimal places, tables look nicer.\npd.options.display.float_format = '{:,.2f}'.format\npd.set_option('display.max_columns', 15) \npd.set_option('display.max_rows', 50)","metadata":{"execution":{"iopub.status.busy":"2022-01-02T20:25:01.754334Z","iopub.execute_input":"2022-01-02T20:25:01.75548Z","iopub.status.idle":"2022-01-02T20:25:01.766065Z","shell.execute_reply.started":"2022-01-02T20:25:01.755429Z","shell.execute_reply":"2022-01-02T20:25:01.764986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Define some of the notebook parameters for future experiment replication.\nSEED   = 42","metadata":{"execution":{"iopub.status.busy":"2022-01-02T20:25:01.768432Z","iopub.execute_input":"2022-01-02T20:25:01.768845Z","iopub.status.idle":"2022-01-02T20:25:01.77758Z","shell.execute_reply.started":"2022-01-02T20:25:01.768799Z","shell.execute_reply":"2022-01-02T20:25:01.776196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# Loading CSV and Creating Dataframes. <a name=\"2\"></a>","metadata":{}},{"cell_type":"code","source":"%%time\n# Define the datasets locations...\n\nTRN_PATH = '/kaggle/input/tabular-playground-series-jan-2022/train.csv'\nTST_PATH = '/kaggle/input/tabular-playground-series-jan-2022/test.csv'\nSUB_PATH = '/kaggle/input/tabular-playground-series-jan-2022/sample_submission.csv'","metadata":{"execution":{"iopub.status.busy":"2022-01-02T20:25:01.779793Z","iopub.execute_input":"2022-01-02T20:25:01.780242Z","iopub.status.idle":"2022-01-02T20:25:01.788259Z","shell.execute_reply.started":"2022-01-02T20:25:01.780195Z","shell.execute_reply":"2022-01-02T20:25:01.787179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Read the datasets and create dataframes...\n\ntrain_df = pd.read_csv(TRN_PATH)\ntest_df = pd.read_csv(TST_PATH)\nsubmission_df = pd.read_csv(SUB_PATH)","metadata":{"execution":{"iopub.status.busy":"2022-01-02T20:25:01.789906Z","iopub.execute_input":"2022-01-02T20:25:01.791199Z","iopub.status.idle":"2022-01-02T20:25:01.845954Z","shell.execute_reply.started":"2022-01-02T20:25:01.791003Z","shell.execute_reply":"2022-01-02T20:25:01.844851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# Exploring the Dataframes, (Size, Stats, Nulls and Others) <a name=\"3\"></a>","metadata":{}},{"cell_type":"code","source":"%%time\n# Explore the size of the dataset loaded...\n\ntrain_df.info()","metadata":{"execution":{"iopub.status.busy":"2022-01-02T20:25:01.847354Z","iopub.execute_input":"2022-01-02T20:25:01.84769Z","iopub.status.idle":"2022-01-02T20:25:01.876206Z","shell.execute_reply.started":"2022-01-02T20:25:01.847647Z","shell.execute_reply":"2022-01-02T20:25:01.875184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Explore the first 5 rows to have an idea what we are dealing with...\n\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-02T20:25:01.877801Z","iopub.execute_input":"2022-01-02T20:25:01.878337Z","iopub.status.idle":"2022-01-02T20:25:01.900321Z","shell.execute_reply.started":"2022-01-02T20:25:01.878295Z","shell.execute_reply":"2022-01-02T20:25:01.899324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Explore the size of the dataset loaded...\n\ntest_df.info()","metadata":{"execution":{"iopub.status.busy":"2022-01-02T20:25:01.901962Z","iopub.execute_input":"2022-01-02T20:25:01.902535Z","iopub.status.idle":"2022-01-02T20:25:01.920503Z","shell.execute_reply.started":"2022-01-02T20:25:01.90249Z","shell.execute_reply":"2022-01-02T20:25:01.919325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Explore the first 5 rows to have an idea what we are dealing with, in this case the Test Set...\n\ntest_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-02T20:25:01.922682Z","iopub.execute_input":"2022-01-02T20:25:01.923254Z","iopub.status.idle":"2022-01-02T20:25:01.941981Z","shell.execute_reply.started":"2022-01-02T20:25:01.923003Z","shell.execute_reply":"2022-01-02T20:25:01.940808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Review some statistical information for the numeric variables...\n\ntrain_df.describe()","metadata":{"execution":{"iopub.status.busy":"2022-01-02T20:25:01.943825Z","iopub.execute_input":"2022-01-02T20:25:01.944193Z","iopub.status.idle":"2022-01-02T20:25:01.968457Z","shell.execute_reply.started":"2022-01-02T20:25:01.944128Z","shell.execute_reply":"2022-01-02T20:25:01.96732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Review some information for the categorical variables...\n\ncountry_list = train_df['country'].unique()\nstore_list = train_df['store'].unique()\nproduct_list = train_df['product'].unique()\n\nprint(f'Country List:{country_list}')\nprint(f'Store List:{store_list}')\nprint(f'Product List:{product_list}')","metadata":{"execution":{"iopub.status.busy":"2022-01-02T20:25:01.974492Z","iopub.execute_input":"2022-01-02T20:25:01.974774Z","iopub.status.idle":"2022-01-02T20:25:01.991363Z","shell.execute_reply.started":"2022-01-02T20:25:01.974729Z","shell.execute_reply":"2022-01-02T20:25:01.99048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Review if there is missing information in the dataset...\n\ntrain_df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-01-02T20:25:01.993806Z","iopub.execute_input":"2022-01-02T20:25:01.994365Z","iopub.status.idle":"2022-01-02T20:25:02.018102Z","shell.execute_reply.started":"2022-01-02T20:25:01.99432Z","shell.execute_reply":"2022-01-02T20:25:02.017178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a simple function to evaluate the time-ranges of the information provided.\n# It will help with the train / validation separations\n\ndef evaluate_time(df):\n    min_date = df['date'].min()\n    max_date = df['date'].max()\n    print(f'Min Date: {min_date} /  Max Date: {max_date}')\n    return None\n\nevaluate_time(train_df)\nevaluate_time(test_df)","metadata":{"execution":{"iopub.status.busy":"2022-01-02T20:25:02.019768Z","iopub.execute_input":"2022-01-02T20:25:02.020127Z","iopub.status.idle":"2022-01-02T20:25:02.039166Z","shell.execute_reply.started":"2022-01-02T20:25:02.020086Z","shell.execute_reply":"2022-01-02T20:25:02.038161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"___","metadata":{}},{"cell_type":"markdown","source":"# Feature Engineering <a name=\"4\"></a>","metadata":{}},{"cell_type":"code","source":"TARGET = 'num_sold'","metadata":{"execution":{"iopub.status.busy":"2022-01-02T20:25:02.040505Z","iopub.execute_input":"2022-01-02T20:25:02.041585Z","iopub.status.idle":"2022-01-02T20:25:02.046287Z","shell.execute_reply.started":"2022-01-02T20:25:02.04154Z","shell.execute_reply":"2022-01-02T20:25:02.045174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Country List:['Finland' 'Norway' 'Sweden']\nholiday_FI = holidays.CountryHoliday('FI', years=[2015, 2016, 2017, 2018, 2019])\nholiday_NO = holidays.CountryHoliday('NO', years=[2015, 2016, 2017, 2018, 2019])\nholiday_SE = holidays.CountryHoliday('SE', years=[2015, 2016, 2017, 2018, 2019])\n\nholiday_dict = holiday_FI.copy()\nholiday_dict.update(holiday_NO)\nholiday_dict.update(holiday_SE)\n\ntrain_df['date'] = pd.to_datetime(train_df['date']) # Convert the date to datetime.\ntrain_df['holiday_name'] = train_df['date'].map(holiday_dict)\ntrain_df['is_holiday'] = np.where(train_df['holiday_name'].notnull(), 1, 0)\ntrain_df['holiday_name'] = train_df['holiday_name'].fillna('Not Holiday')\n\ntest_df['date'] = pd.to_datetime(test_df['date']) # Convert the date to datetime.\ntest_df['holiday_name'] = test_df['date'].map(holiday_dict)\ntest_df['is_holiday'] = np.where(test_df['holiday_name'].notnull(), 1, 0)\ntest_df['holiday_name'] = test_df['holiday_name'].fillna('Not Holiday')","metadata":{"execution":{"iopub.status.busy":"2022-01-02T20:25:02.04776Z","iopub.execute_input":"2022-01-02T20:25:02.048693Z","iopub.status.idle":"2022-01-02T20:25:02.093952Z","shell.execute_reply.started":"2022-01-02T20:25:02.048646Z","shell.execute_reply":"2022-01-02T20:25:02.093117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.sample(10)","metadata":{"execution":{"iopub.status.busy":"2022-01-02T20:25:02.095235Z","iopub.execute_input":"2022-01-02T20:25:02.095759Z","iopub.status.idle":"2022-01-02T20:25:02.118854Z","shell.execute_reply.started":"2022-01-02T20:25:02.095715Z","shell.execute_reply":"2022-01-02T20:25:02.117858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def add_holydays(df):\n    \"\"\"\n    Flag the dataframe with a is_holyday field = 1 if the date is on the\n    dictionary of holydays loaded.\n    Args\n        df\n    Returs\n        df\n    \"\"\"\n    new_years_eve = ['12/31/2015','12/31/2016','12/31/2017','12/31/2018','12/31/2019']\n    chrismas_day = ['12/24/2015','12/24/2016','12/24/2017','12/24/2018','12/24/2019']","metadata":{"execution":{"iopub.status.busy":"2022-01-02T20:25:02.121004Z","iopub.execute_input":"2022-01-02T20:25:02.121924Z","iopub.status.idle":"2022-01-02T20:25:02.129263Z","shell.execute_reply.started":"2022-01-02T20:25:02.121882Z","shell.execute_reply":"2022-01-02T20:25:02.128053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create some simple features base on the Date field...\n\ndef create_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Create features base on the date variable, the idea is to extract as much \n    information from the date componets.\n    Args\n        df: Input data to create the features.\n    Returns\n        df: A DataFrame with the new time base features.\n    \"\"\"\n    \n    df['date'] = pd.to_datetime(df['date']) # Convert the date to datetime.\n    \n    # Start the creating future process.\n    df['year'] = df['date'].dt.year\n    df['quarter'] = df['date'].dt.quarter\n    df['month'] = df['date'].dt.month\n    df['day'] = df['date'].dt.day\n    df['dayofweek'] = df['date'].dt.dayofweek\n    df['dayofmonth'] = df['date'].dt.days_in_month\n    df['dayofyear'] = df['date'].dt.dayofyear\n    df['weekofyear'] = df['date'].dt.weekofyear\n    df['weekday'] = df['date'].dt.weekday\n    df['is_weekend'] = np.where((df['weekday'] == 5) | (df['weekday'] == 6), 1, 0)\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2022-01-02T20:25:02.131262Z","iopub.execute_input":"2022-01-02T20:25:02.13202Z","iopub.status.idle":"2022-01-02T20:25:02.143847Z","shell.execute_reply.started":"2022-01-02T20:25:02.131977Z","shell.execute_reply":"2022-01-02T20:25:02.142534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Apply the function 'create_time_features' to the dataset...\ntrain_df = create_time_features(train_df)\ntest_df = create_time_features(test_df)","metadata":{"execution":{"iopub.status.busy":"2022-01-02T20:25:02.145711Z","iopub.execute_input":"2022-01-02T20:25:02.146332Z","iopub.status.idle":"2022-01-02T20:25:02.244089Z","shell.execute_reply.started":"2022-01-02T20:25:02.146288Z","shell.execute_reply":"2022-01-02T20:25:02.243029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"___","metadata":{}},{"cell_type":"markdown","source":"# Processing the Datasets for Training <a name=\"5\"></a>","metadata":{}},{"cell_type":"code","source":"# Convert the Categorical variables to one-hoe encoded features...\n# It will help in the training process\n\nCATEGORICAL = ['country', 'store', 'product', 'holiday_name']\ndef create_one_hot(df, categ_colums = CATEGORICAL):\n    \"\"\"\n    Creates one_hot encoded fields for the specified categorical columns...\n    Args\n        df\n        categ_colums\n    Returns\n        df\n    \"\"\"\n    df = pd.get_dummies(df, columns=CATEGORICAL)\n    return df\n\n\ndef encode_categ_features(df, categ_colums = CATEGORICAL):\n    \"\"\"\n    Use the label encoder to encode categorical features...\n    Args\n        df\n        categ_colums\n    Returns\n        df\n    \"\"\"\n    le = LabelEncoder()\n    for col in categ_colums:\n        df['enc_'+col] = le.fit_transform(df[col])\n    return df\n\ntrain_df = encode_categ_features(train_df)\ntest_df = encode_categ_features(test_df)","metadata":{"execution":{"iopub.status.busy":"2022-01-02T20:25:02.24588Z","iopub.execute_input":"2022-01-02T20:25:02.246207Z","iopub.status.idle":"2022-01-02T20:25:02.298797Z","shell.execute_reply.started":"2022-01-02T20:25:02.246162Z","shell.execute_reply":"2022-01-02T20:25:02.297934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def transform_target(df, taget = TARGET):\n    \"\"\"\n    Apply a log transformation to the target for better optimization \n    during training.\n    \"\"\"\n    df[TARGET] = np.log(df[TARGET])\n    return df\n\ntrain_df = transform_target(train_df, TARGET)","metadata":{"execution":{"iopub.status.busy":"2022-01-02T20:25:02.300053Z","iopub.execute_input":"2022-01-02T20:25:02.300365Z","iopub.status.idle":"2022-01-02T20:25:02.309346Z","shell.execute_reply.started":"2022-01-02T20:25:02.300325Z","shell.execute_reply":"2022-01-02T20:25:02.308019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-02T20:25:02.311499Z","iopub.execute_input":"2022-01-02T20:25:02.311878Z","iopub.status.idle":"2022-01-02T20:25:02.342852Z","shell.execute_reply.started":"2022-01-02T20:25:02.311835Z","shell.execute_reply":"2022-01-02T20:25:02.341977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extract features and avoid certain columns from the dataframe for training purposes...\navoid = ['row_id', 'date', 'num_sold']\nFEATURES = [feat for feat in train_df.columns if feat not in avoid]\n\n# Print a list of all the features created...\nprint(FEATURES)","metadata":{"execution":{"iopub.status.busy":"2022-01-02T20:25:02.34502Z","iopub.execute_input":"2022-01-02T20:25:02.345985Z","iopub.status.idle":"2022-01-02T20:25:02.353604Z","shell.execute_reply.started":"2022-01-02T20:25:02.34594Z","shell.execute_reply":"2022-01-02T20:25:02.352381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Selecting Features....\nprint(FEATURES)","metadata":{"execution":{"iopub.status.busy":"2022-01-02T20:25:02.355509Z","iopub.execute_input":"2022-01-02T20:25:02.356056Z","iopub.status.idle":"2022-01-02T20:25:02.365278Z","shell.execute_reply.started":"2022-01-02T20:25:02.356011Z","shell.execute_reply":"2022-01-02T20:25:02.36399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FEATURES = [\n            #'country',\n            #'store',\n            #'product',\n            #'holiday_name',\n            #'is_holiday',\n            'year',\n            #'quarter',\n            'month',\n            'day',\n            'dayofweek',\n            #'dayofmonth',\n            #'dayofyear',\n            #'weekofyear',\n            #'weekday',\n            'is_weekend',\n            'enc_country',\n            'enc_store',\n            'enc_product',\n            #'enc_holiday_name'\n            ]","metadata":{"execution":{"iopub.status.busy":"2022-01-02T20:25:02.367365Z","iopub.execute_input":"2022-01-02T20:25:02.36825Z","iopub.status.idle":"2022-01-02T20:25:02.375354Z","shell.execute_reply.started":"2022-01-02T20:25:02.368205Z","shell.execute_reply":"2022-01-02T20:25:02.374409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"___","metadata":{}},{"cell_type":"markdown","source":"# Creates a Simple Train / Validation Strategy <a name=\"6\"></a>","metadata":{}},{"cell_type":"code","source":"# Creates the Train and Validation sets to train the model...\n# Define a cutoff date to split the datasets\nCUTOFF_DATE = '2018-01-01'\n\n# Split the data into train and validation datasets using timestamp best suited for timeseries...\nX_train = train_df[train_df['date'] < CUTOFF_DATE][FEATURES]\ny_train = train_df[train_df['date'] < CUTOFF_DATE][TARGET]\n\nX_val = train_df[train_df['date'] >= CUTOFF_DATE][FEATURES]\ny_val = train_df[train_df['date'] >= CUTOFF_DATE][TARGET]","metadata":{"execution":{"iopub.status.busy":"2022-01-02T20:25:02.376834Z","iopub.execute_input":"2022-01-02T20:25:02.378577Z","iopub.status.idle":"2022-01-02T20:25:02.404406Z","shell.execute_reply.started":"2022-01-02T20:25:02.378521Z","shell.execute_reply":"2022-01-02T20:25:02.403472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def SMAPE(y_true, y_pred):\n    denominator = (y_true + np.abs(y_pred)) / 200.0\n    diff = np.abs(y_true - y_pred) / denominator\n    diff[denominator == 0] = 0.0\n    return np.mean(diff)","metadata":{"execution":{"iopub.status.busy":"2022-01-02T20:25:02.405863Z","iopub.execute_input":"2022-01-02T20:25:02.406851Z","iopub.status.idle":"2022-01-02T20:25:02.41344Z","shell.execute_reply.started":"2022-01-02T20:25:02.406803Z","shell.execute_reply":"2022-01-02T20:25:02.412358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# Train a Simple Model (XGBoost Regressor) <a name=\"7\"></a>","metadata":{}},{"cell_type":"code","source":"# Defines a really simple XGBoost Regressor...\n\nxgboost_params = {'eta'              : 0.1,\n                  'n_estimators'     : 16384,\n                  'max_depth'        : 8,\n                  'max_leaves'       : 256,\n                  'colsample_bylevel': 0.75,\n                  'colsample_bytree' : 0.75,\n                  'subsample'        : 0.75, # XGBoost would randomly sample 'subsample_value' of the training data prior to growing trees\n                  'min_child_weight' : 512,\n                  'min_split_loss'   : 0.002,\n                  'alpha'            : 0.08,\n                  'lambda'           : 128,\n                  'objective'        : 'reg:squarederror',\n                  'eval_metric'      : 'rmse', # Originally using RMSE, trying new functions...\n                  'tree_method'      : 'gpu_hist',\n                  'seed'             : SEED\n                  }\n\n# Create an instance of the XGBRegressor and set the model parameters...\nregressor = XGBRegressor(**xgboost_params)\n\n# Train the XGBRegressor using the train and validation datasets, \n# Utilizes early_stopping_rounds to control overfitting...\nregressor.fit(X_train,\n              y_train,\n              eval_set=[(X_val, y_val)],\n              early_stopping_rounds = 250,\n              verbose = 500)","metadata":{"execution":{"iopub.status.busy":"2022-01-02T20:25:02.415047Z","iopub.execute_input":"2022-01-02T20:25:02.416444Z","iopub.status.idle":"2022-01-02T20:25:07.376218Z","shell.execute_reply.started":"2022-01-02T20:25:02.416396Z","shell.execute_reply":"2022-01-02T20:25:07.375025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_pred = regressor.predict(X_val[FEATURES])\n# Convert the target back to non-logaritmic.\nval_pred = np.exp(val_pred)\ny_val = np.exp(y_val)\n\nscore = np.sqrt(mean_squared_error(y_val, val_pred))\nprint(f'RMSE: {score} / SMAPE: {SMAPE(y_val, val_pred)}')","metadata":{"execution":{"iopub.status.busy":"2022-01-02T20:25:07.378171Z","iopub.execute_input":"2022-01-02T20:25:07.37867Z","iopub.status.idle":"2022-01-02T20:25:07.513208Z","shell.execute_reply.started":"2022-01-02T20:25:07.378625Z","shell.execute_reply":"2022-01-02T20:25:07.51212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model Results vs. Features Used in the Traininf and Validation...\n1. Plain features, nothing added to the model. Removed Id, Datetime and Target </br>\nRMSE: 141.17269369190075 / SMAPE: 17.040551866223385\n\n2. Added Datetime features,'year', 'month', 'day', 'dayofweek', 'dayofmonth', 'dayofyear', 'weekofyear', 'weekday' </br>\nRMSE: 66.89475324109723 / SMAPE: 9.30006322183181\n\n3. Added Datetime features,'year', 'month', 'day', 'dayofweek', 'dayofmonth', 'dayofyear', 'weekofyear', 'weekday', 'quarter' </br>\nRMSE: 67.4018691784641 / SMAPE: 9.343389593022566\n\n4. Added Datetime features,'year', 'month', 'day', 'dayofweek', 'dayofmonth', 'dayofyear', 'weekofyear', 'weekday', 'quarter' </br>\nAdded new Features,'is_holiday'</br>\nRMSE: 66.59882566819414 / SMAPE: 9.477461518875648\n\n5. Added Datetime features,'year', 'month', 'day', 'dayofweek', 'dayofmonth', 'dayofyear', 'weekofyear', 'weekday', 'quarter' </br>\nAdded new Features,'is_holiday', 'is_weekend'</br>\nRMSE: 66.27489712300181 / SMAPE: 9.370856195608114\n\n6. Added Datetime features,'year', 'month', 'day', 'dayofweek', 'dayofmonth', 'dayofyear', 'weekofyear', 'weekday', 'quarter' </br>\nAdded new Features,'is_holiday', 'is_weekend','enc_holiday_name' </br>\nRMSE: 65.93668135230337 / SMAPE: 9.428644170683123\n\n7. Added Datetime features,'year', 'month', 'day', 'dayofweek', 'dayofmonth', 'dayofyear', 'weekofyear', 'weekday'</br>\nAdded new Features,'is_weekend' </br>\nRMSE: 66.73112188359103 / SMAPE: 9.29087254951728\n\n8. Added Datetime features,'year', 'month', 'day', 'dayofweek', 'dayofyear', 'weekofyear', 'weekday'</br>\nAdded new Features,'is_weekend' </br>\nRMSE: 66.1329325693428 / SMAPE: 9.290678813131464\n\n9. Added Datetime features,'year', 'month', 'day', 'dayofweek', 'weekofyear', 'weekday'</br>\nAdded new Features,'is_weekend' </br>\nRMSE: 66.13737123847237 / SMAPE: 9.256808780901792\n\n10. Added Datetime features,'year', 'month', 'day', 'dayofweek', 'weekday'</br>\nAdded new Features,'is_weekend' </br>\nRMSE: 65.40444050929132 / SMAPE: 9.045220024208168\n\n11. Added Datetime features,'year', 'month', 'day', 'dayofweek'</br>\nAdded new Features,'is_weekend' </br>\nRMSE: 65.20180075198031 / SMAPE: 9.049180607434174\n\n12. Added Datetime features,'year', 'month', 'day', 'dayofweek'</br>\nUsing RMSE and Log of the Target... </br>\nAdded new Features,'is_weekend' </br>\nRMSE: 63.544532908755954 / SMAPE: 8.460984766381136","metadata":{}},{"cell_type":"code","source":"feats = {} # a dict to hold feature_name: feature_importance\nfor feature, importance in zip(FEATURES, regressor.feature_importances_):\n    feats[feature] = importance #add the name/value pair \n\nimportances = pd.DataFrame.from_dict(feats, orient='index').rename(columns={0: 'Gini-importance'})\nimportances.sort_values(by='Gini-importance', ascending=False).plot(kind='bar', rot=45, figsize=(10,5))","metadata":{"execution":{"iopub.status.busy":"2022-01-02T20:25:07.514735Z","iopub.execute_input":"2022-01-02T20:25:07.51577Z","iopub.status.idle":"2022-01-02T20:25:07.95695Z","shell.execute_reply.started":"2022-01-02T20:25:07.515708Z","shell.execute_reply":"2022-01-02T20:25:07.956016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# Train a Simple Model (XGBoost Regressor) using a CV Loop. <a name=\"8\"></a>","metadata":{}},{"cell_type":"code","source":"%%time\nN_SPLITS = 3\nEARLY_STOPPING_ROUNDS = 150 # Will stop training if one metric of one validation data doesn‚Äôt improve in last round\nVERBOSE = 0 # Controls the level of information, verbosity","metadata":{"execution":{"iopub.status.busy":"2022-01-02T20:25:07.958517Z","iopub.execute_input":"2022-01-02T20:25:07.959629Z","iopub.status.idle":"2022-01-02T20:25:07.966636Z","shell.execute_reply.started":"2022-01-02T20:25:07.959583Z","shell.execute_reply":"2022-01-02T20:25:07.965647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Define a Pipeline to process the data for the Model.\ntransformer = Pipeline(steps=[('scaler',StandardScaler()), ('min_max', MinMaxScaler(feature_range=(0, 1)))])\npreprocessor = ColumnTransformer(transformers=[('first', transformer, FEATURES)])       ","metadata":{"execution":{"iopub.status.busy":"2022-01-02T20:25:07.968344Z","iopub.execute_input":"2022-01-02T20:25:07.969228Z","iopub.status.idle":"2022-01-02T20:25:07.985072Z","shell.execute_reply.started":"2022-01-02T20:25:07.969178Z","shell.execute_reply":"2022-01-02T20:25:07.983966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Cross Validation Loop for the Classifier.\ndef cross_validation_train(train, labels, test, model, model_params, n_folds = 5):\n    \"\"\"\n    The following function is responsable of training a model in a\n    cross validation loop and generate predictions on the specified test set.\n    The function provides the model feature importance list as other variables.\n\n    Args:\n    train  (Dataframe): ...\n    labels (Series): ...\n    test   (Dataframe): ...\n    model  (Model): ...\n    model_params (dict of str: int): ...\n\n    Return:\n    classifier  (Model): ...\n    feat_import (Dataframe): ...\n    test_pred   (Dataframe): ...\n    ...\n\n    \"\"\"\n    # Creates empty place holders for out of fold and test predictions.\n    oof_pred  = np.zeros(len(train)) # We are predicting prob. we need more dimensions.\n    oof_label = np.zeros(len(train))\n    test_pred = np.zeros(len(test)) # We are predicting prob. we need more dimensions\n    val_indexes_used = []\n    \n    # Creates empty place holder for the feature importance.\n    feat_import = np.zeros(len(FEATURES))\n    \n    # Creates Stratified Kfold object to be used in the train / validation\n    # phase of the model.\n    Kf = TimeSeriesSplit(n_splits = n_folds)\n    \n    # Start the training and validation loops.\n    for fold, (train_idx, val_idx) in enumerate(Kf.split(train)):\n        # Creates the index for each fold\n        print(f'Fold: {fold}')        \n        train_min_date = train_df.iloc[train_idx]['date'].min()\n        train_max_date = train_df.iloc[train_idx]['date'].max()\n        \n        valid_min_date = train_df.iloc[val_idx]['date'].min()\n        valid_max_date = train_df.iloc[val_idx]['date'].max()\n        \n        print(f'Train Min / Max Dates: {train_min_date} / {train_max_date}')\n        print(f'Valid Min / Max Dates: {valid_min_date} / {valid_max_date}')\n\n        print(f'Training on {train_df.iloc[train_idx].shape[0]} Records')\n        print(f'Validating on {train_df.iloc[val_idx].shape[0]} Records')\n        \n        # Generates the Fold. Train and Validation datasets\n        X_trn, y_trn = train.iloc[train_idx], labels.iloc[train_idx]\n        X_val, y_val = train.iloc[val_idx], labels.iloc[val_idx]\n        \n        val_indexes_used = np.concatenate((val_indexes_used, val_idx), axis=None)\n        \n        # Instanciate a classifier based on the model parameters\n        regressor = model(**model_params)\n \n        regressor.fit(X_trn, \n                      y_trn, \n                      eval_set = [(X_val, y_val)], \n                      early_stopping_rounds = EARLY_STOPPING_ROUNDS, \n                      verbose = VERBOSE)\n        \n        # Generate predictions using the trained model\n        val_pred = regressor.predict(X_val)\n        oof_pred[val_idx]  = val_pred # store the predictions for that fold.\n        oof_label[val_idx] = y_val # store the true labels for that fold.\n\n        # Calculate the model error based on the selected metric\n        error =  np.sqrt(mean_squared_error(y_val, val_pred))\n\n        # Print some of the model performance metrics\n        print(f'RMSE: {error}')\n        print(f'SMAPE: {SMAPE(y_val, val_pred)}')\n        print(\".\"*50)\n\n        # Populate the feature importance matrix\n        feat_import += regressor.feature_importances_\n\n        # Generate predictions for the test set\n        test_pred += (regressor.predict(test)) / n_folds\n                        \n    # Calculate the error across all the folds and print the reuslts\n    val_indexes_used = val_indexes_used.astype(int)\n    global_error = np.sqrt(mean_squared_error(labels.iloc[val_indexes_used], oof_pred[val_indexes_used]))\n    \n    print('')\n    print(f'RMSE: {global_error}...')\n    print(f'SMAPE: {SMAPE(labels.iloc[val_indexes_used], oof_pred[val_indexes_used])}...')\n    \n    return regressor, feat_import, test_pred, oof_label, oof_pred","metadata":{"execution":{"iopub.status.busy":"2022-01-02T20:25:07.987115Z","iopub.execute_input":"2022-01-02T20:25:07.987843Z","iopub.status.idle":"2022-01-02T20:25:08.004744Z","shell.execute_reply.started":"2022-01-02T20:25:07.987751Z","shell.execute_reply":"2022-01-02T20:25:08.003486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Uses the cross_validation_train to build and train the model with XGBoost\nxgbr, feat_imp, predictions, oof_label, oof_pred = cross_validation_train(train  = train_df[FEATURES], \n                                                                          labels = train_df[TARGET], \n                                                                          test   = test_df[FEATURES], \n                                                                          model  = XGBRegressor, \n                                                                          model_params = xgboost_params,\n                                                                          n_folds = N_SPLITS\n                                                                          )","metadata":{"execution":{"iopub.status.busy":"2022-01-02T20:25:08.006919Z","iopub.execute_input":"2022-01-02T20:25:08.007797Z","iopub.status.idle":"2022-01-02T20:25:15.94099Z","shell.execute_reply.started":"2022-01-02T20:25:08.007723Z","shell.execute_reply":"2022-01-02T20:25:15.939819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.shape","metadata":{"execution":{"iopub.status.busy":"2022-01-02T20:25:15.942844Z","iopub.execute_input":"2022-01-02T20:25:15.943387Z","iopub.status.idle":"2022-01-02T20:25:15.951294Z","shell.execute_reply.started":"2022-01-02T20:25:15.943344Z","shell.execute_reply":"2022-01-02T20:25:15.94982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"oof_label.shape","metadata":{"execution":{"iopub.status.busy":"2022-01-02T20:25:15.953631Z","iopub.execute_input":"2022-01-02T20:25:15.954365Z","iopub.status.idle":"2022-01-02T20:25:15.967255Z","shell.execute_reply.started":"2022-01-02T20:25:15.954318Z","shell.execute_reply":"2022-01-02T20:25:15.966194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"oof_pred.shape","metadata":{"execution":{"iopub.status.busy":"2022-01-02T20:25:15.968736Z","iopub.execute_input":"2022-01-02T20:25:15.969524Z","iopub.status.idle":"2022-01-02T20:25:15.978232Z","shell.execute_reply.started":"2022-01-02T20:25:15.969477Z","shell.execute_reply":"2022-01-02T20:25:15.976955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feats = {} # a dict to hold feature_name: feature_importance\nfor feature, importance in zip(FEATURES, xgbr.feature_importances_):\n    feats[feature] = importance #add the name/value pair \n\nimportances = pd.DataFrame.from_dict(feats, orient='index').rename(columns={0: 'Gini-importance'})\nimportances.sort_values(by='Gini-importance', ascending=False).plot(kind='bar', rot=45, figsize=(12,5))","metadata":{"execution":{"iopub.status.busy":"2022-01-02T20:25:15.985641Z","iopub.execute_input":"2022-01-02T20:25:15.986439Z","iopub.status.idle":"2022-01-02T20:25:16.271338Z","shell.execute_reply.started":"2022-01-02T20:25:15.986395Z","shell.execute_reply":"2022-01-02T20:25:16.270363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Inference (Submission to Kaggle) <a name=\"1\"></a>","metadata":{}},{"cell_type":"code","source":"# Use the created model to predict the sales for 2019...\npred = regressor.predict(test_df[FEATURES])\npred = np.exp(pred)\nsubmission_df['num_sold'] = pred\nsubmission_df.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-01-02T20:26:17.250456Z","iopub.execute_input":"2022-01-02T20:26:17.250801Z","iopub.status.idle":"2022-01-02T20:26:17.385938Z","shell.execute_reply.started":"2022-01-02T20:26:17.250752Z","shell.execute_reply":"2022-01-02T20:26:17.385028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creates a submission file for Kaggle...\nsubmission_df.to_csv('submission.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-02T20:26:18.285248Z","iopub.execute_input":"2022-01-02T20:26:18.285914Z","iopub.status.idle":"2022-01-02T20:26:18.324165Z","shell.execute_reply.started":"2022-01-02T20:26:18.285879Z","shell.execute_reply":"2022-01-02T20:26:18.323129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Use the created model to predict the sales for 2019...\npred = regressor.predict(test_df[FEATURES])\nsubmission_df['num_sold'] = predictions\nsubmission_df.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-01-02T20:25:16.435563Z","iopub.execute_input":"2022-01-02T20:25:16.435963Z","iopub.status.idle":"2022-01-02T20:25:16.569293Z","shell.execute_reply.started":"2022-01-02T20:25:16.435918Z","shell.execute_reply":"2022-01-02T20:25:16.568165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Results and Ideas...","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}