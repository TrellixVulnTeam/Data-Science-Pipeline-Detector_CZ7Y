{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Advanced linear model for the January TPS with customer confidence index\n\nThis notebook contains application of Ensemble appoach to winning solution of @AMBROSM \nhttps://www.kaggle.com/ambrosm/tpsjan22-10-advanced-linear-model-with-cci\nPart of the code is also taken from @samuelcortinhas\nhttps://www.kaggle.com/samuelcortinhas/tps-jan-22-quick-eda-hybrid-model\nThe privaite score increased slightly, my knowledge increased significantly","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport pickle\nimport itertools\nimport gc\nimport math\nimport matplotlib.pyplot as plt\nimport dateutil.easter as easter\nfrom matplotlib.ticker import MaxNLocator, FormatStrFormatter, PercentFormatter\nfrom datetime import datetime, date, timedelta\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import make_pipeline\nimport scipy.stats","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-06T07:09:21.199611Z","iopub.execute_input":"2022-02-06T07:09:21.200605Z","iopub.status.idle":"2022-02-06T07:09:21.206818Z","shell.execute_reply.started":"2022-02-06T07:09:21.200558Z","shell.execute_reply":"2022-02-06T07:09:21.205975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nsns.set(style='darkgrid', font_scale=1.4)\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom itertools import combinations\nimport math\nimport statistics\nimport time\nfrom datetime import datetime\nimport matplotlib.dates as mdates\nimport dateutil.easter as easter\n\n# Sklearn\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, TimeSeriesSplit\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LinearRegression, Ridge, HuberRegressor\n# Models\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom pyearth import Earth\n\n# Tensorflow\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import callbacks","metadata":{"execution":{"iopub.status.busy":"2022-02-06T07:09:21.209032Z","iopub.execute_input":"2022-02-06T07:09:21.209631Z","iopub.status.idle":"2022-02-06T07:09:21.223357Z","shell.execute_reply.started":"2022-02-06T07:09:21.209583Z","shell.execute_reply":"2022-02-06T07:09:21.222507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NO_STORE = True\n\noriginal_train_df = pd.read_csv('../input/tabular-playground-series-jan-2022/train.csv')\noriginal_test_df = pd.read_csv('../input/tabular-playground-series-jan-2022/test.csv')\n\ngdp_df = pd.read_csv('../input/gdp-20152019-finland-norway-and-sweden/GDP_data_2015_to_2019_Finland_Norway_Sweden.csv')\ngdp_df.set_index('year', inplace=True)\n\ncci_df = pd.read_csv('../input/oecd-consumer-confidence-index/DP_LIVE_21012022073653464.csv')\ncci_df.set_index(['LOCATION', 'TIME'], inplace=True)\n\n# The dates are read as strings and must be converted\nfor df in [original_train_df, original_test_df]:\n    df['date'] = pd.to_datetime(df.date)\noriginal_train_df.head(6)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T07:09:21.224978Z","iopub.execute_input":"2022-02-06T07:09:21.225263Z","iopub.status.idle":"2022-02-06T07:09:21.32137Z","shell.execute_reply.started":"2022-02-06T07:09:21.225222Z","shell.execute_reply":"2022-02-06T07:09:21.320698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Before starting, we calculate the ratio between KaggleRama and KaggleMart sales. This ratio is always the same and does not depend on any other features. A direct calculation is more accurate than linear regression:","metadata":{}},{"cell_type":"code","source":"kaggle_rama_log_diff = \\\n    np.log(original_train_df[original_train_df.store == \"KaggleRama\"].num_sold.values).mean() - \\\n    np.log(original_train_df[original_train_df.store == \"KaggleMart\"].num_sold.values).mean()\nprint(f\"KaggleRama always sells {np.exp(kaggle_rama_log_diff):.5f} more than KaggleMart.\")","metadata":{"execution":{"iopub.status.busy":"2022-02-06T07:09:21.32308Z","iopub.execute_input":"2022-02-06T07:09:21.323721Z","iopub.status.idle":"2022-02-06T07:09:21.339248Z","shell.execute_reply.started":"2022-02-06T07:09:21.323672Z","shell.execute_reply":"2022-02-06T07:09:21.338361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def smape_loss(y_true, y_pred):\n    \"\"\"SMAPE Loss\"\"\"\n    return np.abs(y_true - y_pred) / (y_true + np.abs(y_pred)) * 200\n","metadata":{"execution":{"iopub.status.busy":"2022-02-06T07:09:21.341209Z","iopub.execute_input":"2022-02-06T07:09:21.342164Z","iopub.status.idle":"2022-02-06T07:09:21.348123Z","shell.execute_reply.started":"2022-02-06T07:09:21.342124Z","shell.execute_reply":"2022-02-06T07:09:21.34718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature engineering\n","metadata":{}},{"cell_type":"code","source":"# Feature engineering\ndef engineer(df):\n    \"\"\"Return a new dataframe with the engineered features\"\"\"\n    \n    def get_gdp(row):\n        country = 'GDP_' + row.country\n        return gdp_df.loc[row.date.year, country]\n        \n    def get_cci(row):\n        country = row.country\n        time = f\"{row.date.year}-{row.date.month:02d}\"\n        # There is no monthly CCI data for Norway.\n        # We use the Finland data instead.\n        if country == 'Norway': country = 'Finland'\n        return cci_df.loc[country[:3].upper(), time].Value\n        \n    new_df = pd.DataFrame({'gdp': np.log(df.apply(get_gdp, axis=1)),\n                           'cci': df.apply(get_cci, axis=1),\n                           'wd4': df.date.dt.weekday == 4, # Friday\n                           'wd56': df.date.dt.weekday >= 5, # Saturday and Sunday\n                          })\n\n    # One-hot encoding (no need to encode the last categories)\n    for country in ['Finland', 'Norway']:\n        new_df[country] = df.country == country\n    new_df['KaggleRama'] = df.store == 'KaggleRama'\n    for product in ['Kaggle Mug', 'Kaggle Hat']:\n        new_df[product] = df['product'] == product\n        \n    # Seasonal variations (Fourier series)\n    # The three products have different seasonal patterns\n    dayofyear = df.date.dt.dayofyear\n    for k in range(1, 3):\n        sink = np.sin(dayofyear / 365 * 2 * math.pi * k)\n        cosk = np.cos(dayofyear / 365 * 2 * math.pi * k)\n        new_df[f'mug_sin{k}'] = sink * new_df['Kaggle Mug']\n        new_df[f'mug_cos{k}'] = cosk * new_df['Kaggle Mug']\n        new_df[f'hat_sin{k}'] = sink * new_df['Kaggle Hat']\n        new_df[f'hat_cos{k}'] = cosk * new_df['Kaggle Hat']\n\n    new_df.drop(columns=['mug_sin1'], inplace=True)\n    new_df.drop(columns=['mug_sin2'], inplace=True)\n\n    # Special days\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({f\"dec{d}\":\n                                      (df.date.dt.month == 12) & (df.date.dt.day == d)\n                                      for d in range(24, 32)}),\n                        pd.DataFrame({f\"n-dec{d}\":\n                                      (df.date.dt.month == 12) & (df.date.dt.day == d) & (df.country == 'Norway')\n                                      for d in range(25, 32)}),\n                        pd.DataFrame({f\"f-jan{d}\":\n                                      (df.date.dt.month == 1) & (df.date.dt.day == d) & (df.country == 'Finland')\n                                      for d in range(1, 15)}),\n                        pd.DataFrame({f\"n-jan{d}\":\n                                      (df.date.dt.month == 1) & (df.date.dt.day == d) & (df.country == 'Norway')\n                                      for d in range(1, 10)}),\n                        pd.DataFrame({f\"s-jan{d}\":\n                                      (df.date.dt.month == 1) & (df.date.dt.day == d) & (df.country == 'Sweden')\n                                      for d in range(1, 15)})\n                       ],\n                       axis=1)\n    \n    # May and June\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({f\"may{d}\":\n                                      (df.date.dt.month == 5) & (df.date.dt.day == d) \n                                      for d in list(range(1, 10))}),\n#                         pd.DataFrame({f\"f-may{d}\":\n#                                       (df.date.dt.month == 5) & (df.date.dt.day == d) & (df.country == 'Finland') # end of the war\n#                                       for d in [9]}),\n                        pd.DataFrame({f\"may{d}\":\n                                      (df.date.dt.month == 5) & (df.date.dt.day == d) & (df.country == 'Norway')\n                                     for d in list(range(18, 26)) + [27]}),\n                        pd.DataFrame({f\"june{d}\":\n                                      (df.date.dt.month == 6) & (df.date.dt.day == d) & (df.country == 'Sweden')\n                                      for d in list(range(8, 15))}),\n                       ],\n                       axis=1)\n    \n    # Last Wednesday of June\n    wed_june_date = df.date.dt.year.map({2015: pd.Timestamp(('2015-06-24')),\n                                         2016: pd.Timestamp(('2016-06-29')),\n                                         2017: pd.Timestamp(('2017-06-28')),\n                                         2018: pd.Timestamp(('2018-06-27')),\n                                         2019: pd.Timestamp(('2019-06-26'))})\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({f\"wed_june{d}\": \n                                      (df.date - wed_june_date == np.timedelta64(d, \"D\")) & (df.country != 'Norway')\n                                      for d in list(range(-4, 5))})],\n                       axis=1)\n    \n    # First Sunday of November\n    sun_nov_date = df.date.dt.year.map({2015: pd.Timestamp(('2015-11-1')),\n                                         2016: pd.Timestamp(('2016-11-6')),\n                                         2017: pd.Timestamp(('2017-11-5')),\n                                         2018: pd.Timestamp(('2018-11-4')),\n                                         2019: pd.Timestamp(('2019-11-3'))})\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({f\"sun_nov{d}\": \n                                      (df.date - sun_nov_date == np.timedelta64(d, \"D\")) & (df.country != 'Norway')\n                                      for d in list(range(0, 9))})],\n                       axis=1)\n    \n    # First half of December (Independence Day of Finland, 6th of December)\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({f\"dec{d}\":\n                                      (df.date.dt.month == 12) & (df.date.dt.day == d) & (df.country == 'Finland')\n                                      for d in list(range(6, 15))})],\n                       axis=1)\n\n    # Easter\n    easter_date = df.date.apply(lambda date: pd.Timestamp(easter.easter(date.year)))\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({f\"easter{d}\": \n                                      (df.date - easter_date == np.timedelta64(d, \"D\"))\n                                      for d in list(range(-2, 11)) + list(range(40, 48)) + list(range(51, 58))}),\n                        pd.DataFrame({f\"n_easter{d}\": \n                                      (df.date - easter_date == np.timedelta64(d, \"D\")) & (df.country == 'Norway')\n                                      for d in list(range(-3, 8)) + list(range(50, 61))})],\n                       axis=1)\n    \n    return new_df.astype(np.float32)\n\ntrain_df = engineer(original_train_df)\ntrain_df['date'] = original_train_df.date\ntrain_df['num_sold'] = original_train_df.num_sold.astype(np.float32)\ntest_df = engineer(original_test_df)\n\nfeatures = list(test_df.columns)\nif NO_STORE: features.remove('KaggleRama')\nprint(list(features))\n","metadata":{"execution":{"iopub.status.busy":"2022-02-06T07:09:21.448551Z","iopub.execute_input":"2022-02-06T07:09:21.448843Z","iopub.status.idle":"2022-02-06T07:09:31.159127Z","shell.execute_reply.started":"2022-02-06T07:09:21.448814Z","shell.execute_reply":"2022-02-06T07:09:31.158252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# A class is a collection of properties and methods (like models from Sklearn)\nclass HybridModel:\n    def __init__(self, model_1, model_2, grid=None):\n        self.model_1 = model_1\n        self.model_2 = model_2\n        self.grid=grid\n        \n    def fit(self, X_train_1, X_train_2, y):\n        # Train model 1\n        self.model_1.fit(X_train_1, y)\n        \n        # Predictions from model 1 (trend)\n        y_trend = self.model_1.predict(X_train_1)\n\n        if self.grid:\n            # Grid search\n            tscv = TimeSeriesSplit(n_splits=3)\n            grid_model = GridSearchCV(estimator=self.model_2, cv=tscv, param_grid=self.grid)\n        \n            # Train model 2 on detrended series\n            grid_model.fit(X_train_2, y-y_trend)\n            \n            # Model 2 preditions (for residual analysis)\n            y_resid = grid_model.predict(X_train_2)\n            \n            # Save model\n            self.grid_model=grid_model\n        else:\n            # Train model 2 on residuals\n            self.model_2.fit(X_train_2, y-y_trend)\n            \n            # Model 2 preditions (for residual analysis)\n            y_resid = self.model_2.predict(X_train_2)\n        \n        # Save data\n        self.y_train_trend = y_trend\n        self.y_train_resid = y_resid\n        \n    def predict(self, X_test_1, X_test_2):\n        # Predict trend using model 1\n        y_trend = self.model_1.predict(X_test_1)\n        \n        if self.grid:\n            # Grid model predictions\n            y_resid = self.grid_model.predict(X_test_2)\n        else:\n            # Model 2 predictions\n            y_resid = self.model_2.predict(X_test_2)\n        \n        # Add predictions together\n        y_pred = y_trend + y_resid\n        \n        # Save data\n        self.y_test_trend = y_trend\n        self.y_test_resid = y_resid\n        \n        return y_pred","metadata":{"execution":{"iopub.status.busy":"2022-02-06T07:09:31.160957Z","iopub.execute_input":"2022-02-06T07:09:31.161187Z","iopub.status.idle":"2022-02-06T07:09:31.170047Z","shell.execute_reply.started":"2022-02-06T07:09:31.161158Z","shell.execute_reply":"2022-02-06T07:09:31.169176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Validation\n\nWe train on a GroupKFold with the years as groups. For the validation, we separate the SMAPE for the first quarter from the rest of the year. As the private leaderboard is computed on the predictions for April through December, SMAPE for these nine months is more important than SMAPE for the first quarter.\n\nThe data is scaled using three separate MinMaxScalers. Ridge regression will penalize high weights on the customer confidence index more than on other features.","metadata":{}},{"cell_type":"code","source":"def predict(features, preproc, model, X):\n    y = (np.exp(model.predict(preproc.transform(X[features]),preproc.transform(X[features])))).reshape(-1, 1)\n    if NO_STORE: y[X.KaggleRama.values > 0] = y[X.KaggleRama.values > 0] * np.exp(kaggle_rama_log_diff)\n    return y\n\n\ndef fit_model(X_tr, X_va=None, score_list=[], mse_list=[], run=0, fold=0, oof=None, outliers=False, correction=1.0):\n    \"\"\"Scale the data, fit a model, plot the training history and validate the model\"\"\"\n    start_time = datetime.now()\n\n    # Preprocess the data\n    X_tr_f = X_tr[features]\n    preproc = make_pipeline(ColumnTransformer([('general', MinMaxScaler(), ['gdp', 'wd4', 'wd56', 'Finland', 'Norway',\n                                                                            'Kaggle Mug', 'Kaggle Hat',\n                                                                            'mug_cos1', 'hat_sin1', 'hat_cos1', 'mug_cos2',\n                                                                            'hat_sin2', 'hat_cos2']),\n                                               ('cci', MinMaxScaler((0, 0.06)), ['cci']),\n                                              ],\n                                              remainder=MinMaxScaler((0, 2.8))),\n                            StandardScaler(with_std=False))\n\n    #preproc = make_pipeline(MinMaxScaler(), StandardScaler(with_std=False))\n    #preproc = StandardScaler()\n    X_tr_f = preproc.fit_transform(X_tr_f)\n    y_tr = X_tr.num_sold.values.reshape(-1, 1).copy()\n    if NO_STORE: y_tr[X_tr.KaggleRama != 0] = y_tr[X_tr.KaggleRama != 0] / np.exp(kaggle_rama_log_diff)\n\n    # Train the model\n    #model = Ridge(alpha=0.2, tol=0.00001, max_iter=10000)\n    # training model predictions\n    model_1=Ridge(alpha=0.2, tol=0.00001, max_iter=10000)\n    model_2=LinearRegression()\n    #model_2=HuberRegressor(epsilon=1.20, max_iter=500)\n    #model_2=Earth()\n    #model_1=RandomForestRegressor()\n    #model_2=CatBoostRegressor(random_state=0, verbose=False)\n    #model_2=LGBMRegressor()\n    \n    \n    model = HybridModel(model_1, model_2)\n    model.fit(X_tr_f, X_tr_f, np.log(y_tr).ravel())\n    #model.fit(X_tr_f, np.log(y_tr).ravel())\n    \n\n    if X_va is not None:\n        # Preprocess the validation data\n        y_va = X_va.num_sold.values.reshape(-1, 1)\n\n        # Inference for validation\n        y_va_pred = predict(features, preproc, model, X_va)\n        oof.update(pd.Series(y_va_pred.ravel(), index=X_va.index))\n        \n        # Evaluation: Execution time and SMAPE\n        smape_before_correction = np.mean(smape_loss(y_va, y_va_pred))\n        y_va_pred *= correction\n        smape = np.mean(smape_loss(y_va, y_va_pred))\n        mse = mean_squared_error(np.log(y_va), np.log(y_va_pred))\n        print(f\"Fold {run}.{fold} | {str(datetime.now() - start_time)[-12:-7]}\"\n              f\" | SMAPE: {smape:.5f}   (before correction: {smape_before_correction:.5f})\"\n              f\" | MSE: {mse:.5f}\")\n        score_list.append(smape)\n        mse_list.append(mse)\n        \n    return preproc, model\n","metadata":{"execution":{"iopub.status.busy":"2022-02-06T07:09:31.171311Z","iopub.execute_input":"2022-02-06T07:09:31.17193Z","iopub.status.idle":"2022-02-06T07:09:31.190969Z","shell.execute_reply.started":"2022-02-06T07:09:31.171895Z","shell.execute_reply":"2022-02-06T07:09:31.190271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"RUNS = 1 # should be 1\n\ndef validate(train_df, correction_factor=1.0):\n    # Make the results reproducible\n    np.random.seed(202100)\n\n    total_start_time = datetime.now()\n    oof = pd.Series(0.0, index=train_df.index)\n    score_list, mse_list = [], []\n    for run in range(RUNS):\n        kf = GroupKFold(n_splits=4)\n        for fold, (train_idx, val_idx) in enumerate(kf.split(train_df, groups=train_df.date.dt.year)):\n            X_tr = train_df.iloc[train_idx]\n            X_va = train_df.iloc[val_idx]\n            print(f\"Fold {run}.{fold} validating on {train_df.iloc[val_idx].iloc[0].date.year}\")\n            preproc, model = fit_model(X_tr, X_va, score_list, mse_list, run=run, fold=fold, oof=oof, correction=correction_factor)\n\n    print(f\"Average SMAPE: {sum(score_list) / len(score_list):.5f} | MSE: {sum(mse_list) / len(mse_list):.7f}   \"\n          f\"| Y: {smape_loss(train_df.num_sold, oof).mean():.5f}   \"\n          f\"| Q1: {smape_loss(train_df.num_sold, oof)[train_df.date.dt.month <= 3].mean():.5f}   \"\n          f\"| Q234: {smape_loss(train_df.num_sold, oof)[train_df.date.dt.month > 3].mean():.5f}\")\n    with open('oof.pickle', 'wb') as handle: pickle.dump(oof, handle)\n        \nvalidate(train_df)\n","metadata":{"execution":{"iopub.status.busy":"2022-02-06T07:09:31.192209Z","iopub.execute_input":"2022-02-06T07:09:31.192623Z","iopub.status.idle":"2022-02-06T07:09:35.24493Z","shell.execute_reply.started":"2022-02-06T07:09:31.192593Z","shell.execute_reply":"2022-02-06T07:09:35.243838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference and submission\n","metadata":{}},{"cell_type":"code","source":"# Fit the model on the complete training data\npreproc, model = fit_model(train_df, None)\n\n# Inference for test\ntest_pred_list = []\ntest_pred_list.append(predict(features, preproc, model, test_df) * 1.003) # magic scaling factor\n\n# Prepare the submission file\nsub = original_test_df[['row_id']].copy()\nsub['num_sold'] = sum(test_pred_list) / len(test_pred_list)\n\n# Plot the distribution of the test predictions\nplt.figure(figsize=(16,3))\nplt.hist(train_df['num_sold'], bins=np.linspace(0, 3000, 201),\n         density=True, label='Training')\nplt.hist(sub['num_sold'], bins=np.linspace(0, 3000, 201),\n         density=True, rwidth=0.5, label='Test predictions')\nplt.xlabel('num_sold')\nplt.ylabel('Frequency')\nplt.legend()\nplt.show()\n\n# Create a rounded submission file\nsub_rounded = sub.copy()\nsub_rounded['num_sold'] = sub_rounded['num_sold'].round()\nsub_rounded.to_csv('submission.csv', index=False)\nsub_rounded","metadata":{"execution":{"iopub.status.busy":"2022-02-06T07:09:35.251465Z","iopub.execute_input":"2022-02-06T07:09:35.252397Z","iopub.status.idle":"2022-02-06T07:09:37.437623Z","shell.execute_reply.started":"2022-02-06T07:09:35.252333Z","shell.execute_reply":"2022-02-06T07:09:37.436849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## congratuation to all the winners and thanks for sharing the notebooks.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}