{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#### Title\nIn this exercise I show one of possible ways of using Catboost for time series forecast.<br>\nThis is a very basic model, so please don't expect high score, you can use it as a baseline for further experments.","metadata":{}},{"cell_type":"code","source":"import os\nimport datetime\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom catboost import CatBoostRegressor\n\nplt.rcParams[\"figure.figsize\"] = (20,10)\npd.options.display.max_rows = None\npd.options.mode.chained_assignment = None  #Disable pandas warnings","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_raw = pd.read_csv('../input/tabular-playground-series-jan-2022/train.csv')\ntest_raw = pd.read_csv('../input/tabular-playground-series-jan-2022/test.csv')\nprint(train_raw.shape, test_raw.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is not SMAPE metric in sklearn, so I created the own one.<br>\nAs soon as SMAPE is a symmetrical metric you can send predictions and real results (A and B arguments) in any order","metadata":{}},{"cell_type":"code","source":"def SMAPE (A, B):\n    return 100/len(A) * np.sum(2 * np.abs(B - A) / (np.abs(A) + np.abs(B)))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here I add some basic additional features","metadata":{}},{"cell_type":"code","source":"train_tmp = train_raw.copy()\ntest_tmp = test_raw.copy()\n\n#Convert date column to pandas datetime\ntrain_tmp['date'] = pd.to_datetime(train_tmp['date'])\ntest_tmp['date'] = pd.to_datetime(test_tmp['date'])\n\n#Day of month\ntrain_tmp['day'] = train_tmp['date'].dt.day\ntest_tmp['day'] = test_tmp['date'].dt.day\n\n#Day of year\ntrain_tmp['day_year'] = train_tmp['date'].dt.dayofyear\ntest_tmp['day_year'] = train_tmp['date'].dt.dayofyear\n\n#Month\ntrain_tmp['month'] = train_tmp['date'].dt.month\ntest_tmp['month'] = test_tmp['date'].dt.month\n\n#Day of week (0-6 for Mon-Sun)\ntrain_tmp['week_day'] = train_tmp['date'].dt.dayofweek\ntest_tmp['week_day'] = test_tmp['date'].dt.dayofweek\n\n#Week of year\ntrain_tmp['week'] = train_tmp['date'].dt.isocalendar().week.astype(int)\ntest_tmp['week'] = test_tmp['date'].dt.isocalendar().week.astype(int)\n\n#Weekend (0 if not, 1 if yes)\ntrain_tmp['weekend'], test_tmp['weekend'] = 0, 0\ntrain_tmp.loc[train_tmp['week_day'] >= 5, 'weekend'] = 1\ntest_tmp.loc[test_tmp['week_day'] >= 5, 'weekend'] = 1","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Segments encoding\nWe have 3 contries, 2 stores and 3 products. It means that actually we have 3 x 2 x 3 = 18 segments and have to do all operations 18 times, for each segment<br>\nHere I create and encode 18 segments (0-17) from ```country```, ```store``` and ```product``` features.","metadata":{}},{"cell_type":"code","source":"train_tmp['seg'] = (train_tmp['country'] + train_tmp['store'] + train_tmp['product']).astype('category').cat.codes\ntest_tmp['seg'] = (test_tmp['country'] + test_tmp['store'] + test_tmp['product']).astype('category').cat.codes\n\nseg_count = train_tmp['seg'].nunique()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here the plot for all 18 segments","metadata":{}},{"cell_type":"code","source":"for i in range(seg_count):\n    train_tmp[train_tmp['seg'] == i]['num_sold'].plot()   ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Additional features\nFortunately, the test set continues in time with the training set that let us use 365 days lag. Here I create it for every of 18 segments.","metadata":{}},{"cell_type":"code","source":"#Function to create lag for one segment\ndef lag_365 (df, seg):\n    df.loc[df['seg'] == seg, 'Lag_365'] = df.loc[df['seg'] == seg]['num_sold'].shift(365)\n    return df\n\n#Merge train and test set to get Lag data from the train set\nmerged = pd.concat([train_tmp, test_tmp])\n\n#The loop to create lags for 18 sements\nfor i in range(seg_count):\n    merged = lag_365(merged, i)\n\n#Split train and test set back\ntrain_tmp = merged.iloc[:len(train_tmp)]\ntest_tmp = merged.iloc[len(train_tmp):].drop('num_sold', axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Drop meanless features","metadata":{}},{"cell_type":"code","source":"train_tmp = train_tmp.drop(['country', 'store', 'product', 'row_id'], axis=1)\ntest_tmp = test_tmp.drop(['country', 'store', 'product', 'row_id'], axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Split validation set from train set. I wil use 2018 for validation and all the rest for training.","metadata":{}},{"cell_type":"code","source":"split_date = datetime.datetime(2018, 1, 1)\n\ntrain = train_tmp[train_tmp['date'] < split_date]\nvalid = train_tmp[train_tmp['date'] >= split_date]\n\nX_train = train.drop('num_sold', axis=1)\ny_train = train[['seg','num_sold']]\nX_valid = valid.drop('num_sold', axis=1)\ny_valid = valid[['seg','num_sold']]\n\nprint(X_train.shape, y_train.shape, X_valid.shape, y_valid.shape)\nprint(train.shape, valid.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Out of fold validation\nAs we have 18 segments we have to create the function to train and to validate model for every segment","metadata":{}},{"cell_type":"code","source":"def oof_validation (train_X, train_y, valid_X, valid_y, seg):\n    \n    #This is very simple model, almost by defalut\n    mod = CatBoostRegressor(random_seed = 17,      #Random seed\n                            thread_count = 4,      #CPU cores at Kaggle notebook\n                            verbose = 0,           #Silent mode\n                            eval_metric = 'SMAPE', #Copmetition metric\n                            has_time = True)       #Turn off shuffle\n                            \n    fit = mod.fit(\n                  train_X[train_X['seg'] == seg],\n                  train_y[train_y['seg'] == seg]['num_sold'],\n                  eval_set = (\n                              valid_X[valid_X['seg'] == seg],\n                              valid_y[valid_y['seg'] == seg]['num_sold']\n                              ),\n                  )\n    pred = mod.predict(valid_X[valid_X['seg'] == seg])\n    \n    #Here I use my function to calculate SMAPE\n    smape = SMAPE(pred, valid_y[valid_y['seg'] == seg]['num_sold'])\n    print('OOF SMAPE for segment', seg, 'is:', \"%.5f\" % smape)\n    \n    #The fuction returns SMAPE for every segment\n    return smape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here I call the OOF function 18 times and collect SMAPE for each run","metadata":{}},{"cell_type":"code","source":"score = []\nfor i in range(seg_count):\n    score.append(oof_validation(X_train, y_train, X_valid, y_valid, i))\nprint('---')\nprint('Mean OOF SMAPE is:', \"%.5f\" % (sum(score) / seg_count)) ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Predictions\nI will train the model on a full training set","metadata":{}},{"cell_type":"code","source":"X_train_full = train_tmp.drop('num_sold', axis=1)\ny_train_full = train_tmp[['seg','num_sold']]\nX_test_full = test_tmp\nprint(X_train_full.shape, y_train_full.shape, X_test_full.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The very similiar function to generate predictions","metadata":{}},{"cell_type":"code","source":"def predictions (train_X, train_y, test_X, seg):\n    \n    mod = CatBoostRegressor(random_seed = 17,      \n                            thread_count = 4,      #CPU cores at Kaggle notebook\n                            verbose = 0,           #Silent mode\n                            eval_metric = 'SMAPE', #Copmetition metric\n                            has_time = True)       #Turn off shuffle \n                            \n    fit = mod.fit(train_X[train_X['seg'] == seg], train_y[train_y['seg'] == seg]['num_sold'])\n    pred = mod.predict(test_X[test_X['seg'] == seg])\n    test_X.loc[test_X['seg'] == seg, 'num_sold'] = pred\n    print('Predictions for segment', seg, 'complete')\n    return test_X","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Call the function 18 times to generate predictions for every segment","metadata":{}},{"cell_type":"code","source":"for j in range(seg_count):\n    df = predictions(X_train_full, y_train_full, X_test_full, j)\n    X_test_full.loc[X_test_full['seg'] == j, 'num_sold'] = df['num_sold']\nprint('---')\nprint('Predictions complete')\nX_test_full.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Test plot (predictions vs. previous year) to check that prediction looks realistic","metadata":{}},{"cell_type":"code","source":"segment = 5 #choose any\nax = y_valid[y_valid['seg'] == segment]['num_sold'].reset_index(drop = True).plot(color = 'black')\nX_test_full[X_test_full['seg'] == segment]['num_sold'].reset_index(drop = True).plot(ax=ax)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Submission","metadata":{}},{"cell_type":"code","source":"sub = pd.read_csv('../input/tabular-playground-series-jan-2022/sample_submission.csv')\nsub['num_sold'] = X_test_full['num_sold']\nsub.to_csv('submission.csv', index=False) \nsub.head()","metadata":{},"execution_count":null,"outputs":[]}]}