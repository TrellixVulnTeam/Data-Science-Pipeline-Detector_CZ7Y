{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-18T05:13:40.588418Z","iopub.execute_input":"2022-01-18T05:13:40.589243Z","iopub.status.idle":"2022-01-18T05:13:40.625364Z","shell.execute_reply.started":"2022-01-18T05:13:40.589111Z","shell.execute_reply":"2022-01-18T05:13:40.624254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv(\"../input/tabular-playground-series-jan-2022/train.csv\")\ndf_test = pd.read_csv(\"../input/tabular-playground-series-jan-2022/test.csv\")\ndf_sample = pd.read_csv(\"../input/tabular-playground-series-jan-2022/sample_submission.csv\")\n\ndf_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-18T05:13:40.626888Z","iopub.execute_input":"2022-01-18T05:13:40.627137Z","iopub.status.idle":"2022-01-18T05:13:40.724336Z","shell.execute_reply.started":"2022-01-18T05:13:40.627108Z","shell.execute_reply":"2022-01-18T05:13:40.723302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"targets = df_train.iloc[:, -1]\ndf_train.drop('num_sold', axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T05:13:40.725657Z","iopub.execute_input":"2022-01-18T05:13:40.725996Z","iopub.status.idle":"2022-01-18T05:13:40.749094Z","shell.execute_reply.started":"2022-01-18T05:13:40.725942Z","shell.execute_reply":"2022-01-18T05:13:40.748194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = pd.DataFrame()","metadata":{"execution":{"iopub.status.busy":"2022-01-18T05:13:40.75139Z","iopub.execute_input":"2022-01-18T05:13:40.75172Z","iopub.status.idle":"2022-01-18T05:13:40.757795Z","shell.execute_reply.started":"2022-01-18T05:13:40.751656Z","shell.execute_reply":"2022-01-18T05:13:40.756356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.columns","metadata":{"execution":{"iopub.status.busy":"2022-01-18T05:13:40.759147Z","iopub.execute_input":"2022-01-18T05:13:40.759895Z","iopub.status.idle":"2022-01-18T05:13:40.772526Z","shell.execute_reply.started":"2022-01-18T05:13:40.759838Z","shell.execute_reply":"2022-01-18T05:13:40.77176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['product'].unique()","metadata":{"execution":{"iopub.status.busy":"2022-01-18T05:13:40.773753Z","iopub.execute_input":"2022-01-18T05:13:40.77399Z","iopub.status.idle":"2022-01-18T05:13:40.795527Z","shell.execute_reply.started":"2022-01-18T05:13:40.773964Z","shell.execute_reply":"2022-01-18T05:13:40.794377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"country = {'Finland':0, 'Norway':1, 'Sweden':2}\nfeatures['country'] = df_train.country.map(country)\nstore = {'KaggleMart':0, 'KaggleRama':1}\nfeatures['store'] = df_train.store.map(store)\nproduct = {'Kaggle Mug':0, 'Kaggle Hat':1, 'Kaggle Sticker':2}\nfeatures['product'] = df_train['product'].map(product)\n\nfeatures.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-18T05:13:40.797046Z","iopub.execute_input":"2022-01-18T05:13:40.797517Z","iopub.status.idle":"2022-01-18T05:13:40.827257Z","shell.execute_reply.started":"2022-01-18T05:13:40.797484Z","shell.execute_reply":"2022-01-18T05:13:40.826275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features.corrwith(targets)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T05:13:40.828416Z","iopub.execute_input":"2022-01-18T05:13:40.828659Z","iopub.status.idle":"2022-01-18T05:13:40.845113Z","shell.execute_reply.started":"2022-01-18T05:13:40.828629Z","shell.execute_reply":"2022-01-18T05:13:40.844392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Normalization\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\nfeatures = MinMaxScaler().fit_transform(features)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T05:13:40.846938Z","iopub.execute_input":"2022-01-18T05:13:40.847327Z","iopub.status.idle":"2022-01-18T05:13:41.950414Z","shell.execute_reply.started":"2022-01-18T05:13:40.84729Z","shell.execute_reply":"2022-01-18T05:13:41.949482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Applying the same mechanism to the test set\ntest_set = pd.DataFrame()\ntest_set['country'] = df_test.country.map(country)\ntest_set['store'] = df_test.store.map(store)\ntest_set['product'] = df_test['product'].map(product)\n\ntest_set.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-18T05:13:41.95155Z","iopub.execute_input":"2022-01-18T05:13:41.952009Z","iopub.status.idle":"2022-01-18T05:13:41.97188Z","shell.execute_reply.started":"2022-01-18T05:13:41.951963Z","shell.execute_reply":"2022-01-18T05:13:41.971079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import matplotlib.pyplot as plt\n\n# plt.figure(dpi=100)\n# plt.plot(targets[:500])\n# plt.show","metadata":{"execution":{"iopub.status.busy":"2022-01-18T05:13:41.973048Z","iopub.execute_input":"2022-01-18T05:13:41.97354Z","iopub.status.idle":"2022-01-18T05:13:41.976991Z","shell.execute_reply.started":"2022-01-18T05:13:41.973496Z","shell.execute_reply":"2022-01-18T05:13:41.976402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split, cross_validate, RandomizedSearchCV, GridSearchCV\nX_train, X_valid, y_train, y_valid = train_test_split(features, targets, test_size=0.2, random_state=1334)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T05:13:41.978042Z","iopub.execute_input":"2022-01-18T05:13:41.978584Z","iopub.status.idle":"2022-01-18T05:13:42.061262Z","shell.execute_reply.started":"2022-01-18T05:13:41.978541Z","shell.execute_reply":"2022-01-18T05:13:42.060461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor","metadata":{"execution":{"iopub.status.busy":"2022-01-18T05:13:42.062467Z","iopub.execute_input":"2022-01-18T05:13:42.062836Z","iopub.status.idle":"2022-01-18T05:13:42.271016Z","shell.execute_reply.started":"2022-01-18T05:13:42.062807Z","shell.execute_reply":"2022-01-18T05:13:42.270165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define the parameter space that will be searched over\n# param_distribution = {\"n_estimators\": np.random.randint(1, 101, 5),\n#                      \"max_depth\": np.random.randint(1, 10, 5),\n#                      \"max_leaf_nodes\":np.random.randint(2, 10, 5),\n#                      }\n\n# # now create a searchCV object and fit it to that data\n# search = RandomizedSearchCV(estimator=RandomForestRegressor(random_state=0),\n#                            param_distributions=param_distribution,\n#                            n_iter=5)\n\n# rf_model = search.fit(X_train, y_train)\n# # call rf_model.best_params to get the parameters obtained by the randomized search\n\n# using the best values obtained\n# prev n_e = 66\nrf_best_model = RandomForestRegressor(n_estimators=100,\n                                     max_leaf_nodes=4, \n                                     max_depth=9).fit(X_train, y_train)\n\nrf_best_predictions = rf_best_model.predict(X_valid)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T05:13:42.273116Z","iopub.execute_input":"2022-01-18T05:13:42.273365Z","iopub.status.idle":"2022-01-18T05:13:42.675186Z","shell.execute_reply.started":"2022-01-18T05:13:42.273334Z","shell.execute_reply":"2022-01-18T05:13:42.674289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ada_distributions = {\"n_estimators\":np.random.randint(50, 100, 5),\n#                     \"learning_rate\":np.linspace(1e-5, 1, 5),\n#                     \"loss\":['linear', 'square']}\n\n# ada_search = RandomizedSearchCV(estimator=AdaBoostRegressor(random_state=0),\n#                                param_distributions=ada_distributions,\n#                                n_iter=5)\n\n# ada_model = ada_search.fit(X_train, y_train)\n# prev n_e = 62\nada_best_model = AdaBoostRegressor(n_estimators=100, loss='linear', learning_rate=1.0).fit(X_train, y_train)\nada_best_prediction = ada_best_model.predict(X_valid)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T05:13:42.676438Z","iopub.execute_input":"2022-01-18T05:13:42.677336Z","iopub.status.idle":"2022-01-18T05:13:42.915817Z","shell.execute_reply.started":"2022-01-18T05:13:42.677291Z","shell.execute_reply":"2022-01-18T05:13:42.915024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# grad_params = {\"loss\":['ls', 'huber'],\n#               \"learning_rate\":np.linspace(1e-5, 1, 10)}\n\n# grad_search = RandomizedSearchCV(estimator=GradientBoostingRegressor(random_state=0),\n#                                 param_distributions=grad_params,\n#                                 n_iter=5)\n\n# grad_model = grad_search.fit(X_train, y_train)\n# prev lr = 0.556\ngrad_best_model = GradientBoostingRegressor(loss='huber',\n                                           learning_rate=0.1,\n                                           max_depth=7,\n                                           n_estimators=500,\n                                           min_samples_leaf=5).fit(X_train, y_train)\ngrad_best_prediction = grad_best_model.predict(X_valid)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T05:13:42.916927Z","iopub.execute_input":"2022-01-18T05:13:42.91726Z","iopub.status.idle":"2022-01-18T05:13:50.045052Z","shell.execute_reply.started":"2022-01-18T05:13:42.91723Z","shell.execute_reply":"2022-01-18T05:13:50.044339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error, mean_squared_error\n\ndef print_metrics(name, pred, true_val):\n    print(name)\n    print('.'*30)\n    print('MAE: ', mean_absolute_error(pred, true_val))\n    print('MSE: ', mean_squared_error(pred, true_val))\n    print()\n    \nprint_metrics(name='Random Forest', pred=rf_best_predictions, true_val=y_valid)\nprint_metrics(name='Ada boost', pred=ada_best_prediction, true_val=y_valid)\nprint_metrics(name='Gradient boosting', pred=grad_best_prediction, true_val=y_valid)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T05:13:50.04618Z","iopub.execute_input":"2022-01-18T05:13:50.046495Z","iopub.status.idle":"2022-01-18T05:13:50.060651Z","shell.execute_reply.started":"2022-01-18T05:13:50.046468Z","shell.execute_reply":"2022-01-18T05:13:50.059767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# From the evaluation, gradient boostin performs better than the other two\n# So, I will be using it \npredictions = grad_best_model.predict(test_set)\n\ndf_sample.num_sold = np.round(predictions,0).astype(int)\ndf_sample.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T05:13:50.06231Z","iopub.execute_input":"2022-01-18T05:13:50.062539Z","iopub.status.idle":"2022-01-18T05:13:50.1151Z","shell.execute_reply.started":"2022-01-18T05:13:50.06251Z","shell.execute_reply":"2022-01-18T05:13:50.114173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_sample.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-18T05:13:50.116493Z","iopub.execute_input":"2022-01-18T05:13:50.116979Z","iopub.status.idle":"2022-01-18T05:13:50.12564Z","shell.execute_reply.started":"2022-01-18T05:13:50.116946Z","shell.execute_reply":"2022-01-18T05:13:50.124999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}