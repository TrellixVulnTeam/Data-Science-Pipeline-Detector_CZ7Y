{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport math\nimport dateutil\n\nfrom statsmodels.tsa.deterministic import CalendarFourier, DeterministicProcess\nfrom category_encoders import MEstimateEncoder\nfrom sklearn.model_selection import KFold, cross_val_score\n\nfrom scipy.special import boxcox1p, inv_boxcox1p\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        pass\n        #print(os.path.join(dirname, filename))\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom warnings import simplefilter\nsimplefilter(\"ignore\")  # ignore warnings to clean up output cells\n\n# Set Matplotlib defaults\nplt.style.use(\"seaborn-whitegrid\")\nplt.rc(\"figure\", autolayout=True, figsize=(11, 5))\nplt.rc(\n    \"axes\",\n    labelweight=\"bold\",\n    labelsize=\"large\",\n    titleweight=\"bold\",\n    titlesize=14,\n    titlepad=10,\n)\nplot_params = dict(\n    color=\"0.75\",\n    style=\".-\",\n    markeredgecolor=\"0.25\",\n    markerfacecolor=\"0.25\",\n    legend=False,\n)\n%config InlineBackend.figure_format = 'retina'\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-30T13:05:54.174954Z","iopub.execute_input":"2022-01-30T13:05:54.175528Z","iopub.status.idle":"2022-01-30T13:05:54.745111Z","shell.execute_reply.started":"2022-01-30T13:05:54.175428Z","shell.execute_reply":"2022-01-30T13:05:54.744075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv(\"/kaggle/input/tabular-playground-series-jan-2022/train.csv\",parse_dates=[\"date\"])\n#GDP = pd.read_csv(\"/kaggle/input/gdp-20152019-finland-norway-and-sweden/GDP_data_2015_to_2019_Finland_Norway_Sweden.csv\")\nHolidays = pd.read_csv(\"/kaggle/input/holidays-finland-norway-sweden-20152019/Holidays_Finland_Norway_Sweden_2015-2019.csv\", parse_dates=['Date'])\nGDP = pd.read_csv(\"/kaggle/input/gdp-per-capita-finland-norway-sweden-201519/GDP_per_capita_2015_to_2019_Finland_Norway_Sweden.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-01-30T13:05:54.746937Z","iopub.execute_input":"2022-01-30T13:05:54.74725Z","iopub.status.idle":"2022-01-30T13:05:54.792512Z","shell.execute_reply.started":"2022-01-30T13:05:54.747207Z","shell.execute_reply":"2022-01-30T13:05:54.791779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Restructring GDP:\nGDP_df = GDP.copy()\n# For working with GDP instead of GDP per capita\n# GDP_df['Finland'] = GDP_df['Finland'].copy()\n# GDP_df['Norway']  = GDP_df['Norway'].copy()\n# GDP_df['Sweden']  = GDP_df['GDP_Sweden'].copy()\n# GDP_df.drop(['GDP_Finland','GDP_Norway','GDP_Sweden'], axis=1, inplace=True)\n#GDP_df.set_index(['year'], inplace=True)\nGDP_df = GDP_df.melt(id_vars=['year'], var_name=['country'], value_name='GDP')","metadata":{"execution":{"iopub.status.busy":"2022-01-30T13:05:54.794882Z","iopub.execute_input":"2022-01-30T13:05:54.795816Z","iopub.status.idle":"2022-01-30T13:05:54.804755Z","shell.execute_reply.started":"2022-01-30T13:05:54.795777Z","shell.execute_reply":"2022-01-30T13:05:54.804127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA:","metadata":{}},{"cell_type":"markdown","source":"## Holiday effect:\nSales are expected to rise with Holidays, so here I'll study the effect of Holidays through a Rolling window of 7 days to suppress the weekend effect. and see how it changes with regards to Holidays extracted from the Holiday dataset.","metadata":{}},{"cell_type":"code","source":"country = 'Finland'\nstore = 'KaggleMart'\nproduct = 'Kaggle Mug'\nyear = 2016\n\nex_df  = df_train.query('country == @country and store == @store and product == @product and date.dt.year==@year')\nex_df.set_index(['date'],inplace=True)\n\nholiday_dates = Holidays.query('Country == @country and Date.dt.year == @year')['Date']\nmoving_average = ex_df.rolling(\n    window=7,       # 365-day window\n    center=True,      # puts the average at the center of the window\n    min_periods=4,  # choose about half the window size\n).mean()\n\nax = ex_df.num_sold.plot(y='num_sold',style=\".\", color=\"0.5\")\nfor date in holiday_dates:\n    ax.axvline(date, color='red', alpha=0.6, linestyle = '--')\n\nmoving_average.plot(y='num_sold',\n    ax=ax, linewidth=3, title=\"Weekly moving Average\", legend=False,\n);","metadata":{"execution":{"iopub.status.busy":"2022-01-30T13:05:54.806551Z","iopub.execute_input":"2022-01-30T13:05:54.806789Z","iopub.status.idle":"2022-01-30T13:05:55.527608Z","shell.execute_reply.started":"2022-01-30T13:05:54.806761Z","shell.execute_reply":"2022-01-30T13:05:55.526882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It appears the effect of Holidays starts in a window after each Holiday. We may need to add extra features to encompass this.","metadata":{}},{"cell_type":"markdown","source":"## Seasonality Effect (Weekends...etc):","metadata":{}},{"cell_type":"markdown","source":"Before looking for any seasonality effects, a Periodogram might be useful:","metadata":{}},{"cell_type":"code","source":"#Bloc of code from Kaggle's Time-Series course. Modified to allow Periodogram to have different title and color\n\nfrom statsmodels.tsa.deterministic import CalendarFourier, DeterministicProcess\n\n# annotations: https://stackoverflow.com/a/49238256/5769929\ndef seasonal_plot(X, y, period, freq, ax=None):\n    if ax is None:\n        _, ax = plt.subplots()\n    palette = sns.color_palette(\"husl\", n_colors=X[period].nunique(),)\n    ax = sns.lineplot(\n        x=freq,\n        y=y,\n        hue=period,\n        data=X,\n        ci=False,\n        ax=ax,\n        palette=palette,\n        legend=False,\n    )\n    ax.set_title(f\"Seasonal Plot ({period}/{freq})\")\n    for line, name in zip(ax.lines, X[period].unique()):\n        y_ = line.get_ydata()[-1]\n        ax.annotate(\n            name,\n            xy=(1, y_),\n            xytext=(6, 0),\n            color=line.get_color(),\n            xycoords=ax.get_yaxis_transform(),\n            textcoords=\"offset points\",\n            size=14,\n            va=\"center\",\n        )\n    return ax\n\n\ndef plot_periodogram(ts, detrend='linear', ax=None, title=\"Periodogram\", color=\"purple\", label=None):\n    from scipy.signal import periodogram\n    fs = pd.Timedelta(\"1Y\") / pd.Timedelta(\"1D\")\n    freqencies, spectrum = periodogram(\n        ts,\n        fs=fs,\n        detrend=detrend,\n        window=\"boxcar\",\n        scaling='spectrum',\n    )\n    if ax is None:\n        _, ax = plt.subplots()\n    ax.step(freqencies, spectrum, color=color, label=label)\n    ax.set_xscale(\"log\")\n    ax.set_xticks([1, 2, 4, 6, 12, 26, 52, 104])\n    ax.set_xticklabels(\n        [\n            \"Annual (1)\",\n            \"Semiannual (2)\",\n            \"Quarterly (4)\",\n            \"Bimonthly (6)\",\n            \"Monthly (12)\",\n            \"Biweekly (26)\",\n            \"Weekly (52)\",\n            \"Semiweekly (104)\",\n        ],\n        rotation=30,\n    )\n    ax.ticklabel_format(axis=\"y\", style=\"sci\", scilimits=(0, 0))\n    ax.set_ylabel(\"Variance\")\n    ax.set_title(title)\n    return ax\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-30T13:05:55.528826Z","iopub.execute_input":"2022-01-30T13:05:55.529622Z","iopub.status.idle":"2022-01-30T13:05:55.540816Z","shell.execute_reply.started":"2022-01-30T13:05:55.529585Z","shell.execute_reply":"2022-01-30T13:05:55.540222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To account for country/store differences, I'll plot all country periodicities for each product (One store):","metadata":{}},{"cell_type":"code","source":"fig, (ax1, ax2, ax3) = plt.subplots(3, figsize=(15,15))\n\ncountry = 'Norway'\nstore = 'KaggleRama'\nproduct = 'Kaggle Mug'\n\n\nex_df  = df_train.query('country == @country and store == @store and product == @product')\nex_df.set_index(['date'],inplace=True)\nplot_periodogram(ex_df.num_sold, ax=ax1, title=\"Periodogram: Kaggle Mug\", label=\"Norway\");\n\ncountry = 'Finland'\nex_df  = df_train.query('country == @country and store == @store and product == @product')\nex_df.set_index(['date'],inplace=True)\nplot_periodogram(ex_df.num_sold, ax=ax1, label=\"Finland\", color=\"red\", title=\"Periodogram: Kaggle Mug\");\n\ncountry = 'Sweden'\nex_df  = df_train.query('country == @country and store == @store and product == @product')\nex_df.set_index(['date'],inplace=True)\nplot_periodogram(ex_df.num_sold, ax=ax1, label=\"Sweden\", color=\"blue\", title=\"Periodogram: Kaggle Mug\");\nax1.legend()\n\ncountry = 'Norway'\nstore = 'KaggleRama'\nproduct = 'Kaggle Sticker'\n\n\nex_df  = df_train.query('country == @country and store == @store and product == @product')\nex_df.set_index(['date'],inplace=True)\nplot_periodogram(ex_df.num_sold, ax=ax2, title=\"Periodogram: Kaggle Sticker\", label=\"Norway\");\n\ncountry = 'Finland'\nex_df  = df_train.query('country == @country and store == @store and product == @product')\nex_df.set_index(['date'],inplace=True)\nplot_periodogram(ex_df.num_sold, ax=ax2, label=\"Finland\", color=\"red\", title=\"Periodogram: Kaggle Sticker\");\n\ncountry = 'Sweden'\nex_df  = df_train.query('country == @country and store == @store and product == @product')\nex_df.set_index(['date'],inplace=True)\nplot_periodogram(ex_df.num_sold, ax=ax2, label=\"Sweden\", color=\"blue\", title=\"Periodogram: Kaggle Sticker\");\nax2.legend()\n\ncountry = 'Norway'\nstore = 'KaggleRama'\nproduct = 'Kaggle Hat'\n\n\nex_df  = df_train.query('country == @country and store == @store and product == @product')\nex_df.set_index(['date'],inplace=True)\nplot_periodogram(ex_df.num_sold, ax=ax3, title=\"Periodogram: Kaggle Hat\", label=\"Norway\");\n\ncountry = 'Finland'\nex_df  = df_train.query('country == @country and store == @store and product == @product')\nex_df.set_index(['date'],inplace=True)\nplot_periodogram(ex_df.num_sold, ax=ax3, label=\"Finland\", color=\"red\", title=\"Periodogram: Kaggle Hat\");\n\ncountry = 'Sweden'\nex_df  = df_train.query('country == @country and store == @store and product == @product')\nex_df.set_index(['date'],inplace=True)\nplot_periodogram(ex_df.num_sold, ax=ax3, label=\"Sweden\", color=\"blue\", title=\"Periodogram: Kaggle Hat\");\nax3.legend()\n\n","metadata":{"execution":{"iopub.status.busy":"2022-01-30T13:05:55.541994Z","iopub.execute_input":"2022-01-30T13:05:55.542502Z","iopub.status.idle":"2022-01-30T13:05:57.532567Z","shell.execute_reply.started":"2022-01-30T13:05:55.542471Z","shell.execute_reply":"2022-01-30T13:05:57.531714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Then vary the store for fixed country (To avoid cluttering the notebook, only one country was used; you can edit the notebook and change the country):","metadata":{}},{"cell_type":"code","source":"fig, (ax1, ax2, ax3) = plt.subplots(3, figsize=(15,15))\n\ncountry = 'Sweden'\nstore = 'KaggleRama'\nproduct = 'Kaggle Mug'\n\nex_df  = df_train.query('country == @country and store == @store and product == @product')\nex_df.set_index(['date'],inplace=True)\nplot_periodogram(ex_df.num_sold, ax=ax1, title=f\"Periodogram: Kaggle Mug ({country})\", label=\"KaggleRama\", color=\"blue\");\n\nstore = 'KaggleMart'\nex_df  = df_train.query('country == @country and store == @store and product == @product')\nex_df.set_index(['date'],inplace=True)\nplot_periodogram(ex_df.num_sold, ax=ax1, label=\"KaggleMart\", color=\"red\", title=f\"Periodogram: Kaggle Mug ({country})\");\nax1.legend()\n\nstore = 'KaggleRama'\nproduct = 'Kaggle Sticker'\n\nex_df  = df_train.query('country == @country and store == @store and product == @product')\nex_df.set_index(['date'],inplace=True)\nplot_periodogram(ex_df.num_sold, ax=ax2, title=f\"Periodogram: Kaggle Sticker ({country})\", label=\"KaggleRama\", color=\"blue\");\n\nstore = 'KaggleMart'\nex_df  = df_train.query('country == @country and store == @store and product == @product')\nex_df.set_index(['date'],inplace=True)\nplot_periodogram(ex_df.num_sold, ax=ax2, label=\"KaggleMart\", color=\"red\", title=f\"Periodogram: Kaggle Sticker ({country})\");\nax2.legend()\n\nstore = 'KaggleRama'\nproduct = 'Kaggle Hat'\n\nex_df  = df_train.query('country == @country and store == @store and product == @product')\nex_df.set_index(['date'],inplace=True)\nplot_periodogram(ex_df.num_sold, ax=ax3, title=f\"Periodogram: Kaggle Hat ({country})\", label=\"KaggleRama\", color=\"blue\");\n\nstore = 'KaggleMart'\nex_df  = df_train.query('country == @country and store == @store and product == @product')\nex_df.set_index(['date'],inplace=True)\nplot_periodogram(ex_df.num_sold, ax=ax3, label=\"KaggleMart\", color=\"red\", title=f\"Periodogram: Kaggle Sticker ({country})\");\nax3.legend()\n","metadata":{"execution":{"iopub.status.busy":"2022-01-30T13:05:57.533796Z","iopub.execute_input":"2022-01-30T13:05:57.534263Z","iopub.status.idle":"2022-01-30T13:05:59.300295Z","shell.execute_reply.started":"2022-01-30T13:05:57.534224Z","shell.execute_reply":"2022-01-30T13:05:59.299632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The periodicity appears to vary significantly from country to country, from store to store and from product to product. Though the product to product periodicity appears to be the strongest and seems de-coupled from the country or store; The country/store combination should be taken into account. (Thanks ambrosM)","metadata":{}},{"cell_type":"code","source":"X = ex_df\n\nX[\"day\"] = X.index.dayofweek  # the x-axis (freq)\nX[\"week\"] = X.index.week  # the seasonal period (period)\n\n# days within a year\nX[\"dayofyear\"] = X.index.dayofyear\nX[\"year\"] = X.index.year\nfig, (ax0, ax1) = plt.subplots(2, 1, figsize=(11, 6))\nseasonal_plot(X, y=\"num_sold\", period=\"week\", freq=\"day\", ax=ax0)\nseasonal_plot(X, y=\"num_sold\", period=\"year\", freq=\"dayofyear\", ax=ax1);","metadata":{"execution":{"iopub.status.busy":"2022-01-30T13:05:59.301238Z","iopub.execute_input":"2022-01-30T13:05:59.301983Z","iopub.status.idle":"2022-01-30T13:06:08.13576Z","shell.execute_reply.started":"2022-01-30T13:05:59.30194Z","shell.execute_reply":"2022-01-30T13:06:08.134798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Sold unit numbers vary with weekdays (predictibly) so encoding Weekdays/Weekends should be done. There is a yearly pattern as well as found from the Periodogram.","metadata":{}},{"cell_type":"markdown","source":"Conclusion:\n\n+ Add a x-day window after each Holiday. (Probably by augmenting the Holiday Dataframe itself)\n+ Encode weekdays.\n+ Encode a bimonthly/monthly Fourier Feature tied to each product at a time and each store/country combination.","metadata":{}},{"cell_type":"markdown","source":"## Distribution of num_sold:","metadata":{}},{"cell_type":"code","source":"sns.displot(df_train['num_sold'])\nsns.displot(np.log1p(df_train['num_sold']))","metadata":{"execution":{"iopub.status.busy":"2022-01-30T13:06:08.136872Z","iopub.execute_input":"2022-01-30T13:06:08.137148Z","iopub.status.idle":"2022-01-30T13:06:09.295641Z","shell.execute_reply.started":"2022-01-30T13:06:08.137117Z","shell.execute_reply":"2022-01-30T13:06:09.295085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Apparently the distribution of `num_sold` is not Gaussian of its own. Log-Transforming it makes it behave better.","metadata":{}},{"cell_type":"markdown","source":"# Feature Engineering:","metadata":{}},{"cell_type":"markdown","source":"Before doing any feature engineering on time features we should one-hot encode the country store and product as well as adding a GDP column.","metadata":{}},{"cell_type":"markdown","source":"Now we need to change the Holiday dataset by adding a 7-day window after each Holiday for its effect to be pronounced.","metadata":{}},{"cell_type":"code","source":"def add_holiday_window(holiday_df, window=7):\n    hol_df = holiday_df.copy()\n    for year in [2015,2016,2017,2018,2019]:\n        for country in ['Norway','Finland']:\n            date = pd.Timestamp(f\"{year}-12-31\")\n            LocalName = 'na'\n            Name = 'New Year\\'s Eve'\n            Fixed = True\n            row = pd.Series({'Date':date, 'Country':country, 'LocalName':LocalName, 'Name':Name, 'Fixed':Fixed})\n            hol_df = hol_df.append(row, ignore_index=True)\n    hol_df.sort_values(['Country','Date'], axis=0, inplace=True, ignore_index=True)\n    hol_dfiter = hol_df.copy()\n    \n    hol_dfiter.drop([\"LocalName\",\"Fixed\"], axis=1, inplace=True)\n    hol_df.drop([\"LocalName\",\"Fixed\"], axis=1, inplace=True)\n    \n    hol_dfiter['next_date'] = hol_dfiter['Date'].shift(-1)\n    for index, row in hol_dfiter.iterrows():\n        date = row['Date']\n        next_date = row['next_date']\n        if next_date == pd.Timestamp('2015-01-01'):\n            next_date = pd.Timestamp('2020-01-05')\n        \n        delta_date = (next_date - date).days - 1\n\n        delta_date = window if math.isnan(delta_date) or delta_date>(window-1) else delta_date\n        name = row['Name']\n        country = row['Country']\n        \n        new_df= pd.DataFrame()\n        new_df['Date'] = [date + pd.Timedelta(days=deltaday) for deltaday in range(1,delta_date+1)]\n        new_df['Country'] = [country for _ in range(1,delta_date+1)]\n        new_df['Name'] = [f\"{name}{deltaday}\" for deltaday in range(1,delta_date+1)]\n\n        hol_df = hol_df.append(new_df, ignore_index=True) # DO NOT REMOVE DUPLICATES\n    hol_df.sort_values(['Date'], axis=0, inplace=True, ignore_index=True)\n    #hol_df.drop(range(1057,1064),axis=0, inplace=True) #Remove the last values for Sweden for 2020\n    hol_df.columns = ['date','country','day']\n    #Add Dummies:\n    #hol_df = pd.get_dummies(hol_df, columns=['country'], drop_first=True)\n    hol_df = pd.get_dummies(hol_df, columns=['day'], drop_first=False)\n    return hol_df","metadata":{"execution":{"iopub.status.busy":"2022-01-30T13:06:09.297945Z","iopub.execute_input":"2022-01-30T13:06:09.298373Z","iopub.status.idle":"2022-01-30T13:06:09.310009Z","shell.execute_reply.started":"2022-01-30T13:06:09.298342Z","shell.execute_reply":"2022-01-30T13:06:09.309282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Before one-hot encoding, it may be a good idea to do Target Encoding on the country/store/product combination. This will be done through a cross-fold trick (Got it from the Feature Engineering course). (See hidden cell below)","metadata":{}},{"cell_type":"code","source":"class CrossFoldEncoder:\n    def __init__(self, encoder, **kwargs):\n        self.encoder_ = encoder\n        self.kwargs_ = kwargs  # keyword arguments for the encoder\n        self.cv_ = KFold(n_splits=5)\n\n    # Fit an encoder on one split and transform the feature on the\n    # other. Iterating over the splits in all folds gives a complete\n    # transformation. We also now have one trained encoder on each\n    # fold.\n    def fit_transform(self, X, y, cols):\n        self.fitted_encoders_ = []\n        self.cols_ = cols\n        X_encoded = []\n        for idx_encode, idx_train in self.cv_.split(X):\n            fitted_encoder = self.encoder_(cols=cols, **self.kwargs_)\n            fitted_encoder.fit(\n                X.iloc[idx_encode, :], y.iloc[idx_encode],\n            )\n            X_encoded.append(fitted_encoder.transform(X.iloc[idx_train, :])[cols])\n            self.fitted_encoders_.append(fitted_encoder)\n        X_encoded = pd.concat(X_encoded)\n        X_encoded.columns = [name + \"_encoded\" for name in X_encoded.columns]\n        return X_encoded\n\n    # To transform the test data, average the encodings learned from\n    # each fold.\n    def transform(self, X):\n        from functools import reduce\n\n        X_encoded_list = []\n        for fitted_encoder in self.fitted_encoders_:\n            X_encoded = fitted_encoder.transform(X)\n            X_encoded_list.append(X_encoded[self.cols_])\n        X_encoded = reduce(\n            lambda x, y: x.add(y, fill_value=0), X_encoded_list\n        ) / len(X_encoded_list)\n        X_encoded.columns = [name + \"_encoded\" for name in X_encoded.columns]\n        return X_encoded","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-30T13:06:09.311313Z","iopub.execute_input":"2022-01-30T13:06:09.312251Z","iopub.status.idle":"2022-01-30T13:06:09.327679Z","shell.execute_reply.started":"2022-01-30T13:06:09.312206Z","shell.execute_reply":"2022-01-30T13:06:09.326787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Then we encode Fourier features and week encodings for the countrystore combinations as well as products.","metadata":{}},{"cell_type":"code","source":"def fourier_encoding(df_train, drop_common=True): #Returns encodings for countrystore combinations and products.\n    fourier = CalendarFourier(freq=\"A\", order=8)\n    \n    dp = DeterministicProcess(\n        index=df_train.date.unique(),\n        constant=True,               # dummy feature for bias (y-intercept)\n        order=2,                     # trend (order 1 means linear)\n        seasonal=True,               # weekly seasonality (indicators)\n        additional_terms=[fourier],  # annual seasonality (fourier)\n        drop=True,                   # drop terms to avoid collinearity\n    )\n    train_encoding = dp.in_sample()\n    train_encoding['date'] = train_encoding.index\n    train_encoding = df_train.merge(train_encoding,on=\"date\",how=\"left\");\n    train_encoding.drop(['num_sold','year','GDP','num_sold'], axis=1, inplace=True)\n    train_encoding['countrystore'] = train_encoding['country'].astype(str) + train_encoding['store'].astype(str)\n    train_encoding = train_encoding.merge(pd.get_dummies(train_encoding, columns=['countrystore','product'], drop_first=False), how=\"left\")\n    encoded_cols = train_encoding.columns.tolist()[5:30].copy()\n    for encoded_col in encoded_cols:\n        for country in train_encoding['country'].unique():\n            for store in train_encoding['store'].unique():\n                train_encoding[f\"{encoded_col}_{country}{store}\"] = train_encoding[f\"countrystore_{country}{store}\"] * train_encoding[encoded_col]\n        for product in train_encoding['product'].unique():\n            train_encoding[f\"{encoded_col}_{product}\"] = train_encoding[f\"product_{product}\"] * train_encoding[encoded_col]\n    cols_to_drop = train_encoding.columns.tolist()[30:40]\n    train_encoding.drop(cols_to_drop, axis=1, inplace=True) #Drop countrystore/product encodings.\n    if drop_common: #If we want to drop the common columns\n        cols_to_drop = train_encoding.columns.tolist()[5:30]\n        train_encoding.drop(cols_to_drop, axis=1, inplace=True) #Drop countrystore/product encodings.\n    return train_encoding","metadata":{"execution":{"iopub.status.busy":"2022-01-30T13:06:09.328836Z","iopub.execute_input":"2022-01-30T13:06:09.329122Z","iopub.status.idle":"2022-01-30T13:06:09.344822Z","shell.execute_reply.started":"2022-01-30T13:06:09.329091Z","shell.execute_reply":"2022-01-30T13:06:09.343891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from  category_encoders.cat_boost import CatBoostEncoder\nlam=0.00\ndef process_data(df, df_test=None):\n    df = df.copy()\n    if type(df_test) != type(None):\n        df_test = df_test.copy()\n        df_test['num_sold'] = 0\n        df = df.append(df_test)\n        \n    #Step 1: GDP:\n    \n    df['year'] = df.date.dt.year\n    #df['week'] = df.date.dt.week\n    df = df.merge(GDP_df, on=['country','year'], how=\"left\")\n    #Step 2: Encoding Fourier features and week features\n    encoding = fourier_encoding(df)\n    df = df.merge(encoding, how=\"left\")\n\n    #Step 3: Holidays:\n    hol_df = add_holiday_window(Holidays, window=9)\n    df = df.merge(hol_df, on=[\"date\",\"country\"], how=\"left\");\n    \n    #Filling missing values with 0, due to left join:\n    df = df.fillna(0)\n\n    #Step 4: Log-transforming num_sold:\n    df['num_sold'] =boxcox1p(df['num_sold'],lam)\n    \n    #Starting here, the train-validation split must be done to avoid data leakage during Target Encoding.\n    if type(df_test) != type(None):\n        df_test = df.query(\"year==2019\")\n        df_test.drop(['row_id','num_sold'], axis=1, inplace=True)\n    \n    \n    df_val = df.query(\"year==2018\")\n    df = df.query(\"year < 2018\")\n    #df, df_val = df.loc[:19727], df.loc[19728:]\n    \n    y_train = df.pop('num_sold')\n    y_val   = df_val.pop('num_sold')\n    \n    df.drop(['row_id'], axis=1, inplace=True)\n    df_val.drop(['row_id'], axis=1, inplace=True)\n    \n    #Step 4: Target Encoding: \n    df['CSP'] = df['country'].astype(str) + df['store'].astype(str) + df['product'].astype(str) #+ df['week'].astype(str) #Combine for encoding\n    df_val['CSP'] = df_val['country'].astype(str) + df_val['store'].astype(str) + df_val['product'].astype(str) #+ df['week'].astype(str)\n    \n    country_store_product_cols = [\"CSP\"]\n    csp_encoder = CrossFoldEncoder(CatBoostEncoder, a=10)\n    train_csp_encoded = csp_encoder.fit_transform(df, y_train, cols=country_store_product_cols)\n    val_csp_encoded = csp_encoder.transform(df_val)\n    \n    df.drop(['CSP'], axis=1, inplace=True)\n    df_val.drop(['CSP'], axis=1, inplace=True)\n    \n    df['CSP_encoded'] = train_csp_encoded\n    df_val['CSP_encoded'] = val_csp_encoded\n    \n    prod_encoder = CrossFoldEncoder(CatBoostEncoder, a=10)\n    train_prod_encoded = prod_encoder.fit_transform(df, y_train, cols=['product']) * 1e4#/ df.GDP\n    val_prod_encoded = prod_encoder.transform(df_val)* 1e4\n    \n    df['prod_encoded'] = train_prod_encoded['product_encoded'] #/ df.GDP\n    df_val['prod_encoded'] = val_prod_encoded['product_encoded'] #/ df_val.GDP\n    \n    if type(df_test) != type(None):\n        df_test['CSP'] = df_test['country'].astype(str) + df_test['store'].astype(str) + df_test['product'].astype(str) #+ df['week'].astype(str)\n        test_csp_encoded = csp_encoder.transform(df_test)\n        df_test.drop(['CSP'], axis=1, inplace=True)\n        df_test['CSP_encoded'] = test_csp_encoded\n        test_prod_encoded = prod_encoder.transform(df_test)* 1e4\n        df_test['prod_encoded'] = test_prod_encoded['product_encoded'] #/ df_test.GDP\n    #TODO: Perhaps encode product alone and divide it by GDP?\n    \n    #Step5: One-hot Encoding:\n    if type(df_test) != type(None):\n        df_test = pd.get_dummies(df_test, columns=['country','store','product'], drop_first=True)\n        df_test.drop(['date'], axis=1, inplace=True)\n        \n    df = pd.get_dummies(df, columns=['country','store','product'], drop_first=True)\n    df_val = pd.get_dummies(df_val, columns=['country','store','product'], drop_first=True)\n\n    df.drop(['date'], axis=1, inplace=True)\n    df_val.drop(['date'], axis=1, inplace=True)\n    \n    if type(df_test) == type(None):\n        df_test = pd.DataFrame(columns=['year']) #A trick to avoid issues in training\n    \n    return df, df_val, y_train, y_val, df_test, csp_encoder   \n\n","metadata":{"execution":{"iopub.status.busy":"2022-01-30T13:06:09.346293Z","iopub.execute_input":"2022-01-30T13:06:09.346532Z","iopub.status.idle":"2022-01-30T13:06:09.365576Z","shell.execute_reply.started":"2022-01-30T13:06:09.346494Z","shell.execute_reply":"2022-01-30T13:06:09.364756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training:\nAs per ambrosM's advice, I've implemented a small bloc of code that clears away variables with too low coefficients and returns a model based on the reduced variable set.","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import Ridge\ndef SMAPE(y_true, y_pred):\n    denominator = (y_true + np.abs(y_pred)) / 200.0\n    diff = np.abs(y_true - y_pred) / denominator\n    diff[denominator == 0] = 0.0\n    return np.round(np.mean(diff),5)\n\ndef remove_parameters_iter(df_train, df_test=None, n=1, eps=1e-12):\n    train_x, val_x, train_y, val_y, test_x, csp_encoder = process_data(df_train, df_test=df_test)\n    orig_train_x, orig_val_x, orig_test_x = train_x.copy(), val_x.copy(), test_x.copy()\n    train_x.drop('year', axis=1, inplace=True)\n    val_x.drop('year', axis=1, inplace=True)\n    test_x.drop('year', axis=1, inplace=True)\n    model = Ridge(alpha=1e-4,fit_intercept=False)\n    model.fit(train_x, train_y)\n    prev_score = 100\n    dropped_columns = []\n    ret = (model, train_x.copy(), val_x.copy(), train_y, val_y, test_x, orig_test_x, csp_encoder, dropped_columns, orig_train_x, orig_val_x)\n    \n    for _ in range(n):\n        bad_columns = [train_x.columns.tolist()[ind] for ind in list(np.where(model.coef_<eps)[0])]\n        dropped_columns = dropped_columns + bad_columns\n        train_x.drop(bad_columns, axis=1, inplace=True)\n        val_x.drop(bad_columns, axis=1, inplace=True)\n        test_x.drop(bad_columns, axis=1, inplace=True)\n        model = Ridge(alpha=1e-4,fit_intercept=False)\n        model.fit(train_x, train_y)\n        y_pred = model.predict(val_x)\n        score_ = SMAPE(inv_boxcox1p(val_y,lam), inv_boxcox1p(y_pred,lam))\n        print(score_)\n        if score_ >= prev_score:\n            print(\"Max precision attained\")\n            if score_ > prev_score:\n                return prev\n            break\n        prev = (model, train_x.copy(), val_x.copy(), train_y, val_y, test_x.copy(), orig_test_x, csp_encoder, dropped_columns, orig_train_x, orig_val_x)\n        prev_score = score_\n        \n    return model, train_x, val_x, train_y, val_y, test_x, orig_test_x, csp_encoder, dropped_columns, orig_train_x, orig_val_x","metadata":{"execution":{"iopub.status.busy":"2022-01-30T13:06:09.366865Z","iopub.execute_input":"2022-01-30T13:06:09.367661Z","iopub.status.idle":"2022-01-30T13:06:09.403813Z","shell.execute_reply.started":"2022-01-30T13:06:09.367629Z","shell.execute_reply":"2022-01-30T13:06:09.40322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test = pd.read_csv(\"/kaggle/input/tabular-playground-series-jan-2022/test.csv\",parse_dates=[\"date\"])","metadata":{"execution":{"iopub.status.busy":"2022-01-30T13:06:09.404687Z","iopub.execute_input":"2022-01-30T13:06:09.40537Z","iopub.status.idle":"2022-01-30T13:06:09.421562Z","shell.execute_reply.started":"2022-01-30T13:06:09.405335Z","shell.execute_reply":"2022-01-30T13:06:09.420963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model, train_x, val_x, train_y, val_y, test_x, orig_test_x, csp_encoder, dropped_columns, orig_train_x, orig_val_x = remove_parameters_iter(df_train, df_test=df_test, n=5, eps=1e-5)\n#train_x = remove_parameters_iter(df_train, df_test=df_test, n=5, eps=1e-5)","metadata":{"execution":{"iopub.status.busy":"2022-01-30T13:06:09.42271Z","iopub.execute_input":"2022-01-30T13:06:09.42311Z","iopub.status.idle":"2022-01-30T13:06:15.00309Z","shell.execute_reply.started":"2022-01-30T13:06:09.423073Z","shell.execute_reply":"2022-01-30T13:06:15.002084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.displot(train_y)","metadata":{"execution":{"iopub.status.busy":"2022-01-30T13:06:15.005091Z","iopub.execute_input":"2022-01-30T13:06:15.005804Z","iopub.status.idle":"2022-01-30T13:06:15.515271Z","shell.execute_reply.started":"2022-01-30T13:06:15.005749Z","shell.execute_reply":"2022-01-30T13:06:15.51468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ex_df = orig_val_x.query(\"country_Norway==1 and country_Sweden==0 and store_KaggleRama==0 and `product_Kaggle Mug`==1\")\nex_df_y = pd.DataFrame()\nex_df_y['y'] = np.expm1(val_y.loc[ex_df.index])\n\nex_df_y['date'] = pd.date_range(start='1/1/2018', end='31/12/2018')\n\nex_df.drop(dropped_columns, axis=1, inplace=True)\nex_df.drop('year', axis=1, inplace=True)\n\npredicted_y = model.predict(ex_df)\npredicted_y = pd.Series(predicted_y)\nplot_df = pd.DataFrame()\nplot_df['y'] = np.expm1(predicted_y)\nplot_df['date'] = pd.date_range(start='1/1/2018', end='31/12/2018')\n\ndelta_y = pd.DataFrame()\ndelta_y['y'] = np.abs(np.array(plot_df.y) - np.array(ex_df_y.y))\ndelta_y['date'] = plot_df['date']\nax = ex_df_y.plot(x = 'date', y='y', style=\".\", color=\"0.5\")\ndelta_y.plot(x ='date', y='y', ax=ax,\n      linewidth=3, legend=False,\n);\nplot_df.plot(x ='date', y='y', ax=ax,\n      linewidth=3, title=\"Weekly moving Average\", legend=False,\n);","metadata":{"execution":{"iopub.status.busy":"2022-01-30T13:06:15.516447Z","iopub.execute_input":"2022-01-30T13:06:15.516817Z","iopub.status.idle":"2022-01-30T13:06:16.306739Z","shell.execute_reply.started":"2022-01-30T13:06:15.516786Z","shell.execute_reply":"2022-01-30T13:06:16.305723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"country = 'Sweden'\nyear = 2018\n\ndelta_y.set_index('date', inplace=True)\nholiday_dates = Holidays.query('Country == @country and Date.dt.year == @year')['Date']\n\nmoving_average = delta_y.rolling(\n    window=7,       # 365-day window\n    center=True,      # puts the average at the center of the window\n    min_periods=4,  # choose about half the window size\n).mean()\n\nax = moving_average.plot(y='y', #ax=ax,\n      linewidth=3, legend=False,\n);\nfor date in holiday_dates:\n    ax.axvline(date, color='red', alpha=0.6, linestyle = '--')\n","metadata":{"execution":{"iopub.status.busy":"2022-01-30T13:06:16.308027Z","iopub.execute_input":"2022-01-30T13:06:16.308352Z","iopub.status.idle":"2022-01-30T13:06:16.966991Z","shell.execute_reply.started":"2022-01-30T13:06:16.308311Z","shell.execute_reply":"2022-01-30T13:06:16.966094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# y_pred = model.predict(val_x)\n# SMAPE(inv_boxcox1p(val_y,lam), inv_boxcox1p(y_pred,lam))","metadata":{"execution":{"iopub.status.busy":"2022-01-30T13:06:16.968242Z","iopub.execute_input":"2022-01-30T13:06:16.968444Z","iopub.status.idle":"2022-01-30T13:06:16.972882Z","shell.execute_reply.started":"2022-01-30T13:06:16.968418Z","shell.execute_reply":"2022-01-30T13:06:16.971987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It seems that the error is not too bad and the values only need some tweaking. So an XGBoost finetuning model will be implemented on the un-trimmed dataset on the residuals.\n\n# Hybrid Model:","metadata":{}},{"cell_type":"code","source":"# import lightgbm as lgb'\n\n# def train_residuals(train_x, orig_train_x, train_y):\n#     lin_model_result = model.predict(train_x)\n#     residual = np.expm1(train_y) - np.expm1(lin_model_result)\n    \n","metadata":{"execution":{"iopub.status.busy":"2022-01-30T13:06:16.974341Z","iopub.execute_input":"2022-01-30T13:06:16.974583Z","iopub.status.idle":"2022-01-30T13:06:16.983075Z","shell.execute_reply.started":"2022-01-30T13:06:16.974556Z","shell.execute_reply":"2022-01-30T13:06:16.98226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# residual_model = train_residuals(train_x, orig_train_x, train_y)","metadata":{"execution":{"iopub.status.busy":"2022-01-30T13:06:16.984362Z","iopub.execute_input":"2022-01-30T13:06:16.98462Z","iopub.status.idle":"2022-01-30T13:06:16.999236Z","shell.execute_reply.started":"2022-01-30T13:06:16.984583Z","shell.execute_reply":"2022-01-30T13:06:16.998413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# y_pred = predict_total(orig_val_x)\n# SMAPE(np.expm1(val_y), y_pred)","metadata":{"execution":{"iopub.status.busy":"2022-01-30T13:06:17.000431Z","iopub.execute_input":"2022-01-30T13:06:17.000685Z","iopub.status.idle":"2022-01-30T13:06:17.009613Z","shell.execute_reply.started":"2022-01-30T13:06:17.000655Z","shell.execute_reply":"2022-01-30T13:06:17.008987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def predict_total(orig_x,dropped_columns=dropped_columns):\n#     x = orig_x.drop(dropped_columns, axis=1)\n#     x.drop('year', axis=1, inplace=True)\n#     trended_result = model.predict(x)\n#     finetuned_result = residual_model.predict(orig_x)\n#     transformed_result = np.expm1(trended_result) + finetuned_result\n#     return np.round(transformed_result)","metadata":{"execution":{"iopub.status.busy":"2022-01-30T13:06:17.010823Z","iopub.execute_input":"2022-01-30T13:06:17.011226Z","iopub.status.idle":"2022-01-30T13:06:17.021012Z","shell.execute_reply.started":"2022-01-30T13:06:17.011183Z","shell.execute_reply":"2022-01-30T13:06:17.020122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# y_pred = predict_total(orig_val_x)\n# SMAPE(np.expm1(val_y), y_pred)","metadata":{"execution":{"iopub.status.busy":"2022-01-30T13:06:17.022279Z","iopub.execute_input":"2022-01-30T13:06:17.022707Z","iopub.status.idle":"2022-01-30T13:06:17.03218Z","shell.execute_reply.started":"2022-01-30T13:06:17.02266Z","shell.execute_reply":"2022-01-30T13:06:17.031372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ex_df = orig_val_x.query(\"country_Norway==1 and country_Sweden==0 and store_KaggleRama==0 and `product_Kaggle Mug`==1\")\n# ex_df_y = pd.DataFrame()\n# ex_df_y['y'] = np.expm1(val_y.loc[ex_df.index])\n\n# ex_df_y['date'] = pd.date_range(start='1/1/2018', end='31/12/2018')\n\n# # ex_df.drop(dropped_columns, axis=1, inplace=True)\n# # ex_df.drop('year', axis=1, inplace=True)\n\n# predicted_y = predict_total(ex_df)\n# predicted_y = pd.Series(predicted_y)\n# plot_df = pd.DataFrame()\n# plot_df['y'] = predicted_y\n# plot_df['date'] = pd.date_range(start='1/1/2018', end='31/12/2018')\n\n# delta_y = pd.DataFrame()\n# delta_y['y'] = np.abs(np.array(plot_df.y) - np.array(ex_df_y.y))\n# delta_y['date'] = plot_df['date']\n# ax = ex_df_y.plot(x = 'date', y='y', style=\".\", color=\"0.5\")\n# delta_y.plot(x ='date', y='y', ax=ax,\n#       linewidth=3, legend=False,\n# );\n# plot_df.plot(x ='date', y='y', ax=ax,\n#       linewidth=3, title=\"Weekly moving Average\", legend=False,\n# );","metadata":{"execution":{"iopub.status.busy":"2022-01-30T13:06:17.03342Z","iopub.execute_input":"2022-01-30T13:06:17.033799Z","iopub.status.idle":"2022-01-30T13:06:17.04348Z","shell.execute_reply.started":"2022-01-30T13:06:17.033757Z","shell.execute_reply":"2022-01-30T13:06:17.042673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Testing:","metadata":{"execution":{"iopub.status.busy":"2022-01-26T16:50:13.242815Z","iopub.execute_input":"2022-01-26T16:50:13.243133Z","iopub.status.idle":"2022-01-26T16:50:13.249667Z","shell.execute_reply.started":"2022-01-26T16:50:13.2431Z","shell.execute_reply":"2022-01-26T16:50:13.248861Z"}}},{"cell_type":"code","source":"def test_model(test_x, final=False):\n    id_col = df_test['row_id']\n    predicted_y = model.predict(test_x)\n    ret_df = pd.DataFrame()\n    ret_df['row_id'] = id_col\n    ret_df['num_sold'] = np.round(inv_boxcox1p(predicted_y,lam))\n    df_plot = df_test.merge(ret_df, how=\"left\", on=\"row_id\")\n    return df_plot, ret_df","metadata":{"execution":{"iopub.status.busy":"2022-01-30T13:06:17.044757Z","iopub.execute_input":"2022-01-30T13:06:17.045042Z","iopub.status.idle":"2022-01-30T13:06:17.056443Z","shell.execute_reply.started":"2022-01-30T13:06:17.045012Z","shell.execute_reply":"2022-01-30T13:06:17.055501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_plot, ret_df = test_model(test_x, final=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-30T13:06:17.059941Z","iopub.execute_input":"2022-01-30T13:06:17.060358Z","iopub.status.idle":"2022-01-30T13:06:17.092125Z","shell.execute_reply.started":"2022-01-30T13:06:17.060312Z","shell.execute_reply":"2022-01-30T13:06:17.091108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ret_df","metadata":{"execution":{"iopub.status.busy":"2022-01-30T13:06:17.093864Z","iopub.execute_input":"2022-01-30T13:06:17.094483Z","iopub.status.idle":"2022-01-30T13:06:17.114608Z","shell.execute_reply.started":"2022-01-30T13:06:17.094428Z","shell.execute_reply":"2022-01-30T13:06:17.113744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"country = 'Finland'\nstore = 'KaggleMart'\nproduct = 'Kaggle Mug'\n\nex_df  = df_plot.query('country == @country and store == @store and product == @product')\nex_df.set_index(['date'],inplace=True)\n\nholiday_dates = Holidays.query('Country == @country and Date.dt.year == 2019')['Date']\nmoving_average = ex_df.rolling(\n    window=7,       # 365-day window\n    center=True,      # puts the average at the center of the window\n    min_periods=4,  # choose about half the window size\n).mean()\n\nax = ex_df.num_sold.plot(y='num_sold', color=\"0.5\")\nfor date in holiday_dates:\n    ax.axvline(date, color='red', alpha=0.6, linestyle = '--')\n\nmoving_average.plot(y='num_sold',\n    ax=ax, linewidth=3, title=\"Weekly moving Average\", legend=False,\n)","metadata":{"execution":{"iopub.status.busy":"2022-01-30T13:06:17.116339Z","iopub.execute_input":"2022-01-30T13:06:17.116928Z","iopub.status.idle":"2022-01-30T13:06:17.882432Z","shell.execute_reply.started":"2022-01-30T13:06:17.116864Z","shell.execute_reply":"2022-01-30T13:06:17.8816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ret_df.to_csv('submission.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-30T13:06:17.883981Z","iopub.execute_input":"2022-01-30T13:06:17.884428Z","iopub.status.idle":"2022-01-30T13:06:17.903701Z","shell.execute_reply.started":"2022-01-30T13:06:17.884384Z","shell.execute_reply":"2022-01-30T13:06:17.902797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}