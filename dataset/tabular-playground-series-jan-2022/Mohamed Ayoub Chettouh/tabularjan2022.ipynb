{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport math\nfrom datetime import datetime\nimport dateutil\n\nfrom numpy import abs\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Input Data**:","metadata":{}},{"cell_type":"code","source":"Train = pd.read_csv(\"/kaggle/input/tabular-playground-series-jan-2022/train.csv\",parse_dates=[\"date\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Exploratory Data Analysis**","metadata":{}},{"cell_type":"code","source":"Train_plot = Train.copy()\nfor ind in Train_plot.index.values:\n    #print(ind)\n    if (Train.iloc[ind]['country'] != 'Finland' or\n        Train.iloc[ind]['store'] != 'KaggleMart' or\n        Train.iloc[ind]['product']  != 'Kaggle Mug'):\n        Train_plot = Train.drop(ind,0)\n\n#sns.lineplot('date','num_sold',data=Train_plot,)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" Holidays appear to affect the sales gravely.","metadata":{"execution":{"iopub.status.busy":"2022-01-04T10:30:16.073375Z","iopub.execute_input":"2022-01-04T10:30:16.073736Z","iopub.status.idle":"2022-01-04T10:30:16.081799Z","shell.execute_reply.started":"2022-01-04T10:30:16.073698Z","shell.execute_reply":"2022-01-04T10:30:16.080317Z"}}},{"cell_type":"markdown","source":"Distribution of Data:","metadata":{}},{"cell_type":"code","source":"sns.displot(Train['country'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.displot(Train['product'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nsns.displot(data=Train, x='store', col='country', height=3, aspect=1.6)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"per_month = Train.copy()\nper_month['month'] = Train['date'].dt.month\nper_month['countrystore'] = [Train.iloc[ind]['country'] + Train.iloc[ind]['store'] for ind in Train.index.values]\n\n\nsns.displot(data=per_month, x='month', y='num_sold', row='countrystore', col='product', color='red', height=3, aspect=1.6)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.displot(data=Train,x='num_sold')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Sales distribution","metadata":{}},{"cell_type":"markdown","source":"## Adding Holiday and day-of-week features:","metadata":{}},{"cell_type":"code","source":"Train_copy = Train.copy()\nTrain_copy['dayofyear'] = Train_copy['date'].dt.dayofyear\nTrain_copy['dayofweek'] = Train_copy['date'].dt.day_of_week\n\nTrain_copy['year'] = Train_copy['date'].dt.year\ndef dtchristmas(date):\n    \n    christmas_curr = pd.Timestamp(date.year,12,25)\n    \n    curr_dt = (date - christmas_curr).days\n    \n    if curr_dt >= -70 and curr_dt <=8:\n        return curr_dt\n    else:\n        return 100\n    \ndef dteaster(date):\n    easter_curr = pd.Timestamp(dateutil.easter.easter(date.year))\n    dt = (date - easter_curr).days\n    if dt >= -1 and dt <=15:\n        return dt\n    else:\n        return 100 \n     \n\nTrain_copy['dtchristmas'] = [dtchristmas(Train_copy.iloc[ind]['date']) for ind in Train_copy.index.values]\nTrain_copy['dteaster'] = [dteaster(Train_copy.iloc[ind]['date']) for ind in Train_copy.index.values]\nTrain_copy['1jan'] = [int(Train_copy.iloc[ind]['date'] == pd.Timestamp(Train_copy.iloc[ind]['date'].year,1,1)) for ind in Train_copy.index.values]\nTrain_copy['wd4'] = Train_copy['date'].dt.day_of_week == 4\nTrain_copy['wd56'] = Train_copy['date'].dt.day_of_week >= 5\n# for k in range(1, 3): # 20\n#         Train_copy[f'sin{k}'] = np.sin(Train_copy['dayofyear'] / 365 * 2 * math.pi * k)\n#         Train_copy[f'cos{k}'] = np.cos(Train_copy['dayofyear'] / 365 * 2 * math.pi * k)\n#         Train_copy[f'mug_sin{k}'] = Train_copy[f'sin{k}'] * (Train_copy['product'] == 'Kaggle Mug')\n#         Train_copy[f'mug_cos{k}'] = Train_copy[f'cos{k}'] * (Train_copy['product'] == 'Kaggle Mug')\n#         Train_copy[f'sticker_sin{k}'] = Train_copy[f'sin{k}'] * (Train_copy['product'] == 'Kaggle Sticker')\n#         Train_copy[f'sticker_cos{k}'] = Train_copy[f'cos{k}'] * (Train_copy['product'] == 'Kaggle Sticker')\n#         Train_copy[f'sticker_sin{k}'] = Train_copy[f'sin{k}'] * (Train_copy['product'] == 'Kaggle Hat')\n#         Train_copy[f'sticker_cos{k}'] = Train_copy[f'cos{k}'] * (Train_copy['product'] == 'Kaggle Hat')\nTrain_copy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Segregating December and first 10 days of January","metadata":{}},{"cell_type":"code","source":"def remove_december(data):\n    data_ = data.copy()\n    for ind in data_.index.values:\n        if data_.iloc[ind]['date'].month == 12:\n            data_.drop(ind,axis=0,inplace=True)\n    return data_\ndef remove_10_jan(data):\n    data_ = data.copy()\n    for ind in data_.index.values:\n        if data_.iloc[ind]['date'].month == 1 and data_.iloc[ind]['date'].day <= 10:\n            data_.drop(ind,axis=0,inplace=True)\n    return data_\ndef remove_easter(data):\n    data_ = data.copy()\n    for ind in data_.index.values:\n        date = data_.iloc[ind]['date']\n        easter_curr = pd.Timestamp(dateutil.easter.easter(date.year))\n        dt = (date - easter_curr).days\n        if dt >= -1 and dt <=15:\n            data_.drop(ind,axis=0,inplace=True)\n    return data_\n\nTrain_dec = Train_copy.query('date.dt.month == 12')\nTrain_clean = pd.concat([Train_copy,Train_dec]).drop_duplicates(keep=False)\nTrain_10jan = Train_copy.query('date.dt.month == 1 and date.dt.day <= 10')\nTrain_clean = pd.concat([Train_clean,Train_10jan]).drop_duplicates(keep=False)\n\nTrain_easter = Train_copy.query('dteaster != 100')\nTrain_clean = pd.concat([Train_clean,Train_easter]).drop_duplicates(keep=False)\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Train_easter","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from numpy import abs\nfrom pandas import Timestamp\n\n\nsns.lineplot(x=range(1,63),y=Train_copy.query('country == \"Finland\" and store == \"KaggleMart\" and product == \"Kaggle Mug\" and ((date.dt.year == 2015 and ( date.dt.month ==12)) or (date.dt.year == 2016 and (date.dt.month == 1)))')['num_sold'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.heatmap(Train_copy.query('country == \"Finland\" and store == \"KaggleMart\" and product == \"Kaggle Mug\"').corr(),annot=True)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Fitting one part of the data**:","metadata":{}},{"cell_type":"markdown","source":"The plan is as follows: Each country/store/product combination is treated seperately. I get the sales data per day for each year and divide by the average sales for that year. Then a SVR is fitted to the data and the next sales average is extrapolated via linear regression.","metadata":{}},{"cell_type":"markdown","source":"### Extracting Data:","metadata":{}},{"cell_type":"code","source":"cnt = \"Finland\"\nstor = \"KaggleMart\"\nprod = \"Kaggle Mug\"\nF_KM_KM_2015 = Train_copy.query('country == @cnt and store == @stor and product == @prod and date.dt.year == 2015')\nF_KM_KM_2016 = Train_copy.query('country == @cnt and store == @stor and product == @prod and date.dt.year == 2016')\nF_KM_KM_2017 = Train_copy.query('country == @cnt and store == @stor and product == @prod and date.dt.year == 2017')\nF_KM_KM_2018 = Train_copy.query('country == @cnt and store == @stor and product == @prod and date.dt.year == 2018')\n\nF_KM_KM_2015_y = F_KM_KM_2015['num_sold']\nF_KM_KM_2016_y = F_KM_KM_2016['num_sold']\nF_KM_KM_2017_y = F_KM_KM_2017['num_sold']\nF_KM_KM_2018_y = F_KM_KM_2018['num_sold']\n\nF_KM_KM_2015.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\nF_KM_KM_2016.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\nF_KM_KM_2017.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True) #, 'dteaster', 'dtchristmas', '1jan'\nF_KM_KM_2018.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\n\nsample_x_true_2015 = np.array(F_KM_KM_2015)\nsample_x_true_2016 = np.array(F_KM_KM_2016)\nsample_x_true_2017 = np.array(F_KM_KM_2017)\nsample_x_true_2018 = np.array(F_KM_KM_2018)\n\nsample_y_true_2015 = np.array(F_KM_KM_2015_y)\nsample_y_true_2016 = np.array(F_KM_KM_2016_y)\nsample_y_true_2017 = np.array(F_KM_KM_2017_y)\nsample_y_true_2018 = np.array(F_KM_KM_2018_y)\n\nmax_true_2015 = np.max(np.array(sample_y_true_2015))\nmax_true_2016 = np.max(np.array(sample_y_true_2016))\nmax_true_2017 = np.max(np.array(sample_y_true_2017))\nmax_true_2018 = np.max(np.array(sample_y_true_2018))\n\nsample_y_true_2015 = sample_y_true_2015 / max_true_2015\nsample_y_true_2016 = sample_y_true_2016 / max_true_2016\nsample_y_true_2017 = sample_y_true_2017 / max_true_2017\nsample_y_true_2018 = sample_y_true_2018 / max_true_2018","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ncnt = \"Finland\"\nstor = \"KaggleMart\"\nprod = \"Kaggle Mug\"\nF_KM_KM_2015 = Train_clean.query('country == @cnt and store == \"KaggleMart\" and product == \"Kaggle Mug\" and date.dt.year == 2015')\nF_KM_KM_2016 = Train_clean.query('country == @cnt and store == \"KaggleMart\" and product == \"Kaggle Mug\" and date.dt.year == 2016')\nF_KM_KM_2017 = Train_clean.query('country == @cnt and store == \"KaggleMart\" and product == \"Kaggle Mug\" and date.dt.year == 2017')\nF_KM_KM_2018 = Train_clean.query('country == @cnt and store == \"KaggleMart\" and product == \"Kaggle Mug\" and date.dt.year == 2018')\n\nF_KM_KM_2015_y = F_KM_KM_2015['num_sold']\nF_KM_KM_2016_y = F_KM_KM_2016['num_sold']\nF_KM_KM_2017_y = F_KM_KM_2017['num_sold']\nF_KM_KM_2018_y = F_KM_KM_2018['num_sold']\n\nF_KM_KM_2015.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\nF_KM_KM_2016.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\nF_KM_KM_2017.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\nF_KM_KM_2018.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\n\nsample_x_2015 = np.array(F_KM_KM_2015)\nsample_x_2016 = np.array(F_KM_KM_2016)\nsample_x_2017 = np.array(F_KM_KM_2017)\nsample_x_2018 = np.array(F_KM_KM_2018)\n\nsample_y_2015 = np.array(F_KM_KM_2015_y)\nsample_y_2016 = np.array(F_KM_KM_2016_y)\nsample_y_2017 = np.array(F_KM_KM_2017_y)\nsample_y_2018 = np.array(F_KM_KM_2018_y)\n\nsample_y_2015 = sample_y_2015 / max_true_2015\nsample_y_2016 = sample_y_2016 / max_true_2016\nsample_y_2017 = sample_y_2017 / max_true_2017\nsample_y_2018 = sample_y_2018 / max_true_2018","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ncnt = \"Finland\"\nstor = \"KaggleMart\"\nprod = \"Kaggle Mug\"\nF_KM_KM_2015 = Train_dec.query('country == @cnt and store == \"KaggleMart\" and product == \"Kaggle Mug\" and date.dt.year == 2015')\nF_KM_KM_2016 = Train_dec.query('country == @cnt and store == \"KaggleMart\" and product == \"Kaggle Mug\" and date.dt.year == 2016')\nF_KM_KM_2017 = Train_dec.query('country == @cnt and store == \"KaggleMart\" and product == \"Kaggle Mug\" and date.dt.year == 2017')\nF_KM_KM_2018 = Train_dec.query('country == @cnt and store == \"KaggleMart\" and product == \"Kaggle Mug\" and date.dt.year == 2018')\n\nF_KM_KM_2015_y = F_KM_KM_2015['num_sold']\nF_KM_KM_2016_y = F_KM_KM_2016['num_sold']\nF_KM_KM_2017_y = F_KM_KM_2017['num_sold']\nF_KM_KM_2018_y = F_KM_KM_2018['num_sold']\n\nF_KM_KM_2015.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\nF_KM_KM_2016.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\nF_KM_KM_2017.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\nF_KM_KM_2018.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\n\nsample_x_dec_2015 = np.array(F_KM_KM_2015)\nsample_x_dec_2016 = np.array(F_KM_KM_2016)\nsample_x_dec_2017 = np.array(F_KM_KM_2017)\nsample_x_dec_2018 = np.array(F_KM_KM_2018)\n\nsample_y_dec_2015 = np.array(F_KM_KM_2015_y)\nsample_y_dec_2016 = np.array(F_KM_KM_2016_y)\nsample_y_dec_2017 = np.array(F_KM_KM_2017_y)\nsample_y_dec_2018 = np.array(F_KM_KM_2018_y)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cnt = \"Finland\"\nstor = \"KaggleMart\"\nprod = \"Kaggle Mug\"\nF_KM_KM_2015 = Train_easter.query('country == @cnt and store == \"KaggleMart\" and product == \"Kaggle Mug\" and date.dt.year == 2015')\nF_KM_KM_2016 = Train_easter.query('country == @cnt and store == \"KaggleMart\" and product == \"Kaggle Mug\" and date.dt.year == 2016')\nF_KM_KM_2017 = Train_easter.query('country == @cnt and store == \"KaggleMart\" and product == \"Kaggle Mug\" and date.dt.year == 2017')\nF_KM_KM_2018 = Train_easter.query('country == @cnt and store == \"KaggleMart\" and product == \"Kaggle Mug\" and date.dt.year == 2018')\n\nF_KM_KM_2015_y = F_KM_KM_2015['num_sold']\nF_KM_KM_2016_y = F_KM_KM_2016['num_sold']\nF_KM_KM_2017_y = F_KM_KM_2017['num_sold']\nF_KM_KM_2018_y = F_KM_KM_2018['num_sold']\n\nF_KM_KM_2015.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\nF_KM_KM_2016.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\nF_KM_KM_2017.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\nF_KM_KM_2018.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\n\nsample_x_easter_2015 = np.array(F_KM_KM_2015)\nsample_x_easter_2016 = np.array(F_KM_KM_2016)\nsample_x_easter_2017 = np.array(F_KM_KM_2017)\nsample_x_easter_2018 = np.array(F_KM_KM_2018)\n\nsample_y_easter_2015 = np.array(F_KM_KM_2015_y)\nsample_y_easter_2016 = np.array(F_KM_KM_2016_y)\nsample_y_easter_2017 = np.array(F_KM_KM_2017_y)\nsample_y_easter_2018 = np.array(F_KM_KM_2018_y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cnt = \"Finland\"\nstor = \"KaggleMart\"\nprod = \"Kaggle Mug\"\nF_KM_KM_2015 = Train_10jan.query('country == @cnt and store == \"KaggleMart\" and product == \"Kaggle Mug\" and date.dt.year == 2015')\nF_KM_KM_2016 = Train_10jan.query('country == @cnt and store == \"KaggleMart\" and product == \"Kaggle Mug\" and date.dt.year == 2016')\nF_KM_KM_2017 = Train_10jan.query('country == @cnt and store == \"KaggleMart\" and product == \"Kaggle Mug\" and date.dt.year == 2017')\nF_KM_KM_2018 = Train_10jan.query('country == @cnt and store == \"KaggleMart\" and product == \"Kaggle Mug\" and date.dt.year == 2018')\n\nF_KM_KM_2015_y = F_KM_KM_2015['num_sold']\nF_KM_KM_2016_y = F_KM_KM_2016['num_sold']\nF_KM_KM_2017_y = F_KM_KM_2017['num_sold']\nF_KM_KM_2018_y = F_KM_KM_2018['num_sold']\n\nF_KM_KM_2015.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\nF_KM_KM_2016.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\nF_KM_KM_2017.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\nF_KM_KM_2018.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\n\nsample_x_10jan_2015 = np.array(F_KM_KM_2015)\nsample_x_10jan_2016 = np.array(F_KM_KM_2016)\nsample_x_10jan_2017 = np.array(F_KM_KM_2017)\nsample_x_10jan_2018 = np.array(F_KM_KM_2018)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_x = np.concatenate((sample_x_2017,sample_x_2016,sample_x_2015),axis = 0)\nsample_y = np.concatenate((sample_y_2017,sample_y_2016,sample_y_2015),axis = 0)\n\nsample_x_true = np.concatenate((sample_x_true_2017,sample_x_true_2016,sample_x_true_2015),axis = 0)\nsample_y_true = np.concatenate((sample_y_true_2017,sample_y_true_2016,sample_y_true_2015),axis = 0)\n\nsample_x_dec = np.concatenate((sample_x_dec_2017,sample_x_dec_2016,sample_x_dec_2015),axis = 0)\nsample_y_dec = np.concatenate((sample_y_dec_2017,sample_y_dec_2016,sample_y_dec_2015),axis = 0)\n\nsample_x_easter = np.concatenate((sample_x_easter_2017,sample_x_easter_2016,sample_x_easter_2015),axis = 0)\nsample_x_10jan = np.concatenate((sample_x_10jan_2017,sample_x_10jan_2016,sample_x_10jan_2015),axis = 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Fitting a SVR to the data:","metadata":{}},{"cell_type":"code","source":"from sklearn import svm\nfrom sklearn.linear_model import HuberRegressor, LinearRegression, Ridge\nfrom sklearn.ensemble import AdaBoostRegressor,GradientBoostingRegressor\nfrom sklearn.gaussian_process import GaussianProcessRegressor\n\ndef chosen_model():\n    #return Ridge()\n    return GradientBoostingRegressor(loss=\"huber\",max_depth=4,n_estimators=70,alpha=0.95,random_state=1)\n    #return svm.SVR(kernel=\"rbf\", C=10000, gamma=0.25, epsilon=1e-10, tol=1e-10)\n    #return AdaBoostRegressor(n_estimators = 1000, loss='square', random_state = 1)\nsample_regr = chosen_model()\nsample_regr.fit(sample_x, sample_y)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"deltay_2018 = sample_y_true_2018-sample_regr.predict(sample_x_true_2018)\ndeltay = sample_y_true-sample_regr.predict(sample_x_true)\ng = sns.lineplot(range(1,1097),deltay)\ng.axhline(0,color='red')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that predictibly the largest residual is situated at the hollidays.","metadata":{}},{"cell_type":"code","source":"len(deltay)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Correcting for December","metadata":{}},{"cell_type":"code","source":"deltay_dec = np.zeros(len(sample_x_dec))\nfor i in range(len(sample_x_dec)):\n    for j in range(len(sample_x_true)):\n        if np.all(sample_x_true[j] == sample_x_dec[i]):\n            deltay_dec[i] = deltay[j]\n            break\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.lineplot(range(1,94),deltay_dec)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"deltay_dec_2017 = np.zeros(31)\nj=0\nfor i in range(len(sample_x_dec)):\n    if sample_x_dec[i][2] == 2017:\n        deltay_dec_2017[j] = deltay_dec[i]\n        j += 1\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.lineplot(range(1,32),deltay_dec_2017)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dec_regr = chosen_model()\ndec_regr.fit(sample_x_dec, deltay_dec)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_dec(sample,sample_dec):  \n    y = np.zeros(len(sample))\n    y = sample_regr.predict(sample)\n    dec_residual = dec_regr.predict(sample_dec)\n    \n    for i in range(len(sample)):\n        if sample[i][0] >= 335:\n            for j in range(len(sample_dec)):\n                if np.all(sample[i] == sample_dec[j]):\n                    y[i] +=  dec_residual[j]\n                    break\n    return y","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dec_predict_2018 = predict_dec(sample_x_true_2018,sample_x_dec_2018)\n\nlen(dec_predict_2018)\nsns.lineplot(range(1,366),dec_predict_2018)\nsns.lineplot(range(1,366),sample_y_true_2018,style=2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dec_predict = predict_dec(sample_x_true,sample_x_dec)\ndeltay = sample_y_true - dec_predict #Updating residuals\nsns.lineplot(range(1,1097),deltay)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Correcting for Easter","metadata":{}},{"cell_type":"code","source":"deltay_easter = np.zeros(len(sample_x_easter))\nfor i in range(len(sample_x_easter)):\n    for j in range(len(sample_x_true)):\n        if np.all(sample_x_true[j] == sample_x_easter[i]):\n            deltay_easter[i] = deltay[j]\n            break\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"deltay_easter_2017 = np.array([])\nj=0\nfor i in range(len(sample_x_easter)):\n    if sample_x_easter[i][2] == 2017:\n        deltay_easter_2017 = np.append(deltay_easter_2017,deltay_easter[i])\n        j += 1\n\nlen(deltay_easter_2017)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.lineplot(range(17),deltay_easter_2017)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"easter_regr = chosen_model()\neaster_regr.fit(sample_x_easter, deltay_easter)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_x_easter_2018","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_easter(sample,sample_easter,y):  \n    y = y.copy()\n    #y = predict_dec(sample,sample_dec)\n    easter_residual = easter_regr.predict(sample_easter)\n    \n    for i in range(len(sample)):\n        if sample[i][0] >= 86 and sample[i][0] <= 121:\n            for j in range(len(sample_easter)):\n                if np.all(sample[i] == sample_easter[j]):\n                    y[i] +=  easter_residual[j]\n                    break\n    return y","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"easter_predict_2018 = predict_easter(sample_x_true_2018,sample_x_easter_2018,dec_predict_2018)\n\nlen(easter_predict_2018)\nsns.lineplot(range(1,366),easter_predict_2018)\nsns.lineplot(range(1,366),sample_y_true_2018,style=2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"easter_predict = predict_easter(sample_x_true,sample_x_easter, dec_predict)\ndeltay = sample_y_true - easter_predict #Updating residuals","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.lineplot(range(1,1097), easter_predict)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"deltay_10jan = np.zeros(len(sample_x_10jan))\nfor i in range(len(sample_x_10jan)):\n    for j in range(len(sample_x_true)):\n        if np.all(sample_x_true[j] == sample_x_10jan[i]):\n            deltay_10jan[i] = deltay[j]\n            break\n\nlen(deltay_10jan)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.lineplot(range(30),deltay_10jan)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"deltay_10jan_2017 = np.array([])\nj=0\nfor i in range(len(sample_x_10jan)):\n    if sample_x_10jan[i][2] == 2017:\n        deltay_10jan_2017 = np.append(deltay_10jan_2017,deltay_10jan[i])\n        j += 1\n\nlen(deltay_10jan_2017)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.lineplot(range(10),deltay_10jan_2017)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"jan10_regr = chosen_model()\njan10_regr.fit(sample_x_10jan, deltay_10jan)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_10jan(sample,sample_jan10,y):  \n    y = y.copy()\n    jan10_residual = jan10_regr.predict(sample_jan10)\n    \n    for i in range(len(sample)):\n        if sample[i][0] <= 10:\n            for j in range(len(sample_jan10)):\n                if np.all(sample[i] == sample_jan10[j]):\n                    y[i] +=  jan10_residual[j]\n                    break\n    return y","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"jan10_predict_2018 = predict_10jan(sample_x_10jan_2018,sample_x_10jan_2018,easter_predict_2018)\nprint(np.all(jan10_predict_2018 == easter_predict_2018))\nlen(jan10_predict_2018)\nsns.lineplot(range(1,366),jan10_predict_2018)\n#sns.lineplot(range(1,366),easter_predict_2018)\n#sns.lineplot(range(1,366),sample_y_true_2018,style=2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\n\ndef SMAPE(y_true, y_pred):\n    denominator = (y_true + np.abs(y_pred)) / 200.0\n    diff = np.abs(y_true - y_pred) / denominator\n    diff[denominator == 0] = 0.0\n    return np.round(np.mean(diff),5)\n\n#print(mean_squared_error(sample_y_2018,sample_mean))\nprint(SMAPE(sample_y_true_2018,(sample_regr.predict(sample_x_true_2018))))\nprint(SMAPE(sample_y_true_2018,dec_predict_2018))\nprint(SMAPE(sample_y_true_2018,easter_predict_2018))\nprint(SMAPE(sample_y_true_2018,jan10_predict_2018))\nprint(mean_squared_error(sample_y_2018,sample_regr.predict(sample_x_2018)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The error is decent.","metadata":{}},{"cell_type":"markdown","source":"Fitting the Linear regressor for the year-by-year average","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n\nmax_model = LinearRegression()\nmax_model.fit(np.array(range(2015,2018)).reshape(-1,1),[max_true_2015,max_true_2016,max_true_2017])\n\n(max_model.predict(np.array(2018).reshape(1,-1))[0])\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Seems Good! Now applying to to all country/store/product combinations (3x2x3 = 18 different combinations)","metadata":{}},{"cell_type":"code","source":"country = 'Norway'\nstore = 'KaggleMart'\nproduct = 'Kaggle Sticker'\n\n\nF_KM_KM_2015 = Train_copy.query('country == @country and store == @store and product == @product and date.dt.year == 2015')\nF_KM_KM_2016 = Train_copy.query('country == @country and store == @store and product == @product and date.dt.year == 2016')\nF_KM_KM_2017 = Train_copy.query('country == @country and store == @store and product == @product and date.dt.year == 2017')\nF_KM_KM_2018 = Train_copy.query('country == @country and store == @store and product == @product and date.dt.year == 2018')\n\nF_KM_KM_2015_y = F_KM_KM_2015['num_sold']\nF_KM_KM_2016_y = F_KM_KM_2016['num_sold']\nF_KM_KM_2017_y = F_KM_KM_2017['num_sold']\nF_KM_KM_2018_y = F_KM_KM_2018['num_sold']\n\nF_KM_KM_2015.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\nF_KM_KM_2016.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\nF_KM_KM_2017.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True) #, 'dteaster', 'dtchristmas', '1jan'\nF_KM_KM_2018.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\n\nsample_x_true_2015 = np.array(F_KM_KM_2015)\nsample_x_true_2016 = np.array(F_KM_KM_2016)\nsample_x_true_2017 = np.array(F_KM_KM_2017)\nsample_x_true_2018 = np.array(F_KM_KM_2018)\n\nsample_y_true_2015 = np.array(F_KM_KM_2015_y)\nsample_y_true_2016 = np.array(F_KM_KM_2016_y)\nsample_y_true_2017 = np.array(F_KM_KM_2017_y)\nsample_y_true_2018 = np.array(F_KM_KM_2018_y)\n\nmax_true_2015 = np.max(np.array(sample_y_true_2015))\nmax_true_2016 = np.max(np.array(sample_y_true_2016))\nmax_true_2017 = np.max(np.array(sample_y_true_2017))\nmax_true_2018 = np.max(np.array(sample_y_true_2018))\n\nsample_y_true_2015 = sample_y_true_2015 / max_true_2015\nsample_y_true_2016 = sample_y_true_2016 / max_true_2016\nsample_y_true_2017 = sample_y_true_2017 / max_true_2017\nsample_y_true_2018 = sample_y_true_2018 / max_true_2018\n\nF_KM_KM_2015 = Train_clean.query('country == @country and store == @store and product == @product and date.dt.year == 2015')\nF_KM_KM_2016 = Train_clean.query('country == @country and store == @store and product == @product and date.dt.year == 2016')\nF_KM_KM_2017 = Train_clean.query('country == @country and store == @store and product == @product and date.dt.year == 2017')\nF_KM_KM_2018 = Train_clean.query('country == @country and store == @store and product == @product and date.dt.year == 2018')\n\nF_KM_KM_2015_y = F_KM_KM_2015['num_sold']\nF_KM_KM_2016_y = F_KM_KM_2016['num_sold']\nF_KM_KM_2017_y = F_KM_KM_2017['num_sold']\nF_KM_KM_2018_y = F_KM_KM_2018['num_sold']\n\nF_KM_KM_2015.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\nF_KM_KM_2016.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\nF_KM_KM_2017.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\nF_KM_KM_2018.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\n\nsample_x_2015 = np.array(F_KM_KM_2015)\nsample_x_2016 = np.array(F_KM_KM_2016)\nsample_x_2017 = np.array(F_KM_KM_2017)\nsample_x_2018 = np.array(F_KM_KM_2018)\n\nsample_y_2015 = np.array(F_KM_KM_2015_y)\nsample_y_2016 = np.array(F_KM_KM_2016_y)\nsample_y_2017 = np.array(F_KM_KM_2017_y)\nsample_y_2018 = np.array(F_KM_KM_2018_y)\n\nsample_y_2015 = sample_y_2015 / max_true_2015\nsample_y_2016 = sample_y_2016 / max_true_2016\nsample_y_2017 = sample_y_2017 / max_true_2017\nsample_y_2018 = sample_y_2018 / max_true_2018\n\nF_KM_KM_2015 = Train_dec.query('country == @country and store == @store and product == @product and date.dt.year == 2015')\nF_KM_KM_2016 = Train_dec.query('country == @country and store == @store and product == @product and date.dt.year == 2016')\nF_KM_KM_2017 = Train_dec.query('country == @country and store == @store and product == @product and date.dt.year == 2017')\nF_KM_KM_2018 = Train_dec.query('country == @country and store == @store and product == @product and date.dt.year == 2018')\n\nF_KM_KM_2015_y = F_KM_KM_2015['num_sold']\nF_KM_KM_2016_y = F_KM_KM_2016['num_sold']\nF_KM_KM_2017_y = F_KM_KM_2017['num_sold']\nF_KM_KM_2018_y = F_KM_KM_2018['num_sold']\n\nF_KM_KM_2015.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\nF_KM_KM_2016.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\nF_KM_KM_2017.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\nF_KM_KM_2018.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\n\nsample_x_dec_2015 = np.array(F_KM_KM_2015)\nsample_x_dec_2016 = np.array(F_KM_KM_2016)\nsample_x_dec_2017 = np.array(F_KM_KM_2017)\nsample_x_dec_2018 = np.array(F_KM_KM_2018)\n\nF_KM_KM_2015 = Train_10jan.query('country == @country and store == @store and product == @product and date.dt.year == 2015')\nF_KM_KM_2016 = Train_10jan.query('country == @country and store == @store and product == @product and date.dt.year == 2016')\nF_KM_KM_2017 = Train_10jan.query('country == @country and store == @store and product == @product and date.dt.year == 2017')\nF_KM_KM_2018 = Train_10jan.query('country == @country and store == @store and product == @product and date.dt.year == 2018')\n\nF_KM_KM_2015_y = F_KM_KM_2015['num_sold']\nF_KM_KM_2016_y = F_KM_KM_2016['num_sold']\nF_KM_KM_2017_y = F_KM_KM_2017['num_sold']\nF_KM_KM_2018_y = F_KM_KM_2018['num_sold']\n\nF_KM_KM_2015.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\nF_KM_KM_2016.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\nF_KM_KM_2017.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\nF_KM_KM_2018.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\n\nsample_x_10jan_2015 = np.array(F_KM_KM_2015)\nsample_x_10jan_2016 = np.array(F_KM_KM_2016)\nsample_x_10jan_2017 = np.array(F_KM_KM_2017)\nsample_x_10jan_2018 = np.array(F_KM_KM_2018)\n\nsample_x = np.concatenate((sample_x_2017,sample_x_2016,sample_x_2015),axis = 0)\nsample_y = np.concatenate((sample_y_2017,sample_y_2016,sample_y_2015),axis = 0)\n\nsample_x_true = np.concatenate((sample_x_true_2017,sample_x_true_2016,sample_x_true_2015),axis = 0)\nsample_y_true = np.concatenate((sample_y_true_2017,sample_y_true_2016,sample_y_true_2015),axis = 0)\n\nsample_x_dec = np.concatenate((sample_x_dec_2017,sample_x_dec_2016,sample_x_dec_2015),axis = 0)\nsample_y_dec = np.concatenate((sample_y_dec_2017,sample_y_dec_2016,sample_y_dec_2015),axis = 0)\n\nsample_x_easter = np.concatenate((sample_x_easter_2017,sample_x_easter_2016,sample_x_easter_2015),axis = 0)\nsample_x_10jan = np.concatenate((sample_x_10jan_2017,sample_x_10jan_2016,sample_x_10jan_2015),axis = 0)\n\n\nsample_regr = chosen_model()\nsample_regr.fit(sample_x, sample_y)\n\ndeltay = sample_y_true-sample_regr.predict(sample_x_true)\n\ndeltay_dec = np.zeros(len(sample_x_dec))\nfor i in range(len(sample_x_dec)):\n    for j in range(len(sample_x_true)):\n        if np.all(sample_x_true[j] == sample_x_dec[i]):\n            deltay_dec[i] = deltay[j]\n            break\n\ndec_regr = chosen_model()\ndec_regr.fit(sample_x_dec, deltay_dec)\n\n\n\ndec_predict = predict_dec(sample_x_true,sample_x_dec)\ndeltay = sample_y_true - dec_predict #Updating residuals\n\ndeltay_easter = np.zeros(len(sample_x_easter))\nfor i in range(len(sample_x_easter)):\n    for j in range(len(sample_x_true)):\n        if np.all(sample_x_true[j] == sample_x_easter[i]):\n            deltay_easter[i] = deltay[j]\n            break\n\neaster_regr = chosen_model()\neaster_regr.fit(sample_x_easter, deltay_easter)\n\n\neaster_predict = predict_easter(sample_x_true,sample_x_easter, dec_predict)\ndeltay = sample_y_true - easter_predict #Updating residuals\n\ndeltay_10jan = np.zeros(len(sample_x_10jan))\nfor i in range(len(sample_x_10jan)):\n    for j in range(len(sample_x_true)):\n        if np.all(sample_x_true[j] == sample_x_10jan[i]):\n            deltay_10jan[i] = deltay[j]\n            break\n\n\njan10_regr = chosen_model()\njan10_regr.fit(sample_x_10jan, deltay_10jan)\n\n\nmax_model = LinearRegression()\nmax_model.fit(np.array(range(2015,2019)).reshape(-1,1),[max_true_2015,max_true_2016,max_true_2017,max_true_2018])\n","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# country = 'Finland'\n# store = 'KaggleRama'\n# product = 'Kaggle Mug'\n\n\n# sample_regr, dec_regr, easter_regr, jan10_regr, max_model = model[country][store][product]#learn_product(country,store,product)\n\n# F_KM_KM_2018 = Train_copy.query('country == @country and store == @store and product == @product and date.dt.year == 2018')\n# F_KM_KM_2018_y = F_KM_KM_2018['num_sold']\n# F_KM_KM_2018.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\n# sample_x_true_2018 = np.array(F_KM_KM_2018)\n# sample_y_true_2018 = np.array(F_KM_KM_2018_y)\n# max_true_2018 = np.max(np.array(sample_y_true_2018))\n# sample_y_true_2018 = sample_y_true_2018 / max_true_2018\n\n# row_arr = sample_x_true_2018\n# #y = np.zeros(len(row_arr))\n# #for i = range(len(row_arr))\n\n# y = sample_regr.predict(row_arr)\n# for i in range(len(row_arr)):\n#     if row_arr[i][0] >= 335:\n#         y[i] = y[i] + dec_regr.predict([row_arr[i]])\n#     elif row_arr[i][0] <= 10:\n#         y[i] += jan10_regr.predict([row_arr[i]])\n#     elif row_arr[0][4] != 100:\n#         y[i] += easter_regr.predict([row_arr[i]])\n# year_max = max_model.predict([[2018]])[0]\n\n# #dec_predict_2018 = predict_dec(sample_x_true_2018,sample_x_dec_2018)\n# #easter_predict_2018 = predict_easter(sample_x_true_2018,sample_x_easter_2018,dec_predict_2018)\n# #sns.lineplot(x=range(1,366),y=easter_predict_2018)\n# #y = predict_10jan(sample_x_true_2018,sample_x_10jan_2018,easter_predict_2018)\n\n# print(year_max)\n\n# #\n# sns.lineplot(x=range(1,366),y=y)\n# #sns.lineplot(x=range(1,366),y=dec_regr.predict(row_arr))\n# #sns.lineplot(x=range(1,366),y=dec_regr.predict(row_arr) + y)\n# sns.lineplot(x=range(1,366),y=sample_y_true_2018)\n# print(SMAPE(sample_y_true_2018*max_true_2018,y*year_max))","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def learn_product(country,store,product):\n\n    F_KM_KM_2015 = Train_copy.query('country == @country and store == @store and product == @product and date.dt.year == 2015')\n    F_KM_KM_2016 = Train_copy.query('country == @country and store == @store and product == @product and date.dt.year == 2016')\n    F_KM_KM_2017 = Train_copy.query('country == @country and store == @store and product == @product and date.dt.year == 2017')\n    F_KM_KM_2018 = Train_copy.query('country == @country and store == @store and product == @product and date.dt.year == 2018')\n\n    F_KM_KM_2015_y = F_KM_KM_2015['num_sold']\n    F_KM_KM_2016_y = F_KM_KM_2016['num_sold']\n    F_KM_KM_2017_y = F_KM_KM_2017['num_sold']\n    F_KM_KM_2018_y = F_KM_KM_2018['num_sold']\n\n    F_KM_KM_2015.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\n    F_KM_KM_2016.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\n    F_KM_KM_2017.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True) #, 'dteaster', 'dtchristmas', '1jan'\n    F_KM_KM_2018.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\n\n    sample_x_true_2015 = np.array(F_KM_KM_2015)\n    sample_x_true_2016 = np.array(F_KM_KM_2016)\n    sample_x_true_2017 = np.array(F_KM_KM_2017)\n    sample_x_true_2018 = np.array(F_KM_KM_2018)\n\n    sample_y_true_2015 = np.array(F_KM_KM_2015_y)\n    sample_y_true_2016 = np.array(F_KM_KM_2016_y)\n    sample_y_true_2017 = np.array(F_KM_KM_2017_y)\n    sample_y_true_2018 = np.array(F_KM_KM_2018_y)\n\n    max_true_2015 = np.max(np.array(sample_y_true_2015))\n    max_true_2016 = np.max(np.array(sample_y_true_2016))\n    max_true_2017 = np.max(np.array(sample_y_true_2017))\n    max_true_2018 = np.max(np.array(sample_y_true_2018))\n\n    sample_y_true_2015 = sample_y_true_2015 / max_true_2015\n    sample_y_true_2016 = sample_y_true_2016 / max_true_2016\n    sample_y_true_2017 = sample_y_true_2017 / max_true_2017\n    sample_y_true_2018 = sample_y_true_2018 / max_true_2018\n\n    F_KM_KM_2015 = Train_clean.query('country == @country and store == @store and product == @product and date.dt.year == 2015')\n    F_KM_KM_2016 = Train_clean.query('country == @country and store == @store and product == @product and date.dt.year == 2016')\n    F_KM_KM_2017 = Train_clean.query('country == @country and store == @store and product == @product and date.dt.year == 2017')\n    F_KM_KM_2018 = Train_clean.query('country == @country and store == @store and product == @product and date.dt.year == 2018')\n\n    F_KM_KM_2015_y = F_KM_KM_2015['num_sold']\n    F_KM_KM_2016_y = F_KM_KM_2016['num_sold']\n    F_KM_KM_2017_y = F_KM_KM_2017['num_sold']\n    F_KM_KM_2018_y = F_KM_KM_2018['num_sold']\n\n    F_KM_KM_2015.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\n    F_KM_KM_2016.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\n    F_KM_KM_2017.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\n    F_KM_KM_2018.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\n\n    sample_x_2015 = np.array(F_KM_KM_2015)\n    sample_x_2016 = np.array(F_KM_KM_2016)\n    sample_x_2017 = np.array(F_KM_KM_2017)\n    sample_x_2018 = np.array(F_KM_KM_2018)\n\n    sample_y_2015 = np.array(F_KM_KM_2015_y)\n    sample_y_2016 = np.array(F_KM_KM_2016_y)\n    sample_y_2017 = np.array(F_KM_KM_2017_y)\n    sample_y_2018 = np.array(F_KM_KM_2018_y)\n\n    sample_y_2015 = sample_y_2015 / max_true_2015\n    sample_y_2016 = sample_y_2016 / max_true_2016\n    sample_y_2017 = sample_y_2017 / max_true_2017\n    sample_y_2018 = sample_y_2018 / max_true_2018\n\n    F_KM_KM_2015 = Train_dec.query('country == @country and store == @store and product == @product and date.dt.year == 2015')\n    F_KM_KM_2016 = Train_dec.query('country == @country and store == @store and product == @product and date.dt.year == 2016')\n    F_KM_KM_2017 = Train_dec.query('country == @country and store == @store and product == @product and date.dt.year == 2017')\n    F_KM_KM_2018 = Train_dec.query('country == @country and store == @store and product == @product and date.dt.year == 2018')\n\n    F_KM_KM_2015_y = F_KM_KM_2015['num_sold']\n    F_KM_KM_2016_y = F_KM_KM_2016['num_sold']\n    F_KM_KM_2017_y = F_KM_KM_2017['num_sold']\n    F_KM_KM_2018_y = F_KM_KM_2018['num_sold']\n\n    F_KM_KM_2015.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\n    F_KM_KM_2016.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\n    F_KM_KM_2017.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\n    F_KM_KM_2018.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\n\n    sample_x_dec_2015 = np.array(F_KM_KM_2015)\n    sample_x_dec_2016 = np.array(F_KM_KM_2016)\n    sample_x_dec_2017 = np.array(F_KM_KM_2017)\n    sample_x_dec_2018 = np.array(F_KM_KM_2018)\n\n    F_KM_KM_2015 = Train_10jan.query('country == @country and store == @store and product == @product and date.dt.year == 2015')\n    F_KM_KM_2016 = Train_10jan.query('country == @country and store == @store and product == @product and date.dt.year == 2016')\n    F_KM_KM_2017 = Train_10jan.query('country == @country and store == @store and product == @product and date.dt.year == 2017')\n    F_KM_KM_2018 = Train_10jan.query('country == @country and store == @store and product == @product and date.dt.year == 2018')\n\n    F_KM_KM_2015_y = F_KM_KM_2015['num_sold']\n    F_KM_KM_2016_y = F_KM_KM_2016['num_sold']\n    F_KM_KM_2017_y = F_KM_KM_2017['num_sold']\n    F_KM_KM_2018_y = F_KM_KM_2018['num_sold']\n\n    F_KM_KM_2015.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\n    F_KM_KM_2016.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\n    F_KM_KM_2017.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\n    F_KM_KM_2018.drop(['date', 'country', 'store', 'product', 'row_id', 'num_sold'],axis=1,inplace=True)\n\n    sample_x_10jan_2015 = np.array(F_KM_KM_2015)\n    sample_x_10jan_2016 = np.array(F_KM_KM_2016)\n    sample_x_10jan_2017 = np.array(F_KM_KM_2017)\n    sample_x_10jan_2018 = np.array(F_KM_KM_2018)\n\n    sample_x = np.concatenate((sample_x_2017,sample_x_2016,sample_x_2015),axis = 0)\n    sample_y = np.concatenate((sample_y_2017,sample_y_2016,sample_y_2015),axis = 0)\n\n    sample_x_true = np.concatenate((sample_x_true_2017,sample_x_true_2016,sample_x_true_2015),axis = 0)\n    sample_y_true = np.concatenate((sample_y_true_2017,sample_y_true_2016,sample_y_true_2015),axis = 0)\n\n    sample_x_dec = np.concatenate((sample_x_dec_2017,sample_x_dec_2016,sample_x_dec_2015),axis = 0)\n    sample_y_dec = np.concatenate((sample_y_dec_2017,sample_y_dec_2016,sample_y_dec_2015),axis = 0)\n\n    sample_x_easter = np.concatenate((sample_x_easter_2017,sample_x_easter_2016,sample_x_easter_2015),axis = 0)\n    sample_x_10jan = np.concatenate((sample_x_10jan_2017,sample_x_10jan_2016,sample_x_10jan_2015),axis = 0)\n\n\n    sample_regr = chosen_model()\n    sample_regr.fit(sample_x, sample_y)\n\n    deltay = sample_y_true-sample_regr.predict(sample_x_true)\n\n    deltay_dec = np.zeros(len(sample_x_dec))\n    for i in range(len(sample_x_dec)):\n        for j in range(len(sample_x_true)):\n            if np.all(sample_x_true[j] == sample_x_dec[i]):\n                deltay_dec[i] = deltay[j]\n                break\n\n    dec_regr = chosen_model()\n    dec_regr.fit(sample_x_dec, deltay_dec)\n\n\n\n    dec_predict = predict_dec(sample_x_true,sample_x_dec)\n    deltay = sample_y_true - dec_predict #Updating residuals\n\n    deltay_easter = np.zeros(len(sample_x_easter))\n    for i in range(len(sample_x_easter)):\n        for j in range(len(sample_x_true)):\n            if np.all(sample_x_true[j] == sample_x_easter[i]):\n                deltay_easter[i] = deltay[j]\n                break\n\n    easter_regr = chosen_model()\n    easter_regr.fit(sample_x_easter, deltay_easter)\n\n\n    easter_predict = predict_easter(sample_x_true,sample_x_easter, dec_predict)\n    deltay = sample_y_true - easter_predict #Updating residuals\n\n    deltay_10jan = np.zeros(len(sample_x_10jan))\n    for i in range(len(sample_x_10jan)):\n        for j in range(len(sample_x_true)):\n            if np.all(sample_x_true[j] == sample_x_10jan[i]):\n                deltay_10jan[i] = deltay[j]\n                break\n\n\n    jan10_regr = chosen_model()\n    jan10_regr.fit(sample_x_10jan, deltay_10jan)\n\n\n    max_model = LinearRegression()\n    max_model.fit(np.array(range(2015,2019)).reshape(-1,1),[max_true_2015,max_true_2016,max_true_2017,max_true_2018])\n\n    return sample_regr, dec_regr, easter_regr, jan10_regr, max_model\n\ndef fit_model():\n    country_dict = {}\n    for country in ['Finland','Sweden','Norway']:\n        curr_country_dict = {}\n        for store in ['KaggleMart','KaggleRama']:\n            curr_store_dict = {}\n            for product in ['Kaggle Mug','Kaggle Sticker', 'Kaggle Hat']:\n                curr_store_dict[product] = learn_product(country,store,product)\n            curr_country_dict[store] = curr_store_dict\n        country_dict[country] = curr_country_dict\n    return country_dict","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = fit_model()","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prediction","metadata":{}},{"cell_type":"code","source":"Test  = pd.read_csv(\"/kaggle/input/tabular-playground-series-jan-2022/test.csv\",parse_dates=[\"date\"])\n\n\nTest_copy = Test.copy()\nTest_copy['dayofyear'] = Test_copy['date'].dt.dayofyear\nTest_copy['dayofweek'] = Test_copy['date'].dt.day_of_week\n\nTest_copy['year'] = Test_copy['date'].dt.year\n\nTest_copy['dtchristmas'] = [dtchristmas(Test_copy.iloc[ind]['date']) for ind in Test_copy.index.values]\nTest_copy['dteaster'] = [dteaster(Test_copy.iloc[ind]['date']) for ind in Test_copy.index.values]\nTest_copy['1jan'] = [int(Test_copy.iloc[ind]['date'] == pd.Timestamp(Test_copy.iloc[ind]['date'].year,1,1)) for ind in Test_copy.index.values]\nTest_copy['wd4'] = Test_copy['date'].dt.day_of_week == 4\nTest_copy['wd56'] = Test_copy['date'].dt.day_of_week >= 5\n# for k in range(1, 3): # 20\n#         Test_copy[f'sin{k}'] = np.sin(Test_copy['dayofyear'] / 365 * 2 * math.pi * k)\n#         Test_copy[f'cos{k}'] = np.cos(Test_copy['dayofyear'] / 365 * 2 * math.pi * k)\n#         Test_copy[f'mug_sin{k}'] = Test_copy[f'sin{k}'] * (Test_copy['product'] == 'Kaggle Mug')\n#         Test_copy[f'mug_cos{k}'] = Test_copy[f'cos{k}'] * (Test_copy['product'] == 'Kaggle Mug')\n#         Test_copy[f'sticker_sin{k}'] = Test_copy[f'sin{k}'] * (Test_copy['product'] == 'Kaggle Sticker')\n#         Test_copy[f'sticker_cos{k}'] = Test_copy[f'cos{k}'] * (Test_copy['product'] == 'Kaggle Sticker')\n#         Test_copy[f'sticker_sin{k}'] = Test_copy[f'sin{k}'] * (Test_copy['product'] == 'Kaggle Hat')\n#         Test_copy[f'sticker_cos{k}'] = Test_copy[f'cos{k}'] * (Test_copy['product'] == 'Kaggle Hat')\n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_data(model,row):\n    country = row['country']\n    store = row['store']\n    product = row['product']\n    #return country,store,product\n    sample_regr, dec_regr, easter_regr, jan10_regr, max_model = model[country][store][product] \n    \n    row_arr = row.copy()\n    row_arr.drop(['date', 'country', 'store', 'product', 'row_id'],inplace=True)\n    row_arr = np.array(row_arr)\n    row_arr = [row_arr]\n    \n    #product_model = sample_regr, dec_regr, easter_regr, jan10_regr, max_model\n    #return row_arr\n    y = sample_regr.predict(row_arr)\n    if row_arr[0][0] >= 335:\n        y += dec_regr.predict(row_arr)\n    elif row_arr[0][0] <= 10:\n        y += jan10_regr.predict(row_arr)\n    elif row_arr[0][4] != 100:\n        y += easter_regr.predict(row_arr)\n    year_max = max_model.predict([[2018]])[0]\n    \n    return y[0] * year_max\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#SMAPE(Train_copy['num_sold'],[predict_data(model,Train_copy.iloc[ind]) for ind in Train_copy.index.values])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Submission","metadata":{}},{"cell_type":"code","source":"Result_DF = pd.DataFrame()\nResult_DF['row_id'] = Test['row_id']\nResult_DF['num_sold'] = [predict_data(model,Test_copy.iloc[ind]) for ind in Test_copy.index.values]","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Result_DF['num_sold'] = np.round(Result_DF['num_sold']).astype(int)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Result_DF","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Result_DF.to_csv('submission.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}