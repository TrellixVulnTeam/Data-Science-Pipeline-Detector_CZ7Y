{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set Matplotlib defaults\nplt.style.use(\"seaborn-whitegrid\")\nplt.rc(\"figure\", autolayout=True, figsize=(14, 6))\nplt.rc(\n    \"axes\",\n    labelweight=\"bold\",\n    labelsize=\"large\",\n    titleweight=\"bold\",\n    titlesize=14,\n    titlepad=10,\n)\n\nplot_params = dict(\n    color=\"0.75\",\n    style=\".-\",\n    markeredgecolor=\"0.25\",\n    markerfacecolor=\"0.25\",\n    legend=False,\n)\n\nfrom sklearn.linear_model import LinearRegression\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-06T02:54:37.901042Z","iopub.execute_input":"2022-01-06T02:54:37.901318Z","iopub.status.idle":"2022-01-06T02:54:37.91278Z","shell.execute_reply.started":"2022-01-06T02:54:37.901286Z","shell.execute_reply":"2022-01-06T02:54:37.911896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Motivation\n\nThis notebook is based on my learnings from the second lesson in Kaggle's [Time Series](https://www.kaggle.com/learn/time-series) course, *Trend*, applied to the January 2022 Tabular Playground Series challenge. I don't expect the results to be much improved over my [linear baseline](https://www.kaggle.com/bcruise/tps-jan-2022-linear-baseline/notebook), but it's worth a shot.","metadata":{}},{"cell_type":"markdown","source":"### Load the training data set","metadata":{}},{"cell_type":"code","source":"TRAIN_CSV = '/kaggle/input/tabular-playground-series-jan-2022/train.csv'\nTEST_CSV = '/kaggle/input/tabular-playground-series-jan-2022/test.csv'\nSAMPLE_CSV = '/kaggle/input/tabular-playground-series-jan-2022/sample_submission.csv'\n\n# Use the date column as the index\ntrain_df = pd.read_csv(TRAIN_CSV, parse_dates=['date'])\ntrain_df = train_df.set_index('date').to_period('D')\ntrain_df","metadata":{"execution":{"iopub.status.busy":"2022-01-06T02:54:37.914278Z","iopub.execute_input":"2022-01-06T02:54:37.914657Z","iopub.status.idle":"2022-01-06T02:54:37.983572Z","shell.execute_reply.started":"2022-01-06T02:54:37.914624Z","shell.execute_reply":"2022-01-06T02:54:37.982915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We'll start with just a small sample of the data so we can show a rolling average for one product, country, and store.","metadata":{}},{"cell_type":"code","source":"hat_sales = train_df.loc[train_df['product'] == 'Kaggle Hat'].copy()\ndata_sample=hat_sales.loc[(hat_sales['country']=='Finland') & (hat_sales['store']=='KaggleMart')]\ndata_sample = data_sample['num_sold']","metadata":{"execution":{"iopub.status.busy":"2022-01-06T02:54:37.984799Z","iopub.execute_input":"2022-01-06T02:54:37.98522Z","iopub.status.idle":"2022-01-06T02:54:38.000714Z","shell.execute_reply.started":"2022-01-06T02:54:37.985172Z","shell.execute_reply":"2022-01-06T02:54:37.999903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Plotting a rolling average","metadata":{}},{"cell_type":"code","source":"trend = data_sample.rolling(\n    window=365,       # 365-day window\n    center=True,      # puts the average at the center of the window\n    min_periods=183,  # choose about half the window size\n).mean()              # compute the mean (could also do median, std, min, max, ...)\n\n# Make a plot\nax = data_sample.plot(**plot_params, alpha=0.5)\nax = trend.plot(ax=ax, linewidth=3, legend=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-06T02:54:38.002614Z","iopub.execute_input":"2022-01-06T02:54:38.003148Z","iopub.status.idle":"2022-01-06T02:54:38.309956Z","shell.execute_reply.started":"2022-01-06T02:54:38.003117Z","shell.execute_reply":"2022-01-06T02:54:38.309115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The rolling average is used to determine if there's an overall trend in the data by averaging over the longest period (a year in this case). The data seems to be trending slightly upwards each year, but it looks fairly linear. This is why I suspect the results will not be much better than the baseline. Let's try a quadratic fit to see if it's any better.","metadata":{}},{"cell_type":"code","source":"from statsmodels.tsa.deterministic import DeterministicProcess\n\ny = data_sample.copy()  # the target\n\n# Instantiate `DeterministicProcess` with arguments appropriate for a quadratic trend model\ndp = DeterministicProcess(index=data_sample.index, order=2)\n\n# Create the feature set for the dates given in y.index\nX = dp.in_sample()\n\n# Create features for a 1-year forecast.\nX_fore = dp.out_of_sample(steps=365)\n\nmodel = LinearRegression()\nmodel.fit(X, y)\n\ny_pred = pd.Series(model.predict(X), index=X.index)\ny_fore = pd.Series(model.predict(X_fore), index=X_fore.index)\n\nax = y.plot(**plot_params, alpha=0.5, title=\"Average Sales\", ylabel=\"items sold\")\nax = y_pred.plot(ax=ax, linewidth=3, label=\"Trend\", color='C0')\nax = y_fore.plot(ax=ax, linewidth=3, label=\"Trend Forecast\", color='C3')","metadata":{"execution":{"iopub.status.busy":"2022-01-06T02:54:38.311117Z","iopub.execute_input":"2022-01-06T02:54:38.311336Z","iopub.status.idle":"2022-01-06T02:54:38.647892Z","shell.execute_reply.started":"2022-01-06T02:54:38.31131Z","shell.execute_reply":"2022-01-06T02:54:38.647034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note the slight upwards curve of the trendline and forecast. The predictions for this model should be slightly higher than for a linear model.","metadata":{}},{"cell_type":"markdown","source":"### Estimate SMAPE score\nTo estimate our leaderboard score, we can train out models on the first three years of training data, then compute the SMAPE score for the last year of training data.","metadata":{}},{"cell_type":"code","source":"df_pre_2018 = train_df.loc[train_df.index < '2018-01-01']\ndf_2018 = train_df.loc[train_df.index > '2017-12-31']\ndf_2018","metadata":{"execution":{"iopub.status.busy":"2022-01-06T02:54:38.649326Z","iopub.execute_input":"2022-01-06T02:54:38.649624Z","iopub.status.idle":"2022-01-06T02:54:38.671008Z","shell.execute_reply.started":"2022-01-06T02:54:38.649588Z","shell.execute_reply":"2022-01-06T02:54:38.669947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Build the quadratic models and make predictions","metadata":{}},{"cell_type":"code","source":"from statsmodels.tsa.deterministic import DeterministicProcess\n\ncountries = ['Finland', 'Norway', 'Sweden']\nproducts = ['Kaggle Hat', 'Kaggle Mug', 'Kaggle Sticker']\nstores = ['KaggleMart', 'KaggleRama']\n\ndef train_and_predict(train_df, test_df):\n    pred_dfs = list()\n\n    for country in countries:\n        for product in products:\n            for store in stores:\n                df = train_df.loc[(train_df['country'] == country) &\n                                  (train_df['product'] == product) &\n                                  (train_df['store'] == store)].copy()\n                test_sample = test_df.loc[(test_df['country'] == country) &\n                                          (test_df['product'] == product) &\n                                          (test_df['store'] == store)].copy()\n                \n                y = df['num_sold'].copy()  # the target\n                \n                # Instantiate `DeterministicProcess` with arguments appropriate for a quadratic trend model.\n                # If we use order=1 here, it will be exactly the same as a linear model.\n                dp = DeterministicProcess(index=df.index, order=2)\n\n                # Create the feature set for the dates given in y.index\n                X = dp.in_sample()\n\n                # Create features for a forecast.\n                X_fore = dp.out_of_sample(steps=len(test_sample))\n\n                # Train the model\n                model = LinearRegression()\n                model.fit(X, y)\n                \n                # Make the forecast\n                y_fore = model.predict(X_fore)\n                \n                pred_df = pd.DataFrame({'row_id': test_sample['row_id'], 'y_pred': y_fore}).reset_index(drop=True)\n                pred_dfs.append(pred_df)\n\n    predictions_df = pd.concat(pred_dfs)\n    return predictions_df\n\npreds_2018_df = train_and_predict(df_pre_2018, df_2018)\npreds_2018_df","metadata":{"execution":{"iopub.status.busy":"2022-01-06T02:54:38.672338Z","iopub.execute_input":"2022-01-06T02:54:38.672684Z","iopub.status.idle":"2022-01-06T02:54:39.102218Z","shell.execute_reply.started":"2022-01-06T02:54:38.672649Z","shell.execute_reply":"2022-01-06T02:54:39.10164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_pred_df = df_2018.merge(preds_2018_df, how='left', on='row_id')\nval_pred_df","metadata":{"execution":{"iopub.status.busy":"2022-01-06T02:54:39.103981Z","iopub.execute_input":"2022-01-06T02:54:39.104642Z","iopub.status.idle":"2022-01-06T02:54:39.125628Z","shell.execute_reply.started":"2022-01-06T02:54:39.104611Z","shell.execute_reply":"2022-01-06T02:54:39.124816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def smape(a, f):\n    return 1/len(a) * np.sum(2 * np.abs(f-a) / (np.abs(a) + np.abs(f))*100)\n\nactual = np.array(val_pred_df['num_sold'])\nforecast = np.array(val_pred_df['y_pred'])\nsmape(actual, forecast)","metadata":{"execution":{"iopub.status.busy":"2022-01-06T02:54:39.127009Z","iopub.execute_input":"2022-01-06T02:54:39.127529Z","iopub.status.idle":"2022-01-06T02:54:39.137469Z","shell.execute_reply.started":"2022-01-06T02:54:39.127485Z","shell.execute_reply":"2022-01-06T02:54:39.136628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This score is (as expected) worse than using a linear model. We could try higher-order polynomials, but those are only likely to be even worse for forecasting.\n\nThere's not much use in making predictions to submit to the competition based on this model, but let's do it anyway. We've made it this far, and the hard part is already done.","metadata":{}},{"cell_type":"markdown","source":"### Load the test data set","metadata":{}},{"cell_type":"code","source":"# Keep the row_id for the sample submission\ntest_df = pd.read_csv(TEST_CSV, index_col='date', parse_dates=['date'])\ntest_df","metadata":{"execution":{"iopub.status.busy":"2022-01-06T02:54:39.138857Z","iopub.execute_input":"2022-01-06T02:54:39.139681Z","iopub.status.idle":"2022-01-06T02:54:39.173161Z","shell.execute_reply.started":"2022-01-06T02:54:39.139617Z","shell.execute_reply":"2022-01-06T02:54:39.172388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Re-train models and make predictions using all training data","metadata":{}},{"cell_type":"code","source":"preds_2019_df = train_and_predict(train_df, test_df)\npreds_2019_df","metadata":{"execution":{"iopub.status.busy":"2022-01-06T02:54:39.174469Z","iopub.execute_input":"2022-01-06T02:54:39.175126Z","iopub.status.idle":"2022-01-06T02:54:39.677323Z","shell.execute_reply.started":"2022-01-06T02:54:39.175086Z","shell.execute_reply":"2022-01-06T02:54:39.67668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_pred_df = test_df.merge(preds_2019_df, how='left', on='row_id')\ntest_pred_df","metadata":{"execution":{"iopub.status.busy":"2022-01-06T02:54:39.678533Z","iopub.execute_input":"2022-01-06T02:54:39.678915Z","iopub.status.idle":"2022-01-06T02:54:39.700252Z","shell.execute_reply.started":"2022-01-06T02:54:39.678866Z","shell.execute_reply":"2022-01-06T02:54:39.699044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df = test_pred_df[['row_id', 'y_pred']]\nsubmission_df.columns = ['row_id', 'num_sold']\nsubmission_df.to_csv('./submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-06T02:54:39.702121Z","iopub.execute_input":"2022-01-06T02:54:39.702356Z","iopub.status.idle":"2022-01-06T02:54:39.734119Z","shell.execute_reply.started":"2022-01-06T02:54:39.702328Z","shell.execute_reply":"2022-01-06T02:54:39.733249Z"},"trusted":true},"execution_count":null,"outputs":[]}]}