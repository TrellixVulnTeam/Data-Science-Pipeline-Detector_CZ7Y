{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom sklearn.linear_model import LinearRegression\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set Matplotlib defaults\nplt.style.use(\"seaborn-whitegrid\")\nplt.rc(\"figure\", autolayout=True, figsize=(13, 5))\nplt.rc(\n    \"axes\",\n    labelweight=\"bold\",\n    labelsize=\"large\",\n    titleweight=\"bold\",\n    titlesize=16,\n    titlepad=10,\n)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-09T01:48:33.218871Z","iopub.execute_input":"2022-01-09T01:48:33.219899Z","iopub.status.idle":"2022-01-09T01:48:33.232465Z","shell.execute_reply.started":"2022-01-09T01:48:33.219841Z","shell.execute_reply":"2022-01-09T01:48:33.231514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Motivation\n\nThis notebook is based on my learnings from the third lesson in Kaggle's [Time Series](https://www.kaggle.com/learn/time-series) course, *Seasonality*, applied to the January 2022 Tabular Playground Series challenge. There's a definite annual cycle to the sales of two out of three products that we're trying to predict, so I expect we'll be able to improve on the linear baseline by using seasonality in our models.","metadata":{}},{"cell_type":"markdown","source":"### Load the training data set","metadata":{}},{"cell_type":"code","source":"TRAIN_CSV = '/kaggle/input/tabular-playground-series-jan-2022/train.csv'\nTEST_CSV = '/kaggle/input/tabular-playground-series-jan-2022/test.csv'\nSAMPLE_CSV = '/kaggle/input/tabular-playground-series-jan-2022/sample_submission.csv'\n\n# Use the date column as the index\ntrain_df = pd.read_csv(TRAIN_CSV, parse_dates=['date'])\ntrain_df = train_df.set_index('date').to_period('D')","metadata":{"execution":{"iopub.status.busy":"2022-01-09T01:48:33.234869Z","iopub.execute_input":"2022-01-09T01:48:33.235195Z","iopub.status.idle":"2022-01-09T01:48:33.28952Z","shell.execute_reply.started":"2022-01-09T01:48:33.235152Z","shell.execute_reply":"2022-01-09T01:48:33.288836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In my previous two notebooks, I had a lot of code duplication that I'm trying to reduce. Let's start by defining a simple function to extract a subset of the data for a country, store, and product. This will be useful for creating charts and for building models.","metadata":{}},{"cell_type":"code","source":"def extract_subset(data_set, country, store, product):\n    df = data_set.loc[(data_set['country'] == country) &\n                      (data_set['store'] == store) &\n                      (data_set['product'] == product)].copy()\n    return df\n\nsubset_df = extract_subset(train_df, 'Finland', 'KaggleMart', 'Kaggle Mug')\nsubset_df","metadata":{"execution":{"iopub.status.busy":"2022-01-09T01:48:33.290717Z","iopub.execute_input":"2022-01-09T01:48:33.291659Z","iopub.status.idle":"2022-01-09T01:48:33.326016Z","shell.execute_reply.started":"2022-01-09T01:48:33.291612Z","shell.execute_reply":"2022-01-09T01:48:33.325186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"subset_df['num_sold'].plot(title='Time plot of Mug sales at KaggleMart in Finland');","metadata":{"execution":{"iopub.status.busy":"2022-01-09T01:48:33.327545Z","iopub.execute_input":"2022-01-09T01:48:33.328073Z","iopub.status.idle":"2022-01-09T01:48:33.685995Z","shell.execute_reply.started":"2022-01-09T01:48:33.328027Z","shell.execute_reply":"2022-01-09T01:48:33.685158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I'm also stealing two functions for making seasonal plots and periodogram from the Kaggle Time Series [Seasonality tutorial](https://www.kaggle.com/ryanholbrook/seasonality/tutorial). Expand the following code section to view their source.","metadata":{}},{"cell_type":"code","source":"# annotations: https://stackoverflow.com/a/49238256/5769929\ndef seasonal_plot(X, y, period, freq, ax=None):\n    if ax is None:\n        _, ax = plt.subplots()\n    palette = sns.color_palette(\"husl\", n_colors=X[period].nunique(),)\n    ax = sns.lineplot(\n        x=freq,\n        y=y,\n        hue=period,\n        data=X,\n        ci=False,\n        ax=ax,\n        palette=palette,\n        legend=False,\n    )\n    ax.set_title(f\"Seasonal Plot ({period}/{freq})\")\n    for line, name in zip(ax.lines, X[period].unique()):\n        y_ = line.get_ydata()[-1]\n        ax.annotate(\n            name,\n            xy=(1, y_),\n            xytext=(6, 0),\n            color=line.get_color(),\n            xycoords=ax.get_yaxis_transform(),\n            textcoords=\"offset points\",\n            size=14,\n            va=\"center\",\n        )\n    return ax\n\ndef plot_periodogram(ts, detrend='linear', ax=None):\n    from scipy.signal import periodogram\n    fs = pd.Timedelta(\"1Y\") / pd.Timedelta(\"1D\")\n    freqencies, spectrum = periodogram(\n        ts,\n        fs=fs,\n        detrend=detrend,\n        window=\"boxcar\",\n        scaling='spectrum',\n    )\n    if ax is None:\n        _, ax = plt.subplots()\n    ax.step(freqencies, spectrum, color=\"purple\")\n    ax.set_xscale(\"log\")\n    ax.set_xticks([1, 2, 4, 6, 12, 26, 52, 104])\n    ax.set_xticklabels(\n        [\n            \"Annual (1)\",\n            \"Semiannual (2)\",\n            \"Quarterly (4)\",\n            \"Bimonthly (6)\",\n            \"Monthly (12)\",\n            \"Biweekly (26)\",\n            \"Weekly (52)\",\n            \"Semiweekly (104)\",\n        ],\n        rotation=30,\n    )\n    ax.ticklabel_format(axis=\"y\", style=\"sci\", scilimits=(0, 0))\n    ax.set_ylabel(\"Variance\")\n    ax.set_title(\"Periodogram\")\n    return ax","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-09T01:48:33.687946Z","iopub.execute_input":"2022-01-09T01:48:33.68818Z","iopub.status.idle":"2022-01-09T01:48:33.701631Z","shell.execute_reply.started":"2022-01-09T01:48:33.688127Z","shell.execute_reply":"2022-01-09T01:48:33.701074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's take a look at seasonal plots for the mug sales sample data over a week and over a year.","metadata":{}},{"cell_type":"code","source":"X = subset_df.copy()\n\n# days within a week\nX[\"day\"] = X.index.dayofweek  # the x-axis (freq)\nX[\"week\"] = X.index.week  # the seasonal period (period)\n\n# days within a year\nX[\"dayofyear\"] = X.index.dayofyear\nX[\"year\"] = X.index.year\n\nfig, (ax0, ax1) = plt.subplots(2, 1, figsize=(11, 6))\nseasonal_plot(X, y=\"num_sold\", period=\"week\", freq=\"day\", ax=ax0)\nseasonal_plot(X, y=\"num_sold\", period=\"year\", freq=\"dayofyear\", ax=ax1);","metadata":{"execution":{"iopub.status.busy":"2022-01-09T01:48:33.702484Z","iopub.execute_input":"2022-01-09T01:48:33.703094Z","iopub.status.idle":"2022-01-09T01:48:44.502655Z","shell.execute_reply.started":"2022-01-09T01:48:33.703063Z","shell.execute_reply":"2022-01-09T01:48:44.501863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_periodogram(subset_df['num_sold']);","metadata":{"execution":{"iopub.status.busy":"2022-01-09T01:48:44.504088Z","iopub.execute_input":"2022-01-09T01:48:44.504319Z","iopub.status.idle":"2022-01-09T01:48:45.032928Z","shell.execute_reply.started":"2022-01-09T01:48:44.504292Z","shell.execute_reply":"2022-01-09T01:48:45.032176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There's definitely very strong annual seasonality and weekly seasonality. There are several other time periods as well, but for this notebook I really just want to capture the annual seasons. There will be other notebooks in the series where I might explore other trends.\n\nLet's see the sales, seasonal plots and periodograms for hat and sticker sales as well.","metadata":{}},{"cell_type":"code","source":"subset_df = extract_subset(train_df, 'Finland', 'KaggleMart', 'Kaggle Hat')\nsubset_df['num_sold'].plot(title='Time plot of Hat sales at KaggleMart in Finland');","metadata":{"execution":{"iopub.status.busy":"2022-01-09T01:48:45.034364Z","iopub.execute_input":"2022-01-09T01:48:45.034554Z","iopub.status.idle":"2022-01-09T01:48:45.359961Z","shell.execute_reply.started":"2022-01-09T01:48:45.034531Z","shell.execute_reply":"2022-01-09T01:48:45.359115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = subset_df.copy()\n\n# days within a week\nX[\"day\"] = X.index.dayofweek  # the x-axis (freq)\nX[\"week\"] = X.index.week  # the seasonal period (period)\n\n# days within a year\nX[\"dayofyear\"] = X.index.dayofyear\nX[\"year\"] = X.index.year\n\nfig, (ax0, ax1) = plt.subplots(2, 1, figsize=(11, 6))\nseasonal_plot(X, y=\"num_sold\", period=\"week\", freq=\"day\", ax=ax0)\nseasonal_plot(X, y=\"num_sold\", period=\"year\", freq=\"dayofyear\", ax=ax1);","metadata":{"execution":{"iopub.status.busy":"2022-01-09T01:48:45.361044Z","iopub.execute_input":"2022-01-09T01:48:45.361615Z","iopub.status.idle":"2022-01-09T01:48:56.196783Z","shell.execute_reply.started":"2022-01-09T01:48:45.361585Z","shell.execute_reply":"2022-01-09T01:48:56.196173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_periodogram(subset_df['num_sold']);","metadata":{"execution":{"iopub.status.busy":"2022-01-09T01:48:56.197903Z","iopub.execute_input":"2022-01-09T01:48:56.198086Z","iopub.status.idle":"2022-01-09T01:48:56.719622Z","shell.execute_reply.started":"2022-01-09T01:48:56.198063Z","shell.execute_reply":"2022-01-09T01:48:56.719094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"subset_df = extract_subset(train_df, 'Finland', 'KaggleMart', 'Kaggle Sticker')\nsubset_df['num_sold'].plot(title='Time plot of Sticker sales at KaggleMart in Finland');","metadata":{"execution":{"iopub.status.busy":"2022-01-09T01:48:56.720766Z","iopub.execute_input":"2022-01-09T01:48:56.721109Z","iopub.status.idle":"2022-01-09T01:48:57.053283Z","shell.execute_reply.started":"2022-01-09T01:48:56.721081Z","shell.execute_reply":"2022-01-09T01:48:57.052743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = subset_df.copy()\n\n# days within a week\nX[\"day\"] = X.index.dayofweek  # the x-axis (freq)\nX[\"week\"] = X.index.week  # the seasonal period (period)\n\n# days within a year\nX[\"dayofyear\"] = X.index.dayofyear\nX[\"year\"] = X.index.year\n\nfig, (ax0, ax1) = plt.subplots(2, 1, figsize=(11, 6))\nseasonal_plot(X, y=\"num_sold\", period=\"week\", freq=\"day\", ax=ax0)\nseasonal_plot(X, y=\"num_sold\", period=\"year\", freq=\"dayofyear\", ax=ax1);","metadata":{"execution":{"iopub.status.busy":"2022-01-09T01:48:57.054375Z","iopub.execute_input":"2022-01-09T01:48:57.054735Z","iopub.status.idle":"2022-01-09T01:49:07.952644Z","shell.execute_reply.started":"2022-01-09T01:48:57.054706Z","shell.execute_reply":"2022-01-09T01:49:07.951899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_periodogram(subset_df['num_sold']);","metadata":{"execution":{"iopub.status.busy":"2022-01-09T01:49:07.9538Z","iopub.execute_input":"2022-01-09T01:49:07.953999Z","iopub.status.idle":"2022-01-09T01:49:08.499173Z","shell.execute_reply.started":"2022-01-09T01:49:07.953974Z","shell.execute_reply":"2022-01-09T01:49:08.498282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For hat sales, the annual seasonality is even stronger than for other time periods. For sticker sales, the weekly seasonality period is much stronger.","metadata":{}},{"cell_type":"markdown","source":"### Estimate SMAPE score\n\nTo estimate our leaderboard score, we can train out models on the first three years of training data, then compute the SMAPE score for the last year of training data. I'll use the SMAPE function provided by CPMP during the [Web Traffic Time Series Forecasting](https://www.kaggle.com/c/web-traffic-time-series-forecasting/discussion/36414) competition a few years ago.","metadata":{}},{"cell_type":"code","source":"# https://www.kaggle.com/c/web-traffic-time-series-forecasting/discussion/36414\n\ndef SMAPE(y_true, y_pred):\n    denominator = (y_true + np.abs(y_pred)) / 200.0\n    diff = np.abs(y_true - y_pred) / denominator\n    diff[denominator == 0] = 0.0\n    return np.round(np.mean(diff),5)","metadata":{"execution":{"iopub.status.busy":"2022-01-09T01:49:08.501846Z","iopub.execute_input":"2022-01-09T01:49:08.502053Z","iopub.status.idle":"2022-01-09T01:49:08.507138Z","shell.execute_reply.started":"2022-01-09T01:49:08.502026Z","shell.execute_reply":"2022-01-09T01:49:08.506405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train on (2015, 2016, 2017) data, test on 2018\ndf_pre_2018 = train_df.loc[train_df.index < '2018-01-01']\ndf_2018 = train_df.loc[train_df.index > '2017-12-31']","metadata":{"execution":{"iopub.status.busy":"2022-01-09T01:49:08.508222Z","iopub.execute_input":"2022-01-09T01:49:08.508557Z","iopub.status.idle":"2022-01-09T01:49:08.524175Z","shell.execute_reply.started":"2022-01-09T01:49:08.508529Z","shell.execute_reply":"2022-01-09T01:49:08.523652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Build the models and make predictions","metadata":{}},{"cell_type":"code","source":"from statsmodels.tsa.deterministic import CalendarFourier, DeterministicProcess\n\ncountries = ['Finland', 'Norway', 'Sweden']\nproducts = ['Kaggle Hat', 'Kaggle Mug', 'Kaggle Sticker']\nstores = ['KaggleMart', 'KaggleRama']\n\ndef train_and_predict(train_df, test_df):\n    pred_dfs = list()\n\n    for country in countries:\n        for product in products:\n            for store in stores:\n                df = extract_subset(train_df, country, store, product)\n                test_sample = extract_subset(test_df, country, store, product)\n                \n                y = df['num_sold'].copy()  # the target\n                \n                if product == 'Kaggle Sticker':\n                    # Use a simple linear model for sticker sales\n                    dp = DeterministicProcess(index=df.index, order=1)\n                else:\n                    # Add an annual seasonal component for mug sales and for hats\n                    order = 1 if product == 'Kaggle Mug' else 2\n                    fourier = CalendarFourier(freq=\"A\", order=order)  # sin/cos pairs for \"A\"nnual seasonality\n\n                    dp = DeterministicProcess(\n                        index=df.index,\n                        constant=True,               # dummy feature for bias (y-intercept)\n                        order=1,                     # trend (order 1 means linear)\n                        seasonal=True,               # weekly seasonality (indicators)\n                        additional_terms=[fourier],  # annual seasonality (fourier)\n                        drop=True,                   # drop terms to avoid collinearity\n                    )\n\n                # Create the feature set for the dates given in y.index\n                X = dp.in_sample()\n\n                # Create features for a forecast.\n                X_fore = dp.out_of_sample(steps=len(test_sample))\n\n                # Train the model\n                model = LinearRegression()\n                model.fit(X, y)\n                \n                # Make the forecast\n                y_fore = model.predict(X_fore)\n                # y_fore = pd.Series(model.predict(X), index=X.index)\n                \n                pred_df = pd.DataFrame({'row_id': test_sample['row_id'], 'y_pred': y_fore}).reset_index(drop=True)\n                pred_dfs.append(pred_df)\n\n    predictions_df = pd.concat(pred_dfs)\n    return predictions_df\n\npreds_2018_df = train_and_predict(df_pre_2018, df_2018)\npreds_2018_df","metadata":{"execution":{"iopub.status.busy":"2022-01-09T01:49:08.525363Z","iopub.execute_input":"2022-01-09T01:49:08.525748Z","iopub.status.idle":"2022-01-09T01:49:09.36244Z","shell.execute_reply.started":"2022-01-09T01:49:08.525708Z","shell.execute_reply":"2022-01-09T01:49:09.361411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_pred_df = df_2018.merge(preds_2018_df, how='left', on='row_id')","metadata":{"execution":{"iopub.status.busy":"2022-01-09T01:49:09.364286Z","iopub.execute_input":"2022-01-09T01:49:09.364577Z","iopub.status.idle":"2022-01-09T01:49:09.375227Z","shell.execute_reply.started":"2022-01-09T01:49:09.36454Z","shell.execute_reply":"2022-01-09T01:49:09.374264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"actual = np.array(val_pred_df['num_sold'])\nforecast = np.array(val_pred_df['y_pred'])\nSMAPE(actual, forecast)","metadata":{"execution":{"iopub.status.busy":"2022-01-09T01:49:09.379505Z","iopub.execute_input":"2022-01-09T01:49:09.379706Z","iopub.status.idle":"2022-01-09T01:49:09.38669Z","shell.execute_reply.started":"2022-01-09T01:49:09.379675Z","shell.execute_reply":"2022-01-09T01:49:09.385979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is a much better score than we saw in previous notebooks! My linear baseline (and best score so far) was 15.6. Let's make a submission with these models.","metadata":{}},{"cell_type":"markdown","source":"### Load the test data set","metadata":{}},{"cell_type":"code","source":"# Keep the row_id for the sample submission\ntest_df = pd.read_csv(TEST_CSV, index_col='date', parse_dates=['date'])\ntest_df","metadata":{"execution":{"iopub.status.busy":"2022-01-09T01:49:09.387863Z","iopub.execute_input":"2022-01-09T01:49:09.38862Z","iopub.status.idle":"2022-01-09T01:49:09.419185Z","shell.execute_reply.started":"2022-01-09T01:49:09.388577Z","shell.execute_reply":"2022-01-09T01:49:09.418362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Re-train models and make predictions using all training data","metadata":{}},{"cell_type":"code","source":"preds_2019_df = train_and_predict(train_df, test_df)\npreds_2019_df","metadata":{"execution":{"iopub.status.busy":"2022-01-09T01:49:09.420352Z","iopub.execute_input":"2022-01-09T01:49:09.42057Z","iopub.status.idle":"2022-01-09T01:49:10.361512Z","shell.execute_reply.started":"2022-01-09T01:49:09.420543Z","shell.execute_reply":"2022-01-09T01:49:10.360707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_pred_df = test_df.merge(preds_2019_df, how='left', on='row_id')\ntest_pred_df","metadata":{"execution":{"iopub.status.busy":"2022-01-09T01:49:10.362856Z","iopub.execute_input":"2022-01-09T01:49:10.363712Z","iopub.status.idle":"2022-01-09T01:49:10.38482Z","shell.execute_reply.started":"2022-01-09T01:49:10.363665Z","shell.execute_reply":"2022-01-09T01:49:10.384288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's combine the predictions with the original data to see how the forecasts look.","metadata":{}},{"cell_type":"code","source":"for country in countries:\n    for product in products:\n        for store in stores:\n            df = extract_subset(train_df, country, store, product)\n            pred_df = extract_subset(test_pred_df, country, store, product)\n            pred_df = pred_df.rename(columns = {'y_pred': 'num_sold'})\n            pred_df.index = pd.date_range(start='1/1/2019', end='12/31/2019')\n            \n            title = '{} - {} - {}'.format(country, product, store)\n            sales_and_pred = pd.concat([df[['num_sold']], pred_df[['num_sold']]])\n            \n            sales_and_pred.plot(title=title, legend=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-09T01:49:10.386176Z","iopub.execute_input":"2022-01-09T01:49:10.386959Z","iopub.status.idle":"2022-01-09T01:49:16.325529Z","shell.execute_reply.started":"2022-01-09T01:49:10.386919Z","shell.execute_reply":"2022-01-09T01:49:16.324797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It looks like some of the 2019 predictions might be a little bit high, maybe due to the spike in sales at the end of 2018. This led to a LB score of 14.3, much higher than the estimate. Hopefully we'll see how to deal with this in one of the next lessons.","metadata":{}},{"cell_type":"code","source":"submission_df = test_pred_df[['row_id', 'y_pred']]\nsubmission_df.columns = ['row_id', 'num_sold']\nsubmission_df.to_csv('./submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-09T01:49:16.326682Z","iopub.execute_input":"2022-01-09T01:49:16.326968Z","iopub.status.idle":"2022-01-09T01:49:16.357859Z","shell.execute_reply.started":"2022-01-09T01:49:16.326934Z","shell.execute_reply":"2022-01-09T01:49:16.357167Z"},"trusted":true},"execution_count":null,"outputs":[]}]}