{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom sklearn.linear_model import LinearRegression\nfrom xgboost import XGBRegressor\nfrom statsmodels.tsa.deterministic import DeterministicProcess\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set Matplotlib defaults\nplt.style.use(\"seaborn-whitegrid\")\nplt.rc(\"figure\", autolayout=True, figsize=(13, 5))\nplt.rc(\n    \"axes\",\n    labelweight=\"bold\",\n    labelsize=\"large\",\n    titleweight=\"bold\",\n    titlesize=16,\n    titlepad=10,\n)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-13T16:15:26.072101Z","iopub.execute_input":"2022-01-13T16:15:26.072429Z","iopub.status.idle":"2022-01-13T16:15:26.089394Z","shell.execute_reply.started":"2022-01-13T16:15:26.072397Z","shell.execute_reply":"2022-01-13T16:15:26.088261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Motivation\n\nThis notebook is based on my learnings from the fifth lesson in Kaggle's [Time Series](https://www.kaggle.com/learn/time-series) course, Hybrid Models, applied to the January 2022 Tabular Playground Series challenge. In my previous notebooks I applied linear models and a seasonal model to this challenge with mixed results. Hopefully a hybrid model is what's needed to improve further and advance a bit on the learderboard.","metadata":{}},{"cell_type":"markdown","source":"### Load the training data set","metadata":{}},{"cell_type":"code","source":"TRAIN_CSV = '/kaggle/input/tabular-playground-series-jan-2022/train.csv'\nTEST_CSV = '/kaggle/input/tabular-playground-series-jan-2022/test.csv'\nSAMPLE_CSV = '/kaggle/input/tabular-playground-series-jan-2022/sample_submission.csv'\n\n# Use the date column as the index\ntrain_df = pd.read_csv(TRAIN_CSV, parse_dates=['date'])\ntrain_df = train_df.set_index('date').to_period('D')","metadata":{"execution":{"iopub.status.busy":"2022-01-13T16:15:26.091742Z","iopub.execute_input":"2022-01-13T16:15:26.092099Z","iopub.status.idle":"2022-01-13T16:15:26.156911Z","shell.execute_reply.started":"2022-01-13T16:15:26.092063Z","shell.execute_reply":"2022-01-13T16:15:26.155857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def extract_subset(data_set, country, store, product):\n    \"\"\"\n    Extract a subset of sales data for one country, store, and product.\n    \"\"\"\n    df = data_set.loc[(data_set['country'] == country) &\n                      (data_set['store'] == store) &\n                      (data_set['product'] == product)].copy()\n    return df\n\nsubset_df = extract_subset(train_df, 'Finland', 'KaggleMart', 'Kaggle Mug')\nsubset_df","metadata":{"execution":{"iopub.status.busy":"2022-01-13T16:15:26.158675Z","iopub.execute_input":"2022-01-13T16:15:26.158977Z","iopub.status.idle":"2022-01-13T16:15:26.195532Z","shell.execute_reply.started":"2022-01-13T16:15:26.158943Z","shell.execute_reply":"2022-01-13T16:15:26.194696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### The BoostedHybrid class\n\nThe tutorial shows us how to define a `BoostedHybrid` class that will apply two different models to a data set and combine the results. This is a powerful idea that could be expanded to include any number of models, but I'd expect diminishing returns.","metadata":{}},{"cell_type":"code","source":"class BoostedHybrid:\n    def __init__(self, model_1, model_2):\n        self.model_1 = model_1\n        self.model_2 = model_2\n        self.y_columns = None  # store column names from fit method\n        \n    def fit(self, X_1, X_2, y):\n        # fit self.model_1\n        self.model_1.fit(X_1, y)\n\n        y_fit = pd.DataFrame(\n            # make predictions with self.model_1\n            self.model_1.predict(X_1),\n            index=X_1.index, columns=y.columns,\n        )\n\n        # compute residuals\n        y_resid = y - y_fit\n        y_resid = y_resid.stack().squeeze() # wide to long\n\n        # fit self.model_2 on residuals\n        self.model_2.fit(X_2, y_resid)\n\n        # Save column names for predict method\n        self.y_columns = y.columns\n        self.y_fit = y_fit\n        self.y_resid = y_resid\n        \n    def predict(self, X_1, X_2):\n        y_pred = pd.DataFrame(\n            # predict with self.model_1\n            self.model_1.predict(X_1),\n            index=X_1.index, columns=self.y_columns,\n        )\n        y_pred = y_pred.stack().squeeze()  # wide to long\n\n        # add self.model_2 predictions to y_pred\n        y_pred += self.model_2.predict(X_2)\n\n        return y_pred.unstack()  # long to wide","metadata":{"execution":{"iopub.status.busy":"2022-01-13T16:15:26.19903Z","iopub.execute_input":"2022-01-13T16:15:26.199592Z","iopub.status.idle":"2022-01-13T16:15:26.212118Z","shell.execute_reply.started":"2022-01-13T16:15:26.19955Z","shell.execute_reply":"2022-01-13T16:15:26.210969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We'll create a `BoostedHybrid` model for each of our 18 sales categories. We could pass different models for each category, but we'll start out keeping it simple and use the same models for all. Let's see what shape the data should take for each model.","metadata":{}},{"cell_type":"code","source":"y = subset_df[['num_sold']]\n\n# X_1: Features for Linear Regression\ndp = DeterministicProcess(index=y.index, order=1)\nX_1 = dp.in_sample()\n\nX_2 = X_1.copy()\n\nX_2[\"day_of_week\"] = X_2.index.dayofweek\nX_2[\"day_of_month\"] = X_2.index.day\nX_2[\"day_of_year\"] = X_2.index.dayofyear\nX_2 = X_2.drop('trend', axis='columns')","metadata":{"execution":{"iopub.status.busy":"2022-01-13T16:15:26.215169Z","iopub.execute_input":"2022-01-13T16:15:26.215611Z","iopub.status.idle":"2022-01-13T16:15:26.237214Z","shell.execute_reply.started":"2022-01-13T16:15:26.215578Z","shell.execute_reply":"2022-01-13T16:15:26.236182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we'll split each of those data sets in two for training and validation.","metadata":{}},{"cell_type":"code","source":"# train on (2015, 2016, 2017) data, test on 2018\nX_1_pre_2018 = X_1.loc[X_1.index < '2018-01-01']\nX_1_2018 = X_1.loc[X_1.index > '2017-12-31']\n\nX_2_pre_2018 = X_2.loc[X_2.index < '2018-01-01']\nX_2_2018 = X_2.loc[X_1.index > '2017-12-31']\n\ny_pre_2018 = y.loc[y.index < '2018-01-01']\ny_2018 = y.loc[y.index > '2017-12-31']","metadata":{"execution":{"iopub.status.busy":"2022-01-13T16:15:26.238521Z","iopub.execute_input":"2022-01-13T16:15:26.239061Z","iopub.status.idle":"2022-01-13T16:15:26.253095Z","shell.execute_reply.started":"2022-01-13T16:15:26.23902Z","shell.execute_reply":"2022-01-13T16:15:26.251958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train boosted hybrid\n\nCreate the hybrid model by initializing a `BoostedHybrid` class with `LinearRegression()` and `XGBRegressor()` instances.","metadata":{}},{"cell_type":"code","source":"# Create LinearRegression + XGBRegressor hybrid with BoostedHybrid\nmodel = BoostedHybrid(\n    model_1=LinearRegression(),\n    model_2=XGBRegressor(),\n)\n\n# Fit and predict\nmodel.fit(X_1_pre_2018, X_2_pre_2018, y_pre_2018)\ny_pred = model.predict(X_1_2018, X_2_2018)\n\ny_pred = y_pred.clip(0.0)\n# y_pred.plot()\n# y_2018.plot()\n\naxs = y_2018.plot(subplots=True, sharex=True, figsize=(13, 5), color='gray', alpha=0.5, legend=False,\n                  title='Finland KaggleMart Mug Sales')\ny_pred.plot(subplots=True, color='C0', ax=axs, legend=False);","metadata":{"execution":{"iopub.status.busy":"2022-01-13T16:15:26.254425Z","iopub.execute_input":"2022-01-13T16:15:26.255286Z","iopub.status.idle":"2022-01-13T16:15:27.287764Z","shell.execute_reply.started":"2022-01-13T16:15:26.255247Z","shell.execute_reply":"2022-01-13T16:15:27.286806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Some of the peaks aren't quite as high as expected, but this looks like a really good set of predictions for such a simple model. Let's look at how the same model does for Hat and Sticker sales.","metadata":{}},{"cell_type":"code","source":"subset_df = extract_subset(train_df, 'Finland', 'KaggleMart', 'Kaggle Hat')\n\ny = subset_df[['num_sold']]\n\n# X_1: Features for Linear Regression\ndp = DeterministicProcess(index=y.index, order=1)\nX_1 = dp.in_sample()\n\nX_2 = X_1.copy()\n\nX_2[\"day_of_week\"] = X_2.index.dayofweek\nX_2[\"day_of_month\"] = X_2.index.day\nX_2[\"day_of_year\"] = X_2.index.dayofyear\nX_2 = X_2.drop('trend', axis='columns')\n\n# train on (2015, 2016, 2017) data, test on 2018\nX_1_pre_2018 = X_1.loc[X_1.index < '2018-01-01']\nX_1_2018 = X_1.loc[X_1.index > '2017-12-31']\n\nX_2_pre_2018 = X_2.loc[X_2.index < '2018-01-01']\nX_2_2018 = X_2.loc[X_1.index > '2017-12-31']\nX_2_2018\n\ny_pre_2018 = y.loc[y.index < '2018-01-01']\ny_2018 = y.loc[y.index > '2017-12-31']\n\n# Create LinearRegression + XGBRegressor hybrid with BoostedHybrid\nmodel = BoostedHybrid(\n    model_1=LinearRegression(),\n    model_2=XGBRegressor(),\n)\n\n# Fit and predict\nmodel.fit(X_1_pre_2018, X_2_pre_2018, y_pre_2018)\ny_pred = model.predict(X_1_2018, X_2_2018)\n\ny_pred = y_pred.clip(0.0)\n# y_pred.plot()\n# y_2018.plot()\n\naxs = y_2018.plot(subplots=True, sharex=True, figsize=(13, 5), color='gray', alpha=0.5, legend=False,\n                  title='Finland KaggleMart Hat Sales')\ny_pred.plot(subplots=True, color='C0', ax=axs, legend=False);","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-13T16:15:27.290535Z","iopub.execute_input":"2022-01-13T16:15:27.29164Z","iopub.status.idle":"2022-01-13T16:15:28.218509Z","shell.execute_reply.started":"2022-01-13T16:15:27.291551Z","shell.execute_reply":"2022-01-13T16:15:28.216964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"subset_df = extract_subset(train_df, 'Finland', 'KaggleMart', 'Kaggle Sticker')\n\ny = subset_df[['num_sold']]\n\n# X_1: Features for Linear Regression\ndp = DeterministicProcess(index=y.index, order=1)\nX_1 = dp.in_sample()\n\nX_2 = X_1.copy()\n\nX_2[\"day_of_week\"] = X_2.index.dayofweek\nX_2[\"day_of_month\"] = X_2.index.day\nX_2[\"day_of_year\"] = X_2.index.dayofyear\nX_2 = X_2.drop('trend', axis='columns')\n\n# train on (2015, 2016, 2017) data, test on 2018\nX_1_pre_2018 = X_1.loc[X_1.index < '2018-01-01']\nX_1_2018 = X_1.loc[X_1.index > '2017-12-31']\n\nX_2_pre_2018 = X_2.loc[X_2.index < '2018-01-01']\nX_2_2018 = X_2.loc[X_1.index > '2017-12-31']\nX_2_2018\n\ny_pre_2018 = y.loc[y.index < '2018-01-01']\ny_2018 = y.loc[y.index > '2017-12-31']\n\n# Create LinearRegression + XGBRegressor hybrid with BoostedHybrid\nmodel = BoostedHybrid(\n    model_1=LinearRegression(),\n    model_2=XGBRegressor(),\n)\n\n# Fit and predict\nmodel.fit(X_1_pre_2018, X_2_pre_2018, y_pre_2018)\ny_pred = model.predict(X_1_2018, X_2_2018)\n\ny_pred = y_pred.clip(0.0)\n# y_pred.plot()\n# y_2018.plot()\n\naxs = y_2018.plot(subplots=True, sharex=True, figsize=(13, 5), color='gray', alpha=0.5, legend=False,\n                  title='Finland KaggleMart Sticker Sales')\ny_pred.plot(subplots=True, color='C0', ax=axs, legend=False);","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-13T16:15:28.220105Z","iopub.execute_input":"2022-01-13T16:15:28.220355Z","iopub.status.idle":"2022-01-13T16:15:29.133443Z","shell.execute_reply.started":"2022-01-13T16:15:28.220327Z","shell.execute_reply":"2022-01-13T16:15:29.132434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There's definitely room for improvement, but this looks much better than my previous notebooks. Let's put this all together and see how it does on the entire data set.","metadata":{}},{"cell_type":"code","source":"# https://www.kaggle.com/c/web-traffic-time-series-forecasting/discussion/36414\n\ndef SMAPE(y_true, y_pred):\n    denominator = (y_true + np.abs(y_pred)) / 200.0\n    diff = np.abs(y_true - y_pred) / denominator\n    diff[denominator == 0] = 0.0\n    return np.round(np.mean(diff),5)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T16:15:29.134742Z","iopub.execute_input":"2022-01-13T16:15:29.135302Z","iopub.status.idle":"2022-01-13T16:15:29.143367Z","shell.execute_reply.started":"2022-01-13T16:15:29.135235Z","shell.execute_reply":"2022-01-13T16:15:29.14182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train on (2015, 2016, 2017) data, test on 2018\ndf_pre_2018 = train_df.loc[train_df.index < '2018-01-01']\ndf_2018 = train_df.loc[train_df.index > '2017-12-31']","metadata":{"execution":{"iopub.status.busy":"2022-01-13T16:15:29.145149Z","iopub.execute_input":"2022-01-13T16:15:29.145405Z","iopub.status.idle":"2022-01-13T16:15:29.163921Z","shell.execute_reply.started":"2022-01-13T16:15:29.145376Z","shell.execute_reply":"2022-01-13T16:15:29.162341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_2018","metadata":{"execution":{"iopub.status.busy":"2022-01-13T16:15:29.165591Z","iopub.execute_input":"2022-01-13T16:15:29.166107Z","iopub.status.idle":"2022-01-13T16:15:29.187742Z","shell.execute_reply.started":"2022-01-13T16:15:29.166074Z","shell.execute_reply":"2022-01-13T16:15:29.186922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Build the models and make predictions","metadata":{}},{"cell_type":"code","source":"countries = ['Finland', 'Norway', 'Sweden']\nproducts = ['Kaggle Hat', 'Kaggle Mug', 'Kaggle Sticker']\nstores = ['KaggleMart', 'KaggleRama']\n\ndef train_and_predict(train_df, test_df):\n    pred_dfs = list()\n\n    for country in countries:\n        for product in products:\n            for store in stores:\n                subset_df = extract_subset(train_df, country, store, product)\n                test_sample = extract_subset(test_df, country, store, product)\n                \n                y = subset_df[['num_sold']]\n\n                # X_1: Features for Linear Regression\n                dp = DeterministicProcess(index=y.index, order=1)\n                X_1 = dp.in_sample()\n\n                X_2 = X_1.copy()\n                X_2[\"day_of_week\"] = X_2.index.dayofweek\n                X_2[\"day_of_month\"] = X_2.index.day\n                X_2[\"day_of_year\"] = X_2.index.dayofyear\n                X_2 = X_2.drop('trend', axis='columns')\n                \n\n                # Create features for a forecast.\n                X_1_fore = dp.out_of_sample(steps=len(test_sample))\n                \n                X_2_fore = X_1_fore.copy()\n                X_2_fore[\"day_of_week\"] = X_2_fore.index.dayofweek\n                X_2_fore[\"day_of_month\"] = X_2_fore.index.day\n                X_2_fore[\"day_of_year\"] = X_2_fore.index.dayofyear\n                X_2_fore = X_2_fore.drop('trend', axis='columns')\n\n                # Create LinearRegression + XGBRegressor hybrid with BoostedHybrid\n                model = BoostedHybrid(\n                    model_1=LinearRegression(),\n                    model_2=XGBRegressor(),\n                )\n\n                # Fit and predict\n                model.fit(X_1, X_2, y)\n                y_pred = model.predict(X_1_fore, X_2_fore)\n                y_fore = y_pred.clip(0.0)\n                \n                pred_df = pd.DataFrame({'row_id': test_sample['row_id'],\n                                        'y_pred': y_fore['num_sold']}).reset_index(drop=True)\n                pred_dfs.append(pred_df)\n\n    predictions_df = pd.concat(pred_dfs)\n    return predictions_df\n\npreds_2018_df = train_and_predict(df_pre_2018, df_2018)\npreds_2018_df","metadata":{"execution":{"iopub.status.busy":"2022-01-13T16:15:29.189615Z","iopub.execute_input":"2022-01-13T16:15:29.189891Z","iopub.status.idle":"2022-01-13T16:15:36.007817Z","shell.execute_reply.started":"2022-01-13T16:15:29.189854Z","shell.execute_reply":"2022-01-13T16:15:36.006813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_pred_df = df_2018.merge(preds_2018_df, how='left', on='row_id')\nval_pred_df","metadata":{"execution":{"iopub.status.busy":"2022-01-13T16:15:36.009267Z","iopub.execute_input":"2022-01-13T16:15:36.009482Z","iopub.status.idle":"2022-01-13T16:15:36.033438Z","shell.execute_reply.started":"2022-01-13T16:15:36.009457Z","shell.execute_reply":"2022-01-13T16:15:36.032275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"actual = np.array(val_pred_df['num_sold'])\nforecast = np.array(val_pred_df['y_pred'])\nSMAPE(actual, forecast)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T16:15:36.036655Z","iopub.execute_input":"2022-01-13T16:15:36.036985Z","iopub.status.idle":"2022-01-13T16:15:36.044767Z","shell.execute_reply.started":"2022-01-13T16:15:36.036949Z","shell.execute_reply":"2022-01-13T16:15:36.044151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"That's a big improvement over previous notebooks, but the estimated SMAPE score isn't always close to what I've seen on the leaderboard when making forecasts with new data. Let's load the test data and make a submission.","metadata":{}},{"cell_type":"markdown","source":"### Load the test data set","metadata":{}},{"cell_type":"code","source":"# Keep the row_id for the sample submission\ntest_df = pd.read_csv(TEST_CSV, parse_dates=['date'])\ntest_df = test_df.set_index('date').to_period('D')\ntest_df","metadata":{"execution":{"iopub.status.busy":"2022-01-13T16:15:36.045958Z","iopub.execute_input":"2022-01-13T16:15:36.04635Z","iopub.status.idle":"2022-01-13T16:15:36.08767Z","shell.execute_reply.started":"2022-01-13T16:15:36.046319Z","shell.execute_reply":"2022-01-13T16:15:36.087068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Re-train models and make predictions using all training data","metadata":{}},{"cell_type":"code","source":"preds_2019_df = train_and_predict(train_df, test_df)\npreds_2019_df","metadata":{"execution":{"iopub.status.busy":"2022-01-13T16:15:36.089166Z","iopub.execute_input":"2022-01-13T16:15:36.089665Z","iopub.status.idle":"2022-01-13T16:15:44.276577Z","shell.execute_reply.started":"2022-01-13T16:15:36.08962Z","shell.execute_reply":"2022-01-13T16:15:44.275611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_pred_df = test_df.merge(preds_2019_df, how='left', on='row_id')\ntest_pred_df","metadata":{"execution":{"iopub.status.busy":"2022-01-13T16:15:44.27794Z","iopub.execute_input":"2022-01-13T16:15:44.278238Z","iopub.status.idle":"2022-01-13T16:15:44.301198Z","shell.execute_reply.started":"2022-01-13T16:15:44.278197Z","shell.execute_reply":"2022-01-13T16:15:44.300585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's combine the predictions with the original data to see how the forecasts look.","metadata":{}},{"cell_type":"code","source":"for country in countries:\n    for product in products:\n        for store in stores:\n            df = extract_subset(train_df, country, store, product)\n            pred_df = extract_subset(test_pred_df, country, store, product)\n            pred_df = pred_df.rename(columns = {'y_pred': 'num_sold'})\n            pred_df.index = pd.date_range(start='1/1/2019', end='12/31/2019')\n            \n            title = '{} - {} - {}'.format(country, product, store)\n            sales_and_pred = pd.concat([df[['num_sold']], pred_df[['num_sold']]])\n            \n            sales_and_pred.plot(title=title, legend=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T16:15:44.302104Z","iopub.execute_input":"2022-01-13T16:15:44.302899Z","iopub.status.idle":"2022-01-13T16:15:49.825625Z","shell.execute_reply.started":"2022-01-13T16:15:44.302853Z","shell.execute_reply":"2022-01-13T16:15:49.824435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"These plots all look like reasonable forecasts. The models we used are extremely simple two-model hybrids without any hyperparameter tuning. There's still a lot of room for improvement by selecting different models for different products, tuning those models, and adding additional models to the `BoostedHybrid` class.","metadata":{}},{"cell_type":"markdown","source":"### Create the submission file","metadata":{}},{"cell_type":"code","source":"submission_df = test_pred_df[['row_id', 'y_pred']]\nsubmission_df.columns = ['row_id', 'num_sold']\nsubmission_df.to_csv('./submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T16:15:49.826815Z","iopub.execute_input":"2022-01-13T16:15:49.827107Z","iopub.status.idle":"2022-01-13T16:15:49.865116Z","shell.execute_reply.started":"2022-01-13T16:15:49.827078Z","shell.execute_reply":"2022-01-13T16:15:49.863352Z"},"trusted":true},"execution_count":null,"outputs":[]}]}