{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\nimport optuna\nfrom sklearn.metrics import mean_squared_error\nimport seaborn as sns\nimport warnings\nfrom sklearn.linear_model import Ridge\nwarnings.filterwarnings(\"ignore\")\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-06T15:46:49.12631Z","iopub.execute_input":"2022-02-06T15:46:49.12662Z","iopub.status.idle":"2022-02-06T15:46:49.144913Z","shell.execute_reply.started":"2022-02-06T15:46:49.126589Z","shell.execute_reply":"2022-02-06T15:46:49.144156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is my first entry into a TPS competition. From my experience with time series data ,and from what I gathered by going through the discussion forum and public noteboks, I could make some key points:\n1. GDP data of the countries should help capture the year-on-year increasing trend in the data\n2. Decision trees (XGBoost, LGBM etc) alone are not likely to give sound predictions. Atleast not without some ingenious feature engineering. This is because tree based models do not capture macro trends and are consequently poor at extrapolation. In our case,for instance, a tree model will branch out based on some condition GDP>X and for will treat all data instances satisfying that condition similarly. Hence, when I solely used a LGB Regressor, my test predictions and validation predictions(2018 data) were same.\n3. Linear models are likely to perform better at capturing this macro trend of yearly increment in sales.\n4. A custom objective function that penalizes underpredictions more than overpredictions should help given that the evaluation metric(SMAPE) behaves similarly. Refer [this discussion](http://https://www.kaggle.com/c/tabular-playground-series-jan-2022/discussion/300611) to understand more.","metadata":{}},{"cell_type":"code","source":"train=pd.read_csv('../input/tabular-playground-series-jan-2022/train.csv') \ntest=pd.read_csv('../input/tabular-playground-series-jan-2022/test.csv')\nhol=pd.read_csv('../input/holidays/holidays.csv') #List of holidays in Nordic countries\ngdp=pd.read_csv('../input/gdp-nordic1/GDP_data_2015_to_2019_Finland_Norway_Sweden.csv',index_col='year')","metadata":{"execution":{"iopub.status.busy":"2022-02-06T15:46:51.345168Z","iopub.execute_input":"2022-02-06T15:46:51.345682Z","iopub.status.idle":"2022-02-06T15:46:51.404056Z","shell.execute_reply.started":"2022-02-06T15:46:51.345633Z","shell.execute_reply":"2022-02-06T15:46:51.403328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Preprocessing the dataframes\ntrain['product']=train['product'].str.split(' ').str[0]+'_'+train['product'].str.split(' ').str[1]\ntest['product']=test['product'].str.split(' ').str[0]+'_'+test['product'].str.split(' ').str[1]\nhol.rename(columns={'Date':'date','Country':'country'},inplace=True)\nhol['date']=pd.to_datetime(hol['date'])\ntrain['date']=pd.to_datetime(train['date'])\ntest['date']=pd.to_datetime(test['date'])\ntrain=pd.merge(train,hol[['country','Name','date']],on=('country','date'),how='left')\ntest=pd.merge(test,hol[['country','Name','date']],on=('country','date'),how='left')\ngdp.rename(columns={'GDP_Finland':'Finland','GDP_Norway':'Norway','GDP_Sweden':'Sweden'},inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T15:46:53.216894Z","iopub.execute_input":"2022-02-06T15:46:53.217182Z","iopub.status.idle":"2022-02-06T15:46:53.334992Z","shell.execute_reply.started":"2022-02-06T15:46:53.21715Z","shell.execute_reply":"2022-02-06T15:46:53.334328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Extracting necessary information from date field\ndef date_extraction(train,gdp):\n    train['weekday']=train['date'].dt.weekday\n    train['month']=train['date'].dt.month\n    train['year']=train['date'].dt.year\n    train['quarter']=train['date'].dt.quarter\n    train['day_of_year']=train['date'].dt.dayofyear\n    train['week_of_year']=train['date'].dt.isocalendar().week.astype(int)\n    #train['is_weekend']=np.where(train['weekday']>3,1,0)\n    train['day']=train['date'].dt.day\n    train['week_of_mon']=np.round(train['day']/7)+1\n    a=gdp.unstack().to_dict()\n    train['gdp']=train.set_index(['country','year']).index.map(a.get)\n    #train=pd.merge(train,gdp[['year','country','gdp','quarter']],on=('year','quarter','country'),how='left')\n    return train\ntrain=date_extraction(train,gdp)\ntest=date_extraction(test,gdp)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T15:46:55.020549Z","iopub.execute_input":"2022-02-06T15:46:55.020832Z","iopub.status.idle":"2022-02-06T15:46:55.08236Z","shell.execute_reply.started":"2022-02-06T15:46:55.0208Z","shell.execute_reply":"2022-02-06T15:46:55.081344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f,ax=plt.subplots(1,2,figsize=(25,8))\nax[0].hist(train['num_sold'])\nax[0].set_title('num_sold')\nax[1].hist(np.log(train['num_sold']))\nax[1].set_title('log(num_sold)')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-06T15:46:56.520805Z","iopub.execute_input":"2022-02-06T15:46:56.521717Z","iopub.status.idle":"2022-02-06T15:46:56.829245Z","shell.execute_reply.started":"2022-02-06T15:46:56.52165Z","shell.execute_reply":"2022-02-06T15:46:56.82825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The log transformation of num_sold reasonably approximates a standard normal distribution. Linear models assume normality, so it might be better to predict the log transformation of the variable. Besides, doing so also conforms with the SMAPE loss function.\nCheck this [discussion](http://https://www.kaggle.com/c/tabular-playground-series-jan-2022/discussion/300611) to know more about that.","metadata":{}},{"cell_type":"markdown","source":"# BASIC EDA","metadata":{}},{"cell_type":"code","source":"def country_week(train,prod):\n    f,ax=plt.subplots(2,3,figsize=(20,10))\n    for i,s in enumerate(train['store'].unique()):\n        for j,con in enumerate(train['country'].unique()):\n            for y in [2015,2016,2017,2018]:\n                l=[]\n                db=train.groupby(['country','store','product']).get_group((con,s,prod))\n                x=np.arange(1,6)\n                for w in range(1,6):\n                    l.append(db.groupby(['week_of_mon','year']).get_group((w,y))['num_sold'].mean())\n                ax[i][j].plot(x,l,label=y)\n                ax[i][j].legend(loc='upper right')\n            ax[i][j].set_title(con+' '+s)\n            f.suptitle(prod+' '+'weekly average',size=30)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T15:46:59.0968Z","iopub.execute_input":"2022-02-06T15:46:59.097048Z","iopub.status.idle":"2022-02-06T15:46:59.105398Z","shell.execute_reply.started":"2022-02-06T15:46:59.097019Z","shell.execute_reply":"2022-02-06T15:46:59.104558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Before moving to modeling, I performed some elementary EDA to extract some micro trends. Let's start with averaging the sales value for weeks(1-5) of every month. So, if the average for week 1 in 2015 is X, then in any given month, on average, the product was sold X times in week 1.","metadata":{}},{"cell_type":"code","source":"country_week(train,'Kaggle_Hat')\ncountry_week(train,'Kaggle_Mug')\ncountry_week(train,'Kaggle_Sticker')","metadata":{"execution":{"iopub.status.busy":"2022-02-06T15:47:01.248285Z","iopub.execute_input":"2022-02-06T15:47:01.24854Z","iopub.status.idle":"2022-02-06T15:47:05.279793Z","shell.execute_reply.started":"2022-02-06T15:47:01.248498Z","shell.execute_reply":"2022-02-06T15:47:05.279006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here, I have defined week 1 as the first 7 days of the month and so on. Week 5 thus consists of only 2-3 days.\n\nObservations:\n\n1. Product sale trend is country dependent. The variation between products among different stores seem minor and erratic. \n2. On average,sales increase significantly towards the end of the month (week 5,last 2-3 days)\n3. I'll create interaction features between week and country\n4. Sales reduced in 2016 in Norway. Can be attributed to drop in GDP","metadata":{}},{"cell_type":"code","source":"def country_mon(train,prod):\n    f,ax=plt.subplots(2,3,figsize=(20,10))\n    for i,s in enumerate(train['store'].unique()):\n        for j,con in enumerate(train['country'].unique()):\n            for y in [2015,2016,2017,2018]:\n                l=[]\n                db=train.groupby(['country','store','product']).get_group((con,s,prod))\n                x=np.arange(1,13)\n                for m in range(1,13):\n                    l.append(db.groupby(['month','year']).get_group((m,y))['num_sold'].mean())\n                ax[i][j].plot(x,l,label=y)\n                ax[i][j].legend(loc='upper right')\n            ax[i][j].set_title(con+' '+s)\n        f.suptitle(prod+' '+'monthly average',size=30)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T15:47:05.28119Z","iopub.execute_input":"2022-02-06T15:47:05.281458Z","iopub.status.idle":"2022-02-06T15:47:05.290709Z","shell.execute_reply.started":"2022-02-06T15:47:05.281422Z","shell.execute_reply":"2022-02-06T15:47:05.289723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"country_mon(train,'Kaggle_Hat')\ncountry_mon(train,'Kaggle_Mug')\ncountry_mon(train,'Kaggle_Sticker')","metadata":{"execution":{"iopub.status.busy":"2022-02-06T15:47:05.624112Z","iopub.execute_input":"2022-02-06T15:47:05.624365Z","iopub.status.idle":"2022-02-06T15:47:09.842825Z","shell.execute_reply.started":"2022-02-06T15:47:05.624338Z","shell.execute_reply":"2022-02-06T15:47:09.842333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observations:\n1. Sales increase during the last 2-3 months for all the products, but otherwise the trends differ.\n2. Mug:\n    2.1 Sales drop consistently till month 7 before beginning to rise again\n    2.2 There's an increase, however, in month 4 which might be associated with Easter\n3. Hat:\n    3.1 Sales increase consistently from month 2-4 following which there's a drop till month 10\n4. Stikcer:\n    4.1 Sales increase generally from month 2-5, but there's variation between countries\n    4.2 In Norway, the sales peak in month 5. However, Sweden and Finland witness a second peak in month 6. Again this too varies\n        across the 4 years\n5. An interaction term between month product and country should capture these trends. For now, I'll avoid adding store to this interaction term, as the variation seems minor","metadata":{}},{"cell_type":"code","source":"def country_mons(train,prod):\n    f,ax=plt.subplots(3,2,figsize=(20,10))\n    for i,con in enumerate(train['country'].unique()):\n        for j,m in enumerate([5,6]):\n            for y in [2015,2016,2017,2018]:\n                l=[]\n                db=train.groupby(['month','product','country']).get_group((m,prod,con))\n                x=np.arange(1,db['day'].max()+1)\n                for d in range(1,db['day'].max()+1):\n                    l.append(db.groupby(['day','year']).get_group((d,y))['num_sold'].mean())\n                ax[i][j].plot(x,l,label=y)\n                ax[i][j].legend(loc='upper left')\n            ax[i][j].set_title(con+' '+'month'+' '+str(m))\n    f.suptitle(prod+' '+'daily average for April and December',size=20)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T15:47:09.844032Z","iopub.execute_input":"2022-02-06T15:47:09.844299Z","iopub.status.idle":"2022-02-06T15:47:09.850941Z","shell.execute_reply.started":"2022-02-06T15:47:09.844275Z","shell.execute_reply":"2022-02-06T15:47:09.850522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"country_mons(train,'Kaggle_Hat')\ncountry_mons(train,'Kaggle_Mug')\ncountry_mons(train,'Kaggle_Sticker')","metadata":{"execution":{"iopub.status.busy":"2022-02-06T15:47:09.851824Z","iopub.execute_input":"2022-02-06T15:47:09.852071Z","iopub.status.idle":"2022-02-06T15:47:15.421444Z","shell.execute_reply.started":"2022-02-06T15:47:09.852048Z","shell.execute_reply":"2022-02-06T15:47:15.420559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observations:\n1. Contrary to expectations the peak of sales does not coincide with the holidays (Easter in April, and Chirstmas,NYE in December)\n2. Sales in the last week of December surge after Christmas and then begin to decline around New year's eve. 30th December is generally the peak\n3. In April, the sales peak in the week post Easter(7-8 days from Easter). \n\nI'll add features to address these trends","metadata":{}},{"cell_type":"code","source":"def country_weekday(train,prod):\n    f,ax=plt.subplots(2,3,figsize=(20,10))\n    for i,s in enumerate(train['store'].unique()):\n        for j,con in enumerate(train['country'].unique()):\n            for y in [2015,2016,2017,2018]:\n                l=[]\n                db=train.groupby(['store','product','country']).get_group((s,prod,con))\n                x=np.arange(1,8)\n                for m in range(0,7):\n                    l.append(db.groupby(['weekday','year']).get_group((m,y))['num_sold'].mean())\n                ax[i][j].plot(x,l,label=y)\n                ax[i][j].legend(loc='lower right')\n            ax[i][j].set_title(con+' '+s)\n    f.suptitle(prod+' '+'weekday average',size=20)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T15:47:15.423221Z","iopub.execute_input":"2022-02-06T15:47:15.423924Z","iopub.status.idle":"2022-02-06T15:47:15.433384Z","shell.execute_reply.started":"2022-02-06T15:47:15.423855Z","shell.execute_reply":"2022-02-06T15:47:15.432546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"country_weekday(train,'Kaggle_Hat')\ncountry_weekday(train,'Kaggle_Mug')\ncountry_weekday(train,'Kaggle_Sticker')","metadata":{"execution":{"iopub.status.busy":"2022-02-06T15:47:15.434447Z","iopub.execute_input":"2022-02-06T15:47:15.435243Z","iopub.status.idle":"2022-02-06T15:47:19.820264Z","shell.execute_reply.started":"2022-02-06T15:47:15.435206Z","shell.execute_reply":"2022-02-06T15:47:19.819559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1. Sales increase significantly from Thusday to Saturday and then remain constant till Sunday. Ergo, I'll keep weekday as a tertiary variable: 0 (Mon-Thu) 1(Fri) 2(Sat-Sun). Perhaps, Sunday can be considered separately as well.\n2. The trend between Saturday and Sunday is non-uniform\n3. An interaction feature between weekday,country,product and store might help","metadata":{}},{"cell_type":"code","source":"def country_day_of_mon(train,prod):\n    f,ax=plt.subplots(2,3,figsize=(20,10))\n    for i,s in enumerate(train['store'].unique()):\n        for j,con in enumerate(train['country'].unique()):\n            for y in [2015,2016,2017,2018]:\n                l=[]\n                db=train.groupby(['store','product','country']).get_group((s,prod,con))\n                x=np.arange(1,32)\n                for m in range(1,32):\n                    l.append(db.groupby(['day','year']).get_group((m,y))['num_sold'].mean())\n                ax[i][j].plot(x,l,label=y)\n                ax[i][j].legend(loc='upper right')\n            ax[i][j].set_title(con+' '+s)\n        f.suptitle(prod+' '+'day wise average',size=20)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T15:47:22.730155Z","iopub.execute_input":"2022-02-06T15:47:22.73101Z","iopub.status.idle":"2022-02-06T15:47:22.739556Z","shell.execute_reply.started":"2022-02-06T15:47:22.730976Z","shell.execute_reply":"2022-02-06T15:47:22.738886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"country_day_of_mon(train,'Kaggle_Hat')\ncountry_day_of_mon(train,'Kaggle_Mug')\ncountry_day_of_mon(train,'Kaggle_Sticker')","metadata":{"execution":{"iopub.status.busy":"2022-02-06T15:47:24.705444Z","iopub.execute_input":"2022-02-06T15:47:24.706199Z","iopub.status.idle":"2022-02-06T15:47:31.089205Z","shell.execute_reply.started":"2022-02-06T15:47:24.706156Z","shell.execute_reply":"2022-02-06T15:47:31.088131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Nothing new here. Sales increase sharply towards the last 2-3 days of any month. I belive the 'week_no' feature discussed above should capture this trend. Otherwise, I'll include a day of the month (1-31) feature and see the performace.","metadata":{}},{"cell_type":"markdown","source":"**HOLIDAYS**\n\nI will now try to gauge the impact of holidays on sales. How are sales affected on a holiday, the day before a holiday, and the day after a holiday.","metadata":{}},{"cell_type":"code","source":"hol['is_special']=0\nfor con in ['Sweden','Norway','Finland']:\n    db=train.groupby('country').get_group(con)\n    db=db.fillna('NO')\n    a={}\n    b=[]\n    for n in db['Name'].unique():\n        if n!='NO':\n            a[n]=db[db['Name']==n]['num_sold'].mean()\n            b.append(db[db['Name']==n]['date'].unique())\n    m=db[~db['date'].isin(b)]['num_sold'].mean()\n    for n in a.keys():\n        if a[n]>m:\n            hol['is_special']=np.where((hol['Name']==n)&(hol['country']==con),1,hol['is_special'])","metadata":{"execution":{"iopub.status.busy":"2022-02-06T15:47:31.090624Z","iopub.execute_input":"2022-02-06T15:47:31.090821Z","iopub.status.idle":"2022-02-06T15:47:32.098867Z","shell.execute_reply.started":"2022-02-06T15:47:31.090796Z","shell.execute_reply":"2022-02-06T15:47:32.098049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Assuming that certian holidays have more impact on sales, I have created a is_special feature to identify holidays with average sales over the four years more than the four years average sales of the non-holiday days.","metadata":{}},{"cell_type":"code","source":"def holiday_feat(train,hol):\n    c=-1\n    l=['is_prev_holiday','is_holiday','is_foll_holiday']\n    for i,j in enumerate(l):\n        hol[j]=1\n        hol['temp']=hol['date']\n        train['temp']=train['date']+pd.Timedelta(c,unit='D')\n        train=pd.merge(train,hol[['temp','country',j]],how='left',on=('country','temp'))\n        c+=1\n        train[j]=np.where(train['temp']==pd.to_datetime('2014-12-31'),1,train[j])\n        train[j]=train[j].fillna(0)\n    train.drop(columns=['temp'],inplace=True)\n    hol.drop(columns=['temp'],inplace=True)\n    hol.drop(columns=l,inplace=True)\n    train=pd.merge(train,hol[['country','is_special','Name','date']],on=('country','Name','date'),how='left')\n    train['is_special']=train['is_special'].fillna(0)\n    train['Name']=train['Name'].fillna('NONE')\n    return train\ntrain=holiday_feat(train,hol)\ntest=holiday_feat(test,hol)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T15:47:32.100051Z","iopub.execute_input":"2022-02-06T15:47:32.100292Z","iopub.status.idle":"2022-02-06T15:47:32.203527Z","shell.execute_reply.started":"2022-02-06T15:47:32.10026Z","shell.execute_reply":"2022-02-06T15:47:32.202767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Three more features to identify whether a day is a holiday and to mark the days preceding and following a holiday. ","metadata":{}},{"cell_type":"code","source":"def hol_dist(df,con,y):\n    f,ax=plt.subplots(2,3,figsize=(20,10))\n    for i,s in enumerate(train['store'].unique()):\n        for j,p in enumerate(train['product'].unique()):\n            d1=df.groupby(['country','store','product','year']).get_group((con,s,p,y))\n            e=d1[d1['is_holiday']==1]['num_sold']\n            b=d1[d1['is_prev_holiday']==1]['num_sold']\n            c=d1[d1['is_foll_holiday']==1]['num_sold']\n            d=d1[(d1['is_prev_holiday']==0)&(d1['is_foll_holiday']==0)&(d1['is_holiday']==0)]['num_sold']\n            sns.distplot(ax=ax[i][j],a=e, hist=False, kde=True,color = 'darkblue',label='is_holiday')\n            sns.distplot(ax=ax[i][j],a=b, hist=False, kde=True,color = 'orange',label='is_prev_holiday')\n            sns.distplot(ax=ax[i][j],a=c, hist=False, kde=True,color = 'red',label='is_foll_holiday')\n            sns.distplot(ax=ax[i][j],a=d, hist=False, kde=True,color = 'green',label='none')\n            ax[i][j].legend(loc='upper right')\n            ax[i][j].set_title(s+' '+p)\n        f.suptitle(con+' '+'holiday num_sold distributions for year'+' '+str(y),size=20)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T15:47:32.732833Z","iopub.execute_input":"2022-02-06T15:47:32.733728Z","iopub.status.idle":"2022-02-06T15:47:32.748185Z","shell.execute_reply.started":"2022-02-06T15:47:32.733682Z","shell.execute_reply":"2022-02-06T15:47:32.747305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hol_dist(train,'Finland',2015)\nhol_dist(train,'Finland',2016)\nhol_dist(train,'Finland',2017)\nhol_dist(train,'Finland',2018)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T15:47:34.190879Z","iopub.execute_input":"2022-02-06T15:47:34.191744Z","iopub.status.idle":"2022-02-06T15:47:39.959465Z","shell.execute_reply.started":"2022-02-06T15:47:34.191665Z","shell.execute_reply":"2022-02-06T15:47:39.958755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observation:\n\nFor some reason, the average sales on days following a holiday are generally higher than in other cases. I'll include two of these variables (is_prev_holiday and is_holiday in the model). Both are binary features.","metadata":{}},{"cell_type":"code","source":"def final_engineering(d,hol):\n    df=d.copy()\n    #df['hat_spring']=0\n    df['is_holiday']=np.where((df['day']==24)&(df['month']==12),1,df['is_holiday'])\n    df['is_prev_holiday']=np.where((df['day']==25)&(df['month']==12),1,df['is_prev_holiday'])\n    df['is_holiday']=np.where((df['day']==31)&(df['month']==12),1,df['is_holiday'])\n    df['is_prev_holiday']=np.where((df['day']==1)&(df['month']==1),1,df['is_prev_holiday'])\n    df['is_special']=np.where((df['day']==24)&(df['month']==12),1,df['is_special'])\n    df['is_holiday']=np.where((df['day']==31)&(df['month']==12),1,df['is_special'])\n    #df['hat_spring']=np.where((df['month']==3)&(df['product']=='Kaggle_Hat'),1,df['hat_spring'])\n    #df['sticker_may']=np.where((df['month']==5)&(df['product']=='Kaggle_Sticker'),1,0)\n    df['week_day']=0\n    df['week_day']=np.where(df['weekday']==4,1,df['week_day'])\n    df['week_day']=np.where(df['weekday']==5,2,df['week_day'])\n    df['week_day']=np.where(df['weekday']==6,2,df['week_day'])\n    for i in range(26,31):\n        df['is_last_week']=np.where((df['day']==i)&(df['month']==12),1,0)\n        df['is_special']=np.where((df['day']==i)&(df['month']==12),4,df['is_special'])\n    df.drop(columns=['weekday'],inplace=True)\n    df['easter_eff']=0\n    for col in ['month','week_day','week_of_mon','day','day_of_year']:\n        df['cos'+'_'+col]=np.cos(2*np.pi*df[col]/max(df[col]))\n        df['sin'+'_'+col]=np.sin(2*np.pi*df[col]/max(df[col]))\n        df.drop(columns=[col],inplace=True)\n    spec=hol[hol['Name'].str.contains('Easter')]\n    l=[]\n    for d in spec['date']:\n        for i in range(2,8):\n            e=d+pd.Timedelta(i,unit='D')\n            df['easter_eff']=np.where(df['date']==e,1,df['easter_eff'])\n    for d in ['Easter','New','Christmas']:\n        df['is_special']=np.where(df['Name'].str.contains(d),2,df['is_special'])\n    df['is_special']=df['is_special']+df['is_holiday']\n    import dateutil.easter as easter\n\n    easter_timestamp = df.date.apply(lambda date: pd.Timestamp(easter.easter(date.year)))\n    df['days_from_easter'] = (df.date - easter_timestamp).dt.days.clip(-3, 59)\n    df.loc[df['days_from_easter'].isin(range(12, 39)), 'days_from_easter'] = 12 # reduce overfitting\n    \n    # Last Wednesday of June\n    wed_june_timestamp = df.date.dt.year.map({2015: pd.Timestamp(('2015-06-24')),\n                                         2016: pd.Timestamp(('2016-06-29')),\n                                         2017: pd.Timestamp(('2017-06-28')),\n                                         2018: pd.Timestamp(('2018-06-27')),\n                                         2019: pd.Timestamp(('2019-06-26'))})\n    df['days_from_wed_jun'] = (df.date - wed_june_timestamp).dt.days.clip(-5, 5)\n    \n    # First Sunday of November (second Sunday is Father's Day)\n    sun_nov_timestamp = df.date.dt.year.map({2015: pd.Timestamp(('2015-11-1')),\n                                         2016: pd.Timestamp(('2016-11-6')),\n                                         2017: pd.Timestamp(('2017-11-5')),\n                                         2018: pd.Timestamp(('2018-11-4')),\n                                         2019: pd.Timestamp(('2019-11-3'))})\n    df['days_from_sun_nov'] = (df.date - sun_nov_timestamp).dt.days.clip(-1, 9)\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2022-02-06T15:47:39.961455Z","iopub.execute_input":"2022-02-06T15:47:39.96216Z","iopub.status.idle":"2022-02-06T15:47:39.992418Z","shell.execute_reply.started":"2022-02-06T15:47:39.962112Z","shell.execute_reply":"2022-02-06T15:47:39.991563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Final feature engineeing includes:\n1. Converting ordinal date features to cyclic date features\n2. Including a last_week feature to capture the surge in sales during the last week of the year\n3. An easter effect feature for the sales peak in the weak following Easter\n4. Combining is_holiday and is_special into a single feature","metadata":{}},{"cell_type":"code","source":"train=final_engineering(train,hol)\ntest=final_engineering(test,hol)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T15:47:39.993526Z","iopub.execute_input":"2022-02-06T15:47:39.993769Z","iopub.status.idle":"2022-02-06T15:47:40.537001Z","shell.execute_reply.started":"2022-02-06T15:47:39.993735Z","shell.execute_reply":"2022-02-06T15:47:40.536256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def interaction(df):\n    df['gdp']=np.log(df['gdp'])\n    #for c in ['Sweden','Norway']:\n     #   for p in ['Kaggle_Mug','Kaggle_Sticker']:\n      #      for func in ['cos','sin']:\n       #         df[f'{c}_{p}_week_{func}']=(df.country==con)*(df['product']==p)*(df[f'{func}_week_of_mon'])\n    for c in ['Sweden','Norway']:\n        for p in ['Kaggle_Mug','Kaggle_Sticker']:\n            for func in ['cos','sin']:\n                df[f'{c}_{p}_month_{func}']=(df.country==con)*(df['product']==p)*(df[f'{func}_month'])\n    df['cos_week*mon']=df['cos_week_of_mon']*df['cos_month']\n    df['sin_week*mon']=df['sin_week_of_mon']*df['sin_month']\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-02-06T15:47:41.499483Z","iopub.execute_input":"2022-02-06T15:47:41.500163Z","iopub.status.idle":"2022-02-06T15:47:41.505349Z","shell.execute_reply.started":"2022-02-06T15:47:41.50013Z","shell.execute_reply":"2022-02-06T15:47:41.50469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train=interaction(train)\ntest=interaction(test)\nreg_train=train.copy()","metadata":{"execution":{"iopub.status.busy":"2022-02-06T15:47:43.279616Z","iopub.execute_input":"2022-02-06T15:47:43.279918Z","iopub.status.idle":"2022-02-06T15:47:43.337919Z","shell.execute_reply.started":"2022-02-06T15:47:43.279883Z","shell.execute_reply":"2022-02-06T15:47:43.337097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_dummy(df):\n    for c in ['country','store','product']:\n        a=pd.get_dummies(df[c],drop_first=True)\n        df=pd.concat([df,a],axis=1)\n        df.drop(columns=[c],inplace=True)\n    return df\ntrain=get_dummy(train)\ntest=get_dummy(test)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T15:47:45.158199Z","iopub.execute_input":"2022-02-06T15:47:45.158482Z","iopub.status.idle":"2022-02-06T15:47:45.212754Z","shell.execute_reply.started":"2022-02-06T15:47:45.15845Z","shell.execute_reply":"2022-02-06T15:47:45.211943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# MODELING","metadata":{}},{"cell_type":"markdown","source":"I'll start with a linear model without the holiday features to capture the macro trend. I'll then train a tree model on the residuals of the linear model.","metadata":{}},{"cell_type":"code","source":"def gdp_regression(data,t,p,con,st,TEST,al,show=True):\n    #df=data.groupby(['country','store','product']).get_group((con,st,p))\n    df=data.copy()\n    te=TEST.copy()\n    #'Finland','Sweden','Norway','KaggleRama','KaggleMart','Kaggle_Mug','Kaggle_Hat','Kaggle_Sticker'\n    temp=t[t['date']>=pd.to_datetime('2018-01-01')]\n    x_tr=df[df['date']<pd.to_datetime('2018-01-01')][['cos_month', 'sin_month', 'cos_week_day', 'sin_week_day',\n       'cos_week_of_mon', 'sin_week_of_mon','Sweden_Kaggle_Mug_month_cos',\n       'Sweden_Kaggle_Mug_month_sin', 'Sweden_Kaggle_Sticker_month_cos',\n       'Sweden_Kaggle_Sticker_month_sin', 'Norway_Kaggle_Mug_month_cos',\n       'Norway_Kaggle_Mug_month_sin', 'Norway_Kaggle_Sticker_month_cos',\n       'Norway_Kaggle_Sticker_month_sin', 'cos_week*mon', 'sin_week*mon',\n       'Norway', 'Sweden', 'KaggleRama', 'Kaggle_Mug', 'Kaggle_Sticker','gdp','cos_day','sin_day']]\n    x_ts=df[df['date']>=pd.to_datetime('2018-01-01')][['cos_month', 'sin_month', 'cos_week_day', 'sin_week_day',\n       'cos_week_of_mon', 'sin_week_of_mon','Sweden_Kaggle_Mug_month_cos',\n       'Sweden_Kaggle_Mug_month_sin', 'Sweden_Kaggle_Sticker_month_cos',\n       'Sweden_Kaggle_Sticker_month_sin', 'Norway_Kaggle_Mug_month_cos',\n       'Norway_Kaggle_Mug_month_sin', 'Norway_Kaggle_Sticker_month_cos',\n       'Norway_Kaggle_Sticker_month_sin', 'cos_week*mon', 'sin_week*mon',\n       'Norway', 'Sweden', 'KaggleRama', 'Kaggle_Mug', 'Kaggle_Sticker','gdp','cos_day','sin_day']]\n    y_tr=np.log(df[df['date']<pd.to_datetime('2018-01-01')][['num_sold']])\n    y_ts=np.log(df[df['date']>=pd.to_datetime('2018-01-01')][['num_sold']])\n    \n    for c in ['gdp','cos_month', 'sin_month', 'cos_week_day', 'sin_week_day',\n       'cos_week_of_mon', 'sin_week_of_mon','Sweden_Kaggle_Mug_month_cos',\n       'Sweden_Kaggle_Mug_month_sin', 'Sweden_Kaggle_Sticker_month_cos',\n       'Sweden_Kaggle_Sticker_month_sin', 'Norway_Kaggle_Mug_month_cos',\n       'Norway_Kaggle_Mug_month_sin', 'Norway_Kaggle_Sticker_month_cos',\n       'Norway_Kaggle_Sticker_month_sin', 'cos_week*mon', 'sin_week*mon','cos_day','sin_day']:\n        ma=x_tr[c].max()\n        mi=x_tr[c].min()\n        x_tr[c]=((x_tr[c]-mi)/(ma-mi))+1\n        x_ts[c]=((x_ts[c]-mi)/(ma-mi))+1\n        te[c]=((te[c]-mi)/(ma-mi))+1\n        \n    #x_tr=s.fit_transform(x_tr)\n    #x_ts=s.transform(x_ts)\n    reg=Ridge(alpha=al)\n    reg.fit(x_tr,y_tr)\n    preds=reg.predict(x_ts)\n    temp['preds']=preds\n    tr=reg.predict(x_tr)\n    te['num_sold']=reg.predict(te[['cos_month', 'sin_month', 'cos_week_day', 'sin_week_day',\n       'cos_week_of_mon', 'sin_week_of_mon','Sweden_Kaggle_Mug_month_cos',\n       'Sweden_Kaggle_Mug_month_sin', 'Sweden_Kaggle_Sticker_month_cos',\n       'Sweden_Kaggle_Sticker_month_sin', 'Norway_Kaggle_Mug_month_cos',\n       'Norway_Kaggle_Mug_month_sin', 'Norway_Kaggle_Sticker_month_cos',\n       'Norway_Kaggle_Sticker_month_sin', 'cos_week*mon', 'sin_week*mon',\n       'Norway', 'Sweden', 'KaggleRama', 'Kaggle_Mug', 'Kaggle_Sticker','gdp','cos_day','sin_day']])\n    if show==True:\n        #plt.subplots(1,1,figsize=(20,5))\n        #plt.plot(y_ts.reset_index(drop=True))\n        #plt.plot(preds)\n        #plt.show()\n        f,ax=plt.subplots(2,3,figsize=(20,10))\n        for i,s in enumerate(temp['store'].unique()):\n            for j,p in enumerate(temp['product'].unique()):\n                a=np.log(temp.groupby(['country','store','product']).get_group((con,st,p))['num_sold'].values)\n                b=temp.groupby(['country','store','product']).get_group((con,st,p))['preds'].values\n                ax[i][j].plot(a,label='True')\n                ax[i][j].plot(b,label='Predicted')\n                ax[i][j].legend(loc='upper right')\n                ax[i][j].set_title(s+' '+p)\n    df['res']=np.log(df['num_sold'])-np.append(reg.predict(x_tr),reg.predict(x_ts))\n    sc=mean_squared_error(y_ts,preds)\n    return reg.coef_,df,te,preds,tr,sc","metadata":{"execution":{"iopub.status.busy":"2022-02-06T15:47:47.969007Z","iopub.execute_input":"2022-02-06T15:47:47.969281Z","iopub.status.idle":"2022-02-06T15:47:47.987447Z","shell.execute_reply.started":"2022-02-06T15:47:47.96925Z","shell.execute_reply":"2022-02-06T15:47:47.986746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gdp_slope,df,te,va,tr,sc=gdp_regression(train,reg_train,'Kaggle_Mug','Norway','KaggleRama',test,1,show=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T15:47:50.170837Z","iopub.execute_input":"2022-02-06T15:47:50.171104Z","iopub.status.idle":"2022-02-06T15:47:51.162282Z","shell.execute_reply.started":"2022-02-06T15:47:50.171072Z","shell.execute_reply":"2022-02-06T15:47:51.161648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(1,20):\n    gdp_slope,df,te,va,tr,sc=gdp_regression(train,reg_train,'Kaggle_Mug','Norway','KaggleRama',test,i/10,show=False)\n    print(i/10,sc)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T15:48:03.969362Z","iopub.execute_input":"2022-02-06T15:48:03.970081Z","iopub.status.idle":"2022-02-06T15:48:05.686186Z","shell.execute_reply.started":"2022-02-06T15:48:03.970045Z","shell.execute_reply":"2022-02-06T15:48:05.685553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The residuals, I believe, correspond to holidays and other seasonality factors.","metadata":{}},{"cell_type":"code","source":"train['res']=df['res']","metadata":{"execution":{"iopub.status.busy":"2022-02-06T15:48:09.864163Z","iopub.execute_input":"2022-02-06T15:48:09.864683Z","iopub.status.idle":"2022-02-06T15:48:09.869426Z","shell.execute_reply.started":"2022-02-06T15:48:09.86464Z","shell.execute_reply":"2022-02-06T15:48:09.868819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X=train[train['date']<pd.to_datetime('2018-01-01')]\nY=train[train['date']>=pd.to_datetime('2018-01-01')]","metadata":{"execution":{"iopub.status.busy":"2022-02-06T15:48:11.912104Z","iopub.execute_input":"2022-02-06T15:48:11.912411Z","iopub.status.idle":"2022-02-06T15:48:11.928261Z","shell.execute_reply.started":"2022-02-06T15:48:11.912373Z","shell.execute_reply":"2022-02-06T15:48:11.927556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#'cos_month','sin_month','cos_week_of_mon','sin_week_of_mon','day','sin_week_day','cos_week_day',\nx_train=X[['cos_day','sin_day','cos_month','sin_month','cos_week_of_mon','sin_week_of_mon','sin_week_day','cos_week_day','is_special','Sweden','Norway','KaggleRama','Kaggle_Mug','Kaggle_Sticker','days_from_easter', 'days_from_wed_jun', 'days_from_sun_nov']]\nx_test=Y[['cos_day','sin_day','cos_month','sin_month','cos_week_of_mon','sin_week_of_mon','sin_week_day','cos_week_day','is_special','Sweden','Norway','KaggleRama','Kaggle_Mug','Kaggle_Sticker','days_from_easter', 'days_from_wed_jun', 'days_from_sun_nov']]\ny_test=Y[['res']]\ny_train=X[['res']]","metadata":{"execution":{"iopub.status.busy":"2022-02-06T15:48:13.210173Z","iopub.execute_input":"2022-02-06T15:48:13.210899Z","iopub.status.idle":"2022-02-06T15:48:13.219683Z","shell.execute_reply.started":"2022-02-06T15:48:13.210857Z","shell.execute_reply":"2022-02-06T15:48:13.219026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"va=va.reshape(6570,) #Test predictions from the linear model\ngrid = {'n_estimators':150,\n        'max_depth': 6,\n        'learning_rate': 0.1}\nmodel=LGBMRegressor(**grid)\n#model=LGBMRegressor(lambda_l1=0, lambda_l2=40, learning_rate=0.2823128119188338,\n             # max_depth=20, n_estimators=1000, num_leaves=27)\nmodel.fit(x_train,y_train)\nlgb_pred=model.predict(x_test)\npred=lgb_pred+va","metadata":{"execution":{"iopub.status.busy":"2022-02-06T15:48:14.832619Z","iopub.execute_input":"2022-02-06T15:48:14.832887Z","iopub.status.idle":"2022-02-06T15:48:15.064901Z","shell.execute_reply.started":"2022-02-06T15:48:14.832857Z","shell.execute_reply":"2022-02-06T15:48:15.064344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.barh(x_train.columns,model.feature_importances_)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T15:48:17.489549Z","iopub.execute_input":"2022-02-06T15:48:17.489858Z","iopub.status.idle":"2022-02-06T15:48:17.711349Z","shell.execute_reply.started":"2022-02-06T15:48:17.489823Z","shell.execute_reply":"2022-02-06T15:48:17.710552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def SMAPE(y_true, y_pred):\n    denominator = (y_true + np.abs(y_pred)) / 200.0\n    diff = np.abs(y_true - y_pred) / denominator\n    return np.mean(diff)\n\ndef objective(trial,x_train,y_train,x_test,y_test,Y,va):\n    params = {\n        'max_depth': trial.suggest_int('max_depth',3, 20),\n        'num_leaves':trial.suggest_int('num_leaves',10,1500),\n        'n_estimators':trial.suggest_int('n_estimators',150,1000,step=50),\n        'learning_rate':trial.suggest_float('learning_rate',0.01,0.3),\n        'lambda_l1':trial.suggest_int('lambda_l1',0,100,step=5),\n        'lambda_l2':trial.suggest_int('lambda_l2',0,100,step=5)\n    }\n    model = LGBMRegressor(**params, \n                         random_state=42)\n    model.fit(x_train, y_train, eval_set=[(x_test, y_test)], early_stopping_rounds=20, verbose=False)\n    preds = model.predict(x_test)+va\n    score = SMAPE(Y[['num_sold']].values.reshape(6570,), np.exp(preds))\n    \n    return score\nstudy = optuna.create_study(direction='minimize')\nfunc = lambda trial: objective(trial, x_train,y_train,x_test,y_test,Y,va)\nstudy.optimize(func, n_trials=30)\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial parameter:', study.best_trial.params)\nmodel=LGBMRegressor(**study.best_trial.params)\nmodel.fit(x_train,y_train)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T15:48:19.92059Z","iopub.execute_input":"2022-02-06T15:48:19.920954Z","iopub.status.idle":"2022-02-06T15:48:27.843606Z","shell.execute_reply.started":"2022-02-06T15:48:19.920918Z","shell.execute_reply":"2022-02-06T15:48:27.84291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgb_pred=model.predict(x_test)\npred=lgb_pred+va","metadata":{"execution":{"iopub.status.busy":"2022-02-06T15:48:28.228959Z","iopub.execute_input":"2022-02-06T15:48:28.229269Z","iopub.status.idle":"2022-02-06T15:48:28.256877Z","shell.execute_reply.started":"2022-02-06T15:48:28.229228Z","shell.execute_reply":"2022-02-06T15:48:28.255952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score=SMAPE(Y[['num_sold']].values.reshape(6570,),np.exp(pred))\nscore","metadata":{"execution":{"iopub.status.busy":"2022-02-06T15:48:29.782862Z","iopub.execute_input":"2022-02-06T15:48:29.783106Z","iopub.status.idle":"2022-02-06T15:48:29.789493Z","shell.execute_reply.started":"2022-02-06T15:48:29.783079Z","shell.execute_reply":"2022-02-06T15:48:29.788942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Training score\nSMAPE(X[['num_sold']].values.reshape(19728,),np.exp(tr.reshape(19728,)+model.predict(x_train)))","metadata":{"execution":{"iopub.status.busy":"2022-02-06T15:48:31.571059Z","iopub.execute_input":"2022-02-06T15:48:31.571655Z","iopub.status.idle":"2022-02-06T15:48:31.637959Z","shell.execute_reply.started":"2022-02-06T15:48:31.571603Z","shell.execute_reply":"2022-02-06T15:48:31.63421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.set_index('row_id',drop=True,inplace=True)\nt=te['num_sold'].values.reshape(6570,) #test set predictions fromt the linear model","metadata":{"execution":{"iopub.status.busy":"2022-02-06T15:48:34.255376Z","iopub.execute_input":"2022-02-06T15:48:34.255676Z","iopub.status.idle":"2022-02-06T15:48:34.260876Z","shell.execute_reply.started":"2022-02-06T15:48:34.255645Z","shell.execute_reply":"2022-02-06T15:48:34.260383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds_test = model.predict(test[['cos_day','sin_day','cos_month','sin_month','cos_week_of_mon','sin_week_of_mon','sin_week_day','cos_week_day','is_special','Sweden','Norway','KaggleRama','Kaggle_Mug','Kaggle_Sticker','days_from_easter', 'days_from_wed_jun', 'days_from_sun_nov']])\npreds_test=np.ceil(np.exp(preds_test+t))\noutput = pd.DataFrame({'row_id': test.index,\n                       'num_sold': preds_test})\noutput.to_csv('submission.csv',index=False)\noutput.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-06T15:49:03.732615Z","iopub.execute_input":"2022-02-06T15:49:03.733552Z","iopub.status.idle":"2022-02-06T15:49:03.777257Z","shell.execute_reply.started":"2022-02-06T15:49:03.733499Z","shell.execute_reply":"2022-02-06T15:49:03.776623Z"},"trusted":true},"execution_count":null,"outputs":[]}]}