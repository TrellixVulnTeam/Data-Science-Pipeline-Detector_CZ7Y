{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction","metadata":{}},{"cell_type":"markdown","source":"In this notebook, I'll explore the TPS Jan 2022 dataset in order to forecast the sales of certain products from two stores. Several feature engineering techniques are reviewed (especially the interaction betweeen features performed in this [notebook](https://www.kaggle.com/lucamassaron/kaggle-merchandise-eda-with-baseline-linear-model/notebook) by [Luca Massaron](https://www.kaggle.com/lucamassaron)).","metadata":{}},{"cell_type":"markdown","source":"# Libraries and data","metadata":{}},{"cell_type":"code","source":"# Libraries needed\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2022-01-12T18:55:50.270603Z","iopub.execute_input":"2022-01-12T18:55:50.271139Z","iopub.status.idle":"2022-01-12T18:55:50.274838Z","shell.execute_reply.started":"2022-01-12T18:55:50.2711Z","shell.execute_reply":"2022-01-12T18:55:50.27429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Importing the train and test datasets\ntrain = pd.read_csv(\"../input/tabular-playground-series-jan-2022/train.csv\", parse_dates=['date'])\ntest = pd.read_csv(\"../input/tabular-playground-series-jan-2022/test.csv\", parse_dates=['date'])\n\n# Obtaining the full dataset with a label train/test\nfull_df = pd.concat([train.assign(dataset='train'), test.assign(dataset='test')])\n\n# Notice we get NaN values for the unknown sales of the test dataset\nfull_df","metadata":{"execution":{"iopub.status.busy":"2022-01-12T18:55:50.276761Z","iopub.execute_input":"2022-01-12T18:55:50.277048Z","iopub.status.idle":"2022-01-12T18:55:50.35971Z","shell.execute_reply.started":"2022-01-12T18:55:50.277Z","shell.execute_reply":"2022-01-12T18:55:50.35885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's quickly confirm what countries, store and products belong to this dataset.","metadata":{}},{"cell_type":"code","source":"print('Countries:', full_df['country'].unique().tolist())\nprint('Stores:', full_df['store'].unique().tolist())\nprint('Products:', full_df['product'].unique().tolist())","metadata":{"execution":{"iopub.status.busy":"2022-01-12T18:55:50.360751Z","iopub.execute_input":"2022-01-12T18:55:50.360941Z","iopub.status.idle":"2022-01-12T18:55:50.374868Z","shell.execute_reply.started":"2022-01-12T18:55:50.360916Z","shell.execute_reply":"2022-01-12T18:55:50.374087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In order to be sure which years we're working on, let's create a 'year' column from the date timestamps.","metadata":{}},{"cell_type":"code","source":"full_df['year'] = full_df['date'].apply(lambda date: date.year) \nfull_df['year'].unique()","metadata":{"execution":{"iopub.status.busy":"2022-01-12T18:55:50.376145Z","iopub.execute_input":"2022-01-12T18:55:50.379229Z","iopub.status.idle":"2022-01-12T18:55:50.579349Z","shell.execute_reply.started":"2022-01-12T18:55:50.379183Z","shell.execute_reply":"2022-01-12T18:55:50.578551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It should be noted that the dates are already sorted and include all days from 2015 to 2019. To confirm this, we can generate a timestamp for every day in these years with pandas, and then compare it to our date columns.","metadata":{}},{"cell_type":"code","source":"# Array of all days from the first day of 2015 to the last day of 2019\ndays_range = pd.date_range(start='2015-01-01', end='2019-12-31')\n\n# Is this equal to the (unique) dates from our date column?\n(days_range != full_df['date'].unique()).sum()\n\n# Amount of falses is zero:","metadata":{"execution":{"iopub.status.busy":"2022-01-12T18:55:50.581194Z","iopub.execute_input":"2022-01-12T18:55:50.581417Z","iopub.status.idle":"2022-01-12T18:55:50.589259Z","shell.execute_reply.started":"2022-01-12T18:55:50.581389Z","shell.execute_reply":"2022-01-12T18:55:50.588491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We're also provided with the GDP of the three countries during these years. In previous versions, I used the GDP values provided by [Carl McBride](https://www.kaggle.com/carlmcbrideellis/gdp-20152019-finland-norway-and-sweden). \n\nIn this version, however, I use the GDP per capita dataset by [Samuel Cortinhas](https://www.kaggle.com/samuelcortinhas/gdp-per-capita-finland-norway-sweden-201519), as it has improved the score by a bit.","metadata":{}},{"cell_type":"code","source":"gdp = pd.read_csv('../input/gdp-per-capita-finland-norway-sweden-201519/GDP_per_capita_2015_to_2019_Finland_Norway_Sweden.csv')\ngdp.columns = ['year', 'GDP_Finland', 'GDP_Norway', 'GDP_Sweden']\ngdp","metadata":{"execution":{"iopub.status.busy":"2022-01-12T18:55:50.598954Z","iopub.execute_input":"2022-01-12T18:55:50.599268Z","iopub.status.idle":"2022-01-12T18:55:50.616789Z","shell.execute_reply.started":"2022-01-12T18:55:50.599235Z","shell.execute_reply":"2022-01-12T18:55:50.616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory Data Analysis","metadata":{}},{"cell_type":"markdown","source":"First, we want to make sure which years correspond to the train dataset, and which years to the test dataset.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(8,3))\nsns.countplot(data=full_df, x='year', hue='dataset', palette='Set2')\nplt.legend(title='dataset', bbox_to_anchor=(1.05, 1));","metadata":{"execution":{"iopub.status.busy":"2022-01-12T18:55:50.617868Z","iopub.execute_input":"2022-01-12T18:55:50.618118Z","iopub.status.idle":"2022-01-12T18:55:50.903211Z","shell.execute_reply.started":"2022-01-12T18:55:50.618081Z","shell.execute_reply":"2022-01-12T18:55:50.902193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above result, we see that it would be a good idea to train our model on the years 2015, 2016 and 2017 and then evaluate it on 2018 as a hold-out test set.","metadata":{}},{"cell_type":"markdown","source":"Let's first see what the sales look like between the three available products.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(15,6))\nsns.lineplot(data=full_df[full_df['year']<2019], x='date', y='num_sold', hue='product');","metadata":{"execution":{"iopub.status.busy":"2022-01-12T18:55:50.904614Z","iopub.execute_input":"2022-01-12T18:55:50.905081Z","iopub.status.idle":"2022-01-12T18:57:57.019472Z","shell.execute_reply.started":"2022-01-12T18:55:50.905035Z","shell.execute_reply":"2022-01-12T18:57:57.018535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above graph, we can clearly infer that there's a certain sales seasonality, and that we can order the amounf of sales as Hat > Mug > Sticker.","metadata":{}},{"cell_type":"markdown","source":"Also, we should check if a store sells more than the other two.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(15,6))\nsns.lineplot(data=full_df[full_df['year']<2019], x='date', y='num_sold', hue='store', palette='hls');","metadata":{"execution":{"iopub.status.busy":"2022-01-12T18:57:57.020664Z","iopub.execute_input":"2022-01-12T18:57:57.020901Z","iopub.status.idle":"2022-01-12T18:59:20.294412Z","shell.execute_reply.started":"2022-01-12T18:57:57.020871Z","shell.execute_reply":"2022-01-12T18:59:20.293586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So it seems that KaggleRama sells more products than KaggleMart.","metadata":{}},{"cell_type":"markdown","source":"We should also check how the sales differ from one country to another.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(15,6))\nsns.lineplot(data=full_df[full_df['year']<2019], x='date', y='num_sold', hue='country', palette='Set1');","metadata":{"execution":{"iopub.status.busy":"2022-01-12T18:59:20.295717Z","iopub.execute_input":"2022-01-12T18:59:20.296542Z","iopub.status.idle":"2022-01-12T19:01:24.539946Z","shell.execute_reply.started":"2022-01-12T18:59:20.296495Z","shell.execute_reply":"2022-01-12T19:01:24.539078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see that it's pretty much Norway > Sweden > Finland, regarding the amount of sales by country. ","metadata":{}},{"cell_type":"markdown","source":"# Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"Since the seasonily is important, we should get as much information as possible from the date column, such as year (which we already did), month, week, day of the month and day of the week.","metadata":{}},{"cell_type":"code","source":"full_df['month'] = full_df['date'].dt.month\nfull_df['week'] = full_df['date'].dt.isocalendar().week\nfull_df['week'][full_df['week']>52] = 52\nfull_df['day'] = full_df['date'].dt.day\nfull_df['dayofweek'] = full_df['date'].dt.dayofweek\n\nfull_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-12T19:01:24.541327Z","iopub.execute_input":"2022-01-12T19:01:24.542114Z","iopub.status.idle":"2022-01-12T19:01:24.595084Z","shell.execute_reply.started":"2022-01-12T19:01:24.542039Z","shell.execute_reply":"2022-01-12T19:01:24.594207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, we want to create interactions between features. To understand how this works, first pick up a couple of rows as an example:","metadata":{}},{"cell_type":"code","source":"rows_example = full_df.iloc[:2]\nrows_example","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-01-12T19:01:24.598109Z","iopub.execute_input":"2022-01-12T19:01:24.598341Z","iopub.status.idle":"2022-01-12T19:01:24.614788Z","shell.execute_reply.started":"2022-01-12T19:01:24.598312Z","shell.execute_reply":"2022-01-12T19:01:24.613952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Say we want to create interactions between three features: country, product and month. For every row, we will join their (string) values along the mentioned columns (features).","metadata":{}},{"cell_type":"code","source":"# Features we want to interact between each other\nfeatures_example = ['country', 'product', 'month']\n\n# For the two selected rows, join their respective values\ninteractions = rows_example[features_example].apply(lambda row: '_'.join(row.astype(str)), axis=1)\ninteractions","metadata":{"execution":{"iopub.status.busy":"2022-01-12T19:01:24.616004Z","iopub.execute_input":"2022-01-12T19:01:24.616378Z","iopub.status.idle":"2022-01-12T19:01:24.633109Z","shell.execute_reply.started":"2022-01-12T19:01:24.616335Z","shell.execute_reply":"2022-01-12T19:01:24.63216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# One-hot encode them\npd.get_dummies(interactions)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-01-12T19:01:24.634307Z","iopub.execute_input":"2022-01-12T19:01:24.634572Z","iopub.status.idle":"2022-01-12T19:01:24.64591Z","shell.execute_reply.started":"2022-01-12T19:01:24.634529Z","shell.execute_reply":"2022-01-12T19:01:24.645155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If we select the rows' features we're interested in, and concat them to the above frame, we get something like this:","metadata":{}},{"cell_type":"code","source":"pd.concat((rows_example[features_example], pd.get_dummies(interactions)), axis=1)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-01-12T19:01:24.647395Z","iopub.execute_input":"2022-01-12T19:01:24.647792Z","iopub.status.idle":"2022-01-12T19:01:24.667212Z","shell.execute_reply.started":"2022-01-12T19:01:24.647758Z","shell.execute_reply":"2022-01-12T19:01:24.666326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In other words, since the first row satisfies having Finland as the country, Kaggle Mug as the product and month equal to 1 (January), it gets a 1 (one) in the 'Finland_Kaggle Mug_1' column, but a 0 in the Kaggle Hat column, whereas the second row gets a 1 there.","metadata":{}},{"cell_type":"markdown","source":"Now, to the real work, we will create four interactions and perform the process on the full dataframe as follows. \n(Also, keep in mind that too many interactions can result in overfit when training the model, as I found out when I was testing this, as well as an insanely long time training the model.)","metadata":{}},{"cell_type":"code","source":"desired_interactions = [\n        ['country', 'product', 'month'],\n        ['country', 'product', 'week'],\n        ['country', 'store', 'week'],\n        ['country', 'product', 'month', 'day'],\n        ]\n\nfor interaction in desired_interactions:\n    \n    interaction_features = full_df[interaction].apply(lambda row: '_'.join(row.astype(str)), axis=1)\n    interaction_features = pd.get_dummies(interaction_features)\n    \n    full_df = pd.concat((full_df, interaction_features), axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-01-12T19:01:24.668736Z","iopub.execute_input":"2022-01-12T19:01:24.669699Z","iopub.status.idle":"2022-01-12T19:01:34.245844Z","shell.execute_reply.started":"2022-01-12T19:01:24.669657Z","shell.execute_reply":"2022-01-12T19:01:34.244981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It should be noted that it may appear that we get columns with the same name, because the month and week values can overlap as in the following example (first month (1) plus first week (1)). But this isn't directly a problem so we leave it as is. ","metadata":{}},{"cell_type":"code","source":"full_df[['country', 'product', 'month', 'week', 'Finland_Kaggle Hat_1']].head()","metadata":{"execution":{"iopub.status.busy":"2022-01-12T19:01:34.247086Z","iopub.execute_input":"2022-01-12T19:01:34.247381Z","iopub.status.idle":"2022-01-12T19:01:34.379719Z","shell.execute_reply.started":"2022-01-12T19:01:34.247343Z","shell.execute_reply":"2022-01-12T19:01:34.379123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will now add the GDP values of each country. Remember we had the GDP dataset:","metadata":{}},{"cell_type":"code","source":"gdp","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-01-12T19:01:34.380601Z","iopub.execute_input":"2022-01-12T19:01:34.381273Z","iopub.status.idle":"2022-01-12T19:01:34.390777Z","shell.execute_reply.started":"2022-01-12T19:01:34.381235Z","shell.execute_reply":"2022-01-12T19:01:34.389868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will create three new columns 'GDP_Finland', 'GDP_Norway', 'GDP_Sweden\" such that their values will be either zero or the corresponding GDP value if the country and the year match with those of the row.","metadata":{}},{"cell_type":"code","source":"# Function to get the GDP value from the dataset\ndef get_gdp(country_from_row, year, country):\n    \n    if country_from_row == country:\n        var_name = 'GDP_' + country\n        return gdp[gdp['year']==year][var_name].values[0]\n    \n    else: \n        return 0","metadata":{"execution":{"iopub.status.busy":"2022-01-12T19:01:34.392076Z","iopub.execute_input":"2022-01-12T19:01:34.39232Z","iopub.status.idle":"2022-01-12T19:01:34.402848Z","shell.execute_reply.started":"2022-01-12T19:01:34.39229Z","shell.execute_reply":"2022-01-12T19:01:34.402014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Example \n# If the row's country matches the country we choose, return the GDP \nprint( get_gdp('Sweden', 2018, 'Sweden') )\n\n# Else return 0\nprint( get_gdp('Finland', 2019, 'Sweden') )","metadata":{"execution":{"iopub.status.busy":"2022-01-12T19:01:34.404359Z","iopub.execute_input":"2022-01-12T19:01:34.404628Z","iopub.status.idle":"2022-01-12T19:01:34.417717Z","shell.execute_reply.started":"2022-01-12T19:01:34.404597Z","shell.execute_reply":"2022-01-12T19:01:34.41686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating the columns with the corresponding GDP values\n\nfull_df['GDP_Finland'] = np.vectorize(get_gdp, otypes=['float'])(\n    full_df['country'], full_df['year'], 'Finland' )\n\nfull_df['GDP_Norway'] = np.vectorize(get_gdp, otypes=['float'])(\n    full_df['country'], full_df['year'], 'Norway' )\n\nfull_df['GDP_Sweden'] = np.vectorize(\n    get_gdp, otypes=['float'])(\n    full_df['country'], full_df['year'], 'Sweden' )","metadata":{"execution":{"iopub.status.busy":"2022-01-12T19:01:34.419245Z","iopub.execute_input":"2022-01-12T19:01:34.419727Z","iopub.status.idle":"2022-01-12T19:01:46.909871Z","shell.execute_reply.started":"2022-01-12T19:01:34.419692Z","shell.execute_reply":"2022-01-12T19:01:46.908908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Random example\n\nfull_df[['country', 'year', 'GDP_Finland', 'GDP_Norway', 'GDP_Sweden']].iloc[[2000, 15000, 25000]]","metadata":{"execution":{"iopub.status.busy":"2022-01-12T19:01:46.910937Z","iopub.execute_input":"2022-01-12T19:01:46.91118Z","iopub.status.idle":"2022-01-12T19:01:46.931302Z","shell.execute_reply.started":"2022-01-12T19:01:46.911151Z","shell.execute_reply":"2022-01-12T19:01:46.930152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We also drop the 'date' and 'dataset' columns as we won't use them anymore.","metadata":{}},{"cell_type":"code","source":"full_df.drop(['date', 'dataset'], axis=1, inplace=True)\nfull_df.head()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-01-12T19:01:46.933307Z","iopub.execute_input":"2022-01-12T19:01:46.933532Z","iopub.status.idle":"2022-01-12T19:01:47.337024Z","shell.execute_reply.started":"2022-01-12T19:01:46.933505Z","shell.execute_reply":"2022-01-12T19:01:47.336004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we will perform one-hot encoding for the categorical features, namely: country, store, product, month, week, day and dayofweek. Note that although these last features are stored as integers, they should be treated as categorical since they don't actually follow a certain hierarchical order. Thus, we will convert them to strings and then get the dummies of these categorical features from the entire dataframe.","metadata":{}},{"cell_type":"code","source":"# List of categorical features\ncat_features = ['country', 'store', 'product', 'month', 'week', 'day', 'dayofweek']\n\n# Convert all of them to strings just in case\nfull_df[cat_features] = full_df[cat_features].astype(str)\n\n# Getting dummies for the selected columns\nfull_df = pd.get_dummies(full_df)\n\nfull_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-12T19:01:47.339038Z","iopub.execute_input":"2022-01-12T19:01:47.339462Z","iopub.status.idle":"2022-01-12T19:01:48.059281Z","shell.execute_reply.started":"2022-01-12T19:01:47.33943Z","shell.execute_reply":"2022-01-12T19:01:48.058482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally, we can get our dataset ready for our model to be trained. Remember that we first train the model on the years 2015, 2016 and 2017, so that we can evaluate on 2018.","metadata":{}},{"cell_type":"code","source":"X_train = full_df[full_df['year']<2018].drop(['row_id', 'num_sold'], axis=1)\ny_train = full_df[full_df['year']<2018]['num_sold']\n\nX_test = full_df[full_df['year']==2018].drop(['row_id', 'num_sold'], axis=1)\ny_test = full_df[full_df['year']==2018]['num_sold']","metadata":{"execution":{"iopub.status.busy":"2022-01-12T19:01:48.060423Z","iopub.execute_input":"2022-01-12T19:01:48.060652Z","iopub.status.idle":"2022-01-12T19:01:48.944793Z","shell.execute_reply.started":"2022-01-12T19:01:48.060623Z","shell.execute_reply":"2022-01-12T19:01:48.943861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model and Evaluation","metadata":{}},{"cell_type":"markdown","source":"To evaluate the model, we consider the SMAPE measure.","metadata":{}},{"cell_type":"code","source":"def SMAPE(y_true, y_pred):\n    # Reference  https://www.kaggle.com/cpmpml/smape-weirdness\n    denominator = (y_true + np.abs(y_pred)) / 200.0\n    diff = np.abs(y_true - y_pred) / denominator\n    diff[denominator == 0] = 0.0\n    return np.mean(diff)","metadata":{"execution":{"iopub.status.busy":"2022-01-12T19:01:48.946236Z","iopub.execute_input":"2022-01-12T19:01:48.946488Z","iopub.status.idle":"2022-01-12T19:01:48.952048Z","shell.execute_reply.started":"2022-01-12T19:01:48.946451Z","shell.execute_reply":"2022-01-12T19:01:48.951221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will construct a linear multiplicative model. Thus, we will import the LinearRegression model from scikit learn, and we will also transform the label 'y' via the natural logarithm. ","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression","metadata":{"execution":{"iopub.status.busy":"2022-01-12T19:01:48.953131Z","iopub.execute_input":"2022-01-12T19:01:48.953682Z","iopub.status.idle":"2022-01-12T19:01:49.217383Z","shell.execute_reply.started":"2022-01-12T19:01:48.953647Z","shell.execute_reply":"2022-01-12T19:01:49.216479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating an instance of a Linear Regression\nmodel = LinearRegression()\n\n# Training the model\nmodel.fit(X_train, np.log(y_train))","metadata":{"execution":{"iopub.status.busy":"2022-01-12T19:01:49.220726Z","iopub.execute_input":"2022-01-12T19:01:49.220969Z","iopub.status.idle":"2022-01-12T19:02:31.67744Z","shell.execute_reply.started":"2022-01-12T19:01:49.220938Z","shell.execute_reply":"2022-01-12T19:02:31.676504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"First, let's make sure that the model actually got close to the very y_train.","metadata":{}},{"cell_type":"code","source":"# Computing the predicted values from X_train\ntrain_predictions = model.predict(X_train)\n\n# Remember to inverse-transform via the exp function\ntrain_predictions = np.exp(train_predictions)\n\n# Calculating the SMAPE\nSMAPE(y_train, train_predictions)","metadata":{"execution":{"iopub.status.busy":"2022-01-12T19:02:31.679277Z","iopub.execute_input":"2022-01-12T19:02:31.679823Z","iopub.status.idle":"2022-01-12T19:02:32.036322Z","shell.execute_reply.started":"2022-01-12T19:02:31.679772Z","shell.execute_reply":"2022-01-12T19:02:32.035474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We now evaluate on 2018.","metadata":{}},{"cell_type":"code","source":"test_predictions = model.predict(X_test)\ntest_predictions = np.exp(test_predictions)\n\nSMAPE(y_test, test_predictions)","metadata":{"execution":{"iopub.status.busy":"2022-01-12T19:02:32.038239Z","iopub.execute_input":"2022-01-12T19:02:32.038877Z","iopub.status.idle":"2022-01-12T19:02:32.217787Z","shell.execute_reply.started":"2022-01-12T19:02:32.038828Z","shell.execute_reply":"2022-01-12T19:02:32.216896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This isn't too bad, so we retrain the model including 2018 and make our predictions on 2019 for the competition.","metadata":{}},{"cell_type":"code","source":"X_full_train = full_df[full_df['year']<2019].drop(['row_id', 'num_sold'], axis=1)\ny_full_train = full_df[full_df['year']<2019]['num_sold']","metadata":{"execution":{"iopub.status.busy":"2022-01-12T19:02:32.219785Z","iopub.execute_input":"2022-01-12T19:02:32.220471Z","iopub.status.idle":"2022-01-12T19:02:33.262835Z","shell.execute_reply.started":"2022-01-12T19:02:32.22042Z","shell.execute_reply":"2022-01-12T19:02:33.262124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = LinearRegression()\nmodel.fit(X_full_train, np.log(y_full_train))","metadata":{"execution":{"iopub.status.busy":"2022-01-12T19:02:33.26398Z","iopub.execute_input":"2022-01-12T19:02:33.264274Z","iopub.status.idle":"2022-01-12T19:03:22.671203Z","shell.execute_reply.started":"2022-01-12T19:02:33.264242Z","shell.execute_reply":"2022-01-12T19:03:22.670303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Grabbing the rows from 2019\nX_competition = full_df[full_df['year']==2019].drop(['row_id', 'num_sold'], axis=1)\n\n# Calculating our predictions\nfinal_predictions = np.exp(model.predict(X_competition))\n\n# Formatting for the submission, including the row_id\nX_competition['num_sold'] = final_predictions\nX_competition['row_id'] = full_df[full_df['year']==2019]['row_id']\nX_competition = X_competition[['row_id', 'num_sold']]\n\nX_competition","metadata":{"execution":{"iopub.status.busy":"2022-01-12T19:03:22.673159Z","iopub.execute_input":"2022-01-12T19:03:22.673766Z","iopub.status.idle":"2022-01-12T19:03:23.029879Z","shell.execute_reply.started":"2022-01-12T19:03:22.673717Z","shell.execute_reply":"2022-01-12T19:03:23.029002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"At last, the submission!","metadata":{}},{"cell_type":"code","source":"X_competition.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-12T19:03:23.031711Z","iopub.execute_input":"2022-01-12T19:03:23.032379Z","iopub.status.idle":"2022-01-12T19:03:23.066047Z","shell.execute_reply.started":"2022-01-12T19:03:23.03233Z","shell.execute_reply":"2022-01-12T19:03:23.065348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"That's all. Thank you for reading.","metadata":{}}]}