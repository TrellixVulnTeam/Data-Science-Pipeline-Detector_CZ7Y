{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport pickle\nimport itertools\nimport gc\nimport math\nimport matplotlib.pyplot as plt\nimport dateutil.easter as easter\nfrom matplotlib.ticker import MaxNLocator, FormatStrFormatter\nfrom datetime import datetime, date\nimport scipy.stats\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.pipeline import make_pipeline\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, LearningRateScheduler, EarlyStopping\nfrom tensorflow.keras.layers import Dense, Input, InputLayer, Add","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-28T17:43:38.176741Z","iopub.execute_input":"2022-01-28T17:43:38.177034Z","iopub.status.idle":"2022-01-28T17:43:46.131823Z","shell.execute_reply.started":"2022-01-28T17:43:38.17697Z","shell.execute_reply":"2022-01-28T17:43:46.130773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_history(history, *, n_epochs=None, plot_lr=False, plot_acc=True, title=None, bottom=None, top=None):\n    plt.figure(figsize=(15, 6))\n    from_epoch = 0 if n_epochs is None else max(len(history['loss']) - n_epochs, 0)\n    \n    plt.plot(np.arange(from_epoch, len(history['loss'])), history['loss'][from_epoch:], label='TRAINING LOSS')\n    try:\n        plt.plot(np.arange(from_epoch, len(history['loss'])), history['val_loss'][from_epoch:], label='VALIDATION LOSS')\n        best_epoch = np.argmin(np.array(history['val_loss']))\n        best_val_loss = history['val_loss'][best_epoch]\n        \n        if best_epoch >= from_epoch:\n            plt.scatter([best_epoch], [best_val_loss], c='magenta', label=f'BEST VAL_LOSS = {best_val_loss:.5f}')\n        \n        if best_epoch > 0:\n            almost_epoch = np.argmin(np.array(history['val_loss'])[:best_epoch])\n            almost_val_loss = history['val_loss'][almost_epoch]\n            \n            if almost_epoch >= from_epoch:\n                plt.scatter([almost_epoch], [almost_val_loss], c='red', label='SECOND BEST VAL_LOSS')\n    except KeyError:\n        pass\n    \n    if bottom is not None:\n        plt.ylim(bottom=bottom)\n        \n    if top is not None:\n        plt.ylim(top=top)\n        \n    plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n    plt.xlabel('EPOCH')\n    plt.ylabel('LOSS')\n    plt.legend(loc='lower left')\n    \n    if title is not None:\n        plt.title(title)\n        \n    if plot_lr and 'lr' in history:\n        ax2 = plt.gca().twinx()\n        ax2.plot(np.arange(from_epoch, len(history['lr'])), np.array(history['lr'][from_epoch:]), c='green', label='LEARNING RATE')\n        ax2.set_ylabel('LEARNING_RATE')\n        ax2.legend(loc='upper right')\n        \n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-28T17:43:46.136364Z","iopub.execute_input":"2022-01-28T17:43:46.136575Z","iopub.status.idle":"2022-01-28T17:43:46.154996Z","shell.execute_reply.started":"2022-01-28T17:43:46.136543Z","shell.execute_reply":"2022-01-28T17:43:46.15417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"original_train_df = pd.read_csv('../input/tabular-playground-series-jan-2022/train.csv')\noriginal_test_df = pd.read_csv('../input/tabular-playground-series-jan-2022/test.csv')\n\nfor df in [original_train_df, original_test_df]:\n    df['date'] = pd.to_datetime(df.date)\n    \noriginal_train_df.head()\ngdp_df = pd.read_csv('../input/gdp-20152019-finland-norway-and-sweden/GDP_data_2015_to_2019_Finland_Norway_Sweden.csv')\ngdp_df.set_index('year', inplace=True)\n\n# qgdp_df = pd.read_csv('../input/tsp-jan2022-gdp-per-quarter/GDP_Quarterly.csv')\n# qgdp_df['GDP'] = qgdp_df['GDP'].apply(lambda s: int(s.replace(',', '')))\n# qgdp_df.set_index('Base_Key', inplace=True)\n# qgdp_df","metadata":{"execution":{"iopub.status.busy":"2022-01-28T17:43:46.156194Z","iopub.execute_input":"2022-01-28T17:43:46.156436Z","iopub.status.idle":"2022-01-28T17:43:46.301463Z","shell.execute_reply.started":"2022-01-28T17:43:46.156406Z","shell.execute_reply":"2022-01-28T17:43:46.300125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def smape_loss(y_true, y_pred):\n    return tf.abs(y_true - y_pred) / (y_true + tf.abs(y_pred)) * 200","metadata":{"execution":{"iopub.status.busy":"2022-01-28T17:43:46.303638Z","iopub.execute_input":"2022-01-28T17:43:46.304585Z","iopub.status.idle":"2022-01-28T17:43:46.309621Z","shell.execute_reply.started":"2022-01-28T17:43:46.304523Z","shell.execute_reply":"2022-01-28T17:43:46.308395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def engineer(df):\n    \n    def get_gdp(row):\n        country = 'GDP_' + row.country\n        \n        return gdp_df.loc[row.date.year, country]\n    \n    new_df = pd.DataFrame({\n        'gdp': np.log(df.apply(get_gdp, axis=1)),\n        'wd4': df.date.dt.weekday == 4,\n        'wd56': df.date.dt.weekday >= 5\n     })\n    \n    for country in ['Finland', 'Norway']:\n        new_df[country] = df.country == country\n        \n    new_df['KaggleRama'] = df.store == 'KaggleRama'\n    \n    for product in ['Kaggle Mug', 'Kaggle Hat']:\n        new_df[product] = df['product'] == product\n        \n    dayofyear = df.date.dt.dayofyear\n    for k in range(1, 3):\n        new_df[f'sin{k}'] = np.sin(dayofyear / 365 * 2 * math.pi * k)\n        new_df[f'cos{k}'] = np.cos(dayofyear / 365 * 2 * math.pi * k)\n        new_df[f'mug_sin{k}'] = new_df[f'sin{k}'] * new_df['Kaggle Mug']\n        new_df[f'mug_cos{k}'] = new_df[f'cos{k}'] * new_df['Kaggle Mug']\n        new_df[f'hat_sin{k}'] = new_df[f'sin{k}'] * new_df['Kaggle Hat']\n        new_df[f'hat_cos{k}'] = new_df[f'cos{k}'] * new_df['Kaggle Hat']\n        \n    new_df = pd.concat([new_df,\n                        pd.DataFrame({\n                            f'dec{d}': (df.date.dt.month == 12) & (df.date.dt.day == d) for d in range(24, 32)\n                        }),\n                        pd.DataFrame({\n                            f'n-dec{d}': (df.date.dt.month == 12) & (df.date.dt.day == d) & (df.country == 'Norway') for d in range(24, 32)\n                        }),\n                        pd.DataFrame({\n                            f'f-jan{d}': (df.date.dt.month == 1) & (df.date.dt.day == d) & (df.country == 'Finland') for d in range(1, 14)\n                        }),\n                        pd.DataFrame({\n                            f'jan{d}': (df.date.dt.month == 1) & (df.date.dt.day == d) & (df.country == 'Norway') for d in range(1, 10)\n                        }),\n                        pd.DataFrame({\n                            f's-jan{d}': (df.date.dt.month == 1) & (df.date.dt.day == d) & (df.country == 'Sweden') for d in range(1, 15)\n                        })],\n                       axis=1)\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({\n                            f'may{d}': (df.date.dt.month == 5) & (df.date.dt.day == d) for d in list(range(1, 10))\n                        }),\n                        pd.DataFrame({\n                            f'may{d}': (df.date.dt.month == 5) & (df.date.dt.day == d) & (df.country == 'Norway') for d in list(range(18, 28))\n                        })],\n                       axis=1)\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({\n                            f'june{d}': (df.date.dt.month == 6) & (df.date.dt.day == d) & (df.country == 'Sweden') for d in list(range(8, 14))\n                        })],\n                       axis=1)\n    wed_june_date = df.date.dt.year.map({\n        2015: pd.Timestamp(('2015-06-24')),\n        2016: pd.Timestamp(('2016-06-29')),\n        2017: pd.Timestamp(('2017-06-28')),\n        2018: pd.Timestamp(('2018-06-27')),\n        2019: pd.Timestamp(('2019-06-26'))\n    })\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({\n                            f'wed_jun{d}': (df.date - wed_june_date == np.timedelta64(d, 'D')) & (df.country != 'Norway') for d in list(range(-4, 6))\n                        })],\n                       axis=1)\n    sun_nov_date = df.date.dt.year.map({\n        2015: pd.Timestamp(('2015-11-1')),\n        2016: pd.Timestamp(('2016-11-6')),\n        2017: pd.Timestamp(('2017-11-5')),\n        2018: pd.Timestamp(('2018-11-4')),\n        2019: pd.Timestamp(('2019-11-3'))\n    })\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({\n                            f'sun_nov{d}': (df.date - sun_nov_date == np.timedelta64(d, 'D')) & (df.country != 'Norway') for d in list(range(0, 9))\n                        })],\n                       axis=1)\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({\n                            f'dec{d}': (df.date.dt.month == 12) & (df.date.dt.day == d) & (df.country == 'Finland') for d in list(range(6, 14)) \n                        })],\n                       axis=1)\n    easter_date = df.date.apply(lambda date: pd.Timestamp(easter.easter(date.year)))\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({\n                            f'easter{d}': (df.date - easter_date == np.timedelta64(d, 'D')) for d in list(range(-2, 11)) + list(range(40, 48)) + list(range(50, 59))\n                        })],\n                       axis=1)\n    return new_df.astype(np.float32)\n\ntrain_df = engineer(original_train_df)\ntrain_df['date'] = original_train_df.date\ntrain_df['num_sold'] = original_train_df.num_sold.astype(np.float32)\n\ntest_df = engineer(original_test_df)\n\nfeatures = list(test_df.columns)\nprint(list(features))","metadata":{"execution":{"iopub.status.busy":"2022-01-28T17:43:46.311481Z","iopub.execute_input":"2022-01-28T17:43:46.311982Z","iopub.status.idle":"2022-01-28T17:43:50.191955Z","shell.execute_reply.started":"2022-01-28T17:43:46.311942Z","shell.execute_reply":"2022-01-28T17:43:50.191172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nEPOCHS = 500\nEPOCHS_COSINEDECAY = 120\nVERBOSE = 1\nRUNS = 10\nDIAGRAMS = True\nUSE_PLATEAU = True\nINFERENCE = False\n\nwd_features = [f for f in features if f.startswith('wd')]\nother_features = [f for f in features if f not in wd_features]\n\ndef tpsjan_model():\n    wd = Input(shape=(len(wd_features), ))\n    other = Input(shape=(len(other_features), ))\n    wd_contribution = Dense(1,\n                            kernel_regularizer=tf.keras.regularizers.l2(1e-7),\n                            use_bias=False)(wd)\n    other_contribution = Dense(1,\n                               kernel_regularizer=tf.keras.regularizers.l2(1e-7),\n                               use_bias=True,\n                               bias_initializer=tf.keras.initializers.Constant(value=5.7))(other)\n    output = Add()([wd_contribution, other_contribution])\n    model = Model([wd, other], output)\n    \n    return model\n\ndef tpsjan_model_2():\n    other = Input(shape=(len(features), ))\n    output = Dense(1, \n                   kernel_regularizer=tf.keras.regularizers.l2(1e-6),\n                   use_bias=True,\n                   bias_initializer=tf.keras.initializers.Constant(value=5.74))(other)\n    model = Model(other, output)\n    return model\n\ndef fit_model(X_tr, X_va=None):\n    start_time = datetime.now()\n    preproc = make_pipeline(MinMaxScaler(), StandardScaler(with_std=False))\n    X_tr_f = pd.DataFrame(preproc.fit_transform(X_tr[features]), columns=features, index=X_tr.index)\n    y_tr = X_tr.num_sold.values.reshape(-1, 1)\n    \n    if X_va is not None:\n        X_va_f = pd.DataFrame(preproc.transform(X_va[features]), columns=features, index=X_va.index)\n        y_va = X_va.num_sold.values.reshape(-1, 1)\n        validation_data = ([X_va_f[features]], np.log(y_va))\n    else:\n        validation_data = None\n        \n    if USE_PLATEAU and X_va is not None:\n        epochs = EPOCHS\n        lr = ReduceLROnPlateau(monitor='val_loss', factor=0.7, patience=4, verbose=VERBOSE)\n        es = EarlyStopping(monitor='val_loss', patience=25, verbose=1, mode='min', restore_best_weights=True)\n        callbacks = [lr, es, tf.keras.callbacks.TerminateOnNaN()]\n    else:\n        epochs = EPOCHS_COSINEDECAY\n        lr_start = 0.02\n        lr_end = 0.00001\n        \n        def cosine_decay(epoch):\n            if epochs > 1:\n                w = (1 + math.cos(epoch / (epochs - 1) * math.pi)) / 2\n            else:\n                w = 1\n            return w * lr_start + (1 - w) * lr_end\n        \n        lr = LearningRateScheduler(cosine_decay, verbose=0)\n        callbacks = [lr, tf.keras.callbacks.TerminateOnNaN()]\n        \n    model = tpsjan_model_2()\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01), loss='mse')\n    #model.compile(optimizer=tf.keras.optimizers.SGD(), loss='mse')\n    \n    history = model.fit([X_tr_f[features]],\n                        np.log(y_tr),\n                        validation_data=validation_data,\n                        epochs=epochs,\n                        verbose=VERBOSE,\n                        batch_size=512,\n                        shuffle=True,\n                        callbacks=callbacks)\n    \n    history_list.append(history.history)\n    callbacks, es, lr, history = None, None, None, None\n    #print(f\"Loss:            {history_list[-1]['loss'][-1]:.6f}\")\n    #print(f\"Bias:  {model.get_layer(index=-1).get_weights()[1]}\")\n    \n    if X_va is not None:\n        y_va_pred = np.exp(model.predict([X_va_f[features]]))\n        oof_list[run][val_idx] = y_va_pred\n        \n        smape = np.mean(smape_loss(y_va, y_va_pred))\n        print(f'FOLD {run}.{fold} | {str(datetime.now() - start_time)[-12:-7]}'\n              f' | SMAPE: {smape:.5f} VALIDATED ON {X_va.iloc[0].date.year}')\n        score_list.append(smape)\n        \n        if DIAGRAMS and fold == 0 and run == 0:\n            plot_history(history_list[-1], title=f'VALIDATION SMAPE = {smape:.5f}', plot_lr=True, n_epochs=110)\n            \n            plt.figure(figsize=(10, 10))\n            plt.scatter(y_va, y_va_pred, s=1, color='r')\n            plt.scatter(np.log(y_va), np.log(y_va_pred), s=1, color='g')\n            plt.plot([plt.xlim()[0], plt.xlim()[1]], [plt.xlim()[0], plt.xlim()[1]], '--', c='k')\n            plt.gca().set_aspect('equal')\n            plt.xlabel('y_true')\n            plt.ylabel('y_pred')\n            plt.title('OOF PREDICTIONS')\n            plt.show()\n            \n    return preproc, model\nnp.random.seed(2022)\ntotal_start_time = datetime.now()\n\nhistory_list, score_list, test_pred_list = [], [], []\n\noof_list = [np.full((len(train_df), 1), -1.0, dtype='float32') for run in range(RUNS)]\nfor run in range(RUNS):\n    preproc, model = None, None\n    kf = GroupKFold(n_splits=4)\n    \n    for fold, (train_idx, val_idx) in enumerate(kf.split(train_df, groups=train_df.date.dt.year)):\n        X_tr = train_df.iloc[train_idx]\n        X_va = train_df.iloc[val_idx]\n        print(f'FOLD {run}.{fold}')\n        preproc, model = fit_model(X_tr, X_va)\n        \n        if INFERENCE:\n            test_df_f = pd.DataFrame(preproc.transform(test_df[features]), columns=features, index=test_df.index)\n            test_pred_list.append(np.exp(model.predict([test_df_f[wd_features], test_df_f[other_features]])))\nprint(f'AVERAGE SMAPE: {sum(score_list) / len(score_list):.5f}')\nwith open('oof.pickle', 'wb') as handle:\n    pickle.dump(oof_list, handle)\n    \nif RUNS > 1:\n    y_va = train_df.num_sold\n    print(f'ENSEMBLE SMAPE: {np.mean(smape_loss(y_va, sum(oof_list).ravel() / len(oof_list))):.5f}')\n    \nprint(f'TOTAL TIME: {str(datetime.now() - total_start_time)[:-7]}')","metadata":{"execution":{"iopub.status.busy":"2022-01-28T17:43:50.193576Z","iopub.execute_input":"2022-01-28T17:43:50.193877Z","iopub.status.idle":"2022-01-28T17:49:00.819868Z","shell.execute_reply.started":"2022-01-28T17:43:50.193844Z","shell.execute_reply":"2022-01-28T17:49:00.819194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"w = pd.Series(model.get_layer(index=-1).get_weights()[0].ravel(), index=features)\nws = w * preproc.named_steps['minmaxscaler'].scale_\n\ndef plot_feature_weights_numbered(prefix):\n    prefix_features = [f for f in features if f.startswith(prefix)]\n    plt.figure(figsize=(12, 2))\n    plt.bar([int(f[len(prefix):]) for f in prefix_features], ws[prefix_features])\n    plt.title(f'FEATURE WEIGHTS FOR {prefix}')\n    plt.ylabel('WEIGHT')\n    plt.xlabel('DAY')\n    plt.show()\n    \nplot_feature_weights_numbered('easter')","metadata":{"execution":{"iopub.status.busy":"2022-01-28T17:49:00.820774Z","iopub.execute_input":"2022-01-28T17:49:00.82094Z","iopub.status.idle":"2022-01-28T17:49:00.999528Z","shell.execute_reply.started":"2022-01-28T17:49:00.820917Z","shell.execute_reply":"2022-01-28T17:49:00.998375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_five_years_combination(engineer, country='Norway', store='KaggleMart', product='Kaggle Hat'):\n    demo_df = pd.DataFrame({'row_id': 0,\n                            'date': pd.date_range('2015-01-01', '2019-12-31', freq='D'),\n                            'country': country,\n                            'store': store,\n                            'product': product})\n    demo_df.set_index('date', inplace=True, drop=False)\n    demo_df = engineer(demo_df)\n    demo_df_f = pd.DataFrame(preproc.transform(demo_df[features]), columns=features, index=demo_df.index)\n    demo_df['num_sold'] = np.exp(model.predict([demo_df_f[features]]))\n    plt.figure(figsize=(20, 6))\n    plt.plot(np.arange(len(demo_df)), demo_df.num_sold, label='prediction')\n    train_subset = train_df[(original_train_df.country == country) & (original_train_df.store == store) & (original_train_df['product'] == product)]\n    plt.scatter(np.arange(len(train_subset)), train_subset.num_sold, label='true', alpha=0.5, c='r', s=3)\n    plt.legend()\n    plt.title('PREDICTIONS AND TRUE NUM_SOLD IN FIVE YEARS')\n    plt.show()\n    \nplot_five_years_combination(engineer)","metadata":{"execution":{"iopub.status.busy":"2022-01-28T17:49:01.000659Z","iopub.execute_input":"2022-01-28T17:49:01.000867Z","iopub.status.idle":"2022-01-28T17:49:01.692811Z","shell.execute_reply.started":"2022-01-28T17:49:01.000845Z","shell.execute_reply":"2022-01-28T17:49:01.69196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"RETRAIN_RUNS = 33\nif RETRAIN_RUNS > 1:\n    total_start_time = datetime.now()\n    test_pred_list = []\n    for run in range(RETRAIN_RUNS):\n        preproc, model = None, None\n        print(f'RETRAINING {run}')\n        preproc, model = fit_model(train_df)\n        print(f\"TRAINING LOSS:            {history_list[-1]['loss'][-1]:.6f}\")\n        test_df_f = pd.DataFrame(preproc.transform(test_df[features]), columns=features, index=test_df.index)\n        test_pred_list.append(np.exp(model.predict([test_df_f[features]])))\n    print(f'TOTAL TIME: {str(datetime.now() - total_start_time)[:-7]}')","metadata":{"execution":{"iopub.status.busy":"2022-01-28T17:49:01.693737Z","iopub.execute_input":"2022-01-28T17:49:01.693945Z","iopub.status.idle":"2022-01-28T17:55:10.556082Z","shell.execute_reply.started":"2022-01-28T17:49:01.693914Z","shell.execute_reply":"2022-01-28T17:55:10.555371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = None\nif len(test_pred_list) > 0:\n    print(f'ENSEMBLING {len(test_pred_list)} PREDICTIONS...')\n    sub = original_test_df[['row_id']].copy()\n    sub['num_sold'] = sum(test_pred_list) / len(test_pred_list)\n    sub.to_csv('submission_keras_02.csv', index=False)\n    \n    plt.figure(figsize=(16, 3))\n    plt.hist(train_df['num_sold'], bins=np.linspace(0, 3000, 201), density=True, label='TRAINING')\n    plt.hist(sub['num_sold'], bins=np.linspace(0, 3000, 201), density=True, rwidth=0.5, label='TEST PREDICTIONS')\n    plt.xlabel('NUM_SOLD')\n    plt.ylabel('FREQUENCY')\n    plt.legend()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-28T17:55:10.557213Z","iopub.execute_input":"2022-01-28T17:55:10.55738Z","iopub.status.idle":"2022-01-28T17:55:11.725305Z","shell.execute_reply.started":"2022-01-28T17:55:10.557357Z","shell.execute_reply":"2022-01-28T17:55:11.724449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_rounded = None\nif sub is not None:\n    sub_rounded = sub.copy()\n    sub_rounded['num_sold'] = sub_rounded['num_sold'].round()\n    sub_rounded.to_csv('submission_keras_rounded_02.csv', index=False)\n    \nsub_rounded","metadata":{"execution":{"iopub.status.busy":"2022-01-28T17:55:11.726643Z","iopub.execute_input":"2022-01-28T17:55:11.726943Z","iopub.status.idle":"2022-01-28T17:55:11.761652Z","shell.execute_reply.started":"2022-01-28T17:55:11.726908Z","shell.execute_reply":"2022-01-28T17:55:11.760814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_preds = np.exp(model.predict([preproc.transform(train_df[features])])).ravel()\nresiduals = (train_df.num_sold - train_preds) / (train_df.num_sold + train_preds) * 200\n\nplt.figure(figsize=(20, 6))\nplt.scatter(residuals.index, residuals, s=1, c='m')\nplt.hlines([0], 0, residuals.index.max(), color='y')\nplt.title('RESIDUALS FOR ALL 26298 TRAINING SAMPLES')\nplt.ylabel('RESIDUAL {percent}')\nplt.xlabel('ROW_ID')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-28T17:55:11.762839Z","iopub.execute_input":"2022-01-28T17:55:11.763067Z","iopub.status.idle":"2022-01-28T17:55:12.488782Z","shell.execute_reply.started":"2022-01-28T17:55:11.763001Z","shell.execute_reply":"2022-01-28T17:55:12.488213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mu, std = scipy.stats.norm.fit(residuals)\n\nplt.figure(figsize=(20, 4))\nplt.hist(residuals, bins=100, color='g', density=True)\nx = np.linspace(plt.xlim()[0], plt.xlim()[1], 200)\nplt.plot(x, scipy.stats.norm.pdf(x, mu, std), 'r', linewidth=2)\nplt.title(f'HISTOGRAM OF RESIDUALS: MEAN = {residuals.mean():.4f}, '\n          f'$\\sigma = {residuals.std():.1f}$, SMAPE = {residuals.abs().mean():.5f}')\nplt.xlabel('RESIDUALS {percent}')\nplt.ylabel('DENSITY')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-28T17:55:12.491312Z","iopub.execute_input":"2022-01-28T17:55:12.49178Z","iopub.status.idle":"2022-01-28T17:55:13.018548Z","shell.execute_reply.started":"2022-01-28T17:55:12.491747Z","shell.execute_reply":"2022-01-28T17:55:13.017496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_unexplained(residuals, groups, labels=None, label_z_score=False, title=None):\n    residuals_grouped = residuals.groupby(groups)\n    means = residuals_grouped.mean()\n    counts = residuals_grouped.count()\n    z_score = np.sqrt(counts) * means / residuals.std()\n    z_threshold = scipy.stats.norm.ppf(1 - 0.25 / len(means))\n    m_threshold = z_threshold * residuals.std() / np.sqrt(counts.mean())\n    outliers = np.abs(z_score) > z_threshold\n    \n    plt.figure(figsize=(17, 4))\n    plt.hlines([-z_threshold, +z_threshold] if label_z_score else [-m_threshold, +m_threshold], 0, len(means)-1, color='k')\n    plt.bar(range(len(means)), z_score if label_z_score else means,\n            color=outliers.apply(lambda b: 'r' if b else 'g'), width=0.6)\n    if labels is not None:\n        plt.xticks(ticks=range(len(means)), labels=labels)\n        plt.ylabel('Z SCORE' if label_z_score else 'PERCENT')\n        plt.title(title)\n        plt.show()\n        \nplot_unexplained(residuals, [train_df.date.dt.day], labels=np.arange(1, 32), title='RESIDUALS FOR 31 DAYS MONTH')\nplot_unexplained(residuals, [(train_df.date.dt.dayofyear) // 7], labels=None, title='RESIDUALS FOR 53 WEEKS YEAR')\nplot_unexplained(residuals, [train_df.date.dt.month], labels='JFMAMJJASOND', title='RESIDUALS FOR All 12 MONTHS')","metadata":{"execution":{"iopub.status.busy":"2022-01-28T17:55:13.020256Z","iopub.execute_input":"2022-01-28T17:55:13.02044Z","iopub.status.idle":"2022-01-28T17:55:13.808425Z","shell.execute_reply.started":"2022-01-28T17:55:13.020418Z","shell.execute_reply":"2022-01-28T17:55:13.807526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_unexplained(residuals, [(train_df.date - train_df.date.min()).dt.days // 7],\n                 labels=None,\n                 title='MEAN RESIDUALS OF ALL 213 WEEKS OF TRAINING DATA')\nplot_unexplained(residuals, [train_df.date.dt.year, train_df.date.dt.month],\n                 labels='JFMAMJJASOND' * 4,\n                 title='MEAN RESIDUALS OF ALL 48 MONTHS')\nplot_unexplained(residuals, [train_df.date.dt.year, train_df.date.dt.quarter],\n                 labels=[f'{q//4}Q{q%4+1}' for q in range(60, 76)],\n                 title='MEAN RESIDUALS OF ALL 16 QUARTERS')","metadata":{"execution":{"iopub.status.busy":"2022-01-28T17:55:13.809648Z","iopub.execute_input":"2022-01-28T17:55:13.809905Z","iopub.status.idle":"2022-01-28T17:55:15.399562Z","shell.execute_reply.started":"2022-01-28T17:55:13.809876Z","shell.execute_reply":"2022-01-28T17:55:15.39869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_unexplained(residuals, groups, labels=None, label_z_score=False, title=None, label=None):\n    residuals_grouped = residuals.groupby(groups)\n    means = residuals_grouped.mean()\n    plt.plot(range(len(means)), z_score if label_z_score else means,\n             label=label)\n    if labels is not None:\n        plt.xticks(ticks=range(len(means)), labels=labels)\n        \nplt.figure(figsize=(17,6))\nplt.subplot(2, 1, 1)\nfor i, c in enumerate(original_train_df.country.unique()):\n    selection = original_train_df.country == c\n    plot_unexplained(residuals[selection], [train_df.date.dt.year[selection], train_df.date.dt.month[selection]],\n                     labels='JFMAMJJASOND' * 4,\n                     title='MEAN RESIDUALS OF ALL 48 MONTHS',\n                    label=c)\nplt.legend()\nplt.subplot(2, 1, 2)\nfor i, c in enumerate(original_train_df['product'].unique()):\n    selection = original_train_df['product'] == c\n    plot_unexplained(residuals[selection], [train_df.date.dt.year[selection], train_df.date.dt.month[selection]],\n                     labels='JFMAMJJASOND' * 4,\n                     title='MEAN RESIDUALS OF ALL 48 MONTHS',\n                     label=c)\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-28T17:55:15.40097Z","iopub.execute_input":"2022-01-28T17:55:15.40117Z","iopub.status.idle":"2022-01-28T17:55:16.150264Z","shell.execute_reply.started":"2022-01-28T17:55:15.401144Z","shell.execute_reply":"2022-01-28T17:55:16.149466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}