{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**대회 목표:** \n\n대회의 데이터셋은 자유 이용 저작물인 으스스한 작가들이 쓴 소설 작품 텍스트입니다:  \n\n1. Edgar Allan Poe (EAP)\n2. HP Lovecraft (HPL)\n3. Mary Wollstonecraft Shelley (MWS)\n\n목표은 테스트 셋의 문장들의 작가를 정확하게 구분하는 것입니다.\n\n**노트북의 목표:**\n\n이 노트북에서는 spooky 작가들을 구분하는 데에 도움을 주는 다양한 feature들을 만들기를 시도할 것입니다.  \n\n첫번째 단계로, 우리는 피처 엔지니어링 파트를 깊게 공부하기 전에 기본적인 데이터 시각화와 전처리를 할 것입니다.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nimport xgboost as xgb\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn import ensemble, metrics, model_selection, naive_bayes\ncolor = sns.color_palette()\n\n%matplotlib inline\n\neng_stopwords = set(stopwords.words('english'))\npd.options.mode.chained_assignment = None","metadata":{"execution":{"iopub.status.busy":"2021-07-30T06:49:21.39025Z","iopub.execute_input":"2021-07-30T06:49:21.390807Z","iopub.status.idle":"2021-07-30T06:49:21.487416Z","shell.execute_reply.started":"2021-07-30T06:49:21.390761Z","shell.execute_reply":"2021-07-30T06:49:21.486491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv('../input/spooky-author-identification/train.zip')\ntest_df = pd.read_csv('../input/spooky-author-identification/test.zip')\nprint(f\"Number of rows in train dataset : {train_df.shape[0]}\")\nprint(f\"Number of rows in test dataset : {test_df.shape[0]}\")","metadata":{"execution":{"iopub.status.busy":"2021-07-30T08:08:41.289373Z","iopub.execute_input":"2021-07-30T08:08:41.289746Z","iopub.status.idle":"2021-07-30T08:08:41.421858Z","shell.execute_reply.started":"2021-07-30T08:08:41.289714Z","shell.execute_reply":"2021-07-30T08:08:41.420654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-30T08:08:41.617515Z","iopub.execute_input":"2021-07-30T08:08:41.61787Z","iopub.status.idle":"2021-07-30T08:08:41.630766Z","shell.execute_reply.started":"2021-07-30T08:08:41.617842Z","shell.execute_reply":"2021-07-30T08:08:41.629968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"클래스가 균형있게 분포했는지를 확인하고자 각 작가들의 출현빈도를 확인해보겠습니다.","metadata":{}},{"cell_type":"code","source":"cnt_srs = train_df['author'].value_counts()\n\nplt.figure(figsize=(8,4))\nsns.barplot(cnt_srs.index, cnt_srs.values, alpha=0.8)\nplt.ylabel('Number of Occurences', fontsize=12)\nplt.xlabel('Author Name', fontsize=12)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-30T08:08:42.139613Z","iopub.execute_input":"2021-07-30T08:08:42.14021Z","iopub.status.idle":"2021-07-30T08:08:42.277693Z","shell.execute_reply.started":"2021-07-30T08:08:42.140167Z","shell.execute_reply":"2021-07-30T08:08:42.27689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"아주 좋네요. 클래스 불균형이 있는 것처럼 보이지는 않습니다. 이제 가능하다면 각 작가들의 문체를 확인해봅시다.","metadata":{}},{"cell_type":"code","source":"grouped_df = train_df.groupby('author')\n\nfor name, group in grouped_df:\n    print(f\"Author name : {name}\")\n    cnt = 0\n    \n    for idx, row in group.iterrows():\n        print(row['text'])\n        cnt += 1\n        \n        if cnt == 5:\n            break\n    \n    print(\"\\n\")","metadata":{"execution":{"iopub.status.busy":"2021-07-30T08:08:42.663075Z","iopub.execute_input":"2021-07-30T08:08:42.663657Z","iopub.status.idle":"2021-07-30T08:08:42.681959Z","shell.execute_reply.started":"2021-07-30T08:08:42.663613Z","shell.execute_reply":"2021-07-30T08:08:42.680823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"테스트 데이터를 보면 몇개의 꽤 특별한 문자들을 볼 수 있습니다. 그래서 이 특별한 문자들의 횟수는 좋은 feature가 될 수 있습니다. 나중에 이 문자들을 처리하도록 하겠습니다.\n\n그 외에는 따로 단서가 있지는 않습니다... 만약에 흥미로운 스타일(만들 수 있는 feature)이 있다면 댓글로 달아주세요!\n\n**Feature Engineering (특징 추출):**\n\n이제 feature engineerin을 해보도록 하겠습니다. 이 과정은 2개의 주요 파트로 구성됩니다:\n\n1. Meta fetures - 단어의 수, 불용어의 수, 구두점의 수와 같은 텍스트로부터 뽑아낼 수 있는 특징들입니다.\n2. Text based features - 빈도수, svd, word2vec등과 같은 텍스트/단어를 직접적으로 기반으로 하는 특징들입니다.\n\n**Meta Features:**\n\n우리는 meata 특징들을 만들고 얼마나 그것들이 작가들을 잘 예측하는지 보겠습니다.  \nfeature의 종류는 아래와 같습니다:\n\n1. 텍스트에 있는 모든 단어의 수\n2. 텍스트에 있는 중복을 제외한 모든 단어의 수\n3. 텍스트에 있는 문장의 길이\n4. 불용어의 수\n5. 구두점의 수\n6. 대문자 단어들의 수\n7. 제목의 역할을 하는 단어의 수\n8. 단어들의 평균길이","metadata":{}},{"cell_type":"code","source":"## Number of words in the text ##\ntrain_df['num_words'] = train_df['text'].apply(lambda x: len(str(x).split()))\ntest_df['num_words'] = test_df['text'].apply(lambda x: len(str(x).split()))\n\n## Number of unique words in the text ##\ntrain_df['num_unique_words'] = train_df['text'].apply(lambda x: len(set(str(x).split())))\ntest_df['num_unique_words'] = test_df['text'].apply(lambda x: len(set(str(x).split())))\n\n## Number of characters in the text ##\ntrain_df['num_chars'] = train_df['text'].apply(lambda x: len(str(x)))\ntest_df['num_chars'] = test_df['text'].apply(lambda x: len(str(x)))\n\n## Number of soptwords in the text ##\ntrain_df['num_stopwrods'] = train_df['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\ntest_df['num_stopwrods'] = test_df['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n\n## Number of punctuations in the text ##\ntrain_df['num_punctuations'] = train_df['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\ntest_df['num_punctuations'] = test_df['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n\n## Number of title case words in the text ##\ntrain_df['num_words_upper'] = train_df['text'].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\ntest_df['num_words_upper'] = test_df['text'].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n\n## Average length of the words in the text ##\ntrain_df['mean_word_len'] = train_df['text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\ntest_df['mean_word_len'] = test_df['text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))","metadata":{"execution":{"iopub.status.busy":"2021-07-30T08:11:34.649195Z","iopub.execute_input":"2021-07-30T08:11:34.649574Z","iopub.status.idle":"2021-07-30T08:11:35.8861Z","shell.execute_reply.started":"2021-07-30T08:11:34.649531Z","shell.execute_reply":"2021-07-30T08:11:35.884743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"이제 새로 만든 변수들이 예측을 할때 도움이 되는지 확인하기 위해 그래프를 그려봅시다","metadata":{"execution":{"iopub.status.busy":"2021-07-30T07:43:20.670768Z","iopub.execute_input":"2021-07-30T07:43:20.671322Z","iopub.status.idle":"2021-07-30T07:43:20.678619Z","shell.execute_reply.started":"2021-07-30T07:43:20.671277Z","shell.execute_reply":"2021-07-30T07:43:20.677767Z"}}},{"cell_type":"code","source":"train_df['num_words'].loc[train_df['num_words']>80] = 80 # truncation for better visuals\nplt.figure(figsize=(12, 8))\nsns.violinplot(x='author', y='num_words', data=train_df)\nplt.xlabel('Author Name', fontsize=12)\nplt.ylabel('Number of words in text', fontsize=12)\nplt.title('Number of words by author', fontsize=15)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-30T08:11:42.419013Z","iopub.execute_input":"2021-07-30T08:11:42.419363Z","iopub.status.idle":"2021-07-30T08:11:42.66936Z","shell.execute_reply.started":"2021-07-30T08:11:42.419333Z","shell.execute_reply":"2021-07-30T08:11:42.668314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"EAP가 MWS와 HPL보다 모든 단어의 수가 약간 적은 것을 볼 수 있습니다.","metadata":{}},{"cell_type":"code","source":"train_df['num_punctuations'].loc[train_df['num_punctuations']>10] = 10 # truncation for better visuals\nplt.figure(figsize=(12, 8))\nsns.violinplot(x='author', y='num_punctuations', data=train_df)\nplt.xlabel('Author Name', fontsize=12)\nplt.ylabel('Number of punctuations in text', fontsize=12)\nplt.title('Number of punctuations by author', fontsize=15)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-30T08:11:43.414952Z","iopub.execute_input":"2021-07-30T08:11:43.415471Z","iopub.status.idle":"2021-07-30T08:11:43.69142Z","shell.execute_reply.started":"2021-07-30T08:11:43.415392Z","shell.execute_reply":"2021-07-30T08:11:43.690207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"이건 다소 쓸모가 있어보입니다. 이제 feature에 기반한 몇 텍스트를 만드는데 집중해봅시다.\n\n우선 이 meta 특징들이 얼마나 도움이 되는지 확인하고자 기본적인 모델을 만들어봅시다.","metadata":{}},{"cell_type":"code","source":"## Prepare the data for modeling ##\nauthor_mapping_dict = {'EAP':0, 'HPL':1, 'MWS':2}\ntrain_y = train_df['author'].map(author_mapping_dict)\ntrain_id = train_df['id'].values\ntest_id = test_df['id'].values\n\n### recompute the truncated variables again ###\ntrain_df['num_words'] = train_df['text'].apply(lambda x: len(str(x).split()))\ntest_df['num_words'] = test_df['text'].apply(lambda x: len(str(x).split()))\ntrain_df['mean_word_len'] = train_df['text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\ntest_df['mean_word_len'] = test_df['text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n\ncols_to_drop = ['id', 'text']\ntrain_X = train_df.drop(cols_to_drop+['author'], axis=1)\ntest_X = test_df.drop(cols_to_drop, axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T08:29:25.293529Z","iopub.execute_input":"2021-07-30T08:29:25.294005Z","iopub.status.idle":"2021-07-30T08:29:25.829717Z","shell.execute_reply.started":"2021-07-30T08:29:25.293972Z","shell.execute_reply":"2021-07-30T08:29:25.828792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"단순히 이 meta 특징들만 사용해서 간단한 XGBoost model을 훈련하겠습니다.","metadata":{}},{"cell_type":"code","source":"def runXGB(train_X, train_y, test_X, test_y=None, test_X2=None, seed_val=0, child=1, colsample=0.3):\n    param = {}\n    param['objective'] = 'multi:softprob'\n    param['eta'] = 0.1\n    param['max_depth'] = 3\n    param['silent'] = 1\n    param['num_class'] = 3\n    param['eval_metric'] = 'mlogloss'\n    param['min_child_weight'] = child\n    param['subsample'] = 0.8\n    param['colsample_bytree'] = colsample\n    param['seed'] = seed_val\n    num_rounds = 2000\n    \n    plst = list(param.items())\n    xgtrain = xgb.DMatrix(train_X, label=train_y)\n    \n    if test_y is not None:\n        xgtest = xgb.DMatrix(test_X, label=test_y)\n        watchlist = [(xgtrain, 'train'), (xgtest, 'test')]\n        model = xgb.train(plst, xgtrain, num_rounds, watchlist, early_stopping_rounds=50, verbose_eval=20)\n    else:\n        xgtest = xgb.DMatrix(test_X)\n        model = xgb.train(plst, xgtrain, num_rounds)\n        \n    pred_test_y = model.predict(xgtest, ntree_limit=model.best_ntree_limit)\n    \n    if test_X2 is not None:\n        xgtest2 = xgb.DMatrix(test_X2)\n        pred_test_y2 = model.predict(xgtest2, ntree_limit=model.best_ntree_limit)\n    \n    return pred_test_y, pred_test_y2, model","metadata":{"execution":{"iopub.status.busy":"2021-07-30T08:43:17.213197Z","iopub.execute_input":"2021-07-30T08:43:17.213547Z","iopub.status.idle":"2021-07-30T08:43:17.22216Z","shell.execute_reply.started":"2021-07-30T08:43:17.213516Z","shell.execute_reply":"2021-07-30T08:43:17.221032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"커널의 런타임을 위해 우리는 k-fold 교차검증 점수를 첫번째 fold만 체크할 수 있습니다. 로컬 환경에서 돌린다면 `break`를 제거하고 돌려주세요","metadata":{}},{"cell_type":"code","source":"kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\ncv_scores = []\npred_full_test = 0\npred_train = np.zeros([train_df.shape[0], 3])\n\nfor dev_index, val_index in kf.split(train_X):\n    dev_X, val_X = train_X.loc[dev_index], train_X.loc[val_index]\n    dev_y, val_y = train_y[dev_index], train_y[val_index]\n    pred_val_y, pred_test_y, model = runXGB(dev_X, dev_y, val_X, val_y, test_X, seed_val=0)\n    pred_full_test = pred_full_test + pred_test_y\n    pred_train[val_index, :] = pred_val_y\n    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n    break\n\nprint(f'cv scores : ', cv_scores)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T08:45:00.602854Z","iopub.execute_input":"2021-07-30T08:45:00.603416Z","iopub.status.idle":"2021-07-30T08:45:04.609871Z","shell.execute_reply.started":"2021-07-30T08:45:00.603367Z","shell.execute_reply":"2021-07-30T08:45:04.609161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"meta feature들을 활용하여 '1.0010'의 mlogloss를 얻었습니다. 나쁘지 않은 점수입니다. 이제 이중 어떤 feature가 중요한지 알아봅시다.","metadata":{}},{"cell_type":"code","source":"### Plot the important variables ###\nfig, ax = plt.subplots(figsize=(12, 12))\nxgb.plot_importance(model, max_num_features=50, height=0.8, ax=ax)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-30T08:47:02.632852Z","iopub.execute_input":"2021-07-30T08:47:02.633224Z","iopub.status.idle":"2021-07-30T08:47:02.933692Z","shell.execute_reply.started":"2021-07-30T08:47:02.633194Z","shell.execute_reply":"2021-07-30T08:47:02.932861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"문장의 길이, 단어의 평균 길이 그리고 중복되지 않은 단어의 개수는 top3의 변수로 확인됩니다. 이제 텍스트를 기반으로하는 feature들을 만들고 확인해봅시다.","metadata":{}},{"cell_type":"markdown","source":"**Text Based Features:**\n\n우리가 만들 수 있는 기본적인 feature는 텍스트에 있는 단어의 tf-idf 변수입니다. 이걸로 한번 시작해봅시다.","metadata":{}},{"cell_type":"code","source":"### Fit transform the tfidf vectorizer ###\ntfidf_vec = TfidfVectorizer(stop_words='english', ngram_range=(1, 3))\nfull_tfidf = tfidf_vec.fit_transform(train_df['text'].values.tolist() + test_df['text'].values.tolist())\ntrain_tfidf = tfidf_vec.transform(train_df['text'].values.tolist())\ntest_tfidf = tfidf_vec.transform(test_df['text'].values.tolist())","metadata":{"execution":{"iopub.status.busy":"2021-07-30T08:57:17.34662Z","iopub.execute_input":"2021-07-30T08:57:17.347185Z","iopub.status.idle":"2021-07-30T08:57:23.619652Z","shell.execute_reply.started":"2021-07-30T08:57:17.347136Z","shell.execute_reply":"2021-07-30T08:57:23.618653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"이제 우리는 tfidf 벡터를 얻었습니다만 여기엔 까다로운 부분이 있습니다. tfidf의 결과값은 희소 행렬이어서 만약 다른 밀집 특징(feature)들과 사용한다면 2가지 선택지가 존재합니다.\n\n1. tfidf vectorizerfhqnxj 상위 'n'개의 feature들(시스템 구성에 따라 다름)을 선택하고 밀집 형태로 변환하여 다른 feature들과 병합할 수 있습니다.\n2. 모델을 단지 희소 특징들을 사용하면 만들고 다른 밀집 특징들 중 하나를 예측에 사용합니다.\n\n데이터셋에 따르면 둘 중 한개는 좋은 성능을 보일 것입니다. 우리는 tfidf의 모든 특징을 사용하는 아주 좋은 채점방식이 있으므로 2번째 접근법을 사용하겠습니다.\n\n또한 Naive Bayes가 이 데이터셋에서 더 좋은 성능을 보이는 것 같습니다. 그래서 우리는 학습이 빠른 tfidf 특징을 사용하는 naive bayes 모델을 설계할 수도 있습니다.","metadata":{}},{"cell_type":"code","source":"def runMNB(train_X, train_y, test_X, test_y, test_X2):\n    model = naive_bayes.MultinomialNB()\n    model.fit(train_X, train_y)\n    pred_test_y = model.predict_proba(test_X)\n    pred_test_y2 = model.predict_proba(test_X2)\n    \n    return pred_test_y, pred_test_y2, model","metadata":{"execution":{"iopub.status.busy":"2021-07-30T09:06:33.650885Z","iopub.execute_input":"2021-07-30T09:06:33.65124Z","iopub.status.idle":"2021-07-30T09:06:33.656931Z","shell.execute_reply.started":"2021-07-30T09:06:33.651209Z","shell.execute_reply":"2021-07-30T09:06:33.65581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Word Tfidf Vectorizer를 사용한 Naive Bayes**","metadata":{}},{"cell_type":"code","source":"cv_scores = []\npred_full_test = 0\npred_train = np.zeros([train_df.shape[0], 3])\nkf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n\nfor dev_index, val_index in kf.split(train_X):\n    dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n    dev_y, val_y = train_y[dev_index], train_y[val_index]\n    pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_tfidf)\n    pred_full_test = pred_full_test + pred_test_y\n    pred_train[val_index, :] = pred_val_y\n    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n\nprint(f'Mean cv score : {np.mean(cv_scores)}')\npred_full_test = pred_full_test / 5.","metadata":{"execution":{"iopub.status.busy":"2021-07-30T09:10:34.041923Z","iopub.execute_input":"2021-07-30T09:10:34.042363Z","iopub.status.idle":"2021-07-30T09:10:34.511036Z","shell.execute_reply.started":"2021-07-30T09:10:34.042329Z","shell.execute_reply":"2021-07-30T09:10:34.509947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"단지 tfidf vectorizer만 사용했는데도 0.842의 mlogloss를 얻었습니다. meta features보다 더 좋습니다. 이제 [혼동 행렬](https://ko.wikipedia.org/wiki/혼동_행렬)을 보겠습니다.","metadata":{}},{"cell_type":"code","source":"### Function to create confusion matrix ###\nimport itertools\nfrom sklearn.metrics import confusion_matrix\n\ndef plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applies by setting 'normalize=True'\n    \"\"\"\n    \n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        # print(\"Normalized confusionn matrix\")\n    #else:\n    #   print('Confusion matrix, without normalization')\n    \n    #print(cm)\n    \n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n    \n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    \n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                horizontalalignment='center',\n                color='white' if cm[i, j] > thresh else 'black')\n    \n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","metadata":{"execution":{"iopub.status.busy":"2021-07-30T09:51:42.447575Z","iopub.execute_input":"2021-07-30T09:51:42.447927Z","iopub.status.idle":"2021-07-30T09:51:42.456218Z","shell.execute_reply.started":"2021-07-30T09:51:42.447899Z","shell.execute_reply":"2021-07-30T09:51:42.455235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cnf_matrix = confusion_matrix(val_y, np.argmax(pred_val_y, axis=1))\nnp.set_printoptions(precision=2)\n\n# Plot non-normalized confusion matrix\nplt.figure(figsize=(8, 8))\nplot_confusion_matrix(cnf_matrix, classes=['EAP', 'HPL', 'MWS'],\n                     title='Confusion matrix, without normalization')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-30T09:51:45.631931Z","iopub.execute_input":"2021-07-30T09:51:45.6323Z","iopub.status.idle":"2021-07-30T09:51:45.921861Z","shell.execute_reply.started":"2021-07-30T09:51:45.632265Z","shell.execute_reply":"2021-07-30T09:51:45.920857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"많은 인스턴스가 EAP라고 예측을 했고 해당 클래스에 크게 치우쳐져 있습니다.\n\n**word TFIDF를 사용한 SVD:**\n\ntfidf 벡터가 희소하기 때문에 정보를 압축하고 훨씬 간결하게 표현하는 또 다른 방법은 SVD입니다. 또한 일반적으로 SVD feature들은 과거 텍스트 기반의 대회에서 저에게 좋은 성능을 보여줬습니다. 그래서 우리는 단어 tfidf의 svd feature들을 만들고 우리가 만든 feature셋에 추가하겠습니다.","metadata":{}},{"cell_type":"code","source":"n_comp = 20\nsvd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')\nsvd_obj.fit(full_tfidf)\n\ntrain_svd = pd.DataFrame(svd_obj.transform(train_tfidf))\ntest_svd = pd.DataFrame(svd_obj.transform(test_tfidf))\n\ntrain_svd.columns = ['svd_word_'+str(i) for i in range(n_comp)]\ntest_svd.columns = ['svd_word_'+str(i) for i in range(n_comp)]\ntrain_df = pd.concat([train_df, train_svd], axis=1)\ntest_df = pd.concat([test_df, test_svd], axis=1)\n\ndel full_tfidf, train_tfidf, test_tfidf, train_svd, test_svd","metadata":{"execution":{"iopub.status.busy":"2021-07-30T09:59:37.486953Z","iopub.execute_input":"2021-07-30T09:59:37.487428Z","iopub.status.idle":"2021-07-30T09:59:40.234936Z","shell.execute_reply.started":"2021-07-30T09:59:37.487384Z","shell.execute_reply":"2021-07-30T09:59:40.233734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Word Count Vectorizer을 활용한 Naive Bayes:**","metadata":{}},{"cell_type":"code","source":"### Fit transform the count vectorizer ###\ntfidf_vec = CountVectorizer(stop_words='english', ngram_range=(1,3))\ntfidf_vec.fit(train_df['text'].values.tolist()+test_df['text'].values.tolist())\ntrain_tfidf = tfidf_vec.transform(train_df['text'].values.tolist())\ntest_tfidf = tfidf_vec.transform(test_df['text'].values.tolist())","metadata":{"execution":{"iopub.status.busy":"2021-07-30T10:02:00.726933Z","iopub.execute_input":"2021-07-30T10:02:00.72744Z","iopub.status.idle":"2021-07-30T10:02:05.780882Z","shell.execute_reply.started":"2021-07-30T10:02:00.727383Z","shell.execute_reply":"2021-07-30T10:02:05.779838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"이제 feature들에 기반한 count vectorizer를 사용한 다항 나이브 베이즈 모델을 만들어봅시다.","metadata":{}},{"cell_type":"code","source":"cv_scores = []\npred_full_test = 0\npred_train = np.zeros([train_df.shape[0], 3])\nkf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\nfor dev_index, val_index in kf.split(train_X):\n    dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n    dev_y, val_y = train_y[dev_index], train_y[val_index]\n    pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_tfidf)\n    pred_full_test = pred_full_test + pred_test_y\n    pred_train[val_index, :] = pred_val_y\n    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n    \nprint(f'Mean cv score : {np.mean(cv_scores)}')\npred_full_test = pred_full_test / 5.\n\n# add the predictions as new fetures #\ntrain_df['nb_cvec_eap'] = pred_train[:,0]\ntrain_df['nb_cvec_hpl'] = pred_train[:,1]\ntrain_df['nb_cvec_mws'] = pred_train[:,2]\ntest_df['nb_cvec_eap'] = pred_full_test[:,0]\ntest_df['nb_cvec_hpl'] = pred_full_test[:,1]\ntest_df['nb_cvec_mws'] = pred_full_test[:,2]","metadata":{"execution":{"iopub.status.busy":"2021-07-30T10:08:31.472898Z","iopub.execute_input":"2021-07-30T10:08:31.473253Z","iopub.status.idle":"2021-07-30T10:08:31.889528Z","shell.execute_reply.started":"2021-07-30T10:08:31.473221Z","shell.execute_reply":"2021-07-30T10:08:31.888482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cnf_matrix = confusion_matrix(val_y, np.argmax(pred_val_y, axis=1))\nnp.set_printoptions(precision=2)\n\n# Plot non-normalized confusion matrix\nplt.figure(figsize=(8, 8))\nplot_confusion_matrix(cnf_matrix, classes=['EAP', 'HPL', 'MWS'],\n                     title='Confusion matrix of NB on word count, without normalization')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-30T10:09:28.475702Z","iopub.execute_input":"2021-07-30T10:09:28.476053Z","iopub.status.idle":"2021-07-30T10:09:28.751237Z","shell.execute_reply.started":"2021-07-30T10:09:28.476021Z","shell.execute_reply":"2021-07-30T10:09:28.750023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"좋습니다! tfidf vectorizer대신 count vectorizer을 사용한 교차 검증이 mlogloss가 0.451이 나왔습니다.\n모델을 사용한 LB 점수는 0.468입니다. 또한 혼동 행렬을 보면 이전보다 훨씬 좋은 것을 알 수 있습니다.\n\n**Character Count Vectorizer를 사용한 Naive Bayes:**\n\n\"data eyeballin\"의 아이디어는 특별한 문자를 세는 것이 도움을 줄 것입니다. 특별한 문자를 세는 것 대신에 우리는 문자 수준에서 count vectorizer를 사용하여 일부 feature들을 얻을 수 있습니다. 다시 위에서 쓴 다항 나이브 베이즈를 실행해 보겠습니다.","metadata":{}},{"cell_type":"code","source":"### Fit transform the tfidf vectorizer ###\ntfidf_vec = CountVectorizer(ngram_range=(1,7), analyzer='char')\ntfidf_vec.fit(train_df['text'].values.tolist()+test_df['text'].values.tolist())\ntrain_tfidf = tfidf_vec.transform(train_df['text'].values.tolist())\ntest_tfidf = tfidf_vec.transform(test_df['text'].values.tolist())\n\ncv_scores = []\npred_full_test = 0\npred_train = np.zeros([train_df.shape[0], 3])\nkf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n\nfor dev_index, val_index in kf.split(train_X):\n    dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n    dev_y, val_y = train_y[dev_index], train_y[val_index]\n    pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_tfidf)\n    pred_full_test = pred_full_test + pred_test_y\n    pred_train[val_index, :] = pred_val_y\n    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n\nprint(f'Mean cv score : {np.mean(cv_scores)}')\npred_full_test = pred_full_test / 5.\n\n# add the predictions as new fetures #\ntrain_df['nb_cvec_char_eap'] = pred_train[:,0]\ntrain_df['nb_cvec_char_hpl'] = pred_train[:,1]\ntrain_df['nb_cvec_char_mws'] = pred_train[:,2]\ntest_df['nb_cvec_char_eap'] = pred_full_test[:,0]\ntest_df['nb_cvec_char_hpl'] = pred_full_test[:,1]\ntest_df['nb_cvec_char_mws'] = pred_full_test[:,2]","metadata":{"execution":{"iopub.status.busy":"2021-07-30T10:21:52.210394Z","iopub.execute_input":"2021-07-30T10:21:52.210935Z","iopub.status.idle":"2021-07-30T10:22:52.875307Z","shell.execute_reply.started":"2021-07-30T10:21:52.210904Z","shell.execute_reply":"2021-07-30T10:22:52.874294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"교차 검증 수치가 3.75로 아주 높네요. 하지만 이건 단어 수준의 feature들 대신에 다른 정보를 추가할 수 있으므로 마지막 모델에서 사용하겠습니다.","metadata":{}},{"cell_type":"code","source":"#### Fit transform the tfidf vectorizer ###\ntfidf_vec = TfidfVectorizer(ngram_range=(1,5), analyzer='char')\nfull_tfidf = tfidf_vec.fit_transform(train_df['text'].values.tolist()+test_df['text'].values.tolist())\ntrain_tfidf = tfidf_vec.transform(train_df['text'].values.tolist())\ntest_tfidf = tfidf_vec.transform(test_df['text'].values.tolist())\n\ncv_scores = []\npred_full_test = 0\npred_train = np.zeros([train_df.shape[0], 3])\nkf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n\nfor dev_index, val_index in kf.split(train_X):\n    dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n    dev_y, val_y = train_y[dev_index], train_y[val_index]\n    pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_tfidf)\n    pred_full_test = pred_full_test + pred_test_y\n    pred_train[val_index,:] = pred_val_y\n    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n\nprint(f'Mean cv score : {np.mean(cv_scores)}')\npred_full_test = pred_full_test / 5.\n\n# add the predictions as new fetures #\ntrain_df['nb_tfidf_char_eap'] = pred_train[:,0]\ntrain_df['nb_tfidf_char_hpl'] = pred_train[:,1]\ntrain_df['nb_tfidf_char_mws'] = pred_train[:,2]\ntest_df['nb_tfidf_char_eap'] = pred_full_test[:,0]\ntest_df['nb_tfidf_char_hpl'] = pred_full_test[:,1]\ntest_df['nb_tfidf_char_mws'] = pred_full_test[:,2]","metadata":{"execution":{"iopub.status.busy":"2021-07-30T10:28:09.713746Z","iopub.execute_input":"2021-07-30T10:28:09.714081Z","iopub.status.idle":"2021-07-30T10:28:42.431415Z","shell.execute_reply.started":"2021-07-30T10:28:09.714052Z","shell.execute_reply":"2021-07-30T10:28:42.430598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Character TFIDF를 사용한 SVD:**\n\n우리는 또한 tfidf feature들을 활용한 svd feature들을 만들 수 있고 모델링에 사용할 수 있습니다.","metadata":{}},{"cell_type":"code","source":"n_comp = 20\nsvd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')\nsvd_obj.fit(full_tfidf)\ntrain_svd = pd.DataFrame(svd_obj.transform(train_tfidf))\ntest_svd = pd.DataFrame(svd_obj.transform(test_tfidf))\n    \ntrain_svd.columns = ['svd_char_'+str(i) for i in range(n_comp)]\ntest_svd.columns = ['svd_char_'+str(i) for i in range(n_comp)]\ntrain_df = pd.concat([train_df, train_svd], axis=1)\ntest_df = pd.concat([test_df, test_svd], axis=1)\n\ndel full_tfidf, train_tfidf, test_tfidf, train_svd, test_svd","metadata":{"execution":{"iopub.status.busy":"2021-07-30T10:30:01.101822Z","iopub.execute_input":"2021-07-30T10:30:01.102338Z","iopub.status.idle":"2021-07-30T10:30:11.728598Z","shell.execute_reply.started":"2021-07-30T10:30:01.102298Z","shell.execute_reply":"2021-07-30T10:30:11.72785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**XGBoost 모델:**\n\n이 새로운 feature들 사용해서 우리는 xgboost를 다시 돌리고 결과를 평가하겠습니다.","metadata":{}},{"cell_type":"code","source":"cols_to_drop = ['id','text']\ntrain_X = train_df.drop(cols_to_drop+['author'], axis=1)\ntest_X = test_df.drop(cols_to_drop, axis=1)\n\nkf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\ncv_scores = []\npred_full_test = 0\npred_train = np.zeros([train_df.shape[0], 3])\nfor dev_index, val_index in kf.split(train_X):\n    dev_X, val_X = train_X.loc[dev_index], train_X.loc[val_index]\n    dev_y, val_y = train_y[dev_index], train_y[val_index]\n    pred_val_y, pred_test_y, model = runXGB(dev_X, dev_y, val_X, val_y, test_X, seed_val=0, colsample=0.7)\n    pred_full_test = pred_full_test + pred_test_y\n    pred_train[val_index,:] = pred_val_y\n    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n    break\nprint(\"cv scores : \", cv_scores)\n\nout_df = pd.DataFrame(pred_full_test)\nout_df.columns = ['EAP', 'HPL', 'MWS']\nout_df.insert(0, 'id', test_id)\nout_df.to_csv(\"sub_fe.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T10:35:36.130474Z","iopub.execute_input":"2021-07-30T10:35:36.130806Z","iopub.status.idle":"2021-07-30T10:36:04.683419Z","shell.execute_reply.started":"2021-07-30T10:35:36.130777Z","shell.execute_reply":"2021-07-30T10:36:04.682692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**검증 데이터 점수는 0.3044가 나왔고 LB 점수는 0.32xx가 나왔습니다.** 모든 fold에 대해서 돌린다면 더 좋은 점수를 얻을 것입니다. 이제 중요한 변수를 다시 확인해봅시다.","metadata":{}},{"cell_type":"code","source":"### Plot the important variables ###\nfig, ax = plt.subplots(figsize=(12, 12))\nxgb.plot_importance(model, max_num_features=50, height=0.8, ax=ax)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-30T10:37:25.184639Z","iopub.execute_input":"2021-07-30T10:37:25.185151Z","iopub.status.idle":"2021-07-30T10:37:26.004866Z","shell.execute_reply.started":"2021-07-30T10:37:25.185118Z","shell.execute_reply":"2021-07-30T10:37:26.003823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"기대했듯이 나이브 베이즈 feature가 top feature입니다. 이제 혼동 행렬을 통해 오분류 오차를 봅시다.","metadata":{}},{"cell_type":"code","source":"cnf_matrix = confusion_matrix(val_y, np.argmax(pred_val_y, axis=1))\nnp.set_printoptions(precision=2)\n\n# Plot non-normalized confusio matrix\nplt.figure(figsize=(8,8))\nplot_confusion_matrix(cnf_matrix, classes=['EAP', 'HPL', 'MWS'],\n                     title='Confusion matrix of XGB, without normalization')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-30T10:39:28.930264Z","iopub.execute_input":"2021-07-30T10:39:28.930615Z","iopub.status.idle":"2021-07-30T10:39:29.215436Z","shell.execute_reply.started":"2021-07-30T10:39:28.930586Z","shell.execute_reply":"2021-07-30T10:39:29.214343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"EAP와 MWS가 다른 것보다 더 많이 오분류된 것으로 보입니다. 우리는 잠재적으로 이런 쌍에 대해서 예측을 향상하는 feature를 만들 수 있었습니다.\n\n**이 FE 노트북의 다음 과정:**\n\n- feautre에 기반한 워드 임베딩을 사용하는 것\n- 가능한 다른 meta feture가 있는 경우\n- 문장의 감정","metadata":{}},{"cell_type":"markdown","source":"**더 향상하기 위한 아이디어**\n\n- tfidf와 count vectorizer의 파라미터 튜닝\n- XGB 모델과 나이브 베이즈 파라미터 튜닝\n- 다른 모델을 앙상블하거나 스태킹하기","metadata":{}}]}