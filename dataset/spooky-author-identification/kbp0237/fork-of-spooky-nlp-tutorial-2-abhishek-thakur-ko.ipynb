{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Approaching (Almost) Any NLP Problem on Kaggle","metadata":{"_uuid":"74a11d09-2bca-4585-99e7-71762a21400e","_cell_guid":"ba26cb0c-a3a7-4b1f-8fd9-f135d4e3a06e","trusted":true}},{"cell_type":"markdown","source":"우선 이 노트북은 [Abhishek Thakur의 노트북](https://www.kaggle.com/abhishek/approaching-almost-any-nlp-problem-on-kaggle/notebook)을 필사하였고 한국어로 번역하였습니다.","metadata":{}},{"cell_type":"markdown","source":"이 글에서 저는 캐글의 NLP문제의 접근법에 대해 얘기할 예정입니다.  \n예시로 저는 이 대회의 데이터를 사용하겠습니다.  \n처음으로 아주 간단한 첫 모델을 만들 것이고 그것을 다른 feature들을 통해 향상시킬 것입니다.  \n또한 어떻게 심층 신경망이 사용되는가와 이 글의 마지막에서는 일반적으로 여러 아이디어를 앙상블하는 것을 다루겠습니다.","metadata":{}},{"cell_type":"markdown","source":"Spooky NLP tutorial 2 - Abhishek Thakur Kor.ver### This covers:\n\n* tfidf\n* count features\n* logistic regressing\n* naive bayes\n* svm\n* xgboost\n* grid search\n* word vectors\n* LSTM\n* GRU\n* Ensembling\n\n*NOTE*: 이 노트북은 이 데이터 셋에서 높은 점수를 리더보드에 기록하는 것이 목적이 아닙니다.  \n하지만 적절하게 모든 과정들을 따라오신다면, 여러분은 일부 수정작업을 통해 높은 점수를 기록할 수 있을겁니다. ;)\n\n\n이제 각설하고, 제가 사용할 중요한 파이썬 모듈을 임포트하고 시작해봅시다!","metadata":{"_uuid":"ec292868-293d-449e-9f3c-0a6d7729e6ca","_cell_guid":"7cf6389e-fb2f-4a85-9ed3-4eedea5aa1dc","trusted":true}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport xgboost as xgb\nfrom tqdm import tqdm\nfrom sklearn.svm import SVC\nfrom keras.models import Sequential\nfrom keras.layers.recurrent import LSTM, GRU\nfrom keras.layers.core import Dense, Activation, Dropout\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.utils import np_utils\nfrom sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\nfrom keras.preprocessing import sequence, text\nfrom keras.callbacks import EarlyStopping\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')","metadata":{"_uuid":"ede953ef-9c79-4002-96c8-3bb5b24648eb","_cell_guid":"3ba5eb39-78d7-49d9-b66b-2589477992c8","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-26T16:27:47.372119Z","iopub.execute_input":"2021-07-26T16:27:47.37248Z","iopub.status.idle":"2021-07-26T16:27:47.407944Z","shell.execute_reply.started":"2021-07-26T16:27:47.37243Z","shell.execute_reply":"2021-07-26T16:27:47.407245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"이제 데이터셋을 불러옵시다.","metadata":{"_uuid":"b4e7f53f-2c81-4119-9cdc-46ce9c5d34aa","_cell_guid":"caaf298d-0b72-42c7-b048-d188de78f9b1","trusted":true}},{"cell_type":"code","source":"train = pd.read_csv('../input/spooky/train.csv')\ntest = pd.read_csv('../input/spooky/test.csv')\nsample = pd.read_csv('../input/spooky/sample_submission.csv')","metadata":{"_uuid":"8ca347da-2016-49c4-8e06-ae66aa5e17a0","_cell_guid":"e06f1787-18a0-48bd-8644-93524ce2e359","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-26T16:27:47.409342Z","iopub.execute_input":"2021-07-26T16:27:47.409785Z","iopub.status.idle":"2021-07-26T16:27:47.610678Z","shell.execute_reply.started":"2021-07-26T16:27:47.409628Z","shell.execute_reply":"2021-07-26T16:27:47.609883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"이번에는 데이터를 빠르게 확인해봅시다.","metadata":{"_uuid":"31a2d820-4452-476f-9edb-4158f71a20ef","_cell_guid":"e1c08ca6-fa5b-45d8-9283-8da281cc762e","trusted":true}},{"cell_type":"code","source":"train.head()","metadata":{"_uuid":"8d1df310-efeb-4253-9a1b-93bfb1c65b36","_cell_guid":"67ff64b1-e9af-4cdf-ac4a-4d5fbd930bf5","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-26T16:27:47.613387Z","iopub.execute_input":"2021-07-26T16:27:47.613623Z","iopub.status.idle":"2021-07-26T16:27:47.627759Z","shell.execute_reply.started":"2021-07-26T16:27:47.613579Z","shell.execute_reply":"2021-07-26T16:27:47.626973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head()","metadata":{"_uuid":"5d7857f0-57ce-4cb7-99ec-05e4bb976ae0","_cell_guid":"d7a0f826-04b6-49bb-93b9-87f2a2c15fb2","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-26T16:27:47.62925Z","iopub.execute_input":"2021-07-26T16:27:47.629682Z","iopub.status.idle":"2021-07-26T16:27:47.641293Z","shell.execute_reply.started":"2021-07-26T16:27:47.629505Z","shell.execute_reply":"2021-07-26T16:27:47.640486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample.head()","metadata":{"_uuid":"72b103f6-3b2c-42c3-a4f5-cd002fa37c3d","_cell_guid":"3a67c7f3-2310-4659-9e28-9efab86d0302","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-26T16:27:47.642618Z","iopub.execute_input":"2021-07-26T16:27:47.64296Z","iopub.status.idle":"2021-07-26T16:27:47.65626Z","shell.execute_reply.started":"2021-07-26T16:27:47.642893Z","shell.execute_reply":"2021-07-26T16:27:47.655584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"이 대회는 텍스트가 주어진 저자인 EAP, HPL, MWS를 예측하는 것이 목적입니다.  \n간단하게 말하면 서로 다른 3개의 클래스를 가진 텍스트 분류 문제입니다.  \n\n\n이 특정 문제에 대해, 캐글은 multi-calss log-loss 평가 지표를 사용합니다.  \n이것은 아래의 링크로 구현이 되어 있습니다.  \n[https://github.com/dnouri/nolearn/blob/master/nolearn/lasagne/util.py](https://github.com/dnouri/nolearn/blob/master/nolearn/lasagne/util.py)  \n\n$$ L_{\\log}(y, p) = -(y \\log (p) + (1 - y) \\log (1 - p)) $$","metadata":{"_uuid":"7e9b093a-4baa-4ac8-baa6-5731e4e2d171","_cell_guid":"2bce2650-e091-4e7a-91ac-3a4793f31522","trusted":true}},{"cell_type":"code","source":"def multiclass_logloss(actual, predicted, eps=1e-15):\n    \"\"\"Multi class version of Logarithmic Loss metric.\n    :param actual: Array containing the actual target classes\n    :param predicted: Matrix with class predictions, one probability per class\n    \"\"\"\n    # Convert 'actual' to a binary array if it's not already:\n    if len(actual.shape) == 1:\n        actual2 = np.zeros((actual.shape[0], predicted.shape[1]))\n        for i, val in enumerate(actual):\n            actual2[i, val] = 1\n        actual = actual2\n\n    clip = np.clip(predicted, eps, 1 - eps)\n    rows = actual.shape[0]\n    vsota = np.sum(actual * np.log(clip))\n    return -1.0 / rows * vsota","metadata":{"_uuid":"546d5673-b156-4646-932b-02ec12687fa2","_cell_guid":"da1e46e9-7653-4368-b69a-fbe36937f737","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-26T16:27:47.657465Z","iopub.execute_input":"2021-07-26T16:27:47.657768Z","iopub.status.idle":"2021-07-26T16:27:47.673088Z","shell.execute_reply.started":"2021-07-26T16:27:47.657708Z","shell.execute_reply":"2021-07-26T16:27:47.672114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"scikit-learn의 LabelEncoder를 활용하여 텍스트 라벨을 0, 1, 2의 정수로 변환하겠습니다.","metadata":{"_uuid":"6dce2c69-06d9-4866-b626-c36409a6fb38","_cell_guid":"cb8b9951-d61b-4412-bd4a-462713063436","trusted":true}},{"cell_type":"code","source":"lbl_enc = preprocessing.LabelEncoder()\ny = lbl_enc.fit_transform(train.author.values)","metadata":{"_uuid":"37b00387-8d42-463c-a82d-6bb3735038c0","_cell_guid":"f53f4243-3b1e-4213-9f8b-df713d0c23e9","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-26T16:27:47.674319Z","iopub.execute_input":"2021-07-26T16:27:47.674674Z","iopub.status.idle":"2021-07-26T16:27:47.69864Z","shell.execute_reply.started":"2021-07-26T16:27:47.674609Z","shell.execute_reply":"2021-07-26T16:27:47.698071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"더 진행하기 전에 데이터셋을 훈련집합과 검증집합으로 분리하는 것이 중요합니다.  \nscikit-learn모듈에서 `model_selection`의 `train_test_split`을 사용하면 됩니다.","metadata":{"_uuid":"af87a7bd-75d4-4b7d-b415-5690e8e9bc0f","_cell_guid":"9390e332-b92f-4a3f-8cb8-e683d093601a","trusted":true}},{"cell_type":"code","source":"xtrain, xvalid, ytrain, yvalid = train_test_split(train.text.values, y, \n                                                  stratify=y, \n                                                  random_state=42, \n                                                  test_size=0.1, shuffle=True)","metadata":{"_uuid":"06680957-5e57-4b5b-ba37-b78d61156690","_cell_guid":"26c541e5-ff5b-41d8-9a87-09ff911b9c7a","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-26T16:27:47.699722Z","iopub.execute_input":"2021-07-26T16:27:47.699999Z","iopub.status.idle":"2021-07-26T16:27:47.715092Z","shell.execute_reply.started":"2021-07-26T16:27:47.699943Z","shell.execute_reply":"2021-07-26T16:27:47.714519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(xtrain.shape)\nprint(xvalid.shape)","metadata":{"_uuid":"5d9d6240-0946-486e-b17e-d910b86b85d9","_cell_guid":"e4827abe-0a5f-46e5-b4ac-fdfe056c930a","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-26T16:27:47.716327Z","iopub.execute_input":"2021-07-26T16:27:47.716633Z","iopub.status.idle":"2021-07-26T16:27:47.722287Z","shell.execute_reply.started":"2021-07-26T16:27:47.716567Z","shell.execute_reply":"2021-07-26T16:27:47.721165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Building Basic Models\n\n이제 첫번째 모델을 만들어봅시다!  \n\n\n첫모델은 아주 간단한 로지스틱 회귀에 기반한 TF-IDF (Term Frequency - Inverse Document Frequency)입니다.","metadata":{"_uuid":"12ba095e-bf64-4126-b621-c41dc929d23f","_cell_guid":"c30730ed-d55c-48ad-b3fb-2a15342a997f","trusted":true}},{"cell_type":"code","source":"# Always start with these features. They work (almost) everytime!\ntfv = TfidfVectorizer(min_df=3,  max_features=None, \n            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n            ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1,\n            stop_words = 'english')\n\n# Fitting TF-IDF to both training and test sets (semi-supervised learning)\ntfv.fit(list(xtrain) + list(xvalid))\nxtrain_tfv =  tfv.transform(xtrain) \nxvalid_tfv = tfv.transform(xvalid)","metadata":{"_uuid":"c0fdc77b-8d71-44b0-9e45-33f446a62ce1","_cell_guid":"8ddabead-9730-4f22-adb6-724797742b4f","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-26T16:27:47.723906Z","iopub.execute_input":"2021-07-26T16:27:47.724246Z","iopub.status.idle":"2021-07-26T16:27:52.178132Z","shell.execute_reply.started":"2021-07-26T16:27:47.724181Z","shell.execute_reply":"2021-07-26T16:27:52.177466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fitting a simple Logistic Regression on Counts\nclf = LogisticRegression(C=1.0)\nclf.fit(xtrain_tfv, ytrain)\npredictions = clf.predict_proba(xvalid_tfv)\n\nprint('logloss: %0.3f ' %multiclass_logloss(yvalid, predictions))","metadata":{"execution":{"iopub.status.busy":"2021-07-26T16:27:52.181292Z","iopub.execute_input":"2021-07-26T16:27:52.181516Z","iopub.status.idle":"2021-07-26T16:27:52.392204Z","shell.execute_reply.started":"2021-07-26T16:27:52.181474Z","shell.execute_reply":"2021-07-26T16:27:52.391231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"첫번째 모델의 multiclass logloss값은 0.626를 기록했습니다.  \n\n\n하지만 저는 좀 더 좋은 점수가 욕심이 납니다. 그래서 다른 데이터를 사용해서 동일한 모델을 보겠습니다.  \n\nTF-IDF를 사용하는 대신 단어 수를 feature로 사용할 수 있습니다. 이 방법은 scikit-learn의 CountVectorizer을 활용하면 쉽게 사용할 수 있습니다.","metadata":{}},{"cell_type":"code","source":"ctv = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}',\n                     ngram_range=(1, 3), stop_words='english')\n\n# Fitting Count Vectorizer to both training and test sets (semi-supervised learning)\nctv.fit(list(xtrain) + list(xvalid))\nxtrain_ctv = ctv.transform(xtrain)\nxvalid_ctv = ctv.transform(xvalid)","metadata":{"_uuid":"6ebd1dec-08c3-4adb-8fca-85fc3d1713fb","_cell_guid":"d33526cf-44ff-4383-a244-113872133194","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-26T16:27:52.393687Z","iopub.execute_input":"2021-07-26T16:27:52.39402Z","iopub.status.idle":"2021-07-26T16:27:56.202052Z","shell.execute_reply.started":"2021-07-26T16:27:52.393952Z","shell.execute_reply":"2021-07-26T16:27:56.201348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fitting a simple Logistic Regression on Counts\nclf = LogisticRegression(C=1.0)\nclf.fit(xtrain_ctv, ytrain)\npredictions = clf.predict_proba(xvalid_ctv)\n\nprint('logloss: %0.3f ' %multiclass_logloss(yvalid, predictions))","metadata":{"_uuid":"08253373-27ee-46d2-94d4-ed99f438bd2e","_cell_guid":"f6efc6dd-c8fe-4829-8eb7-801478fc6013","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-26T16:27:56.203377Z","iopub.execute_input":"2021-07-26T16:27:56.203678Z","iopub.status.idle":"2021-07-26T16:27:57.628159Z","shell.execute_reply.started":"2021-07-26T16:27:56.203621Z","shell.execute_reply":"2021-07-26T16:27:57.627407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"와~! 첫 모델보다 0.1점을 향상시켰습니다!!  \n\n다음으로, 예전에 꽤 유명했던 모델인 Naive Bayes 모델을 사용해보겠습니다.  \n\n두 데이터셋에 대해서 naive bayes가 어떤 결과를 보여주는지 한번 봅시다!","metadata":{}},{"cell_type":"code","source":"# Fitting a simple Naive Bayes on TFIDF\nclf = MultinomialNB()\nclf.fit(xtrain_tfv, ytrain)\npredictions = clf.predict_proba(xvalid_tfv)\n\nprint('logloss: %0.3f ' %multiclass_logloss(yvalid, predictions))","metadata":{"_uuid":"7678c69a-f7f8-44ec-9f8c-229c462cf610","_cell_guid":"2f2e38f9-1152-4ea2-b33e-0e283c3ba4bb","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-26T16:27:57.629391Z","iopub.execute_input":"2021-07-26T16:27:57.629803Z","iopub.status.idle":"2021-07-26T16:27:57.646277Z","shell.execute_reply.started":"2021-07-26T16:27:57.629752Z","shell.execute_reply":"2021-07-26T16:27:57.645502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"괜찮은 결과입니다!  \n하지만 횟수를 사용한 로지스틱 회귀가 좀 더 낫습니다!  \n횟수를 사용한 데이터를 사용하면 어떤 일이 벌어질까요?","metadata":{}},{"cell_type":"code","source":"# Fitting a simple Naive Bayes on Counts\nclf = MultinomialNB()\nclf.fit(xtrain_ctv, ytrain)\npredictions = clf.predict_proba(xvalid_ctv)\n\nprint('logloss: %0.3f ' %multiclass_logloss(yvalid, predictions))","metadata":{"execution":{"iopub.status.busy":"2021-07-26T16:27:57.647455Z","iopub.execute_input":"2021-07-26T16:27:57.64776Z","iopub.status.idle":"2021-07-26T16:27:57.704656Z","shell.execute_reply.started":"2021-07-26T16:27:57.647697Z","shell.execute_reply":"2021-07-26T16:27:57.70384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"와! 오래된 방식이 여전히 잘 동작하는 것 같네요!  \n또 다른 예전 방식에는 SVM이 있습니다. 어떤 사람들은 SVM을 사랑할 정도로 정말 좋아합니다.  \n이제 한번 SVM을 데이터셋에 적용시켜봅시다.  \n\nSVM은 많은 시간이 걸리므로 SVM을 적용하기 전에 Singular Value Decomposition(특잇값 분해)을 활용하여 TF-IDF의 기능 수를 줄일 것입니다.  \n\n또한, SVM을 적용하기 전에 데이터를 반드시 표준화해야 합니다.","metadata":{}},{"cell_type":"code","source":"# Apply SVD, I chose 120 components. 120-200 components are good enough for SVM model.\nsvd = decomposition.TruncatedSVD(n_components=120)\nsvd.fit(xtrain_tfv)\nxtrain_svd = svd.transform(xtrain_tfv)\nxvalid_svd = svd.transform(xvalid_tfv)\n\n# Scale the data obtained from SVD. Renaming variable to reuse without scaling.\nscl = preprocessing.StandardScaler()\nscl.fit(xtrain_svd)\nxtrain_svd_scl = scl.transform(xtrain_svd)\nxvalid_svd_scl = scl.transform(xvalid_svd)","metadata":{"_uuid":"519144f6-cd74-4215-8e15-c14f34cdc304","_cell_guid":"6cae06aa-d082-46bf-a054-28d7d070828e","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-26T16:27:57.705936Z","iopub.execute_input":"2021-07-26T16:27:57.706314Z","iopub.status.idle":"2021-07-26T16:27:59.210462Z","shell.execute_reply.started":"2021-07-26T16:27:57.706251Z","shell.execute_reply":"2021-07-26T16:27:59.209727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"이제 SVM을 적용해봅시다. 다음 셀을 실행하고 여자친구나 남자친구랑 잠깐 밖에 산책하면서 자유를 느끼세요 :P","metadata":{}},{"cell_type":"code","source":"# Fitting a simple SVM\nclf = SVC(C=1.0, probability=True) # since we need probabilities\nclf.fit(xtrain_svd_scl, ytrain)\npredictions = clf.predict_proba(xvalid_svd_scl)\n\nprint('logloss: %0.3f ' %multiclass_logloss(yvalid, predictions))","metadata":{"_uuid":"4526d5be-e4c9-4e13-a3a6-ac1156d200b7","_cell_guid":"8130e255-d8cd-4902-a215-40e34e4847c1","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-26T16:27:59.211845Z","iopub.execute_input":"2021-07-26T16:27:59.212308Z","iopub.status.idle":"2021-07-26T16:33:29.110558Z","shell.execute_reply.started":"2021-07-26T16:27:59.212176Z","shell.execute_reply":"2021-07-26T16:33:29.022785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"시간이 다 되었네요! SVM은 이 데이터에 대해서 좋은 성능을 내지는 못하는 걸로 보입니다...  \n\n다음으로 넘어가기 전에, 캐글에서 유명한 알고리즘인 xgboost를 한번 봅시다!","metadata":{}},{"cell_type":"code","source":"# Fitting a simple xgboost on tf-idf\nclf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8,\n                       subsample=0.8, nthread=10, learnig_rate=0.1)\nclf.fit(xtrain_tfv.tocsc(), ytrain)\npredictions = clf.predict_proba(xvalid_tfv.tocsc())\n\nprint('logloss: %0.3f ' %multiclass_logloss(yvalid, predictions))","metadata":{"_uuid":"568dce41-9d7d-4f50-8573-dd7d8616dcec","_cell_guid":"696dfc4a-6b59-4370-abd8-c2d6e376f294","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-26T16:33:29.023608Z","iopub.status.idle":"2021-07-26T16:33:29.02402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fitting a simple xgboost on tf-idf\nclf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8,\n                       subsample=0.8, nthread=10, learnig_rate=0.1)\nclf.fit(xtrain_ctv.tocsc(), ytrain)\npredictions = clf.predict_proba(xvalid_ctv.tocsc())\n\nprint('logloss: %0.3f ' %multiclass_logloss(yvalid, predictions))","metadata":{"execution":{"iopub.status.busy":"2021-07-26T16:33:29.024809Z","iopub.status.idle":"2021-07-26T16:33:29.025363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fitting a simple xgboost on tf-idf\nclf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8,\n                       subsample=0.8, nthread=10, learnig_rate=0.1)\nclf.fit(xtrain_svd, ytrain)\npredictions = clf.predict_proba(xvalid_svd)\n\nprint('logloss: %0.3f ' %multiclass_logloss(yvalid, predictions))","metadata":{"execution":{"iopub.status.busy":"2021-07-26T16:33:29.026121Z","iopub.status.idle":"2021-07-26T16:33:29.026657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf = xgb.XGBClassifier(nthread=10)\nclf.fit(xtrain_svd, ytrain)\npredictions = clf.predict_proba(xvalid_svd)\n\nprint('logloss: %0.3f ' %multiclass_logloss(yvalid, predictions))","metadata":{"execution":{"iopub.status.busy":"2021-07-26T16:33:29.027395Z","iopub.status.idle":"2021-07-26T16:33:29.027829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"XGBoost는 별로 운이 없는 거 같네요! 하지만 그게 항상 맞는 말인건 아닙니다.  \n아직 하이퍼파라미터 최적화를 진행하지 않았기 때문입니다. 저는 게으르기 때문에 그걸 어떻게 하는지를 알려드리고 여러분 맘대로 하셔도 괜찮습니다! ;)  \n이건 다음 섹션에서 얘기하겠습니다.","metadata":{}},{"cell_type":"markdown","source":"## Grid Search\n\n하이퍼파라미터 최적화를 위한 기술입니다. 그렇게 효과적이진 않지만 사용하고자 하는 그리드를 알고 있다면 좋은 결과를 가져올 수 있습니다.  \n저는 이 글에서 일반적으로 사용해야하는 매개변수를 특정합니다:  \n[http://blog.kaggle.com/2016/07/21/approaching-almost-any-machine-learning-problem-abhishek-thakur/](http://blog.kaggle.com/2016/07/21/approaching-almost-any-machine-learning-problem-abhishek-thakur/)  \n이 파라미터들은 제가 자주 사용하는 파라미터들입니다.  \n효과적일 수도 있고 아닐 수도 있는 다른 하이퍼파라미터 최적화 방법들이 있습니다.  \n\n\n이 섹션에서는 저는 로지스틱 회귀를 활용한 그리드 서치에 대해 말씀드리겠습니다.  \n\n\n그리드 서치를 진행하기에 앞서 저는 scoring함수를 만들겠습니다. 이 함수는 scikit-learn의 `make_scorer`를 활용하여 만들어집니다.","metadata":{}},{"cell_type":"code","source":"mll_scorer = metrics.make_scorer(multiclass_logloss, greater_is_better=False, needs_proba=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-26T16:33:29.028623Z","iopub.status.idle":"2021-07-26T16:33:29.029122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"다음으로 저는 파이프라인이 필요합니다.  \n여기에선 임시적으로 파이프라인에 SVD, scaling 그리고 로지스틱 회귀를 사용하겠습니다.  \n파이프라인에는 1개의 모듈만 있는 것보단 여러개가 있는 것이 좋습니다.","metadata":{}},{"cell_type":"code","source":"# Initialize SVD\nsvd = TruncatedSVD()\n\n# Initialize the standard scaler\nscl = preprocessing.StandardScaler()\n\n# We will use logistic regression here.\nlr_model = LogisticRegression()\n\n# Create the pipeline\nclf = pipeline.Pipeline([('svd', svd),\n                         ('scl', scl),\n                         ('lr', lr_model)])","metadata":{"execution":{"iopub.status.busy":"2021-07-26T16:33:29.029942Z","iopub.status.idle":"2021-07-26T16:33:29.030479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"다음으로는 파라미터의 그리드가 필요합니다:","metadata":{}},{"cell_type":"code","source":"param_grid = {'svd__n_components': [120, 180],\n             'lr__C': [0.1, 1.0, 10],\n             'lr__penalty': ['l1', 'l2']}","metadata":{"execution":{"iopub.status.busy":"2021-07-26T16:33:29.031213Z","iopub.status.idle":"2021-07-26T16:33:29.03175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"SVD의 경우 120에서 180개의 요소를 평가하고 로지스틱 회귀의 경우 l1과 l2 패널티로 C값의 3가지 다른 값을 평가할 것입니다.  \n이제 이 매개변수들에서 그리드 서치를 할 수 있습니다.","metadata":{}},{"cell_type":"code","source":"# Initialize Grid Search Model\nmodel = GridSearchCV(estimator=clf, param_grid=param_grid, scoring=mll_scorer,\n                    verbose=10, n_jobs=-1, iid=True, refit=True, cv=2)\n\n# Fit Grid Search Model\nmodel.fit(xtrain_tfv, ytrain) # we can use the full data here but im only using xtrain\nprint('Best score: %0.3f' %model.best_score_)\nprint('Best parameters set:')\nbest_parameters = model.best_estimator_.get_params()\n\nfor param_name in sorted(param_grid.keys()):\n    print('\\t%s: %r' %(param_name, best_parameters[param_name]))","metadata":{"execution":{"iopub.status.busy":"2021-07-26T16:33:29.032512Z","iopub.status.idle":"2021-07-26T16:33:29.033067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"점수가 제가 가진 SVM과 유사하게 나타났습니다.  \n이 기술은 xgboost나 다항 naive bayes를 미세 조정하는 데에 사용할 수 있습니다.  \n여기서는 tfidf 데이터를 사용합니다.","metadata":{}},{"cell_type":"code","source":"nb_model = MultinomialNB()\n\n# Create the pipeline\nclf = pipeline.Pipeline([('nb', nb_model)])\n\n# parameter grid\nparam_grid = {'nb__alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n\n# Initialize Grid Search Model\nmodel = GridSearchCV(estimator=clf, param_grid=param_grid, scoring=mll_scorer,\n                    verbose=10, n_jobs=-1, iid=True, refit=True, cv=2)\n\n# Fit Grid Search Model\nmodel.fit(xtrain_tfv, ytrain) # we can use the full data here but im only using xtrain\nprint('Best score: %0.3f' %model.best_score_)\nprint('Best parameters set:')\nbest_parameters = model.best_estimator_.get_params()\n\nfor param_name in sorted(param_grid.keys()):\n    print('\\t%s: %r' %(param_name, best_parameters[param_name]))","metadata":{"execution":{"iopub.status.busy":"2021-07-26T16:33:29.033924Z","iopub.status.idle":"2021-07-26T16:33:29.034412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"이 방식은 기존의 naive bayes score에서 8%정도 향상을 했습니다!  \n\nNLP문제에서 단어 벡터를 보는 것이 일반적이비다. Word vector들은 데이터에 대한 많은 통찰력을 제공합니다.  \n그 내용들을 한번 알아봅시다.","metadata":{}},{"cell_type":"markdown","source":"## Word Vectors\n\n너무 많은 디테일은 빼고 저는 어떻게 문장 벡터를 만드는지와 어떻게 그것들을 사용해서 위에서 같이 학습 모델을 만들 수 있는지를 설명하겠습니다.  \n저는 GloVe 벡터, word2vec 그리고 fasttext의 팬입니다. 저는 이번에는 GloVe 벡터를 사용하겠습니다.  \nGloVe 벡터는 여기에서 다운받을 수 있습니다.  \n[http://www-nlp.stanford.edu/data/glove.840B.300d.zip](http://www-nlp.stanford.edu/data/glove.840B.300d.zip)","metadata":{}},{"cell_type":"code","source":"# load the GloVe vectors in a dictionary:\n\nembeddings_index = {}\n\nf = open('../input/glove840b300dtxt/glove.840B.300d.txt')\n\nfor line in tqdm(f):\n    values = line.split()\n    word = values[0]\n    \n    try:\n        coefs = np.asarray(values[1:], dtype='float32')\n    except:\n        continue\n    \n    embeddings_index[word] = coefs\n    \nf.close()\n\nprint(f'Found {len(embeddings_index)} word vectors.')","metadata":{"execution":{"iopub.status.busy":"2021-07-26T16:33:29.03525Z","iopub.status.idle":"2021-07-26T16:33:29.035755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# this function creates a normalized vector for the whole sentence\ndef sent2vec(s):\n    words = str(s).lower()\n    words = word_tokenize(words)\n    words = [w for w in words if not w in stop_words]\n    words = [w for w in words if w.isalpha()]\n\n    M = []\n    \n    for w in words:\n        try:\n            M.append(embedding_index[w])\n        except:\n            continue\n    \n    M = np.array(M)\n    v = M.sum(axis=0)\n    \n    if type(v) != np.ndarray:\n        return np.zeros(300)\n    \n    return v / np.sqrt((v ** 2).sum())","metadata":{"execution":{"iopub.status.busy":"2021-07-26T16:33:29.0366Z","iopub.status.idle":"2021-07-26T16:33:29.037083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create sentence vectors using the above function for training and validation set\nxtrain_glove = [sent2vec(x) for x in tqdm(xtrain)]\nxvalid_glove = [sent2vec(x) for x in tqdm(xvalid)]","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2021-07-26T16:33:29.037789Z","iopub.status.idle":"2021-07-26T16:33:29.038419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xtrain_glove = np.array(xtrain_glove)\nxvalid_glove = np.array(xvalid_glove)","metadata":{"execution":{"iopub.status.busy":"2021-07-26T16:33:29.039238Z","iopub.status.idle":"2021-07-26T16:33:29.039778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"이제 glove feature를 사용한 xgboost의 성능을 봅시다:","metadata":{}},{"cell_type":"code","source":"# Fitting a simple xgboost on globe features\nclf = xgb.XGBClassifier(nthread=10, silent=False)\nclf.fit(xtrain_glove, ytrain)\npredictions = clf.predict_proba(xvalid_glove)\n\nprint('logloss: %.3f ' %multiclass_logloss(yvalid, predictions))","metadata":{"execution":{"iopub.status.busy":"2021-07-26T16:33:29.040526Z","iopub.status.idle":"2021-07-26T16:33:29.041057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"이제 매개변수의 간단한 조정이 GloVe로 xgboost 점수를 향상시키는 것을 알 수 있습니다.","metadata":{}},{"cell_type":"markdown","source":"## Deep Learning\n\n하지만 이 파트는 딥러닝입니다! 우리는 신경망의 훈련없이는 살 수 없습니다.  \n그래서 저는 LSTM을 훈련하고 간단한 선형 네트워크에 GloVe를 적용할 것입니다.  \n일단 선형 네트워크부터 해봅시다!","metadata":{}},{"cell_type":"code","source":"# scale the data vefore any neural net:\nscl = preprocessing.StandardScaler()\nxtrain_glove_scl = scl.fit_transform(xtrain_glove)\nxvalid_glove_scl = scl.transform(xvalid_glove)","metadata":{"execution":{"iopub.status.busy":"2021-07-26T16:33:29.041738Z","iopub.status.idle":"2021-07-26T16:33:29.042278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we need to binarize the labels for the neural net\nytrain_enc = np_utils.to_categorical(ytrain)\nyvalid_enc = np_utils.to_categorical(yvalid)","metadata":{"execution":{"iopub.status.busy":"2021-07-26T16:33:29.043042Z","iopub.status.idle":"2021-07-26T16:33:29.043544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create a simple 3 layer sequential neural net\nmodel = Sequential()\n\nmodel.add(Dense(300, input_dim=300, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(BatchNormalization())\n\nmodel.add(Dense(300, activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(BatchNormalization())\n\nmodel.add(Dense(3))\nmodel.add(Activation('softmax'))\n\n# compile the model\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')","metadata":{"execution":{"iopub.status.busy":"2021-07-26T16:33:29.04428Z","iopub.status.idle":"2021-07-26T16:33:29.044808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(xtrain_glove_scl, y=ytrain_enc, batch_size=64,\n         epochs=5, verbose=1,\n         validation_data=(xvalid_glove_scl, yvalid_enc))","metadata":{"execution":{"iopub.status.busy":"2021-07-26T16:33:29.045537Z","iopub.status.idle":"2021-07-26T16:33:29.046025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"더 좋은 결과를 얻기위해 신경망의 매개변수들을 조정하고, 더 많은 층을 추가하고, dropout비율을 올릴 필요가 있습니다.  \n제가 어떤 조정도 없이 xgboost보다 더 좋은 결과를 빠르게 얻는 방법을 보여드리겠습니다.  \n\nLSTM과 텍스트를 토큰화 합니다.","metadata":{}},{"cell_type":"code","source":"# using keras tokenizer here\ntoken = text.Tokenizer(num_words=None)\nmax_len = 70\n\ntoken.fit_on_texts(list(xtrain) + list(xvalid))\nxtrain_seq = token.texts_to_sequences(xtrain)\nxvalid_seq = token.texts_to_sequences(xvalid)\n\n# zero pad the sequences\nxtrain_pad = sequence.pad_sequences(xtrain_seq, maxlen=max_len)\nxvalid_pad = sequence.pad_sequences(xvalid_seq, maxlen=max_len)\n\nword_index = token.word_index","metadata":{"execution":{"iopub.status.busy":"2021-07-26T16:33:29.046766Z","iopub.status.idle":"2021-07-26T16:33:29.047293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create an embedding matrix for the words we have in the dataset\nembedding_matrix = np.zeros((len(word_index) + 1, 300))\n\nfor word, i in tqdm(word_index.items()):\n    embedding_vector = embeddings_index.get(word)\n    \n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","metadata":{"execution":{"iopub.status.busy":"2021-07-26T16:33:29.048023Z","iopub.status.idle":"2021-07-26T16:33:29.048593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# A simple LSTM with glove embeddings and two dense layers\nmodel = Sequential()\nmodel.add(Embedding(len(word_index) + 1,\n                   300,\n                   weights=[embedding_matrix],\n                   input_length=max_len,\n                   trainable=False))\nmodel.add(SpatialDropout1D(0.3))\nmodel.add(LSTM(100, dropout=0.3, recurrent_dropout=0.3))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(3))\nmodel.add(Activation('softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')","metadata":{"execution":{"iopub.status.busy":"2021-07-26T16:33:29.049449Z","iopub.status.idle":"2021-07-26T16:33:29.049907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=100, verbose=1, validation_data=(xvalid_pad, yvalid_enc))","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-07-26T16:33:29.050603Z","iopub.status.idle":"2021-07-26T16:33:29.05117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"이제 점수가 0.5보다 아래로 내려간 것을 볼 수 있습니다.  \n멈춤 없이 많은 수의 에포크를 돌렸는데요, 하지만 조기멈춤을 사용하면 최적의 반복으로 값을 찾을 수 있습니다.  \n그렇다면 조기멈춤은 어떻게 할까요?  \n\n\n아주 쉽습니다. 모델을 다시 컴파일 해볼까요?","metadata":{}},{"cell_type":"code","source":"# A simple LSTM with glove embeddings and two dense layers\nmodel = Sequential()\nmodel.add(Embedding(len(word_index) + 1,\n                   300,\n                   weights=[embedding_matrix],\n                   input_length=max_len,\n                   trainable=False))\nmodel.add(SpatialDropout1D(0.3))\nmodel.add(LSTM(100, dropout=0.3, recurrent_dropout=0.3))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(3))\nmodel.add(Activation('softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\n\n# Fit the model with early stopping callback\nearlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\nmodel.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=100, \n          verbose=1, validation_data=(xvalid_pad, yvalid_enc), callbacks=[earlystop])","metadata":{"execution":{"iopub.status.busy":"2021-07-26T16:33:29.051905Z","iopub.status.idle":"2021-07-26T16:33:29.052363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"한가지 의문점이 생깁니다: 왜 드롭아웃을 썼을까요?  \n만약, 드롭아웃이 적거나 없다면 모델을 학습할 때, 오버피팅이 발생할 수 있습니다. :)  \n\n이제 양방향 LSTM이 더 좋은 결과를 주는지 확인해 봅시다. 케라스를 쓴다면 식은 죽 먹기입니다:)","metadata":{}},{"cell_type":"code","source":"# A simple bidirectional LSTM with glove embeddings and two dense layers\nmodel = Sequential()\nmodel.add(Embedding(len(word_index) + 1,\n                   300,\n                   weights=[embedding_matrix],\n                   input_length=max_len,\n                   trainable=False))\nmodel.add(SpatialDropout1D(0.3))\nmodel.add(Bidirectional(LSTM(100, dropout=0.3, recurrent_dropout=0.3)))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(3))\nmodel.add(Activation('softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\n\n# Fit the model with early stopping callback\nearlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\nmodel.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=100, \n          verbose=1, validation_data=(xvalid_pad, yvalid_enc), callbacks=[earlystop])","metadata":{"execution":{"iopub.status.busy":"2021-07-26T16:33:29.053382Z","iopub.status.idle":"2021-07-26T16:33:29.053832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"꽤 가까워졌네요! 이제 2개층의 GRU를 써봅시다.","metadata":{}},{"cell_type":"code","source":"# GRU with glove embeddings and two dense layers\nmodel = Sequential()\nmodel.add(Embedding(len(word_index) + 1,\n                   300,\n                   weights=[embedding_matrix],\n                   input_length=max_len,\n                   trainable=False))\nmodel.add(SpatialDropout1D(0.3))\nmodel.add(GRU(300, dropout=0.3, recurrent_dropout=0.3, return_sequences=True))\nmodel.add(GRU(300, dropout=0.3, recurrent_dropout=0.3))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(3))\nmodel.add(Activation('softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\n\n# Fit the model with early stopping callback\nearlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\nmodel.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=100,\n         verbose=1, validation_data=(xvalid_pad, yvalid_enc), callbacks=[earlystop])","metadata":{"execution":{"iopub.status.busy":"2021-07-26T16:33:29.054679Z","iopub.status.idle":"2021-07-26T16:33:29.055243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"좋습니다! 이전보다 훨씬 더 좋아졌습니다! 계속해서 조정을 하고 성능을 향상하겠습니다.  \n시도해 볼 가치로는 형태소 분석과 표제어 추출이 있습니다.  \n일단 저는 이 방법은 현재 넘어가겠습니다.  \n\n\n캐글의 세계에서는 높은 점수를 얻기 위해서는 앙상블 모델을 사용하는 것이 좋습니다.  \n이제 약간의 앙상블을 해봅시다!","metadata":{}},{"cell_type":"markdown","source":"## Ensembling\n\n몇 달 전 저는 간단한 앙상블러를 만들었습니다. 하지만 완전히 개발할 시간이 없었습니다.  \n그때 만든 모델을 사용하겠습니다.","metadata":{}},{"cell_type":"code","source":"# this is the main ensembling class. how to use it is in the next cell!\nimport numpy as np\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold, KFold\nimport pandas as pd\nimport os\nimport sys\nimport logging\n\nlogging.basicConfig(\n    level=logging.DEBUG,\n    format=\"[%(asctime)s] %(levelname)s %(message)s\",\n    datefmt=\"%H:%M:%S\", stream=sys.stdout)\nlogger = logging.getLogger(__name__)\n\n\nclass Ensembler(object):\n    def __init__(self, model_dict, num_folds=3, task_type='classification', optimize=roc_auc_score,\n                 lower_is_better=False, save_path=None):\n        \"\"\"\n        Ensembler init function\n        :param model_dict: model dictionary, see README for its format\n        :param num_folds: the number of folds for ensembling\n        :param task_type: classification or regression\n        :param optimize: the function to optimize for, e.g. AUC, logloss, etc. Must have two arguments y_test and y_pred\n        :param lower_is_better: is lower value of optimization function better or higher\n        :param save_path: path to which model pickles will be dumped to along with generated predictions, or None\n        \"\"\"\n\n        self.model_dict = model_dict\n        self.levels = len(self.model_dict)\n        self.num_folds = num_folds\n        self.task_type = task_type\n        self.optimize = optimize\n        self.lower_is_better = lower_is_better\n        self.save_path = save_path\n\n        self.training_data = None\n        self.test_data = None\n        self.y = None\n        self.lbl_enc = None\n        self.y_enc = None\n        self.train_prediction_dict = None\n        self.test_prediction_dict = None\n        self.num_classes = None\n\n    def fit(self, training_data, y, lentrain):\n        \"\"\"\n        :param training_data: training data in tabular format\n        :param y: binary, multi-class or regression\n        :return: chain of models to be used in prediction\n        \"\"\"\n\n        self.training_data = training_data\n        self.y = y\n\n        if self.task_type == 'classification':\n            self.num_classes = len(np.unique(self.y))\n            logger.info(\"Found %d classes\", self.num_classes)\n            self.lbl_enc = LabelEncoder()\n            self.y_enc = self.lbl_enc.fit_transform(self.y)\n            kf = StratifiedKFold(n_splits=self.num_folds)\n            train_prediction_shape = (lentrain, self.num_classes)\n        else:\n            self.num_classes = -1\n            self.y_enc = self.y\n            kf = KFold(n_splits=self.num_folds)\n            train_prediction_shape = (lentrain, 1)\n\n        self.train_prediction_dict = {}\n        for level in range(self.levels):\n            self.train_prediction_dict[level] = np.zeros((train_prediction_shape[0],\n                                                          train_prediction_shape[1] * len(self.model_dict[level])))\n\n        for level in range(self.levels):\n\n            if level == 0:\n                temp_train = self.training_data\n            else:\n                temp_train = self.train_prediction_dict[level - 1]\n\n            for model_num, model in enumerate(self.model_dict[level]):\n                validation_scores = []\n                foldnum = 1\n                for train_index, valid_index in kf.split(self.train_prediction_dict[0], self.y_enc):\n                    logger.info(\"Training Level %d Fold # %d. Model # %d\", level, foldnum, model_num)\n\n                    if level != 0:\n                        l_training_data = temp_train[train_index]\n                        l_validation_data = temp_train[valid_index]\n                        model.fit(l_training_data, self.y_enc[train_index])\n                    else:\n                        l0_training_data = temp_train[0][model_num]\n                        if type(l0_training_data) == list:\n                            l_training_data = [x[train_index] for x in l0_training_data]\n                            l_validation_data = [x[valid_index] for x in l0_training_data]\n                        else:\n                            l_training_data = l0_training_data[train_index]\n                            l_validation_data = l0_training_data[valid_index]\n                        model.fit(l_training_data, self.y_enc[train_index])\n\n                    logger.info(\"Predicting Level %d. Fold # %d. Model # %d\", level, foldnum, model_num)\n\n                    if self.task_type == 'classification':\n                        temp_train_predictions = model.predict_proba(l_validation_data)\n                        self.train_prediction_dict[level][valid_index,\n                        (model_num * self.num_classes):(model_num * self.num_classes) +\n                                                       self.num_classes] = temp_train_predictions\n\n                    else:\n                        temp_train_predictions = model.predict(l_validation_data)\n                        self.train_prediction_dict[level][valid_index, model_num] = temp_train_predictions\n                    validation_score = self.optimize(self.y_enc[valid_index], temp_train_predictions)\n                    validation_scores.append(validation_score)\n                    logger.info(\"Level %d. Fold # %d. Model # %d. Validation Score = %f\", level, foldnum, model_num,\n                                validation_score)\n                    foldnum += 1\n                avg_score = np.mean(validation_scores)\n                std_score = np.std(validation_scores)\n                logger.info(\"Level %d. Model # %d. Mean Score = %f. Std Dev = %f\", level, model_num,\n                            avg_score, std_score)\n\n            logger.info(\"Saving predictions for level # %d\", level)\n            train_predictions_df = pd.DataFrame(self.train_prediction_dict[level])\n            train_predictions_df.to_csv(os.path.join(self.save_path, \"train_predictions_level_\" + str(level) + \".csv\"),\n                                        index=False, header=None)\n\n        return self.train_prediction_dict\n\n    def predict(self, test_data, lentest):\n        self.test_data = test_data\n        if self.task_type == 'classification':\n            test_prediction_shape = (lentest, self.num_classes)\n        else:\n            test_prediction_shape = (lentest, 1)\n\n        self.test_prediction_dict = {}\n        for level in range(self.levels):\n            self.test_prediction_dict[level] = np.zeros((test_prediction_shape[0],\n                                                         test_prediction_shape[1] * len(self.model_dict[level])))\n        self.test_data = test_data\n        for level in range(self.levels):\n            if level == 0:\n                temp_train = self.training_data\n                temp_test = self.test_data\n            else:\n                temp_train = self.train_prediction_dict[level - 1]\n                temp_test = self.test_prediction_dict[level - 1]\n\n            for model_num, model in enumerate(self.model_dict[level]):\n\n                logger.info(\"Training Fulldata Level %d. Model # %d\", level, model_num)\n                if level == 0:\n                    model.fit(temp_train[0][model_num], self.y_enc)\n                else:\n                    model.fit(temp_train, self.y_enc)\n\n                logger.info(\"Predicting Test Level %d. Model # %d\", level, model_num)\n\n                if self.task_type == 'classification':\n                    if level == 0:\n                        temp_test_predictions = model.predict_proba(temp_test[0][model_num])\n                    else:\n                        temp_test_predictions = model.predict_proba(temp_test)\n                    self.test_prediction_dict[level][:, (model_num * self.num_classes): (model_num * self.num_classes) +\n                                                                                        self.num_classes] = temp_test_predictions\n\n                else:\n                    if level == 0:\n                        temp_test_predictions = model.predict(temp_test[0][model_num])\n                    else:\n                        temp_test_predictions = model.predict(temp_test)\n                    self.test_prediction_dict[level][:, model_num] = temp_test_predictions\n\n            test_predictions_df = pd.DataFrame(self.test_prediction_dict[level])\n            test_predictions_df.to_csv(os.path.join(self.save_path, \"test_predictions_level_\" + str(level) + \".csv\"),\n                                       index=False, header=None)\n\n        return self.test_prediction_dict","metadata":{"execution":{"iopub.status.busy":"2021-07-26T16:33:29.056118Z","iopub.status.idle":"2021-07-26T16:33:29.0567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# specify the data to be used for every level of ensembling:\ntrain_data_dict = {0: [xtrain_tfv, xtrain_ctv, xtrain_tfv, xtrain_ctv], 1: [xtrain_glove]}\ntest_data_dict = {0: [xvalid_tfv, xvalid_ctv, xvalid_tfv, xvalid_ctv], 1: [xvalid_glove]}\n\nmodel_dict = {0: [LogisticRegression(), LogisticRegression(), MultinomialNB(alpha=0.1), MultinomialNB()],\n\n              1: [xgb.XGBClassifier(silent=True, n_estimators=120, max_depth=7)]}\n\nens = Ensembler(model_dict=model_dict, num_folds=3, task_type='classification',\n                optimize=multiclass_logloss, lower_is_better=True, save_path='')\n\nens.fit(train_data_dict, ytrain, lentrain=xtrain_glove.shape[0])\npreds = ens.predict(test_data_dict, lentest=xvalid_glove.shape[0])","metadata":{"execution":{"iopub.status.busy":"2021-07-26T16:33:29.057448Z","iopub.status.idle":"2021-07-26T16:33:29.057972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check error\nmulticlass_logloss(yvalid, preds[1])","metadata":{"execution":{"iopub.status.busy":"2021-07-26T16:33:29.058826Z","iopub.status.idle":"2021-07-26T16:33:29.059459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"게다가, 우리는 앙상블이 점수를 크게 향상시키는 것을 볼 수 있습니다!  \n이건 듀토리얼이므로 따로 제출 csv는 제공하지 않습니다!","metadata":{}}]}