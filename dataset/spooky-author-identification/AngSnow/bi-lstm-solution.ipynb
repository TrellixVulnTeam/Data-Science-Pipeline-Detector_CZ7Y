{"cells":[{"execution_count":null,"cell_type":"code","outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Activation, Dropout\nfrom keras.layers.embeddings import Embedding\nfrom keras.utils import np_utils\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn import preprocessing\nfrom keras.utils import to_categorical\nfrom keras.layers.recurrent import LSTM\nfrom keras.layers import Dense, GlobalAveragePooling1D, Embedding\nfrom keras.layers import Bidirectional\nfrom keras.layers.normalization import BatchNormalization\nfrom keras import regularizers\nfrom keras.callbacks import EarlyStopping\nimport time\n\n#read the data\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\n","metadata":{"_uuid":"8a72d0f0d29657e818fcc1e8954e7538ea584526","_cell_guid":"f6dcd221-0263-4b47-95cf-2c764248db8a"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"content1=[] #train_content\ncontent2=[] #test content\n\nfor i in range(train.shape[0]):\n    mytext=train.iloc[i,1]\n    content1.append(mytext)\nfor i in range(test.shape[0]):\n    mytext=test.iloc[i,1]\n    content2.append(mytext)\n\n#word embedding,got x\ntokenizer = Tokenizer(num_words=None)\ntokenizer.fit_on_texts(content1 + content2)\ntrain_seq= tokenizer.texts_to_sequences(content1)\ntest_seq=tokenizer.texts_to_sequences(content2)\ntrain_pad = pad_sequences(train_seq,maxlen=256,padding='post') #pad to the same length\ntest_pad = pad_sequences(test_seq,maxlen=256,padding='post')\nprint('train input data')\nprint(train_pad)","metadata":{}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"#y one hot","metadata":{"collapsed":true}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"label= preprocessing.LabelEncoder()\nlabel_y= label.fit_transform(train.author.values)\ny = to_categorical(label_y,num_classes=3)\nprint(y)","metadata":{}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"train_x, test_x, train_y, test_y = train_test_split(train_pad,y,random_state=42,test_size=0.2)\ntime_start=time.time()","metadata":{"collapsed":true}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"model = Sequential()\nmodel.add(Embedding(len(tokenizer.word_index) + 1, 256))\nmodel.add(Bidirectional(LSTM(256,dropout=0.3,kernel_regularizer=regularizers.l2(0.03),return_sequences=True)))\nmodel.add(Bidirectional(LSTM(256,dropout=0.3,kernel_regularizer=regularizers.l2(0.03))))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(3))\nmodel.add(Activation('softmax'))\nmodel.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()\nearlystop = EarlyStopping(monitor='val_loss', patience=2)\nhist=model.fit(train_x, train_y,batch_size=16,epochs=25,validation_data=(test_x, test_y),callbacks=[earlystop])","metadata":{}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"","metadata":{"collapsed":true}}],"nbformat":4,"nbformat_minor":1,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"mimetype":"text/x-python","version":"3.6.3","pygments_lexer":"ipython3","nbconvert_exporter":"python","name":"python","file_extension":".py"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}}}