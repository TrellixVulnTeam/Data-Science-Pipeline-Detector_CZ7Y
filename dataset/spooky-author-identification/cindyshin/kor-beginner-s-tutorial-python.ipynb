{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# # This Python 3 environment comes with many helpful analytics libraries installed\n# # It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# # For example, here's several helpful packages to load in \n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# # Input data files are available in the \"../input/\" directory.\n# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('../kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# # Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"이 튜토리얼에서 우리는 베이직한 자연어처리(NLP) 기법 몇 개를 다룰 것입니다. 다룰 목록은 다음과 같습니다:\n* 유용한 일부 NLP 라이브러리 & 데이터셋 읽기\n* 각 작가들이 각 단어를 사용한 빈도수 찾기\n* 어느 작가가 문장을 썼는지 추측하는 데에 단어 빈도수 사용하기\n\n준비되었나요? 시작해봅시다! :D"},{"metadata":{},"cell_type":"markdown","source":"# 개요\n해당 튜토리얼에서는 정규화된 유니그램 빈도를 기준으로 어느 작가가 텍스트 문자열을 작성했는지 추측합니다. 이는 각 작가가 훈련 데이터에서 모든 단어를 얼마나 자주 사용하는지를 세고 그들이 쓴 총 단어 갯수로 나누는 멋진 방법입니다. 그런 다음, 테스트 문장에서 한 작가가 다른 작가들보다 더 많은 단어를 사용하는 것을 본다면 우리는 이 작가가 그 작가임을 추측할 수 있습니다.\n다음을 우리의 트레이닝 말뭉치라고 가정해봅시다:\n\n* 작가 1: \"A very spooky thing happened. The thing was so spooky I screamed.\"\n* 작가 2: \"I ate a tasty candy apple. It was delicious\"\n\n그리고 다음은 어느 작가가 썼는지 알아내고 싶은 테스트 문장입니다:\n\n* 작가 ??? : \"What a spooky thing!\"\n\n그냥 봐서는, 이 문장은 작가 1이 쓴 것 같습니다. 작가 1은 'spooky'와 'thing'을 둘 다 많이 쓰는 반면, 작가 2는 그렇지 않습니다(적어도 우리의 훈련 데이터에 따르면). 테스트 문장에서 'spooky'와 'thing'을 둘 다 볼 수 있기 때문에, 이는 작가 2보다는 작가 1이 쓴 것이라고 볼 수 있습니다 -- 비록 테스트 문장에는 'a'라는 단어가 있고 작가 2도 이를 사용했지만 말입니다. \n\n튜토리얼의 나머지 부분에서는 이 직관을 코드로 변환할 수 있는지 알아볼 것입니다."},{"metadata":{},"cell_type":"markdown","source":"# 유용한 NLP 라이브러리 & 데이터셋\n\n이 튜토리얼에서는 'NLTK' 라고 하는 Natural Language Toolkit을 사용하겠습니다. 이는 언어 데이터를 분석하기 위한 오픈 소스 파이썬 라이브러리입니다. NLTK의 좋은 점은 많은 일반적인 NLP 작업을 단계별로 진행하는 유용한 책이 있다는 것입니다. 더 좋은 점은 여기에서 책을 무료로 얻을 수 있다는 것입니다."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# read in some helpful libraries\nimport nltk # the natural language toolkit, open-source NLP\nimport pandas as pd # dataframes\nimport zipfile\n\n### Read in the data\n\n# read our data into a dataframe\ndf = pd.DataFrame()\n#pd.read_csv() - df = pd.read_csv('../input/spooky-author-identification/train.zip', compression='zip', header=0, sep=',', quotechar='\"')\nDataset = 'train'\n\n# Will unzip the files so that you can see them..\nwith zipfile.ZipFile(\"../input/spooky-author-identification/\"+Dataset+\".zip\",\"r\") as z:\n    z.extractall(\".\")\n\ntexts = pd.read_csv(Dataset + '.csv')\n\n# look at the first few rows\ntexts.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#import os\n#print(os.listdir('../input/spooky-author-identification/train.zip'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 각 작가가 각 단어를 사용한 빈도 수 알아보기\n\n각 작가가 각 단어를 얼마나 자주 사용하는지 알아보십시오. 많은 NLP 응용 프로그램은 특정 단어가 얼마나 자주 사용되는지 계산하는 데 의존합니다. (이 용어에 대한 멋진 용어는 '단어 빈도' 입니다.) 데이터셋에서 각 저자의 단어 빈도를 살펴보겠습니다. NLTK에는 이를 위해 사용할 수 있는 멋진 내장함수와 데이터 구조가 많이 있습니다."},{"metadata":{"trusted":true},"cell_type":"code","source":"### Split data\n\n# split the data by author\nbyAuthor = texts.groupby('author')\n\n### Tokenize (split into individual words) our text\n\n# word frequency by author\nwordFreqByAuthor = nltk.probability.ConditionalFreqDist()\n\n# for each author...\nfor name, group in byAuthor:\n    # get all of the sentences they wrote and collapse them into a\n    # single long string\n    sentences = group['text'].str.cat(sep = ' ')\n    \n    # convert everything to lower case (so 'The' and 'the' get counted as\n    # the same word rather than two different words)\n    sentences = sentences.lower()\n    \n    # split the text into individual tokens\n    tokens = nltk.tokenize.word_tokenize(sentences)\n    \n    # calculate the frequency of each token\n    frequency = nltk.FreqDist(tokens)\n    \n    # add the frequencies for each author to our dictionary\n    wordFreqByAuthor[name] = (frequency)\n    \n# now we have an dictionary where each entry is the frequency distribution\n# of words for a specific author","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"이제 우리는 각 작가가 특정 단어를 어느 빈도로 사용하는지 볼 수 있습니다. 이것이 할로윈 대회용이니까 'blood', 'scream', 'fear' 같은 건 어떨까요?"},{"metadata":{"trusted":true},"cell_type":"code","source":"# see how often each author says 'blood'\nfor i in wordFreqByAuthor.keys():\n    print('blood: ' + i)\n    print(wordFreqByAuthor[i].freq('blood'))\n\n# print a blank line\nprint()\n\n# see how often each author says 'scream'\nfor i in wordFreqByAuthor.keys():\n    print('scream: ' + i)\n    print(wordFreqByAuthor[i].freq('scream'))\n    \n# print a blank line\nprint()\n\n# see how often each author says 'fear'\nfor i in wordFreqByAuthor.keys():\n    print('fear: ' + i)\n    print(wordFreqByAuthor[i].freq('fear'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 어느 작가가 문장을 썼는지 추측하는 데에 단어 빈도수 사용하기\n\n일반적인 아이디어는 다른 사람들은 다른 단어들을 더 자주 또는 덜 사용하는 경향이 있다는 것입니다. (나의 경우에는 특히 'gestalt'를 좋아하셨던 교수님이 계셨습니다.) 누가 무엇을 말했는지 확실하지 않지만 그 안에 한 사람이 많이 사용하는 단어가 많을 경우, 그 단어들은 어떤 한 사람이 썼을 것이라고 추측 가능합니다.\n\n이 일반적인 원칙을 사용하여 누가 'It was a dark and stormy night.'이라는 문장을 쓸 가능성이 더 높을지 추측해봅시다."},{"metadata":{"trusted":true},"cell_type":"code","source":"# One way to guess authorship is to use the joint probability that each\n# author used each word in a given sentence\n\n# first, let's start with a test sentence\ntestSentence = \"It was a dark and stormy night.\"\n\n# and then lowercase & tokenize our test sentence\npreProcessedTestSentence = nltk.tokenize.word_tokenize(testSentence.lower())\n\n# create an empty dataframe to put our output in\ntestProbabilities = pd.DataFrame(columns = ['author', 'word','probability'])\n\n# For each author...\nfor i in wordFreqByAuthor.keys():\n    # for each word in our test sentence..\n    for j in preProcessedTestSentence:\n        # find out how frequentyly the author used that word\n        wordFreq = wordFreqByAuthor[i].freq(j)\n        # and add a very small amount to every prob. so none of them are 0\n        smoothedWordFreq = wordFreq + 0.000001\n        # add the author, word and smoothed freq. to our dataframe\n        output = pd.DataFrame([[i,j,smoothedWordFreq]], columns = ['author', 'word','probability'])\n        testProbabilities = testProbabilities.append(output, ignore_index = True)\n\n# empty dataframe for the probability that each author wrote the snetence\ntestProbabilitiesByAuthor = pd.DataFrame(columns = ['author','jointProbability'])\n\n# now let's group the dataframe with our frequency by author\nfor i in wordFreqByAuthor.keys():\n    # get the joint probability that each author wrote each word\n    oneAuthor = testProbabilities.query('author == \"' + i + '\"')\n    jointProbability = oneAuthor.product(numeric_only = True)[0]\n    \n    # and add that to our dataframe\n    output = pd.DataFrame([[i, jointProbability]], columns = ['author','jointProbability'])\n    testProbabilitiesByAuthor = testProbabilitiesByAuthor.append(output, ignore_index=True)\n    \n# and our winner is...\ntestProbabilitiesByAuthor.loc[testProbabilitiesByAuthor['jointProbability'].idxmax(),'author']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"우리가 훈련 데이터에서 본 것을 바탕으로, 세 명의 작가들 중 H.P.Lovecraft가 'It was a dark and stormy night\" 라는 문장을 썼을 가능성이 높은 것으로 나타납니다."},{"metadata":{"trusted":true},"cell_type":"code","source":"testProbabilitiesByAuthor.to_csv(\"testProbabilityByAuthor.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}