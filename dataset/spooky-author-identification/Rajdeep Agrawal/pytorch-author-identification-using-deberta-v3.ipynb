{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"> In this notebook I try to take you through the complete process for finetuning a DeBerta-v3-base model for the task. Various components of training setup - and how they all connect - are explained, with even occasional references to basic dimensions of tensors to understand the underlying operations. Even though this may not be the most complete work on this data, I hope you leave with a better theoretical understanding of how the model layers interact. Constructive criticism absolutely welcome in the comments! And so are upvotes, lol.","metadata":{}},{"cell_type":"markdown","source":"# I. Importing Libraries and Reading Data","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport re\nfrom collections import defaultdict\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom transformers import AutoTokenizer, AutoModel","metadata":{"execution":{"iopub.status.busy":"2022-06-26T04:03:51.152904Z","iopub.execute_input":"2022-06-26T04:03:51.153366Z","iopub.status.idle":"2022-06-26T04:04:00.870728Z","shell.execute_reply.started":"2022-06-26T04:03:51.153257Z","shell.execute_reply":"2022-06-26T04:04:00.869591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = pd.read_csv('../input/spooky-author-identification/train.zip')\ntest_data = pd.read_csv('../input/spooky-author-identification/test.zip')\ntrain_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CONFIG = {\n    'batch_size': 8,\n    'model_name': 'microsoft/deberta-v3-base',\n    'num_classes': 3,\n    'lr': 2e-3,\n    'epochs': 7,\n    'n_accumulate': 4,\n    'weight_decay': 1e-6,\n    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# II. Text Preprocessing","metadata":{}},{"cell_type":"code","source":"contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n\n                           \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n\n                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n\n                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n\n                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n\n                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n\n                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n\n                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n\n                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n\n                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n\n                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n\n                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n\n                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n\n                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n\n                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n\n                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n\n                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n\n                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n\n                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n\n                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n\n                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n\n                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n\n                           \"you're\": \"you are\", \"you've\": \"you have\"}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def text_clean(text):\n    text = re.sub(' +', ' ', text).strip() #Remove leading, trailing and in-between whitespaces\n    text = ' '.join([contraction_mapping[word] if word in contraction_mapping else word for word in text.split()])\n    text = re.sub('[^a-zA-Z]', ' ', text) #Remove non-alphabetical characters\n    #text = ' '.join([word.lower() for word in text.split() if (word not in StopWords) and (len(word) > 2)])\n    return text\n\ntrain_data['text'] = train_data['text'].apply(text_clean)\ntest_data['text'] = test_data['text'].apply(text_clean)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Distribution of Train Target Labels\nsns.countplot(x = train_data['author'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# III. Preparing Data for Model\n* Train-Validation Splitting\n* Creating Custom Dataset Class\n* Creating an iterable wrapper called DataLoader for train, val, test data while splitting into batches.","metadata":{}},{"cell_type":"code","source":"#Encoding Labels and Preparing Training, Validation Sets.\nencoder = LabelEncoder()\ntrain_data['author'] = encoder.fit_transform(train_data['author'])\nX_train, X_val, y_train, y_val = train_test_split(train_data['text'], train_data['author'], train_size=0.8)\ntrain_df = pd.DataFrame()\nval_df = pd.DataFrame()\ntrain_df['text'] = X_train\ntrain_df['author'] = y_train\nval_df['text'] = X_val\nval_df['author'] = y_val","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Load the BERT model and tokenizer to be used\ntokenizer = AutoTokenizer.from_pretrained(CONFIG['model_name'])\nbert_model = AutoModel.from_pretrained(CONFIG['model_name'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Custom Dataset class, subclass of the Dataset Module.\n#Necessarily contains the below 3 methods: __init__, __len__, __getitem__\nclass CustomDataset(Dataset):\n    def __init__(self, text, tokenizer, label=None, train=False):\n        self.text  = text\n        self.label = label\n        self.tokenizer  = tokenizer\n        self.train = train\n    \n    def __len__(self):\n        #Number of examples\n        return len(self.text)\n    \n    def __getitem__(self, idx):\n        #Return a dict of text, labels, tokenized data at specific idx\n        sample_text = str(self.text[idx])\n        sample_label = []\n        if self.train:\n            sample_label = self.label[idx]\n        tokenizer_dict = self.tokenizer.encode_plus(\n        sample_text,\n        add_special_tokens=True,\n        max_length=512,\n        padding='max_length',\n        return_token_type_ids=False,\n        return_attention_mask=True,\n        truncation=True,\n        return_tensors='pt' #pytorch tensor format\n        )\n        if self.train:\n            return {\n                'text' : sample_text,\n                'input_ids': tokenizer_dict['input_ids'].flatten(),\n                'attn_mask': tokenizer_dict['attention_mask'].flatten(),\n                'label' : torch.tensor(sample_label, dtype=torch.int64)\n            }\n        else:\n            return {\n                'text' : sample_text,\n                'input_ids': tokenizer_dict['input_ids'].flatten(),\n                'attn_mask': tokenizer_dict['attention_mask'].flatten()\n            }\n            ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Instantiate custom datasets by passing constructor arguments\ndata_train = CustomDataset(train_df['text'].to_numpy(), tokenizer, train_df['author'].to_numpy(), train=True)\ndata_val = CustomDataset(val_df['text'].to_numpy(), tokenizer, val_df['author'].to_numpy(), train=True)\ndata_test = CustomDataset(test_data['text'].to_numpy(), tokenizer)\n\n#Create iterable dataloaders, which create batches of given size returned as the return type of __getitem__ (dict here)\ntrain_dataloader = DataLoader(data_train, batch_size=CONFIG['batch_size'], pin_memory=True)\nval_dataloader = DataLoader(data_val, batch_size=CONFIG['batch_size'], pin_memory=True)\ntest_dataloader = DataLoader(data_test, batch_size=CONFIG['batch_size'], pin_memory=True)\n#How a batch looks:\nnext(iter(train_dataloader))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# IV. Model Definition and Training","metadata":{}},{"cell_type":"code","source":"#Custom Pooling Layer to be applied to the model output.last_hidden_state\n#Last hidden state returns literally the last hidden state vector for every token for every sequence in the batch.\n#Hence Dimension of Last Hidden state = (batch_size, seq_length, hidden_state_dim)\n\nclass MeanPooling(nn.Module):\n    def __init__(self):\n        super(MeanPooling, self).__init__()\n    \n    def forward(self, last_hidden_state, attn_mask): # attention_mask dims= (batch_size, seq_length=512)\n        \n        # After unsqueeze, (batch_size, seq_length, 1). Further expand to (batch_size, seq_length, hidden_state_dim).\n        # expanded_mask creates a mask vector of size hidden_state_dim for all seq_length number of tokens for each \n        # sequence in the batch. Earlier a binary 0/1 mask for each token is now a vector of zeros/ones resp.\n        expanded_mask = attn_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        # Weighted sum of hidden_state_vectors with masks.\n        masked_embeddings = torch.sum(last_hidden_state * expanded_mask, 1)\n        # Sum the mask vectors for all tokens of each sequence. mask_sum returns (batch_size, hidden_state_dim)\n        mask_sum = expanded_mask.sum(1)\n        # Clamps every value of tensor b/w (min, max). Values < min set to min and values > max to max. \n        mask_sum = torch.clamp(mask_sum, min=1e-9) \n        # Computes the mean embeddings to be returned.\n        mean_embeddings = masked_embeddings / mask_sum\n        return mean_embeddings","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Create custom NN, always inheriting from nn.Module\nclass BERT_Classifier(nn.Module):\n    def __init__(self, num_classes):\n        super(BERT_Classifier, self).__init__()\n        self.num_classes = num_classes\n        self.bert_layer = bert_model\n        self.pooler = MeanPooling()\n        self.dropout = nn.Dropout(p=0.2)\n        self.linear = nn.Linear(self.bert_layer.config.hidden_size, self.num_classes) #Outputs logits for num_classes\n        \n    def forward(self, input_ids, attn_mask):\n        outputs = self.bert_layer(input_ids=input_ids, attention_mask=attn_mask)\n        pooled_output = self.pooler(outputs.last_hidden_state, attn_mask)\n        dropout = self.dropout(pooled_output)\n        logits = self.linear(dropout)\n        #probs = nn.Softmax(dim=1)(logits) - later.\n        return logits","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Instantiate model, transfer to gpu device if available\nmodel = BERT_Classifier(CONFIG['num_classes']).to(CONFIG['device'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Trial feed forward on one smaple batch.\ninput_ids = next(iter(train_dataloader))['input_ids'].to(CONFIG['device'])\nattn_mask = next(iter(train_dataloader))['attn_mask'].to(CONFIG['device'])\nlogits = model(input_ids, attn_mask)\nprobs = nn.Softmax(dim=1)(logits)\ntorch.max(probs, dim=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Training Essentials\noptimizer = AdamW(model.parameters(), lr=CONFIG['lr'], weight_decay=CONFIG['weight_decay'])\nscheduler = CosineAnnealingLR(optimizer, T_max=10000, eta_min=2e-5)\nloss = nn.CrossEntropyLoss().to(CONFIG['device']) #Categorical CrossEntropy\nepochs = CONFIG['epochs']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Train Loop, iterates over all batches\ndef train_loop(model, dataloader, loss, optimizer, scheduler):\n    model = model.train() #Setting to train mode activates Dropouts, Batch Norm, etc\n    batch_losses = []\n    pred_correct = 0\n    for batch_num, batch in enumerate(dataloader): \n        #Each batch as a Dict from __getitem__, with dict values having batch-size num of elements\n        input_ids = batch['input_ids'].to(CONFIG['device'], non_blocking=True)\n        attn_mask = batch['attn_mask'].to(CONFIG['device'], non_blocking=True)\n        ground_truths = batch['label'].to(CONFIG['device'], non_blocking=True)\n        #Set mixed floating point precision for allocating float16 and float32 operations - only for forward prop.\n        with torch.cuda.amp.autocast():\n            logits = model(input_ids, attn_mask)\n            output = nn.Softmax(dim=1)(logits) #Generate probabilites on output of NN\n            batch_loss = loss(logits, ground_truths) #Compute Loss\n         \n        batch_loss = batch_loss / CONFIG['n_accumulate']\n        prob, labels = torch.max(output, dim=1) #Extract class labels from max(logits) or max(probs)\n        batch_losses.append(batch_loss.item())\n        pred_correct += torch.sum(labels == ground_truths) #Compute num of correct predictions for accuracy\n        \n        #Backprop Steps - Essential.\n        batch_loss.backward() #Performs backprop\n        #Update parameters only after n_accumulate number of batches, to mimic a larger batch_size.\n        if (batch_num + 1) % CONFIG['n_accumulate'] == 0:\n            optimizer.step() #Updates parameters\n            optimizer.zero_grad(set_to_none=True) #Resets gradients to zero for next batch\n            scheduler.step()\n    \n    return np.mean(batch_losses), pred_correct.double() / len(train_df) #Return train loss, train acc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Validation Loop, almost same - No backprop\ndef validation_loop(model, dataloader, loss):\n    model = model.eval() #Deactivates dropouts, batch norm etc\n    batch_losses = []\n    pred_correct = 0\n    with torch.no_grad(): #Disable Autograd - No parameter optimization during validation\n        for batch in dataloader:\n            input_ids = batch['input_ids'].to(CONFIG['device'], non_blocking=True)\n            attn_mask = batch['attn_mask'].to(CONFIG['device'], non_blocking=True)\n            ground_truths = batch['label'].to(CONFIG['device'], non_blocking=True)\n            logits = model(input_ids, attn_mask)\n            output = nn.Softmax(dim=1)(logits)\n            prob, labels = torch.max(output, dim=1)\n            batch_loss = loss(logits, ground_truths)\n            batch_losses.append(batch_loss.item())\n            pred_correct += torch.sum(labels == ground_truths)\n    \n    return np.mean(batch_losses), pred_correct.double() / len(val_df) #Returns Val Loss, Val Acc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#To extract predictions on test data\ndef get_predictions(model, dataloader):\n    model.eval()\n    pred_labels = []\n    pred_probs = []\n    with torch.no_grad():\n        for batch in dataloader:\n            input_ids = batch['input_ids'].to(CONFIG['device'], non_blocking=True)\n            attn_mask = batch['attn_mask'].to(CONFIG['device'], non_blocking=True)\n            logits = model(input_ids, attn_mask)\n            probs = nn.Softmax(dim=1)(logits)\n            pred_probs.extend(probs)\n            _, labels = torch.max(probs, dim=1)\n            pred_labels.extend(labels)\n    \n    return pred_labels, pred_probs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Running the train, validation loops over all epochs. All batches over each epoch.\nhistory = defaultdict(list)\nfor epoch in range(epochs):\n    print('Epoch ', epoch+1, ' of', epochs, ' :')\n    train_loss, train_acc = train_loop(model, train_dataloader, loss, optimizer, scheduler)\n    print('Training Loss: ', train_loss, '    Training Acc: ', train_acc)\n    val_loss, val_acc = validation_loop(model, val_dataloader, loss)\n    print('Val Loss: ', val_loss, '    Val Acc: ', val_acc)\n    history['train_loss'].append(train_loss)\n    history['train_acc'].append(train_acc)\n    history['val_loss'].append(val_loss)\n    history['val_acc'].append(val_acc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(1)\nplt.plot(history['train_loss'], label='Train Loss')\nplt.plot(history['val_loss'], label='Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.grid()\nplt.show()\nplt.figure(2)\nplt.plot(history['train_acc'], label='Train Accuracy')\nplt.plot(history['val_acc'], label='Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.grid()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# V. Making Predictions and Generate Submissions","metadata":{}},{"cell_type":"code","source":"y_pred, y_prob = get_predictions(model, test_dataloader)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_prob = torch.stack(y_prob).cpu().numpy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame()\ndf['id'] = test_data['id']\ndf['EAP'] = y_prob[:,0]\ndf['HPL'] = y_prob[:,1]\ndf['MWS'] = y_prob[:,2]\ndf.to_csv('Submission.csv', index=False)\ndf","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}