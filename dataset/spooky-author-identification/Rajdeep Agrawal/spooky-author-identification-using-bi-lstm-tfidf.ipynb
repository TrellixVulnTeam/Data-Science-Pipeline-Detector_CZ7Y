{"cells":[{"metadata":{},"cell_type":"markdown","source":"# I. Importing Libraries and Data","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nimport keras\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nimport tensorflow.keras.utils as ku\nimport nltk\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem import PorterStemmer\nfrom nltk.corpus import stopwords\nimport string","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data_train = pd.read_csv(\"/kaggle/input/spooky-author-identification/train.zip\")\ndata_val = pd.read_csv(\"/kaggle/input/spooky-author-identification/test.zip\")\n\nprint('Training data shape:',data_train.shape)\nprint('Validation data shape:',data_val.shape)\ndata_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# II. Text Preprocessing","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"StopWords = set(stopwords.words('english'))\n\ndef text_preprocess(text):\n    trans = str.maketrans('','',string.punctuation)\n    text = text.translate(trans)\n    text = ' '.join([word.lower() for word in text.split() if word.lower() not in StopWords])\n    return text\n\ndata_train['text'] = data_train['text'].apply(text_preprocess)\ndata_val['text'] = data_val['text'].apply(text_preprocess)\ndata_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# III. Tokenization and Lemmatization","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"label_encoder = LabelEncoder()\nX_train = data_train['text']\nX_train = X_train.tolist()\nX_test = data_val['text']\nX_test = X_test.tolist()\ny_train = data_train['author']\ny_train = label_encoder.fit_transform(y_train)\ny_train_cat = ku.to_categorical(y_train, num_classes=3)\nval_id = data_val['id']\n\nlemmatizer = WordNetLemmatizer()\nX_train_lemm = []\nfor text in X_train:\n    lem_text = ''\n    for word in text.split():\n        lem_word = lemmatizer.lemmatize(word, pos='v')\n        lem_word = lemmatizer.lemmatize(lem_word)\n        lem_text = lem_text + ' ' + lem_word\n    X_train_lemm.append(lem_text)\n\nX_test_lemm = []\nfor text in X_test:\n    lem_text = ''\n    for word in text.split():\n        lem_word = lemmatizer.lemmatize(word, pos='v')\n        lem_word = lemmatizer.lemmatize(lem_word)\n        lem_text = lem_text + ' ' + lem_word\n    X_test_lemm.append(lem_text)\n\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(X_train_lemm)\nvocab_size = len(tokenizer.word_index)\nmax_len = 150\ntrain_seq = tokenizer.texts_to_sequences(X_train_lemm)\ntrain_pad = pad_sequences(train_seq, maxlen=max_len)\ntest_seq = tokenizer.texts_to_sequences(X_test_lemm)\ntest_pad = pad_sequences(test_seq, maxlen=max_len)\n\nlabel2idx = {\n    'EAP': 0,\n    'HPL': 1,\n    'MWS': 2\n}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# IV. Training using TFIDF Vectorizer","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf = TfidfVectorizer(ngram_range=(1,3), min_df=5, max_df=0.5)\nX_train_tfidf = tfidf.fit_transform(X_train_lemm)\nX_test_tfidf = tfidf.transform(X_test_lemm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = LogisticRegression(max_iter=1000).fit(X_train_tfidf, y_train)\ny_pred = clf.predict(X_test_tfidf)\nprint(y_pred)\noutput_prob = clf.predict_proba(X_test_tfidf)\noutput_prob[:,0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# V. Training using Bi-LSTM NN","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model = keras.Sequential([\n    keras.layers.Embedding(vocab_size+1, 300, input_length=max_len),\n    keras.layers.SpatialDropout1D(0.5),\n    keras.layers.Bidirectional(keras.layers.LSTM(64, return_sequences=True, dropout=0.3, recurrent_dropout=0.3)),\n    keras.layers.Bidirectional(keras.layers.LSTM(32, dropout=0.3, recurrent_dropout=0.3)),\n    keras.layers.Dense(300, activation='relu'),\n    keras.layers.Dense(3, activation='softmax')\n])\nopt = keras.optimizers.Adam(learning_rate=0.01)\nmodel.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\nhistory = model.fit(train_pad, y_train_cat, epochs=20, batch_size=512)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_nn = model.predict_classes(test_pad)\nprint(y_pred_nn)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#cosine similarity between outputs from both methods.\nfrom sklearn.metrics.pairwise import cosine_similarity\ncosine_similarity([y_pred], [y_pred_nn])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Submission file.\ndf = pd.DataFrame()\ndf['id'] = val_id\ndf['EAP'] = output_prob[:,0]\ndf['HPL'] = output_prob[:,1]\ndf['MWS'] = output_prob[:,2]\n\ndf.to_csv('Submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}