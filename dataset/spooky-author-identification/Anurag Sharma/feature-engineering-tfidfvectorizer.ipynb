{"cells":[{"source":"import numpy as np\nimport os\nimport pandas as pd\nimport sys\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom nltk.corpus import wordnet as wn\nfrom nltk.corpus import stopwords\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.stem import PorterStemmer\nimport nltk\nfrom nltk import word_tokenize, ngrams\nfrom nltk.classify import SklearnClassifier\nfrom wordcloud import WordCloud,STOPWORDS\nnp.random.seed(25)","cell_type":"code","metadata":{"_uuid":"09c13dcbcf37d42b7ddff577ebca12ebe3c5048c","_cell_guid":"f3fa79a9-3350-46a9-995c-16c7c9792840","collapsed":true},"execution_count":null,"outputs":[]},{"source":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils.np_utils import to_categorical\nfrom keras.layers import Dense, Input, Flatten, merge, LSTM, Lambda, Dropout\nfrom keras.layers import Conv1D, MaxPooling1D, Embedding\nfrom keras.models import Model\nfrom keras.layers.wrappers import TimeDistributed, Bidirectional\nfrom keras.layers.normalization import BatchNormalization\nfrom keras import backend as K\nimport codecs","cell_type":"code","metadata":{"_uuid":"7a18143c2d2c89e2c4a764e9c1f8f700335bb6f9","_cell_guid":"9e84ca73-0915-4bdf-a02b-a3fcbaaed930","collapsed":true},"execution_count":null,"outputs":[]},{"source":"train = pd.read_csv(\"../input/hotel-review/train.csv\")\ntest = pd.read_csv(\"../input/hotel-review/test.csv\")","cell_type":"code","metadata":{"_uuid":"ce6631f5a3daf80925cc8ba32773fbe955de7ebc","_cell_guid":"92831422-0dc4-4278-bce6-a497c41da0a9","collapsed":true},"execution_count":null,"outputs":[]},{"source":"# Target Mapping\nmapping_target = {'happy':0, 'not happy':1}\ntrain = train.replace({'Is_Response':mapping_target})\n\n# Browser Mapping\nmapping_browser = {'Firefox':0, 'Mozilla':0, 'Mozilla Firefox':0,\n                  'Edge': 1, 'Internet Explorer': 1 , 'InternetExplorer': 1, 'IE':1,\n                   'Google Chrome':2, 'Chrome':2,\n                   'Safari': 3, 'Opera': 4\n                  }\ntrain = train.replace({'Browser_Used':mapping_browser})\ntest = test.replace({'Browser_Used':mapping_browser})\n# Device mapping\nmapping_device = {'Desktop':0, 'Mobile':1, 'Tablet':2}\ntrain = train.replace({'Device_Used':mapping_device})\ntest = test.replace({'Device_Used':mapping_device})","cell_type":"code","metadata":{"_uuid":"cc1a3dc4180aaddc41fc75c8048799fc630a6039","_cell_guid":"9ab34b6b-2731-4884-914c-81e2ed792ea7","collapsed":true},"execution_count":null,"outputs":[]},{"source":"GLOVE_DIR = '../input/glove-global-vectors-for-word-representation/'\nMAX_SEQUENCE_LENGTH = 300\nMAX_NB_WORDS = 200000\nEMBEDDING_DIM = 200\nVALIDATION_SPLIT = 0.01","cell_type":"code","metadata":{"_uuid":"6b545e1090771d7582ede1f73713f898d15de42f","_cell_guid":"f74d4820-c800-445a-9322-2fda8101c1a0","collapsed":true},"execution_count":null,"outputs":[]},{"source":"test_id = test['User_ID']\ntarget = train['Is_Response']","cell_type":"code","metadata":{"_uuid":"e01bd7c899e2da37ebcb2651909e97b3d3a27bda","_cell_guid":"7c079219-1497-4519-9cf1-37451ce0300b","collapsed":true},"execution_count":null,"outputs":[]},{"source":"# function to clean data\nimport string\nimport itertools \nimport re\nfrom nltk.stem import WordNetLemmatizer\nfrom string import punctuation\n\nstops = ['the','a','an','and','but','if','or','because','as','what','which','this','that','these','those','then',\n              'just','so','than','such','both','through','about','for','is','of','while','during','to','What','Which',\n              'Is','If','While','This']\n# punct = list(string.punctuation)\n# punct.append(\"''\")\n# punct.append(\":\")\n# punct.append(\"...\")\n# punct.append(\"@\")\n# punct.append('\"\"')\ndef cleanData(text, lowercase = False, remove_stops = False, stemming = False, lemmatization = False):\n    txt = str(text)\n    \n    # Replace apostrophes with standard lexicons\n    txt = txt.replace(\"isn't\", \"is not\")\n    txt = txt.replace(\"aren't\", \"are not\")\n    txt = txt.replace(\"ain't\", \"am not\")\n    txt = txt.replace(\"won't\", \"will not\")\n    txt = txt.replace(\"didn't\", \"did not\")\n    txt = txt.replace(\"shan't\", \"shall not\")\n    txt = txt.replace(\"haven't\", \"have not\")\n    txt = txt.replace(\"hadn't\", \"had not\")\n    txt = txt.replace(\"hasn't\", \"has not\")\n    txt = txt.replace(\"don't\", \"do not\")\n    txt = txt.replace(\"wasn't\", \"was not\")\n    txt = txt.replace(\"weren't\", \"were not\")\n    txt = txt.replace(\"doesn't\", \"does not\")\n    txt = txt.replace(\"'s\", \" is\")\n    txt = txt.replace(\"'re\", \" are\")\n    txt = txt.replace(\"'m\", \" am\")\n    txt = txt.replace(\"'d\", \" would\")\n    txt = txt.replace(\"'ll\", \" will\")\n    \n    # More cleaning\n    txt = re.sub(r\"review\", \"\", txt)\n    txt = re.sub(r\"Review\", \"\", txt)\n    txt = re.sub(r\"TripAdvisor\", \"\", txt)\n    txt = re.sub(r\"reviews\", \"\", txt)\n    txt = re.sub(r\"Hotel\", \"\", txt)\n    txt = re.sub(r\"what's\", \"\", txt)\n    txt = re.sub(r\"What's\", \"\", txt)\n    txt = re.sub(r\"\\'s\", \" \", txt)\n    txt = txt.replace(\"pic\", \"picture\")\n    txt = re.sub(r\"\\'ve\", \" have \", txt)\n    txt = re.sub(r\"can't\", \"cannot \", txt)\n    txt = re.sub(r\"n't\", \" not \", txt)\n    txt = re.sub(r\"I'm\", \"I am\", txt)\n    txt = re.sub(r\" m \", \" am \", txt)\n    txt = re.sub(r\"\\'re\", \" are \", txt)\n    txt = re.sub(r\"\\'d\", \" would \", txt)\n    txt = re.sub(r\"\\'ll\", \" will \", txt)\n    txt = re.sub(r\"60k\", \" 60000 \", txt)\n    txt = re.sub(r\" e g \", \" eg \", txt)\n    txt = re.sub(r\" b g \", \" bg \", txt)\n    txt = re.sub(r\"\\0s\", \"0\", txt)\n    txt = re.sub(r\" 9 11 \", \"911\", txt)\n    txt = re.sub(r\"e-mail\", \"email\", txt)\n    txt = re.sub(r\"\\s{2,}\", \" \", txt)\n    txt = re.sub(r\"quikly\", \"quickly\", txt)\n    txt = re.sub(r\" usa \", \" America \", txt)\n    txt = re.sub(r\" USA \", \" America \", txt)\n    txt = re.sub(r\" u s \", \" America \", txt)\n    txt = re.sub(r\" uk \", \" England \", txt)\n    txt = re.sub(r\" UK \", \" England \", txt)\n    txt = re.sub(r\"india\", \"India\", txt)\n    txt = re.sub(r\"switzerland\", \"Switzerland\", txt)\n    txt = re.sub(r\"china\", \"China\", txt)\n    txt = re.sub(r\"chinese\", \"Chinese\", txt) \n    txt = re.sub(r\"imrovement\", \"improvement\", txt)\n    txt = re.sub(r\"intially\", \"initially\", txt)\n    txt = re.sub(r\"quora\", \"Quora\", txt)\n    txt = re.sub(r\" dms \", \"direct messages \", txt)  \n    txt = re.sub(r\"demonitization\", \"demonetization\", txt) \n    txt = re.sub(r\"actived\", \"active\", txt)\n    txt = re.sub(r\"kms\", \" kilometers \", txt)\n    txt = re.sub(r\"KMs\", \" kilometers \", txt)\n    txt = re.sub(r\" cs \", \" computer science \", txt) \n    txt = re.sub(r\" upvotes \", \" up votes \", txt)\n    txt = re.sub(r\" iPhone \", \" phone \", txt)\n    txt = re.sub(r\"\\0rs \", \" rs \", txt) \n    txt = re.sub(r\"calender\", \"calendar\", txt)\n    txt = re.sub(r\"ios\", \"operating system\", txt)\n    txt = re.sub(r\"gps\", \"GPS\", txt)\n    txt = re.sub(r\"gst\", \"GST\", txt)\n    txt = re.sub(r\"programing\", \"programming\", txt)\n    txt = re.sub(r\"bestfriend\", \"best friend\", txt)\n    txt = re.sub(r\"dna\", \"DNA\", txt)\n    txt = re.sub(r\"III\", \"3\", txt) \n    txt = re.sub(r\"the US\", \"America\", txt)\n    txt = re.sub(r\"Astrology\", \"astrology\", txt)\n    txt = re.sub(r\"Method\", \"method\", txt)\n    txt = re.sub(r\"Find\", \"find\", txt) \n    txt = re.sub(r\"banglore\", \"Banglore\", txt)\n    txt = re.sub(r\" J K \", \" JK \", txt)\n\n    # Emoji replacement\n    txt = re.sub(r':\\)',r' Happy ',txt)\n    txt = re.sub(r':D',r' Happy ',txt)\n    txt = re.sub(r':P',r' Happy ',txt)\n    txt = re.sub(r':\\(',r' Sad ',txt)\n    \n    # Remove urls and emails\n    txt = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', ' ', txt, flags=re.MULTILINE)\n    txt = re.sub(r'[\\w\\.-]+@[\\w\\.-]+', ' ', txt, flags=re.MULTILINE)\n    \n    # Remove punctuation from text\n    txt = ''.join([c for c in text if c not in punctuation])\n#     txt = txt.replace(\".\", \" \")\n#     txt = txt.replace(\":\", \" \")\n#     txt = txt.replace(\"!\", \" \")\n#     txt = txt.replace(\"&\", \" \")\n#     txt = txt.replace(\"#\", \" \")\n    \n    # Remove all symbols\n    txt = re.sub(r'[^A-Za-z0-9\\s]',r' ',txt)\n    txt = re.sub(r'\\n',r' ',txt)\n    \n    txt = re.sub(r'[0-9]',r' ',txt)\n    \n    # Replace words like sooooooo with so\n    txt = ''.join(''.join(s)[:2] for _, s in itertools.groupby(txt))\n    \n    # Split attached words\n    #txt = \" \".join(re.findall('[A-Z][^A-Z]*', txt))   \n    \n    if lowercase:\n        txt = \" \".join([w.lower() for w in txt.split()])\n        \n    if remove_stops:\n        txt = \" \".join([w for w in txt.split() if w not in stops])\n    if stemming:\n        st = PorterStemmer()\n#         print (len(txt.split()))\n#         print (txt)\n        txt = \" \".join([st.stem(w) for w in txt.split()])\n    \n    if lemmatization:\n        wordnet_lemmatizer = WordNetLemmatizer()\n        txt = \" \".join([wordnet_lemmatizer.lemmatize(w, pos='v') for w in txt.split()])\n\n    return txt","cell_type":"code","metadata":{"_uuid":"7a8db0dce6abd3f2191911e09aeac01b7dea8602","_cell_guid":"544cc63c-c8c4-4378-9899-006fc9caac11","collapsed":true},"execution_count":null,"outputs":[]},{"source":"# clean description\ntrain['Description'] = train['Description'].map(lambda x: cleanData(x, lowercase=True, remove_stops=True, stemming=True, lemmatization = True))\ntest['Description'] = test['Description'].map(lambda x: cleanData(x, lowercase=True, remove_stops=True, stemming=True, lemmatization = True))","cell_type":"code","metadata":{"_uuid":"90911e4c16ea8148206eda7cffbeb413292e1f4c","_cell_guid":"d9dc880e-c6f3-4057-afd0-0a4dc63a10ea","collapsed":true},"execution_count":null,"outputs":[]},{"source":"test['Is_Response'] = np.nan\nalldata = pd.concat([train, test]).reset_index(drop=True)","cell_type":"code","metadata":{"_uuid":"111ff92a819b477289039b89b8e7100044fa9526","_cell_guid":"10125f89-3daa-4fb0-b58e-c0e466598207","collapsed":true},"execution_count":null,"outputs":[]},{"source":"tfidfvec = TfidfVectorizer(analyzer='word', ngram_range = (1,5), max_features=8000,sublinear_tf=True,\n                             use_idf=True)\ntfidfdata = tfidfvec.fit_transform(alldata['Description'])","cell_type":"code","metadata":{"_uuid":"af1a7a898517000bc344755bf2c18fc7995fa44e","_cell_guid":"62f6c755-9732-4ce5-a48a-ddbd2ff79e01","collapsed":true},"execution_count":null,"outputs":[]},{"source":"# create dataframe for features\ntfidf_df = pd.DataFrame(tfidfdata.todense())","cell_type":"code","metadata":{"_uuid":"bc01300b8f5fb499fc0b89ad2e8622b37aaf4c3f","_cell_guid":"1117cdcb-76cc-4531-bd55-de269e75629b","collapsed":true},"execution_count":null,"outputs":[]},{"source":"tfidf_df.columns = ['col' + str(x) for x in tfidf_df.columns]","cell_type":"code","metadata":{"_uuid":"4addab62fcc75d4bd5e23900916f9e8947fc383c","_cell_guid":"85ee1dc9-73ca-4321-836b-ca2cf3ff9f58","collapsed":true},"execution_count":null,"outputs":[]},{"source":"tfid_df_train = tfidf_df[:len(train)]\ntfid_df_test = tfidf_df[len(train):]","cell_type":"code","metadata":{"_uuid":"fff08bc1e0a3042f122af7c6a0bd3c75e57f631e","_cell_guid":"a14a125c-95aa-476e-90f2-c00b6a22cf0f","collapsed":true},"execution_count":null,"outputs":[]},{"source":"# split the merged data file into train and test respectively\ntrain_feats = alldata[~pd.isnull(alldata.Is_Response)]\ntest_feats = alldata[pd.isnull(alldata.Is_Response)]","cell_type":"code","metadata":{"_uuid":"af4f93a445fc4dd949b7f7ab6c84127ea79467ca","_cell_guid":"7e994804-e238-4d95-b881-ec626a1e9897","collapsed":true},"execution_count":null,"outputs":[]},{"source":"### set target variable\n\ntrain_feats['Is_Response'] = [1 if x == 'happy' else 0 for x in train_feats['Is_Response']]","cell_type":"code","metadata":{"_uuid":"f3571a0d3da337faff7cc507b41ee4b5dc950ae5","_cell_guid":"ae52ab9a-7add-48d2-933c-45eb6e46f854","collapsed":true},"execution_count":null,"outputs":[]},{"source":"# merge into a new data frame with tf-idf features\ncols = ['Browser_Used','Device_Used']\ntrain_feats2 = pd.concat([train_feats[cols], tfid_df_train], axis=1)\ntest_feats2 = pd.concat([test_feats[cols], tfid_df_test], axis=1)","cell_type":"code","metadata":{"_uuid":"7508e84c1f9dd2a5ab7c215db415ffeb9bf1594d","_cell_guid":"f04d7900-7ea8-4db5-9d01-a5021af58ce1","collapsed":true},"execution_count":null,"outputs":[]},{"source":"from sklearn.linear_model import LogisticRegression\n\nmod1 = LogisticRegression()\n#mod1 = LinearSVC()\ntarget = train['Is_Response']","cell_type":"code","metadata":{"_uuid":"34e77075c461fa7366379caf388cfc8560497aa4","_cell_guid":"4dc15d57-40a9-499a-bf41-325268c27409","collapsed":true},"execution_count":null,"outputs":[]},{"source":"## Naive Bayes 2 - tfidf is giving higher CV score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score, make_scorer\n#print(cross_val_score(mod1, train_feats2, target, cv=5, scoring=make_scorer(accuracy_score)))","cell_type":"code","metadata":{"_uuid":"99efe1b02067e79726a6644422a550ca705b510e","_cell_guid":"bd798b10-8436-4aaa-964f-1ec7934d2850","collapsed":true},"execution_count":null,"outputs":[]},{"source":"mod1.fit(train_feats2, target)","cell_type":"code","metadata":{"_uuid":"50ac7a4ecaef4ab0a2eb30ce6a3ebec521ca9ae5","_cell_guid":"a28ca5a8-6efe-48fe-8921-26a18dff55df","collapsed":true},"execution_count":null,"outputs":[]},{"source":"preds = mod1.predict(test_feats2)","cell_type":"code","metadata":{"_uuid":"d3af9fb25d059f65956ec0a0c903f3ba5cb34d48","_cell_guid":"bbee9e05-b4d3-41d6-bd7a-337cdebe0fb2","collapsed":true},"execution_count":null,"outputs":[]},{"source":"result = pd.DataFrame()\nresult['User_ID'] = test_id\nresult['Is_Response'] = preds\nmapping = {0:'happy', 1:'not_happy'}\nresult = result.replace({'Is_Response':mapping})\n\nresult.to_csv(\"lr_predicted_result_1.csv\", index=False)","cell_type":"code","metadata":{"_uuid":"a3ada5bfdbc3f8ad00925402e1ae6ea0a6b1aca3","_cell_guid":"ffdf85a8-7d83-402b-be9c-cb8abc50d7e1","collapsed":true},"execution_count":null,"outputs":[]},{"source":"","cell_type":"code","metadata":{"_uuid":"57d4e28385f699cf9d4d37d9b2473e310f204bfd","_cell_guid":"2b81dfd4-cc2b-480b-bdd7-dcd403f7ca77","collapsed":true},"execution_count":null,"outputs":[]}],"nbformat":4,"nbformat_minor":1,"metadata":{"kernelspec":{"language":"python","name":"python3","display_name":"Python 3"},"language_info":{"mimetype":"text/x-python","name":"python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","file_extension":".py","version":"3.6.3","nbconvert_exporter":"python"}}}