{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"![image.png](https://github.com/negrinij/NLP-AuthorsChallenge/blob/main/images/Spooky.png?raw=true)<br>\n>*Code to create this WordCloud below in section 1.5 [Word Cloud](#1.5-Word-Cloud)<br>*\n\nThis notebook is an analysis of the Kaggle Hallowen challenge from 2017. The goal is to predict the author from a dataset built from excerpts from horror stories by Edgar Allan Poe, Mary Shelley, and HP Lovecraft. \n\n## Contents\n1. [Data Analysis](#1.-Data-Analysis)<br>\n    1.1 [Analitycal Text Analysis](#1.1-Analitycal-Text-Analysis)<br>\n    1.2 [Sentiment Analysis](#1.2-Sentiment-Analysis)<br>\n    1.3 [Word Frequency](#1.3-Word-Frequency)<br>\n    1.4 [Co-occurrence Word Pattern](#1.4-Co-occurrence-Word-Pattern)<br>\n    1.5 [Word Cloud](#1.4-Word-Cloud)<br>\n2. [NLP Model](#2.-NLP-Model)<br>\n3. [Result and Conclusion](#3.-Result-and-Conclusion)<br>\n\n***If you find this Notebook helpful, please remember to upvote !***","metadata":{}},{"cell_type":"markdown","source":">Libraries","metadata":{}},{"cell_type":"code","source":"# initialize afinn sentiment analyzer\n!pip install afinn\nfrom afinn import Afinn\naf = Afinn()","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os,string,path\n\n#wordCloud\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nfrom urllib.request import urlopen\nfrom PIL import Image\n\n#Text Processing\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nimport re\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n%matplotlib inline\n\n#ML Model\nfrom sklearn import decomposition\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nimport lightgbm as lgbm\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import confusion_matrix\n\n#Optimisation\nimport pickle\nimport optuna\noptuna.logging.set_verbosity(optuna.logging.WARNING) #Disable Warnings","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":">Functions","metadata":{}},{"cell_type":"code","source":"def basic_EDA(df):\n    size = df.shape\n    sum_duplicates = df.duplicated().sum()\n    sum_null = df.isnull().sum().sum()\n    is_NaN = df. isnull()\n    row_has_NaN = is_NaN. any(axis=1)\n    rows_with_NaN = df[row_has_NaN]\n    count_NaN_rows = rows_with_NaN.shape\n    return print(\"Number of Samples: %d,\\nNumber of Features: %d,\\nDuplicated Entries: %d,\\nNull Entries: %d,\\nNumber of Rows with Null Entries: %d %.1f%%\" %(size[0],size[1], sum_duplicates, sum_null,count_NaN_rows[0],(count_NaN_rows[0] / df.shape[0])*100))\n\ndef countplot(df, x, x_axis_title,y_axys_title, plot_title):\n    plt.figure(figsize=(20,8))\n    sns.set(style=\"ticks\", font_scale = 1)\n    ax = sns.countplot(data = df,x=x,order = df[x].value_counts().index,palette=\"Blues_d\")\n    for p in ax.patches:\n        ax.annotate(\"%d\" % p.get_height(), (p.get_x() + p.get_width() / 2., abs(p.get_height())),\n        ha='center', va='bottom', color='black', xytext=(0, 3),rotation = 'horizontal',textcoords='offset points')\n    sns.despine(top=True, right=True, left=True, bottom=False)\n    plt.xticks(rotation=0,fontsize = 12)\n    ax.set_xlabel(x_axis_title,fontsize = 14,weight = 'bold')\n    ax.set(yticklabels=[])\n    ax.axes.get_yaxis().set_visible(False)  \n    plt.title(plot_title, fontsize = 16,weight = 'bold')  \n\ndef preprocess_sentence(df): #returns the whole sentence, with preprocessed text\n    word_list = []\n    text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', df) #remove punctuations\n    #text = text.lower() #lower case\n    tokenized_word=word_tokenize(text) #separate into words\n    for word in tokenized_word:\n        if word not in stop_words: #filter stop-words\n            word = stem.stem(word) #stemming\n            word_list.append(word) #append to general list\n    return ' '.join(word_list) #rejoins the sentence without the stopwords\n\ndef process_list(text): #returns a list of preprocessed words\n        word_list = []\n        #for t in text:            \n        text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text) #remove punctuations\n        text = text.lower() #lower case\n        tokenized_word=word_tokenize(text) #separate into words\n        for word in tokenized_word:\n            if word not in stop_words: #filter stop-words\n                word = stem.stem(word) #stemming\n                word_list.append(word) #append to general list\n        return word_list\n    \ndef build_freqs(texts, author):\n    authorslist = np.squeeze(author).tolist()\n    # Start with an empty dictionary and populate it by looping over all samples\n    # and over all processed words in each sample.\n    freqs = {}\n    words_sample = []\n    for text, author in zip(texts,authorslist):\n        for word in process_list(text):\n            words_sample.append(word)\n            pair = (word, author)\n            freqs[pair] = freqs.get(pair, 0) + 1  \n    return freqs,words_sample\n\nstop_words=set(stopwords.words(\"english\"))\nstem = PorterStemmer()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. Data Analysis\n\nIn this section we will perform an overall analysis of the data. First, we analyse if there are any duplicates, missing entries or any need for data cleansing. Next, we start to create new features to understand the trends between the authors, such as: word count per sample, number of characters and so on. In the following section, we use the Affin Sentiment analysis library to see if there is any interesting pattern between the authors, perhaps one author has a more negative style of writing than the others. The Word Frequency of the individual authors and for the whole corpus is also performed. Finally, a Word Co-Occurrence analysis is presented. \n\nThe analysis start below, by importing the training and test sets. A sample of the data is shown below:","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/input/spooky-author-identification/train.zip')\ndf_test = pd.read_csv('/kaggle/input/spooky-author-identification/test.zip')\npd.set_option('display.max_colwidth', None)\ndf_train.head()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Dictionary with Author names for more understandable Plots\nauthor_dict = {\n    'EAP': 'Edgar Allan Poe',\n    'HPL': 'HP Lovecraft',\n    'MWS': 'Mary Shelley'\n}","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From a basic EDA we conclude that there is no need for data cleansing, as there are no missing entries or duplicated values. The bar plot below shows the number of samples per author:","metadata":{}},{"cell_type":"code","source":"basic_EDA(df_train)\ndf_train['full_name'] = df_train['author'].map(author_dict.get) \ncountplot(df_train, 'full_name', 'Author','Count', 'Samples per Author - Training Set')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Edgar Allan Poe contains more samples if compared to the other two authors. However, the difference is not as critical as it would require any strategy for data unbalance for now\n* In case the model shows a bias toward EAP, one could remove some samples or use strategies to generate additional samples for this author","metadata":{}},{"cell_type":"markdown","source":"## 1.1 Analitycal Text Analysis\n\nIn the analytical analysis, we create the following features: \n\n* Word Count - Total number of words in the sample\n* Character Count - Total number of characters in the sample excluding spaces\n* Word Density - Average length of the words used in the sample\n* Punctuation Count - Total number of punctuations used in the sample\n* Upper-Case to Lower-Case Words ratio - ratio of upper case words used and lower case words used in the text","metadata":{}},{"cell_type":"code","source":"punctuations = string.punctuation\n\ndef new_feat(df):\n    df['word_count'] = df['text'].apply(lambda x : len(x.split()))\n    df['char_count'] = df['text'].apply(lambda x : len(x.replace(\" \",\"\")))\n    df['word_density'] = df['word_count'] / (df['char_count'] + 1)\n\n    #Adding +1 to allow ratio calculation\n    df['Uppercase'] = df['text'].str.findall(r'[A-Z]').str.len()+1\n    df['Lowercase'] = df['text'].str.findall(r'[a-z]').str.len()+1\n    df['UppLowRatio'] = df['Uppercase'] / (df['Lowercase'] + 1)\n\n    df['punc_count'] = df['text'].apply(lambda x : len([a for a in x if a in punctuations]))\n    \n    return df\n\ndf_train_new = new_feat(df_train)\n#df_test_new = new_feat(df_test)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A KDE plot can help us understand the pattern of the individual authors regarding the new features we created:","metadata":{}},{"cell_type":"code","source":"feat_list = ['word_count','char_count','word_density','Uppercase','Lowercase','UppLowRatio','punc_count']\n\nfor i in feat_list:\n    plt.figure(figsize=(20,5))\n    ax = sns.kdeplot(data = df_train_new, x = i, linewidth=1,alpha=.3, fill = True, hue = 'full_name',palette = 'husl') \n    ax.set_xlabel(i)\n    plt.title('KDE Plot - ' + i, fontsize = 16,weight = 'bold',pad=20);  \n    sns.despine(top=True, right=True, left=False, bottom=False)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* In a general manner, all authors have a similar distribution pattern \n* As EAP contains more samples, it usually presents higher density than the remaining authors on most analysis. However, for punctuation count, Upper and Lowercase Ratio it is surpassed by HP Lovecraft and Mary Shelley, respectively. It could mean that HP Lovecraft has a tendency for shorter sentences or more commas or semmi-colons than the other two. Regarding the Upper and Lower case ratio, it could be that Mary Shelley contains more characters or places that require Upper case when compared to EAP and HP.\n* All, but the word_density plot, presents a positive skewed distribution, meaning outliers will present higher values than the mean\n\nNow, let's visualise the outliers with a box-plot:","metadata":{}},{"cell_type":"code","source":"fig, axarr = plt.subplots(1,5, figsize=(20, 5))\n\nsns.set(style=\"ticks\", font_scale = 1)\nsns.despine(top=True, right=True, left=False, bottom=False)\n\nfeats = ['word_count','char_count','word_density','UppLowRatio','punc_count']\nz = 0\nfor j in range(0,5):\n    ax = sns.boxplot(data = df_train, x = 'full_name', y = df_train[feats[z]], ax=axarr[j],palette = 'husl');\n    axarr[j].tick_params(axis='x', rotation=70)\n    z +=1\n\naxarr[0].set_title(\"Number of Words per Author\")\naxarr[1].set_title(\"Number of Characters per Author\")\naxarr[2].set_title(\"Words and Characters Ratio per Author\")\naxarr[3].set_title(\"Upper/Lower Case Letters Ratio per Author\")\naxarr[4].set_title(\"Use of Punctuation per Author\")\n\nfig.tight_layout(pad=3.0)\nplt.suptitle('Statistical Count Features',fontsize=16, weight = 'bold');\n\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* The median, upper and lower quartile of the authors are quite similar across the features\n* Regarding the number of Words and Characters, (1st and 2nd plot), Mary Shelley presents more outliers with samples upto 800 words. Since the sentences have a lot of words, it is expected the same behaviour in the number of characters\n* Note how Mary Shelley presents less outliers for Upper/Lower Case ratio than the other two authors, not surprisingly she presents the higher peak in the previous KDE plot for this same feature","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(20,5))\nsns.set(style=\"ticks\", font_scale = 1)\nax = sns.scatterplot(data=df_train, x=\"word_count\", y='char_count', hue = 'author',alpha=0.8, palette = 'husl');\n#ax.set(xscale=\"log\", yscale=\"log\")\nsns.despine(top=True, right=True, left=False, bottom=False)\nplt.xticks(rotation=0,fontsize = 12)\nax.set_xlabel('Words',fontsize = 14,weight = 'bold')\nax.set_ylabel('Number of Characters',fontsize = 14,weight = 'bold')\nplt.title('Words and Number of Characters per Sample', fontsize = 16,weight = 'bold');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.2 Sentiment Analysis\n\nIn this section we perform a sentiment analysis using AFFIN lexicon. See https://github.com/darenr/afinn for more info. AFINN is a list of words rated for *valence* with an integer between minus five (negative) and plus five (positive). Here, we are using AFINN-en-165. Basically, for every word of the samples, a value is assigned. After evaluating all the words of a sample, the final result has a positive or negative score, indicating the author intent. \n\nClearly, this is a simplistic approach to evaluate the authors ideas. However, it can give us insights and it is also fun to see the algorithm results. \n\nLook at the result of some samples below, it does seem correct:","metadata":{}},{"cell_type":"code","source":"# compute sentiment scores (polarity) and labels\nsentiment_scores = [af.score(text) for text in df_train['text']]\nsentiment_category = ['positive' if score > 0 \n                          else 'negative' if score < 0 \n                              else 'neutral' \n                                  for score in sentiment_scores]\n    \n    \n# sentiment statistics per news category\nsentiment_df = pd.DataFrame([list(df_train['text']), list(df_train['full_name']),sentiment_scores, sentiment_category]).T\nsentiment_df.columns = ['text', 'author','score', 'sentiment_category']\nsentiment_df['score'] = sentiment_df.score.astype('float')\nsentiment_df.head()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Below is the sample with the most positive sentiment score. The score is computed by word, so longer samples will have higher scores. Something to keep in mind, and perhaps use some kind of normalisation for a more accurate result, but for this analysis it does no harm.","metadata":{}},{"cell_type":"code","source":"sentiment_df.loc[sentiment_df['score'].idxmax()]","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now the more negative sample found:","metadata":{}},{"cell_type":"code","source":"sentiment_df.loc[sentiment_df['score'].idxmin()]","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, let's count the number of positive, negative and neutral samples each author has:","metadata":{}},{"cell_type":"code","source":"g = sns.FacetGrid(sentiment_df, col='author', height=10,col_wrap = 3)\n\ng.map_dataframe(sns.countplot,data=sentiment_df, x='sentiment_category',palette=\"Blues_d\")\n\ng.set_titles(col_template=\"{col_name}\", row_template=\"{row_name}\", size = 18)\ng.set_xticklabels(rotation = 0, size = 16) \ng.fig.subplots_adjust(top=.5)\ng.fig.suptitle('Sentiment Analysis per Author',fontsize=20, weight = 'bold')\n\naxes = g.axes.flatten()\naxes[0].set_ylabel('Count')\n\ng.fig.tight_layout()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* All three authors contain a similar number of positive samples. It is interesting since Edgar Allan Poe represents 60% of the dataset but only a third of the positive samples\n* Edgar Allan Poe has double the amount of negative samples compared to HP Lovecraft. Even though EAP has more samples overall, it is the only author where the negative samples surpass the positives\n* Mary Shelley has a similar number of positive and negative entries. HP Lovecraft presents ~40% more positive samples than negative samples\n\nA quick look at the outliers for the Sentiment Analysis is performed below using Box-Plot:","metadata":{}},{"cell_type":"code","source":"authors = list(author_dict.values())\n\nfig, axarr = plt.subplots(1,3, figsize=(20, 5))\n\nz = 0\nfor j in authors:\n    df = sentiment_df[sentiment_df['author'] == j]\n    ax = sns.boxplot(data = df, x = 'sentiment_category', y = 'score', ax=axarr[z],palette = 'husl', order=[\"positive\", \"neutral\", \"negative\"])\n    ax.set(ylim=(-20, 20))\n    axarr[z].set_title(j)\n    axarr[z].set_xlabel('Sentiment Category',fontsize = 12,weight = 'bold')\n    axarr[z].set_ylabel('Score',fontsize = 12,weight = 'bold')\n    z +=1\n\nsns.despine(top=True, right=True, left=False, bottom=False)\nfig.tight_layout(pad=3.0)\nplt.suptitle('Sentiment Score Variation per Author',fontsize=16, weight = 'bold');","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Overall, all authors present a similar pattern for positive and negative samples (median and lower/upper quartiles)\n* Most samples are within the values of 5 and -5\n* Mary Shelley presents more extreme outliers, but the y-axis was limited to improve visualisation","metadata":{}},{"cell_type":"markdown","source":"## 1.3 Word Frequency By Author\n\nHere we analyse the commonly used words according to each author. Common **Stop Words** are excluded, and stemming is performed. The result is shown in the plots below:","metadata":{}},{"cell_type":"code","source":"freqs_author, words = build_freqs(df_train['text'], df_train['author'])\n\nfreq_words = []\n\nfor word in words:\n    MWS = 0\n    HPL = 0\n    EAP = 0\n    if (word, 'MWS') in freqs_author:\n        MWS = freqs_author[(word, 'MWS')]\n    if (word, 'HPL') in freqs_author:\n        HPL = freqs_author[(word, 'HPL')]\n    if (word, 'EAP') in freqs_author:\n        EAP = freqs_author[(word, 'EAP')]      \n    freq_words.append([word, MWS,HPL,EAP])   \n\nfreq_wordsDF = pd.DataFrame(freq_words, columns = ['word', 'MWS','HPL','EAP'])    \nfreq_wordsDF['sum'] =  freq_wordsDF.loc[:, ['MWS','HPL','EAP']].sum(axis=1)\nfreq_wordsDF.sort_values('sum', ascending=False,inplace=True)\nfreq_wordsDF.drop_duplicates(inplace=True)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"z = 0; j = 0\nfig, axarr = plt.subplots(1,3, figsize=(20, 5))\n\nauthors_abbr = list(author_dict.keys())\n\nfor i in authors_abbr:\n    df = freq_wordsDF.loc[:,['word',i]]\n    df.sort_values(i, ascending=False,inplace=True)\n    ax = sns.lineplot(data=df[0:20],x=\"word\", y=i, marker='o',ax=axarr[z])\n    axarr[z].tick_params(axis='x', rotation=70)    \n    axarr[z].set_xlabel('The 20 Most Common Words for ' + i,fontsize = 12,weight = 'bold')\n    axarr[z].set_ylabel('Count',fontsize = 12,weight = 'bold')\n    axarr[z].set_title(i, fontsize = 14,weight = 'bold');\n    sns.despine(top=True, right=True, left=False, bottom=False)\n    z+=1\n    #print(FreqDF[0:15]['word'])\n    \nfig.tight_layout(pad=3.0)\nplt.suptitle('Word Frequency per Author - Top 20 Words',fontsize=16, weight = 'bold');\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Edgar Allan Poe contains more samples on this dataset, what explains (sort of) the impressive number of times the author uses the **upon** word\n* Mary Shelley and HP Lovecraft have a more similar curve, where the word frequency reduces linearly upto the 5th word. Edgar Allan Poe shows a sharp decrease between the 1st, 2nd  and 3rd word. \n* We can see from the results that the word frequency follows a exponential curve, i.e. if we were to add more words to this analysis, the word frequency count would slowly decrease until reached 0 \n* The initial words are somewhat similar, however, note how the words start to vary significantly accross the authors as we arrive to the top 10th or 15th word. It is interesting how the difference between the authors show soon on this analysis\n\nBelow we have a plot that compares the usage of the same word across the three authors. A 3D plot and a 2d cross-sections are shown to better visualise:","metadata":{}},{"cell_type":"code","source":"fig, axarr = plt.subplots(1,3, figsize=(20, 5))\n\nax = sns.scatterplot(x =(freq_wordsDF['MWS']+1), y = (freq_wordsDF['HPL']+1), ax=axarr[0],hue = freq_wordsDF['sum'],hue_norm=(0,800), size = freq_wordsDF['sum'])\naxarr[0].plot([0, 500], [0, 500], color = 'red') # Plot the red line that divides the 2 areas.\nax.set(xscale=\"log\", yscale=\"log\")\naxarr[0].set_xlabel('MWS',fontsize = 12,weight = 'bold')\naxarr[0].set_ylabel('HPL',fontsize = 12,weight = 'bold')\naxarr[0].set_title('MWS and HPL Word Frequency Visualisation', fontsize = 14,weight = 'bold');\n\nax1 = sns.scatterplot(x = freq_wordsDF['MWS']+1, y = freq_wordsDF['EAP']+1, ax=axarr[1],hue = freq_wordsDF['sum'],hue_norm=(0,800), size = freq_wordsDF['sum']) \nax1.set(xscale=\"log\", yscale=\"log\")\naxarr[1].plot([0, 1000], [0, 1000], color = 'red') # Plot the red line that divides the 2 areas\naxarr[1].set_xlabel('MWS',fontsize = 12,weight = 'bold')\naxarr[1].set_ylabel('EAP',fontsize = 12,weight = 'bold')\naxarr[1].set_title('MWS and EAP Word Frequency Visualisation', fontsize = 14,weight = 'bold');\n\nax2 = sns.scatterplot(x = freq_wordsDF['HPL']+1, y = freq_wordsDF['EAP']+1, ax=axarr[2],hue = freq_wordsDF['sum'],hue_norm=(0,800), size = freq_wordsDF['sum']) \nax2.set(xscale=\"log\", yscale=\"log\")\naxarr[2].plot([0, 1000], [0, 1000], color = 'red') # Plot the red line that divides the 2 areas\naxarr[2].set_xlabel('HPL',fontsize = 12,weight = 'bold')\naxarr[2].set_ylabel('EAP',fontsize = 12,weight = 'bold')\naxarr[2].set_title('EAP and HPL Word Frequency Visualisation', fontsize = 14,weight = 'bold');\nsns.despine(top=True, right=True, left=False, bottom=False)\n\nfig.tight_layout(pad=3.0)\nplt.suptitle('Comparison of Word Frequency per Author  - Top 50 Words',fontsize=16, weight = 'bold');\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* The graphs are plotted in logarithmic scale to better visualise the pattern of words with lower incidence\n* A 3D plot is also shown below. However, sometimes a 3D plot is not as helpful as 2D cross-sections\n* The plots demonstrate the most common words (darker colour and larger markers) are mostly neutral, near the red line that separate the authors \n* Note the outlier for EAP at the top (2nd and 3rd graphs). This author seems obsessed with the word **upon**, as we saw in the previous plot\n* The vertical and horizontal patterns are words that are used by one author and are either 0 or a very low value for the other author. \n* Words that have a lower total count (bright colour) seem to be the ones that are more particular for each author, i.e points that are further from the red line, and present a high value in only one of the axis","metadata":{}},{"cell_type":"code","source":"import plotly.express as px\nfig = px.scatter_3d(x=(freq_wordsDF['MWS']+1), y=(freq_wordsDF['HPL']+1), z=(freq_wordsDF['EAP']+1),\n                    opacity=0.9,\n                    log_x=True,log_y=True,log_z=True,color=freq_wordsDF[\"sum\"], size=freq_wordsDF[\"sum\"])\n#fig.update_traces(marker=dict(size=3))#, selector=dict(type='scatter3d'))\n\nfig.update_layout(title=dict(text = 'Word Frequency by Author - 3D Scatter'), \n                  scene = dict(xaxis_title='MWS',yaxis_title='HPL',zaxis_title='EAP',\n                              xaxis = dict(backgroundcolor=\"rgb(200, 200, 230)\"),\n                              yaxis = dict(backgroundcolor=\"rgb(230, 200,230)\"),\n                              zaxis = dict(backgroundcolor=\"rgb(230, 230,200)\")))\n\nfig.update_traces(hovertemplate='MWS: %{x} <br>HPL: %{y} <br>EAP: %{z}') #\nfig.update_layout(margin=dict(l=0, r=0, b=0, t=0))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.4 Co-occurrence Word Pattern\n\nHere we study the relantioship between words. The intent is to understand which words are most commonly used in the same sample. \n\nThe **CountVecorizer** library is used to perform this analysis, and it is defined to extract the top 20 words by setting **max_features = 20**. Adding more features makes it harder to interpret the data. \n\nA heatmap is used to demonstrate the relationship between the words:","metadata":{}},{"cell_type":"code","source":"Corpus = df_train['text'].sample(frac=0.6, random_state=1) #extract sample from Dataset, Adding the whole dataset does not change the pattern\nCorpus = list(Corpus) #The CountVectorizer needs the inputs as list\n\ncount_model = CountVectorizer(preprocessor = preprocess_sentence, max_features = 20) #extracts the top most used words, uses my function to preprocess\nCountVectData = count_model.fit_transform(Corpus) #fits and transforms to my corpus\nCountVectDatac = (CountVectData.T * CountVectData) # this is co-occurrence matrix in sparse csr format\nCountVectDatac.setdiag(0) # fill same word co-occurence as zero, they have much higher numbers than the remaining words\n\n#Create DF\nCountVectDF = pd.DataFrame(CountVectDatac.A, columns=count_model.get_feature_names(), index = count_model.get_feature_names()) #creates DF \nhour_count = CountVectDF.sum(axis=1) #Creates a sum column to be used to order entries\n\nsorter = hour_count.sort_values(ascending = False).index #sorter of rows and columns\nCountVectDF = CountVectDF[sorter] #sorts columns according to most occurred word\nCountVectDF = CountVectDF.reindex(sorter) #sorts rows in the same order as columns\n\n#Plot\nmask = np.zeros_like(CountVectDF)\nmask[np.triu_indices_from(mask)] = True\nwith sns.axes_style(\"white\"):\n    f, ax = plt.subplots(figsize=(14,10))\n    ax = sns.heatmap(CountVectDF, mask=mask, vmax = 70, vmin = 20, linewidths=.8,annot=False,cmap = 'Reds',annot_kws={\"size\": 10},cbar=True)\nplt.title('Words  Co-occurrence Pattern Plot', fontsize = 16,weight = 'bold');    ","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* By ranking the words according to the number of co-occurrence, the plot created a nice vertical gradient\n* There is a clear vertical pattern break from the word **even** and onwards\n* The gradient shows that the words co-occur the most are used together most often, i.e. the top three words **could, would, upon** are commonly used with **one** (the most co-occurred word)\n* For most words, the peak of occurrence occurs with one of the top four words (**one, could, would, upon**)\n* Outlier from this trend are the words **seem** and **look**, which can also be commonly used with **eye** and **like**\n\nAnother interesting analysis would be to create the same plot by author or by sentiment analysis class (positive, negative, neutral).","metadata":{}},{"cell_type":"markdown","source":"## 1.5 Word Cloud\n\nNo text analysis is complete without a word cloud. Here, I created the mask using PowerPoint Icons and saved as PNG image. Just in case is not clear, the image is a pumpkin, a bat and a headstone with Rest in peace (RIP), or from the *Latin*, Requiescat In Pace.","metadata":{}},{"cell_type":"code","source":"author_mask = np.array(Image.open(urlopen('https://github.com/negrinij/NLP-AuthorsChallenge/blob/main/images/Hallowen1.PNG?raw=true')))\n   \nwc = WordCloud(background_color=\"black\",contour_width=0.1,colormap = 'YlOrRd',max_words=2000, mask=author_mask,max_font_size=36, random_state=42)\ntext = \" \".join(review for review in df_train.text)\nwc.generate(text)\n\nplt.figure(figsize=[20,10])\nplt.imshow(wc, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. NLP Model\n\nTo build the NLP model, this [notebook](https://www.kaggle.com/abhishek/approaching-almost-any-nlp-problem-on-kaggle) has really helped to better understand the NLP process. As mentioned on that Notebook, Kaggle is using a multi-class log-loss as evaluation metric. The LogLoss is a Logarithmic Loss metric, as such, the lower the value the better.\n\nThe implementation was taken from [here](https://github.com/dnouri/nolearn/blob/master/nolearn/lasagne/util.py):","metadata":{}},{"cell_type":"code","source":"def multiclass_logloss(actual, predicted, eps=1e-15):\n    \"\"\"Multi class version of Logarithmic Loss metric.\n    :param actual: Array containing the actual target classes\n    :param predicted: Matrix with class predictions, one probability per class\n    \"\"\"\n    # Convert 'actual' to a binary array if it's not already:\n    if len(actual.shape) == 1:\n        actual2 = np.zeros((actual.shape[0], predicted.shape[1]))\n        for i, val in enumerate(actual):\n            actual2[i, val] = 1\n        actual = actual2\n\n    clip = np.clip(predicted, eps, 1 - eps)\n    rows = actual.shape[0]\n    vsota = np.sum(actual * np.log(clip))\n    return -1.0 / rows * vsota","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.1 Data Preprocessing\n\nFor the Data Analysis section we played with preprocessing. Now we are going to prepare our dataset for our ML model:\n* Encode the Labels\n* Pre-Process text samples (stemming, stopwords, lowercase, remove punctuations)\n* Oragnise the Training, Validation and Test Sets\n* Use CountVectorizes to build the word vector\n\nAn additional step that has improved performance was to add the most common words to the **Stop_Words** list. The model improved by removing *one, could, and would* from the samples. In addition, not transforming the words to lowercase also increased model performance.","metadata":{}},{"cell_type":"markdown","source":">Data Preprocessing and Creation of Training and Test Sets","metadata":{}},{"cell_type":"code","source":"stop_words.update(('one','could','would'))\n\ndf_train = pd.read_csv('/kaggle/input/spooky-author-identification/train.zip')\ndf_test = pd.read_csv('/kaggle/input/spooky-author-identification/test.zip')\n\ndf_train['text_pre'] = df_train['text'].apply(lambda x : preprocess_sentence(x))\ndf_test['text_pre'] = df_test['text'].apply(lambda x : preprocess_sentence(x))\n\nLabelEnc = preprocessing.LabelEncoder()\ntarget_train = LabelEnc.fit_transform(df_train.author.values)\nfeatures_train = df_train.text_pre.values\n\nfeatures_test = df_test.text_pre.values\n\n# Create First Train and Test sets\nx_train, x_test, y_train, y_test = train_test_split(features_train, target_train, test_size=0.20,random_state=123)\n\nprint (\"Training set size\", x_train.shape[0])\nprint (\"Test set size\",x_test.shape[0])","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":">Using CountVectorize to transform the data and train our Naive-Bayes and Logistic Regression Models","metadata":{}},{"cell_type":"code","source":"CVec = CountVectorizer(analyzer='word',ngram_range=(1, 3),dtype=np.float32)\n\n# Fitting Count Vectorizer to training and test sets\nx_train_CVec =  CVec.fit_transform(x_train) \nx_test_CVec = CVec.transform(x_test)\n\nsubmission_test = CVec.transform(features_test)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":">I have also tried using TF.IDF to transform the data and train the models. However, CountVectorizer has shown to be the best option for this application","metadata":{}},{"cell_type":"code","source":"tfidf_vec = TfidfVectorizer(dtype=np.float32, sublinear_tf=True, use_idf=True, smooth_idf=True)\n\n# Fitting TFIDF to trainingand test sets\nx_train_tfidf = tfidf_vec.fit_transform(x_train)\nx_test_tfidf = tfidf_vec.transform(x_test)\n\n# Apply SVD, I chose 120 components. 120-200 components are good enough for SVM model.\n#svd = decomposition.TruncatedSVD(n_components=200)\n#xtrain_svd = svd.fit_transform(x_train_tfidf)\n#xtest_svd = svd.transform(x_test_tfidf)\n\n# Scale the data obtained from SVD. Renaming variable to reuse without scaling.\n#Scaler = preprocessing.StandardScaler()\n#xtrain_svd_scl = Scaler.fit_transform(xtrain_svd)\n#xtest_svd_scl = Scaler.transform(xtest_svd)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The models used for this study are the classic Naive-Bayes (NB) and Logistic Regression (LR), commonly used for NLP tasks. In addition, I am also using LGBM as it usually provides a good trade-off between accuracy and training time.\n\nThe model hyperparameters were optimised using the Optuna Library. The code for the optimisation is commented out as it requires a long time for the LGBM and LR.","metadata":{}},{"cell_type":"code","source":"# The classification Models\nLR = LogisticRegression(C=1, solver = 'lbfgs', max_iter = 1000)\nNB = MultinomialNB(alpha = 1.3)\nclf_LGBM = lgbm.LGBMClassifier(objective='multiclass',\n                               metric = 'multi_logloss',\n                               verbose=-1, \n                               learning_rate=0.08, \n                               max_depth=87, \n                               num_leaves=91, \n                               early_stopping_round = 100, \n                               n_estimators = 10000,\n                               reg_alpha = 0.0003,\n                               reg_lambda = 0.02897)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We validate the models using a K-Fold strategy. Five folds were selected as this ratio provides a good balance between training and validation sets. For the cross-validation, approximately 12.000 samples are used for training and 3.000 for validation for each fold. The mean LogLoss for the five folds and the Standard Deviation are recorded for model evaluation. Ideally, the CV result and the separate Test set scores should be similar as an indication that the model is not overfitting.","metadata":{}},{"cell_type":"code","source":"kf = KFold(n_splits=5, shuffle = True, random_state = 123)\n\ndef CV(model,x_train,x_test):\n    logloss = []\n    for train_index, test_index in kf.split(x_train):\n            X_train, X_val = x_train[train_index], x_train[test_index]\n            Y_train, Y_val = y_train[train_index], y_train[test_index]        \n            model.fit(X_train, Y_train)\n            predictions = model.predict_proba(X_val)\n            logloss.append(multiclass_logloss(Y_val, predictions))\n    test_score = multiclass_logloss(y_test, model.predict_proba(x_test))\n    mean_res = np.mean(logloss)\n    std_dev = np.std(logloss)\n    return mean_res,std_dev, test_score, model    \n\ndef CV_LGBM(model,x_train,x_test):\n    logloss = []\n    for train_index, test_index in kf.split(x_train):\n            X_train, X_val = x_train[train_index], x_train[test_index]\n            Y_train, Y_val = y_train[train_index], y_train[test_index]\n            model.fit(X_train, Y_train, eval_metric='multi_logloss', eval_set=[(X_val, Y_val)],verbose=0)\n            predictions = model.predict_proba(X_val)\n            logloss.append(multiclass_logloss(Y_val, predictions))\n    test_score = multiclass_logloss(y_test, model.predict_proba(x_test))\n    mean_res = np.mean(logloss)\n    std_dev = np.std(logloss)\n    return mean_res,std_dev,test_score,model  \n\nLR_Model = CV(LR,x_train_CVec,x_test_CVec)\nNB_Model = CV(NB,x_train_CVec,x_test_CVec)\nLGBM_Model = CV_LGBM(clf_LGBM,x_train_CVec,x_test_CVec)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"An additional ensemble model is created to verify if such strategy could provide any significant improvement to our model. The sklearn library provides the VotingClassifier module to facilitate the ensemble model construction.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier\nensemble = VotingClassifier(estimators=[('LR', LR), ('NB', NB)], voting='soft', weights=[1,1.5])\nEnsemble_Model = CV(ensemble,x_train_CVec,x_test_CVec)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_model(trial):\n    #regularization = trial.suggest_loguniform('logistic-regularization', 0.0001, 1)\n    #lr_solver = trial.suggest_categorical('solver', [\"sag\", \"saga\",'lbfgs'])\n    #model = LogisticRegression( C=regularization, solver='lbfgs', max_iter = 100)#,intercept_scaling = lr_intercept_scaling)\n    \n    #NBAlpha = trial.suggest_loguniform('logistic-regularization', 0.0001, 1)\n    #model = MultinomialNB(alpha = NBAlpha)\n    \n    #LGBM_alpha = trial.suggest_loguniform('LGBM_alpha', 0.0001, 1)\n    #LGBM_lambda = trial.suggest_loguniform('LGBM_lambda', 0.0001, 1)\n    LGBM_num_leaves = trial.suggest_int('LGBM_num_leaves', 5, 500)\n    LGBM_max_depth = trial.suggest_int('LGBM_max_depth', 5, 500)\n    LGBM_LR = trial.suggest_loguniform('LGBM_LR', 0.01, 0.1)\n    #LGBM_estimators = trial.suggest_int('LGBM_estimators', 50, 10000)\n    \n    model = lgbm.LGBMClassifier(objective = 'multiclass',\n                           metric = 'multi_logloss',\n                           learning_rate = LGBM_LR,\n                           early_stopping_round = 50,\n                           n_estimators = 10000,\n                           max_depth=LGBM_max_depth, num_leaves=LGBM_num_leaves,reg_alpha = 0.0003,reg_lambda = 0.02897,verbose = -1)  \n    \n    if trial.should_prune():\n            raise optuna.TrialPruned()\n    return model\n\ndef objective(trial):\n    model = create_model(trial)\n    accuracy,_,_,_ = CV_LGBM(clf_LGBM,x_train_CVec,x_test_CVec)\n    # Save a trained model to a file.\n    with open(\"{}.pickle\".format(trial.number), \"wb\") as fout:\n        pickle.dump(model, fout)\n    return accuracy\n\n#study = optuna.create_study(direction=\"minimize\");\n#study.optimize(objective, n_trials=10);\n#print(\"Mean of Cross-Validation Sets: %.4f\"% (study.best_value))\n#print(\"The best parameters are: \", (study.best_params))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Result\n\nHere we discuss the results from the modelling strategy we applied. The print statement below show us the LogLoss for each model for the Cross-Validation and Test sets.","metadata":{}},{"cell_type":"code","source":"print(\"Logistic Regression \\n LogLoss: %.3f +/- %.4f \\n Test Set LogLoss: %.3f\" % (LR_Model[0],LR_Model[1],LR_Model[2]))\nprint(\"Naive-Bayes \\n LogLoss: %.3f +/- %.4f \\n Test Set LogLoss: %.4f\" % (NB_Model[0],NB_Model[1],NB_Model[2]))\nprint(\"LGBM \\n LogLoss: %.3f +/- %.4f \\n Test Set LogLoss: %.3f\" % (LGBM_Model[0],LGBM_Model[1],LGBM_Model[2]))\nprint(\"Ensemble NB and LR \\n LogLoss: %.3f +/- %.4f \\n Test Set LogLoss: %.3f\" % (Ensemble_Model[0],Ensemble_Model[1],Ensemble_Model[2]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Logistic Regression and Naive-Bayes outperformed LGBM by almost a decimal point\n* The Ensemble Model has improved the result considerably, showing a smaller LogLoss and also reducing the Standard Deviation between the validation sets\n* As LGBM has shown the worse result, it will not be considered for further Error Analysis","metadata":{}},{"cell_type":"code","source":"LR_preds = LR_Model[3].predict(x_test_CVec)\nNB_preds = NB_Model[3].predict(x_test_CVec)\nEnsemble_preds = Ensemble_Model[3].predict(x_test_CVec)\n\ncategories = ['EAP', 'HPL','MWS']\n\ndef CMatrix(predictions, title):\n    CMatrix = pd.DataFrame(confusion_matrix(y_test, predictions), columns=categories, index =categories)\n\n    plt.figure(figsize=(12, 6))\n    ax = sns.heatmap(CMatrix, annot = True, fmt = 'g' ,vmin = 0, vmax = 500,cmap = 'Blues')\n    ax.set_xlabel('Predicted',fontsize = 14,weight = 'bold')\n    ax.set_xticklabels(ax.get_xticklabels(),rotation =90);\n    ax.set_ylabel('Actual',fontsize = 14,weight = 'bold')    \n    ax.set_title('Confusion Matrix - Test Set ' + title,fontsize = 16,weight = 'bold',pad=20);\n\nCMatrix(LR_preds,'Logistic Regression')\nCMatrix(NB_preds,'Naive-Bayes')\nCMatrix(Ensemble_preds,'Ensemble')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* EAP is most often misclassified as MWS than HPL, this trend is seen on all three ML models\n* In a similar manner, HPL is most often misclassified as EAP\n* MWS samples also present higher number of samples misclassified as EAP\n* All three ML models present the same pattern of misclassifications, i.e. HPL and MWS samples being misclassified as EAP\n* The Ensemble Model presents a higher number of TP for MWS and EAP and reduced the TP for HPL\n\nThe code below is to prepare the CSV file for submission. Since the competition is no longer open, only manual submission is allowed. ","metadata":{}},{"cell_type":"code","source":"ensemble_preds = Ensemble_Model[3].predict_proba(submission_test)\nids = df_test['id']\npredict = pd.DataFrame(ensemble_preds, columns=['EAP','HPL','MWS'])\nsubmission = pd.concat([ids, predict] ,axis = 1)\nsubmission.to_csv('submission.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Conclusion\n\nAs final remarks, it was quite surprising that Logistic Regression and Naive-Bayes have outperformed the LGBM model. Perhaps more effort into the hyperparameter optimisation could improve the LGBM model, but I am not sure if it would overcome the results for NB. As most Data Science problems, great improvements can be achieved by better understanding the data and where the model is finding difficulties. \n\nAs the next step, it would be advised to individually analyse the samples our model are mislabelling and see if there are any hints. Perhaps, shorter samples are more challenging or specific words could be removed to improve our model. In addition, other steps of the text preprocessing (such as removing symbols and punctuation) could be harming the prediction performance. Balancing the samples could be beneficial to increase the prediction performance for MWS and HPL.\n\n## I hope you enjoyed this notebook as it is my initial insight into NLP. Any suggestions or corrections are welcome. ","metadata":{}}]}