{"cells":[{"metadata":{},"cell_type":"markdown","source":"### What is Topic Modeling\nIn machine learning and natural language processing, a topic model is a type of statistical model for discovering the abstract \"topics\" that occur in a collection of documents. Topic modeling is a frequently used text-mining tool for discovery of hidden semantic structures in a text body. Intuitively, given that a document is about a particular topic, one would expect particular words to appear in the document more or less frequently: \"dog\" and \"bone\" will appear more often in documents about dogs, \"cat\" and \"meow\" will appear in documents about cats, and \"the\" and \"is\" will appear approximately equally in both. A document typically concerns multiple topics in different proportions; thus, in a document that is 10% about cats and 90% about dogs, there would probably be about 9 times more dog words than cat words. The \"topics\" produced by topic modeling techniques are clusters of similar words. A topic model captures this intuition in a mathematical framework, which allows examining a set of documents and discovering, based on the statistics of the words in each, what the topics might be and what each document's balance of topics is."},{"metadata":{},"cell_type":"markdown","source":"## Import Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nfrom sklearn.manifold import TSNE\nimport matplotlib.colors as mcolors\nimport gensim\nfrom gensim.utils import simple_preprocess\nfrom gensim.models import Phrases, phrases, ldamodel, CoherenceModel\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nimport spacy\nimport gensim.corpora as corpora\nfrom pprint import pprint\nimport pyLDAvis\nimport pyLDAvis.gensim \nfrom collections import Counter","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_df = pd.read_csv('/kaggle/input/spooky-author-identification/train.zip')\ndata_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# shape of the dataset\ndata_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# value count for eavh author\nsns.countplot(data_df['author'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# null value\ndata_df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We don't required id column so we will going to drop it"},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop id column\ndata_df = data_df.drop(columns = ['id'], axis=1)\ndata_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Visulization"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_df['Number_of_words'] = data_df['text'].apply(lambda x:len(str(x).split()))\ndata_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.style.use('ggplot')\nplt.figure(figsize=(12,6))\nsns.distplot(data_df['Number_of_words'],kde = False,color=\"red\",bins=200)\nplt.title(\"Frequency distribution of number of words for each text extracted\", size=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cloud=WordCloud(colormap=\"winter\",width=600,height=400).generate(str(data_df[\"text\"]))\nfig=plt.figure(figsize=(13,18))\nplt.axis(\"off\")\nplt.imshow(cloud,interpolation='bilinear')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now it's time to clean the text"},{"metadata":{},"cell_type":"markdown","source":"## Data Cleaning "},{"metadata":{},"cell_type":"markdown","source":"Clean the text by lowering all words,removing special characters, numbers and stopwords"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_df['text_processed'] = data_df['text'].map(lambda x: re.sub('[,\\.!?]','',x))\ndata_df['text_processed'] = data_df['text_processed'].map(lambda x:x.lower())\nprint(data_df['text_processed'].head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove all characters, number or characters\ndef cleanText(input_string):\n    modified_string = re.sub('[^A-Za-z0-9]+', ' ', input_string)\n    return(modified_string)\ndata_df['text_processed'] = data_df.text_processed.apply(cleanText)\ndata_df['text_processed'][150]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove stopwords\nstopWords = stopwords.words('english')\ndef removeStopWords(stopWords, rvw_txt):\n    newtxt = ' '.join([word for word in rvw_txt.split() if word not in stopWords])\n    return newtxt\ndata_df['text_processed'] = [removeStopWords(stopWords,x) for x in data_df['text_processed']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## WordCloud"},{"metadata":{"trusted":true},"cell_type":"code","source":"# join the different text together\nlongText = ','.join(list(data_df['text_processed'].values))\n# generate the word cloud\nwordcloud = WordCloud(background_color=\"black\",\n                      max_words= 600,\n                      contour_width = 10,\n                      contour_color = \"steelblue\",\n                     collocations=False).generate(longText)\n# visualize the word cloud\nfig = plt.figure(1, figsize = (12, 12))\nplt.axis('off')\nplt.imshow(wordcloud)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(1, figsize = (20,10))\n# split() returns list of all the words in the string\nsplit_it = longText.split()\n# Pass the split_it list to instance of Counter class.\nCounter = Counter(split_it)\n#print(Counter)\n# most_common() produces k frequently encountered\n# input values and their respective counts.\nmost_occur = Counter.most_common(30)\nx_df = pd.DataFrame(most_occur, columns=(\"words\",\"count\"))\nsns.barplot(x = 'words', y = 'count', data = x_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preparing data for Topic Modelling"},{"metadata":{},"cell_type":"markdown","source":"First of all we will do tokenization then will do lemmatization"},{"metadata":{"trusted":true},"cell_type":"code","source":"nltk.download(\"punkt\")\n# word_tokenize \ndata_df[\"tokenized\"] = data_df[\"text_processed\"].apply(lambda x: nltk.word_tokenize(x))\ndata_df[\"tokenized\"] = data_df[\"tokenized\"].apply(lambda words: [word for word in words if word.isalnum()])\ndata_df\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.stem import WordNetLemmatizer \nnltk.download('wordnet')\ndef word_lemmatizer(text):\n  lem_text = [WordNetLemmatizer().lemmatize(i,pos='v') for i in text]\n  return lem_text\ndata_df[\"lemmatized\"] = data_df[\"tokenized\"].apply(lambda x: word_lemmatizer(x))\ndata_df[\"lemmatize_joined\"] = data_df[\"lemmatized\"].apply(lambda x: ' '.join(x))\npd.set_option('display.max_colwidth', 100)\ndata_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's see 30 most frequent words"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.style.use('ggplot')\nplt.figure(figsize=(14,6))\nfreq=pd.Series(\" \".join(data_df[\"lemmatize_joined\"]).split()).value_counts()[:30]\nfreq.plot(kind=\"bar\", color = \"orangered\")\nplt.title(\"30 most frequent words\",size=20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Vectorization using Word2Vec**"},{"metadata":{"trusted":true},"cell_type":"code","source":"tokens = data_df[\"lemmatize_joined\"].apply(lambda x: nltk.word_tokenize(x))\ntokens","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gensim \nfrom gensim.models import Word2Vec \nw2v_model = Word2Vec(tokens,\n                     min_count=20,\n                     window=10,\n                     size=250,\n                     alpha=0.03, \n                     min_alpha=0.0007,\n                     workers = 4,\n                     seed = 42)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model"},{"metadata":{},"cell_type":"markdown","source":"The input will be in the form of document-term matrix, and we will convert that using the below piece of code."},{"metadata":{"trusted":true},"cell_type":"code","source":"dictionary = corpora.Dictionary(data_df[\"lemmatized\"])\ndoc_term_matrix = [dictionary.doc2bow(rev) for rev in data_df[\"lemmatized\"]]\n\nLDA = gensim.models.ldamodel.LdaModel\n\n# Build LDA model\nlda_model = LDA(corpus=doc_term_matrix, id2word=dictionary, num_topics=3, random_state=100,\n                chunksize=200, passes=100)\nlda_model.print_topics()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# COHERENCE SCORE\ncoherence_model_lda = CoherenceModel(model=lda_model,\ntexts= data_df[\"lemmatized\"], dictionary=dictionary, coherence='c_v')\ncoherence_lda = coherence_model_lda.get_coherence()\nprint('\\nCoherence Score: ', coherence_lda)\n\n# # Compute Perplexity\nprint('\\nPerplexity: ', lda_model.log_perplexity(doc_term_matrix))  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pyLDAvis.enable_notebook()\nvis = pyLDAvis.gensim.prepare(lda_model, doc_term_matrix, dictionary)\npyLDAvis.display(vis)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}