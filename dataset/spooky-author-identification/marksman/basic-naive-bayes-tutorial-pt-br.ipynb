{"cells":[{"source":"# Spooky Author Identification (pt-BR)\nThis is an instructional notebook for Brazilian Portuguese Speakers\n\nEste notebook é um exemplo de submissão para a competição Spooky Author Identification do Kaggle. Ele foi pensado com o objetivo de que iniciantes no ramo de Machine Learning tenham um guia simples de como fazer sua primeira submissão.\n\nO que estamos usando aqui é um classificador do tipo Naive Bayes e usando o básico dos pacotes d sicikit learn para trabalhar com texto.\n\nVamos iniciar importando as bibliotecas básicas","metadata":{},"cell_type":"markdown"},{"source":"import pandas as pd   # manipulacao de dados do CSV\nimport numpy as np    # algebra linear e calculos em geral","metadata":{"collapsed":true},"cell_type":"code","outputs":[],"execution_count":1},{"source":"Nosso próximo passo é carregar os arquivos usando pandas.","metadata":{},"cell_type":"markdown"},{"source":"train_df = pd.read_csv('../input/train.csv')\ntest_df = pd.read_csv('../input/test.csv')\nsubmission_df = pd.read_csv('../input/sample_submission.csv')","metadata":{},"cell_type":"code","outputs":[],"execution_count":3},{"source":"Vamos dar uma olhada nesses arquivos? O primeiro ponto importante é sabermos que tipo de informação nós temos.","metadata":{},"cell_type":"markdown"},{"source":"train_df.head()","metadata":{},"cell_type":"code","outputs":[],"execution_count":4},{"source":"Legal. O arquivo test_df tem o mesmo format, exceto que não tem a coluna de author. É o que estamos querendo prever, certo?","metadata":{},"cell_type":"markdown"},{"source":"test_df.head()","metadata":{},"cell_type":"code","outputs":[],"execution_count":5},{"source":"O arquivo de submission é um exemplo de como devemos mandas as previsões. Vamos usar ele como template mais pra frente. O importante aqui é notar que na submissão, a previsão é baseada em probabilidades.","metadata":{},"cell_type":"markdown"},{"source":"submission_df.head()","metadata":{},"cell_type":"code","outputs":[],"execution_count":6},{"source":"Mais um passo importante na hora de entendermos os dados é saber se o nosso dataset está equilibrado. Isso pode alterar o nosso tipo de abordagem de classificação.","metadata":{},"cell_type":"markdown"},{"source":"count_by_author = train_df.groupby('author')['id'].count()\ncount_by_author","metadata":{},"cell_type":"code","outputs":[],"execution_count":7},{"source":"Ok. Ele não é perfeitamente equilibrado, mas também não é muito ruim. Vamos calcular o nosso baseline, ou seja: qual seria a performance de um classificador tirivial, que sempre prediz a classe mais provável? (Nesse caso, Edgard Allan Poe)","metadata":{},"cell_type":"markdown"},{"source":"count_by_author.max()/count_by_author.sum()","metadata":{},"cell_type":"code","outputs":[],"execution_count":8},{"source":"Em resumo: nosso classificador \"inteligente\" tem que acertar mais de 40% pra ser melhor que o classificador trivial.","metadata":{},"cell_type":"markdown"},{"source":"## Preparação dos Dados\n\nO sklear, que é a biblioteca de ML que vamos usar, trabalha apenas com dados numéricos. Logo, vamos ter que dar uma massageada nos dados, porque tudo o que temos no CSV é texto. Lembra o que vimos lá em cima?\n\n### Bag of Words\nO primeiro passo que vamos fazer é transformar o texto de cada linha em uma representação de Bag of Words. Isso vai incluir o processo de tokenização.","metadata":{},"cell_type":"markdown"},{"source":"from sklearn.feature_extraction.text import CountVectorizer\nbow = CountVectorizer()","metadata":{"collapsed":true},"cell_type":"code","outputs":[],"execution_count":9},{"source":"Vamos pedir para que o CountVectorizer \"aprenda\" o vocabulário do nosso texto. Isso vai fazer com que ele seja capaz de trabalhar com Bag of Words.\n\n*Importante*: veja que estamos ensinando o vocabulário usando os dois datasets: train e test! Isso é importante porque pode ser que existam palavras no dataset de test que não exista no dataset de treino.","metadata":{},"cell_type":"markdown"},{"source":"bow.fit(test_df.append(train_df)['text'])","metadata":{},"cell_type":"code","outputs":[],"execution_count":10},{"source":"Ótimo! Agora ele conhece o vocabulário. Quer ver?\n\nVamos começar espiando o testo da primeira amostra do dataset de treino.","metadata":{},"cell_type":"markdown"},{"source":"print(train_df.iloc[0].text)","metadata":{},"cell_type":"code","outputs":[],"execution_count":11},{"source":"print(bow.vocabulary_['process'])\nprint(bow.vocabulary_['afforded'])\nprint(bow.vocabulary_['ascertaining'])\nprint(bow.vocabulary_['this'])","metadata":{},"cell_type":"code","outputs":[],"execution_count":12},{"source":"print(\"Tamanho do Vocabulario aprendido:\", len(bow.vocabulary_))","metadata":{},"cell_type":"code","outputs":[],"execution_count":13},{"source":"Nesse processo as palavras foram todas convertidas para minusculas","metadata":{},"cell_type":"markdown"},{"source":"'This' in bow.vocabulary_","metadata":{},"cell_type":"code","outputs":[],"execution_count":14},{"source":"Ótimo. Agora que já temos um vocabulário, hora de transformar o nosso texto em números, usando a representação de Bag of Words. O resultado vai ser uma matriz em que cada linha é uma amostra (alinhada com o dataset de treinamento) e cada coluna representa o número de vezes que aquela palavra apareceu.","metadata":{},"cell_type":"markdown"},{"source":"train_X_bow = bow.transform(train_df['text'])\ntrain_X_bow","metadata":{},"cell_type":"code","outputs":[],"execution_count":15},{"source":"É um pouco difícil de trabalhar com matrizes esparsas. Sabemos que essa matriz tem 19579 linhas (são as amostras) e 28300 colunas (são as palavras). Vamos espiar uma linha pra entender melhor o que está acontecendo? Vamos espiar a primeira linha.","metadata":{},"cell_type":"markdown"},{"source":"x_bow_0 = train_X_bow[0].toarray().reshape(-1)\nx_bow_0","metadata":{},"cell_type":"code","outputs":[],"execution_count":16},{"source":"Ainda difícil de ver. Vamos reordenar, pra ficar mais fácil. Vamos ordenar em ordem decrescente, para saber quais as palavras mais frequentes nesse texto. Pelo modelo de bag of words, essas seriam as mais importantes.","metadata":{},"cell_type":"markdown"},{"source":"idx = np.argsort(-x_bow_0)[:20]\nprint(idx)\nprint(x_bow_0[idx])\nprint(np.array(bow.get_feature_names())[idx])","metadata":{},"cell_type":"code","outputs":[],"execution_count":17},{"source":"Fica aí a reflexão: você acha que essa ordenação de fato reflete a importância dessas palavras? Tem jeito de melhorar isso?\n\nDica: stop words e TF-IDF","metadata":{},"cell_type":"markdown"},{"source":"### TF-IDF\nTF-IDF para os íntimos, é a sigla de Term Frequency - Inverse Document Frequency. Trata-se de uma transformação sobre o modelo de Bag of Words para tentar resolver alguns dos problemas que vimos ali em cima.\n\nEssa transformação faz uma mágica: ele diminui a importância de palavras que aparecem em muitos documentos (como the, of, etc) e aumenta a importância de palavras que são mais raras naquelo documento: ou seja - que provavelmente são mais alinhadas com o estilo do autor ou do assunto. Quer saber como calcular o TF-IDF? É fácil, mas a gente não vai tratar disso aqui. [https://en.wikipedia.org/wiki/Tf%E2%80%93idf]\n\nPrimeiro nós precisamos fazer o TF-IDF Transformer \"entender\" quais as palavras comuns e quais não são comuns.","metadata":{},"cell_type":"markdown"},{"source":"from sklearn.feature_extraction.text import TfidfTransformer\ntfidf = TfidfTransformer()\ntfidf.fit(train_X_bow)","metadata":{},"cell_type":"code","outputs":[],"execution_count":18},{"source":"Agora que ele já sabe quais as palavras comuns, hora de transformar o nosso bag of words","metadata":{},"cell_type":"markdown"},{"source":"train_X_tfidf = tfidf.transform(train_X_bow)\ntrain_X_tfidf","metadata":{},"cell_type":"code","outputs":[],"execution_count":19},{"source":"Note que o tamanho da matriz é exatamente o mesmo. Vamos espiar o conteúdo?","metadata":{},"cell_type":"markdown"},{"source":"x_tfidf_0 = train_X_tfidf[0].toarray().reshape(-1)\nx_tfidf_0","metadata":{},"cell_type":"code","outputs":[],"execution_count":20},{"source":"Ainda difícil de ver porque é bem esparso. Vamos ordenar?","metadata":{},"cell_type":"markdown"},{"source":"idx = np.argsort(-x_tfidf_0)[:20]\nprint(idx)\nprint(x_tfidf_0[idx])\nprint(np.array(bow.get_feature_names())[idx])","metadata":{},"cell_type":"code","outputs":[],"execution_count":21},{"source":"E aí? Agora faz mais sentido esse critério de importância (ou característica) de cada autor?\n\nFica aqui mais uma reflexão: é sempre desejável termos essa representação","metadata":{},"cell_type":"markdown"},{"source":"### LabelEncoding do target\nO nome do autor também é texto. Vamos ter que dar um jeito de converter os nomes dos autores para valores numérico, porque é o jeito que o sklearn trabalhar. Os 3 textos que temos que transformar são EAP, HPL e MWS. \n\nO método que vamos usar é chamado de Label Encoding. Ou seja: a cada texto, vamos atribuir um inteiro. Como por exemplo: EAP = 0, HPL = 1 e MWS = 2.\n\nAntes de atribuirmos, precisamos que o Encoder \"aprenda\" quais as categorias existentes.","metadata":{},"cell_type":"markdown"},{"source":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train_df['author'])\nle.classes_","metadata":{},"cell_type":"code","outputs":[],"execution_count":22},{"source":"Ótimo! Agora o LabelEncoder já sabe quais são as categorias que ele tem que mapear. Agora precisamos efetivamente converter a coluna author. Bora lá.","metadata":{},"cell_type":"markdown"},{"source":"train_y = le.transform(train_df['author'])\ntrain_y","metadata":{},"cell_type":"code","outputs":[],"execution_count":23},{"source":"## Treinando um Modelo Preditivo\nAgora que já temos os dados preparados e transformados em dados numéricos, podemos treinar o nosso modelo de machine learning.\n\nPara essa competição, vamos usar um classificador que em geral tem uma boa performance trabalhando com texto. Ele é baseado em estatística bayesiana e é normalmente chamado de Naive Bayes (Naive porque ele acredita que os dados não tenham relação entre si...). Não vamos entrar em detalhes aqui do que é um modelo bayesiano. O que importrante pra gente: ele é bom em calcular probabilidades. Se ele sabe que 40% dos textos são do Edgard Alan Poe e que a palavra \"ascertaining\" tem 0,03% de chance de aparecer num texto do Poe e que \"uniform\" tem 0,01% de chance de aparecer num texto do Poe, um texto que tem as palavras \"uniform\" e \"ascertaining\" tem qual a probabilidade de ser um texto do Poe? E da Mary Shelley? E do Lovecraft?\n\nEsse é o tipo de cálculo que esse classificador faz. Quer ver mais detalhes? https://en.wikipedia.org/wiki/Naive_Bayes_classifier\n\nCuriosidade: os filtros de Spam funcionam exatamente com esse tipo de classificador.","metadata":{},"cell_type":"markdown"},{"source":"from sklearn.naive_bayes import MultinomialNB\nnb = MultinomialNB(alpha=1.0)","metadata":{"collapsed":true},"cell_type":"code","outputs":[],"execution_count":24},{"source":"Para treinar o modelo, temos que passar pra ele as \"features\" ou características - nesse caso, TFIDF - e quais são os targets, pra que ele possa calcular as probabilidades.","metadata":{},"cell_type":"markdown"},{"source":"nb.fit(train_X_tfidf, train_y)","metadata":{},"cell_type":"code","outputs":[],"execution_count":25},{"source":"Modelo treinado. Com o modelo treinado, podemos começar usá-lo para fazer predições. Só de farra, vamos usar o próprio dataset de treinamento para fazer uma previsão e ver como ele se comporta.","metadata":{},"cell_type":"markdown"},{"source":"y_pred = nb.predict(train_X_tfidf)\ny_pred","metadata":{},"cell_type":"code","outputs":[],"execution_count":26},{"source":"Ou seja: ele previu que o primeiro texto é do Poe (0), o segundo também, ... e o último é do Lovecraft (1)\n\n## Avaliando o Modelo\nMas e aí. Esse modelo é bom? Qual é a taxa de acerto?","metadata":{},"cell_type":"markdown"},{"source":"from sklearn.metrics import accuracy_score\naccuracy_score(train_y, y_pred)","metadata":{},"cell_type":"code","outputs":[],"execution_count":27},{"source":"Boa! 89%! Nada mal! ;-) Bem melhor que os 40% do nosso classificador trivial.\n\nPonto de atenção: veja que nós estamos usando os dados de treino pra prever os dados de treino. No mundo real não é assim que funciona: o seu modelo tem que prever dados que nunca viu na vida. Logo, não é um número que dá pra confiar. Vamos ver isso um pouco mais pra frente.","metadata":{},"cell_type":"markdown"},{"source":"No comecinho desse notebok nós falamos que o que a competição pede na verdade é qual é a probabilidade de autoria. Veja um exemplo do arquivo de submissão.","metadata":{},"cell_type":"markdown"},{"source":"submission_df.head()","metadata":{},"cell_type":"code","outputs":[],"execution_count":28},{"source":"Nós conseguimos gerar essas probabilidades usando o método predict_proba.","metadata":{},"cell_type":"markdown"},{"source":"y_pred_proba = nb.predict_proba(train_X_tfidf)\ny_pred_proba","metadata":{},"cell_type":"code","outputs":[],"execution_count":29},{"source":"Nessa competição, a métrica que eles escolheram não foi accuracy.\n\nÉ uma outra métrica chamada Log Loss, ou Cross Entropy. O que essa métrica faz (através de uma fórmula não muito complicada) é penalizar quando você calcula as probabilidades de forma muito errada e te premia quando as probabilidades estão certas. \n\nOu seja: se você previou com 100% de certeza que o texto é do Edgard Alan Poe mas errou, ele te dá uma penalização alta. Se você previou com 40% e errou, ele ainda te penaliza, mas a penalização é menor.\n\nSe quiser saber como calular, pode dar uma espiada aqui: https://en.wikipedia.org/wiki/Cross_entropy\n\n*Importante*: ao contrário da Acurácia, o Log Loss melhora quando diminui. Quando menor, melhor.","metadata":{},"cell_type":"markdown"},{"source":"from sklearn.metrics import log_loss\nlog_loss(y_pred=y_pred_proba, y_true=train_y)","metadata":{},"cell_type":"code","outputs":[],"execution_count":30},{"source":"E isso aí? É bom ou ruim? Log Loss é uma métrica difícil de interpretar. O que posso dizer é que nesse momento, com esse Log Loss você estaria mais ou menos na metade do Leader Board da competição. Mas lembre-se: não dá pra confiar, porque estamos avaliando o modelo com os próprios dados que usamos pra treinar.","metadata":{},"cell_type":"markdown"},{"source":"### Cross Validation\nPara poder avaliar o modelo mais próximo do mundo real, nós temos que fazer previsões com dados que não foram vistos durante o treino. Isso normalmente é feito separando os dados, reservando parte apenas para avaliar o modelo. Essa técnica é chamada de Holdout ou Split e funciona bem quando você tem muitos dados.\n\nO que nós vamos usar aqui é um método diferente, chamado de K-Fold cross validation. É parecido com o holdout, só o que nós vamos fazer é separar o dataset em k grupos diferentes. E aí vamos usar o primeiro grupo como holdout e os seguintes para treinar. Em seguida pegamos o segundo como holdout e os outros pra treinar. E assim por diante. Na prática, se temos um 10-Fold cross validation, vamos treinar o modelo 10 vezes e avaliar 10 vezes usando dados que não foram usados durante o treinamento.\n\nVamos começar criando a classe que \"splita\" o nosso DataFrame","metadata":{},"cell_type":"markdown"},{"source":"from sklearn.model_selection import cross_val_predict, StratifiedKFold\ncv = StratifiedKFold(n_splits=10, random_state=42)","metadata":{"collapsed":true},"cell_type":"code","outputs":[],"execution_count":31},{"source":"O próximo passo é usar o cross_val_predict (dá pra usar o cross_val_score, também). O que ele faz é fazer fit e predict nas 10 folds e gerar o que a comunidade chama de \"Out of Fold Prediction\". No final, calculamos a acurácia.","metadata":{},"cell_type":"markdown"},{"source":"y_pred = cross_val_predict(MultinomialNB(alpha=1.0),\n                           train_X_tfidf,\n                           train_y,\n                           cv=cv)\naccuracy_score(train_y, y_pred)","metadata":{},"cell_type":"code","outputs":[],"execution_count":32},{"source":"Notou que o valor é menor do que quando calculamos a acurácia apenas usando o dataset de treinamento? Esse número provavelmente é muito mais próximo do desempenho que ele vai ter na vida real.\n\nVamos fazer o mesmo com o log_loss, que é a métrica oficial dessa competição.","metadata":{},"cell_type":"markdown"},{"source":"y_pred_proba = cross_val_predict(MultinomialNB(alpha=1.0),\n                                 train_X_tfidf,\n                                 train_y,\n                                 cv=cv,\n                                 method='predict_proba')\nfrom sklearn.metrics import log_loss\nlog_loss(train_y, y_pred_proba)","metadata":{},"cell_type":"code","outputs":[],"execution_count":33},{"source":"Notou que ele é pior que o train score?","metadata":{},"cell_type":"markdown"},{"source":"## Preparando a Submissão\n\nHora de preparar a nossa submissão. Vamos pegar o nosso modelo e prever os resultados a partir do arquivo de teste. É assim que o Kaggle avalia o seu modelo: entendendo como ele se comporta em um conjunto de dados que não foi visto durante o treinamento. Antes de fazer a previsão, precisamos aplicar exatamente as mesmas transformações que fizemos no dataset de treinamento.\n\nComeçando com a conversão pra bag of words...","metadata":{},"cell_type":"markdown"},{"source":"test_X_bow = bow.transform(test_df['text'])\ntest_X_bow","metadata":{},"cell_type":"code","outputs":[],"execution_count":34},{"source":"Aplicando o TF-IDF...","metadata":{},"cell_type":"markdown"},{"source":"test_X_tfidf = tfidf.transform(test_X_bow)\ntest_X_tfidf","metadata":{},"cell_type":"code","outputs":[],"execution_count":35},{"source":"E, finalmente, fazendo a previsão. Vamos usar o predict_proba, porque é o que o Kaggle espera.","metadata":{},"cell_type":"markdown"},{"source":"y_pred_proba = nb.predict_proba(test_X_tfidf)\ny_pred_proba","metadata":{},"cell_type":"code","outputs":[],"execution_count":36},{"source":"### Gerando o arquivo de submissão\n\nAgora só falta gerar o arquivo que vai ser submetido. Lembrando, o formato de arquivo esperado é o seguinte:","metadata":{},"cell_type":"markdown"},{"source":"submission_df.head()","metadata":{},"cell_type":"code","outputs":[],"execution_count":37},{"source":"Agora precisamos atribuir os valores que foram previstos pelo modelo.","metadata":{},"cell_type":"markdown"},{"source":"submission_df['EAP'] = y_pred_proba[:, 0]\nsubmission_df['HPL'] = y_pred_proba[:, 1]\nsubmission_df['MWS'] = y_pred_proba[:, 2]","metadata":{"collapsed":true},"cell_type":"code","outputs":[],"execution_count":38},{"source":"O resultado final vai ser parecido com isso.","metadata":{},"cell_type":"markdown"},{"source":"submission_df.head()","metadata":{},"cell_type":"code","outputs":[],"execution_count":39},{"source":"Ou seja: parao primeiro texto, nosso modelo acha que tem 29% de chance de ser do Poe, 8% de chance de ser do Lovecraft e 62% da Mary Shelley. E aí? Qual é o autor real? Bom, a realidade é que niguém sabe. O kaggle mantem essa informação secreta e usa esses dados secretos para calcular o score real do seu modelo.\n\nSó por referência, esse é o texto que ele está prevendo na primeira linha. Você acha que acertamos dizendo que é da Mary Shelley? Nunca saberemos. :-)","metadata":{},"cell_type":"markdown"},{"source":"test_df.iloc[0].text","metadata":{},"cell_type":"code","outputs":[],"execution_count":40},{"source":"Agora só falta gravar o arquivo. E submeter.","metadata":{},"cell_type":"markdown"},{"source":"submission_df.to_csv('basic-submission-multonmial-nb.csv', index=False)","metadata":{},"cell_type":"code","outputs":[],"execution_count":43},{"source":"Anote aqui, pra depois deixar documentado.\n\nQual foi o seu Log Loss calculado no Cross Validation?\n\nCV=?\n\nQual foi o seu Los Loss calculado pelo Kaggle no Leader Board?\n\nLB=?","metadata":{},"cell_type":"markdown"},{"source":"## Sugestões de Exercícios\nDaqui pra frente, a sugestão é tentar melhorar o modelo. Tente alterar os parâmetros de modelagem e descobrir qual o melhor modelo que você conseguir. Tente alterar o parâmetro alpha no classificador. Seus resultados melhoram? E se não usarmos Tf-Idf, usando direto o Bag of Words? E se usássemos stop words, teria diferença?\n\nSe souber trabalhar com outros classificadores, tente mudar o tipo de classificador (Ex: RandomForest).\n\nUse a sua Cross Validation para descobrir qual o melhor modelo e no final faça uma nova submissão. Documente os resultados.","metadata":{},"cell_type":"markdown"},{"source":"","metadata":{"collapsed":true},"cell_type":"code","outputs":[],"execution_count":null}],"nbformat":4,"metadata":{"language_info":{"nbconvert_exporter":"python","version":"3.6.3","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"name":"python","file_extension":".py","mimetype":"text/x-python"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}},"nbformat_minor":1}