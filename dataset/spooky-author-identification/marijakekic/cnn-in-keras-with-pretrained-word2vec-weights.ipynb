{"nbformat_minor":1,"metadata":{"language_info":{"version":"3.6.3","codemirror_mode":{"version":3,"name":"ipython"},"name":"python","nbconvert_exporter":"python","file_extension":".py","pygments_lexer":"ipython3","mimetype":"text/x-python"},"kernelspec":{"name":"python3","language":"python","display_name":"Python 3"}},"cells":[{"metadata":{"_uuid":"1010815f52d2195d8e92d78ddb841ad3c7460385","_cell_guid":"7ba4023b-1fc8-4be6-92e3-6e29794d3804"},"source":"This kernel is building and training convolutional network for text classification. The architecture of the netwok is the same as in the paper of Kim https://arxiv.org/pdf/1408.5882.pdf","cell_type":"raw"},{"metadata":{"_uuid":"089876c2d029abb38330ccbdf33545ae031c39e5","_cell_guid":"96b710c9-7a89-4b90-a119-6ff2724aa971"},"source":"### Let's load data to see what we are dealing with ###","cell_type":"markdown"},{"metadata":{"_uuid":"78bb9ac640d6625586229d046c67a7c2461c5242","collapsed":true,"_cell_guid":"fd1f69c4-82bc-4d9f-9e8d-943100b19382"},"source":"import numpy as np\nimport pandas as pd","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"84556514b1f02ead3472578ae471e5fef0761f09","_cell_guid":"50771083-1694-4319-be99-2d41b83ac492"},"source":"train_data=pd.read_csv('../input/train.csv')\ntest_data=pd.read_csv('../input/test.csv')","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"663f6daba8931118803fdd688cd73fc363e7b0d3","_cell_guid":"29e22d03-3aca-4ffd-a832-84baa7a18c9a"},"source":"train_data.head(3)","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"b566fe50ebd12e87e4ddfcd7d6a8395b01d4869e","_cell_guid":"e18aa2f8-4795-4af7-ae02-369b14663a81"},"source":"test_data.head(3)","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"b64134fa96c9556ee5e2ee989b1ef077e30a268c","_cell_guid":"89c33848-6339-48c7-86ef-63881e4b1940"},"source":"### Lets see the size, and if we have missing values in the datasets ###","cell_type":"markdown"},{"metadata":{"_uuid":"4c040610445465e7a206129f7920dc1cbae2d98f","_cell_guid":"a89365e0-b3b3-44c1-8c7e-eeeaaa4aaaf6"},"source":"print(train_data.shape,test_data.shape)\nprint(train_data.isnull().sum())\nprint(test_data.isnull().sum())","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"2ac19cb93b0baf1703c670e6ac96031d5d9804a9","_cell_guid":"69eb323f-8978-4eed-8534-0eed635856ec"},"source":"### Ok, seems clean ###\n","cell_type":"markdown"},{"metadata":{"_uuid":"97cbcd59c301c8225d5fb22e70878bf4190bd32e","_cell_guid":"760bf481-0f8e-4aea-a5a6-478c87ab8e90"},"source":"### Lets make categories of authors ###","cell_type":"markdown"},{"metadata":{"_uuid":"c48fed101812eaea561e1333a1c9158cd73ae128","collapsed":true,"_cell_guid":"6951f4bb-1519-4683-aa03-15789ad76668"},"source":"authors=train_data.author.unique()\ndic={}\nfor i,author in enumerate(authors):\n    dic[author]=i\nlabels=train_data.author.apply(lambda x:dic[x])","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"8a56c49cdedaba88d515945f486374888e2dc71f","_cell_guid":"52e669fb-03cc-4feb-af2c-f9d7e6cee600"},"source":"### Lets divide our training data to train and validation ###","cell_type":"markdown"},{"metadata":{"_uuid":"c42864d9c3033c9b77632b869fbcecfc6a0a3b16","collapsed":true,"_cell_guid":"39119655-fc12-41eb-b739-c0f68cdf57a3"},"source":"val_data=train_data.sample(frac=0.2,random_state=200)\ntrain_data=train_data.drop(val_data.index)","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"4e3a192bd5a5aae37bd6a2fcb527e9855e760f47","_cell_guid":"8a86b60b-7303-42fe-bf6d-1e6bbd6c5e77"},"source":"### Tokenize text of the training data with keras text preprocessing functions ###","cell_type":"markdown"},{"metadata":{"_uuid":"de2ae8c8e7548657bac8ece11ca928205779bd33","_cell_guid":"67810d98-bc8c-49ea-8285-55cf14c74100"},"source":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"c8d1f2f7dc08f93c34d91bedc9002b5fd5524c1d","collapsed":true,"_cell_guid":"e7381224-7e75-4733-9de1-5e60d353bc7f"},"source":"texts=train_data.text","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"d8a393afae2528f97f24fa1d49179340212582bf","_cell_guid":"73f85346-389c-481e-b182-07436c5004cc"},"source":"NUM_WORDS=20000\ntokenizer = Tokenizer(num_words=NUM_WORDS,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\'',\n                      lower=True)\ntokenizer.fit_on_texts(texts)\nsequences_train = tokenizer.texts_to_sequences(texts)\nsequences_valid=tokenizer.texts_to_sequences(val_data.text)\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"e453033d2b7514d2d184d623e5d1acbd537f9487","_cell_guid":"2af3b95b-1560-4a2e-89c4-921a31fb9120"},"source":"X_train = pad_sequences(sequences_train)\nX_val = pad_sequences(sequences_valid,maxlen=X_train.shape[1])\ny_train = to_categorical(np.asarray(labels[train_data.index]))\ny_val = to_categorical(np.asarray(labels[val_data.index]))\nprint('Shape of X train and X validation tensor:', X_train.shape,X_val.shape)\nprint('Shape of label train and validation tensor:', y_train.shape,y_val.shape)\n\n","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"08b2df3d3ad03e9d2472c7b6ba757dee91292199","_cell_guid":"64d4a5f7-2d3c-487d-9689-5761e79be267"},"source":"### word embedding ###","cell_type":"markdown"},{"metadata":{"_uuid":"7df6473b83620f055e7ef090b91d967e9cd63618","_cell_guid":"2fb012c6-8166-4124-b8dd-f8c69f289e9b"},"source":"### lets load the pretrain Word2Vec model from Google https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit ###\n### It might take time since it contains contains 300-dimensional vectors for 3 million words and phrases ###","cell_type":"markdown"},{"metadata":{"_uuid":"6478a820872e903b7d5a3a8b0b3a543c773eb926","collapsed":true,"_cell_guid":"b5b196d3-a98e-4eb4-b731-c3f0650567ed"},"source":"import gensim\nfrom gensim.models import Word2Vec\nfrom gensim.utils import simple_preprocess\n\nfrom gensim.models.keyedvectors import KeyedVectors\n\nword_vectors = KeyedVectors.load_word2vec_format('../GoogleNews-vectors-negative300.bin', binary=True)\n\nEMBEDDING_DIM=300\nvocabulary_size=min(len(word_index)+1,NUM_WORDS)\nembedding_matrix = np.zeros((vocabulary_size, EMBEDDING_DIM))\nfor word, i in word_index.items():\n    if i>=NUM_WORDS:\n        continue\n    try:\n        embedding_vector = word_vectors[word]\n        embedding_matrix[i] = embedding_vector\n    except KeyError:\n        embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),EMBEDDING_DIM)\n\ndel(word_vectors)\n\nfrom keras.layers import Embedding\nembedding_layer = Embedding(vocabulary_size,\n                            EMBEDDING_DIM,\n                            weights=[embedding_matrix],\n                            trainable=True)","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"f632644d6ea3d74261786848b3fe117a238bb955","_cell_guid":"40c23df6-23b2-4b31-9036-3376b583cee1"},"source":"### Without pretrained data we can just initalize embedding matrixs as: ###","cell_type":"markdown"},{"metadata":{"_uuid":"77ef76ef55d7a8d63ab10c131739559a09ff7654","_cell_guid":"286d1203-7f84-4e2f-a086-945062fdfa01"},"source":"from keras.layers import Embedding\nEMBEDDING_DIM=300\nvocabulary_size=min(len(word_index)+1,NUM_WORDS)\n\nembedding_layer = Embedding(vocabulary_size,\n                            EMBEDDING_DIM)","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{},"source":"### Lets create the network and train it as long as validation loss goes down  ###","cell_type":"markdown"},{"metadata":{"_uuid":"6b15fab22852ebbe324b9b0143baa98b58074dff","_cell_guid":"e2f3c6e4-ff65-4129-8860-3ead1a5dcb4b"},"source":"from keras.layers import Dense, Input, GlobalMaxPooling1D\nfrom keras.layers import Conv1D, MaxPooling1D, Embedding\nfrom keras.models import Model\nfrom keras.layers import Input, Dense, Embedding, Conv2D, MaxPooling2D, Dropout,concatenate\nfrom keras.layers.core import Reshape, Flatten\nfrom keras.callbacks import EarlyStopping\nfrom keras.optimizers import Adam\nfrom keras.models import Model\nfrom keras import regularizers\nsequence_length = X_train.shape[1]\nfilter_sizes = [3,4,5]\nnum_filters = 100\ndrop = 0.5\n\n\n\ninputs = Input(shape=(sequence_length,))\nembedding = embedding_layer(inputs)\nreshape = Reshape((sequence_length,EMBEDDING_DIM,1))(embedding)\n\nconv_0 = Conv2D(num_filters, (filter_sizes[0], EMBEDDING_DIM),activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\nconv_1 = Conv2D(num_filters, (filter_sizes[1], EMBEDDING_DIM),activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\nconv_2 = Conv2D(num_filters, (filter_sizes[2], EMBEDDING_DIM),activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n\nmaxpool_0 = MaxPooling2D((sequence_length - filter_sizes[0] + 1, 1), strides=(1,1))(conv_0)\nmaxpool_1 = MaxPooling2D((sequence_length - filter_sizes[1] + 1, 1), strides=(1,1))(conv_1)\nmaxpool_2 = MaxPooling2D((sequence_length - filter_sizes[2] + 1, 1), strides=(1,1))(conv_2)\n\nmerged_tensor = concatenate([maxpool_0, maxpool_1, maxpool_2], axis=1)\nflatten = Flatten()(merged_tensor)\nreshape = Reshape((3*num_filters,))(flatten)\ndropout = Dropout(drop)(flatten)\noutput = Dense(units=3, activation='softmax',kernel_regularizer=regularizers.l2(0.01))(dropout)\n\n# this creates a model that includes\nmodel = Model(inputs, output)","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"203f236bfd9d22392cf8de296ba162e6ea1fc341","_cell_guid":"dcfdc51e-7c75-4314-b34f-25bf2e47b3de"},"source":"adam = Adam(lr=1e-3)\n\nmodel.compile(loss='categorical_crossentropy',\n              optimizer=adam,\n              metrics=['acc'])\ncallbacks = [EarlyStopping(monitor='val_loss')]\nmodel.fit(X_train, y_train, batch_size=1000, epochs=10, verbose=1, validation_data=(X_val, y_val),\n         callbacks=callbacks)  # starts training\n\n\n","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"89ced26b55b7fb07ee6ee6f5d866b1a54e27b612","collapsed":true,"_cell_guid":"048734be-7046-4f3d-bb66-aa4321f56205"},"source":"### now lets use our model to predict test data ###","cell_type":"markdown"},{"metadata":{"_uuid":"b1ccc147faeb6eb331be53d3d70489cb24b5d294","collapsed":true,"_cell_guid":"aab8ced3-1224-4155-9d3d-f7a33e9575dc"},"source":"sequences_test=tokenizer.texts_to_sequences(test_data.text)\nX_test = pad_sequences(sequences_test,maxlen=X_train.shape[1])\ny_pred=model.predict(X_test)","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"57402e363150b39eea3af18f97e7ad73594a40ac","collapsed":true,"_cell_guid":"8fd503ca-579a-499d-a3d9-3e1f52fb380d"},"source":"to_submit=pd.DataFrame(index=test_data.id,data={'EAP':y_pred[:,dic['EAP']],\n                                                'HPL':y_pred[:,dic['HPL']],\n                                                'MWS':y_pred[:,dic['MWS']]})","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"da218d48054d2fa0d6c67cd77a1dba36c1497e2c","collapsed":true,"_cell_guid":"68af55f4-eafc-4b2f-b327-34fb6d21e6be"},"source":"to_submit.to_csv('submit.csv')","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"2a7dafacf61e979c315b05c1f4b7c590d9aeda5b","collapsed":true,"_cell_guid":"a2446c4f-10fa-486d-913a-bae4dbe3d34a"},"source":"","outputs":[],"execution_count":null,"cell_type":"code"}],"nbformat":4}