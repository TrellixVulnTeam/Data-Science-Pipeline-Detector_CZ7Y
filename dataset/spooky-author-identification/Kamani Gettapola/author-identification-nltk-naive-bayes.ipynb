{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Author Identification","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2022-01-29T13:44:19.887929Z","iopub.execute_input":"2022-01-29T13:44:19.888713Z","iopub.status.idle":"2022-01-29T13:44:19.897943Z","shell.execute_reply.started":"2022-01-29T13:44:19.888673Z","shell.execute_reply":"2022-01-29T13:44:19.896872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import zipfile\nwith zipfile.ZipFile('/kaggle/input/spooky-author-identification/train.zip', 'r') as zip_ref:\n    zip_ref.extractall('./')\n    \nwith zipfile.ZipFile('/kaggle/input/spooky-author-identification/test.zip', 'r') as zip_ref:\n    zip_ref.extractall('./')","metadata":{"execution":{"iopub.status.busy":"2022-01-29T13:47:11.48175Z","iopub.execute_input":"2022-01-29T13:47:11.482071Z","iopub.status.idle":"2022-01-29T13:47:11.634271Z","shell.execute_reply.started":"2022-01-29T13:47:11.482039Z","shell.execute_reply":"2022-01-29T13:47:11.633163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is created as a part of the [kaggle competition](https://www.kaggle.com/c/spooky-author-identification/data?select=train.zip) to identify the author based on several phrases of the books they have written. The given data set contains two training and test csv files, where I have generated a validation set from the training data set.\nHere, I have used the **nltk** toolkit in python to conduct the natural language processing, and created several classification models to find the best suited model to identify the author. ","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2022-01-29T13:47:42.57748Z","iopub.execute_input":"2022-01-29T13:47:42.578122Z","iopub.status.idle":"2022-01-29T13:47:42.582417Z","shell.execute_reply.started":"2022-01-29T13:47:42.578085Z","shell.execute_reply":"2022-01-29T13:47:42.581676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv('train.csv',index_col=0)\ndf_test = pd.read_csv('test.csv',index_col=0)\n\nprint(df_train.shape)\nprint(df_test.shape)\ndf_train.head()\n","metadata":{"execution":{"iopub.status.busy":"2022-01-29T13:47:45.13768Z","iopub.execute_input":"2022-01-29T13:47:45.138454Z","iopub.status.idle":"2022-01-29T13:47:45.243507Z","shell.execute_reply.started":"2022-01-29T13:47:45.138418Z","shell.execute_reply":"2022-01-29T13:47:45.242553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lables = df_train.author.unique().tolist()\nlables","metadata":{"execution":{"iopub.status.busy":"2022-01-29T13:47:48.733719Z","iopub.execute_input":"2022-01-29T13:47:48.73436Z","iopub.status.idle":"2022-01-29T13:47:48.744849Z","shell.execute_reply.started":"2022-01-29T13:47:48.734314Z","shell.execute_reply":"2022-01-29T13:47:48.743643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Create function to clean the text data. This function removes the punctuations, makes all the text in to lower case, words are seperated by non word charachters, stopwords are removed and then stemmed. ","metadata":{}},{"cell_type":"code","source":"import nltk\nimport re\nimport string\n\nstopwords = nltk.corpus.stopwords.words('english')\nps = nltk.PorterStemmer()\n\ndef clean_data(text) : \n    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n    tokens = re.split('\\W+',text)\n    text = [ps.stem(word) for word in tokens if word not in stopwords]\n    return text\n","metadata":{"execution":{"iopub.status.busy":"2022-01-29T13:47:50.353131Z","iopub.execute_input":"2022-01-29T13:47:50.353929Z","iopub.status.idle":"2022-01-29T13:47:51.857485Z","shell.execute_reply.started":"2022-01-29T13:47:50.353891Z","shell.execute_reply":"2022-01-29T13:47:51.856611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The training data set obtained form the given csv is seperated to a training and a validation set.  \nTo vectorize, **TFIDF vectorization** is used since it gives higher weights to unique words.   ","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train, x_valid, y_train, y_valid = train_test_split(df_train['text'],df_train['author'],test_size = 0.3, random_state=42)\nx_test = df_test['text']\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ntfidf_vect = TfidfVectorizer(analyzer = clean_data)\ntfidf_vect_fit = tfidf_vect.fit(x_train)\n\ntfidf_train = tfidf_vect_fit.transform(x_train)\ntfidf_valid  = tfidf_vect_fit.transform(x_valid)\ntfidf_test  = tfidf_vect_fit.transform(x_test)\n\nx_train_vect = pd.DataFrame(tfidf_train.toarray())\nx_valid_vect = pd.DataFrame(tfidf_valid.toarray())\nx_test_vect  = pd.DataFrame(tfidf_test.toarray())\n","metadata":{"execution":{"iopub.status.busy":"2022-01-29T13:47:52.000041Z","iopub.execute_input":"2022-01-29T13:47:52.000337Z","iopub.status.idle":"2022-01-29T13:48:14.845076Z","shell.execute_reply.started":"2022-01-29T13:47:52.000305Z","shell.execute_reply":"2022-01-29T13:48:14.84409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train_vect.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-29T13:48:14.847396Z","iopub.execute_input":"2022-01-29T13:48:14.847747Z","iopub.status.idle":"2022-01-29T13:48:14.885431Z","shell.execute_reply.started":"2022-01-29T13:48:14.847698Z","shell.execute_reply":"2022-01-29T13:48:14.884587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_valid_vect.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-29T13:48:14.887092Z","iopub.execute_input":"2022-01-29T13:48:14.88735Z","iopub.status.idle":"2022-01-29T13:48:14.923369Z","shell.execute_reply.started":"2022-01-29T13:48:14.88732Z","shell.execute_reply":"2022-01-29T13:48:14.922427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_test_vect.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-29T13:48:14.925845Z","iopub.execute_input":"2022-01-29T13:48:14.926271Z","iopub.status.idle":"2022-01-29T13:48:14.962277Z","shell.execute_reply.started":"2022-01-29T13:48:14.926221Z","shell.execute_reply":"2022-01-29T13:48:14.961346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Feature Extraction","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2022-01-29T13:48:14.963888Z","iopub.execute_input":"2022-01-29T13:48:14.96412Z","iopub.status.idle":"2022-01-29T13:48:14.970143Z","shell.execute_reply.started":"2022-01-29T13:48:14.96409Z","shell.execute_reply":"2022-01-29T13:48:14.969095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Drop low value words and keep rare words  ","metadata":{}},{"cell_type":"code","source":"# col_list = x_train_vect[x_train_vect.max(axis=0)>0.1].index.values.tolist()\n# print(len(col_list))\n\n# x_train_vect = x_train_vect[col_list]\n# x_test_vect = x_test_vect[col_list]\n# x_train_vect.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Evaluate models","metadata":{}},{"cell_type":"markdown","source":"Here, we analyze several types of Classification models, and find the accuray of each model and visualize them by confusion metrices. To find the best parameters, GridSearchCV is run on each model.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import precision_recall_fscore_support as score\nfrom sklearn.metrics import plot_confusion_matrix\n","metadata":{"execution":{"iopub.status.busy":"2022-01-29T13:48:14.971234Z","iopub.execute_input":"2022-01-29T13:48:14.971468Z","iopub.status.idle":"2022-01-29T13:48:15.102145Z","shell.execute_reply.started":"2022-01-29T13:48:14.971438Z","shell.execute_reply":"2022-01-29T13:48:15.100922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Naive Bayes","metadata":{}},{"cell_type":"code","source":"nb = MultinomialNB()\nparam = {'alpha':[0.01, 0.1, 1,10, 100]}\n\ngsn = GridSearchCV(nb, param, cv=5) \ngb_fit = gsn.fit(x_train_vect,y_train)\npd.DataFrame(gb_fit.cv_results_).sort_values('mean_test_score',ascending=False)[0:3]","metadata":{"execution":{"iopub.status.busy":"2022-01-29T13:48:15.103977Z","iopub.execute_input":"2022-01-29T13:48:15.104234Z","iopub.status.idle":"2022-01-29T13:48:51.217589Z","shell.execute_reply.started":"2022-01-29T13:48:15.1042Z","shell.execute_reply":"2022-01-29T13:48:51.216572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_nb = gsn.best_estimator_.predict(x_valid_vect)\nprecision,recall,fscore,support = score(y_valid,y_pred_nb, average='weighted')\nprint('precision:{}, recall:{}, accuracy:{}'.format(round(precision,3), round(recall,3), round((y_valid==y_pred_nb).sum()/len(y_pred_nb) ,3)))","metadata":{"execution":{"iopub.status.busy":"2022-01-29T13:48:51.21926Z","iopub.execute_input":"2022-01-29T13:48:51.220184Z","iopub.status.idle":"2022-01-29T13:48:51.660087Z","shell.execute_reply.started":"2022-01-29T13:48:51.22013Z","shell.execute_reply":"2022-01-29T13:48:51.658998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_confusion_matrix(gsn.best_estimator_, x_valid_vect, y_valid, normalize='pred',xticks_rotation ='vertical')","metadata":{"execution":{"iopub.status.busy":"2022-01-29T13:48:51.665342Z","iopub.execute_input":"2022-01-29T13:48:51.66856Z","iopub.status.idle":"2022-01-29T13:48:52.277882Z","shell.execute_reply.started":"2022-01-29T13:48:51.668499Z","shell.execute_reply":"2022-01-29T13:48:52.276784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Logistic Regression","metadata":{}},{"cell_type":"code","source":"lr = LogisticRegression()\nparam = {'C' : [0.01,0.1,1,10,100], 'max_iter': [100000]}\n\ngsl = GridSearchCV(lr, param, cv=5) \nlr_fit = gsl.fit(x_train_vect,y_train)\npd.DataFrame(lr_fit.cv_results_).sort_values('mean_test_score',ascending=False)[0:3]","metadata":{"execution":{"iopub.status.busy":"2022-01-29T13:48:52.280726Z","iopub.execute_input":"2022-01-29T13:48:52.280989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_lr = gsl.best_estimator_.predict(x_valid_vect)\nprecision,recall,fscore,support = score(y_valid,y_pred_lr, average='weighted')\nprint('precision:{}, recall:{}, accuracy:{}'.format(round(precision,3), round(recall,3), round((y_valid==y_pred_lr).sum()/len(y_pred_lr) ,3)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_confusion_matrix(gsl.best_estimator_, x_valid_vect, y_valid, normalize='pred',xticks_rotation ='vertical')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Random Forest Classifier","metadata":{}},{"cell_type":"code","source":"rf = RandomForestClassifier()\nparam = {'n_estimators': [100,200],'max_depth' : [None] , 'n_jobs' : [-1]}\n\ngs = GridSearchCV(rf, param, cv=3)\nrf_fit = gs.fit(x_train_vect,y_train)\npd.DataFrame(rf_fit.cv_results_).sort_values('mean_test_score',ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_rf = gs.best_estimator_.predict(x_valid_vect)\nprecision,recall,fscore,support = score(y_valid,y_pred_rf, average='weighted')\nprint('precision:{}, recall:{}, accuracy:{}'.format(round(precision,3), round(recall,3), round((y_valid==y_pred_rf).sum()/len(y_pred_rf) ,3)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_confusion_matrix(gs.best_estimator_, x_valid_vect, y_valid, normalize='pred', xticks_rotation ='vertical' )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Gradient Boosing","metadata":{}},{"cell_type":"code","source":"# gb = GradientBoostingClassifier()\n# param = {'n_estimators':[50,100,200], 'max_depth': [10,20],'learning_rate':[0.1], 'n_jobs' : [-1]}\n\n# gsg = GridSearchCV(gb, param, cv=5) \n# gb_fit = gsg.fit(x_train_vect,y_train)\n# pd.DataFrame(gb_fit.cv_results_).sort_values('mean_test_score',ascending=False)[0:3]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# y_pred_gb = gsg.best_estimator_.predict(x_valid_vect)\n# precision,recall,fscore,support = score(y_valid,y_pred_gb, average='weighted')\n# print('precision:{}, recall:{}, accuracy:{}'.format(round(precision,3), round(recall,3), round((y_valid==y_pred_gb).sum()/len(y_pred_gb) ,3)))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot_confusion_matrix(gsg.best_estimator_, x_valid_vect, y_valid, normalize='pred',xticks_rotation ='vertical')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Create the submision file based on the best model","metadata":{}},{"cell_type":"markdown","source":"the model that perfomed the best was the Naive Bayes model. Hence, the submission file was created with the probabilities for each category for the test.csv","metadata":{}},{"cell_type":"code","source":"y_test = gsn.best_estimator_.predict_proba(x_test_vect)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test_out = pd.DataFrame(y_test,columns=[['EAP','HPL','MWS']],index=x_test.index)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test_out.to_csv('submission.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}