{"nbformat_minor":1,"cells":[{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"39751e9921e7f4b68cf7a85fab16864417f223ff","_cell_guid":"8e229ed0-0ee1-46a7-b3aa-acc8fc002750"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn import metrics\nimport lightgbm as lgb\nfrom time import time\nimport spacy\nnlp = spacy.load('en')\n\n## ~~ Happy modeling ~~ ##\nt0 = time()\nprint(\"[*] Loading dataframes\")\ntrain = pd.read_csv(\"../input/train.csv\", encoding='utf-8')\ntest = pd.read_csv(\"../input/test.csv\", encoding='utf-8')\n\n# test the extract function\ndef extract_pos(txt):\n    return \" \".join([i.pos_ for i in nlp(txt)])\n\ndef run_gdbt(x1,y1,x2,y2,x_test,n_iter=5000,seed=42,max_depth=10,lr=0.02):\n    params = {\n        'boosting_type': 'gbdt',\n        'max_depth': max_depth,\n        'learning_rate': lr,\n        'num_leaves': 20,\n        'verbose': 0, \n        'metric': 'multi_logloss',\n        'objective': 'multiclass',\n        'num_classes': 3,\n        'num_threads': 6,\n        'bagging_fraction_seed': seed,\n        'feature_fraction_seed': seed,\n    }\n    n_estimators = n_iter\n    d_train = lgb.Dataset(x1, label=y1)\n    d_valid = lgb.Dataset(x2, label=y2)\n    model = lgb.train(params, d_train, n_estimators, [d_train, d_valid], verbose_eval=200,early_stopping_rounds=120)\n\n    y2_hat = model.predict(x2)\n    y_hat = model.predict(x_test)\n    return y2_hat, y_hat, model\n\nprint(\"Hello world! -> (POS)\", extract_pos(u\"hello world!\"))\nprint(\"[*] Extract POS features\")\ntrain['pos'] = train.text.apply(extract_pos)\ntest['pos'] = test.text.apply(extract_pos)\n\nle = LabelEncoder()\ny_train = le.fit_transform(train.author.values)\n\n\n\ncounter = CountVectorizer(stop_words=None, ngram_range=(1,3), input='content',\\\n                          encoding='utf-8', decode_error='replace', strip_accents='unicode',\\\n                          lowercase=True, analyzer='word')\n\ncounter.fit(train['pos'].values.tolist() + test['pos'].values.tolist())\ntrain_bow = counter.transform(train['pos'].values.tolist())\ntest_bow = counter.transform(test['pos'].values.tolist())\n\ntrain_bow = train_bow.toarray()\ntest_bow = test_bow.toarray()\n\nprint(\"[*] Train shape\", train_bow.shape, y_train.shape)\nprint(\"[*] Test shape\", test_bow.shape)\n\n## main code ##\nn_folds = 5\nn_classes = 3\ncv_scores = []\ny_test_cv = 0\ny_train_cv = np.zeros([train.shape[0], n_classes])\nkf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=2017)\nfor train_index, val_index in kf.split(train.id,y_train):\n    x1, x2 = train_bow[train_index], train_bow[val_index]\n    y1, y2 = y_train[train_index], y_train[val_index]\n    \n    y2_hat, y_test_hat, model = run_gdbt(x1, y1, x2, y2, test_bow,n_iter=1000,max_depth=10)\n    y_test_cv = y_test_cv + y_test_hat\n    y_train_cv[val_index,:] = y2_hat\n    cv_scores.append(metrics.log_loss(y2, y2_hat))\n\nmodeling_time = time() - t0\nprint(\"Mean cv score : \", np.mean(cv_scores),np.std(cv_scores))\nprint(\"Modeling time: %0.3fs\" % modeling_time)\nprint(\"[*] Extract submission dataframe\")\ny_test_cv = y_test_cv / n_folds\nsub = pd.DataFrame({'id': test.id,\\\n                    '{}'.format(le.inverse_transform([0])[0]): y_test_cv[:,0], \\\n                    '{}'.format(le.inverse_transform([1])[0]): y_test_cv[:,1], \\\n                    '{}'.format(le.inverse_transform([2])[0]): y_test_cv[:,2]})\nprint(\"DONE!\")\n## Voila!, you will see POS would help. Linguistic structure is back.(in ~ 3 minutes)\n## Mean cv score :  0.80320359782 0.00983586058765\n## Modeling time: 177.148s"},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":true},"outputs":[],"source":""}],"nbformat":4,"metadata":{"language_info":{"nbconvert_exporter":"python","version":"3.6.3","name":"python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","file_extension":".py","mimetype":"text/x-python"},"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"}}}