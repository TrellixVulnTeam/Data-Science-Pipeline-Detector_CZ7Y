{"cells":[{"execution_count":null,"outputs":[],"source":"import numpy as np # linear algebra \nfrom math import log, sqrt # neperian logarithm, square root\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport nltk\n\n\n## Read the train\ntrain_df = pd.read_csv(\"../input/train.csv\")\n\nauthors = {'EAP':0,'HPL':1,'MWS':2}\nauthors_rev = ['EAP','HPL','MWS']","cell_type":"code","metadata":{"_uuid":"d5f3f98e4f2d3a6051fc3c7fe90b9f9da2d0b854","_kg_hide-input":false,"_cell_guid":"f340aa47-08c8-4085-a2f7-44afb2dc46ec","collapsed":true,"_kg_hide-output":false,"scrolled":true}},{"metadata":{"_uuid":"ec65c3f51f4f402245bea8877d5e3c258fcf1503","_cell_guid":"7e307dfd-0549-429c-8f0d-74a78627f92b"},"source":"# A step-by-step mathematical approach to Spooky | [English]/[French]\n\nIt is often regretted that computer scientists are not always very rigorous.\nThey manipulate mathematics ... but in their own way and sometimes make strange mishmash with!\n\nWe aim to show that one can harmoniously combine mathematics &amp; computer science; that is to say, at the same time to make the computer science properly and the mathematics usefull!\n\nHere is the course of this tutorial:\n- we will begin with a short introduction about this area straddling the linguistics and the satistics which aims to identify the authors of texts (and so which is at the heart of our competition Soopky) : * ** the stylometry ** *\n- we will then think about Spooky's notation system: the \"log loss\" (or * cross-entropy *) notation: \n$-\\frac{1}{N}\\sum_{i = 1}^N\\sum_{j = 1}^3y_{ij}\\log(p_ {ij}).$\nAssuming that we already have the program that transforms a sentence into a vector of $ \\mathbb R ^ d $, we will see what are the quantities $p_{ij}^{opt}$ to return to minimize the expectation of the \"log loss\"\n- after, we will present a very simple algorithm: the k-nearest-neighbors (kNN) algorithm. This algorithm has the mathematical property of asymptotically returning (when $ N \\to \\infty $) the optimal probabilities $ p_ {ij} ^ {opt} $ that we are looking for.\n- finally, we'll show some examples of programs (with the code python!) to transform our sentences into vectors. At first, some common examples (length of sentences, length of words, presence of some typical words, etc ...) Then a more elaborate example which calculates the log-likelihood of a sentence for each of the authors from their vocabulary. Finally, we will propose an example of an even more elaborate kernel using the library \"nltk\": it allows to transform a sentence into a tree from its grammatical structure.\n- we will devote the last part to the visualization of the data. We will present two algorithms: that of the Principal Component Analysis (PCA) which consists of an orthogonal transformation and that of the \"t-SNE\" which decreases the dimension of the space $ \\mathbb R ^ d $ while seeking to report the distances of the original space in the new space.\n\n\n<ul>\n  <li> <a href=\"#aintroduction\"> Introduction to stylometry </a></li>\n  <li> <a href=\"#alogloss\"> What to look for to minimize log loss? </a></li>\n  <li> <a href=\"#akppv\"> The k-nearest-neighbors algorithm (which asymptotically minimizes log loss) </a></li>\n  <li> <a href=\"#aexemples\"> Some examples of features </a>\n      <ul style = \"list-style-type: circle\">\n      <li> <a href=\"#aexemples_courants\"> Classic examples of stylometry </a></li>\n      <li> <a href=\"#avoca_auteurs\"> Calculating log-likelihood for each author by vocabulary </a></li>\n      <li> <a href=\"#akernel\"> Kernel on the grammatical structure of sentences </a></li>\n      </ul>\n  </li>\n  <li> <a href=\"#avisualisation\"> Data Visualization </a>\n        <ul style = \"list-style-type: circle\">\n      <li> <a href=\"#aACP\"> Principal Component Analysis (PCA) </a> </li>\n      <li> <a href=\"#atSNE\"> t-SNE </a> </li>\n      </ul>\n  </li>\n   <li> <a href=\"#aresultats\"> Submission and results (score : 0.40)</a></li>\n</ul>\n\n**NB**: we ask the reader to kindly excuse the many faults of English disseminated throughout this tutorial; English is not my mother tongue.\nMoreover, the French speaking reader can find a French version of this tutorial at the end: <a href=\"#francais\">french version</a>.","cell_type":"markdown"},{"metadata":{"_uuid":"9566eaa53a1b65fd5cfa0ef14490e64092dca506","_cell_guid":"6598308d-d3de-49b3-86ca-cb909c66bfff"},"source":"<div id=\"aintroduction\" />\n\n# Introduction to stylometry\nKaggle offers us, with the help of a training corpus, to find the authors (who are three in number: Edgar A. Poe, Howard P. Lovecraft and Mary Shelley) of about twenty thousand sentences.\n\nWhat Kaggle asks us to do is named: *** stylometry ***.\n\nStylometry is this *art* (taken in its Greek sense of *τέχνη*) at the crossroads of statistics and linguistics whose purpose is to produce from text an *information* (which can be treated statistically ) reporting on the *style* of these; that is to say, which would characterize - ideally, this means ... - at the same time its author, but also its kind, its time, etc ...\nMore practically, these statistics generally relate to the vocabulary used (which can be refined by identifying the meaning in which the author uses such a word), the grammatical categories (nouns / pronouns, adjectives, adverbs, verbs ... at what time / mode?) as well as the grammatical structure of the sentences, punctuation, etc ...\n\nIt must be understood that to make good stylometry, it is necessary to play in both ways:\n- that of linguistics by proposing criteria (features) that are most relevant to isolate the style of an author,\n- that of statistics by using adapted mathematical and computer tools.\n\nThroughout this tutorial, we'll try to take up these two aspects of stylometry by always being as rigorous as possible and justifying (as far as possible) everything we do.\n\nBut first, let's look at Spooky's evaluation system:","cell_type":"markdown"},{"metadata":{"_uuid":"89d908e75c2deddbd6e91a688e55ea723de3f971","_cell_guid":"d3aacf3e-a51a-4cf6-9e5d-bdb3393d4a9f"},"source":"<div id=\"alogloss\" />\n# What to look for to minimize log loss?\n\nFirst we will see why the evaluation system \"logloss\" has been chosen and what are the mathematical values to look for to minimize it in expectation.\nFrom these values to look for, we will deduce a suitable algorithm (that is to say that we justify from a mathematical point of view) for the computation of the probabilities $ p_ {ij} $ to return. This is the agorithm of the k-nearest-neighbors.\n\nThe notation system that has been chosen for Spooky is the \"log loss\" (or *cross entropy*):\n        \n$$ log loss = - \\frac{1}{N} \\sum_{\\substack {i \\in \\{1, ..., N \\} \\\\ j \\in \\{EAP, HPL, MWS \\}}} y_{ij} \\log (p_ {ij}) $$\nwhere $ p_ {ij} $ is the estimate: the probability according to our program that the text $ i $ is of the author $ j $, $ y_{ij} $ is the solution: it equals $ 1 $ when the text $ i $ is of the author $ j $, $ 0 $ otherwise.\n\nThe goal is to minimize this log loss.\n\nWe will now formalize a little bit the problem from a mathematical point of view in order to compute the expected log loss and to find the optimal values $ p_{ij}^{opt} $ to return to achieve the minimal expectation.\n\nSuppose we already have the first part of the program: the one that transforms a sentence into a computer-processable \"information\". In this case we will assume that this information is a vector of $ \\mathbb R ^ d $ and that they are all independently fetched according to the same law $ X $ admitting a compact $ f $ density. It is also assumed that this law $ \\mathcal L (X) $ can be broken down into two stages:\n\n- first, we choose the author of the sentence according to a law of \"Bernoulli with three issues\": 'EAP' for Edgar A. Poe, 'HPL' for Howard P. Lovecraft and 'MWS' for Mary Shelley. We note $ \\mathcal B (\\lambda_1, \\lambda_2, \\lambda_3) $ this law (with $ \\sum \\lambda_j = 1 $); $ \\lambda_j $ being the proportion of $ j $ author's sentences.\n- According to the author $ j $ which is chosen in the first step, our sentence is drawn according to a law of density $ f_j $ on $ \\mathbb R ^ d $.\n\n![shema_bernouilli](http://bourg-la-reine-echecs.fr/telechargements/shema_bernoulli_auteurs_corrige.png)\n\nSo we have that: $ f = \\sum \\lambda_j f_j $.\n\nOn the training corpus, we get:","cell_type":"markdown"},{"execution_count":null,"outputs":[],"source":"nb_texts = len(train_df)\nlambdas = (train_df.author.value_counts()/nb_texts).to_dict()\nprint(\"Lambdas :\",lambdas)","cell_type":"code","metadata":{"_uuid":"6546b4287cf458361e78d8c453f72ecc50977f61","collapsed":true,"_cell_guid":"08dc435d-f04b-4d61-be58-cb206b01fa7e"}},{"metadata":{"_uuid":"ee5c5818e04ec7f7651301aef530e8c5cb015c04","_cell_guid":"a240bb1c-bcf8-4346-b40f-be2400d2e94f"},"source":"- $ \\lambda_1 = 0.403493538995863 $\n- $ \\lambda_2 = 0.287808366106543 $\n- $ \\lambda_3 = 0.308698094897594 $\n\nThe attentive reader may have noticed that the sample file \"sample\\_submission\" for each sentence precisely returns the probabilities $ (\\lambda_1, \\lambda_2, \\lambda_3) $ ... it's no coincidence !! It is the return that minimizes the \"log loss\" with the worst program: the one that does not distinguish anything and that sends all the sentences on the null vector ($ \\mathcal L (X) = \\delta_0 $ where $ \\delta_0 $ is the measure of \"dirac\" at the point $ 0 $).\nThis follows from a classical result of information theory: if $ (\\lambda_j)_j $ and $ (p_j)_j $ are two discrete probability measures, then cross entropy:\n$$ H_\\lambda (p) = \\mathbb E [log loss] = - \\sum_j \\lambda_j \\log (p_j) $$\nis minimized when $ p_j = \\lambda_j $ for all $ j $.\n\nWe will now generalize this result!\n\nGiven a sentence transformed by our program into a $ x $ vector (according to the $ X $ law), the best solution (in the sense that it is the one that minimizes the expectation of the final \"log loss\" score) of Probabilities to return is the triplet:\n\n$$ p_1 ^ {opt} (x) = \\frac{\\lambda_1 f_1 (x)} {\\sum_j \\lambda_j f_j (x)}, \\ \\ \\ \\\np_2 ^ {opt} (x) = \\frac{\\lambda_2 f_2 (x)}{\\sum_j \\lambda_j f_j (x)}, \\ \\ \\ \\\np_3 ^ {opt} (x) = \\frac{\\lambda_3 f_3 (x)} {\\sum_j \\lambda_j f_j (x)}. $$\n\nGood! now that it has been said, there is more to roll up the sleeves!\n\nThe expectation of the \"log loss\" rating is given in this case by the following formula:\n\n$$ \\mathbb E [log loss] = - \\sum_j {\\lambda_j \\int_ {\\mathbb R ^ d} \\log (p_j (x)) f_j (x) \\mathrm d x} $$\nwhere $ p_j (x) $ is the probability that we return for the author $ j $ when we receive the vector $ x $; it is our estimation that the sentence that has been transformed into the vector $ x $ is of the author $ j $ (we want to show that $ p_j ^ {opt} (x) = \\frac {\\lambda_j f_j (x )} {\\sum_k \\lambda_k f_k (x)} $ is indeed the optimal solution).\n\nIn general, the program will return a probability that can be written in the form:\n$$ q_j (x) = \\frac {\\lambda_j g_j (x)} {\\sum_k \\lambda_k g_k (x)} $$\nwhere $ g_j $ is a positive continuous function (which is assumed to be at least as large as $ f_j $ because if there exists $ x $ such that $ q_j (x) = 0 $ and $ f_j ( x)> 0 $, then $ \\mathbb P (logloss = \\infty)> 0 $ ...).\nSo that the $ g_j $ will \"lose\" their weight outside the support of $ f $ (that is, if $ f (x) = 0 $, $ x $ has no chance to appear, so we do not care about the value of $ g_j $ in these points), we can also assume that $ g_j $ are density functions (that is, $ \\int _{\\mathbb R ^ d} g_j (x) dx = 1 $).\n\nWe can take, for example, $ g_j = \\frac {1} {\\lambda_j} f_j $ to disregard the difference in the number of sentences between authors (here, $g_j$ are not density functions since their integral does not equal $ 1 $, but one could come back to it as said just before).\nIf we want \"extreme\" probabilities (close to $ 0 $ or $ 1 $), we can ask: $ g_j = f_j ^ 2 \\times \\frac {1} {\\int f_j ^ 2} $.\nOn the contrary, if one wishes to \"qualify\" these probabilities, one will ask: $ g_j = \\sqrt {f_j} \\times \\frac {1} {\\int \\sqrt f_j} $.\n\nNow let us show that the best choice for minimizing log loss is: $ p_j ^ {opt} = \\frac {\\lambda_j f_j (x)} {\\sum_k \\lambda_k f_k (x)} $. Let us note $ log loss ^ {opt} $ the score obtained with this last choice and $ log loss ^ {q_j} $ the score obtained with the probabilities $ q_j $ defined using the functions $ g_j $.\nWe then have:\n\n\\begin{align}\n\\mathbb E[log loss^{opt}] - \\mathbb E[log loss^{q_j}] & = - \\sum_j{\\lambda_j \\int_{\\mathbb R ^d}\\log(p_j^{opt})f_j(x) \\mathrm d x} - \\sum_j{\\lambda_j \\int_{\\mathbb R ^d}\\log(q_j)f_j(x) \\mathrm d x}\\\\\n & = - \\sum_j{\\lambda_j \\int_{\\mathbb R^d}(\\log(p_j^{opt})-\\log(q_j))f_j(x) \\mathrm d x} \\\\\n & = - \\sum_j{\\lambda_j \\int_{\\mathbb R^d}\\log(\\frac{\\lambda_j f_j(x)}{\\lambda_j g_j(x)}\\frac{\\sum_k \\lambda_k g_k(x)}{\\sum_k \\lambda_k f_k(x)})f_j(x) \\mathrm d x} \\\\\n & = - \\sum_j{\\lambda_j \\int_{\\mathbb R ^d}\\log(\\frac{ f_j(x)}{ g_j(x)})f_j(x) \\mathrm d x} + {\\int_{\\mathbb R ^d}\\log(\\frac{\\sum_k \\lambda_k f_k(x)}{\\sum_k \\lambda_k g_k(x)})(\\sum_j \\lambda_j f_j(x))\\mathrm d x} \\\\\n & = - \\sum_j \\lambda_j D_{KL}(f_j || g_j) + D_{KL}(\\sum_k \\lambda_k f_k || \\sum_k \\lambda_k g_k)\\\\\n & \\text{(where $ D_ {KL} $ denotes the Kullback-Leibler divergence ... we'll get back to it!)} \\\\\n & \\le 0 \\text{ by the convexity property of $ D_ {KL} $ in the function pairs;}\\\\\n & \\text{here, there is $ 3 $: $ (f_1, g_1) $, $ (f_2, g_2) $ and $ (f_3, g_3) $. We conclude that $ p_j ^ {opt} $ is optimal:}\\\\\n \\mathbb E[log loss^{opt}] & \\le \\mathbb E[log loss^{q_j}]\n\\end{align}\n\nThe Kullback-Leibler divergence between two densities $ p $ and $ q $ on $ \\mathbb R ^ d $ is given by the following formula:\n$ D_ {KL} (p || q) = \\int_{\\mathbb R ^ d} \\log (\\frac{p (x)}{q (x)}) p (x) \\mathrm d x $. To see it quickly, in information theory, it is the average amount of additional bits that will be needed to code from the source $ q $ an optimal code of the source $p$.\nWe will talk again about the divergence of Kullback-Leibler in the part devoted to the \"t-SNE\" (algorithm that works precisely on a minimization of this same divergence during the change of dimension of the data space).\n\n\nSo we took the first step boldly (our point being to show that we can do IT cleanly and rigorously!): We now know that once we have our program that turns sentences into vectors, the amount to look for (and return to Kaggle) is:\n$$ p_j ^ {opt} (x) = \\frac {\\lambda_j f_j (x)}{\\sum_k \\lambda_k f_k (x)}. $$\n\n.\n","cell_type":"markdown"},{"metadata":{"_uuid":"3275483ab9b100c1ab4d6be6e2da5d090dafc6d5","_cell_guid":"1c440496-dfd5-48ae-a747-36a345e58b56"},"source":"\n<div id=\"akppv\" />\n\n# K-nearest-neighbors (to find the optimal $ p_ {ij} ^ {opt} $ probabilities)\n\nWe will show a way to recover these probabilities. Oh ! it is certainly not an extraordinary algorithm since it is the nearest-neighbors algorithm.\nNevertheless, it has the advantage of asymptotically giving (when $ N $ the number of texts tends to infinity) the best possible result. Other algorithms often used are, for example, the hypothesis of a Gaussian distribution of data (this is the case of classifiers naive Bayes) which give good results on small samples (since they are based on the computation of expectations and covariances of the variables, which requires much less data than to estimate a density) but these algorithms cease to give good results as soon as the learning base is sufficient.\n\nHere is the algorithm of K-nearest-neighbors:\n\n- We have a learning base $ \\{(x_1, y_1), \\ ... \\ , \\ (x_N, y_N) \\} $ containing the $ N $ vectors $ x_1, ..., x_N \\in \\mathbb R ^ d $ formed from $ N $ sentences with their respective author $ y_1, ..., y_N \\in \\{1,2,3 \\} $. We also choose a distance $ d $ on $ \\mathbb R ^ d $ (usually the Euclidean distance, but this is not obligatory, the asymptotic results will remain true for any distance: we say that the operator of the kNN is *universally* consistent).\n- We choose an integer $ k $ depending on $ N $ which must be small in front of $ N $ (we want: $ k / N \\underset {N \\to \\infty} {\\longrightarrow} 0 $) but still big enough (it must be that: $ k \\underset {N \\to \\infty} {\\longrightarrow} \\infty $).\n- Now, each time we receive a new vector $ x $, we consider the set $kNN$ of the $k$ indices  of the points the closer to $x$:\n\n$$kNN = \\{i_1 <..<i_k \\ | \\ \\forall \\ j \\notin \\{i_1, ..., i_N \\}, \\forall \\ i \\in \\{i_1, ..., i_N \\},  \\ d (x, x_i) \\le d (x , x_j) \\}$$\n\n\n- Finally, we return probabilities $ \\hat p_j (x) $ proportional to the number of more-near-neighbors who are of the author $ j $. So :\n$$ \\text {For $ j \\in \\{1,2,3 \\} $, } \\hat p_j (x) = \\frac {1} {k} \\# \\{i \\in kNN \\ | \\ y_i = j \\} $$\n\n\nWe have therefore presented the very simple $ k $-NN algorithm which returns $ \\hat p_j (x) $ estimates of $ p_j ^ {opt} (x) $. It checks the following \"weak consistency\" property:\n$$ \\text{If $ k \\underset {N \\to \\infty} {\\longrightarrow} \\infty $ and $ k / N \\underset {N \\to \\infty} {\\longrightarrow} 0 $, then:} \\ \\forall \\ \\epsilon> 0, \\ \\mathbb P (| \\hat p_j - p_j ^ {opt} |> \\epsilon) \\underset {N \\to \\infty} {\\longrightarrow} 0 $$\n\nAsymptotically, we thus find empirically the value of $ p_j ^ {opt} $. This does not tell us anything about the speed of convergence. There are refinements of this algorithm (for example by using proximity graphs) which make it possible to improve this speed of convergence.\n","cell_type":"markdown"},{"metadata":{"_uuid":"b0adf5a0ec08ea5757ba9f9ea178bb273bc005c5","_cell_guid":"8b9b371e-2d5f-417d-b19d-be3c966f50fb"},"source":"<div id=\"aexemples\" />\n# Examples of features","cell_type":"markdown"},{"metadata":{"_uuid":"87ef1f36fb96ba1d726ad8cfe1fa850e9d308db5","_cell_guid":"e39239c7-4357-441c-bb95-78a7f8408902"},"source":"<div id=\"aexemples_courants\" />\n## - Current Examples\nTODO","cell_type":"markdown"},{"metadata":{"_uuid":"5c9ef814be18b362e6281b31b4d4ab120fef2596","_cell_guid":"a6e24674-d0d0-463c-8972-bde6a48a963e"},"source":"<div id=\"avoca_auteurs\" />\n## - Example of vocabulary usage\n\nThe examples we saw in the previous section are common examples of stylometry. They are also very general examples in that they do not require the presence of a training corpus: they could equally well be used in the case of *unsupervised classification* (and not only *supervised* as this is the case here).\n\nWe'll explain now an example of a function that uses the vocabulary used in the training set. Given a sentence, this function returns a $ 3 $ size vector containing the log-likelihood of using that vocabulary for each of the $ 3 $ authors.\n\nLet's split our training corpus in two parts: \n- a training set to build the dictionary ($95\\%$ of the data)\n- a test set that will allow us to visualize the results ($5\\%$ of the data).","cell_type":"markdown"},{"execution_count":null,"outputs":[],"source":"N_train = len(train_df)\n\n# cutting of the train set\ncut = round(N_train * 0.95)\n\ntrain = pd.DataFrame(train_df, index=range(cut))\ntest = pd.DataFrame(train_df, index=range(cut,N_train))","cell_type":"code","metadata":{"_uuid":"27c4410830361e185629d38509fed3aa6c9bff47","collapsed":true,"_cell_guid":"0b8fb185-e561-454e-a145-2d0bdf1922c8"}},{"metadata":{"_uuid":"efd1d7e2f303404d571cf3b27a8e784a6e100e15","_cell_guid":"13fc4597-8fa4-496a-8675-98dfcf2eb356"},"source":"\nLet's start by building a dictionary containing all the words appearing in the training corpus. To each entry (a word) in this dictionary we'll associate its occurrences for each of the authors.\n","cell_type":"markdown"},{"execution_count":null,"outputs":[],"source":"# built from a training corpus a dictionary of used words. For each word, we fill a dictionary that each author associates the number of times this word appeared in the author\ndef voca_Authors(train):\n    voca = {}\n    for i,line in train.iterrows():\n        words = nltk.word_tokenize(line['text'])\n        for word in words:\n            word = word.lower()\n            if not word in voca:\n                voca[word] = {auth:0 for auth in authors}\n                voca[word][line['author']] = 1\n            else:\n                voca[word][line['author']] += 1\n    return voca\n\nprint('Computation of vocabulary dictionary.')\nvoca_authors = voca_Authors(train)\nprint('Dictionary size:',len(voca_authors),'\\n20 examples from this dictionary :\\n')\nfor i,word in enumerate(voca_authors):\n    if i < 20:\n        print(word,voca_authors[word])","cell_type":"code","metadata":{"_uuid":"93bd3539fd0c343ad7aec167667e7371693fa1bd","collapsed":true,"_cell_guid":"65aeeb94-675f-45be-8ec9-163e0ab4d7ec"}},{"metadata":{"_uuid":"b15547c149b0c07fc8f352dd41524b4e8ebb0c82","_cell_guid":"b02a44f6-8e79-49a2-a965-b49302c6889d"},"source":"All we need know is to understand what has been computed and what is computable.\n\nWe will make the hypothesis quite strong (that is to say rather false) that:\n- the length of the sentences follows a law independent of the author (one can look at the histograms of the lengths of sentence according to the author with the following address: link and to note that indeed these 3 histograms are quite close) ,\n- all the words of a sentence are drawn independently and identically according to a law that depends only on the author (this hypothesis is very strong and very false).\n\nGiven those independence hypotheses, the computation of the likelihood for an author who has written the sentence $ s = [w_1, ..., w_k] $ is quite straightforward :\n\n$$ L(w_1, \\ ... \\ , \\ w_k \\ | \\ author = j) = \\prod_ {i = 1} ^ k \\mathbb P (\\text {author of $ w_i $ = $ j $} \\ | \\ \\text {word = $ w_i $}) $$\n\nNow, let's take the word \"of\" as an example. We get the following statistics:","cell_type":"markdown"},{"execution_count":null,"outputs":[],"source":"voca_authors['of']","cell_type":"code","metadata":{"_uuid":"57d8a5010356966d78ff2ced4c00858de0a752ff","collapsed":true,"_cell_guid":"b0421299-6a7c-4613-b5d6-635a3561b4c8"}},{"metadata":{"_uuid":"20807310fb11005414ad34b95cdcd3ee942c9f6d","_cell_guid":"41e86e7f-7d28-41b3-91e6-d1d2cbf507ef"},"source":"There are $ 8539 + 5568 + 5850 = 19957 $ occurrences of the word \"of\" in our training corpus. Edgar A. Poe is the author of 8539 of them, that is, a proportion $ x_1=8539/19957=0.43$. \nHe is also the author of a proportion $\\lambda_1=0.40<x_1$ of all the sentences of the corpus. Since we've been hypothesizing that the law on the length of sentences was author independent, this means that he therefore writes more the word 'of' than his overrepresentativeness in the corpus would allow; that is, $ p_1 : = \\mathbb P (\\text {author of $ 'of' = 1 $})> 1/3 $.\n\nIf we write $ p_j = \\mathbb P (\\text {author of 'of' = j }) $ and $ x_j $ the proportion of appearances in the training corpus, we get the following relation:\n$$ x_j = \\frac {\\lambda_j p_j} {\\sum_k \\lambda_k p_k}. $$\n\nWe thus know the $ x_j $ and the $ \\lambda_j $ and we want to know the $ p_j $ to get our log-likelihoods!\n\nNote that one can very easily compute $ \\sum_k \\lambda_k p_k $ using :$$ \\sum_j \\frac {x_j} {\\lambda_j} = \\sum_j \\frac{p_j}{\\sum_k  \\lambda_k p_k } = \\frac{1}{\\sum_k \\lambda_k p_k }$$\n... and thus find the $ p_j $ we're looking for:\n$$ p_j = \\frac {x_j} {\\lambda_j}\\left( \\sum_k \\frac {x_k} {\\lambda_k}\\right)^{-1} $$\n\nThis gets us the log-likelihood for each author, Yay !\n$$ \\log L (w_1, ..., w_k \\ | \\ author = j) = \\sum_ {i = 1} ^ k \\log (p_j (w_i)) $$","cell_type":"markdown"},{"execution_count":null,"outputs":[],"source":"def vect_Voca(sentence, dict = voca_authors, zero = 0.01):\n    words = nltk.word_tokenize(sentence)\n    ret = [0]*len(authors)\n    for word in words:\n        word = word.lower()\n        if word in dict:\n            vect = [0]*len(authors)\n            nb_appar = 0\n            for auth in dict[word]:\n                nb_appar += dict[word][auth]\n            for auth in dict[word]:\n                vect[authors[auth]] = dict[word][auth]/nb_appar/lambdas[auth] # vect[j] = x_j/lambda_j = p_j / (Sum lambda_j p_j)\n            s = 1/sum(vect) # s = (Sum lambda_j p_j) \n            vect = [p_j_div_sum * s for p_j_div_sum in vect] # vect[j] = p_j\n\n            for j,p_j in enumerate(vect):\n                if p_j == 0:\n                    ret[j] += log(zero)\n                else:\n                    ret[j] += log(p_j)\n    return ret","cell_type":"code","metadata":{"_uuid":"005874f260caf95194cd656beb71eb991b45c51e","collapsed":true,"_cell_guid":"7597b1c3-ef4d-497a-85ac-70d246e1f8de"}},{"metadata":{"_uuid":"37d206b84a8a89fbe290ecbfd71f828b6bc8dc1d","_cell_guid":"0143fefe-fedb-4eff-989d-03094fc862c9"},"source":"... and visualize the comet's tail-like results:\n","cell_type":"markdown"},{"execution_count":null,"outputs":[],"source":"# the matrix of sentences of the corpus of text transformed using vect_Voca\nX = []\ncol = [] # the color according to the author: Poe => red, Lovecraft => green, Shelley => blue\n# colours = {'EAP':'rgb(150, 5, 5)','HPL':'rgb(5, 150, 5)','MWS':'rgb(5, 5, 150)'}\ncolours = {'EAP':'r','HPL':'g','MWS':'b'}\n# computation of X, the size matrix N_tests * 3\nfor i,line in test.iterrows():\n    vect = vect_Voca(line['text'])\n    X.append(vect)\n    col.append(colours[line['author']])\nX = np.array(X)\n\n## The display\n\nfrom mpl_toolkits.mplot3d import Axes3D\nimport matplotlib.pyplot as plt\n\n\nfig = plt.figure(figsize=(15,15))\nax = fig.add_subplot(111, projection='3d')\n\nax.scatter(X[:,0], X[:,1], X[:,2], c=col, marker='+')\n\n","cell_type":"code","metadata":{"_uuid":"650cc0d6dd30df822e1f94267711b9611bb70d57","collapsed":true,"_cell_guid":"79c3035d-5dff-45a8-9ca8-ec55630c6901"}},{"metadata":{"_uuid":"36a74374c1505ff8a10a22dd54b091d2e4db9592","_cell_guid":"dda3ef8a-60e4-4e94-aaf8-920fc2835f7e"},"source":"<div id=\"akernel\" />\n## - Example of kernel on the tree structures of sentences\nTODO","cell_type":"markdown"},{"metadata":{"_uuid":"3cae76a78e01e8e73907aacc7b97202d31a80e27","_cell_guid":"a8b1c84b-84fb-4a64-ba34-f714489b74e2"},"source":"<div id=\"avisualisation\" />\n# Data visualization\n","cell_type":"markdown"},{"metadata":{"_uuid":"341093d1f0ebc6584338935c484aa2d27348367e","_cell_guid":"5891cf0d-7252-477f-a15e-988a9e751270"},"source":"<div id=\"aACP\" />\n## - The Principal Components Analysis (PCA)\nTODO","cell_type":"markdown"},{"metadata":{"_uuid":"4896d46176ebb825c69e4e56cc96de26393d6edc","_cell_guid":"fe214d31-c3d6-4c46-835a-e03c85918e3d"},"source":"<div id=\"atSNE\" />\n## - The t-SNE\nTODO","cell_type":"markdown"},{"metadata":{"_uuid":"e2f133975ddfbb68a725687646660364c8b1c87d","_cell_guid":"e7e226d8-b4ca-4a0e-adf3-e4e15f5c9284"},"source":"<div id=\"aresultats\" />\n# Submission (score : 0.40)\n\nA simple submission with kNN just on 3 size vectors of vocabulary log-likelihood :","cell_type":"markdown"},{"execution_count":null,"outputs":[],"source":"from sklearn.neighbors import NearestNeighbors\n\ntrain = pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv(\"../input/test.csv\")\n\nvoca_authors = voca_Authors(train)\n# la matrice des phrases du corpus de texte transformées à l'aide de vect_Voca\nM = [] # the train matrix\nSol = [] # the solutions of the train matrix\nX = [] # the test matrix\n\n\nfor i,line in train.iterrows():\n    vect = vect_Voca(line['text'])\n    M.append(vect)\n    Sol.append(line['author'])\nM = np.array(M)\n\nfor i,line in test.iterrows():\n    vect = vect_Voca(line['text'])\n    X.append(vect)\nX = np.array(X)\n\n\n# The kNN algorithm\nadjunction = 0.025\nnb_ppv = 200 # number of nearest neighbors\nM_ppv = NearestNeighbors(n_neighbors=nb_ppv)\nM_ppv.fit(M)\ntab_kneighbors = M_ppv.kneighbors(X, return_distance=False)\n\nProbas = [] # the return\n\nfor i,line in test.iterrows():\n    p = [0]*3\n    kneighbors = tab_kneighbors[i]\n    for kneighbor in kneighbors:\n        p[authors[Sol[kneighbor]]] += 1\n    s = sum(p)\n    p = [x/s for x in p]\n    Probas.append(p)\n\n# harmonisation of the probabilities\nfor v in Probas:\n    for i,prob in enumerate(v):\n        v[i] = (prob + adjunction)/(1+3*adjunction)\n\n\nsubmission = pd.read_csv('../input/sample_submission.csv')\nsubmission.loc[:,['EAP', 'HPL', 'MWS']] = Probas\nsubmission.to_csv(\"Log_likelihoof_on_vocabulary.csv\", index=False)\nsubmission.head()","cell_type":"code","metadata":{"_uuid":"4b937734be45f9401993270c917fd46f6a9e468e","collapsed":true,"_cell_guid":"c3dfd3f7-d95f-433d-ba7b-a1b8824ac8f7"}},{"metadata":{"_uuid":"5d86bafb60813143952daec65dd354c73c533ad4","_cell_guid":"40278fac-626a-464e-8f5e-3efa0a7b85f3"},"source":"<div id=\"francais\" />\n# La version française / the french version :\n","cell_type":"markdown"},{"metadata":{"_uuid":"dab2ce215b3500fe6a30d532be9f3fb103a2422e","_cell_guid":"ccdce257-67ee-4203-8468-b1424e945a3d"},"source":"# Une approche (pas à pas) mathématique de la stylométrie\n\nOn déplore souvent que les informaticiens ne soient pas toujours très rigoureux.\nIls manipulent les mathématiques... mais à leur sauce et font parfois de drôles de tambouilles/une drôle de cuisine avec !?\n\nLe but de ce tutoriel est de tenter de montrer qu'une telle situation n'est pas une fatalité : qu'on peut allier harmonieusement mathématiques & informatique ; c'est-à-dire à la fois faire de l'informatique proprement et des mathématiques qui servent à quelque chose !!\n\nVoici donc le déroulé de ce tutoriel :\n- on commencera par une courte introduction à propos de ce domaine à cheval sur la linguistique et la satistiqu consistant à identifier les auteurs et qui est au cœur de notre compétition Soopky... j'ai nommé : la **\"stylométrie\"**\n- on réfléchira ensuite au système de notation de Spooky : la note \"log loss\" (ou *entropie croisée*) : $ -\\frac{1}{N}\\sum_{i=1}^N\\sum_{j=1}^3 y_{ij}\\log(p_{ij})$. En supposant que l'on possède déjà le programme qui transforme une phrase en vecteur de $\\mathbb R ^d$, on verra quelles sont les quantités $p_{ij}^{opt}$ à renvoyer pour minimiser l'espérance de la \"log loss\"\n- après, on présentera un algorithme très simple : celui des k-plus-proches-voisins. Cet algorithme possède la propriété mathématique de renvoyer asymptotiquement (quand $N \\to \\infty$) les probabilités optimales $p_{ij}^{opt}$ que l'on recherche.\n- enfin, on présentera quelques exemples de programmes (avec le code python !) pour transformer nos phrases en vecteurs. Dans un premier temps, quelques exemples courants (longueur de phrases, longueur des mots, présence de certains mots typiques, etc...) Puis un exemple un peu plus élaboré qui calcule la log-vraisemblance d'une phrase pour chacun des auteurs à partir de leur vocabulaire. Pour finir, on proposera un exemple de kernel encore plus élaboré utilisant la bibliothèque \"nltk\" : elle permet de transformer une phrase en un arbre à partir de sa structure grammaticale. \n- on consacrera la dernière partie à la visualisation des données. On présentera deux algorithmes : celui de l'Analyse en Composantes Principales qui consiste en un changement de base orthnormée et celui de la \"t-SNE\" qui diminue la dimension de l'espace $\\mathbb R^d$ tout en cherchant à rendre compte le mieux possible des distances de l'espace original dans le nouvel espace.\n\n\n<ul>\n  <li><a href=\"#introduction\" >Introduction à la stylométrie</a></li>\n  <li><a href=\"#logloss\" >Que rechercher pour minimiser la \"log loss\" ?</a></li>\n  <li><a href=\"#kppv\">L'algorithme des k-plus-proches-voisins (qui minimise asymptotiquement la \"log loss\")</a></li>\n  <li><a href=\"#exemples\">Quelques exemples de features</a>\n      <ul style=\"list-style-type:circle\">\n      <li><a href=\"#exemples_courants\">Exemples classiques de stylométrie</a></li>\n      <li><a href=\"#voca_auteurs\">Calcul de la log-vraisemblance pour chacun des auteurs en fonction du vocabulaire</a></li>\n      <li><a href=\"#kernel\">Kernel sur la structure grammaticale des phrases</a></li>\n      </ul>\n  </li>\n  <li><a href=\"#visualisation\">Kernel sur la structure grammaticale des phrases</a>\n        <ul style=\"list-style-type:circle\">\n      <li><a href=\"#ACP\">l'Analyse en Composantes Principale (ACP)</a></li>\n      <li><a href=\"#tSNE\">la t-SNE</a></li>\n      </ul>\n  </li>\n   <li> <a href=\"#resultats\"> Résultats (score : 0,40)</a></li>\n</ul>\n","cell_type":"markdown"},{"metadata":{"_uuid":"9715e491e6cebe988a6dabc0347c07b8a7f7f87c","_cell_guid":"27587b05-4e85-401b-bcda-0563d0e85a7c"},"source":"<div id=\"introduction\" />\n# Introduction à la stylométrie\nKaggle nous propose, avec l'aide d'un corpus d'entraînement, de retrouver les auteurs (qui sont au nombre de trois : Edgar A. Poe,  Howard P. Lovecraft et Mary Shelley) d'une petite dizaine de mille de phrases.\n\nCe que nous demande de faire Kaggle porte un nom : la* **stylométrie***.\n\nLa *stylométrie* est cet *art* (pris dans son sens grec de *τέχνη*) à la croisée de la statistique et de la linguistique dont le but est de produire à partir de textes une*information* (qui puisse être traitée statistiquement) rendant compte du *style* de ces derniers ; c'est-à-dire qui caractériserait -- idéalement, cela s'entend... -- à la fois son auteur, mais aussi son genre, son époque, etc...\nPlus pratiquement, ces statistiques portent généralement sur le vocabulaire utilisé (qui peut être affiné en repérant le sens en lequel l'auteur emploie tel mot), les catégories grammaticales (noms/pronoms, adjectifs, adverbes, verbes... à quel temps/mode ?) ainsi que la structure grammaiticale des phrases, la ponctuation, etc... \n\nIl faut donc bien comprendre que pour faire de la bonne stylométrie, il faut à la fois jouer sur les deux tableaux :\n- celui de la linguistitique en proposant des critères qui soient les plus pertinents pour isoler le style d'un auteur,\n- celui de la statistique en employant des outils mathématiques et informatiques adaptés.\n\nAu cours de ce tutoriel, nous essayerons d'aborder ces deux aspects de la stylométrie en restant toujours le plus possible rigoureux et en justifiant (autant que faire se peut) tout ce que nous ferons.\n\nMais d'abord, intéressons-nous au sytème d'évaluation de Spooky :","cell_type":"markdown"},{"metadata":{"_uuid":"1d44bd3e5f0023e50776cb803d5b9f667063e969","_cell_guid":"cd46fa10-961e-4f20-9857-b9f227fa9bcb"},"source":"<div id=\"logloss\" />\n# Que rechercher pour minimiser la \"log loss\" ?\n\nTout d'abord nous allons voir pourquoi le système d'évaluation \"logloss\" a été choisi  et quelles sont les valeurs mathématiques à rechercher pour le minimiser en espérance.\nDe ces valeurs à rechercher, nous en déduirons un algorithme adapté (c'est-à-dire que nous justifierons d'un point de vue mathématique) pour le calcul des probabilités $p_{ij}$ à retourner. C'est l'agorithme des k-plus proches-voisins.\n\nLe système de notation qui a été choisi pour Spooky est celui de \"log loss\" (ou *entropie croisée*) :\n        \n$$ log loss = - \\frac{1}{N}\\sum_{\\substack{i \\in \\{1,...,N\\} \\\\ j \\in \\{ EAP, HPL, MWS \\}}}y_{ij} \\log(p_{ij})$$\noù $p_{ij}$ est l'estimation que l'on doit rendre ; la probabilité selon notre programme que le texte $i$ soit de l'auteur $j$.\n$y_{ij}$ est la solution : il vaut $1$ lorsque le texte $i$ est de l'auteur $j$, $0$ sinon.\n\nLe but est de minimiser cette \"log loss\".\n\nNous allons maintenant un tout petit peu formaliser le problème d'un point de vue mathématique afin de pouvoir calculer l'espérance de la \"log loss\" et de trouver les valeurs optimales $p_{ij}^{opt}$ à retourner afin de minimiser cette espérance.\n\nSupposons que nous disposions déjà de la première partie du programme : celle qui transforme une phrase en une \"information\" traitable par l'informatique. En l'occurrence nous supposerons que ces informations sont des vecteurs de $\\mathbb R ^d$ et qu'ils sont tous tirés de façon indépendante selon une même loi $X$ admettant une densité $f$ à support compact. On suppose aussi qu'on peut décomposer cette loi $\\mathcal L(X)$ selon deux étapes :\n\n-  tout d'abord, on choisit l'auteur de la phrase selon une loi de \"Bernoulli à trois issues\" : 'EAP' pour Edgar A. Poe, 'HPL' pour Howard P. Lovecraft et 'MWS' pour Mary Shelley. On notera $\\mathcal B(\\lambda_1, \\lambda_2, \\lambda_3)$ cette loi (avec $\\sum \\lambda_j = 1$) ; $\\lambda_j$ étant la proportion de phrases de l'auteur $j$.\n-  En fonction de l'auteur $j$ qui est choisi à la première étape, notre phrase est tirée selon une loi de densité $f_j$ sur $\\mathbb R ^d$.\n\n![shema_bernouilli_fr](http://bourg-la-reine-echecs.fr/telechargements/shema_bernoulli_auteurs_corrige.png)\n\nOn a donc que : $f = \\sum \\lambda_j f_j$.\n\nAvec le corpus d'entraînement, on obtient que : \n","cell_type":"markdown"},{"execution_count":null,"outputs":[],"source":"nb_texts = len(train_df)\nlambdas = (train_df.author.value_counts()/nb_texts).to_dict()\nprint(\"Lambdas :\",lambdas)","cell_type":"code","metadata":{"_uuid":"f5b3f6602bb7be0d95d15ce87e4fcab249feb7d3","collapsed":true,"_cell_guid":"bd32b467-bbf1-4a70-a786-31e3199aafde"}},{"metadata":{"_uuid":"ae5e7ddeaf1023ab69ebd3c84689af41dbf11153","_cell_guid":"7d702003-ce64-4ce3-82c2-e552e847e919"},"source":"- $\\lambda_1 = 0.403493538995863$ \n- $\\lambda_2 = 0.287808366106543$\n- $\\lambda_3 = 0.308698094897594$\n\nLe lecteur attentif aura peut-être remarqué que le fichier d'exemple \"sample\\_submission\" renvoie justement pour chaque phrase les probabilités $(\\lambda_1, \\lambda_2, \\lambda_3)$... ce n'est pas un hasard !! C'est le retour qui minimise la \"log loss\" avec le plus mauvais programme qui soit : celui qui ne distingue rien et qui envoie toutes les phrases sur le vecteur nul ($\\mathcal L (X) = \\delta_0$ où $\\delta_0$ est la mesure de \"dirac\" au point $0$).\nCela découle d'un résultat classique de théorie de l'information : si $(\\lambda_j)_j$ et $(p_j)_j$ sont deux mesures de probabilité discrètes, alors l'entropie croisée :\n$$H_\\lambda(p) = \\mathbb E[log loss] = -\\sum_j \\lambda_j \\log(p_j)$$\nest minimisée quand $p_j = \\lambda_j$ pour tout $j$.\n\nNous allons maintenant généraliser ce résultat !\n\nÉtant donné une phrase transformée par notre programme en un vecteur $x$ (selon donc la loi $X$), la meilleure solution (au sens que c'est celle qui minimise l'espérance de la note finale \"log loss\") de probabilités  à renvoyer est le triplet :\n\n$$ p_1^{opt}(x) = \\frac{\\lambda_1 f_1(x)}{\\sum_j \\lambda_j f_j(x)}, \\ \\ \\ \\ \np_2^{opt}(x) = \\frac{\\lambda_2 f_2(x)}{\\sum_j \\lambda_j f_j(x)}, \\ \\ \\ \\ \np_3^{opt}(x) = \\frac{\\lambda_3 f_3(x)}{\\sum_j \\lambda_j f_j(x)}.$$\n\nBon ! maintenant que cela a été dit, il n'y a plus qu'à se retrousser les manches !\n\nL'espérance de la note \"log loss\" est donnée dans ce cas par la formule suivante :\n\n$$ \\mathbb E [log loss] = - \\sum_j{\\lambda_j \\int_{\\mathbb R ^d}\\log(p_j(x))f_j(x) \\mathrm d x}$$\noù $p_j(x)$ est la probabilité que l'on renvoie pour l'auteur $j$ quand on reçoit le vecteur $x$  ; c'est notre estimation que la phrase qui a été transformé en le vecteur $x$ soit de l'auteur $j$ (on veut donc montrer que $p_j^{opt}(x)= \\frac{\\lambda_j f_j(x)}{\\sum_k \\lambda_k f_k(x)}$ est effectivement la solution optimale).\n\nEn toute généralité, le programme  renverra une probabilité qui peut s'écrire sous la forme :\n$$ q_j(x)= \\frac{\\lambda_j g_j(x)}{\\sum_k \\lambda_k g_k(x)}$$\noù $g_j$ est une fonction continue positive (qu'on supposera quand même de support au moins aussi grand que celui de $f_j$ car s'il existe $x$ tel que $q_j(x) = 0$ et $f_j(x) > 0$, alors $\\mathbb P(logloss = \\infty) >0$...). \nQuitte à ce que les $g_j$ aillent \"perdre\" de leur poids en dehors du support de $f$ (c'est-à-dire que si $f(x) = 0$, $x$ n'a aucune chance d'apparaître ; peu nous importe donc la valeur des $g_j$ en ces points), on peut aussi supposer que les $g_j$ sont des fonctions de densité (c'est-à-dire que : $\\int_{\\mathbb R^d} g_j(x) dx = 1$).\n\nOn peut prendre, par exemple, $g_j = \\frac{1}{\\lambda_j}f_j$ pour ne plus tenir compte de la différence du nombre de phrases entre les auteurs (ici, les $g_j$ ne sont pas des fonctions de densité puisque leur intégrale ne vaut pas $1$, mais l'on pourrait s'y ramener comme dit juste avant).\nSi l'on veut des probabilités \"extrêmes\" (proches de $0$ ou de $1$), on pourra poser : $g_j = f_j^2 \\times \\frac{1}{\\int f_j^2}$.\nAu contraire, si l'on souhaite \"nuancer\" ces probabilités, on posera : $ g_j = \\sqrt{f_j} \\times \\frac{1}{\\int \\sqrt f_j}$. \n\nMontrons à présent que le meilleur choix pour minimiser la note \"log loss\" est bel et bien : $p_j^{opt} = \\frac{\\lambda_j f_j(x)}{\\sum_k \\lambda_k f_k(x)}$. Notons $log loss^{opt}$ la note obtenue avec ce dernier choix et $log loss^{q_j}$ la note obtenue avec les probabilités $q_j$ définies à l'aide des fonctions $g_j$. \nOn a alors :\n\n\\begin{align}\n\\mathbb E[log loss^{opt}] - \\mathbb E[log loss^{q_j}] & = - \\sum_j{\\lambda_j \\int_{\\mathbb R ^d}\\log(p_j^{opt})f_j(x) \\mathrm d x} - \\sum_j{\\lambda_j \\int_{\\mathbb R ^d}\\log(q_j)f_j(x) \\mathrm d x}\\\\\n & = - \\sum_j{\\lambda_j \\int_{\\mathbb R ^d}(\\log(p_j^{opt})-\\log(q_j))f_j(x) \\mathrm d x} \\\\\n & = - \\sum_j{\\lambda_j \\int_{\\mathbb R ^d}\\log(\\frac{\\lambda_j f_j(x)}{\\lambda_j g_j(x)}\\frac{\\sum_k \\lambda_k g_k(x)}{\\sum_k \\lambda_k f_k(x)})f_j(x) \\mathrm d x} \\\\\n & = - \\sum_j{\\lambda_j \\int_{\\mathbb R ^d}\\log(\\frac{ f_j(x)}{ g_j(x)})f_j(x) \\mathrm d x} + {\\int_{\\mathbb R ^d}\\log(\\frac{\\sum_k \\lambda_k f_k(x)}{\\sum_k \\lambda_k g_k(x)})(\\sum_j \\lambda_j f_j(x))\\mathrm d x} \\\\\n & = - \\sum_j \\lambda_j D_{KL}(f_j || g_j) + D_{KL}(\\sum_k \\lambda_k f_k || \\sum_k \\lambda_k g_k)\\\\\n & \\text{(où $D_{KL}$ désigne la divergence de Kullback-Leibler... nous y reviendrons !)} \\\\\n & \\le 0 \\text{ par la propriété de convexité de $D_{KL}$ en les paires de fonctions ;}\\\\\n & \\text{ici, il y en a $3$ : $(f_1,g_1)$, $(f_2, g_2)$ et $(f_3,g_3)$. On conclut que $p_j^{opt}$ est bien optimal :}\\\\\n \\mathbb E[log loss^{opt}] & \\le \\mathbb E[log loss^{q_j}]\n\\end{align}\n\nLa divergence de Kullback-Leibler entre deux densités $p$ et $q$ sur $\\mathbb R^d$ est donnée par la formule suivante :\n$D_{KL}(p||q) = \\int_{\\mathbb R^d}\\log(\\frac{p(x)}{q(x)})p(x)\\mathrm d x$. Pour le voir rapidement, en théorie de l'information, c'est la quantité moyenne de bits supplémentaires qu'il va falloir pour coder à partir de la source $q$ un code optimal de la source $p$. \nNous reparlerons de la divergence de Kullback-Leibler dans la partie consacrée à la \"t-SNE\" (algorithme qui fonctionne justement sur une minimisation de cette même divergence lors du changement de dimension de l'espace des données).\n\n\nNous avons donc franchi vaillamment la première étape (notre propos étant de montrer qu'on peut faire de l'informatique de façon propre et rigoureuse !) : on sait maintenant que, une fois que l'on possédera notre programme qui transforme les phrases en vecteurs, la quantité à rechercher (et à renvoyer à Kaggle) est :\n$$ p_j^{opt}(x) = \\frac{\\lambda_jf_j(x)}{\\sum_k \\lambda_k f_k(x)}.$$\n\n","cell_type":"markdown"},{"metadata":{"_uuid":"9c31cf5999d1b500a43ea5ca086945d071eda8d6","_cell_guid":"58541e73-4def-4a4b-9efe-39319e8000f5"},"source":"<div id=\"kppv\" />\n# k-plus-proches-voisins (pour retrouver les probabilités $p_{ij}^{opt}$ optimales)\n\nNous allons montrer une manière de récupérer ces probabilités. Oh ! ce n'est certes pas un algorithme extraordinaire puisque c'est celui des plus-proches-voisins.\nNéanmoins, il a cet avantage de donner asymptotiquement (lorsque $N$ le nombre de textes tend vers l'infini) le meilleur résultat possible. D'autres algorithmes souvent employés font par exemple l'hypothèse d'une distribution gaussienne des données (c'est le cas des classificateurs naïfs de Bayes) qui donnent de bons résultats sur de petits échantillons (puisqu'il se basent sur le calcul des espérances et des covariances des variables ; ce qui nécessite beaucoup moins de données que pour estimer une densité) mais ces algorithmes cessent de donner de bons résultats sitôt que la base d'apprentissage est suffisante.\n\nVoici donc l'algorithme des k-plus-proches-voisins :\n\n- On possède une base d'apprentissage $\\{(x_1,y_1), \\ ... \\ , \\ (x_N,y_N)\\}$ contenant les $N$ vecteurs $x_1, ..., x_N \\in \\mathbb R ^d$ formés à partir des $N$ phrases avec leur auteur respectif $y_1,...,y_N \\in \\{1,2,3\\}$. On choisit aussi une distance $d$ sur $\\mathbb R^d$ (généralement la distance euclidienne, mais cela n'est pas obligatoire ; les résultats asymptotiques resteront vrais pour n'importe quelle distance : on dit que l'opérateur des k-plus-proches voisins est \\textit{universellement} consistant).\n- On choisit un entier $k$ dépendant de $N$ qui doit être petit devant $N$ (on veut : $k/N \\underset{N \\to \\infty}{\\longrightarrow} 0$) mais assez grand tout de même (il faut que : $k \\underset{N\\to \\infty}{\\longrightarrow} \\infty$).\n- Maintenant, à chaque fois que l'on reçoit une nouvelle phrase (en fait un nouveau vecteur $x$), on considère l'ensemble $kNN$ des $k$ indices $i_1 <... < i_k$ des points les plus proches de $x$ :\n$$ kNN = \\{i_1 < ... < i_k \\ | \\ \\forall \\ j \\notin \\{i_1,...,i_N\\}, \\forall \\ i \\in \\{i_1,...,i_N\\}, \\ \\ d(x,x_i) \\le d(x,x_j)\\} $$\n- Enfin, on retourne des probabilités $\\hat p_j(x)$ proportionnelles au nombre de plus-proches-voisins qui sont de l'auteur $j$. Ainsi :\n$$\\text{Pour $j \\in \\{1,2,3\\}$, } \\hat p_j(x) = \\frac{1}{k} \\# \\{ i \\in kNN \\ | \\ y_i = j\\}$$\n\n\nNous avons donc présenté l'algorithme très simple des $k$-plus-proches-voisins qui renvoie des estimations $\\hat p_j(x)$ de $p_j^{opt}(x)$. Il vérifie la propriété de \"consistance faible\" suivante :\n$$\\text{Si $k \\underset{N\\to \\infty}{\\longrightarrow} \\infty$ et que $k/N \\underset{N \\to \\infty}{\\longrightarrow} 0$, alors : } \\forall \\ \\epsilon >0,\\ \\mathbb P (|\\hat p_j - p_j^{opt}| > \\epsilon ) \\underset{N\\to \\infty}{\\longrightarrow} 0$$\n\nAsymptotiquement, on retrouve donc bien empiriquement la valeur de $p_j^{opt}$. Cela ne nous dit rien pour autant sur la vitesse de convergence. Il existe des raffinements de cet algorithme (par exemple en ayant recours à des graphes de proximité) qui permettent d'améliorer cette vitesse de convergence.\n\n","cell_type":"markdown"},{"metadata":{"_uuid":"d01a638acf2526829fc659941c5982f07d4a2a75","_cell_guid":"99a66cf8-5e2a-4a93-9e0e-fa00fe3017db"},"source":"<div id=\"exemples\" />\n# Exemples","cell_type":"markdown"},{"metadata":{"_uuid":"64f533d93f75d3b563bb47636a716e2ea51f3660","_cell_guid":"28294278-765c-4b3a-b3ac-077aae087dfd"},"source":"\n<div id=\"exemples_courants\" />\n## Exemples courants\nTODO\n","cell_type":"markdown"},{"metadata":{"_uuid":"4316b025c6aa0e383360cfdee4ddf20fdaab1ede","_cell_guid":"3997f0b6-9d59-4fa2-ad13-66f5be4facbb"},"source":"\n\n<div id=\"voca_auteurs\" />\n## Exemple d'utilisation du vocabulaire\n\nLes exemples que nous avons vus dans la section précédente sont des exemples courants de stylométrie. Ce sont aussi des exemples très généraux en ce sens qu'ils ne nécessitent pas la présence d'un corpus d'entraînement : ils pourraient tout aussi bien servir dans le cas de classification *non-supervisée* (et pas seulement *supervisée* comme c'est le cas ici).\n\nNous allons voir maintenant un exemple de fonction qui recourt au vocabulaire utilisé dans le corpus d'entraînement. Étant donné une phrase, cette fonction retourne un vecteur de taille $3$ contenant la log-vraisemblance de l'utilisation de ce vocabulaire pour chacun des $3$ auteurs.\n\nDécoupons notre corpus d'apprentisage en deux : \n- un nouveau corpus d'apprentissage pour construire le dictionnaire (95% des données)\n- un corpus de test qui nous permettra de visualiser les résultats (5% des données)\n\n","cell_type":"markdown"},{"execution_count":null,"outputs":[],"source":"N_train = len(train_df)\n\n# découpe du train\ncut = round(N_train * 0.95)\n\ntrain = pd.DataFrame(train_df, index=range(cut))\ntest = pd.DataFrame(train_df, index=range(cut,N_train))","cell_type":"code","metadata":{"_kg_hide-input":false,"_uuid":"74e0d147b6062c14f29b225555b80efa52bbeb4c","collapsed":true,"_kg_hide-output":false,"_cell_guid":"1f92cf5c-5aac-4af0-9d99-c78b01b8d835"}},{"metadata":{"_uuid":"3ece3bdcf0b34e38ac1eef9083af905b564b438c","_cell_guid":"bccac24e-5e6b-48be-a761-fcd84970f97d"},"source":"\nOn commence par construire un dictionnaire contenant tous les mots apparaissant dans le corpus d'entraînement. Chaque entrée de ce dictionnaire est associée à un tableau donnant le nombre d'apparitions du mot pour chacun des différents auteurs.\n","cell_type":"markdown"},{"execution_count":null,"outputs":[],"source":"# construit à partir d'un corpus d'entraînement un dictionnaire des mots utilisés. Pour chaque mot, on remplit un dictionnaire qui à chaque auteur associe le nombre de fois que ce mot est apparu chez l'auteur\ndef voca_Authors(train):\n    voca = {}\n    for i,line in train.iterrows():\n        words = nltk.word_tokenize(line['text'])\n        for word in words:\n            word = word.lower()\n            if not word in voca:\n                voca[word] = {auth:0 for auth in authors}\n                voca[word][line['author']] = 1\n            else:\n                voca[word][line['author']] += 1\n    return voca\n\nprint('Calcul du dictionnaire du vocabulaire.')\nvoca_authors = voca_Authors(train)\nprint('Taille du dictionnaire :',len(voca_authors),'\\n20 exemples tirés de ce dictionnaire :\\n')\nfor i,word in enumerate(voca_authors):\n    if i < 20:\n        print(word,voca_authors[word])","cell_type":"code","metadata":{"_uuid":"4bdc96bd995f1c820f96f473d32418e2d0aef837","collapsed":true,"_cell_guid":"45af3900-a473-42ca-85e5-30e60d229f69"}},{"metadata":{"_uuid":"b241c0c0a9914bba5c54bbacdd9f5b73314f9976","_cell_guid":"3e4ad7ae-1541-45d7-8ba6-abc46ed486f2"},"source":"Le tout maintenant est de bien voir ce que l'on calcule.\nOn fera l'hypothèse assez forte (c'est-à-dire assez fausse) que :\n- la longueur d'une phrase suit une loi indépendante de l'auteur (on peut regarder les histogrammes des longueurs de phrase en fonction de l'auteur à l'adresse suivante : lien et constater qu'effectivement ces 3 histogrammes sont assez proches),\n- tous les mots d'une phrase sont tirés indépendamment et identiquement selon une loi ne dépendant que de l'auteur (c'est cette hypothèse qui est très forte et très fausse).\n\nGrâce aux hypothèses d'indépendances, on peut calculer la vraisemblance que l'auteur $j$ ait écrit la phrase $s = [w_1,...,w_k]$ :\n$$ L (w_1, \\ ... \\ , \\ w_k \\ | \\ author = j) = \\prod_{i=1}^k \\mathbb P(\\text{author of $w_i$ = $j$} \\ | \\ \\text{word = $w_i$})$$\nÀ présent, prenons comme exemple le mot \"of\" : on a les statistiques suivantes : \n\n","cell_type":"markdown"},{"execution_count":null,"outputs":[],"source":"voca_authors['of']","cell_type":"code","metadata":{"_uuid":"9ba8d66874b1b17b4b178f422cdf8d18d0ebffc7","collapsed":true,"_cell_guid":"a2e64cb2-9ea5-47a0-8609-e4df5e52fc7e"}},{"metadata":{"_uuid":"b3aada10014ab914f5da0e8468ddbf5c0693fbc5","_cell_guid":"df3b5f6c-1602-418d-bdf6-907d1c0c49cb"},"source":"Il y a $8539+5568+5850 = 19957$ occurrences du mot \"of\" dans notre corpus d'entraînement. Edgar Poe est l'auteur de 8539 d'entre elles, c'est-à-dire d'une proportion $x_1 = 8539/19957 = 0.428$. Il est aussi l'auteur d'une proportion $\\lambda_1 = 0.40 < x_1$ de l'ensemble des phrases du corpus. Comme on a fait l'hypothèse que la loi sur la longueur des phrases était indépendante de l'auteur, cela signifie qu'il écrit donc davantage le mot 'of' que sa sur-représentativité dans le corpus lui permettrait ; c'est-à-dire que : $ p_1 := \\mathbb P(\\text{author of $'of' = 1$}) > 1/3$.\nSi l'on note $ p_j = \\mathbb P(\\text{author of $'of'  = j$})$ et $x_j$ la proportion d'apparitions dans le corpus d'entraînement, on a la relation suivante :\n$$ x_j = \\frac{\\lambda_j p_j}{\\sum_k \\lambda_k p_k}.$$\n\nOn a donc les $x_j$ et les $\\lambda_j$ et l'on veut retrouver les $p_j$ pour calculer nos log-vraisemblances !\n\nÀ noter que l'on peut calculer très facilement $\\sum_k \\lambda_k p_k$ : $$ \\sum_j \\frac{x_j}{\\lambda_j} = \\sum_j \\sum_k \\lambda_k p_k p_j = \\sum_k \\lambda_k p_k$$\n... et ainsi retrouver la valeur $p_j$ que l'on recherche :\n$$ p_j = \\frac{x_j}{\\lambda_j} \\times \\sum_k\\frac{x_k}{\\lambda_k}$$\n\nOn peut alors calculer la log-vraisemblance pour chacun des auteurs :\n$$ \\log L(w_1,...,w_k \\ | \\ author = j) = \\sum_{i=1}^k \\log(p_j(w_i)) $$\n\n","cell_type":"markdown"},{"execution_count":null,"outputs":[],"source":"def vect_Voca(sentence, dict = voca_authors, zero = 0.01):\n    words = nltk.word_tokenize(sentence)\n    ret = [0]*len(authors)\n    for word in words:\n        word = word.lower()\n        if word in dict:\n            vect = [0]*len(authors)\n            nb_appar = 0\n            for auth in dict[word]:\n                nb_appar += dict[word][auth]\n            for auth in dict[word]:\n                vect[authors[auth]] = dict[word][auth]/nb_appar/lambdas[auth] # vect[j] = x_j/lambda_j = p_j / (Sum lambda_j p_j)\n            s = 1/sum(vect) # s = (Sum lambda_j p_j) \n            vect = [p_j_div_sum * s for p_j_div_sum in vect] # vect[j] = p_j\n\n            for j,p_j in enumerate(vect):\n                if p_j == 0:\n                    ret[j] += log(zero)\n                else:\n                    ret[j] += log(p_j)\n    return ret","cell_type":"code","metadata":{"_uuid":"74cd03a6781236026d98fc574a863f5fe076fac0","collapsed":true,"_cell_guid":"0c032078-7be7-4638-9d3d-b1379f7fefda"}},{"metadata":{"_uuid":"edd3b6d45fff503ad256d634ff9e6bbc284720c3","_cell_guid":"b6cd9df7-2333-44b1-94db-e849b5fe943d"},"source":"\n... et visualiser les résultats en queue de comète :\n","cell_type":"markdown"},{"execution_count":null,"outputs":[],"source":"# la matrice des phrases du corpus de texte transformées à l'aide de vect_Voca\nX = []\ncol = [] # la couleur en fonction de l'auteur : Poe=>rouge, Lovecraft=>vert, Shelley=>bleu\n\n# calcul de X la matrice de taille N_tests * 3\nfor i,line in test.iterrows():\n    vect = vect_Voca(line['text'])\n    X.append(vect)\n    col.append(colours[line['author']])\nX = np.array(X)\n\n## L'affichage\n\n#import plotly\n#from plotly.graph_objs import Scatter, Layout, Scatter3d\nfig = plt.figure(figsize=(15,15))\nax = fig.add_subplot(111, projection='3d')\n\nax.scatter(X[:,0], X[:,1], X[:,2], c=col, marker='+')\n#plotly.iplot({\n #   \"data\": [Scatter3d(x=X[:,0], y=X[:,1], z=X[:,2], mode='markers', \n #   marker=dict(size=4,\n #       color=col, \n  #      opacity=0.7\n  #  ))],\n  #  \"layout\": Layout(title=\"vect_Voca\", scene=dict(camera= dict(\n #   up=dict(x=0, y=0, z=1),\n  #  center=dict(x=0, y=0, z=0),\n  #  eye=dict(x=1.5, y=0.75, z=0.475)\n#)))\n#})","cell_type":"code","metadata":{"_uuid":"c78f9435a933373a067f9a9ce44cc103cd60d85c","collapsed":true,"_cell_guid":"2e8496fc-cf9e-484d-a7ef-5944f142212d"}},{"metadata":{"_uuid":"04b239cbe0bfcf311b13584dcbc515da8a70061d","_cell_guid":"eb312e20-22fa-4684-ae60-a2822007d71f"},"source":"<div id=\"kernel\" />\n## Exemple de kernel sur les structures arborescentes des phrases\nTODO","cell_type":"markdown"},{"metadata":{"_uuid":"6009daab7b109dcb74032980f569c07727a55ae2","collapsed":true,"_cell_guid":"2ed5f3b6-e5bb-4569-831d-11d86872699b"},"source":"<div id=\"visualisation\" />\n# Visualisation des données\n","cell_type":"markdown"},{"metadata":{"_uuid":"03138e5a4f2d83b2fd88ef0db455a142e54e6a2e","_cell_guid":"1a6a445b-2e08-438c-a714-fe050fd457a8"},"source":"<div id=\"ACP\" />\n## L'Analyse en Composantes Principales (ACP)\nTODO","cell_type":"markdown"},{"metadata":{"_uuid":"98b519f65441168ece4cccee7c3ca1c873133fce","_cell_guid":"5b186af8-6962-4627-aa3a-38c0d2bcc280"},"source":"<div id=\"tSNE\" />\n## La t-SNE\nTODO","cell_type":"markdown"},{"metadata":{"_uuid":"4249749011e75bd4e46f4a8fdbbd058379312bcc","_cell_guid":"c0d96ae4-eb47-4eac-ada1-5e7b267286c3"},"source":"<div id=\"resultats\" />\n# Résultats (score : 0,40)\n\nScore obtenu simplement avec la log-vraisemblance de l'utilisation du vocabulaire :","cell_type":"markdown"},{"execution_count":null,"outputs":[],"source":"from sklearn.neighbors import NearestNeighbors\n\ntrain = pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv(\"../input/test.csv\")\n\nvoca_authors = voca_Authors(train)\n# la matrice des phrases du corpus de texte transformées à l'aide de vect_Voca\nM = [] # the train matrix\nSol = [] # the solutions of the train matrix\nX = [] # the test matrix\n\n\nfor i,line in train.iterrows():\n    vect = vect_Voca(line['text'])\n    M.append(vect)\n    Sol.append(line['author'])\nM = np.array(M)\n\nfor i,line in test.iterrows():\n    vect = vect_Voca(line['text'])\n    X.append(vect)\nX = np.array(X)\n\n\n# The kNN algorithm\nadjunction = 0.025\nnb_ppv = 200 # number of nearest neighbors\nM_ppv = NearestNeighbors(n_neighbors=nb_ppv)\nM_ppv.fit(M)\ntab_kneighbors = M_ppv.kneighbors(X, return_distance=False)\n\nProbas = [] # the return\n\nfor i,line in test.iterrows():\n    p = [0]*3\n    kneighbors = tab_kneighbors[i]\n    for kneighbor in kneighbors:\n        p[authors[Sol[kneighbor]]] += 1\n    s = sum(p)\n    p = [x/s for x in p]\n    Probas.append(p)\n\n# harmonisation of the probabilities\nfor v in Probas:\n    for i,prob in enumerate(v):\n        v[i] = (prob + adjunction)/(1+3*adjunction)\n\n\nsubmission = pd.read_csv('../input/sample_submission.csv')\nsubmission.loc[:,['EAP', 'HPL', 'MWS']] = Probas\nsubmission.to_csv(\"Log_likelihoof_on_vocabulary.csv\", index=False)\nsubmission.head()","cell_type":"code","metadata":{"_uuid":"24e292a576b6715eeed15f0caf35d630a74e53d2","collapsed":true,"_cell_guid":"831fcc88-c6c6-40e3-9949-eb91654cf539"}}],"metadata":{"language_info":{"nbconvert_exporter":"python","mimetype":"text/x-python","pygments_lexer":"ipython3","version":"3.6.3","codemirror_mode":{"name":"ipython","version":3},"name":"python","file_extension":".py"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}},"nbformat_minor":1,"nbformat":4}