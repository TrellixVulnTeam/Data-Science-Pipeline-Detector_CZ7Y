{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\n\nimport string\nimport numpy as np \nimport pandas as pd \nimport os\nfrom tqdm import tqdm\nimport re\nimport statistics\nfrom collections import Counter\nfrom sklearn.preprocessing import LabelEncoder\nfrom keras.optimizers import SGD\n\nfrom keras.preprocessing import sequence, text\nfrom keras.models import Sequential\nfrom keras.layers.recurrent import LSTM\nfrom keras.layers.core import Dense,Dropout, Activation\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers import  Bidirectional, SpatialDropout1D, GlobalMaxPooling1D\nfrom keras.utils import np_utils\nfrom keras.callbacks import EarlyStopping\nfrom keras.utils.vis_utils import plot_model\nfrom keras.utils import to_categorical\n\n\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nstopwords = stopwords.words('english')\n\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/spooky-author-identification/train.zip')\ntest = pd.read_csv('/kaggle/input/spooky-author-identification/test.zip')\nsample = pd.read_csv('/kaggle/input/spooky-author-identification/sample_submission.zip')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()\ntrain.isnull().sum()\nsample.head()\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def label_encoding(text):\n    lbl_enc = LabelEncoder()\n    integer_encoding = lbl_enc.fit_transform(text)\n    return integer_encoding\n\ntrain['author_integer_encode'] = label_encoding (train['author'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = to_categorical(train['author_integer_encode'],3)\ny_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = pd.DataFrame(y_train,columns=['EAP','HPL','MWS'])\ny.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def replace_typical_misspell(text):\n        miss_spell = {\"aren't\": \"are not\", \"can't\": \"cannot\", \"couldn't\": \"could not\",\n              \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\",\n              \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n                 \"he'd\": \"he would\", \"he'll\": \"he will\", \"he's\": \"he is\",\n                 \"i'd\": \"I had\", \"i'll\": \"I will\", \"i'm\": \"I am\", \"isn't\": \"is not\",\n                 \"it's\": \"it is\", \"it'll\": \"it will\", \"i've\": \"I have\", \"let's\": \"let us\",\n                 \"mightn't\": \"might not\", \"mustn't\": \"must not\", \"shan't\": \"shall not\",\n                 \"she'd\": \"she would\", \"she'll\": \"she will\", \"she's\": \"she is\", \"shouldn't\": \"should not\", \"that's\": \"that is\", \"there's\": \"there is\",\n                 \"they'd\": \"they would\", \"they'll\": \"they will\", \"they're\": \"they are\",\n                 \"they've\": \"they have\", \"we'd\": \"we would\", \"we're\": \"we are\",\n                 \"weren't\": \"were not\", \"we've\": \"we have\", \"what'll\": \"what will\",\n                 \"what're\": \"what are\", \"what's\": \"what is\", \"what've\": \"what have\",\n                 \"where's\": \"where is\", \"who'd\": \"who would\", \"who'll\": \"who will\",\n                 \"who're\": \"who are\", \"who's\": \"who is\", \"who've\": \"who have\",\n                 \"won't\": \"will not\", \"wouldn't\": \"would not\", \"you'd\": \"you would\",\n                 \"you'll\": \"you will\", \"you're\": \"you are\", \"you've\": \"you have\",\n                 \"'re\": \" are\", \"wasn't\": \"was not\", \"we'll\": \" will\", \"tryin'\": \"trying\"\n             }\n        misspell_re = re.compile('(%s)' %'|'.join(miss_spell.keys()))\n        def replace(match):\n            return miss_spell[match.group(0)]\n        return misspell_re.sub(replace,text)\n   \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_punctuations(text):\n    extra_chars = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']',\n          '>', '%', '=', '#', '*', '+', '\\\\', '•', '~', '@', '£', '·', '_', '{', '}', '©', '^',\n          '®', '`', '<', '→', '°', '€', '™', '›', '♥', '←', '×', '§', '″', '′', 'Â', '█',\n          '½', 'à', '…', '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶',\n          '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', '▒', '：', '¼',\n          '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲',\n          'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', '∙', '）', '↓', '、', '│', '（', '»', '，', '♪',\n          '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√']\n    text =''.join(x for x in str(text) if x not in list(string.punctuation)+extra_chars)\n    return text\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_digits(text):\n    return re.sub(r'\\d+',' ',text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntrain['text']=train['text'].apply(replace_typical_misspell)\ntrain['text'] = train['text'].apply(remove_punctuations)\ntrain['text']= train['text'].apply(remove_digits)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['text']=test['text'].apply(replace_typical_misspell)\ntest['text'] = test['text'].apply(remove_punctuations)\ntest['text']= test['text'].apply(remove_digits)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.text.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def info(df):\n    df['line_num'] = train['text'].apply(lambda x : len(x.split()))\n    max_len = max(df['line_num'])\n    min_len = min(df['line_num'])\n    median= statistics.median(df['line_num'])\n    count = Counter(df['line_num'])\n    return max_len, min_len, median,count\n    \nmax_len, min_len, median,count = info(train)\nprint('Train set: \\n maximum number of words: {0}\\n minimum number of words: {1} \\n Median of the number of words: {2} \\n Varying lengths of the texts and their frequencies : \\n {3} \\n '.format(max_len,min_len,median,count))\nprint('number of samples: {}'.format(train.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def loading_embedding(path):\n    def get_coeffs(word, *a):\n        return word,np.asarray(a,dtype = np.float32)\n    embeddings =  dict(get_coeffs(*o.rstrip().split(\" \")) for o in open(path) if len(o) >300  ) \n    return embeddings \n\nfast_vecs = loading_embedding('/kaggle/input/fasttext-crawl-300d-2m/crawl-300d-2M.vec')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"maxlen = 123\nmax_features = 15000\nembed_size = 300\ntokenizer = text.Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train['text'])+list(test['text']))\n\nx_train = tokenizer.texts_to_sequences(train['text'])\nx_train = sequence.pad_sequences(x_train,maxlen=maxlen, padding = 'post', truncating= 'pre')\n\ny_test = tokenizer.texts_to_sequences(test['text'])\ny_test = sequence.pad_sequences(y_test,maxlen=maxlen, padding= 'post',truncating='pre')\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def embedding_matrix(fast_vecs, word_index,max_features):\n    fast_vals = np.stack(fast_vecs.values())\n    embed_size = fast_vals.shape[1]\n    matrix = np.random.normal(size =(max_features,embed_size))\n    for word,i in word_index.items():\n        if i < max_features:\n            embedding_vector = fast_vecs.get(word)\n            if embedding_vector is not None:\n                matrix[i]=embedding_vector\n    return matrix\nmatrix = embedding_matrix(fast_vecs,tokenizer.word_index,max_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"matrix.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def Model(max_features, embedding_size, embedding_matrix):\n    model = Sequential()\n    model.add(Embedding(max_features,embedding_size,weights = [embedding_matrix], input_length=123, trainable = False))\n    model.add(Bidirectional(LSTM(256, return_sequences = True,unit_forget_bias= True)))\n    model.add(SpatialDropout1D(0.2))\n    model.add(GlobalMaxPooling1D())\n    model.add(Dense(128,activation='sigmoid' ))\n    model.add(Dropout(0.5))\n    model.add(Dense(3,activation='softmax'))\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics = ['accuracy'])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Model(max_features,embed_size,matrix)\nplot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"es = EarlyStopping(monitor='val_loss', mode='min',verbose=1)\nhistory = model.fit(x_train,y,batch_size=64,epochs=25,verbose=1,validation_split=0.2,callbacks=[es])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit = model.predict_proba(y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample.head()\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nids = test['id']\npredict = pd.DataFrame(submit, columns=['EAP','HPL','MWS'])\nsubmission = pd.concat([ids, predict] ,axis = 1)\nsubmission.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}