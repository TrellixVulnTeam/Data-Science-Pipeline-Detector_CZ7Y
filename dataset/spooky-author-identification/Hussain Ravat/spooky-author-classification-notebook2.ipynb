{"cells":[{"source":"##import necessary packages\n#####\n\nimport numpy as np # linear algebra\nimport scipy as sp \nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib\nimport seaborn as sns \nimport nltk\nfrom nltk.corpus import stopwords\nimport wordcloud\nfrom wordcloud import WordCloud, STOPWORDS\nfrom matplotlib import pyplot as plt\n\n##read in the data file an display \ndf_spooky_author=pd.read_csv('../input/train.csv')\ndf_spooky_author\n\nsentence_list=df_spooky_author['text'].values.tolist()\nauthor_list=df_spooky_author['author'].values.tolist()\nid_list=df_spooky_author['id'].values.tolist()\nsentence_list\nauthor_list\n\n\ncombined_list=[list(author_list) for author_list in zip(author_list, sentence_list)]\n\ntokenized_list=[]\n\nfor authors,sentence in combined_list:\n    tokenized_list.append([authors,nltk.word_tokenize(sentence)])\n\ntokenized_list\n\n##make individual authorwise list of words after removing stop words.\n##This is to prepare the data for wordcloud\nstop = set(stopwords.words('english'))    \n\ntokenized_stop_words_list=[]\n\n\nfor author,sentence in tokenized_list:\n    tokenized_stop_words_list_temp=[]   \n    for word in sentence:\n        if not word in stop:\n            tokenized_stop_words_list_temp.append(word)\n    tokenized_stop_words_list.append([author,tokenized_stop_words_list_temp])        \n            \ntokenized_stop_words_list\n\ntokenized_words_EAP=[]\ntokenized_words_MWS=[]\ntokenized_words_HPL=[]\n\nfor author,sentence in tokenized_stop_words_list:\n    if author=='EAP':\n        for words in sentence:\n            tokenized_words_EAP.append(words)\n    if author=='MWS':\n        for words in sentence:\n            tokenized_words_MWS.append(words)\n    if author=='HPL':\n        for words in sentence:\n            tokenized_words_HPL.append(words)\n            \n##word cloud for HPLovenCraft            \nplt.figure(figsize=(30,30))\nwc = WordCloud(background_color=\"black\", max_words=10000, \n               stopwords=stop, max_font_size= 40)\nwc.generate(\" \".join(tokenized_words_HPL))\n##plt.title(\"HP Lovecraft (Cthulhu-Squidy)\", fontsize=16)\n##Uncomment below line and run to see wordcloud for HP Lovencraft\n##plt.imshow(wc.recolor( colormap= 'Pastel1_r' , random_state=17), alpha=0.98)\nplt.axis('off')\n\n\n\n##word cloud for Edgar Allen Poe         \nplt.figure(figsize=(30,30))\nwc = WordCloud(background_color=\"black\", max_words=10000, \n               stopwords=stop, max_font_size= 40)\nwc.generate(\" \".join(tokenized_words_EAP))\n##plt.title(\"HP Lovecraft (Cthulhu-Squidy)\", fontsize=16)\n##Uncomment below line and run to see wordcloud for Edgar Allen Poe\n##plt.imshow(wc.recolor( colormap= 'Pastel1_r' , random_state=17), alpha=0.98)\nplt.axis('off')\n\n\n\n##word cloud for Mary Shelley            \nplt.figure(figsize=(30,30))\nwc = WordCloud(background_color=\"black\", max_words=10000, \n               stopwords=stop, max_font_size= 40)\nwc.generate(\" \".join(tokenized_words_HPL))\n##plt.title(\"HP Lovecraft (Cthulhu-Squidy)\", fontsize=16)\n##Uncomment below line and run to see wordcloud for Mary Shelley \n##plt.imshow(wc.recolor( colormap= 'Pastel1_r' , random_state=17), alpha=0.98)\nplt.axis('off')\n\ndf_test=pd.DataFrame(tokenized_words_HPL)\ndf_test[0].value_counts()\n\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing  \nfrom sklearn import linear_model\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score\nfrom nltk.stem import WordNetLemmatizer\n\n## TF-IDF implementation\ncount_vect = CountVectorizer()\nwordnet_lemmatizer = WordNetLemmatizer()\n\nsentence_new=''\nsentence_new_list=[]\nsentence_list_lemmatized=[]\n\nfor sentence in sentence_list:\n    for words in nltk.word_tokenize(sentence):\n        sentence_new=sentence_new+wordnet_lemmatizer.lemmatize(words)\n    sentence_list_lemmatized.append(sentence_new)\n    sentence_new=''  \n    \n \nsentence_train_counts=count_vect.fit_transform(sentence_list_lemmatized)\nsentence_train_counts.shape\n\ntfidf_transformer = TfidfTransformer()\nsentence_train_tfidf = tfidf_transformer.fit_transform(sentence_train_counts)\nsentence_train_tfidf.shape\n\n##Label Encoder to encode values\nle = preprocessing.LabelEncoder()\nle.fit(author_list)\n\nlist(le.classes_)\n\nauthor_list_encoded=le.transform(author_list)\nauthor_list_encoded\n\nX_scale_train,X_scale_test,Y_scale_train,Y_scale_test = train_test_split(sentence_train_tfidf,author_list_encoded,test_size=0.3,random_state=78)\n\nC_list=[0.0001,0.0003,0.001,0.003,0.01,0.01,0.1,0.3,1,3]\naccuracy=0\nc_final=0\naccuracy_1=0\n\nfor c in C_list:\n    mul_lr = linear_model.LogisticRegression(penalty='l2',multi_class='multinomial', solver='newton-cg',C=c)\n    mul_lr.fit(X_scale_train,Y_scale_train)    \n    accuracy_1=accuracy_score(Y_scale_test,mul_lr.predict(X_scale_test)) \n    accuracy_1\n    if  accuracy_1 > accuracy:  \n        accuracy=accuracy_1                     \n        c_final=c\n        ##Uncomment below line and run to see how acuuracy improves with change in value of c\n        print ('C is',c,'Accuracy is ',accuracy_score(Y_scale_test,mul_lr.predict(X_scale_test)))\n    \n\n    \nmul_lr = linear_model.LogisticRegression(penalty='l2',multi_class='multinomial', solver='newton-cg',C=c)\nmul_lr.fit(X_scale_train,Y_scale_train)    \naccuracy_1=accuracy_score(Y_scale_test,mul_lr.predict(X_scale_test))     \naccuracy_1\n\n\ny_predict=mul_lr.predict(X_scale_test)\ny_predict_author=le.inverse_transform(y_predict)\n##list of predicted authors \ny_predict_author\n\n###Predict Probabilities\nmul_lr.predict_proba(X_scale_test)\n\n########Preparing the data for training predictions\ndf_spooky_author_test=pd.read_csv('../input/test.csv')\ndf_spooky_author_test\n\nsentence_list_test=df_spooky_author_test['text'].values.tolist()\nid_list_test=df_spooky_author_test['id'].values.tolist()\n\n\n#####Very important use transorm here instread of fit transofrm\n#####Training data has been fitted above hence here we need to use only transform\n##### If fit transform is used we get dimensional mismatch while making predictions from the trained model\nsentence_test_count=count_vect.transform(sentence_list_test)\nsentence_test_count.shape\n\nsentence_test_tfidf = tfidf_transformer.transform(sentence_test_count)\nsentence_test_tfidf.shape\n\nX_prob_predict=mul_lr.predict_proba(sentence_test_tfidf)\ndf_X_prob_predict=pd.DataFrame(X_prob_predict)\ndf_X_prob_predict.columns=['EAP', 'HPL', 'MWS']\n\ndf_X_prob_predict\ndf_X_ID=pd.DataFrame(id_list_test)\ndf_X_ID.columns=['id']\ndf_predict_final=df_X_ID.join(df_X_prob_predict)\ndf_predict_final\n\n###Export Data to csv for final submission\ndf_predict_final.to_csv('hravat_spooky_autor_predict.csv',sep=',')\n","outputs":[],"metadata":{"_uuid":"056ff353273e0845a64b92eb2d2322134aa1852a","_kg_hide-output":true,"_cell_guid":"22b5ecdf-73f3-45dd-b400-780b58b35d1b","_kg_hide-input":false},"cell_type":"code","execution_count":null}],"nbformat":4,"metadata":{"language_info":{"file_extension":".py","name":"python","codemirror_mode":{"version":3,"name":"ipython"},"nbconvert_exporter":"python","version":"3.6.3","mimetype":"text/x-python","pygments_lexer":"ipython3"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}},"nbformat_minor":1}