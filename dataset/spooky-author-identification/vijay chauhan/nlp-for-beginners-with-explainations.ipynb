{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **Many times while creating machine learning models we come across textual data so this notebook will help you get familiar with it and how to build a model with text input** ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"In this notebook we will try to understand how to make sense of our textual data and methods to make a classification model for the textual data, So without wasting any time let's jump into it","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"In this notebook we will cover the following\n* How to extract data out of the zip files provided by Kaggle.\n* How to use Tf-idf and convectors \n* How to make a simple classification model using text input\n","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# let's start by importing the files \n\nimport numpy as np \nimport pandas as pd \n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# As we can see the input in is zip files ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's extract these files first\nimport zipfile\n\nzip_files=['/kaggle/input/spooky-author-identification/train.zip','/kaggle/input/spooky-author-identification/test.zip']\nfor zip_file in zip_files:\n    with zipfile.ZipFile(zip_file,'r') as z:\n        z.extractall()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.listdir()\n# As we can see now the files have been extracted and now we have our train and text csv files","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Since we will we dealing with text in English Language let's import the stop words for english.\n\n\nfrom nltk.corpus import stopwords\n\n# these are the general words i.e ( a , the , is , an , i me , my) which are very general and does not \n#make much sense so we will fiter out these words and will consider the more robust words in our text\n# example ( he is a dangerous man , here is ,a can be skipped and we get the essence of the sentence from the \n# word dangerous )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's look at some of the stop words\nstop=stopwords.words('english')\nstop","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's read our data into our dataframes\ntrain=pd.read_csv('train.csv',index_col=False)\ntest=pd.read_csv('test.csv',index_col=False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's take a peek at our data\ntrain.sample(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#let's see how our target variable is distributed in our overall  data\nimport seaborn as sns\nsns.countplot(train['author'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# since our target varibale has 3 different values we will now convert these into 0,1,2 using the label encoder\n\nfrom sklearn.preprocessing import LabelEncoder\ny=train.loc[:,['author']]\nen=LabelEncoder()\ny=en.fit_transform(y.author.values)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# now let's split our data into train and test datasets\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(train.text,y,stratify=y,test_size=.25,shuffle=True)\nsns.countplot(y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# defining the loss function ( log_loss)\nfrom sklearn.metrics import log_loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's import the TfidfVectorizer and see how it works\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# please pay attention as this is the most import part of this notebook, We will try to understand how the Tfidf(term frequency - inverse document frequency) works","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Here we have some text in XX varibale and we will try to fit the Tfidf to this text","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's create and instance of the vectorizer \n\ntfid=TfidfVectorizer(stop_words='english')\n\nxx=['hello today we will try' ,\n   'to understand how  tf-idf works',\n   'this is sample text for this nlp program']\n\ncc=tfid.fit_transform(xx)\n\ndd=pd.DataFrame(cc.toarray(),columns=tfid.get_feature_names())\ndd\n\n# I have created a dataframe to demonstrate what happens under the hood\n\n# In the text we have 4 lines, \n\n# let's start with first line which has 4 distinct words but notice that (will and we got dropped because those\n# are stop words and the rest of the words are assigned a numeric value which depends on how many times the word\n# appeared in the line and how many times the words appears in all the text)\n\n# there is math involved in this process but that is beyond the scope of this notebook, so let's try to understand\n# the working and leave the math for another notebook.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we pass all the text into our vectorizer and fit it\ntfid.fit(list(x_train)+list(x_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we then transform our x_train and x_test using this vetcorizer \nx_train=tfid.transform(x_train)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# if we want to understand this process we can think of it as we tell our model these are the total words we have in\n# our text ,  \n#1.please remove the stop words,\n#2. Please assign a numeric value to each word so that we can feed it to our machine learning model\nx_test=tfid.transform(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now that we have our data in numeric Values we can implement any of our Machine learning models\n\n# let's try a simple linear model , Logistic Regression\n\nfrom sklearn.linear_model import LogisticRegression\nlgr=LogisticRegression(C=1.0)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's fit our model\nlgr.fit(x_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# now our target variable is in shape (4895,) but we want it to be in shape (4895,3) because our data has there\n# different classes so let's do binarization of our target variable \nx_test\na=lgr.predict_proba(x_test)\ndef conver(actual,a):\n    if len(actual.shape)==1:\n        temp=np.zeros((actual.shape[0],a.shape[1]))\n        for i,j in enumerate(actual):\n            temp[i,j]=1\n        actual=temp    \n    return actual\naa=conver(y_test,a)\n\na.shape\n\n\n# To put it in simple words , intially our target ( y_test ) was in form 2,1,0,0,0,1,2,1 but we want it to \n#be in th form \n[[1,0,0],\n [0,1,0],\n [0,1,0]]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# now that our predictions and target are in the same shape we can calculate the log loss\naa.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nlog_loss(aa,a)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's try to use a support vetcor classifier for the same data\nfrom sklearn.svm import SVC\nsv_model=SVC(probability=True)\nsv_model.fit(x_train,y_train)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted=sv_model.predict_proba(x_test)\npredicted.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# As we can see our SVC model performs better than the Logistic Regression model\nlog_loss(y_test,predicted)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# If you like the Notebook kindly upvote and if you are interested please check my other notebooks, if you any questions or suggestions you can write in the comments.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}