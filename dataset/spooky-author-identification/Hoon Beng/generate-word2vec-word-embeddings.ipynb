{"cells":[{"metadata":{},"cell_type":"markdown","source":"Started on 30 May 2019"},{"metadata":{},"cell_type":"markdown","source":"# Introduction"},{"metadata":{"_cell_guid":"06e8c4a5-feea-41ea-8831-13ab7089dcc5","_uuid":"8f9dbcf8df12e40c0daee0f48d91fd07cd084cae"},"cell_type":"markdown","source":"#### Here I explore using word embeddings on the \"Spooky Author Identification\" datasets.\n#### I am using Gensim's Word2Vec to generate the word representations from all the text in the training and test sets."},{"metadata":{"_cell_guid":"8f6a95ee-cc95-4c9f-a8f7-72ae58ec13d6","_uuid":"5c5f4cc8865644748e11336736bbe584adebe7b1","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\n%matplotlib inline\nimport re\nimport gensim \nfrom gensim.models import Word2Vec","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"80d61838-9025-4cba-bb0e-58175586b21b","_uuid":"4f65d03ddbfd127307d3e415003346eb898b4d6b"},"cell_type":"markdown","source":"# Load data"},{"metadata":{"_cell_guid":"094fff47-db10-447c-965e-08056f718bde","_uuid":"4e35cd5fcae1581dbd6bc51f14728e27fe63fe70","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/train.csv')\ntest_df = pd.read_csv('../input/test.csv')\nprint(train_df.shape, test_df.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Let's clean up the text; make lowercase & remove punctuations. Then split the text into words. This prepares the text for use in Gensim's Word2Vec."},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_text(text):\n    \"\"\"\n    Convert all to lowercase and remove punctuations\n    \"\"\"\n    text = text.lower()\n    text = re.sub(r'[^\\w\\s]', '', text) # remove everything that isn't word or space\n    text = re.sub(r'\\_', '', text)      # remove underscore\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# clean train_df['text']\ntrain_df['text'] = train_df['text'].map(lambda x: clean_text(x))\ntrain_df['text'] = train_df['text'].map(lambda x: x.strip().split())\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# clean test_df['text']\ntest_df['text'] = test_df['text'].map(lambda x: clean_text(x))\ntest_df['text'] = test_df['text'].map(lambda x: x.strip().split())\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Generate word embeddings with Word2Vec"},{"metadata":{},"cell_type":"markdown","source":"#### Create corpus from all the words in the train & test datasets, and use Word2Vec to generate the word embeddings."},{"metadata":{"trusted":true},"cell_type":"code","source":"data = []  \n# iterate through each row in train_df \nfor i in range(len(train_df)):\n    data.append(train_df['text'][i])\nfor j in range(len(test_df)):\n    data.append(test_df['text'][j])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(data))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create Word2Vec model using CBOW (sg=0)\n# Set min_count to 1 so as to include all words\nembedding = gensim.models.Word2Vec(data, size=50, window=10, min_count=1, sg=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(embedding)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### The size of the vocabulary is 28,727 words. I am using a smaller dimension of 50 as I thought the vocab size is small."},{"metadata":{"trusted":true},"cell_type":"code","source":"# train & generate the embeddings\nembedding.train(data,total_examples=len(data),epochs=30)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Examine the generated word embeddings"},{"metadata":{},"cell_type":"markdown","source":"#### Let's examine the generated Word2Vec model we have created."},{"metadata":{"trusted":true},"cell_type":"code","source":"words = list(embedding.wv.vocab)\nprint(len(words))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Each word in the above corpus is expressed as a 50-dimension vector as shown below."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(embedding['capered'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### We can try pick random words in the corpus and find out what are the most similar words according to this generated Word2Vec model. Let's try a few examples."},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding.most_similar('dark', topn=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding.most_similar('shocked', topn=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding.most_similar('sprang', topn=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding.most_similar('pride', topn=5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Looking at the above words listed as similar (vectors which are closer to each other), I thought the word embeddings are quite decent."},{"metadata":{},"cell_type":"markdown","source":"# Use the word embeddings"},{"metadata":{"_cell_guid":"3c3f238d-1b6a-48a0-bcee-40031e4a2536","_uuid":"d0709c2d7c606f27b7b1abeeed821aaeee00fb6e"},"cell_type":"markdown","source":"#### Let's use the generated Word2Vec embeddings for the \"Spooky Authors Identification\" problem."},{"metadata":{},"cell_type":"markdown","source":"* One-hot encode the target variable to facilitate modelling."},{"metadata":{"_cell_guid":"1a1a7fb8-9dfc-47c9-9114-f4708d109854","_uuid":"46438c851ef26de2bd7a3736c13abeb938519800","trusted":true},"cell_type":"code","source":"# convert author labels into one-hot encodings\ntrain_df['author'] = pd.Categorical(train_df['author'])\ndf_Dummies = pd.get_dummies(train_df['author'], prefix='author')\ntrain_df = pd.concat([train_df, df_Dummies], axis=1)\n# Check the conversion\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Baseline model\n#### One simple way to use word embeddings would be to average the word embeddings of words in the text, and then feed into a softmax layer (3 classes) for training. To predict the class, I would pass the average word embeddings of the test text and determine the class with the highest probability."},{"metadata":{},"cell_type":"markdown","source":"* Create X and Y from train_df, test_df, limiting to first 100 words."},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train_df['text'].str[:100]\nY = train_df[['author_EAP', 'author_HPL', 'author_MWS']].values\nprint(X.shape, X[0], Y.shape, Y[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = test_df['text'].str[:50]\nprint(X_test.shape, X_test[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def text_to_avg(text):\n    \"\"\"\n    Given a list of words, extract the respective word embeddings\n    and average the values into a single vector encoding the text meaning.\n    \"\"\"\n    # initialize the average word vector\n    avg = np.zeros((50,))\n    # average the word vector by looping over the words in text\n    for w in text:\n        avg += embedding[w]\n    avg = avg/len(text)\n    return avg","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_avg = np.zeros((X.shape[0], 50)) # initialize X_avg\nfor i in range(X.shape[0]):\n    X_avg[i] = text_to_avg(X[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_avg.shape)\nprint(X_avg[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_avg = np.zeros((X_test.shape[0], 50)) # initialize X_test_avg\nfor i in range(X_test.shape[0]):\n    X_test_avg[i] = text_to_avg(X_test[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_test_avg.shape)\nprint(X_test_avg[0])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4a6e6a69-91b7-43a9-8e91-3c8fd20d2790","_uuid":"903c319c5b83198edf0af59d49818bd9071ec8dc"},"cell_type":"markdown","source":"### Split train data into a train and a dev set"},{"metadata":{"_cell_guid":"438ceb39-2687-41b5-b7cb-8784a1aab35d","_uuid":"32e74067044fb018d0a71ed97c3326d578b1c0a6","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_dev, Y_train, Y_dev = train_test_split(X_avg, Y, test_size=0.2, random_state=123)\nprint(X_train.shape, Y_train.shape, X_dev.shape, Y_dev.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Build the dense neural network model from keras"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import models\nfrom keras import layers\n\nmodel = models.Sequential()\nmodel.add(layers.Dense(512, activation='relu', input_shape=(50,)))\nmodel.add(layers.Dense(3, activation='softmax'))\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# compile the model\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train the model"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# train and validate the model\nepochs = 50\nhistory = model.fit(X_train, Y_train, epochs=epochs, batch_size=128, validation_data=(X_dev, Y_dev))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot and visualise the training and validation losses\nloss = history.history['loss']\ndev_loss = history.history['val_loss']\nepochs = range(1, len(loss) + 1)\nplt.plot(epochs, loss, 'bo', label='training loss')\nplt.plot(epochs, dev_loss, 'b', label='validation loss')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b2d898ae-79bc-48b8-8544-fb077f876c67","_uuid":"cc4c2aeef221f5a97e1bd5ea1154052172175351"},"cell_type":"markdown","source":"## Re-train with the entire training set"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = models.Sequential()\nmodel.add(layers.Dense(512, activation='relu', input_shape=(50,)))\nmodel.add(layers.Dense(3, activation='softmax'))\n# compile the model\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# train the model\nepochs = 10\nmodel.fit(X_avg, Y, epochs=epochs, batch_size=128)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Predict on the test set and compute the probabilities for submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"# predict on test set\npreds = model.predict(X_test_avg)\nprint(preds.shape)\nprint(preds[7])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# set the predicted labels to be the one with the highest probability\npred_labels = []\nfor i in range(len(X_test_avg)):\n    pred_label = np.argmax(preds[i])\n    pred_labels.append(pred_label)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(pred_labels[7])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Create submission file"},{"metadata":{"_cell_guid":"ef99474c-ee91-40af-a4e0-7f86445e6841","_uuid":"ee067d4b61d323f1589970031ca121364e52a9f6","trusted":true},"cell_type":"code","source":"result = pd.DataFrame(preds, columns=['EAP','HPL','MWS'])\nresult.insert(0, 'id', test_df['id'])\nresult.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"bef9209e-c843-4ca6-be4f-35820ffa258d","_uuid":"86d7d2fa2b5671183438fe0769cfb7594eb4efa5","trusted":true},"cell_type":"code","source":"# Generate submission file in csv format\nresult.to_csv('submission.csv', index=False, float_format='%.20f')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7a012382-ad72-4c76-b53c-1f756544e23e","_uuid":"cbbce014f73f54e9bf3e88e096ad66b89b88fcf4"},"cell_type":"markdown","source":"#### Thank you for reading this.\n#### Please upvote if you find it useful. Cheers!"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}