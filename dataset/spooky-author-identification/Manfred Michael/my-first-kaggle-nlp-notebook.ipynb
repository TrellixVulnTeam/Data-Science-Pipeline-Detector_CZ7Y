{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Introduction","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Hello! I'm Manfred Michael, a beginner Machine Learning enthusiast and this is my first Kaggle NLP notebook. How did I end up here? I joined a challange called #66DaysData initiated by [Ken jee](https://www.youtube.com/channel/UCiT9RITQ9PW6BhXK0y2jaeg), a Data Scientist Youtuber. From there, i found a minigroup where we discussed about how to tackle this notebook.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"This notebook will contain scikit-learn pipeline, nltk modules, and custom transformers. This is the highest score i could achieve so far.","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"def warn(*args, **kwargs):\n    pass\nimport warnings\nwarnings.warn = warn #remove spooky warning when working on this spooky notebook","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prepare data","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nnp.set_printoptions(suppress=True)\n\nauthor_data = pd.read_csv('../input/spooky-author-identification/train.zip')\nauthor_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 3 authors: \n* EAP: Edgar Allan Poe\n* HPL: HP Lovecraft\n* MWS: Mary Wollstonecraft Shelley\n\nEach data has author tag and a chunk of text from on of the author's books","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(author_data['author'].value_counts())\nauthor_data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Seems like the dataset is free of missing value. Now, let's see each text length.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"author = author_data.copy()\nauthor['text_length'] = author['text'].apply(lambda text: len(text))\nauthor.head()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print(author['text_length'].describe())\nax = author['text_length'].hist(bins=100);\nplt.axis([0, 1000, 0, 5000])\nplt.xlabel('text length', fontsize=14)\nplt.title('Text length distribution', fontsize=18)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"author.groupby('author').mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Kinda disappointing, there is no obvious difference between authors text length. Which probably makes sense, the provider of this dataset might have taken similar length of texts chunk from authors book. Having to much variance in text length will give us trouble.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Split data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"For now, let's split our data. Here's our plan:\n1. We split the data to create test set\n2. Play with the train set\n3. Don't touch the test set\n4. Build our model and measure its performance with only train set\n5. Still don't touch the test set\n6. Only when we are confident with our model, we could try it with the test set\n\n\nBut why not using the test set to develop our model? Because we don't want to make a model which performs well on the test set, but we want it to perform well on any general dataset.\n\n**NOTE:** This test set is the one we created, not the test.zip","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X = author_data.drop(['author'], axis=1)\ny = author_data.copy()['author']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y ,test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(X_train), len(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"And here's the plan for feature Engineering:\n* Tokenize text\n* Stem text\n* Remove stopwords\n* Use word count to vectorize text\n* Use additional features","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Importing text feature exctraction modules","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Tokenizer","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nnltk.word_tokenize('We don\\'t do that here')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Stemmer","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"stemmer = nltk.PorterStemmer()\n\nfor word in ('computing','computed','compulsive'):\n    print(word,'=>',stemmer.stem(word))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Stopwords","execution_count":null},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction import stop_words\nstop_words.ENGLISH_STOP_WORDS","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_list = 'i myself believe it was beautiful'.split()\nless_than_zero = list(filter(lambda x: not x in stop_words.ENGLISH_STOP_WORDS, text_list))\nprint(less_than_zero)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Word Counter","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nfrom collections import Counter\n\nsentence = 'I love real madrid for real but I dont like real betis'\nsentence = re.sub(r'\\W+', ' ', sentence, flags=re.M) #remove punctuation from string\n\nc = Counter(sentence.split())\nprint(c.most_common())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"c['love'] += 3  \nprint(c.most_common())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Extra Feature","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Credit: https://www.kaggle.com/sudalairajkumar/simple-feature-engg-notebook-spooky-author (sudalairajkumar's spooky notebook)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import string\n\ndef derive_new_features(data):\n    data = data[['text']].copy()\n\n    #Unique words Count\n    data['unique_words_count'] = data.text.apply(lambda x: len(set(str(x).split())))     \n\n    #Punctuation count\n    data['punctuation_count'] = data.text.apply(lambda x: len([x for x in x.lower().split() if x in string.punctuation]))\n\n    #Upper case words count\n    data['uppercase_words_count'] = data.text.apply(lambda x: sum([x.isupper() for x in x.split()]))\n\n    #Title words count\n    data['title_words_count'] = data.text.apply(lambda x: sum([x.istitle() for x in x.split()]))\n\n    return data.drop(['text'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"derive_new_features(X_train).head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Create Pipelines","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now, we put all of our feature extractor in pipelines. The goal is to automate our data preparation, so we can tune its parameters later. We are using custom transformers to use feature extractor modules we have imported.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Credit: https://github.com/ageron/handson-ml2/blob/master/03_classification.ipynb (Aurelien Geron's Hands-On ML Chapter 3 Exercise)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\n\nclass TextToWordCounterTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self, remove_punctuation=True, lower_case=True, stemming=True, replace_numbers=True, remove_stopwords=True):\n        self.remove_punctuation = remove_punctuation\n        self.lower_case = lower_case\n        self.stemming = stemming\n        self.replace_numbers = replace_numbers\n        self.remove_stopwords = remove_stopwords\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X, y=None):\n        word_counters = []\n        for text in X['text']:\n            if self.lower_case:\n                text = text.lower()\n            if self.replace_numbers:\n                text = re.sub(r'\\d+(?:\\.\\d*(?:[eE]\\d+))?', 'NUMBER', text) #replace any numerical character with 'NUMBER'\n            if self.remove_punctuation:\n                text = re.sub(r'\\W+', ' ', text, flags=re.M)\n            if self.remove_stopwords:\n                words = [word for word in text.split() if not word in stop_words.ENGLISH_STOP_WORDS]\n                text = ' '.join(words)\n            word_list = nltk.word_tokenize(text)\n            word_count = Counter(word_list)\n            if self.stemming:\n                stemmed_word_count = Counter()\n                for word in word_list:\n                    stemmed_word = stemmer.stem(word)\n                    stemmed_word_count[stemmed_word] += 1\n                word_count = stemmed_word_count\n            word_counters.append(word_count)\n        return np.array(word_counters)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using TextToWordCounterTransformer, we transformed the text dataframe into a Counter objects. These Counter objects count how many times each words(on a single text) occured on the text.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_word_count = TextToWordCounterTransformer().fit_transform(X_train)\nX_word_count","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.sparse import csr_matrix\n\nclass WordCounterToVectorTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self, vocabulary_size=1000):\n        self.vocabulary_size = vocabulary_size\n    def fit(self, X, y=None):\n        total_count = Counter()\n        for word_count in X:\n            for word, count in word_count.items():\n                total_count[word] += min(count, 10) \n        most_common = total_count.most_common()[:self.vocabulary_size]\n        self.most_common_ = most_common\n        self.vocabulary_ = {word: index + 1 for index, (word, count) in enumerate(most_common)}  #spare an index for excluded words\n        return self\n    def transform(self, X, y=None):\n        data = []\n        rows = []\n        cols = []\n        for row, word_count in enumerate(X):\n            for word, count in word_count.items():\n                data.append(count)\n                rows.append(row)\n                cols.append(self.vocabulary_.get(word, 0))\n        return csr_matrix((data, (rows, cols)), shape=(len(X), self.vocabulary_size + 1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, WordCounterToVectorTransformer will transform the Counter objects from previous transformer into sparse metric. The words used as vector is limited too (this time only 1000 most common words).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer = WordCounterToVectorTransformer(vocabulary_size=1000)\nvectorizer.fit_transform(X_word_count).toarray()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true},"cell_type":"markdown","source":"These are 10 most common words:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer.most_common_[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, putting these 2 transformers in a single pipeline","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.pipeline import Pipeline\n\npreprocess_pipeline = Pipeline([\n    ('text_to_word_count', TextToWordCounterTransformer()),\n    ('word_count_to_vector', WordCounterToVectorTransformer(vocabulary_size=14000)), \n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Remember that we have derive_new_features() function. In order to put it in a pipeline, we need to turn it into custom transformer.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class NewFeaturesAdderTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        pass\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X, y=None):\n        return np.array(derive_new_features(X))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NewFeaturesAdderTransformer().fit_transform(X_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, combining the last transformer with previous pipeline gives us the full pipeline","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.compose import ColumnTransformer\n\nfull_pipeline = ColumnTransformer([\n    ('feature_adder', NewFeaturesAdderTransformer(), ['text']),\n    ('text_pipeline', preprocess_pipeline, ['text']),\n])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train Model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The y_train is in object type (string). We need to transform it into 3 categories. We will use Scikit-learn LabelEncoder transformer","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nlabel_encoder = LabelEncoder()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_transformed = full_pipeline.fit_transform(X_train)\ny_train_transformed = label_encoder.fit_transform(y_train.values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Be aware that the submission format requires the categories to be in this order: EAP,HPL,MWS","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"label_encoder.classes_    #submission format: id, EAP,HPL,MWS","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, let's train several algorithms.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import SGDClassifier\nsgd_clf = SGDClassifier(random_state=42, loss='hinge')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n\ncv_score = cross_val_score(\n        sgd_clf, \n        X_train_transformed, y_train_transformed, \n        cv=5,\n        verbose=3,\n    )\nprint('mean score :', cv_score.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nlog_clf = LogisticRegression(random_state=42)\ncv_score = cross_val_score(\n        log_clf, \n        X_train_transformed, y_train_transformed, \n        cv=5,\n        verbose=3,\n    )\nprint('mean score :', cv_score.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\n\nmnb_clf = MultinomialNB()\ncv_score = cross_val_score(\n        mnb_clf, \n        X_train_transformed, y_train_transformed, \n        cv=5,\n        verbose=3,\n    )\nprint('mean score :', cv_score.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Evaluate Model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"This competition use logloss as scoring. I took this code from abishek's notebook.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"credit: https://www.kaggle.com/abhishek/approaching-almost-any-nlp-problem-on-kaggle (Abhishek Thakur's spooky notebook)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def multiclass_logloss(actual, predicted, eps=1e-15):\n    \"\"\"Multi class version of Logarithmic Loss metric.\n    :param actual: Array containing the actual target classes\n    :param predicted: Matrix with class predictions, one probability per class\n    \"\"\"\n    # Convert 'actual' to a binary array if it's not already:\n    if len(actual.shape) == 1:\n        actual2 = np.zeros((actual.shape[0], predicted.shape[1]))\n        for i, val in enumerate(actual):\n            actual2[i, val] = 1\n        actual = actual2\n\n    clip = np.clip(predicted, eps, 1 - eps)\n    rows = actual.shape[0]\n    vsota = np.sum(actual * np.log(clip))\n    return -1.0 / rows * vsota","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_predict\n\ny_scores = cross_val_predict(\n        log_clf, \n        X_train_transformed, y_train_transformed, \n        cv=5,\n        verbose=3,\n        method='predict_proba',\n    )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(columns=label_encoder.classes_, data=y_scores).head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"multiclass_logloss(y_train_transformed, y_scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_scores = cross_val_predict(\n        mnb_clf, \n        X_train_transformed, y_train_transformed, \n        cv=5,\n        verbose=3,\n        method='predict_proba',\n    )\n\nmulticlass_logloss(y_train_transformed, y_scores)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Try different preprocessing parameters","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We have automated our data preparation. Now, let's try several parameters for our pipeline in hoping to find better score","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import itertools\n\nstopword_params = [True, False]\nvocabulary_params = [7000, 8000, 9000, 10000]\ndata_scores = []\n\nfor stopword, vocabulary in list(itertools.product(stopword_params, vocabulary_params)):\n   \n    full_pipeline.set_params(\n            text_pipeline__text_to_word_count__remove_stopwords=stopword,\n            text_pipeline__word_count_to_vector__vocabulary_size=vocabulary,\n        )\n    X_train_processed = full_pipeline.fit_transform(X_train)\n    y_scores = cross_val_predict(\n            log_clf, \n            X_train_processed, y_train_transformed, \n            cv=3,\n            method='predict_proba',\n        )\n    data_scores += [(stopword, vocabulary, multiclass_logloss(y_train_transformed, y_scores))]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for stopword, vocabulary, logloss in data_scores:\n    print('remove_stopwords:', stopword, ',', 'vocabulary_size:',vocabulary)\n    print('logloss: ', logloss)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's change our final pipeline parameters with its best parameters","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"full_pipeline.set_params(\n            text_pipeline__text_to_word_count__remove_stopwords=False,\n            text_pipeline__word_count_to_vector__vocabulary_size=8_000,\n        )\nX_train_transformed = full_pipeline.fit_transform(X_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Grid Search","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The algorithms we used are not on their best hyperparameters. Tuning their parameters manually would waste our energy. Let's just use Scikit-learn GridSearchCV to do the job for us.","execution_count":null},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"%%time\nfrom sklearn.model_selection import GridSearchCV\n\nlog_grid_params = {\n    'penalty': ['L1', 'l2'],\n    'dual': [False],\n    'tol':[1e-4, 1e-5],\n    'class_weight': ['balanced', None],\n    'solver': ['newton-cg', 'lbfgs', 'sag', 'saga'],\n    'multi_class': ['ovr', 'auto', 'multinomial'],\n}\nLogisticRegression #\n\nlog_grid_search = GridSearchCV(\n    estimator=LogisticRegression(random_state=42),\n    param_grid=log_grid_params,\n    scoring='neg_log_loss',\n    cv=3,\n    verbose=2,\n)\n\nlog_grid_search.fit(X_train_transformed, y_train_transformed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(log_grid_search.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import joblib\n\njoblib.dump(log_grid_search, 'log_grid_best.pkl')\nlog_clf_best = joblib.load('log_grid_best.pkl').best_estimator_\nprint(log_clf_best.get_params())","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"%%time\nfrom sklearn.model_selection import GridSearchCV\n\nsgd_grid_params = {\n    'loss': ['log'],\n    'penalty' : ['l2'],\n    'eta0':[0.1],\n    'alpha': [1e-4, 1e-5],\n    'tol': [1e-3, 1e-4],\n    'epsilon': [0.3, 0.5, 1],\n    'learning_rate': ['adaptive'],\n    'class_weight': ['balanced', None],\n    'average':[True, False],\n}\n\nsgd_grid_search = GridSearchCV(\n    estimator=SGDClassifier(random_state=42),\n    param_grid=sgd_grid_params,\n    scoring='neg_log_loss',\n    cv=3,\n    verbose=2,\n)\n\nsgd_grid_search.fit(X_train_transformed, y_train_transformed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(sgd_grid_search.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"joblib.dump(sgd_grid_search, 'sgd_grid_best.pkl')\nsgd_clf_best = joblib.load('sgd_grid_best.pkl').best_estimator_\nprint(sgd_clf_best.get_params())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfrom sklearn.model_selection import GridSearchCV\n\nmnb_grid_params = {\n    'alpha': [0, 0.25, 0.5, 0.75, 1],\n    'fit_prior': [False, True],\n}\n\nGridSearchCV\n\nmnb_grid_search = GridSearchCV(\n    estimator=MultinomialNB(),\n    param_grid=mnb_grid_params,\n    scoring='neg_log_loss',\n    cv=3,\n    verbose=2,\n)\n\nmnb_grid_search.fit(X_train_transformed, y_train_transformed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(mnb_grid_search.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"joblib.dump(mnb_grid_search, 'mnb_grid_best.pkl')\nmnb_clf_best = joblib.load('mnb_grid_best.pkl').best_estimator_\nprint(mnb_clf_best.get_params())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Ensemble","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Ensembling some models often gives you better score. Let's try some combination of ensemble models!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier\n\nsgd_clf_best.set_params(loss='log') #need log soft voting classifier\nestimators=[('log_clf', log_clf_best), ('sgd_clf',sgd_clf_best), ('mnb_clf',mnb_clf_best)]\nvot_clf = VotingClassifier(\n    estimators=estimators,\n    voting='soft',\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_scores = cross_val_predict(\n        vot_clf, \n        X_train_transformed, y_train_transformed, \n        cv=5,\n        verbose=3,\n        method='predict_proba',\n    )\n\nmulticlass_logloss(y_train_transformed, y_scores)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Amazing score! It is almost less than 0.4","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We used log_loss on our SGD model GridSearch. Because of that, we couldn't use several hyperparameters like loss='hinge'. I later found out that changing our SGD model 'loss' hyperparameter to 'hinge' gives us better score.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sgd_clf_best.set_params(loss='hinge')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom sklearn.ensemble import StackingClassifier\n\nestimators=[('sgd_clf',sgd_clf_best), ('log_clf', log_clf_best),  ('mnb_clf',mnb_clf_best)]\nstk_clf = StackingClassifier(\n    estimators=estimators,\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_scores = cross_val_predict(\n        stk_clf, \n        X_train_transformed, y_train_transformed, \n        cv=5,\n        verbose=3,\n        method='predict_proba',\n    )\n\nmulticlass_logloss(y_train_transformed, y_scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"This is the our best score! Now that we have got the score less than 0.4, we could test it on the test set.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Testing Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"final_model = stk_clf\nfinal_model.fit(X_train_transformed, y_train_transformed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_transformed = full_pipeline.transform(X_test)\ny_test_transformed = label_encoder.transform(y_test)\n\ny_scores = final_model.predict_proba(X_test_transformed)\ny_scores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"multiclass_logloss(y_test_transformed, y_scores)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Fantastic! We have never touch the test set, but we have pretty similar score from the train set score.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Now that we are satisfied with our score, I will end this notebook here. I would appreciate any critics and suggestion. Thanks for your time going through this notebook!","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Create Submission File","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_transformed = full_pipeline.transform(X)\ny_transformed = label_encoder.transform(y)\nfinal_model.fit(X_transformed, y_transformed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data = pd.read_csv('../input/spooky-author-identification/test.zip')\ntest_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data_prepared = full_pipeline.transform(test_data)\ntest_scores = final_model.predict_proba(test_data_prepared)\ntest_scores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_file = pd.DataFrame({\n    'id': test_data['id'].values,\n    'EAP': test_scores[:,0],\n    'HPL': test_scores[:,1],\n    'MWS': test_scores[:,2],\n}) \nsubmission_file.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_file.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}