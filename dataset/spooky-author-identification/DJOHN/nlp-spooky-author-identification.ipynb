{"cells":[{"metadata":{},"cell_type":"markdown","source":"The dataset contains texts from books written by spooky authors of the public domain:\n\n1. Edgar Allan Poe (EAP)\n2. HP Lovecraft (HPL)\n3. Mary Wollstonecraft Shelley (MWS)\n\nThe goal is to identify the author based on the text","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!unzip '/kaggle/input/spooky-author-identification/train.zip'\n!unzip '/kaggle/input/spooky-author-identification/test.zip'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check if the files have been unzipped\nfor dirname, _, filenames in os.walk('/kaggle/working'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport nltk\nfrom nltk.corpus import stopwords\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nfrom sklearn import ensemble, model_selection, metrics, naive_bayes\n\nimport time\nimport gc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1. Importing the data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# read train and test\ntrain_df = pd.read_csv('/kaggle/working/train.csv')\ntest_df = pd.read_csv('/kaggle/working/test.csv')\nprint('train dataset shape', train_df.shape)\nprint('test dataset shape', test_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.1 Check for class imbalances\nTo see if there is an equal representation of text from the authors in the dataset\nThere is `no high imbalance` in the classes, which could have resulted in unwanted bias","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,6))\nsns.countplot(train_df['author'])\nplt.xlabel('Authors', fontsize=12)\nplt.ylabel('Number of text occurences', fontsize=12)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Examine some of the text from each author\ngrouped_df = train_df.groupby('author')['text']\nfor author, group in grouped_df:\n    print('Author name :',author,'\\n', 'Text\\n',group.tolist()[:5])\n    print('\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Feature engineering\nLearn some features from the existing text which can be done in two parts:\n> 1. __Meta features__: features extracted from existing texts such as number of of words, stop words, punctuations, characters etc\n> 2. __Text based features__: features extracted directly from text such as term frequency, idf, word2vec, countvectorizer etc","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 2.1 Extract 'meta features'\n\nExtract meta features to see how good they are at predicting the authors for which we will extract:\n\n>Number of:\n    1. words in text\n    2. unique words in text\n    3. punctuations\n    4. stop words\n    5. length of text\n    6. upper case words\n    7. title case words\n    8. characters in text\n    9. average length of words","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import string  # for use in string.punctuation here\neng_stopwords = set(stopwords.words('english'))\n\n# number of words in text\ntrain_df['num_words'] = train_df['text'].apply(lambda x: len(str(x).split()))\ntest_df['num_words'] = test_df['text'].apply(lambda x: len(str(x).split()))\n\n# unique words count\ntrain_df['num_uniq_words'] = train_df['text'].apply(lambda x: len(set(str(x).split())))\ntest_df['num_uniq_words'] = test_df['text'].apply(lambda x: len(set(str(x).split())))\n\n# punctuation count\ntrain_df['num_punctuations'] = train_df['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\ntest_df['num_punctuations'] = test_df['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n\n# character count\ntrain_df['num_chars'] = train_df['text'].apply(lambda x: len(str(x)))\ntest_df['num_chars'] = test_df['text'].apply(lambda x: len(str(x)))\n\n# stop words count\ntrain_df['num_stopwords'] = train_df['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\ntest_df['num_stopwords'] = test_df['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n\n# title case words count\ntrain_df['num_words_title'] = train_df['text'].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\ntest_df['num_words_title'] = test_df['text'].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n\n# upper case words\ntrain_df['num_words_upper'] = train_df['text'].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\ntest_df['num_words_upper'] = test_df['text'].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n\n# average length of words\ntrain_df['num_words_upper'] = train_df['text'].apply(lambda x: np.mean(len([w for w in str(x).split()])))\ntest_df['num_words_upper'] = test_df['text'].apply(lambda x: np.mean(len([w for w in str(x).split()])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# after adding new features\ntrain_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.3 Plot using meta features\nTo see how these new features can be helpful with predictions","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 2.3.1 Number of words","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# truncating for better visuals\ntrain_df.loc[train_df['num_words'] > 80, 'num_words']=80\nplt.figure(figsize=(12,5))\nsns.boxplot(x = 'author', y = 'num_words', data=train_df)\nplt.title('Number of words in text for each author', fontsize=12)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# using violin plot\nplt.figure(figsize=(12,5))\nsns.violinplot(x = 'author', y = 'num_words', data=train_df)\nplt.title('Number of words in text for each author', fontsize=12)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.3.2 Number of unique words","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# using violin plot\nplt.figure(figsize=(12,5))\nsns.violinplot(x = 'author', y = 'num_uniq_words', data=train_df)\nplt.title('Number of unqiue words in text for each author', fontsize=12)\nplt.ylim(0,100)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.3.3 Number of punctuations","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# using violin plot\ntrain_df.loc[train_df['num_punctuations'] > 10, 'num_punctuations'] = 10\nplt.figure(figsize=(12,6))\nsns.violinplot(x = 'author', y = 'num_punctuations', data=train_df)\nplt.title('Number of punctuations in text for each author', fontsize=12)\n#plt.ylim(0,10)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3. Prepare the data for modeling","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# encode label for author\nauthor_mapping_dict ={'EAP': 0, 'HPL': 1, 'MWS': 2}\ntrain_y = train_df['author'].map(author_mapping_dict)\n\n# store the ids\ntrain_id = train_df['id'].values\nprint('train_id shape', train_id.shape)\ntest_id = test_df['id'].values\nprint('test_id shape', test_id.shape)\n\ncols_to_drop = ['id', 'text']\ntrain_X = train_df.drop(cols_to_drop + ['author'], axis=1)\ntest_X = test_df.drop(cols_to_drop , axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4. Train a simple XGBoost model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\n\ndef runXGB(train_X, train_y, val_X, val_y = None, test_X2=None, seed_val=0, child=1, colsample=0.3):\n    \n    # parameters\n    params = {}\n    params['max_depth'] = 3\n    params['objective'] = 'multi:softprob'\n    params['eval_metric'] = 'mlogloss'\n    params['num_class'] = 3\n    params['eta'] = 0.1\n    params['silent'] = 1   # verbosity\n    params['min_child_weight'] = child\n    params['colsample_bytree'] = colsample\n    params['subsample'] = 0.8\n    params['seed'] = seed_val\n    \n    num_rounds = 2000\n    \n    # list of paramaters values to be passed to train XGB\n    params_list = list(params.items())\n    \n    # create the dense matrix for train\n    xgtrain = xgb.DMatrix(train_X, label= train_y)\n    \n    if not val_y is None:\n        xgval = xgb.DMatrix(val_X, label = val_y)\n        watchlist = [(xgtrain, 'train'), (xgval, 'val')]\n        \n        # stop training the model if no improvement for 50 iterations\n        model = xgb.train(params_list, xgtrain, num_rounds, watchlist, early_stopping_rounds=50, verbose_eval=20)\n    \n    else:\n        xgval = xgb.DMatrix(val_X)\n        model.train(params_list, xgtrain, num_rounds)\n        \n    pred_val_y = model.predict(xgval, ntree_limit = model.best_ntree_limit)\n        \n        \n    # On the actual test set\n    if test_X2 is not None:\n        xgtest2 = xgb.DMatrix(test_X2)\n        pred_test_y2 = model.predict(xgtest2, ntree_limit = model.best_ntree_limit)\n        \n    \n    return (pred_val_y, pred_test_y2, model)    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Creating a k-fold cross validation**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2020)\ncv_scores =[]\n\nfor dev_index, val_index in kf.split(train_X):\n    dev_X, val_X = train_X.loc[dev_index], train_X.loc[val_index]\n    dev_y, val_y = train_y[dev_index], train_y[val_index]\n    \n    # test_X is the actual test set\n    pred_val_y, pred_test_y, model =  runXGB(dev_X, dev_y, val_X, val_y, test_X, seed_val=0)\n    \n    # cv scores\n    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n    \n    break # running only 1 iteration for now\n\nprint('cv scores: for logloss on simple XGBoost model', cv_scores)    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"__Plot the important features identified__","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(12,10))\nxgb.plot_importance(model, max_num_features=50, height=0.7, ax=ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5. Using Text based features\nWe can use count vectorizer, tf-idf etc. Here I will extract tf-idf for each word","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n\n# instantiate\ntfidf_vec = TfidfVectorizer(stop_words='english', ngram_range=(1,3))\n# fit_transform on the combined train and test,as there can be pattern in test set not seen in train\nfull_tfidf = tfidf_vec.fit_transform(train_df['text'].values.tolist() + test_df['text'].values.tolist())\ntrain_tfidf = tfidf_vec.transform(train_df['text'].values.tolist())\ntest_tfidf = tfidf_vec.transform(test_df['text'].values.tolist())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_tfidf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full_tfidf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The output from tfidf is a `sparse matrix`. So if we need to combine this with other `dense features` we have extracted there are 2 choices:\n> 1. Choose the top `n features` (depending on system config) from the tfidf vectorizer, `convert` it into a dense format and `concat` with other features\n> 2. Build a `model` using just the `sparse features` and then use the `predictions` as one of the features along with the `dense features`\n\nThe approach to take depends on the dataset. Here I will be using the second approach\n\nAlso `Naive Bayes` tends to perform well on this dataset. So we will build a NB model using tf-idf as it if `faster` to train","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# since there are 3 classes, we will use a Multinomial Naive Bayes(NB) model\n\ndef runMNB(train_X, train_y, val_X, val_y, test_X2):\n    model = naive_bayes.MultinomialNB()\n    model.fit(train_X, train_y)\n    # use model.predict_proba to detect the probabilites for each of the classes\n    # if using model.predict, the model just return a prediction\n    pred_proba_val_y = model.predict_proba(val_X)\n    pred_proba_test_y = model.predict_proba(test_X2)\n    return pred_proba_val_y, pred_proba_test_y, model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.1 Using 'Naive Bayes' on Word 'Tfidf vectorizer'","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ncv_scores = []\npredict_val = np.zeros([train_df.shape[0], 3]) # store prediction on validation set to use in confusion matrix\n\nkf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2020)\n\n# create splits on train_X and use the tf-idf values from train_tfidf and test_tfidf\n\nfor dev_index, val_index in kf.split(train_X):\n    dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n    dev_y, val_y = train_y[dev_index], train_y[val_index]\n    pred_proba_val_y, pred_proba_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_tfidf)\n    \n    \n    cv_scores.append(metrics.log_loss(val_y, pred_proba_val_y))\n    predict_val[val_index, :] = pred_proba_val_y\n    \nprint('cv scores for mlogloss using NB on tf-idf at word level: ', cv_scores)\nprint('Mean cv scores for mlogloss using NB on tf-idf at word level: ', np.mean(cv_scores))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we are getting mlogloss of 0.84 using tf-idf vectorizer, which is better than using just the meta features","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"__Plot confusion matrix__\n- Using sklearn confusion_matrix only provides a one Vz rest tranformation so it is not useful here\n- Another option is to use a custom script as mentioned [here](https://datascience.stackexchange.com/questions/40067/confusion-matrix-three-classes-python)\n- I have used mlxtend library for cleaner code","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from mlxtend.evaluate import confusion_matrix\nfrom mlxtend.plotting import plot_confusion_matrix\n\ncm = confusion_matrix(val_y, np.argmax(pred_proba_val_y, axis=1), binary=False)\nfig, ax = plot_confusion_matrix(cm)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.2 Using SVD (Singular Value Decomposition) on Tf-idf\n- this will help to compress the information in the high dimensional sparse tf-idf vectors\n- we will compress tf-idf vectors using SVD and use them as features in the ML models\n- from the helper notes:\n      In particular, truncated SVD works on term count/tf-idf matrices as\n      returned by the vectorizers in sklearn.feature_extraction.text. In that\n      context, it is known as latent semantic analysis (LSA).\n\n      This estimator supports two algorithms: a fast randomized SVD solver, and\n      a \"naive\" algorithm that uses ARPACK as an eigensolver on (X * X.T) or\n      (X.T * X), whichever is more efficient.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import TruncatedSVD\n\nn_comp= 20\n\nsvd_obj = TruncatedSVD(n_components=n_comp, algorithm ='arpack')\nsvd_obj.fit(full_tfidf)\n\ntrain_svd = pd.DataFrame(svd_obj.transform(train_tfidf))\ntest_svd = pd.DataFrame(svd_obj.transform(test_tfidf))\n\n# renaming columns\ntrain_svd.columns = ['svd_tfidf_word_'+ str(i) for i in range(n_comp)]\ntest_svd.columns = ['svd_tfidf_word_'+ str(i) for i in range(n_comp)]\n\n# concat the information original train_df\ntrain_df = pd.concat([train_df, train_svd], axis=1)\ntest_df = pd.concat([test_df, test_svd], axis=1)\n\ndel full_tfidf, train_tfidf, test_tfidf, train_svd, test_svd\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# glimse after the merge with SVD features\ndisplay(train_df.sample(2))\ndisplay(test_df.sample(2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.3 Using Naive Bayes on Count Vectorizer","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ncount_vec = CountVectorizer(stop_words='english', analyzer='word', ngram_range=(1,3))\ncount_vec.fit(train_df['text'].values.tolist() + test_df['text'].values.tolist())\n\ntrain_count_vec = count_vec.transform(train_df['text'].values.tolist())\ntest_count_vec = count_vec.transform(test_df['text'].values.tolist())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_count_vec.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_count_vec.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Use MultinomialNB model with count vectorized features*\n- and then use the predictions as features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_scores =[]\npred_full_test = 0\npred_train = np.zeros([train_df.shape[0], 3])\n\nkf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2020)\n\nfor dev_index, val_index in kf.split(train_df):\n    dev_X, val_X = train_count_vec[dev_index], train_count_vec[val_index]\n    dev_y, val_y = train_y[dev_index], train_y[val_index]\n    \n    pred_proba_val_y, pred_proba_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_count_vec)\n    # append the scores in test from each fold\n    pred_full_test += pred_proba_test_y\n    # store the data corresponding to the validation indexes\n    pred_train[val_index,:] = pred_proba_val_y    \n    \n    cv_scores.append(metrics.log_loss(val_y, pred_proba_val_y))\n    \nprint('Mean logloss score: using NB on Count Vectorize at word level ', np.mean(cv_scores))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using count vectorizer in place of tf-idf results in better logloss score","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# columns reprsent the probability of belonging to the 3 classes\npred_train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* __Add the predictions from (NB on CountVectoriser) as new features into our data__","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# check if the number of rows is the same\nassert pred_train.shape[0] == train_df.shape[0]\n\ntrain_df['nb_cvec_eap'] = pred_train[:, 0]\ntrain_df['nb_cvec_hpl'] = pred_train[:, 1]\ntrain_df['nb_cvec_mws'] = pred_train[:, 2]\n\nassert pred_full_test.shape[0] == test_df.shape[0]\n\ntest_df['nb_cvec_eap'] = pred_full_test[:, 0]\ntest_df['nb_cvec_hpl'] = pred_full_test[:, 1]\ntest_df['nb_cvec_mws'] = pred_full_test[:, 2]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.3.1 Confusion matrix for NB on CountVectorizer","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The confusion matrix looks to be better than using with tf-idf","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# using mlxtend\ncm = confusion_matrix(val_y, np.argmax(pred_proba_val_y, axis=1), binary=False)\n#plt.figure(figsize=(10,10))\nplot_confusion_matrix(cm)\nplt.title('Confusion matrix for NB on CountVectorizer')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.4 Using Naive Bayes on 'character' Count vectorizer\nInstead of counting only the special characters, we will use count vectorizer at character level","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\ncount_vec_char = CountVectorizer(ngram_range=(1,7), analyzer='char')\ncount_vec_char.fit(train_df['text'].values.tolist() + test_df['text'].values.tolist())\n\ntrain_cvec_char = count_vec_char.transform(train_df['text'])\ntest_cvec_char = count_vec_char.transform(test_df['text'])\n\ncv_scores = []\npred_full_test = 0\npred_train = np.zeros([train_df.shape[0], 3])\n\nkf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2020)\n\nfor dev_index, val_index in kf.split(train_df):\n    dev_X, val_X = train_cvec_char[dev_index], train_cvec_char[val_index]\n    dev_y, val_y = train_y[dev_index], train_y[val_index]\n    \n    pred_proba_val_y, pred_proba_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_cvec_char)\n    # append the scores in test from each fold\n    pred_full_test += pred_proba_test_y\n    # store the data corresponding to the validation indexes\n    pred_train[val_index,:] = pred_proba_val_y    \n    \n    cv_scores.append(metrics.log_loss(val_y, pred_proba_val_y))\n\nprint('Mean logloss score using Naive Bayes on character Count vectorizer', np.mean(cv_scores))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The cv score is very high using `character level` representation. But this could add some different information that `word` level features and this will be used in the final model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* __Add the predictions from (NB on CountVectoriser at character level) as new features into our data__","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# check if the number of rows is the same\nassert pred_train.shape[0] == train_df.shape[0]\n\ntrain_df['nb_cvec_char_eap'] = pred_train[:, 0]\ntrain_df['nb_cvec_char_hpl'] = pred_train[:, 1]\ntrain_df['nb_cvec_char_mws'] = pred_train[:, 2]\n\nassert pred_full_test.shape[0] == test_df.shape[0]\n\ntest_df['nb_cvec_char_eap'] = pred_full_test[:, 0]\ntest_df['nb_cvec_char_hpl'] = pred_full_test[:, 1]\ntest_df['nb_cvec_char_mws'] = pred_full_test[:, 2]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 6. Use Naive Bayes on 'character' tf-idf vectorizer","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# tf-idf tranformation at character level\n\ntfidf_char_vec = TfidfVectorizer(ngram_range=(1,5), analyzer='char')\n# use the fit_transform method if need to apply transformation on the data at a later stage. Since we are using SVD at a later\n# stage it is better to apply the fit_transform method\nfull_tfidf = tfidf_char_vec.fit_transform(train_df['text'].values.tolist() +  test_df['text'].values.tolist())\n\ntrain_tfidf_char_vec = tfidf_char_vec.transform(train_df['text'].values.tolist())\ntest_tfidf_char_vec = tfidf_char_vec.transform(test_df['text'].values.tolist())\n\ncv_scores = []\npred_full_test = 0\npred_train = np.zeros([train_df.shape[0], 3])\n\n# use naive bayes\nkf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2020)\n\nfor dev_index, val_index in kf.split(train_df):\n    dev_X, val_X = train_tfidf_char_vec[dev_index], train_tfidf_char_vec[val_index]\n    dev_y, val_y = train_y[dev_index], train_y[val_index]\n    \n    pred_proba_val_y, pred_proba_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_tfidf_char_vec)\n    # append the scores in test from each fold\n    pred_full_test += pred_proba_test_y\n    # store the data corresponding to the validation indexes\n    pred_train[val_index,:] = pred_proba_val_y    \n    \n    cv_scores.append(metrics.log_loss(val_y, pred_proba_val_y))\n\n\nprint('Mean logloss score: using NB on tf-idf tranformation at character level', np.mean(cv_scores))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check if the number of rows is the same\nassert pred_train.shape[0] == train_df.shape[0]\n\ntrain_df['nb_tfidf_char_eap'] = pred_train[:, 0]\ntrain_df['nb_tfidf_char_hpl'] = pred_train[:, 1]\ntrain_df['nb_tfidf_char_mws'] = pred_train[:, 2]\n\nassert pred_full_test.shape[0] == test_df.shape[0]\n\nprint('test_df shape', test_df.shape)\n\ntest_df['nb_tfidf_char_eap'] = pred_full_test[:, 0]\ntest_df['nb_tfidf_char_hpl'] = pred_full_test[:, 1]\ntest_df['nb_tfidf_char_mws'] = pred_full_test[:, 2]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 6.1 Using SVD on 'character' tf-idf\nSince we saw some improvements in using 'char' implementation of Count Vectorizer, I think we should also explore using the same with SVD on tf-idf vectorizer and use Naive Bayes to make prediction using features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# extract 20 features from the high dimensional tf-idf vector space\nn_comp = 20\nsvd_obj = TruncatedSVD(n_components = n_comp, algorithm='arpack')\nsvd_obj.fit(full_tfidf)\n\ntrain_svd_char = svd_obj.transform(train_tfidf_char_vec)\ntest_svd_char  = svd_obj.transform(test_tfidf_char_vec)\n\n# create pandas dataframe, so that we can add them as features and renaming the columns for more clarity\ntrain_svd_char = pd.DataFrame(train_svd_char, columns = ['svd_tfidf_char_'+ str(i) for i in range(n_comp)])\ntest_svd_char = pd.DataFrame(test_svd_char, columns = ['svd_tfidf_char_'+ str(i) for i in range(n_comp)])\n\n# concat them with the original train and test to add these new features there\ntrain_df = pd.concat([train_df, train_svd_char], axis=1)\ntest_df  = pd.concat([test_df, test_svd_char], axis=1)\n\ndel svd_obj, train_svd_char, test_svd_char, full_tfidf, train_tfidf_char_vec, test_tfidf_char_vec\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 7. XGBoost model with all created features\nWith all the new features we can re-run a XGBoost model and evaluate results","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_to_drop = ['id', 'text']\ntrain_X = train_df.drop(cols_to_drop + ['author'], axis=1)\ntest_X = test_df.drop(cols_to_drop, axis=1)\n\ncv_scores = []\npred_full_test = 0\npred_val = np.zeros([train_df.shape[0], 3])\n\n# run K-Fold cross validation\nkf = model_selection.KFold(n_splits = 5, shuffle = True, random_state = 2020)\n\nfor dev_index, val_index in kf.split(train_X):\n    dev_X, val_X = train_X.loc[dev_index], train_X.loc[val_index]\n    dev_y, val_y = train_y[dev_index], train_y[val_index]\n        \n    pred_val_y, pred_test_y, model = runXGB(dev_X, dev_y, val_X, val_y, test_X, seed_val=0, colsample=0.7)\n    \n    pred_full_test = pred_full_test + pred_test_y\n    pred_val[val_index, :] = pred_val_y\n    \n    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n    #break # remove to run across all 5 folds\n\n# create a dataframe for the predictions\nout_df = pd.DataFrame(pred_full_test, columns = ['EAP', 'HPL', 'MWS'])\n# insert the id column at column 0\nout_df.insert(0, 'id', test_id)\nout_df.to_csv('submission.csv', index=False)\n\nprint('cv scores for log_loss using tf-idf at char level using xgboost is:', cv_scores)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The resultant score is better than what we have seen so far. This means the model has learnt quite well to distinguish between the author based on features derived\nLets look at some of the important features again\n\nMost of the important features are those derived from naive bayes","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(14,14))\nxgb.plot_importance(model, max_num_features=50, height=0.6, ax=ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 7.1 Plot confusion matrix for the final model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from mlxtend.evaluate import confusion_matrix\nfrom mlxtend.plotting import plot_confusion_matrix\n\ncm = confusion_matrix(val_y, np.argmax(pred_val_y, axis=1))\nplot_confusion_matrix(cm)\nplt.title('Confusion matrix authors: EAP: 0, HPL: 1, MWS: 2')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the confusion matrix, we there are still misclassifications for 'EAP', class 0 and 'MWS, class 2,\nNext step would be to explore `word embeddings`, or other `meta features` or even `sentiment analysis`","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}