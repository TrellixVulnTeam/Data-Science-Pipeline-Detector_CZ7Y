{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nimport xgboost as xgb\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn import ensemble, metrics, model_selection, naive_bayes\ncolor = sns.color_palette()\n\n\neng_stopwords = set(stopwords.words(\"english\"))\npd.options.mode.chained_assignment = None","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import zipfile\n\ntrain_zip = zipfile.ZipFile('/kaggle/input/spooky-author-identification/train.zip')\ntrain_df = pd.read_csv(train_zip.open('train.csv'))\ntest_zip = zipfile.ZipFile('/kaggle/input/spooky-author-identification/test.zip')\ntest_df = pd.read_csv(test_zip.open('test.csv'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Посчитаем количество рядов в тренировочном и тестовом наборе данных.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Количество рядов в тренировочных данных: \",train_df.shape[0])\nprint(\"Количество рядов в тестовых данных: \",test_df.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Посчитаем число отрывков каждого автора в датасете, чтобы узнать сбалансированы ли классы.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cnt_srs = train_df['author'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,4))\nsns.barplot(cnt_srs.index, cnt_srs.values, alpha=0.8)\nplt.ylabel('Число вхождений', fontsize=14)\nplt.xlabel('Имя автора', fontsize=14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Сильной разницы не наблюдается. Выведем несколько строк, чтобы понять, чем стиль этих авторов отличается друг от друга.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"grouped_df = train_df.groupby('author')\nfor name, group in grouped_df:\n    print(\"Имя автора: \", name)\n    cnt = 0\n    for ind, row in group.iterrows():\n        print(row[\"text\"])\n        cnt += 1\n        if cnt == 5:\n            break\n    print(\"\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Посчитаем количество слов в тексте.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[\"num_words\"] = train_df[\"text\"].apply(lambda x: len(str(x).split()))\ntest_df[\"num_words\"] = test_df[\"text\"].apply(lambda x: len(str(x).split()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Количество уникальных слов в тексте.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[\"num_unique_words\"] = train_df[\"text\"].apply(lambda x: len(set(str(x).split())))\ntest_df[\"num_unique_words\"] = test_df[\"text\"].apply(lambda x: len(set(str(x).split())))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Количество символов в тексте**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[\"num_chars\"] = train_df[\"text\"].apply(lambda x: len(str(x)))\ntest_df[\"num_chars\"] = test_df[\"text\"].apply(lambda x: len(str(x)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Количество стоп-слов в тексте**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[\"num_stopwords\"] = train_df[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\ntest_df[\"num_stopwords\"] = test_df[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Количество знаков препинания**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[\"num_punctuations\"] =train_df['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\ntest_df[\"num_punctuations\"] =test_df['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Количество слов, написанных заглавными буквами**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[\"num_words_upper\"] = train_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\ntest_df[\"num_words_upper\"] = test_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Количество слов, начинающихся с заглавной буквы**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[\"num_words_title\"] = train_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\ntest_df[\"num_words_title\"] = test_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Средняя длина слова**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[\"mean_word_len\"] = train_df[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\ntest_df[\"mean_word_len\"] = test_df[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Построим графики на основе вычисленных выше данных и сравним писателей между собой.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['num_words'].loc[train_df['num_words']>80] = 80 # усекаем данные для лучшей визуализации\nplt.figure(figsize=(12,8))\nsns.violinplot(x='author', y='num_words', data=train_df)\nplt.xlabel('Имя автора', fontsize=12)\nplt.ylabel('Количество слов в тексте', fontsize=12)\nplt.title(\"Количество слов данного автора в тексте\", fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['num_punctuations'].loc[train_df['num_punctuations']>10] = 10 # усекаем данные для лучшей визуализации\nplt.figure(figsize=(12,8))\nsns.violinplot(x='author', y='num_punctuations', data=train_df)\nplt.xlabel('Имя автора', fontsize=12)\nplt.ylabel('Количество знаков препинания в тексте', fontsize=12)\nplt.title('Количество знаков препинания данного автора в тексте', fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Подготовим данные для создания модели**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"author_mapping_dict = {'EAP':0, 'HPL':1, 'MWS':2}\ntrain_y = train_df['author'].map(author_mapping_dict)\ntrain_id = train_df['id'].values\ntest_id = test_df['id'].values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Пересчитаем усеченные переменные**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[\"num_words\"] = train_df[\"text\"].apply(lambda x: len(str(x).split()))\ntest_df[\"num_words\"] = test_df[\"text\"].apply(lambda x: len(str(x).split()))\ntrain_df[\"mean_word_len\"] = train_df[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\ntest_df[\"mean_word_len\"] = test_df[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_to_drop = ['id', 'text']\ntrain_X = train_df.drop(cols_to_drop+['author'], axis=1)\ntest_X = test_df.drop(cols_to_drop, axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Тренируем простую XGBoost модель с посчитаными выше характеристиками**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def runXGB(train_X, train_y, test_X, test_y=None, test_X2=None, seed_val=0, child=1, colsample=0.3):\n    param = {}\n    param['objective'] = 'multi:softprob'\n    param['eta'] = 0.1\n    param['max_depth'] = 3\n    param['silent'] = 1\n    param['num_class'] = 3\n    param['eval_metric'] = \"mlogloss\"\n    param['min_child_weight'] = child\n    param['subsample'] = 0.8\n    param['colsample_bytree'] = colsample\n    param['seed'] = seed_val\n    num_rounds = 2000\n\n    plst = list(param.items())\n    xgtrain = xgb.DMatrix(train_X, label=train_y)\n\n    if test_y is not None:\n        xgtest = xgb.DMatrix(test_X, label=test_y)\n        watchlist = [ (xgtrain,'train'), (xgtest, 'test') ]\n        model = xgb.train(plst, xgtrain, num_rounds, watchlist, early_stopping_rounds=50, verbose_eval=20)\n    else:\n        xgtest = xgb.DMatrix(test_X)\n        model = xgb.train(plst, xgtrain, num_rounds)\n\n    pred_test_y = model.predict(xgtest, ntree_limit = model.best_ntree_limit)\n    if test_X2 is not None:\n        xgtest2 = xgb.DMatrix(test_X2)\n        pred_test_y2 = model.predict(xgtest2, ntree_limit = model.best_ntree_limit)\n    return pred_test_y, pred_test_y2, model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\ncv_scores = []\npred_full_test = 0\npred_train = np.zeros([train_df.shape[0], 3])\nfor dev_index, val_index in kf.split(train_X):\n    dev_X, val_X = train_X.loc[dev_index], train_X.loc[val_index]\n    dev_y, val_y = train_y[dev_index], train_y[val_index]\n    pred_val_y, pred_test_y, model = runXGB(dev_X, dev_y, val_X, val_y, test_X, seed_val=0)\n    pred_full_test = pred_full_test + pred_test_y\n    pred_train[val_index,:] = pred_val_y\n    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n    break\nprint(\"cv scores : \", cv_scores)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Сравним важность характеристик с помощью графика**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(12,12))\nxgb.plot_importance(model, max_num_features=50, height=0.8, ax=ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Преобразуем (fit transform, не знаю, правильно ли перевела) TF-IDF векторизатор**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf_vec = TfidfVectorizer(stop_words='english', ngram_range=(1,3))\nfull_tfidf = tfidf_vec.fit_transform(train_df['text'].values.tolist() + test_df['text'].values.tolist())\ntrain_tfidf = tfidf_vec.transform(train_df['text'].values.tolist())\ntest_tfidf = tfidf_vec.transform(test_df['text'].values.tolist())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Построим модель наивного байесовского классификатора**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def runMNB(train_X, train_y, test_X, test_y, test_X2):\n    model = naive_bayes.MultinomialNB()\n    model.fit(train_X, train_y)\n    pred_test_y = model.predict_proba(test_X)\n    pred_test_y2 = model.predict_proba(test_X2)\n    return pred_test_y, pred_test_y2, model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_scores = []\npred_full_test = 0\npred_train = np.zeros([train_df.shape[0], 3])\nkf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\nfor dev_index, val_index in kf.split(train_X):\n    dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n    dev_y, val_y = train_y[dev_index], train_y[val_index]\n    pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_tfidf)\n    pred_full_test = pred_full_test + pred_test_y\n    pred_train[val_index,:] = pred_val_y\n    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\nprint(\"Mean cv score: \", np.mean(cv_scores))\npred_full_test = pred_full_test / 5.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Построим матрицу ошибки**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import itertools\nfrom sklearn.metrics import confusion_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Нормализованная матрица ошибки\")\n    else:\n        print('Матрица ошибки без нормализации')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('Настоящий label')\n    plt.xlabel('Предсказанный label')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cnf_matrix = confusion_matrix(val_y, np.argmax(pred_val_y,axis=1))\nnp.set_printoptions(precision=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,8))\nplot_confusion_matrix(cnf_matrix, classes=['EAP', 'HPL', 'MWS'],\n                      title='Матрица ошибки без нормализации')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Попробуем улучшить с помощью SVD (Singular Value Decomposition)**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"n_comp = 20\nsvd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')\nsvd_obj.fit(full_tfidf)\ntrain_svd = pd.DataFrame(svd_obj.transform(train_tfidf))\ntest_svd = pd.DataFrame(svd_obj.transform(test_tfidf))\n    \ntrain_svd.columns = ['svd_word_'+str(i) for i in range(n_comp)]\ntest_svd.columns = ['svd_word_'+str(i) for i in range(n_comp)]\ntrain_df = pd.concat([train_df, train_svd], axis=1)\ntest_df = pd.concat([test_df, test_svd], axis=1)\ndel full_tfidf, train_tfidf, test_tfidf, train_svd, test_svd","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Преобразуем CountVectorizer**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf_vec = CountVectorizer(stop_words='english', ngram_range=(1,3))\ntfidf_vec.fit(train_df['text'].values.tolist() + test_df['text'].values.tolist())\ntrain_tfidf = tfidf_vec.transform(train_df['text'].values.tolist())\ntest_tfidf = tfidf_vec.transform(test_df['text'].values.tolist())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Построим полиноминальную модель наивного байеса, используя характеристики CountVectorizer*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_scores = []\npred_full_test = 0\npred_train = np.zeros([train_df.shape[0], 3])\nkf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\nfor dev_index, val_index in kf.split(train_X):\n    dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n    dev_y, val_y = train_y[dev_index], train_y[val_index]\n    pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_tfidf)\n    pred_full_test = pred_full_test + pred_test_y\n    pred_train[val_index,:] = pred_val_y\n    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\nprint(\"Mean cv score: \", np.mean(cv_scores))\npred_full_test = pred_full_test / 5.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Добавим предсказания как новые характеристики**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[\"nb_cvec_eap\"] = pred_train[:,0]\ntrain_df[\"nb_cvec_hpl\"] = pred_train[:,1]\ntrain_df[\"nb_cvec_mws\"] = pred_train[:,2]\ntest_df[\"nb_cvec_eap\"] = pred_full_test[:,0]\ntest_df[\"nb_cvec_hpl\"] = pred_full_test[:,1]\ntest_df[\"nb_cvec_mws\"] = pred_full_test[:,2]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Построим матрицу ошибки**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cnf_matrix = confusion_matrix(val_y, np.argmax(pred_val_y,axis=1))\nnp.set_printoptions(precision=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,8))\nplot_confusion_matrix(cnf_matrix, classes=['EAP', 'HPL', 'MWS'],\n                      title='Матрица ошибки для подсчета слов (наивный байес), без нормализации')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Используем CountVectorizer, чтобы посчитать случайные символы, с использованием полиноминального наивного байеса**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Преобразуем TF-IDF векторизатор**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf_vec = CountVectorizer(ngram_range=(1,7), analyzer='char')\ntfidf_vec.fit(train_df['text'].values.tolist() + test_df['text'].values.tolist())\ntrain_tfidf = tfidf_vec.transform(train_df['text'].values.tolist())\ntest_tfidf = tfidf_vec.transform(test_df['text'].values.tolist())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_scores = []\npred_full_test = 0\npred_train = np.zeros([train_df.shape[0], 3])\nkf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\nfor dev_index, val_index in kf.split(train_X):\n    dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n    dev_y, val_y = train_y[dev_index], train_y[val_index]\n    pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_tfidf)\n    pred_full_test = pred_full_test + pred_test_y\n    pred_train[val_index,:] = pred_val_y\n    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\nprint(\"Mean cv score: \", np.mean(cv_scores))\npred_full_test = pred_full_test / 5.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Добавим предсказания как новые характеристики**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[\"nb_cvec_char_eap\"] = pred_train[:,0]\ntrain_df[\"nb_cvec_char_hpl\"] = pred_train[:,1]\ntrain_df[\"nb_cvec_char_mws\"] = pred_train[:,2]\ntest_df[\"nb_cvec_char_eap\"] = pred_full_test[:,0]\ntest_df[\"nb_cvec_char_hpl\"] = pred_full_test[:,1]\ntest_df[\"nb_cvec_char_mws\"] = pred_full_test[:,2]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Получим так же предсказания NB для TF-IDF векторизатора для символов**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Преобразуем TF-IDF векторизатор**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf_vec = TfidfVectorizer(ngram_range=(1,5), analyzer='char')\nfull_tfidf = tfidf_vec.fit_transform(train_df['text'].values.tolist() + test_df['text'].values.tolist())\ntrain_tfidf = tfidf_vec.transform(train_df['text'].values.tolist())\ntest_tfidf = tfidf_vec.transform(test_df['text'].values.tolist())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_scores = []\npred_full_test = 0\npred_train = np.zeros([train_df.shape[0], 3])\nkf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\nfor dev_index, val_index in kf.split(train_X):\n    dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n    dev_y, val_y = train_y[dev_index], train_y[val_index]\n    pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_tfidf)\n    pred_full_test = pred_full_test + pred_test_y\n    pred_train[val_index,:] = pred_val_y\n    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\nprint(\"Mean cv score: \", np.mean(cv_scores))\npred_full_test = pred_full_test / 5.\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Добавим предсказания как новые характеристики**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[\"nb_tfidf_char_eap\"] = pred_train[:,0]\ntrain_df[\"nb_tfidf_char_hpl\"] = pred_train[:,1]\ntrain_df[\"nb_tfidf_char_mws\"] = pred_train[:,2]\ntest_df[\"nb_tfidf_char_eap\"] = pred_full_test[:,0]\ntest_df[\"nb_tfidf_char_hpl\"] = pred_full_test[:,1]\ntest_df[\"nb_tfidf_char_mws\"] = pred_full_test[:,2]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Так же создадим SVD характеристики для характеристик TF-IDF для символов и используем их в моделировании **","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"n_comp = 20\nsvd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')\nsvd_obj.fit(full_tfidf)\ntrain_svd = pd.DataFrame(svd_obj.transform(train_tfidf))\ntest_svd = pd.DataFrame(svd_obj.transform(test_tfidf))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_svd.columns = ['svd_char_'+str(i) for i in range(n_comp)]\ntest_svd.columns = ['svd_char_'+str(i) for i in range(n_comp)]\ntrain_df = pd.concat([train_df, train_svd], axis=1)\ntest_df = pd.concat([test_df, test_svd], axis=1)\ndel full_tfidf, train_tfidf, test_tfidf, train_svd, test_svd","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Перезапустим XGBoost модель с новыми переменными и улучшим результаты**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_to_drop = ['id', 'text']\ntrain_X = train_df.drop(cols_to_drop+['author'], axis=1)\ntest_X = test_df.drop(cols_to_drop, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\ncv_scores = []\npred_full_test = 0\npred_train = np.zeros([train_df.shape[0], 3])\nfor dev_index, val_index in kf.split(train_X):\n    dev_X, val_X = train_X.loc[dev_index], train_X.loc[val_index]\n    dev_y, val_y = train_y[dev_index], train_y[val_index]\n    pred_val_y, pred_test_y, model = runXGB(dev_X, dev_y, val_X, val_y, test_X, seed_val=0, colsample=0.7)\n    pred_full_test = pred_full_test + pred_test_y\n    pred_train[val_index,:] = pred_val_y\n    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n    break\nprint(\"cv scores: \", cv_scores)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Выведем результаты в CSV файл для конкурса**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"out_df = pd.DataFrame(pred_full_test)\nout_df.columns = ['EAP', 'HPL', 'MWS']\nout_df.insert(0, 'id', test_id)\nout_df.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Ещё раз проверим важность характеристик**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(12,12))\nxgb.plot_importance(model, max_num_features=50, height=0.8, ax=ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**И построим матрицу ошибки**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cnf_matrix = confusion_matrix(val_y, np.argmax(pred_val_y,axis=1))\nnp.set_printoptions(precision=2)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,8))\nplot_confusion_matrix(cnf_matrix, classes=['EAP', 'HPL', 'MWS'],\n                      title='Матрица ошибки для XGB, без нормализации')\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}