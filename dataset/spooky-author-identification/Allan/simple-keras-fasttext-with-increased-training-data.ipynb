{"metadata":{"language_info":{"file_extension":".py","version":"3.6.3","pygments_lexer":"ipython3","name":"python","mimetype":"text/x-python","nbconvert_exporter":"python","codemirror_mode":{"name":"ipython","version":3}},"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"}},"nbformat_minor":1,"cells":[{"metadata":{"_uuid":"872d52a2365d7d717bbf3e8b5cab0f3691221ce6","_cell_guid":"e6746c13-b3ce-4b20-beb1-5c0f8c047902"},"cell_type":"markdown","source":"> This fork retains the text data in lines longer than 256 characters by creating new training data entries with them"},{"metadata":{"_uuid":"7ccc3b4516a4fcde346a162ae4f9461016bbfbbf","_cell_guid":"d5736ce6-a0b0-4cf0-beaf-a73837398da9"},"cell_type":"markdown","source":"# **1. Few Preprocessings**\n# **2. Model: FastText by Keras**"},{"metadata":{"_uuid":"b05ef71268db76a4e2565177bf6a5668a5fc428e","_kg_hide-input":false,"_cell_guid":"93e00783-a024-4e87-a5e1-6709cb8cc981","_kg_hide-output":true},"execution_count":null,"cell_type":"code","source":"import numpy as np\n\nimport pandas as pd\n\nfrom collections import defaultdict\n\nimport keras\nfrom keras.layers import Dense, GlobalAveragePooling1D, Embedding\nimport keras.backend as K\nfrom keras.callbacks import EarlyStopping\nfrom keras.models import Sequential\n\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.utils import to_categorical\nfrom sklearn.model_selection import train_test_split\n\n\nnp.random.seed(7)","outputs":[]},{"metadata":{"_uuid":"d700f739101e37903112e1de293323dcfbb577be","_cell_guid":"a5cc2c3e-7960-482e-b548-c447b89925ec","collapsed":true},"execution_count":null,"cell_type":"code","source":"df = pd.read_csv('./../input/train.csv')\n# The below lines are moved to after the processing, since it will be creating new entries in the training data\n# a2c = {'EAP': 0, 'HPL' : 1, 'MWS' : 2}\n# y = np.array([a2c[a] for a in df.author])\n# y = to_categorical(y)","outputs":[]},{"metadata":{"_uuid":"a01bab31ed7b8a55820612063576963488d99eb6","_cell_guid":"a45cb3ba-d1bc-48e0-956c-27d0f49a9943"},"cell_type":"markdown","source":"# 1. **Few Preprocessings**\n\nIn traditional NLP tasks, preprocessings play an important role, but...\n\n## **Low-frequency words**\nIn my experience, fastText is very fast, but I need to delete rare words to avoid overfitting.\n\n**NOTE**:\nSome keywords are rare words, such like *Cthulhu* in *Cthulhu Mythos* of *Howard Phillips Lovecraft*.\nBut these are useful for this task.\n\n## **Removing Stopwords**\n\nNothing.\nTo identify author from a sentence, some stopwords play an important role because one has specific usages of them.\n\n## **Stemming and Lowercase**\n\nNothing.\nThis reason is the same for stopwords removing.\nAnd I guess some stemming rules provided by libraries is bad for this task because all author is the older author.\n\n## **Cutting long sentence**\n\n~~Too long documents are cut.~~ Longer documents are used to create multiple entries for the same author from the single long line.\n\n## **Punctuation**\n\nBecause I guess each author has unique punctuations's usage in the novel, I separate them from words.\n\ne.g. `Don't worry` -> `Don ' t worry`\n\n## **Is it slow?**\n\nDon't worry! FastText is a very fast algorithm if it runs on CPU. "},{"metadata":{"_uuid":"0023cd1542d866d931deb8472f8a0d6fb0262d9a","_cell_guid":"8182b25a-f490-4b41-9865-ee1c04afecee"},"cell_type":"markdown","source":"# **Let's check character distribution per author**"},{"metadata":{"_uuid":"246a428ca3a063294c15c8c08d234ecf01e4ddbb","_cell_guid":"c1d00b0d-90e0-4f19-842c-51a82de42a10","collapsed":true,"_kg_hide-output":true},"execution_count":null,"cell_type":"code","source":"counter = {name : defaultdict(int) for name in set(df.author)}\nfor (text, author) in zip(df.text, df.author):\n    text = text.replace(' ', '')\n    for c in text:\n        counter[author][c] += 1\n\nchars = set()\nfor v in counter.values():\n    chars |= v.keys()\n    \nnames = [author for author in counter.keys()]\n\nprint('c ', end='')\nfor n in names:\n    print(n, end='   ')\nprint()\nfor c in chars:    \n    print(c, end=' ')\n    for n in names:\n        print(counter[n][c], end=' ')\n    print()\n","outputs":[]},{"metadata":{"_uuid":"8e72d6f22587780364ed24cae13ece4a403479dd","_cell_guid":"7a3fdf4e-039d-4c93-bc21-9bad7dfc6ff8"},"cell_type":"markdown","source":"# **Summary of character distribution**\n\n- HPL and EAP used non ascii characters like a `Ã¤`.\n- The number of punctuations seems to be good feature\n"},{"metadata":{"_uuid":"fee49fd9139b78ae03603d7d37eafa38f3cb29dc","_cell_guid":"ce97fc0a-b85c-4f34-92c5-ae66a0730ace"},"cell_type":"markdown","source":"# **Preprocessing**\n\nMy preproceeings are \n\n- Separate punctuation from words\n- Remove lower frequency words ( <= 2)\n- Cut a longer document which contains `256` words"},{"metadata":{"_uuid":"999012010cd8b9b20d3c5b16c11a2374a5ce44c0","_cell_guid":"72ff2ff5-0945-4f39-8b02-39e4d5df16c5","collapsed":true},"execution_count":null,"cell_type":"code","source":"def preprocess(text):\n#   Added code to strip leading spaces\n    text = text.strip()\n    text = text.replace(\"' \", \" ' \")\n    signs = set(',.:;\"?!')\n    prods = set(text) & signs\n    if not prods:\n        return text\n\n    for sign in prods:\n        text = text.replace(sign, ' {} '.format(sign) )\n    return text","outputs":[]},{"metadata":{"_uuid":"755a3735358494113eb40789419407bb6a82efce","_cell_guid":"7495acce-6127-497e-87cd-46a76e408b67","collapsed":true},"execution_count":null,"cell_type":"code","source":"# Pre-process the text outside of the create_docs function\ndf['text'] = df['text'].apply(preprocess)","outputs":[]},{"metadata":{"_uuid":"ae3b130a1a33cdb1bd5f4a8c2cd1886d2568b2ee","_cell_guid":"f06dbf45-3b18-4ef5-8481-0f8cd8cb01b5","collapsed":true},"execution_count":null,"cell_type":"code","source":"# Split lines on 256 characters, retaining integrity of word boundaries\nx = df.assign(**{'text':df['text'].str.wrap(256).str.split('\\n')})\ndf = pd.DataFrame({\n    col:np.repeat(x[col].values, x['text'].str.len())\n    for col in x.columns.difference(['text'])\n    }).assign(**{'text':np.concatenate(x['text'].values)})[x.columns.tolist()]","outputs":[]},{"metadata":{"_uuid":"d994151fa7c5ca5eb863d07025c26871ae507229","_cell_guid":"3c9310f8-aa7b-4aae-8a8d-67c31a172a7d"},"execution_count":null,"cell_type":"code","source":"df.shape","outputs":[]},{"metadata":{"_uuid":"791b9c55790956f07d30466df621a51dc17d402a","_cell_guid":"5a548ef9-65f3-42cd-9456-a6c75907942c"},"execution_count":null,"cell_type":"code","source":"df = df[df['text'].str.split().apply(len) >= 5]\ndf.shape","outputs":[]},{"metadata":{"_uuid":"e762c9afedd23d916426bfebcbe6c34fbfedde9e","_cell_guid":"72357954-6d42-4cfa-9ddc-7ce3696c0834","collapsed":true},"execution_count":null,"cell_type":"code","source":"# Now we can create the y values:\na2c = {'EAP': 0, 'HPL' : 1, 'MWS' : 2}\ny = np.array([a2c[a] for a in df.author])\ny = to_categorical(y)","outputs":[]},{"metadata":{"_uuid":"53f325a090a44f7109f0537022398797704cdc80","_cell_guid":"f123742f-540f-438d-aba3-ebbca69235be","collapsed":true},"execution_count":null,"cell_type":"code","source":"def create_docs(df, n_gram_max=2):\n    def add_ngram(q, n_gram_max):\n            ngrams = []\n            for n in range(2, n_gram_max+1):\n                for w_index in range(len(q)-n+1):\n                    ngrams.append('--'.join(q[w_index:w_index+n]))\n            return q + ngrams\n        \n    docs = []\n    for doc in df.text:\n#       preprocess already run\n#       doc = preprocess(doc).split()        \n        doc = doc.split()\n        docs.append(' '.join(add_ngram(doc, n_gram_max)))\n    \n    return docs","outputs":[]},{"metadata":{"_uuid":"150f9f6643e6753386b2021ac812ecc0cac66202","_cell_guid":"888047de-806e-4ad2-9fff-18b4d6583d30","collapsed":true},"execution_count":null,"cell_type":"code","source":"min_count = 2\n\ndocs = create_docs(df)\ntokenizer = Tokenizer(lower=False, filters='')\nnum_words = sum([1 for _, v in tokenizer.word_counts.items() if v >= min_count])\n\ntokenizer = Tokenizer(num_words=num_words, lower=False, filters='')\ntokenizer.fit_on_texts(docs)\ndocs = tokenizer.texts_to_sequences(docs)\n\nmaxlen = 256\n\ndocs = pad_sequences(sequences=docs, maxlen=maxlen)","outputs":[]},{"metadata":{"_uuid":"b9e353b548b0dfbd4b42a40d8a2643efeb359a20","_cell_guid":"f9ebc033-2a26-4656-9472-8990c1a27c79"},"cell_type":"markdown","source":"# **2. Model: FastText by Keras**\n\nFastText is very fast and strong baseline algorithm for text classification based on Continuous Bag-of-Words model a.k.a Word2vec.\n\nFastText contains only three layers:\n\n1. Embeddings layer: Input words (and word n-grams) are all words in a sentence/document\n2. Mean/AveragePooling Layer: Taking average vector of Embedding vectors\n3. Softmax layer\n\nThere are some implementations of FastText:\n\n- Original library provided by Facebook AI research: https://github.com/facebookresearch/fastText\n- Keras: https://github.com/fchollet/keras/blob/master/examples/imdb_fasttext.py\n- Gensim: https://radimrehurek.com/gensim/models/wrappers/fasttext.html\n\nOriginal Paper: https://arxiv.org/abs/1607.01759 : More detail information about fastText classification model"},{"metadata":{"_uuid":"8b56b2ef90e519b939b7bf9ec5a146f749807b02","_cell_guid":"636eb75e-6fba-413e-996d-1395609b422c"},"cell_type":"markdown","source":"# My FastText parameters are:\n\n- The dimension of word vector is 20\n- Optimizer is `Adam`\n- Inputs are words and word bi-grams\n  - you can change this parameter by passing the max n-gram size to argument of `create_docs` function.\n"},{"metadata":{"_uuid":"bba1d1a6416876e74ed688f56e4d5bc4990ec12a","_cell_guid":"393d1ddb-0a87-42a3-8575-53ff7abff1da","collapsed":true},"execution_count":null,"cell_type":"code","source":"input_dim = np.max(docs) + 1\nembedding_dims = 20","outputs":[]},{"metadata":{"_uuid":"e6c16572e6b32923af39dfd29467e32b52561bb1","_cell_guid":"2e3e1e3e-22f4-4727-ba6c-67f7b3e80d2f","collapsed":true},"execution_count":null,"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(input_dim=input_dim, output_dim=embedding_dims))\nmodel.add(GlobalAveragePooling1D())\nmodel.add(Dense(3, activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])","outputs":[]},{"metadata":{"_uuid":"22e57e010206a3044adf7b82160c7c3ca78030f8","_cell_guid":"0db889db-0b3e-4025-8847-e3eb5f853f37","collapsed":true,"scrolled":true},"execution_count":null,"cell_type":"code","source":"epochs = 45\nx_train, x_test, y_train, y_test = train_test_split(docs, y, test_size=0.15)\n\nn_samples = x_train.shape[0]\n\nhist = model.fit(x_train, y_train,\n                 batch_size=16,\n                 validation_data=(x_test, y_test),\n                 epochs=epochs,\n                 callbacks=[EarlyStopping(patience=2, monitor='val_loss')])","outputs":[]},{"metadata":{"_uuid":"f5d9f261c1befdb6b05bbd39536c78ad74396d90","_cell_guid":"52a07f6b-2a3a-401a-aee3-5421c53f0588","collapsed":true},"execution_count":null,"cell_type":"code","source":"test_df = pd.read_csv('../input/test.csv')\n# We've commented this out of the create_docs function, so need to run it manually here:\ntest_df['text'] = test_df['text'].apply(preprocess)\ndocs = create_docs(test_df)\ndocs = tokenizer.texts_to_sequences(docs)\ndocs = pad_sequences(sequences=docs, maxlen=maxlen)\ny = model.predict_proba(docs)\n\nresult = pd.read_csv('../input/sample_submission.csv')\nfor a, i in a2c.items():\n    result[a] = y[:, i]","outputs":[]},{"metadata":{"_uuid":"eaf5b87353d9a1e7367def64b453555c23d24e7a","_cell_guid":"409b8663-bbf5-4757-99c1-40010264de04","collapsed":true},"execution_count":null,"cell_type":"code","source":"# to_submit=result","outputs":[]}],"nbformat":4}