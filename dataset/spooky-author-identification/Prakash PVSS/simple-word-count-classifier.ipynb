{"metadata":{"language_info":{"mimetype":"text/x-python","version":"3.6.3","pygments_lexer":"ipython3","file_extension":".py","codemirror_mode":{"version":3,"name":"ipython"},"nbconvert_exporter":"python","name":"python"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"outputs":[],"metadata":{"_uuid":"6f6fe84deebeef8fd2540637d31307b38c1f9609","_cell_guid":"13b4a25f-258c-4e43-8e25-50da7ad63a0f","collapsed":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\nsample = pd.read_csv('../input/sample_submission.csv')","execution_count":1},{"outputs":[],"metadata":{"_uuid":"0cbb47de1292eda9b57935246b92ecd7556e79b8","_cell_guid":"43771189-6157-472e-ab72-d98b3ed84b75","collapsed":true},"cell_type":"code","source":"import nltk","execution_count":null},{"outputs":[],"metadata":{"_uuid":"ba95f3d5ffb08c144ead9a7dfca0c46278719c73","_cell_guid":"88eec9b5-4c7a-4120-a1ce-020efd77d9a6","collapsed":true},"cell_type":"code","source":"from nltk.stem import WordNetLemmatizer\nwordnet_lemmatizer = WordNetLemmatizer()\n\nfrom nltk.corpus import stopwords\nstop_words = set(stopwords.words('english'))\n\ndef myTokenizer(s):\n    s=s.lower()\n    tokens = nltk.tokenize.word_tokenize(s)\n    tokens = [t for t in tokens if len(t) > 2]\n    tokens = [wordnet_lemmatizer.lemmatize(t) for t in tokens]\n    tokens = [t for t in tokens if t not in stop_words]\n    return tokens\n    ","execution_count":null},{"outputs":[],"metadata":{"_uuid":"d379d554ae0d0cfbf8c385d2f47e631f853da431","_cell_guid":"831983ce-4e00-4921-9066-34b50b17238d","collapsed":true},"cell_type":"code","source":"word_index_map={}\ncurr_index = 0\nfor index,row in train.iterrows():\n    tokens = myTokenizer(row['text'])\n    for t in tokens:\n        if t not in word_index_map:\n            word_index_map[t]=curr_index\n            curr_index +=1 \nprint(len(word_index_map))\n","execution_count":null},{"outputs":[],"metadata":{"_uuid":"9e9258608f3a66964c767428cb3cf4ffb247d13e","_cell_guid":"0a484233-050c-4631-9a0f-0ba9a7be1e37","collapsed":true},"cell_type":"code","source":"N = len(train)\ndata = np.zeros((N,len(word_index_map)+1)) # +1 for Label\n","execution_count":null},{"outputs":[],"metadata":{"_uuid":"063c4ac435836f1249849ed0fedbe2f2c9759833","_cell_guid":"6b47e1d9-c172-46f0-9f50-246c835dc4cb","collapsed":true},"cell_type":"code","source":"from sklearn import preprocessing","execution_count":null},{"outputs":[],"metadata":{"_uuid":"21a8200658d69aa5f93fb6904e3ee90ee18cc0de","_cell_guid":"fd278350-1a78-4a21-91e1-db0bffab2681","collapsed":true},"cell_type":"code","source":"lbl_enc = preprocessing.LabelEncoder()\ny = lbl_enc.fit_transform(train.author.values)","execution_count":null},{"outputs":[],"metadata":{"_uuid":"dad79bd41c0c08de2c69496ae06e2b410c31877e","_cell_guid":"0a999815-033f-4f85-bcad-91f9f7832f3f","collapsed":true},"cell_type":"code","source":"def tokens_to_vectors(tokens,label):\n    if(len(tokens) == 0):\n        print('No Tokens Available')\n    x = np.zeros(len(word_index_map)+1) # +1 is for label\n    for t in tokens:\n        if t in word_index_map:\n            x[word_index_map[t]] +=1\n    if x.sum() != 0:\n        x = x/x.sum()\n    x[-1] = label\n    return x\nfor idx, row in train.iterrows():\n    tokens = myTokenizer(row['text'])\n    if len(tokens) == 0:\n        print(row['text'])\n    data[idx,:] = tokens_to_vectors(tokens,y[idx])\n    ","execution_count":null},{"outputs":[],"metadata":{"_uuid":"35cbcda3424e0acd21dd6eda229104aa79318af4","_cell_guid":"5e928d6e-d867-4164-840a-d0abda65db17","collapsed":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression","execution_count":null},{"outputs":[],"metadata":{"_uuid":"77a791fc1f66352b2e161d480db4beb5700d5b80","_cell_guid":"47eee6ba-db37-470e-a098-15e1dcddd3fb","collapsed":true},"cell_type":"code","source":"model = LogisticRegression()\nmodel.fit(data[:,:-1],data[:,-1])","execution_count":null},{"outputs":[],"metadata":{"_uuid":"14f1d2dfb124a9625656da2cf3692347eacd8d4a","_cell_guid":"d48affaf-7280-4fb5-898c-af0d064ea4ba","collapsed":true},"cell_type":"code","source":"model.score(data[:,:-1],data[:,-1])","execution_count":null},{"outputs":[],"metadata":{"_uuid":"ed7a5a8e1818d3f675d729d13df3c2200a0194a7","_cell_guid":"1962a6e0-f936-4081-99b9-a3795be54651","collapsed":true},"cell_type":"code","source":"N=len(test)\ntd = np.zeros((N,len(word_index_map)+1))\nfor idx,row in test.iterrows():\n    tokens = myTokenizer(row['text'])\n    td[idx,:] = tokens_to_vectors(tokens,0)\ntd = td[:,:-1]\n    \np = model.predict_proba(td)# 0 is dummy\nprint(p.shape)\n    ","execution_count":null},{"outputs":[],"metadata":{"_uuid":"68b03a54cd5d039401c2a2a30d5f2d62765855e8","_cell_guid":"eb15990c-346a-4a16-879c-113ef18f951a","collapsed":true},"cell_type":"code","source":"result = pd.DataFrame()\nresult['id'] = test['id']\nresult['EAP'] = p[:,0]\nresult['HPL'] = p[:,1]\nresult['MWS'] = p[:,2]\n","execution_count":null},{"outputs":[],"metadata":{"_uuid":"6041413d65e79c0ec5272de42cdb2c2be809c720","_cell_guid":"a3c44972-42db-4149-9458-2628a8b0fed5","collapsed":true},"cell_type":"code","source":"result.head()\nresult.to_csv(\"result.csv\", index=False)","execution_count":null},{"outputs":[],"metadata":{"_uuid":"4642db9dbb832b186dd847278f85725225671c13","_cell_guid":"de003339-c508-412c-85e1-083008b939b5","collapsed":true},"cell_type":"code","source":"","execution_count":null}],"nbformat":4,"nbformat_minor":1}