{"nbformat_minor":1,"cells":[{"execution_count":null,"source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\n\neng_stopwords = set(stopwords.words(\"english\"))","cell_type":"code","outputs":[],"metadata":{"_uuid":"22307f6388aeabe2580713a1257f7181cdce932e","_cell_guid":"18b00353-fd83-47b7-8355-340b1c8b2998","collapsed":true}},{"execution_count":null,"source":"train_df = pd.read_csv('../input/spooky-author-identification/train.csv')\ntest_df = pd.read_csv('../input/spooky-author-identification/test.csv')\nsample = pd.read_csv('../input/spooky-author-identification/sample_submission.csv')","cell_type":"code","outputs":[],"metadata":{"_uuid":"15632aeae44d1805ed65d930694a01cc49f9291a","_cell_guid":"034371d6-81f7-44d6-bd3c-c20f398df1b3","collapsed":true}},{"execution_count":null,"source":"train_df['num_words'] = train_df['text'].apply(lambda x: len(str(x).split()))\ntest_df['num_words'] = test_df['text'].apply(lambda x: len(str(x).split()))\n","cell_type":"code","outputs":[],"metadata":{"_uuid":"31365d2449f114412b09bbfb3e2d35f8c4e4c832","_cell_guid":"b8627659-8afc-4c19-9ccb-93bd0b61e17c","collapsed":true}},{"execution_count":null,"source":"train_df['num_unique_words'] = train_df['text'].apply(lambda x: len(set(str(x).split())))\ntest_df['num_unique_words'] = test_df['text'].apply(lambda x: len(set(str(x).split())))","cell_type":"code","outputs":[],"metadata":{"_uuid":"10df68dcf016d3599fd6b74f2cc5549dd7a2db6b","_cell_guid":"03567576-8016-48bf-b35a-63aa8fc3f6ff","collapsed":true}},{"execution_count":null,"source":"train_df[\"num_chars\"] = train_df[\"text\"].apply(lambda x: len(str(x)))\ntest_df[\"num_chars\"] = test_df[\"text\"].apply(lambda x: len(str(x)))\n\n## Number of stopwords in the text ##\ntrain_df[\"num_stopwords\"] = train_df[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\ntest_df[\"num_stopwords\"] = test_df[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n\n## Number of punctuations in the text ##\ntrain_df[\"num_punctuations\"] =train_df['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\ntest_df[\"num_punctuations\"] =test_df['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n\n## Number of title case words in the text ##\ntrain_df[\"num_words_upper\"] = train_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\ntest_df[\"num_words_upper\"] = test_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n\n## Number of title case words in the text ##\ntrain_df[\"num_words_title\"] = train_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\ntest_df[\"num_words_title\"] = test_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n\n## Average length of the words in the text ##\ntrain_df[\"mean_word_len\"] = train_df[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\ntest_df[\"mean_word_len\"] = test_df[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))","cell_type":"code","outputs":[],"metadata":{"_uuid":"75a138775bf04e3f7d035c8f14b31cf5c0f6a0c6","_cell_guid":"9851fb02-264c-4ebd-8ec2-dd6587b20644","collapsed":true}},{"execution_count":null,"source":"author_mapping_dict = {'EAP':0, 'HPL':1, 'MWS':2}\ntrain_y = train_df['author'].map(author_mapping_dict)\ntrain_id = train_df['id'].values\ntest_id = test_df['id'].values","cell_type":"code","outputs":[],"metadata":{"_uuid":"a098d230b75d17384800a9ed1c9fb87afb34c432","_cell_guid":"097138ec-8702-43b3-a036-cf3fbf47b6b0","collapsed":true}},{"execution_count":null,"source":"cols_to_drop = ['id','text']\ntrain_X = train_df.drop(cols_to_drop+['author'],axis=1)\ntest_X = test_df.drop(cols_to_drop,axis=1)","cell_type":"code","outputs":[],"metadata":{"_uuid":"53069ea8830700f9ee555d79d822e431614cd9ec","_cell_guid":"ee4455e3-baef-4524-a9e0-809d7f57537f","collapsed":true}},{"execution_count":null,"source":"train_X.head()","cell_type":"code","outputs":[],"metadata":{"_uuid":"5fb959807ddc9ac721bf2c7c080fc6f298cf5318","_cell_guid":"8acd31c3-6b0e-4ca9-9b8f-f3c06cfae567","collapsed":true}},{"execution_count":null,"source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn import ensemble, metrics, model_selection, naive_bayes\ndef runLR(train_X,train_y,test_X,test_y=None,test_X2=None):\n    model = LogisticRegression()\n    model.fit(train_X,train_y)\n    return model.predict_proba(test_X),model.predict_proba(test_X2),model\ndef runMNB(train_X, train_y, test_X, test_y, test_X2):\n    model = naive_bayes.MultinomialNB()\n    model.fit(train_X, train_y)\n    pred_test_y = model.predict_proba(test_X)\n    pred_test_y2 = model.predict_proba(test_X2)\n    return pred_test_y, pred_test_y2, model","cell_type":"code","outputs":[],"metadata":{"_uuid":"a77a05792f59f81580dd5f8ce034c39865ca697d","_cell_guid":"01ce210e-e997-4fb3-bbc0-87fe3cee5dec","collapsed":true}},{"execution_count":null,"source":"kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\ncv_scores = []\npred_full_test = 0\npred_train = np.zeros([train_df.shape[0], 3])\nfor dev_index, val_index in kf.split(train_X):\n    dev_X, val_X = train_X.loc[dev_index], train_X.loc[val_index]\n    dev_y, val_y = train_y[dev_index], train_y[val_index]\n    pred_val_y, pred_test_y, model = runLR(dev_X, dev_y, val_X, val_y, test_X)\n    pred_full_test = pred_full_test + pred_test_y\n    pred_train[val_index,:] = pred_val_y\n    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n    \nprint(\"cv scores : \", cv_scores)","cell_type":"code","outputs":[],"metadata":{"_uuid":"c05892ed7e02ee309d9ddfa6e44e6b5b6e7348c6","scrolled":true,"_cell_guid":"d90a864e-aa27-4047-a5fc-1656a2801d6a"}},{"execution_count":null,"source":"from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD\ntfidf_vec = TfidfVectorizer(stop_words='english',ngram_range=(1,3))\nfull_tfidf = tfidf_vec.fit_transform(train_df['text'].values.tolist()+test_df['text'].values.tolist())\ntrain_tfidf = tfidf_vec.transform(train_df['text'].values.tolist())\ntest_tfidf = tfidf_vec.transform(test_df['text'].values.tolist())","cell_type":"code","outputs":[],"metadata":{"_uuid":"65857d2cff4847986a823c70b3f4c995140f5b5e","_cell_guid":"14bb9409-3e5c-47fb-8478-41a5e9402fe2","collapsed":true}},{"execution_count":null,"source":"kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\ncv_scores = []\npred_full_test = 0\npred_train = np.zeros([train_df.shape[0], 3])\nfor dev_index, val_index in kf.split(train_X):\n    dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n    dev_y, val_y = train_y[dev_index], train_y[val_index]\n    pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_tfidf)\n    pred_full_test = pred_full_test + pred_test_y\n    pred_train[val_index,:] = pred_val_y\n    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n    \nprint(\"cv scores : \", cv_scores)","cell_type":"code","outputs":[],"metadata":{"_uuid":"d708f8bdb7e09c304093aeb15bddc5e4a4cbb73e","_cell_guid":"f5c3aef6-233d-49f4-8f65-4cdb05c90550"}},{"execution_count":null,"source":"n_comp =20\nsvd_tf = TruncatedSVD(n_components=20,algorithm='arpack')\nsvd_tf.fit(full_tfidf)\ntrain_svd = pd.DataFrame(svd_tf.transform(train_tfidf))\ntest_svd = pd.DataFrame(svd_tf.transform(test_tfidf))\n\ntrain_svd_cols = ['svd_word_'+str(i) for i in range(n_comp)]\ntest_svd_cols = ['svd_word_'+str(i) for i in range(n_comp)]\n\ntrain_df = pd.concat([train_df,train_svd],axis=1)\ntest_df = pd.concat([test_df,test_svd],axis=1)\ntrain_df.head()","cell_type":"code","outputs":[],"metadata":{"_uuid":"1af107d02b818ac4ee4f418b00c26b3ffce247e4","_cell_guid":"c58bfe44-354c-4f0c-acf4-178ac80443df"}},{"execution_count":null,"source":"cols_to_drop = ['id','text']\ntrain_X = train_df.drop(cols_to_drop+['author'],axis=1)\ntest_X = test_df.drop(cols_to_drop,axis=1)\nkf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\ncv_scores = []\npred_full_test = 0\npred_train = np.zeros([train_df.shape[0], 3])\nfor dev_index, val_index in kf.split(train_df):\n    dev_X, val_X = train_X.loc[dev_index], train_X.loc[val_index]\n    dev_y, val_y = train_y[dev_index], train_y[val_index]\n    pred_val_y, pred_test_y, model = runLR(dev_X, dev_y, val_X, val_y, test_X)\n    pred_full_test = pred_full_test + pred_test_y\n    pred_train[val_index,:] = pred_val_y\n    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n    \nprint(\"cv scores : \", cv_scores)","cell_type":"code","outputs":[],"metadata":{"_uuid":"5be6cf1bb8cf165aa763e235b09dc8ab8227326f","_cell_guid":"a289de63-d894-409f-b0d4-c39b276154a7"}},{"execution_count":null,"source":"tfidf_vec = CountVectorizer(stop_words='english', ngram_range=(1,2))\ntfidf_vec.fit(train_df['text'].values.tolist() + test_df['text'].values.tolist())\ntrain_tfidf = tfidf_vec.transform(train_df['text'].values.tolist())\ntest_tfidf = tfidf_vec.transform(test_df['text'].values.tolist())","cell_type":"code","outputs":[],"metadata":{"_uuid":"9b57b0688e7a7569e5c1dd467dfbf13d7f2a0c95","_cell_guid":"262d59fd-b43e-4ecb-b14d-26c6780f7ba2","collapsed":true}},{"execution_count":null,"source":"cv_scores = []\npred_full_test = 0\npred_train = np.zeros([train_df.shape[0], 3])\nkf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\nfor dev_index, val_index in kf.split(train_X):\n    dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n    dev_y, val_y = train_y[dev_index], train_y[val_index]\n    pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_tfidf)\n    pred_full_test = pred_full_test + pred_test_y\n    pred_train[val_index,:] = pred_val_y\n    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\nprint(\"Mean cv score : \", np.mean(cv_scores))\npred_full_test = pred_full_test / 5.\nprint(\"cv scores : \", cv_scores)\n\ntrain_df[\"nb_cvec_eap\"] = pred_train[:,0]\ntrain_df[\"nb_cvec_hpl\"] = pred_train[:,1]\ntrain_df[\"nb_cvec_mws\"] = pred_train[:,2]\ntest_df[\"nb_cvec_eap\"] = pred_full_test[:,0]\ntest_df[\"nb_cvec_hpl\"] = pred_full_test[:,1]\ntest_df[\"nb_cvec_mws\"] = pred_full_test[:,2]\n","cell_type":"code","outputs":[],"metadata":{"_uuid":"dc3aa6f9367b91ff78fbdd27dc9aa885693a153d","_cell_guid":"225637fd-761b-4202-b27a-0eba3770e8f1"}},{"execution_count":null,"source":"cols_to_drop = ['id','text']\ntrain_X = train_df.drop(cols_to_drop+['author'],axis=1)\ntest_X = test_df.drop(cols_to_drop,axis=1)\nkf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\ncv_scores = []\npred_full_test = 0\npred_train = np.zeros([train_df.shape[0], 3])\nfor dev_index, val_index in kf.split(train_df):\n    dev_X, val_X = train_X.loc[dev_index], train_X.loc[val_index]\n    dev_y, val_y = train_y[dev_index], train_y[val_index]\n    pred_val_y, pred_test_y, model = runLR(dev_X, dev_y, val_X, val_y, test_X)\n    pred_full_test = pred_full_test + pred_test_y\n    pred_train[val_index,:] = pred_val_y\n    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\nprint(\"cv scores : \", cv_scores)\n","cell_type":"code","outputs":[],"metadata":{"_uuid":"e4d69272b4bd35779eb9d152f3ab46bb0c49ef84","_cell_guid":"9110ad80-0e30-4e35-85b7-9412396ee87d"}},{"execution_count":null,"source":"wv = \"../input/glove-global-vectors-for-word-representation/glove.6B.100d.txt\"\nfrom tqdm import tqdm\nfrom nltk import word_tokenize\ndef loadWordVecs():\n    embeddings_index = {}\n    f = open(wv)\n    for line in f:\n        values = line.split()\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n        embeddings_index[word] = coefs\n    f.close()\n    print('Found %s word vectors.' % len(embeddings_index))\n    return embeddings_index\n\ndef sent2vec(embeddings_index,s): # this function creates a normalized vector for the whole sentence\n    words = str(s).lower()\n    words = word_tokenize(words)\n    words = [w for w in words if not w in stopwords.words('english')]\n    words = [w for w in words if w.isalpha()]\n    M = []\n    for w in words:\n        try:\n            M.append(embeddings_index[w])\n        except:\n            continue\n    M = np.array(M)\n    if M.shape[0] != 0:\n        v = M.min(axis=0)\n        w = M.max(axis=0)\n        return np.concatenate((v,w),axis=0)\n    else:\n        return np.zeros(200)\n\ndef doGlove(x_train,x_test):\n    embeddings_index = loadWordVecs()\n    # create sentence vectors using the above function for training and validation set\n    xtrain_glove = [sent2vec(embeddings_index,x) for x in tqdm(x_train)]\n    xtest_glove = [sent2vec(embeddings_index,x) for x in tqdm(x_test)]\n    xtrain_glove = np.array(xtrain_glove)\n    xtest_glove = np.array(xtest_glove)\n    return xtrain_glove,xtest_glove,embeddings_index\nglove_vecs_train,glove_vecs_test,embeddings_index = doGlove(train_df['text'],test_df['text'])","cell_type":"code","outputs":[],"metadata":{"_uuid":"4c93c1156f07290da9ff24e0b30b32da7635be76","_cell_guid":"8a6be7cd-1b73-4ab8-aec7-0a1696a4a62b"}},{"execution_count":null,"source":"\ntrain_df[['sent_vec_'+str(i) for i in range(200)]] = pd.DataFrame(glove_vecs_train.tolist())\ntest_df[['sent_vec_'+str(i) for i in range(200)]] = pd.DataFrame(glove_vecs_test.tolist())\ntrain_df.head()","cell_type":"code","outputs":[],"metadata":{"_uuid":"0c2f22b4a83a06c41a23a2d982b548a18cf68e9a","_cell_guid":"67c8f5ea-fa39-4761-861d-14b410903295"}},{"execution_count":null,"source":"import xgboost as xgb\ndef runXGB(train_X, train_y, test_X, test_y=None, test_X2=None, seed_val=0, child=1, colsample=0.3):\n    param = {}\n    param['objective'] = 'multi:softprob'\n    param['eta'] = 0.1\n    param['max_depth'] = 3\n    param['silent'] = 1\n    param['num_class'] = 3\n    param['eval_metric'] = \"mlogloss\"\n    param['min_child_weight'] = child\n    param['subsample'] = 0.8\n    param['colsample_bytree'] = colsample\n    param['seed'] = seed_val\n    num_rounds = 2000\n\n    plst = list(param.items())\n    xgtrain = xgb.DMatrix(train_X, label=train_y)\n\n    if test_y is not None:\n        xgtest = xgb.DMatrix(test_X, label=test_y)\n        watchlist = [ (xgtrain,'train'), (xgtest, 'test') ]\n        model = xgb.train(plst, xgtrain, num_rounds, watchlist, early_stopping_rounds=50, verbose_eval=20)\n    else:\n        xgtest = xgb.DMatrix(test_X)\n        model = xgb.train(plst, xgtrain, num_rounds)\n\n    pred_test_y = model.predict(xgtest, ntree_limit = model.best_ntree_limit)\n    if test_X2 is not None:\n        xgtest2 = xgb.DMatrix(test_X2)\n        pred_test_y2 = model.predict(xgtest2, ntree_limit = model.best_ntree_limit)\n    return pred_test_y, pred_test_y2, model\n","cell_type":"code","outputs":[],"metadata":{"_uuid":"b14091579444efc91d4fffa0086ae0b4118719a9","_cell_guid":"6fcb07e7-bb53-4766-8512-ecbca495a7f7","collapsed":true}},{"execution_count":null,"source":"\ncols_to_drop = ['id','text']\ntrain_X = train_df.drop(cols_to_drop+['author'],axis=1)\ntest_X = test_df.drop(cols_to_drop,axis=1)\nkf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\ncv_scores = []\npred_full_test = 0\npred_train = np.zeros([train_df.shape[0], 3])\nfor dev_index, val_index in kf.split(train_df):\n    dev_X, val_X = train_X.loc[dev_index], train_X.loc[val_index]\n    dev_y, val_y = train_y[dev_index], train_y[val_index]\n    pred_val_y, pred_test_y, model = runXGB(dev_X, dev_y, val_X, val_y, test_X, seed_val=0, colsample=0.7)\n    pred_full_test = pred_full_test + pred_test_y\n    pred_train[val_index,:] = pred_val_y\n    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\nprint(\"cv scores : \", cv_scores)\n","cell_type":"code","outputs":[],"metadata":{"_uuid":"63675a954264084a2bc9cd21a1b44a52d6330000","_cell_guid":"abbfe337-9c30-4f1d-93c6-365cc1842e0e"}},{"execution_count":null,"source":"p = pred_full_test/5\nresult = pd.DataFrame()\nresult['id'] = test_id\nresult['EAP'] = p[:,0]\nresult['HPL'] = p[:,1]\nresult['MWS'] = p[:,2]","cell_type":"code","outputs":[],"metadata":{"_uuid":"3786bdab74a74b5d53c6fa1e6333c84ce914728d","_cell_guid":"a486af79-dabf-471e-9b1d-708ccd60279e","collapsed":true}},{"execution_count":null,"source":"result.head()\nresult.to_csv(\"result.csv\", index=False)","cell_type":"code","outputs":[],"metadata":{"_uuid":"416a6fcc69a626913fa33d2dc3c6dd2409c7a803","_cell_guid":"7cf1c637-ef4b-416b-9298-9b6de17ed1e1","collapsed":true}},{"execution_count":null,"source":"","cell_type":"code","outputs":[],"metadata":{"_uuid":"a74d8f462c0424a154192899bfb78fd986ddd87a","_cell_guid":"6930c8c9-abbb-4b31-af42-f46b5f3de6e3","collapsed":true}}],"nbformat":4,"metadata":{"language_info":{"codemirror_mode":{"version":3,"name":"ipython"},"file_extension":".py","mimetype":"text/x-python","version":"3.6.3","nbconvert_exporter":"python","pygments_lexer":"ipython3","name":"python"},"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"}}}