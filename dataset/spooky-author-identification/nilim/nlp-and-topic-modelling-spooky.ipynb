{"cells":[{"metadata":{},"cell_type":"markdown","source":"---\n\n<h1 style=\"text-align: center;font-size: 30px; color: #013b86;\">NLP and Topic Modelling</h1>\n\n---\n\n<center><img style=\"width: 700px;\" src=\"https://miro.medium.com/max/2796/1*jpytbqadO3FtdIyOjx2_yg.png\"></center>\n\n---\n<i>Source: Base image from Google</i>"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport nltk\nimport re\nimport string\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_data = pd.read_csv(\"/kaggle/input/spooky-author-identification/train.zip\")\n# train_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Process\n* Data Cleaning\n* Stopword Removal\n* Find out common words\n* EDA\n* Lemmatization\n* Vectorizing\n* Tokenization\n* Topic Modelling"},{"metadata":{},"cell_type":"markdown","source":"# Data Cleaning"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = train_data.drop('id', axis='columns')\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_train_data(x):\n    text = x\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text) # remove square brackets\n    text = re.sub(r'[^\\w\\s]','',text) # remove punctuation\n    text = re.sub('\\w*\\d\\w*', '', text) # remove words containing numbers\n    text = re.sub('\\n', '', text)\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_data = train_data.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_data['text'] = train_data.text.apply(lambda x : clean_train_data(x))\nclean_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Stopword Removal"},{"metadata":{"trusted":true},"cell_type":"code","source":"eng_stopwords = nltk.corpus.stopwords.words(\"english\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_eng_stopwords(text):\n    token_text = nltk.word_tokenize(text)\n    remove_stop = [word for word in token_text if word not in eng_stopwords]\n    join_text = ' '.join(remove_stop)\n    return join_text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"remove_stop_data = clean_data.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"remove_stop_data['text'] = clean_data.text.apply(lambda x : remove_eng_stopwords(x))\nremove_stop_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Before remove stopwords\", len(clean_data['text'][0]))\nprint(\"After remove stopwords\", len(remove_stop_data['text'][0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Find out common words"},{"metadata":{"trusted":true},"cell_type":"code","source":"from itertools import chain\nfrom collections import Counter","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list_words = remove_stop_data['text'].str.split()\nlist_words_merge = list(chain(*list_words))\n\nd = Counter(list_words_merge)\ndf = pd.DataFrame(data=d, index=['count'])\ntop_common_words = df.T.sort_values(by=['count'], ascending=False).reset_index().head(50)\ntop_common_words.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,7))\nsns.set(style=\"darkgrid\")\nsns.barplot(x=\"index\", y='count', data=top_common_words)\nplt.xticks(rotation=90)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"common_words_value = top_common_words['index'].values\nremove_words = ['man', 'life', 'night', 'house', 'heart']\nnew_stop_words = [x for x in common_words_value if x not in remove_words]\nnew_stop_words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_new_stopwords(text):\n    token_text = nltk.word_tokenize(text)\n    remove_stop = [word for word in token_text if word not in new_stop_words]\n    join_text = ' '.join(remove_stop)\n    return join_text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_stop_data = remove_stop_data.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_stop_data['text'] = remove_stop_data.text.apply(lambda x : remove_new_stopwords(x))\nnew_stop_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Before remove stopwords\", len(remove_stop_data['text'][4]))\nprint(\"After remove stopwords\", len(new_stop_data['text'][4]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,7))\nsns.set(style=\"darkgrid\")\nsns.countplot(x=\"author\", data=train_data)\nplt.title('Author text distribution')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_words_after = train_data['text'].str.split()\nmerged = list(chain(*all_words_after))\nd = Counter(merged)\ndf = pd.DataFrame(data=d, index=['count'])\ntop_count_words = df.T.sort_values(by=['count'], ascending=False).reset_index().head(50)\ntop_count_words.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,7))\nsns.set(style=\"darkgrid\")\nsns.barplot(x=\"index\", y='count', data=top_count_words)\nplt.xticks(rotation=90)\n\nplt.title(\"Most common words before removing stop words\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_words_before = new_stop_data['text'].str.split()\nmerged = list(chain(*all_words_before))\nd = Counter(merged)\ndf = pd.DataFrame(data=d, index=['count'])\nbefore_top_words = df.T.sort_values(by=['count'], ascending=False).reset_index().head(50)\nbefore_top_words.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,7))\nsns.set(style=\"darkgrid\")\nsns.barplot(x=\"index\", y='count', data=before_top_words)\nplt.xticks(rotation=90)\n\nplt.title(\"Most common words after removing stop words\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eap_cloud = train_data[train_data.author == 'EAP'].text.values\nhpl_cloud = train_data[train_data.author == 'HPL'].text.values\nmws_cloud = train_data[train_data.author == 'MWS'].text.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nwc = WordCloud(stopwords=STOPWORDS, background_color=\"white\", colormap=\"Dark2\",\n               max_font_size=150, random_state=42)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20, 15))\n\nplt.subplot(1, 3, 1)\nasd = \" \".join(eap_cloud)\nwc.generate(asd)\nplt.imshow(wc, interpolation=\"bilinear\")\nplt.axis('off')\nplt.title('Edgar Allen Poe')\n\nplt.subplot(1, 3, 2)\nasd = \" \".join(hpl_cloud)\nwc.generate(asd)\nplt.imshow(wc, interpolation=\"bilinear\")\nplt.axis('off')\nplt.title('HP Lovecraft')\n\nplt.subplot(1, 3, 3)\nasd = \" \".join(mws_cloud)\nwc.generate(asd)\nplt.imshow(wc, interpolation=\"bilinear\")\nplt.axis('off')\nplt.title('Mary Shelley')\n\nplt.figtext(.5,.63,'All writers, word clouds before stop word removal', color='b', fontsize=25, ha='center')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eap_cloud_before = new_stop_data[new_stop_data.author == 'EAP'].text.values\nhpl_cloud_before = new_stop_data[new_stop_data.author == 'HPL'].text.values\nmws_cloud_before = new_stop_data[new_stop_data.author == 'MWS'].text.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nwc = WordCloud(stopwords=STOPWORDS, background_color=\"white\", colormap=\"Dark2\",\n               max_font_size=150, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20, 15))\n\nplt.subplot(1, 3, 1)\nasd = \" \".join(eap_cloud_before)\nwc.generate(asd)\nplt.imshow(wc, interpolation=\"bilinear\")\nplt.axis('off')\nplt.title('Edgar Allen Poe')\n\nplt.subplot(1, 3, 2)\nasd = \" \".join(hpl_cloud_before)\nwc.generate(asd)\nplt.imshow(wc, interpolation=\"bilinear\")\nplt.axis('off')\nplt.title('HP Lovecraft')\n\nplt.subplot(1, 3, 3)\nasd = \" \".join(mws_cloud_before)\nwc.generate(asd)\nplt.imshow(wc, interpolation=\"bilinear\")\nplt.axis('off')\nplt.title('Mary Shelley')\n\nplt.figtext(.5,.63,'All writers, word clouds After stop word removal', color='b', fontsize=25, ha='center')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full_name = {'EAP': 'Edgar Allen Poe', 'MWS': 'Mary Shelley', 'HPL': 'HP Lovecraft'}\nwriter_name = ['EAP', 'MWS', 'HPL']\nwriter_count_obj = {'writer_full_name': [], 'total_words': [], 'unique_words': []}\nfor name in writer_name:\n    name_all_words = new_stop_data[new_stop_data.author == name].text.str.split()\n    name_merged = list(chain(*name_all_words))\n    name_total_len = len(name_merged)\n    myset = set(name_merged)\n    \n    writer_count_obj['writer_full_name'].append(full_name[name])\n    writer_count_obj['total_words'].append(name_total_len)\n    writer_count_obj['unique_words'].append(len(myset))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"words_df = pd.DataFrame(writer_count_obj)\nwords_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tidy = words_df.melt(id_vars='writer_full_name').rename(columns=str.title)\ntidy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax1 = plt.subplots(figsize=(15, 10))\ntidy = words_df.melt(id_vars='writer_full_name').rename(columns=str.title)\nsns.barplot(x='Writer_Full_Name', y='Value', hue='Variable', data=tidy, ax=ax1)\nsns.despine(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Lemmatization"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.stem import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lemm = WordNetLemmatizer()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def word_lemmatizer(text):\n    token_text = nltk.word_tokenize(text)\n    remove_stop = [lemm.lemmatize(w) for w in token_text]\n    join_text = ' '.join(remove_stop)\n    return join_text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lemmatize_data = new_stop_data.copy()\nlemmatize_data['text'] = new_stop_data.text.apply(lambda x : word_lemmatizer(x))\nlemmatize_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Vectorizing"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\ncount_vec = CountVectorizer(stop_words='english')\ndata_count_vec = count_vec.fit_transform(lemmatize_data.text)\ndata_count_vec","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_count_df = pd.DataFrame(data_count_vec.toarray(), columns=count_vec.get_feature_names())\ndata_count_df.index = lemmatize_data.author\ndata_count_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Topic modelling"},{"metadata":{},"cell_type":"markdown","source":"\n\n<center><img style=\"width: 700px;\" src=\"https://miro.medium.com/max/638/0*Sj65xR38wDwuxhtr.jpg\"></center>\n\n---\n<i>Source: Base image from Google</i>"},{"metadata":{},"cell_type":"markdown","source":"## Way-1 Latent Dirichlet Allocation"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import NMF, LatentDirichletAllocation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lda_model = LatentDirichletAllocation(n_components=5, max_iter=5, learning_method = 'online', learning_offset = 50.,random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lda_model.fit(data_count_vec)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lda_model.components_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print_words = 20\nget_feature_names = count_vec.get_feature_names()\nfor index, topic in enumerate(lda_model.components_):\n    words = \" \".join([get_feature_names[i] for i in topic.argsort()[:-print_words - 1 :-1]])\n    print(f\"Topic - {index}:\")\n    print(words)\n    print(\"-\"*100)\n    print('\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Topic List\n1. Topic 1: sea, water\n1. Topic 2: house, room\n1. Topic 3: dream, star\n1. Topic 4: time, table\n1. Topic 5: friend, heart"},{"metadata":{},"cell_type":"markdown","source":"## Way-2 LDA with gensim "},{"metadata":{"trusted":true},"cell_type":"code","source":"from gensim import matutils, models\nimport scipy.sparse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_count_df.index.name = None\nnew_dtm_t_data = data_count_df.T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"spare_counts = scipy.sparse.csr_matrix(new_dtm_t_data)\nnew_corpus = matutils.Sparse2Corpus(spare_counts)\nnew_corpus","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv = count_vec\nid2word = dict((v, k) for k, v in cv.vocabulary_.items())\n# id2word","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gensim_lda_topic = models.LdaModel(corpus=new_corpus, id2word=id2word, num_topics=5, passes=10)\ngensim_lda_topic.print_topics()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Topic List\n1. Topic 1: friend, gentleman\n1. Topic 2: life, love\n1. Topic 3: sea, night\n1. Topic 4: sea, water\n1. Topic 5: man, moon"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}