{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#importing libraries\n\nimport pandas as pd\nimport numpy as np\nimport xgboost as xgb\nfrom tqdm import tqdm\nfrom sklearn.svm import SVC\nfrom keras.models import Sequential\nfrom keras.layers.recurrent import LSTM, GRU\nfrom keras.layers.core import Dense, Activation, Dropout\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.utils import np_utils\nfrom sklearn import model_selection, pipeline, metrics, preprocessing, decomposition\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom keras.layers import GlobalMaxPooling1D, MaxPooling1D, Conv1D, Flatten, Bidirectional, SpatialDropout1D\nfrom keras.preprocessing import sequence, text\nfrom keras.callbacks import EarlyStopping\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/spooky-author-identification/train.csv\")\ntest = pd.read_csv(\"../input/spooky-author-identification/test.csv\")\nsample = pd.read_csv(\"../input/spooky-author-identification/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape, test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['id'].unique().shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##multiclass_logloss\ndef multiclass_logloss(actual, predicted, eps = 1e-15):\n    \"\"\"Multi class version of Logarithmic Loss metric.\n    :param actual: Array containing the actual target classes\n    :param predicted: Matrix with class predictions, one probability per class\n    \"\"\"\n    #converting actual to a binary array \n    if(len(actual.shape)==1):\n        actual2 = np.zeros((actual.shape[0], predicted.shape[1]))\n        for i, val in enumerate(actual):\n            actual2[i,val] = 1\n        actual = actual2\n        \n    clip = np.clip(predicted, eps, 1-eps)\n    rows = actual.shape[0]\n    vsota = np.sum(actual * np.log(clip))\n    return -1.0 / rows *vsota\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Label encoder to convert text labels to 0,1,2\nlbl_enc = preprocessing.LabelEncoder()\ny = lbl_enc.fit_transform(train['author'].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## train test split\n\nxtrain, xvalid, ytrain, yvalid = train_test_split(train['text'].values, y , \n                                                 stratify = y, shuffle = True,\n                                                 test_size = 0.1, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xtrain.shape, xvalid.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Baseline model"},{"metadata":{"trusted":true},"cell_type":"code","source":"## 1st approach\n#tfidfvectorizer - creating features\n# Convert a collection of raw documents to a matrix of TF-IDF features.\ntfv = TfidfVectorizer(min_df = 3, #When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. cut-off\n                      strip_accents = 'unicode',#Remove accents and perform other character normalization during the preprocessing step. ‘ascii’ is a fast method that only works on characters that have an direct ASCII mapping. ‘unicode’ is a slightly slower method that works on any characters. None (default) does nothing. \n                      stop_words = 'english',\n                      ngram_range = (1,3),\n                      analyzer = 'word',\n                      token_pattern = r'\\w{1,}', # pattern for word, 1 or more words\n                      max_features = None, #If not None, build a vocabulary that only consider the top max_features ordered by term frequency across the corpus.\n                      use_idf = 1, # Enable inverse-document-frequency reweighting.\n                      smooth_idf = 1, #Smooth idf weights by adding one to document frequencies, as if an extra document was seen containing every term in the collection exactly once. Prevents zero divisions.\n                      sublinear_tf = 1 #Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf).\n                     )\n\ntfv.fit_transform(list(xtrain) + list(xvalid))\nxtrain_tfv = tfv.transform(xtrain)\nxvalid_tfv = tfv.transform(xvalid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Logistic regression\nclf = LogisticRegression(C=1.0 ) #Inverse of regularization strength..small value means high regularization\nclf.fit(xtrain_tfv, ytrain)\npredictions = clf.predict_proba(xvalid_tfv)\n\nprint(\"Logloss : %0.3f\" %multiclass_logloss(yvalid, predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## 2nd approach - using word count as features using CountVectorizer\n\nctv = CountVectorizer(analyzer = 'word', token_pattern =r'\\w{1,}', stop_words='english',ngram_range=(1,3))\nctv.fit(list(xtrain) + list(xvalid))\nxtrain_ctv = ctv.transform(xtrain)\nxvalid_ctv = ctv.transform(xvalid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Logistic regression on count features\nclf = LogisticRegression(C=1.0)\nclf.fit(xtrain_ctv, ytrain)\npredictions = clf.predict_proba(xvalid_ctv)\nprint(\"Logloss : %0.3f\" %multiclass_logloss(yvalid, predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## 3rd approach - Naive Bayes on tfidf\n\nclf = MultinomialNB()\nclf.fit(xtrain_tfv, ytrain)\npredictions = clf.predict_proba(xvalid_tfv)\n\nprint(\"Logloss : %0.3f\" %multiclass_logloss(yvalid, predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## 4th approach - Naive Bayes on count vectorizer\n\nclf = MultinomialNB()\nclf.fit(xtrain_ctv, ytrain)\npredictions = clf.predict_proba(xvalid_ctv)\n\nprint(\"Logloss : %0.3f\" %multiclass_logloss(yvalid, predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## 5th approach - SVM\n# SVM takes lot of time so, we need to reduce number of features from tfidf using SVD \n# Need to standardise \n\n#choosing 120 components for svd, 120 to 200 components are good to go for.\n\nsvd = decomposition.TruncatedSVD(n_components= 120)\nsvd.fit(xtrain_tfv)\nxtrain_svd = svd.transform(xtrain_tfv)\nxvalid_svd = svd.transform(xvalid_tfv)\n\nscl = preprocessing.StandardScaler()\nscl.fit(xtrain_svd)\nxtrain_svd_scl = scl.transform(xtrain_svd)\nxvalid_svd_scl = scl.transform(xvalid_svd)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Fitting svm\n\nclf = SVC(C=1.0, #Penalty parameter C of the error term.\n          probability= True) # since we need probability\nclf.fit(xtrain_svd_scl, ytrain)\npredictions = clf.predict_proba(xvalid_svd_scl)\nprint(\"Logloss : %0.3f\" %multiclass_logloss(yvalid, predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## 6th approach - xgboost on tfidf\nclf = xgb.XGBClassifier(max_depth = 7,\n                       n_estimators = 200, \n                       colsample_bytree = 0.8,\n                       subsample = 0.8,\n                       nthread = 10,\n                       learning_rate = 0.1)\nclf.fit(xtrain_tfv.tocsc(), ytrain)\npredictions = clf.predict_proba(xvalid_tfv.tocsc())\n\nprint(\"Logloss : %0.3f\" %multiclass_logloss(yvalid, predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## 7th approach - XGB on count vectorizer\n\nclf = xgb.XGBClassifier(max_depth = 7,\n                       n_estimators = 200, \n                       colsample_bytree = 0.8,\n                       subsample = 0.8,\n                       nthread = 10,\n                       learning_rate = 0.1)\nclf.fit(xtrain_ctv.tocsc(), ytrain)\npredictions = clf.predict_proba(xvalid_ctv.tocsc())\n\nprint(\"Logloss : %0.3f\" %multiclass_logloss(yvalid, predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## 8th approach - xgb with tfidf - svd features\n\nclf = xgb.XGBClassifier(max_depth = 7,\n                       n_estimators = 200, \n                       colsample_bytree = 0.8,\n                       subsample = 0.8,\n                       nthread = 10,\n                       learning_rate = 0.1)\nclf.fit(xtrain_svd, ytrain)\npredictions = clf.predict_proba(xvalid_svd)\n\nprint(\"Logloss : %0.3f\" %multiclass_logloss(yvalid, predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## 9th approach - simple xgboost - tfidf - svd\n\nclf = xgb.XGBClassifier( nthread = 10)\n                       \nclf.fit(xtrain_svd, ytrain)\npredictions = clf.predict_proba(xvalid_svd)\n\nprint(\"Logloss : %0.3f\" %multiclass_logloss(yvalid, predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"xgb seems not good..lets try hyperparameter tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"## hyperparameter tuning - 10th approach\n\n## Grid Search\n\n##creating scoring function\nmll_scorer = metrics.make_scorer(multiclass_logloss, greater_is_better = False, needs_proba = True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##pipeline construction with svd, standardscalar, logisticregression\n\nsvd = TruncatedSVD()\nscl = preprocessing.StandardScaler()\nlr_model = LogisticRegression()\n\nclf = pipeline.Pipeline([('svd',svd),\n                        ('scl',scl),\n                        ('lr_model',lr_model)])\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## grid parameters\n\nparam_grid = {'svd__n_components' : [120,180],\n             'lr_model__C':[0.1,1.0,10],\n             'lr_model__penalty':['l1','l2']}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Grid search model\nmodel = GridSearchCV(estimator = clf, scoring= mll_scorer, param_grid = param_grid,\n                    verbose = 10, #Controls the verbosity: the higher, the more messages.\n                    n_jobs = -1, #Number of jobs to run in parallel. None means 1 unless in a joblib.parallel_backend context. -1 means using all processors.\n                    iid = True, #If True, return the average score across folds, weighted by the number of samples in each test set.\n                    refit = True, #Refit an estimator using the best found parameters on the whole dataset.\n                    cv = 2 #Determines the cross-validation splitting strategy.  integer, to specify the number of folds in a (Stratified)KFold,\n                    )\n\nmodel.fit(xtrain_tfv, ytrain)\nprint(\"Best score: \", model.best_score_)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Best parameter set: \")\nbest_parameters = model.best_estimator_.get_params()\nfor param_name in sorted(param_grid.keys()):\n    print(\"\\t %s : %r \" %(param_name, best_parameters[param_name]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## hyperparameter tuning in Naive bayes - 11th approach\nnb_model = MultinomialNB()\n\n# create pipeline\nclf = pipeline.Pipeline([('nb',nb_model)])\nparam_grid = {'nb__alpha' : [0.001, 0.01, 0.1, 1, 10, 100]}\n\nmodel = GridSearchCV(estimator = clf, scoring = mll_scorer, param_grid = param_grid,\n                    verbose = 10, n_jobs = -1, iid = True, refit = True, cv = 2)\n\nmodel.fit(xtrain_tfv, ytrain)\nprint(\"Best score: %0.3f\" %model.best_score_)\nprint(\"Best parameter set:\")\nbest_parameters = model.best_estimator_.get_params()\nfor param_name in sorted(param_grid.keys()):\n    print(\"\\t %s : %r \" %(param_name, best_parameters[param_name]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# word vectors \n#Loading glove embeddings\nembedding_index = {}\nf = open('../input/glove840b300dtxt/glove.840B.300d.txt')\nfor line in tqdm(f):\n    values = line.split()\n    word = values[0]\n    try:\n        coefs = np.asarray(values[1:], dtype = 'float32')\n    except ValueError as v:\n        pass\n    embedding_index[word] = coefs\nf.close()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Found %s vectors\" %(len(embedding_index)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## function for creating normalized vectors for sentence\n\ndef sent2vec(sent):\n    words = str(sent).lower()\n    word = word_tokenize(words)\n    word = [w for w in word if w not in stop_words]\n    word = [w for w in word if w.isalpha()]\n    M=[]\n    for w in word:\n        try:\n            M.append(embedding_index[w])\n        except:\n            continue\n    M = np.array(M)\n    v = M.sum(axis = 0)\n    if type(v) != np.ndarray:\n        return np.zeros(300)\n    return v / np.sqrt((v**2).sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating sentence vectors for train and test dataset\n\nxtrain_glove = [ sent2vec(x) for x in tqdm(xtrain)]\nxvalid_glove = [sent2vec(x) for x in tqdm(xvalid)]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xtrain_glove = np.array(xtrain_glove)\nxvalid_glove = np.array(xvalid_glove)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# - 11th approach -simple xgboost on glove embeddings\n\nclf = xgb.XGBClassifier(nthread = 10, silent = False)\nclf.fit(xtrain_glove, ytrain)\npredictions = clf.predict_proba(xvalid_glove)\nprint(\"Logloss : %0.3f\" %multiclass_logloss(yvalid, predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 12th approach -  Fitting xgboost with parameters to glove embeddings\nclf = xgb.XGBClassifier(max_depth =7, n_estimators = 200, colsample_bytree= 0.8, subsample=0.8,\n                       nthread =10, learning_rate = 0.1, silent = False)\nclf.fit(xtrain_glove, ytrain)\npredictions = clf.predict_proba(xvalid_glove)\nprint(\"Logloss : %0.3f\" %multiclass_logloss(yvalid, predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Deep learning \n#scale data before neural network\n\n# 13th approach - Dense neural network\n\nscl = preprocessing.StandardScaler()\nxtrain_glove_scl = scl.fit_transform(xtrain_glove)\nxvalid_glove_scl = scl.fit_transform(xvalid_glove)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Binarize the output for neural network\nytrain_enc = np_utils.to_categorical(ytrain)\nyvalid_enc = np_utils.to_categorical(yvalid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\n\nmodel.add(Dense(300, input_dim = 300, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(BatchNormalization())\n\nmodel.add(Dense(300, activation ='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(BatchNormalization())\n\nmodel.add(Dense(3))\nmodel.add(Activation('softmax'))\n\nmodel.compile(loss ='categorical_crossentropy', optimizer='adam')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(xtrain_glove_scl, y = ytrain_enc, batch_size = 64, epochs = 5, verbose =1,\n              validation_data= (xvalid_glove_scl, yvalid_enc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 14th approach - LSTM with keras tokenizer\n\n#using keras tokenizer\ntoken = text.Tokenizer(num_words= None)\nmax_len = 70\n\ntoken.fit_on_texts(list(xtrain)+list(xvalid))\nxtrain_seq = token.texts_to_sequences(xtrain)\nxvalid_seq = token.texts_to_sequences(xvalid)\n\nxtrain_pad = sequence.pad_sequences(xtrain_seq, maxlen = max_len)\nxvalid_pad = sequence.pad_sequences(xvalid_seq, maxlen = max_len)\n\nword_index = token.word_index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## create embedding matrix\nembedding_matrix  = np.zeros((len(word_index)+1, 300))\nfor word, i in tqdm(word_index.items()):\n    embedding_vector = embedding_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##LSTM with glove embeddings\n\nmodel = Sequential()\nmodel.add(Embedding(len(word_index)+1 , 300, \n                    weights = [embedding_matrix],\n                    trainable = False,\n                    input_length = max_len\n                   ))\nmodel.add(SpatialDropout1D(0.3))\nmodel.add(LSTM(100, dropout=0.3, recurrent_dropout = 0.3))\n\nmodel.add(Dense(1024, activation ='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(1024, activation ='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(3))\nmodel.add(Activation('softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer = 'adam')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# commenting this as this will take lot of time,..as next step adding early stopping to stop the iteration if not improvement in validation loss.\n#model.fit(xtrain_pad, y = ytrain_enc, batch_size = 512, epochs = 100, verbose = 1, validation_data = (xvalid_pad, yvalid_enc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Adding earlystopping in the same model\n\nmodel = Sequential()\nmodel.add(Embedding(len(word_index)+1 , 300,\n                   weights = [embedding_matrix],\n                   input_length = max_len,\n                   trainable = False))\nmodel.add(SpatialDropout1D(0.3))\nmodel.add(LSTM(300, dropout = 0.3, recurrent_dropout = 0.3))\n\nmodel.add(Dense(1024, activation ='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(1024, activation = 'relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(3))\nmodel.add(Activation('softmax'))\n\nmodel.compile(loss = 'categorical_crossentropy', optimizer = 'adam')\n\n## Fit the model with earlystopping callback\n\nearlystop = EarlyStopping(monitor ='val_loss', min_delta = 0, patience = 3, verbose= 0, mode = 'auto')\n\nmodel.fit(xtrain_pad, ytrain_enc, batch_size = 512, epochs = 100, validation_data = (xvalid_pad, yvalid_enc),\n         callbacks = [earlystop])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## 15th approach- Bidirectional LSTM with glove embeddings \n\nmodel = Sequential()\nmodel.add(Embedding(len(word_index)+1, 300,\n                   weights = [embedding_matrix],\n                   trainable = False, \n                   input_length = max_len))\nmodel.add(SpatialDropout1D(0.3))\n\nmodel.add(Bidirectional(LSTM(300, dropout = 0.3, recurrent_dropout = 0.3)))\nmodel.add(Dense(1024, activation = 'relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(1024, activation ='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(3))\nmodel.add(Activation('softmax'))\n\nmodel.compile(loss = 'categorical_crossentropy', optimizer = 'adam')\n\nearlystop = EarlyStopping(monitor ='val_loss', min_delta = 0, patience =3, verbose =0, mode = 'auto')\n\nmodel.fit(xtrain_pad, ytrain_enc, batch_size = 512, epochs = 100, verbose =0, callbacks = [earlystop])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## 16th approach - 2 layers of GRU\nmodel = Sequential()\nmodel.add(Embedding(len(word_index)+1, 300,\n                   weights = [embedding_matrix],\n                   trainable = False, \n                   input_length = max_len))\nmodel.add(SpatialDropout1D(0.3))\n\nmodel.add(GRU(300, dropout = 0.3, recurrent_dropout = 0.3, return_sequences = True))\nmodel.add(GRU(300, dropout =0.3, recurrent_dropout = 0.3))\n\nmodel.add(Dense(1024, activation = 'relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(1024, activation ='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(3))\nmodel.add(Activation('softmax'))\n\nmodel.compile(loss = 'categorical_crossentropy', optimizer = 'adam')\n\nearlystop = EarlyStopping(monitor ='val_loss', min_delta = 0, patience =3, verbose =0, mode = 'auto')\n\nmodel.fit(xtrain_pad, ytrain_enc, batch_size = 512, epochs = 100, verbose =0, callbacks = [earlystop])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Reference: https://www.kaggle.com/abhishek/approaching-almost-any-nlp-problem-on-kaggle/data\n"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}