{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing\n\nfrom keras.models import Sequential\nfrom keras.layers.recurrent import LSTM\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers import Bidirectional, SpatialDropout1D\nfrom keras.optimizers import SGD,Adam\nfrom keras.layers.core import Dense,Activation,Dropout\nfrom keras.utils import np_utils\nfrom keras.callbacks import EarlyStopping\nfrom keras.preprocessing import sequence,text\nfrom keras.callbacks import Callback,EarlyStopping,ReduceLROnPlateau\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import zipfile\n\ntrain_zip = zipfile.ZipFile('/kaggle/input/spooky-author-identification/train.zip')\ntrain_df = pd.read_csv(train_zip.open('train.csv'))\n\ntest_zip = zipfile.ZipFile('/kaggle/input/spooky-author-identification/test.zip')\ntest_df = pd.read_csv(test_zip.open('test.csv'))\n\nsample_zip = zipfile.ZipFile('/kaggle/input/spooky-author-identification/sample_submission.zip')\nsample_df = pd.read_csv(sample_zip.open('sample_submission.csv'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check authors\ntrain_df.author.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check what is the maximum and the minimum length of text\nprint('max: ',len(train_df.text.max()))\nprint('min: ',len(train_df.text.min()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print the max and min length text\nprint('max: ',train_df.text.max())\nprint('min',train_df.text.min())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#convert the authors/labels into one hot encoded values\nlbl_enc = LabelEncoder()\ny = lbl_enc.fit_transform(train_df.author.values)\ny = np_utils.to_categorical(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#divide the data into train and validation \nx_train, x_valid, y_train, y_valid = train_test_split(train_df.text.values,\n                                                      y,\n                                                      stratify = y,\n                                                     random_state = 2018,\n                                                     test_size = 0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#use Keras Tokenizer to tokenize the texts\ntoken = text.Tokenizer(num_words = None)\nmax_len = 80\n\ntoken.fit_on_texts(list(x_train) + list(x_valid))\nxtrain_seq = token.texts_to_sequences(x_train)\nxvalid_seq = token.texts_to_sequences(x_valid)\n\nprint(xtrain_seq[:1])\n# zero pad the sequences\nxtrain_pad = sequence.pad_sequences(xtrain_seq, maxlen=max_len,padding = 'post')\nxvalid_pad = sequence.pad_sequences(xvalid_seq, maxlen=max_len,padding = 'post')\nprint(xtrain_pad[:1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_index = token.word_index\ndef get_model():\n    model = Sequential()\n    model.add(Embedding(len(word_index) + 1,\n                         300,\n                         input_length=max_len))\n    model.add(SpatialDropout1D(0.3))\n    model.add(Bidirectional(LSTM(300, dropout=0.3, recurrent_dropout=0.3,return_sequences = True)))\n    model.add(Bidirectional(LSTM(300, dropout=0.3, recurrent_dropout=0.3)))\n\n    model.add(Dense(512, activation='relu'))\n    model.add(Dropout(0.5))\n\n    model.add(Dense(512, activation='relu'))\n    model.add(Dropout(0.5))\n\n    model.add(Dense(3))\n    model.add(Activation('softmax'))\n    adam = Adam(lr=0.01, decay = 0.05)\n    model.compile(loss='categorical_crossentropy', optimizer=adam,\n                 metrics=['accuracy'])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Callback for loss logging per epoch\nclass LossHistory(Callback):\n    def on_train_begin(self, logs={}):\n        self.losses = []\n        self.val_losses = []\n        self.acc = []\n        self.val_acc = []\n        \n    def on_epoch_end(self, batch, logs={}):\n        self.losses.append(logs.get('loss'))\n        self.val_losses.append(logs.get('val_loss'))\n        self.acc.append(logs.get('acc'))\n        self.val_acc.append(logs.get('val_acc'))\n\nhistory = LossHistory()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_loss():\n    #plot training curve\n    loss = history.losses\n    val_loss = history.val_losses\n    acc = history.acc\n    val_acc = history.val_acc\n\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.title('Acc - Loss Trend')\n    plt.plot(loss, 'blue', label='Training Loss')\n    plt.plot(val_loss, 'green', label='Validation Loss')\n    plt.plot(acc, 'black', label='Training Accuracy')\n    plt.plot(val_acc, 'red', label='Validation Accuracy')\n    plt.xticks(range(0,10)[0::2])\n    plt.legend()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"earlystop = EarlyStopping(monitor='val_loss', patience=6, verbose=0, mode='auto')\nreduceLR = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1, mode='min', epsilon=0.0001, min_lr=0.0000001)\n\nmodel = get_model()\nmodel.fit(xtrain_pad, y=y_train, batch_size=64, epochs=70, verbose=1, \n          validation_data=(xvalid_pad, y_valid), callbacks=[earlystop,history,reduceLR])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_loss()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#the same operations are also carried out on the test set\nxtest_seq = token.texts_to_sequences(test_df.text)\nxtest_pad = sequence.pad_sequences(xtest_seq,maxlen = max_len,padding = 'post')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now predict\nprediction = model.predict(xtest_pad)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a2c = {'EAP': 0, 'HPL' : 1, 'MWS' : 2}\nresult = sample_df\nfor a, i in a2c.items():\n    result[a] = prediction[:, i]\n    \nresult.to_csv('lstmsubmission2.csv',index = False)\nresult.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"result.head(10)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}