{"cells":[{"metadata":{},"cell_type":"markdown","source":"Just a few minutes back, came across `Philip Vollet's` post on NLP Profiler who describes it has \n> **Simple NLP library that allows profiling datasets with one or more text columns. When given a dataset and a column name containing text data, NLP Profiler will return either high-level insights or low-level /granular statistical information about the text in that column.**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"As per the developer repo, `NLP PROFILER` would be the describe() function for text data and pretty much more! <br>\nNLP Profiler will return either high-level insights or low-level/granular statistical information about the dataframe text column. Ok thats cool!","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"I am yet to explore more on this and this is a very basic notebook that follows pretty much similar approach to the starter notebooks shared in the developer's page! <br>\n\nDo check out the Repo and the starter notebook here - [nlp_profiler](https://github.com/neomatrix369/nlp_profiler) and\n[Notebook](https://github.com/neomatrix369/nlp_profiler/blob/master/notebooks/google-colab/nlp_profiler.ipynb)","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#Importing the libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport string\nimport time","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#To ignore warning messages\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Pulling the dataset\ndf = pd.read_csv(\"/kaggle/input/spooky-author-identification/train.zip\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#To display the full text column instead of truncating one\npd.set_option('display.max_colwidth', -1)\n#To display a maximum of 100 columns\npd.set_option('display.max_columns',100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are having 19579 records in hand. We will try to cut short and go with just 100 records for our easiness.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Retaining just first 100 records\ndf = df[:100]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Python provides a constant called string.punctuation that provides a great list of punctuation characters. \nprint(string.punctuation)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_punctuations(input_col):\n    \"\"\"To remove all the punctuations present in the text.Input the text column\"\"\"\n    table = str.maketrans('','',string.punctuation)\n    return input_col.translate(table)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Applying the remove_punctuation function\ndf['text'] = df['text'].apply(remove_punctuations)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Pip installing the NLP Profiler directly from the GitHub repo\n !pip install git+https://github.com/neomatrix369/nlp_profiler.git@master","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install git+https://github.com/neomatrix369/nlp_profiler.git@master","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Importing the apply_text_profiling\nfrom nlp_profiler.core import apply_text_profiling","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Applying on the text column of the dataframe\n#Official git mentions Pandas dataframe series as input param to be passed\nstart = time.time()\nprofiled_df = apply_text_profiling(df,'text')\nend = time.time()\ntotal_time = end - start / 60*60\nprint(\"Time taken(in secs) for the apply_text_profiling to run on 100 records: \",total_time)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"profiled_df.head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can observe now what all features `apply_text_profiling` function as got!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"profiled_df.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Sentiment Polarity, Subjectivity and spelling quality would be some great features to check on top of our text data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Hist plot for the sentiment polarity for the first 100 sentences\nprofiled_df['sentiment_polarity'].hist()\nplt.title(\"Sentiment Polarity\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"OK! So apply_text_profiling is saying most of the sentences by spooky authors are pretty positive sentences! BTW dont forget we are running this just on 100 sentences, not the entire train set!! <b>\nI feel sentiment Polarity will surely help in getting a gist on the underlying data!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Subjective or Objective sentence\nprofiled_df['sentiment_subjectivity_summarised'].hist()\nplt.title(\"Sentiment Subjectivity\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Pretty Much Subjective sentences!!!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Histogram on the words_count\nprofiled_df['words_count'].hist()\nplt.title(\"Word Count Distribution with NLP_Profiler\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Average stop word count with the sentences\nprofiled_df['stop_words_count'].mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(profiled_df[['sentiment_polarity_score','sentiment_subjectivity_score']].corr(),annot=True,cmap='Blues')\nplt.title(\"Correlation Between Sentiment Polarity and Sentiment Subjectivity\")\nplt.xticks(rotation=45)\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}