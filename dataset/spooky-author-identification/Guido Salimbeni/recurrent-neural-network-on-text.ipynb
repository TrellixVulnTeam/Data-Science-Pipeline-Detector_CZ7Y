{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"# Read the text column and convert into a text corpus"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\ndf = pd.read_csv(\"/kaggle/input/spooky-author-identification/train.csv\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text = []\n\nfor row in df[\"text\"][df[\"author\"] == \"EAP\"]:\n    text.append(str(row))\n    \ncorpusEAP = \" \".join(text)\ncorpusEAP = corpusEAP[0:100000]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Tokenize and Clean Text"},{"metadata":{"trusted":true},"cell_type":"code","source":"len(corpusEAP)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import spacy\nnlp = spacy.load('en',disable=['parser', 'tagger','ner']) # only for tokenisation\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def separate_punc(doc_text):\n    return [token.text.lower() for token in nlp(doc_text) if token.text not in '\\n\\n \\n\\n\\n!\"-#$%&()--.*+,-/:;<=>?@[\\\\]^_`{|}~\\t\\n ']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntokens = separate_punc(corpusEAP)\nlen(tokens)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Create Sequences of Tokens"},{"metadata":{"trusted":true},"cell_type":"code","source":"# organize into sequences of tokens\ntrain_len = 25+1\n\n# Empty list of sequences\ntext_sequences = []\n\nfor i in range(train_len, len(tokens)):\n    \n    # Grab train_len# amount of characters\n    seq = tokens[i-train_len:i]\n    \n    # Add to list of sequences\n    text_sequences.append(seq)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print (' '.join(text_sequences[0]))\nprint (' '.join(text_sequences[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(text_sequences)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Keras Tokenization"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\n# integer encode sequences of words\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(text_sequences)\nsequences = tokenizer.texts_to_sequences(text_sequences)\nprint (sequences[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print (tokenizer.index_word)\nprint ()\nprint (\" --------------------- \")\nprint (len(tokenizer.word_counts))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Convert to Numpy Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"sequences = np.array(sequences)\nsequences[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train / Test Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = sequences[:,:-1]\ny = sequences[:,-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print (X.shape, y.shape)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# LSTM based model\n\n* LSTM layer take a 3D tensor with shape (batch_size, timesteps, input_dim). \n* in this case the correct shape is generated from the embedding layer"},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense,LSTM,Embedding\n\nvocabulary_size = len(tokenizer.word_counts)\nvocabulary_size = vocabulary_size + 1\nseq_len = X.shape[1]\n\nmodel = Sequential()\nmodel.add(Embedding(vocabulary_size, 25, input_length=seq_len))\nmodel.add(LSTM(150, return_sequences=True)) # to stack LSTM we need return seq \nmodel.add(LSTM(150))\nmodel.add(Dense(150, activation='relu'))\n\nmodel.add(Dense(vocabulary_size, activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* How embedding works in Keras\n* input_dim: int > 0. Size of the vocabulary, i.e. maximum integer index + 1.\n* output_dim: int >= 0. Dimension of the dense embedding. indicates the size of the embedding vectors\n\neach input integer is used as the index to access a table that contains all posible vectors. That is the reason why it needs to specify the size of the vocabulary as the first argument (so the table can be initialized).Once the network has been trained, we can get the weights of the embedding layer,and can be thought as the table used to map integers to embedding vectors.the underlying automatic differentiation engines (e.g., Tensorflow or Theano) manage to optimize these vectors associated to each input integer just like any other parameter.\n\n* it also possible to use pretained embedding https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_test = Sequential()\nmodel_test.add(Embedding(input_dim = vocabulary_size, output_dim = 2, input_length=seq_len))\nmodel_test.compile('rmsprop', 'mse')\noutput_array = model_test.predict(X)\nprint (output_array.shape)\nout = pd.DataFrame(output_array[0])\nout.head()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.utils import to_categorical\ny = to_categorical(y, num_classes=vocabulary_size)\n# fit model\nmodel.fit(X, y, batch_size=256, epochs=100,verbose=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Generating New Text"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.sequence import pad_sequences\ndef generate_text(model, tokenizer, seq_len, seed_text, num_gen_words):\n    '''\n    INPUTS:\n    model : model that was trained on text data\n    tokenizer : tokenizer that was fit on text data\n    seq_len : length of training sequence\n    seed_text : raw string text to serve as the seed\n    num_gen_words : number of words to be generated by model\n    '''\n    \n    # Final Output\n    output_text = []\n    \n    # Intial Seed Sequence\n    input_text = seed_text\n    \n    # Create num_gen_words\n    for i in range(num_gen_words):\n        \n        # Take the input text string and encode it to a sequence\n        encoded_text = tokenizer.texts_to_sequences([input_text])[0]\n        \n        # Pad sequences to our trained rate \n        pad_encoded = pad_sequences([encoded_text], maxlen=seq_len, truncating='pre')\n        \n        # Predict Class Probabilities for each word\n        pred_word_ind = model.predict_classes(pad_encoded, verbose=0)[0]\n        \n        # Grab word\n        pred_word = tokenizer.index_word[pred_word_ind] \n        \n        # Update the sequence of input text (shifting one over with the new word)\n        input_text += ' ' + pred_word\n        \n        output_text.append(pred_word)\n        \n    # Make it look like a sentence.\n    return ' '.join(output_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_sequences[100]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nseed_text = ' '.join(text_sequences[100])\ngenerate_text(model,tokenizer,seq_len,seed_text=seed_text,num_gen_words=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Classification"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n\n### augument the data for EAP"},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\n\ndf_augumented = pd.DataFrame(columns=['text', 'author'])\n\nfor i in range (200):\n    random_pick = random.randint(0,len(text_sequences))\n    seed_text = ' '.join(text_sequences[random_pick])\n    text = generate_text(model,tokenizer,seq_len,seed_text=seed_text,num_gen_words=20)\n    df_augumented[\"text\"]\n    df_augumented = df_augumented.append({'text': text, \"author\" : \"EAP\"}, ignore_index=True)\n    \ndf_augumented.head()    \n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## concat"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_prev = df[[\"text\", \"author\"]]\ndata = pd.concat([data_prev, df_augumented], axis = 0)\ndata.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['author_num'] = data[\"author\"].map({'EAP':0, 'HPL':1, 'MWS':2})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# X and y and Multinomial Naive Bayes\n\n* with augumented data"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data['text']\ny = data['author_num']\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\nfrom sklearn.feature_extraction.text import CountVectorizer\nvect = CountVectorizer(stop_words = 'english')\nX_train_matrix = vect.fit_transform(X_train) \nfrom sklearn.naive_bayes import MultinomialNB\nclf=MultinomialNB()\nclf.fit(X_train_matrix, y_train)\nprint(clf.score(X_train_matrix, y_train))\nX_test_matrix = vect.transform(X_test) \nprint (clf.score(X_test_matrix, y_test))\npredicted_result=clf.predict(X_test_matrix)\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test,predicted_result))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* in my previous notebook https://www.kaggle.com/guidosalimbeni/multinomial-naive-bayes the recall for EAP was lower ... so maybe it worked to augument the data "},{"metadata":{},"cell_type":"markdown","source":"# Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data['text']\ny = data['author_num']\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nvect = CountVectorizer(stop_words = 'english')\nX_train_matrix = vect.fit_transform(X) \nfrom sklearn.naive_bayes import MultinomialNB\nclf=MultinomialNB()\nclf.fit(X_train_matrix, y)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv(\"/kaggle/input/spooky-author-identification/test.csv\")\ntest_matrix = vect.transform(test[\"text\"])\npredicted_result = clf.predict_proba(test_matrix)\nresult=pd.DataFrame()\nresult[\"id\"]=test[\"id\"]\nresult[\"EAP\"]=predicted_result[:,0]\nresult[\"HPL\"]=predicted_result[:,1]\nresult[\"MWS\"]=predicted_result[:,2]\nresult.head()\nresult.to_csv(\"submission_v5.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}