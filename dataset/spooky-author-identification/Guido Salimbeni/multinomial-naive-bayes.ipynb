{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#READING INPUT\ndata = pd.read_csv(\"/kaggle/input/spooky-author-identification/train.csv\")\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we map \"EAP\" to 0 \"HPL\" to 1 and \"MWS\" to 2 as it will be more convenient for our classifier. \nIn other words we are just telling our computer that if classifier predicts 0 for the text then it means that it is preicting \"EAP\", if 1 then it means that it is predicting \"HPL\", if 2 then it means that it is predicting \"MWS\"."},{"metadata":{"trusted":true},"cell_type":"code","source":"data['author_num'] = data[\"author\"].map({'EAP':0, 'HPL':1, 'MWS':2})\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Define X and y"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data['text']\ny = data['author_num']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Split training and test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Vectorisation\n\n#### Count Vectorizer: builds a dictionary of features and transforms documents to feature vectors.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* example\n* below: the word \"life\" has been found 2 times in sentence 0 and in sentence 1\n* the word paul has been found 1 time in sentence 0 and 0 times in sentence 1\n* and so on..."},{"metadata":{"trusted":true},"cell_type":"code","source":"text=[\"My name is Paul my life is Jane! And we live our life together\" , \"My name is Guido my life is Victoria! And we live our life together\"]\ntoy = CountVectorizer(stop_words = 'english')\ntoy.fit_transform(text)\nmatrix = toy.transform(text)\nfeatures = toy.get_feature_names()\ndf_res = pd.DataFrame(matrix.toarray(), columns=features)\ndf_res","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vect = CountVectorizer(stop_words = 'english')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_matrix = vect.fit_transform(X_train) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model 1 with count vectorizer"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nclf=MultinomialNB()\nclf.fit(X_train_matrix, y_train)\nprint(clf.score(X_train_matrix, y_train))\nX_test_matrix = vect.transform(X_test) \nprint (clf.score(X_test_matrix, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_result=clf.predict(X_test_matrix)\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test,predicted_result))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Tf-idf: \n\n* Since longer documents will have higher average count values than shorter documents, even though they might talk about the same topics, we can divide the number of occurrences of each word in a document by the total number of words in the document: **tf** for Term Frequencies.\n\n* **idf** for “Term Frequency times Inverse Document Frequency” : Downscale weights for words that occur in many documents in the corpus and are therefore less informative than those that occur only in a smaller portion of the corpus.\n\n* CountVectorizer and TfidTransformer steps into one using [TfidVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html):"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer(stop_words = 'english')\n\nX_train_tfidf = vectorizer.fit_transform(X_train) \nX_train_tfidf.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model 2 with TfidVectorizer"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nclf2=MultinomialNB()\nclf2.fit(X_train_tfidf, y_train)\nprint(clf2.score(X_train_tfidf, y_train))\nX_test_tfidf = vectorizer.transform(X_test) \nprint (clf2.score(X_test_tfidf, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* it doesn't perform better in term of accuracy"},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_result_2=clf2.predict(X_test_tfidf)\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test,predicted_result_2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* there might be something to learn from the predictions on class 2"},{"metadata":{},"cell_type":"markdown","source":"# Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"sample = pd.read_csv(\"/kaggle/input/spooky-author-identification/sample_submission.csv\")\nsample.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv(\"/kaggle/input/spooky-author-identification/test.csv\")\ntest_matrix = vect.transform(test[\"text\"])\npredicted_result = clf.predict_proba(test_matrix)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result=pd.DataFrame()\nresult[\"id\"]=test[\"id\"]\nresult[\"EAP\"]=predicted_result[:,0]\nresult[\"HPL\"]=predicted_result[:,1]\nresult[\"MWS\"]=predicted_result[:,2]\nresult.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result.to_csv(\"submission_v1.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}