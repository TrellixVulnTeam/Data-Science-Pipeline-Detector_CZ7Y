{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport spacy\nimport seaborn as sns\nimport string","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"markdown","source":"### Logloss for this competition\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def multiclass_logloss(actual, predicted, eps=1e-15):\n    \"\"\"Multi class version of Logarithmic Loss metric.\n    :param actual: Array containing the actual target classes\n    :param predicted: Matrix with class predictions, one probability per class\n    \"\"\"\n    # Convert 'actual' to a binary array if it's not already:\n    if len(actual.shape) == 1:\n        actual2 = np.zeros((actual.shape[0], predicted.shape[1]))\n        for i, val in enumerate(actual):\n            actual2[i, val] = 1\n        actual = actual2\n\n    clip = np.clip(predicted, eps, 1 - eps)\n    rows = actual.shape[0]\n    vsota = np.sum(actual * np.log(clip))\n    return -1.0 / rows * vsota","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* https://spacy.io/\nspaCy is the best way to prepare text for deep learning. It interoperates seamlessly with TensorFlow, PyTorch, scikit-learn, Gensim and the rest of Python's awesome AI ecosystem. With spaCy, you can easily construct linguistically sophisticated statistical models for a variety of NLP problems."},{"metadata":{"trusted":true},"cell_type":"code","source":"nlp = spacy.load(\"en_core_web_sm\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#READING INPUT\ndata = pd.read_csv(\"/kaggle/input/spooky-author-identification/train.csv\")\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv(\"/kaggle/input/spooky-author-identification/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Tokenisation\n\n* the process of breaking up the original text into components (tokens)"},{"metadata":{"trusted":true},"cell_type":"code","source":"doc = nlp(data[\"text\"][0])\nfor token in doc[0:5]:\n    print(token.text, token.pos , token.pos_, token.dep_) # part of speach and syntax dependency","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Lemmatisation\n\n* in linguistics is the process of grouping together the inflected forms of a word so they can be analysed as a single item, identified by the word's lemma, or dictionary form."},{"metadata":{"trusted":true},"cell_type":"code","source":"for token in doc[0:5]:\n    print(token.text,  token.pos_, token.lemma_) # part of speach and syntax dependency","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# target"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(x=['Edgar Allen Poe', 'Mary Wollstonecraft Shelley', 'H.P. Lovecraft'], y=data['author'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['author_num'] = data[\"author\"].map({'EAP':0, 'HPL':1, 'MWS':2})\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we map \"EAP\" to 0 \"HPL\" to 1 and \"MWS\" to 2 as it will be more convenient for our classifier. \nIn other words we are just telling our computer that if classifier predicts 0 for the text then it means that it is preicting \"EAP\", if 1 then it means that it is predicting \"HPL\", if 2 then it means that it is predicting \"MWS\"."},{"metadata":{},"cell_type":"markdown","source":"# Meta features\n\n* features that are extracted from the text like number of words, number of stop words, number of punctuations etc Number of words in the text\n* Number of unique words in the text\n* Number of characters in the text\n* Number of stopwords\n* Number of punctuations\n* Number of upper case words\n* Number of title case words\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords\nstopwords = stopwords.words('english')\nprint (stopwords)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Number of words in the text ##\ndata[\"num_words\"] = data[\"text\"].apply(lambda x: len(str(x).split()))\ntest[\"num_words\"] = test[\"text\"].apply(lambda x: len(str(x).split()))\n## Number of unique words in the text ##\ndata[\"num_unique_words\"] = data[\"text\"].apply(lambda x: len(set(str(x).split())))\ntest[\"num_unique_words\"] = test[\"text\"].apply(lambda x: len(set(str(x).split())))\n## Number of characters in the text ##\ndata[\"num_chars\"] = data[\"text\"].apply(lambda x: len(str(x)))\ntest[\"num_chars\"] = test[\"text\"].apply(lambda x: len(str(x)))\n## Number of stopwords in the text ##\ndata[\"num_stopwords\"] = data[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in stopwords]))\ntest[\"num_stopwords\"] = test[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in stopwords]))\n## Number of punctuations in the text ##\ndata[\"num_punctuations\"] =data['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\ntest[\"num_punctuations\"] =test['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n## Number of title case words in the text ##\ndata[\"num_words_upper\"] = data[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\ntest[\"num_words_upper\"] = test[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n## Number of title case words in the text ##\ndata[\"num_words_title\"] = data[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\ntest[\"num_words_title\"] = test[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n## Max length of the words in the text ##\ndata[\"max_word_len\"] = data[\"text\"].apply(lambda x: np.max([len(w) for w in str(x).split()]))\ntest[\"max_word_len\"] = test[\"text\"].apply(lambda x: np.max([len(w) for w in str(x).split()]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Spacy\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n# Define function to cleanup text by removing personal pronouns, stopwords, and puncuation\ndef cleanup_text(docs, logging=False):\n    texts = []\n    counter = 1\n    for doc in docs:\n        if counter % 1000 == 0 and logging:\n            print(\"Processed %d out of %d documents.\" % (counter, len(docs)))\n        counter += 1\n        doc = nlp(doc, disable=['parser', 'ner'])\n        tokens = [tok.lemma_.lower().strip() for tok in doc if tok.lemma_ != '-PRON-']\n        tokens = [tok for tok in tokens if tok not in stopwords and tok not in string.punctuation]\n        #tokens = [tok for tok in tokens if tok not in punctuations]\n        tokens = ' '.join(tokens)\n        texts.append(tokens)\n    return pd.Series(texts)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Original training data shape: ', data['text'].shape)\ndata[\"text_cleaned\"]= cleanup_text(data['text'], logging=True)\nprint('Cleaned up training data shape: ', data[\"text_cleaned\"].shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Original training data shape: ', test['text'].shape)\ntest[\"text_cleaned\"] = cleanup_text(test['text'], logging=True)\nprint('Cleaned up training data shape: ', test[\"text_cleaned\"].shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## feature extraction on Spacy"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndata[\"num_unique_words_clenaed\"] = data[\"text_cleaned\"].apply(lambda x: len(set(str(x).split())))\ntest[\"num_unique_words_cleaned\"] = test[\"text_cleaned\"].apply(lambda x: len(set(str(x).split())))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def numberOfADV(docs, logging=False):\n    numberOfADV = []\n    counter = 1\n    for doc in docs:\n        if counter % 1000 == 0 and logging:\n            print(\"Processed %d out of %d documents.\" % (counter, len(docs)))\n        counter += 1\n        doc = nlp(doc, disable=['parser', 'ner'])\n        tokens = [tok.lemma_.lower().strip() for tok in doc if tok.pos_ == 'ADP']\n        #tokens = [tok for tok in tokens if tok not in stopwords and tok not in string.punctuation]\n        #tokens = [tok for tok in tokens if tok not in punctuations]\n        #tokens = ' '.join(tokens)\n        \n        numberOfADV.append(len(tokens))\n    return pd.Series(numberOfADV)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"num_of_ADV\"] = numberOfADV(data['text_cleaned'], logging=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test[\"num_of_ADV\"] = numberOfADV(test['text_cleaned'], logging=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def numberOfADJ(docs, logging=False):\n    numberOfADV = []\n    counter = 1\n    for doc in docs:\n        if counter % 1000 == 0 and logging:\n            print(\"Processed %d out of %d documents.\" % (counter, len(docs)))\n        counter += 1\n        doc = nlp(doc, disable=['parser', 'ner'])\n        tokens = [tok.lemma_.lower().strip() for tok in doc if tok.pos_ == 'ADJ']\n        \n        numberOfADV.append(len(tokens))\n    return pd.Series(numberOfADV)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"num_of_ADJ\"] = numberOfADJ(data['text_cleaned'], logging=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test[\"num_of_ADJ\"] = numberOfADJ(test['text_cleaned'], logging=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(x= data[\"author\"], y = data[\"num_of_ADJ\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(x= data[\"author\"], y = data[\"num_of_ADV\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Vectorisation"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vect = CountVectorizer()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_matrix = vect.fit_transform(data[\"text_cleaned\"]) \nX_test_matrix = vect.transform(test[\"text_cleaned\"]) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = vect.get_feature_names()\ndf_X_train_matrix = pd.DataFrame(X_train_matrix.toarray(), columns=features)\ndf_X_train_matrix.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_X_test_matrix = pd.DataFrame(X_test_matrix.toarray(), columns=features)\ndf_X_test_matrix.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Concatenate"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_df = data.drop([\"id\",\"text\", \"text_cleaned\", \"author\"], axis = 1)\n\ndf_train = pd.concat([data_df, df_X_train_matrix], axis=1)\n\ntest_df = test.drop([\"id\",\"text\", \"text_cleaned\"], axis = 1)\n\ndf_test = pd.concat([test_df, df_X_test_matrix], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# X and y"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df_train.drop(\"author_num\", axis = 1)\ny = data['author_num']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Split training and test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, stratify = y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nclf=MultinomialNB()\nclf.fit(X_train, y_train)\nprint(clf.score(X_train, y_train))\n\nprint (clf.score(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_result=clf.predict(X_test)\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test,predicted_result))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = clf.predict_proba(X_test)\n\nprint (\"logloss: %0.3f \" % multiclass_logloss(y_test, predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"sample = pd.read_csv(\"/kaggle/input/spooky-author-identification/sample_submission.csv\")\nsample.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\npredicted_result = clf.predict_proba(df_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result=pd.DataFrame()\nresult[\"id\"]=test[\"id\"]\nresult[\"EAP\"]=predicted_result[:,0]\nresult[\"HPL\"]=predicted_result[:,1]\nresult[\"MWS\"]=predicted_result[:,2]\nresult.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result.to_csv(\"submission_v3.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":1}