{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Spooky Classification for 3 Authors\nBy : Hesham Asem\n\n_____\n\nhere we have 3 kinds of phrases , for 3 fammous authors (Edgar Allan Poe  , HP Lovecraft , Mary Wollstonecraft Shelley) , & we need to build a model which is able to know the author depend on the phrase \n\nlet's first import needed libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set(style=\"whitegrid\")\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom nltk.corpus import stopwords\nfrom wordcloud import WordCloud\nimport collections\nimport spacy\nnlp = spacy.load('en_core_web_sm')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_selection import SelectPercentile , chi2\nfrom sklearn.naive_bayes import MultinomialNB","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"then to read the training data"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/spooky-author-identification/train/train.csv')  \nprint(f'Data Shape is {data.shape}')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"now to define needed functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_details() : \n    global data\n    for col in data.columns : \n        print(f'for feature : {col}')\n        print(f'Number of Nulls is   {data[col].isna().sum()}')\n        print(f'Number of Unique values is   {len(data[col].unique())}')\n        print(f'random Value {data[col][0]}')\n        print(f'random Value {data[col][10]}')\n        print(f'random Value {data[col][20]}')\n        print('--------------------------')\n\ndef CountWords(text) :  \n    \n    all_words = []\n\n    for i in range(text.shape[0]) : \n        this_phrase = list(text)[i]\n        for word in this_phrase.split() : \n            all_words.append(word)\n\n    print(f'Total words are {len(all_words)} words')   \n    print('')\n    print(f'Total unique words are {len(set(all_words))} words')   \n    \ndef CommonWords(text ,show = True , kk=10) : \n    all_words = []\n\n    for i in range(text.shape[0]) : \n        this_phrase = list(text)[i]\n        for word in this_phrase.split() : \n            all_words.append(word)\n    common_words = collections.Counter(all_words).most_common()\n    k=0\n    word_list =[]\n    for word, i in common_words : \n        if not word.lower() in  nlp.Defaults.stop_words :\n            if show : \n                print(f'The word is   {word}   repeated   {i}  times')\n            word_list.append(word)\n            k+=1\n        if k==kk : \n            break\n            \n    return word_list\n\ndef SelectedData(feature , value , operation, selected_feature ):\n    global data\n    if operation==0 : \n        result = data[data[feature]==value][selected_feature]\n    elif operation==1 : \n        result = data[data[feature] > value][selected_feature]\n    elif operation==2 : \n        result = data[data[feature]< value][selected_feature]\n    \n    return result \n\ndef LowerCase(feature , newfeature) : \n    global data\n    def ApplyLower(text) : \n        return text.lower()\n    data[newfeature] = data[feature].apply(ApplyLower)\n    \ndef Drop(feature) :\n    global data\n    data.drop([feature],axis=1, inplace=True)\n    data.head()\ndef Unique(feature) : \n    global data\n    print(f'Number of unique vaure are {len(list(data[feature].unique()))} which are : \\n {list(data[feature].unique())}')\n    \ndef Encoder(feature , new_feature, drop = False) : \n    global data\n    enc  = LabelEncoder()\n    enc.fit(data[feature])\n    data[new_feature] = enc.transform(data[feature])\n    if drop == True : \n        data.drop([feature],axis=1, inplace=True)\n        \ndef MakeCloud(text , title = 'Word Clouds' , w = 15 , h = 15):\n    plt.figure(figsize=(w,h))\n    plt.imshow(WordCloud(background_color=\"white\",stopwords=set(stopwords.words('english')))\n               .generate(\" \".join([i for i in text.str.lower()])))\n    plt.axis(\"off\")\n    plt.title(title)\n    plt.show()\ndef BPlot(feature_1,feature_2) :\n    global data\n    sns.barplot(x=feature_1, y=feature_2 , data=data)\n    \ndef CPlot(feature) : \n    global data\n    sns.countplot(x=feature, data=data,facecolor=(0, 0, 0, 0),linewidth=5,edgecolor=sns.color_palette(\"dark\", 3))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"____\n\n# Text Processing\n\nso let's start make basic processes in our Text here \n\nfirst to drop the feature id"},{"metadata":{"trusted":true},"cell_type":"code","source":"Drop('id')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"then to lowercase all texts , & to drop original text feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"LowerCase('text' , 'lower text')\nDrop('text')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"now how data looks like"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"let's be sure no Nulls exists"},{"metadata":{"trusted":true},"cell_type":"code","source":"show_details()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"who are authors we have here ? "},{"metadata":{"trusted":true},"cell_type":"code","source":"Unique('author')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"so we need to labelencode our authors here , so EAP will be 0 , HPL will be 1 & MWS will be 2 , we need to memorize these numbers to use in predicting test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"Encoder('author' , 'author code')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"now how many words we have in all phrases  ? "},{"metadata":{"trusted":true},"cell_type":"code","source":"CountWords(data['lower text'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"about half million word , which depend on about 45K unique words\n\n____\n\n\nand we need to be sure that we have balances about of phrases for each category in the output\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"BPlot(data['author'].value_counts().index , data['author'].value_counts().values )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"great , balanced amount\n\n_____\n\n# Common Words\n\n\nlet's have a look to most common used words ( stopwords will be excluded automatically from this function)"},{"metadata":{"trusted":true},"cell_type":"code","source":"AllCommon = CommonWords(data['lower text'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"& it will be better to make wordcloud for all words"},{"metadata":{"trusted":true},"cell_type":"code","source":"MakeCloud(data['lower text'] , 'All Words')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"how about common words in phrases written by EAP ? "},{"metadata":{"trusted":true},"cell_type":"code","source":"ECommon = CommonWords(SelectedData('author','EAP',0,'lower text'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"also we can make it in cloud form "},{"metadata":{"trusted":true},"cell_type":"code","source":"MakeCloud(SelectedData('author','EAP',0,'lower text') , 'EAP Words')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"then repeat same process for HPL"},{"metadata":{"trusted":true},"cell_type":"code","source":"HCommon = CommonWords(SelectedData('author','HPL',0,'lower text'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MakeCloud(SelectedData('author','HPL',0,'lower text') , 'HPL Words')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"and for MWS"},{"metadata":{"trusted":true},"cell_type":"code","source":"MCommon = CommonWords(SelectedData('author','MWS',0,'lower text'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MakeCloud(SelectedData('author','MWS',0,'lower text') , 'MWS Words')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"____\n\n# More Data\n\nwe might need to know more features about these phrases , which might be helpful in our training\n\nlet's make a new feature about number of words in each phrase , & check if it will be a helpful feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['number of words'] = data['lower text'].apply(lambda x : len(x.split()))\nprint('mean words for EAP is  ' , SelectedData('author', 'EAP' , 0 , 'number of words').mean())  \nprint('mean words for HPL is  ' , SelectedData('author', 'HPL' , 0 , 'number of words').mean())  \nprint('mean words for MWS is  ' , SelectedData('author', 'MWS' , 0 , 'number of words').mean())  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"ok , the 3 means are very close to each other \n\nhow about the number of charachters"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['number of chars'] = data['lower text'].apply(lambda x : len(x))\nprint('mean chars for EAP is  ' , SelectedData('author', 'EAP' , 0 , 'number of chars').mean())  \nprint('mean chars for HPL is  ' , SelectedData('author', 'HPL' , 0 , 'number of chars').mean())  \nprint('mean chars for MWS is  ' , SelectedData('author', 'MWS' , 0 , 'number of chars').mean())  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"HPL prefer to write more letters , but still close to each other \n\nso how about number of punctuations ? "},{"metadata":{"trusted":true},"cell_type":"code","source":"data['number of punctuations'] = data['lower text'].apply(lambda x : len([k for k in  x if k in r'.,;:!?|\\#$%^&*/']))\nprint('mean punctuations for EAP is  ' , SelectedData('author', 'EAP' , 0 , 'number of punctuations').mean())  \nprint('mean punctuations for HPL is  ' , SelectedData('author', 'HPL' , 0 , 'number of punctuations').mean())  \nprint('mean punctuations for MWS is  ' , SelectedData('author', 'MWS' , 0 , 'number of punctuations').mean())  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"still not a big difference . so how about number of stop words ? "},{"metadata":{"trusted":true},"cell_type":"code","source":"data['number of stop'] = data['lower text'].apply(lambda x : len([k for k in  x if k in nlp.Defaults.stop_words ]))\nprint('mean stop for EAP is  ' , SelectedData('author', 'EAP' , 0 , 'number of stop').mean())  \nprint('mean stop for HPL is  ' , SelectedData('author', 'HPL' , 0 , 'number of stop').mean())  \nprint('mean stop for MWS is  ' , SelectedData('author', 'MWS' , 0 , 'number of stop').mean())  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"almost equal each other . . \n\nso it might not be helpful to use any of those features\n\n_____\n\n# Training the Data\n\nso let's start prepare our data to be ready for training "},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"let's define X & y , since we'll not use any of those new features "},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data['lower text']\ny = data['author code']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"then we'll have to vectorize the text & check its shape"},{"metadata":{"trusted":true},"cell_type":"code","source":"VecModel = TfidfVectorizer()\nXVec = VecModel.fit_transform(X)\n\nprint(f'The new shape for X is {XVec.shape}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"about 25K features . . \n\nhow about reducing them using SelectPercentile from sklearn , to its half , using chi2 function"},{"metadata":{"trusted":true},"cell_type":"code","source":"FeatureSelection = SelectPercentile(score_func = chi2, percentile=50)\nX_data = FeatureSelection.fit_transform(XVec, y)\n\nprint('X Shape is ' , X_data.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"ok looks fine , not to split it into training & testing data"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X_data, y, test_size=0.33, random_state=44, shuffle =True)\n\nprint('X_train shape is ' , X_train.shape)\nprint('X_test shape is ' , X_test.shape)\nprint('y_train shape is ' , y_train.shape)\nprint('y_test shape is ' , y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"now to use MultiNomial Naive Bayes model for classification , with alpha only 0.05 to get the best score"},{"metadata":{"trusted":true},"cell_type":"code","source":"MultinomialNBModel = MultinomialNB(alpha=0.05)\nMultinomialNBModel.fit(X_train, y_train)\n\nprint('MultinomialNBModel Train Score is : ' , MultinomialNBModel.score(X_train, y_train))\nprint('MultinomialNBModel Test Score is : ' , MultinomialNBModel.score(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"well , 87% accuracy in test score is not the best but it's acceptable , may be we can increase it by more tuning for the parameters\n\n_____\n\n# Predicting Test Data\n\nnow let's moving to predicting test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/spooky-author-identification/test/test.csv')  \nprint(f'Test data Shape is {data.shape}')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we have to lower case it as well "},{"metadata":{"trusted":true},"cell_type":"code","source":"LowerCase('text' , 'lower text')\nDrop('text')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"then define X"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data['lower text']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"now to apply Vectorizing Model to it , & it have bring the same 25K features"},{"metadata":{"trusted":true},"cell_type":"code","source":"XVec = VecModel.transform(X)\nprint(f'The new shape for X is {XVec.shape}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"perfect  . again we have to apply SelectPercentile Model to it , to select same half features"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_data = FeatureSelection.transform(XVec)\nprint('X Shape is ' , X_data.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"now it's ready for predicting"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = MultinomialNBModel.predict(X_data)\ny_pred_prob = MultinomialNBModel.predict_proba(X_data)\nprint('Predicted Value for MultinomialNBModel is : ' , y_pred[:10])\nprint('Prediction Probabilities Value for MultinomialNBModel is : ' , y_pred_prob[:10])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"great , now to open the submission file , to insert the answer"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/spooky-author-identification/sample_submission/sample_submission.csv')  \nprint(f'Test data Shape is {data.shape}')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"here they don't need the argmax prediction , but the probabilities , so we'll use predict probability method from same model , then transform it into dataframe , & concatenate to the original id feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"idd = data['id']\nFinalResults = pd.DataFrame(y_pred_prob  ,columns= ['EAP','HPL','MWS'])\nFinalResults.insert(0,'id',idd)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"how it looks now ? "},{"metadata":{"trusted":true},"cell_type":"code","source":"FinalResults.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"great, now submission "},{"metadata":{"trusted":true},"cell_type":"code","source":"FinalResults.to_csv(\"sample_submission.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"hope you enjoyed it !\n"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}