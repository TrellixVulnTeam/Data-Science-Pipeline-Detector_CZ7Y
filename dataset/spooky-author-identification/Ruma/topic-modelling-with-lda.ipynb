{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\nimport pandas as pd\nimport numpy as np\nimport re\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nimport gensim\nfrom gensim.utils import simple_preprocess\nfrom gensim.models import Phrases, phrases, ldamodel, CoherenceModel\nimport nltk\nfrom nltk.corpus import stopwords\nimport spacy\nimport gensim.corpora as corpora\nfrom pprint import pprint\nimport pyLDAvis.gensim \nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this notebook we will apply topic modelling and create three topics as given dataset consists of three authors and the excerpts from their horror stories. These authors are Edgar Allan Poe, Mary Shelley, and HP Lovecraft. Dataset contains text from works of fiction written by spooky authors of the public domain: Edgar Allan Poe, HP Lovecraft and Mary Shelley.EAP work is around tales of mystery and the macabre. Mary Shelley work is around science fiction and  HP Lovecraft, best known as a writer of weird fiction."},{"metadata":{"trusted":true},"cell_type":"code","source":"authors_data_df = pd.read_csv('/kaggle/input/spooky-author-identification/train.zip')\nauthors_data_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# the length of the dataset\nauthors_data_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# any null value\nauthors_data_df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Do not find any null column"},{"metadata":{"trusted":true},"cell_type":"code","source":"#for the topic modelling we will focus only on the text data\nauthors_data_df = authors_data_df.drop(columns = ['id'], axis=1)\nauthors_data_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Inspecting data"},{"metadata":{"trusted":true},"cell_type":"code","source":"authors_data_df['author'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### The distribution of the excerpts script by author and we can view the number of excerpts are more for EAP then MWS and then HPL"},{"metadata":{},"cell_type":"markdown","source":"## Data cleaning and preparation"},{"metadata":{"trusted":true},"cell_type":"code","source":"authors_data_df['text_processed'] = authors_data_df['text'].map(lambda x: re.sub('[,\\.!?]','',x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"authors_data_df['text_processed'] = authors_data_df['text_processed'].map(lambda x:x.lower())\nprint(authors_data_df['text_processed'].head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove all characters, number or characters\ndef cleanText(input_string):\n    modified_string = re.sub('[^A-Za-z0-9]+', ' ', input_string)\n    return(modified_string)\nauthors_data_df['text_processed'] = authors_data_df.text_processed.apply(cleanText)\nauthors_data_df['text_processed'][150]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#NLTK stop words\nnltk.download('stopwords')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stopWords = stopwords.words('english')\nstopWords.extend([\"make\",\"mr\",\"de\",\"without\",\"let\",\"rather\",\"upon\",\"within\",\"made\",\"must\",\"much\",\"yet\",\"thought\",\"see\",\n                  \"said\",\"us\",\"say\",\"whose\",\"though\",\"every\",\"know\",\n                  \"many\",\"will\",\"never\",\"even\",\"found\",\"might\",\"almost\",'although','indeed','thus','still',\n                  'this','me','of','may', 'would', 'ever','could','shall','come','go','soon','however','become',\n                  'give','take','well'])\ndef removeStopWords(stopWords, rvw_txt):\n    newtxt = ' '.join([word for word in rvw_txt.split() if word not in stopWords])\n    return newtxt\nauthors_data_df['text_processed'] = [removeStopWords(stopWords,x) for x in authors_data_df['text_processed']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Exploratory data analysis with Wordcloud"},{"metadata":{"trusted":true},"cell_type":"code","source":"# join the different text together\nlongText = ','.join(list(authors_data_df['text_processed'].values))\n# generate the word cloud\nwordcloud = WordCloud(background_color=\"white\",\n                      max_words= 500,\n                      contour_width = 8,\n                      contour_color = \"steelblue\",\n                     collocations=False).generate(longText)\n# visualize the word cloud\nfig = plt.figure(1, figsize = (10, 10))\nplt.axis('off')\nplt.imshow(wordcloud)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter\nfig = plt.figure(1, figsize = (20,10))\n# split() returns list of all the words in the string\nsplit_it = longText.split()\n# Pass the split_it list to instance of Counter class.\nCounter = Counter(split_it)\n#print(Counter)\n# most_common() produces k frequently encountered\n# input values and their respective counts.\nmost_occur = Counter.most_common(30)\nx_df = pd.DataFrame(most_occur, columns=(\"words\",\"count\"))\nsns.barplot(x = 'words', y = 'count', data = x_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hplDatadf = authors_data_df[authors_data_df.author==\"HPL\"]\nhplDatadf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mwsDatadf = authors_data_df[authors_data_df.author==\"MWS\"]\nmwsDatadf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eapDatadf = authors_data_df[authors_data_df.author==\"EAP\"]\neapDatadf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# join the different text together\nlongText = ','.join(list(hplDatadf['text_processed'].values))\n# generate the word cloud\nwordcloud = WordCloud(background_color=\"white\",\n                      max_words= 500,\n                      contour_width = 8,\n                      contour_color = \"steelblue\",\n                     collocations=False).generate(longText)\n\n# visualize the word cloud\nfig = plt.figure(1, figsize = (10, 10))\nplt.axis('off')\nplt.imshow(wordcloud)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter\nfig = plt.figure(1, figsize = (20,10))\n# split() returns list of all the words in the string\nsplit_it = longText.split()\n# Pass the split_it list to instance of Counter class.\nCounter = Counter(split_it)\n#print(Counter)\n# most_common() produces k frequently encountered\n# input values and their respective counts.\nmost_occur = Counter.most_common(30)\nx_df = pd.DataFrame(most_occur, columns=(\"words\",\"count\"))\nsns.barplot(x = 'words', y = 'count', data = x_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"HP lovecraft wordcloud displays words like \"night\", \"death\",\"dream\", \"dead\",\"fear\",\"horror\", \"strange\", \"window\", \"ancient\" which seem to resonate with themes that the author was famous fo"},{"metadata":{"trusted":true},"cell_type":"code","source":"# join the different text together\nlongText = ','.join(list(mwsDatadf['text_processed'].values))\n# generate the word cloud\nwordcloud = WordCloud(background_color=\"white\",\n                      max_words= 500,\n                      contour_width = 8,\n                      contour_color = \"steelblue\",\n                     collocations=False).generate(longText)\n# visualize the word cloud\nfig = plt.figure(1, figsize = (10, 10))\nplt.axis('off')\nplt.imshow(wordcloud)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Mary Shelley wordcloud displays words as fear, heart, raymond, mind, soul, power, hope, feeling, death,spirit,friend, death. Positive as well as negative words"},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter\nfig = plt.figure(1, figsize = (20,10))\n# split() returns list of all the words in the string\nsplit_it = longText.split()\n# Pass the split_it list to instance of Counter class.\nCounter = Counter(split_it)\n#print(Counter)\n# most_common() produces k frequently encountered\n# input values and their respective counts.\nmost_occur = Counter.most_common(30)\nx_df = pd.DataFrame(most_occur, columns=(\"words\",\"count\"))\nsns.barplot(x = 'words', y = 'count', data = x_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# join the different text together\nlongText = ','.join(list(eapDatadf['text_processed'].values))\n# generate the word cloud\nwordcloud = WordCloud(background_color=\"white\",\n                      max_words= 500,\n                      contour_width = 8,\n                      contour_color = \"steelblue\",\n                     collocations=False).generate(longText)\n# visualize the word cloud\nfig = plt.figure(1, figsize = (10, 10))\nplt.axis('off')\nplt.imshow(wordcloud)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter\nfig = plt.figure(1, figsize = (20,10))\n# split() returns list of all the words in the string\nsplit_it = longText.split()\n# Pass the split_it list to instance of Counter class.\nCounter = Counter(split_it)\n#print(Counter)\n# most_common() produces k frequently encountered\n# input values and their respective counts.\nmost_occur = Counter.most_common(30)\nx_df = pd.DataFrame(most_occur, columns=(\"words\",\"count\"))\nsns.barplot(x = 'words', y = 'count', data = x_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Edgar Allan Poe wordcloud displays words as life,end,friend, night, far, open,eye, great, one, little, time, good, manner, moment etc"},{"metadata":{},"cell_type":"markdown","source":"### Preparing data for Topic Modelling"},{"metadata":{},"cell_type":"markdown","source":"#### Step 1 Processed text to words or tokens"},{"metadata":{"trusted":true},"cell_type":"code","source":"def text_to_tokens (textSentences):\n    for sent in textSentences:\n        yield(simple_preprocess(str(sent),deacc=True))\n\nwordsData=authors_data_df.text_processed.values.tolist()\nwordsDataList = list(text_to_tokens(wordsData))\nprint(wordsDataList[:1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Step 2 Building N grams"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Building the tokens\ntokens = Phrases(wordsDataList,min_count=5,threshold=100)\ntokensModel = phrases.Phraser(tokens)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_tokens_model(textSentences):\n   return[tokensModel[doc] for doc in textSentences]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def lemmatizedText(textSentences, allowed_postags=['NOUN','ADJ','VERB','ADV']):\n    textSent_Output = []\n    for sent in textSentences:\n        doc = nlp(\" \".join(sent))\n        textSent_Output.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n    return textSent_Output","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Step 3 Lemmatize Is the process of converting the words to the root words"},{"metadata":{"trusted":true},"cell_type":"code","source":"# form n grams\ndataWordsngrams = make_tokens_model(wordsDataList)\ndataWordsngrams[:2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# initialize spacy \"en\" model keeping only the tagger component \nnlp = spacy.load(\"en_core_web_sm\",disable=['parser','ner'])\n# lemmatize keeping only noun, adj, adv and verbs\nlemmatizedTextData = lemmatizedText(dataWordsngrams, allowed_postags=[\"NOUN\",\"ADJ\",\"VERB\",\"ADV\"])\nprint(lemmatizedTextData[:1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Step 4 Building the corpora"},{"metadata":{"trusted":true},"cell_type":"code","source":"# create dictionary\ndictObject = corpora.Dictionary(lemmatizedTextData)\n# create corpus\ntextData = lemmatizedTextData\ndictObject[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# term document frequency\ncorpusData = [dictObject.doc2bow(text) for text in textData]\nprint(corpusData[:1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Human readable format of the term frequency corpus \n[[(dictObject[idx], count) for idx, count in x] for x in corpusData[:1]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Build the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"n_topics = 3\nldaModel = ldamodel.LdaModel(corpus=corpusData,\n                            id2word = dictObject,\n                            num_topics=n_topics,\n                            random_state=123,\n                            chunksize=100,\n                            passes=10,\n                            alpha=0.01,\n                            eta='auto',\n                            iterations=400,\n                            per_word_topics = True                            \n                            )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print the keyword in the topics\npprint(ldaModel.print_topics())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"coherenceModelLda = CoherenceModel(model=ldaModel, texts = lemmatizedTextData,dictionary=dictObject,coherence='c_v')\ncoherenceScore = coherenceModelLda.get_coherence()\nprint(coherenceScore)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pyLDAvis.enable_notebook()\nvis=pyLDAvis.gensim.prepare(ldaModel,corpusData,dictObject)\nvis","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The human interpretability of these topics returned by the statistical analysis is not easy. We may involve domain expertise to check whether the topics makes sense."},{"metadata":{},"cell_type":"markdown","source":"When we compare the top 30 words from the model per topic and then look into the top 30 words from the work of each of the author then may be Topic1 maps to EAP, Topic2 maps tp MWS and Topic3 maps to HPL based on the subset of words that overlap the most."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}