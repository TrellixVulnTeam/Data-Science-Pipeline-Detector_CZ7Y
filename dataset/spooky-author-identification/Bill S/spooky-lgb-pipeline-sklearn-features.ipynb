{"cells":[{"source":"This is just a brief dive, some visual fun, and then a quick model. ","cell_type":"markdown","metadata":{}},{"outputs":[],"metadata":{},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \nimport pandas as pd\nimport numpy as np\nimport lightgbm as lgb\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nfrom nltk.corpus import stopwords\nfrom collections import Counter\n\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nimport cv2\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD, LatentDirichletAllocation, NMF\nfrom sklearn import preprocessing\nfrom sklearn import pipeline\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n# Any results you write to the current directory are saved as output.","execution_count":15},{"outputs":[],"metadata":{},"cell_type":"code","source":"trainDf = pd.read_csv('../input/spooky-author-identification/train.csv', index_col = 0)\ntrainDf.head()","execution_count":16},{"outputs":[],"metadata":{"collapsed":true},"cell_type":"code","source":"def textClean(text):\n    text=text.lower().split()\n    stops = {'so', 'his', 't', 'y', 'ours', 'herself', \n         'your', 'all', 'some', 'they', 'i', 'of', 'didn', \n         'them', 'when', 'will', 'that', 'its', 'because', \n         'while', 'those', 'my', 'don', 'again', 'her', 'if',\n         'further', 'now', 'does', 'against', 'won', 'same', \n         'a', 'during', 'who', 'here', 'have', 'in', 'being', \n         'it', 'other', 'once', 'itself', 'hers', 'after', 're',\n         'just', 'their', 'himself', 'theirs', 'whom', 'then', 'd', \n         'out', 'm', 'mustn', 'where', 'below', 'about', 'isn',\n         'shouldn', 'wouldn', 'these', 'me', 'to', 'doesn', 'into',\n         'the', 'until', 'she', 'am', 'under', 'how', 'yourself',\n         'couldn', 'ma', 'up', 'than', 'from', 'themselves', 'yourselves',\n         'off', 'above', 'yours', 'having', 'mightn', 'needn', 'on', \n         'too', 'there', 'an', 'and', 'down', 'ourselves', 'each',\n         'hadn', 'ain', 'such', 've', 'did', 'be', 'or', 'aren', 'he', \n         'should', 'for', 'both', 'doing', 'this', 'through', 'do', 'had',\n         'own', 'but', 'were', 'over', 'not', 'are', 'few', 'by', \n         'been', 'most', 'no', 'as', 'was', 'what', 's', 'is', 'you', \n         'shan', 'between', 'wasn', 'has', 'more', 'him', 'nor',\n         'can', 'why', 'any', 'at', 'myself', 'very', 'with', 'we', \n         'which', 'hasn', 'weren', 'haven', 'our', 'll', 'only',\n         'o', 'before'}\n#     stops = set(stopwords.words(\"English\"))\n    text = [w for w in text if not w in stops]\n    text = \" \".join(text)\n    return text","execution_count":19},{"outputs":[],"metadata":{},"cell_type":"code","source":"texts = []\nfor t in trainDf.text:\n    texts.append(textClean(t))","execution_count":20},{"outputs":[],"metadata":{},"cell_type":"code","source":"texts[0][:100]","execution_count":21},{"outputs":[],"metadata":{},"cell_type":"code","source":"tops = Counter(str(texts).split()).most_common()[:30]\nlabs, vals = zip(*tops)\nidx = np.arange(len(labs))\nwid=0.6\nfig, ax=plt.subplots(1,1,figsize=(14,8))\nax=plt.bar(idx, vals, wid, color='orange')\nax=plt.xticks(idx - wid/8, labs, rotation=30, size=14)\nplt.title('Top Thirty Counts of Most-Common Words Among Text')","execution_count":22},{"source":"Once __UPON__ a midnight dreary... This is great. The words dont seem all that specific though.","cell_type":"markdown","metadata":{}},{"outputs":[],"metadata":{},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nax = sns.countplot(x=\"author\", data=trainDf, \n#                    color='purple',\n                   palette=\"Greys\")\nplt.ylabel('Frequency'); plt.xlabel('Author')\nplt.title('Freq. of Authors')\nplt.show()","execution_count":23},{"source":"A little more __POE__ than __Shelley__ and __Lovecraft__.","cell_type":"markdown","metadata":{}},{"outputs":[],"metadata":{},"cell_type":"code","source":"trainDf['text'].to_csv('wc_text.txt')\nimg = cv2.imread(\"..\")\nspookyColors = 255.0-cv2.imread(\"../input/pumpkin-pic/hp.jpg\")","execution_count":26},{"outputs":[],"metadata":{},"cell_type":"code","source":"wc_text = open('wc_text.txt').read()\nwordcloud = WordCloud(background_color=\"black\",\n                      width=1200,height=800, mask=spookyColors).generate(wc_text)\nimage_colors = ImageColorGenerator(spookyColors)","execution_count":27},{"outputs":[],"metadata":{},"cell_type":"code","source":"fig, ax=plt.subplots(1,1,figsize=(16,16))\nax.grid(False);\nax.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\");","execution_count":28},{"source":"[](http://)LOL....sweeeet. Its a pumpkin.","cell_type":"markdown","metadata":{}},{"source":"### And now for some modeling...","cell_type":"markdown","metadata":{}},{"outputs":[],"metadata":{},"cell_type":"code","source":"trainDf = pd.read_csv('../input/spooky-author-identification/train.csv', index_col = 0)\ntrainDf.head()","execution_count":30},{"outputs":[],"metadata":{"collapsed":true},"cell_type":"code","source":"lbl = preprocessing.LabelEncoder()\ny = lbl.fit_transform(trainDf.author)","execution_count":31},{"outputs":[],"metadata":{},"cell_type":"code","source":"testDf = pd.read_csv('../input/spooky-author-identification/test.csv',index_col=0)\npid = testDf.index","execution_count":33},{"outputs":[],"metadata":{"collapsed":true},"cell_type":"code","source":"texts = []\nfor t in trainDf.text:\n    texts.append(textClean(t))\ntestText = []\nfor t in testDf.text:\n    testText.append(textClean(t))","execution_count":34},{"outputs":[],"metadata":{},"cell_type":"code","source":"testText[0][:100]","execution_count":35},{"outputs":[],"metadata":{},"cell_type":"code","source":"trainDf['newText'] = texts\ntrainDf.drop(['author','text'],axis=1,inplace=True)\ntrainDf.head()","execution_count":36},{"outputs":[],"metadata":{},"cell_type":"code","source":"testDf['newText'] = testText\ntestDf.drop(['text'],axis=1,inplace=True)\ntestDf.head()","execution_count":37},{"outputs":[],"metadata":{},"cell_type":"code","source":"ax = sns.countplot(x=y, color='black')\n## Quick Confirmation","execution_count":38},{"source":"### Build some features...","cell_type":"markdown","metadata":{}},{"outputs":[],"metadata":{"collapsed":true},"cell_type":"code","source":"cvec = CountVectorizer(analyzer=u'char', ngram_range=(1, 8), max_features=1000,\n                       strip_accents='unicode', stop_words='english',\n                       token_pattern=r'\\w+')\n\ntfidf = TfidfVectorizer(ngram_range=(1, 3), max_features=1000, \n                        strip_accents='unicode',\n                        lowercase =True, analyzer='word', token_pattern=r'\\w+',\n                        use_idf=True, smooth_idf=True, sublinear_tf=True, \n                        stop_words = 'english')\n\nnmfNC = 50\nnmf = NMF(n_components=nmfNC, random_state=42,\n          alpha=.1, l1_ratio=.5)\nldaNT = 50\nlda = LatentDirichletAllocation(n_topics=ldaNT, max_iter=10,\n                                learning_method='online',\n                                learning_offset=50.,\n                                random_state=42)\n\ntextNC = 150\ntsvdText = TruncatedSVD(n_components=textNC, n_iter=25, random_state=42)\ntsvdCount = TruncatedSVD(n_components=textNC, n_iter=25, random_state=42)","execution_count":39},{"outputs":[],"metadata":{},"cell_type":"code","source":"class cust_txt_col(BaseEstimator, TransformerMixin):\n    def __init__(self, key):\n        self.key = key\n    def fit(self, x, y=None):\n        print('fit...')\n        return self\n    def transform(self, x):\n        print('transform...')\n        return x[self.key].apply(str)\n    \nclass cust_regression_vals(BaseEstimator, TransformerMixin):\n    def fit(self, x, y=None):\n        print('fit...')\n        return self\n    def transform(self, x):\n        print('transform and drop...')\n        x = x.drop(['newText'],axis=1).values\n        return x\n    \nprint('Pipeline...')\nfp = pipeline.Pipeline([\n    ('union', pipeline.FeatureUnion(\n#            n_jobs = -1,\n        transformer_list = [\n            ('standard', cust_regression_vals()),\n            ('pip1', pipeline.Pipeline([('newText', cust_txt_col('newText')),('counts', cvec),('tsvdCountText', tsvdCount)])),       \n            ('pip2', pipeline.Pipeline([('nmf_Text', cust_txt_col('newText')),('tfidf_Text', tfidf),('nmfText', nmf)])),\n            ('pip3', pipeline.Pipeline([('lda_Text', cust_txt_col('newText')),('tfidf_Text', tfidf),('ldaText', lda)])),\n            ('pip4', pipeline.Pipeline([('newText', cust_txt_col('newText')),('tfidf_Text', tfidf),('tsvdText', tsvdText)]))\n        ])\n    )])","execution_count":40},{"outputs":[],"metadata":{"collapsed":true},"cell_type":"code","source":"for c in trainDf.columns:\n    if c == 'newText':\n        trainDf[c+'_len'] = trainDf[c].map(lambda x: len(str(x)))\n        trainDf[c+'_words'] = trainDf[c].map(lambda x: len(str(x).split(' ')))        \n        \nfor c in testDf.columns:\n    if c == 'newText':\n        testDf[c+'_len'] = testDf[c].map(lambda x: len(str(x)))\n        testDf[c+'_words'] = testDf[c].map(lambda x: len(str(x).split(' ')))     ","execution_count":41},{"outputs":[],"metadata":{},"cell_type":"code","source":"trainDf.head()","execution_count":42},{"outputs":[],"metadata":{"scrolled":false},"cell_type":"code","source":"train = fp.fit_transform(trainDf); print(train.shape)\ntest = fp.transform(testDf); print(test.shape)","execution_count":43},{"source":"This LGB model is taking quite a long time to run in this notebook and Im not sure why.","cell_type":"markdown","metadata":{}},{"outputs":[],"metadata":{"collapsed":true},"cell_type":"code","source":"params = {'learning_rate':0.05\n         ,'max_depth':4\n         ,'objective':'multiclass'\n         ,'num_class':3\n         ,'metric':{'multi_logloss'}\n#          ,'num_iterations':256\n         ,'num_leaves':128\n         ,'min_data_in_leaf':128\n         ,'bagging_fraction':0.85 \n         ,'feature_fraction':0.85 \n         ,'lambda_l1':1.0}","execution_count":49},{"source":"Edited this for just 1 fold because it was taking so long. ","cell_type":"markdown","metadata":{}},{"outputs":[],"metadata":{},"cell_type":"code","source":"fold=1\npreds=0\nfor i in range(fold):\n    xt,xv,yt,yv = train_test_split(train, y, test_size=0.15, random_state=i*314)\n    dtx = lgb.Dataset(xt, label=yt)\n    dtv = lgb.Dataset(xv, label=yv)\n    model = lgb.train(params, train_set=dtx, valid_sets=dtv, valid_names=['val'],\n                      num_boost_round=1000,\n                      early_stopping_rounds=100,\n                      verbose_eval=False)\n    pred = model.predict(test)\n    preds += pred\npreds /= fold    ","execution_count":51},{"outputs":[],"metadata":{},"cell_type":"code","source":"ax = lgb.plot_importance(model, max_num_features=50, figsize=(12,8), **{'color':'orange'})\nax.grid(False)\nax.set_facecolor('black')","execution_count":52},{"outputs":[],"metadata":{"collapsed":true},"cell_type":"code","source":"import datetime\nnow = datetime.datetime.now()\nsubmission = pd.DataFrame(preds, columns=['EAP','HPL','MWS'])\nsubmission['ID'] = pid\nsubmission.to_csv('lgbPipeline_'+str(now.strftime(\"%Y-%m-%d-%H-%M\"))+'.csv', index=False)","execution_count":53},{"outputs":[],"metadata":{"collapsed":true},"cell_type":"code","source":"","execution_count":null}],"nbformat_minor":1,"nbformat":4,"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"version":3,"name":"ipython"},"nbconvert_exporter":"python","version":"3.5.3","file_extension":".py","pygments_lexer":"ipython3","mimetype":"text/x-python","name":"python"}}}