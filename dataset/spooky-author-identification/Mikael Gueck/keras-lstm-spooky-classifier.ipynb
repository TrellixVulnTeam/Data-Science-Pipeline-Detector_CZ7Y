{"nbformat_minor":1,"cells":[{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"fcdd50d0287ef09de79d8c479161d5aa0eb53cf6","_cell_guid":"89e6ac61-d437-493f-acf6-d36e14c6d403"},"outputs":[],"source":"import numpy as np\nimport pandas as pd"},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":true},"outputs":[],"source":"NUM_WORDS = 10000\nN = 128\nMAX_LEN = 50\nNUM_CLASSES = 3"},{"execution_count":null,"cell_type":"code","metadata":{},"outputs":[],"source":"from keras.layers import Embedding, LSTM, Dense, Flatten\nfrom keras.models import Sequential\n\nmodel = Sequential()\nmodel.add(Embedding(NUM_WORDS, N, input_length=MAX_LEN))\nmodel.add(LSTM(N, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))\nmodel.add(Flatten())\nmodel.add(Dense(NUM_CLASSES, activation='softmax'))\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()"},{"execution_count":null,"cell_type":"code","metadata":{},"outputs":[],"source":"train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\ntrain.sample(10)"},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":true},"outputs":[],"source":"from keras.preprocessing.sequence import pad_sequences\nfrom keras.preprocessing.text import Tokenizer\nfrom sklearn import preprocessing\n\nX = train['text']\nY = train['author']\n\ntokenizer = Tokenizer(num_words=NUM_WORDS)\ntokenizer.fit_on_texts(X)\n\ntrain_x = tokenizer.texts_to_sequences(X)\ntrain_x = pad_sequences(train_x, maxlen=MAX_LEN)\n\nlb = preprocessing.LabelBinarizer()\nlb.fit(Y)\n\ntrain_y = lb.transform(Y)"},{"execution_count":null,"cell_type":"code","metadata":{},"outputs":[],"source":"model.fit(train_x, train_y, validation_split=0.2, batch_size=1024, epochs=8, verbose=2)"},{"execution_count":null,"cell_type":"code","metadata":{},"outputs":[],"source":"score = model.evaluate(train_x, train_y, batch_size=1024, verbose=2)\nprint(score)"},{"execution_count":null,"cell_type":"code","metadata":{},"outputs":[],"source":"p = model.predict(pad_sequences(tokenizer.texts_to_sequences(test['text']), maxlen=MAX_LEN),\n                  batch_size=1024)\n\nfor i in range(10):\n    row = p[i]\n    print(TX[i])\n    for j in range(len(lb.classes_)):\n        print('{0:>5} {1:02.2f}'.format(lb.classes_[j], row[j]))\n    print()"},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":true},"outputs":[],"source":"import pickle, h5py\nmodel.save('spooky_model.hdf5')\nwith open('tokenizer.pickle', 'wb') as f:\n    pickle.dump(tokenizer, f)\nwith open('binarizer.pickle', 'wb') as f:\n    pickle.dump(lb, f)"}],"nbformat":4,"metadata":{"language_info":{"nbconvert_exporter":"python","version":"3.6.3","name":"python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","file_extension":".py","mimetype":"text/x-python"},"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"}}}