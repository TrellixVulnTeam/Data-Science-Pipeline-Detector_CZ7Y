{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#  Pick a Dataset you might be interested in.\n#  Say, all airline-safety files...\nimport zipfile\n\nDataset = \"train\"\n\n# Will unzip the files so that you can see them..\nwith zipfile.ZipFile(\"../input/spooky-author-identification/\"+Dataset+\".zip\",\"r\") as z:\n    z.extractall(\".\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import base64\nimport numpy as np\nimport pandas as pd\n\n# Plotly imports\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\n# Other imports\nfrom collections import Counter\n# from scipy.misc import imread\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import NMF, LatentDirichletAllocation\nfrom matplotlib import pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport xgboost as xgb\nfrom tqdm import tqdm\nfrom sklearn.svm import SVC\nfrom keras.models import Sequential\nfrom keras.layers.recurrent import LSTM, GRU\nfrom keras.layers.core import Dense, Activation, Dropout\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.utils import np_utils\nfrom sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\nfrom keras.preprocessing import sequence, text\nfrom keras.callbacks import EarlyStopping\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# There's only one file above...we'll select it.\ntrain=pd.read_csv(\"/kaggle/working/train.csv\")\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"z = {'EAP': 'Edgar Allen Poe', 'MWS': 'Mary Shelley', 'HPL': 'HP Lovecraft'}\n\n# Basic Statistics\ndata = [go.Bar(\n            x = train.author.map(z).unique(),\n            y = train.author.value_counts().values,\n            marker= dict(colorscale='Jet',\n                         color = train.author.value_counts().values\n                        ),\n            text='Text entries attributed to Author'\n    )]\n\nlayout = go.Layout(\n    title='Target variable distribution'\n)\n\nfig = go.Figure(data=data, layout=layout)\n\npy.iplot(fig, filename='basic-bar')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_words = train['text'].str.split(expand=True).unstack().value_counts()\ndata = [go.Bar(\n            x = all_words.index.values[2:50],\n            y = all_words.values[2:50],\n            marker= dict(colorscale='Jet',\n                         color = all_words.values[2:100]\n                        ),\n            text='Word counts'\n    )]\n\nlayout = go.Layout(\n    title='Top 50 (Uncleaned) Word frequencies in the training dataset'\n)\n\nfig = go.Figure(data=data, layout=layout)\n\npy.iplot(fig, filename='basic-bar')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Store the text of each author in a Python list\n\nWe first create three different python lists that store the texts of Edgar Allen Poe, HP Lovecraft and Mary Shelley respectively as foll","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"eap = train[train.author==\"EAP\"][\"text\"].values\nhpl = train[train.author==\"HPL\"][\"text\"].values\nmws = train[train.author==\"MWS\"][\"text\"].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eap","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud, STOPWORDS\n# The wordcloud of Cthulhu/squidy thing for HP Lovecraft\nplt.figure(figsize=(16,13))\nwordcloud1 = WordCloud( background_color='white',\n                        width=600,\n                        height=400).generate(\" \".join(hpl))\nplt.imshow(wordcloud1)\nplt.axis('off')\nplt.title('HP Lovecraft (Cthulhu-Squidy)',fontsize=40);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud, STOPWORDS\n# The wordcloud of Cthulhu/squidy thing for HP Lovecraft\nplt.figure(figsize=(16,13))\nwordcloud1 = WordCloud( background_color='white',\n                        width=600,\n                        height=400).generate(\" \".join(eap))\nplt.imshow(wordcloud1)\nplt.axis('off')\nplt.title('Edgar Allen Poe (The Raven)',fontsize=40);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud, STOPWORDS\n# The wordcloud of Cthulhu/squidy thing for HP Lovecraft\nplt.figure(figsize=(16,13))\nwordcloud1 = WordCloud( background_color='white',\n                        width=600,\n                        height=400).generate(\" \".join(mws))\nplt.imshow(wordcloud1)\nplt.axis('off')\nplt.title(\"Mary Shelley (Frankenstein's Monster)\",fontsize=40);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tokenization\n\nfirst_text = train.text.values[0]\nprint(first_text)\nprint(\"=\"*90)\nprint(first_text.split(\" \"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"first_text_list = nltk.word_tokenize(first_text)\nprint(first_text_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stopwords = nltk.corpus.stopwords.words('english')\nlen(stopwords)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(stopwords)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"first_text_list_cleaned = [ word for word in first_text_list if word.lower() not in stopwords]\nprint(first_text_list_cleaned)\nprint(\"=\"*90)\nprint(\"Length of original list: {0} words\\n\"\n      \"Length of list after stopwords removal: {1} words\"\n       .format(len(first_text_list),len(first_text_list_cleaned)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Stemming and Lemmatization","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"stemmer = nltk.stem.PorterStemmer()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The stemmed form of running is: {}\".format(stemmer.stem(\"running\")))\nprint(\"The stemmed form of runs is: {}\".format(stemmer.stem(\"runs\")))\nprint(\"The stemmed form of run is: {}\".format(stemmer.stem(\"run\")))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The stemmed form of leaves is: {}\".format(stemmer.stem(\"leaves\")))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.stem import WordNetLemmatizer\nlemm = WordNetLemmatizer()\nprint(\"The lemmatized form of leaves is :{}\".format(lemm.lemmatize(\"leaves\")))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Vectorizing the text using Bag of Words approach","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sentence = [\"I love to eat Burgers\",\n            \"I love to eat Fries\"]\nvectorizer = CountVectorizer(min_df = 0)\nsentence_transform = vectorizer.fit_transform(sentence)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Fitting vectorize to the dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The features are:\\n {}\".format(vectorizer.get_feature_names()))\nprint(\"\\nThe vectorized array looks like:\\n {}\".format(sentence_transform.toarray()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentence_transform","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### TOPIC MODELLING\n- Latent Dirichlet Allocation - Probabilistic, generative model which uncovers the topics latent to a dataset by assigning weights to words in a corpus, where each topic will assign different probability weights to each word.\n- Non-negative Matrix Factorization - Approximation method that takes an input matrix and approximates the factorization of this matrix into two other matrices, with the caveat that the values in the matrix be non-negative.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define helper function to print top words\ndef print_top_words(model,feature_names,n_top_words):\n    for index,topic in enumerate(model.components_):\n        message = \"\\nTopic #{}:\".format(index)\n        message +=\" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1 : -1]])\n        print(message)\n        print(\"=\"*70)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Putting all the preprocessing steps together","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extending the countvectorizer Class with a lemmatizer\nlemm = WordNetLemmatizer()\nclass LemmaCountVectorizer(CountVectorizer):\n    def build_analyzer(self):\n        analyzer = super(LemmaCountVectorizer,self).build_analyzer()\n        return lambda doc: (lemm.lemmatize(w) for w in analyzer(doc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sorting the entire training text in a list\ntext = list(train.text.values)\n# Calling our overwrittren Count Vectorizer\ntf_vectorizer = LemmaCountVectorizer(max_df = 0.95,\n                                    min_df = 2,\n                                    stop_words = \"english\",\n                                    decode_error = \"ignore\")\ntf= tf_vectorizer.fit_transform(text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_names = tf_vectorizer.get_feature_names()\ncount_vec = np.asarray(tf.sum(axis=0)).ravel()\nzipped = list(zip(feature_names, count_vec))\nx, y = (list(x) for x in zip(*sorted(zipped, key=lambda x: x[1], reverse=True)))\n# Now I want to extract out on the top 15 and bottom 15 words\nY = np.concatenate([y[0:15], y[-16:-1]])\nX = np.concatenate([x[0:15], x[-16:-1]])\n\n# Plotting the Plot.ly plot for the Top 50 word frequencies\ndata = [go.Bar(\n            x = x[0:50],\n            y = y[0:50],\n            marker= dict(colorscale='Jet',\n                         color = y[0:50]\n                        ),\n            text='Word counts'\n    )]\n\nlayout = go.Layout(\n    title='Top 50 Word frequencies after Preprocessing'\n)\n\nfig = go.Figure(data=data, layout=layout)\n\npy.iplot(fig, filename='basic-bar')\n\n# Plotting the Plot.ly plot for the Top 50 word frequencies\ndata = [go.Bar(\n            x = x[-100:],\n            y = y[-100:],\n            marker= dict(colorscale='Portland',\n                         color = y[-100:]\n                        ),\n            text='Word counts'\n    )]\n\nlayout = go.Layout(\n    title='Bottom 100 Word frequencies after Preprocessing'\n)\n\nfig = go.Figure(data=data, layout=layout)\n\npy.iplot(fig, filename='basic-bar')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lda = LatentDirichletAllocation(n_components=11, max_iter=5,\n                                learning_method = 'online',\n                                learning_offset = 50.,\n                                random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##\nlda.fit(tf)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Topics generated by LDA\n> We will utilise our helper function we defined earlier \"print_top_words\" to return the top 10 words attributed to each of the LDA generated topics. To select the number of topics, this is handled through the parameter n_components in the function.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"n_top_words = 40\nprint(\"\\nTopics in LDA model: \")\ntf_feature_names = tf_vectorizer.get_feature_names()\nprint_top_words(lda, tf_feature_names, n_top_words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"first_topic = lda.components_[0]\nsecond_topic = lda.components_[1]\nthird_topic = lda.components_[2]\nfourth_topic = lda.components_[3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"first_topic.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Word Cloud visualizations of the topics\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"first_topic_words = [tf_feature_names[i] for i in first_topic.argsort()[:-50 - 1 :-1]]\nsecond_topic_words = [tf_feature_names[i] for i in second_topic.argsort()[:-50 - 1 :-1]]\nthird_topic_words = [tf_feature_names[i] for i in third_topic.argsort()[:-50 - 1 :-1]]\nfourth_topic_words = [tf_feature_names[i] for i in fourth_topic.argsort()[:-50 - 1 :-1]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generating the wordcloud with the values under the category dataframe\nfirstcloud = WordCloud(\n                          stopwords=STOPWORDS,\n                          background_color='black',\n                          width=2500,\n                          height=1800\n                         ).generate(\" \".join(first_topic_words))\nplt.imshow(firstcloud)\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generating the wordcloud with the values under the category dataframe\ncloud = WordCloud(\n                          stopwords=STOPWORDS,\n                          background_color='black',\n                          width=2500,\n                          height=1800\n                         ).generate(\" \".join(second_topic_words))\nplt.imshow(cloud)\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generating the wordcloud with the values under the category dataframe\ncloud = WordCloud(\n                          stopwords=STOPWORDS,\n                          background_color='black',\n                          width=2500,\n                          height=1800\n                         ).generate(\" \".join(third_topic_words))\nplt.imshow(cloud)\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generating the wordcloud with the values under the category dataframe\ncloud = WordCloud(\n                          stopwords=STOPWORDS,\n                          background_color='black',\n                          width=2500,\n                          height=1800\n                         ).generate(\" \".join(fourth_topic_words))\nplt.imshow(cloud)\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/spooky/train.csv\")\ntest = pd.read_csv(\"../input/spooky/test.csv\")\nsample = pd.read_csv(\"../input/spooky/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def multiclass_logloss(actual, predicted, eps=1e-15):\n    \"\"\"Multi class version of Logarithmic Loss metric.\n    :param actual: Array containing the actual target classes\n    :param predicted: Matrix with class predictions, one probability per class\n    \"\"\"\n    # Convert 'actual' to a binary array if it's not already:\n    if len(actual.shape) == 1:\n        actual2 = np.zeros((actual.shape[0], predicted.shape[1]))\n        for i, val in enumerate(actual):\n            actual2[i, val] = 1\n        actual = actual2\n\n    clip = np.clip(predicted, eps, 1 - eps)\n    rows = actual.shape[0]\n    vsota = np.sum(actual * np.log(clip))\n    return -1.0 / rows * vsota","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We use the LabelEncoder from scikit-learn to convert text labels to integers, 0, 1 2\nlbl_enc = preprocessing.LabelEncoder()\ny = lbl_enc.fit_transform(train.author.values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Before going further it is important that we split the data into training and validation sets. We can do it using train_test_split from the model_selection module of scikit-learn.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"xtrain,xvalid,ytrain,yvalid = train_test_split(train.text.values,y,\n                                              stratify = y,\n                                              random_state = 42,\n                                              test_size = 0.1,\n                                              shuffle =True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print (xtrain.shape)\nprint (xvalid.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Building Basic Models\n> Let's start building our very first model.\n\n> Our very first model is a simple TF-IDF (Term Frequency - Inverse Document Frequency) followed by a simple Logistic Regression.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Always start with these features. They work (almost) everytime!\ntfv = TfidfVectorizer(min_df=3,max_features=None,\n                     strip_accents=\"unicode\",\n                      analyzer='word',token_pattern = r'\\w{1,}',\n                     ngram_range = (1,3),use_idf = 1,smooth_idf=1,sublinear_tf = 1,\n                     stop_words = \"english\")\n\n# Fitting TF-IDF to both training and test sets (semi-supervised learning)\ntfv.fit(list(xtrain) + list(xvalid))\nxtrain_tfv = tfv.transform(xtrain)\nxvalid_tfv = tfv.transform(xvalid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting a simple Logistic Regression on TFIDF\nclf = LogisticRegression(C=1.0)\nclf.fit(xtrain_tfv, ytrain)\npredictions = clf.predict_proba(xvalid_tfv)\n\nprint (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ctv = CountVectorizer(analyzer='word',token_pattern=r'\\w{1,}',\n            ngram_range=(1, 3), stop_words = 'english')\n\n# Fitting Count Vectorizer to both training and test sets (semi-supervised learning)\nctv.fit(list(xtrain) + list(xvalid))\nxtrain_ctv =  ctv.transform(xtrain) \nxvalid_ctv = ctv.transform(xvalid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting a simple Logistic Regression on Counts\nclf = LogisticRegression(C=1.0)\nclf.fit(xtrain_ctv, ytrain)\npredictions = clf.predict_proba(xvalid_ctv)\n\nprint (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting a simple Naive Bayes on TFIDF\nclf = MultinomialNB()\nclf.fit(xtrain_tfv,ytrain)\npredictions = clf.predict_proba(xvalid_tfv)\n\nprint(\"lgoloss: %0.3f \" % multiclass_logloss(yvalid,predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting a simple Naive Bayes on Counts\nclf = MultinomialNB()\nclf.fit(xtrain_ctv, ytrain)\npredictions = clf.predict_proba(xvalid_ctv)\n\nprint (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Apply SVD, I chose 120 components. 120-200 components are good enough for SVM model.\nsvd = decomposition.TruncatedSVD(n_components=120)\nsvd.fit(xtrain_tfv)\nxtrain_svd = svd.transform(xtrain_tfv)\nxvalid_svd = svd.transform(xvalid_tfv)\n\n# Scale the data obtained from SVD. Renaming variable to reuse without scaling.\nscl = preprocessing.StandardScaler()\nscl.fit(xtrain_svd)\nxtrain_svd_scl = scl.transform(xtrain_svd)\nxvalid_svd_scl = scl.transform(xvalid_svd)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Now it's time to apply SVM. After running the following cell, \n# feel free to go for a walk or talk to your girlfriend/boyfriend. :P\n\n# Fitting a simple SVM\nclf = SVC(C=1.0, probability=True) # since we need probabilities\nclf.fit(xtrain_svd_scl, ytrain)\npredictions = clf.predict_proba(xvalid_svd_scl)\n\nprint (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# XGBOOST\n\n# Fitting a simple xgboost on tf-idf\nclf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n                        subsample=0.8, nthread=10, learning_rate=0.1)\nclf.fit(xtrain_tfv.tocsc(), ytrain)\npredictions = clf.predict_proba(xvalid_tfv.tocsc())\n\nprint (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting a simple xgboost on tf-idf\nclf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n                        subsample=0.8, nthread=10, learning_rate=0.1)\nclf.fit(xtrain_ctv.tocsc(), ytrain)\npredictions = clf.predict_proba(xvalid_ctv.tocsc())\n\nprint (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting a simple xgboost on tf-idf svd features\nclf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n                        subsample=0.8, nthread=10, learning_rate=0.1)\nclf.fit(xtrain_svd, ytrain)\npredictions = clf.predict_proba(xvalid_svd)\n\nprint (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting a simple xgboost on tf-idf svd features\nclf = xgb.XGBClassifier(nthread=10)\nclf.fit(xtrain_svd, ytrain)\npredictions = clf.predict_proba(xvalid_svd)\n\nprint (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Grid Search¶\n> Its a technique for hyperparameter optimization. Not so effective but can give good results if you know the grid you want to use. I specify the parameters that should usually be used in this post: http://blog.kaggle.com/2016/07/21/approaching-almost-any-machine-learning-problem-abhishek-thakur/ Please keep in mind that these are the parameters I usually use. There are many other methods of hyperparameter optimization which may or may not be as effective.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"mll_scorer = metrics.make_scorer(multiclass_logloss, greater_is_better=False, needs_proba=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initialize SVD\nsvd = TruncatedSVD()\n    \n# Initialize the standard scaler \nscl = preprocessing.StandardScaler()\n\n# We will use logistic regression here..\nlr_model = LogisticRegression()\n\n# Create the pipeline \nclf = pipeline.Pipeline([('svd', svd),\n                         ('scl', scl),\n                         ('lr', lr_model)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = {'svd__n_components' : [120, 180],\n              'lr__C': [0.1, 1.0, 10], \n              'lr__penalty': ['l1', 'l2']}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initialize Grid Search Model\nmodel = GridSearchCV(estimator=clf, param_grid=param_grid, scoring=mll_scorer,\n                                 verbose=10, n_jobs=-1, iid=True, refit=True, cv=2)\n\n# Fit Grid Search Model\nmodel.fit(xtrain_tfv, ytrain)  # we can use the full data here but im only using xtrain\nprint(\"Best score: %0.3f\" % model.best_score_)\nprint(\"Best parameters set:\")\nbest_parameters = model.best_estimator_.get_params()\nfor param_name in sorted(param_grid.keys()):\n    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nb_model = MultinomialNB()\n\n# Create the pipeline \nclf = pipeline.Pipeline([('nb', nb_model)])\n\n# parameter grid\nparam_grid = {'nb__alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n\n# Initialize Grid Search Model\nmodel = GridSearchCV(estimator=clf, param_grid=param_grid, scoring=mll_scorer,\n                                 verbose=10, n_jobs=-1, iid=True, refit=True, cv=2)\n\n# Fit Grid Search Model\nmodel.fit(xtrain_tfv, ytrain)  # we can use the full data here but im only using xtrain. \nprint(\"Best score: %0.3f\" % model.best_score_)\nprint(\"Best parameters set:\")\nbest_parameters = model.best_estimator_.get_params()\nfor param_name in sorted(param_grid.keys()):\n    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Word Vectors\nWithout going into too much details, I would explain how to create sentence vectors and how can we use them to create a machine learning model on top of it. I am a fan of GloVe vectors, word2vec and fasttext. In this post, I'll be using the GloVe vectors. You can download the GloVe vectors from here http://www-nlp.stanford.edu/data/glove.840B.300d.zip","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# load the GloVe vectors in a dictionary:\n\nembeddings_index = {}\nf = open('../input/glove840b300dtxt/glove.840B.300d.txt')\nfor line in tqdm(f):\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\n\nprint('Found %s word vectors.' % len(embeddings_index))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# this function creates a normalized vector for the whole sentence\ndef sent2vec(s):\n    words = str(s).lower().decode('utf-8')\n    words = word_tokenize(words)\n    words = [w for w in words if not w in stop_words]\n    words = [w for w in words if w.isalpha()]\n    M = []\n    for w in words:\n        try:\n            M.append(embeddings_index[w])\n        except:\n            continue\n    M = np.array(M)\n    v = M.sum(axis=0)\n    if type(v) != np.ndarray:\n        return np.zeros(300)\n    return v / np.sqrt((v ** 2).sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xtrain_glove = np.array(xtrain_glove)\nxvalid_glove = np.array(xvalid_glove)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting a simple xgboost on glove features\nclf = xgb.XGBClassifier(nthread=10, silent=False)\nclf.fit(xtrain_glove, ytrain)\npredictions = clf.predict_proba(xvalid_glove)\n\nprint (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting a simple xgboost on glove features\nclf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n                        subsample=0.8, nthread=10, learning_rate=0.1, silent=False)\nclf.fit(xtrain_glove, ytrain)\npredictions = clf.predict_proba(xvalid_glove)\n\nprint (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Deep Learning\n#But this is an era of deep learning! We cant live without training a few neural networks.\n#Here, we will train LSTM and a simple dense network on the GloVe features. Let's start with the dense network first:\n\n# scale the data before any neural net:\nscl = preprocessing.StandardScaler()\nxtrain_glove_scl = scl.fit_transform(xtrain_glove)\nxvalid_glove_scl = scl.transform(xvalid_glove)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we need to binarize the labels for the neural net\nytrain_enc = np_utils.to_categorical(ytrain)\nyvalid_enc = np_utils.to_categorical(yvalid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a simple 3 layer sequential neural net\nmodel = Sequential()\n\nmodel.add(Dense(300, input_dim=300, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(BatchNormalization())\n\nmodel.add(Dense(300, activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(BatchNormalization())\n\nmodel.add(Dense(3))\nmodel.add(Activation('softmax'))\n\n# compile the model\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(xtrain_glove_scl, y=ytrain_enc, batch_size=64, \n          epochs=5, verbose=1, \n          validation_data=(xvalid_glove_scl, yvalid_enc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# using keras tokenizer here\ntoken = text.Tokenizer(num_words=None)\nmax_len = 70\n\ntoken.fit_on_texts(list(xtrain) + list(xvalid))\nxtrain_seq = token.texts_to_sequences(xtrain)\nxvalid_seq = token.texts_to_sequences(xvalid)\n\n# zero pad the sequences\nxtrain_pad = sequence.pad_sequences(xtrain_seq, maxlen=max_len)\nxvalid_pad = sequence.pad_sequences(xvalid_seq, maxlen=max_len)\n\nword_index = token.word_index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create an embedding matrix for the words we have in the dataset\nembedding_matrix = np.zeros((len(word_index) + 1, 300))\nfor word, i in tqdm(word_index.items()):\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# A simple LSTM with glove embeddings and two dense layers\nmodel = Sequential()\nmodel.add(Embedding(len(word_index) + 1,\n                     300,\n                     weights=[embedding_matrix],\n                     input_length=max_len,\n                     trainable=False))\nmodel.add(SpatialDropout1D(0.3))\nmodel.add(LSTM(100, dropout=0.3, recurrent_dropout=0.3))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(3))\nmodel.add(Activation('softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=100, verbose=1, validation_data=(xvalid_pad, yvalid_enc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# A simple LSTM with glove embeddings and two dense layers\nmodel = Sequential()\nmodel.add(Embedding(len(word_index) + 1,\n                     300,\n                     weights=[embedding_matrix],\n                     input_length=max_len,\n                     trainable=False))\nmodel.add(SpatialDropout1D(0.3))\nmodel.add(LSTM(300, dropout=0.3, recurrent_dropout=0.3))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(3))\nmodel.add(Activation('softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\n\n# Fit the model with early stopping callback\nearlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\nmodel.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=100, \n          verbose=1, validation_data=(xvalid_pad, yvalid_enc), callbacks=[earlystop])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n# A simple bidirectional LSTM with glove embeddings and two dense layers\nmodel = Sequential()\nmodel.add(Embedding(len(word_index) + 1,\n                     300,\n                     weights=[embedding_matrix],\n                     input_length=max_len,\n                     trainable=False))\nmodel.add(SpatialDropout1D(0.3))\nmodel.add(Bidirectional(LSTM(300, dropout=0.3, recurrent_dropout=0.3)))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(3))\nmodel.add(Activation('softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\n\n# Fit the model with early stopping callback\nearlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\nmodel.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=100, \n          verbose=1, validation_data=(xvalid_pad, yvalid_enc), callbacks=[earlystop])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# GRU with glove embeddings and two dense layers\nmodel = Sequential()\nmodel.add(Embedding(len(word_index) + 1,\n                     300,\n                     weights=[embedding_matrix],\n                     input_length=max_len,\n                     trainable=False))\nmodel.add(SpatialDropout1D(0.3))\nmodel.add(GRU(300, dropout=0.3, recurrent_dropout=0.3, return_sequences=True))\nmodel.add(GRU(300, dropout=0.3, recurrent_dropout=0.3))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(3))\nmodel.add(Activation('softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\n\n# Fit the model with early stopping callback\nearlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\nmodel.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=100, \n          verbose=1, validation_data=(xvalid_pad, yvalid_enc), callbacks=[earlystop])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Ensembling","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# this is the main ensembling class. how to use it is in the next cell!\nimport numpy as np\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold, KFold\nimport pandas as pd\nimport os\nimport sys\nimport logging\n\nlogging.basicConfig(\n    level=logging.DEBUG,\n    format=\"[%(asctime)s] %(levelname)s %(message)s\",\n    datefmt=\"%H:%M:%S\", stream=sys.stdout)\nlogger = logging.getLogger(__name__)\n\n\nclass Ensembler(object):\n    def __init__(self, model_dict, num_folds=3, task_type='classification', optimize=roc_auc_score,\n                 lower_is_better=False, save_path=None):\n        \"\"\"\n        Ensembler init function\n        :param model_dict: model dictionary, see README for its format\n        :param num_folds: the number of folds for ensembling\n        :param task_type: classification or regression\n        :param optimize: the function to optimize for, e.g. AUC, logloss, etc. Must have two arguments y_test and y_pred\n        :param lower_is_better: is lower value of optimization function better or higher\n        :param save_path: path to which model pickles will be dumped to along with generated predictions, or None\n        \"\"\"\n\n        self.model_dict = model_dict\n        self.levels = len(self.model_dict)\n        self.num_folds = num_folds\n        self.task_type = task_type\n        self.optimize = optimize\n        self.lower_is_better = lower_is_better\n        self.save_path = save_path\n\n        self.training_data = None\n        self.test_data = None\n        self.y = None\n        self.lbl_enc = None\n        self.y_enc = None\n        self.train_prediction_dict = None\n        self.test_prediction_dict = None\n        self.num_classes = None\n\n    def fit(self, training_data, y, lentrain):\n        \"\"\"\n        :param training_data: training data in tabular format\n        :param y: binary, multi-class or regression\n        :return: chain of models to be used in prediction\n        \"\"\"\n\n        self.training_data = training_data\n        self.y = y\n\n        if self.task_type == 'classification':\n            self.num_classes = len(np.unique(self.y))\n            logger.info(\"Found %d classes\", self.num_classes)\n            self.lbl_enc = LabelEncoder()\n            self.y_enc = self.lbl_enc.fit_transform(self.y)\n            kf = StratifiedKFold(n_splits=self.num_folds)\n            train_prediction_shape = (lentrain, self.num_classes)\n        else:\n            self.num_classes = -1\n            self.y_enc = self.y\n            kf = KFold(n_splits=self.num_folds)\n            train_prediction_shape = (lentrain, 1)\n\n        self.train_prediction_dict = {}\n        for level in range(self.levels):\n            self.train_prediction_dict[level] = np.zeros((train_prediction_shape[0],\n                                                          train_prediction_shape[1] * len(self.model_dict[level])))\n\n        for level in range(self.levels):\n\n            if level == 0:\n                temp_train = self.training_data\n            else:\n                temp_train = self.train_prediction_dict[level - 1]\n\n            for model_num, model in enumerate(self.model_dict[level]):\n                validation_scores = []\n                foldnum = 1\n                for train_index, valid_index in kf.split(self.train_prediction_dict[0], self.y_enc):\n                    logger.info(\"Training Level %d Fold # %d. Model # %d\", level, foldnum, model_num)\n\n                    if level != 0:\n                        l_training_data = temp_train[train_index]\n                        l_validation_data = temp_train[valid_index]\n                        model.fit(l_training_data, self.y_enc[train_index])\n                    else:\n                        l0_training_data = temp_train[0][model_num]\n                        if type(l0_training_data) == list:\n                            l_training_data = [x[train_index] for x in l0_training_data]\n                            l_validation_data = [x[valid_index] for x in l0_training_data]\n                        else:\n                            l_training_data = l0_training_data[train_index]\n                            l_validation_data = l0_training_data[valid_index]\n                        model.fit(l_training_data, self.y_enc[train_index])\n\n                    logger.info(\"Predicting Level %d. Fold # %d. Model # %d\", level, foldnum, model_num)\n\n                    if self.task_type == 'classification':\n                        temp_train_predictions = model.predict_proba(l_validation_data)\n                        self.train_prediction_dict[level][valid_index,\n                        (model_num * self.num_classes):(model_num * self.num_classes) +\n                                                       self.num_classes] = temp_train_predictions\n\n                    else:\n                        temp_train_predictions = model.predict(l_validation_data)\n                        self.train_prediction_dict[level][valid_index, model_num] = temp_train_predictions\n                    validation_score = self.optimize(self.y_enc[valid_index], temp_train_predictions)\n                    validation_scores.append(validation_score)\n                    logger.info(\"Level %d. Fold # %d. Model # %d. Validation Score = %f\", level, foldnum, model_num,\n                                validation_score)\n                    foldnum += 1\n                avg_score = np.mean(validation_scores)\n                std_score = np.std(validation_scores)\n                logger.info(\"Level %d. Model # %d. Mean Score = %f. Std Dev = %f\", level, model_num,\n                            avg_score, std_score)\n\n            logger.info(\"Saving predictions for level # %d\", level)\n            train_predictions_df = pd.DataFrame(self.train_prediction_dict[level])\n            train_predictions_df.to_csv(os.path.join(self.save_path, \"train_predictions_level_\" + str(level) + \".csv\"),\n                                        index=False, header=None)\n\n        return self.train_prediction_dict\n\n    def predict(self, test_data, lentest):\n        self.test_data = test_data\n        if self.task_type == 'classification':\n            test_prediction_shape = (lentest, self.num_classes)\n        else:\n            test_prediction_shape = (lentest, 1)\n\n        self.test_prediction_dict = {}\n        for level in range(self.levels):\n            self.test_prediction_dict[level] = np.zeros((test_prediction_shape[0],\n                                                         test_prediction_shape[1] * len(self.model_dict[level])))\n        self.test_data = test_data\n        for level in range(self.levels):\n            if level == 0:\n                temp_train = self.training_data\n                temp_test = self.test_data\n            else:\n                temp_train = self.train_prediction_dict[level - 1]\n                temp_test = self.test_prediction_dict[level - 1]\n\n            for model_num, model in enumerate(self.model_dict[level]):\n\n                logger.info(\"Training Fulldata Level %d. Model # %d\", level, model_num)\n                if level == 0:\n                    model.fit(temp_train[0][model_num], self.y_enc)\n                else:\n                    model.fit(temp_train, self.y_enc)\n\n                logger.info(\"Predicting Test Level %d. Model # %d\", level, model_num)\n\n                if self.task_type == 'classification':\n                    if level == 0:\n                        temp_test_predictions = model.predict_proba(temp_test[0][model_num])\n                    else:\n                        temp_test_predictions = model.predict_proba(temp_test)\n                    self.test_prediction_dict[level][:, (model_num * self.num_classes): (model_num * self.num_classes) +\n                                                                                        self.num_classes] = temp_test_predictions\n\n                else:\n                    if level == 0:\n                        temp_test_predictions = model.predict(temp_test[0][model_num])\n                    else:\n                        temp_test_predictions = model.predict(temp_test)\n                    self.test_prediction_dict[level][:, model_num] = temp_test_predictions\n\n            test_predictions_df = pd.DataFrame(self.test_prediction_dict[level])\n            test_predictions_df.to_csv(os.path.join(self.save_path, \"test_predictions_level_\" + str(level) + \".csv\"),\n                                       index=False, header=None)\n\n        return self.test_prediction_dict\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# specify the data to be used for every level of ensembling:\ntrain_data_dict = {0: [xtrain_tfv, xtrain_ctv, xtrain_tfv, xtrain_ctv], 1: [xtrain_glove]}\ntest_data_dict = {0: [xvalid_tfv, xvalid_ctv, xvalid_tfv, xvalid_ctv], 1: [xvalid_glove]}\n\nmodel_dict = {0: [LogisticRegression(), LogisticRegression(), MultinomialNB(alpha=0.1), MultinomialNB()],\n\n              1: [xgb.XGBClassifier(silent=True, n_estimators=120, max_depth=7)]}\n\nens = Ensembler(model_dict=model_dict, num_folds=3, task_type='classification',\n                optimize=multiclass_logloss, lower_is_better=True, save_path='')\n\nens.fit(train_data_dict, ytrain, lentrain=xtrain_glove.shape[0])\npreds = ens.predict(test_data_dict, lentest=xvalid_glove.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check error:\nmulticlass_logloss(yvalid, preds[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}