{"cells":[{"metadata":{},"cell_type":"markdown","source":"> **Beginners Guide to Text Generation using GRUs**\n\nText Generation is a type of Language Modelling problem. Language Modelling is the core problem for a number of of natural language processing tasks such as speech to text, conversational system, and text summarization. A trained language model learns the likelihood of occurrence of a word based on the previous sequence of words used in the text. Language models can be operated at character level, n-gram level, sentence level or even paragraph level. In this notebook, I will explain how to create a language model for generating natural language text by implement and training state-of-the-art Recurrent Neural Network."},{"metadata":{},"cell_type":"markdown","source":"**Import the libraries**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport nltk\nimport string\nimport unidecode\nimport random\nimport torch","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check if GPU is available\ntrain_on_gpu = torch.cuda.is_available()\nif(train_on_gpu):\n    print('Training on GPU!')\nelse: \n    print('No GPU available, training on CPU; consider making n_epochs very small.')","execution_count":25,"outputs":[{"output_type":"stream","text":"Training on GPU!\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"**Load the dataset**"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/train.csv')\nauthor = train_df[train_df['author'] == 'EAP'][\"text\"]\nauthor[:5]","execution_count":26,"outputs":[{"output_type":"execute_result","execution_count":26,"data":{"text/plain":"0    This process, however, afforded me no means of...\n2    In his left hand was a gold snuff box, from wh...\n6    The astronomer, perhaps, at this point, took r...\n7          The surcingle hung in ribands from my body.\n8    I knew that you could not say to yourself 'ste...\nName: text, dtype: object"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"**Dataset cleaning**"},{"metadata":{"trusted":true},"cell_type":"code","source":"text = list(author[:100])\ndef joinStrings(text):\n    return ' '.join(string for string in text)\ntext = joinStrings(text)\n# text = [item for sublist in author[:5].values for item in sublist]\nlen(text.split())","execution_count":37,"outputs":[{"output_type":"execute_result","execution_count":37,"data":{"text/plain":"344"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"stop = set(nltk.corpus.stopwords.words('english'))\nexclude = set(string.punctuation) \nlemma = nltk.stem.wordnet.WordNetLemmatizer()\ndef clean(doc):\n        stop_free = \" \".join([i for i in doc.split() if i not in stop])\n        punc_free = \"\".join(ch for ch in stop_free if ch not in exclude)\n        normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n        return normalized\ntest_sentence = clean(text).lower().split()","execution_count":38,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> **N-Gram Language Modeling**\n\nRecall that in an n-gram language model, given a sequence of words w, we want to compute.\n                                      * P(wi|wi−1,wi−2,…,wi−n+1)                                                     \nWhere wi is the ith word of the sequence.                                                                              here we will take n=2."},{"metadata":{"trusted":true},"cell_type":"code","source":"trigrams = [([test_sentence[i], test_sentence[i + 1]], test_sentence[i + 2])\n            for i in range(len(test_sentence) - 2)]\nchunk_len=len(trigrams)\nprint(trigrams[:3])","execution_count":40,"outputs":[{"output_type":"stream","text":"[(['this', 'process'], 'however'), (['process', 'however'], 'afforded'), (['however', 'afforded'], 'mean')]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab = set(test_sentence)\nvoc_len=len(vocab)\nword_to_ix = {word: i for i, word in enumerate(vocab)}\n","execution_count":66,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"inp=[]\ntar=[]\nfor context, target in trigrams:\n        context_idxs = torch.tensor([word_to_ix[w] for w in context], dtype=torch.long)\n        inp.append(context_idxs)\n        targ = torch.tensor([word_to_ix[target]], dtype=torch.long)\n        tar.append(targ)","execution_count":42,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**GRU model for Text Generation**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\nclass RNN(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n        super(RNN, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        self.n_layers = n_layers\n        \n        self.encoder = nn.Embedding(input_size, hidden_size)\n        self.gru = nn.GRU(hidden_size*2, hidden_size, n_layers,batch_first=True,\n                          bidirectional=False)\n        self.decoder = nn.Linear(hidden_size, output_size)\n    \n    def forward(self, input, hidden):\n        input = self.encoder(input.view(1, -1))\n        output, hidden = self.gru(input.view(1, 1, -1), hidden)\n        output = self.decoder(output.view(1, -1))\n        return output, hidden\n\n    def init_hidden(self):\n        return Variable(torch.zeros(self.n_layers, 1, self.hidden_size))","execution_count":43,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(inp, target):\n    hidden = decoder.init_hidden().cuda()\n    decoder.zero_grad()\n    loss = 0\n    \n    for c in range(chunk_len):\n        output, hidden = decoder(inp[c].cuda(), hidden)\n        loss += criterion(output, target[c].cuda())\n\n    loss.backward()\n    decoder_optimizer.step()\n\n    return loss.data.item() / chunk_len","execution_count":44,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import time, math\n\ndef time_since(since):\n    s = time.time() - since\n    m = math.floor(s / 60)\n    s -= m * 60\n    return '%dm %ds' % (m, s)","execution_count":45,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_epochs = 300\nprint_every = 100\nplot_every = 10\nhidden_size = 100\nn_layers = 1\nlr = 0.015\n\ndecoder = RNN(voc_len, hidden_size, voc_len, n_layers)\ndecoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\ncriterion = nn.CrossEntropyLoss()\n\nstart = time.time()\nall_losses = []\nloss_avg = 0\nif(train_on_gpu):\n    decoder.cuda()\nfor epoch in range(1, n_epochs + 1):\n    loss = train(inp,tar)       \n    loss_avg += loss\n\n    if epoch % print_every == 0:\n        print('[%s (%d %d%%) %.4f]' % (time_since(start), epoch, epoch / n_epochs * 50, loss))\n#         print(evaluate('ge', 200), '\\n')\n\n    if epoch % plot_every == 0:\n        all_losses.append(loss_avg / plot_every)\n        loss_avg = 0","execution_count":46,"outputs":[{"output_type":"stream","text":"[0m 16s (100 16%) 0.0001]\n[0m 33s (200 33%) 0.0000]\n[0m 51s (300 50%) 0.0000]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\n%matplotlib inline\n\nplt.figure()\nplt.plot(all_losses)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Generating the text**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluate(prime_str='this process', predict_len=100, temperature=0.8):\n    hidden = decoder.init_hidden().cuda()\n\n    for p in range(predict_len):\n        \n        prime_input = torch.tensor([word_to_ix[w] for w in prime_str.split()], dtype=torch.long).cuda()\n        inp = prime_input[-2:] #last two words as input\n        output, hidden = decoder(inp, hidden)\n        \n        # Sample from the network as a multinomial distribution\n        output_dist = output.data.view(-1).div(temperature).exp()\n        top_i = torch.multinomial(output_dist, 1)[0]\n        \n        # Add predicted word to string and use as next input\n        predicted_word = list(word_to_ix.keys())[list(word_to_ix.values()).index(top_i)]\n        prime_str += \" \" + predicted_word\n#         inp = torch.tensor(word_to_ix[predicted_word], dtype=torch.long)\n\n    return prime_str","execution_count":49,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(evaluate('this process', 40, temperature=1))","execution_count":62,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"CUDA error: device-side assert triggered","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-62-db81abe0acf5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'this process'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-49-fc837930d50f>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(prime_str, predict_len, temperature)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprime_str\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'this process'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(evaluate('i might', 30, temperature=1))","execution_count":67,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"CUDA error: device-side assert triggered","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-67-dd2f83a6c177>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i might'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-49-fc837930d50f>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(prime_str, predict_len, temperature)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprime_str\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'this process'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered"]}]},{"metadata":{},"cell_type":"markdown","source":"> **Improvement Ideas**\n\nAs we can see, the model has produced the output which looks fairly fine. The results can be improved further with following points:\n\n* Adding more data\n* Fine Tuning the network architecture\n* Fine Tuning the network parameters\n\nThanks for going through the notebook, please upvote if you liked."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}