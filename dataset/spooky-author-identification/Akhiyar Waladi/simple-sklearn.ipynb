{"nbformat_minor":1,"metadata":{"kernelspec":{"language":"python","name":"python3","display_name":"Python 3"},"language_info":{"file_extension":".py","name":"python","pygments_lexer":"ipython3","mimetype":"text/x-python","version":"3.6.3","nbconvert_exporter":"python","codemirror_mode":{"version":3,"name":"ipython"}}},"cells":[{"execution_count":null,"metadata":{"_uuid":"2d2597817515e1c84f48de80e43bd04d42a7858a","_cell_guid":"fdbb5a43-6a91-4e5f-b7fd-b541aa99523f"},"outputs":[],"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output."},{"execution_count":null,"metadata":{"_uuid":"dc2a434a0c61b7d9e5955076ab2e0252b844095e","_cell_guid":"b0c43711-0bf2-4db4-b54a-ecd6983a079a"},"outputs":[],"cell_type":"code","source":"dataTrain = pd.read_csv(\"../input/train.csv\")\ndataTrain.head(5)"},{"execution_count":null,"metadata":{"_uuid":"0779983526bc634992527fa924222b5454d559a3","_cell_guid":"22c24c9d-2137-4e66-9612-930de027f714"},"outputs":[],"cell_type":"code","source":"import seaborn as sns\nsns.countplot(dataTrain['author'])"},{"execution_count":null,"metadata":{"collapsed":true},"outputs":[],"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain, test = train_test_split(dataTrain, test_size = 0.3)"},{"execution_count":null,"metadata":{},"outputs":[],"cell_type":"code","source":"from bs4 import BeautifulSoup\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nenglish_stemmer=nltk.stem.SnowballStemmer('english')\nimport re\ndef text_to_wordlist( text, remove_stopwords=True):\n    # Function to convert a document to a sequence of words,\n    # optionally removing stop words.  Returns a list of words.\n    #\n    # 1. Remove HTML\n    review_text = BeautifulSoup(review, \"lxml\").get_text()\n\n    #\n    # 2. Remove non-letters\n    review_text = re.sub(\"[^a-zA-Z]\",\" \", review)\n    #\n    # 3. Convert words to lower case and split them\n    words = review_text.lower().split()\n    #\n    # 4. Optionally remove stop words (True by default)\n    if remove_stopwords:\n        stops = set(stopwords.words(\"english\"))\n        words = [w for w in words if not w in stops]\n\n    b=[]\n    stemmer = english_stemmer #PorterStemmer()\n    for word in words:\n        b.append(stemmer.stem(word))\n\n    # 5. Return a list of words\n    return(b)"},{"execution_count":null,"metadata":{"_uuid":"16990b87d4ea96e8c0e50bd9284ca4897e326a7a","_cell_guid":"7564d427-eeeb-40f3-89b5-d49a0c813441"},"outputs":[],"cell_type":"code","source":"clean_train_reviews = []\nfor review in train['text']:\n    clean_train_reviews.append( \" \".join(text_to_wordlist(review)))\n    \nclean_test_reviews = []\nfor review in test['text']:\n    clean_test_reviews.append( \" \".join(text_to_wordlist(review)))"},{"execution_count":null,"metadata":{},"outputs":[],"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer( min_df=2, max_df=0.95, max_features = 200000, ngram_range = ( 1, 4 ),\n                              sublinear_tf = True )\n\nvectorizer = vectorizer.fit(clean_train_reviews)\n\ntrain_features = vectorizer.transform(clean_train_reviews)\ntest_features = vectorizer.transform(clean_test_reviews)"},{"execution_count":null,"metadata":{},"outputs":[],"cell_type":"code","source":"from sklearn.feature_selection.univariate_selection import SelectKBest, chi2, f_classif\nfselect = SelectKBest(chi2 , k=10000)\ntrain_features = fselect.fit_transform(train_features, train[\"author\"])\ntest_features = fselect.transform(test_features)"},{"execution_count":null,"metadata":{},"outputs":[],"cell_type":"code","source":"from sklearn.linear_model import SGDClassifier, SGDRegressor\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.naive_bayes import MultinomialNB\n\nmodel1 = MultinomialNB(alpha=0.001)\nmodel1.fit( train_features, train[\"author\"] )\n\nmodel2 = SGDClassifier(loss='modified_huber', n_iter=5, random_state=0, shuffle=True)\nmodel2.fit( train_features, train[\"author\"] )\n\nmodel3 = RandomForestClassifier()\nmodel3.fit( train_features, train[\"author\"] )\n\nmodel4 = GradientBoostingClassifier()\nmodel4.fit( train_features, train[\"author\"] )\n\npred_1 = model1.predict( test_features.toarray() )\npred_2 = model2.predict( test_features.toarray() )\npred_3 = model3.predict( test_features.toarray() )\npred_4 = model4.predict( test_features.toarray() )"},{"execution_count":null,"metadata":{},"outputs":[],"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nprint('prediction 1 accuracy: ', accuracy_score(test['author'], pred_1))\nprint('prediction 2 accuracy: ', accuracy_score(test['author'], pred_2))\nprint('prediction 3 accuracy: ', accuracy_score(test['author'], pred_3))\nprint('prediction 4 accuracy: ', accuracy_score(test['author'], pred_4))"}],"nbformat":4}