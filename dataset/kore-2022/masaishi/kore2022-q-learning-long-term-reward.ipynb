{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"I made Q-learning starter notebook.\n\nIt's first time to try reinforcement learning, so if you find mistake, please tell me by comment. Also, I'm glad if you share me opinion to make good state key by comment.\n\n\nTherer are two improvements from the code in the beta version.\n1. This is Strategy base. Before that, agent select each action by observing the number of kore. It didn't work well maybe because it has to spend a large amount of time learning.\n2. Learn by long time reward. After launching ships, it require dozens of steps to get kore.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from kaggle_environments import make\n\nenv = make(\"kore_fleets\", debug=True)\nprint(env.name, env.version)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-26T19:33:46.571576Z","iopub.execute_input":"2022-04-26T19:33:46.572187Z","iopub.status.idle":"2022-04-26T19:33:46.77808Z","shell.execute_reply.started":"2022-04-26T19:33:46.572075Z","shell.execute_reply":"2022-04-26T19:33:46.777005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from kaggle_environments.envs.kore_fleets.helpers import *\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport random\nimport seaborn as sns\nfrom tqdm import tqdm\nimport itertools\nimport pickle","metadata":{"execution":{"iopub.status.busy":"2022-04-26T19:33:46.780497Z","iopub.execute_input":"2022-04-26T19:33:46.780938Z","iopub.status.idle":"2022-04-26T19:33:47.91293Z","shell.execute_reply.started":"2022-04-26T19:33:46.780868Z","shell.execute_reply":"2022-04-26T19:33:47.912012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Decide nvm\n# 0~50 -> 0.97\n# 50~ -> 0.98\n# 100~ -> 0.99\n\nrepeat_size=1000\nx = [i for i in range(repeat_size)]\ny = [min(0.977 + np.log(i*0.02+1)*0.005, 0.99) for i in range(repeat_size)]\nplt.plot(x, y)","metadata":{"execution":{"iopub.status.busy":"2022-04-16T20:12:38.904755Z","iopub.execute_input":"2022-04-16T20:12:38.905325Z","iopub.status.idle":"2022-04-16T20:12:39.11738Z","shell.execute_reply.started":"2022-04-16T20:12:38.905285Z","shell.execute_reply":"2022-04-16T20:12:39.116577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(x[:100], y[:100])","metadata":{"execution":{"iopub.status.busy":"2022-04-16T07:33:59.374548Z","iopub.execute_input":"2022-04-16T07:33:59.374803Z","iopub.status.idle":"2022-04-16T07:33:59.555816Z","shell.execute_reply.started":"2022-04-16T07:33:59.374774Z","shell.execute_reply":"2022-04-16T07:33:59.554907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Start Q-learning\nI refer to this article [kaggleで強化学習をやってみた](https://yukoishizaki.hatenablog.com/entry/2020/04/05/202935) to make QTable and QLearningAgent.","metadata":{}},{"cell_type":"code","source":"action_ratios = [\n    (5, 2, 2, 2),\n    (2, 5, 2, 2),\n    (2, 2, 5, 2),\n    (2, 2, 2, 5),\n    (1, 7, 4, 1),\n    (1, 4, 7, 1),\n    (1, 5, 1, 1),\n    (3, 5, 0, 0),\n    (1, 5, 5, 0),\n    (1, 5, 5, 5),\n    (1, 1, 5, 1),\n    (1, 4, 4, 8),\n    (1, 5, 2, 5),\n    (3, 5, 2, 5),\n    (1, 4, 8, 4),\n    (8, 5, 1, 1),\n    (3, 3, 3, 5),\n    (3, 3, 5, 3),\n    (3, 5, 3, 3),\n    (5, 3, 3, 3)\n]","metadata":{"execution":{"iopub.status.busy":"2022-04-16T07:34:01.310169Z","iopub.execute_input":"2022-04-16T07:34:01.311555Z","iopub.status.idle":"2022-04-16T07:34:01.320242Z","shell.execute_reply.started":"2022-04-16T07:34:01.311497Z","shell.execute_reply":"2022-04-16T07:34:01.319184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ship_max=3000\nx = [i for i in range(ship_max)]\ny = [np.log(i*0.01+1)*10 for i in range(ship_max)]\nplt.plot(x, y)\n","metadata":{"execution":{"iopub.status.busy":"2022-04-16T07:34:02.707577Z","iopub.execute_input":"2022-04-16T07:34:02.708549Z","iopub.status.idle":"2022-04-16T07:34:02.9332Z","shell.execute_reply.started":"2022-04-16T07:34:02.70849Z","shell.execute_reply":"2022-04-16T07:34:02.932198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class QTable():\n    def __init__(self, ratios):\n        self.Q = {} # Qテーブル\n        self.ratios = ratios\n    \n    def get_state_key(self, state):\n        board = Board(state, env.configuration)\n        me=board.current_player\n        me_obser = me._observation\n        \n        turn = board.step\n        kore_left = me.kore\n        \n        ship_number = 0\n        for value in me_obser[1].values():\n            ship_number += value[1]\n        \n        fleet_number = 0\n        for value in me_obser[2].values():\n            fleet_number += value[2]\n\n        return f'{int(turn/20)}'\n        \n    def get_q_values(self, state):\n        # 状態に対して、全actionのQ値の配列を出力\n        state_key = self.get_state_key(state)\n        if state_key not in self.Q.keys():\n            self.Q[state_key] = [0] * len(self.ratios)\n        return self.Q[state_key]\n    \n    def update(self, state, ratio_index, add_q):\n        # Q値を更新\n        state_key = self.get_state_key(state)\n        self.Q[state_key] = [q + add_q if idx == ratio_index else q for idx, q in enumerate(self.Q[state_key])]","metadata":{"execution":{"iopub.status.busy":"2022-04-16T07:34:04.291476Z","iopub.execute_input":"2022-04-16T07:34:04.291815Z","iopub.status.idle":"2022-04-16T07:34:04.301207Z","shell.execute_reply.started":"2022-04-16T07:34:04.291775Z","shell.execute_reply":"2022-04-16T07:34:04.300441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Helper\ndef pos_to_index(pos):\n    x = pos[0]\n    y = pos[1]\n    x = (x + 21) if x < 0 else x\n    x = (x - 21) if x > 20 else x\n    y = (y + 21) if y < 0 else y\n    y = (y - 21) if y > 20 else y\n\n    return x * 21 + y\n\n# Get straight score\ndef get_straight_score(pos, kore, length, direction):\n    score = 0\n    for i in range(length):\n        pos += direction.to_point()\n        score += kore[pos_to_index(pos)]\n    return score\n\ndef get_max_straight_plan(pos, kore, length):\n    scores = []\n    for i in range(4):\n        direction = Direction.from_index(i)\n        scores.append(get_straight_score(pos, kore, length, direction))\n    scores = sorted(scores, reverse=True)\n#     direction_index = scores.index(scores[np.random.randint(2)])\n    direction_index = scores.index(scores[0])\n    \n    direction = Direction.from_index(direction_index)\n    flight_plan = direction.to_char()\n    flight_plan += str(length)\n    flight_plan += direction.opposite().to_char()\n    return flight_plan\n\n\n# Get maximum cycle\ndef get_max_cycle_plan(pos, kore, length, direct_i):\n    direction = Direction.from_index(direct_i) \n    each_lengths = [i for i in range(1,length)]\n    each_lengths = [i for i in itertools.product(each_lengths, repeat=2)]\n    each_lengths = [i + tuple([i[0], i[1]]) for i in each_lengths]\n    \n    # Get max score\n    scores = []\n    for rotate_i in range(2):\n        for each_length in each_lengths:\n            each_pos = pos\n            each_direction = direction\n            score = 0\n            for direct_i in range(4):\n                for i in range(each_length[direct_i]):\n                    each_pos += each_direction.to_point()\n                    score += kore[pos_to_index(each_pos)]\n                if rotate_i == 0:\n                    each_direction = each_direction.rotate_right()\n                else:\n                    each_direction = each_direction.rotate_left()\n            scores.append(score)\n    \n    # Get flight plan\n    rotate_right = True\n    best_index = scores.index(max(scores))\n    if best_index >= len(each_lengths):\n        best_index -= len(each_lengths)\n        rotate_right = False\n        \n    best_each_length = each_lengths[best_index]\n    each_direction = direction\n    flight_plan = \"\"\n    for i in range(4):\n        flight_plan += each_direction.to_char()\n        \n        length = best_each_length[i] - 1\n        if i != 3 and length > 0:\n            flight_plan += str(length)\n        if rotate_right:\n            each_direction = each_direction.rotate_right()\n        else:\n            each_direction = each_direction.rotate_left()\n    return flight_plan\n\ndef get_closest_enemy_shipyard(board, position, me):\n    min_dist = 1000000\n    enemy_shipyard = None\n    for shipyard in board.shipyards.values():\n        if shipyard.player_id == me.id:\n            continue\n        dist = position.distance_to(shipyard.position, board.configuration.size)\n        if dist < min_dist:\n            min_dist = dist\n            enemy_shipyard = shipyard\n    return enemy_shipyard","metadata":{"execution":{"iopub.status.busy":"2022-04-16T08:32:12.270399Z","iopub.execute_input":"2022-04-16T08:32:12.270694Z","iopub.status.idle":"2022-04-16T08:32:12.290006Z","shell.execute_reply.started":"2022-04-16T08:32:12.270663Z","shell.execute_reply":"2022-04-16T08:32:12.288523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def shipyard_action(action_number, board, shipyard):\n    me = board.current_player\n    kore_left = me.kore\n    pos = shipyard.position\n    turn = board.step\n    spawn_cost = board.configuration.spawn_cost\n    observation = board.observation\n    kore = observation['kore']\n\n    if action_number == 0:\n        # Spawn\n        if int(kore_left / spawn_cost) == 0:\n            return None\n        spawn_max = shipyard.max_spawn\n        return ShipyardAction.spawn_ships(min(spawn_max, int(kore_left / spawn_cost)))\n    \n    elif action_number == 1:\n        # Cycle\n        if shipyard.ship_count < 21:\n            return None\n        length = random.randint(min(int(turn/20)+2, 6), 9)\n        direct_i = random.randint(0, 3)\n        flight_plan = get_max_cycle_plan(pos, kore, length, direct_i)\n        ship_number = 21\n        return ShipyardAction.launch_fleet_with_flight_plan(ship_number, flight_plan)\n    \n    elif action_number == 2:\n        # Invade\n        if shipyard.ship_count < 100 + 21:\n            return None\n\n        closest_enemy_shipyard = get_closest_enemy_shipyard(board, pos, me)\n        if not closest_enemy_shipyard:\n            return None\n        enemy_pos = closest_enemy_shipyard.position\n        my_pos = pos\n        flight_plan = \"N\" if enemy_pos.y > my_pos.y else \"S\"\n        flight_plan += str(abs(enemy_pos.y - my_pos.y) - 1)\n        flight_plan += \"W\" if enemy_pos.x < my_pos.x else \"E\"\n        if (abs(enemy_pos.y - my_pos.y) - 1) < 0:\n            return None\n        if not all([c in \"NESWC0123456789\" for c in flight_plan]):\n            return None\n\n        ship_number = 100\n        return ShipyardAction.launch_fleet_with_flight_plan(ship_number, flight_plan)\n\n    elif action_number == 3:\n        # Make shipyard\n        if shipyard.ship_count < 75 + 21:\n            return None\n\n        length = random.randint(3, 8)\n        direct_i = random.randint(0, 3)\n        flight_plan = get_max_cycle_plan(pos, kore, length, direct_i)\n        flight_plan = flight_plan[:-3]\n        flight_plan += 'C'\n        ship_number = max(75, int(shipyard.ship_count*(random.random()/2 + 0.5)))\n        return ShipyardAction.launch_fleet_with_flight_plan(ship_number, flight_plan)\n    ","metadata":{"execution":{"iopub.status.busy":"2022-04-16T08:31:59.959829Z","iopub.execute_input":"2022-04-16T08:31:59.960614Z","iopub.status.idle":"2022-04-16T08:31:59.972807Z","shell.execute_reply.started":"2022-04-16T08:31:59.960571Z","shell.execute_reply":"2022-04-16T08:31:59.971577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"env = make(\"kore_fleets\", debug=True)\n# # Balanced without attack\n# trainer = env.train([None, \"./balanced.py\"])\n\n# My best rule based model with attack\ntrainer = env.train([None, \"../input/kore2022bots/builder (46).py\"])\n\nclass QLearningAgent():\n    def __init__(self, env, epsilon=0.99):\n        self.env = env\n        self.ratios = action_ratios.copy()\n        self.q_table = QTable(self.ratios)\n        self.epsilon = epsilon\n        self.reward_log = []\n        self.current_policy = -1\n        \n    def policy(self, state, shipyard, kore_left):\n        if np.random.random() < self.epsilon:\n            # epsilonの割合で、ランダムにactionを選択する\n            return random.randint(0, len(self.ratios)-1)\n        else:\n            # ゲーム上選択可能で、Q値が最大なactionを選択する\n            q_values = self.q_table.get_q_values(state)\n            return int(np.argmax(q_values))\n        \n    def custom_reward(self, reward, done):\n        if done:\n            if reward == 1: # 勝ち\n                return 20\n            elif reward == 0: # 負け\n                return -20\n            else: # 引き分け\n                return 10\n        else:\n            return -0.05 # 勝負がついてない\n        \n    def learn(self, trainer, episode_cnt=100, gamma=0.6, \n              learn_rate=0.3, epsilon_decay_rate=0.99, min_epsilon=0.1):\n        for episode in tqdm(range(episode_cnt)):\n            # ゲーム環境リセット\n            state = trainer.reset() \n            # epsilonを徐々に小さくする\n            self.epsilon = max(min_epsilon, self.epsilon * epsilon_decay_rate) \n            while not env.done:\n                # Update q to each shipyard.\n                board = Board(state, self.env.configuration)\n                spawn_cost = board.configuration.spawn_cost\n\n                me = board.current_player\n                turn = board.step\n                kore_left = me.kore\n                \n                each_selections = []\n                # loop through all shipyards you control\n                for shipyard in me.shipyards:\n                    spawn_max = shipyard.max_spawn\n                    if int(kore_left / spawn_cost) > 0:\n                        shipyard.next_action = ShipyardAction.spawn_ships(min(spawn_max, int(kore_left / spawn_cost)))\n                    \n                    if turn % 10 == 0:\n                        ratio_idx = self.policy(state, shipyard, kore_left)\n                        self.current_policy = ratio_idx\n                    else:\n                        ratio_idx = self.current_policy\n                    ratios = self.ratios[ratio_idx]\n                    each_selections.append(ratio_idx)\n                    \n                    ratios = list(itertools.chain.from_iterable([[i]*n for i, n in enumerate(ratios)]))\n                    action_number = random.choice(ratios)\n                    action = shipyard_action(action_number, board, shipyard)\n                    if action != None:\n                        shipyard.next_action = action\n                next_state, reward, done, info = trainer.step(me.next_actions)\n                # reward = self.custom_reward(reward, done)\n                \n                if turn % 10 == 9:\n                    reward = kore_left\n                    action_value_rate = 1.2\n                    for value in me._observation[2].values():\n                        reward += (value[2] * (10 * action_value_rate))\n                    reward += (len(me.shipyards) * (50 * action_value_rate))\n                    gain = reward + gamma * max(self.q_table.get_q_values(next_state))\n                    \n                    estimate = self.q_table.get_q_values(state)[self.current_policy]\n                    self.q_table.update(state, self.current_policy, learn_rate * (gain - estimate))\n                    \n                state = next_state\n            self.reward_log.append(reward)","metadata":{"execution":{"iopub.status.busy":"2022-04-16T08:32:00.526175Z","iopub.execute_input":"2022-04-16T08:32:00.52648Z","iopub.status.idle":"2022-04-16T08:32:00.596167Z","shell.execute_reply.started":"2022-04-16T08:32:00.52645Z","shell.execute_reply":"2022-04-16T08:32:00.595442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"episode_cnt=1500\nepsilon_decay_rate=0.9995\nx = [i for i in range(episode_cnt)]\ny = [max(0.99*(epsilon_decay_rate**i), 0.1) for i in range(episode_cnt)]\nplt.plot(x, y)","metadata":{"execution":{"iopub.status.busy":"2022-04-23T01:52:44.009612Z","iopub.execute_input":"2022-04-23T01:52:44.009925Z","iopub.status.idle":"2022-04-23T01:52:44.221361Z","shell.execute_reply.started":"2022-04-23T01:52:44.009894Z","shell.execute_reply":"2022-04-23T01:52:44.220235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Learning\nqa = QLearningAgent(env)\nqa.learn(trainer, episode_cnt=episode_cnt, epsilon_decay_rate=epsilon_decay_rate)\n# qa.learn(trainer, episode_cnt=1000)","metadata":{"execution":{"iopub.status.busy":"2022-04-16T08:33:20.740515Z","iopub.execute_input":"2022-04-16T08:33:20.740743Z","iopub.status.idle":"2022-04-16T08:33:58.393608Z","shell.execute_reply.started":"2022-04-16T08:33:20.740717Z","shell.execute_reply":"2022-04-16T08:33:58.392629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Mean of transition of reward\nsns.set(style='darkgrid')\npd.DataFrame({'Average Reward': qa.reward_log}).rolling(10).mean().plot(figsize=(10,5))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-16T08:33:58.395282Z","iopub.execute_input":"2022-04-16T08:33:58.395954Z","iopub.status.idle":"2022-04-16T08:33:58.619502Z","shell.execute_reply.started":"2022-04-16T08:33:58.39592Z","shell.execute_reply":"2022-04-16T08:33:58.61858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set(style='darkgrid')\npd.DataFrame({'Average Reward': qa.reward_log}).plot(figsize=(10,5))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-16T08:33:58.620766Z","iopub.execute_input":"2022-04-16T08:33:58.621018Z","iopub.status.idle":"2022-04-16T08:33:58.868611Z","shell.execute_reply.started":"2022-04-16T08:33:58.62099Z","shell.execute_reply":"2022-04-16T08:33:58.867703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tmp_dict_q_table = qa.q_table.Q.copy()\ndict_q_table = dict()\n\n# Replace best q-value action ratio\nfor k in tmp_dict_q_table:\n    if np.count_nonzero(tmp_dict_q_table[k]) > 0:\n        dict_q_table[k] = np.argsort(tmp_dict_q_table[k]).argsort().tolist()[0]","metadata":{"execution":{"iopub.status.busy":"2022-04-16T08:33:58.870383Z","iopub.execute_input":"2022-04-16T08:33:58.8706Z","iopub.status.idle":"2022-04-16T08:33:58.876293Z","shell.execute_reply.started":"2022-04-16T08:33:58.870573Z","shell.execute_reply":"2022-04-16T08:33:58.875292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"my_agent = '''\nfrom kaggle_environments.envs.kore_fleets.helpers import *\nimport numpy as np\nimport itertools\nimport random\n# Helper\ndef pos_to_index(pos):\n    x = pos[0]\n    y = pos[1]\n    x = (x + 21) if x < 0 else x\n    x = (x - 21) if x > 20 else x\n    y = (y + 21) if y < 0 else y\n    y = (y - 21) if y > 20 else y\n\n    return x * 21 + y\n\n# Get straight score\ndef get_straight_score(pos, kore, length, direction):\n    score = 0\n    for i in range(length):\n        pos += direction.to_point()\n        score += kore[pos_to_index(pos)]\n    return score\n\ndef get_max_straight_plan(pos, kore, length):\n    scores = []\n    for i in range(4):\n        direction = Direction.from_index(i)\n        scores.append(get_straight_score(pos, kore, length, direction))\n    scores = sorted(scores, reverse=True)\n    direction_index = scores.index(scores[0])\n\n    direction = Direction.from_index(direction_index)\n    flight_plan = direction.to_char()\n    flight_plan += str(length)\n    flight_plan += direction.opposite().to_char()\n    return flight_plan\n\n\n# Get maximum cycle\ndef get_max_cycle_plan(pos, kore, length, direct_i):\n    direction = Direction.from_index(direct_i) \n    each_lengths = [i for i in range(1,length)]\n    each_lengths = [i for i in itertools.product(each_lengths, repeat=2)]\n    each_lengths = [i + tuple([i[0], i[1]]) for i in each_lengths]\n    \n    # Get max score\n    scores = []\n    for rotate_i in range(2):\n        for each_length in each_lengths:\n            each_pos = pos\n            each_direction = direction\n            score = 0\n            for direct_i in range(4):\n                for i in range(each_length[direct_i]):\n                    each_pos += each_direction.to_point()\n                    score += kore[pos_to_index(each_pos)]\n                if rotate_i == 0:\n                    each_direction = each_direction.rotate_right()\n                else:\n                    each_direction = each_direction.rotate_left()\n            scores.append(score)\n    \n    # Get flight plan\n    rotate_right = True\n    best_index = scores.index(max(scores))\n    if best_index >= len(each_lengths):\n        best_index -= len(each_lengths)\n        rotate_right = False\n        \n    best_each_length = each_lengths[best_index]\n    each_direction = direction\n    flight_plan = \"\"\n    for i in range(4):\n        flight_plan += each_direction.to_char()\n        \n        length = best_each_length[i] - 1\n        if i != 3 and length > 0:\n            flight_plan += str(length)\n        if rotate_right:\n            each_direction = each_direction.rotate_right()\n        else:\n            each_direction = each_direction.rotate_left()\n    return flight_plan\n\n\ndef get_closest_enemy_shipyard(board, position, me):\n    min_dist = 1000000\n    enemy_shipyard = None\n    for shipyard in board.shipyards.values():\n        if shipyard.player_id == me.id:\n            continue\n        dist = position.distance_to(shipyard.position, board.configuration.size)\n        if dist < min_dist:\n            min_dist = dist\n            enemy_shipyard = shipyard\n    return enemy_shipyard\n\n\ndef shipyard_action(action_number, board, shipyard):\n    me = board.current_player\n    kore_left = me.kore\n    pos = shipyard.position\n    turn = board.step\n    spawn_cost = board.configuration.spawn_cost\n    observation = board.observation\n    kore = observation['kore']\n\n    if action_number == 0:\n        # Spawn\n        if int(kore_left / spawn_cost) == 0:\n            return None\n        spawn_max = shipyard.max_spawn\n        return ShipyardAction.spawn_ships(min(spawn_max, int(kore_left / spawn_cost)))\n    \n    elif action_number == 1:\n        # Cycle\n        if shipyard.ship_count < 21:\n            return None\n        length = random.randint(min(int(turn/20)+2, 6), 9)\n        direct_i = random.randint(0, 3)\n        flight_plan = get_max_cycle_plan(pos, kore, length, direct_i)\n        ship_number = 21\n        return ShipyardAction.launch_fleet_with_flight_plan(ship_number, flight_plan)\n    \n    elif action_number == 2:\n        # Invade\n        if shipyard.ship_count < 100 + 21:\n            return None\n\n        closest_enemy_shipyard = get_closest_enemy_shipyard(board, pos, me)\n        if not closest_enemy_shipyard:\n            return None\n        enemy_pos = closest_enemy_shipyard.position\n        my_pos = pos\n        flight_plan = \"N\" if enemy_pos.y > my_pos.y else \"S\"\n        flight_plan += str(abs(enemy_pos.y - my_pos.y) - 1)\n        flight_plan += \"W\" if enemy_pos.x < my_pos.x else \"E\"\n        if (abs(enemy_pos.y - my_pos.y) - 1) < 0:\n            return None\n        if not all([c in \"NESWC0123456789\" for c in flight_plan]):\n            return None\n\n        ship_number = 100\n        return ShipyardAction.launch_fleet_with_flight_plan(ship_number, flight_plan)\n\n    elif action_number == 3:\n        # Make shipyard\n        if shipyard.ship_count < 75 + 21:\n            return None\n\n        length = random.randint(3, 8)\n        direct_i = random.randint(0, 3)\n        flight_plan = get_max_cycle_plan(pos, kore, length, direct_i)\n        flight_plan = flight_plan[:-3]\n        flight_plan += 'C'\n        ship_number = max(75, int(shipyard.ship_count*(random.random()/2 + 0.5)))\n        return ShipyardAction.launch_fleet_with_flight_plan(ship_number, flight_plan)\n\naction_ratios = [\n    (5, 2, 2, 2),\n    (2, 5, 2, 2),\n    (2, 2, 5, 2),\n    (2, 2, 2, 5),\n    (1, 7, 4, 1),\n    (1, 4, 7, 1),\n    (1, 5, 1, 1),\n    (3, 5, 0, 0),\n    (1, 5, 5, 0),\n    (1, 5, 5, 5),\n    (1, 1, 5, 1),\n    (1, 4, 4, 8),\n    (1, 5, 2, 5),\n    (3, 5, 2, 5),\n    (1, 4, 8, 4),\n    (8, 5, 1, 1),\n    (3, 3, 3, 5),\n    (3, 3, 5, 3),\n    (3, 5, 3, 3),\n    (5, 3, 3, 3)\n]\n\ndef agent(obs, config):\n    # Convert table to string for using as python file.\n    q_table = ''' \\\n    + str(dict_q_table).replace(' ', '') \\\n    + '''\n    \n    board = Board(obs, config)\n    me=board.current_player\n    me_obser = me._observation\n\n    spawn_cost = board.configuration.spawn_cost\n    turn = board.step\n    kore_left = me.kore\n    \n    observation = board.observation\n    kore = observation['kore']\n\n    ship_number = 0\n    for value in me_obser[1].values():\n        ship_number += value[1]\n    fleet_number = 0\n    for value in me_obser[2].values():\n        fleet_number += value[2]\n        \n    state_key = f'{int(turn/20)}'\n\n    each_selections = []\n    # loop through all shipyards you control\n    for shipyard in me.shipyards:\n        spawn_max = shipyard.max_spawn\n        if int(kore_left / spawn_cost) > 0:\n            shipyard.next_action = ShipyardAction.spawn_ships(min(spawn_max, int(kore_left / spawn_cost)))\n\n        if state_key not in q_table.keys():\n            ratio_idx = random.randint(0, len(action_ratios)-1)\n        else:\n            ratio_idx = q_table[state_key]\n        ratios = action_ratios[ratio_idx]\n        ratios = list(itertools.chain.from_iterable([[i]*n for i, n in enumerate(ratios)]))\n        each_selections.append(ratio_idx)\n        action_number = random.choice(ratios)\n        action = shipyard_action(action_number, board, shipyard)\n        if action != None:\n            shipyard.next_action = action\n    return me.next_actions\n    '''\n\nwith open('submission.py', 'w') as f:\n    f.write(my_agent)","metadata":{"execution":{"iopub.status.busy":"2022-04-16T08:33:58.877772Z","iopub.execute_input":"2022-04-16T08:33:58.878081Z","iopub.status.idle":"2022-04-16T08:33:58.895921Z","shell.execute_reply.started":"2022-04-16T08:33:58.878048Z","shell.execute_reply":"2022-04-16T08:33:58.894916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"env.run([\"./submission.py\", \"../input/kore2022bots/builder (26).py\"])\nenv.render(mode=\"ipython\", width=1000, height=800)","metadata":{"execution":{"iopub.status.busy":"2022-04-16T08:33:58.89732Z","iopub.execute_input":"2022-04-16T08:33:58.897642Z","iopub.status.idle":"2022-04-16T08:34:04.707084Z","shell.execute_reply.started":"2022-04-16T08:33:58.897601Z","shell.execute_reply":"2022-04-16T08:34:04.706198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I made a q-leraning starter notebook, but I can't train this agent well.\nI try 700 times in 11 hours by competing my best model which don't includes combat action. However, that agent's public score is 554.0.\n\n<br />\n\nThere are several hypothetical reasons why it did not work.\n1. The way the state is created is wrong.\n2. I need to train more times such as 10000.\n3. I need to let agent observe the board more using deep q-laning, etc.\n4. In the end, the rule-based approach is stronger.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}