{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Optimizing the Neural Network\nThis aim of this jupyter notebook is to build a simple Neural Network and optimize it, not to solve the uncertainty problem in the m5 forecasting dataset. Please don't try notebook for M5 forecasting competition. Learning source: **[Introduction to Deep Learning](https://learn.datacamp.com/courses/introduction-to-deep-learning-in-python) ** in Python from DataCamp\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"![](https://imgur.com/0aY4rqr.png)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Import necessary dataset & libraries","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Clear previous memories\nfrom IPython import get_ipython\nget_ipython().magic('reset -sf')\n\n# Import necessary libraries\nimport pandas as pd\n\n# Import the dataset\ndata_calendar = pd.read_csv('../input/m5-forecasting-uncertainty/calendar.csv')\ndata_sales_train_evaluation = pd.read_csv('../input/m5-forecasting-uncertainty/sales_train_evaluation.csv')\ndata_sales_train_validation = pd.read_csv('../input/m5-forecasting-uncertainty/sales_train_validation.csv')\ndata_sample_submission = pd.read_csv('../input/m5-forecasting-uncertainty/sample_submission.csv')\ndata_sell_prices = pd.read_csv('../input/m5-forecasting-uncertainty/sell_prices.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Shape of the datasets\nprint('data_calendar \\nShape: ', data_calendar.shape,)\nprint('data_sales_train_evaluation \\nShape: ', data_sales_train_evaluation.shape)\nprint('data_sales_train_validation \\nShape: ', data_sales_train_validation.shape)\nprint('data_sample_submission \\nShape: ', data_sample_submission.shape)\nprint('data_sell_prices \\nShape: ', data_sell_prices.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print('data_calendar \\nShape: ', data_calendar.shape, '\\n', data_calendar.head())\n#print('---------------\\ndata_sales_train_evaluation \\nShape: ', data_sales_train_evaluation.shape, '\\n', data_sales_train_evaluation.head())\nprint('---------------\\ndata_sales_train_validation \\nShape: ', data_sales_train_validation.shape, '\\n', data_sales_train_validation.head())\n#print('---------------\\ndata_sample_submission \\nShape: ', data_sample_submission.shape, '\\n', data_sample_submission.head())\n#print('---------------\\ndata_sell_prices \\nShape: ', data_sell_prices.shape, '\\n', data_sell_prices.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print('---------------\\ndata_sales_train_evaluation \\nShape: ', data_sales_train_evaluation.shape, '\\n', data_sales_train_evaluation.head())\n# I know it almost does not make sense to build the following dataset, but I am doing it just to implement the NN I learnt from the \"Intro to Deep Learning\" course.\n# I am just slicing last column as response & previous 10 columns as predictors\npredictors = data_sales_train_evaluation.iloc[:,1936:1945]\npredictors.head()\nresponse = data_sales_train_evaluation.iloc[:,1946]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model Building - Neural Network","execution_count":null},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Note: Recurrent Neural Network or Bayesian Neural Network would be more appropriate. But, I am going to implement a simple generic NN anyway.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Specifying a Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import necessary modules\nimport keras\nfrom keras.layers import Dense\nfrom keras.models import Sequential\n# Save the number of columns in predictors: \nn_cols = predictors.shape[1]\n# Set up the model: model\nmodel = Sequential()\n# Add the first layer\nmodel.add(Dense(50, activation='relu', input_shape=(n_cols,)))\n# Add the second layer\nmodel.add(Dense(32, activation='relu', input_shape=(n_cols,)))\n# Add the output layer\nmodel.add(Dense(1, input_shape=(n_cols,)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Compile & Fit the model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# fitting a model \n# Applying backpropagation and gradient descent with your data to update the weights.\nmodel.compile(optimizer='sgd', loss='mean_squared_error')\nmodel.fit(predictors, response)\n# Scale the data before fitting can ease optimization\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, our MSE is 31.50 at first epoch. We may run 10 epoch to get a better performance! Lets optimize","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Optimizing the Neural Network","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model_1_training = model.fit(predictors, response, epochs=30, validation_split=0.2, verbose=False)\nmodel_1_training","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# But technically we may not need 10 epoch or may be more epoch. Running extra epoch is definitely computationally costly. \n# Lets perform Early Stopping\nfrom keras.callbacks import EarlyStopping\nimport matplotlib.pyplot as plt\n\n# Compile the Model\nmodel.compile(optimizer='sgd', loss='mean_squared_error')\n# Define early_stopping_monitor\nearly_stopping_monitor = EarlyStopping(patience=3)\n\n# Fit model_2\nmodel_2_training = model.fit(predictors, response, epochs=20, validation_split=0.3, callbacks=[early_stopping_monitor], verbose=False)\n\n# Create the plot\nplt.plot(model_1_training.history['val_loss'], 'r', model_2_training.history['val_loss'], 'b')\nplt.xlabel('Epochs')\nplt.ylabel('Validation score')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Using models\n* Save\n* Reload\n* Make prediction","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import load_model\nmodel.save('model_file_M5.h5')\nnn_model = load_model('model_file_M5.h5')\n# predictions = nn_model.predict(data_to_predict_with)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}