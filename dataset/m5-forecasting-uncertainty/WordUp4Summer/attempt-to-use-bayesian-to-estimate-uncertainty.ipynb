{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Goal  ⛳️\n\n* Use bayesian model to forecast daily sales and estimate the posterior interval\n\n### Why bayesian model?\n\n* In this competition we can utilize a lot of historical data (prior); and by updating the prior belief we can make a forecast (which is the posterior)\n* Bayesian model allows to estimate posterior predictive interval on parameters and response variables\n\n### Limitation\n\n* Here we only modeled the total sale and its uncertainty\n* To make the full prediction we need to scale it up to full hierarchies (aggregated by state, by store, by department etc)\n* There can be more features/dimensions\n* Needs a lot of experimentation on initializing the paramter, due to the overdispersion\n\n### Referenced Notebook\n\nhttps://www.kaggle.com/allunia/m5-uncertainty\n\n\n## Problem formulation\n\nWe considered the sale of products is a poisson process, i.e, increasing exposing variable (days of operating) in this case, the rate of daily product sale is $\\lambda$\n\n\n$$ y | \\beta, X_i \\sim indep. Poisson(\\lambda_{i})$$\nwhere $$  \\lambda = rt$$\n\n$$ log(\\lambda) \\sim log(t) + log(r) $$\nwhere $$r = \\beta_{i}X $$\n\nhere we call log(t) offset, and ideally X doesn't include information on exposure variable (t)\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"✈️*please upvote if you like it* 🚀","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Import Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom  datetime import datetime, timedelta\nimport gc\nimport numpy as np\n#plt.style.use('ggplot')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\nimport sys\nimport re\n\nplt.style.use('seaborn-darkgrid')\nimport seaborn as sns\nimport patsy as pt\nimport pymc3 as pm\n\nplt.rcParams['figure.figsize'] = 14, 6\nnp.random.seed(0)\nprint('Running on PyMC3 v{}'.format(pm.__version__))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#os.listdir('../m5-forecasting-uncertainty/')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv('../input/m5-forecasting-uncertainty/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#submission.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sale = pd.read_csv('../input/m5-forecasting-uncertainty/sales_train_validation.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sale.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sale.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total_historical = sale.iloc[:,6:].sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total_historical.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"calendar = pd.read_csv('../input/m5-forecasting-uncertainty/calendar.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"calendar['event_true_1'] = calendar.event_name_1.notna()\ncalendar['event_true_2'] = calendar.event_name_2.notna()\n\ncalendar['event_true_all'] = calendar.event_true_1 + calendar.event_true_2\ncalendar['event_true_all'] = calendar.event_true_all.apply(lambda x: x>0)\ncalendar['event_true_all'] = calendar.event_true_all.astype('int')\ncalendar['date'] = pd.to_datetime(calendar.date)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#calendar.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#calendar.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"calendar['d_parse'] = calendar.d.apply(lambda x: int(x.split('_')[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#calendar.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"calendar_feature = calendar[['wm_yr_wk', 'wday', 'month', 'year', \\\n       'snap_CA', 'snap_TX', 'snap_WI', \\\n       'event_true_all', 'd_parse']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"calendar_feature.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Build Bayesian model in Pymc3","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# specify formula\nfml = 'total ~ wday + month + year + snap_CA + snap_TX + snap_WI + event_true_all + d_parse'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Standardize data\nTo help with model convergence, it is better to standardardize your data first","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler, StandardScaler, Normalizer\nscaler = StandardScaler()\n#minmax = MinMaxScaler()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"calendar_feature = calendar[['wm_yr_wk', 'wday', 'month', 'year', \\\n       'snap_CA', 'snap_TX', 'snap_WI', \\\n       'event_true_all', 'd_parse']]\n\nscaled_feature = pd.DataFrame(scaler.fit_transform(calendar_feature))\nscaled_feature.columns = calendar_feature.columns\nscaled_feature.min()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Correct outliers\n\nIt seems the outliers heavily impacted the way the model converged, so we also corrected those","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"np.where(total_historical < 10000)[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total_historical.iloc[[ 330,  696, 1061, 1426, 1791]]=np.quantile(total_historical, 0.025)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.min(total_historical)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Create model and update it using MCMC","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#minmax_feature.iloc[:1913,9]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create data frame\ndf = scaled_feature.iloc[:1913,:]\ndf.loc[:,'total'] = total_historical.values\ndf.loc[:, 'd_parse'] = calendar_feature.iloc[:1913, 8] - np.min(calendar_feature.d_parse) + 1\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(mx_en, mx_ex) = pt.dmatrices(fml, df, return_type='dataframe', NA_action='raise')\npd.concat((mx_ex.head(3),mx_ex.tail(3)))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with pm.Model() as mdl_first:\n\n    # define priors, weakly informative Normal\n    # here we tried to remove all the time variable and \n    # treat all these as 'attributes' of data rather than the exposure\n    b0 = pm.Normal('b0_intercept', mu=0, sigma=1)\n    b2 = pm.Normal('b2_wday', mu=0, sigma=1)\n    b3 = pm.Normal('b3_month', mu=0, sigma=1)\n    b4 = pm.Normal('b4_year', mu=0, sigma=1)\n    b5 = pm.Normal('b5_snapCA', mu=0, sigma=1)\n    b6 = pm.Normal('b6_snapTX', mu=0, sigma=1)\n    b7 = pm.Normal('b7_snapWI', mu=0, sigma=1)\n    b8 = pm.Normal('b8_event_true_all', mu=-0.01, sigma=1)\n\n    # define linear model and exp link function\n    theta = (b0 +\n            b2 * mx_ex['wday'] +\n            b3 * mx_ex['month'] + \n            b4 * mx_ex['year'] + \n            b5 * mx_ex['snap_CA'] + \n             b6 * mx_ex['snap_TX'] + \n             b7 * mx_ex['snap_WI'] + \n             b8 * mx_ex['event_true_all'] + \n              np.log(mx_ex['d_parse'] ))  ## there is the log(t) as an offset\n\n    ## Define Poisson likelihood\n    y = pm.Poisson('y', mu=np.exp(theta), observed=mx_en['total'].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with mdl_first:\n    trace = pm.sample(1000, tune=2000, init='adapt_diag', target_accept =.8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mdl_first.check_test_point()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## helper function from pymc documentation\ndef strip_derived_rvs(rvs):\n    '''Convenience fn: remove PyMC3-generated RVs from a list'''\n    ret_rvs = []\n    for rv in rvs:\n        if not (re.search('_log',rv.name) or re.search('_interval',rv.name)):\n            ret_rvs.append(rv)\n    return ret_rvs\n\n\ndef plot_traces_pymc(trcs, varnames=None):\n    ''' Convenience fn: plot traces with overlaid means and values '''\n\n    nrows = len(trcs.varnames)\n    if varnames is not None:\n        nrows = len(varnames)\n\n    ax = pm.traceplot(trcs, var_names=varnames, figsize=(12,nrows*1.4),\n                      lines=tuple([(k, {}, v['mean'])\n                                   for k, v in pm.summary(trcs, varnames=varnames).iterrows()]))\n\n    for i, mn in enumerate(pm.summary(trcs, varnames=varnames)['mean']):\n        ax[i,0].annotate('{:.2f}'.format(mn), xy=(mn,0), xycoords='data',\n                         xytext=(5,10), textcoords='offset points', rotation=90,\n                         va='bottom', fontsize='large', color='#AA0022')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rvs_fish = [rv.name for rv in strip_derived_rvs(mdl_first.unobserved_RVs)]\npm.summary(trace, varnames=rvs_fish)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Results\n\nWe can see the posterior parameters have been estimated with very little variance; r_hat is the [gelman-rubin statistics for convergence ](https://www.stata.com/new-in-stata/gelman-rubin-convergence-diagnostic/). The r_hat = 1 indicates that the simulated chains have been converged. Although this is not a good estimation (by looking at the ess effective sample size), so far we will temporily use this to estimate the **posterior interval**.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pm.plot_trace(trace)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Sample posterior predictive parameters","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"with mdl_first:\n    pp_trace = pm.sample_posterior_predictive(trace, var_names=rvs_fish, samples=4000)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Create submission data set","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Here since we only modeled the total sales, we will specifically use test set that indicates the total sale","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_2 = scaled_feature.iloc[1913:,:]\ntotal_id = [i for i in submission.id if 'Total' in i]\n# change back d_parse\ndf_2['d_parse']= calendar_feature.iloc[1913:,:].d_parse.values\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_2.d_parse.max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_validation = df_2.iloc[:28, :]\nsubmission_evaluation = df_2.iloc[28:, :]\nsubmission_validation.shape,submission_evaluation.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Use posterior predictive parameters to estimate the posterior interval of Y (uncertainty)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pp_trace.keys()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pp_trace['b0_intercept']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def return_y(df):\n    result = 1*pp_trace['b0_intercept']\n    for (i,j) in zip([*pp_trace.keys()][1:], df.index[1:]):\n        #print(i, j)\n        result += pp_trace[i]*df[j]\n        #print(result)\n    return np.exp(result + np.log(df['d_parse']))\n    #return result\nvalidation_y = np.zeros((28, 4000))\nevaluation_y = np.zeros((28, 4000))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_validation.iloc[0].index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#submission_evaluation.iloc[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for row in range(len(submission_validation)):\n    validation_y[row, :] = return_y(submission_validation.iloc[row])\n    evaluation_y[row, :] = return_y(submission_evaluation.iloc[row])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.mean(validation_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.mean(total_historical)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## organize the data\ntotal_qt = [float(i.split('_')[2]) for i in total_id]\n\ntotal_only_submission = submission[submission.id.isin(total_id)]\n\ntotal_only_submission['qt']=total_qt\n\ntotal_only_submission.reset_index(inplace=True)\n\ntotal_only_submission.loc[:7]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(1,29):\n    col_name = 'F' + str(i)\n    total_only_submission.loc[:8,col_name] =np.quantile(validation_y[i-1], total_qt[:9])\n\nfor i in range(1,29):\n    col_name = 'F' + str(i)\n    total_only_submission.loc[9:,col_name] =np.quantile(evaluation_y[i-1], total_qt[:9])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total_only_submission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total_only_submission.to_csv('total_submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## For improvement...\n\n* Adding more features\n* Adding different hierachies \n* Better prior: it seems really tricky to update the MCMC chain because the overdispersed prior, I have to tune the prior condition multiple times to get a good convergence\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}