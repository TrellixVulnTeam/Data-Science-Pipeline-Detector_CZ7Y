{"cells":[{"metadata":{"trusted":true},"cell_type":"markdown","source":"If you like it, please upvote:)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"My aim is to show that uncertainty, in some sense, decreases on higher levels of aggregation.\n\nThe bulk of the work in this kernel is done by [@kneroma](https://www.kaggle.com/kneroma) in [this kernel](https://www.kaggle.com/kneroma/from-point-to-uncertainty-prediction), so make sure to check it out.\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd, numpy as np\nfrom matplotlib import pyplot as plt\n\nimport scipy.stats  as stats","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.options.display.max_columns = 50","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best = pd.read_csv(\"../input/m5-accuracy-submission-file/submission.csv\")\nbest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales = pd.read_csv(\"../input/m5-forecasting-uncertainty/sales_train_evaluation.csv\")\nsales.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = best.merge(sales[[\"id\", \"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"]], on = \"id\")\nsub[\"_all_\"] = \"Total\"\nsub.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Different ratios for different aggregation levels\n\nThe higher the aggregation level, the more confident we are in the point prediction --> lower coef, relatively smaller range of quantiles","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"qs = np.array([0.005,0.025,0.165,0.25, 0.5, 0.75, 0.835, 0.975, 0.995])\nqs.shape\n\n\ndef get_ratios(coef=0.15):\n    qs2 = np.log(qs/(1-qs))*coef\n    ratios = stats.norm.cdf(qs2)\n    ratios /= ratios[4]\n    ratios = pd.Series(ratios, index=qs)\n    return ratios.round(3)\n\n#coef between 0.05 and 0.24 is used, probably suboptimal values for now\n\nlevel_coef_dict = {\"id\": get_ratios(coef=0.3), \"item_id\": get_ratios(coef=0.15),\n                   \"dept_id\": get_ratios(coef=0.08), \"cat_id\": get_ratios(coef=0.07),\n                   \"store_id\": get_ratios(coef=0.08), \"state_id\": get_ratios(coef=0.07), \"_all_\": get_ratios(coef=0.05),\n                   (\"state_id\", \"item_id\"): get_ratios(coef=0.19),  (\"state_id\", \"dept_id\"): get_ratios(coef=0.1),\n                    (\"store_id\",\"dept_id\") : get_ratios(coef=0.11), (\"state_id\", \"cat_id\"): get_ratios(coef=0.08),\n                    (\"store_id\",\"cat_id\"): get_ratios(coef=0.1)\n                  }\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see how ranges differ!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"level_coef_dict[\"id\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"level_coef_dict[\"cat_id\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For the the lowest level (30490 series), the smallest and biggest quantiles are 20% and 180% of the point prediction. For categories (3 series), the model will be way more confident: the smallest quantile will be 71%, the biggest will be 129% of the point prediction.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ndef quantile_coefs(q, level):\n    ratios = level_coef_dict[level]\n               \n    return ratios.loc[q].values\n\ndef get_group_preds(pred, level):\n    df = pred.groupby(level)[cols].sum()\n    q = np.repeat(qs, len(df))\n    df = pd.concat([df]*9, axis=0, sort=False)\n    df.reset_index(inplace = True)\n    df[cols] *= quantile_coefs(q, level)[:, None]\n    if level != \"id\":\n        df[\"id\"] = [f\"{lev}_X_{q:.3f}_validation\" for lev, q in zip(df[level].values, q)]\n    else:\n        df[\"id\"] = [f\"{lev.replace('_validation', '')}_{q:.3f}_validation\" for lev, q in zip(df[level].values, q)]\n    df = df[[\"id\"]+list(cols)]\n    return df\n\ndef get_couple_group_preds(pred, level1, level2):\n    df = pred.groupby([level1, level2])[cols].sum()\n    q = np.repeat(qs, len(df))\n    df = pd.concat([df]*9, axis=0, sort=False)\n    df.reset_index(inplace = True)\n    df[cols] *= quantile_coefs(q, (level1, level2))[:, None]\n    df[\"id\"] = [f\"{lev1}_{lev2}_{q:.3f}_validation\" for lev1,lev2, q in \n                zip(df[level1].values,df[level2].values, q)]\n    df = df[[\"id\"]+list(cols)]\n    return df\n\nlevels = [\"id\", \"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\", \"_all_\"]\ncouples = [(\"state_id\", \"item_id\"),  (\"state_id\", \"dept_id\"),(\"store_id\",\"dept_id\"),\n                            (\"state_id\", \"cat_id\"),(\"store_id\",\"cat_id\")]\ncols = [f\"F{i}\" for i in range(1, 29)]\n\ndf = []\nfor level in levels :\n    df.append(get_group_preds(sub, level))\nfor level1,level2 in couples:\n    df.append(get_couple_group_preds(sub, level1, level2))\ndf = pd.concat(df, axis=0, sort=False)\ndf.reset_index(drop=True, inplace=True)\ndf2 = df.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ndef quantile_coefs(q, level):\n    ratios = level_coef_dict[level]\n               \n    return ratios.loc[q].values\n\ndef get_group_preds(pred, level):\n    df = pred.groupby(level)[cols].sum()\n    q = np.repeat(qs, len(df))\n    df = pd.concat([df]*9, axis=0, sort=False)\n    df.reset_index(inplace = True)\n    df[cols] *= quantile_coefs(q, level)[:, None]\n    if level != \"id\":\n        df[\"id\"] = [f\"{lev}_X_{q:.3f}_evaluation\" for lev, q in zip(df[level].values, q)]\n    else:\n        df[\"id\"] = [f\"{lev.replace('_evaluation', '')}_{q:.3f}_evaluation\" for lev, q in zip(df[level].values, q)]\n    df = df[[\"id\"]+list(cols)]\n    return df\n\ndef get_couple_group_preds(pred, level1, level2):\n    df = pred.groupby([level1, level2])[cols].sum()\n    q = np.repeat(qs, len(df))\n    df = pd.concat([df]*9, axis=0, sort=False)\n    df.reset_index(inplace = True)\n    df[cols] *= quantile_coefs(q, (level1, level2))[:, None]\n    df[\"id\"] = [f\"{lev1}_{lev2}_{q:.3f}_evaluation\" for lev1,lev2, q in \n                zip(df[level1].values,df[level2].values, q)]\n    df = df[[\"id\"]+list(cols)]\n    return df\n\nlevels = [\"id\", \"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\", \"_all_\"]\ncouples = [(\"state_id\", \"item_id\"),  (\"state_id\", \"dept_id\"),(\"store_id\",\"dept_id\"),\n                            (\"state_id\", \"cat_id\"),(\"store_id\",\"cat_id\")]\ncols = [f\"F{i}\" for i in range(1, 29)]\n\ndf = []\nfor level in levels :\n    df.append(get_group_preds(sub, level))\nfor level1,level2 in couples:\n    df.append(get_couple_group_preds(sub, level1, level2))\ndf = pd.concat(df, axis=0, sort=False)\ndf.reset_index(drop=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.concat([df2,df] , axis=0, sort=False)\ndf.reset_index(drop=True, inplace=True)\n\ndf.loc[df.index < len(df.index)//2, \"id\"] = df.loc[df.index < len(df.index)//2, \"id\"].str.replace(\"_evaluation$\", \"_validation\")\n\ndf.shape\n\ndf.head()\n\ndf.to_csv(\"submission.csv\", index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}