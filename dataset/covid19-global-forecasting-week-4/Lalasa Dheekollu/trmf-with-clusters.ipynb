{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from math import sqrt\nimport numba as nb\nimport scipy.sparse as sp\nfrom scipy.optimize import fmin_l_bfgs_b, fmin_ncg\nfrom numpy.lib.stride_tricks import as_strided\nfrom sklearn.utils.extmath import safe_sparse_dot\nfrom sklearn.utils import check_consistent_length, check_array\nfrom sklearn.utils import check_random_state\nfrom sklearn.base import BaseEstimator\n\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom functools import reduce\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from numpy.random import normal\nfrom matplotlib import pyplot as plt\nimport os\nimport copy\nfrom sklearn.cluster import KMeans","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tsfresh import extract_relevant_features\nfrom tsfresh import select_features\nfrom tsfresh.utilities.dataframe_functions import impute\nfrom tsfresh.feature_extraction.extraction import extract_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def trcg(Ax, r, x, n_iterations=1000, tr_delta=0, rtol=1e-5, atol=1e-8, args=(), verbose=False):\n    if n_iterations > 0:\n        n_iterations = min(n_iterations, len(x))\n\n    p, iteration = r.copy(), 0\n    tr_delta_sq = tr_delta ** 2\n\n    rtr, rtr_old = np.dot(r, r), 1.0\n    cg_tol = sqrt(rtr) * rtol + atol\n    region_breached = False\n    while (iteration < n_iterations) and (sqrt(rtr) > cg_tol):\n        Ap = Ax(p, *args)\n        iteration += 1\n        if verbose:\n            print(\"\"\"iter %2d |Ap| %5.3e |p| %5.3e \"\"\"\n                  \"\"\"|r| %5.3e |x| %5.3e beta %5.3e\"\"\" %\n                  (iteration, np.linalg.norm(Ap), np.linalg.norm(p),\n                   np.linalg.norm(r), np.linalg.norm(x), rtr / rtr_old))\n        # end if\n\n        # ddot(&n, p, &inc, Ap, &inc);\n        alpha = rtr / np.dot(p, Ap)\n        # daxpy(&n, &alpha, p, &inc, x, &inc);\n        x += alpha * p\n        # daxpy(&n, &( -alpha ), Ap, &inc, r, &inc);\n        r -= alpha * Ap\n\n        # check trust region (diverges from tron.cpp in liblinear and leml-imf)\n        if tr_delta_sq > 0:\n            xTx = np.dot(x, x)\n            if xTx > tr_delta_sq:\n                xTp = np.dot(x, p)\n                if xTp > 0:\n                    # backtrack into the trust region\n                    p_nrm = np.linalg.norm(p)\n\n                    q = xTp / p_nrm\n                    eta = (q - sqrt(max(q * q + tr_delta_sq - xTx, 0))) / p_nrm\n\n                    # reproject onto the boundary of the region\n                    r += eta * Ap\n                    x -= eta * p\n                else:\n                    # this never happens maybe due to CG iteration properties\n                    pass\n                # end if\n\n                region_breached = True\n                break\n            # end if\n        # end if\n\n        # ddot(&n, r, &inc, r, &inc);\n        rtr, rtr_old = np.dot(r, r), rtr\n        # dscal(&n, &(rtr / rtr_old), p, &inc);\n        p *= rtr / rtr_old\n        # daxpy(&n, &one, r, &1, p, &1);\n        p += r\n    # end while\n\n    return iteration, region_breached\n\n\ndef tron(func, x, n_iterations=1000, rtol=1e-3, atol=1e-5, args=(),\n         verbose=False):\n    \n    eta0, eta1, eta2 = 1e-4, 0.25, 0.75\n    sigma1, sigma2, sigma3 = 0.25, 0.5, 4.0\n\n    f_valp_, f_grad_, f_hess_ = func\n\n    iteration, cg_iter = 0, 0\n\n    fval = f_valp_(x, *args)\n    grad = f_grad_(x, *args)\n    grad_norm = np.linalg.norm(grad)\n\n    # make a copy of `-grad` and zeros like `x`\n    # r, z = -grad, np.zeros_like(x)\n    delta, grad_norm_tol = grad_norm, grad_norm * rtol + atol\n    while iteration < n_iterations and grad_norm > grad_norm_tol:\n        r, z = -grad, np.zeros_like(x)\n        # tolerances and n_iterations as in leml-imf\n        cg_iter, region_breached = trcg(\n            f_hess_, r, z, tr_delta=delta, args=args,\n            n_iterations=20, rtol=1e-1, atol=0.0)\n\n        z_norm = np.linalg.norm(z)\n        if iteration == 0:\n            delta = min(delta, z_norm)\n\n        # trcg finds z and r s.t. r + A z = -g and \\|r\\|\\to \\min\n        # f(x) - f(x+z) ~ -0.5 * (2 g'z + z'Az) = -0.5 * (g'z + z'(-r))\n        linear = np.dot(z, grad)\n        approxred = -0.5 * (linear - np.dot(z, r))\n\n        # The value and the actual reduction: compute the forward pass.\n        fnew = f_valp_(x + z, *args)\n        actualred = fval - fnew\n\n        if linear + actualred < 0:\n            alpha = max(sigma1, 0.5 * linear / (linear + actualred))\n\n        else:\n            alpha = sigma3\n\n        # end if\n\n        if actualred < eta0 * approxred:\n            delta = min(max(alpha, sigma1) * z_norm, sigma2 * delta)\n\n        elif actualred < eta1 * approxred:\n            delta = max(sigma1 * delta, min(alpha * z_norm, sigma2 * delta))\n\n        elif actualred < eta2 * approxred:\n            delta = max(sigma1 * delta, min(alpha * z_norm, sigma3 * delta))\n\n        else:\n            # patch 2018-08-30: new addition from tron.cpp at\n            #  https://github.com/cjlin1/liblinear/blob/master/tron.cpp\n            if region_breached:\n                delta = sigma3 * delta\n\n            else:\n                delta = max(delta, min(alpha * z_norm, sigma3 * delta))\n\n        # end if\n\n        if verbose:\n            print(\"\"\"iter %2d act %5.3e pre %5.3e delta %5.3e \"\"\"\n                  \"\"\"f %5.3e |z| %5.3e |g| %5.3e CG %3d\"\"\" %\n                  (iteration, actualred, approxred,\n                   delta, fval, z_norm, grad_norm, cg_iter))\n        # end if\n\n        if actualred > eta0 * approxred:\n            x += z\n            fval, grad = fnew, f_grad_(x, *args)\n            grad_norm = np.linalg.norm(grad)\n            iteration += 1\n\n            # r, z = -grad, np.zeros_like(x)\n        # end if\n\n        if fval < -1e32:\n            if verbose:\n                print(\"WARNING: f < -1.0e+32\")\n            break\n        # end if\n\n        if abs(actualred) <= 0 and approxred <= 0:\n            if verbose:\n                print(\"WARNING: actred and prered <= 0\")\n            break\n        # end if\n\n        if abs(actualred) <= 1e-12 * abs(fval) and \\\n           abs(approxred) <= 1e-12 * abs(fval):\n            if verbose:\n                print(\"WARNING: actred and prered too small\")\n            break\n        # end if\n\n        if delta <= rtol * (z_norm + atol):\n            if verbose:\n                print(\"WARNING: degenerate trust region\")\n            break\n        # end if\n    # end while\n\n    return cg_iter","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"@nb.njit(\"float64[:,::1](float64[:,::1], float64[:,::1])\",\n         fastmath=True, cache=False, error_model=\"numpy\")\ndef ar_resid(Z, phi):\n    \"\"\"Compute the AR(p) residuals of the multivariate data in Z.\"\"\"\n    n_components, n_order = phi.shape\n\n    resid = Z[n_order:].copy()\n    for k in range(n_order):\n        # r_t -= y_{t-(p-k)} * \\beta_{p - k} (\\phi is reversed \\beta)\n        resid -= Z[k:k - n_order] * phi[:, k]\n\n    return resid\n\n@nb.njit(\"float64[:,::1](float64[:,::1], float64[:,::1], float64[:,::1])\",\n         fastmath=True, cache=False, error_model=\"numpy\")\ndef ar_hess_vect(V, Z, phi):\n    \"\"\"Compute the Hessian-vector product of the AR(p) square loss for `V`.\"\"\"\n    n_components, n_order = phi.shape\n\n    # compute the AR(p) residuals over V\n    resid = ar_resid(V, phi)\n\n    # get the derivative w.r.t. the series\n    hess_v = np.zeros_like(V)\n    hess_v[n_order:] = resid\n    for k in range(n_order):\n        hess_v[k:k - n_order] -= resid * phi[:, k]\n\n    return hess_v\n\n@nb.njit(\"float64[:,::1](float64[:,::1], float64[:,::1])\",\n         fastmath=True, cache=False, error_model=\"numpy\")\ndef ar_grad(Z, phi):\n    \"\"\"Compute the gradient of the AR(p) l2 loss w.r.t. the time-series `Z`.\"\"\"\n    return ar_hess_vect(Z, Z, phi)\n\n\ndef precompute_graph_reg(adj):\n    \"\"\"Precompute the neighbor average discrepancy operator.\"\"\"\n\n    # make a copy of the adjacency matrix and the outbound degree\n    resid, deg = adj.astype(float), adj.getnnz(axis=1)\n\n    # scale the rows : D^{-1} A\n    resid.data /= deg[adj.nonzero()[0]]\n\n    # subtract the matrix from the diagonalized mask\n    return sp.diags((deg > 0).astype(float)) - resid\n\n\ndef graph_resid(F, adj):\n    \"\"\"Get the residual of the outgoing neighbor average of `F`.\"\"\"\n    return safe_sparse_dot(adj, F.T).T\n\n\ndef graph_grad(F, adj):\n    \"\"\"Compute the gradient of the outgoing neighbors average w.r.t. `F`.\"\"\"\n    return safe_sparse_dot(adj.T, graph_resid(F, adj).T).T\n\n\ndef graph_hess_vect(V, F, adj):\n    \"\"\"Get the Hessian-vector product of the outgoing neighbors average.\"\"\"\n    return graph_grad(V, adj)\n\ndef l2_loss_valj(Y, Z, F):\n    if sp.issparse(Y):\n        R = csr_gemm(1, Z, F, -1, Y.copy())\n        return sp.linalg.norm(R, ord=\"fro\") ** 2\n\n    return np.linalg.norm(Y - np.dot(Z, F), ord=\"fro\") ** 2\n\ndef f_step_tron_valj(f, Y, Z, C_F, eta_F, adj):\n    \"\"\"Compute current value the f-step loss.\"\"\"\n    (n_samples, n_targets), n_components = Y.shape, Z.shape[1]\n\n    F = f.reshape(n_components, n_targets)\n    objective = l2_loss_valj(Y, Z, F)\n\n    if sp.issparse(Y):\n        coef = C_F * Y.nnz / (n_components * n_targets)\n    else:\n        coef = C_F * n_samples / n_components\n\n    if C_F > 0:\n        if eta_F < 1:\n            reg_f_l2 = np.linalg.norm(F, ord=\"fro\") ** 2\n        else:\n            reg_f_l2, eta_F = 0., 1.\n        # end if\n\n        if sp.issparse(adj) and (eta_F > 0):\n            reg_f_graph = np.linalg.norm(graph_resid(F, adj), ord=\"fro\") ** 2\n        else:\n            reg_f_graph, eta_F = 0., 0.\n        # end if\n\n        reg_f = reg_f_l2 * (1 - eta_F) + reg_f_graph * eta_F\n        objective += reg_f * coef\n    # end if\n\n    return 0.5 * objective\n\ndef f_step_tron_grad(f, Y, Z, C_F, eta_F, adj):\n    \"\"\"Compute the gradient of the f-step objective.\"\"\"\n    (n_samples, n_targets), n_components = Y.shape, Z.shape[1]\n\n    F = f.reshape(n_components, n_targets)\n    if sp.issparse(Y):\n        coef = C_F * Y.nnz / (n_components * n_targets)\n\n        grad = safe_sparse_dot(Z.T, csr_gemm(1, Z, F, -1, Y.copy()))\n        grad += (1 - eta_F) * coef * F\n\n    else:\n        coef = C_F * n_samples / n_components\n\n        ZTY, ZTZ = np.dot(Z.T, Y), np.dot(Z.T, Z)\n        if C_F > 0 and eta_F < 1:\n            ZTZ.flat[::n_components + 1] += (1 - eta_F) * coef\n\n        grad = np.dot(ZTZ, F) - ZTY\n    # end if\n\n    if C_F > 0 and sp.issparse(adj) and eta_F > 0:\n        grad += graph_grad(F, adj) * (eta_F * coef)\n\n    return grad.reshape(-1)\n\ndef f_step_tron_hess(v, Y, Z, C_F, eta_F, adj):\n    \"\"\"Get the Hessian-vector product for the f-step objective.\"\"\"\n    (n_samples, n_targets), n_components = Y.shape, Z.shape[1]\n\n    V = v.reshape(n_components, n_targets)\n    if sp.issparse(Y):\n        coef = C_F * Y.nnz / (n_components * n_targets)\n\n        hess_v = safe_sparse_dot(Z.T, csr_gemm(1, Z, V, 0, Y.copy()))\n        hess_v += (1 - eta_F) * coef * V\n\n    else:\n        coef = C_F * n_samples / n_components\n\n        ZTZ = np.dot(Z.T, Z)\n        if C_F > 0 and eta_F < 1:\n            ZTZ.flat[::n_components + 1] += (1 - eta_F) * coef\n\n        hess_v = np.dot(ZTZ, V)\n    # end if\n\n    if C_F > 0 and sp.issparse(adj) and eta_F > 0:\n        hess_v += graph_grad(V, adj) * (eta_F * coef)\n\n    return hess_v.reshape(-1)\n\n\ndef f_step_tron(F, Y, Z, C_F, eta_F, adj, rtol=5e-2, atol=1e-4, verbose=False,\n                **kwargs):\n    \"\"\"TRON solver for the f-step minimization problem.\"\"\"\n    f_call = f_step_tron_valj, f_step_tron_grad, f_step_tron_hess\n\n    tron(f_call, F.ravel(), n_iterations=5, rtol=rtol, atol=atol,\n         args=(Y, Z, C_F, eta_F, adj), verbose=verbose)\n\n    return F\n\n\ndef f_step_ncg_hess_(F, v, Y, Z, C_F, eta_F, adj):\n    \"\"\"A wrapper of the hess-vector product for ncg calls.\"\"\"\n    return f_step_tron_hess(v, Y, Z, C_F, eta_F, adj)\n\n\ndef f_step_ncg(F, Y, Z, C_F, eta_F, adj, **kwargs):\n    \"\"\"Solve the F-step using scipy's Newton-CG.\"\"\"\n    FF = fmin_ncg(f=f_step_tron_valj, x0=F.ravel(), disp=False,\n                  fprime=f_step_tron_grad, fhess_p=f_step_ncg_hess_,\n                  args=(Y, Z, C_F, eta_F, adj))\n\n    return FF.reshape(F.shape)\n\n\ndef f_step_lbfgs(F, Y, Z, C_F, eta_F, adj, **kwargs):\n    \"\"\"Solve the F-step using scipy's L-BFGS method.\"\"\"\n    FF, f, d = fmin_l_bfgs_b(func=f_step_tron_valj, x0=F.ravel(), iprint=0,\n                             fprime=f_step_tron_grad,\n                             args=(Y, Z, C_F, eta_F, adj))\n\n    return FF.reshape(F.shape)\n\n\n# In[33]:\ndef f_step_prox_func(F, Y, Z, C_F, eta_F, adj):\n    \"\"\"An interface to the f-step objective for unraveled matrices.\"\"\"\n    return f_step_tron_valj(F.ravel(), Y, Z, C_F, eta_F, adj)\n\n\n# In[34]:\ndef f_step_prox_grad(F, Y, Z, C_F, eta_F, adj):\n    \"\"\"An interface to the f-step objective gradient for unraveled matrices.\"\"\"\n    return f_step_tron_grad(F.ravel(), Y, Z, C_F, eta_F, adj).reshape(F.shape)\n\n\n# In[35]:\ndef f_step_prox(F, Y, Z, C_F, eta_F, adj, lip=1e-2, n_iter=25, alpha=1.0,\n                **kwargs):\n    gamma_u, gamma_d = 2, 1.1\n\n    # get the gradient\n    grad = f_step_prox_grad(F, Y, Z, C_F, eta_F, adj)\n    grad_F = np.dot(grad.flat, F.flat)\n\n    f0, lip0 = f_step_prox_func(F, Y, Z, C_F, eta_F, adj), lip\n    for _ in range(n_iter):\n        # F_new = (1 -  alpha) * F + alpha * np.maximum(F - lr * grad, 0.)\n        # prox-sgd operation\n        F_new = np.maximum(F - grad / lip, 0.)\n\n        # FGM Lipschitz search\n        delta = f_step_prox_func(F_new, Y, Z, C_F, eta_F, adj) - f0\n        linear = np.dot(grad.flat, F_new.flat) - grad_F\n        quad = np.linalg.norm(F_new - F, ord=\"fro\") ** 2\n        if delta <= linear + lip * quad / 2:\n            break\n\n        lip *= gamma_u\n    # end for\n\n    # lip = max(lip0, lip / gamma_d)\n    lip = lip / gamma_d\n\n    return F_new, lip\n\ndef f_step(F, Y, Z, C_F, eta_F, adj, kind=\"fgm\", **kwargs):\n    \"\"\"A common subroutine solving the f-step minimization problem.\"\"\"\n    lip = np.inf\n    if kind == \"fgm\":\n        F, lip = f_step_prox(F, Y, Z, C_F, eta_F, adj, **kwargs)\n    elif kind == \"tron\":\n        F = f_step_tron(F, Y, Z, C_F, eta_F, adj, **kwargs)\n    elif kind == \"ncg\":\n        F = f_step_ncg(F, Y, Z, C_F, eta_F, adj, **kwargs)\n    elif kind == \"lbfgs\":\n        F = f_step_lbfgs(F, Y, Z, C_F, eta_F, adj, **kwargs)\n    else:\n        raise ValueError(f\"\"\"Unrecognized method `{kind}`\"\"\")\n\n    return F, lip\n\n\n# In[41]:\ndef phi_step(phi, Z, C_Z, C_phi, eta_Z, nugget=1e-8):\n    # return a set of independent AR(p) ridge estimates.\n    (n_components, n_order), n_samples = phi.shape, Z.shape[0]\n    if n_order < 1 or n_components < 1:\n        return np.empty((n_components, n_order))\n\n    if not (C_Z > 0 and eta_Z > 0):\n        return np.zeros_like(phi)\n\n    # embed into the last dimensions\n    shape = Z.shape[1:] + (Z.shape[0] - n_order, n_order + 1)\n    strides = Z.strides[1:] + Z.strides[:1] + Z.strides[:1]\n    Z_view = as_strided(Z, shape=shape, strides=strides)\n\n    # split into y (d x T-p) and Z (d x T-p x p) (all are views!)\n    y, Z_lagged = Z_view[..., -1], Z_view[..., :-1]\n\n    # compute the SVD: thin, but V is d x p x p\n    U, s, Vh = np.linalg.svd(Z_lagged, full_matrices=False)\n    if C_phi > 0:\n        # the {V^{H}}^{H} (\\Sigma^2 + C I)^{-1} \\Sigma part is reduced\n        #  to columnwise operations\n        gain = (C_Z * eta_Z * n_order) * s\n        gain /= gain * s + C_phi * (n_samples - n_order)\n    else:\n        # do the same cutoff as in np.linalg.pinv(...)\n        large = s > nugget * np.max(s, axis=-1, keepdims=True)\n        gain = np.divide(1, s, where=large, out=s)\n        gain[~large] = 0\n    # end if\n\n    # get the U' y part and the final estimate\n    # $\\phi_j$ corresponds to $p-j$-th lag $j = 0,\\,\\ldots,\\,p-1$\n    return np.einsum(\"ijk,ij,isj,is->ik\", Vh, gain, U, y)\n\ndef z_step_tron_valh(z, Y, F, phi, C_Z, eta_Z):\n    \"\"\"Get the value of the z-step objective.\"\"\"\n    n_samples, n_targets = Y.shape\n    n_components, n_order = phi.shape\n\n    Z = z.reshape(n_samples, n_components)\n    objective = l2_loss_valj(Y, Z, F)\n\n    if sp.issparse(Y):\n        coef = C_Z * Y.nnz / (n_samples * n_components)\n    else:\n        coef = C_Z * n_targets / n_components\n\n    if C_Z > 0:\n        if eta_Z < 1:\n            reg_z_l2 = np.linalg.norm(Z, ord=\"fro\") ** 2\n        else:\n            reg_z_l2, eta_Z = 0., 1.\n        # end if\n\n        if eta_Z > 0 and n_samples > n_order:\n            reg_z_ar_j = np.linalg.norm(ar_resid(Z, phi), ord=2, axis=0) ** 2\n            reg_z_ar = np.sum(reg_z_ar_j) * n_samples / (n_samples - n_order)\n        else:\n            reg_z_ar, eta_Z = 0., 0.\n        # end if\n\n        # reg_z was implicitly scaled by T d or nnz(Y)\n        reg_z = reg_z_l2 * (1 - eta_Z) + reg_z_ar * eta_Z\n        objective += reg_z * coef\n    # end if\n\n    return 0.5 * objective\n\ndef z_step_tron_grad(z, Y, F, phi, C_Z, eta_Z):\n    \"\"\"Compute the gradient of the z-step objective.\"\"\"\n    n_samples, n_targets = Y.shape\n    n_components, n_order = phi.shape\n\n    Z = z.reshape(n_samples, n_components)\n    if sp.issparse(Y):\n        coef = C_Z * Y.nnz / (n_samples * n_components)\n\n        grad = safe_sparse_dot(csr_gemm(1, Z, F, -1, Y.copy()), F.T)\n        grad += (1 - eta_Z) * coef * Z\n\n    else:\n        coef = C_Z * n_targets / n_components\n\n        YFT, FFT = np.dot(Y, F.T), np.dot(F, F.T)\n        if C_Z > 0 and eta_Z < 1:\n            FFT.flat[::n_components + 1] += (1 - eta_Z) * coef\n\n        grad = np.dot(Z, FFT) - YFT\n    # end if\n\n    if C_Z > 0 and eta_Z > 0:\n        ratio = n_samples / (n_samples - n_order)\n        grad += ar_grad(Z, phi) * (ratio * eta_Z * coef)\n\n    return grad.reshape(-1)\n\n\n# In[49]:\ndef z_step_tron_hess(v, Y, F, phi, C_Z, eta_Z):\n    \"\"\"Compute the Hessian-vector product of the z-step objective for v.\"\"\"\n    n_samples, n_targets = Y.shape\n    n_components, n_order = phi.shape\n\n    V = v.reshape(n_samples, n_components)\n    if sp.issparse(Y):\n        coef = C_Z * Y.nnz / (n_samples * n_components)\n\n        hess_v = safe_sparse_dot(csr_gemm(1, V, F, 0, Y.copy()), F.T)\n        hess_v += (1 - eta_Z) * coef * V\n\n    else:\n        coef = C_Z * n_targets / n_components\n\n        FFT = np.dot(F, F.T)\n        if C_Z > 0 and eta_Z < 1:\n            FFT.flat[::n_components + 1] += (1 - eta_Z) * coef\n\n        hess_v = np.dot(V, FFT)\n    # end if\n\n    if C_Z > 0 and eta_Z > 0:\n        # should call ar_hess_vect(V, Z, adj) but no Z is available\n        ratio = n_samples / (n_samples - n_order)\n        hess_v += ar_grad(V, phi) * ratio * eta_Z * coef\n\n    return hess_v.reshape(-1)\n\n\n# In[50]:\ndef z_step_tron(Z, Y, F, phi, C_Z, eta_Z, rtol=5e-2, atol=1e-4, verbose=False):\n    \"\"\"TRON solver for the f-step minimization problem.\"\"\"\n    f_call = z_step_tron_valh, z_step_tron_grad, z_step_tron_hess\n\n    tron(f_call, Z.ravel(), n_iterations=5, rtol=rtol, atol=atol,\n         args=(Y, F, phi, C_Z, eta_Z), verbose=verbose)\n\n    return Z\n\n\ndef z_step_ncg_hess_(Z, v, Y, F, phi, C_Z, eta_Z):\n    \"\"\"A wrapper of the hess-vector product for ncg calls.\"\"\"\n    return z_step_tron_hess(v, Y, F, phi, C_Z, eta_Z)\n\n\ndef z_step_ncg(Z, Y, F, phi, C_Z, eta_Z, **kwargs):\n    \"\"\"Solve the Z-step using scipy's Newton-CG.\"\"\"\n    ZZ = fmin_ncg(f=z_step_tron_valh, x0=Z.ravel(), disp=False,\n                  fprime=z_step_tron_grad, fhess_p=z_step_ncg_hess_,\n                  args=(Y, F, phi, C_Z, eta_Z))\n    return ZZ.reshape(Z.shape)\n\n\ndef z_step_lbfgs(Z, Y, F, phi, C_Z, eta_Z, **kwargs):\n    \"\"\"Solve the Z-step using scipy's L-BFGS method.\"\"\"\n    ZZ, f, d = fmin_l_bfgs_b(func=z_step_tron_valh, x0=Z.ravel(), iprint=0,\n                             fprime=z_step_tron_grad,\n                             args=(Y, F, phi, C_Z, eta_Z))\n\n    return ZZ.reshape(Z.shape)\n\n\ndef z_step(Z, Y, F, phi, C_Z, eta_Z, kind=\"tron\", **kwargs):\n    \"\"\"A common subroutine solving the Z-step minimization problem.\"\"\"\n    if kind == \"tron\":\n        Z = z_step_tron(Z, Y, F, phi, C_Z, eta_Z, **kwargs)\n    elif kind == \"ncg\":\n        Z = z_step_ncg(Z, Y, F, phi, C_Z, eta_Z, **kwargs)\n    elif kind == \"lbfgs\":\n        Z = z_step_lbfgs(Z, Y, F, phi, C_Z, eta_Z, **kwargs)\n    else:\n        raise ValueError(f\"\"\"Unrecognized method `{kind}`\"\"\")\n\n    return Z","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"@nb.njit(\"(float64, float64[:,::1], float64[:,::1], \"\n         \"float64, int32[::1], int32[::1], float64[::1])\",\n         fastmath=True, error_model=\"numpy\", parallel=False, cache=False)\ndef _csr_gemm(alpha, X, D, beta, Sp, Sj, Sx):\n    # computes\\mathcal{P}_\\Omega(X D) -- n1 x n2 sparse matrix\n    if abs(beta) > 0:\n        for i in nb.prange(len(X)):\n            # compute e_i' XD e_{Sj[j]}\n            for j in range(Sp[i], Sp[i+1]):\n                dot = np.dot(X[i], D[:, Sj[j]])\n                Sx[j] = beta * Sx[j] + alpha * dot\n        # end for\n    else:\n        for i in nb.prange(len(X)):\n            # compute e_i' XD e_{Sj[j]}\n            for j in range(Sp[i], Sp[i+1]):\n                Sx[j] = alpha * np.dot(X[i], D[:, Sj[j]])\n        # end for\n    # end if\n\n\ndef csr_gemm(alpha, X, D, beta, Y):\n    _csr_gemm(alpha, X, D, beta, Y.indptr, Y.indices, Y.data)\n    return Y\n\n\ndef csr_column_means(X):\n    f_sums = np.bincount(X.indices, weights=X.data, minlength=X.shape[1])\n    n_nnz = np.maximum(np.bincount(X.indices, minlength=X.shape[1]), 1.)\n\n    return (f_sums / n_nnz)[np.newaxis]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def b_step_tron_valj(b, Y, X, C_B):\n    \"\"\"Compute current value the b-step loss.\"\"\"\n    (n_samples, n_targets), n_features = Y.shape, X.shape[1]\n\n    B = b.reshape(n_features, n_targets)\n    objective = l2_loss_valj(Y, X, B)\n\n    if sp.issparse(Y):\n        coef = C_B * Y.nnz / (n_features * n_targets)\n    else:\n        coef = C_B * n_samples / n_features\n\n    if C_B > 0:\n        reg_b = np.linalg.norm(B, ord=\"fro\") ** 2\n\n        objective += reg_b * coef\n    # end if\n\n    return 0.5 * objective\n\n\ndef b_step_tron_grad(b, Y, X, C_B):\n    \"\"\"Compute the gradient of the b-step objective.\"\"\"\n    (n_samples, n_targets), n_features = Y.shape, X.shape[1]\n\n    B = b.reshape(n_features, n_targets)\n    if sp.issparse(Y):\n        coef = C_B * Y.nnz / (n_features * n_targets)\n\n        grad = safe_sparse_dot(X.T, csr_gemm(1, X, B, -1, Y.copy()))\n        grad += coef * B\n\n    else:\n        coef = C_B * n_samples / n_features\n\n        XTY, XTX = np.dot(X.T, Y), np.dot(X.T, X)\n        if C_B > 0:\n            XTX.flat[::n_features + 1] += coef\n        grad = np.dot(XTX, B) - XTY\n    return grad.reshape(-1)\n\ndef b_step_tron_hess(v, Y, X, C_B):\n    \"\"\"Get the Hessian-vector product for the b-step objective.\"\"\"\n    (n_samples, n_targets), n_features = Y.shape, X.shape[1]\n    V = v.reshape(n_features, n_targets)\n    if sp.issparse(Y):\n        coef = C_B * Y.nnz / (n_features * n_targets)\n        hess_v = safe_sparse_dot(X.T, csr_gemm(1, X, V, 0, Y.copy()))\n        hess_v += coef * V\n\n    else:\n        coef = C_B * n_samples / n_features\n        XTX = np.dot(X.T, X)\n        if C_B > 0:\n            XTX.flat[::n_features + 1] += coef\n        hess_v = np.dot(XTX, V)\n    return hess_v.reshape(-1)\n\ndef b_step_tron(B, Y, X, C_B, rtol=5e-2, atol=1e-4, verbose=False, **kwargs):\n    \"\"\"TRON solver for the b-step minimization problem.\"\"\"\n    f_call = b_step_tron_valj, b_step_tron_grad, b_step_tron_hess\n\n    tron(f_call, B.ravel(), n_iterations=5, rtol=rtol, atol=atol,\n         args=(Y, X, C_B), verbose=verbose)\n\n    return B\n\ndef soft_prox(x, c):\n    return np.maximum(x - c, 0.) + np.minimum(x + c, 0.)\n\ndef b_step(B, Y, X, C_B, kind=\"tron\", **kwargs):\n    \"\"\"A common subroutine solving the b-step minimization problem.\"\"\"\n    lip = np.inf\n    if kind == \"tron\":\n        B = b_step_tron(B, Y, X, C_B, **kwargs)\n    # elif kind == \"fgm\":\n    #     B, lip = b_step_prox(B, Y, X, C_B, **kwargs)\n    else:\n        raise ValueError(f\"\"\"Unrecognozed optimization `{kind}`\"\"\")\n\n    return B, lip","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def trmf_init(data, n_components, n_order, random_state=None):\n    random_state = check_random_state(random_state)\n    n_samples, n_targets = data.shape\n    if sp.issparse(data):\n        U, s, Vh = sp.linalg.svds(data, k=n_components)\n\n        order = np.argsort(s)[::-1]\n        U, s, Vh = U[:, order], s[order], Vh[order]\n    else:\n        U, s, Vh = np.linalg.svd(data, full_matrices=False)\n\n    factors = U[:, :n_components].copy()\n    loadings = Vh[:n_components].copy()\n    loadings *= s[:n_components, np.newaxis]\n\n    n_svd_factors = factors.shape[1]\n    if n_svd_factors < n_components:\n        random_factors = random_state.normal(\n            scale=0.01, size=(n_samples, n_components - n_svd_factors))\n        factors = np.concatenate([factors, random_factors], axis=1)\n\n    n_svd_loadings = loadings.shape[0]\n    if n_svd_loadings < n_components:\n        random_loadings = random_state.normal(\n            scale=0.01, size=(n_components - n_svd_loadings, n_targets))\n        loadings = np.concatenate([loadings, random_loadings], axis=0)\n\n    phi = np.zeros((n_components, n_order))\n    ar_coef = phi_step(phi, factors, 1.0, 0., 1.0)\n    return factors, loadings, ar_coef\n\ndef trmf(data, n_components, n_order, C_Z, C_F, C_phi, eta_Z,\n         eta_F=0., adj=None, fit_intercept=False, regressors=None, C_B=0.0,\n         tol=1e-6, n_max_iterations=2500, n_max_mf_iter=5,\n         f_step_kind=\"fgm\", z_step_kind=\"tron\", random_state=None):\n    if not all(C >= 0 for C in (C_Z, C_F, C_phi, C_B)):\n        raise ValueError(\"\"\"Negative ridge regularizer coefficient.\"\"\")\n\n    if not all(0 <= eta <= 1 for eta in (eta_Z, eta_F)):\n        raise ValueError(\"\"\"Share `eta` is not within `[0, 1]`.\"\"\")\n\n    if not (n_components > 0):\n        raise ValueError(\"\"\"Empty latent factors are not supported.\"\"\")\n\n    if C_F > 0 and eta_F > 0:\n        if not sp.issparse(adj):\n            raise TypeError(\"\"\"The adjacency matrix must be sparse.\"\"\")\n\n        # precompute the outbound average dsicrepancy operator\n        adj = precompute_graph_reg(adj)\n    # end if\n\n    # prepare the regressors\n    n_samples, n_targets = data.shape\n    if isinstance(regressors, str):\n        if regressors != \"auto\" or True:\n            raise ValueError(f\"\"\"Invalid regressor setting `{regressors}`\"\"\")\n\n        if sp.issparse(data):\n            raise ValueError(\"\"\"`data` cannot be sparse in \"\"\"\n                             \"\"\"autoregression mode\"\"\")\n\n        # assumes order-1 explicit autoregression\n        regressors, data = data[:-1], data[1:]\n        n_samples, n_targets = data.shape\n\n    elif regressors is None:\n        regressors = np.empty((n_samples, 0))\n\n    # end if\n\n    check_consistent_length(regressors, data)\n\n    # by default the intercept is zero\n    intercept = np.zeros((1, n_targets))\n\n    # initialize the regression coefficients\n    n_regressors = regressors.shape[1]\n    beta = np.zeros((n_regressors, n_targets))\n\n    # prepare smart guesses\n    factors, loadings, ar_coef = trmf_init(data, n_components, n_order,\n                                           random_state=random_state)\n\n    # prepare for estimating the coefs of the exogenous ridge regression\n    if fit_intercept and n_regressors > 0:\n        regressors_mean = regressors.mean(axis=0, keepdims=True)\n        regressors_cntrd = regressors - regressors_mean\n    else:\n        regressors_cntrd = regressors\n    # end if\n\n    # initialize the outer loop\n    ZF, lip_f, lip_b = np.dot(factors, loadings), 500.0, 500.0\n    ZF_old_norm, delta = np.linalg.norm(ZF, ord=\"fro\"), +np.inf\n\n    if sp.issparse(data):\n        # run the trmf algo\n        for iteration in range(n_max_iterations):\n            if (delta <= ZF_old_norm * tol) and (iteration > 0):\n                break\n\n            # Fit the exogenous ridge-regression with an optional intercept\n            if fit_intercept or n_regressors > 0:\n                resid = csr_gemm(-1, factors, loadings, 1, data.copy())\n\n                if fit_intercept:\n                    intercept = csr_column_means(resid)\n\n                if n_regressors > 0:\n                    if fit_intercept:\n                        resid.data -= intercept[0, resid.indices]\n                    # end if\n\n                    # solve for beta\n                    beta, lip_b = b_step(beta, resid, regressors_cntrd, C_B,\n                                         kind=\"tron\")\n\n                    # mean(R) - mean(X) beta = mu\n                    if fit_intercept:\n                        intercept -= np.dot(regressors_mean, beta)\n                    # end if\n                # end if\n\n                # prepare the residuals for the trmf loop\n                resid = data.copy()\n                if n_regressors > 0:\n                    resid = csr_gemm(-1, regressors, beta, 1, resid)\n\n                if fit_intercept:\n                    resid.data -= intercept[0, resid.indices]\n            else:\n                resid = data\n            # end if\n\n            # update (F, Z), then phi\n            for inner_iter in range(n_max_mf_iter):\n                loadings, lip_f = f_step(loadings, resid, factors, C_F, eta_F,\n                                         adj, kind=f_step_kind, lip=lip_f)\n\n                factors = z_step(factors, resid, loadings, ar_coef,\n                                 C_Z, eta_Z, kind=z_step_kind)\n            # end for\n\n            if n_order > 0:\n                ar_coef = phi_step(ar_coef, factors, C_Z, C_phi, eta_Z)\n            # end if\n\n            # recompute the reconstruction and convergence criteria\n            ZF, ZF_old = np.dot(factors, loadings), ZF\n            delta = np.linalg.norm(ZF - ZF_old, ord=\"fro\")\n            ZF_old_norm = np.linalg.norm(ZF_old, ord=\"fro\")\n        # end for\n    else:\n        # run the trmf algo\n        for iteration in range(n_max_iterations):\n            #print(\"loop1\")\n            if (delta <= ZF_old_norm * tol) and (iteration > 0):\n                break\n\n            # Fit the exogenous ridge-regression with an optional intercept\n            if fit_intercept or n_regressors > 0:\n                resid = data - ZF\n                if fit_intercept:\n                    intercept = resid.mean(axis=0, keepdims=True)\n                # end if\n\n                if n_regressors > 0:\n                    if fit_intercept:\n                        resid -= intercept\n                    # end if\n\n                    # solve for beta\n                    beta, lip_b = b_step(beta, resid, regressors_cntrd, C_B,\n                                         kind=\"tron\")\n\n                    # mean(R) - mean(X) beta = mu\n                    if fit_intercept:\n                        intercept -= np.dot(regressors_mean, beta)\n                    # end if\n                # end if\n\n                resid = data.copy()\n                if n_regressors > 0:\n                    resid -= np.dot(regressors, beta)\n\n                if fit_intercept:\n                    resid -= intercept\n            else:\n                resid = data\n            # end if\n\n            # update (F, Z), then phi\n            for inner_iter in range(n_max_mf_iter):\n                #print(\"loop2\")\n                loadings, lip_f = f_step(loadings, resid, factors, C_F, eta_F,\n                                         adj, kind=f_step_kind, lip=lip_f)\n\n                factors = z_step(factors, resid, loadings, ar_coef,\n                                 C_Z, eta_Z, kind=z_step_kind)\n            # end for\n\n            if n_order > 0:\n                ar_coef = phi_step(ar_coef, factors, C_Z, C_phi, eta_Z)\n            # end if\n\n            # recompute the reconstruction and convergence criteria\n            ZF, ZF_old = np.dot(factors, loadings), ZF\n            delta = np.linalg.norm(ZF - ZF_old, ord=\"fro\")\n            ZF_old_norm = np.linalg.norm(ZF_old, ord=\"fro\")\n        # end for\n    # end if\n\n    return factors, loadings, ar_coef, intercept, beta\n\n\n# Modified `In[50]`\ndef trmf_forecast_factors(n_ahead, ar_coef, prehist):\n    n_components, n_order = ar_coef.shape\n    if n_ahead < 1:\n        raise ValueError(\"\"\"`n_ahead` must be a positive integer.\"\"\")\n\n    if len(prehist) < n_order:\n        raise TypeError(\"\"\"Factor history is too short.\"\"\")\n\n    forecast = np.concatenate([\n        prehist[-n_order:] if n_order > 0 else prehist[:0],\n        np.zeros((n_ahead, n_components))\n    ], axis=0)\n\n    # compute the dynamic forecast\n    for h in range(n_order, n_order + n_ahead):\n        # ar_coef are stored in little endian lag order: from lag p to lag 1\n        #  from the least recent to the most recent!\n        forecast[h] = np.einsum(\"il,li->i\", ar_coef, forecast[h - n_order:h])\n\n    return forecast[-n_ahead:]\n\n\n# Extra functionality\ndef trmf_forecast_targets(n_ahead, loadings, ar_coef, intercept, beta,\n                          factors, regressors=None, mode=\"exog\"):\n    n_regressors, n_targets = beta.shape\n    if regressors is None:\n        if n_regressors > 0:\n            raise TypeError(\"\"\"Regressors must be provided.\"\"\")\n        regressors = np.empty((n_ahead, 0))\n\n    #regressors = check_array(regressors, dtype=\"numeric\",\n     #                        accept_sparse=False, ensure_min_features=0)\n\n    if regressors.shape[1] != n_regressors:\n        raise TypeError(\"\"\"Invalid number of regressor features.\"\"\")\n\n    if mode == \"exog\":\n        if regressors.shape[0] < n_ahead:\n            raise TypeError(\"\"\"Not enough future observations.\"\"\")\n\n    elif mode == \"auto\":\n        if n_regressors != n_targets:\n            raise TypeError(\"\"\"Invalid `beta` for mode `auto`.\"\"\")\n\n        if regressors.shape[0] < 1:\n            raise TypeError(\"\"\"Insufficient history of targets.\"\"\")\n    # end if\n\n    # step 1: predict the latent factors\n    forecast = trmf_forecast_factors(n_ahead, ar_coef, factors)\n    factors_forecast = np.dot(forecast, loadings)\n\n    # step 2: predict the targets\n    if mode == \"exog\":\n        # assume the regressors are exogenous\n        targets = np.dot(regressors, beta) + factors_forecast + intercept\n\n    elif mode == \"auto\":\n        # Assume the regressors are order 1 autoregressors (can be\n        #  order-q but needs embedding).\n        targets = np.concatenate([\n            regressors[-1:],\n            np.zeros((n_ahead, n_regressors), dtype=regressors.dtype)\n        ], axis=0)\n\n        # compute the dynamic forecast\n        for h in range(n_ahead):\n            targets[h + 1] = intercept + np.dot(targets[h], beta) \\\n                             + factors_forecast[h]\n        # end for\n    # end if\n\n    return targets[-n_ahead:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TRMFRegressor(BaseEstimator):\n    def __init__(self,\n                 n_components,\n                 n_order,\n                 C_Z=1e-1,\n                 C_F=1e-1,\n                 C_phi=1e-2,\n                 eta_Z=0.5,\n                 eta_F=0.,\n                 adj=None,\n                 C_B=0.0,\n                 fit_regression=False,\n                 fit_intercept=True,\n                 nonnegative_factors=True,\n                 tol=1e-5,\n                 n_max_iterations=1000,\n                 n_max_mf_iter=5,\n                 z_step_kind=\"tron\",\n                 f_step_kind=\"tron\",\n                 random_state=None):\n        super(TRMFRegressor, self).__init__()\n\n        self.n_components = n_components\n        self.n_order = n_order\n        self.C_Z = C_Z\n        self.C_F = C_F\n        self.C_phi = C_phi\n        self.eta_Z = eta_Z\n        self.eta_F = eta_F\n        self.adj = adj\n        self.C_B = C_B\n        self.fit_regression = fit_regression\n        self.fit_intercept = fit_intercept\n        self.tol = tol\n        self.n_max_iterations = n_max_iterations\n        self.n_max_mf_iter = n_max_mf_iter\n        self.nonnegative_factors = nonnegative_factors\n        self.random_state = random_state\n        self.z_step_kind = z_step_kind\n        self.f_step_kind = f_step_kind\n\n    def fit(self, X, y=None, sample_weight=None):\n        if not self.fit_regression:\n            if y is not None:\n                raise TypeError(\"\"\"Exogenous regressors provided in `X`, \"\"\"\n                                \"\"\"yet `fit_regression` is false.\"\"\")\n            X, y = None, X\n\n        else:\n            if y is None:\n                raise TypeError(\"\"\"Endogenous data are is not provided \"\"\"\n                                \"\"\"in `y`, yet `fit_regression` is True.\"\"\")\n        # end if\n\n        if X is None:\n            X = np.empty((y.shape[0], 0))\n        # end if\n\n        check_consistent_length(X, y)\n\n        f_step_kind = \"fgm\" if self.nonnegative_factors else self.f_step_kind\n        estimates = trmf(y, self.n_components, self.n_order, self.C_Z,\n                         self.C_F, self.C_phi, self.eta_Z, self.eta_F,\n                         adj=self.adj, fit_intercept=self.fit_intercept,\n                         regressors=X, C_B=self.C_B, tol=self.tol,\n                         n_max_iterations=self.n_max_iterations,\n                         n_max_mf_iter=self.n_max_mf_iter,\n                         f_step_kind=f_step_kind,\n                         z_step_kind=self.z_step_kind,\n                         random_state=self.random_state)\n\n        # Record the estimates in this instance's properties\n        factors, loadings, ar_coef, intercept, beta = estimates\n\n        self.factors_, self.loadings_ = factors, loadings\n        self.ar_coef_ = ar_coef\n\n        self.coef_, self.intercept_ = beta, intercept\n\n        # self.fitted_ = np.dot(X, beta) + np.dot(factors, loadings) \\\n        #                + intercept\n\n        return self\n    \n    def fit_predict(self, n_ahead):\n        fitted_targets = np.dot(self.factors_,self.loadings_)\n        forecast_targets = trmf_forecast_targets(n_ahead, self.loadings_, self.ar_coef_, self.intercept_, self.coef_,self.factors_)\n        predicted = np.concatenate([fitted_targets,forecast_targets],axis=0)\n        return predicted\n        \n    def forecast_factors(self, n_ahead):\n        return trmf_forecast_factors(n_ahead, self.ar_coef_,\n                                     prehist=self.factors_)\n\n    def predict(self, X=None, n_ahead=10):\n        if self.fit_regression:\n            X = check_array(X, dtype=\"numeric\", accept_sparse=False)\n        else:\n            X = np.empty((n_ahead, 0))\n        # end if\n\n        return trmf_forecast_targets(\n            n_ahead, self.loadings_, self.ar_coef_, self.intercept_,\n            self.coef_, self.factors_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-4/train.csv')\ntest = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-4/test.csv')\nsubmission = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-4/submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.rename(columns={'Country_Region':'Country'}, inplace=True)\ntrain.rename(columns={'Province_State':'State'}, inplace=True)\ntrain.State=train.State.fillna('NA')\ntrain['Date'] = pd.to_datetime(train['Date'], infer_datetime_format=True)\n\ntest.rename(columns={'Country_Region':'Country'}, inplace=True)\ntest.rename(columns={'Province_State':'State'}, inplace=True)\ntest.State=test.State.fillna('NA')\ntest['Date'] = pd.to_datetime(test['Date'], infer_datetime_format=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import preprocessing\nle_state_country = preprocessing.LabelEncoder()\ntrain['state_country_id'] = le_state_country.fit_transform(train.State.astype('str')+\":\"+train.Country.astype('str'))\ntest['state_country_id'] = le_state_country.transform(test.State.astype('str')+\":\"+test.Country.astype('str'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_confirmed_cases = train[['state_country_id','Date','ConfirmedCases']]\ntrain_confirmed_cases.rename(columns={'ConfirmedCases':'y'},inplace=True)\ntrain_fatalities = train[['state_country_id','Date','Fatalities']]\ntrain_fatalities.rename(columns={'Fatalities':'y'},inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_fatalities = train[['state_country_id','Date','Fatalities','ConfirmedCases']]\n# train_fatalities['Fatalities'] = train_fatalities['Fatalities']/train_fatalities['ConfirmedCases']\ntrain_fatalities.drop(columns=['ConfirmedCases'],inplace=True)\ntrain_fatalities.rename(columns={'Fatalities':'y'},inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ## cluster creation for Fatalities\n\n# def normalise(df, col, scaler=MinMaxScaler()):\n#     df[col] = scaler.fit_transform(df[col].values.reshape(len(df), 1)).reshape(len(df))\n#     return df\n\n# d = train_fatalities[train_fatalities.Date<='2020-04-01'].reset_index(drop=True).sort_values(['state_country_id', 'Date'], ascending=[True, False]).reset_index(drop=True)\n# tsfresh_features = extract_features(d[['Date', 'state_country_id', 'y']], column_id='state_country_id', column_sort='Date')\n# tsfresh_features = tsfresh_features.reset_index(level=0)\n# tsfresh_features = tsfresh_features.rename(columns={'id': 'state_country_id'})\n# tsfresh_features.fillna(0, inplace=True)\n# x = copy.deepcopy(tsfresh_features)\n# inf_columns = x.T[x.T.isin([np.nan, np.inf, -np.inf]).any(1)].T.columns\n# x.drop(columns=inf_columns,inplace=True)\n# for col in x.columns:\n#     if col != 'state_country_id':\n#         #print(col,tsfresh_features[col].isna().sum())\n#         x = normalise(x, col)\n# x1 = np.array(x.drop(['state_country_id'], 1).astype(float))\n# kmeans = KMeans(n_clusters=3, max_iter=200, algorithm='auto')\n# kmeans.fit(x1)\n\n# l1 = kmeans.predict(x1)\n# unique_elements, counts_elements = np.unique(l1, return_counts=True)\n# print(counts_elements)\n\n# x['clusterId'] = l1\n# cluster_fatalities = x[['state_country_id','clusterId']].reset_index(drop=True)\n# clusterId_to_id_fatalities = dict(cluster_fatalities.groupby(\"clusterId\")['state_country_id'].apply(list))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def normalise(df, col, scaler=MinMaxScaler()):\n#     df[col] = scaler.fit_transform(df[col].values.reshape(len(df), 1)).reshape(len(df))\n#     return df\n# d = train_confirmed_cases.sort_values(['state_country_id', 'Date'], ascending=[True, False]).reset_index(drop=True)\n# tsfresh_features = extract_features(d[['Date', 'state_country_id', 'y']], column_id='state_country_id', column_sort='Date')\n# tsfresh_features = tsfresh_features.reset_index(level=0)\n# tsfresh_features = tsfresh_features.rename(columns={'id': 'state_country_id'})\n# tsfresh_features.fillna(0, inplace=True)\n# x = copy.deepcopy(tsfresh_features)\n# inf_columns = x.T[x.T.isin([np.nan, np.inf, -np.inf]).any(1)].T.columns\n# x.drop(columns=inf_columns,inplace=True)\n# for col in x.columns:\n#     if col != 'state_country_id':\n#         #print(col,tsfresh_features[col].isna().sum())\n#         x = normalise(x, col)\n# x1 = np.array(x.drop(['state_country_id'], 1).astype(float))\n# kmeans = KMeans(n_clusters=4, max_iter=200, algorithm='auto')\n# kmeans.fit(x1)\n\n# l1 = kmeans.predict(x1)\n# unique_elements, counts_elements = np.unique(l1, return_counts=True)\n# print(counts_elements)\n\n# x['clusterId'] = l1\n# cluster_confirmed_cases = x[['state_country_id','clusterId']].reset_index(drop=True)\n# clusterId_to_id_confirmed_cases = dict(cluster_confirmed_cases.groupby(\"clusterId\")['state_country_id'].apply(list))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def normalise(df, scaler=MinMaxScaler()):\n    df['y_norm'] = scaler.fit_transform(df['y'].values.reshape(len(df), 1)).reshape(len(df))\n    return df\ndef inverse_normalise(s_c_min_max, y, s_c):\n    return y * (s_c_min_max[s_c]['max'] - s_c_min_max[s_c]['min']) + s_c_min_max[s_c]['min']\ndef preprocess(df,s_c_ids,dates):  \n    working_df = df\n    df_normalised_y = []   # list that stores dataframe that has normalised y values.\n    s_c_min_max = {}      # stores min and max values of each state-country .\n    \n    for i, considering_s_c in enumerate(s_c_ids):\n            source_df = working_df[working_df['state_country_id'] == considering_s_c]  \n            # stores min and max values of each state-country\n            s_c_min_max[considering_s_c] = {'min': source_df['y'].min(), 'max': source_df['y'].max()}\n            # filling in null values with 0\n            for d in dates:\n                if d not in(source_df.Date.unique()):\n                    source_df = source_df.append({'state_country_id':considering_s_c,'Date':d, 'y':0} , ignore_index=True)\n            source_df = normalise(source_df)\n            df_normalised_y.append(source_df)\n\n    df_normalised_y = pd.concat(df_normalised_y).reset_index(drop=True)\n    working_df = df_normalised_y\n    working_df = working_df.sort_values(by=['state_country_id','Date']).reset_index(drop=True)\n    \n    table = pd.pivot_table(working_df[['state_country_id','Date','y_norm']], values ='y_norm', index =['Date'], \n                         columns =['state_country_id']) \n    df_input = pd.DataFrame(table.to_records()).set_index('Date')\n    targets = df_input.values\n    \n    return targets,s_c_min_max\ndef to_original_format(data,s_c_ids,dates,s_c_min_max):\n    \n    d = pd.DataFrame(data,columns = s_c_ids,index = dates)   \n    d = pd.melt(d.rename_axis('Date').reset_index(), var_name='state_country_id', value_name='y_norm', id_vars='Date')    \n    d = d.sort_values(by=['state_country_id','Date'])\n    \n    result = []\n    for i, s_c_id in enumerate(s_c_ids):\n        temp = d[d['state_country_id']==s_c_id]\n        temp['y'] = temp['y_norm'].apply(lambda y: inverse_normalise(s_c_min_max,y,s_c_id))\n        result.append(temp)\n    result = pd.concat(result).reset_index(drop=True)\n\n    return result\ndef predict(targets,params,n_ahead,mode='training'):\n    if mode=='training':\n        train_targets, test_targets = train_test_split(targets, test_size=11, shuffle=False)\n        n_ahead = len(test_targets)\n    else:\n        train_targets = targets\n        \n    print(\"Training the model...\")\n    regressor = TRMFRegressor(**params)\n    regressor.fit(train_targets)\n    print(\"Forecasting...\")\n    predicted = regressor.fit_predict(n_ahead = n_ahead)\n    return predicted,n_ahead\ndef get_forecasts_df(predicted, s_c_ids, dates, n_ahead, s_c_min_max, mode=\"training\"):\n    if mode!='training':\n        dates_new = pd.date_range(datetime.strptime(dates[-1:][0], '%Y-%m-%d'),periods=n_ahead+1, freq='1D')[1:]\n        dates = dates.tolist()\n        dates.extend(dates_new.strftime(\"%Y-%m-%d\"))\n        dates = np.array(dates)\n    \n    df_pred = to_original_format(predicted, s_c_ids, dates, s_c_min_max)[['state_country_id','Date','y']]\n    df_pred = df_pred.rename(columns = {'y':'yhat'})\n    return df_pred\ndef cluster_forecasts(active_df,clusterId,s_c_ids,curr_dates,n_ahead,n_components,n_order,mode='training',plot=False):\n    print(f\"Working on cluster - {clusterId}\")\n    print(\"setting the parameters...\")\n    \n    params = {\n        \"n_components\" : n_components,\n        \"n_order\" : n_order,\n        \"C_Z\" : 5e1,\n        \"C_F\" : 5e-4,\n        \"C_phi\" : 1e-4,\n        \"eta_Z\" : 0.25,\n        \"C_B\" : 0.,\n        \"fit_regression\" : False,\n        \"fit_intercept\" : False,\n        \"nonnegative_factors\" : False,\n        \"n_max_iterations\" : 1000,\n        \"f_step_kind\" : \"tron\",\n        \"tol\" : 1e-5,\n    }\n    \n    print(\"Preparing the data...\")\n    considering_s_cs = np.sort(s_c_ids)  \n    working_df = active_df[active_df['state_country_id'].isin(considering_s_cs)]\n    working_df = working_df.sort_values(by=['state_country_id','Date'])[['state_country_id','Date','y']]\n    targets, s_c_min_max = preprocess(working_df,considering_s_cs,curr_dates)\n\n    predicted,n_ahead = predict(targets,params,n_ahead,mode=mode)\n#     print(np.isnan(predicted).sum())\n#     print(predicted)\n    if plot==True:\n        print(\"plotting graphs-------\")\n        plot_predictions(targets,predicted,len(considering_s_cs),n_ahead,n_cols=4)\n        \n    df_pred = get_forecasts_df(predicted, s_c_ids, dates,n_ahead, s_c_min_max,mode=mode)\n    # print(df_pred.tail())\n    df_comb = pd.merge(df_pred,working_df,on=['state_country_id','Date'],how='left')\n    # pri\n    return df_comb\n\ndef plot_predictions(targets,predicted,no_uuids,n_ahead,n_cols):\n    n_rows = (no_uuids + n_cols - 1) // n_cols\n    fig, axes = plt.subplots(n_rows, n_cols, figsize=(12, 1.5*n_rows),\n                             sharex=True, sharey=False)\n\n    for j, ax in zip(range(no_uuids), axes.flat):\n        ax.plot(targets[:, j], lw=2)\n        ax.plot(predicted[:-n_ahead, j], zorder=-1)\n        ax.plot(predicted[:, j], zorder=-2, alpha=0.5)\n\n    plt.show()\n    plt.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_ahead = 60","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_confirmed_cases['Date'] = pd.to_datetime(train_confirmed_cases['Date'], format='%Y-%m-%d')\ntrain_confirmed_cases['Date'] = train_confirmed_cases['Date'].dt.strftime('%Y-%m-%d')\ntrain_confirmed_cases['Date'] = train_confirmed_cases['Date'].apply(lambda row: datetime.strptime(row, \"%Y-%m-%d\").strftime(\"%Y-%m-%d\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_fatalities['Date'] = pd.to_datetime(train_confirmed_cases['Date'], format='%Y-%m-%d')\ntrain_fatalities['Date'] = train_fatalities['Date'].dt.strftime('%Y-%m-%d')\ntrain_fatalities['Date'] = train_fatalities['Date'].apply(lambda row: datetime.strptime(row, \"%Y-%m-%d\").strftime(\"%Y-%m-%d\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_confirmed_cases = train_confirmed_cases[train_confirmed_cases.Date<='2020-04-03'].reset_index(drop=True)\ntrain_confirmed_cases = train_confirmed_cases.sort_values(by=['state_country_id','Date']).reset_index(drop=True)\n\ntrain_fatalities = train_fatalities[train_fatalities.Date<='2020-04-03'].reset_index(drop=True)\ntrain_fatalities = train_fatalities.sort_values(by=['state_country_id','Date']).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_confirmed_cases['y'] = train_confirmed_cases['y'].fillna(0)\ntrain_fatalities['y'] = train_fatalities['y'].fillna(0)\ndates = np.sort(train_confirmed_cases.Date.unique())\ns_c_ids = np.sort(train_confirmed_cases.state_country_id.unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"n_components = 60\nn_order = 12\nfitted_confirmed_cases = cluster_forecasts(train_confirmed_cases,\"confirmed_cases\",s_c_ids,dates,n_ahead,n_components,n_order,mode=\"training\",plot=False)\n\n# fitted_confirmed_cases = []\n# for clusterId, s_c_id in clusterId_to_id_confirmed_cases.items():\n#     s_c_id = np.sort(s_c_id)\n#     n_components = 60\n#     n_order = 12\n    \n#     df_cluster = cluster_forecasts(train_confirmed_cases,clusterId,s_c_id,dates,n_ahead,n_components,n_order,mode=\"training\",plot=False)\n#     fitted_confirmed_cases.append(df_cluster)\n#     print(\"======================================\")\n# fitted_confirmed_cases = pd.concat(fitted_confirmed_cases)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_components = 60\nn_order = 12\npred_confirmed_cases = cluster_forecasts(train_confirmed_cases,\"confirmed_cases\",s_c_ids,dates,n_ahead,n_components,n_order,mode=\"forecasts\",plot=False)\n# pred_confirmed_cases = []\n# for clusterId, s_c_id in clusterId_to_id_confirmed_cases.items():\n#     s_c_id = np.sort(s_c_id)\n#     n_components = 20\n#     n_order = 6\n    \n#     df_cluster = cluster_forecasts(train_confirmed_cases,clusterId,s_c_id,dates[:-4],n_ahead,n_components,n_order,mode=\"forecasts\",plot=True)\n#     pred_confirmed_cases.append(df_cluster)\n#     print(\"======================================\")\n# pred_confirmed_cases = pd.concat(pred_confirmed_cases)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dates[-13:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.sort(train.Date.unique())[-7:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fitted_fatalities = []\n# for clusterId, s_c_id in clusterId_to_id_fatalities.items():\n#     s_c_id = np.sort(s_c_id)\n#     n_components = 30\n#     n_order = 15\n#     df_cluster = cluster_forecasts(train_fatalities,clusterId,s_c_id,dates,n_ahead,n_components,n_order,mode=\"training\",plot=False)\n#     fitted_fatalities.append(df_cluster)\n#     print(\"======================================\")\n# fitted_fatalities = pd.concat(fitted_fatalities)\n\nn_components = 30\nn_order = 15\nfitted_fatalities = cluster_forecasts(train_fatalities,\"fatalities\",s_c_ids,dates,n_ahead,n_components,n_order,mode=\"training\",plot=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pred_fatalities = []\n# for clusterId, s_c_id in clusterId_to_id_fatalities.items():\n#     s_c_id = np.sort(s_c_id)\n#     n_components = 45\n#     n_order = 15\n#     df_cluster = cluster_forecasts(train_fatalities,clusterId,s_c_id,dates,n_ahead,n_components,n_order,mode=\"forecasts\",plot=False)\n#     pred_fatalities.append(df_cluster)\n#     print(\"======================================\")\n# pred_fatalities = pd.concat(pred_fatalities)\n\nn_components = 30\nn_order = 15\npred_fatalities = cluster_forecasts(train_fatalities,\"fatalities\",s_c_ids,dates,n_ahead,n_components,n_order,mode=\"forecasts\",plot=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fitted_confirmed_cases.rename(columns={'y':'y_cc','yhat':'yhat_cc'},inplace=True)\nfitted_fatalities.rename(columns={'y':'y_f','yhat':'yhat_f'},inplace=True)\nfitted_total = pd.merge(fitted_confirmed_cases,fitted_fatalities,on=['state_country_id','Date'],how='inner')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fitted_total['y_f'] = fitted_total['y_f']*fitted_total['y_cc']\n# fitted_total['yhat_f'] = fitted_total['yhat_f']*fitted_total['yhat_cc']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dates[-11:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def RMSLE(pred,actual):\n    return np.sqrt(np.mean(np.power((np.log(pred+1)-np.log(actual+1)),2)))\ndates_sub = dates[-11:]\nsub = fitted_total[fitted_total.Date.isin(dates_sub)].reset_index(drop=True)\nrmsle_confirmed = RMSLE(sub.yhat_cc,sub.y_cc)\nrmsle_fatalites = RMSLE(sub.yhat_f,sub.y_f)\nprint(rmsle_confirmed)\nprint(rmsle_fatalites)\nprint((rmsle_confirmed+rmsle_fatalites)/2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fitted_total['Date'] = pd.to_datetime(fitted_total['Date'], format='%Y-%m-%d')\nfitted_total.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# result = pd.merge(train[['Id','State','Country','Date','state_country_id']],fitted_total,on=['state_country_id','Date'],how='inner')\n# result.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# result.to_csv(\"/kaggle/working/result.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pred_confirmed_cases['yhat_exp'] = pred_confirmed_cases['yhat'].astype(int)\n# pred_fatalities['yhat_exp'] = pred_fatalities['yhat'].astype(int)\n# # pred_confirmed_cases['yhat'] = pred_confirmed_cases['yhat'].astype(int)\n# # pred_fatalities['yhat'] = pred_fatalities['yhat'].astype(int)\n\nfitted_confirmed_cases['yhat_exp'] = fitted_confirmed_cases['yhat'].astype(int)\nfitted_fatalities['yhat_exp'] = fitted_fatalities['yhat'].astype(int)\n# pred_confirmed_cases['yhat'] = pred_confirmed_cases['yhat'].astype(int)\n# pred_fatalities['yhat'] = pred_fatalities['yhat'].astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['Date'] = test['Date'].dt.strftime('%Y-%m-%d')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def util(x):\n#     if(x['Date']<='2020-04-09'):\n#         return x['y']\n#     else:\n#         return x['yhat']\n# pred_confirmed_cases['yhat'] = pred_confirmed_cases.apply(lambda x:util(x),1)\n# pred_fatalities['yhat'] = pred_fatalities.apply(lambda x:util(x),1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_updated = pd.merge(test,pred_confirmed_cases,on=['state_country_id','Date'],how='left')\ntest_updated.rename(columns={'yhat':'ConfirmedCases'},inplace=True)\ntest_updated.drop(columns=['y'],inplace=True)\n\ntest_updated = pd.merge(test_updated,pred_fatalities,on=['state_country_id','Date'],how='left')\ntest_updated.rename(columns={'yhat':'Fatalities'},inplace=True)\ntest_updated.drop(columns=['y'],inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntest_updated.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_updated = test_updated[['ForecastId','ConfirmedCases','Fatalities']]\ntest_updated.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_updated.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_updated.to_csv(\"submission.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}