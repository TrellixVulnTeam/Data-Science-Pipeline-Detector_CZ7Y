{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n#%matplotlib inline\n#%config InlineBackend.figure_format = 'retina'\n\nimport matplotlib.pyplot as plt\nimport datetime\nfrom tqdm import tqdm\n\nfrom sklearn.metrics import mean_squared_error\nimport tensorflow as tf\nimport tensorflow.keras.layers as KL\nfrom tensorflow.keras.optimizers import Nadam\nfrom datetime import timedelta\nimport csv\nimport seaborn as sns\nfrom sklearn.preprocessing import OneHotEncoder\n\ndatapath       = '../input/covid19-global-forecasting-week-4/'\ndatapath2      = '../input/worldpopulationinfo/'\ndatapath3      = '../input/country-ppp/'\ndatapath4      = '../input/populationandcountryinfo/'\ndatapath5      = '../input/usstateland/'\ndatapath_week1 = '../input/covid19week1/'\n\nadd_other = True\nCURVE_SMOOTHING = True\nUSE_NEW = False\n\nnormfactor = 1.0\nNUM_SHIFT = 25\ndays_shift =[0,1,2,3,5,7,10,14,18,22,27,32,37,42]\ndays_shift =[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,16,18,20,22,24,26,28,30,32,34,36]\n#days_shift =[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25]  #virkede med 5\nNUM_MODELS = 10\nTRAIN_START_DAY = 1   # betyder reelt ingenting nå rvi tager så meget med bagud\n\nTARGETS = [\"ConfirmedCases\", \"Fatalities\"]\nif USE_NEW:\n    TARGETS = [\"NewCases\", \"NewFatalities\"]\n    \nloc_group = [\"Province_State\", \"Country_Region\"] # in part1 \n\nif add_other:\n    base_features = ['ages 0-14', 'ages 15-64', 'ages 64-',\n       'Population (2020)', 'Density (P/Km²)', 'Med. Age', 'Urban Pop %',\n       'Apr', 'year', 'Low Temp', 'ppp', 'Season',\n       'R_Africa','R_China',\n       'R_Australia and New Zealand', 'R_Eastern Asia', 'R_Europe',\n       'R_Latin America and the Caribbean', 'R_Northern America', 'R_Oceania',\n       'R_Southern Asia', 'R_Western Asia',\n       'I_High income', 'I_Low income', 'I_Lower middle income',\n       'I_Upper middle income']\n    base_features = ['ages 64-',\n        'Urban Pop %',\n       'Season','Apr','ppp']\n#    ,\n#       'R_Africa',\n#       'R_Australia and New Zealand', 'R_Eastern Asia', 'R_Europe',\n#       'R_Latin America and the Caribbean', 'R_Northern America', 'R_Oceania',\n#       'R_Southern Asia', 'R_Western Asia']\n\n    if USE_NEW:\n        base_features =['NewFatalities_ppm','NewCases_ppm']    + base_features\n    else:\n        a=1\n        #base_features =['Fatalities_ppm','ConfirmedCases_ppm'] + base_features\nelse:\n    base_features=[]\n\ndef get_shift_features():\n    shift_features = []\n    for s in range(1, NUM_SHIFT+1):\n        for col in TARGETS:\n            shift_features.append(\"prev_{}_{}\".format(col, days_shift[s]))\n    return shift_features\n\nshift_features = get_shift_features()\nprev_targets   = shift_features[0:2] # f.eks. 'prev_ConfirmedCases_1', 'prev_Fatalities_1']\n    \n\ndef fill_shift_columns(df,targets):\n    for s in range(1, NUM_SHIFT+1):    #1->5   # Laver shiftede kolonner\n        for col in targets:\n            df[\"prev_{}_{}\".format(col, days_shift[s])] = df.groupby(loc_group)[col].shift(days_shift[s])\n    return df\n\n\ndef add_seasons(coor_df):\n    coor_df[\"Season\"] = 0\n    mask = coor_df[\"Lat\"]>20\n    coor_df.loc[mask,\"Season\"] = 1\n    mask = coor_df[\"Lat\"]<-20\n    coor_df.loc[mask,\"Season\"] = -1\n    return coor_df\n\ndef preprocess(df):\n    df[\"Date\"] = df[\"Date\"].astype(\"datetime64[ms]\")\n    df[\"days\"] = (df[\"Date\"] - pd.to_datetime(\"2020-01-01\")).dt.days\n\n    for col in loc_group:\n        df[col].fillna(\"none\", inplace=True)    # countries with no information in a \"none\" group\n    return df\n\ndef print_rows_with_nan(df,features=False):\n    if not features:\n        dyt = df.isna()\n        dyt2 = dyt.any(axis=1)\n        dyt2.sum()\n        #print(dyt2)\n        print(df.loc[dyt2])\n    else:\n        dyt = df[features].isna()\n        dyt2 = dyt.any(axis=1)\n        dyt2.sum()\n        #print(dyt2)\n        print( df[features].loc[dyt2])\n    #df.loc[dyt2,['Country_Region','Province_State','ConfirmedCases','Fatalities','Date','prev_NewCases_1']]\n    return\n\ndef diff(first, second):\n    second = set(second)\n    return [item for item in first if item not in second]\n\ndef has_nan(df,features=False):\n    if not features:\n        dyt = df.isnull()\n    else:\n        dyt = df[features].isnull()\n    dyt2 = dyt.any(axis=1)\n    return dyt2.sum()\n# NUMPY VERSION:     if np.isnan(np.sum(df)):\n\ndef print_washinton(df,features,values):\n    print(len(values))\n    if len(values)== 1:\n        mask1 = df[features[0]]==values[0]\n        print(df.loc[mask1,features+[\"Date\",\"ConfirmedCases\",\"Fatalities\"]].tail(50))\n    elif len(values)== 2:\n        mask1 = df[features[0]]==values[0]\n        mask2 = df[features[1]]==values[1]\n        mask4 = pd.concat((mask1,mask2), axis=1)\n        mask5 = mask4.all(axis=1)\n        print(df.loc[mask5,features+[\"Date\",\"ConfirmedCases\",\"Fatalities\"]].tail(50))\n    else: \n        mask1 = df[features[0]]==values[0]\n        mask2 = df[features[1]]==values[1]\n        mask3 = df[features[2]]==values[2]\n        mask4 = pd.concat((mask1,mask2,mask3), axis=1)\n        mask5 = mask4.all(axis=1)\n        print(df.loc[mask5,[\"Date\"]])\n    return\n    \n   \n    \ndef find_anomalities(df):\n#    df[['Cases_test','Fatality_test']] =  df.groupby(['Country_Region', 'Province_State']) \\\n#                [['ConfirmedCases','Fatalities']].shift(1) \n    df[['Cases_test']] =  df.groupby([ 'Country_Region', 'Province_State']) \\\n                [['ConfirmedCases']].shift(1) \n    df[['Fatality_test']] =  df.groupby(['Country_Region', 'Province_State']) \\\n                [['Fatalities']].shift(1) \n\n    mask1 =  df['ConfirmedCases'] < df['Cases_test']\n    mask2 =  df['Fatalities']     < df['Fatality_test']\n    print(\"ANOMALITIES ConfirmedCases\")\n    print(df.loc[mask1])\n    print(\"ANOMALITIES Fatalities\")\n    print(df.loc[mask2])\n    print(\"SLUT\")\n    return\n\ndef correct_anomalities(df):\n    df[['Cases_testm']] =  df.groupby([ 'Country_Region', 'Province_State']) \\\n                [['ConfirmedCases']].shift(1) \n    df[['Fatalities_testm']] =  df.groupby(['Country_Region', 'Province_State']) \\\n                [['Fatalities']].shift(1) \n    df[['Cases_testp']] =  df.groupby([ 'Country_Region', 'Province_State']) \\\n                [['ConfirmedCases']].shift(-1) \n    df[['Fatalities_testp']] =  df.groupby(['Country_Region', 'Province_State']) \\\n                [['Fatalities']].shift(-1) \n    print(\"CORRECTING ConfirmedCases\")\n    #dyk\n    mask1 =  df['ConfirmedCases'] < df['Cases_testm']\n    mask2 =  df['Cases_testp']    >= df['Cases_testm']\n    mask3 =  pd.concat((mask1,mask2),axis=1)\n    mask4 =  mask3.all(axis=1)\n    df.loc[mask4,'ConfirmedCases'] = 0.5* (df.loc[mask4,'Cases_testp']+df.loc[mask4,'Cases_testm'])\n    print(df.loc[mask4,['Country_Region', 'Province_State','Cases_testm','ConfirmedCases','Cases_testp']])\n    \n    #spids\n    mask1 =  df['ConfirmedCases'] > df['Cases_testm']\n    mask2 =  df['Cases_testp']    == df['Cases_testm']\n    mask3 =  pd.concat((mask1,mask2),axis=1)\n    mask4a =  mask3.all(axis=1)\n    df.loc[mask4a,'ConfirmedCases'] = 0.5* (df.loc[mask4a,'Cases_testp']+df.loc[mask4a,'Cases_testm'])\n    print(df.loc[mask4a,['Country_Region', 'Province_State','Cases_testm','ConfirmedCases','Cases_testp']])  \n    print(\"CORRECTED ConfirmedCases:\", mask4.sum(),\"+\", mask4a.sum())\n    \n    print(\"CORRECTING Fatalities\")\n    mask1 =  df['Fatalities']       < df['Fatalities_testm']\n    mask2 =  df['Fatalities_testp'] >= df['Fatalities_testm']\n    mask3 =  pd.concat((mask1,mask2),axis=1)\n    mask4 =  mask3.all(axis=1)\n    df.loc[mask4,'Fatalities'] = 0.5* (df.loc[mask4,'Fatalities_testp']+df.loc[mask4,'Fatalities_testm'])\n    print(df.loc[mask4,['Country_Region', 'Province_State','Fatalities_testm','Fatalities','Fatalities_testp']])\n\n    mask1 =  df['Fatalities']       >  df['Fatalities_testm']\n    mask2 =  df['Fatalities_testp'] == df['Fatalities_testm']\n    mask3 =  pd.concat((mask1,mask2),axis=1)\n    mask4a =  mask3.all(axis=1)\n    df.loc[mask4a,'Fatalities'] = 0.5* (df.loc[mask4a,'Fatalities_testp']+df.loc[mask4a,'Fatalities_testm'])\n    print(df.loc[mask4a,['Country_Region', 'Province_State','Fatalities_testm','Fatalities','Fatalities_testp']])\n    print(\"CORRECTED Fatalities:\",mask4.sum(),\"+\",mask4a.sum())\n    \n    #MULTIPLE FEJL\n    print(\"CORRECTING Series (in Mix)\")\n    cases = 1\n    sumcases = 0\n    while cases > 0:\n        mask1 =  df['ConfirmedCases'] < df['Cases_testm']\n        df.loc[mask1,'ConfirmedCases'] = df.loc[mask1,'Cases_testm']\n        print(\"Cases\",df.loc[mask1,['Country_Region', 'Province_State','Cases_testm','ConfirmedCases']])\n\n        mask2 =  df['Fatalities']  < df['Fatalities_testm']\n        df.loc[mask2,'Fatalities'] = df.loc[mask2,'Fatalities_testm']\n        print(\"Fatas\",df.loc[mask2,['Country_Region', 'Province_State','Fatalities_testm','Fatalities']])\n        \n        cases = mask1.sum()+mask2.sum()\n        sumcases+=cases\n        print\n        df[['Cases_testm']] =  df.groupby([ 'Country_Region', 'Province_State']) \\\n                    [['ConfirmedCases']].shift(1) \n        df[['Fatalities_testm']] =  df.groupby(['Country_Region', 'Province_State']) \\\n                    [['Fatalities']].shift(1) \n\n    df.drop([ 'Cases_testm', 'Fatalities_testm', 'Cases_testp','Fatalities_testp'],axis=1,inplace=True)\n    print(\"Corrected in Series in all\",sumcases)\n    return ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\npopulation_by_age_df  = pd.read_csv(datapath4 + \"population_age_info.csv\")\npopulation_by_age_df.drop('ID',axis=1,inplace=True)\npopulation_by_age_df[['ages 0-14', 'ages 15-64','Density (P/Km²)','Med. Age', 'ages 64-','Urban Pop %']] = \\\n            population_by_age_df[['ages 0-14', 'ages 15-64','Density (P/Km²)','Med. Age', 'ages 64-','Urban Pop %']].apply(lambda x: x.fillna(x.mean()))\n\ntranslations = [[\"Brunei Darussalam\",\"Brunei\"],\n                [\"Myanmar\",\"Bruma\"],\n                [\"Gambia, The\",\"Gambia\"],\n                [\"Egypt, Arab Rep.\",\"Egypt\"],\n                [\"Congo, Rep.\",\"Congo (Brazzaville)\"],\n                [\"Congo, Dem. Rep.\",\"Congo (Kinshasa)\"],\n                [\"Iran, Islamic Rep.\",\"Iran\"],\n                [\"Korea, Rep.\",\"Korea, South\"],\n                [\"Russian Federation\",\"Russia\"],\n                [\"Syrian Arab Republic\",\"Syria\"],\n                [\"Venezuela, RB\",\"Venezuela\"],\n                [\"Slovak Republic\",\"Slovakia\"]]\nfor post in translations:\n    #print(post[0],post[1])\n    mask = population_by_age_df[\"Country_Region\"] == post[0]\n    population_by_age_df.loc[mask,\"Country_Region\"] = post[1]\n\nusstates_land       = pd.read_csv(datapath5 + \"us-state-land.csv\", sep='\\t')\nusstates_population = pd.read_csv(datapath2 + \"us-state-population.csv\", sep='\\t')\nusstates_info       = pd.merge(usstates_land,usstates_population, on=['Province_State'], how='left')#, indicator=True)\nusstates_info['Country_Region'] = 'US'\nusstates_info[\"Province_State\"].replace( '_',' ', regex=True,inplace=True)  # _ var indført for at få den til at læse\n#print(usstates_info)\n\nsupp_info = pd.read_csv(datapath2 + \"population_info_supplement.csv\")\nsupp_info[\"Country_Region\"].replace( ':',',', regex=True,inplace=True)  # _ var indført for at få den til at læse\n#print(supp_info)\n\nCountryRegion         = pd.read_csv(datapath2+'CountryRegion.csv')\nCountryRegion.drop('ID',axis=1,inplace=True)\n\nstate_temperatures    = pd.read_csv(datapath4+'state_temperature_info.csv')\nstate_temperatures.drop('ID',axis=1,inplace=True)\nstate_temperatures[\"Province_State\"].fillna(\"\", inplace=True)    # countries with no information in a \"\" group\n\n#\nppp_tabel = pd.read_csv(datapath3 + 'Country_PPP.csv', sep='\\s+')#.sort_values(by=['Country'])\nppp_tabel.drop('Id', 1,inplace=True)\nppp_tabel = ppp_tabel.append({'Country' : 'Burma' , 'ppp' : 8000} , ignore_index=True)\nppp_tabel = ppp_tabel.append({'Country' : 'MS_Zaandam' , 'ppp' : 40000} , ignore_index=True)\nppp_tabel = ppp_tabel.append({'Country' : 'West_Bank_and_Gaza' , 'ppp' : 20000} , ignore_index=True)\nppp_tabel[\"Country\"].replace( '_',' ', regex=True,inplace=True)  # _ var indført for at få den til at læse\nppp_tabel[\"Country\"].replace( 'United States','US', regex=True,inplace=True)  # _ var indført for at få den til at læse\nppp_tabel.rename(columns={'Country':'Country_Region'},inplace=True)\nppp_tabel.sort_values('Country_Region',inplace=True)\n\n\n# Week1 data are aquired to get the (latitude,longitude) coordinates information for countries\ncoor_df = pd.read_csv(datapath_week1 + \"train.csv\").rename(columns={\"Country/Region\": \"Country_Region\",\"Province/State\":\"Province_State\"})\ncoor_df[\"Province_State\"].fillna(\"\", inplace=True)    # countries with no information in a \"\" group\ncoor_df = coor_df[coor_df[\"Country_Region\"].notnull()]\ncoor_df = coor_df.groupby([\"Country_Region\",\"Province_State\"])[[\"Lat\", \"Long\"]].mean().reset_index()\ncoor_df = add_seasons(coor_df)\n\ncoor_country = coor_df.groupby([\"Country_Region\"])[[\"Lat\", \"Long\"]].mean().reset_index()\ncoor_country = add_seasons(coor_country)\ncoor_country.rename(columns={'Season':'Season2'},inplace=True)\n\ncoor_df = pd.merge(coor_df,coor_country, on=['Country_Region'], how='left')#, indicator=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef add_other_info(df,istest_df,usstates_info,supp_info,population_by_age_df,CountryRegion,state_temperatures,ppp_tabel,coor_df):\n    \n    df = pd.merge(df, usstates_info, on=['Country_Region','Province_State'], how='left')#, indicator=True)\n\n    df = pd.merge(df, population_by_age_df, on=['Country_Region'], how='left')#, indicator=True)\n    df[['ages 0-14', 'ages 15-64','Density (P/Km²)','Med. Age', 'ages 64-','Urban Pop %']] = \\\n            df[['ages 0-14', 'ages 15-64','Density (P/Km²)','Med. Age', 'ages 64-','Urban Pop %']].apply(lambda x: x.fillna(x.mean()))\n    print(df[df['IncomeGroup'].isnull()][\"Country_Region\"].unique())\n\n    df = pd.merge(df, CountryRegion, on=['Country_Region'], how='left')#, indicator=True)\n    \n    mask = df['Country_Region']=='US'\n    df.loc[mask,'Population (2020)'] = df.loc[mask,'population']\n    df.loc[mask,'Density (P/Km²)']   = df.loc[mask,'population']/df.loc[mask,'land']\n    df.drop([\"population\",\"land\"],axis=1,inplace=True)\n\n    df = pd.merge(df, supp_info, on=['Country_Region'], how='left')#, indicator=True)\n\n    mask = df['Population (2020)'].isnull()\n    df.loc[mask,'Population (2020)'] = df.loc[mask,'population']\n    df.loc[mask,'Density (P/Km²)']   = df.loc[mask,'population']/df.loc[mask,'land']\n    df.drop([\"population\",\"land\"],axis=1,inplace=True)\n       \n    df = pd.merge(df, state_temperatures, on=['Country_Region','Province_State'], how='left')#, indicator=True)\n    df[['Jan', 'Feb', 'Mar', 'Apr', 'May',\n       'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec', 'year', 'Low Temp']] = \\\n            df[['Jan', 'Feb', 'Mar', 'Apr', 'May',\n       'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec', 'year', 'Low Temp']].apply(lambda x: x.fillna(x.mean()))\n    df.drop(['May','Jun', 'Jul', 'Aug', 'Sep', 'Oct','Nov', 'Dec'],axis=1,inplace=True )\n    df.drop(['Jan', 'Feb', 'Mar'],axis=1,inplace=True )\n    \n    df = pd.merge(df,ppp_tabel, on=['Country_Region'], how='left')#, indicator=True)\n    \n    df = pd.merge(df,coor_df[[\"Country_Region\",'Province_State',\"Season\",\"Season2\"]], \n                          on=[\"Country_Region\",'Province_State'], how=\"left\")#, indicator=True)\n\n#    coor_df.rename(columns={\"Season2\":\"Season\"},inplace= True)\n    mask = df['Season'].isna()\n    df.loc[mask,[\"Season\"]]= df.loc[mask,\"Season2\"]\n    df[\"Season\"].fillna(1, inplace=True)\n    \n    mask = df['Country_Region']== 'China'\n    df.loc[mask,[\"sub-region\"]]= 'China'\n\n    onehot = pd.get_dummies(df['sub-region'],prefix='R')\n    onehot2 = pd.get_dummies(df['IncomeGroup'],prefix='I')\n    df.drop([\"Season2\",\"sub-region\",'IncomeGroup'],axis=1,inplace= True)\n\n    df = pd.concat( [df,onehot],axis=1,ignore_index=False)\n    df = pd.concat( [df,onehot2],axis=1,ignore_index=False)\n\n    # Calculate ppm\n    if istest_df:\n        if USE_NEW:\n            df[\"NewCases_ppm\"]       = np.log1p(1000000/df[\"Population (2020)\"])\n            df[\"NewFatalities_ppm\"]  = np.log1p(1000000/df[\"Population (2020)\"])\n        else:\n            df[\"ConfirmedCases_ppm\"] = np.log1p(1000000/df[\"Population (2020)\"])\n            df[\"Fatalities_ppm\"]     = np.log1p(1000000/df[\"Population (2020)\"])\n    else:\n        if USE_NEW:\n            df[\"NewCases_ppm\"]       = np.log1p(df[\"NewCases\"]*1000000/df[\"Population (2020)\"])\n            df[\"NewFatalities_ppm\"]  = np.log1p(df[\"NewFatalities\"]*1000000/df[\"Population (2020)\"])\n        else:\n            df[\"ConfirmedCases_ppm\"] = np.log1p(df[\"ConfirmedCases\"]*1000000/df[\"Population (2020)\"])\n            df[\"Fatalities_ppm\"]     = np.log1p(df[\"Fatalities\"]*1000000/df[\"Population (2020)\"])\n\n    #normeringer\n    df['ages 0-14'] = df['ages 0-14']/100.\n    df['ages 15-64'] = df['ages 15-64']/100.\n    df['ages 64-'] = df['ages 64-']/100.\n    df['Population (2020)'] = np.log1p(df['Population (2020)'])\n    df['Density (P/Km²)'] = np.log1p(df['Density (P/Km²)'])\n    df['Med. Age'] = df['Med. Age']/50.\n    df['Urban Pop %'] = df['Urban Pop %']/100.\n    df['Apr'] = df['Apr']/30.\n    df['year'] = df['year']/30.\n    df['Low Temp'] = df['Low Temp']/30.\n    df['ppp'] = np.log1p(df['ppp'])\n    return df\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"\ndf     = pd.read_csv(datapath + \"train.csv\")\nsub_df = pd.read_csv(datapath + \"test.csv\")\n\ndf['Province_State'].fillna('', inplace=True)\nsub_df['Province_State'].fillna('', inplace=True)\n\n#tag kopi af TARGETS til senere brug\ngem_targets = df[['Country_Region','Province_State','Date']+TARGETS]\ngem_targets[\"Date\"] = gem_targets[\"Date\"].astype(\"datetime64[ms]\")\n\n#find alle combinationer af Countrt/State\ncombi= df.groupby([\"Country_Region\",\"Province_State\"]).size().reset_index()\ncombi =combi[[\"Country_Region\",\"Province_State\"]].values.tolist()\n\n\nmask1 = df[\"Country_Region\"]=='Guyana'             # håndrettes\nmask2 = df[\"Date\"] > '2020-03-21'\nmask3 = df[\"Date\"] < '2020-03-28'\nmask4 = pd.concat((mask1,mask2,mask3),axis=1)\nmask5 = mask4.all(axis=1)\ndf.loc[mask5,\"ConfirmedCases\"] = 7.0\n\nmask1 = df[\"Province_State\"]=='Northern Territory' # håndrettes\nmask2 = df[\"Date\"] > '2020-03-05'\nmask3 = df[\"Date\"] < '2020-03-10'\nmask4 = pd.concat((mask1,mask2,mask3),axis=1)\nmask5 = mask4.all(axis=1)\ndf.loc[mask5,\"ConfirmedCases\"] = 1.0\n\nmask1 = df[\"Country_Region\"]=='Philippines'        # håndrettess\nmask2 = df[\"Date\"] > '2020-03-17'\nmask3 = df[\"Date\"] < '2020-03-19'\nmask4 = pd.concat((mask1,mask2,mask3),axis=1)\nmask5 = mask4.all(axis=1)\ndf.loc[mask5,\"Fatalities\"] = 14.0\n\ncorrect_anomalities(df)\n\n#find_anomalities(df)\n\nif USE_NEW:\n    df[['NewCases','NewFatalities']] = df.groupby(['Country_Region', 'Province_State']) \\\n                [['ConfirmedCases','Fatalities']].transform(lambda x: x.diff()) \n    mask = df['NewCases'].isnull()\n    df.loc[mask,['NewCases']] = 0\n    df.loc[mask,['NewFatalities']] = 0\n    provins = 'Virgin Islands'\n    print(\"testprint for\",provins)\n    print(df[df['Province_State']== provins][['NewCases','NewFatalities']])\n\ndef row_count(filename):\n    with open(filename) as in_file:\n        return sum(1 for _ in in_file)   \n\ndf     = preprocess(df)\nsub_df = preprocess(sub_df)\n\nGIVEN_FIRST_DATE = df[\"Date\"].min()\nGIVEN_LAST_DATE  = df[\"Date\"].max()\nSUBMISSION_FIRST_DATE =  sub_df[\"Date\"].min()\nESTIMATE_FIRST_DATE = df[\"Date\"].max() + timedelta(days=1)\nESTIMATE_LAST_DATE  = sub_df[\"Date\"].max()\nESTIMATE_DAYS = (ESTIMATE_LAST_DATE - ESTIMATE_FIRST_DATE).days +1\n\nif add_other:\n    df     = add_other_info(df,False,usstates_info,supp_info,population_by_age_df,CountryRegion,state_temperatures,ppp_tabel,coor_df)\n    sub_df = add_other_info(sub_df,True,usstates_info,supp_info,population_by_age_df,CountryRegion,state_temperatures,ppp_tabel,coor_df)\n\nprint(\"SUB_DF COLUMNS NYLAVET:\")\nprint(sub_df.columns)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nif CURVE_SMOOTHING:\n    #Add averages\n    df['Cases_m'] =  df.groupby(['Country_Region', 'Province_State'])[['ConfirmedCases']].transform(lambda x: x.shift(1)) \n    df['Cases_p']  = df.groupby(['Country_Region', 'Province_State'])[['ConfirmedCases']].transform(lambda x: x.shift(-1)) \n    df['Cases_ave'] = 0.5*(df['ConfirmedCases']+0.5*(df['Cases_p']+df['Cases_m']))\n    case_cols = ['ConfirmedCases','Cases_m','Cases_p','Cases_ave']\n\n    df['Fatalities_m'] =  df.groupby(['Country_Region', 'Province_State'])[['Fatalities']].transform(lambda x: x.shift(1)) \n    df['Fatalities_p']  = df.groupby(['Country_Region', 'Province_State'])[['Fatalities']].transform(lambda x: x.shift(-1)) \n    df['Fatalities_ave'] = 0.5*(df['Fatalities']+0.5*(df['Fatalities_p']+df['Fatalities_m']))\n    fata_cols = ['Fatalities','Fatalities_m','Fatalities_p','Fatalities_ave']\n\n    if USE_NEW:\n        df[['NewCases','NewFatalities']] = df.groupby(['Country_Region', 'Province_State']) \\\n                            [['ConfirmedCases','Fatalities']].transform(lambda x: x.diff()) \n\n        df['NewCases_m'] =  df.groupby(['Country_Region', 'Province_State'])[['NewCases']].transform(lambda x: x.shift(1)) \n        df['NewCases_p']  = df.groupby(['Country_Region', 'Province_State'])[['NewCases']].transform(lambda x: x.shift(-1)) \n        df['NewCases_m2'] =  df.groupby(['Country_Region', 'Province_State'])[['NewCases']].transform(lambda x: x.shift(2)) \n        df['NewCases_p2']  = df.groupby(['Country_Region', 'Province_State'])[['NewCases']].transform(lambda x: x.shift(-2)) \n#        df['NewCases_ave'] = 0.5*(df['NewCases']+0.5*(df['NewCases_p']+df['NewCases_m']))\n        df['NewCases_ave'] = 0.2*(df['NewCases']+df['NewCases_p']+df['NewCases_m']+df['NewCases_p2']+df['NewCases_m2'])\n\n        df['NewFatalities_m'] =  df.groupby(['Country_Region', 'Province_State'])[['NewFatalities']].transform(lambda x: x.shift(1)) \n        df['NewFatalities_p']  = df.groupby(['Country_Region', 'Province_State'])[['NewFatalities']].transform(lambda x: x.shift(-1)) \n        df['NewFatalities_m2'] =  df.groupby(['Country_Region', 'Province_State'])[['NewFatalities']].transform(lambda x: x.shift(2)) \n        df['NewFatalities_p2']  = df.groupby(['Country_Region', 'Province_State'])[['NewFatalities']].transform(lambda x: x.shift(-2)) \n        df['NewFatalities_ave'] =0.2*(df['NewFatalities']+df['NewFatalities_p']+df['NewFatalities_m']+df['NewFatalities_p2']+df['NewFatalities_m2'])\n\n    date_max = df[\"Date\"].max()\n    mask = df[\"Date\"] ==date_max\n    mask2 = df[\"Date\"]==date_max - timedelta(days=1)\n    df.loc[mask,'Cases_ave']         = 0.75*df.loc[mask,'ConfirmedCases']+0.25*df.loc[mask,'Cases_m']\n    df.loc[mask,'Fatalities_ave']    = 0.75*df.loc[mask,'Fatalities']    +0.25*df.loc[mask,'Fatalities_m']\n    df.loc[mask,'Cases_ave']         = 0.75*df.loc[mask,'ConfirmedCases']+0.25*df.loc[mask,'Cases_m']\n    df.loc[mask,'Fatalities_ave']    = 0.75*df.loc[mask,'Fatalities']    +0.25*df.loc[mask,'Fatalities_m']\n    df.drop(['Cases_m', 'Cases_p', 'Fatalities_m','Fatalities_p','ConfirmedCases','Fatalities'],axis=1,inplace=True)\n    df.rename(columns={'Cases_ave':'ConfirmedCases','Fatalities_ave':'Fatalities'},inplace=True)\n\n    if USE_NEW:\n        df.loc[mask2,'NewCases_ave']      = 0.3*df.loc[mask2,'NewCases_p']+0.3*df.loc[mask2,'NewCases']+\\\n                        0.3*df.loc[mask2,'NewCases_m']+0.1*df.loc[mask2,'NewCases_m2']\n        df.loc[mask2,'NewFatalities_ave'] = 0.3*df.loc[mask2,'NewFatalities_p']+0.3*df.loc[mask2,'NewFatalities']+\\\n                        0.3*df.loc[mask2,'NewFatalities_m']+0.1*df.loc[mask2,'NewFatalities_m2']\n        df.loc[mask,'NewCases_ave']      = 0.55*df.loc[mask,'NewCases']  +0.3*df.loc[mask,'NewCases_m']+\\\n                        0.2*df.loc[mask,'NewCases_m2']\n        df.loc[mask,'NewFatalities_ave'] = 0.5*df.loc[mask,'NewFatalities'] +0.3*df.loc[mask,'NewFatalities_m']+\\\n                        0.2*df.loc[mask,'NewFatalities_m2']\n        df.drop(['NewCases_m', 'NewCases_p', 'NewFatalities_m','NewFatalities_p','NewCases','NewFatalities'],axis=1,inplace=True)\n        df.drop(['NewCases_m2', 'NewCases_p2', 'NewFatalities_m2','NewFatalities_p2'],axis=1,inplace=True)\n        df.rename(columns={'NewCases_ave':'NewCases','NewFatalities_ave':'NewFatalities'},inplace=True)\n\n        if add_other:\n            df[\"NewCases_ppm\"]      = np.log1p(df[\"NewCases\"]*1000000/df[\"Population (2020)\"])\n            df[\"NewFatalities_ppm\"] = np.log1p(df[\"NewFatalities\"]*1000000/df[\"Population (2020)\"])\n\ndf.fillna(0, inplace=True)\nsub_df.fillna(0,inplace =True)\n \nDEFAULT_VALUE = 0\nif df is None:\n    df = DEFAULT_VALUE\nif sub_df is None:\n    sub_df = DEFAULT_VALUE\n\nif has_nan(df,base_features):\n    print(\"DYT-DYYYYYT: der er NAN\")\n    print_rows_with_nan(df)\nelse:\n    print(\"HURRA, HURRA\")\n\nif has_nan(df)>0:\n    print(\"DOOOOOOOOOT: der er NAN\")\n    print_rows_with_nan(df)\n    print(\"DOT DONE\")\nelse:\n    print(\"HIP HIP\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndf = df[df[\"Date\"] >= df[\"Date\"].min() + timedelta(days=days_shift[NUM_SHIFT])].copy() # Cutter af så alle shift er med i df (i princippet)\n\n#df[features] = df[features].fillna(method='ffill',inplace=True)\n#df[features] = df[features].fillna(method='bfill',inplace=True)\n#df[features] = df[features].fillna(0)\n\nfor col in TARGETS:                        #Laver targets om til Logaritmer\n    df[col] = np.log1p(df[col])/normfactor\n        \ndf = df[df['days']>TRAIN_START_DAY]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef nn_block(input_layer, size, dropout_rate, activation):\n    out_layer = KL.Dense(size, activation=None)(input_layer)\n    #out_layer = KL.BatchNormalization()(out_layer)\n    out_layer = KL.Activation(activation)(out_layer)\n    out_layer = KL.Dropout(dropout_rate)(out_layer)\n    return out_layer\n\ndef get_model(feature_length,target_length,):\n    inp = KL.Input(shape=(feature_length,))\n\n    hidden_layer = nn_block(inp, 64, 0.0, \"relu\")\n    gate_layer = nn_block(hidden_layer, 32, 0.0, \"sigmoid\")\n    hidden_layer = nn_block(hidden_layer, 64, 0.0, \"relu\")\n    hidden_layer = nn_block(hidden_layer, 32, 0.0, \"relu\")\n    hidden_layer = KL.multiply([hidden_layer, gate_layer])\n\n    out = KL.Dense(target_length, activation=\"linear\")(hidden_layer)\n\n    model = tf.keras.models.Model(inputs=[inp], outputs=out)\n    return model\n\n\ndef get_input(df,features):\n    return [df[features]]\n\n\ndef train_models(df,features,targets,save=False):\n    print()\n    print(\"TRAINING. Der regnes på denne model:\")\n    get_model(len(features),len(targets)).summary()\n    models = []\n    for i in range(NUM_MODELS):\n        print(\"PHS1\")\n        model = get_model(len(features),len(targets))\n        print(\"PHS2\")\n        model.compile(loss=\"mean_squared_error\", optimizer=Nadam(lr=1e-4))\n        print(\"PHS3 - Targets:\",targets)\n        hist = model.fit(get_input(df,features), df[targets],\n                         batch_size=2048, epochs=500, verbose=0, shuffle=True)\n        print(\"PHS4\")\n        if save:\n            print(\"PHS5\")\n            model.save_weights(\"model{}.h5\".format(i))\n            print(\"PHS6\")\n        models.append(model)\n        print(\"PHS7\")\n    return models\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef rmse(y_true, y_pred):\n    return np.sqrt(mean_squared_error(y_true, y_pred))\n\ndef evaluate(df,targets):\n    error = 0\n    for col in targets:\n        error += rmse(df[col].values, df[\"pred_{}\".format(col)].values)\n    return np.round(error/len(targets), 5)\n\ndef predict_one(df,features,prev_targets, models):\n    pred = np.zeros((df.shape[0], 2))           \n        \n    for model in models:\n        pred += model.predict(get_input(df,features))/len(models)\n    if np.isnan(np.sum(pred)):\n        print(\"DYYYYYYYYYYYYT: der er NAN\")\n        print(pred)\n#    if USE_NEW:\n#        pred[:, 0] = np.maximum(pred[:, 0], 0)          #new cases kan ikke gå under 0\n#        pred[:, 1] = np.maximum(pred[:, 1], 0)          #new fatalities kan ikke gå under 0\n#    else:\n    pred = np.maximum(pred, df[prev_targets].values)\n    #pred[:, 0] = np.maximum(pred[:, 0], df[prev_targets[0]].values)          #sum kan aldrig gå ned\n    #pred[:, 1] = np.maximum(pred[:, 1], df[prev_targets[1]].values)          #sum kan aldrig gå ned#\n    pred[:, 0] = np.log1p(np.expm1(normfactor*pred[:, 0]) + 0.1)/normfactor\n    pred[:, 1] = np.log1p(np.expm1(normfactor*pred[:, 1]) + 0.01)/normfactor\n         \n    if np.isnan(np.sum(pred)):\n        print(\"DYT DYT DYT: der er NAN\")\n\n    return np.clip(pred, None, 15)    # intet minimum men tilladt højst 15\n\ndef predict(test_df,features,targets,prev_targets,first_day, num_days, models, val=False):\n\n    for d in range(0, num_days):\n        print(\"DAY NO.\",d)\n        test_df = fill_shift_columns(test_df,targets)       \n\n        date = first_day + timedelta(days=d)\n        temp_df = test_df.loc[test_df[\"Date\"] == date].copy()\n        y_pred = predict_one(temp_df,features,prev_targets, models)\n\n        for i, col in enumerate(targets):\n            test_df.loc[test_df[\"Date\"] == date, col] = y_pred[:, i]\n\n        if add_other:   \n            if USE_NEW:\n                test_df.loc[test_df[\"Date\"] == date,\"NewCases_ppm\"] =  \\\n                                    np.log1p(np.expm1(y_pred[:, 0])*1000000/ \\\n                                             np.expm1(test_df.loc[test_df[\"Date\"] == date,\"Population (2020)\"]))\n                test_df.loc[test_df[\"Date\"] == date,\"NewFatalities_ppm\"] =      \\\n                                    np.log1p(np.expm1(y_pred[:, 1])*1000000/ \\\n                                             np.expm1(test_df.loc[test_df[\"Date\"] == date,\"Population (2020)\"]))\n            else:\n                test_df.loc[test_df[\"Date\"] == date,\"ConfirmedCases_ppm\"] =  \\\n                                np.log1p(np.expm1(y_pred[:, 0])*1000000/ \\\n                                         np.expm1(test_df.loc[test_df[\"Date\"] == date,\"Population (2020)\"]))\n                test_df.loc[test_df[\"Date\"] == date,\"Fatalities_ppm\"] =      \\\n                                np.log1p(np.expm1(y_pred[:, 1])*1000000/ \\\n                                         np.expm1(test_df.loc[test_df[\"Date\"] == date,\"Population (2020)\"]))\n\n        if val:\n            print(evaluate(test_df[test_df[\"Date\"] == date]))\n\n    return test_df\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nall_features = base_features + shift_features\n\ndf = fill_shift_columns(df,TARGETS)\ndf[all_features] = df[all_features].fillna(0)\n\nprint(\"Kolonner i modelller\",all_features)    \nprint(\"BEFORE TRAINING\")\nprint(df[(df['Country_Region']=='Germany') & (df['days'] >75)])\n\n\nfinal_models = train_models(df,all_features,TARGETS, save=True)          #final models","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nsub_df_public  = sub_df[sub_df[\"Date\"] <= GIVEN_LAST_DATE].copy()     # den del vi allerede kender\nsub_df_private = sub_df[sub_df[\"Date\"] >  GIVEN_LAST_DATE].copy()     # den del der skal etimeres\n\nprint(\"df Fra/Til  \",df[\"Date\"].min(),df[\"Date\"].max())\nprint(\"Public Fra/Til  \",sub_df_public[\"Date\"].min(),sub_df_public[\"Date\"].max())\nprint(\"Private Fra/Til \",sub_df_private[\"Date\"].min(),sub_df_private[\"Date\"].max())\nprint(\"GIVEN DATES FIRST/LAST\",GIVEN_FIRST_DATE,GIVEN_LAST_DATE)\nprint(\"ESTIMATES DATES FIRST/LAST\",ESTIMATE_FIRST_DATE,ESTIMATE_LAST_DATE)\nprint(\"ESTIMATE_DAYS\",ESTIMATE_DAYS)\n\n#lav ekstra kolonner\nnew_columns = diff(df.columns,sub_df_private.columns)\nfor a in new_columns:\n   sub_df_private[a] = np.NaN\nfull_df = pd.concat([df,sub_df_private])\nprint (\"Full Fra/Til \",full_df[\"Date\"].min(),full_df[\"Date\"].max())\n\n#Flyt ForecastId over på full_df\nsub_df.rename(columns={\"ForecastId\":\"ForecastId2\"},inplace=True)\nfull_df = full_df.merge(sub_df[[\"Date\"] + loc_group+ [\"ForecastId2\"]], how=\"left\", on=[\"Date\"] + loc_group)\nsub_df.rename(columns={\"ForecastId2\":\"ForecastId\"},inplace=True)\nmask= full_df[\"ForecastId\"].isna()\nfull_df.loc[mask,[\"ForecastId\"]]= full_df.loc[mask,[\"ForecastId2\"]]\nfull_df.drop([\"ForecastId2\"],axis=1,inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfull_df_pred= predict(full_df,all_features,TARGETS,prev_targets,ESTIMATE_FIRST_DATE,ESTIMATE_DAYS, final_models)\n\nfor col in TARGETS:                                                      \n    full_df_pred[col] = np.expm1(full_df_pred[col]) # regner tilbage fra logaritme\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gem_targets = gem_targets[gem_targets[\"Date\"]>=SUBMISSION_FIRST_DATE]\nprint(gem_targets.head(15))\n\nvalues_to_submit = full_df_pred[full_df_pred[\"Date\"]>=ESTIMATE_FIRST_DATE]\nvalues_to_submit = values_to_submit[['Date','Country_Region','Province_State','ConfirmedCases', 'Fatalities']]\nprint(values_to_submit.head(15))\n\nvalues_to_submit = gem_targets.append(values_to_submit, sort=False)          # lægger pivate stykke til public\nprint(values_to_submit.tail(10))\n\nprint(sub_df[['Date','Country_Region','Province_State','ForecastId']])\nvalues_to_submit   = pd.merge(values_to_submit,sub_df[['Date','Country_Region','Province_State','ForecastId']], on=['Date','Country_Region','Province_State'], how='left')#, indicator=True)\nprint(values_to_submit.head(20))\nprint(values_to_submit.tail(20))\n\nsub2 =  values_to_submit[[\"ForecastId\"] + TARGETS]\nsub2[\"ForecastId\"] = sub2[\"ForecastId\"].astype(np.int16)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub2.sort_values(\"ForecastId\", inplace=True)\nsub2.to_csv(\"submission.csv\", index=False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(sub2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full_df_pred['Cases_Estimate']      =  full_df_pred['ConfirmedCases']\nfull_df_pred['Fatalities_Estimate'] =  full_df_pred['Fatalities']\nfull_df_pred2 = full_df_pred[['Date','Country_Region','Province_State']+TARGETS]\n\nfull_df2 = full_df[['Date','Country_Region','Province_State']+TARGETS]\n\n\nfull_df3 = pd.merge(full_df2,full_df_pred[['Date','Country_Region','Province_State','Cases_Estimate','Fatalities_Estimate']], on=['Date','Country_Region','Province_State'], how='left')#, indicator=True)\nmask = full_df3['Cases_Estimate'].notna()\nfull_df3.loc[mask,'CasesConfirmed'] =  full_df3.loc[mask,'Cases_Estimate']\nfull_df3.loc[mask,'Fatalities']     =  full_df3.loc[mask,'Fatalities_Estimate']\n             \nsplit_on = 'Country_Region'\nsplit_values = full_df3[split_on].unique()\nkolonne = 'CasesConfirmed'\n#print(\"ConfirmedCases / Private\")\nfor imin in range(0,175,25):\n    plt.figure(figsize=(30,30))\n    imax = imin+25\n    for i in range(imin,imax):\n        plt.subplot(5,5,i-imin+1)\n        idx = i\n        df_interest = full_df3[full_df3[split_on]==split_values[idx]].reset_index(drop=True)\n        tmp = df_interest[kolonne].values\n    #    tmp = np.cumsum(tmp)\n        sns.lineplot(x=df_interest['Date'], y=tmp, label='pred')\n        df_interest2 = full_df3[(full_df3[split_on]==split_values[idx]) & (full_df3['Date']<=GIVEN_LAST_DATE)].reset_index(drop=True)\n        sns.lineplot(x=df_interest2['Date'].values, y=df_interest2[kolonne].values, label='true')\n        plt.title(split_on+'  '+str(split_values[idx]))\n    plt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":4}