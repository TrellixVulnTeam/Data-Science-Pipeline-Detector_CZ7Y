{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n\n#submission = pd.read_csv(\"../input/covid19-global-forecasting-week-1/submission.csv\")\ntest = pd.read_csv(\"../input/covid19-global-forecasting-week-4/test.csv\", parse_dates=[\"Date\"])\ntrain = pd.read_csv(\"../input/covid19-global-forecasting-week-4/train.csv\", parse_dates=[\"Date\"])\nglobal_data = pd.read_csv(\"../input/external/Global_Data_by_Country_2019.csv\")\ncoordinates=pd.read_csv(\"../input/external/coordinates.csv\")\nrestrictions =pd.read_csv(\"../input/restrictions/mainrestrictions.csv\", sep=\";\", parse_dates=[\"BorderClosure\",\"Curfews\",\"DomesticTravel\",\"FullLockdown\",\"InternationalFlights\",\"MassTesting\",\"PartialLockdown\",\"ServicesClosure\",\"SchoolsClosure\"])\n\nrestrictions.rename(columns={\"Country\": \"RestCountry\"}, inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(restrictions.BorderClosure[0])\n#type(train.Date[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def adds_to_cumulative(df, col, newlabel):\n    work=df[df.Province==\"No Such Country\"]\n    work[newlabel]=np.NaN\n    print(1)\n    for province in df.Province.unique():\n        subframe=df[df.Province==province].copy()\n        subframe=subframe.sort_values(\"Date\")\n        minvalue=subframe[col].min()\n        subframesize=df[df.Province==province].size\n        temparray=subframe[col].values.tolist()\n        for i in range(1,len(temparray)-1 ):\n            temparray[i]=temparray[i]+temparray[i-1]\n            #print(temparray[i],temparray[i]-temparray[i-1])\n        temparray[0]=minvalue\n        temparray[-1]=temparray[-1]+temparray[-2]\n        subframe[newlabel]=temparray\n        #print(country, temparray[-1])\n        work=pd.concat([work,subframe])\n        del(subframe)\n        del(temparray)\n        #work.to_csv(\"work.csv\")\n    #df=df.merge(subframe[[\"Country\", \"Date\", \"add\"]], how=\"left\", left_on=[\"Country\", \"Date\"], right_on=[\"Country\", \"Date\"])\n    return work \n\n\n\ndef cumulative_to_adds(df, col, newlabel):\n    work=df[df.Province==\"No Such Country\"]\n    work[newlabel]=np.NaN\n    print(1)\n    for province in df.Province.unique():\n        subframe=df[df.Province==province].copy()\n        minvalue=subframe[col].min()\n        df=df.sort_values(\"Date\")\n        subframesize=df[df.Province==province].size\n        temparray=subframe[col].values.tolist()\n        for i in range(len(temparray)-1,1,-1 ):\n            temparray[i]=temparray[i]-temparray[i-1]\n            #print(temparray[i],temparray[i]-temparray[i-1])\n        temparray[0]=minvalue\n        subframe[newlabel]=temparray\n        #print(country, temparray[-1])\n        work=pd.concat([work,subframe])\n        del(subframe)\n        del(temparray)\n        #work.to_csv(\"work.csv\")\n    #df=df.merge(subframe[[\"Country\", \"Date\", \"add\"]], how=\"left\", left_on=[\"Country\", \"Date\"], right_on=[\"Country\", \"Date\"])\n    return work \n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"coordinates.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"coordinates=coordinates[[\"Country/Region\", \"Province/State\", \"Lat\", \"Long\"]]\ncoordinates.loc[coordinates[\"Province/State\"].isnull(), \"Province/State\"]=coordinates.loc[coordinates[\"Province/State\"].isnull(), \"Country/Region\"]\ncoordinates.drop_duplicates(keep='first', inplace=True)\n\n\nfrom math import radians, sin, cos, asin, sqrt\ndef haversine(lon1, lat1, lon2, lat2):\n    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n    dlon = lon2 - lon1\n    dlat = lat2 - lat1\n    a = sin(dlat / 2) ** 2 + cos(lat1) * cos(lat2) * sin(dlon / 2) ** 2\n    return 2 * 6371 * asin(sqrt(a))\n\n\ncoordinates.set_index(\"Province/State\")\nlatitude=coordinates[coordinates[\"Province/State\"]==\"Hubei\"].Lat\nlongitude=coordinates[coordinates[\"Province/State\"]==\"Hubei\"].Long\n\ncoordinates[\"Distance\"]=coordinates.apply(lambda x : haversine(lat1=x.Lat, lon1=x.Long, lat2=latitude, lon2=longitude), axis=1)\ncoordinates=coordinates[[\"Province/State\", \"Distance\"]]\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"coordinates.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### when there's no province replace it by country name\ntrain.loc[train[\"Province_State\"].isnull(), \"Province_State\"]=train.loc[train[\"Province_State\"].isnull(), \"Country_Region\"]\n###join train data with external data about country mainly : area, population, life expectancy, health spendings\ntrain.rename(columns = {'Country_Region':'Country', 'Province_State':'Province'}, inplace = True) \ntrain=train.merge(global_data, how='left', left_on=['Country', 'Province'], right_on=['CountryName', 'Province'])\ntrain=train.merge(coordinates, how='left', left_on=['Province'], right_on=['Province/State'])\ntrain=train.drop(\"Province/State\", axis=1)\ntrain=train.drop(\"Population\", axis=1)\ntrain=train.drop(\"CountryName\", axis=1)\ntrain=train.rename(columns={\"ExtraColumn\": \"Population\"})\nmindates = train[train[\"ConfirmedCases\"]>0].groupby(['Province'])[\"Date\"].min()\nmindates.reset_index()\nmindatesDF = mindates.to_frame()\nmindatesDF.rename(columns={\"Date\":\"MinDate\"}, inplace=True)\ntrain=train.merge(mindatesDF, how='left', left_on=\"Province\", right_on=\"Province\")\ntrain[\"DaysFrom1stCase\"]=(train[\"Date\"]-train[\"MinDate\"]).dt.days\ntrain.loc [train[\"DaysFrom1stCase\"]<0 , \"DaysFrom1stCase\"] =0\n\ntrain=train.merge(restrictions, how='left', left_on=[\"Country\"], right_on=\"RestCountry\")\nfor col in restrictions.drop(\"RestCountry\", axis=1).columns:\n    train[col + \"days\"] = (train[col]-train[\"MinDate\"]).dt.days\n    train[col + \"days\"]=train[col + \"days\"].fillna(0)\n\nto_drop=restrictions.columns\ntrain=train.drop(to_drop,axis=1)    \n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[train.Country==\"Algeria\"].sort_values(\"Date\", ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train=cumulative_to_adds(train, \"ConfirmedCases\", \"ConfirmedCumul\")\ntrain=train.drop(\"ConfirmedCases\", axis=1)\ntrain=train.rename(columns={\"ConfirmedCumul\":\"ConfirmedCases\"})\ntrain.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[train.Country==\"Algeria\"].sort_values(\"Date\", ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.rename(columns = {'Country_Region':'Country', 'Province_State':'Province'}, inplace = True) \ntest.loc[test[\"Province\"].isnull(), \"Province\"]=test.loc[test[\"Province\"].isnull(), \"Country\"]\nX_test=test.merge(global_data, how='left', left_on=['Country', 'Province'], right_on=['CountryName', 'Province'])\nX_test=X_test.drop(\"Population\", axis=1)\nX_test=X_test.drop(\"CountryName\", axis=1)\nX_test=X_test.rename(columns={\"ExtraColumn\": \"Population\"})\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test=X_test.merge(coordinates, how='left', left_on=['Province'], right_on=['Province/State'])\nX_test=X_test.drop(\"Province/State\", axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder \n\nlabelencoder = LabelEncoder()\ncountrylabelencoder = LabelEncoder()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"province=labelencoder.fit_transform(train[\"Province\"])\n\ncountry=countrylabelencoder.fit_transform(train[\"Country\"])\n\ntrain=pd.concat([train, pd.DataFrame(province)], axis=1)\ntrain=train.rename(columns={0: \"Province_Encoded\"})\ntrain=pd.concat([train,  pd.DataFrame(country)], axis=1)\ntrain=train.rename(columns={0: \"Country_Encoded\"})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntrain=train.drop([\"MinDate\", \"Province\", \"Country\"], axis=1)\n#train=train.drop([\"Country\",\"MinDate\"], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[train.ConfirmedCases>0].head(100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import RandomForestRegressor\n\n\n#X_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nimport lightgbm as lgb\nimport xgboost as xgb\n\n\n### Columns to drop after selection\n\n\nthreshold=train.DaysFrom1stCase.max()*.7\nminrmse_CC=train[\"ConfirmedCases\"].max()**2\nminrmse_F=train[\"Fatalities\"].max()**2\n\n\nparams = {\n        'task': 'train',\n        'boosting_type': 'gbdt',\n        'objective': 'regression',\n        'metric': {'rmse'},\n        'learning_rate': 0.3,\n        'num_leaves': 30,\n        'min_data_in_leaf': 1,\n        'num_iteration': 100,\n        'verbose': 20\n}\n\ncollist=train.drop([\"ConfirmedCases\", \"Fatalities\",  \"Date\",  \"DaysFrom1stCase\"], axis=1)\nlist_CC= ['Id']\n#'HealthExpenditure', 'Province_Encoded', 'BorderClosuredays', 'Curfewsdays' , 'InternationalFlightsdays', 'MassTestingdays',\n#         'ServicesClosuredays', 'SchoolsClosuredays']\ncollist=collist.drop(list_CC, axis=1)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"collist=train.drop([\"ConfirmedCases\", \"Fatalities\",  \"Date\",  \"DaysFrom1stCase\"], axis=1)\nlist_F= ['Id']\n#, 'HealthExpenditure', 'LifeExpectancy', 'DomesticTraveldays', 'PartialLockdowndays', 'InternationalFlightsdays', 'ServicesClosuredays',\n#        'BorderClosuredays', 'MassTestingdays', 'FullLockdowndays', 'Curfewsdays']\ncollist=collist.drop(list_F, axis=1)\n\n\nscore_store={}\nfor col1 in collist.columns:\n    y_train_real_CC=train.loc[train[\"DaysFrom1stCase\"]<=threshold,\"ConfirmedCases\"]\n    y_test_val_CC=train.loc[train[\"DaysFrom1stCase\"]>threshold,\"ConfirmedCases\"]\n    y_train_real_F=train.loc[train[\"DaysFrom1stCase\"]<=threshold, \"Fatalities\"]\n    y_test_val_F=train.loc[train[\"DaysFrom1stCase\"]>threshold, \"Fatalities\"]\n\n\n    \n    df1=train.drop(col1, axis=1)\n    df1=df1.drop([\"ConfirmedCases\", \"Fatalities\",  \"Date\"], axis=1)\n    df1=df1.drop(list_F, axis=1)\n    collist2=df1.drop(\"DaysFrom1stCase\", axis=1)\n    second_col_score={}\n    for col2 in collist2.columns:\n        df2=df1.drop(col2, axis=1)\n        X_train=df2.copy()\n        \n        for col in X_train.columns:\n            X_train[col]=X_train[col].fillna(X_train[col].median())\n        \n        standardscaler= StandardScaler()\n        standardscaler.fit(X_train)\n        \n        X_train_real_CC=X_train[df2[\"DaysFrom1stCase\"]<=threshold]\n        X_test_val_CC=X_train[df2[\"DaysFrom1stCase\"]>threshold]\n        \n        X_train_real_F=standardscaler.transform(X_train_real_F)\n        X_test_val_F=standardscaler.transform(X_test_val_F)\n        X_train_real_F=X_train_real_CC\n        X_test_val_F=X_test_val_CC\n        X_train_real=X_train_real_CC\n        X_test_val=X_test_val_CC\n        y_train_F=y_train_real_F\n\n\n        \n        #X_train_real, X_test_val, y_train_real_F, y_test_val_F = train_test_split(df2, y_train_F, test_size=0.3, random_state=0)\n        #X_train_real, X_test_val, y_train_real_F, y_test_val_F = train_test_split(df2, y_train_F, test_size=0.3, random_state=0)\n        lgb_train_F = lgb.Dataset(X_train_real, y_train_real_F)\n        lgb_eval_F = lgb.Dataset(X_test_val, y_test_val_F, reference=lgb_train_F)\n         \n        gbm_F = lgb.train(params,\n            lgb_train_F,\n            num_boost_round=100,\n            valid_sets=lgb_eval_F,\n            early_stopping_rounds=10, verbose_eval = -1)\n        \n        if gbm_F.best_score['valid_0']['rmse'] < minrmse_F:\n            minrmse_F=gbm_F.best_score['valid_0']['rmse']\n            print (\"MIN RMSE F\" , col1 , col2, minrmse_F)\n            second_col_score[col2]=gbm_CC.best_score['valid_0']['rmse']\n            score_F=[col1, col2]\n\n"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"print (\"Score CC: \", score_CC, minrmse_CC)\nprint (\"Score F: \", score_F, minrmse_F)"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_CC = RandomForestRegressor(n_estimators = 10, random_state = 0)\n\nmodel_F = RandomForestRegressor(n_estimators = 10, random_state = 0)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nfrom sklearn.model_selection import train_test_split\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.impute import SimpleImputer \n\n\nf= SimpleImputer(missing_values= np.nan, strategy='median')\n\n\n\ny_train_CC=train.loc[:,\"ConfirmedCases\"].fillna(train.loc[:,\"ConfirmedCases\"].median())\ny_train_F=train.loc[:, \"Fatalities\"].fillna(train.loc[:,\"Fatalities\"].median())\nX_train=train.drop([\"ConfirmedCases\", \"Fatalities\",  \"Date\", \"Id\"], axis=1)\n    \nf.fit(X_train)\nX_train=f.transform(X_train)\n    \n    \n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_CC[y_train_CC.isna()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nmodel_CC.fit(X_train, y_train_CC)\nmodel_F.fit(X_train, y_train_F)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Transform the Test Dataset before prediction**"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n## add days since first case column ##\nX_test=X_test.merge(mindatesDF, how='left', left_on=\"Province\", right_on=\"Province\")\nX_test[\"DaysFrom1stCase\"]=(X_test[\"Date\"]-X_test[\"MinDate\"]).dt.days\nX_test.loc [X_test[\"DaysFrom1stCase\"]<0 , \"DaysFrom1stCase\"] =0\nX_test=X_test.merge(restrictions, how='left', left_on=[\"Country\"], right_on=\"RestCountry\")\nfor col in restrictions.drop(\"RestCountry\", axis=1).columns:\n    X_test[col + \"days\"] = (X_test[col]-X_test[\"MinDate\"]).dt.days\n    X_test[col + \"days\"]=X_test[col + \"days\"].fillna(0)\n\nto_drop=restrictions.columns\nX_test=X_test.drop(to_drop,axis=1)    \n\nX_test=X_test.drop([\"MinDate\",\"ForecastId\", \"Date\"], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"province=labelencoder.transform(X_test[\"Province\"])\n\ncountry=countrylabelencoder.transform(X_test[\"Country\"])\n\nX_test=pd.concat([X_test,pd.DataFrame(province) ], axis=1)\nX_test=X_test.rename(columns={0:\"Province_Encoded\"})\nX_test=pd.concat([X_test, pd.DataFrame(country) ], axis=1)\nX_test=X_test.rename(columns={0:\"Country_Encoded\"})\nX_test=X_test.drop([\"Province\", \"Country\"], axis=1)\n\nfor col in X_test.columns:\n    X_test[col]=X_test[col].fillna(X_test[col].median())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#X_test= pca.transform(X_test)\n\n#y_pred=regressor.predict(X_test)\ny_pred_CC = model_CC.predict(X_test)\ny_pred_F = model_F.predict(X_test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#forecastId=test.ForecastId.to_numpy()\nsubmission_CC=pd.DataFrame(y_pred_CC)\nsubmission_CC=submission_CC.rename(columns={0:\"ConfirmedCases\"})\nsubmission_F=pd.DataFrame(y_pred_F)\nsubmission_F=submission_F.rename(columns={0:\"Fatalities\"})\n#submission[\"ForecastId\"]=test[\"ForecastId\"]\nsubmission=test.copy()\nsubmission=submission.drop(columns=[\"Province\", \"Country\", \"Date\"])\nsubmission[\"ConfirmedCases\"]=submission_CC[\"ConfirmedCases\"]\nsubmission[\"Fatalities\"]=submission_F[\"Fatalities\"]\nsubmission.head()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nsubmission=submission.dropna()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test=pd.concat([test, submission ], axis=1)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test[test[\"Country\"]==\"France\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test=adds_to_cumulative(test, \"ConfirmedCases\", \"ConfirmedAdds\")\ntest=test.drop(\"ConfirmedCases\", axis=1)\ntest=test.rename(columns={\"ConfirmedAdds\":\"ConfirmedCases\"})\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test[test[\"Country\"]==\"Algeria\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.head()\ntest=pd.concat([test,X_test], axis=1 )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nsns.set(style=\"darkgrid\")\n\n# Load an example dataset with long-form data\n\n\n# Plot the responses for different events and regions\nsns.lineplot(x=\"DaysFrom1stCase\", y=\"ConfirmedCases\",\n             hue=\"Country\", \n             data=test[test[\"Country\"] ==\"China\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":4}