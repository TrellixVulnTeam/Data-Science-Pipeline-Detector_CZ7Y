{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nfilelist = []\ni = 0\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        filelist.append(os.path.join(dirname, filename))\n        print('file',i,':',os.path.join(dirname, filename))\n        i+=1\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import datetime\n\nimport matplotlib.pyplot as plt\n\nimport statsmodels.api as sm\nfrom linearmodels.panel import PooledOLS\nfrom linearmodels.panel import RandomEffects\nfrom linearmodels import PanelOLS\nfrom linearmodels.panel import compare\n\nfrom statsmodels.tsa.stattools import adfuller, acf, pacf,arma_order_select_ic\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfrom statsmodels.tsa.arima_model import ARIMA\nimport itertools\nimport warnings\nwarnings.simplefilter('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Cleaned Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df =  pd.read_csv(filelist[2])\ndf.drop(['Unnamed: 0'], axis=1, inplace=True)\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = [x for x in list(df.columns) if x not in ['Id','ConfirmedCases', 'Fatalities','Province_State', 'Country_Region','Date']] \nX","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#manage date for panel regression\ndf.Date = pd.DatetimeIndex(df['Date'])\ndf.fillna({'Province_State':'All'},inplace=True) \ndf['Area']=df['Country_Region']+'_'+df['Province_State']\ndf.set_index(['Area','Date'],inplace= True)\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Pooled OLS","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"mod = PooledOLS(df['Fatalities'], sm.add_constant(df[X[:]]))\npooled_res = mod.fit(cov_type='robust')\npooled_res","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Panel Random Effect","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"mod = RandomEffects(df['Fatalities'], sm.add_constant(df[X]))\nre_res = mod.fit(cov_type='robust')\nre_res","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Panel Fixed effect","execution_count":null},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"mod = PanelOLS(df['Fatalities'], sm.add_constant(df[X[-2:]+X[:2]]), entity_effects=True,time_effects=True)\nfe_res = mod.fit(cov_type='robust')\nfe_res","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"compare({'FE':fe_res,'RE':re_res,'Pooled':pooled_res})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Times series\n\nDue to limited high-frequency data, panel regression cannot applied. The second best should be seperated time series model.","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"filelist","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test =  pd.read_csv(filelist[6])\nprint(test.describe())\ntest = test.sort_values(by=['Country_Region','Date'])\ntest.Date = pd.DatetimeIndex(test['Date'])\ntest.fillna({'Province_State':'All'},inplace=True) \ntest['Area']=test['Country_Region']+'_'+test['Province_State']\ntest","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df =  pd.read_csv(filelist[4])\nprint(df.describe())\ndf = df.sort_values(by=['Country_Region','Date'])\ndf.Date = pd.DatetimeIndex(df['Date'])\ndf.fillna({'Province_State':'All'},inplace=True) \ndf['Area']=df['Country_Region']+'_'+df['Province_State']\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(set(df.Area))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Looping for ARIMA","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Arima modeling for ts\n\ndef arima(ts):\n    p=d=q=range(0,6) #set maximum of(p,d,q) to be (6,6,6)\n    a=99999\n    pdq=list(itertools.product(p,d,q))\n    global param\n    #Determining the best parameters\n    for var in pdq:\n        try:\n            model = ARIMA(ts, order=var)\n            result = model.fit()\n\n            if (result.aic<=a) :\n                a=result.aic\n                param=var\n        except:\n            continue\n    return param","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"markdown","source":"optimal=pd.DataFrame()\nk = 10 #overlapping period\n\nfor i in df.Area.unique()[:]:\n    print(f'Predicted Area: {i} \\n')\n    tstest=test.loc[(test['Area']==f'{i}')]  \n    tstest=tstest[['Date']]\n\n    ts=df.loc[(df['Area']==f'{i}')]  \n    ts=ts[['Date','ConfirmedCases','Fatalities']]\n    \n    startdate = tstest.reset_index()['Date'].min()\n    enddate = tstest.reset_index()['Date'].max()\n    print('Starttest:',startdate)\n    print('Endtest:',enddate)\n    \n    endtrain = startdate + datetime.timedelta(days=k)\n    ts = ts.loc[ts['Date']<endtrain]\n    \n    starttrain = ts.reset_index()['Date'].min()\n    endtrain = ts.reset_index()['Date'].max()\n\n    print('\\nStartTrain:',starttrain)\n    print('EndTrain:',endtrain)\n    \n    #set date index to train data\n    ts.index= ts['Date']\n    #print(ts)\n    \n    #Create empty dataframe\n    product=pd.DataFrame()\n    \n    #Array for each country and each type\n    for case in ['ConfirmedCases','Fatalities']:\n        tsC = ts[case].values\n        global par\n        par = arima(tsC)\n        print(par)\n        product[f'{case}'] = ['1']\n        product.loc[:,f'{case}'] = pd.Series([tuple(par)])\n\n    product['Area'] = f'{i}'    \n    optimal = pd.concat([product,optimal])","execution_count":null},{"metadata":{"trusted":true},"cell_type":"markdown","source":"#optimal.to_csv('File\\\\optimal_ARIMA.csv')\noptimal","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"optimal=pd.read_csv(filelist[1])\noptimal","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"results=pd.DataFrame()\nk = 10 #overlapping period\n\nfor i in df.Area.unique():\n    print(f'Predicted Area: {i} \\n')\n    tstest=test.loc[(test['Area']==f'{i}')]  \n    tstest=tstest[['Date']]\n\n    ts=df.loc[(df['Area']==f'{i}')]  \n    ts=ts[['Date','ConfirmedCases','Fatalities']]\n    \n    startdate = tstest.reset_index()['Date'].min()\n    enddate = tstest.reset_index()['Date'].max()\n    print('Starttest:',startdate)\n    print('Endtest:',enddate)\n    \n    endtrain = startdate + datetime.timedelta(days=k)\n    ts = ts.loc[ts['Date']<endtrain]\n    \n    starttrain = ts.reset_index()['Date'].min()\n    endtrain = ts.reset_index()['Date'].max()\n\n    print('\\nStartTrain:',starttrain)\n    print('EndTrain:',endtrain)\n    \n    #set date index to train data\n    ts.index= ts['Date']\n    print(ts)\n    \n    #Create empty dataframe\n    product=pd.DataFrame()\n    #product['Date']=pd.date_range(startdate, enddate)\n    \n    #Array for each country and each type\n    for case in ['ConfirmedCases','Fatalities']:\n        tsC = ts[case].values\n        order_str = optimal.loc[optimal['Area']==f'{i}'][case].iloc[0]\n        res = tuple(map(int, order_str.replace('(','').replace(')','').split(', ')))\n        model = ARIMA(ts[case].dropna(), order=res)\n        if ~(ts[case]==0).all():\n            result = model.fit()\n            pred= result.predict(start=startdate, end=enddate,typ='levels')\n            fig, ax = plt.subplots()\n            ax = ts[case].loc[starttrain:].plot(ax=ax)\n            fig = result.plot_predict(start=startdate, end=enddate, dynamic=False, ax=ax, plot_insample=False) \n            plt.legend()\n            plt.title(f'True vs predicted values: {case}')\n            #if i =='Taiwan*_All':\n                #plt.savefig(f'Figure\\\\{case}\\\\Taiwan_All.png',bbox_inches='tight')\n            #else:\n                #plt.savefig(f'Figure\\\\{case}\\\\{i}.png',bbox_inches='tight')\n            plt.show()\n            product[f'{case}'] = pred\n            \n    product['Area'] = f'{i}'    \n    results = pd.concat([product,results])","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#get result after looping\n#results = results.reset_index().rename(index=str, columns={'index':'Date'})\n#results.set_index(['Area','Date'], inplace=True)\n#results.to_csv('File\\\\predicted_ARIMA.csv')\n#results = results.reset_index()\nresults = pd.read_csv(filelist[3])\nresults.drop(['Unnamed: 0'], axis=1, inplace=True)\nresults.Date = pd.DatetimeIndex(results['Date'])\nresults","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nonarima.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## After result from ARIMA, Using ffill for region with zero case in the model","execution_count":null},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"arima_re =  results.loc[~pd.isnull(results['Fatalities'])]\nnonarima = results.loc[pd.isnull(results['Fatalities'])]\nnonarima = pd.merge(nonarima,df, on=['Area','Date'], how='left')\nnonarima['Fatalities'] = nonarima['Fatalities_y'].ffill()\nnonarima['ConfirmedCases'] = nonarima['ConfirmedCases_x']\nnonarima.drop(['ConfirmedCases_x', 'ConfirmedCases_y','Fatalities_x','Fatalities_y','Id','Country_Region','Province_State'], axis=1, inplace=True)\nnonarima","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"forecast = pd.merge(test,arima_re, on=['Area','Date'], how='left')\nforecast = pd.merge(forecast,nonarima, on=['Area','Date'], how='left')\nforecast['Fatalities'] = forecast['Fatalities_y'].fillna(forecast['Fatalities_x'])\nforecast['ConfirmedCases'] = forecast['ConfirmedCases_y'].fillna(forecast['ConfirmedCases_x'])\nforecast.drop(['Province_State','Country_Region','Date','Area','ConfirmedCases_x', 'ConfirmedCases_y','Fatalities_x','Fatalities_y'], axis=1, inplace=True)\nforecast.to_csv('submission.csv',index=False)\nforecast","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}