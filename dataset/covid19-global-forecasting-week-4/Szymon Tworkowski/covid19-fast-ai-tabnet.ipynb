{"cells":[{"metadata":{},"cell_type":"markdown","source":"# [#masks4all](https://masks4all.co/why-we-need-mandatory-mask-laws-masks4all/)\n\n# Introduction\n\nThe goal of this notebook is to provide some basic [fast.ai](https://www.fast.ai/), [TabNet (paper here)](https://arxiv.org/abs/1908.07442) model for COVID-19 dataset.\n\nAlthough it is not the best approach here (doesn't have an RNN part, for instance), it requires reasonably small amount of code and obviously no feature engineering.\n\nThe solution utilizes mostly [fast.ai](https://www.fast.ai/) library with [fast_tabnet](https://github.com/mgrankin/fast_tabnet) and stuff included in [this course](https://course.fast.ai/)"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install fastai2\n!pip install fast_tabnet","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"from fastai2.basics import *\nfrom fastai2.tabular.all import *\nfrom fast_tabnet.core import *\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load input data"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"PATH = '/kaggle/input/covid19-global-forecasting-week-4/'\ntrain_df = pd.read_csv(PATH + 'train.csv', parse_dates=['Date'])\ntest_df = pd.read_csv(PATH + 'test.csv', parse_dates=['Date'])\n\nadd_datepart(train_df, 'Date', drop=False)\nadd_datepart(test_df, 'Date', drop=False)\ntrain_df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Metadata (continent, population, tests per million etc.)"},{"metadata":{"trusted":true},"cell_type":"code","source":"PATH1 = '/kaggle/input/covid19-country-data-wk3-release/'\n\nmeta_convert_fun = lambda x: np.float32(x) if x not in ['N.A.', '#N/A', '#NULL!'] else np.nan\n\nmeta_df = pd.read_csv(PATH1 + 'Data Join - RELEASE.csv', thousands=\",\",\n                     converters={\n                         ' TFR ': meta_convert_fun,\n                         'Personality_uai': meta_convert_fun,\n                     }).rename(columns=lambda x: x.strip())\n# meta_df.rename(columns{' TFR '}: 'TFR')\n\nPATH2 = '/kaggle/input/countryinfo/'\ncountryinfo = pd.read_csv(PATH2 + 'covid19countryinfo.csv', thousands=\",\", parse_dates=['quarantine', 'schools', 'publicplace', 'gathering', 'nonessential'])\ntestinfo = pd.read_csv(PATH2 + 'covid19tests.csv', thousands=\",\")\n\ncountryinfo.rename(columns={'region': 'Province_State', 'country': 'Country_Region'}, inplace=True)\ntestinfo.rename(columns={'region': 'Province_State', 'country': 'Country_Region'}, inplace=True)\ntestinfo = testinfo.drop(['alpha3code', 'alpha2code', 'date'], axis=1)\n\nPATH3 = '/kaggle/input/covid19-forecasting-metadata/'\ncontinent_meta = pd.read_csv(PATH3 + 'region_metadata.csv').rename(columns={'density': 'pop_density'})\ncontinent_meta = continent_meta[['Country_Region' ,'Province_State', 'continent', 'lat', 'lon', 'pop_density']]\n\nrecoveries_meta = pd.read_csv(PATH3 + 'region_date_metadata.csv', parse_dates=['Date'])\n\ndef fill_unknown_state(df):\n    df.fillna({'Province_State': 'Unknown'}, inplace=True)\n    \nfor d in [train_df, test_df, meta_df, countryinfo, testinfo, continent_meta, recoveries_meta]:\n    fill_unknown_state(d)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Remove outliers - we don't trust China"},{"metadata":{"trusted":true},"cell_type":"code","source":"outliars = ['China']\nout_inputs = {}\n\nfor out in outliars:\n    out_inputs[out] = train_df[train_df['Country_Region'] == out]\n    train_df.drop(train_df.index[train_df['Country_Region'] == out], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Save for later before submission.\ntest_ori = test_df.copy()\n\n\nmerge_cols = ['Province_State', 'Country_Region', 'Date']\ntest_hlp = test_df[merge_cols + ['ForecastId']]\nfst_date = test_hlp.Date.min()\noutlier_dfs = []\n\nfor out, in_df in out_inputs.items():\n    last_date = in_df.Date.max()\n    merged = in_df[in_df['Date'] >= fst_date].merge(test_hlp, on=merge_cols, how='left')[['ForecastId', 'ConfirmedCases', 'Fatalities']]\n    future_test = test_hlp[(test_hlp['Country_Region'] == out) & (test_hlp['Date'] > last_date)]\n    to_add = in_df.groupby(['Province_State', 'Country_Region']).last().reset_index()[['Province_State', 'Country_Region', 'ConfirmedCases', 'Fatalities']]\n    merged_future = future_test.merge(to_add, on=['Province_State', 'Country_Region'], how='left')[['ForecastId', 'ConfirmedCases', 'Fatalities']]\n    merged = pd.concat([merged, merged_future], sort=True)\n    test_df.drop(test_df[test_df['ForecastId'].isin(merged.ForecastId)].index, inplace=True)\n    outlier_dfs.append(merged)\n    \noutlier_df = pd.concat(outlier_dfs, sort=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"outlier_df.index = outlier_df['ForecastId']\noutlier_df.drop('ForecastId', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_max_date = train_df.Date.max()\noutlier_all = test_df[test_df['Date'] <= train_max_date].merge(train_df, on=merge_cols, how='left')[['ForecastId', 'ConfirmedCases', 'Fatalities']]\noutlier_all.index = outlier_all.ForecastId\ntest_df.drop(test_df[test_df['ForecastId'].isin(outlier_all.ForecastId)].index, inplace=True)\noutlier_all.drop('ForecastId', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"outlier_df = pd.concat([outlier_df, outlier_all], sort=True)\noutlier_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Add external features to input dataframes\nFor each country, we want to extract the first day when it reached at least 1 and 50 cases.\n\nAverage fatality rate will be calculated from last data available, simply taking deaths / cases."},{"metadata":{"trusted":true},"cell_type":"code","source":"idx_group = ['Country_Region', 'Province_State']\n\ndef day_reached_cases(df, name, no_cases=1):\n    \"\"\"For each country/province get first day of year with at least given number of cases.\"\"\"\n    gb = df[df['ConfirmedCases'] >= no_cases].groupby(idx_group)\n    return gb.Dayofyear.first().reset_index().rename(columns={'Dayofyear': name})\n\ndef area_fatality_rate(df):\n    \"\"\"Get average fatality rate for last known entry, for each country/province.\"\"\"\n    gb = df[df['Fatalities'] >= 22].groupby(idx_group)\n    res_df = (gb.Fatalities.last() / gb.ConfirmedCases.last()).reset_index()\n    return res_df.rename(columns={0 : 'FatalityRate'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def joined_data(df):\n    res = df.copy()\n    \n    fatality = area_fatality_rate(train_df)\n    first_nonzero = day_reached_cases(train_df, 'FirstCaseDay', 1)\n    first_fifty = day_reached_cases(train_df, 'First50CasesDay', 50)\n    \n    # Add external features\n    res = pd.merge(res, continent_meta, how='left')\n    res = pd.merge(res, recoveries_meta, how='left')\n    res = pd.merge(res, meta_df, how='left')\n    res = pd.merge(res, countryinfo, how='left')\n    res = pd.merge(res, testinfo, how='left', left_on=idx_group, right_on=idx_group)\n    \n    # Add calculated features\n    res = pd.merge(res, fatality, how='left')\n    res = pd.merge(res, first_nonzero, how='left')\n    res = pd.merge(res, first_fifty, how='left')\n    return res\n\ntrain_df = joined_data(train_df)\ntest_df = joined_data(test_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# It turns out any country in train dataset has at least one case.\ntrain_df.FirstCaseDay.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Add temporal features\n\nSome basic features like number of days since the first case in each country/province with analogous feature for 50 days may be worth adding,\nincluding containment measures such as closed schools and quarantine."},{"metadata":{"trusted":true},"cell_type":"code","source":"def with_new_features(df):\n    res = df.copy()\n    add_datepart(res, 'quarantine', prefix='qua')\n    add_datepart(res, 'schools', prefix='sql')\n    \n    res['DaysSinceFirst'] = res['Dayofyear'] - res['FirstCaseDay']\n    res['DaysSince50'] = res['Dayofyear'] - res['First50CasesDay']\n    res['DaysQua'] = res['Dayofyear'] - res['quaDayofyear']\n    res['DaysSql'] = res['Dayofyear'] - res['sqlDayofyear']\n\n    return res\n    \ntrain_df = with_new_features(train_df)\ntest_df = with_new_features(test_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"PATH4 = '/kaggle/input/covid19forecastforvalidset/'\nvalid_preds = pd.read_csv(PATH4 + 'forecast.csv')\n\ntest_with_preds = test_df.merge(valid_preds, how='left')\ntest_with_preds = test_with_preds[(test_with_preds['Date'] > train_df.Date.max()) & (test_with_preds['Date'] <= '2020-04-28')]\ntest_with_preds.index += train_df.shape[0]\n\ntrain_new = pd.concat([train_df, test_with_preds], sort=True).sort_values(by='Date')\ntrain_new","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def with_new_objectives(df):\n#     res = df.sort_values(['Country_Region', 'Province_State', 'Date'])\n#     res['dCases'] = np.clip(res.ConfirmedCases.diff(), a_min=0, a_max=None)\n#     res['dFatalities'] = np.clip(res.Fatalities.diff(), a_min=0, a_max=None)\n#     return res\n\n# train_new = with_new_objectives(train_new)\ntrain_new = train_new[train_new['Date'] >= '2020-02-20']\ntrain_len = train_new[train_new['Date'] <= train_df['Date'].max()].shape[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature selection\nIn fast.ai we can easily select categorical and continuous variables for training.\n\nI decided not to choose any external data in baseline model. Adding numerical values from country data provided in this notebook doesn't seem to improve the validation score much."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Categorical variables - only basic identifiers, some features like continent will be worth adding.\ncat_vars = ['Country_Region', 'Province_State',\n            'continent',\n#             'publicplace', 'gathering', 'nonessential'\n           ]\n\n# Continuous variables - just ones directly connected with time.\ncont_vars = ['DaysSinceFirst', 'DaysSince50', 'Dayofyear',\n            'DaysQua', 'DaysSql', 'Recoveries',\n            'recovered', 'active3', 'newcases3', 'critical3',\n            'TRUE POPULATION', 'TFR', 'Personality_uai', 'murder',\n            'testper1m', 'positiveper1m',\n            'casediv1m', 'deathdiv1m', \n            'FatalityRate',\n            'lat', 'lon', 'pop_density', 'urbanpop', 'medianage', 'hospibed','healthperpop',\n            'smokers', 'lung', \n#             'continent_gdp_pc', 'continent_happiness', 'continent_Life_expectancy','GDP_region', \n#             'abs_latitude', 'temperature', 'humidity',\n            ]\n\n# dep_var = ['dCases', 'dFatalities']\ndep_var = ['ConfirmedCases', 'Fatalities']\n\n# TODO: change to train_df when avoiding leakage\n# df = train_df[cont_vars + cat_vars + dep_var +['Date']].copy().sort_values('Date')\ndf = train_new[cont_vars + cat_vars + dep_var +['Date']].copy().sort_values('Date')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Avoid leakage - take only non-overlapping values for training\n\nFor now, the only available data to validate our model is in training set. \n\nAs our test set starts on **02.04.2020**, we should take only rows before that date for training to avoid leakage."},{"metadata":{"trusted":true},"cell_type":"code","source":"# print(test_df.Date.min())\n# MAX_TRAIN_IDX = df[df['Date'] < test_df.Date.min()].shape[0]\nMAX_TRAIN_IDX = train_len","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preparing training set\n\nIn fast.ai v2, we have to manually take log of dependent variables. Preprocessing setup is vanilla fast.ai stuff,\nwe utilize our training index to make a unleaky train/valid split. \n\nAn important thing is also picking the right batch size in our DataLoader, I've chosen value 512 which is pretty big but doesn't seem to affect the training negatively."},{"metadata":{"trusted":true},"cell_type":"code","source":"df1 = df.copy()\n# df1['dCases'] = np.log1p(df1['dCases'])\n# df1['dFatalities'] = np.log1p(df1['dFatalities'])\ndf1['ConfirmedCases'] = np.log1p(df1['ConfirmedCases'])\ndf1['Fatalities'] = np.log1p(df1['Fatalities'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = '/kaggle/working/'\n\nprocs=[FillMissing, Categorify, Normalize]\n\nsplits = list(range(MAX_TRAIN_IDX)), (list(range(MAX_TRAIN_IDX, len(df))))\n\n%time to = TabularPandas(df1, procs, cat_vars.copy(), cont_vars.copy(), dep_var, y_block=TransformBlock(), splits=splits)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dls = to.dataloaders(bs=512, path=path)\ndls.show_batch()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Processed test set"},{"metadata":{},"cell_type":"markdown","source":"# Model\n\nThe model provided here is a simple baseline from fast_tabnet documentation, without any fine tuning.\n\nIt is capable of predicting both confirmed cases and fatalities at once, which is worth noting."},{"metadata":{"trusted":true},"cell_type":"code","source":"emb_szs = get_emb_sz(to); print(emb_szs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dls.c = 2 # Number of outputs we expect from our network - in this case 2.\nmodel = TabNetModel(emb_szs, len(to.cont_names), dls.c, n_d=64, n_a=32, n_steps=3)\n# opt_func = partial(Adam, wd=0.01, eps=1e-5)\nlearn = Learner(dls, model, MSELossFlat(), opt_func=ranger, lr=3e-2, metrics=[rmse])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.lr_find()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.fit_one_cycle(10, lr_max=0.33)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.fit_one_cycle(50, lr_max=0.091)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.recorder.plot_sched()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.fit_one_cycle(80, lr_max=5e-2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# cb = SaveModelCallback()\nlearn.fit_one_cycle(300, lr_max=1e-2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.show_results()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preparing submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"# learn.load('model')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"to_tst = to.new(test_df)\nto_tst.process()\nto_tst.all_cols.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tst_dl = dls.valid.new(to_tst)\ntst_dl.show_batch()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.metrics = []\ntst_preds,_ = learn.get_preds(dl=tst_dl)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"res1 = np.expm1(tst_preds)\nres2 = list(map(lambda x: x[0], res1.numpy()))\nres3 = list(map(lambda x: x[1], res1.numpy()))\nsubmit = pd.DataFrame({'ConfirmedCases': res2, 'Fatalities': res3})\nsubmit.index = test_df.ForecastId","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test = submit.merge(test_df[['ForecastId', 'Date', 'Country_Region', 'Province_State']], on='ForecastId', how='left')\n# test.index = test_df.ForecastId\n# begin_date = test.Date.min()\n# hlp_train = train_df[train_df['Date'] == begin_date - np.timedelta64(1, 'D')]\n\n# def get_cases(row):\n#     if row['Date'] == begin_date:\n#         starting = hlp_train[(hlp_train['Country_Region'] == row['Country_Region']) & (hlp_train['Province_State'] == row['Province_State'])].iloc[0]\n#         return starting.ConfirmedCases, starting.Fatalities\n#     return 0., 0.\n\n# cases = test.apply(get_cases, axis=1)\n# test.ConfirmedCases += list(map(lambda x: x[0], cases))\n# test.Fatalities += list(map(lambda x: x[1], cases))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# submit.ConfirmedCases = test.groupby(['Country_Region', 'Province_State']).ConfirmedCases.cumsum()\n# submit.Fatalities = test.groupby(['Country_Region', 'Province_State']).Fatalities.cumsum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit = pd.concat([submit, outlier_df], sort=True).sort_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = submit.merge(test_ori[['ForecastId', 'Date', 'Country_Region', 'Province_State']], on='ForecastId', how='left')\ntest.index = test_ori.ForecastId\nsubmit.ConfirmedCases = test.groupby(['Country_Region', 'Province_State']).ConfirmedCases.cummax()\nsubmit.Fatalities = test.groupby(['Country_Region', 'Province_State']).Fatalities.cummax()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit.to_csv('submission.csv')\nsubmit","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Example predictions on our validation set.\n\nDisplay some interesting countries to see if our model performs any good."},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\n\nmin_date = test_df.Date.min()\nmax_date = train_df.Date.max()\n\nf, axes = plt.subplots(10, 1, figsize=(16, 60))\n\n# def plot_preds(country, ax):\n#     targets = train_df[(train_df['Country_Region'] == country) & (train_df['Date'] >= min_date)].ConfirmedCases\n#     subset = test_hlp[(test_hlp['Country_Region'] == country) & (test_hlp['Date'] <= max_date)]\n    \n#     idx = subset.index\n#     dates = subset.Date\n#     predicted = submit.iloc[idx].ConfirmedCases\n    \n#     targets.index = dates\n#     predicted.index = dates\n    \n#     combined = pd.DataFrame({'real' : targets, 'pred': predicted})\n    \n#     sns.lineplot(data=combined, ax=axes[ax]).set_title(country)\n\ndef plot_preds(country, ax):\n    subset = test_hlp[(test_hlp['Country_Region'] == country)]\n    \n    idx = subset.index\n    dates = subset.Date\n    predicted = submit.iloc[idx].ConfirmedCases\n    predicted.index = dates\n    \n    combined = pd.DataFrame({'pred': predicted})\n    \n    sns.lineplot(data=combined, ax=axes[ax]).set_title(country)\n\nplot_preds('Italy', 0)\nplot_preds('Spain', 1)\nplot_preds('Germany', 2)\nplot_preds('Poland', 3)\nplot_preds('Czechia', 4)\nplot_preds('Russia', 5)\nplot_preds('Iran', 6)\nplot_preds('Sweden', 7)\nplot_preds('Japan', 8)\nplot_preds('Belgium', 9)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit.iloc[test_hlp[test_hlp['Country_Region'] == 'Poland'].index]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion\n\nAs neural networks are thought not to perform well on tabular data, which is true especially for some trivial architectures like MLP used in fast.ai tabular v1.\n\nThere is quite a lot to explore in this TabNet, in terms of hyperparameters and feature selection, so I would expect a much better performance with tuning.\n\n# [#masks4all](https://masks4all.co/why-we-need-mandatory-mask-laws-masks4all/)"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":4}