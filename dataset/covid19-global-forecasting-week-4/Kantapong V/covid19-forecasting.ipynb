{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt #for plotting graph\nfrom statsmodels.tsa.ar_model import AutoReg\nfrom statsmodels.tsa.api import VAR\nfrom matplotlib import pyplot\nfrom statsmodels.graphics.tsaplots import plot_acf\nfrom statsmodels.graphics.tsaplots import plot_pacf\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df_train=pd.read_csv(\"../input/covid19-global-forecasting-week-4/train.csv\")\ndf_test=pd.read_csv(\"../input/covid19-global-forecasting-week-4/test.csv\")\ndf_sub=pd.read_csv(\"../input/covid19-global-forecasting-week-4/submission.csv\")\n\nprint(df_train.shape) # dimension of df_train\nprint(df_test.shape)\nprint(df_sub.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.tail(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sub.tail(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### STEP 1: Clean the data\n#### One problem is that there is NaN in Province_State, so we need a combination of both Province_State and Country_Region as one attribute.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(df_train.Country_Region.unique()), len(df_train.Province_State.unique()))\nprint(len(df_test.Country_Region.unique()), len(df_test.Province_State.unique()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.Country_Region.unique()  # It lists out the countries","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.Province_State.unique()  # It lists out provinces/states as the smaller category of country","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train[\"Unique_Region\"] = df_train.Country_Region\ndf_train.Unique_Region[df_train.Province_State.isna() == False] = df_train.Province_State+\" , \"+df_train.Country_Region\ndf_train.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.drop(labels=[\"Id\",\"Province_State\",\"Country_Region\"], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test[\"Unique_Region\"] = df_test.Country_Region\ndf_test.Unique_Region[df_test.Province_State.isna() == False] = df_test.Province_State+\" , \"+df_test.Country_Region\ndf_test.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.drop(labels=[\"Province_State\",\"Country_Region\"], axis=1, inplace=True)\ndf_test.sample(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### STEP 2: Glancing at the data (Time-series and Cross-sectional dimension as the panel)\n#### Next, by looking at time-series dimension, visualize the world trend.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(list(df_train.Date.unique()))\nprint(len(df_train.Date.unique()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(list(df_test.Date.unique()))\nprint(len(df_test.Date.unique()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"world_list_confirmed = []\nworld_list_fatality = []\nfor date in list(df_train.Date.unique()):\n    confirmed = df_train.ConfirmedCases[df_train.Date == date]\n    world_list_confirmed.append(sum(confirmed))\n    fatality = df_train.Fatalities[df_train.Date == date]\n    world_list_fatality.append(sum(fatality))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(world_list_confirmed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(world_list_fatality)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_acf(pd.Series(world_list_fatality))\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(pd.Series.diff(pd.Series(world_list_confirmed)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(pd.Series.diff(pd.Series(world_list_fatality)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_acf(pd.Series.diff(pd.Series(world_list_fatality))[60:])\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_pacf(pd.Series.diff(pd.Series(world_list_fatality))[60:])\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### From the first two plots, confirmed cases and fatality show a rapidly increasing trend clearly. Also, they are not weakly stationary from ACF plot.\n#### However, for differenced plots, the difference shows a rapidly increasing trend early on, but the trend disappeared at the very end. Hence, we will use data from 60th day onward, since the differenced data are stationary since then.\n#### ACF and PACF plot suggest using AR(1). And, seeing from both differenced confirmed cases and fatality plots, they are strongly related to each other, and this may suggest VAR(1).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"date_list = list(df_train.Date.unique())\nprint(date_list[61])\nprint(date_list[71])\ndate_test = list(df_test.Date.unique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### We use the data only from 23 March to 14 April. And we will forecast from 15 April to May 14","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### After looking at time-series dimension, let's look at cross-sectional dimension. We look at min, first quartile, median, third-quartile and maximum.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"q00_confirmed = []\nq25_confirmed = []\nq50_confirmed = []\nq75_confirmed = []\nq10_confirmed = []\nfor date in date_list:\n    list_confirm = pd.Series(df_train.ConfirmedCases[df_train.Date == date])\n    q00_confirmed.append(list_confirm.quantile(q = 0))\n    q25_confirmed.append(list_confirm.quantile(q = 0.25))\n    q50_confirmed.append(list_confirm.quantile(q = 0.5))\n    q75_confirmed.append(list_confirm.quantile(q = 0.75))\n    q10_confirmed.append(list_confirm.quantile(q = 1))\n\nprint(q00_confirmed[61:83])\nprint(q25_confirmed[61:83])\nprint(q50_confirmed[61:83])\nprint(q75_confirmed[61:83])\nprint(q10_confirmed[61:83])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### The 25% percentile is around 60, and the median is around 500.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"q00_fatality = []\nq25_fatality = []\nq50_fatality = []\nq75_fatality = []\nq10_fatality = []\nfor date in date_list:\n    list_confirm = pd.Series(df_train.Fatalities[df_train.Date == date])\n    q00_fatality.append(list_confirm.quantile(q = 0))\n    q25_fatality.append(list_confirm.quantile(q = 0.25))\n    q50_fatality.append(list_confirm.quantile(q = 0.5))\n    q75_fatality.append(list_confirm.quantile(q = 0.75))\n    q10_fatality.append(list_confirm.quantile(q = 1))\n\nprint(q00_fatality[61:83])\nprint(q25_fatality[61:83])\nprint(q50_fatality[61:83])\nprint(q75_fatality[61:83])\nprint(q10_fatality[61:83])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### The median is around 10, and 75% percentile is around 70.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Plugging in fixed value should be used for a very few number of confirmed cases, or fatalities, since the number would not be likely to change after 14 April.\n#### Using two separate AR(1) processes should be used for the region where the number of confirmed cases, or fatalities were not high, since there would not be strong relationship between the two series.\n#### Using VAR(1) process should be used for the region where the number of confirmed cases and fatalities were high since there would be strong relationship between the two series.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### After glancing at the summary statistics, we used (60,10) as the first threshold of confirmed cases and fatalities, switching from plugging in fixed values to using AR(1) process. \n#### Then, the next threshold of confirmed cases and fatalities is (500, 70), switching from AR(1) to VAR(1) process","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### STEP 3 (final): Learning Algorithm\n#### 1. For confirmed cases <= 60 and falities <= 10 on 14 April, keep predicting those values.\n#### 2. For confirmed case > 60, but fatalities <= 10 on 14 April, use simple AR(1) process of differenced series to predict confirmed cases, and plug in the most recent value on fatalities.\n#### 3. For confirmed case <= 60, but fatalities > 10 on 14 April, use simple AR(1) process of differenced series to predict fatality, and plug in the most recent value on confirmed cases.\n#### 4. If both series are greater than (60, 10), but either confirmed case <= 500 or fatalities <= 70, use two simple AR(1) processes on differenced series to predict each variable.\n#### 4. Otherwise, use VAR(1) process using differenced series to predict both confirmed cases and fatalities simultaneously. Motivation: If the number of confirmed cases and fatalities are high enough, there is a strong relationship between these two variables.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Now, we are ready to build the model and submission","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Date_submission = date_list[71:84]\nDate_prediction = date_test[13:]\nprint(Date_submission)\nprint(Date_prediction)\nprint(len(Date_prediction))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" def submit_from_pred (date_list, value_list, Unique_Region, test_df, submission_df, num): #If num = 0, confirmed cases. Else, fatality.\n    if len(date_list) != len(value_list):\n        print(\"Error, the length of these two lists are not equal\")\n    else:\n        for i in range(len(date_list)):\n            pred = value_list[i]\n            selected_df = test_df[(test_df[\"Date\"] == date_list[i]) & (test_df[\"Unique_Region\"] == Unique_Region)]\n            forecastID = selected_df[\"ForecastId\"].iloc[0]\n            if num == 0:\n                submission_df.ConfirmedCases[submission_df[\"ForecastId\"] == forecastID] = pred\n            else:\n                submission_df.Fatalities[submission_df[\"ForecastId\"] == forecastID] = pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def from_diff_to_var (diff_series, start_value):\n    value = start_value\n    for i in range(len(diff_series)):\n        value += diff_series[i]\n        diff_series[i] = value\n    return diff_series","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"regions = list(df_train.Unique_Region.unique())\n\nfor region in regions:\n    selected_df = df_train[df_train.Unique_Region == region]\n    data_df = selected_df.iloc[61:84]\n\n    confirmed_train = data_df[\"ConfirmedCases\"]\n    fatality_train = data_df[\"Fatalities\"]\n    \n    confirmed_diff = confirmed_train.diff()[1:] # Differencing causes one lost observation\n    fatality_diff = fatality_train.diff()[1:]\n    \n    latest_confirmed = data_df.iloc[-1][\"ConfirmedCases\"]\n    latest_fatality = data_df.iloc[-1][\"Fatalities\"]\n    \n    if latest_confirmed <= 60 and latest_fatality <= 10: # Plug in fixed values\n        confirmed = [latest_confirmed]*30\n        fatality = [latest_fatality]*30\n        # Plug in the latest value directly\n        submit_from_pred(Date_prediction, confirmed, region, df_test, df_sub, 0) \n        submit_from_pred(Date_prediction, fatality, region, df_test, df_sub, 1)\n    \n    else:\n        if latest_confirmed <= 60 or latest_fatality <= 10: # Use only one AR(1) process\n            if latest_confirmed <= 60:\n                # Plug in latest value on confirmed\n                confirmed = [latest_confirmed]*30\n                submit_from_pred(Date_prediction, confirmed, region, df_test, df_sub, 0)\n                # Conduct AR(1) process on fatality\n                model_fatality = AutoReg(fatality_diff, lags=1)\n                model_fatality = model_fatality.fit()\n                prediction_fatality = list(model_fatality.predict(start=len(fatality_diff), end=len(fatality_diff)+len(Date_prediction)-1))\n                prediction_fatality = from_diff_to_var(prediction_fatality, latest_fatality)\n                prediction_fatality = [round(prediction) for prediction in prediction_fatality]\n                submit_from_pred(Date_prediction, list(prediction_fatality), region, df_test, df_sub, 1)\n                \n                \n            else:\n                # Plug in latest value on fatality\n                fatality = [latest_fatality]*30\n                submit_from_pred(Date_prediction, fatality, region, df_test, df_sub, 1)\n                # Conduct AR(1) process on confirmed\n                model_confirmed = AutoReg(confirmed_diff, lags=1)\n                model_confirmed = model_confirmed.fit()\n                prediction_confirmed = list(model_confirmed.predict(start=len(confirmed_diff), end=len(confirmed_diff)+len(Date_prediction)-1))\n                prediction_confirmed = from_diff_to_var(prediction_confirmed, latest_confirmed)\n                prediction_confirmed = [round(prediction) for prediction in prediction_confirmed]\n                submit_from_pred(Date_prediction, list(prediction_confirmed), region, df_test, df_sub, 0)\n                \n        else:\n            if latest_confirmed <= 500 or latest_fatality <= 70: # Use two separated AR(1) processes\n                # Conduct AR(1) process on confirmed\n                model_confirmed = AutoReg(confirmed_diff, lags=1)\n                model_confirmed = model_confirmed.fit()\n                prediction_confirmed = list(model_confirmed.predict(start=len(confirmed_diff), end=len(confirmed_diff)+len(Date_prediction)-1))\n                prediction_confirmed = from_diff_to_var(prediction_confirmed, latest_confirmed)\n                prediction_confirmed = [round(prediction) for prediction in prediction_confirmed]\n                submit_from_pred(Date_prediction, list(prediction_confirmed), region, df_test, df_sub, 0)\n                # Conduct AR(1) process on fatality\n                model_fatality = AutoReg(fatality_diff, lags=1)\n                model_fatality = model_fatality.fit()\n                prediction_fatality = list(model_fatality.predict(start=len(fatality_diff), end=len(fatality_diff)+len(Date_prediction)-1))\n                prediction_fatality = from_diff_to_var(prediction_fatality, latest_fatality)\n                prediction_fatality = [round(prediction) for prediction in prediction_fatality]\n                submit_from_pred(Date_prediction, list(prediction_fatality), region, df_test, df_sub, 1)\n                \n                \n            else: # If these two variables are so high, use VAR(1) process\n                # Conduct VAR(1) on these two variables simultaneously\n                data_VAR = pd.DataFrame({'confirmed_diff':confirmed_diff, 'fatality_diff':fatality_diff})\n                model_VAR = VAR(data_VAR)\n                model_VAR = model_VAR.fit(1)\n                # Forecast using VAR(1)\n                forecast_input = data_VAR.values[-1:]\n                forecast = model_VAR.forecast(y=forecast_input, steps=len(Date_prediction))\n                confirmed_forecast = []\n                fatality_forecast = []\n                # Copy those values in submission.csv\n                for k in range(len(forecast)):\n                    confirmed_forecast.append(forecast[k][0])\n                    fatality_forecast.append(forecast[k][1])\n                prediction_confirmed = from_diff_to_var(confirmed_forecast, latest_confirmed)\n                prediction_confirmed = [round(prediction) for prediction in prediction_confirmed]\n                submit_from_pred(Date_prediction, list(prediction_confirmed), region, df_test, df_sub, 0)\n                prediction_fatality = from_diff_to_var(fatality_forecast, latest_fatality)\n                prediction_fatality = [round(prediction) for prediction in prediction_fatality]\n                submit_from_pred(Date_prediction, list(prediction_fatality), region, df_test, df_sub, 1)\n                                                             \n                \n\n    \n    \n\n                ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"regions = list(df_train.Unique_Region.unique())\nfor region in regions:\n    selected_df = df_train[df_train.Unique_Region == region]\n    data_df = selected_df.iloc[71:84]\n    \n    confirmed = list(data_df[\"ConfirmedCases\"])\n    fatality = list(data_df[\"Fatalities\"])\n    \n    submit_from_pred(Date_submission, confirmed, region, df_test, df_sub, 0)\n    submit_from_pred(Date_submission, fatality, region, df_test, df_sub, 1)\n        \n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sub.head(30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sub.to_csv(\"submission.csv\", index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}