{"cells":[{"metadata":{},"cell_type":"markdown","source":"# COVID-19 Forecasting using Ensembles\nIn this Project we aim at forecasting COVID-19 cases and fatalities globally by using past three months data. The model used for training will be an **Ensemble** of models. Visualisations have been forked from [this notebook](https://www.kaggle.com/anshuls235/covid19-explained-through-visualizations). We will use GPU for the training deep neural network as part of our regressors.\n## <a id='main'>Table of Contents</a>\n- [Exploring data](#exp)\n- [Exploratory Data Analysis (EDA)](#eda)\n    1. [Universal growth of COVID19 over time](#world)\n    2. [Trend of COVID-19 in top 10 affected countries](#top10)\n    3. [Mortality Rate](#dr)\n    4. [Country Specific growth of COVID19](#country)\n        - [United States of America](#us)\n        - [Italy](#it)\n        - [Spain](#sp)\n        - [China](#ch)\n        - [Australia](#AU)\n- [Training and Prediction](#tp)"},{"metadata":{},"cell_type":"markdown","source":"# <a id='exp'>Exploring data</a>"},{"metadata":{},"cell_type":"markdown","source":"*installing some dependencies ...*"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install pycountry_convert","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Imports .......\nimport pandas as pd\nimport numpy as np\nimport datetime as dt\nimport pycountry\nimport pycountry_convert as pc\nimport plotly_express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.preprocessing import OrdinalEncoder, LabelEncoder, OneHotEncoder\nfrom sklearn import metrics\nimport xgboost as xgb\nfrom xgboost import XGBRegressor\nfrom xgboost import plot_importance, plot_tree\nfrom sklearn.model_selection import GridSearchCV\nfrom scipy.optimize import curve_fit\n%config InlineBackend.figure_format = 'retina'\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression, Ridge\nfrom sklearn.metrics import log_loss\nimport tensorflow as tf\nfrom tensorflow.keras.optimizers import Nadam\nfrom sklearn.metrics import mean_squared_error, mean_squared_error\nimport tensorflow.keras.layers as KL\nfrom datetime import timedelta\nimport datetime\nimport gc\nfrom tqdm import tqdm\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-4/train.csv') \ndf_test = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-4/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display(df_train.head())\ndisplay(df_train.describe())\ndisplay(df_train.info())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will reformat the Date on dataframe to facilitate the inferences and visualisations."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['Date'] = pd.to_datetime(df_train['Date'], format = '%Y-%m-%d')\ndf_test['Date'] = pd.to_datetime(df_test['Date'], format = '%Y-%m-%d')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Minimum date from training set: {}'.format(df_train['Date'].min()))\nprint('Maximum date from training set: {}'.format(df_train['Date'].max()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Minimum date from test set: {}'.format(df_test['Date'].min()))\nprint('Maximum date from test set: {}'.format(df_test['Date'].max()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a id='eda'>Exploratory Data Analysis (EDA)</a>\n[Go back to the main page](#main)\n\nHere we perform some EDA on the data in order to get a better understanding of the data and how COVID19 is spreading across the globe."},{"metadata":{},"cell_type":"markdown","source":"### <a id='world'>Universal growth of COVID19 over time</a>\nIn this section, we will explore the growth of COVID19 across the world since 22nd january 2020. Tree maps are used to illustrate the share of COVID19 Cases worldwide and chloropleth maps with a time slider to show the daily impact of virus.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"class country_utils():\n    def __init__(self):\n        self.d = {}\n    \n    def get_dic(self):\n        return self.d\n    \n    def get_country_details(self,country):\n        \"\"\"Returns country code(alpha_3) and continent\"\"\"\n        try:\n            country_obj = pycountry.countries.get(name=country)\n            continent_code = pc.country_alpha2_to_continent_code(country_obj.alpha_2)\n            continent = pc.convert_continent_code_to_continent_name(continent_code)\n            return country_obj.alpha_3, continent\n        except:\n            if 'Congo' in country:\n                country = 'Congo'\n            elif country == 'Diamond Princess' or country == 'Laos' or country == 'MS Zaandam'\\\n            or country == 'Holy See' or country == 'Timor-Leste':\n                return country, country\n            elif country == 'Korea, South':\n                country = 'Korea, Republic of'\n            elif country == 'Taiwan*':\n                country = 'Taiwan'\n            elif country == 'Burma':\n                country = 'Myanmar'\n            elif country == 'West Bank and Gaza':\n                country = 'Gaza'\n            country_obj = pycountry.countries.search_fuzzy(country)\n            continent_code = pc.country_alpha2_to_continent_code(country_obj[0].alpha_2)\n            continent = pc.convert_continent_code_to_continent_name(continent_code)\n            return country_obj[0].alpha_3, continent\n    \n    def get_iso3(self, country):\n        return self.d[country]['code']\n    \n    def get_continent(self,country):\n        return self.d[country]['continent']\n    \n    def add_values(self,country):\n        self.d[country] = {}\n        self.d[country]['code'],self.d[country]['continent'] = self.get_country_details(country)\n    \n    def fetch_iso3(self,country):\n        if country in self.d.keys():\n            return self.get_iso3(country)\n        else:\n            self.add_values(country)\n            return self.get_iso3(country)\n        \n    def fetch_continent(self,country):\n        if country in self.d.keys():\n            return self.get_continent(country)\n        else:\n            self.add_values(country)\n            return self.get_continent(country)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_tm = df_train.copy()\ndf_tm = df_tm[:25500]\ndate = df_tm.Date.max()#get current date\ndf_tm = df_tm[df_tm['Date']==date]\nobj = country_utils()\ndf_tm.Province_State.fillna('',inplace=True)\ndf_tm['continent'] = df_tm.apply(lambda x: obj.fetch_continent(x['Country_Region']), axis=1)\ndf_tm[\"world\"] = \"World\" # in order to have a single root node\nfig = px.treemap(df_tm, path=['world', 'continent', 'Country_Region','Province_State'], values='ConfirmedCases',\n                  color='ConfirmedCases', hover_data=['Country_Region'],\n                  color_continuous_scale='dense', title='Current share of Worldwide COVID19 Cases')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Some insights to the above plot:**\n1. Spain, France, Germany and Italy lead the share of COVID-19 cases in Europe.\n2. New York is leading the number of cases reported in USA, followed by New Jersey.\n3. Hubei, Iran and Turkey are among top locations for the spread across Asia.\n\n*Europe seems to be epicentre of the spread at the moment. Note the share differences between Europe, North America and Asia.*"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.treemap(df_tm, path=['world', 'continent', 'Country_Region','Province_State'], values='Fatalities',\n                  color='Fatalities', hover_data=['Country_Region'],\n                  color_continuous_scale='matter', title='Current share of Worldwide COVID19 Deaths')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Some insights to the above plot:**\n* Italy, Spain, France and UK are among top fatalities across the globe.\n\nConfirmed Cases and Fatalities are cummulative sums of all the previous days. In order to understand the daily trend,  we will create a column for daily cases and deaths that will be the difference between the current value and the previous day's value."},{"metadata":{"trusted":true},"cell_type":"code","source":"def add_daily_measures(df):\n    df.loc[0,'Daily Cases'] = df.loc[0,'ConfirmedCases']\n    df.loc[0,'Daily Deaths'] = df.loc[0,'Fatalities']\n    for i in range(1,len(df)):\n        df.loc[i,'Daily Cases'] = df.loc[i,'ConfirmedCases'] - df.loc[i-1,'ConfirmedCases']\n        df.loc[i,'Daily Deaths'] = df.loc[i,'Fatalities'] - df.loc[i-1,'Fatalities']\n    #Make the first row as 0 because we don't know the previous value\n    df.loc[0,'Daily Cases'] = 0\n    df.loc[0,'Daily Deaths'] = 0\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_world = df_train.copy()\ndf_world = df_world.groupby('Date',as_index=False)['ConfirmedCases','Fatalities'].sum()\ndf_world = add_daily_measures(df_world)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure(data=[\n    go.Bar(name='Cases', x=df_world['Date'], y=df_world['Daily Cases']),\n    go.Bar(name='Deaths', x=df_world['Date'], y=df_world['Daily Deaths'])\n])\n# Change the bar mode\nfig.update_layout(barmode='overlay', title='Worldwide daily Case and Death count')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_map = df_train.copy()\ndf_map = df_map[:24500]\ndf_map['Date'] = df_map['Date'].astype(str)\ndf_map = df_map.groupby(['Date','Country_Region'], as_index=False)['ConfirmedCases','Fatalities'].sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_map['iso_alpha'] = df_map.apply(lambda x: obj.fetch_iso3(x['Country_Region']), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_map['ln(ConfirmedCases)'] = np.log(df_map.ConfirmedCases + 1)\ndf_map['ln(Fatalities)'] = np.log(df_map.Fatalities + 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":">Since, cases and fatalities have grown exponentially over the last two months and countries like China, Italy, USA,and Spain, the choropleth maps are plotted on logarithmic scale. You can hover on the country to know the total confirmed cases or fatalities."},{"metadata":{"trusted":true},"cell_type":"code","source":"px.choropleth(df_map, \n              locations=\"iso_alpha\", \n              color=\"ln(ConfirmedCases)\", \n              hover_name=\"Country_Region\", \n              hover_data=[\"ConfirmedCases\"] ,\n              animation_frame=\"Date\",\n              color_continuous_scale=px.colors.sequential.dense, \n              title='Total Confirmed Cases growth(Logarithmic Scale)')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"px.choropleth(df_map, \n              locations=\"iso_alpha\", \n              color=\"ln(Fatalities)\", \n              hover_name=\"Country_Region\",\n              hover_data=[\"Fatalities\"],\n              animation_frame=\"Date\",\n              color_continuous_scale=px.colors.sequential.OrRd,\n              title = 'Total Deaths growth(Logarithmic Scale)')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <a id='top10'>Trend of COVID19 in top 10 affected countries</a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Get the top 10 countries\nlast_date = df_train.Date.max()\ndf_countries = df_train[df_train['Date']==last_date]\ndf_countries = df_countries.groupby('Country_Region', as_index=False)['ConfirmedCases','Fatalities'].sum()\ndf_countries = df_countries.nlargest(10,'ConfirmedCases')\n#Get the trend for top 10 countries\ndf_trend = df_train.groupby(['Date','Country_Region'], as_index=False)['ConfirmedCases','Fatalities'].sum()\ndf_trend = df_trend.merge(df_countries, on='Country_Region')\ndf_trend.drop(['ConfirmedCases_y','Fatalities_y'],axis=1, inplace=True)\ndf_trend.rename(columns={'Country_Region':'Country', 'ConfirmedCases_x':'Cases', 'Fatalities_x':'Deaths'}, inplace=True)\n#Add columns for studying logarithmic trends\ndf_trend['ln(Cases)'] = np.log(df_trend['Cases']+1)# Added 1 to remove error due to log(0).\ndf_trend['ln(Deaths)'] = np.log(df_trend['Deaths']+1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"px.line(df_trend, x='Date', y='Cases', color='Country', title='COVID19 Total Cases growth for top 10 worst affected countries')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"px.line(df_trend, x='Date', y='Deaths', color='Country', title='COVID19 Total Deaths growth for top 10 worst affected countries')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The daily variation of Confirmed Cases and Deaths for Top 10 affected countries on a logarithmic scale have also been plotted."},{"metadata":{"trusted":true},"cell_type":"code","source":"px.line(df_trend, x='Date', y='ln(Cases)', color='Country', title='COVID19 Total Cases growth for top 10 worst affected countries (Logarithmic Scale)')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"px.line(df_trend, x='Date', y='ln(Deaths)', color='Country', title='COVID19 Total Deaths growth for top 10 worst affected countries (Logarithmic Scale)')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <a id='dr'>Mortality Rate</a>\nThe mortality rate as the number of fatalities divided by the number of confirmed cases has been obtained as follows."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_map['Mortality Rate%'] = round((df_map.Fatalities/df_map.ConfirmedCases)*100,2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"px.choropleth(df_map, \n                    locations=\"iso_alpha\", \n                    color=\"Mortality Rate%\", \n                    hover_name=\"Country_Region\",\n                    hover_data=[\"ConfirmedCases\",\"Fatalities\"],\n                    animation_frame=\"Date\",\n                    color_continuous_scale=px.colors.sequential.Magma_r,\n                    title = 'Worldwide Daily Variation of Mortality Rate%')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_trend['Mortality Rate%'] = round((df_trend.Deaths/df_trend.Cases)*100,2)\npx.line(df_trend, x='Date', y='Mortality Rate%', color='Country', title='Variation of Mortality Rate% \\n(Top 10 worst affected countries)')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> **Tip**: Click on Iran in the legends tab to hide it and get a better understanding of the plot."},{"metadata":{},"cell_type":"markdown","source":"#### <a id='us'>United States of America</a>\nAs can be seen through the below graphs: \n- The COVID19 outbreak started from Washington state on the west coast and later on picked up pace in New york on the east coast . \n- Now, New York itself has around 40% of the total cases in USA.\n- New Jersey is the second in the list of worst affected states.\n- Cases and Fatalities in the East Coast are more than that of West Coast's."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dictionary to get the state codes from state names for US\nus_state_abbrev = {\n    'Alabama': 'AL',\n    'Alaska': 'AK',\n    'American Samoa': 'AS',\n    'Arizona': 'AZ',\n    'Arkansas': 'AR',\n    'California': 'CA',\n    'Colorado': 'CO',\n    'Connecticut': 'CT',\n    'Delaware': 'DE',\n    'District of Columbia': 'DC',\n    'Florida': 'FL',\n    'Georgia': 'GA',\n    'Guam': 'GU',\n    'Hawaii': 'HI',\n    'Idaho': 'ID',\n    'Illinois': 'IL',\n    'Indiana': 'IN',\n    'Iowa': 'IA',\n    'Kansas': 'KS',\n    'Kentucky': 'KY',\n    'Louisiana': 'LA',\n    'Maine': 'ME',\n    'Maryland': 'MD',\n    'Massachusetts': 'MA',\n    'Michigan': 'MI',\n    'Minnesota': 'MN',\n    'Mississippi': 'MS',\n    'Missouri': 'MO',\n    'Montana': 'MT',\n    'Nebraska': 'NE',\n    'Nevada': 'NV',\n    'New Hampshire': 'NH',\n    'New Jersey': 'NJ',\n    'New Mexico': 'NM',\n    'New York': 'NY',\n    'North Carolina': 'NC',\n    'North Dakota': 'ND',\n    'Northern Mariana Islands':'MP',\n    'Ohio': 'OH',\n    'Oklahoma': 'OK',\n    'Oregon': 'OR',\n    'Pennsylvania': 'PA',\n    'Puerto Rico': 'PR',\n    'Rhode Island': 'RI',\n    'South Carolina': 'SC',\n    'South Dakota': 'SD',\n    'Tennessee': 'TN',\n    'Texas': 'TX',\n    'Utah': 'UT',\n    'Vermont': 'VT',\n    'Virgin Islands': 'VI',\n    'Virginia': 'VA',\n    'Washington': 'WA',\n    'West Virginia': 'WV',\n    'Wisconsin': 'WI',\n    'Wyoming': 'WY'\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_us = df_train[df_train['Country_Region']=='US']\ndf_us['Date'] = df_us['Date'].astype(str)\ndf_us['state_code'] = df_us.apply(lambda x: us_state_abbrev.get(x.Province_State,float('nan')), axis=1)\ndf_us['ln(ConfirmedCases)'] = np.log(df_us.ConfirmedCases + 1)\ndf_us['ln(Fatalities)'] = np.log(df_us.Fatalities + 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"px.choropleth(df_us,\n              locationmode=\"USA-states\",\n              scope=\"usa\",\n              locations=\"state_code\",\n              color=\"ln(ConfirmedCases)\",\n              hover_name=\"Province_State\",\n              hover_data=[\"ConfirmedCases\"],\n              animation_frame=\"Date\",\n              color_continuous_scale=px.colors.sequential.Darkmint,\n              title = 'Total Cases growth for USA(Logarithmic Scale)')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"px.choropleth(df_us,\n              locationmode=\"USA-states\",\n              scope=\"usa\",\n              locations=\"state_code\",\n              color=\"ln(Fatalities)\",\n              hover_name=\"Province_State\",\n              hover_data=[\"Fatalities\"],\n              animation_frame=\"Date\",\n              color_continuous_scale=px.colors.sequential.OrRd,\n              title = 'Total deaths growth for USA(Logarithmic Scale)')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_usa = df_train.query(\"Country_Region=='US'\")\ndf_usa = df_usa.groupby('Date',as_index=False)['ConfirmedCases','Fatalities'].sum()\ndf_usa = add_daily_measures(df_usa)\nfig = go.Figure(data=[\n    go.Bar(name='Cases', x=df_usa['Date'], y=df_usa['Daily Cases']),\n    go.Bar(name='Deaths', x=df_usa['Date'], y=df_usa['Daily Deaths'])\n])\n# Change the bar mode\nfig.update_layout(barmode='overlay', title='Daily Case and Death count(USA)')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### <a id='it'>Italy</a>\nWe are interested in analysis of the outbreak in Italy. Here we will explore the similar analysis with Italy as we have performed with the USA."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.Province_State.fillna('NaN', inplace=True)\ndf_plot = df_train.groupby(['Date','Country_Region','Province_State'], as_index=False)['ConfirmedCases','Fatalities'].sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df_plot.query(\"Country_Region=='Italy'\")\ndf.reset_index(inplace = True)\ndf = add_daily_measures(df)\nfig = go.Figure(data=[\n    go.Bar(name='Cases', x=df['Date'], y=df['Daily Cases']),\n    go.Bar(name='Deaths', x=df['Date'], y=df['Daily Deaths'])\n])\n# Change the bar mode\nfig.update_layout(barmode='overlay', title='Daily Case and Death count (Italy)')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Some insights:**\n\n* Number of cases in Italy seem to be decreasing, however, this is different to number of fatalities as it represents a constant behaviour. "},{"metadata":{},"cell_type":"markdown","source":"#### <a id='sp'>Spain</a>"},{"metadata":{},"cell_type":"markdown","source":"Spain has surpassed Italy in the number of COVID-19 cases across Europe. We will illustrate the number of cases and fatalities as the above analysis we performed for Italy. "},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.Province_State.fillna('NaN', inplace=True)\ndf_plot = df_train.groupby(['Date','Country_Region','Province_State'], as_index=False)['ConfirmedCases','Fatalities'].sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df_plot.query(\"Country_Region=='Spain'\")\ndf.reset_index(inplace = True)\ndf = add_daily_measures(df)\nfig = go.Figure(data=[\n    go.Bar(name='Cases', x=df['Date'], y=df['Daily Cases']),\n    go.Bar(name='Deaths', x=df['Date'], y=df['Daily Deaths'])\n])\n# Change the bar mode\nfig.update_layout(barmode='overlay', title='Daily Case and Death count (Spain)')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Some insights:**\n\n* Number of cases in Spain seem to be decreasing, however, this is different to number of fatalities as it represents a constant behaviour. "},{"metadata":{},"cell_type":"markdown","source":"#### <a id='ch'>China</a>\n- This is where it all started! By looking at the graph it can be seen that China has been able to almost stop the spread of COVID19 substantially.\n- Almost all the cases are from the Hubei Province which can be attributed to the fact that the outbreak started from its capital, Wuhan.\n\n> In order to get a better understanding of the cases/fatalities growth from other provinces, you can click on Hubei in the legend so that it gets hidden and the scale will autoscale."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df_plot.query(\"Country_Region=='China'\")\npx.line(df, x='Date', y='ConfirmedCases', color='Province_State', title='Total Cases growth for China')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"px.line(df, x='Date', y='Fatalities', color='Province_State', title='Total Deaths growth for China')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_ch = df_train.query(\"Country_Region=='China'\")\ndf_ch = df_ch.groupby('Date',as_index=False)['ConfirmedCases','Fatalities'].sum()\ndf_ch = add_daily_measures(df_ch)\nfig = go.Figure(data=[\n    go.Bar(name='Cases', x=df_ch['Date'], y=df_ch['Daily Cases']),\n    go.Bar(name='Deaths', x=df_ch['Date'], y=df_ch['Daily Deaths'])\n])\n# Change the bar mode\nfig.update_layout(barmode='overlay', title='Daily Case and Death count (China)')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### <a id='AU'>Australia</a>\nCOVID-19 spread has been initiated here in Australia with a small growth rate. But the outbreak surged abruptly especially in New South Wales. Thanks to the Australian government for comprehensive and actionable measures. You will find how the growth slows down by referring to the curves below."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.Province_State.fillna('NaN', inplace=True)\ndf_plot = df_train.groupby(['Date','Country_Region','Province_State'], as_index=False)['ConfirmedCases','Fatalities'].sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df_plot.query(\"Country_Region=='Australia'\")\npx.line(df, x='Date', y='ConfirmedCases', color='Province_State', title='Total Cases growth for Australia')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"px.line(df, x='Date', y='Fatalities', color='Province_State', title='Total Deaths growth for Australia')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_AU = df_train.query(\"Country_Region=='Australia'\")\ndf_AU = df_AU.groupby('Date',as_index=False)['ConfirmedCases','Fatalities'].sum()\ndf_AU = add_daily_measures(df_AU)\nfig = go.Figure(data=[\n    go.Bar(name='Cases', x=df_AU['Date'], y=df_AU['Daily Cases']),\n    go.Bar(name='Deaths', x=df_AU['Date'], y=df_AU['Daily Deaths'])\n])\n# Change the bar mode\nfig.update_layout(barmode='overlay', title='Daily Case and Death count (Australia)')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a id='tp'>Training and Prediction</a>\n[Go back to the main page](#main)\n\nWe will use XG-BOOST, and deep neural network for training in this section. We will use ensemble of models for training and prediction."},{"metadata":{"trusted":true},"cell_type":"code","source":"# set the session to pool available GPU\nconfig = tf.compat.v1.ConfigProto( device_count = {'GPU': 1 , 'CPU': 10} ) \nsess = tf.compat.v1.Session(config=config) \ntf.compat.v1.keras.backend.set_session(sess)\n\n# here we will train a dense network using tensorflow backend on keras as part of our regressors\n\n# defining helper functions ...\ndef main_for_train(save_model_train=False, save_public_test=False):\n    train = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-4/train.csv')\n    train['Province_State'].fillna('', inplace=True)\n    train['Date'] = pd.to_datetime(train['Date'])\n    train['day'] = train.Date.dt.dayofyear\n    train['my_geoloc'] = ['_'.join(x) for x in zip(train['Country_Region'], train['Province_State'])]\n    train\n\n    test = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-4/test.csv')\n    test['Province_State'].fillna('', inplace=True)\n    test['Date'] = pd.to_datetime(test['Date'])\n    test['day'] = test.Date.dt.dayofyear\n    test['my_geoloc'] = ['_'.join(x) for x in zip(test['Country_Region'], test['Province_State'])]\n    test\n\n    day_min = train['day'].min()\n    train['day'] -= day_min\n    test['day'] -= day_min\n\n    min_test_val_day = test.day.min()\n    max_test_val_day = train.day.max()\n    max_test_day = test.day.max()\n    num_days = max_test_day + 1\n\n    min_test_val_day, max_test_val_day, num_days\n\n    train['ForecastId'] = -1\n    test['Id'] = -1\n    test['ConfirmedCases'] = 0\n    test['Fatalities'] = 0\n\n    debug = False\n\n    data = pd.concat([train,\n                      test[test.day > max_test_val_day][train.columns]\n                     ]).reset_index(drop=True)\n    if debug:\n        data = data[data['my_geoloc'] >= 'France_'].reset_index(drop=True)\n    #del train, test\n    gc.collect()\n\n    dates = data[data['my_geoloc'] == 'France_'].Date.values\n\n    if 0:\n        gr = data.groupby('my_geoloc')\n        data['ConfirmedCases'] = gr.ConfirmedCases.transform('cummax')\n        data['Fatalities'] = gr.Fatalities.transform('cummax')\n\n    my_geoloc_data = data.pivot(index='my_geoloc', columns='day', values='ForecastId')\n    num_my_geoloc = my_geoloc_data.shape[0]\n    my_geoloc_data\n\n    my_geoloc_id = {}\n    for i,g in enumerate(my_geoloc_data.index):\n        my_geoloc_id[g] = i\n\n\n    ConfirmedCases = data.pivot(index='my_geoloc', columns='day', values='ConfirmedCases')\n    Fatalities = data.pivot(index='my_geoloc', columns='day', values='Fatalities')\n\n    if debug:\n        cases = ConfirmedCases.values\n        deaths = Fatalities.values\n    else:\n        cases = np.log1p(ConfirmedCases.values)\n        deaths = np.log1p(Fatalities.values)\n\n\n    def load_my_dataset(start_pred, num_train, lag_period):\n        days = np.arange( start_pred - num_train + 1, start_pred + 1)\n        lag_cases = np.vstack([cases[:, d - lag_period : d] for d in days])\n        lag_deaths = np.vstack([deaths[:, d - lag_period : d] for d in days])\n        target_cases = np.vstack([cases[:, d : d + 1] for d in days])\n        target_deaths = np.vstack([deaths[:, d : d + 1] for d in days])\n        my_geoloc_ids = np.vstack([my_geoloc_ids_base for d in days])\n        country_ids = np.vstack([country_ids_base for d in days])\n        return lag_cases, lag_deaths, target_cases, target_deaths, my_geoloc_ids, country_ids, days\n\n    def update_valid_dataset(data, pred_death, pred_case):\n        lag_cases, lag_deaths, target_cases, target_deaths, my_geoloc_ids, country_ids, days = data\n        day = days[-1] + 1\n        new_lag_cases = np.hstack([lag_cases[:, 1:], pred_case])\n        new_lag_deaths = np.hstack([lag_deaths[:, 1:], pred_death]) \n        new_target_cases = cases[:, day:day+1]\n        new_target_deaths = deaths[:, day:day+1] \n        new_my_geoloc_ids = my_geoloc_ids  \n        new_country_ids = country_ids  \n        new_days = 1 + days\n        return new_lag_cases, new_lag_deaths, new_target_cases, new_target_deaths, new_my_geoloc_ids, new_country_ids, new_days\n\n    def infer_model(lr_death, lr_case, data, start_lag_death, end_lag_death, num_lag_case, fit, score):\n        lag_cases, lag_deaths, target_cases, target_deaths, my_geoloc_ids, country_ids, days = data\n\n        X_death = np.hstack([lag_cases[:, -start_lag_death:-end_lag_death], country_ids])\n        X_death = np.hstack([lag_deaths[:, -num_lag_case:], country_ids])\n        X_death = np.hstack([lag_cases[:, -start_lag_death:-end_lag_death], lag_deaths[:, -num_lag_case:], country_ids])\n        y_death = target_deaths\n        y_death_prev = lag_deaths[:, -1:]\n        if fit:\n            if 0:\n                keep = (y_death > 0).ravel()\n                X_death = X_death[keep]\n                y_death = y_death[keep]\n                y_death_prev = y_death_prev[keep]\n            lr_death.fit(X_death, y_death)\n        y_pred_death = lr_death.predict(X_death)\n        y_pred_death = np.maximum(y_pred_death, y_death_prev)\n\n        X_case = np.hstack([lag_cases[:, -num_lag_case:], my_geoloc_ids])\n        X_case = lag_cases[:, -num_lag_case:]\n        y_case = target_cases\n        y_case_prev = lag_cases[:, -1:]\n        if fit:\n            lr_case.fit(X_case, y_case)\n        y_pred_case = lr_case.predict(X_case)\n        y_pred_case = np.maximum(y_pred_case, y_case_prev)\n\n        if score:\n            death_score = val_score(y_death, y_pred_death)\n            case_score = val_score(y_case, y_pred_case)\n        else:\n            death_score = 0\n            case_score = 0\n\n        return death_score, case_score, y_pred_death, y_pred_case\n\n    def train_model(train, valid, start_lag_death, end_lag_death, num_lag_case, num_val, score=True):\n        alpha = 2\n        lr_death = Ridge(alpha=alpha, fit_intercept=False)\n        lr_case = Ridge(alpha=alpha, fit_intercept=True)\n\n        (train_death_score, train_case_score, train_pred_death, train_pred_case,\n        ) = infer_model(lr_death, lr_case, train, start_lag_death, end_lag_death, num_lag_case, fit=True, score=score)\n\n        death_scores = []\n        case_scores = []\n\n        death_pred = []\n        case_pred = []\n\n        for i in range(num_val):\n\n            (valid_death_score, valid_case_score, valid_pred_death, valid_pred_case,\n            ) = infer_model(lr_death, lr_case, valid, start_lag_death, end_lag_death, num_lag_case, fit=False, score=score)\n\n            death_scores.append(valid_death_score)\n            case_scores.append(valid_case_score)\n            death_pred.append(valid_pred_death)\n            case_pred.append(valid_pred_case)\n\n            if 0:\n                print('val death: %0.3f' %  valid_death_score,\n                      'val case: %0.3f' %  valid_case_score,\n                      'val : %0.3f' %  np.mean([valid_death_score, valid_case_score]),\n                      flush=True)\n            valid = update_valid_dataset(valid, valid_pred_death, valid_pred_case)\n\n        if score:\n            death_scores = np.sqrt(np.mean([s**2 for s in death_scores]))\n            case_scores = np.sqrt(np.mean([s**2 for s in case_scores]))\n            if 0:\n                print('train death: %0.3f' %  train_death_score,\n                      'train case: %0.3f' %  train_case_score,\n                      'val death: %0.3f' %  death_scores,\n                      'val case: %0.3f' %  case_scores,\n                      'val : %0.3f' % ( (death_scores + case_scores) / 2),\n                      flush=True)\n            else:\n                print('%0.4f' %  case_scores,\n                      ', %0.4f' %  death_scores,\n                      '= %0.4f' % ( (death_scores + case_scores) / 2),\n                      flush=True)\n        death_pred = np.hstack(death_pred)\n        case_pred = np.hstack(case_pred)\n        return death_scores, case_scores, death_pred, case_pred\n\n    countries = [g.split('_')[0] for g in my_geoloc_data.index]\n    countries = pd.factorize(countries)[0]\n\n    country_ids_base = countries.reshape((-1, 1))\n    ohe = OneHotEncoder(sparse=False)\n    country_ids_base = 0.2 * ohe.fit_transform(country_ids_base)\n    country_ids_base.shape\n\n    my_geoloc_ids_base = np.arange(num_my_geoloc).reshape((-1, 1))\n    ohe = OneHotEncoder(sparse=False)\n    my_geoloc_ids_base = 0.1 * ohe.fit_transform(my_geoloc_ids_base)\n    my_geoloc_ids_base.shape\n\n    def val_score(true, pred):\n        pred = np.log1p(np.round(np.expm1(pred) - 0.2))\n        return np.sqrt(mean_squared_error(true.ravel(), pred.ravel()))\n\n    def val_score(true, pred):\n        return np.sqrt(mean_squared_error(true.ravel(), pred.ravel()))\n\n\n\n    start_lag_death, end_lag_death = 14, 6,\n    num_train = 6\n    num_lag_case = 14\n    lag_period = max(start_lag_death, num_lag_case)\n\n    def load_outputs_fit(start_val_delta=0):   \n        start_val = min_test_val_day + start_val_delta\n        last_train = start_val - 1\n        num_val = max_test_val_day - start_val + 1\n        print(dates[start_val], start_val, num_val)\n        train_data = load_my_dataset(last_train, num_train, lag_period)\n        valid_data = load_my_dataset(start_val, 1, lag_period)\n        _, _, val_death_preds, val_case_preds = train_model(train_data, valid_data, \n                                                            start_lag_death, end_lag_death, num_lag_case, num_val)\n\n        pred_deaths = Fatalities.iloc[:, start_val:start_val+num_val].copy()\n        pred_deaths.iloc[:, :] = np.expm1(val_death_preds)\n        pred_deaths = pred_deaths.stack().reset_index()\n        pred_deaths.columns = ['my_geoloc', 'day', 'Fatalities']\n        pred_deaths\n\n        pred_cases = ConfirmedCases.iloc[:, start_val:start_val+num_val].copy()\n        pred_cases.iloc[:, :] = np.expm1(val_case_preds)\n        pred_cases = pred_cases.stack().reset_index()\n        pred_cases.columns = ['my_geoloc', 'day', 'ConfirmedCases']\n        pred_cases\n\n        sub = train[['Date', 'Id', 'my_geoloc', 'day']]\n        sub = sub.merge(pred_cases, how='left', on=['my_geoloc', 'day'])\n        sub = sub.merge(pred_deaths, how='left', on=['my_geoloc', 'day'])\n        #sub = sub.fillna(0)\n        sub = sub[sub.day >= start_val]\n        sub = sub[['Id', 'ConfirmedCases', 'Fatalities']].copy()\n        return sub\n\n\n    if save_model_train:\n        for start_val_delta, date in zip(range(3, -8, -3),\n                                  ['2020-04-27', '2020-04-24', '2020-04-21', '2020-04-18']):\n            print(date, end=' ')\n            outputs_fit = load_outputs_fit(start_val_delta)\n            outputs_fit.to_csv('../submissions/cpmp-%s.csv' % date, index=None)\n\n    def get_sub(start_val_delta=0):   \n        start_val = min_test_val_day + start_val_delta\n        last_train = start_val - 1\n        num_val = max_test_val_day - start_val + 1\n        print(dates[last_train], start_val, num_val)\n        num_lag_case = 14\n        train_data = load_my_dataset(last_train, num_train, lag_period)\n        valid_data = load_my_dataset(start_val, 1, lag_period)\n        _, _, val_death_preds, val_case_preds = train_model(train_data, valid_data, \n                                                            start_lag_death, end_lag_death, num_lag_case, num_val)\n\n        pred_deaths = Fatalities.iloc[:, start_val:start_val+num_val].copy()\n        pred_deaths.iloc[:, :] = np.expm1(val_death_preds)\n        pred_deaths = pred_deaths.stack().reset_index()\n        pred_deaths.columns = ['my_geoloc', 'day', 'Fatalities']\n        pred_deaths\n\n        pred_cases = ConfirmedCases.iloc[:, start_val:start_val+num_val].copy()\n        pred_cases.iloc[:, :] = np.expm1(val_case_preds)\n        pred_cases = pred_cases.stack().reset_index()\n        pred_cases.columns = ['my_geoloc', 'day', 'ConfirmedCases']\n        pred_cases\n\n        sub = test[['Date', 'ForecastId', 'my_geoloc', 'day']]\n        sub = sub.merge(pred_cases, how='left', on=['my_geoloc', 'day'])\n        sub = sub.merge(pred_deaths, how='left', on=['my_geoloc', 'day'])\n        sub = sub.fillna(0)\n        sub = sub[['ForecastId', 'ConfirmedCases', 'Fatalities']]\n        return sub\n        return sub\n\n\n    known_test = train[['my_geoloc', 'day', 'ConfirmedCases', 'Fatalities']\n              ].merge(test[['my_geoloc', 'day', 'ForecastId']], how='left', on=['my_geoloc', 'day'])\n    known_test = known_test[['ForecastId', 'ConfirmedCases', 'Fatalities']][known_test.ForecastId.notnull()].copy()\n    known_test\n\n    unknow_test = test[test.day > max_test_val_day]\n    unknow_test\n\n    def get_final_sub():   \n        start_val = max_test_val_day + 1\n        last_train = start_val - 1\n        num_val = max_test_day - start_val + 1\n        print(dates[last_train], start_val, num_val)\n        num_lag_case = num_val + 3\n        train_data = load_my_dataset(last_train, num_train, lag_period)\n        valid_data = load_my_dataset(start_val, 1, lag_period)\n        (_, _, val_death_preds, val_case_preds\n        ) = train_model(train_data, valid_data, start_lag_death, end_lag_death, num_lag_case, num_val, score=False)\n\n        pred_deaths = Fatalities.iloc[:, start_val:start_val+num_val].copy()\n        pred_deaths.iloc[:, :] = np.expm1(val_death_preds)\n        pred_deaths = pred_deaths.stack().reset_index()\n        pred_deaths.columns = ['my_geoloc', 'day', 'Fatalities']\n        pred_deaths\n\n        pred_cases = ConfirmedCases.iloc[:, start_val:start_val+num_val].copy()\n        pred_cases.iloc[:, :] = np.expm1(val_case_preds)\n        pred_cases = pred_cases.stack().reset_index()\n        pred_cases.columns = ['my_geoloc', 'day', 'ConfirmedCases']\n        pred_cases\n        print(unknow_test.shape, pred_deaths.shape, pred_cases.shape)\n\n        sub = unknow_test[['Date', 'ForecastId', 'my_geoloc', 'day']]\n        sub = sub.merge(pred_cases, how='left', on=['my_geoloc', 'day'])\n        sub = sub.merge(pred_deaths, how='left', on=['my_geoloc', 'day'])\n        #sub = sub.fillna(0)\n        sub = sub[['ForecastId', 'ConfirmedCases', 'Fatalities']]\n        sub = pd.concat([known_test, sub])\n        return sub\n\n    if save_public_test:\n        sub = get_sub()\n    else:\n        sub = get_final_sub()\n    return sub\n\n\n\n# here we will load data and make it ready for training\ndef load_deep_nn():\n    df = pd.read_csv(\"../input/covid19-global-forecasting-week-4/train.csv\")\n    dataframe_for_submission = pd.read_csv(\"../input/covid19-global-forecasting-week-4/test.csv\")\n\n    co_train_data = pd.read_csv(\"../input/mytrainweek7/train (3).csv\").rename(columns={\"Country/Region\": \"Country_Region\"})\n    co_train_data = co_train_data.groupby(\"Country_Region\")[[\"Lat\", \"Long\"]].mean().reset_index()\n    co_train_data = co_train_data[co_train_data[\"Country_Region\"].notnull()]\n\n    loc_group = [\"Province_State\", \"Country_Region\"]\n\n\n    def preprocess(df):\n        df[\"Date\"] = df[\"Date\"].astype(\"datetime64[ms]\")\n        df[\"days\"] = (df[\"Date\"] - pd.to_datetime(\"2020-01-01\")).dt.days\n        df[\"weekend\"] = df[\"Date\"].dt.dayofweek//5\n\n        df = df.merge(co_train_data, how=\"left\", on=\"Country_Region\")\n        df[\"Lat\"] = (df[\"Lat\"] // 30).astype(np.float32).fillna(0)\n        df[\"Long\"] = (df[\"Long\"] // 60).astype(np.float32).fillna(0)\n\n        for col in loc_group:\n            df[col].fillna(\"none\", inplace=True)\n        return df\n\n    df = preprocess(df)\n    dataframe_for_submission = preprocess(dataframe_for_submission)\n\n    print(df.shape)\n\n    TARGETS = [\"ConfirmedCases\", \"Fatalities\"]\n\n    for col in TARGETS:\n        df[col] = np.log1p(df[col])\n\n    NUM_SHIFT = 5\n\n    features = [\"Lat\", \"Long\"]\n\n    for s in range(1, NUM_SHIFT+1):\n        for col in TARGETS:\n            df[\"prev_{}_{}\".format(col, s)] = df.groupby(loc_group)[col].shift(s)\n            features.append(\"prev_{}_{}\".format(col, s))\n\n    df = df[df[\"Date\"] >= df[\"Date\"].min() + timedelta(days=NUM_SHIFT)].copy()\n\n    TEST_FIRST = dataframe_for_submission[\"Date\"].min() # pd.to_datetime(\"2020-03-13\") #\n    TEST_DAYS = (df[\"Date\"].max() - TEST_FIRST).days + 1\n\n    dev_df, test_df = df[df[\"Date\"] < TEST_FIRST].copy(), df[df[\"Date\"] >= TEST_FIRST].copy()\n\n    def nn_block(input_layer, size, dropout_rate, activation):\n        out_layer = KL.Dense(size, activation=None)(input_layer)\n        #out_layer = KL.BatchNormalization()(out_layer)\n        out_layer = KL.Activation(activation)(out_layer)\n        out_layer = KL.Dropout(dropout_rate)(out_layer)\n        return out_layer\n\n\n    def get_model():\n        inp = KL.Input(shape=(len(features),))\n\n        hidden_layer = nn_block(inp, 208, 0.0, \"relu\")\n        gate_layer = nn_block(hidden_layer, 104, 0.0, \"hard_sigmoid\")\n        hidden_layer = nn_block(hidden_layer, 104, 0.0, \"relu\")\n        hidden_layer = KL.multiply([hidden_layer, gate_layer])\n\n        out = KL.Dense(len(TARGETS), activation=\"linear\")(hidden_layer)\n\n        model = tf.keras.models.Model(inputs=[inp], outputs=out)\n        return model\n\n    get_model().summary()\n\n    def get_input(df):\n        return [df[features]]\n\n    NUM_MODELS = 100\n\n\n    def train_models(df, save=False):\n        models = []\n        for i in range(NUM_MODELS):\n            model = get_model()\n            model.compile(loss=\"mean_squared_error\", optimizer=Nadam(lr=1e-4))\n            hist = model.fit(get_input(df), df[TARGETS],\n                             batch_size=2250, epochs=1000, verbose=0, shuffle=True)\n            if save:\n                model.save_weights(\"model{}.h5\".format(i))\n            models.append(model)\n        return models\n\n    models = train_models(dev_df)\n\n\n    prev_targets = ['prev_ConfirmedCases_1', 'prev_Fatalities_1']\n\n    def predict_one(df, models):\n        pred = np.zeros((df.shape[0], 2))\n        for model in models:\n            pred += model.predict(get_input(df))/len(models)\n        pred = np.maximum(pred, df[prev_targets].values)\n        pred[:, 0] = np.log1p(np.expm1(pred[:, 0]) + 0.1)\n        pred[:, 1] = np.log1p(np.expm1(pred[:, 1]) + 0.01)\n        return np.clip(pred, None, 15)\n\n    print([mean_squared_error(dev_df[TARGETS[i]], predict_one(dev_df, models)[:, i]) for i in range(len(TARGETS))])\n\n\n    def rmse(y_true, y_pred):\n        return np.sqrt(mean_squared_error(y_true, y_pred))\n\n    def evaluate(df):\n        error = 0\n        for col in TARGETS:\n            error += rmse(df[col].values, df[\"pred_{}\".format(col)].values)\n        return np.round(error/len(TARGETS), 5)\n\n\n    def predict(test_df, first_day, num_days, models, val=False):\n        temp_df = test_df.loc[test_df[\"Date\"] == first_day].copy()\n        y_pred = predict_one(temp_df, models)\n\n        for i, col in enumerate(TARGETS):\n            test_df[\"pred_{}\".format(col)] = 0\n            test_df.loc[test_df[\"Date\"] == first_day, \"pred_{}\".format(col)] = y_pred[:, i]\n\n        print(first_day, np.isnan(y_pred).sum(), y_pred.min(), y_pred.max())\n        if val:\n            print(evaluate(test_df[test_df[\"Date\"] == first_day]))\n\n\n        y_prevs = [None]*NUM_SHIFT\n\n        for i in range(1, NUM_SHIFT):\n            y_prevs[i] = temp_df[['prev_ConfirmedCases_{}'.format(i), 'prev_Fatalities_{}'.format(i)]].values\n\n        for d in range(1, num_days):\n            date = first_day + timedelta(days=d)\n            print(date, np.isnan(y_pred).sum(), y_pred.min(), y_pred.max())\n\n            temp_df = test_df.loc[test_df[\"Date\"] == date].copy()\n            temp_df[prev_targets] = y_pred\n            for i in range(2, NUM_SHIFT+1):\n                temp_df[['prev_ConfirmedCases_{}'.format(i), 'prev_Fatalities_{}'.format(i)]] = y_prevs[i-1]\n\n            y_pred, y_prevs = predict_one(temp_df, models), [None, y_pred] + y_prevs[1:-1]\n\n\n            for i, col in enumerate(TARGETS):\n                test_df.loc[test_df[\"Date\"] == date, \"pred_{}\".format(col)] = y_pred[:, i]\n\n            if val:\n                print(evaluate(test_df[test_df[\"Date\"] == date]))\n\n        return test_df\n\n    test_df = predict(test_df, TEST_FIRST, TEST_DAYS, models, val=True)\n    print(evaluate(test_df))\n\n    for col in TARGETS:\n        test_df[col] = np.expm1(test_df[col])\n        test_df[\"pred_{}\".format(col)] = np.expm1(test_df[\"pred_{}\".format(col)])\n\n    models = train_models(df, save=True)\n\n    dataframe_for_submission_public = dataframe_for_submission[dataframe_for_submission[\"Date\"] <= df[\"Date\"].max()].copy()\n    dataframe_for_submission_private = dataframe_for_submission[dataframe_for_submission[\"Date\"] > df[\"Date\"].max()].copy()\n\n    pred_cols = [\"pred_{}\".format(col) for col in TARGETS]\n    #dataframe_for_submission_public = dataframe_for_submission_public.merge(test_df[[\"Date\"] + loc_group + pred_cols].rename(columns={col: col[5:] for col in pred_cols}), \n    #                                    how=\"left\", on=[\"Date\"] + loc_group)\n    dataframe_for_submission_public = dataframe_for_submission_public.merge(test_df[[\"Date\"] + loc_group + TARGETS], how=\"left\", on=[\"Date\"] + loc_group)\n\n    SUB_FIRST = dataframe_for_submission_private[\"Date\"].min()\n    SUB_DAYS = (dataframe_for_submission_private[\"Date\"].max() - dataframe_for_submission_private[\"Date\"].min()).days + 1\n\n    dataframe_for_submission_private = df.append(dataframe_for_submission_private, sort=False)\n\n    for s in range(1, NUM_SHIFT+1):\n        for col in TARGETS:\n            dataframe_for_submission_private[\"prev_{}_{}\".format(col, s)] = dataframe_for_submission_private.groupby(loc_group)[col].shift(s)\n\n    dataframe_for_submission_private = dataframe_for_submission_private[dataframe_for_submission_private[\"Date\"] >= SUB_FIRST].copy()\n\n    dataframe_for_submission_private = predict(dataframe_for_submission_private, SUB_FIRST, SUB_DAYS, models)\n\n    for col in TARGETS:\n        dataframe_for_submission_private[col] = np.expm1(dataframe_for_submission_private[\"pred_{}\".format(col)])\n\n    dataframe_for_submission = dataframe_for_submission_public.append(dataframe_for_submission_private, sort=False)\n    dataframe_for_submission[\"ForecastId\"] = dataframe_for_submission[\"ForecastId\"].astype(np.int16)\n\n    return dataframe_for_submission[[\"ForecastId\"] + TARGETS]\n\n# get the output of the DNN as part of our submission\nsub1 = main_for_train()\nsub1['ForecastId'] = sub1['ForecastId'].astype('int')\nsub2 = load_deep_nn()\n\nsub1.sort_values(\"ForecastId\", inplace=True)\nsub2.sort_values(\"ForecastId\", inplace=True)\n\n\nfrom sklearn.metrics import mean_squared_error\n\nTARGETS = [\"ConfirmedCases\", \"Fatalities\"]\n\n[np.sqrt(mean_squared_error(np.log1p(sub1[t].values), np.log1p(sub2[t].values))) for t in TARGETS]\n\ndataframe_for_submission = sub1.copy()\nfor t in TARGETS:\n    dataframe_for_submission[t] = np.expm1(np.log1p(sub1[t].values)*0.5 + np.log1p(sub2[t].values)*0.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We will train XG-BOOST here\nimport pandas as pd\nimport numpy as np\nfrom xgboost import XGBRegressor\n\ntrain = pd.read_csv('../input/covid19-global-forecasting-week-4/train.csv')\ntrain['Date'] = pd.to_datetime(train['Date'])\ndef dealing_with_null_values(dataset):\n    dataset = dataset\n    for i in dataset.columns:\n        replace = []\n        data  = dataset[i].isnull()\n        count = 0\n        for j,k in zip(data,dataset[i]):\n            if (j==True):\n                count = count+1\n                replace.append('No Information Available')\n            else:\n                replace.append(k)\n        print(\"Num of null values (\",i,\"):\",count)\n        dataset[i] = replace\n    return dataset\ntrain = dealing_with_null_values(train)\ndef fillState(state, country):\n    if state == 'No Information Available': return country\n    return state\ntrain['Province_State'] = train.loc[:, ['Province_State', 'Country_Region']].apply(lambda x : fillState(x['Province_State'], x['Country_Region']), axis=1)\ntrain.loc[:, 'Date'] = train.Date.dt.strftime(\"%m%d\")\ntrain[\"Date\"]  = train[\"Date\"].astype(int)\nfrom sklearn import preprocessing\n\nle = preprocessing.LabelEncoder()\n\ntrain.Country_Region = le.fit_transform(train.Country_Region)\ntrain.Province_State = le.fit_transform(train.Province_State)\ndata = pd.DataFrame()\ndata['Province_State']=train['Province_State']\ndata['Country_Region']=train['Country_Region']\ndata['Date']=train['Date']\n\nxgb_model1 = XGBRegressor(n_estimators=1000) \nxgb_model1.fit(data,train['ConfirmedCases'])\nxgb_model2 = XGBRegressor(n_estimators=1000) \nxgb_model2.fit(data,train['Fatalities'])\ntest = pd.read_csv('../input/covid19-global-forecasting-week-4/test.csv')\ntest['Date'] = pd.to_datetime(test['Date'], infer_datetime_format=True)\ntest = dealing_with_null_values(test)\ntest['Province_State'] = test.loc[:, ['Province_State', 'Country_Region']].apply(lambda x : fillState(x['Province_State'], x['Country_Region']), axis=1)\ntest.loc[:, 'Date'] = test.Date.dt.strftime(\"%m%d\")\ntest[\"Date\"]  = test[\"Date\"].astype(int)\nle = preprocessing.LabelEncoder()\n\ntest.Country_Region = le.fit_transform(test.Country_Region)\ntest.Province_State = le.fit_transform(test.Province_State)\n\ntest_data = pd.DataFrame()\ntest_data['Province_State']  = test['Province_State'] \ntest_data['Country_Region']  = test['Country_Region']\ntest_data['Date'] = test['Date']\nfrom warnings import filterwarnings\nfilterwarnings('ignore')\n\nxgb_sub =  pd.DataFrame({'ForecastId': [], 'ConfirmedCases': [], 'Fatalities': []})\nfor i in test['Country_Region'].unique():\n    s = test[test['Country_Region'] == i].Province_State.unique()\n    for j in s:\n        xgb_sub_train = data[(data['Country_Region'] == i) & (data['Province_State'] == j)]\n        xgb_sub_test = test_data[(test_data['Country_Region']==i) & (test_data['Province_State'] == j)]\n    \n        xgb_sub_train_with_labels = train[(train['Country_Region'] == i) & (train['Province_State'] == j)]\n    \n        index_test = test[(test['Country_Region']==i) & (test['Province_State'] == j)]\n        index_test = index_test['ForecastId']\n    \n        xgb_model1 = XGBRegressor(n_estimators=2000)\n        xgb_model1.fit(xgb_sub_train,xgb_sub_train_with_labels['ConfirmedCases'])\n    \n        xgb_model2 = XGBRegressor(n_estimators=2000)\n        xgb_model2.fit(xgb_sub_train,xgb_sub_train_with_labels['Fatalities'])\n                                        \n        y1_xpred_xgb_sub = xgb_model1.predict(xgb_sub_test)\n        y2_xpred_xgb_sub = xgb_model2.predict(xgb_sub_test)\n    \n    \n        xgb_xgb_subs = pd.DataFrame()\n        xgb_xgb_subs['ForecastId'] = index_test\n        xgb_xgb_subs['ConfirmedCases'] = y1_xpred_xgb_sub\n        xgb_xgb_subs['Fatalities']=y2_xpred_xgb_sub\n    \n        xgb_sub = pd.concat([xgb_sub, xgb_xgb_subs], axis=0)\n        \nxgb_sub['ForecastId']= xgb_sub['ForecastId'].astype('int')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we will train a decision tree as part of our regressors"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nfrom pathlib import Path\nfrom pandas_profiling import ProfileReport\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import LabelEncoder\nimport datetime\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import cross_val_score\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset_path = Path('/kaggle/input/covid19-global-forecasting-week-4')\ntrain = pd.read_csv(dataset_path/'train.csv')\ntest = pd.read_csv(dataset_path/'test.csv')\ndtree_sub = pd.read_csv(dataset_path/'submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_profile = ProfileReport(train, title='COVID19 WEEK 4 Profiling Report', html={'style':{'full_width':True}},progress_bar=False);\ntrain_profile","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fill_state(state,country):\n    if pd.isna(state) : return country\n    return state","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Province_State'] = train.loc[:, ['Province_State', 'Country_Region']].apply(lambda x : fill_state(x['Province_State'], x['Country_Region']), axis=1)\ntest['Province_State'] = test.loc[:, ['Province_State', 'Country_Region']].apply(lambda x : fill_state(x['Province_State'], x['Country_Region']), axis=1)\ntrain['Date'] = pd.to_datetime(train['Date'],infer_datetime_format=True)\ntest['Date'] = pd.to_datetime(test['Date'],infer_datetime_format=True)\ntrain['Day_of_Week'] = train['Date'].dt.dayofweek\ntest['Day_of_Week'] = test['Date'].dt.dayofweek\ntrain['Month'] = train['Date'].dt.month\ntest['Month'] = test['Date'].dt.month\ntrain['Day'] = train['Date'].dt.day\ntest['Day'] = test['Date'].dt.day\ntrain['Day_of_Year'] = train['Date'].dt.dayofyear\ntest['Day_of_Year'] = test['Date'].dt.dayofyear\ntrain['Week_of_Year'] = train['Date'].dt.weekofyear\ntest['Week_of_Year'] = test['Date'].dt.weekofyear\ntrain['Quarter'] = train['Date'].dt.quarter  \ntest['Quarter'] = test['Date'].dt.quarter  \ntrain.drop('Date',1,inplace=True)\ntest.drop('Date',1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dtree_sub=pd.DataFrame(columns=dtree_sub.columns)\nl1=LabelEncoder()\nl2=LabelEncoder()\nl1.fit(train['Country_Region'])\nl2.fit(train['Province_State'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"countries=train['Country_Region'].unique()\nfor country in countries:\n    country_df=train[train['Country_Region']==country]\n    provinces=country_df['Province_State'].unique()\n    for province in provinces:\n            train_df=country_df[country_df['Province_State']==province]\n            train_df.pop('Id')\n            x=train_df[['Province_State','Country_Region','Day_of_Week','Month','Day','Day_of_Year','Week_of_Year','Quarter']]\n            x['Country_Region']=l1.transform(x['Country_Region'])\n            x['Province_State']=l2.transform(x['Province_State'])\n            y1=train_df[['ConfirmedCases']]\n            y2=train_df[['Fatalities']]\n            model_1=DecisionTreeClassifier()\n            model_2=DecisionTreeClassifier()\n            model_1.fit(x,y1)\n            model_2.fit(x,y2)\n            test_df=test.query('Province_State==@province & Country_Region==@country')\n            test_id=test_df['ForecastId'].values.tolist()\n            test_df.pop('ForecastId')\n            test_x=test_df[['Province_State','Country_Region','Day_of_Week','Month','Day','Day_of_Year','Week_of_Year','Quarter']]\n            test_x['Country_Region']=l1.transform(test_x['Country_Region'])\n            test_x['Province_State']=l2.transform(test_x['Province_State'])\n            test_y1=model_1.predict(test_x)\n            test_y2=model_2.predict(test_x)\n            test_res=pd.DataFrame(columns=dtree_sub.columns)\n            test_res['ForecastId']=test_id\n            test_res['ConfirmedCases']=test_y1\n            test_res['Fatalities']=test_y2\n            dtree_sub=dtree_sub.append(test_res)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Prediction and writing to submissin.csv for the competition"},{"metadata":{"trusted":true},"cell_type":"code","source":"# ENSEMBLE .................\ndtree_confirmed=dtree_sub[\"ConfirmedCases\"]\ndtree_fatal=dtree_sub[\"Fatalities\"]\nboost_confirmed = xgb_sub[\"ConfirmedCases\"]\nboost_fatal = xgb_sub[\"Fatalities\"]\ndeep_confirmed = dataframe_for_submission[\"ConfirmedCases\"]\ndeep_fatal = dataframe_for_submission[\"Fatalities\"]\ndataframe_for_submission[\"ConfirmedCases\"] = 0.1 * boost_confirmed.values +  0.70 * deep_confirmed.values + 0.20 *dtree_confirmed.values\ndataframe_for_submission[\"Fatalities\"] = 0.1 * boost_fatal.values  +  0.70 * deep_fatal.values + 0.20  * dtree_fatal.values\ndataframe_for_submission.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}