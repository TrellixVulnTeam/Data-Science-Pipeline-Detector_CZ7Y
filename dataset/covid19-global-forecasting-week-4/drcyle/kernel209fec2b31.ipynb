{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\n\nimport multiprocessing,os,datetime,time,pickle,csv,numpy,pandas,urllib\nfrom keras.preprocessing.text import Tokenizer\nfrom scipy.stats.mstats import gmean\nfrom functools import reduce\n#\n#attempt at custom loss\nfrom keras import backend as K\n\ndef mse_0(y_true, y_pred):\n    \"\"\"\n    #original vision, but numpy methods dont quite work on tensors\n    q5h=numpy.where(y_true!=0)\n    return keras.losses.mean_squared_error(y_true[q5h], y_pred[q5h])\n    \"\"\"\n    \"\"\"\n    q5f=K.eval(y_true)\n    q5h=numpy.where(q5f!=0)\n    q5i=max(q5h0[-1])+1\n    print(y_true)\n    q5f=y_true!=0\n    q5h=y_true[q5f]\n    q5i=y_pred[q5f]\n    i=1\n    while sum(y_true[0,0,-i:])==0:\n     i+=1\n    i-=1\n    \"\"\"\n    y0=K.relu(y_true[:,:,:], alpha=0.0, max_value=0.00001, threshold=0.000001)/.00001\n    #yt=K.dot(y_true[0,:,:],K.transpose(y0))\n    #yp=K.dot(y_pred[0,:,:],K.transpose(y0))\n    yp=y_pred*y0\n    #print(K.int_shape(y0))\n    #print(K.int_shape(y_true))\n    #print(K.int_shape(y_true))\n    #print(K.int_shape(yp))\n    #return K.mean(K.square(y_true - y_pred))\n    #return K.mean(K.square(y_true - y_pred),axis=-1)\n    #return keras.losses.mean_squared_error(y_true,yp)\n    return keras.losses.categorical_crossentropy(y_true,yp)\n\n\n\"\"\"\n#illustration of keras backend functions\ntrue = K.variable(np.array([[1, 1, 0, 0, 0, 0, 2.0, 3.0]]), dtype='float32')\npred = K.variable(np.array([[0.6, 0.1, 0.2, 0.05, 0.05, 0.0]]), dtype='float32')\n\nK.eval(odds_loss(true, pred))\n\"\"\"\n\n#exec(open('fc3b','r').read())\n#exec(open('fc5b','r').read())\n#exec(open('fc5c','r').read())\n#exec(open('fc5d','r').read())\n\nclass preproc:\n  def __init__(self, csv):\n    df = pandas.read_csv(csv)\n    self.df = df\n    df.Date=df.Date.apply(lambda x: datetime.datetime.strptime(x,'%Y-%m-%d'))\n    df0= df.iloc[:,2]+';'+df.iloc[:,1].fillna(value='')\n    df['region']=df0\n    #self.datepen=df['Date'].unique()[-2]\n    #self.datefin=df['Date'].unique()[-1]\n    #label encode via tokenizer\n    #from keras.preprocessing.text import Tokenizer\n    #from keras.utils import to_categorical\n    t0 = Tokenizer(filters='',split='|')\n    self.t0=t0\n    t0.fit_on_texts(df.region.unique())\n    dfi=t0.texts_to_sequences(df.region)\n    df['hot_region']=numpy.array(dfi)\n\n    #def func0(self):\n    #clean up if same data for casea nd fatilites repeated; motivated from italy\n    #so this is necessary before operation with daily differences in cases, where else it would produe zeros and numpy.nan upon division \n    #df=self.df\n    df4=df.iloc[:,4]\n    df5=df.iloc[:,5]\n    #dh=df[((df4==df4.shift(1)) & (df5==df5.shift(1)))]\n    #df=df[~((df4==df4.shift(1)) & (df5==df5.shift(1)))]\n    #dg=df[((df4==df4.shift(1)))]\n    df=df[~((df4==df4.shift(1)))]\n    #df=df[~((df4==df4.shift(1)) & df.iloc[:,4]>100)] #exclude cardinality <100 for first condition\n    df=df.reset_index().drop('index',axis=1)\n    #self.df=df\n    #resetting index is crucial to further shifting operations\n    #\n    #difference\n    for j in df.columns[[4,5]]:\n     dfi=df[j] - df[j].shift(1)\n     dfi[df.iloc[:,3]=='2020-01-22']=0\n     df['diff'+j[:1]]=dfi\n    #the purpose of next line is data correction\n    df=df[df.diffC>0]\n    df=df.reset_index().drop('index',axis=1)\n    self.df=df\n    \n    #Fatality ratio\n    dfi=df.Fatalities/df.ConfirmedCases\n    dfi[dfi==numpy.inf]=numpy.nan\n    df['F_C']=dfi\n    dfi=df.diffF/df.diffC\n    dfi[dfi==numpy.inf]=numpy.nan\n    df['diffF_C']=dfi\n    df['diffF_C_']=dfi.rolling(window=k,center=False).mean()\n    df['logdiffF_C_']=numpy.log(df['diffF_C_'])\n    #self.df=df\n\n  def func0(self,k):\n    df=self.df\n    self.k=k\n    #ratio to prior confirmed\n    #initially this was done on total confirmed; however this masks the underlying numbers via ever incrasing tally; so it is done on daily difference \n    #for j in df.columns[-2:]:\n    #for i in [4,5]:\n    for j in ['diffC',]:\n     #for k in [1,3,5]:\n     #for k in [1,3]:\n     #for k in [1,self.k]:\n     for k in [self.k]:\n      datek=sorted(df['Date'].unique())[k]\n      dfi=df[j] / df[j].shift(k)\n      #this sets first entry for each region to nan insetad of ratio with last entry of prior region\n      dfi[dfi==numpy.inf]=numpy.nan\n      dfi[dfi==0]=numpy.nan\n      dfi[df.Date<datek]=numpy.nan\n      #if k>1:\n      # df['diffC_{}'.format(k)]=dfi\n      dfj=(df.Date-df.Date.shift(k)).apply(lambda x: x.days)\n      #dfi= dfi**(k/dfj)\n      dfi= dfi**(1/dfj)\n      df['exp{}'.format(k)+j]=dfi\n      dfi = dfi.rolling(window=k,center=False).apply(gmean)\n      df['exp{}_'.format(k)+j]=dfi\n      #dfj=1.0/(df.Date-df.Date.shift(k)).apply(lambda x: x.days)\n      #dfi=dfi.apply(lambda x: x**(1.0/k))\n      #dfi= dfi**dfj\n      df['logexp{}_'.format(k)+j]=numpy.log(dfi)\n      \n    \n    #computing exp5_diffC.cummax()\n    ##dgi=dg[[*dg.columns[2:4]]+[*dg.columns[-2:-1]]]\n    ##dgi=dgi[[*dgi.columns[[*[0,-1]]]]]\n    ##dgi=dg[[*dg.columns[2:4]]+[*dg.columns[-2:-1]]].groupby(dg.columns[2]).max()\n    ##dgi=dgi.groupby(dg.columns[2]).max()\n    ##dgi=dg[[*dg.columns[[*[2,3,-2]]]]]\n    #dgi=dg[[*dg.columns[[*[3,4,-2]]]]]\n    dfi=df[[*df.columns[[*[6,7,-2]]]]]\n    ##dgi=pandas.merge(dg,dgi,how='left',on='hot_region')\n    ##dgi=pandas.merge(dg,dgi.groupby(dg.columns[2]).max(),how='left',on='hot_region')\n    ##dg['expmaxratio']=dg.iloc[:,-3]/dg.iloc[:,-1]\n    ##dg['expmaxratio']=dgi.iloc[:,-2]/dgi.iloc[:,-1]\n    ##dgi=dgi.groupby(dg.columns[2]).cummax()\n    ##dg['cummax{}_ratio'.format(self.k)]=dg.iloc[:,-2]/dgi.iloc[:,-1]\n    ##dg['cummmax']=dgi.groupby(dg.columns[2]).cummax().iloc[:,-1]\n    #dg['cummmax']=dgi.groupby(dg.columns[3]).cummax().iloc[:,-1]\n    #dg['cummax{}_ratio'.format(self.k)]=dg['exp{}_'.format(k)+j]/dg.cummmax\n    df['cummmax']=dfi.groupby(df.columns[6]).cummax().iloc[:,-1]\n    df['cummax{}_ratio'.format(self.k)]=df['exp{}_'.format(k)+j]/df.cummmax\n\n    #k=5\n    #k=3\n    #k is carried over from last logexpk column\n    #nonnull logexp rows and index reset timed after cummax computation and before time series\n\n    #def func2(self):\n    #df=self.df\n    #k=self.k\n    #model fitting columns\n    #dg=df[[*df.columns[4:10]]+[*df.columns[12:]]]\n    #dg=df[[*df.columns[4:]]]\n    #dg=df[[*df.columns[[*[3,4,5]]]]+[*df.columns[7:]]]\n    dg=df[[*df.columns[3:]]]\n   \n    #nonnull logexp5_diffC rows\n    #dg=dg.dropna(axis=0)\n    #the above only drops if row isnull\n    dg=dg[~dg['logexp{}_diffC'.format(k)].isnull()]\n    dg=dg.reset_index() #.drop('index',axis=1)\n    \n    #specify number of time series columns\n    self.tscol=3\n\n    #diff_f/c arranged as timeseries\n    for i in range(k+0):\n     #dfi[di['bool']==2]= dg['logexp5_diffC'].shift(6-j)[di['bool']==2]\n     dg['fc'+str(i+1)]= dg['diffF_C_'].shift(k-i)\n     \n    \"\"\"\n    #logexp1_diffC arranged as timeseries\n    for i in range(k+0):\n     #dfi[di['bool']==2]= dg['logexp1_diffC'].shift(6-j)[di['bool']==2]\n     dg['l'+str(i+1)]= dg['logexp1_diffC'].shift(k-i)\n    \"\"\"\n    \n    #exp5.cummax arranged as timeseries\n    for i in range(k+0):\n     #dfi[di['bool']==2]= dg['logexp5_diffC'].shift(6-j)[di['bool']==2]\n     #dh['m'+str(i+1)]= dh['cummaxk_ratio'].shift(k-i)\n     dg['m'+str(i+1)]= dg['cummax{}_ratio'.format(k)].shift(k-i)\n     \n    #logexp5 arranged as timeseries\n    for i in range(k+0):\n     #dfi[di['bool']==2]= dg['logexp5_diffC'].shift(6-j)[di['bool']==2]\n     dg['k'+str(i+1)]= dg['logexp{}_diffC'.format(k)].shift(k-i)\n    \n    #dgi=pandas.merge(dg,dgi,how='right')\n    \n    #dbi=(dh['index'].rolling(window=6).min()+5==dh['index'])\n    #dgi=di[dbi==True] #this doesn't work\n    dbi=(dg['index'].rolling(window=k+1).min()+k==dg['index'])\n    dh=dg[dbi]\n    #self.dh=dh\n    self.dh=dh\n    self.dg=dg\n    #exec(\"self.%s = %d\" % ('dh{}'.format(k),dh))\n\n  #def func1(self,modelcl=cl0,col=3,pat0=6,datefin0=None):\n  def func1(self,modelcl=None,col=3,pat0=6,datefin0=None):\n    self.col=col\n    x000=self.dh.iloc[:,5] #country\n    #y0=d.dh.iloc[:,15].values.reshape(-1,1)\n    #y0=d.dh.iloc[:,[15,17]].values.reshape(-1,2)\n    #y0=d.dh.iloc[:,[14,16]].values.reshape(-1,2)\n    y0=self.dh.loc[:,['logexp{}_diffC'.format(k),'cummax{}_ratio'.format(k)]].values.reshape(-1,2)\n    \n    #in this first attempt we are clustering on raw training data itself just to see \n    sc_X = StandardScaler()\n    #y3 = numpy.concatenate([dh.iloc[:,:3],y2],axis=1)\n    x2=x0.reshape(-1,col*self.k)\n    x2 = sc_X.fit_transform(x2)\n    self.sc_X=sc_X\n    \n    km = KMeans(n_clusters=k0)\n    #km.fit(v3.reshape(-1,4))\n    #km.predict(v3.reshape(-1,4))\n    km.fit(x2) #.reshape(-1,1))\n    km.predict(x2) #.reshape(-1,1))\n    #for i in range(k0):\n    #print(labels.tolist().count(i))\n    labels = km.labels_ +1\n    self.km=km\n    \n    #x001=d.dh.labels\n    #x00=to_categorical(x000)\n    x00=to_categorical(x000,num_classes=self.t0.document_count+1)\n    x01=to_categorical(labels,num_classes=self.km.n_clusters+1)\n    #x01=to_categorical(x001)\n    #x00=numpy.concatenate([x00,x01],axis=1)\n    \n    outputs=y0\n    #cl1=cl0(inputs,outputs)\n    cl1=modelcl(inputs,outputs)\n    self.cl1=cl1\n    cl1.fit0(pat=pat0)\n    cl1.compile()\n    #cl1.fit()\n    cl1.xgbpipe()\n\n  def func2(self,model=None,datefin0=None,rounds=30):\n    k=self.k\n    dg=self.dg\n    #dh=self.dh\n    if not hasattr(self,'dj'):\n     self.dj=self.df.iloc[:,3:]\n     self.dj=self.dj.reset_index() #.drop('index',axis=1)\n    dj=self.dj\n    if datefin0==None:\n      #datefin=dj['Date'].unique().max()\n      datenew=dj['Date'].unique().max()\n    else:\n      #datefin=datefin0\n      datenew=datefin0\n    #di=dg[dg.Date==datefin]\n    #di=dg[dg.Date<=datefin]\n    #di=dg[dg.Date==datenew].copy()\n    #\n    #dii=dg.groupby('hot_region')[['index','Date']].max()\n    dih=dg.groupby('hot_region')[['index']].max()\n    #dii=d.dg['index'].isin(dih.iloc[:,0].values)\n    dii=dg['index'].isin(dih['index'].values)\n    for i in range(rounds):\n     if i==0:\n      #australian capital territory shows di.Date.max() might be omitted for some regions due to identicality omission prior\n      di=dg[dii].copy()\n     else:\n      di=dg.loc[dg.Date==datenew].copy()\n     #strangely di does not retain its memory of x3 after the second loop unless prior line included within loop\n     #also for reference if not already done di needs to be in sorted order by hot_region, Date\n     di.iloc[:,-self.tscol*k:]=di.iloc[:,-self.tscol*k:].shift(-1,axis=1)\n     #update fc5,l5,m5,k5\n     #di['fc{}'.format(k)]=di['diffF_C']\n     #di['l{}'.format(k)]=di['logexp1_diffC']\n     #di['m{}'.format(k)]=di['cummax{}_ratio'.format(k)]\n     #di['k{}'.format(k)]=di['logexp{}_diffC'.format(k)]\n     di.loc[:,'fc{}'.format(k)]=di['diffF_C_']\n     #di.loc[:,'l{}'.format(k)]=di['logexp1_diffC']\n     di.loc[:,'m{}'.format(k)]=di['cummax{}_ratio'.format(k)]\n     di.loc[:,'k{}'.format(k)]=di['logexp{}_diffC'.format(k)]\n     if model!=None:\n       \"\"\"\n       if hot_region!=None:\n        for j in hot_region:\n         pass\n         x00=numpy.zeros((1,self.t0.document_count))\n         x00[0,j]=1\n         model1=model[j]\n       else:\n        pass\n        #model = load_model('model.h5')\n        model1 = load_model(model)\n       \"\"\"\n       #x000=d.di.iloc[:,5] #country\n       x000=di.iloc[:,5] #country\n       #x00=to_categorical(x000,num_classes=304+1)\n       x00=to_categorical(x000,num_classes=self.t0.document_count+1)\n       x2 = self.sc_X.transform(x2)\n       labels = self.km.predict(x2)+1\n       x01=to_categorical(labels,num_classes=self.km.n_clusters+1)\n       #x01=to_categorical(x001)\n       #x00=numpy.concatenate([x00,x01],axis=1)\n \n       inputs=[x00, x01,x0]\n       #inputs=[x00,x0]\n       #x3=cl1.model2.predict(inputs)\n       #x3=model.predict(inputs).reshape(-1,)\n       x3=model.predict(inputs)[:,0].reshape(-1,)\n       #di.iloc[:,14]=x3\n       di.loc[:,'logexp{}_diffC'.format(k)]=x3\n       #\n       #di.iloc[:,13]=numpy.exp(x3)\n       di.loc[:,'exp{}_diffC'.format(k)]=numpy.exp(x3)\n       #dj2.loc[dji,'exp{}_diffC'.format(k)]=numpy.exp(x3)\n       #di.loc[:,'exp{}_diffC'.format(k)]=dj2['exp{}diffC'.format(k)].rolling(window=k,center=False).apply(gmean)[dji]\n       #di.cummmax=di[[*[di.columns[13,15]]]].max(axis=1)\n       #di.cummmax=di[['cummax{}_ratio'.format(k),'cummmax']].max(axis=1)\n       di.cummmax=numpy.maximum(di.cummmax,di['exp{}_diffC'.format(k)])\n       di.loc[:,'cummax{}_ratio'.format(k)]=di['exp{}_diffC'.format(k)]/di.cummmax\n       #\n       #datenew=datefin+numpy.timedelta64(1,'D')\n       datenew=datenew+numpy.timedelta64(1,'D')\n       self.datenew=datenew\n       di.Date=datenew\n       #\n       #dj2 defined for computation\n       #dj=pandas.concat([dj.iloc[:,3:],di.iloc[:,1:15]],axis=0)\n       #dj2=pandas.concat([dj,di.iloc[:,1:18]],axis=0)\n       dj2=pandas.concat([dj,di.iloc[:,1:-self.tscol*k]],axis=0)\n       #dj=dj.sort_values([dj.columns[4],dj.columns[0]])\n       dj2=dj2.sort_values(['hot_region','Date'])\n       dji=dj2.Date==datenew\n       #\n       die=numpy.exp(x3)**k\n       for j in range(1,k):\n         die/=dj2['exp{}diffC'.format(k)].shift(j)[dji]\n       di.loc[:,'exp{}diffC'.format(k)]=die\n       #dj2.loc[dji,'exp{}diffC'.format(k)]=die\n       #\n       ##djh=(dj.diffC.shift(k)[dji])*(dj.exp5_diffC[dji])\n       #djj=(dj2.Date-dj2.Date.shift(k)).apply(lambda x: x.days)\n       ##djh=(dj2.diffC.shift(k)[dji])*(dj2.exp5_diffC[dji])**djj[dji]\n       ##djh=(dj2.diffC.shift(k)[dji])*(dj2.loc[dji,'exp{}_diffC'.format(k)])**djj[dji]\n       #djh=(dj2.diffC.shift(k)[dji])*(dj2.loc[dji,'exp{}diffC'.format(k)])**djj[dji]\n       ##dj.diffC[dji]=djh\n       djh=(dj2.diffC.shift(1)[dji])*(dj2.loc[dji,'exp{}_diffC'.format(k)])\n       #dj.ConfirmedCases[dji]=dj.ConfirmedCases.shift(1)[dji]+dj.diffC[dji]\n       #dj.exp1_diffC[dji]=dj.diffC[dji]/dj.diffC.shift(1)[dji]\n       #dj['logexp{}_diffC'.format(1)][dji]=numpy.log(dj.exp1_diffC[dji])\n       #dj.loc[dji,'diffC']=djh\n       #dj.loc[dji,'ConfirmedCases']=dj.ConfirmedCases.shift(1)[dji]+dj.diffC[dji]\n       #dj.loc[dji,'exp1_diffC']=dj.diffC[dji]/dj.diffC.shift(1)[dji]\n       #dj.loc[dji,'logexp{}_diffC'.format(1)]=numpy.log(dj.exp1_diffC[dji])\n       di.diffC=djh\n       di.ConfirmedCases=dj2.ConfirmedCases.shift(1)[dji]+djh\n       #di.exp1_diffC=djh/dj2.diffC.shift(1)[dji]\n       #di.logexp1_diffC=numpy.log(di.exp1_diffC)\n       \"\"\"\n       diffF_C\n       diffF\n       Fatalities\n       \"\"\"\n       dg=pandas.concat([dg,di],axis=0)\n       self.dg=dg.sort_values(['hot_region','Date'])\n       dj=pandas.concat([dj,di.iloc[:,1:-self.tscol*k]],axis=0)\n       self.dj=dj\n     #self.di=di\n     #self.dj=dj\n\n\nclass postproc:\n  def __init__(self, csv='/kaggle/input/covid19-global-forecasting-week-4/train.csv'):\n    self.df0 = pandas.read_csv(csv)\n    self.df0.Date=self.df0.Date.apply(lambda x: datetime.datetime.strptime(x,'%Y-%m-%d'))\n    self.datefin=self.df0['Date'].unique().max()\n    t0 = Tokenizer(filters='',split='|')\n    self.t0=t0\n    dfa= self.df0.iloc[:,2]+';'+self.df0.iloc[:,1].fillna(value='')\n    #t0.fit_on_texts(df0.region.unique())\n    #self.t0a=t0.texts_to_sequences(df.region)\n    t0.fit_on_texts(dfa.unique())\n    dfb=self.t0.texts_to_sequences(dfa)\n    self.df0['hot_region']=numpy.array(dfb)\n    self.df0=self.df0.drop([*self.df0.columns[:3]],axis=1)\n    #self.t0a=t0.texts_to_sequences(dfa)\n    #check d.t0a equals df.hot_region\n    #\n  def func3(self,csv='/kaggle/input/covid19-global-forecasting-week-4/test.csv'):\n    df1 = pandas.read_csv(csv)\n    self.df1=df1\n    #self.df1=df1\n    df1.Date=df1.Date.apply(lambda x: datetime.datetime.strptime(x,'%Y-%m-%d'))\n    dfa= df1.iloc[:,2]+';'+df1.iloc[:,1].fillna(value='')\n    #self.datepen=df['Date'].unique()[-2]\n    #self.datefin=df['Date'].unique()[-1]\n    #label encode via tokenizer\n    #from keras.preprocessing.text import Tokenizer\n    #from keras.utils import to_categorical\n    dfb=self.t0.texts_to_sequences(dfa)\n    df1['hot_region']=numpy.array(dfb)\n    self.df1=df1.drop([*df1.columns[1:3]],axis=1)\n    #datek=sorted(df['Date'].unique())[k]\n  def func4(self,csv='/kaggle/input/submissions/sub0.csv'):\n    df = pandas.read_csv(csv)\n    self.df = df\n    self.df.Date=self.df.Date.apply(lambda x: datetime.datetime.strptime(x,'%Y-%m-%d'))\n    datefin=self.datefin\n    dfi=self.df0.Date<=datefin\n    dfj=self.df.Date>datefin\n    #self.df1 =df1.merge(self.df[['Date','hot_region','ConfirmedCases','Fatalities']], how='left',  on=['Date','hot_region'])\n    #self.df2 =self.df1.merge(self.df.loc[dfi,['Date','hot_region','ConfirmedCases','Fatalities']], how='left',  on=['Date','hot_region'])\n    #self.df3 =self.df1.merge(self.df.loc[dfj,['Date','hot_region','ConfirmedCases','Fatalities']], how='left',  on=['Date','hot_region'])\n    self.df3 =self.df1.merge(self.df0.loc[dfi,:], how='left',  on=['Date','hot_region'])\n    self.df4 =self.df1.merge(self.df.loc[dfj,:].drop(['Unnamed: 0','Fatalities'],axis=1), how='left',  on=['Date','hot_region'])\n    self.df3.update(self.df4)\n  def func5(self,csv='/kaggle/input/submissions/sub2a.csv'):\n    self.df2 = pandas.read_csv(csv)\n    dfj=self.df1.Date<=self.datefin\n    self.df2.loc[dfj,:]=numpy.nan\n    #self.df2 = self.df2.drop('ConfirmedCases',axis=1)\n    self.df3.update(self.df2)\n    self.df3=self.df3.drop([*self.df3.columns[1:3]],axis=1)\n\n\n\nif not __name__ == '__main__':\n #k=3\n k=5\n #d=preproc(csv='train.csv')\n d=preproc(csv='/kaggle/input/covid19-global-forecasting-week-4/train.csv')\n d.func0(k)\n #d.func1(modelcl=cl0,col=2,pat0=12)\n #d.func2(model=cl1.model2,datefin0=None,rounds=30)\n #d.func2(model=cl1,datefin0=None,rounds=30) #this is just cl1.predict method, which is chained model1 predict followed by xgboost\n #d.func2(model=d.cl1.model0,datefin0=None,rounds=2)\n #d.func2(model=d.cl1.model2,datefin0=None)\n #d.func2(model=d.cl1.model0,datefin0=None)\n #url='https://github.com/drcyle/kaggle/blob/master/sub1.csv'\n #url='https://raw.githubusercontent.com/drcyle/kaggle/master/sub1.csv'\n #sub=pandas.read_csv(url)\n #sub.drop(sub.columns[0],axis=1).to_csv('submission.csv',index=False)\n \"\"\"\n df0.read_csv('sub4.csv')\n df1=df0[['Date','hot_region','ConfirmedCases','Fatalities']]\n df1.to_csv('sub0.csv')\n \"\"\"\n e=postproc()\n e.func3()\n e.func4()\n e.func5()\n \"\"\"\n e.df3.ConfirmedCases[140]=0\n e.df3.ConfirmedCases[174]=0\n e.df3.ConfirmedCases[175]=0\n e.df3.ConfirmedCases[181]=0\n e.df3.ConfirmedCases[182]=0\n \"\"\"\n e.df3.to_csv('submission.csv',index=False)\n    \n\"\"\"\nERROR: Could not parse '' into expected type of Double (Line 140, Column 5)\nERROR: Could not parse '' into expected type of Double (Line 140, Column 6)\nERROR: Could not parse '' into expected type of Double (Line 174, Column 5)\nERROR: Could not parse '' into expected type of Double (Line 174, Column 6)\nERROR: Could not parse '' into expected type of Double (Line 175, Column 5)\nERROR: Could not parse '' into expected type of Double (Line 175, Column 6)\nERROR: Could not parse '' into expected type of Double (Line 181, Column 5)\nERROR: Could not parse '' into expected type of Double (Line 181, Column 6)\nERROR: Could not parse '' into expected type of Double (Line 182, Column 5)\nERROR: Could not parse '' into expected type of Double (Line 182, Column 6)\n\"\"\"\n\nif __name__ == '__main__':\n #k=3\n subf=pandas.read_csv('/kaggle/input/submissions/sub3.csv')\n subf.to_csv('submission.csv',index=False)\n subf=subf.drop(['Unnamed: 0'],axis=1)\n subf.ForecastId=subf.ForecastId.astype(int)\n subf.to_csv('submission.csv',index=False)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}