{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":false},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os, gc\nimport lightgbm as lgb\nimport xgboost as xgb\nimport catboost as ctb\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom datetime import date, datetime, timedelta\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\nfrom sklearn.neighbors import KNeighborsRegressor, NearestNeighbors\nfrom sklearn.linear_model import Ridge\nfrom scipy.optimize import nnls\npd.set_option('display.max_columns', 10)\npd.set_option('display.max_rows', 10)\nnp.set_printoptions(precision=6, suppress=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# note: data update 2020-04-15, wmc, full\nmname = 'gbt5f'\npath = '/kaggle/input/gbt5fx/'\npathk = '/kaggle/input/covid19-global-forecasting-week-4/'\nnhorizon = 30\nskip = 0\nkv = [6,11]\nval_scheme = 'forward'\npw = 'week3'\nprev_test = False\nblend = True\ntrain_full = True\nsave_data = False\n\nbooster = ['lgb','xgb','ctb','rdg']\n# booster = ['cas']\n\nblender = ['nq0j_updated','kaz0z']\n\n# for nq final day adjustment\n# when validating make this the first validation day\n# for final fitting with nhorizon = 30, make it today\n# TODAY = '2020-04-08'\nTODAY = '2020-04-15'\n\nteams = []\n\n# if using updated daily data, also update time-varying external data\n# in COVID-19, covid-19-data, covid-tracking-data, git pull origin master \n# ecdc wget https://opendata.ecdc.europa.eu/covid19/casedistribution/csv\n# weather: https://www.kaggle.com/davidbnn92/weather-data/output?scriptVersionId=31103959\n# google trends: pytrends0d.ipynb\n# data scraped from https://www.worldometers.info/coronavirus/, including past daily snapshots\n# download html for final day (country and us states) at 22:00 UTC and run wm0d.ipynb first","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"code","source":"train = pd.read_csv(pathk+'train.csv')\n\n# helper lists\nynames = ['ConfirmedCases', 'Fatalities']\nny = len(ynames)\ncp = ['Country_Region','Province_State']\ncpd = cp + ['Date']\n\n# from kaz\ntrain[\"key\"] = train[[\"Province_State\",\"Country_Region\"]].apply(lambda row: \\\n                                                str(row[0]) + \"_\" + str(row[1]),axis=1)\n\n# fill missing provinces with blanks, must also do this with external data before merging\n# need to fillna so groupby works\ntrain[cp] = train[cp].fillna('')\ntrain = train.sort_values(cpd).reset_index(drop=True)\n\ntrain","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# use previous week test set in order to compare with previous week leaderboard\nif prev_test:\n    test = pd.read_csv('../'+pw+'/test.csv')\n    ss = pd.read_csv('../'+pw+'/submission.csv')\nelse:\n    test = pd.read_csv(pathk+'test.csv')\n    ss = pd.read_csv(pathk+'submission.csv')\n\n# from kaz\ntest[\"key\"] = test[[\"Province_State\",\"Country_Region\"]].apply(lambda row: \\\n                                            str(row[0]) + \"_\" + str(row[1]),axis=1)\n\ntest[cp] = test[cp].fillna('')\ntest","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# tmax and dmax are the last day of training\ntmax = train.Date.max()\ndmax = datetime.strptime(tmax,'%Y-%m-%d').date()\nprint(tmax, dmax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"fmax = test.Date.max()\nfdate = datetime.strptime(fmax,'%Y-%m-%d').date()\nfdate","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"tmin = train.Date.min()\nfmin = test.Date.min()\ntmin, fmin","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"dmin = datetime.strptime(tmin,'%Y-%m-%d').date()\nprint(dmin)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# prepare for concat\ntrain = train.merge(test[cpd+['ForecastId']], how='left', on=cpd)\ntrain['ForecastId'] = train['ForecastId'].fillna(0).astype(int)\ntrain['y0_pred'] = np.nan\ntrain['y1_pred'] = np.nan\n\ntest['Id'] = test.ForecastId + train.Id.max()\ntest['ConfirmedCases'] = np.nan\ntest['Fatalities'] = np.nan\n# use zeros here instead of nans so monotonic adjustment fills final dates if necessary\ntest['y0_pred'] = 0.0\ntest['y1_pred'] = 0.0","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# concat non-overlapping part of test to train for feature engineering\nd = pd.concat([train,test[test.Date > train.Date.max()]],sort=True).reset_index(drop=True)\nd","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"(dmin + timedelta(30)).isoformat()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"d['Date'].value_counts().std()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# fill missing province with blank, must also do this with external data before merging\nd[cp] = d[cp].fillna('')\n\n# create single location variable\nd['Loc'] = d['Country_Region'] + ' ' + d['Province_State']\nd['Loc'] = d['Loc'].str.strip()\nd['Loc'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# drop new regions in order to compare with previous week leaderboard\nif prev_test:\n    test2 = pd.read_csv('../'+pw+'/test.csv')\n    test2[cp] = test2[cp].fillna('')\n    test2 = test2.drop(['ForecastId','Date'], axis=1).drop_duplicates()\n    test2\n    d = d.merge(test2, how='inner', on=cp)\n    d.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# log1p transform both targets\nyv = []\nfor i in range(ny):\n    v = 'y'+str(i)\n    d[v] = np.log1p(d[ynames[i]])\n    yv.append(v)\n    \n# enforce monotonicity, roughly cleans some data errors\nd[yv] = d.groupby(cp)[yv].cummax()\n\nprint(d[yv].describe())","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# without monotonicity, small changes can make a big difference\n#                 y0            y1\n# count  24174.000000  24174.000000\n# mean       2.210124      0.582766\n# std        2.789791      1.329384\n# min        0.000000      0.000000\n# 25%        0.000000      0.000000\n# 50%        0.693147      0.000000\n# 75%        4.442651      0.693147\n# max       11.993993      9.813563","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# merge predictions from other teams\n# right now these are based only on public lb training set < 2020-03-26\n# need to also use predictions from full set\n# teams = ['dott0b','psi0b','cpmp0b']\ntfeats = [[],[]]\nfor ti in teams:\n    td = pd.read_csv('sub/'+ti+'.csv')\n    t = ti[:-2]\n    print(td.head(), td.shape, ti, t)\n    td[t+'0'] = np.log1p(td.ConfirmedCases)\n    td[t+'1'] = np.log1p(td.Fatalities)\n    td.drop(ynames, axis=1, inplace=True)\n    if 'ForecastId' in list(td.columns):\n        d = d.merge(td, how='left', on='ForecastId')\n    else:\n        d = d.merge(td, how='left', on='Id')\n    print(d.shape)\n    tfeats[0].append(t+'0')\n    tfeats[1].append(t+'1')\ntf2 = len(tfeats[0])\nprint(tfeats, tf2)\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# sort by location then date\nd = d.sort_values(['Loc','Date']).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"d['Country_Region'].value_counts(dropna=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"d['Province_State'].value_counts(dropna=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"d.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# data scraped from https://www.worldometers.info/coronavirus/, including past daily snapshots\n# download html for final day (country and us states) at 22:00 UTC and run wm0d.ipynb first\nwmf = []\nwm = pd.read_csv(path+'wmc.csv')\nwm[cp] = wm[cp].fillna('')\n# 12 new features, all log1p transformed, must be lagged\nwmf = [c for c in wm.columns if c not in cpd]\n\n# since wm leads by a day, shift the date to make it contemporaneous\nwmax = wm.Date.max()\nwmax = datetime.strptime(wmax,'%Y-%m-%d').date()\nwoff = (dmax - wmax).days\nprint(dmax, wmax, woff)\nwm1 = wm.copy()\nwm1['Date'] = (pd.to_datetime(wm1.Date) + timedelta(woff)).dt.strftime('%Y-%m-%d')\n\nwm1.Date.value_counts()[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"wm1['Date'].max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"d = d.merge(wm1, how='left', on=cpd)\nprint(d.shape)\nd[wmf].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# google trends\ngt = pd.read_csv(path+'google_trends.csv')\ngt[cp] = gt[cp].fillna('')\ngt","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# since trends data lags behind a day or two, shift the date to make it contemporaneous\ngmax = gt.Date.max()\ngmax = datetime.strptime(gmax,'%Y-%m-%d').date()\ngoff = (dmax - gmax).days\nprint(dmax, gmax, goff)\ngt['Date'] = (pd.to_datetime(gt.Date) + timedelta(goff)).dt.strftime('%Y-%m-%d')\ngt['google_covid'] = gt['coronavirus'] + gt['covid-19'] + gt['covid19']\ngt.drop(['coronavirus','covid-19','covid19'], axis=1, inplace=True)\ngoogle = ['google_covid']\ngt","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"d = d.merge(gt, how='left', on=['Country_Region','Province_State','Date'])\nd","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"d['google_covid'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# merge country info\ncountry = pd.read_csv(path+'covid19countryinfo2.csv')\n# country[\"pop\"] = country[\"pop\"].str.replace(\",\",\"\").astype(float)\ncountry","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"country.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"d.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# first merge by country\nd = d.merge(country.loc[country.medianage.notnull(),['country','pop','testpop','medianage']],\n            how='left', left_on='Country_Region', right_on='country')\nd","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# then merge by province\nc1 = country.loc[country.medianage.isnull(),['country','pop','testpop']]\nprint(c1.shape)\nc1.columns = ['Province_State','pop1','testpop1']\n# d.update(c1)\nd = d.merge(c1,how='left',on='Province_State')\nd.loc[d.pop1.notnull(),'pop'] = d.loc[d.pop1.notnull(),'pop1']\nd.loc[d.testpop1.notnull(),'testpop'] = d.loc[d.testpop1.notnull(),'testpop1']\nd.drop(['pop1','testpop1'], axis=1, inplace=True)\nprint(d.shape)\nprint(d.loc[(d.Date=='2020-03-25') & (d['Province_State']=='New York')])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# testing data time series, us states only, would love to have this for all countries\nct = pd.read_csv(path+'states_daily_4pm_et.csv')\nsi = pd.read_csv(path+'states_info.csv')\nsi = si.rename(columns={'name':'Province_State'})\nct = ct.merge(si[['state','Province_State']], how='left', on='state')\nct['Date'] = ct['date'].apply(str).transform(lambda x: '-'.join([x[:4], x[4:6], x[6:]]))\nct.loc[ct.Province_State=='US Virgin Islands','Province_State'] = 'Virgin Islands'\nct.loc[ct.Province_State=='District Of Columbia','Province_State'] = 'District of Columbia'\npd.set_option('display.max_rows', 20)\nct\n# ct = ct['Date','state','total']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"ckeep = ['positive','negative','totalTestResults']\nfor c in ckeep: ct[c] = np.log1p(ct[c])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"d = d.merge(ct[['Province_State','Date']+ckeep], how='left',\n            on=['Province_State','Date'])\nd","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# weather data from from davide bonine\nw = pd.read_csv(path+'training_data_with_weather_info_week_4.csv')\nw.drop(['Id','ConfirmedCases','Fatalities','country+province','day_from_jan_first'], axis=1, inplace=True)\nw[cp] = w[cp].fillna('')\nwf = list(w.columns[5:])\nw","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"w.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# replace values\nw['ah'] = w['ah'].replace(to_replace={np.inf:np.nan})\nw['wdsp'] = w['wdsp'].replace(to_replace={999.9:np.nan})\nw['prcp'] = w['prcp'].replace(to_replace={99.99:np.nan})\nw.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"w[['Country_Region','Province_State']].nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"w[['Country_Region','Province_State']].drop_duplicates().shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# since weather data may lag behind a day or two, adjust the date to make it contemporaneous\nwmax = w.Date.max()\nwmax = datetime.strptime(wmax,'%Y-%m-%d').date()\nwoff = (dmax - wmax).days\nprint(dmax, wmax, woff)\nw['Date'] = (pd.to_datetime(w.Date) + timedelta(woff)).dt.strftime('%Y-%m-%d')\nw","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# merge Lat and Long for all times and the time-varying weather data based on date\nd = d.merge(w[cp+['Lat','Long']].drop_duplicates(), how='left', on=cp)\nw.drop(['Lat','Long'],axis=1,inplace=True)\nd = d.merge(w, how='left', on=cpd)\nd","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# combine ecdc and nytimes data as extra y0 and y1\necdc = pd.read_csv(path+'ecdc.csv', encoding = 'latin')\necdc","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# https://opendata.ecdc.europa.eu/covid19/casedistribution/csv\necdc['Date'] = pd.to_datetime(ecdc[['year','month','day']]).dt.strftime('%Y-%m-%d')\necdc = ecdc.rename(mapper={'countriesAndTerritories':'Country_Region'}, axis=1)\necdc['Country_Region'] = ecdc['Country_Region'].replace('_',' ',regex=True)\necdc['Province_State'] = ''\necdc['cc'] = ecdc.groupby(cp)['cases'].cummax()\necdc['extra_y0'] = np.log1p(ecdc.cc)\necdc['cd'] = ecdc.groupby(cp)['deaths'].cummax()\necdc['extra_y1'] = np.log1p(ecdc.cd)\necdc = ecdc[cpd + ['extra_y0','extra_y1']]\necdc[::63]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"dmax","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"ecdc = ecdc[(ecdc.Date >= '2020-01-22')]\necdc","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# https://github.com/nytimes/covid-19-data\nnyt = pd.read_csv(path+'us-states.csv')\nnyt['extra_y0'] = np.log1p(nyt.cases)\nnyt['extra_y1'] = np.log1p(nyt.deaths)\nnyt['Country_Region'] = 'US'\nnyt = nyt.rename(mapper={'date':'Date','state':'Province_State'},axis=1)\nnyt.drop(['fips','cases','deaths'],axis=1,inplace=True)\nnyt","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"extra = pd.concat([ecdc,nyt], sort=True)\nextra","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"d = d.merge(extra, how='left', on=cpd)\nd","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# # enforce monotonicity\n# d = d.sort_values(['Loc','Date']).reset_index(drop=True)\n# for y in yv:\n#     ey = 'extra_'+y\n#     d[ey] = d[ey].fillna(0.)\n#     d[ey] = d.groupby('Loc')[ey].cummax()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"d[['y0','y1','extra_y0','extra_y1']].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# impute us state data prior to march 10\nfor i in range(ny):\n    ei = 'extra_'+yv[i]\n    qm = (d.Country_Region == 'US') & (d.Date < '2020-03-10') & (d[ei].notnull())\n    print(i,sum(qm))\n    d.loc[qm,yv[i]] = d.loc[qm,ei]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"d[['y0','y1']].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.plot(d.loc[d.Province_State=='New York','y0'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# log rates\nd['rate0'] = d.y0 - np.log(d['pop'])\nd['rate1'] = d.y1 - np.log(d['pop'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# recovered data from hopkins, https://github.com/CSSEGISandData/COVID-19\nrecovered = pd.read_csv(path+'time_series_covid19_recovered_global.csv')\nrecovered = recovered.rename(mapper={'Country/Region':'Country_Region','Province/State':'Province_State'}, axis=1)\nrecovered[cp] = recovered[cp].fillna('')\nrecovered = recovered.drop(['Lat','Long'], axis=1)\nrecovered","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# replace US row with identical rows for every US state\nusp = d.loc[d.Country_Region=='US','Province_State'].unique()\nprint(usp, len(usp))\nrus = recovered[recovered.Country_Region=='US']\nrus","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rus = rus.reindex(np.repeat(rus.index.values,len(usp)))\nrus.loc[:,'Province_State'] = usp\nrus","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"recovered =  recovered[recovered.Country_Region!='US']\nrecovered = pd.concat([recovered,rus]).reset_index(drop=True)\nrecovered","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# melt and merge\nrm = pd.melt(recovered, id_vars=cp, var_name='d', value_name='recov')\nrm","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rm['Date'] = pd.to_datetime(rm.d)\nrm.drop('d',axis=1,inplace=True)\nrm['Date'] = rm['Date'].dt.strftime('%Y-%m-%d')\nrm","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"d = d.merge(rm, how='left', on=['Country_Region','Province_State','Date'])\nd","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"d['recov'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# approximate US state recovery via proportion of confirmed cases\nd['ccsum'] = d.groupby(['Country_Region','Date'])['ConfirmedCases'].transform(lambda x: x.sum())\nd.loc[d.Country_Region=='US','recov'] = d.loc[d.Country_Region=='US','recov'] * \\\n                                        d.loc[d.Country_Region=='US','ConfirmedCases'] / \\\n                                        (d.loc[d.Country_Region=='US','ccsum'] + 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"d.loc[:,'recov'] = np.log1p(d.recov)\n# d.loc[:,'recov'] = d['recov'].fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# # enforce monotonicity\n# d = d.sort_values(['Loc','Date']).reset_index(drop=True)\n# d['recov'] = d['recov'].fillna(0.)\n# d['recov'] = d.groupby('Loc')['recov'].cummax()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"d.loc[d.Province_State=='North Carolina','recov'][45:55]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"d = d.sort_values(['Loc','Date']).reset_index(drop=True)\nd.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# compute nearest neighbors\nregions = d[['Loc','Lat','Long']].drop_duplicates('Loc').reset_index(drop=True)\nregions","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# regions.to_csv('regions.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# knn max features\nk = kv[0]\nnn = NearestNeighbors(k)\nnn.fit(regions[['Lat','Long']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# first matrix is distances, second indices to nearest neighbors including self\n# note two cruise ships are replicated and have identical lat, long values\nknn = nn.kneighbors(regions[['Lat','Long']])\nknn","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"ns = d['Loc'].nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# time series matrix\nky = d['y0'].values.reshape(ns,-1)\nprint(ky.shape)\n\nprint(ky[0])\n\n# use knn indices to create neighbors\nknny = ky[knn[1]]\nprint(knny.shape)\n\nknny = knny.transpose((0,2,1)).reshape(-1,k)\nprint(knny.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# knn max features\nnk = len(kv)\nkp = []\nkd = []\nns = regions.shape[0]\nfor k in kv:\n    nn = NearestNeighbors(k)\n    nn.fit(regions[['Lat','Long']])\n    knn = nn.kneighbors(regions[['Lat','Long']])\n    kp.append('knn'+str(k)+'_')\n    kd.append('kd'+str(k)+'_')\n    for i in range(ny):\n        yi = 'y'+str(i)\n        kc = kp[-1]+yi\n        # time series matrix\n        ky = d[yi].values.reshape(ns,-1)\n        # use knn indices to create neighbor matrix\n        km = ky[knn[1]].transpose((0,2,1)).reshape(-1,k)\n        \n        # take maximum value over all neighbors to approximate spreading\n        d[kc] = np.amax(km, axis=1)\n        print(d[kc].describe())\n        print()\n        \n        # distance to max\n        kc = kd[-1]+yi\n        ki = np.argmax(km, axis=1).reshape(ns,-1)\n        kw = np.zeros_like(ki).astype(float)\n        # inefficient indexing, surely some way to do it faster\n        for j in range(ns): \n            kw[j] = knn[0][j,ki[j]]\n        d[kc] = kw.flatten()\n        print(d[kc].describe())\n        print()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"ki[j]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# range of dates for training\n# dates = d[~d.y0.isnull()]['Date'].drop_duplicates()\ndates = d[d.y0.notnull()]['Date'].drop_duplicates()\ndates","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# correlations for knn features\ncols = []\nfor i in range(ny):\n    yi = yv[i]\n    cols.append(yi)\n    for k in kp:\n        cols.append(k+yi)\nd.loc[:,cols].corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"d['Date'] = pd.to_datetime(d['Date'])\nd['Date'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# days since beginning\n# basedate = train['Date'].min()\n# train['dint'] = train.apply(lambda x: (x.name.to_datetime() - basedate).days, axis=1)\nd['dint'] = (d['Date'] - d['Date'].min()).dt.days\nd['dint'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"d.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# reference days since exp(j)th occurrence\nfor i in range(ny):\n    \n    for j in range(3):\n\n        ij = str(i)+'_'+str(j)\n        \n        cut = 2**j if i==0 else j\n        \n        qd1 = (d[yv[i]] > cut) & (d[yv[i]].notnull())\n        d1 = d.loc[qd1,['Loc','dint']]\n        # d1.shape\n        # d1.head()\n\n        # get min for each location\n        d1['dmin'] = d1.groupby('Loc')['dint'].transform(lambda x: x.min())\n        # dintmax = d1['dint'].max()\n        # print(i,j,'dintmax',dintmax)\n        # d1.head()\n\n        d1.drop('dint',axis=1,inplace=True)\n        d1 = d1.drop_duplicates()\n        d = d.merge(d1,how='left',on=['Loc'])\n \n        # if dmin is missing then the series had no occurrences in the training set\n        # go ahead and assume there will be one at the beginning of the test period\n        # the average time between first occurrence and first death is 14 days\n        # if j==0: d[dmi] = d[dmi].fillna(dintmax + 1 + i*14)\n\n        # ref day is days since dmin, must clip at zero to avoid leakage\n        d['ref_day'+ij] = np.clip(d.dint - d.dmin, 0, None)\n        d['ref_day'+ij] = d['ref_day'+ij].fillna(0)\n        d.drop('dmin',axis=1,inplace=True)\n\n        # asymptotic curve may bin differently\n        d['recip_day'+ij] = 1 / (1 + (1 + d['ref_day'+ij])**(-1.0))\n    \n\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"d['dint'].value_counts().std()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def ewma(x, com):\n    return pd.Series.ewm(x, com=com).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# diffs and rolling means\n# note lags are taken dynamically at run time\ne = 1\n# r = 5\nr = 7\nw = 2\nfor i in range(ny):\n    yi = 'y'+str(i)\n    dd = '_d'+str(e)\n    rr = '_r'+str(r)\n    ww = '_w'+str(w)\n    \n    for j in range(5):\n        d[yi+'_d'+str(1+j)] = d.groupby('Loc')[yi].transform(lambda x: x.diff(1+j))\n        d[yi+'_l'+str(1+j)] = d.groupby('Loc')[yi].transform(lambda x: x.shift(1+j))\n    \n    d[yi+rr] = d.groupby('Loc')[yi].transform(lambda x: x.rolling(r).mean())\n    d[yi+ww] = d.groupby('Loc')[yi].transform(lambda x: ewma(x,w))\n    d['rate'+str(i)+dd] = d.groupby('Loc')['rate'+str(i)].transform(lambda x: x.diff(e))\n    d['rate'+str(i)+rr] = d.groupby('Loc')['rate'+str(i)].transform(lambda x: x.rolling(r).mean())\n    d['rate'+str(i)+ww] = d.groupby('Loc')['rate'+str(i)].transform(lambda x: ewma(x,w))\n    d['extra_y'+str(i)+dd] = d.groupby('Loc')['extra_y'+str(i)].transform(lambda x: x.diff(e))\n    d['extra_y'+str(i)+rr] = d.groupby('Loc')['extra_y'+str(i)].transform(lambda x: x.rolling(r).mean())\n    d['extra_y'+str(i)+ww] = d.groupby('Loc')['extra_y'+str(i)].transform(lambda x: ewma(x,w))\n\n    for k in kp:\n        d[k+yi+dd] = d.groupby('Loc')[k+yi].transform(lambda x: x.diff(e))\n        d[k+yi+rr] = d.groupby('Loc')[k+yi].transform(lambda x: x.rolling(r).mean())\n        d[k+yi+ww] = d.groupby('Loc')[k+yi].transform(lambda x: ewma(x,w))\n\n    for k in kd:\n        d[k+yi+dd] = d.groupby('Loc')[k+yi].transform(lambda x: x.diff(e))\n        d[k+yi+rr] = d.groupby('Loc')[k+yi].transform(lambda x: x.rolling(r).mean())\n        d[k+yi+ww] = d.groupby('Loc')[k+yi].transform(lambda x: ewma(x,w))\n        \nvlist = ['recov'] + google + wf\n\nfor v in vlist:\n    d[v+dd] = d.groupby('Loc')[v].transform(lambda x: x.diff(e))\n    d[v+rr] = d.groupby('Loc')[v].transform(lambda x: x.rolling(r).mean())\n    d[v+ww] = d.groupby('Loc')[v].transform(lambda x: ewma(x,w))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"d['y0'+ww].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# final sort before training\nd = d.sort_values(['Loc','dint']).reset_index(drop=True)\nd.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# initial continuous and categorical features\n# dogs = tfeats\n# ref_day0_0 is no longer leaky since every location has at least one confirmed case\n# dogs = ['ref_day0_0']\ndogs = []\ncats = ['Loc']\nprint(dogs, len(dogs))\nprint(cats, len(cats))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# one-hot encode categorical features\nohef = []\nfor i,c in enumerate(cats):\n    print(c, d[c].nunique())\n    ohe = pd.get_dummies(d[c], prefix=c)\n    ohec = [f.translate({ord(c): \"_\" for c in \" !@#$%^&*()[]{};:,./<>?\\|`~-=_+\"}) for f in list(ohe.columns)]\n    ohe.columns = ohec\n    d = pd.concat([d,ohe],axis=1)\n    ohef = ohef + ohec","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"d['Loc_US_North_Carolina'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"d['Loc_US_Colorado'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# must start cas server from gevmlax02 before running this cell\n# ssh rdcgrd001 /opt/vb025/laxnd/TKGrid/bin/caslaunch stat -mode mpp -cfg /u/sasrdw/config.lua\nif 'cas' in booster:\n    from swat import *\n    s = CAS('rdcgrd001.unx.sas.com', 16695)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# boosting hyperparameters\nparams = {}\n\n# # from vopani\n# SEED = 345\n# LGB_PARAMS = {\"objective\": \"regression\",\n#               \"num_leaves\": 5,\n#               \"learning_rate\": 0.013,\n#               \"bagging_fraction\": 0.91,\n#               \"feature_fraction\": 0.81,\n#               \"reg_alpha\": 0.13,\n#               \"reg_lambda\": 0.13,\n#               \"metric\": \"rmse\",\n#               \"seed\": SEED\n#              }\n\n# from oscii\nSEED = 42\nLGB_PARAMS = {'num_leaves': 8,\n          'min_data_in_leaf': 5,  # 42,\n          'objective': 'regression',\n          'max_depth': 8,\n          'learning_rate': 0.02,\n          'boosting': 'gbdt',\n          'bagging_freq': 5,  # 5\n          'bagging_fraction': 0.8,  # 0.5,\n          'feature_fraction': 0.8201,\n          'bagging_seed': SEED,\n          'reg_alpha': 1,  # 1.728910519108444,\n          'reg_lambda': 4.9847051755586085,\n          'random_state': SEED,\n          'metric': 'rmse',\n          # 'verbosity': 100,\n          'min_gain_to_split': 0.02,  # 0.01077313523861969,\n          'min_child_weight': 5,  # 19.428902804238373,\n          # 'num_threads': 6,\n          }\n\nparams[('lgb','y0')] = LGB_PARAMS\nparams[('lgb','y1')] = LGB_PARAMS\n# params[('lgb','y0')] = {'lambda_l2': 1.9079933811271934, 'max_depth': 5}\n# params[('lgb','y1')] = {'lambda_l2': 1.690407455211948, 'max_depth': 3}\nparams[('xgb','y0')] = {'lambda_l2': 1.9079933811271934, 'max_depth': 5}\nparams[('xgb','y1')] = {'lambda_l2': 1.690407455211948, 'max_depth': 3}\nparams[('ctb','y0')] = {'l2_leaf_reg': 1.9079933811271934, 'max_depth': 5}\nparams[('ctb','y1')] = {'l2_leaf_reg': 1.690407455211948, 'max_depth': 3}","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# booster = ['rdg','lgb','xgb','ctb']\n# booster = ['lgb','xgb']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# single horizon validation using one day at a time for 28 days\nnb = len(booster)\nnls = np.zeros((nhorizon-skip,ny,nb+tf2))\nrallv = np.zeros((nhorizon-skip,ny,nb))\niallv = np.zeros((nhorizon-skip,ny,nb)).astype(int)\nyallv = []\npallv = []\nimps = []\n \n# loop over horizons\nfor horizon in range(1+skip,nhorizon+1):\n# for horizon in range(4,5):\n    \n    print()\n#     print('*'*20)\n#     print(f'horizon {horizon}')\n#     print('*'*20)\n    \n    gc.collect()\n    \n    hs = str(horizon)\n    if horizon < 10: hs = '0' + hs\n    \n    # build lists of features\n    lags = []\n    # must lag reference days to avoid validation leakage\n    for i in range(ny):\n        for j in range(3):\n            # omit ref_day0_0 since it is no longer leaky\n            # if (i > 0) | (j > 0): lags.append('ref_day'+str(i)+'_'+str(j))\n            lags.append('ref_day'+str(i)+'_'+str(j))\n            \n    # lag all time-varying features\n    for i in range(ny):\n        yi = 'y'+str(i)\n        lags.append(yi)\n        lags.append('extra_'+yi)\n        lags.append('rate'+str(i))\n        for j in range(5):\n            lags.append(yi+'_d'+str(1+j))\n            lags.append(yi+'_l'+str(1+j))\n        lags.append('extra_'+yi+dd)\n        lags.append('rate'+str(i)+dd)\n        lags.append(yi+rr)\n        lags.append('extra_'+yi+rr)\n        lags.append('rate'+str(i)+rr)\n        lags.append(yi+ww)\n        lags.append('extra_'+yi+ww)\n        lags.append('rate'+str(i)+ww)\n        for k in kp:\n            lags.append(k+yi)\n            lags.append(k+yi+dd)\n            lags.append(k+yi+rr)\n            lags.append(k+yi+ww)\n        for k in kd:\n            lags.append(k+yi)\n            lags.append(k+yi+dd)\n            lags.append(k+yi+rr)\n            lags.append(k+yi+ww)\n       \n    lags.append('recov')\n    \n#     lags = lags + wmf + google + wf + ckeep\n\n    lags = lags + google + ckeep + wf + wmf\n    \n#     cinfo = ['pop', 'tests', 'testpop', 'density', 'medianage',\n#        'urbanpop', 'hospibed', 'smokers']\n    cinfo0 = ['testpop']\n    cinfo1 = ['testpop','medianage']\n    \n    f0 = dogs + lags + cinfo0 + ohef\n    f1 = dogs + lags + cinfo1 + ohef\n    \n    # remove some features based on validation experiments\n#     f0 = [f for f in f0 if not f.startswith('knn11') and not f.startswith('kd') \\\n#          and not f.startswith('rate') and not f.endswith(dd) and not f.endswith(rr)]\n\n    f0 = [f for f in f0 if not f.startswith('knn11') and not f.startswith('kd11')]\n    f1 = [f for f in f1 if not f.startswith('knn6') and not f.startswith('kd6')]\n    \n    # remove any duplicates\n    # f0 = list(set(f0))\n    # f1 = list(set(f1))\n    \n    features = []\n    features.append(f0)\n    features.append(f1)\n    \n    nf = []\n    for i in range(ny):\n        nf.append(len(features[i]))\n        # print(nf[i], features[i][:10])\n     \n    if val_scheme == 'forward':\n        # ddate is the last day of validation training\n        # training data stays constant\n        ddate = dmax - timedelta(days=nhorizon)\n        qtrain = d['Date'] <= ddate.isoformat()\n        # validation day moves forward\n        vdate = ddate + timedelta(days=horizon)\n        qval = d['Date'] == vdate.isoformat()\n        # lag day is last day of training\n        qvallag = d['Date'] == ddate.isoformat()\n        # for saving predictions into main table\n        qsave = qval\n    else: \n        # ddate is the last day of validation training\n        # training data moves backwards\n        ddate = dmax - timedelta(days=horizon)\n        qtrain = d['Date'] <= ddate.isoformat()\n        # validate using the last day with data\n        # validation day stays constant\n        vdate = dmax\n        qval = d['Date'] == vdate.isoformat()\n        # lag day is last day of training\n        qvallag = d['Date'] == ddate.isoformat()\n        # for saving predictions into table, expected rise going backwards\n        sdate = dmax - timedelta(days=horizon-1)\n        qsave = d['Date'] == sdate.isoformat()\n\n    \n    x_train = d[qtrain].copy()\n    # make y training data monotonic nondecreasing\n    y_train = []\n    yd_train = []    \n    for i in range(ny):\n        y_train.append(pd.Series(d.loc[qtrain,['Loc',yv[i]]].groupby('Loc')[yv[i]].cummax()).values)\n        ylag = pd.Series(d.loc[qtrain,['Loc',yv[i]]].groupby('Loc')[yv[i]].cummax().shift(horizon).values)\n        yd_train.append(y_train[i] - ylag)\n        # yd_train[i] = yd_train[i].fillna(0)\n        yd_train[i] = np.nan_to_num(yd_train[i])\n        yd_train[i] = np.clip(yd_train[i], 0, None)\n        \n    x_val = d[qval].copy()\n    \n#     y_val = [d.loc[qval,'y0'].copy(), d.loc[qval,'y1'].copy()]\n#     y_vallag = [d.loc[qvallag,'y0'].copy(), d.loc[qvallag,'y1'].copy()]\n    y_val = [d.loc[qval,'y0'].values, d.loc[qval,'y1'].values]\n    y_vallag = [d.loc[qvallag,'y0'].values, d.loc[qvallag,'y1'].values]\n    yd_val = [y_val[0] - y_vallag[0], y_val[1] - y_vallag[1]]\n    yallv.append(y_val)\n    \n    # lag features\n    x_train.loc[:,lags] = x_train.groupby('Loc')[lags].transform(lambda x: x.shift(horizon))\n    x_val.loc[:,lags] = d.loc[qvallag,lags].values\n\n    print()\n    print(horizon, 'x_train', x_train.shape)\n    print(horizon, 'x_val', x_val.shape)\n    \n    if train_full:\n        \n        qfull = (d['Date'] <= tmax)\n        \n        tdate = dmax + timedelta(days=horizon)\n        qtest = d['Date'] == tdate.isoformat()\n        qtestlag = d['Date'] == dmax.isoformat()\n    \n        x_full = d[qfull].copy()\n        \n        # make y training data monotonic nondecreasing\n        y_full = []\n        yd_full = []\n        for i in range(ny):\n            y_full.append(pd.Series(d.loc[qfull,['Loc',yv[i]]].groupby('Loc')[yv[i]].cummax()).values)\n            ylag = pd.Series(d.loc[qfull,['Loc',yv[i]]].groupby('Loc')[yv[i]].cummax().shift(horizon).values)\n            yd_full.append(y_full[i] - ylag)\n            # yd_full[i] = yd_full[i].fillna(0)\n            yd_full[i] = np.nan_to_num(yd_full[i])\n            yd_full[i] = np.clip(yd_full[i], 0, None)\n        \n        x_test = d[qtest].copy()\n        y_fulllag = [d.loc[qtestlag,'y0'].values, d.loc[qtestlag,'y1'].values]\n        \n        # lag features\n        x_full.loc[:,lags] = x_full.groupby('Loc')[lags].transform(lambda x: x.shift(horizon))\n        x_test.loc[:,lags] = d.loc[qtestlag,lags].values\n\n        print(horizon, 'x_full', x_full.shape)\n        print(horizon, 'x_test', x_test.shape)\n\n    train_set = []\n    val_set = []\n    ny = len(y_train)\n\n#     for i in range(ny):\n#         train_set.append(xgb.DMatrix(x_train[features[i]], y_train[i]))\n#         val_set.append(xgb.DMatrix(x_val[features[i]], y_val[i]))\n\n    gc.collect()\n\n    # loop over multiple targets\n    mod = []\n    pred = []\n    rez = []\n    iters = []\n    \n    for i in range(ny):\n#     for i in range(1):\n        print()\n        print('*'*40)\n        print(f'horizon {horizon} {yv[i]} {ynames[i]} {vdate}')\n        print('*'*40)\n        \n        # use catboost only for y1\n        # nb = 2 if i==0 else 3\n       \n        # matrices to store predictions\n        vpm = np.zeros((x_val.shape[0],nb))\n        tpm = np.zeros((x_test.shape[0],nb))\n        \n        # x_train[features[i]] = x_train[features[i]].fillna(0)\n        # x_val[features[i]] = x_val[features[i]].fillna(0)\n        \n        for b in range(nb):\n            \n            restore_features = False\n                       \n            if booster[b] == 'cas':\n                \n                x_train['Partition'] = 1\n                x_val['Partition'] = 0\n                x_cas_all = pd.concat([x_train, x_val], axis=0)\n                # make copy of target since it is also used for lags\n                x_cas_all['target'] = pd.concat([y_train[i], y_val[i]], axis=0).values\n                s.upload(x_cas_all, casout=\"x_cas_val\")\n\n                target = 'target'\n                inputs = features[i]\n                inputs.append(target)\n\n                s.loadactionset(\"autotune\")\n                res=s.autotune.tuneGradientBoostTree (\n                    trainOptions = {\n                        \"table\":{\"name\":'x_cas_val',\"where\":\"Partition=1\"},\n                        \"target\":target,\n                        \"inputs\":inputs,\n                        \"casOut\":{\"name\":\"model\", \"replace\":True}\n                    },\n                    scoreOptions = {\n                        \"table\":{\"name\":'x_cas_val', \"where\":\"Partition=0\"},\n                        \"model\":{\"name\":'model'},\n                        \"casout\":{\"name\":\"x_valid_preds\",\"replace\":True},\n                        \"copyvars\": ['Id','Loc','Date']\n                    },\n                    tunerOptions = {\n                        \"seed\":54321,  \n                        \"objective\":\"RASE\", \n                        \"userDefinedPartition\":True \n                    }\n                )\n                print()\n                print(res.TunerSummary)\n                print()\n                print(res.BestConfiguration)        \n\n                TunerSummary=pd.DataFrame(res['TunerSummary'])\n                TunerSummary[\"Value\"]=pd.to_numeric(TunerSummary[\"Value\"])\n                BestConf=pd.DataFrame(res['BestConfiguration'])\n                BestConf[\"Value\"]=pd.to_numeric(BestConf[\"Value\"])\n                vpt = s.CASTable(\"x_valid_preds\").to_frame()\n                #FG: resort the CAS predictions by Id\n                vpt = vpt.sort_values(['Loc','Date']).reset_index(drop=True)\n                vp = vpt['P_target'].values\n\n                s.dropTable(\"x_cas_val\")\n                s.dropTable(\"x_valid_preds\")\n                \n            else:\n                # scikit interface automatically uses best model for predictions\n                # params[(booster[b],yv[i])]['n_estimators'] = 5000\n                \n                kwargs = {'verbose':False}\n                if booster[b]=='lgb':\n                    params[(booster[b],yv[i])]['n_estimators'] = 125 if i==0 else 100\n                    model = lgb.LGBMRegressor(**params[(booster[b],yv[i])]) \n                elif booster[b]=='xgb':\n                    params[(booster[b],yv[i])]['n_estimators'] = 75 if i==0 else 50\n                    params[(booster[b],yv[i])]['base_score'] = np.mean(y_train[i])\n                    model = xgb.XGBRegressor(**params[(booster[b],yv[i])])\n                elif booster[b]=='ctb':\n                    params[(booster[b],yv[i])]['n_estimators'] = 400 if i==0 else 350\n                    # change feature list for categorical features\n                    features_save = features[i].copy()\n                    features[i] = [f for f in features[i] if not f.startswith('Loc_')] + ['Loc']\n                    params[(booster[b],yv[i])]['cat_features'] = ['Loc']\n                    restore_features = True\n                    model = ctb.CatBoostRegressor(**params[(booster[b],yv[i])])\n                elif booster[b]=='rdg':\n                    # alpha from cpmp\n                    model = Ridge(alpha=3, fit_intercept=True)\n                    kwargs = {}\n                else:\n                    raise ValueError(f'Unrecognized booster {booster[b]}')\n                    \n                xtrn = x_train[features[i]].copy()\n                xval = x_val[features[i]].copy()\n                if booster[b]=='rdg':\n                    s = StandardScaler()\n                    xtrn = s.fit_transform(xtrn)\n                    xval = s.transform(xval)\n                    xtrn = np.nan_to_num(xtrn)\n                    xval = np.nan_to_num(xval)\n                    xtrn = pd.DataFrame(xtrn, columns=features[i])\n                    xval = pd.DataFrame(xval, columns=features[i])\n                \n                # fit cumulative target\n                model.fit(xtrn, y_train[i],\n#                                   eval_set=[(x_train[features[i]], yd_train[i]),\n#                                             (x_val[features[i]], yd_val[i])],\n#                                   eval_set=[(x_val[features[i]], yd_val[i])],\n#                                   eval_set=[(x_val[features[i]], y_val[i])],\n#                                   early_stopping_rounds=30,\n                                    **kwargs\n                         )\n\n                vp = model.predict(xval)\n\n                # fit diffs from last training y\n                kwargs = {'verbose':False}\n                if booster[b]=='lgb':\n                    # params[(booster[b],yv[i])]['n_estimators'] = 125 if i==0 else 75\n                    model = lgb.LGBMRegressor(**params[(booster[b],yv[i])]) \n                elif booster[b]=='xgb':\n                    # params[(booster[b],yv[i])]['n_estimators'] = 75 if i==0 else 30\n                    params[(booster[b],yv[i])]['base_score'] = np.mean(yd_train[i])\n                    model = xgb.XGBRegressor(**params[(booster[b],yv[i])])\n                elif booster[b]=='ctb':\n                    # params[(booster[b],yv[i])]['n_estimators'] = 400 if i==0 else 200\n                    # hack for categorical features, ctb must be last in booster list\n                    # features[i] = [f for f in features[i] if not f.startswith('Loc_')] + ['Loc']\n                    # params[(booster[b],yv[i])]['cat_features'] = ['Loc']\n                    model = ctb.CatBoostRegressor(**params[(booster[b],yv[i])])\n                elif booster[b]=='rdg':\n                    # alpha from cpmp\n                    model = Ridge(alpha=3, fit_intercept=True)\n                    kwargs = {}\n                else:\n                    raise ValueError(f'Unrecognized booster {booster[b]}')\n\n                model.fit(xtrn, yd_train[i],\n#                                   eval_set=[(x_train[features[i]], yd_train[i]),\n#                                             (x_val[features[i]], yd_val[i])],\n#                                   eval_set=[(x_val[features[i]], yd_val[i])],\n#                                   eval_set=[(x_val[features[i]], y_val[i])],\n#                                   early_stopping_rounds=30,\n                                  **kwargs\n                         )\n\n                vpd = model.predict(xval)\n                vpd = np.clip(vpd,0,None)\n                vpd = y_vallag[i] + vpd\n                \n                # blend two predictions based on horizon\n                alpha = 0.1 + 0.8*(horizon-1)/29\n                vp = alpha*vp + (1-alpha)*vpd\n\n#                 iallv[horizon-skip-1,i,b] = model._best_iteration if booster[b]=='lgb' else \\\n#                                             model.best_iteration if booster[b]=='xgb' else \\\n#                                             model.best_iteration_\n\n                gain = np.abs(model.coef_) if booster[b]=='rdg' else model.feature_importances_\n        #         gain = model.get_score(importance_type='gain')\n        #         split = model.get_score(importance_type='weight')   \n            #     gain = model.feature_importance(importance_type='gain')\n            #     split = model.feature_importance(importance_type='split').astype(float)  \n            #     imp = pd.DataFrame({'feature':features,'gain':gain,'split':split})\n                imp = pd.DataFrame({'feature':features[i],'gain':gain})\n        #         imp = pd.DataFrame({'feature':features[i]})\n        #         imp['gain'] = imp['feature'].map(gain)\n        #         imp['split'] = imp['feature'].map(split)\n\n                imp.set_index(['feature'],inplace=True)\n\n                imp.gain /= np.sum(imp.gain)\n        #         imp.split /= np.sum(imp.split)\n\n                imp.sort_values(['gain'], ascending=False, inplace=True)\n\n                print()\n                print(imp.head(n=10))\n                # print(imp.shape)\n\n                imp.reset_index(inplace=True)\n                imp['horizon'] = horizon\n                imp['target'] = yv[i]\n                imp['set'] = 'valid'\n                imp['booster'] = booster[b]\n\n                mod.append(model)\n                imps.append(imp)\n                \n            # china rule, last observation carried forward, set to zero here\n            qcv = (x_val['Country_Region'] == 'China') & \\\n                  (x_val['Province_State'] != 'Hong Kong') & \\\n                  (x_val['Province_State'] != 'Macau')\n            vp[qcv] = 0.0\n\n            # make sure horizon 1 prediction is not smaller than first lag\n            # because we know series is monotonic\n            # if horizon==1+skip:\n            if True:\n                a = np.zeros((len(vp),2))\n                a[:,0] = vp\n                # note yv is lagged here\n                a[:,1] = x_val[yv[i]].values\n                vp = np.nanmax(a,axis=1)\n            \n            val_score = np.sqrt(mean_squared_error(vp, y_val[i]))\n            vpm[:,b] = vp\n            \n            print()\n            print(f'{booster[b]} validation rmse {val_score:.6f}')\n            rallv[horizon-skip-1,i,b] = val_score\n\n            gc.collect()\n    \n#             break\n\n            if train_full:\n                \n                print()\n                print(f'{booster[b]} training with full data and predicting', tdate.isoformat())\n                    \n                # x_full[features[i]] = x_full[features[i]].fillna(0)\n                # x_test[features[i]] = x_test[features[i]].fillna(0)\n        \n                if booster[b] == 'cas':\n                    \n                    x_full['target'] = y_full[i].values\n                    s.upload(x_full, casout=\"x_full\")\n                    # use hyperparameters from validation fit\n                    s.loadactionset(\"decisionTree\")\n                    result = s.gbtreetrain(\n                        table={\"name\":'x_full'},\n                        target=target,\n                        inputs= inputs,\n                        varimp=True,\n                        ntree=BestConf.iat[0,2], \n                        m=BestConf.iat[1,2],\n                        learningRate=BestConf.iat[2,2],\n                        subSampleRate=BestConf.iat[3,2],\n                        lasso=BestConf.iat[4,2],\n                        ridge=BestConf.iat[5,2],\n                        nbins=BestConf.iat[6,2],\n                        maxLevel=BestConf.iat[7,2],\n                        #quantileBin=True,\n                        seed=326146718,\n                        #savestate={\"name\":\"aStore\",\"replace\":True}\n                        casOut={\"name\":'fullmodel', \"replace\":True}\n                        ) \n\n                    s.upload(x_test, casout=\"x_test_cas\")\n\n                    s.decisionTree.gbtreeScore(\n                        modelTable={\"name\":\"fullmodel\"},        \n                        table={\"name\":\"x_test_cas\"},\n                        casout={\"name\":\"x_test_preds\",\"replace\":True},\n                        copyvars= ['Loc','Date']\n                        ) \n                    # save test predictions back into main table\n                    forecast = s.CASTable(\"x_test_preds\").to_frame()\n                    forecast = forecast.sort_values(['Loc','Date']).reset_index(drop=True)\n                    tp = forecast['_GBT_PredMean_'].values\n                    \n                    s.dropTable(\"x_full\")\n                    s.dropTable(\"x_test_cas\")\n                     \n                else:\n                    \n                    # use number of iterations from validation fit\n                    kwargs = {'verbose':False}\n                    # params[(booster[b],yv[i])]['n_estimators'] = iallv[horizon-skip-1,i,b]\n                    if booster[b]=='lgb':\n                        model = lgb.LGBMRegressor(**params[(booster[b],yv[i])])\n                    elif booster[b]=='xgb':\n                        params[(booster[b],yv[i])]['base_score'] = np.mean(y_full[i])\n                        model = xgb.XGBRegressor(**params[(booster[b],yv[i])])\n                    elif booster[b]=='ctb':\n                        model = ctb.CatBoostRegressor(**params[(booster[b],yv[i])])\n                    elif booster[b]=='rdg':\n                        # alpha from cpmp\n                        model = Ridge(alpha=3, fit_intercept=True)\n                        kwargs = {}\n                    else:\n                        raise ValueError(f'Unrecognized booster {booster[b]}')\n                    \n                    xfull = x_full[features[i]].copy()\n                    xtest = x_test[features[i]].copy()\n                    if booster[b]=='rdg':\n                        s = StandardScaler()\n                        xfull = s.fit_transform(xfull)\n                        xtest = s.transform(xtest)\n                        xfull = np.nan_to_num(xfull)\n                        xtest = np.nan_to_num(xtest)\n                        xfull = pd.DataFrame(xfull, columns=features[i])\n                        xtest = pd.DataFrame(xtest, columns=features[i])\n                        \n                    model.fit(xfull, y_full[i], **kwargs)\n                    \n                    # params[(booster[b],yv[i])]['n_estimators'] = 5000\n\n                    tp = model.predict(xtest)\n                \n                    # use number of iterations from validation fit\n                    # params[(booster[b],yv[i])]['n_estimators'] = iallv[horizon-skip-1,i,b]\n                    kwargs = {'verbose':False}\n                    if booster[b]=='lgb':\n                        model = lgb.LGBMRegressor(**params[(booster[b],yv[i])])\n                    elif booster[b]=='xgb':\n                        params[(booster[b],yv[i])]['base_score'] = np.mean(yd_full[i])\n                        model = xgb.XGBRegressor(**params[(booster[b],yv[i])])\n                    elif booster[b]=='ctb':\n                        model = ctb.CatBoostRegressor(**params[(booster[b],yv[i])])\n                    elif booster[b]=='rdg':\n                        # alpha from cpmp\n                        model = Ridge(alpha=3, fit_intercept=True)\n                        kwargs = {}\n                    else:\n                        raise ValueError(f'Unrecognized booster {booster[b]}')\n                    \n                    # model.fit(x_full[features[i]], y_full[i], verbose=False)\n                    model.fit(xfull, yd_full[i], **kwargs)\n                    \n                    # params[(booster[b],yv[i])]['n_estimators'] = 5000\n\n                    tpd = model.predict(xtest)\n                    tpd = np.clip(tpd,0,None)\n                    tpd = y_fulllag[i] + tpd\n                    \n                    tp = alpha*tp + (1-alpha)*tpd\n                \n                    gain = np.abs(model.coef_) if booster[b]=='rdg' else model.feature_importances_\n            #         gain = model.get_score(importance_type='gain')\n            #         split = model.get_score(importance_type='weight')   \n                #     gain = model.feature_importance(importance_type='gain')\n                #     split = model.feature_importance(importance_type='split').astype(float)  \n                #     imp = pd.DataFrame({'feature':features,'gain':gain,'split':split})\n                    imp = pd.DataFrame({'feature':features[i],'gain':gain})\n            #         imp = pd.DataFrame({'feature':features[i]})\n            #         imp['gain'] = imp['feature'].map(gain)\n            #         imp['split'] = imp['feature'].map(split)\n\n                    imp.set_index(['feature'],inplace=True)\n\n                    imp.gain /= np.sum(imp.gain)\n            #         imp.split /= np.sum(imp.split)\n\n                    imp.sort_values(['gain'], ascending=False, inplace=True)\n\n                    print()\n                    print(imp.head(n=10))\n                    # print(imp.shape)\n\n                    imp.reset_index(inplace=True)\n                    imp['horizon'] = horizon\n                    imp['target'] = yv[i]\n                    imp['set'] = 'full'\n                    imp['booster'] = booster[b]\n\n                    imps.append(imp)\n\n                # china rule, last observation carried forward, set to zero here\n                qct = (x_test['Country_Region'] == 'China') & \\\n                      (x_test['Province_State'] != 'Hong Kong') & \\\n                      (x_test['Province_State'] != 'Macau')\n                tp[qct] = 0.0\n\n                # make sure first horizon prediction is not smaller than first lag\n                # because we know series is monotonic\n                # if horizon==1+skip:\n                if True:\n                    a = np.zeros((len(tp),2))\n                    a[:,0] = tp\n                    # note yv is lagged here\n                    a[:,1] = x_test[yv[i]].values\n                    tp = np.nanmax(a,axis=1)\n\n                tpm[:,b] = tp\n                \n                gc.collect()\n                \n            # restore feature list\n            if restore_features:\n                features[i] = features_save\n                restore_features = False\n                \n        # concat team predictions\n        if len(tfeats[i]):\n            vpm = np.concatenate([vpm,d.loc[qval,tfeats[i]].values], axis=1)\n            tpm = np.concatenate([tpm,d.loc[qtest,tfeats[i]].values], axis=1)\n                \n        # nonnegative least squares to estimate ensemble weights\n        # x, rnorm = nnls(vpm, y_val[i])\n        \n        # smooth weights by shrinking towards all equal\n        # x = (x + np.ones(3)/3.)/2\n        \n        # simple averaging to avoid overfitting\n        # drop ridge from y0\n        if i==0:\n            x = np.array([1., 1., 1., 0.])/3.\n        else:\n            nm = vpm.shape[1]\n            x = np.ones(nm)/nm\n        \n#         # drop catboost from y0\n#         if i == 0:  \n#             x = np.array([0.5, 0.5, 0.0])\n#         else: \n#             nm = vpm.shape[1]\n#             x = np.ones(nm)/nm\n\n        # smooth weights with rolling mean, ewma\n        # alpha = 0.1\n        # if horizon-skip > 1: x = alpha * x + (1 - alpha) * nls[horizon-skip-2,i]\n\n        nls[horizon-skip-1,i] = x\n        \n        val_pred = np.matmul(vpm, x)\n        test_pred = np.matmul(tpm, x)\n        \n        # china rule in case weights do not sum to 1\n        # val_pred[qcv] = vpm[:,0][qcv]\n        # test_pred[qcv] = tpm[:,0][qct]\n        \n        # save validation and test predictions back into main table\n        d.loc[qsave,yv[i]+'_pred'] = val_pred\n        d.loc[qtest,yv[i]+'_pred'] = test_pred\n\n        # ensemble validation score\n        # val_score = np.sqrt(rnorm/vpm.shape[0])\n        val_score = np.sqrt(mean_squared_error(val_pred, y_val[i]))\n        \n        rez.append(val_score)\n        pred.append(val_pred)\n\n    pallv.append(pred)\n    \n    # construct strings of nnls weights for printing\n    w0 = ''\n    w1 = ''\n    for b in range(nb+tf2):\n        w0 = w0 + f' {nls[horizon-skip-1,0,b]:.2f}'\n        w1 = w1 + f' {nls[horizon-skip-1,1,b]:.2f}'\n        \n    print()\n    print('         Validation RMSLE  ', ' '.join(booster), ' '.join(tfeats[0]))\n    print(f'{ynames[0]} \\t {rez[0]:.6f}  ' + w0)\n    print(f'{ynames[1]} \\t {rez[1]:.6f}  ' + w1)\n    print(f'Mean \\t \\t {np.mean(rez):.6f}')\n\n#     # break down RMSLE by day\n#     rp = np.zeros((2,7))\n#     for i in range(ny):\n#         for di in range(50,57):\n#             j = di - 50\n#             qf = x_val.dint == di\n#             rp[i,j] = np.sqrt(mean_squared_error(pred[i][qf], y_val[i][qf]))\n#             print(i,di,f'{rp[i,j]:.6f}')\n#         print(i,f'{np.mean(rp[i,:]):.6f}')\n#         plt.plot(rp[i])\n#         plt.title(ynames[i] + ' RMSLE')\n#         plt.show()\n        \n    # plot actual vs predicted\n    plt.figure(figsize=(10, 5))\n    for i in range(ny):\n        plt.subplot(1,2,i+1)\n        # plt.plot([0, 12], [0, 12], 'black')\n        plt.plot(pred[i], y_val[i], '.')\n        plt.xlabel('Predicted')\n        plt.ylabel('Actual')\n        plt.title(ynames[i])\n        plt.grid()\n    plt.show()\n        \n# save one big table of importances\nimpall = pd.concat(imps)\n\n# remove number suffixes from lag names to aid in analysis\n# impall['feature1'] = impall['feature'].replace(to_replace='lag..', value='lag', regex=True)\n\nos.makedirs('imp', exist_ok=True)\nfname = 'imp/' + mname + '_imp.csv'\nimpall.to_csv(fname, index=False)\nprint()\nprint(fname, impall.shape)\n\n# save scores and weights\nos.makedirs('rez', exist_ok=True)\nfname = 'rez/' + mname+'_rallv.npy'\nnp.save(fname, rallv)\nprint(fname, rallv.shape)\n\nfname = 'rez/' + mname+'_nnls.npy'\nnp.save(fname, nls)\nprint(fname, nls.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"x_train[features[i][42:52]].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"if 'cas' in booster: s.shutdown()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"tdate.isoformat()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rf = [f for f in features[0] if f.startswith('ref')]\nd[rf].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"np.mean(iallv, axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(10, 8))\nfor i in range(ny):\n    plt.subplot(2,2,1+i)\n    plt.plot(rallv[:,i])\n    plt.title(ynames[i] + ' RMSLE vs Horizon')\n    plt.grid()\n    plt.legend(booster)\n    \n    plt.subplot(2,2,3+i)\n    plt.plot(nls[:,i])\n    plt.title(ynames[i] + ' Ensemble Weights')\n    plt.grid()\n    plt.legend(booster+tfeats[i])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# compute validation rmsle\n\n# original targets\nd['y0r'] = np.log1p(d.ConfirmedCases)\nd['y1r'] = np.log1p(d.Fatalities)\n\nm = 0\nlocs = d.loc[:,['Loc']].drop_duplicates().reset_index(drop=True)\n# locs = x_val.copy().reset_index(drop=True)\n# print(locs.shape)\ny_truea = []\ny_preda = []\n\nprint(f'# {mname}')\nfor i in range(ny):\n    y_true = []\n    y_pred = []\n    for j in range(nhorizon-skip):\n        y_true.append(yallv[j][i])\n        y_pred.append(pallv[j][i])\n    y_true = np.stack(y_true)\n    y_pred = np.stack(y_pred)\n    # print(y_pred.shape)\n    # make each series monotonic increasing\n    for j in range(y_pred.shape[1]): \n        y_pred[:,j] = np.maximum.accumulate(y_pred[:,j])\n    # copy updated predictions into main table\n    for horizon in range(1+skip,nhorizon+1):\n        vdate = ddate + timedelta(days=horizon)\n        qval = d['Date'] == vdate.isoformat()\n        d.loc[qval,yv[i]+'_pred'] = y_pred[horizon-1-skip]\n    rmse = np.sqrt(mean_squared_error(y_pred, y_true))\n    print(f'# {rmse:.6f}')\n    m += rmse/2\n    locs['rmse'+str(i)] = np.sqrt(np.mean((y_true-y_pred)**2, axis=0))\n    y_truea.append(y_true)\n    y_preda.append(y_pred)\nprint(f'# {m:.6f}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# enforce monotonicity of forecasts in test set after last date in training\n# loc = d['Loc'].unique()\nlocs1 = d['Loc'].drop_duplicates()\nfor loc in locs1:\n    # q = (d.Loc==loc) & (d.ForecastId > 0)\n    q = (d.Loc==loc) & (d.Date > tmax)\n    # if skip, fill in last observed value\n    if skip: qs0 = (d.Loc==loc) & (d.Date == dmax.isoformat())\n    for yi in yv:\n        yp = yi+'_pred'\n        d.loc[q,yp] = np.maximum.accumulate(d.loc[q,yp])\n        if skip:\n            for j in range(skip):\n                qs1 = (d.Loc==loc) & (d.Date == (dmax + timedelta(1+j)).isoformat())\n                d.loc[qs1,yp] = d.loc[qs0,yi].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# kaz post-processing functions\ndef decay_4_first_10_then_1_f(array):\n    arr=[1.0 for k in range(len(array))]\n    for j in range(len(array)):\n        if j<10:\n            arr[j]=1. + (max(1,array[j])-1.)/4.\n        else :\n            arr[j]=1.\n    return arr\n\ndef decay_16_first_10_then_1_f(array):\n    arr=[1.0 for k in range(len(array))]\n    for j in range(len(array)):\n        if j<10:\n            arr[j]=1. + (max(1,array[j])-1.)/16.\n        else :\n            arr[j]=1.\n    return arr\t\n            \ndef decay_2_f(array):\n    arr=[1.0 for k in range(len(array))]    \n    for j in range(len(array)):\n            arr[j]=1. + (max(1,array[j])-1.)/2.\n    return arr \n\ndef decay_4_f(array):\n    arr=[1.0 for k in range(len(array))]    \n    for j in range(len(array)):\n            arr[j]=1. + (max(1,array[j])-1.)/4.\n    return arr \t\n\ndef acceleratorx2_f(array):\n    arr=[1.0 for k in range(len(array))]    \n    for j in range(len(array)):\n            arr[j]=1. + (max(1,array[j])-1.)*2.\n    return arr \n\ndef decay_1_5_f(array):\n    arr=[1.0 for k in range(len(array))]    \n    for j in range(len(array)):\n            arr[j]=1. + (max(1,array[j])-1.)/1.5\n    return arr            \n           \ndef stay_same_f(array):\n    arr=[1.0 for k in range(len(array))]      \n    for j in range(len(array)):\n        arr[j]=1.\n    return arr   \n\ndef decay_2_last_12_linear_inter_f(array):\n    arr=[1.0 for k in range(len(array))]\n    for j in range(len(array)):\n        arr[j]=1. + (max(1,array[j])-1.)/2.\n    if len(array) >= 12:\n        arr12 = (max(1,arr[-12])-1.)/12. \n        for j in range(0, 12):\n            arr[len(arr)-12 +j]= max(1, 1 + ( (arr12*12) - (j+1)*arr12 ))\n    return arr\n\ndef decay_4_last_12_linear_inter_f(array):\n    arr=[1.0 for k in range(len(array))]\n    for j in range(len(array)):\n        arr[j]=1. + (max(1,array[j])-1.)/4.\n    if len(array) >= 12:\n        arr12 = (max(1,arr[-12])-1.)/12. \n        for j in range(0, 12):\n            arr[len(arr)-12 +j]= max(1, 1 + ( (arr12*12) - (j+1)*arr12 ))\n    return arr\n\ndef linear_last_12_f(array):\n    arr=[1.0 for k in range(len(array))]\n    for j in range(len(array)):\n        arr[j]=max(1,array[j])\n    if len(array) >= 12:\n        arr12 = (max(1,arr[-12])-1.)/12. \n        for j in range(0, 12):\n            arr[len(arr)-12 +j]= max(1, 1 + ( (arr12*12) - (j+1)*arr12 ))\n    return arr\n   \ndecay_4_first_10_then_1 =[ \"Heilongjiang_China\",\"Liaoning_China\",\"Shanghai_China\"]#, \"Hong Kong_China\"\n\ndecay_4_first_10_then_1_fatality=[]\n\ndecay_16_first_10_then_1 =[\"Beijing_China\",\"Fujian_China\",\"Guangdong_China\",\"Shandong_China\",\"Sichuan_China\",\"Zhejiang_China\"]\ndecay_16_first_10_then_1_fatality=[]\n\ndecay_4=[\"nan_Bhutan\",\"nan_Burundi\",\"nan_Cabo Verde\",\"Prince Edward Island_Canada\",\n\"nan_Central African Republic\",\"Inner Mongolia_China\",\"nan_Maldives\",\"Falkland Islands (Malvinas)_United Kingdom\"]\n\ndecay_4_fatality=[\"nan_Congo (Kinshasa)\"]\n\ndecay_2 =[\"nan_Congo (Kinshasa)\",\"Faroe Islands_Denmark\",\"nan_Eritrea\",\"French Guiana_France\",\"nan_Korea, South\",\"nan_MS Zaandam\"]\ndecay_2_fatality=[]\n\nstay_same=[\"nan_Diamond Princess\",\"nan_Timor-Leste\"]\n  \nstay_same_fatality=[\"Beijing_China\",\"Fujian_China\",\"Guangdong_China\",\"Shandong_China\",\n\"Sichuan_China\",\"Zhejiang_China\", \"Heilongjiang_China\",\"Liaoning_China\",\"Shanghai_China\"]#\n\nnormal=[]\nnormal_fatality=[\"nan_Korea, South\",\"New York_US\"]\n\ndecay_4_last_12_linear_inter =[ \"Greenland_Denmark\",\"nan_Dominica\",\"nan_Equatorial Guinea\",\"nan_Eswatini\",\"New Caledonia_France\",\n\"Saint Barthelemy_France\",\"St Martin_France\",\"nan_Gambia\",\"nan_Grenada\",\"nan_Holy See\",\"nan_Mauritania\",\"nan_Namibia\",\"nan_Nicaragua\"\n,\"nan_Papua New Guinea\",\"nan_Saint Lucia\",\"nan_Saint Vincent and the Grenadines\",\"nan_Seychelles\",\"nan_Sierra Leone\",\"nan_Somalia\",\"nan_Suriname\",\n\"Anguilla_United Kingdom\",\"British Virgin Islands_United Kingdom\",\"Montserrat_United Kingdom\",\"Turks and Caicos Islands_United Kingdom\",\n\"nan_Zimbabwe\", \"Hong Kong_China\",\"Curacao_Netherlands\",\n\"Saint Pierre and Miquelon_France\",\"nan_South Sudan\",\"nan_Western Sahara\",\n\"nan_Malawi\",\"Bonaire, Sint Eustatius and Saba_Netherlands\",\"nan_Sao Tome and Principe\"\n]\n\ndecay_4_last_12_linear_inter_fatality=[]\n\ndecay_2_last_12_linear_inter =[ \"nan_Chad\",\n\"nan_Congo (Brazzaville)\",\"nan_Fiji\",\"French Polynesia_France\",\"nan_Gabon\",\n\"nan_Guyana\",\"nan_Laos\",\"nan_Nepal\",\"Sint Maarten_Netherlands\",\n\"nan_Saint Kitts and Nevis\",\"nan_Sudan\",\"nan_Syria\",\"nan_Tanzania\",\n\"Bermuda_United Kingdom\",\"Cayman Islands_United Kingdom\",\"nan_Zambia\",\"Northwest Territories_Canada\",\"Yukon_Canada\"\n,\"nan_Mongolia\",\"nan_Uganda\"]\ndecay_2_last_12_linear_inter_fatality=[]\n\nacceleratorx2=[]\nacceleratorx2_fatality=[]\n\n\nwarm_st=['nan_Angola','nan_Antigua and Barbuda','Northern Territory_Australia','nan_Bahamas',\n'nan_Bangladesh','nan_Belize','nan_Benin','nan_Botswana','nan_Burundi','nan_Cabo Verde','nan_Cameroon',\n'nan_Central African Republic','nan_Chad','Hong Kong_China',\"nan_Cote d'Ivoire\",'nan_Cuba','Greenland_Denmark',\n'nan_Dominica','nan_Equatorial Guinea','nan_Eritrea','nan_Eswatini','nan_Fiji','French Polyneta_France','New Caledonia_France',\n'Saint Barthelemy_France','St Martin_France','nan_Gabon','nan_Gambia','nan_Grenada','nan_Guyana','nan_Haiti','nan_Holy See',\n'nan_Honduras','nan_Ireland','nan_Korea, South','nan_Laos','nan_Liberia','nan_Libya','nan_Maldives','nan_Mali',\n'nan_Mauritania','nan_Mauritius','nan_Mongolia','nan_Mozambique','nan_Namibia','nan_Nepal','Aruba_Netherlands',\n'nan_Nicaragua','nan_Niger','nan_Papua New Guinea','nan_Saint Kitts and Nevis','nan_Saint Lucia',\n'nan_Saint Vincent and the Grenadines','nan_Seychelles','nan_Sierra Leone','nan_Somalia',\n'nan_Spain','nan_Sudan','nan_Suriname','nan_Syria','nan_Tanzania','nan_Togo','nan_Uganda','Anguilla_United Kingdom',\n'Bermuda_United Kingdom','British Virgin Islands_United Kingdom','Channel Islands_United Kingdom',\n'Gibraltar_United Kingdom','Isle of Man_United Kingdom','Montserrat_United Kingdom','nan_United Kingdom',\n'Turks and Caicos Islands_United Kingdom','nan_Uzbekistan','nan_Zimbabwe',\n'Saint Pierre and Miquelon_France','nan_South Sudan','nan_Western Sahara',\n'nan_Malawi','Bonaire, Sint Eustatius and Saba_Netherlands','nan_Sao Tome and Principe',\n'Falkland Islands (Malvinas)_United Kingdom'\n]\n\n\ndecay_1_5 =[\"nan_Angola\" ,\"nan_Antigua and Barbuda\",\"Montana_US\",\"Nebraska_US\",\"nan_Bangladesh\",\"Illinois_US\"\n,\"Northern Territory_Australia\",\"nan_Bahamas\",\"nan_Bahrain\",\"nan_Barbados\" ,\"nan_Belize\",\"nan_Benin\",\n \"nan_Botswana\",\"nan_Brunei\",\"Manitoba_Canada\",\"New Brunswick_Canada\",\"Saskatchewan_Canada\",\n \"nan_Cote d'Ivoire\",\"nan_France\",\"nan_Guinea-Bissau\",\"nan_Haiti\",\"nan_Italy\",\"nan_Libya\",\"nan_Malta\",\"nan_Mauritius\",\n \"Aruba_Netherlands\",\"nan_Niger\",\"nan_Spain\",\"nan_Togo\",\"Guam_US\",\"Iowa_US\",\"Idaho_US\",\"Connecticut_US\",\"California_US\",\"New York_US\",\"Virgin Islands_US\",\n \"Channel Islands_United Kingdom\",\"Gibraltar_United Kingdom\",\"Isle of Man_United Kingdom\",\"nan_United Kingdom\",'nan_Burma']\n\ndecay_1_5_fatality=[\"nan_Cameroon\",\"nan_Mali\",\"nan_Cuba\",\"Delaware_US\",\"District of Columbia_US\",\n\"Kansas_US\",\"Louisiana_US\",\"Michigan_US\",\"New Mexico_US\",\"Ohio_US\",\"Oklahoma_US\",\"Pennsylvania_US\",\"Puerto Rico_US\",\"Rhode Island_US\",\n\"South Dakota_US\" ,\"Tennessee_US\",\"Texas_US\",\"Vermont_US\",\"Virginia_US\",\"West Virginia_US\",\"nan_Uzbekistan\"]\n\nlinear_last_12=[\"nan_Honduras\",\"nan_Ireland\",\"Colorado_US\",\"nan_Liberia\",\"nan_Mozambique\"]\nlinear_last_12_fatality=[]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# kaz post-processsing overrides\n# works on predictions from one location, using his key naming convention as above\n# i = 0 for confirmed and 1 for fatalities\n# current_prediction is the predicted value one day prior to the series\n# inputs assumed to be on log(1+x) scale and the return array is also on that scale\ndef kpp(i, name, preds, current_prediction):\n\n    current_prediction = np.expm1(current_prediction)\n    if current_prediction==0: current_prediction = 0.1\n        \n    # transform to successive ratios, which is what kaz models\n    preds = np.expm1(preds)\n    preds = np.clip(preds,0.1,None)\n    preds[1:] = preds[1:] / preds[:-1]\n    preds[0] /= current_prediction\n        \n    this_preds = preds.tolist()\n    \n    reserve = this_preds[0]\n    \n    # fatality special\n    fdone = True\n    if i==1:\n        if name in normal_fatality:\n            this_preds=this_preds\n\n        elif name in decay_4_first_10_then_1_fatality:\n            this_preds=decay_4_first_10_then_1_f(this_preds) \n\n        elif name in decay_16_first_10_then_1_fatality:\n            this_preds=decay_16_first_10_then_1_f(this_preds)\n\n        elif name in decay_4_last_12_linear_inter_fatality:\n            this_preds=decay_4_last_12_linear_inter_f(this_preds)         \n\n        elif name in decay_4_fatality:\n            this_preds=decay_4_f(this_preds)\n\n        elif name in decay_2_fatality:\n            this_preds=decay_2_f(this_preds)        \n\n        elif name in decay_2_last_12_linear_inter_fatality:\n            this_preds=decay_2_last_12_linear_inter_f(this_preds)\n\n        elif name in decay_1_5_fatality:\n            this_preds=decay_1_5_f(this_preds) \n\n        elif name in linear_last_12_fatality:\n            this_preds=linear_last_12_f(this_preds) \n\n        elif name in acceleratorx2_fatality:\n            this_preds=acceleratorx2_f(this_preds)\n\n        elif name in stay_same_fatality:     \n            this_preds=stay_same_f(this_preds) \n            \n        else:\n            fdone = False\n            \n    if (i==0) or not fdone:\n\n        if name in normal:\n            this_preds=this_preds\n\n        elif name in decay_4_first_10_then_1:\n            this_preds=decay_4_first_10_then_1_f(this_preds)\n\n        elif name in decay_16_first_10_then_1:\n            this_preds=decay_16_first_10_then_1_f(this_preds)\n\n        elif name in decay_4_last_12_linear_inter:\n            this_preds=decay_4_last_12_linear_inter_f(this_preds)       \n\n        elif name in decay_4:\n            this_preds=decay_4_f(this_preds)\n\n        elif name in decay_2:\n            this_preds=decay_2_f(this_preds)\n\n        elif name in decay_2_last_12_linear_inter:\n            this_preds=decay_2_last_12_linear_inter_f(this_preds)\n\n        elif name in decay_1_5:\n            this_preds=decay_1_5_f(this_preds)        \n\n        elif name in linear_last_12:\n            this_preds=linear_last_12_f(this_preds)\n\n        elif name in acceleratorx2:\n            this_preds=acceleratorx2_f(this_preds)         \n\n        elif name in stay_same or  \"China\" in name:\n            this_preds=stay_same_f(this_preds)      \n\n    if name in warm_st:\n        this_preds[0]=reserve\n        \n    n = len(this_preds)\n    p = np.ones(n)\n        \n    for j in range(n):\n        current_prediction *= max(1,this_preds[j])\n        p[j] = current_prediction\n        \n    p = np.log1p(p)\n        \n    return p","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"ddate","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# compute adjusted validation rmsle\n\n# starting prediction from last day of validation training\nqvt = (d.Date == ddate.isoformat())\n# query to subset data to validation range\ndvlow = ddate + timedelta(1+skip)\ndvupp = ddate + timedelta(nhorizon)\nqvd = (dvlow.isoformat() <= d.Date) & (d.Date <= dvupp.isoformat())\n\n# starting prediction from last day of full training\nqst = (d.Date == dmax.isoformat())\n# query to subset data to test range\ndslow = dmax + timedelta(1+skip)\ndsupp = dmax + timedelta(nhorizon)\nqsd = (dslow.isoformat() <= d.Date) & (d.Date <= dsupp.isoformat())\n\nprint('#',dvlow, dvupp, dslow, dsupp)\n\nkeys = d['key'].drop_duplicates()\nm = 0.0\n\nprint(f'# post-process {mname}')\nfor i in range(ny):\n    ya = yv[i]+'_preda'\n    d[ya] = np.nan  \n    \n    # loop over each location, post-process, and save back into main table\n    for k in keys:\n        # validation\n        qvp = (d.key==k) & qvd\n        qvs = (d.key==k) & qvt\n        pred = d.loc[qvp,yv[i]+'_pred'].values\n        start = d.loc[qvs,yv[i]].values\n        preda = kpp(i, k, pred, start)\n        d.loc[qvp, ya] = preda\n        \n        # test\n        qsp = (d.key==k) & qsd\n        qss = (d.key==k) & qst\n        pred = d.loc[qsp,yv[i]+'_pred'].values\n        start = d.loc[qss,yv[i]].values\n        preda = kpp(i, k, pred, start)\n        d.loc[qsp, ya] = preda\n        \n        \n    # rmse = np.sqrt(mean_squared_error(d.loc[qd,ya], d.loc[qd,yv[i]]))\n    rmse = np.sqrt(mean_squared_error(d.loc[qvd,ya], d.loc[qvd,yv[i]+'r']))\n    print(f'# {rmse:.6f}')\n    m += rmse/2\n    \nprint(f'# {m:.6f}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# sort to find worst predictions of y0\nlocs = locs.sort_values('rmse0', ascending=False)\nlocs[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# plot worst fits of y0\nfor i in range(5):\n    li = locs.index[i]\n    plt.plot(y_truea[0][:,li])\n    plt.plot(y_preda[0][:,li])\n    plt.title(locs.loc[li,'Loc'])\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# plt.plot(d.loc[d.Loc=='Belgium','y0'][39:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# sort to find worst predictions of y1\nlocs = locs.sort_values('rmse1', ascending=False)\nlocs[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# plot worst fits of y1\nfor i in range(5):\n    li = locs.index[i]\n    plt.plot(y_truea[1][:,li])\n    plt.plot(y_preda[1][:,li])\n    plt.title(locs.loc[li,'Loc'])\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"dmax","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"tmax","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"fmin","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# compute public lb score\nif not prev_test:\n    # q = (d.Date >= fmin) & (d.Date > ddate.isoformat()) & (d.Date <= tmax)\n    q = (d.Date >= '2020-04-02') & (d.Date <= tmax)\n    # q = (d.Date >= tmax) & (d.Date <= tmax)\n    print(f'# {fmin} {ddate.isoformat()} {tmax} {sum(q)//ns} {mname}')\n    s0 = np.sqrt(mean_squared_error(d.loc[q,'y0r'],d.loc[q,'y0_pred']))\n    s1 = np.sqrt(mean_squared_error(d.loc[q,'y1r'],d.loc[q,'y1_pred']))\n    print(f'# CC \\t {s0:.6f}')\n    print(f'# Fa \\t {s1:.6f}')\n    print(f'# Mean \\t {(s0+s1)/2:.6f}')\n    \n    s0 = np.sqrt(mean_squared_error(d.loc[q,'y0r'],d.loc[q,'y0_preda']))\n    s1 = np.sqrt(mean_squared_error(d.loc[q,'y1r'],d.loc[q,'y1_preda']))\n    print()\n    print(f'# CC \\t {s0:.6f}')\n    print(f'# Fa \\t {s1:.6f}')\n    print(f'# Mean \\t {(s0+s1)/2:.6f}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# d.loc[d.Date=='2020-04-08',['ForecastId','y0_pred','y1_pred']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# sub = d.loc[d.ForecastId > 0, ['ForecastId','ConfirmedCases','Fatalities',\n#                            'y0','y1','y0_preda','y1_preda','Date','dint']]\n# sub.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# nnls to estimate blending weights\nif blend:\n    print('blending with',blender)\n    sub = d.loc[d.ForecastId > 0, ['ForecastId','ConfirmedCases','Fatalities',\n                                   'y0','y1','y0_preda','y1_preda','Date','dint']]\n    sub['dint'] = sub['dint'] - sub['dint'].min()\n    # original data, nonmonotonic in some places\n    sub['y0r'] = np.log1p(sub.ConfirmedCases)\n    sub['y1r'] = np.log1p(sub.Fatalities)\n    sub['ConfirmedCases'] = sub.ConfirmedCases.astype(float)\n    sub['Fatalities'] = sub.Fatalities.astype(float)\n\n    print(sub.shape)\n    print(sub['dint'].describe())\n    hmax = np.max(sub.dint.values) + 1\n    print(hmax)\n    \n    # add nq\n    bs = pd.read_csv(path+blender[0]+'.csv')\n    print(bs.shape)\n    bs['nq0'] = np.log1p(bs.ConfirmedCases)\n    bs['nq1'] = np.log1p(bs.Fatalities)\n    bs.drop(['ConfirmedCases','Fatalities'],axis=1,inplace=True)\n    sub = sub.merge(bs, how='left', on='ForecastId')\n    sub['nq0'] = sub['nq0'].fillna(sub['y0'])\n    sub['nq1'] = sub['nq1'].fillna(sub['y1'])\n\n    # add kaz\n    bs = pd.read_csv(path+blender[1]+'.csv')\n    print(bs.shape)\n    bs['kaz0'] = np.log1p(bs.ConfirmedCases)\n    bs['kaz1'] = np.log1p(bs.Fatalities)\n    bs.drop(['ConfirmedCases','Fatalities'],axis=1,inplace=True)\n    sub = sub.merge(bs, how='left', on='ForecastId')\n    \n    for i in range(ny): sub[mname+str(i)] = sub[yv[i]+'_preda']\n        \n    # qv = (sub.Date >= '2020-04-09') & (sub.Date <= tmax)\n    qv = (sub.Date > tmax)\n    a = sub[qv].copy()\n\n#     # intercept estimate is 0\n#     # a['intercept0'] = 1.0\n#     # a['intercept1'] = 1.0\n#     # m = ['intercept','b0g','v0e','o0e',mname]\n#     # m = ['kaz',mname]\n#     m = ['nq','kaz',mname]\n#     print(m)\n#     n = a.shape[0]\n#     wt= np.zeros((2,len(m)))\n#     s = 0\n#     for i in range(ny):\n#         mi = [c+str(i) for c in m]\n#         wt[i], rnorm = nnls(a[mi].values, a[yv[i]+'r'].values)\n#         r = rnorm/np.sqrt(n)\n#         print(i, wt[i], f'{sum(wt[i]):.6f}', f'{r:.6f}')\n#         s += 0.5*r\n#     print(f'{s:.6f}')\n#     print()\n    print(a[['nq0','kaz0',mname+'0','nq1','kaz1',mname+'1']].corr())","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#              nq0      kaz0    gbt5c0       nq1      kaz1    gbt5c1\n# nq0     1.000000  0.994898  0.990380  0.921206  0.925135  0.928614\n# kaz0    0.994898  1.000000  0.993709  0.912327  0.919076  0.922941\n# gbt5c0  0.990380  0.993709  1.000000  0.910899  0.915789  0.926058\n# nq1     0.921206  0.912327  0.910899  1.000000  0.995339  0.993934\n# kaz1    0.925135  0.919076  0.915789  0.995339  1.000000  0.994931\n# gbt5c1  0.928614  0.922941  0.926058  0.993934  0.994931  1.000000\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# ['nq', 'kaz', 'gbt5a']\n# 0 [0.459556 0.236734 0.300961] 0.997251 0.151962\n# 1 [0.665729 0.12441  0.20578 ] 0.995918 0.157286\n# 0.154624","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# create blended submission, set weights by hand after looking at validation nnls\nif blend:\n    # blend\n    sub['ConfirmedCases'] = np.expm1(0.997*(0.5 * sub['nq0'] + \\\n                                            0.2 * sub['kaz0'] + \\\n                                            0.3 * sub['y0_preda']))\n    sub['Fatalities'] = np.expm1(0.996*(0.666666 * sub['nq1'] + \\\n                                        0.133333 * sub['kaz1'] + \\\n                                        0.2      * sub['y1_preda']))\n            \nelse:\n    # create submission without any blending with others\n    sub = d.loc[d.ForecastId > 0, ['ForecastId','y0_pred','y1_pred']]\n    print(sub.shape)\n\n    sub['ConfirmedCases'] = np.expm1(sub['y0_preda'])\n    sub['Fatalities'] = np.expm1(sub['y1_preda'])    \n\nsub0 = sub.copy()\nprint(sub0.shape)\nsub = sub[['ForecastId','ConfirmedCases','Fatalities']]\n\nos.makedirs('sub',exist_ok=True)\nfname = mname + '.csv'\nsub.to_csv(fname, index=False)\nprint(fname, sub.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sub.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# final day adjustment as per northquay\npname = mname\npred = sub.copy()\n# pred = pd.read_csv(mname + '.csv')\n\n# pname = 'kaz0m'\n# pred = pd.read_csv('../week3/sub/'+pname+'.csv')\n\npred_orig = pred.copy()\n\nif prev_test:\n    test = pd.read_csv('../'+pw+'/test.csv')\nelse:\n    test = pd.read_csv(pathk+'test.csv')\n\ntest[cp] = test[cp].fillna('')\n\n# test.Date = pd.to_datetime(test.Date)\n# train.Date = pd.to_datetime(train.Date)\n\n# TODAY = datetime.datetime(  *datetime.datetime.today().timetuple()[:3] )\n# TODAY = date(2020, 4, 7)\n\n# shift day back one to match wm adjustment\n\nprint(TODAY)\n\nfinal_day = wm[wm.Date == TODAY].copy()\nfinal_day['cases_final'] = np.expm1(final_day.TotalCases)\nfinal_day['cases_chg'] = np.expm1(final_day.NewCases)\nfinal_day['deaths_final'] = np.expm1(final_day.TotalDeaths)\nfinal_day['deaths_chg'] = np.expm1(final_day.NewDeaths)\n\n\n# test.rename(columns={'Country_Region': 'Country'}, inplace=True)\n# test['Place'] = test.Country +  test.Province_State.fillna(\"\")\n\n# final_day = pd.read_excel(path + '../week3/nq/' + 'final_day.xlsx')\n# final_day = final_day.iloc[1:, :5]\n# final_day = final_day.fillna(0)\n# final_day.columns = ['Country', 'cases_final', 'cases_chg', \n#                      'deaths_final', 'deaths_chg']\n\nfinal_day = final_day[['Country_Region','Province_State','cases_final','cases_chg',\n                      'deaths_final','deaths_chg']].fillna(0)\n# final_day = final_day.drop('Date', axis=1).reset_index(drop=True)\nfinal_day = final_day.sort_values('cases_final', ascending=False)\n\nprint()\nprint('final_day')\nprint(final_day.head(n=10), final_day.shape)\n\n# final_day.Country.replace({'Taiwan': 'Taiwan*',\n#                            'S. Korea': 'Korea, South',\n#                            'Myanmar': 'Burma',\n#                            'Vatican City': 'Holy See',\n#                            'Ivory Coast':  \"Cote d'Ivoire\",\n                        \n#                           },\n#                          inplace=True)\n\n\npred = pd.merge(pred, test, how='left', on='ForecastId')\nprint()\nprint('pred')\nprint(pred.head(n=10), pred.shape)\n\n# pred = pd.merge(pred, test[test.Province_State.isnull()], how='left', on='ForecastId')\n\n# compare = pd.merge(pred[pred.Date == TODAY], final_day, on= [ 'Country'],\n#                            validate='1:1')\n\ncompare = pd.merge(pred[pred.Date == TODAY], final_day, on=cp, validate='1:1')\n\ncompare['c_li'] = np.round(np.log(compare.cases_final + 1) - np.log(compare.ConfirmedCases + 1), 2)\ncompare['f_li'] = np.round(np.log(compare.deaths_final + 1) - np.log(compare.Fatalities + 1), 2)\n\nprint()\nprint('compare')\nprint(compare.head(n=10), compare.shape)\nprint(compare.describe())\n\n# compare[compare.c_li > 0.3][['Country', 'ConfirmedCases', 'Fatalities',\n#                                         'cases_final', 'cases_chg',\n#                                     'deaths_final', 'deaths_chg',\n#                                             'c_li', 'f_li']]\n\n# compare[compare.c_li > 0.15][['Country', 'ConfirmedCases', 'Fatalities',\n#                                         'cases_final', 'cases_chg',\n#                                     'deaths_final', 'deaths_chg',\n#                                             'c_li', 'f_li']]\n\n# compare[compare.f_li > 0.3][['Country', 'ConfirmedCases', 'Fatalities',\n#                                         'cases_final', 'cases_chg',\n#                                     'deaths_final', 'deaths_chg',\n#                                             'c_li', 'f_li']]\n\n\n# compare[compare.f_li > 0.15][['Country', 'ConfirmedCases', 'Fatalities',\n#                                         'cases_final', 'cases_chg',\n#                                     'deaths_final', 'deaths_chg',\n#                                             'c_li', 'f_li']]\n\n# compare[compare.c_li < -0.15][['Country', 'ConfirmedCases', 'Fatalities',\n#                                         'cases_final', 'cases_chg',\n#                                     'deaths_final', 'deaths_chg',\n#                                             'c_li', 'f_li']]\n\n# compare[compare.f_li < -0.2][['Country', 'ConfirmedCases', 'Fatalities',\n#                                         'cases_final', 'cases_chg',\n#                                     'deaths_final', 'deaths_chg',\n#                                             'c_li', 'f_li']]\n\nfixes = pd.merge(pred[pred.Date >= TODAY], \n                     compare[cp + ['c_li', 'f_li']], on=cp)\n\n\nfixes['c_li'] = np.where( fixes.c_li < 0,\n                             0,\n                                 fixes.c_li)\nfixes['f_li'] = np.where( fixes.f_li < 0,\n                             0,\n                                 fixes.f_li)\n\nfixes['total_fixes'] = fixes.c_li**2 + fixes.f_li**2\n\nprint()\nprint('most fixes')\nprint(fixes.groupby(cp).last().sort_values(['total_fixes','Date'], ascending = False).head(n=10))\n\n# adjustment\nfixes['Fatalities'] = np.round(np.exp((np.log(fixes.Fatalities + 1) + fixes.f_li))-1, 3)\nfixes['ConfirmedCases'] = np.round(np.exp((np.log(fixes.ConfirmedCases + 1) + fixes.c_li))-1, 3)\n\n\nfix_ids = fixes.ForecastId.unique()\nlen(fix_ids)\n\ncols = ['ForecastId', 'ConfirmedCases', 'Fatalities']\n\n\nfixed = pd.concat((pred.loc[~pred.ForecastId.isin(fix_ids),cols],\n    fixes[cols])).sort_values('ForecastId')\n\n\n# fixed.head()\n# fixed.tail()\n\n# len(pred_orig)\n# len(fixed)\n\nfname = pname + '_updated.csv'\nfixed.to_csv(fname, index=False)\nprint(fname, fixed.shape)\nfixed.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"compare[compare.Country_Region=='US'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# fixed[5:15]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sum(qv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# sub","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# merge final predictions back into main table\nsub1 = fixed.copy()\nfor i in range(ny): \n    mi = mname + str(i)\n    if mi in d.columns: d.drop(mi, axis=1, inplace=True)\n    sub1[mi] = np.log1p(sub1[ynames[i]])\n    sub1.drop(ynames[i],axis=1,inplace=True)\nd = d.merge(sub1, how='left', on='ForecastId')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"fixed.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"ddate","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# compute public lb score after averaging with others\nif not prev_test:\n    # q = (d.Date >= fmin) & (d.Date > ddate.isoformat()) & (d.Date <= tmax)\n    q = (d.Date >= '2020-04-02') & (d.Date <= tmax)\n    # q = (d.Date >= tmax) & (d.Date <= tmax)\n    print(f'# {fmin} {ddate.isoformat()} {tmax} {sum(q)/ns} {mname}')\n    s0 = np.sqrt(mean_squared_error(d.loc[q,'y0r'],d.loc[q,mname+'0']))\n    s1 = np.sqrt(mean_squared_error(d.loc[q,'y1r'],d.loc[q,mname+'1']))\n    print(f'# CC \\t {s0:.6f}')\n    print(f'# Fa \\t {s1:.6f}')\n    print(f'# Mean \\t {(s0+s1)/2:.6f}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# save oof predictions\novars = ['Id','ForecastId','Country_Region','Province_State','Loc',\n         'y0','y1','y0_pred','y1_pred',mname+'0',mname+'1']\noof = d.loc[:,ovars]\n# oof = oof.rename(mapper={'y0_pred':mname+'0','y1_pred':mname+'1'}, axis=1)\nos.makedirs('oof',exist_ok=True)\nfname = 'oof/' + mname + '.csv'\noof.to_csv(fname, index=False)\nprint(fname, oof.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"if save_data:\n    os.makedirs('data',exist_ok=True)\n    fname = 'data/' + mname + '_d.csv'\n    d.to_csv(fname, index=False)\n    print(fname, d.shape)\n    \n    fname = 'data/' + mname + '_x_train.csv'\n    x_train.to_csv(fname, index=False)\n    print(fname, d.shape)\n    \n    fname = 'data/' + mname + '_x_val.csv'\n    x_val.to_csv(fname, index=False)\n    print(fname, d.shape)\n    \n    fname = 'data/' + mname + '_x_full.csv'\n    x_full.to_csv(fname, index=False)\n    print(fname, d.shape)\n    \n    fname = 'data/' + mname + '_x_test.csv'\n    x_test.to_csv(fname, index=False)\n    print(fname, d.shape)\n    \n#     fname = 'data/' + mname + '_y_train.csv'\n#     y_train[0].to_csv(fname, index=False)\n#     print(fname, d.shape)\n    \n#     fname = 'data/' + mname + '_y_val.csv'\n#     y_val[0].to_csv(fname, index=False)\n#     print(fname, d.shape)\n    \n#     fname = 'data/' + mname + '_y_full.csv'\n#     y_full[0].to_csv(fname, index=False)\n#     print(fname, d.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# set(features[i]) - set(lags)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# set(lags) - set(features[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"len(features[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"x_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"np.log(56)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"pd.set_option('display.max_rows', 150)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# q = (d.Date >= '2020-04-02') & (d.Loc=='Cabo Verde')\n# q = (d.Date >= '2020-04-02') & (d.Loc=='Congo (Brazzaville)')\nq = (d.Date >= '2020-04-02') & (d.Loc=='Somalia')\nd.loc[q,['Date','ForecastId','y0','y1','y0r','y1r',\n                        mname + str(0),mname+str(1)]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# most fixes\n#                                     ForecastId  ConfirmedCases   Fatalities  \\\n# Country_Region      Province_State                                            \n# Cabo Verde                                1591       83.069233     2.661215   \n# Congo (Brazzaville)                       3827      188.486683    10.366824   \n# Jamaica                                   6364      310.090653     9.060071   \n# Slovakia                                  9374     2407.449830    17.087912   \n# Netherlands         Aruba                 7869      207.263786     1.729323   \n# Timor-Leste                              10019       15.586367     0.892582   \n# Tanzania                                  9933      250.855338    11.648181   \n# Somalia                                   9460      213.936255    15.164921   \n# Gabon                                     5375      169.672938     2.876582   \n# US                  Maryland             11137    24302.549228  1205.949083   \n\n#                                           Date  c_li  f_li  total_fixes  \n# Country_Region      Province_State                                       \n# Cabo Verde                          2020-05-14  0.49  0.00       0.2401  \n# Congo (Brazzaville)                 2020-05-14  0.28  0.00       0.0784  \n# Jamaica                             2020-05-14  0.18  0.03       0.0333  \n# Slovakia                            2020-05-14 -0.00  0.17       0.0289  \n# Netherlands         Aruba           2020-05-14  0.00  0.14       0.0196  \n# Timor-Leste                         2020-05-14  0.13  0.00       0.0169  \n# Tanzania                            2020-05-14  0.12  0.02       0.0148  \n# Somalia                             2020-05-14  0.02  0.10       0.0104  \n# Gabon                               2020-05-14  0.09  0.00       0.0081  \n# US                  Maryland        2020-05-14  0.04  0.08       0.0080  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# plot actual and predicted curves over time for specific locations\n# locs = ['China Tibet','China Xinjiang','China Hong Kong', 'China Macau',\n#         'Spain','Italy','India',\n#         'US Washington','US New York','US California',\n#         'US North Carolina','US Ohio']\n# xlab = ['03-12','03-18','03-25','04-01','04-08','04-15','04-22']\n# plot all locations\nlocs = d['Loc'].drop_duplicates()\nfor loc in locs:\n    plt.figure(figsize=(14,2))\n    \n    # fig, ax = plt.subplots()\n    # fig.autofmt_xdate()\n    \n    for i in range(ny):\n    \n        plt.subplot(1,2,i+1)\n        plt.plot(d.loc[d.Loc==loc,[yv[i],'Date']].set_index('Date'))\n        plt.plot(d.loc[d.Loc==loc,[mname + str(i),'Date']].set_index('Date'))\n        # plt.plot(d.loc[d.Loc==loc,[yv[i]+'_pred','Date']].set_index('Date'))\n        # plt.plot(d.loc[d.Loc==loc,[yv[i]]])\n        # plt.plot(d.loc[d.Loc==loc,[yv[i]+'_pred']])\n        # plt.xticks(np.arange(len(xlab)), xlab, rotation=-45)\n        # plt.xticks(np.arange(12), calendar.month_name[3:5], rotation=20)\n        # plt.xticks(rotation=-45)\n        plt.xticks([])\n        plt.title(loc + ' ' + ynames[i])\n       \n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"fixed.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# actual submission created locally, should be very similar to that created above\nsub = pd.read_csv(\"/kaggle/input/gbt5fx/gbt5f_updated.csv\")\nprint(sub.describe())\n\nfname = 'submission.csv'\nsub.to_csv(fname, index=False)\nprint(fname, sub.shape)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"2f8637a583ba4bfa8946994db32cde94":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"624f68aaf5684d2ca6f88c940feb59e0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6827a0694afe427ba904c7fe40befda3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b66678a5c7b64b90a555ccfeabb305c2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"b937a9e2638e4fb4932fc16802ad725f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_624f68aaf5684d2ca6f88c940feb59e0","placeholder":"​","style":"IPY_MODEL_cc713908016240288dac63ce30f32779","value":" 163/163 [00:05&lt;00:00, 31.15it/s]"}},"ba5137f29e61409db2c60bf628df8bcd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"100%","description_tooltip":null,"layout":"IPY_MODEL_6827a0694afe427ba904c7fe40befda3","max":163,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b66678a5c7b64b90a555ccfeabb305c2","value":163}},"cc713908016240288dac63ce30f32779":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ebb54321a1de47a1900c2b6f56668c6c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ba5137f29e61409db2c60bf628df8bcd","IPY_MODEL_b937a9e2638e4fb4932fc16802ad725f"],"layout":"IPY_MODEL_2f8637a583ba4bfa8946994db32cde94"}}},"version_major":2,"version_minor":0}}},"nbformat":4,"nbformat_minor":4}