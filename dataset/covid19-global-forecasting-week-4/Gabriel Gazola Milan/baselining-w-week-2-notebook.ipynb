{"cells":[{"metadata":{"trusted":false},"cell_type":"code","source":"# Imports\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.style.use('fivethirtyeight')\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, LSTM\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom lightgbm import LGBMRegressor\nimport time\nfrom sklearn.model_selection import cross_val_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Loading data\ndf_train = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-4/train.csv')\ndf_test = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-4/test.csv')\nsubmission = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-4/submission.csv')\ndf_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_train['Date'] = pd.to_datetime(df_train['Date'], infer_datetime_format=True)\ndf_test['Date'] = pd.to_datetime(df_test['Date'], infer_datetime_format=True)\ndf_intersection = df_test[df_test['Date'] <= np.max(df_train['Date'])]\ndf_intersection","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Following the idea at\n# https://www.kaggle.com/ranjithks/25-lines-of-code-results-better-score#Fill-NaN-from-State-feature\n# Filling NaN states with the Country\n\nEMPTY_VAL = \"EMPTY_VAL\"\n\ndef fillState(state, country):\n    if state == EMPTY_VAL: return country\n    return state\n\ndf_train['Province_State'].fillna(EMPTY_VAL, inplace=True)\ndf_train['Province_State'] = df_train.loc[:, ['Province_State', 'Country_Region']].apply(lambda x : fillState(x['Province_State'], x['Country_Region']), axis=1)\n\ndf_test['Province_State'].fillna(EMPTY_VAL, inplace=True)\ndf_test['Province_State'] = df_test.loc[:, ['Province_State', 'Country_Region']].apply(lambda x : fillState(x['Province_State'], x['Country_Region']), axis=1)\n\ndf_intersection['Province_State'].fillna(EMPTY_VAL, inplace=True)\ndf_intersection['Province_State'] = df_intersection.loc[:, ['Province_State', 'Country_Region']].apply(lambda x : fillState(x['Province_State'], x['Country_Region']), axis=1)\n\ndf_intersection.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Adding validation data into the Intersection DF\nstates = sorted(set(df_intersection['Province_State']))\ndf_intersection['ConfirmedCases'] = float('NaN')\ndf_intersection['Fatalities'] = float('NaN')\n\nfor state in states:\n    dates = sorted(set(df_intersection[df_intersection['Province_State'] == state]['Date']))\n    min_date = np.min(dates)\n    max_date = np.max(dates)\n    idx = df_intersection[df_intersection['Province_State'] == state].index\n    values = df_train[(df_train['Province_State'] == state) & (df_train['Date'] >= min_date) & (df_train['Date'] <= max_date)][['ConfirmedCases', 'Fatalities']].values\n    values = pd.DataFrame(values, index = list(idx), columns=['ConfirmedCases', 'Fatalities'])\n    df_intersection['ConfirmedCases'].loc[idx] = values['ConfirmedCases']\n    df_intersection['Fatalities'].loc[idx] = values['Fatalities']\ndf_intersection","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Filtering data for public leaderboard\ndf_train = df_train[df_train['Date'] < np.min(df_test['Date'])]\n# Check if any Province_State value on test dataset isn't on train dataset\n# If nothing prints, everything is okay\nfor a in set(df_test['Province_State']):\n    if a not in set(df_train['Province_State']):\n        print (a)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Making Date become timestamp\ndf_train['Date'] = df_train['Date'].apply(lambda s: time.mktime(s.timetuple()))\ndf_test['Date'] = df_test['Date'].apply(lambda s: time.mktime(s.timetuple()))\ndf_intersection['Date'] = df_intersection['Date'].apply(lambda s: time.mktime(s.timetuple()))\n\nmin_timestamp = np.min(df_train['Date'])\ndf_train['Date'] = df_train['Date'].apply(lambda s: (s - min_timestamp) / 86400.0)\ndf_test['Date'] = df_test['Date'].apply(lambda s: (s - min_timestamp) / 86400.0)\ndf_intersection['Date'] = df_intersection['Date'].apply(lambda s: (s - min_timestamp) / 86400.0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Generating features based on evolution of COVID-19\n# Idea from https://www.kaggle.com/binhlc/sars-cov-2-exponential-model-week-2\nevolution = [1, 2, 5, 10, 20, 50, 100, 200, 500, 1000]\ndef generateFeatures (state):\n    should_filter = False\n    train = df_train[df_train['Province_State'] == state].drop(columns=['Id'])\n    test  = df_test[df_test['Province_State'] == state].drop(columns=['ForecastId'])\n    y_cases = train['ConfirmedCases']\n    y_fatal = train['Fatalities']\n    for evo_type in ['ConfirmedCases', 'Fatalities']:\n        for value in evolution:\n            min_day = train[train[evo_type] >= value]['Date']\n            if min_day.count() > 0:\n                min_day = np.min(min_day)\n                should_filter = True\n            else:\n                print (\"{} -> Not found min_day for {} {}\".format(state, evo_type, value))\n                continue\n            train['{}_{}'.format(evo_type, value)] = train['Date'].apply(lambda x: x - min_day)\n            test ['{}_{}'.format(evo_type, value)] = test ['Date'].apply(lambda x: x - min_day)\n    train.drop(columns=['ConfirmedCases', 'Fatalities', 'Province_State', 'Country_Region'], inplace=True)\n    test.drop(columns=['Province_State', 'Country_Region'], inplace=True)\n    if should_filter:\n        idx     = train[train['ConfirmedCases_1'] >= 0].index\n        train   = train.loc[idx]\n        y_cases = y_cases.loc[idx]\n        y_fatal = y_fatal.loc[idx]\n    return train, test, y_cases, y_fatal\n\ndataframes = {}\nstates = sorted(set(df_train['Province_State']))\nfor state in states:\n    dataframes[state] = {}\n    train, test, y_cases, y_fatal = generateFeatures(state)\n    dataframes[state]['train']   = train\n    dataframes[state]['test']    = test\n    dataframes[state]['y_cases'] = y_cases\n    dataframes[state]['y_fatal'] = y_fatal","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Checking shapes\nstate = 'Georgia'\nprint (dataframes[state]['train'].shape, dataframes[state]['test'].shape, dataframes[state]['y_cases'].shape, dataframes[state]['y_fatal'].shape)\ndataframes[state]['test'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from tqdm import tqdm\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression, BayesianRidge, Lasso\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.base import TransformerMixin\nfrom sklearn.datasets import make_regression\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression, Ridge\nfrom sklearn.ensemble.weight_boosting import AdaBoostRegressor\nfrom sklearn.linear_model.base import LinearRegression\nfrom sklearn.linear_model.passive_aggressive import PassiveAggressiveRegressor\nfrom sklearn.linear_model.theil_sen import TheilSenRegressor\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n\ndef handle_predictions (predictions, lowest = 0):\n    #predictions = np.round(predictions, 0)\n    # Predictions can't be negative\n    predictions[predictions < 0] = 0\n    # Predictions can't decrease from greatest value on train dataset\n    predictions[predictions < lowest] = lowest\n    # Predictions can't decrease over time\n    for i in range(1, len(predictions)):\n        if predictions[i] < predictions[i - 1]:\n            predictions[i] = predictions[i - 1]\n    #return predictions.astype(int)\n    return predictions\n\ndef fillSubmission (state, column, values,):\n    idx = df_test[df_test['Province_State'] == state].index\n    values = pd.DataFrame(np.array(values), index = list(idx), columns=[column])\n    submission[column].loc[idx] = values[column]\n    return submission\n\ndef avg_rmsle():\n    idx = df_intersection.index\n    my_sub = submission.loc[idx][['ConfirmedCases', 'Fatalities']]\n    cases_pred = my_sub['ConfirmedCases'].values\n    fatal_pred = my_sub['Fatalities'].values\n    pred = np.append(cases_pred, fatal_pred)\n    cases_targ = df_intersection.loc[idx]['ConfirmedCases'].values\n    fatal_targ = df_intersection.loc[idx]['Fatalities'].values\n    targ = np.append(cases_targ, fatal_targ)\n    score = np.sqrt(mean_squared_log_error( targ, pred ))\n    return score\n\nclass CustomEnsemble (BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models, meta_model):\n        self.models = models\n        self.meta_model = meta_model\n    def fit(self,X,y):\n        predictions = np.zeros((X.shape[0], len(self.models)))\n        for i, model in enumerate(self.models):\n            model.fit (X, y)\n            predictions[:,i] = model.predict(X)\n        self.meta_model.fit(predictions, y)\n    def predict(self,X):\n        predictions = np.zeros((X.shape[0], len(self.models)))\n        for i, model in enumerate(self.models):\n            predictions[:,i] = model.predict(X)\n        return self.meta_model.predict(predictions)\n    def __str__ (self):\n        return \"<CustomEnsemble (meta={}, models={})>\".format(self.meta_model, self.models)\n\ndef make_combinations (iterable):\n    from itertools import combinations\n    my_combs = []\n    for item in iterable.copy():\n        iterable.remove(item)\n        for i in range(len(iterable)):\n            for comb in combinations(iterable, i+1):\n                my_combs.append((item, comb))\n        iterable.append(item)\n    return my_combs\n\ntest_models = [\n    make_pipeline(PolynomialFeatures(2), LinearRegression()),              # 0.8990346097108978\n    make_pipeline(PolynomialFeatures(2), TheilSenRegressor()),             # 0.8910456039208402\n    make_pipeline(PolynomialFeatures(2), BayesianRidge()),                 # 0.8997409933399905\n    make_pipeline(PolynomialFeatures(2), Lasso()),                         # 0.8920475587104756\n]\n\n# for model in test_models:\n#     print (' * Model: {}'.format(model))\n#     for state in states:\n#         train   = dataframes[state]['train']\n#         test    = dataframes[state]['test']\n#         y_cases = dataframes[state]['y_cases']\n#         y_fatal = dataframes[state]['y_fatal']\n#         model.fit(train, y_cases)\n#         cases = model.predict(test)\n#         lowest_pred = np.max(df_train[df_train['Province_State'] == state]['ConfirmedCases'].values)\n#         cases = handle_predictions(cases, lowest_pred)\n#         submission = fillSubmission (state, 'ConfirmedCases', cases)\n#         model.fit(train, y_fatal)\n#         fatal = model.predict(test)\n#         lowest_pred = np.max(df_train[df_train['Province_State'] == state]['Fatalities'].values)\n#         fatal = handle_predictions(fatal, lowest_pred)\n#         submission = fillSubmission (state, 'Fatalities', fatal)\n#     print ('   - Score: {}'.format(avg_rmsle()))\nmy_combs = make_combinations(test_models)\nbest = 10000\nresults = []\nwith tqdm(total = len(my_combs) * len(states)) as pbar:\n    for comb in my_combs:\n        for state in states:\n            train   = dataframes[state]['train']\n            test    = dataframes[state]['test']\n            y_cases = dataframes[state]['y_cases']\n            y_fatal = dataframes[state]['y_fatal']\n            model = CustomEnsemble(list(comb[1]), comb[0])\n            model.fit(train, y_cases)\n            cases = model.predict(test)\n            lowest_pred = np.max(df_train[df_train['Province_State'] == state]['ConfirmedCases'].values)\n            cases = handle_predictions(cases, lowest_pred)\n            submission = fillSubmission (state, 'ConfirmedCases', cases)\n            model.fit(train, y_fatal)\n            fatal = model.predict(test)\n            lowest_pred = np.max(df_train[df_train['Province_State'] == state]['Fatalities'].values)\n            fatal = handle_predictions(fatal, lowest_pred)\n            submission = fillSubmission (state, 'Fatalities', fatal)\n            pbar.update(1)\n        score = avg_rmsle()\n        results.append(score)\n        if (score < best):\n            print (\"Score {:.4f} is better than previous best. Saving...\".format(score))\n            best = score\n            best_model = model","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Making predicitons using the best model for the private leaderboard\n# Load raw train\ndf_train = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-4/train.csv')\n# Handle it\ndf_train['Date'] = pd.to_datetime(df_train['Date'], infer_datetime_format=True)\nEMPTY_VAL = \"EMPTY_VAL\"\ndf_train['Province_State'].fillna(EMPTY_VAL, inplace=True)\ndf_train['Province_State'] = df_train.loc[:, ['Province_State', 'Country_Region']].apply(lambda x : fillState(x['Province_State'], x['Country_Region']), axis=1)\ndf_train['Date'] = df_train['Date'].apply(lambda s: time.mktime(s.timetuple()))\ndf_train['Date'] = df_train['Date'].apply(lambda s: (s - min_timestamp) / 86400.0)\n# Re-generate features and predict\ndataframes = {}\nstates = sorted(set(df_train['Province_State']))\nfor state in states:\n    dataframes[state] = {}\n    train, test, y_cases, y_fatal = generateFeatures(state)\n    model = best_model\n    model.fit(train, y_cases)\n    cases = model.predict(test)\n    lowest_pred = np.max(df_train[df_train['Province_State'] == state]['ConfirmedCases'].values)\n    cases = handle_predictions(cases, lowest_pred)\n    submission = fillSubmission (state, 'ConfirmedCases', cases)\n    model.fit(train, y_fatal)\n    fatal = model.predict(test)\n    lowest_pred = np.max(df_train[df_train['Province_State'] == state]['Fatalities'].values)\n    fatal = handle_predictions(fatal, lowest_pred)\n    submission = fillSubmission (state, 'Fatalities', fatal)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def checkState (state):\n    idx = df_test[df_test['Province_State'] == state].index\n    return submission.loc[idx]\n\ndef plotStatus (states):\n    plt.figure(figsize=(14,8))\n    plt.title('COVID-19 on {}'.format(states))\n    if type(states) == list:\n        legend = []\n        for state in states:\n            df = df_train[df_train['Province_State'] == state]\n            idx = df_test[df_test['Province_State'] == state].index\n            plt.xlabel('#Days since dataset')\n            plt.ylabel('Number')\n            plt.plot(df['Date'], df['ConfirmedCases'])\n            plt.plot(range(int(np.max(df['Date'])), int(np.max(df['Date'])) + len(idx)), submission['ConfirmedCases'].loc[idx])\n            plt.plot(df['Date'], df['Fatalities'])\n            plt.plot(range(int(np.max(df['Date'])), int(np.max(df['Date'])) + len(idx)), submission['Fatalities'].loc[idx])\n            legend.append('{} confirmed cases'.format(state))\n            legend.append('{} predicted cases'.format(state))\n            legend.append('{} fatalities'.format(state))\n            legend.append('{} predicted fatalities'.format(state))\n        plt.legend(legend)\n    else:\n        state = states\n        df = df_train[df_train['Province_State'] == state]\n        plt.figure(figsize=(14,8))\n        plt.xlabel('#Days since dataset')\n        plt.ylabel('Number')\n        plt.plot(df['Date'], df['ConfirmedCases'])\n        plt.plot(df['Date'], df['Fatalities'])\n        plt.legend(['Confirmed cases', 'Fatalities'])\n    plt.show()\n\nraw_train = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-4/train.csv')\nraw_test  = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-4/test.csv')\nraw_train['Date'] = pd.to_datetime(raw_train['Date'], infer_datetime_format=True)\nraw_test['Date']  = pd.to_datetime(raw_test['Date'], infer_datetime_format=True)\n\ndef rmsle (state):\n    idx = df_intersection[df_intersection['Province_State'] == state].index\n    my_sub = submission.loc[idx][['ConfirmedCases', 'Fatalities']]\n    cases_pred = my_sub['ConfirmedCases'].values\n    fatal_pred = my_sub['Fatalities'].values\n    cases_targ = df_intersection.loc[idx]['ConfirmedCases'].values\n    fatal_targ = df_intersection.loc[idx]['Fatalities'].values\n    cases = np.sqrt(mean_squared_log_error( cases_targ, cases_pred ))\n    fatal = np.sqrt(mean_squared_log_error( fatal_targ, fatal_pred ))\n    return cases, fatal\n\nplotStatus(['Zhejiang'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"for state in states:\n    print (\"  * {}\".format(state))\n    scores = rmsle(state)\n    print (\"    - {}\\n    - {}\".format(scores[0], scores[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#\n#  FUTURE:\n#  - Get datasets with no confirmed cases and try to implement other features (country relationships maybe? continent? distance between countries?)\n#  - Tune more models\n#  - Check this out: https://stats.stackexchange.com/questions/139042/ensemble-of-different-kinds-of-regressors-using-scikit-learn-or-any-other-pytho\n#","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":4}