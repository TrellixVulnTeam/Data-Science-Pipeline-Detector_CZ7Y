{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Simple LSTM\nUsing a simple LSTM recurrent neural network to predict cases and fatalities"},{"metadata":{},"cell_type":"markdown","source":"## Imports and Load Data"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nimport keras.backend as K\nimport tensorflow as tf\nfrom sklearn.metrics import mean_squared_log_error\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\n\nfrom tqdm import tqdm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.seed(7)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv(\"../input/covid19-global-forecasting-week-4/test.csv\", parse_dates=[\"Date\"])\ntrain = pd.read_csv(\"../input/covid19-global-forecasting-week-4/train.csv\")\nsubmission = pd.read_csv(\"../input/covid19-global-forecasting-week-4/submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## LSTM Model and Helpers"},{"metadata":{"trusted":true},"cell_type":"code","source":"# generate key for each entitiy to predict for\ntrain['geo_id'] = train['Country_Region'].astype(str) + '_' + train['Province_State'].astype(str)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_entity_data_split(geo_id, train_split_factor=1.0):\n    data = train[train[\"geo_id\"] == geo_id]\n    country = data[\"Country_Region\"].unique()[0]\n    province = data[\"Province_State\"].unique()[0]\n    \n    case = data[\"ConfirmedCases\"].to_numpy()\n    fat = data[\"Fatalities\"].to_numpy()\n    case = case.reshape((len(case), 1))\n    fat = fat.reshape((len(fat), 1))\n    \n    train_size = int(len(data) * train_split_factor)\n    test_size = len(data) - train_size\n    \n    case_train, case_test = case[0:train_size,:], case[train_size:len(data),:]\n    fat_train, fat_test = fat[0:train_size,:], fat[train_size:len(data),:]\n    \n    return train_size, test_size, case_train, case_test, fat_train, fat_test\n\ndef create_dataset_for_lstm(data, look_back=1.0):\n    # reshape into X=t and Y=t+1\n    dataX, dataY = [], []\n    for i in range(len(data)-look_back-1):\n        a = data[i:(i+look_back), 0]\n        dataX.append(a)\n        dataY.append(data[i + look_back, 0])\n    X = np.array(dataX)\n    Y = np.array(dataY)\n    \n    # reshape input to be [samples, time steps, features]\n    X = np.reshape(X, (X.shape[0], 1, X.shape[1]))\n    \n    return X, Y\n\ndef calculate_loss(Y, pred):\n    return np.sqrt(mean_squared_log_error(Y[0], pred[:,0]))\n\n'''\ndef rmsle_K(y, y0):\n    return K.sqrt(K.mean(K.square(tf.math.log1p(y) - tf.math.log1p(y0))))\n'''\n\ndef train_lstm(train, test=[], look_back=1, epochs=10, batch_size=1, verbose=0):\n    # scale\n    scaler = MinMaxScaler(feature_range=(0, 1))\n    train_scaled = scaler.fit_transform(train)\n    test_scaled = scaler.transform(test) if (len(test) > 0) else []\n    \n    # prep data\n    trainX, trainY = create_dataset_for_lstm(train_scaled, look_back=look_back)\n    testX, testY = (create_dataset_for_lstm(test_scaled, look_back=look_back)) if (len(test) > 0) else ([], [])\n    \n    # create and fit the LSTM network\n    model = Sequential()\n    model.add(LSTM(4, input_shape=(1, look_back)))\n    model.add(Dense(1))\n    model.compile(loss='mean_squared_error', optimizer='adam')\n    model.fit(trainX, trainY, epochs=epochs, batch_size=batch_size, verbose=verbose)\n    \n    # make predictions\n    trainPredict = model.predict(trainX).clip(0)\n    testPredict = model.predict(testX).clip(0) if (len(test) > 0) else []\n    \n    # invert predictions\n    trainPredict = scaler.inverse_transform(trainPredict)\n    trainY = scaler.inverse_transform([trainY])\n    testPredict = scaler.inverse_transform(testPredict) if (len(testPredict) > 0) else []\n    testY = scaler.inverse_transform([testY]) if (len(testY) > 0) else []\n    \n    # calculate RMSLE\n    trainScore = calculate_loss(trainY, trainPredict)\n    testScore = calculate_loss(testY, testPredict) if (len(test) > 0) else 0\n    if (verbose):\n        print('Train Score: %.2f RMSLE' % (trainScore))\n        print('Test Score: %.2f RMSLE' % (testScore))\n    \n    return {\n        \"model\": model,\n        \"scaler\": scaler,\n        \"train\": train,\n        \"trainX\": trainX,\n        \"trainY\": trainY,\n        \"trainPredict\": trainPredict,\n        \"trainScore\": trainScore,\n        \"test\": test,\n        \"testX\": testX,\n        \"testY\": testY,\n        \"testPredict\": testPredict,\n        \"testScore\": testScore\n    }\n\ndef plot_lstm(model, scaler, train, test, look_back=1.0, title=\"Cases\"):\n    data = np.concatenate((train, test))\n    \n    # scale\n    train = scaler.transform(train)\n    test = scaler.transform(test) if (len(test) > 0) else []\n    \n    # prep data\n    trainX, _ = create_dataset_for_lstm(train, look_back=look_back)\n    testX, _ = (create_dataset_for_lstm(test, look_back=look_back)) if (len(test) > 0) else ([], [])\n    \n    # make predictions\n    trainPredict = model.predict(trainX).clip(0)\n    testPredict = model.predict(testX).clip(0) if (len(testX) > 0) else []\n    \n    # invert predictions\n    trainPredict = scaler.inverse_transform(trainPredict)\n    testPredict = scaler.inverse_transform(testPredict) if (len(testPredict) > 0) else []\n    \n    # plot baseline and predictions\n    plt.plot(range(1, len(data) + 1), data, label=\"Actual\")\n    plt.plot(range(1 + look_back, len(trainPredict) + look_back + 1), trainPredict, label=\"Pred Train\")\n    plt.plot(range(1 + 2*look_back + len(trainPredict) + 1, len(data)), testPredict, label=\"Pred Test\")\n    plt.title(\"Fit of LSTM Preds to Actual \" + title)\n    plt.ylabel(title)\n    plt.xlabel(\"Days\")\n    plt.legend()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"# settings\ntrain_split_factor = 0.8\nlook_back = 1\nverbose = 0\nepochs = 50\n\n# entities\ngeo_ids = train[\"geo_id\"].unique()\n\n# train\nresults_dict = {}\nfor geo_id in tqdm(geo_ids):\n    # get train and test data\n    train_size, test_size, case_train, case_test, fat_train, fat_test = get_entity_data_split(geo_id, train_split_factor=train_split_factor)\n    \n    if (verbose): print(\"train case\")\n    case_results = train_lstm(case_train, test=case_test, look_back=look_back, epochs=epochs, verbose=verbose)\n    if (verbose): print(\"train fat\")\n    fat_results = train_lstm(fat_train, test=fat_test, look_back=look_back, epochs=epochs, verbose=verbose)\n    \n    results_dict[geo_id] = (case_results, fat_results)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Results"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_result(geo_id, results_dict=results_dict, features=(\"case\", \"fat\")):\n    # get train and test data\n    train_size, test_size, case_train, case_test, fat_train, fat_test = get_entity_data_split(geo_id, train_split_factor=train_split_factor)\n\n    # get results\n    case_results, fat_results = results_dict[geo_id]\n\n    # print and plot\n    print(geo_id)\n    # case\n    if (\"case\" in features):\n        print(\"Confirmed Cases:\")\n        print(\"  trainScore: \", case_results[\"trainScore\"])\n        print(\"  testScore: \", case_results[\"testScore\"])\n        plot_lstm(case_results[\"model\"], case_results[\"scaler\"], case_results[\"train\"], case_results[\"test\"], look_back=look_back, title=\"Confirmed Cases\")\n    # fat\n    if (\"fat\" in features):\n        print(\"Fatalities:\")\n        print(\"  trainScore: \", fat_results[\"trainScore\"])\n        print(\"  testScore: \", fat_results[\"testScore\"])\n        plot_lstm(fat_results[\"model\"], fat_results[\"scaler\"], fat_results[\"train\"], fat_results[\"test\"], look_back=look_back, title=\"Fatalities\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# list of entity and results\n# entity: (geo_id, (case_results, fat_results))\n#   result: (model, scaler, trainPredict, testPredict, trainScore, testScore)\nresults_list = list(results_dict.items())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Chosen Examples"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_result(\"China_Hubei\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_result(\"Spain_nan\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_result(\"Germany_nan\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Confirmed Cases: 3 Best Predicted"},{"metadata":{"trusted":true},"cell_type":"code","source":"# sort results by case_model testScore asc\nresults_list.sort(key=lambda entity: entity[1][0][\"testScore\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(0, 3): plot_result(results_list[i][0], features=(\"case\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Confirmed Cases: 3 Worst Predicted"},{"metadata":{"trusted":true},"cell_type":"code","source":"# sort results by case_model testScore desc\nresults_list.sort(key=lambda entity: entity[1][0][\"testScore\"], reverse=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(0, 3): plot_result(results_list[i][0], features=(\"case\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Fatalities: 3 Best Predicted"},{"metadata":{"trusted":true},"cell_type":"code","source":"# sort results by fat_model testScore asc\nresults_list.sort(key=lambda entity: entity[1][1][\"testScore\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(0, 3): plot_result(results_list[i][0], features=(\"fat\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Fatalities: 3 Worst Predicted"},{"metadata":{"trusted":true},"cell_type":"code","source":"# sort results by fat_model testScore desc\nresults_list.sort(key=lambda entity: entity[1][1][\"testScore\"], reverse=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(0, 3): plot_result(results_list[i][0], features=(\"fat\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Overall Model Performance\nModel Performance over all predictions form all countries/provinces"},{"metadata":{"trusted":true},"cell_type":"code","source":"allTrainY = []\nallTrainPredict = []\n\nallTestY = []\nallTestPredict = []\n\n# cumulate Y_true and Y_pred for train and test\nfor geo_id, results in results_dict.items():\n    \n    for result in results: # case and fat\n        # train\n        trainY = result[\"trainY\"]\n        trainPredict = result[\"trainPredict\"]\n        if (trainY.size == trainPredict.size):\n            allTrainY.extend(trainY)\n            allTrainPredict.extend(trainPredict)\n        else:\n            print(\"Warning: trainY.size != trainPredict.size: \", trainY.size, \"!=\", trainPredict.size)\n\n        # test\n        testY = result[\"testY\"]\n        testPredict = result[\"testPredict\"]\n        if (testY.size == testPredict.size):\n            allTestY.extend(testY)\n            allTestPredict.extend(testPredict)\n        else:\n            print(\"Warning: testY.size != testPredict.size: \", testY.size, \"!=\", testPredict.size)\n\n# make nparrays\nallTrainY = np.array(allTrainY)\nallTrainPredict = np.array(allTrainPredict)\nallTestY = np.array(allTestY)\nallTestPredict = np.array(allTestPredict)\n# reshape for calculate_loss()\nallTrainY = allTrainY.reshape((1, allTrainY.size))\nallTrainPredict = allTrainPredict.reshape((allTrainPredict.size, 1))\nallTestY = allTestY.reshape((1, allTestY.size))\nallTestPredict = allTestPredict.reshape((allTestPredict.size, 1))\n\n# calculate overall score (loss) for train and test\noverallTrainScore = calculate_loss(allTrainY, allTrainPredict)\noverallTestScore = calculate_loss(allTestY, allTestPredict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"overallTrainScore: \", overallTrainScore)\nprint(\"overallTestScore: \", overallTestScore)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}