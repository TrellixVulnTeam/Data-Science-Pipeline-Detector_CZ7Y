{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os, gc, pickle, copy, datetime, warnings\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\nfrom sklearn import metrics\npd.set_option('display.max_columns', 100) \nwarnings.filterwarnings('ignore')\n\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n #   for filename in filenames:\n #       print(os.path.join(dirname, filename))\nPATH='/kaggle/input/covid19-global-forecasting-week-4'\ndf_train = pd.read_csv(f'{PATH}/train.csv')\ndf_test = pd.read_csv(f'{PATH}/test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create varibales in the training model"},{"metadata":{},"cell_type":"markdown","source":"Merge datasets"},{"metadata":{"trusted":true},"cell_type":"code","source":"# concat train and test\ndf_traintest = pd.concat([df_train, df_test])\nprint(df_train.shape, df_test.shape, df_traintest.shape)\n# concat Country/Region and Province/State\ndef func(x):\n    try:\n        x_new = x['Country_Region'] + \"/\" + x['Province_State']\n    except:\n        x_new = x['Country_Region']\n    return x_new\n        \ndf_traintest['place_id'] = df_traintest.apply(lambda x: func(x), axis=1)\ntmp = np.sort(df_traintest['place_id'].unique())\nprint(\"num unique places: {}\".format(len(tmp)))\nprint(tmp[:10])\n\n# get place list\nplaces = np.sort(df_traintest['place_id'].unique())\n# process date\ndf_traintest['Date'] = pd.to_datetime(df_traintest['Date'])\ndf_traintest['day'] = df_traintest['Date'].apply(lambda x: x.dayofyear).astype(np.int16)\ndf_traintest.head()\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"\n# calc cases, fatalities, recover per day\ndf_traintest2 = copy.deepcopy(df_traintest)\ndf_traintest2['cases/day'] = 0\ndf_traintest2['fatal/day'] = 0\ntmp_list = np.zeros(len(df_traintest2))\nfor place in places:\n    tmp = df_traintest2['ConfirmedCases'][df_traintest2['place_id']==place].values\n    tmp[1:] -= tmp[:-1]\n    df_traintest2['cases/day'][df_traintest2['place_id']==place] = tmp\n    tmp = df_traintest2['Fatalities'][df_traintest2['place_id']==place].values\n    tmp[1:] -= tmp[:-1]\n    df_traintest2['fatal/day'][df_traintest2['place_id']==place] = tmp\nprint(df_traintest2.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# aggregate cases and fatalities\ndef do_aggregation(df, col, mean_range):\n    df_new = copy.deepcopy(df)\n    col_new = '{}_({}-{})'.format(col, mean_range[0], mean_range[1])\n    df_new[col_new] = 0\n    tmp = df_new[col].rolling(mean_range[1]-mean_range[0]+1).mean()\n    df_new[col_new][mean_range[0]:] = tmp[:-(mean_range[0])]\n    df_new[col_new][pd.isna(df_new[col_new])] = 0\n    return df_new[[col_new]].reset_index(drop=True)\n\ndef do_aggregations(df):\n    df = pd.concat([df, do_aggregation(df, 'cases/day', [1,1]).reset_index(drop=True)], axis=1)\n    df = pd.concat([df, do_aggregation(df, 'cases/day', [1,7]).reset_index(drop=True)], axis=1)\n    df = pd.concat([df, do_aggregation(df, 'cases/day', [8,14]).reset_index(drop=True)], axis=1)\n    df = pd.concat([df, do_aggregation(df, 'cases/day', [15,21]).reset_index(drop=True)], axis=1)\n    df = pd.concat([df, do_aggregation(df, 'fatal/day', [1,1]).reset_index(drop=True)], axis=1)\n    df = pd.concat([df, do_aggregation(df, 'fatal/day', [1,7]).reset_index(drop=True)], axis=1)\n    df = pd.concat([df, do_aggregation(df, 'fatal/day', [8,14]).reset_index(drop=True)], axis=1)\n    df = pd.concat([df, do_aggregation(df, 'fatal/day', [15,21]).reset_index(drop=True)], axis=1)\n    for threshold in [1, 10, 100]:\n        days_under_threshold = (df['ConfirmedCases']<threshold).sum()\n        tmp = df['day'].values - 22 - days_under_threshold\n        tmp[tmp<=0] = 0\n        df['days_since_{}cases'.format(threshold)] = tmp\n            \n    for threshold in [1, 10, 100]:\n        days_under_threshold = (df['Fatalities']<threshold).sum()\n        tmp = df['day'].values - 22 - days_under_threshold\n        tmp[tmp<=0] = 0\n        df['days_since_{}fatal'.format(threshold)] = tmp\n    \n    # process China/Hubei\n    if df['place_id'][0]=='China/Hubei':\n        df['days_since_1cases'] += 35 # 2019/12/8\n        df['days_since_10cases'] += 35-13 # 2019/12/8-2020/1/2 assume 2019/12/8+13\n        df['days_since_100cases'] += 4 # 2020/1/18\n        df['days_since_1fatal'] += 13 # 2020/1/9\n    return df\n\ndf_traintest3 = []\nfor place in places[:]:\n    df_tmp = df_traintest2[df_traintest2['place_id']==place].reset_index(drop=True)\n    df_tmp = do_aggregations(df_tmp)\n    df_traintest3.append(df_tmp)\ndf_traintest3 = pd.concat(df_traintest3).reset_index(drop=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# add additional info from countryinfo dataset\ndf_country = pd.read_csv(\"../input/additionaldata/covid19countryinfo.csv\")\ndf_country.head()\ndf_country['Country_Region'] = df_country['country']\ndf_country = df_country[df_country['country'].duplicated()==False]\nprint(df_country[df_country['country'].duplicated()].shape)\ndf_country[df_country['country'].duplicated()]\ndf_traintest4 = pd.merge(df_traintest3, \n                         df_country.drop(['tests', 'testpop', 'country'], axis=1), \n                         on=['Country_Region',], how='left')\nprint(df_traintest4.shape)\ndf_traintest4.head()\n\ndef encode_label(df, col, freq_limit=0):\n    df[col][pd.isna(df[col])] = 'nan'\n    tmp = df[col].value_counts()\n    cols = tmp.index.values\n    freq = tmp.values\n    num_cols = (freq>=freq_limit).sum()\n    print(\"col: {}, num_cat: {}, num_reduced: {}\".format(col, len(cols), num_cols))\n\n    col_new = '{}_le'.format(col)\n    df_new = pd.DataFrame(np.ones(len(df), np.int16)*(num_cols-1), columns=[col_new])\n    for i, item in enumerate(cols[:num_cols]):\n        df_new[col_new][df[col]==item] = i\n\n    return df_new\n\ndef get_df_le(df, col_index, col_cat):\n    df_new = df[[col_index]]\n    for col in col_cat:\n        df_tmp = encode_label(df, col)\n        df_new = pd.concat([df_new, df_tmp], axis=1)\n    return df_new\n\ndf_traintest4['id'] = np.arange(len(df_traintest4))\ndf_le = get_df_le(df_traintest4, 'id', ['Country_Region', 'Province_State'])\ndf_traintest5 = pd.merge(df_traintest4, df_le, on='id', how='left')\ndf_traintest5['cases/day'] = df_traintest5['cases/day'].astype(np.float)\ndf_traintest5['fatal/day'] = df_traintest5['fatal/day'].astype(np.float)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_traintest5.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Training model"},{"metadata":{"trusted":true},"cell_type":"code","source":"def calc_score(y_true, y_pred):\n    y_true[y_true<0] = 0\n    score = metrics.mean_squared_error(np.log(y_true.clip(0, 1e10)+1), np.log(y_pred[:]+1))**0.5\n    return score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" #train model to predict fatalities/day\n# params\nSEED = 100\nparams = {'num_leaves': 8,\n          'min_data_in_leaf': 5,  \n          'objective': 'regression',\n          'max_depth': 8,\n          'learning_rate': 0.02,\n          'boosting': 'gbdt',\n          'bagging_freq': 5,  # 5\n          'bagging_fraction': 0.8,  \n          'feature_fraction': 0.8201,\n          'bagging_seed': SEED,\n          'reg_alpha': 1,  \n          'reg_lambda': 4.9847051755586085,\n          'random_state': SEED,\n          'metric': 'mse',\n          'verbosity': 100,\n          'min_gain_to_split': 0.02,  \n          'min_child_weight': 5,  \n          'num_threads': 6,\n          }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train model to predict fatalities/day\ncol_target = 'fatal/day'\ncol_var = [\n    'cases/day_(1-1)', \n    'cases/day_(1-7)', \n    'fatal/day_(1-7)', \n    'fatal/day_(8-14)', \n    'fatal/day_(15-21)', \n    'density', \n]\ncol_cat = []\ndf_train = df_traintest5[(pd.isna(df_traintest5['ForecastId'])) & (df_traintest5['day']<61)]\ndf_valid = df_traintest5[(pd.isna(df_traintest5['ForecastId'])) & (df_traintest5['day']>=61) & (df_traintest5['day']<72)]\ndf_test = df_traintest5[pd.isna(df_traintest5['ForecastId'])==False]\nX_train = df_train[col_var]\nX_valid = df_valid[col_var]\ny_train = np.log(df_train[col_target].values.clip(0, 1e10)+1)\ny_valid = np.log(df_valid[col_target].values.clip(0, 1e10)+1)\ntrain_data = lgb.Dataset(X_train, label=y_train, categorical_feature=col_cat)\nvalid_data = lgb.Dataset(X_valid, label=y_valid, categorical_feature=col_cat)\nnum_round = 15000\nmodel = lgb.train(params, train_data, num_round, valid_sets=[train_data, valid_data],\n                  verbose_eval=100,\n                  early_stopping_rounds=150,)\n\nbest_itr = model.best_iteration","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_true = df_valid['fatal/day'].values\ny_pred = np.exp(model.predict(X_valid))-1\nscore = calc_score(y_true, y_pred)\nprint(\"{:.6f}\".format(score))\n# display feature importance\ntmp = pd.DataFrame()\ntmp[\"feature\"] = col_var\ntmp[\"importance\"] = model.feature_importance()\ntmp = tmp.sort_values('importance', ascending=False)\ntmp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = df_traintest5[(pd.isna(df_traintest5['ForecastId'])) & (df_traintest5['day']<72)]\ndf_valid = df_traintest5[(pd.isna(df_traintest5['ForecastId'])) & (df_traintest5['day']<72)]\ndf_test = df_traintest5[pd.isna(df_traintest5['ForecastId'])==False]\nX_train = df_train[col_var]\nX_valid = df_valid[col_var]\ny_train = np.log(df_train[col_target].values.clip(0, 1e10)+1)\ny_valid = np.log(df_valid[col_target].values.clip(0, 1e10)+1)\ntrain_data = lgb.Dataset(X_train, label=y_train, categorical_feature=col_cat)\nvalid_data = lgb.Dataset(X_valid, label=y_valid, categorical_feature=col_cat)\nmodel = lgb.train(params, train_data, best_itr, valid_sets=[train_data, valid_data],\n                  verbose_eval=100,\n                  early_stopping_rounds=150,)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train model to predict fatalities/day\ncol_target2 = 'cases/day'\ncol_var2 = [\n    'days_since_10cases', \n    'cases/day_(1-1)', \n    'cases/day_(1-7)', \n    'cases/day_(8-14)',  \n    'cases/day_(15-21)', \n]\ncol_cat = []\ndf_train = df_traintest5[(pd.isna(df_traintest5['ForecastId'])) & (df_traintest5['day']<61)]\ndf_valid = df_traintest5[(pd.isna(df_traintest5['ForecastId'])) & (df_traintest5['day']>=61) & (df_traintest5['day']<72)]\ndf_test = df_traintest5[pd.isna(df_traintest5['ForecastId'])==False]\nX_train = df_train[col_var2]\nX_valid = df_valid[col_var2]\ny_train = np.log(df_train[col_target2].values.clip(0, 1e10)+1)\ny_valid = np.log(df_valid[col_target2].values.clip(0, 1e10)+1)\ntrain_data = lgb.Dataset(X_train, label=y_train, categorical_feature=col_cat)\nvalid_data = lgb.Dataset(X_valid, label=y_valid, categorical_feature=col_cat)\nmodel2 = lgb.train(params, train_data, num_round, valid_sets=[train_data, valid_data],\n                  verbose_eval=100,\n                  early_stopping_rounds=150,)\nbest_itr = model2.best_iteration","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_true = df_valid['cases/day'].values\ny_pred = np.exp(model2.predict(X_valid))-1\nscore = calc_score(y_true, y_pred)\nprint(\"{:.6f}\".format(score))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = df_traintest5[(pd.isna(df_traintest5['ForecastId'])) & (df_traintest5['day']<72)]\ndf_valid = df_traintest5[(pd.isna(df_traintest5['ForecastId'])) & (df_traintest5['day']<72)]\ndf_test = df_traintest5[pd.isna(df_traintest5['ForecastId'])==False]\nX_train = df_train[col_var2]\nX_valid = df_valid[col_var2]\ny_train = np.log(df_train[col_target2].values.clip(0, 1e10)+1)\ny_valid = np.log(df_valid[col_target2].values.clip(0, 1e10)+1)\ntrain_data = lgb.Dataset(X_train, label=y_train, categorical_feature=col_cat)\nvalid_data = lgb.Dataset(X_valid, label=y_valid, categorical_feature=col_cat)\nmodel2 = lgb.train(params, train_data, best_itr, valid_sets=[train_data, valid_data],\n                  verbose_eval=100,\n                  early_stopping_rounds=150,)\ndf_train = df_traintest5[(pd.isna(df_traintest5['ForecastId']))]\ndf_valid = df_traintest5[(pd.isna(df_traintest5['ForecastId']))]\ndf_test = df_traintest5[pd.isna(df_traintest5['ForecastId'])==False]\nX_train = df_train[col_var]\nX_valid = df_valid[col_var]\ny_train = np.log(df_train[col_target].values.clip(0, 1e10)+1)\ny_valid = np.log(df_valid[col_target].values.clip(0, 1e10)+1)\ntrain_data = lgb.Dataset(X_train, label=y_train, categorical_feature=col_cat)\nvalid_data = lgb.Dataset(X_valid, label=y_valid, categorical_feature=col_cat)\nmodel_pri = lgb.train(params, train_data, best_itr, valid_sets=[train_data, valid_data],\n                  verbose_eval=100,\n                  early_stopping_rounds=150,)\n# train model to predict fatalities/day\ndf_train = df_traintest5[(pd.isna(df_traintest5['ForecastId'])) & (df_traintest5['day']<72)]\ndf_valid = df_traintest5[(pd.isna(df_traintest5['ForecastId'])) & (df_traintest5['day']>=72)]\ndf_test = df_traintest5[pd.isna(df_traintest5['ForecastId'])==False]\nX_train = df_train[col_var2]\nX_valid = df_valid[col_var2]\ny_train = np.log(df_train[col_target2].values.clip(0, 1e10)+1)\ny_valid = np.log(df_valid[col_target2].values.clip(0, 1e10)+1)\ntrain_data = lgb.Dataset(X_train, label=y_train, categorical_feature=col_cat)\nvalid_data = lgb.Dataset(X_valid, label=y_valid, categorical_feature=col_cat)\nmodel2_pri = lgb.train(params, train_data, num_round, valid_sets=[train_data, valid_data],\n                  verbose_eval=100,\n                  early_stopping_rounds=150,)\nbest_itr = model2_pri.best_iteration\n\nlast_day_train = df_traintest5['day'][pd.isna(df_traintest5['ForecastId'])].max()\nprint(last_day_train)\ndf_tmp = df_traintest5[\n    (pd.isna(df_traintest5['ForecastId'])) |\n    ((df_traintest5['day']>last_day_train) & (pd.isna(df_traintest5['ForecastId'])==False))].reset_index(drop=True)\ndf_tmp = df_tmp.drop([\n    'cases/day_(1-1)', 'cases/day_(1-7)', 'cases/day_(8-14)', 'cases/day_(15-21)', \n    'fatal/day_(1-1)', 'fatal/day_(1-7)', 'fatal/day_(8-14)', 'fatal/day_(15-21)',\n    'days_since_1cases', 'days_since_10cases', 'days_since_100cases',\n    'days_since_1fatal', 'days_since_10fatal', 'days_since_100fatal',\n                               ],  axis=1)\ndf_traintest6 = []\nfor i, place in enumerate(places[:]):\n    df_tmp2 = df_tmp[df_tmp['place_id']==place].reset_index(drop=True)\n    df_tmp2 = do_aggregations(df_tmp2)\n    df_traintest6.append(df_tmp2)\ndf_traintest6 = pd.concat(df_traintest6).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nMAke submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"# predict test data in public\nday_before_public = 71\ndf_preds = []\nfor i, place in enumerate(places[:]):\n#     if place!='Japan' and place!='Afghanistan' :continue\n    df_interest = copy.deepcopy(df_traintest5[df_traintest5['place_id']==place].reset_index(drop=True))\n    df_interest['cases/day'][(pd.isna(df_interest['ForecastId']))==False] = -1\n    df_interest['fatal/day'][(pd.isna(df_interest['ForecastId']))==False] = -1\n    len_known = (df_interest['day']<=day_before_public).sum()\n    len_unknown = (day_before_public<df_interest['day']).sum()\n    for j in range(len_unknown): # use predicted cases and fatal for next days' prediction\n        X_valid = df_interest[col_var].iloc[j+len_known]\n        X_valid2 = df_interest[col_var2].iloc[j+len_known]\n        pred_f = model.predict(X_valid)\n        pred_c = model2.predict(X_valid2)\n        pred_c = (np.exp(pred_c)-1).clip(0, 1e10)\n        pred_f = (np.exp(pred_f)-1).clip(0, 1e10)\n        df_interest['fatal/day'][j+len_known] = pred_f\n        df_interest['cases/day'][j+len_known] = pred_c\n        df_interest['Fatalities'][j+len_known] = df_interest['Fatalities'][j+len_known-1] + pred_f\n        df_interest['ConfirmedCases'][j+len_known] = df_interest['ConfirmedCases'][j+len_known-1] + pred_c\n        df_interest = df_interest.drop([\n            'cases/day_(1-1)', 'cases/day_(1-7)', 'cases/day_(8-14)', 'cases/day_(15-21)', \n            'fatal/day_(1-1)', 'fatal/day_(1-7)', 'fatal/day_(8-14)', 'fatal/day_(15-21)',\n            'days_since_1cases', 'days_since_10cases', 'days_since_100cases',\n            'days_since_1fatal', 'days_since_10fatal', 'days_since_100fatal',],  axis=1)\n        df_interest = do_aggregations(df_interest)\n    if (i+1)%10==0:\n        print(\"{:3d}/{}  {}, len known: {}, len unknown: {}\".format(i+1, len(places), place, len_known, len_unknown), df_interest.shape)\n    df_interest['fatal_pred'] = np.cumsum(df_interest['fatal/day'].values)\n    df_interest['cases_pred'] = np.cumsum(df_interest['cases/day'].values)\n    df_preds.append(df_interest)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#concat prediction\ndf_preds= pd.concat(df_preds)\ndf_preds = df_preds.sort_values('day')\ncol_tmp = ['place_id', 'ForecastId', 'day', 'cases/day', 'cases_pred', 'fatal/day', 'fatal_pred',]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predict test data in public\nday_before_private = 84\ndf_preds_pri = []\nfor i, place in enumerate(places[:]):\n    df_interest = copy.deepcopy(df_traintest6[df_traintest6['place_id']==place].reset_index(drop=True))\n    df_interest['cases/day'][(pd.isna(df_interest['ForecastId']))==False] = -1\n    df_interest['fatal/day'][(pd.isna(df_interest['ForecastId']))==False] = -1\n    len_known = (df_interest['day']<=day_before_private).sum()\n    len_unknown = (day_before_private<df_interest['day']).sum()\n    for j in range(len_unknown): # use predicted cases and fatal for next days' prediction\n        X_valid = df_interest[col_var].iloc[j+len_known]\n        X_valid2 = df_interest[col_var2].iloc[j+len_known]\n        pred_f = model_pri.predict(X_valid)\n        pred_c = model2_pri.predict(X_valid2)\n        pred_c = (np.exp(pred_c)-1).clip(0, 1e10)\n        pred_f = (np.exp(pred_f)-1).clip(0, 1e10)\n        df_interest['fatal/day'][j+len_known] = pred_f\n        df_interest['cases/day'][j+len_known] = pred_c\n        df_interest['Fatalities'][j+len_known] = df_interest['Fatalities'][j+len_known-1] + pred_f\n        df_interest['ConfirmedCases'][j+len_known] = df_interest['ConfirmedCases'][j+len_known-1] + pred_c\n        df_interest = df_interest.drop([\n            'cases/day_(1-1)', 'cases/day_(1-7)', 'cases/day_(8-14)', 'cases/day_(15-21)', \n            'fatal/day_(1-1)', 'fatal/day_(1-7)', 'fatal/day_(8-14)', 'fatal/day_(15-21)',\n            'days_since_1cases', 'days_since_10cases', 'days_since_100cases',\n            'days_since_1fatal', 'days_since_10fatal', 'days_since_100fatal',],  axis=1)\n        df_interest = do_aggregations(df_interest)\n    if (i+1)%10==0:\n        print(\"{:3d}/{}  {}, len known: {}, len unknown: {}\".format(i+1, len(places), place, len_known, len_unknown), df_interest.shape)\n    df_interest['fatal_pred'] = np.cumsum(df_interest['fatal/day'].values)\n    df_interest['cases_pred'] = np.cumsum(df_interest['cases/day'].values)\n    df_preds_pri.append(df_interest)\n        \n       ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# concat prediction\ndf_preds_pri= pd.concat(df_preds_pri)\ndf_preds_pri = df_preds_pri.sort_values('day')\ncol_tmp = ['place_id', 'Forecastid', 'Date', 'day', 'cases/day', 'cases_pred', 'fatal/day', 'fatal_pred',]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# merge 2 preds\n#df_preds[df_preds['day']>last_day_train] = df_preds_pri[df_preds['day']>last_day_train]\ndf_preds.to_csv(\"df_preds.csv\", index=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load sample submission\nsub = pd.read_csv(\"../input/covid19-global-forecasting-week-4/submission.csv\")\nsub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# merge prediction with sub\nsub = pd.merge(sub, df_traintest3[['ForecastId', 'place_id', 'day']])\nsub = pd.merge(sub, df_preds[['place_id', 'day', 'cases_pred', 'fatal_pred']], on=['place_id', 'day',], how='left')\n# save\nsub['ConfirmedCases'] = sub['cases_pred']\nsub['Fatalities'] = sub['fatal_pred']\nsub = sub[['ForecastId', 'ConfirmedCases', 'Fatalities']]\nsub =sub.drop_duplicates(\"ForecastId\")\nsub.to_csv(\"submission.csv\", index=None)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}