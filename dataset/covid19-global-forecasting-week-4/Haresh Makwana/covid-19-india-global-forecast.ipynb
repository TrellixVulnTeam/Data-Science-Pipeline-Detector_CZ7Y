{"cells":[{"metadata":{},"cell_type":"markdown","source":"COVID India & Global Forecast"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import preprocessing\nimport time\nfrom datetime import datetime\nfrom scipy import integrate, optimize\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# ML libraries\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom xgboost import plot_importance, plot_tree\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn import linear_model\nfrom sklearn.metrics import mean_squared_error","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Exploratory data analysis (EDA)\n\nFirst of all, let's take a look on the data structure:"},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"submission_example = pd.read_csv(\"../input/covid19-global-forecasting-week-4/submission.csv\")\n\ntest = pd.read_csv(\"../input/covid19-global-forecasting-week-4/test.csv\")\ntrain = pd.read_csv(\"../input/covid19-global-forecasting-week-4/train.csv\")\ntrain.Province_State.fillna(\"None\", inplace=True)\ndisplay(train.head(5))\ndisplay(train.describe())\nprint(\"Number of Country_Region: \", train['Country_Region'].nunique())\nprint(\"Dates go from day\", max(train['Date']), \"to day\", min(train['Date']), \", a total of\", train['Date'].nunique(), \"days\")\nprint(\"Countries with Province/State informed: \", train.loc[train['Province_State']!='None']['Country_Region'].unique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The dataset covers 184 countries and almost 2 full months from 2020, which is enough data to get some clues about the pandemic."},{"metadata":{"trusted":true},"cell_type":"code","source":"#confirmed_country = train.groupby(['Country/Region', 'Province/State']).agg({'ConfirmedCases':['sum']})\n#fatalities_country = train.groupby(['Country/Region', 'Province/State']).agg({'Fatalities':['sum']})\nconfirmed_total_date = train.groupby(['Date']).agg({'ConfirmedCases':['sum']})\nfatalities_total_date = train.groupby(['Date']).agg({'Fatalities':['sum']})\ntotal_date = confirmed_total_date.join(fatalities_total_date)\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(17,7))\ntotal_date.plot(ax=ax1)\nax1.set_title(\"Global confirmed cases\", size=13)\nax1.set_ylabel(\"Number of cases\", size=13)\nax1.set_xlabel(\"Date\", size=13)\nfatalities_total_date.plot(ax=ax2, color='orange')\nax2.set_title(\"Global deceased cases\", size=13)\nax2.set_ylabel(\"Number of cases\", size=13)\nax2.set_xlabel(\"Date\", size=13)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observations**: The global curve shows a rich fine structure, but these numbers are strongly affected by the vector zero country, China. Given that COVID-19 started there, during the initial expansion of the virus there was no reliable information about the real infected cases. In fact,  the criteria to consider infection cases was modified around 2020-02-11, which strongly perturbed the curve as you can see from the figure. "},{"metadata":{},"cell_type":"markdown","source":"Italy, Spain, UK and India\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#confirmed_country_Italy = train[train['Country_Region']=='Italy'].groupby(['Country_Region', 'Province_State']).agg({'ConfirmedCases':['sum']})\n#fatalities_country_Italy = train[train['Country_Region']=='Italy'].groupby(['Country_Region', 'Province_State']).agg({'Fatalities':['sum']})\nconfirmed_total_date_Italy = train[train['Country_Region']=='Italy'].groupby(['Date']).agg({'ConfirmedCases':['sum']})\nfatalities_total_date_Italy = train[train['Country_Region']=='Italy'].groupby(['Date']).agg({'Fatalities':['sum']})\ntotal_date_Italy = confirmed_total_date_Italy.join(fatalities_total_date_Italy)\n\n#confirmed_country_Spain = train[train['Country_Region']=='Spain'].groupby(['Country_Region', 'Province_State']).agg({'ConfirmedCases':['sum']})\n#fatalities_country_Spain = train[train['Country_Region']=='Spain'].groupby(['Country_Region', 'Province_State']).agg({'Fatalities':['sum']})\nconfirmed_total_date_Spain = train[train['Country_Region']=='Spain'].groupby(['Date']).agg({'ConfirmedCases':['sum']})\nfatalities_total_date_Spain = train[train['Country_Region']=='Spain'].groupby(['Date']).agg({'Fatalities':['sum']})\ntotal_date_Spain = confirmed_total_date_Spain.join(fatalities_total_date_Spain)\n\n#confirmed_country_UK = train[train['Country_Region']=='United Kingdom'].groupby(['Country_Region', 'Province_State']).agg({'ConfirmedCases':['sum']})\n#fatalities_country_UK = train[train['Country_Region']=='United Kingdom'].groupby(['Country_Region', 'Province_State']).agg({'Fatalities':['sum']})\nconfirmed_total_date_UK = train[train['Country_Region']=='United Kingdom'].groupby(['Date']).agg({'ConfirmedCases':['sum']})\nfatalities_total_date_UK = train[train['Country_Region']=='United Kingdom'].groupby(['Date']).agg({'Fatalities':['sum']})\ntotal_date_UK = confirmed_total_date_UK.join(fatalities_total_date_UK)\n\n#confirmed_country_Australia = train[train['Country_Region']=='Australia'].groupby(['Country_Region', 'Province_State']).agg({'ConfirmedCases':['sum']})\n#fatalities_country_Australia = train[train['Country_Region']=='Australia'].groupby(['Country_Region', 'Province_State']).agg({'Fatalities':['sum']})\nconfirmed_total_date_Australia = train[train['Country_Region']=='Australia'].groupby(['Date']).agg({'ConfirmedCases':['sum']})\nfatalities_total_date_Australia = train[train['Country_Region']=='Australia'].groupby(['Date']).agg({'Fatalities':['sum']})\ntotal_date_Australia = confirmed_total_date_Australia.join(fatalities_total_date_Australia)\n\n#confirmed_country_Singapore = train[train['Country_Region']=='Singapore'].groupby(['Country_Region', 'Province_State']).agg({'ConfirmedCases':['sum']})\n#fatalities_country_Singapore = train[train['Country_Region']=='Singapore'].groupby(['Country_Region', 'Province_State']).agg({'Fatalities':['sum']})\nconfirmed_total_date_Singapore = train[train['Country_Region']=='Singapore'].groupby(['Date']).agg({'ConfirmedCases':['sum']})\nfatalities_total_date_Singapore = train[train['Country_Region']=='Singapore'].groupby(['Date']).agg({'Fatalities':['sum']})\ntotal_date_Singapore = confirmed_total_date_Singapore.join(fatalities_total_date_Singapore)\n\nconfirmed_total_date_India = train[train['Country_Region']=='India'].groupby(['Date']).agg({'ConfirmedCases':['sum']})\nfatalities_total_date_India = train[train['Country_Region']=='India'].groupby(['Date']).agg({'Fatalities':['sum']})\ntotal_date_India = confirmed_total_date_India.join(fatalities_total_date_India)\n\nplt.figure(figsize=(17,10))\nplt.subplot(2, 2, 1)\ntotal_date_Italy.plot(ax=plt.gca(), title='Italy')\nplt.ylabel(\"Confirmed infection cases\", size=13)\n\nplt.subplot(2, 2, 2)\ntotal_date_Spain.plot(ax=plt.gca(), title='Spain')\n\nplt.subplot(2, 2, 3)\ntotal_date_UK.plot(ax=plt.gca(), title='United Kingdom')\nplt.ylabel(\"Confirmed infection cases\", size=13)\n\nplt.subplot(2, 2, 4)\ntotal_date_India.plot(ax=plt.gca(), title='India')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As a fraction of the total population of each country:"},{"metadata":{"trusted":true},"cell_type":"code","source":"pop_italy = 60486683.\npop_spain = 46749696.\npop_UK = 67784927.\npop_singapore = 5837230.\npop_India = 1380004385.\n\ntotal_date_Italy.ConfirmedCases = total_date_Italy.ConfirmedCases/pop_italy*100.\ntotal_date_Italy.Fatalities = total_date_Italy.ConfirmedCases/pop_italy*100.\ntotal_date_Spain.ConfirmedCases = total_date_Spain.ConfirmedCases/pop_spain*100.\ntotal_date_Spain.Fatalities = total_date_Spain.ConfirmedCases/pop_spain*100.\ntotal_date_UK.ConfirmedCases = total_date_UK.ConfirmedCases/pop_UK*100.\ntotal_date_UK.Fatalities = total_date_UK.ConfirmedCases/pop_UK*100.\ntotal_date_Singapore.ConfirmedCases = total_date_Singapore.ConfirmedCases/pop_singapore*100.\ntotal_date_Singapore.Fatalities = total_date_Singapore.ConfirmedCases/pop_singapore*100.\n\ntotal_date_India.ConfirmedCases = total_date_India.ConfirmedCases/pop_India*100.\ntotal_date_India.Fatalities = total_date_India.ConfirmedCases/pop_India*100.\n\nplt.figure(figsize=(15,10))\nplt.subplot(2, 2, 1)\ntotal_date_Italy.ConfirmedCases.plot(ax=plt.gca(), title='Italy')\nplt.ylabel(\"Fraction of population infected\")\nplt.ylim(0, 0.5)\n\nplt.subplot(2, 2, 2)\ntotal_date_Spain.ConfirmedCases.plot(ax=plt.gca(), title='Spain')\nplt.ylim(0, 0.5)\n\nplt.subplot(2, 2, 3)\ntotal_date_UK.ConfirmedCases.plot(ax=plt.gca(), title='United Kingdom')\nplt.ylabel(\"Fraction of population infected\")\nplt.ylim(0, 0.1)\n\nplt.subplot(2, 2, 4)\ntotal_date_India.ConfirmedCases.plot(ax=plt.gca(), title='India')\nplt.ylim(0, 0.05)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In order to compare the 4 countries, it's also interesting to see the evolution of the infections from the first confirmed case:"},{"metadata":{"trusted":true},"cell_type":"code","source":"#confirmed_country_Italy = train[(train['Country_Region']=='Italy') & train['ConfirmedCases']!=0].groupby(['Country_Region', 'Province_State']).agg({'ConfirmedCases':['sum']})\n#fatalities_country_Italy = train[(train['Country_Region']=='Italy') & train['ConfirmedCases']!=0].groupby(['Country_Region', 'Province_State']).agg({'Fatalities':['sum']})\nconfirmed_total_date_Italy = train[(train['Country_Region']=='Italy') & train['ConfirmedCases']!=0].groupby(['Date']).agg({'ConfirmedCases':['sum']})\nfatalities_total_date_Italy = train[(train['Country_Region']=='Italy') & train['ConfirmedCases']!=0].groupby(['Date']).agg({'Fatalities':['sum']})\ntotal_date_Italy = confirmed_total_date_Italy.join(fatalities_total_date_Italy)\n\n#confirmed_country_Spain = train[(train['Country_Region']=='Spain') & (train['ConfirmedCases']!=0)].groupby(['Country_Region', 'Province_State']).agg({'ConfirmedCases':['sum']})\n#fatalities_country_Spain = train[(train['Country_Region']=='Spain') & (train['ConfirmedCases']!=0)].groupby(['Country_Region', 'Province_State']).agg({'Fatalities':['sum']})\nconfirmed_total_date_Spain = train[(train['Country_Region']=='Spain') & (train['ConfirmedCases']!=0)].groupby(['Date']).agg({'ConfirmedCases':['sum']})\nfatalities_total_date_Spain = train[(train['Country_Region']=='Spain') & (train['ConfirmedCases']!=0)].groupby(['Date']).agg({'Fatalities':['sum']})\ntotal_date_Spain = confirmed_total_date_Spain.join(fatalities_total_date_Spain)\n\n#confirmed_country_UK = train[(train['Country_Region']=='United Kingdom') & (train['ConfirmedCases']!=0)].groupby(['Country_Region', 'Province_State']).agg({'ConfirmedCases':['sum']})\n#fatalities_country_UK = train[(train['Country_Region']=='United Kingdom') & (train['ConfirmedCases']!=0)].groupby(['Country_Region', 'Province_State']).agg({'Fatalities':['sum']})\nconfirmed_total_date_UK = train[(train['Country_Region']=='United Kingdom') & (train['ConfirmedCases']!=0)].groupby(['Date']).agg({'ConfirmedCases':['sum']})\nfatalities_total_date_UK = train[(train['Country_Region']=='United Kingdom') & (train['ConfirmedCases']!=0)].groupby(['Date']).agg({'Fatalities':['sum']})\ntotal_date_UK = confirmed_total_date_UK.join(fatalities_total_date_UK)\n\n#confirmed_country_Australia = train[(train['Country_Region']=='Australia') & (train['ConfirmedCases']!=0)].groupby(['Country_Region', 'Province_State']).agg({'ConfirmedCases':['sum']})\n#fatalities_country_Australia = train[(train['Country_Region']=='Australia') & (train['ConfirmedCases']!=0)].groupby(['Country_Region', 'Province_State']).agg({'Fatalities':['sum']})\nconfirmed_total_date_Australia = train[(train['Country_Region']=='Australia') & (train['ConfirmedCases']!=0)].groupby(['Date']).agg({'ConfirmedCases':['sum']})\nfatalities_total_date_Australia = train[(train['Country_Region']=='Australia') & (train['ConfirmedCases']!=0)].groupby(['Date']).agg({'Fatalities':['sum']})\ntotal_date_Australia = confirmed_total_date_Australia.join(fatalities_total_date_Australia)\n\n#confirmed_country_Singapore = train[(train['Country_Region']=='Singapore') & (train['ConfirmedCases']!=0)].groupby(['Country_Region', 'Province_State']).agg({'ConfirmedCases':['sum']})\n#fatalities_country_Singapore = train[(train['Country_Region']=='Singapore') & (train['ConfirmedCases']!=0)].groupby(['Country_Region', 'Province_State']).agg({'Fatalities':['sum']})\nconfirmed_total_date_Singapore = train[(train['Country_Region']=='Singapore') & (train['ConfirmedCases']!=0)].groupby(['Date']).agg({'ConfirmedCases':['sum']})\nfatalities_total_date_Singapore = train[(train['Country_Region']=='Singapore') & (train['ConfirmedCases']!=0)].groupby(['Date']).agg({'Fatalities':['sum']})\ntotal_date_Singapore = confirmed_total_date_Singapore.join(fatalities_total_date_Singapore)\n\nconfirmed_total_date_India = train[(train['Country_Region']=='India') & (train['ConfirmedCases']!=0)].groupby(['Date']).agg({'ConfirmedCases':['sum']})\nfatalities_total_date_India = train[(train['Country_Region']=='India') & (train['ConfirmedCases']!=0)].groupby(['Date']).agg({'Fatalities':['sum']})\ntotal_date_India = confirmed_total_date_India.join(fatalities_total_date_India)\n\nitaly = [i for i in total_date_Italy.ConfirmedCases['sum'].values]\nitaly_30 = italy[0:70] \nspain = [i for i in total_date_Spain.ConfirmedCases['sum'].values]\nspain_30 = spain[0:70] \nUK = [i for i in total_date_UK.ConfirmedCases['sum'].values]\nUK_30 = UK[0:70] \nIndia = [i for i in total_date_India.ConfirmedCases['sum'].values]\nIndia_30 = India[0:70] \nsingapore = [i for i in total_date_Singapore.ConfirmedCases['sum'].values]\nsingapore_30 = singapore[0:70] \n\n\n# Plots\nplt.figure(figsize=(12,6))\nplt.plot(italy_30)\nplt.plot(spain_30)\nplt.plot(UK_30)\nplt.plot(India_30)\nplt.plot(singapore_30)\nplt.legend([\"Italy\", \"Spain\", \"UK\", \"India\", \"Singapore\"], loc='upper left')\nplt.title(\"COVID-19 infections from the first confirmed case\", size=15)\nplt.xlabel(\"Days\", size=13)\nplt.ylabel(\"Infected cases\", size=13)\nplt.ylim(0, 130000)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observations**:\n"},{"metadata":{},"cell_type":"markdown","source":"SIR model\n\nWe have seen some general behavior of the virus in agregated data, for the country where the coronavirus was originated and for four other interesting countries. There's a lot of information to be extracted from this data; for example, we haven't analyzed the effects of long/lat of countries. However, since our main purpose is to develop a predective model in order to understand the key factors that impact the COVID-19 transmission, I'll move on to one of the most famous epidemiologic models: SIR. \n\nSIR is a simple model that considers a population that belongs to one of the following states:\n1. Susceptible (S) The individual hasn't contracted the disease, but she can be infected due to transmisison from infected people\n2. Infected (I) This person has contracted the disease\n3. Recovered/Deceased (R) The disease may lead to one of two destinies: either the person survives, hence developing inmunity to the disease, or the person is deceased. \n\nThere are many versions of this model, considering birth and death (SIRD with demography), with intermediate states, etc. However, since we are in the early stages of the COVID-19 expansion and our interest is focused in the short term, we will consider that people develops immunity (in the long term, immunity may be lost and the COVID-19 may come back within a certain seasonality like the common flu) and there is no transition from recovered to the remaining two states. With this, the differential equations that govern the system are:\n\n$$ {dS \\over dt} = - {\\beta S I \\over N} $$\n\n$$ {dI \\over dt} = {\\beta S I \\over N} - \\gamma I$$\n\n$$ {dR \\over dt} = \\gamma I$$\n\nWhere $\\beta$ is the contagion rate of the pathogen and $\\gamma$ is the recovery rate."},{"metadata":{},"cell_type":"markdown","source":"Implementing the SIR model\n\nSIR model can be implemented in many ways: from the differential equations governing the system, within a mean field approximation or running the dynamics in a social network (graph). For the sake of simplicity, I'vem chosen the first option, and we will simply run a numerical method (Runge-Kutta) to solve the differential equations system. \n\nThe functions governing the dif.eqs. are:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Susceptible equation\ndef fa(N, a, b, beta):\n    fa = -beta*a*b\n    return fa\n\n# Infected equation\ndef fb(N, a, b, beta, gamma):\n    fb = beta*a*b - gamma*b\n    return fb\n\n# Recovered/deceased equation\ndef fc(N, b, gamma):\n    fc = gamma*b\n    return fc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In order to solve the differential equations system, we develop a  4rth order [Runge-Kutta](https://en.wikipedia.org/wiki/Runge%E2%80%93Kutta_methods) method:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Runge-Kutta method of 4rth order for 3 dimensions (susceptible a, infected b and recovered r)\ndef rK4(N, a, b, c, fa, fb, fc, beta, gamma, hs):\n    a1 = fa(N, a, b, beta)*hs\n    b1 = fb(N, a, b, beta, gamma)*hs\n    c1 = fc(N, b, gamma)*hs\n    ak = a + a1*0.5\n    bk = b + b1*0.5\n    ck = c + c1*0.5\n    a2 = fa(N, ak, bk, beta)*hs\n    b2 = fb(N, ak, bk, beta, gamma)*hs\n    c2 = fc(N, bk, gamma)*hs\n    ak = a + a2*0.5\n    bk = b + b2*0.5\n    ck = c + c2*0.5\n    a3 = fa(N, ak, bk, beta)*hs\n    b3 = fb(N, ak, bk, beta, gamma)*hs\n    c3 = fc(N, bk, gamma)*hs\n    ak = a + a3\n    bk = b + b3\n    ck = c + c3\n    a4 = fa(N, ak, bk, beta)*hs\n    b4 = fb(N, ak, bk, beta, gamma)*hs\n    c4 = fc(N, bk, gamma)*hs\n    a = a + (a1 + 2*(a2 + a3) + a4)/6\n    b = b + (b1 + 2*(b2 + b3) + b4)/6\n    c = c + (c1 + 2*(c2 + c3) + c4)/6\n    return a, b, c","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And finally, to obtain the evolution of the disease we simply define the initial conditions and call the rk4 method:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def SIR(N, b0, beta, gamma, hs):\n    \n    \"\"\"\n    N = total number of population\n    beta = transition rate S->I\n    gamma = transition rate I->R\n    k =  denotes the constant degree distribution of the network (average value for networks in which \n    the probability of finding a node with a different connectivity decays exponentially fast\n    hs = jump step of the numerical integration\n    \"\"\"\n    \n    # Initial condition\n    a = float(N-1)/N -b0\n    b = float(1)/N +b0\n    c = 0.\n\n    sus, inf, rec= [],[],[]\n    for i in range(10000): # Run for a certain number of time-steps\n        sus.append(a)\n        inf.append(b)\n        rec.append(c)\n        a,b,c = rK4(N, a, b, c, fa, fb, fc, beta, gamma, hs)\n\n    return sus, inf, rec","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Results obtained for N=world population, only one initial infected case, $\\beta=0.3$, $\\gamma=0.5$ and a leap pass $h_s = 0.1$ are shown below:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Parameters of the model\nN = 7800*(10**6)\nb0 = 0\nbeta = 0.7\ngamma = 0.2\nhs = 0.1\n\nsus, inf, rec = SIR(N, b0, beta, gamma, hs)\n\nf = plt.figure(figsize=(8,5)) \nplt.plot(sus, 'b.', label='susceptible');\nplt.plot(inf, 'r.', label='infected');\nplt.plot(rec, 'c.', label='recovered/deceased');\nplt.title(\"SIR model\")\nplt.xlabel(\"time\", fontsize=10);\nplt.ylabel(\"Fraction of population\", fontsize=10);\nplt.legend(loc='best')\nplt.xlim(0,1000)\nplt.savefig('SIR_example.png')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Fit SIR parameters to real data\n\nThe SIR model is purely theoretical, and we are interested into a real approximation of the COVID-19 expansion in order to extract insights and understand the transmission of the virus. Hence, we need to extract the $\\beta$ and $\\gamma$ paramaters for each case if we hope to be able to predict the evolution of the system."},{"metadata":{"trusted":true},"cell_type":"code","source":"population = float(46750238)\ncountry_df = pd.DataFrame()\ncountry_df['ConfirmedCases'] = train.loc[train['Country_Region']=='Spain'].ConfirmedCases.diff().fillna(0)\ncountry_df = country_df[10:]\ncountry_df['day_count'] = list(range(1,len(country_df)+1))\n\nydata = [i for i in country_df.ConfirmedCases]\nxdata = country_df.day_count\nydata = np.array(ydata, dtype=float)\nxdata = np.array(xdata, dtype=float)\n\nN = population\ninf0 = ydata[0]\nsus0 = N - inf0\nrec0 = 0.0\n\ndef sir_model(y, x, beta, gamma):\n    sus = -beta * y[0] * y[1] / N\n    rec = gamma * y[1]\n    inf = -(sus + rec)\n    return sus, inf, rec\n\ndef fit_odeint(x, beta, gamma):\n    return integrate.odeint(sir_model, (sus0, inf0, rec0), x, args=(beta, gamma))[:,1]\n\npopt, pcov = optimize.curve_fit(fit_odeint, xdata, ydata)\nfitted = fit_odeint(xdata, *popt)\n\nplt.plot(xdata, ydata, 'o')\nplt.plot(xdata, fitted)\nplt.title(\"Fit of SIR model for Spain infected cases\")\nplt.ylabel(\"Population infected\")\nplt.xlabel(\"Days\")\nplt.show()\nprint(\"Optimal parameters: beta =\", popt[0], \" and gamma = \", popt[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Data enrichment\n\nAnalyzing SIR simulations was meant to understand a model that approximately resembles the transmission mechanism of many virus, including the COVID-19. However, there are alternative methods that may prove being equally useful both to predict and to understand the pandemic evolution. Many of these methods rely on having rich data to extract conclusions and allow algorithms to extrapolate patterns in data, and that is exactly what we are going to do. \n\nMain workflow of this section:\n1. Join data, filter dates and clean missings\n2. Compute lags and trends\n3. Add country details\n\n**Disclaimer**: this data enrichment is not mandatory and we could end up not using all of the new features in our models. However I consider it a didactical step that will surely add some value, for example in an in-depth exploratory analysis."},{"metadata":{},"cell_type":"markdown","source":"Join data, filter dates and clean missings\n\nFirst of all, we perform some pre-processing prepare the dataset, consisting on:\n\n* **Join data**. Join train/test to facilitate data transformations\n* **Filter dates**. According to the challenge conditions, remove ConfirmedCases and Fatalities post 2020-03-12. Create additional date columns\n* **Missings**. Analyze and fix missing values"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Merge train and test, exclude overlap\ndates_overlap = ['2020-04-01', '2020-04-02', '2020-04-03', '2020-04-04', '2020-04-05', '2020-04-06', '2020-04-07', '2020-04-08',\n                 '2020-04-09', '2020-04-10']\ntrain2 = train.loc[~train['Date'].isin(dates_overlap)]\nall_data = pd.concat([train2, test], axis = 0, sort=False)\n\n# Double check that there are no informed ConfirmedCases and Fatalities after 2020-03-11\nall_data.loc[all_data['Date'] >= '2020-04-01', 'ConfirmedCases'] = 0\nall_data.loc[all_data['Date'] >= '2020-04-01', 'Fatalities'] = 0\nall_data['Date'] = pd.to_datetime(all_data['Date'])\n\n# Create date columns\nle = preprocessing.LabelEncoder()\nall_data['Day_num'] = le.fit_transform(all_data.Date)\nall_data['Day'] = all_data['Date'].dt.day\nall_data['Month'] = all_data['Date'].dt.month\nall_data['Year'] = all_data['Date'].dt.year\n\n# Fill null values given that we merged train-test datasets\nall_data['Province_State'].fillna(\"None\", inplace=True)\nall_data['ConfirmedCases'].fillna(0, inplace=True)\nall_data['Fatalities'].fillna(0, inplace=True)\nall_data['Id'].fillna(-1, inplace=True)\nall_data['ForecastId'].fillna(-1, inplace=True)\n\ndisplay(all_data)\ndisplay(all_data.loc[all_data['Date'] == '2020-04-01'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Double-check that there are no remaining missing values:"},{"metadata":{"trusted":true},"cell_type":"code","source":"missings_count = {col:all_data[col].isnull().sum() for col in all_data.columns}\nmissings = pd.DataFrame.from_dict(missings_count, orient='index')\nprint(missings.nlargest(30, 0))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Compute lags and trends\n\nEnriching a dataset is key to obtain good results. In this case we will apply 2 different transformations:\n\nLag. Lags are a way to compute the previous value of a column, so that the lag 1 for ConfirmedCases would inform the this column from the previous day. The lag 3 of a feature X is simply:\n$$X_{lag3}(t) = X(t-3)$$\n\n\nTrend. Transformig a column into its trend gives the natural tendency of this column, which is different from the raw value. The definition of trend I will apply is: \n$$Trend_{X} = {X(t) - X(t-1) \\over X(t-1)}$$\n\nThe backlog of lags I'll apply is 14 days, while for trends is 7 days.  For ConfirmedCases and Fatalities:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def calculate_lag(df, lag_list, column):\n    for lag in lag_list:\n        column_lag = column + \"_\" + str(lag)\n        df[column_lag] = df.groupby(['Country_Region', 'Province_State'])[column].shift(lag, fill_value=0)\n    return df\n\ndef calculate_trend(df, lag_list, column):\n    for lag in lag_list:\n        trend_column_lag = \"Trend_\" + column + \"_\" + str(lag)\n        df[trend_column_lag] = (df.groupby(['Country_Region', 'Province_State'])[column].shift(0, fill_value=0) - \n                                df.groupby(['Country_Region', 'Province_State'])[column].shift(lag, fill_value=0))/df.groupby(['Country_Region', 'Province_State'])[column].shift(lag, fill_value=0.001)\n    return df\n\n\nts = time.time()\nall_data = calculate_lag(all_data.reset_index(), range(1,7), 'ConfirmedCases')\nall_data = calculate_lag(all_data, range(1,7), 'Fatalities')\nall_data = calculate_trend(all_data, range(1,7), 'ConfirmedCases')\nall_data = calculate_trend(all_data, range(1,7), 'Fatalities')\nall_data.replace([np.inf, -np.inf], 0, inplace=True)\nall_data.fillna(0, inplace=True)\nprint(\"Time spent: \", time.time()-ts)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you see, the process is really fast. An example of some of the lag/trend columns for Spain:"},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data[all_data['Country_Region']=='Spain'].iloc[40:50][['Id', 'Province_State', 'Country_Region', 'Date',\n       'ConfirmedCases', 'Fatalities', 'ForecastId', 'Day_num', 'ConfirmedCases_1',\n       'ConfirmedCases_2', 'ConfirmedCases_3', 'Fatalities_1', 'Fatalities_2',\n       'Fatalities_3']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Add country details"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load countries data file\nworld_population = pd.read_csv(\"/kaggle/input/population-by-country-2020/population_by_country_2020.csv\")\n\n# Select desired columns and rename some of them\nworld_population = world_population[['Country (or dependency)', 'Population (2020)', 'Density (P/Km²)', 'Land Area (Km²)', 'Med. Age', 'Urban Pop %']]\nworld_population.columns = ['Country (or dependency)', 'Population (2020)', 'Density', 'Land Area', 'Med Age', 'Urban Pop']\n\n# Replace United States by US\nworld_population.loc[world_population['Country (or dependency)']=='United States', 'Country (or dependency)'] = 'US'\n\n# Remove the % character from Urban Pop values\nworld_population['Urban Pop'] = world_population['Urban Pop'].str.rstrip('%')\n\n# Replace Urban Pop and Med Age \"N.A\" by their respective modes, then transform to int\nworld_population.loc[world_population['Urban Pop']=='N.A.', 'Urban Pop'] = int(world_population.loc[world_population['Urban Pop']!='N.A.', 'Urban Pop'].mode()[0])\nworld_population['Urban Pop'] = world_population['Urban Pop'].astype('int16')\nworld_population.loc[world_population['Med Age']=='N.A.', 'Med Age'] = int(world_population.loc[world_population['Med Age']!='N.A.', 'Med Age'].mode()[0])\nworld_population['Med Age'] = world_population['Med Age'].astype('int16')\n\nprint(\"Cleaned country details dataset\")\ndisplay(world_population)\n\n# Now join the dataset to our previous DataFrame and clean missings (not match in left join)- label encode cities\nprint(\"Joined dataset\")\nall_data = all_data.merge(world_population, left_on='Country_Region', right_on='Country (or dependency)', how='left')\nall_data[['Population (2020)', 'Density', 'Land Area', 'Med Age', 'Urban Pop']] = all_data[['Population (2020)', 'Density', 'Land Area', 'Med Age', 'Urban Pop']].fillna(0)\ndisplay(all_data)\n\nprint(\"Encoded dataset\")\n# Label encode countries and provinces. Save dictionary for exploration purposes\nall_data.drop('Country (or dependency)', inplace=True, axis=1)\nall_data['Country_Region'] = le.fit_transform(all_data['Country_Region'])\nnumber_c = all_data['Country_Region']\ncountries = le.inverse_transform(all_data['Country_Region'])\ncountry_dict = dict(zip(countries, number_c)) \nall_data['Province_State'] = le.fit_transform(all_data['Province_State'])\nnumber_p = all_data['Province_State']\nprovince = le.inverse_transform(all_data['Province_State'])\nprovince_dict = dict(zip(province, number_p)) \ndisplay(all_data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Predictions for the early stages of the transmission\n\nOur obective in this section consists on  predicting the evolution of the expansion from a data-centric perspective, like any other regression problem. To do so, remember that the challenge specifies that submissions on the public LB shouldn only contain data previous to 2020-03-26.\n\nModels to apply:\n1. Linear Regression for one country\n2. Linear Regression for all countries (method 1)\n3. Linear Regression for all countries (method 2)"},{"metadata":{},"cell_type":"markdown","source":"Linear Regression for one country\n\nSince we are interested into predicting the future time evolution of the pandemic, our first approach consists on a simple Linear Regression. However, remind that **the evolution is** not linear but **exponential** (only in the beginning of the infection), so that a preliminar log transformation is needed. \n\nVisual comparison of both cases for Spain and with data from last 10 days informed, starting on March 1st:"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,6))\n\n# Day_num = 38 is March 1st\ny1 = all_data[(all_data['Country_Region']==country_dict['Spain']) & (all_data['Day_num']>39) & (all_data['Day_num']<=49)][['ConfirmedCases']]\nx1 = range(0, len(y1))\nax1.plot(x1, y1, 'bo--')\nax1.set_title(\"Spain ConfirmedCases between days 39 and 49\")\nax1.set_xlabel(\"Days\")\nax1.set_ylabel(\"ConfirmedCases\")\n\ny2 = all_data[(all_data['Country_Region']==country_dict['Spain']) & (all_data['Day_num']>39) & (all_data['Day_num']<=49)][['ConfirmedCases']].apply(lambda x: np.log(x))\nx2 = range(0, len(y2))\nax2.plot(x2, y2, 'bo--')\nax2.set_title(\"Spain Log ConfirmedCases between days 39 and 49\")\nax2.set_xlabel(\"Days\")\nax2.set_ylabel(\"Log ConfirmedCases\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you see, the log transformation results in a fancy straight-like line, which is awesome for Linear Regression. However, let me clarify two important points:\n\n* This \"roughly **exponential behavior**\" **is only true for the initial infection stages of the pandemic** (the initial increasing of infections on the SIR model), but that's exactly the point where most countries are at the moment.\n\n* Why do I only extract the last 10 days of data? For three reasons:\n    1. In order to capture exactly the very short term component of the evolution\n    2. To prevent the effects of certain variables that have been impacting the transmition speed (quarantine vs free circulation)\n    3. To prevent differences on criteria when confirming cases (remember that weird slope on the China plot?)"},{"metadata":{},"cell_type":"markdown","source":"This first model is very simple, and only elemental features will be considered: Country/Region, date information, Long and Lat. Lags. Engineered columns like lags, trends and country details are not introduced as an input. Finally, the workflow for the Basic Linear Regression model is:\n1. **Features**. Select features\n2. **Dates**. Filter train data from 2020-03-01 to 2020-03-18\n2. **Log transformation**. Apply log transformation to ConfirmedCases and Fatalities\n3. **Infinites**. Replace infinites from the logarithm with 0. Given the asymptotic behavior of the logarithm for log(0),this implies that when applying the inverse transformation (exponential) a 1 will be returned instead of a 0. This problem does not impact many countries, but still needs to be tackled sooner or later in order to obtain a clean solution.\n4. **Train/test split**. Split into train/valid/test\n5. **Prediction**. Linear Regression, training country by country and joining data\n6. **Submit**. Submit results in the correct format, and applying exponential to reverse log transformation "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Filter selected features\ndata = all_data.copy()\nfeatures = ['Id', 'ForecastId', 'Country_Region', 'Province_State', 'ConfirmedCases', 'Fatalities', \n       'Day_num']\ndata = data[features]\n\n# Apply log transformation to all ConfirmedCases and Fatalities columns, except for trends\ndata[['ConfirmedCases', 'Fatalities']] = data[['ConfirmedCases', 'Fatalities']].astype('float64')\ndata[['ConfirmedCases', 'Fatalities']] = data[['ConfirmedCases', 'Fatalities']].apply(lambda x: np.log1p(x))\n\n# Replace infinites\ndata.replace([np.inf, -np.inf], 0, inplace=True)\n\n\n# Split data into train/test\ndef split_data(df, train_lim, test_lim):\n    \n    df.loc[df['Day_num']<=train_lim , 'ForecastId'] = -1\n    df = df[df['Day_num']<=test_lim]\n    \n    # Train set\n    x_train = df[df.ForecastId == -1].drop(['ConfirmedCases', 'Fatalities'], axis=1)\n    y_train_1 = df[df.ForecastId == -1]['ConfirmedCases']\n    y_train_2 = df[df.ForecastId == -1]['Fatalities']\n\n    # Test set\n    x_test = df[df.ForecastId != -1].drop(['ConfirmedCases', 'Fatalities'], axis=1)\n\n    # Clean Id columns and keep ForecastId as index\n    x_train.drop('Id', inplace=True, errors='ignore', axis=1)\n    x_train.drop('ForecastId', inplace=True, errors='ignore', axis=1)\n    x_test.drop('Id', inplace=True, errors='ignore', axis=1)\n    x_test.drop('ForecastId', inplace=True, errors='ignore', axis=1)\n    \n    return x_train, y_train_1, y_train_2, x_test\n\n\n# Linear regression model\ndef lin_reg(X_train, Y_train, X_test):\n    # Create linear regression object\n    regr = linear_model.LinearRegression()\n\n    # Train the model using the training sets\n    regr.fit(X_train, Y_train)\n\n    # Make predictions using the testing set\n    y_pred = regr.predict(X_test)\n    \n    return regr, y_pred\n\n\n# Submission function\ndef get_submission(df, target1, target2):\n    \n    prediction_1 = df[target1]\n    prediction_2 = df[target2]\n\n    # Submit predictions\n    prediction_1 = [int(item) for item in list(map(round, prediction_1))]\n    prediction_2 = [int(item) for item in list(map(round, prediction_2))]\n    \n    submission = pd.DataFrame({\n        \"ForecastId\": df['ForecastId'].astype('int32'), \n        \"ConfirmedCases\": prediction_1, \n        \"Fatalities\": prediction_2\n    })\n    submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's try to see results when training with a single country:\n\n* **Spain**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Select train (real) data from March 1 to March 22nd\ndates_list = ['2020-03-01', '2020-03-02', '2020-03-03', '2020-03-04', '2020-03-05', '2020-03-06', '2020-03-07', '2020-03-08', '2020-03-09', \n                 '2020-03-10', '2020-03-11','2020-03-12','2020-03-13','2020-03-14','2020-03-15','2020-03-16','2020-03-17','2020-03-18',\n                 '2020-03-19','2020-03-20','2020-03-21','2020-03-22','2020-03-23', '2020-03-24', '2020-03-25', '2020-03-26', '2020-03-27', \n                 '2020-03-28', '2020-03-29', '2020-03-30', '2020-03-31', '2020-04-01', '2020-04-02', '2020-04-03', '2020-04-04', '2020-04-05', \n                 '2020-04-06', '2020-04-07', '2020-04-08', '2020-04-09', '2020-04-10']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data.loc[all_data['Country_Region']==country_dict['Spain']][50:70]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_linreg_basic_country(data, country_name, dates_list, day_start, shift, train_lim, test_lim):\n    \n    data_country = data[data['Country_Region']==country_dict[country_name]]\n    data_country = data_country.loc[data_country['Day_num']>=day_start]\n    X_train, Y_train_1, Y_train_2, X_test = split_data(data_country, train_lim, test_lim)\n    model, pred = lin_reg(X_train, Y_train_1, X_test)\n\n    # Create a df with both real cases and predictions (predictions starting on March 12th)\n    X_train_check = X_train.copy()\n    X_train_check['Target'] = Y_train_1\n\n    X_test_check = X_test.copy()\n    X_test_check['Target'] = pred\n\n    X_final_check = pd.concat([X_train_check, X_test_check])\n\n    # Select predictions from March 1st to March 25th\n    predicted_data = X_final_check.loc[(X_final_check['Day_num'].isin(list(range(day_start, day_start+len(dates_list)))))].Target\n    real_data = train.loc[(train['Country_Region']==country_name) & (train['Date'].isin(dates_list))]['ConfirmedCases']\n    dates_list_num = list(range(0,len(dates_list)))\n\n    # Plot results\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,6))\n\n    ax1.plot(dates_list_num, np.expm1(predicted_data))\n    ax1.plot(dates_list_num, real_data)\n    ax1.axvline(30-shift, linewidth=2, ls = ':', color='grey', alpha=0.5)\n    ax1.legend(['Predicted cases', 'Actual cases', 'Train-test split'], loc='upper left')\n    ax1.set_xlabel(\"Day count (from March \" + str(1+shift) + \" to March 25th)\")\n    ax1.set_ylabel(\"Confirmed Cases\")\n\n    ax2.plot(dates_list_num, predicted_data)\n    ax2.plot(dates_list_num, np.log1p(real_data))\n    ax2.axvline(30-shift, linewidth=2, ls = ':', color='grey', alpha=0.5)\n    ax2.legend(['Predicted cases', 'Actual cases', 'Train-test split'], loc='upper left')\n    ax2.set_xlabel(\"Day count (from March \" + str(1+shift) + \" to March 30th)\")\n    ax2.set_ylabel(\"Log Confirmed Cases\")\n\n    plt.suptitle((\"ConfirmedCases predictions based on Log-Lineal Regression for \"+country_name))\n    \n    \n    \n# Filter Spain, run the Linear Regression workflow\ncountry_name = \"Spain\"\nmarch_day = 10\nday_start = 39+march_day\ndates_list2 = dates_list[march_day:]\ntrain_lim, test_lim = 69, 112\nplot_linreg_basic_country(data, country_name, dates_list2, day_start, march_day, train_lim, test_lim)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Filter Spain, run the Linear Regression workflow\ncountry_name = \"Spain\"\nmarch_day = 21\nday_start = 39+march_day\ndates_list2 = dates_list[march_day:]\nplot_linreg_basic_country(data, country_name, dates_list2, day_start, march_day, train_lim, test_lim)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **Italy**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Filter Italy, run the Linear Regression workflow\ncountry_name = \"Italy\"\nmarch_day = 10\nday_start = 39+march_day\ndates_list2 = dates_list[march_day:]\nplot_linreg_basic_country(data, country_name, dates_list2, day_start, march_day, train_lim, test_lim)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Filter Italy, run the Linear Regression workflow\ncountry_name = \"Italy\"\nmarch_day = 21\nday_start = 39+march_day\ndates_list2 = dates_list[march_day:]\nplot_linreg_basic_country(data, country_name, dates_list2, day_start, march_day, train_lim, test_lim)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **Germany**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Filter Germany, run the Linear Regression workflow\ncountry_name = \"Germany\"\nmarch_day = 10\nday_start = 39+march_day\ndates_list2 = dates_list[march_day:]\nplot_linreg_basic_country(data, country_name, dates_list2, day_start, march_day, train_lim, test_lim)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Filter Germany, run the Linear Regression workflow\ncountry_name = \"Germany\"\nmarch_day = 21\nday_start = 39+march_day\ndates_list2 = dates_list[march_day:]\nplot_linreg_basic_country(data, country_name, dates_list2, day_start, march_day, train_lim, test_lim)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **India**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Filter Albania, run the Linear Regression workflow\ncountry_name = \"India\"\nmarch_day = 10\nday_start = 39+march_day\ndates_list2 = dates_list[march_day:]\nplot_linreg_basic_country(data, country_name, dates_list2, day_start, march_day, train_lim, test_lim)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Filter Albania, run the Linear Regression workflow\ncountry_name = \"Albania\"\nmarch_day = 21\nday_start = 39+march_day\ndates_list2 = dates_list[march_day:]\nplot_linreg_basic_country(data, country_name, dates_list2, day_start, march_day, train_lim, test_lim)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **Andorra**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Filter Andorra, run the Linear Regression workflow\ncountry_name = \"Andorra\"\nshift = 10\nday_start = 39+shift\ndates_list2 = dates_list[shift:]\nplot_linreg_basic_country(data, country_name, dates_list2, day_start, shift, train_lim, test_lim)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Filter Andorra, run the Linear Regression workflow\ncountry_name = \"Andorra\"\nshift = 21\nday_start = 39+shift\ndates_list2 = dates_list[shift:]\nplot_linreg_basic_country(data, country_name, dates_list2, day_start, shift, train_lim, test_lim)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observations**:\n\n* The general evolution is captured despite the simplicity of the model\n* The cumulative infected cases has been changing in March, so that using thw whole month data for training results in overestimated predictions. When reducing the training set to only a few days prior the testing region, results are better. This is capturing the problem of the exponential behavior, that is only true for the early stages of the spreading. Now the behavior is more complex, and in order to predict the evolution with large portions of the historic evolution, alternative models are required (sigmoid, ARIMA...)\n* Estimations are increasingly worse as time passes (harder to extrapolate)\n* Countries that recently confirmed their first contagions are difficult to predict (less data points) \n* Countries with 0 cases in the whole training dataset are predicted as non-infected (no datapoints)\n\n** Questions to tackle in next subsections**:\n* How to obtain the full submission set? \n* What to do for countries with different Provinces/State informed?\n* Is there any alternative to manually setting the size of the train data? "},{"metadata":{},"cell_type":"markdown","source":"Linear Regression for all countries (method 1)\n\nWe've recently discovered that when fitting only with 10 historical datapoints some problematic scenarios appear, that impact the performance of our Linear Regressor. Let's generalize the model for all countries to verify if it's an unavoidable problem. Steps to run for all countries:\n\n1. Loop for each country\n2. Compute provinces list\n3. If there are provinces, run the Linear Regressor for each of them\n4. Otherwise just run the Linear Regressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\n\ndef linreg_basic_all_countries(data, day_start, train_lim, test_lim):\n    \n    data2 = data.loc[data.Day_num >= day_start]\n\n    # Set the dataframe where we will update the predictions\n    data_pred = data[data.ForecastId != -1][['Country_Region', 'Province_State', 'Day_num', 'ForecastId']]\n    data_pred = data_pred.loc[data_pred['Day_num']>=day_start]\n    data_pred['Predicted_ConfirmedCases'] = [0]*len(data_pred)\n    data_pred['Predicted_Fatalities'] = [0]*len(data_pred)\n\n    print(\"Currently running Linear Regression for all countries\")\n\n    # Main loop for countries\n    for c in data2['Country_Region'].unique():\n\n        # List of provinces\n        provinces_list = data2[data2['Country_Region']==c]['Province_State'].unique()\n\n        # If the country has several Province/State informed\n        if len(provinces_list)>1:\n            for p in provinces_list:\n                data_cp = data2[(data2['Country_Region']==c) & (data2['Province_State']==p)]\n                X_train, Y_train_1, Y_train_2, X_test = split_data(data_cp, train_lim, test_lim)\n                model_1, pred_1 = lin_reg(X_train, Y_train_1, X_test)\n                model_2, pred_2 = lin_reg(X_train, Y_train_2, X_test)\n                data_pred.loc[((data_pred['Country_Region']==c) & (data2['Province_State']==p)), 'Predicted_ConfirmedCases'] = pred_1\n                data_pred.loc[((data_pred['Country_Region']==c) & (data2['Province_State']==p)), 'Predicted_Fatalities'] = pred_2\n\n        # No Province/State informed\n        else:\n            data_c = data2[(data2['Country_Region']==c)]\n            X_train, Y_train_1, Y_train_2, X_test = split_data(data_c, train_lim, test_lim)\n            model_1, pred_1 = lin_reg(X_train, Y_train_1, X_test)\n            model_2, pred_2 = lin_reg(X_train, Y_train_2, X_test)\n            data_pred.loc[(data_pred['Country_Region']==c), 'Predicted_ConfirmedCases'] = pred_1\n            data_pred.loc[(data_pred['Country_Region']==c), 'Predicted_Fatalities'] = pred_2\n\n    # Apply exponential transf. and clean potential infinites due to final numerical precision\n    data_pred[['Predicted_ConfirmedCases', 'Predicted_Fatalities']] = data_pred[['Predicted_ConfirmedCases', 'Predicted_Fatalities']].apply(lambda x: np.expm1(x))\n    data_pred.replace([np.inf, -np.inf], 0, inplace=True) \n    \n    return data_pred\n\n\nday_start = 65\n#data_pred = linreg_basic_all_countries(data, day_start, train_lim, test_lim)\n#get_submission(data_pred, 'Predicted_ConfirmedCases', 'Predicted_Fatalities')\n\nprint(\"Process finished in \", round(time.time() - ts, 2), \" seconds\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Final [LMSE score](https://medium.com/analytics-vidhya/root-mean-square-log-error-rmse-vs-rmlse-935c6cc1802a) for week 2, with training data prior to 2020-03-19 and measures on date 2020-04-01: \n\n**1.19681**"},{"metadata":{},"cell_type":"markdown","source":"Linear Regression for all countries (method 2)\n\nAn alternative method to setting the number of days for the training step is to simply keep all data for each country since the first case was confirmed. However, since there are certain countries were the initial outbreak was very smooth (i.e. in Spain there was only one confirmed case for 7 days in a row), predictions may be biased by these initial periods."},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\n\n# Set the dataframe where we will update the predictions\nday_start = 65\ndata2 = data.loc[data.Day_num >= day_start]\ndata_pred3 = data2[data2.ForecastId != -1][['Country_Region', 'Province_State', 'Day_num', 'ForecastId']]\ndata_pred3['Predicted_ConfirmedCases'] = [0]*len(data_pred3)\ndata_pred3['Predicted_Fatalities'] = [0]*len(data_pred3)\nhow_many_days = test.Date.nunique()\n    \nprint(\"Currently running Linear Regression for all countries\")\n\n# Main loop for countries\nfor c in data['Country_Region'].unique():\n    \n    # List of provinces\n    provinces_list = data2[data2['Country_Region']==c]['Province_State'].unique()\n        \n    # If the country has several Province/State informed\n    if len(provinces_list)>1:\n        \n        for p in provinces_list:\n            # Only fit starting from the first confirmed case in the country\n            train_countries_no0 = data.loc[(data['Country_Region']==c) & (data['Province_State']==p) & (data.ConfirmedCases!=0) & (data.ForecastId==-1)]\n            test_countries_no0 = data.loc[(data['Country_Region']==c) & (data['Province_State']==p) &  (data.ForecastId!=-1)]\n            data2 = pd.concat([train_countries_no0, test_countries_no0])\n\n            # If there are no previous cases, predict 0\n            if len(train_countries_no0) == 0:\n                data_pred3.loc[((data_pred2['Country_Region']==c) & (data_pred3['Province_State']==p)), 'Predicted_ConfirmedCases'] = [0]*how_many_days\n                data_pred3.loc[((data_pred2['Country_Region']==c) & (data_pred3['Province_State']==p)), 'Predicted_Fatalities'] = [0]*how_many_days\n                \n            # Else run LinReg\n            else: \n                data_cp = data2[(data2['Country_Region']==c) & (data2['Province_State']==p)]\n                X_train, Y_train_1, Y_train_2, X_test = split_data(data_cp, train_lim, test_lim)\n                model_1, pred_1 = lin_reg(X_train, Y_train_1, X_test)\n                model_2, pred_2 = lin_reg(X_train, Y_train_2, X_test)\n                data_pred3.loc[((data_pred3['Country_Region']==c) & (data_pred3['Province_State']==p)), 'Predicted_ConfirmedCases'] = pred_1\n                data_pred3.loc[((data_pred3['Country_Region']==c) & (data_pred3['Province_State']==p)), 'Predicted_Fatalities'] = pred_2\n\n    # No Province/State informed\n    else:\n        # Only fit starting from the first confirmed case in the country\n        train_countries_no0 = data.loc[(data['Country_Region']==c) & (data.ConfirmedCases!=0) & (data.ForecastId==-1)]\n        test_countries_no0 = data.loc[(data['Country_Region']==c) &  (data.ForecastId!=-1)]\n        data2 = pd.concat([train_countries_no0, test_countries_no0])\n\n        # If there are no previous cases, predict 0\n        if len(train_countries_no0) == 0:\n            data_pred3.loc[((data_pred3['Country_Region']==c)), 'Predicted_ConfirmedCases'] = [0]*how_many_days\n            data_pred3.loc[((data_pred3['Country_Region']==c)), 'Predicted_Fatalities'] = [0]*how_many_days\n        \n        # Else, run LinReg\n        else:\n            data_c = data2[(data2['Country_Region']==c)]\n            X_train, Y_train_1, Y_train_2, X_test = split_data(data_c, train_lim, test_lim)\n            model_1, pred_1 = lin_reg(X_train, Y_train_1, X_test)\n            model_2, pred_2 = lin_reg(X_train, Y_train_2, X_test)\n            data_pred3.loc[(data_pred3['Country_Region']==c), 'Predicted_ConfirmedCases'] = pred_1\n            data_pred3.loc[(data_pred3['Country_Region']==c), 'Predicted_Fatalities'] = pred_2\n\n# Aplly exponential transf. and clean potential infinites due to final numerical precision\n#data_pred3[['Predicted_ConfirmedCases', 'Predicted_Fatalities']] = data_pred3[['Predicted_ConfirmedCases', 'Predicted_Fatalities']].apply(lambda x: np.expm1(x))\n#data_pred3.replace([np.inf, -np.inf], 0, inplace=True) \n\n#get_submission(data_pred3, 'Predicted_ConfirmedCases', 'Predicted_Fatalities')\n\nprint(\"Process finished in \", round(time.time() - ts, 2), \" seconds\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From my experiments, this apporach is not suitable for our linear regression model. In many cases there are strong transitional periods at the beginning, which frequently biases the regression. Hence, I will not apply this method on following sections, but you are welcome to use it for any other purposes.\n\nFinal [LMSE score](https://medium.com/analytics-vidhya/root-mean-square-log-error-rmse-vs-rmlse-935c6cc1802a) for week 2, with training data prior to 2020-03-19 and measures on date 2020-04-01: \n\n**1.62190**"},{"metadata":{},"cell_type":"markdown","source":"Linear regression with lags\n\nWith all the previous results in mind, I quite believe that Linear Regression is a good approach for the early stages of the COVID-19 spread. Of course, this is only true for the initial outbreak we are analysing, and there's no way our model could predict when the number of new infections is going to decrease. But for short-term prediction purposes everything is fine, and we are in disposition to try to improve the results.\n\nRemember those lagged variables we computed some sections before? Now it's time to use them, but first there's a problem to solve. If we use our dataset to predict the next following days of contagions, for the first day all the lags will be reported (from the previous days), but what about the next days? **Many of the lags will be unknown** (flagged as 0), since the number of ConfirmedCases is only known for the train subset. The most simple approach to overcome this is:\n\n1. Begin with the train dataset, with all cases and lags reported\n2. Forecast only the following day, through the Linear Regression\n3. Set the new prediction as a confirmed case\n4. Recompute lags\n5. Repeat from step 2 to step 4 for all remaining days\n\nAs usual, I'll start training on single countries in order to analyze the behavior of the model with these new features."},{"metadata":{"trusted":true},"cell_type":"code","source":"# New split function, for one forecast day\ndef split_data_one_day(df, d, train_lim, test_lim):\n    \n    df.loc[df['Day_num']<=train_lim , 'ForecastId'] = -1\n    df = df[df['Day_num']<=test_lim]\n    \n    #Train\n    x_train = df[df.Day_num<d]\n    y_train_1 = x_train.ConfirmedCases\n    y_train_2 = x_train.Fatalities\n    x_train.drop(['ConfirmedCases', 'Fatalities'], axis=1, inplace=True)\n    \n    #Test\n    x_test = df[df.Day_num==d]\n    x_test.drop(['ConfirmedCases', 'Fatalities'], axis=1, inplace=True)\n    \n    # Clean Id columns and keep ForecastId as index\n    x_train.drop('Id', inplace=True, errors='ignore', axis=1)\n    x_train.drop('ForecastId', inplace=True, errors='ignore', axis=1)\n    x_test.drop('Id', inplace=True, errors='ignore', axis=1)\n    x_test.drop('ForecastId', inplace=True, errors='ignore', axis=1)\n    \n    return x_train, y_train_1, y_train_2, x_test\n\n\ndef plot_real_vs_prediction_country(data, train, country_name, day_start, dates_list, march_day):\n\n    # Select predictions from March 1st to March 25th\n    predicted_data = data.loc[(data['Day_num'].isin(list(range(day_start, day_start+len(dates_list)))))].ConfirmedCases\n    real_data = train.loc[(train['Country_Region']==country_name) & (train['Date'].isin(dates_list))]['ConfirmedCases']\n    dates_list_num = list(range(0,len(dates_list)))\n\n    # Plot results\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,6))\n\n    ax1.plot(dates_list_num, np.expm1(predicted_data))\n    ax1.plot(dates_list_num, real_data)\n    ax1.axvline(30-march_day, linewidth=2, ls = ':', color='grey', alpha=0.5)\n    ax1.legend(['Predicted cases', 'Actual cases', 'Train-test split'], loc='upper left')\n    ax1.set_xlabel(\"Day count (starting on March \" + str(march_day) + \"))\")\n    ax1.set_ylabel(\"Confirmed Cases\")\n\n    ax2.plot(dates_list_num, predicted_data)\n    ax2.plot(dates_list_num, np.log1p(real_data))\n    ax2.axvline(30-march_day, linewidth=2, ls = ':', color='grey', alpha=0.5)\n    ax2.legend(['Predicted cases', 'Actual cases', 'Train-test split'], loc='upper left')\n    ax2.set_xlabel(\"Day count (starting on March \" + str(march_day) + \")\")\n    ax2.set_ylabel(\"Log Confirmed Cases\")\n\n    plt.suptitle((\"ConfirmedCases predictions based on Log-Lineal Regression for \"+country_name))\n    \n    \ndef plot_real_vs_prediction_country_fatalities(data, train, country_name, day_start, dates_list, march_day):\n\n    # Select predictions from March 1st to March 25th\n    predicted_data = data.loc[(data['Day_num'].isin(list(range(day_start, day_start+len(dates_list)))))].Fatalities\n    real_data = train.loc[(train['Country_Region']==country_name) & (train['Date'].isin(dates_list))]['Fatalities']\n    dates_list_num = list(range(0,len(dates_list)))\n\n    # Plot results\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,6))\n\n    ax1.plot(dates_list_num, np.expm1(predicted_data))\n    ax1.plot(dates_list_num, real_data)\n    ax1.axvline(30-march_day, linewidth=2, ls = ':', color='grey', alpha=0.5)\n    ax1.legend(['Predicted cases', 'Actual cases', 'Train-test split'], loc='upper left')\n    ax1.set_xlabel(\"Day count (starting on March \" + str(march_day) + \")\")\n    ax1.set_ylabel(\"Fatalities Cases\")\n\n    ax2.plot(dates_list_num, predicted_data)\n    ax2.plot(dates_list_num, np.log1p(real_data))\n    ax2.axvline(30-march_day, linewidth=2, ls = ':', color='grey', alpha=0.5)\n    ax2.legend(['Predicted cases', 'Actual cases', 'Train-test split'], loc='upper left')\n    ax2.set_xlabel(\"Day count (starting on March \" + str(march_day) + \")\")\n    ax2.set_ylabel(\"Log Fatalities Cases\")\n\n    plt.suptitle((\"Fatalities predictions based on Log-Lineal Regression for \"+country_name))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **Spain**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function to compute the Linear Regression predictions with lags, for a certain Country/Region\ndef lin_reg_with_lags_country(all_data, country_name, day_start, lag_size, country_dict, train_lim, test_lim):\n    \n    ts = time.time()\n    \n    # Filter country and features from all_data (dataset without data leaking)\n    data = all_data.copy()\n    features = ['Id', 'Province_State', 'Country_Region',\n           'ConfirmedCases', 'Fatalities', 'ForecastId', 'Day_num']\n    data = data[features]\n\n    # Select country an data start (all days)\n    data = data[data['Country_Region']==country_dict[country_name]]\n    data = data.loc[data['Day_num']>=day_start]\n\n    # Lags\n    data = calculate_lag(data, range(1,lag_size), 'ConfirmedCases')\n    data = calculate_lag(data, range(1,8), 'Fatalities')\n\n    filter_col_confirmed = [col for col in data if col.startswith('Confirmed')]\n    filter_col_fatalities= [col for col in data if col.startswith('Fataliti')]\n    filter_col = np.append(filter_col_confirmed, filter_col_fatalities)\n    \n    # Apply log transformation\n    data[filter_col] = data[filter_col].apply(lambda x: np.log1p(x))\n    data.replace([np.inf, -np.inf], 0, inplace=True)\n    data.fillna(0, inplace=True)\n\n\n    # Start/end of forecast\n    start_fcst = all_data[all_data['Id']==-1].Day_num.min()\n    end_fcst = all_data[all_data['Id']==-1].Day_num.max()\n\n    for d in list(range(start_fcst, end_fcst+1)):\n        X_train, Y_train_1, Y_train_2, X_test = split_data_one_day(data, d, train_lim, test_lim)\n        model_1, pred_1 = lin_reg(X_train, Y_train_1, X_test)\n        data.loc[(data['Country_Region']==country_dict[country_name]) \n                 & (data['Day_num']==d), 'ConfirmedCases'] = pred_1[0]\n        model_2, pred_2 = lin_reg(X_train, Y_train_2, X_test)\n        data.loc[(data['Country_Region']==country_dict[country_name]) \n                 & (data['Day_num']==d), 'Fatalities'] = pred_2[0]\n\n        # Recompute lags \n        data = calculate_lag(data, range(1,lag_size), 'ConfirmedCases')\n        data = calculate_lag(data, range(1,8), 'Fatalities')\n        data.replace([np.inf, -np.inf], 0, inplace=True)\n        data.fillna(0, inplace=True)\n\n    #print(\"Process for \", country_name, \"finished in \", round(time.time() - ts, 2), \" seconds\")\n    \n    return data\n\n\n# Function to compute the Linear Regression predictions with lags, for a certain Country/Region and State/province\ndef lin_reg_with_lags_country_province(all_data, country_name, province_name, day_start, lag_size, country_dict, train_lim, test_lim):\n    \n    ts = time.time()\n    \n    # Filter country and features from all_data (dataset without data leaking)\n    data = all_data.copy()\n    features = ['Id', 'Province_State', 'Country_Region',\n           'ConfirmedCases', 'Fatalities', 'ForecastId', 'Day_num']\n    data = data[features]\n\n    # Select country an data start (all days)\n    data = data[(data['Country_Region']==country_dict[country_name]) & (data['Province_State']==province_dict[province_name])]\n    data = data.loc[data['Day_num']>=day_start]\n\n    # Lags\n    data = calculate_lag(data, range(1,lag_size), 'ConfirmedCases')\n    data = calculate_lag(data, range(1,lag_size), 'Fatalities')\n\n    # Apply log transformation\n    filter_col_confirmed = [col for col in data if col.startswith('Confirmed')]\n    filter_col_fatalities= [col for col in data if col.startswith('Fataliti')]\n    filter_col = np.append(filter_col_confirmed, filter_col_fatalities)\n    data[filter_col] = data[filter_col].apply(lambda x: np.log1p(x))\n    data.replace([np.inf, -np.inf], 0, inplace=True)\n    data.fillna(0, inplace=True)\n\n    # Start/end of forecast\n    start_fcst = all_data[all_data['Id']==-1].Day_num.min()\n    end_fcst = all_data[all_data['Id']==-1].Day_num.max()\n\n    for d in list(range(start_fcst, end_fcst+1)):\n        X_train, Y_train_1, Y_train_2, X_test = split_data_one_day(data, d, train_lim, test_lim)\n        model_1, pred_1 = lin_reg(X_train, Y_train_1, X_test)\n        data.loc[(data['Country_Region']==country_dict[country_name]) & (data['Province_State']==province_dict[province_name]) \n                 & (data['Day_num']==d), 'ConfirmedCases'] = pred_1[0]\n        model_2, pred_2 = lin_reg(X_train, Y_train_2, X_test)\n        data.loc[(data['Country_Region']==country_dict[country_name]) & (data['Province_State']==province_dict[province_name])\n                 & (data['Day_num']==d), 'Fatalities'] = pred_2[0]\n\n        # Recompute lags \n        data = calculate_lag(data, range(1,lag_size), 'ConfirmedCases')\n        data = calculate_lag(data, range(1,lag_size), 'Fatalities')\n        data.replace([np.inf, -np.inf], 0, inplace=True)\n        data.fillna(0, inplace=True)\n\n    #print(\"Process for \", country_name, \"/\", province_name, \"finished in \", round(time.time() - ts, 2), \" seconds\")\n    \n    return data\n\n\n\n# Run the model for Spain\ncountry_name = 'Spain'\nmarch_day = 10\nday_start = 39 + march_day\ndates_list2 = dates_list[march_day:]\nlag_size = 30\n\ndata_c = lin_reg_with_lags_country(all_data, country_name, day_start, lag_size, country_dict, train_lim, test_lim)\nplot_real_vs_prediction_country(data_c, train, country_name, day_start, dates_list2, march_day)\nplot_real_vs_prediction_country_fatalities(data_c, train, country_name, day_start, dates_list2, march_day)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **Italy**"},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = time.time()\n\n# Inputs\ncountry_name = \"Italy\"\nmarch_day = 10\nday_start = 39 + march_day\ndates_list2 = dates_list[march_day:]\nlag_size = 30\n\ndata_c = lin_reg_with_lags_country(all_data, country_name, day_start, lag_size, country_dict, train_lim, test_lim)\nplot_real_vs_prediction_country(data_c, train, country_name, day_start, dates_list2, march_day)\nplot_real_vs_prediction_country_fatalities(data_c, train, country_name, day_start, dates_list2, march_day)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **Germany**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Inputs\ncountry_name = \"Germany\"\nmarch_day = 10\nday_start = 39 + march_day\ndates_list2 = dates_list[march_day:]\nlag_size = 30\n\ndata_c = lin_reg_with_lags_country(all_data, country_name, day_start, lag_size, country_dict, train_lim, test_lim)\nplot_real_vs_prediction_country(data_c, train, country_name, day_start, dates_list2, march_day)\nplot_real_vs_prediction_country_fatalities(data_c, train, country_name, day_start, dates_list2, march_day)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **India**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Inputs\ncountry_name = \"India\"\nmarch_day = 10\nday_start = 39 + march_day\ndates_list2 = dates_list[march_day:]\nlag_size = 7\n\ndata_c = lin_reg_with_lags_country(all_data, country_name, day_start, lag_size, country_dict, train_lim, test_lim)\nplot_real_vs_prediction_country(data_c, train, country_name, day_start, dates_list2, march_day)\nplot_real_vs_prediction_country_fatalities(data_c, train, country_name, day_start, dates_list2, march_day)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **Andorra**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Inputs\ncountry_name = \"Andorra\"\nmarch_day = 10\nday_start = 39 + march_day\ndates_list2 = dates_list[march_day:]\nlag_size = 1\n\ndata_c = lin_reg_with_lags_country(all_data, country_name, day_start, lag_size, country_dict, train_lim, test_lim)\nplot_real_vs_prediction_country(data_c, train, country_name, day_start, dates_list2, march_day)\nplot_real_vs_prediction_country_fatalities(data_c, train, country_name, day_start, dates_list2, march_day)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observations**:\n* **Parameters**. Two full weeks of training used (from February 26th to March 11th), with their previous 30 lags\n* **Enough data**. (Spain, Italy, Germany). For countries with several ConfirmedCases!=0 in the train dataset (prior to March 11th), predictions are very precise and similar to actual confirmed data\n* **Poor data**. (Algeria, Andorra). Countries with a small number of datapoints in the train dataset show a potentially disastrous prediction. Given the small number of cases, the log transformation followed by a Linear Regression is not able to capture the future behavior\n* **No data**. When the number of confirmed cases in the train dataset is 0 or negligible, the model predicts always no infections"},{"metadata":{},"cell_type":"markdown","source":"Let's generalize the model with lags for training each country day by day:"},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\n# Inputs\nday_start = 39 \nlag_size = 30\n\ntrain3 = train.copy()\ntrain3.Province_State.fillna(\"None\", inplace=True)\n\nresults_df = pd.DataFrame()\n\ntp = time.time()\n\n# Main loop for countries\nfor country_name in train3['Country_Region'].unique():\n\n    # List of provinces\n    provinces_list = train3[train3['Country_Region']==country_name]['Province_State'].unique()\n        \n    # If the country has several Province/State informed\n    if len(provinces_list)>1:\n        for province_name in provinces_list:\n            pred_province = lin_reg_with_lags_country_province(all_data, country_name, province_name, day_start, lag_size, country_dict, train_lim, test_lim)\n            results_df = pd.concat([results_df, pred_province])\n\n    else:\n        pred_country = lin_reg_with_lags_country(all_data, country_name, day_start, lag_size, country_dict, train_lim, test_lim)\n        results_df = pd.concat([results_df, pred_country])\n        \nresults_df_submit = results_df.copy()\nresults_df_submit['ConfirmedCases'] = results_df_submit['ConfirmedCases'].apply(lambda x: np.expm1(x))\nresults_df_submit['Fatalities'] = results_df_submit['Fatalities'].apply(lambda x: np.expm1(x))\n        \n#get_submission(results_df_submit.loc[results_df_submit['ForecastId']!=-1], 'ConfirmedCases', 'Fatalities')\nprint(\"Complete process finished in \", time.time()-tp)\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Nice, extending the model for all countries and days has been quite easy, but a tricky part needs to be addressed. As we saw when analyzing the results for certain countries, some of them have too few training datapoints different from 0, and these scenarios sometimes end up with the regression algorithm predicting absurd values.  \n\nFor the sake of simplicity, my proposal to overcome this problem consists on mixing the current results with those from [section 4.2.](#section42), where we trained the model for all countries without lags. All countries with too few confirmed cases in the training dataset will be predicted with results from section 4.2."},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nresults_df_2 = results_df.copy()\n\nday_start = 39\ndata_pred2 = linreg_basic_all_countries(data, day_start, train_lim, test_lim)\nday_num_test = 57    # Day 2020-04-18\n\n\n# Main loop for countries\nfor country_name in train3['Country_Region'].unique():\n\n    # List of provinces\n    provinces_list = train3[train3['Country_Region']==country_name]['Province_State'].unique()\n\n    # Countries with several Province_State informed\n    if len(provinces_list)>1:\n        for province_name in provinces_list:\n        \n            tmp_index = all_data.index[(all_data['Country_Region']==country_dict[country_name]) & \n                           (all_data['Province_State']==province_dict[province_name]) & \n                           (all_data['Day_num']<day_num_test) & \n                           (all_data['ConfirmedCases']!=0)]\n\n            # When there is not enough data\n            if len(tmp_index) < 30:\n                \n                # ConfirmedCases\n                results_df_2.loc[((results_df_2['Country_Region']==country_dict[country_name]) & \n                                  (results_df_2['Province_State']==province_dict[province_name]) &\n                                  (results_df_2['Day_num']>=day_num_test)), 'ConfirmedCases'] = data_pred2.loc[((data_pred2['Country_Region']==country_dict[country_name]) & \n                                  (data_pred2['Province_State']==province_dict[province_name]) & \n                                  (data_pred2['Day_num']>=day_num_test)), 'Predicted_ConfirmedCases'].apply(lambda x: np.log1p(x))\n                \n                #Fatalities\n                results_df_2.loc[((results_df_2['Country_Region']==country_dict[country_name]) & \n                                  (results_df_2['Province_State']==province_dict[province_name]) &\n                                  (results_df_2['Day_num']>=day_num_test)), 'Fatalities'] = data_pred2.loc[((data_pred2['Country_Region']==country_dict[country_name]) & \n                                  (data_pred2['Province_State']==province_dict[province_name]) & \n                                  (data_pred2['Day_num']>=day_num_test)), 'Predicted_Fatalities'].apply(lambda x: np.log1p(x))\n                \n    # Countries without Province_State\n    else:\n        tmp_index = all_data.index[(all_data['Country_Region']==country_dict[country_name]) & \n                           (all_data['Day_num']<day_num_test) & \n                           (all_data['ConfirmedCases']!=0)]\n\n        # When there is not enough data\n        if len(tmp_index) < 30:\n            \n            #Confirmed Cases\n            results_df_2.loc[((results_df_2['Country_Region']==country_dict[country_name]) & \n                            (results_df_2['Day_num']>=day_num_test)), 'ConfirmedCases'] = data_pred2.loc[((data_pred2['Country_Region']==country_dict[country_name]) & \n                            (data_pred2['Day_num']>=day_num_test)), 'Predicted_ConfirmedCases'].apply(lambda x: np.log1p(x))\n            \n            results_df_2.loc[((results_df_2['Country_Region']==country_dict[country_name]) & \n                            (results_df_2['Day_num']>=day_num_test)), 'Fatalities'] = data_pred2.loc[((data_pred2['Country_Region']==country_dict[country_name]) & \n                            (data_pred2['Day_num']>=day_num_test)), 'Predicted_Fatalities'].apply(lambda x: np.log1p(x))\n            \nresults_df_2 = results_df_2.loc[results_df_2['Day_num']>=day_num_test]\n#results_df_2[['ConfirmedCases', 'Fatalities']] = results_df_2[['ConfirmedCases', 'Fatalities']].apply(lambda x: np.expm1(x))\n#get_submission(results_df_2, 'ConfirmedCases', 'Fatalities')\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Predictions for the late stages of the transmission (under construction)\n\nAs the transmission progresses, **the exponential regime is left behind** and the Linear Regressor we developed begins to predict worse results. We were aware of this limitation, and now **alternative methods are required** in order to capture the new behavior.\n\nI'd like to clarify that the aim of this section will be to predict the immediate future evolution of the number of cases, not to estimate when the peack of infections will happen or to which extent the COVID-19 will spread in each country. \n\nModels considered in this section:\n\n1. Logistic curve fit\n\n2. Logistic curve fit for all countries\n\n3. ARIMA\n"},{"metadata":{},"cell_type":"markdown","source":"Logistic curve fit "},{"metadata":{"trusted":true},"cell_type":"code","source":"def logistic_function(x, a, b, c, d):\n    return a / (1. + np.exp(-c * (x - d))) + b\n\ndef fit_logistic(all_data, country_name, province_name, train_lim, target):\n    data_cp = all_data.loc[(all_data['Country_Region']==country_dict[country_name]) & (all_data['Province_State']==province_dict[province_name])]\n    y = data_cp.loc[(data_cp['Day_num'])<=train_lim, target].astype(np.int32)\n    x = list(range(0, len(y)))\n\n    # Initial guess\n    p0 = [0,1,1,0]\n\n    (a_, b_, c_, d_), cov = optimize.curve_fit(logistic_function, x, y, bounds=(0, [500000., 10., 1000., 1000., ]), p0=p0, maxfev=10**9)\n    y_fit = logistic_function(x, a_, b_, c_, d_)\n    \n    return x, y, y_fit, (a_, b_, c_, d_), cov\n\ndef plot_logistic(x, y, y_fit, country_name, province_name, target):\n    fig, ax = plt.subplots(1, 1, figsize=(6, 4))\n    ax.plot(x, y, 'o')\n    ax.plot(x, y_fit, '-')\n    ax.set_xlabel(\"Day count (starting on January 22nd)\")\n    ax.set_ylabel(target)\n    ax.set_title(\"Fit to logistic regression for \"+ country_name+\"/\"+province_name)\n\n\n# Fit country to logistic curve\ncountry_name = 'Spain'\nprovince_name = 'None'\ntrain_lim = 69\nx, y, y_fit, (a_, b_, c_, d_), cov = fit_logistic(all_data, country_name, province_name, train_lim, 'ConfirmedCases')\nplot_logistic(x, y, y_fit, country_name, province_name, 'ConfirmedCases')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **Italy**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit country to logistic curve\ncountry_name = 'Italy'\nprovince_name = 'None'\nx, y, y_fit, (a_, b_, c_, d_), cov = fit_logistic(all_data, country_name, province_name, train_lim, 'ConfirmedCases')\nplot_logistic(x, y, y_fit, country_name, province_name, 'ConfirmedCases')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **Germany**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit country to logistic curve\ncountry_name = 'Germany'\nprovince_name = 'None'\nx, y, y_fit, (a_, b_, c_, d_), cov = fit_logistic(all_data, country_name, province_name, train_lim, 'ConfirmedCases')\nplot_logistic(x, y, y_fit, country_name, province_name, 'ConfirmedCases')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **India**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit country to logistic curve\ncountry_name = 'India'\nprovince_name = 'None'\nx, y, y_fit, (a_, b_, c_, d_), cov = fit_logistic(all_data, country_name, province_name, train_lim, 'ConfirmedCases')\nplot_logistic(x, y, y_fit, country_name, province_name, 'ConfirmedCases')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **Andorra**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit country to logistic curve\ncountry_name = 'Andorra'\nprovince_name = 'None'\nx, y, y_fit, (a_, b_, c_, d_), cov = fit_logistic(all_data, country_name, province_name, train_lim, 'ConfirmedCases')\nplot_logistic(x, y, y_fit, country_name, province_name, 'ConfirmedCases')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*** China/Hubei**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit country to logistic curve\ncountry_name = 'China'\nprovince_name = 'Hubei'\nx, y, y_fit, (a_, b_, c_, d_), cov = fit_logistic(all_data, country_name, province_name, train_lim, 'ConfirmedCases')\nplot_logistic(x, y, y_fit, country_name, province_name, 'ConfirmedCases')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Logistic curve fit for all countries"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_lim = 69\ntest_lim = 112\n\ndef logistic_forecast_allcountries(all_data, train, train_lim, test_lim):\n    \n    ts = time.time()\n    data_pred = all_data[all_data.ForecastId != -1][['Country_Region', 'Province_State', 'Day_num', 'ForecastId']]\n    data_pred['Predicted_ConfirmedCases'] = [0]*len(data_pred)\n    data_pred['Predicted_Fatalities'] = [0]*len(data_pred)\n\n    # Main loop for countries\n    for country_name in train['Country_Region'].unique():\n        for province_name in train[train['Country_Region']==country_name]['Province_State'].unique():\n            \n            # ConfirmedCases\n            x, y, y_fit, (a_, b_, c_, d_), cov = fit_logistic(all_data, country_name, province_name, train_lim, 'ConfirmedCases')\n            pred_1 = [logistic_function(t, a_, b_, c_, d_) for t in list(range(train_lim+1, test_lim+1))]\n            data_pred.loc[((data_pred['Country_Region']==country_dict[country_name]) & (data_pred['Province_State']==province_dict[province_name])), 'Predicted_ConfirmedCases'] = pred_1\n\n            # Fatalities\n            x, y, y_fit, (a_, b_, c_, d_), cov = fit_logistic(all_data, country_name, province_name, train_lim, 'Fatalities')\n            pred_2 = [logistic_function(t, a_, b_, c_, d_) for t in list(range(train_lim+1, test_lim+1))]\n            data_pred.loc[((data_pred['Country_Region']==country_dict[country_name]) & (data_pred['Province_State']==province_dict[province_name])), 'Predicted_Fatalities'] = pred_2\n\n\n    print(\"Logistic function fit for all countries finished in \", round(time.time() - ts, 2), \" seconds\")\n    return data_pred\n    \nlogistic_forecast = logistic_forecast_allcountries(all_data, train, train_lim, test_lim) \nget_submission(logistic_forecast, 'Predicted_ConfirmedCases', 'Predicted_Fatalities')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"ARIMA\n\nAutoregressive integrated moving average (ARIMA) models are mainly used for time-series analysis, since they are capable to predict the future values of a variable based only on its historical evolution. ARIMA models can be understood by dividing them in three main concepts, each of them with a related parameter:\n* **AR**: the regression of the predicted variable is based on its previous values (lags). The $p$ parameter defines the number of these terms.\n$$\\sum_{i=1}^p \\phi_i y_{t-i}$$\n\n* **I**: data values used for the regression are differenciated until they are stationary. Instead of working with $y_t$, it uses terms of . Parameter $d$ is related to the number of differences need for stationarity.\n$$y'_t \\sim y_t-y_{t-1}$$\n\n* **MA**: the error of the regression is based on a linear combination of error terms from the past (lags of errors). Parameter $q$ is the number of lagged forecast errors in the prediction equation. \n$$\\sum_{i=1}^q \\theta_i \\epsilon_{t-i}$$\n\nIn general, the model can be written as:\n$$ y_{t}' =  c + \\phi_1 y'_{t-1} + ... + \\phi_p y'_{t-p} + \\theta_1 \\epsilon_{t-1} ... + \\theta_q \\epsilon_{t-q} - \\epsilon_t $$\n\nIn order to keep the notebook clean and don't overextend, I won't go into further details of the model. For a rich explanation of ARIMA models you can visit the [Monash University online textbook](https://otexts.com/fpp2/arima.html)."},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install pyramid.arima\nfrom pyramid.arima import auto_arima","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **Spain**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define a general function to run ARIMA regression\ndef arima_cp(all_data, country_name, province_name, target, train_lim, test_lim):\n    \n    #Select data for the Country/Region, splitting train/test days\n    data_cp = all_data.loc[(all_data['Country_Region']==country_dict[country_name]) & (all_data['Province_State']==province_dict[province_name])]\n    data_cp_train = data_cp.loc[(data_cp['Day_num'])<=train_lim, target].astype(np.int32)\n    data_cp_test = data_cp.loc[(data_cp['Day_num'])>test_lim, target].astype(np.int32)\n\n    # Set the range of parameters to use\n    stepwise_model = auto_arima(data_cp[target], start_p=1, start_q=1, max_p=30, max_q=30, \n                                start_P=0, seasonal=False, d=2, trace=False, error_action='ignore', stepwise=True)\n\n    # Train and predict\n    stepwise_model.fit(data_cp_train, start_ar_lags=2*max(30, 30))\n    forecast = stepwise_model.predict(n_periods=test_lim-train_lim)\n    return forecast\n\n\n# Plot the actual values vs predictions\ndef plot_arima_country(all_data, train, forecast, dates_overlap, country_name, province_name, valid_num, target):\n    df_train = train.loc[(train['Country_Region']==country_name) & (train['Province_State']==province_name), target]\n    df_fcst = np.append(df_train[:-valid_num], forecast[:valid_num])\n    dates = list(range(0,len(df_train)))\n    \n    # Plot results\n    fig, (ax1) = plt.subplots(1, 1, figsize=(10,6))\n    ax1.plot(dates, df_fcst)\n    ax1.plot(dates, df_train)\n    ax1.axvline(len(df_train)-valid_num-1, linewidth=2, ls = ':', color='grey', alpha=0.5)\n    ax1.set_title(\"Actual ConfirmedCases vs predictions based on ARIMA for \"+country_name + \"/\"+province_name)\n    ax1.legend(['Predicted cases', 'Actual cases', 'Train-test split'], loc='upper left')\n    ax1.set_xlabel(\"Day count starting on January 22nd\")\n    ax1.set_ylabel(\"ConfirmedCases\")\n    \n    \n    \n# Inputs\ncountry_name = 'Spain'\nprovince_name = 'None'\ntrain_lim = 69\nvalid_lim = 79 # needs to be changed as more days of training data are included\ntest_lim = 112\nvalid_num=valid_lim-train_lim \n\nforecast = arima_cp(all_data, country_name, province_name, 'ConfirmedCases', train_lim, test_lim)\nplot_arima_country(all_data, train, forecast, dates_overlap, country_name, province_name, valid_num, 'ConfirmedCases')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **Italy**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Inputs\ncountry_name = 'Italy'\nprovince_name = 'None'\n\nforecast = arima_cp(all_data, country_name, province_name, 'ConfirmedCases', train_lim, test_lim)\nplot_arima_country(all_data, train, forecast, dates_overlap, country_name, province_name, valid_num, 'ConfirmedCases')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **Germany**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Inputs\ncountry_name = 'Germany'\nprovince_name = 'None'\n\nforecast = arima_cp(all_data, country_name, province_name, 'ConfirmedCases', train_lim, test_lim)\nplot_arima_country(all_data, train, forecast, dates_overlap, country_name, province_name, valid_num, 'ConfirmedCases')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **India**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Inputs\ncountry_name = 'India'\nprovince_name = 'None'\n\nforecast = arima_cp(all_data, country_name, province_name, 'ConfirmedCases', train_lim, test_lim)\nplot_arima_country(all_data, train, forecast, dates_overlap, country_name, province_name, valid_num, 'ConfirmedCases')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **Andorra**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Inputs\ncountry_name = 'Andorra'\nprovince_name = 'None'\n\nforecast = arima_cp(all_data, country_name, province_name, 'ConfirmedCases', train_lim, test_lim)\nplot_arima_country(all_data, train, forecast, dates_overlap, country_name, province_name, valid_num, 'ConfirmedCases')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **China/Hubei**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Inputs\ncountry_name = 'China'\nprovince_name = 'Hubei'\n\nforecast = arima_cp(all_data, country_name, province_name, 'ConfirmedCases', train_lim, test_lim)\nplot_arima_country(all_data, train, forecast, dates_overlap, country_name, province_name, valid_num, 'ConfirmedCases')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\ntrain_lim = 69\ntest_lim = 112\n\nARIMA_forecast = all_data[all_data.ForecastId != -1][['Country_Region', 'Province_State', 'Day_num', 'ForecastId']]\nARIMA_forecast['Predicted_ConfirmedCases'] = [0]*len(ARIMA_forecast)\nARIMA_forecast['Predicted_Fatalities'] = [0]*len(ARIMA_forecast)\n\n# Main loop for countries\nfor c in train['Country_Region'].unique():\n        for p in train[train['Country_Region']==c]['Province_State'].unique():\n            print(c, p)\n            pred_1 = arima_cp(all_data, c, p, 'ConfirmedCases', train_lim, test_lim)\n            #pred_2 = arima_cp(all_data, c, p, 'Fatalities', train_lim, test_lim)\n            ARIMA_forecast.loc[((ARIMA_forecast['Country_Region']==country_dict[c]) & (ARIMA_forecast['Province_State']==province_dict[p])), 'Predicted_ConfirmedCases'] = pred_1\n            #ARIMA_forecast.loc[((ARIMA_forecast['Country_Region']==country_dict[c]) & (ARIMA_forecast['Province_State']==province_dict[p])), 'Predicted_Fatalities'] = pred_2\n            \n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\" WEIRD OVERESTIMATED CASE\n# Fit country to logistic curve\ncountry_name = 'South Africa'\nprovince_name = 'None'\ntarget = 'ConfirmedCases'\n#target='Fatalities'\nx, y, y_fit, (a_, b_, c_, d_), cov = fit_logistic(all_data, country_name, province_name, train_lim, target)\nplot_logistic(x, y, y_fit, country_name, province_name, target)\n\n#x, y, y_fit, (a_, b_, c_, d_), cov = fit_logistic(all_data, country_name, province_name, train_lim, target)\npred_1 = [logistic_function(t, a_, b_, c_, d_) for t in list(range(train_lim+1, test_lim+1))]\n\npred_1\n\nlogistic_forecast[(logistic_forecast['Country_Region']==66) & (logistic_forecast['Province_State']==81)]\n\"\"\"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}