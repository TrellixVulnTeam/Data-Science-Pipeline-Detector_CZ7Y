{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Imports"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Imports\nimport os\nfrom abc import ABCMeta, abstractmethod\nfrom typing import Dict, List, Tuple, Union\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.metrics import mean_squared_log_error\nfrom scipy.optimize.minpack import curve_fit\nfrom scipy.optimize import curve_fit, OptimizeWarning\nfrom scipy.optimize import least_squares\nfrom xgboost import XGBRegressor","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Helpers\n\nFirst the Root Mean Square Log Error cost function:\n\n$$\\mathrm{RMSLE}=\\sqrt{\\left<\\log(y)-\\log(\\hat{y})\\right>}$$\n\nwhere, $y$ is actual value and $\\hat(y)$ is predicted. Luckily the MSLE is implemented in [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_log_error.html#sklearn.metrics.mean_squared_log_error) so all that is needed is a square root. However, as the predictor may be silly and give values just below 0, we force positive values or 0."},{"metadata":{"trusted":true},"cell_type":"code","source":"def RMSLE(actual: np.ndarray, prediction: np.ndarray) -> float:\n    \"\"\"Calculate Root Mean Square Log Error between actual and predicted values\"\"\"\n    return np.sqrt(mean_squared_log_error(actual, np.maximum(0, prediction)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Prepare data set"},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_kaggle_csv(dataset: str, datadir: str) -> pd.DataFrame:\n    \"\"\"Load andt preprocess kaggle covid-19 csv dataset.\"\"\"\n    df = pd.read_csv(\n        f\"{os.path.join(datadir,dataset)}.csv\", parse_dates=[\"Date\"]\n    )\n    df['country'] = df[\"Country_Region\"]\n    if \"Province_State\" in df:\n        df[\"Country_Region\"] = np.where(\n            df[\"Province_State\"].isnull(),\n            df[\"Country_Region\"],\n            df[\"Country_Region\"] + \"_\" + df[\"Province_State\"],\n        )\n        df.drop(columns=\"Province_State\", inplace=True)\n    if \"ConfirmedCases\" in df:\n        df[\"ConfirmedCases\"] = df.groupby(\"Country_Region\")[\n            \"ConfirmedCases\"\n        ].cummax()\n    if \"Fatalities\" in df:\n        df[\"Fatalities\"] = df.groupby(\"Country_Region\")[\"Fatalities\"].cummax()\n    if \"DayOfYear\" not in df:\n        df[\"DayOfYear\"] = df[\"Date\"].dt.dayofyear\n    df[\"Date\"] = df[\"Date\"].dt.date\n    return df\n\ndef dateparse(x): \n    try:\n        return pd.datetime.strptime(x, '%Y-%m-%d')\n    except:\n        return pd.NaT\n\ndef prepare_lat_long(df):\n    df[\"Country_Region\"] = np.where(\n            df[\"Province/State\"].isnull(),\n            df[\"Country/Region\"],\n            df[\"Country/Region\"] + \"_\" + df[\"Province/State\"],\n        )\n    return df[['Country_Region', 'Lat', 'Long']].drop_duplicates()\n\ndef get_extra_features(df): \n    df['school_closure_status_daily'] = np.where(df['school_closure'] < df['Date'], 1, 0)\n    df['school_closure_first_fatality'] = np.where(df['school_closure'] < df['first_1Fatalities'], 1, 0)\n    df['school_closure_first_10cases'] = np.where( df['school_closure'] < df['first_10ConfirmedCases'], 1, 0)\n    #\n    df['case_delta1_10'] = (df['first_10ConfirmedCases'] - df['first_1ConfirmedCases']).dt.days\n    df['case_death_delta1'] = (df['first_1Fatalities'] - df['first_1ConfirmedCases']).dt.days\n    df['case_delta1_100'] = (df['first_100ConfirmedCases'] - df['first_1ConfirmedCases']).dt.days\n    df['days_since'] = df['DayOfYear']-df['case1_DayOfYear']\n    df['weekday'] = pd.to_datetime(df['Date']).dt.weekday\n    col = df.isnull().mean()\n    rm_null_col = col[col > 0.2].index.tolist()\n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* ## Load train data\n\nWe use week 1 [train data](https://www.kaggle.com/c/covid19-global-forecasting-week-1/data) to get lat/long of geographic locations, then we use our [country health indicators](https://www.kaggle.com/nxpnsv/country-health-indicators) dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"### Train data\n\n# Take lat/long from week 1 data set\ndf_lat = prepare_lat_long(pd.read_csv(\"/kaggle/input/w1train/w1train.csv\"))\n\n# Get current train data\ntrain = load_kaggle_csv(\"train\", \"/kaggle/input/covid19-global-forecasting-week-4\")\n\n# Insert augmentations\n\ncountry_health_indicators = (\n    (pd.read_csv(\"/kaggle/input/country-health-indicators/country_health_indicators_v3.csv\", \n        parse_dates=['first_1ConfirmedCases', 'first_10ConfirmedCases', \n                     'first_50ConfirmedCases', 'first_100ConfirmedCases',\n                     'first_1Fatalities', 'school_closure'], date_parser=dateparse)).rename(\n        columns ={'Country_Region':'country'}))\n# Merge augmentation to kaggle input\ntrain = (pd.merge(train, country_health_indicators,\n                  on=\"country\",\n                  how=\"left\")).merge(df_lat, on='Country_Region', how='left')\ntrain = get_extra_features(train)\n\n# train=train.fillna(0)\ntrain.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### TEST DATA\ntest = load_kaggle_csv(\"test\", \"/kaggle/input/covid19-global-forecasting-week-4\")\ntest = (pd.merge(\n    test, country_health_indicators, on=\"country\", how=\"left\")).merge(\n    df_lat, on ='Country_Region', how='left')\ntest = get_extra_features(test)\ndel country_health_indicators","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Curve fitting\nFirst we add a helper class to fit functions. In previous weeks we went with a [Logistic](https://en.wikipedia.org/wiki/Logistic_function) function, but this is increasingly insuffucient. Instead we tried with a [generalized logistic function](https://en.wikipedia.org/wiki/Generalised_logistic_function) (GLF). However, actions like social distances dramatically change rates, so we create a new model DiXGLF which is a linear interpolation between 2 GLF, where the interpolation term $\\alpha$ is a logistic function."},{"metadata":{"trusted":true},"cell_type":"code","source":"class Fitter(metaclass=ABCMeta):\n    \"\"\"\n    Helper class for 1D fits using scipy fit.\n\n    This version assumes y-data is positive and increasing.\n    \"\"\"\n\n    def __init__(self, name):\n        \"\"\"Make fitter instance.\"\"\"\n        self.kwargs = {\n            \"method\": \"trf\",\n            \"max_nfev\": 20000,\n            \"x_scale\": \"jac\",\n            \"loss\": \"linear\",\n            \"jac\": self.jacobian,\n        }\n        self.name = name\n        self.rmsle = None\n        self.fit_params = None\n        self.fit_cov = None\n        self.y_hat = None\n        self.p0 = None\n        self.bounds = None\n\n    @abstractmethod\n    def function(self, x: np.ndarray, *args) -> np.ndarray:\n        \"\"\"Mathematical function to fit.\"\"\"\n        pass\n\n    @abstractmethod\n    def jacobian(self, x: np.ndarray, *args) -> np.ndarray:\n        \"\"\"Jacobian of funciton.\"\"\"\n        pass\n\n    @abstractmethod\n    def guess(self) -> Tuple[List[float], List]:\n        \"\"\"First guess for fit optimium.\"\"\"\n        pass\n\n    def fit(self, x: np.ndarray, y: np.ndarray, **kwargs) -> Union[None, Tuple]:\n        \"\"\"Fit function to y over x.\"\"\"\n        # Update extra keywords for fit\n        kwargs.update(self.kwargs)\n            \n        # Reset fit results\n        self.rmsle = None\n        self.fit_params = None\n        self.fit_cov = None\n        self.y_hat = None\n        self.p0 = None\n        self.bounds = None\n        if len(x) <= 3:\n            return\n\n        # Guess params\n        self.p0, self.bounds = self.guess(x, y)\n\n        # Perform fit\n        try:\n            res = curve_fit(\n                f=self.function,\n                xdata=np.array(x, dtype=np.float128),\n                ydata=np.array(y, dtype=np.float128),\n                p0=self.p0,\n                bounds=self.bounds,\n                sigma=np.maximum(1, np.sqrt(y)),\n                **kwargs,\n            )\n        except (ValueError, RuntimeError, OptimizeWarning) as e:\n            print(e)\n            return\n\n        # Update fit results\n        self.y_hat = self.function(x, *res[0])\n        self.rmsle = np.sqrt(mean_squared_log_error(y, self.y_hat))\n        self.fit_params = res[0]\n        self.fit_cov = res[1]\n\n    def plot_fit(self, x, y, ax=None, title=None, **kwargs):\n        \"\"\"Fit and plot.\"\"\"\n        self.fit(x, y, **kwargs)\n\n        if self.fit_params is None:\n            print(\"No result, cannot plot\")\n            return\n\n        if ax is None:\n            _, ax = plt.subplots()\n        ax.set_title(f\"{title or ''} {self.name}: rmsle={self.rmsle:.2f}\")\n        color = \"g\"\n        ax.plot(x, y, \"o\", color=color, alpha=0.9)\n        ax.plot(x, self.y_hat, \"-\", color=\"r\")\n        ax.set_ylabel(\"Counts\", color=color)\n        ax.set_xlabel(\"Day of Year\")\n        ax.tick_params(axis=\"y\", labelcolor=color)\n        ax2 = ax.twinx()\n        color = \"b\"\n        ax2.set_ylabel(\"Residual\", color=color)\n        ax2.plot(x, y - self.y_hat, \".\", color=color)\n        ax2.tick_params(axis=\"y\", labelcolor=color)\n        ax.text(\n            0.05,\n            0.95,\n            \"\\n\".join(\n                [f\"$p_{i}$={x:0.2f}\" for i, x in enumerate(self.fit_params)]\n            ),\n            horizontalalignment=\"left\",\n            verticalalignment=\"top\",\n            transform=ax.transAxes,\n        )\n\n        \nclass Logistic(Fitter):\n    def __init__(self):\n        super().__init__(name=\"Logistic\")\n\n    def function(\n        self, x: np.ndarray, K: float, B: float, M: float\n    ) -> np.ndarray:\n        return K / (1 + np.exp(-B * (x - M)))\n\n    def jacobian(\n        self, x: np.ndarray, K: float, B: float, M: float\n    ) -> np.ndarray:\n        dK = 1 / (1 + np.exp(-B * (x - M)))\n        dB = (\n            K\n            * (x - M)\n            * np.exp(-B * (x - M))\n            / np.square(1 + np.exp(-B * (x - M)))\n        )\n        dM = K * B * np.exp(-B * (x - M)) / np.square(1 + np.exp(-B * (x - M)))\n        return np.transpose([dK, dB, dM])\n\n    def guess(\n        self, x: np.ndarray, y: np.ndarray\n    ) -> Tuple[List[float], List[float]]:\n        K = y[-1]\n        B = 0.1\n        M = x[np.argmax(y >= 0.5 * K)]\n        p0 = [K, B, M]\n        bounds = [[y[-1], 1e-4, x[0]], [y[-1] * 8, 0.5, (1+x[-1]) * 2]]\n        return p0, bounds\n\nclass GLF(Fitter):\n    def __init__(self):\n        super().__init__(name=\"GLF\")\n\n    def function(self, x, K, B, M, nu):\n        return K / np.power((1 + np.exp(-B * (x - M))), 1 / nu)\n\n    def jacobian(self, x, K, B, M, nu):\n        nu1 = 1.0 / nu\n        xM = x - M\n        exp_BxM = np.exp(-B * xM)\n        pow0 = np.power(1 + exp_BxM, -nu1)\n        pow1 = K * exp_BxM / (nu * np.power(1 + exp_BxM, nu1 + 1))\n\n        dK = pow0\n        dB = xM * pow1\n        dnu = K * np.log1p(exp_BxM) * pow0 / nu\n        dM = B * pow1\n        return np.transpose([dK, dB, dnu, dM])\n\n    def guess(self, x, y):\n        # Guess params and param bounds\n        K = y[-1]\n        B = 0.1\n        M = x[np.argmax(y >= 0.5 * K)]\n        nu = 0.5\n        p0 = [K, B, M, nu]\n        bounds = [[y[-1], 1e-3, x[0], 1e-2], [(y[-1]+1) * 10, 0.5, (x[-1]+1) * 2, 1.0]]\n        return p0, bounds\n    \nclass DiXGLF(Fitter):\n    \"\"\"Interpolation between 2 logistic function.\n\n    First guess is split by y_max/2 so the first and second logistic\n    start on different partitions of data.\n    \n    Uses 3-point estimator in place of explicit jacobian because of numeric stability.\n    \"\"\"\n\n    def __init__(self):\n        super().__init__(name=\"DiXGLF\")\n        self.glf = GLF()\n        self.logistic = Logistic()\n        self.kwargs.update({\"jac\": \"3-point\"})\n\n    def function(self, x, B0, M0, K1, B1, M1, nu1, K2, B2, M2, nu2):\n        alpha = self.logistic.function(x, 1, B0, M0)\n        return alpha * self.glf.function(x, K1, B1, M1, nu1) + (\n            1 - alpha\n        ) * self.glf.function(x, K2, B2, M2, nu2)\n\n    def jacobian(self, x, B0, M0, K1, B1, M1, nu1, K2, B2, M2, nu2):\n        raise RuntimeError(\"%s jacobian not implemented\", self.name)\n\n    def guess(self, x, y):\n        split = min(max(1, np.argmax(y >= 0.5 * y[-1])), len(x)-2)\n        p01, bounds1 = self.glf.guess(x[:split], y[:split])\n        p02, bounds2 = self.glf.guess(x[split:], y[split:])\n        p0, bounds = self.logistic.guess(x, y)\n        p0 = p0[1:]\n        bounds = [bounds[0][1:], bounds[1][1:]]\n        p0.extend(p01)\n        p0.extend(p02)\n        bounds[0].extend(bounds1[0])\n        bounds[0].extend(bounds2[0])\n        bounds[1].extend(bounds1[1])\n        bounds[1].extend(bounds2[1])\n        return p0, bounds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def apply_fitter(\n    df: pd.DataFrame,\n    fitter: Fitter,\n    x_col: str = \"DayOfYear\",\n    y_cols: List[str] = [\"ConfirmedCases\", \"Fatalities\"],\n) -> pd.DataFrame:\n    \"\"\"Helper to apply fitter to dataframe groups\"\"\"\n    x = df[x_col].astype(np.float128).to_numpy()\n    result = {}\n    for y_col in y_cols:\n        y = df[y_col].astype(np.float128).to_numpy()\n        fitter.fit(x, y)\n        if fitter.rmsle is None:\n            continue\n        result[f\"{y_col}_rmsle\"] = fitter.rmsle\n        df[f\"y_hat_fitter_{y_col}\"] = fitter.y_hat\n        result.update({f\"{y_col}_p_{i}\": p for i, p in enumerate(fitter.fit_params)})\n    return pd.DataFrame([result])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.style.use(\"seaborn-white\")\nsns.set_color_codes()\ndixglf = DiXGLF()\ntrain[\"y_hat_fitter_ConfirmedCases\"]=0\ntrain[\"y_hat_fitter_Fatalities\"]=0\nfig, ax = plt.subplots(2, 4, figsize=(16,8))\nax = ax.flatten()\nfor i, country in enumerate((\"Italy\", \"Austria\", \"Korea, South\", \"Germany\")):\n    c = train[train[\"Country_Region\"] == country]\n    x = c[\"DayOfYear\"].astype(np.float128).to_numpy()\n    dixglf.plot_fit(x, c[\"ConfirmedCases\"].astype(np.float128).to_numpy(), ax=ax[i], title=f\"Cases {country}\")\n    dixglf.plot_fit(x, c[\"Fatalities\"].astype(np.float128).to_numpy(), ax=ax[i+4], title=f\"Deaths {country}\")\nfig.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.merge(\n    train, train.groupby(\n    [\"Country_Region\"], observed=True, sort=False\n).apply(lambda x: apply_fitter(x, fitter=dixglf)).reset_index(), \n    on=[\"Country_Region\"], how=\"left\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"y_hat_fitter_ConfirmedCases\"]=dixglf.function(\n    train[\"DayOfYear\"],\n    train[\"ConfirmedCases_p_0\"],\n    train[\"ConfirmedCases_p_1\"],\n    train[\"ConfirmedCases_p_2\"],\n    train[\"ConfirmedCases_p_3\"],\n    train[\"ConfirmedCases_p_4\"],\n    train[\"ConfirmedCases_p_5\"],\n    train[\"ConfirmedCases_p_6\"],\n    train[\"ConfirmedCases_p_7\"],\n    train[\"ConfirmedCases_p_8\"],\n    train[\"ConfirmedCases_p_9\"])\ntrain[\"y_hat_fitter_Fatalities\"]=dixglf.function(\n    train[\"DayOfYear\"],\n    train[\"Fatalities_p_0\"],\n    train[\"Fatalities_p_1\"],\n    train[\"Fatalities_p_2\"],\n    train[\"Fatalities_p_3\"],\n    train[\"Fatalities_p_4\"],\n    train[\"Fatalities_p_5\"],\n    train[\"Fatalities_p_6\"],\n    train[\"Fatalities_p_7\"],\n    train[\"Fatalities_p_8\"],\n    train[\"Fatalities_p_9\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# XGB boost regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"def apply_xgb_model(train, x_columns, y_column, xgb_params):\n    X = train[x_columns].astype(np.float32).fillna(0).to_numpy()\n    y = train[y_column].astype(np.float32).fillna(0).to_numpy()\n    xgb_fit = XGBRegressor(**xgb_params).fit(X, y)\n    y_hat = xgb_fit.predict(X)\n    train[f\"yhat_xgb_{y_column}\"] = y_hat\n    return RMSLE(y, y_hat), xgb_fit","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_params_c = dict(\n    gamma=0.1,\n    learning_rate=0.35,\n    n_estimators=221,\n    max_depth=15,\n    min_child_weight=1,\n    nthread=8,\n    objective=\"reg:squarederror\")\n\nxgb_params_f = dict(\n    gamma=0.1022,\n    learning_rate=0.338,\n    n_estimators=292,\n    max_depth=14,\n    min_child_weight=1,\n    nthread=8,\n    objective=\"reg:squarederror\")\n\nx_columns = ['DayOfYear', \n       'Diabetes, blood, & endocrine diseases (%)', 'Respiratory diseases (%)',\n       'Diarrhea & common infectious diseases (%)',\n       'Nutritional deficiencies (%)',\n       'obesity - adult prevalence rate',\n       'pneumonia-death-rates', 'animal_fats', 'animal_products', 'eggs',\n       'offals', 'treenuts', 'vegetable_oils', 'nbr_surgeons',\n       'nbr_anaesthesiologists', 'population',\n       'school_shutdown_1case',\n       'school_shutdown_10case', 'school_shutdown_50case',\n       'school_shutdown_1death', 'case1_DayOfYear', 'case10_DayOfYear',\n       'case50_DayOfYear',\n       'school_closure_status_daily', 'case_delta1_10',\n       'case_death_delta1', 'case_delta1_100', 'days_since','Lat','Long','weekday',\n        'y_hat_fitter_ConfirmedCases', 'y_hat_fitter_Fatalities'\n]\n\nxgb_c_rmsle, xgb_c_fit = apply_xgb_model(train, x_columns, \"ConfirmedCases\", xgb_params_c)\nxgb_f_rmsle, xgb_f_fit = apply_xgb_model(train, x_columns, \"Fatalities\", xgb_params_f)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Hybrid fit\n\nFrom logistic curve fit we have $\\hat{y}_L$: `y_hat_fitter_ConfirmedCases`,and from XGB boost regression $\\hat{y}_X$: `yhat_xgb_ConfirmedCases`.\nHere we make a hybrid predictor\n\n $\\hat{y}_H = \\alpha \\hat{y}_L + (1-\\alpha) \\hat{y}_X$ \n \n by fitting alpha with `scipy.optmize.least_squares`. Similarly for `Fatalities`. First we define a few functions to do the work:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def interpolate(alpha, x0, x1):\n    return x0 * alpha + x1 * (1 - alpha)\n\n\ndef RMSLE_interpolate(alpha, y, x0, x1):\n    return RMSLE(y, interpolate(alpha, x0, x1))\n\n\ndef fit_hybrid(\n    train: pd.DataFrame, y_cols: List[str] = [\"ConfirmedCases\", \"Fatalities\"]\n) -> pd.DataFrame:\n    def fit_one(y_col: str):\n        opt = least_squares(\n            fun=RMSLE_interpolate,\n            args=(\n                train[y_col],\n                train[f\"y_hat_fitter_{y_col}\"],\n                train[f\"yhat_xgb_{y_col}\"],\n            ),\n            x0=(0.5,),\n            bounds=((0.0), (1.0,)),\n        )\n        return {f\"{y_col}_alpha\": opt.x[0], f\"{y_col}_cost\": opt.cost}\n\n    result = {}\n    for y_col in y_cols:\n        result.update(fit_one(y_col))\n    return pd.DataFrame([result])\n\n\ndef predict_hybrid(\n    df: pd.DataFrame,\n    x_col: str = \"DayOfYear\",\n    y_cols: List[str] = [\"ConfirmedCases\", \"Fatalities\"],\n):\n    def predict_one(col):\n        df[f\"yhat_hybrid_{col}\"] = interpolate(\n            df[f\"{y_col}_alpha\"].to_numpy(),\n            df[f\"y_hat_fitter_{y_col}\"].to_numpy(),\n            df[f\"yhat_xgb_{y_col}\"].to_numpy(),\n        )\n\n    for y_col in y_cols:\n        predict_one(y_col)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now apply to each `Country_Region`:"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.merge(\n    train,\n    train.groupby([\"Country_Region\"], observed=True, sort=False)\n    .apply(lambda x: fit_hybrid(x))\n    .reset_index(),\n    on=[\"Country_Region\"],\n    how=\"left\",\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict_hybrid(train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Compare approaches"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\n    \"Confirmed:\\n\"\n    f'Fitter\\t{RMSLE(train[\"ConfirmedCases\"], train[\"y_hat_fitter_ConfirmedCases\"])}\\n'\n    f'XGBoost\\t{RMSLE(train[\"ConfirmedCases\"], train[\"yhat_xgb_ConfirmedCases\"])}\\n'\n    f'Hybrid\\t{RMSLE(train[\"ConfirmedCases\"], train[\"yhat_hybrid_ConfirmedCases\"])}\\n'\n    f\"Fatalities:\\n\"\n    f'Fitter\\t{RMSLE(train[\"Fatalities\"], train[\"y_hat_fitter_Fatalities\"])}\\n'\n    f'XGBoost\\t{RMSLE(train[\"Fatalities\"], train[\"yhat_xgb_Fatalities\"])}\\n'\n    f'Hybrid\\t{RMSLE(train[\"Fatalities\"], train[\"yhat_hybrid_Fatalities\"])}\\n'\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predict test cases"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Merge logistic and hybrid fit into test\ntest = pd.merge(\n    test, \n    train[[\"Country_Region\"] +\n          ['ConfirmedCases_p_0', 'ConfirmedCases_p_1', 'ConfirmedCases_p_2', 'ConfirmedCases_p_3', 'ConfirmedCases_p_4', 'ConfirmedCases_p_5', 'ConfirmedCases_p_6', 'ConfirmedCases_p_7', 'ConfirmedCases_p_8', 'ConfirmedCases_p_9']+\n          ['Fatalities_p_0', 'Fatalities_p_1', 'Fatalities_p_2', 'Fatalities_p_3', 'Fatalities_p_4', 'Fatalities_p_5', 'Fatalities_p_6', 'Fatalities_p_7', 'Fatalities_p_8', 'Fatalities_p_9']+\n          [\"Fatalities_alpha\"] + \n          [\"ConfirmedCases_alpha\"]].groupby(['Country_Region']).head(1), on=\"Country_Region\", how=\"left\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test predictions\ntest[\"y_hat_fitter_ConfirmedCases\"]=dixglf.function(\n    test[\"DayOfYear\"],\n    test[\"ConfirmedCases_p_0\"],\n    test[\"ConfirmedCases_p_1\"],\n    test[\"ConfirmedCases_p_2\"],\n    test[\"ConfirmedCases_p_3\"],\n    test[\"ConfirmedCases_p_4\"],\n    test[\"ConfirmedCases_p_5\"],\n    test[\"ConfirmedCases_p_6\"],\n    test[\"ConfirmedCases_p_7\"],\n    test[\"ConfirmedCases_p_8\"],\n    test[\"ConfirmedCases_p_9\"])\ntest[\"y_hat_fitter_Fatalities\"]=dixglf.function(\n    test[\"DayOfYear\"],\n    test[\"Fatalities_p_0\"],\n    test[\"Fatalities_p_1\"],\n    test[\"Fatalities_p_2\"],\n    test[\"Fatalities_p_3\"],\n    test[\"Fatalities_p_4\"],\n    test[\"Fatalities_p_5\"],\n    test[\"Fatalities_p_6\"],\n    test[\"Fatalities_p_7\"],\n    test[\"Fatalities_p_8\"],\n    test[\"Fatalities_p_9\"])\ntest[\"yhat_xgb_ConfirmedCases\"] = xgb_c_fit.predict(test[x_columns].to_numpy())\ntest[\"yhat_xgb_Fatalities\"] = xgb_f_fit.predict(test[x_columns].to_numpy())\npredict_hybrid(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prepare submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = test[[\"ForecastId\", \"yhat_hybrid_ConfirmedCases\", \"yhat_hybrid_Fatalities\"]].round(2).rename(\n        columns={\n            \"yhat_hybrid_ConfirmedCases\": \"ConfirmedCases\",\n            \"yhat_hybrid_Fatalities\": \"Fatalities\",\n        }\n    )\nsubmission[\"ConfirmedCases\"] = np.maximum(0, submission[\"ConfirmedCases\"])\nsubmission[\"Fatalities\"] = np.maximum(0, submission[\"Fatalities\"])\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}