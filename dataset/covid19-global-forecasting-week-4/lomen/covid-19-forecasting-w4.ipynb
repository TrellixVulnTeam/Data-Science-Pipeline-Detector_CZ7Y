{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport geopandas as gpd\nfrom shapely.geometry import Point\nimport os\nimport tensorflow as tf\nfrom tqdm import tqdm\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import mean_squared_log_error\n\nfrom datetime import datetime\nfrom datetime import timedelta\n\nfrom tensorflow.keras.optimizers import Adagrad\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import Input\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_df = gpd.read_file(\"/kaggle/input/covid19-global-forecasting-week-4/train.csv\")\ntrain_df[\"ConfirmedCases\"] = train_df[\"ConfirmedCases\"].astype(\"float\")\ntrain_df[\"Fatalities\"] = train_df[\"Fatalities\"].astype(\"float\")\ntrain_df[\"Country_Region\"] = [ row.Country_Region.replace(\"'\",\"\").strip(\" \") if row.Province_State==\"\" else str(row.Country_Region+\"_\"+row.Province_State).replace(\"'\",\"\").strip(\" \") for idx,row in train_df.iterrows()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"extra_data_df = gpd.read_file(\"/kaggle/input/covid19-enriched-dataset-week-2/enriched_covid_19_week_2.csv\")\n                        \nextra_data_df[\"Country_Region\"] = [country_name.replace(\"'\",\"\") for country_name in extra_data_df[\"Country_Region\"]]\nextra_data_df[\"restrictions\"] = extra_data_df[\"restrictions\"].astype(\"int\")\nextra_data_df[\"quarantine\"] = extra_data_df[\"quarantine\"].astype(\"int\")\nextra_data_df[\"schools\"] = extra_data_df[\"schools\"].astype(\"int\")\nextra_data_df[\"total_pop\"] = extra_data_df[\"total_pop\"].astype(\"float\")\nextra_data_df[\"density\"] = extra_data_df[\"density\"].astype(\"float\")\nextra_data_df[\"hospibed\"] = extra_data_df[\"hospibed\"].astype(\"float\")\nextra_data_df[\"lung\"] = extra_data_df[\"lung\"].astype(\"float\")\nextra_data_df[\"total_pop\"] = extra_data_df[\"total_pop\"]/max(extra_data_df[\"total_pop\"])\nextra_data_df[\"density\"] = extra_data_df[\"density\"]/max(extra_data_df[\"density\"])\nextra_data_df[\"hospibed\"] = extra_data_df[\"hospibed\"]/max(extra_data_df[\"hospibed\"])\nextra_data_df[\"lung\"] = extra_data_df[\"lung\"]/max(extra_data_df[\"lung\"])\nextra_data_df[\"age_100+\"] = extra_data_df[\"age_100+\"].astype(\"float\")\nextra_data_df[\"age_100+\"] = extra_data_df[\"age_100+\"]/max(extra_data_df[\"age_100+\"])\n\nextra_data_df = extra_data_df[[\"Country_Region\",\"Date\",\"restrictions\",\"quarantine\",\"schools\",\"hospibed\",\"lung\",\"total_pop\",\"density\",\"age_100+\"]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ll_df = pd.read_csv(\"/kaggle/input/country-region-lat-long/Country_Region_Lat_Long.csv\")\nll_df = ll_df[[\"Country_Region\",\"Lat\",\"Long\",\"Lat(//15)\",\"Long(//30)\"]]\nll_df = ll_df.drop_duplicates().reset_index(drop=True)\nll_df[\"Lat\"] = ll_df[\"Lat\"]+90\nll_df[\"Lat\"] = ll_df[\"Lat\"]//15\nll_df[\"Lat\"] = ll_df[\"Lat\"]/max(ll_df[\"Lat\"])\n\nll_df[\"Long\"] = ll_df[\"Long\"]+180\nll_df[\"Long\"] = ll_df[\"Long\"]//30\nll_df[\"Long\"] = ll_df[\"Long\"]/max(ll_df[\"Long\"])\n\n#ll_df[\"Lat\"] = ll_df[\"Lat(//15)\"]/max(ll_df[\"Lat(//15)\"])\n#ll_df[\"Long\"] = ll_df[\"Long(//30)\"]/max(ll_df[\"Long(//30)\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ll_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"extra_data_df = extra_data_df.merge(ll_df, how=\"left\", on=[\"Country_Region\"]).drop_duplicates()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = train_df.merge(extra_data_df, how=\"left\", on=['Country_Region','Date']).drop_duplicates()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for country_region in train_df.Country_Region.unique():\n    query_df = train_df.query(\"Country_Region=='\"+country_region+\"' and Date=='2020-03-25'\")\n    train_df.loc[(train_df[\"Country_Region\"]==country_region) & (train_df[\"Date\"]>\"2020-03-25\"),\"total_pop\"] = query_df.total_pop.values[0]\n    train_df.loc[(train_df[\"Country_Region\"]==country_region) & (train_df[\"Date\"]>\"2020-03-25\"),\"hospibed\"] = query_df.hospibed.values[0]\n    train_df.loc[(train_df[\"Country_Region\"]==country_region) & (train_df[\"Date\"]>\"2020-03-25\"),\"density\"] = query_df.density.values[0]\n    train_df.loc[(train_df[\"Country_Region\"]==country_region) & (train_df[\"Date\"]>\"2020-03-25\"),\"lung\"] = query_df.lung.values[0]\n    train_df.loc[(train_df[\"Country_Region\"]==country_region) & (train_df[\"Date\"]>\"2020-03-25\"),\"Lat\"] = query_df.Lat.values[0]\n    train_df.loc[(train_df[\"Country_Region\"]==country_region) & (train_df[\"Date\"]>\"2020-03-25\"),\"Long\"] = query_df.Long.values[0]\n    train_df.loc[(train_df[\"Country_Region\"]==country_region) & (train_df[\"Date\"]>\"2020-03-25\"),\"age_100+\"] = query_df[\"age_100+\"].values[0]\n    train_df.loc[(train_df[\"Country_Region\"]==country_region) & (train_df[\"Date\"]>\"2020-03-25\"),\"restrictions\"] = query_df.restrictions.values[0]\n    train_df.loc[(train_df[\"Country_Region\"]==country_region) & (train_df[\"Date\"]>\"2020-03-25\"),\"quarantine\"] = query_df.quarantine.values[0]\n    train_df.loc[(train_df[\"Country_Region\"]==country_region) & (train_df[\"Date\"]>\"2020-03-25\"),\"schools\"] = query_df.schools.values[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"median_pop = np.median(extra_data_df.total_pop)\nmedian_hospibed = np.median(extra_data_df.hospibed)\nmedian_density = np.median(extra_data_df.density)\nmedian_lung = np.median(extra_data_df.lung)\nmedian_lat = np.median(extra_data_df.Lat)\nmedian_long = np.median(extra_data_df.Long)\nmedian_centenarian_pop = np.median(extra_data_df[\"age_100+\"])\n\nfor country_region in train_df.Country_Region.unique():\n    if extra_data_df.query(\"Country_Region=='\"+country_region+\"'\").empty:\n        \n        train_df.loc[train_df[\"Country_Region\"]==country_region,\"total_pop\"] = median_pop\n        train_df.loc[train_df[\"Country_Region\"]==country_region,\"hospibed\"] = median_hospibed\n        train_df.loc[train_df[\"Country_Region\"]==country_region,\"density\"] = median_density\n        train_df.loc[train_df[\"Country_Region\"]==country_region,\"lung\"] = median_lung\n        train_df.loc[train_df[\"Country_Region\"]==country_region,\"Lat\"] = median_lat\n        train_df.loc[train_df[\"Country_Region\"]==country_region,\"Long\"] = median_long\n        train_df.loc[train_df[\"Country_Region\"]==country_region,\"age_100+\"] = median_centenarian_pop\n        train_df.loc[train_df[\"Country_Region\"]==country_region,\"restrictions\"] = 0\n        train_df.loc[train_df[\"Country_Region\"]==country_region,\"quarantine\"] = 0\n        train_df.loc[train_df[\"Country_Region\"]==country_region,\"schools\"] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trend_df = pd.DataFrame(columns={\"infection_trend\",\"fatality_trend\",\"quarantine_trend\",\"school_trend\",\"total_population\",\"expected_cases\",\"expected_fatalities\"})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = train_df.query(\"Date>'2020-01-21'and Date<'2020-04-15'\")\ndays_in_sequence = 21\n\ntrend_list = []\n\nwith tqdm(total=len(list(train_df.Country_Region.unique()))) as pbar:\n    for country in train_df.Country_Region.unique():\n        for province in train_df.query(f\"Country_Region=='{country}'\").Province_State.unique():\n            province_df = train_df.query(f\"Country_Region=='{country}' and Province_State=='{province}'\")\n            \n            #I added a quick hack to double the number of sequences\n            #Warning: This will later create a minor leakage from the \n            # training set into the validation set.\n            for i in range(0,len(province_df),int(days_in_sequence/3)):\n                if i+days_in_sequence<=len(province_df):\n                    #prepare all the temporal inputs\n                    infection_trend = [float(x) for x in province_df[i:i+days_in_sequence-1].ConfirmedCases.values]\n                    fatality_trend = [float(x) for x in province_df[i:i+days_in_sequence-1].Fatalities.values]\n                    restriction_trend = [float(x) for x in province_df[i:i+days_in_sequence-1].restrictions.values]\n                    quarantine_trend = [float(x) for x in province_df[i:i+days_in_sequence-1].quarantine.values]\n                    school_trend = [float(x) for x in province_df[i:i+days_in_sequence-1].schools.values]\n\n                    #preparing all the demographic inputs\n                    total_population = float(province_df.iloc[i].total_pop)\n                    density = float(province_df.iloc[i].density)\n                    hospibed = float(province_df.iloc[i].hospibed)\n                    lung = float(province_df.iloc[i].lung)\n                    lat = float(province_df.iloc[i].Lat)\n                    long = float(province_df.iloc[i].Long)\n                    centenarian_pop = float(province_df.iloc[i][\"age_100+\"])\n\n                    expected_cases = float(province_df.iloc[i+days_in_sequence-1].ConfirmedCases)\n                    expected_fatalities = float(province_df.iloc[i+days_in_sequence-1].Fatalities)\n\n                    trend_list.append({\"infection_trend\":infection_trend,\n                                     \"fatality_trend\":fatality_trend,\n                                     \"restriction_trend\":restriction_trend,\n                                     \"quarantine_trend\":quarantine_trend,\n                                     \"school_trend\":school_trend,\n                                     \"demographic_inputs\":[total_population,density,hospibed,lung,lat,long,centenarian_pop],\n                                     \"expected_cases\":expected_cases,\n                                     \"expected_fatalities\":expected_fatalities})\n        pbar.update(1)\ntrend_df = pd.DataFrame(trend_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trend_df[\"temporal_inputs\"] = [np.asarray([trends[\"infection_trend\"],trends[\"fatality_trend\"],trends[\"restriction_trend\"],trends[\"quarantine_trend\"],trends[\"school_trend\"]]) for idx,trends in trend_df.iterrows()]\n\ntrend_df = shuffle(trend_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"i=0\ntemp_df = pd.DataFrame()\nfor idx,row in trend_df.iterrows():\n    if sum(row.infection_trend)>0:\n        temp_df = temp_df.append(row)\n    else:\n        if i<25:\n            temp_df = temp_df.append(row)\n            i+=1\ntrend_df = temp_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sequence_length = 20\ntraining_percentage = 0.9","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_item_count = int(len(trend_df)*training_percentage)\nvalidation_item_count = len(trend_df)-int(len(trend_df)*training_percentage)\ntraining_df = trend_df[:training_item_count]\nvalidation_df = trend_df[training_item_count:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_temporal_train = np.asarray(np.transpose(np.reshape(np.asarray([np.asarray(x) for x in training_df[\"temporal_inputs\"].values]),(training_item_count,5,sequence_length)),(0,2,1) )).astype(np.float32)\nX_demographic_train = np.asarray([np.asarray(x) for x in training_df[\"demographic_inputs\"]]).astype(np.float32)\nY_cases_train = np.asarray([np.asarray(x) for x in training_df[\"expected_cases\"]]).astype(np.float32)\nY_fatalities_train = np.asarray([np.asarray(x) for x in training_df[\"expected_fatalities\"]]).astype(np.float32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_temporal_test = np.asarray(np.transpose(np.reshape(np.asarray([np.asarray(x) for x in validation_df[\"temporal_inputs\"]]),(validation_item_count,5,sequence_length)),(0,2,1)) ).astype(np.float32)\nX_demographic_test = np.asarray([np.asarray(x) for x in validation_df[\"demographic_inputs\"]]).astype(np.float32)\nY_cases_test = np.asarray([np.asarray(x) for x in validation_df[\"expected_cases\"]]).astype(np.float32)\nY_fatalities_test = np.asarray([np.asarray(x) for x in validation_df[\"expected_fatalities\"]]).astype(np.float32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#temporal input branch\ntemporal_input_layer = Input(shape=(sequence_length,5))\nmain_rnn_layer = layers.LSTM(100, return_sequences=True, recurrent_dropout=0.2)(temporal_input_layer)\n\n#demographic input branch\ndemographic_input_layer = Input(shape=(7))\ndemographic_dense = layers.Dense(20)(demographic_input_layer)\ndemographic_dropout = layers.Dropout(0.2)(demographic_dense)\n\n#cases output branch\nrnn_c = layers.LSTM(50)(main_rnn_layer)\nmerge_c = layers.Concatenate(axis=-1)([rnn_c,demographic_dropout])\ndense_c = layers.Dense(150)(merge_c)\ndropout_c = layers.Dropout(0.2)(dense_c)\ncases = layers.Dense(1, activation=layers.LeakyReLU(alpha=0.1),name=\"cases\")(dropout_c)\n\n#fatality output branch\nrnn_f = layers.LSTM(50)(main_rnn_layer)\nmerge_f = layers.Concatenate(axis=-1)([rnn_f,demographic_dropout])\ndense_f = layers.Dense(150)(merge_f)\ndropout_f = layers.Dropout(0.2)(dense_f)\nfatalities = layers.Dense(1, activation=layers.LeakyReLU(alpha=0.1), name=\"fatalities\")(dropout_f)\n\n\nmodel = Model([temporal_input_layer,demographic_input_layer], [cases,fatalities])\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"callbacks = [ReduceLROnPlateau(monitor='val_loss', patience=5, verbose=1, factor=0.6),\n             EarlyStopping(monitor='val_loss', patience=30),\n             ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)]\nopt = Adagrad(lr=0.01, epsilon=1e-08, decay=1e-4)\nmodel.compile(loss=[tf.keras.losses.MeanSquaredLogarithmicError(),tf.keras.losses.MeanSquaredLogarithmicError()], optimizer=opt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit([X_temporal_train,X_demographic_train], [Y_cases_train, Y_fatalities_train], \n          epochs = 300, \n          batch_size = 10, \n          validation_data=([X_temporal_test,X_demographic_test],  [Y_cases_test, Y_fatalities_test]), \n          callbacks=callbacks)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Loss over epochs')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='best')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = model.predict([X_temporal_test,X_demographic_test])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(predictions[0][0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = train_df.query(\"Date>'2020-01-21'and Date<'2020-04-01'\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Will retrieve the number of cases and fatalities for the past 6 days from the given date\ndef build_inputs_for_date(country, province, date, df):\n    start_date = date - timedelta(days=20)\n    end_date = date - timedelta(days=1)\n    \n    str_start_date = start_date.strftime(\"%Y-%m-%d\")\n    str_end_date = end_date.strftime(\"%Y-%m-%d\")\n    df = df.query(\"Country_Region=='\"+country+\"' and Province_State=='\"+province+\"' and Date>='\"+str_start_date+\"' and Date<='\"+str_end_date+\"'\")\n    #preparing the temporal inputs\n    temporal_input_data = np.transpose(np.reshape(np.asarray([df[\"ConfirmedCases\"],\n                                                 df[\"Fatalities\"],\n                                                 df[\"restrictions\"],\n                                                 df[\"quarantine\"],\n                                                 df[\"schools\"]]),\n                                     (5,sequence_length)), (1,0) ).astype(np.float32)\n    \n    #preparing all the demographic inputs\n    total_population = float(province_df.iloc[i].total_pop)\n    density = float(province_df.iloc[i].density)\n    hospibed = float(province_df.iloc[i].hospibed)\n    lung = float(province_df.iloc[i].lung)\n    lat = float(province_df.iloc[i].Lat)\n    long = float(province_df.iloc[i].Long)\n    \n    centenarian_pop = float(province_df.iloc[i][\"age_100+\"])\n    demographic_input_data = [total_population,density,hospibed,lung,lat,long,centenarian_pop]\n    \n    return [np.array([temporal_input_data]), np.array([demographic_input_data])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Take a dataframe in input, will do the predictions and return the dataframe with extra rows\n#containing the predictions\ndef predict_for_region(country, province, df):\n    begin_prediction = \"2020-04-01\"\n    start_date = datetime.strptime(begin_prediction,\"%Y-%m-%d\")\n    end_prediction = \"2020-05-14\"\n    end_date = datetime.strptime(end_prediction,\"%Y-%m-%d\")\n    \n    date_list = [start_date + timedelta(days=x) for x in range((end_date-start_date).days+1)]\n    for date in date_list:\n        input_data = build_inputs_for_date(country, province, date, df)\n        result = model.predict(input_data)\n        \n        #just ensuring that the outputs is\n        #higher than the previous counts\n        result[0] = np.round(result[0])\n        if result[0]<input_data[0][0][-1][0]:\n            result[0]=np.array([[input_data[0][0][-1][0]]])\n        \n        result[1] = np.round(result[1])\n        if result[1]<input_data[0][0][-1][1]:\n            result[1]=np.array([[input_data[0][0][-1][1]]])\n        \n        #We assign the quarantine and school status\n        #depending on previous values\n        #e.g Once a country is locked, it will stay locked until the end\n        df = df.append({\"Country_Region\":country, \n                        \"Province_State\":province, \n                        \"Date\":date.strftime(\"%Y-%m-%d\"), \n                        \"restrictions\": 1 if any(input_data[0][0][2]) else 0,\n                        \"quarantine\": 1 if any(input_data[0][0][3]) else 0,\n                        \"schools\": 1 if any(input_data[0][0][4]) else 0,\n                        \"total_pop\": input_data[1][0],\n                        \"density\": input_data[1][0][1],\n                        \"hospibed\": input_data[1][0][2],\n                        \"lung\": input_data[1][0][3],\n                        \"lat\": input_data[1][0][4],\n                        \"long\": input_data[1][0][5],\n                        \n                        \"age_100+\": input_data[1][0][6],\n                        \"ConfirmedCases\":round(result[0][0][0]),\t\n                        \"Fatalities\":round(result[1][0][0])},\n                       ignore_index=True)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#The functions that are called here need to optimise, sorry about that!\ncopy_df = train_df\nwith tqdm(total=len(list(copy_df.Country_Region.unique()))) as pbar:\n    for country in copy_df.Country_Region.unique():\n        for province in copy_df.query(\"Country_Region=='\"+country+\"'\").Province_State.unique():\n            copy_df = predict_for_region(country, province, copy_df)\n        pbar.update(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"groundtruth_df = gpd.read_file(\"/kaggle/input/covid19-global-forecasting-week-4/train.csv\")\n\ngroundtruth_df[\"ConfirmedCases\"] = groundtruth_df[\"ConfirmedCases\"].astype(\"float\")\ngroundtruth_df[\"Fatalities\"] = groundtruth_df[\"Fatalities\"].astype(\"float\")\n#The country_region got modifying in the enriched dataset by @optimo, \n# so we have to apply the same change to this Dataframe.\ngroundtruth_df[\"Country_Region\"] = [ row.Country_Region.replace(\"'\",\"\").strip(\" \") if row.Province_State==\"\" else str(row.Country_Region+\"_\"+row.Province_State).replace(\"'\",\"\").strip(\" \") for idx,row in groundtruth_df.iterrows()]\n\nlast_date = groundtruth_df.Date.unique()[-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#to remove annoying warnings from pandas\npd.options.mode.chained_assignment = None\n\ndef get_RMSLE_per_region(region, groundtruth_df, display_only=False):\n    groundtruth_df[\"ConfirmedCases\"] = groundtruth_df[\"ConfirmedCases\"].astype(\"float\")\n    groundtruth_df[\"Fatalities\"] = groundtruth_df[\"Fatalities\"].astype(\"float\")\n    \n    #we only take data until the 30th of March 2020 as the groundtruth was not available for later dates.\n    groundtruth = groundtruth_df.query(\"Country_Region=='\"+region+\"' and Date>='2020-04-01' and Date<='\"+last_date+\"'\")\n    predictions = copy_df.query(\"Country_Region=='\"+region+\"' and Date>='2020-04-01' and Date<='\"+last_date+\"'\")\n    \n    RMSLE_cases = np.sqrt(mean_squared_log_error( groundtruth.ConfirmedCases.values, predictions.ConfirmedCases.values ))\n    RMSLE_fatalities = np.sqrt(mean_squared_log_error( groundtruth.Fatalities.values, predictions.Fatalities.values ))\n    \n    if display_only:\n        print(region)\n        print(\"RMSLE on cases:\",np.mean(RMSLE_cases))\n        print(\"RMSLE on fatalities:\",np.mean(RMSLE_fatalities))\n    else:\n        return RMSLE_cases, RMSLE_fatalities","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_RMSLE_for_all_regions(groundtruth_df):\n    RMSLE_cases_list = []\n    RMSLE_fatalities_list = []\n    for region in groundtruth_df.Country_Region.unique():\n        RMSLE_cases, RMSLE_fatalities = get_RMSLE_per_region(region, groundtruth_df, False)\n        RMSLE_cases_list.append(RMSLE_cases)\n        RMSLE_fatalities_list.append(RMSLE_fatalities)\n    print(\"RMSLE on cases:\",np.mean(RMSLE_cases_list))\n    print(\"RMSLE on fatalities:\",np.mean(RMSLE_fatalities_list))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_RMSLE_for_all_regions(groundtruth_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = gpd.read_file(\"/kaggle/input/covid19-global-forecasting-week-4/test.csv\")\n#The country_region got modifying in the enriched dataset by @optimo, \n# so we have to apply the same change to the test Dataframe.\ntest_df[\"Country_Region\"] = [ row.Country_Region if row.Province_State==\"\" else row.Country_Region+\"_\"+row.Province_State for idx,row in test_df.iterrows() ]\ntest_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df = pd.DataFrame(columns=[\"ForecastId\",\"ConfirmedCases\",\"Fatalities\"])\nwith tqdm(total=len(test_df)) as pbar:\n    for idx, row in test_df.iterrows():\n        #Had to remove single quotes because of countries like Cote D'Ivoire for example\n        country_region = row.Country_Region.replace(\"'\",\"\").strip(\" \")\n        province_state = row.Province_State.replace(\"'\",\"\").strip(\" \")\n        item = copy_df.query(\"Country_Region=='\"+country_region+\"' and Province_State=='\"+province_state+\"' and Date=='\"+row.Date+\"'\")\n        submission_df = submission_df.append({\"ForecastId\":row.ForecastId,\n                                              \"ConfirmedCases\":int(item.ConfirmedCases.values[0]),\n                                              \"Fatalities\":int(item.Fatalities.values[0])},\n                                             ignore_index=True)\n        pbar.update(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df.to_csv(\"submission.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}