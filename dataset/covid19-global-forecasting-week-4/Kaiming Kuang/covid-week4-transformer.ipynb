{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import copy\nimport os\nfrom collections import OrderedDict\n\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom matplotlib import pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import ExponentialLR, ReduceLROnPlateau\nfrom torch.utils.data import DataLoader, Dataset\nfrom tqdm.notebook import tqdm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.listdir(\"../input\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"main_data_dir = \"../input/covid19-global-forecasting-week-4\"\nmetadata_dir = \"../input/covid19-countrywise-metadata\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train = pd.read_csv(os.path.join(main_data_dir, \"train.csv\"))\ndata_test = pd.read_csv(os.path.join(main_data_dir, \"test.csv\"))\nsample_sub = pd.read_csv(os.path.join(main_data_dir, \"submission.csv\"))\nmetadata = pd.read_csv(os.path.join(metadata_dir, \"covid19_countrywise_metadata.csv\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocess original data"},{"metadata":{"trusted":true},"cell_type":"code","source":"def generate_loc(x):\n    if isinstance(x[\"Province_State\"], float):\n        return x[\"Country_Region\"]\n    else:\n        return \"_\".join([x[\"Country_Region\"], x[\"Province_State\"]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def generate_loc_date(x):\n    return \"_\".join([x[\"location\"], x[\"Date\"]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train[\"location\"] = data_train.apply(generate_loc, axis=1)\ndata_test[\"location\"] = data_test.apply(generate_loc, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train[\"log_cfm\"] = data_train[\"ConfirmedCases\"].map(np.log1p)\ndata_train[\"log_ftl\"] = data_train[\"Fatalities\"].map(np.log1p)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train[\"loc_date\"] = data_train.apply(generate_loc_date, axis=1)\ndata_test[\"loc_date\"] = data_test.apply(generate_loc_date, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train_by_loc = data_train.groupby(\"location\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train[\"Date\"] = pd.to_datetime(data_train[\"Date\"])\ndata_test[\"Date\"] = pd.to_datetime(data_test[\"Date\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train[\"rel_date\"] = data_train.apply(lambda x: (x[\"Date\"] - data_train[\"Date\"].min()), axis=1).dt.days\ndata_test[\"rel_date\"] = data_test.apply(lambda x: (x[\"Date\"] - data_train[\"Date\"].min()), axis=1).dt.days","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train[\"rel_date_pct\"] = data_train.rel_date / data_test.rel_date.max()\ndata_test[\"rel_date_pct\"] = data_test.rel_date / data_test.rel_date.max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_day_zero(df):\n    progress = pd.pivot_table(data_train[[\"location\", \"ConfirmedCases\", \"rel_date\"]],\n        values=\"ConfirmedCases\", index=\"location\", columns=\"rel_date\")\n    day_zero = np.argmax((progress.values > 0).cumsum(axis=1) == 1, axis=1)\n    day_zero_location = {progress.index[i]: day_zero[i] for i in range(len(progress))}\n    \n    return day_zero_location","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_since_day_zero(row, day_zero):\n    cur_date = row[\"rel_date\"]\n    cur_loc = row[\"location\"]\n    cur_day_zero = day_zero[cur_loc]\n    return max(cur_date - cur_day_zero, -1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"day_zero_location = get_day_zero(data_train)\ndata_train[\"since_day_zero\"] = data_train.apply(get_since_day_zero, axis=1, day_zero=day_zero_location)\ndata_test[\"since_day_zero\"] = data_test.apply(get_since_day_zero, axis=1, day_zero=day_zero_location)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train[\"since_day_zero\"] = data_train[\"since_day_zero\"].map(lambda x: -0.1 if x < 0 else x / data_test.since_day_zero.max())\ndata_test[\"since_day_zero\"] = data_test[\"since_day_zero\"].map(lambda x: -0.1 if x < 0 else x / data_test.since_day_zero.max())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Add lockdown data"},{"metadata":{"trusted":true},"cell_type":"code","source":"lockdown_cols = [\"quarantine\", \"close_school\", \"close_public_place\", \"limit_gathering\", \"stay_home\"]\nmetadata[lockdown_cols] = metadata[lockdown_cols].fillna(\"2099-12-31\")\nfor col in lockdown_cols:\n    metadata[col] = pd.to_datetime(metadata[col], format=\"%Y-%m-%d\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def add_lockdown_data(df, metadata):\n    lockdown_cols = [\"quarantine\", \"close_school\", \"close_public_place\", \"limit_gathering\", \"stay_home\"]\n    lockdown_df = pd.DataFrame(np.zeros((len(df), len(lockdown_cols))), columns=lockdown_cols)\n    for i in range(len(df)):\n        cur_date = df.Date[i]\n        cur_loc = df.location[i]\n        idx = metadata.loc[metadata.location == cur_loc, :].index.values[0]\n        lockdown_df.loc[i, :] = (metadata.loc[idx, lockdown_cols] <= cur_date).astype(int)\n    \n    return pd.concat([df, lockdown_df], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train = add_lockdown_data(data_train, metadata)\ndata_test = add_lockdown_data(data_test, metadata)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Metadata preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"metadata.drop(lockdown_cols, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def min_max_normalize(data):\n    scaler = MinMaxScaler()\n    return scaler.fit_transform(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_cols = metadata.columns[3:]\nmetadata[num_cols] = min_max_normalize(metadata[num_cols])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"metadata.drop([\"region\", \"country\"], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"metadata.set_index(\"location\", inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prepare test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_test[\"log_cfm\"] = 0\ndata_test[\"log_ftl\"] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_test_known = data_test.loc[data_test.rel_date <= data_train.rel_date.max(), :].sort_values([\"rel_date\", \"location\"]).reset_index(drop=True)\ndata_test_unknown = data_test.loc[data_test.rel_date > data_train.rel_date.max(), :].sort_values([\"rel_date\", \"location\"]).reset_index(drop=True)\ndata_train.sort_values([\"rel_date\", \"location\"], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_test_known[[\"log_cfm\", \"log_ftl\"]] = data_train.loc[data_train.rel_date >= data_test.rel_date.min(), [\"log_cfm\", \"log_ftl\"]].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_test_input = pd.concat([data_test_known, data_test_unknown], axis=0)\ninput_len = 30\ndata_test_input = pd.concat([data_train.loc[(data_train.rel_date >= data_test_unknown.rel_date.min() - input_len)\\\n    & (data_train.rel_date < data_test_known.rel_date.min()), data_test_input.columns],\n    data_test_input], axis=0).sort_values([\"rel_date\", \"location\"]).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TrainDataset(Dataset):\n    \n    def __init__(self, df, metadata, x_cols, y_cols, input_len=30):\n        self.df = df\n        self.metadata = metadata\n        self.x_cols = x_cols\n        self.y_cols = y_cols\n        self.input_len = input_len\n        \n        self.loc_list = list(self.df.location.unique())\n        self.num_pos_train_period = self.df.rel_date.max() - self.input_len + 1\n    \n    def __len__(self):\n        return len(self.loc_list) * self.num_pos_train_period\n    \n    def __getitem__(self, idx):\n        cur_loc = self.loc_list[idx // self.num_pos_train_period]\n        input_beg = idx % self.num_pos_train_period\n        input_end = input_beg + self.input_len\n        in_input_period = (self.df.rel_date >= input_beg) & (self.df.rel_date < input_end)\n        is_cur_loc = self.df.location == cur_loc\n        inputs = self.df.loc[(is_cur_loc) & (in_input_period), self.x_cols].values\n        meta_inputs = self.metadata.loc[cur_loc, :].values\n        targets = self.df.loc[(is_cur_loc) & (self.df.rel_date == input_end), self.y_cols].values.reshape((-1))\n        \n        return torch.tensor(inputs, dtype=torch.float32),\\\n            torch.tensor(meta_inputs, dtype=torch.float32),\\\n            torch.tensor(targets, dtype=torch.float32)\n    \n    @staticmethod\n    def get_dataloader(dataset, batch_size, shuffle=True):\n        return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rand_seed = 42\nloc_list = data_train.location.unique()\nloc_train, loc_val = train_test_split(loc_list, test_size=0.2, random_state=rand_seed)\ndata_train.set_index(\"location\", inplace=True, drop=True)\ndata_val = data_train.loc[loc_val, :]\ndata_train = data_train.loc[loc_train, :]\ndata_train.reset_index(drop=False, inplace=True)\ndata_val.reset_index(drop=False, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_cols = [\"log_cfm\", \"log_ftl\", \"rel_date_pct\", \"since_day_zero\"] + lockdown_cols\ny_cols = [\"log_cfm\", \"log_ftl\"]\nbatch_size = 512\nd_in = len(x_cols)\nd_meta = len(metadata.columns)\nds_train = TrainDataset(data_train, metadata, x_cols, y_cols, input_len)\ndl_train = TrainDataset.get_dataloader(ds_train, batch_size)\nds_val = TrainDataset(data_val, metadata, x_cols, y_cols, input_len)\ndl_val = TrainDataset.get_dataloader(ds_val, batch_size, shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for _, sample in enumerate(dl_train):\n    for x in sample:\n        print(x.size())\n    break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TestDataset(Dataset):\n    \n    def __init__(self, df, metadata, x_cols, test_period_len, input_len=30):\n        self.df = df\n        self.metadata = metadata\n        self.x_cols = x_cols\n        self.test_period_len = test_period_len\n        self.input_len = input_len\n        \n        self.loc_list = list(self.df.location.unique())\n        self.test_beg = self.df.rel_date.max() - test_period_len + 1\n    \n    def __len__(self):\n        return len(self.df) - self.input_len * len(self.loc_list)\n    \n    def __getitem__(self, idx):\n        df_idx = idx + self.input_len * len(self.loc_list)\n        cur_loc = self.df.location[df_idx]\n        cur_date = self.df.rel_date[df_idx]\n        input_beg = cur_date - self.input_len\n        input_end = cur_date\n        in_input_period = (self.df.rel_date >= input_beg) & (self.df.rel_date < input_end)\n        inputs = self.df.loc[(in_input_period) & (self.df.location == cur_loc), self.x_cols].values\n        inputs = np.expand_dims(inputs, axis=0)\n        meta_inputs = self.metadata.loc[cur_loc, :].values\n        meta_inputs = np.expand_dims(meta_inputs, axis=0)\n        \n        return torch.tensor(inputs, dtype=torch.float32),\\\n            torch.tensor(meta_inputs, dtype=torch.float32)\n    \n    @staticmethod\n    def get_dataloader(dataset, shuffle=False):\n        return DataLoader(dataset, batch_size=None, shuffle=shuffle)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_period_len = data_test.rel_date.max() - data_test.rel_date.min() + 1\nds_test = TestDataset(data_test_input, metadata, x_cols, test_period_len)\ndl_test = TestDataset.get_dataloader(ds_test, shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i, sample in enumerate(dl_test):\n    for x in sample:\n        print(x.size())\n    if i == 10:\n        break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class CovidTransformer(nn.Module):\n    \n    def __init__(self, d_in, d_out, d_meta, d_model=256, d_fwd=512, n_head=4,\n            num_layers=6, dropout=0.1):\n        super().__init__()\n        self.linear = nn.Linear(d_in, d_model)\n        encoder_layer = nn.TransformerEncoderLayer(d_model, n_head, d_fwd, dropout)\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n        self.meta_linear = nn.Linear(d_meta, d_model)\n        self.regressor = nn.Linear(2 * d_model, d_out)\n    \n    def forward(self, inputs, meta_inputs):\n        x = self.linear(inputs)\n        features = self.transformer(x)\n        features = features[-1, :, :]\n        meta_features = self.meta_linear(meta_inputs)\n        features = torch.cat([features, meta_features], dim=-1)\n        output = self.regressor(features)\n        \n        return output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class RMSLE(nn.Module):\n    \n    def forward(self, output, target):\n        return torch.sqrt(F.mse_loss(output, target))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Engine(object):\n    \n    def compile(self, model, criterion, optimizer, scheduler=None):\n        self.model = model\n        self.criterion = criterion\n        self.optimizer = optimizer\n        self.scheduler = scheduler\n    \n    def _init_history(self):\n        self.history = {}\n        self.history[\"loss_train\"] = []\n        self.history[\"loss_val\"] = []\n    \n    def _update_history(self, train_loss, val_loss):\n        self.history[\"loss_train\"].append(train_loss)\n        self.history[\"loss_val\"].append(val_loss)\n    \n    def plot_loss(self):\n        fig, ax = plt.subplots()\n        ax.plot(self.history[\"loss_train\"], label=\"train\")\n        ax.plot(self.history[\"loss_val\"], label=\"val\")\n        plt.legend()\n        plt.show()\n    \n    def _fit_epoch(self, dl_train):\n        train_loss = 0.0\n        for _, sample in enumerate(dl_train):\n            inputs, meta_inputs, target = sample\n            if torch.cuda.is_available():\n                inputs = inputs.cuda()\n                meta_inputs = meta_inputs.cuda()\n                target = target.cuda()\n            inputs = torch.transpose(inputs, 1, 0)\n\n            # zero the parameter gradients\n            self.optimizer.zero_grad()\n\n            # forward + backward + optimize\n            output = self.model(inputs, meta_inputs)\n\n            loss = self.criterion(output, target)\n            loss.backward()\n            self.optimizer.step()\n\n            train_loss += loss.detach().cpu().item()\n        \n        train_loss /= len(dl_train)\n        return train_loss\n    \n    def evaluate(self, dl_val):\n        val_loss = 0.0\n        self.model.eval()\n        \n        with torch.no_grad():\n            for _, sample in enumerate(dl_val):\n                inputs, meta_inputs, target = sample\n                if torch.cuda.is_available():\n                    inputs = inputs.cuda()\n                    meta_inputs = meta_inputs.cuda()\n                    target = target.cuda()\n                inputs = torch.transpose(inputs, 1, 0)\n\n                output = self.model(inputs, meta_inputs)\n\n                loss = self.criterion(output, target)\n                val_loss += loss.detach().cpu().item()\n        \n        val_loss /= len(dl_val)\n        return val_loss\n    \n    def fit(self, epochs, dl_train, dl_val=None):\n        self._init_history()\n        progress = tqdm(total=epochs)\n        \n        for i in range(epochs):\n            progress.set_description_str(\"Epoch {}/{}\"\n                .format(i + 1, epochs))\n            \n            train_loss = self._fit_epoch(dl_train)\n            val_loss = self.evaluate(dl_val)\n            if self.scheduler is not None:\n                if isinstance(self.scheduler, ReduceLROnPlateau):\n                    self.scheduler.step(val_loss)\n                else:\n                    self.scheduler.step()\n            \n            self._update_history(train_loss, val_loss)\n            \n            postfix = \"RMSLE - train: {:.4f}, val: {:.4f}\"\\\n                .format(train_loss, val_loss)\n            progress.set_postfix_str(postfix)\n            progress.update(1)\n    \n    def predict_batch(self, sample):\n        self.model.eval()\n        \n        with torch.no_grad():\n            inputs, meta_inputs = sample\n            if torch.cuda.is_available():\n                inputs = inputs.cuda()\n                meta_inputs = meta_inputs.cuda()\n            inputs = torch.transpose(inputs, 1, 0)\n            output = self.model(inputs, meta_inputs)\n        \n        return output\n    \n    def predict(self, dl):\n        self.model.eval()\n        output = []\n        \n        with torch.no_grad():\n            for _, sample in enumerate(dl):\n                inputs, meta_inputs, target = sample\n                if torch.cuda.is_available():\n                    inputs = inputs.cuda()\n                    meta_inputs = meta_inputs.cuda()\n                    target = target.cuda()\n                inputs = torch.transpose(inputs, 1, 0)\n\n                output_batch = self.model(inputs, meta_inputs)\n                output.append(output_batch)\n        \n        output = torch.cat(output, dim=0).expm1()\n        \n        return output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = CovidTransformer(d_in, 2, d_meta)\nif torch.cuda.is_available():\n    model = model.cuda()\ncriterion = RMSLE()\noptimizer = AdamW(model.parameters())\nepochs = 20\nscheduler = ExponentialLR(optimizer, 0.707)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"engine = Engine()\nengine.compile(model, criterion, optimizer, scheduler)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"engine.fit(epochs, dl_train, dl_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"engine.plot_loss()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_predictions(dl_test, engine, y_cols):\n    progress = tqdm(total=len(dl_test))\n    for i, sample in enumerate(dl_test):\n        idx = i + dl_test.dataset.input_len * len(dl_test.dataset.loc_list)\n        cur_loc = dl_test.dataset.df.location[idx]\n        cur_date = dl_test.dataset.df.rel_date[idx]\n        output = engine.predict_batch(sample).detach().cpu().numpy()\n        prev_date = cur_date - 1\n        prev_output = dl_test.dataset.df.loc[(dl_test.dataset.df.rel_date == prev_date)\n            & (dl_test.dataset.df.location == cur_loc), y_cols].values\n        output = np.maximum(output, prev_output)\n        dl_test.dataset.df.loc[(dl_test.dataset.df.rel_date == cur_date)\n            & (dl_test.dataset.df.location == cur_loc), y_cols] = output\n        progress.update(1)\n        \n    return dl_test.dataset.df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = make_predictions(dl_test, engine, y_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions[\"ConfirmedCases\"] = predictions[\"log_cfm\"].map(np.expm1)\npredictions[\"Fatalities\"] = predictions[\"log_ftl\"].map(np.expm1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = predictions.loc[~predictions.ForecastId.isna(), :]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = predictions.sort_values([\"ForecastId\"]).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = predictions[[\"ForecastId\", \"ConfirmedCases\", \"Fatalities\"]]\nsubmission[\"ForecastId\"] = submission[\"ForecastId\"].astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":4}