{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns#################画图包\nimport matplotlib.pyplot as plt \n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 原训练集测试集特征工程部分","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df_train=pd.read_csv(\"../input/covid19-global-forecasting-week-4/train.csv\")\ndf_test=pd.read_csv(\"../input/covid19-global-forecasting-week-4/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_traintest=pd.concat([df_train,df_test])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## 将所有地区唯一识别成place_id\ndef genplace_id(x):\n    try:\n        place_id=x['Country_Region']+'/'+x['Province_State']\n    except:\n        place_id=x['Country_Region']\n    return place_id\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_traintest['place_id']=df_traintest.apply(lambda x:genplace_id(x),axis=1)\nprint(\"地区个数==>\"+str(len(df_traintest['place_id'].unique())))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## 将时间类型转换：\ndf_traintest['Date']=pd.to_datetime(df_traintest['Date'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## 生成名为dayofyear的\"day\"变量\ndf_traintest[\"day\"]=df_traintest[\"Date\"].apply(lambda x: x.dayofyear).astype(np.int16)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## 生成疫情变化速率变量\nplaces=df_traintest['place_id'].unique()\ndf_traintest2=df_traintest.copy()\ndf_traintest2[\"cases/day\"]=0\ndf_traintest2[\"fatal/day\"]=0\nfor place in places:\n    tmp=df_traintest2[\"ConfirmedCases\"][df_traintest2[\"place_id\"]==place].values\n    tmp[1:]=tmp[1:]-tmp[:-1]   #####################每天的新增确诊数目是隔日之差\n    df_traintest2[\"cases/day\"][df_traintest2[\"place_id\"]==place]=tmp\n    tmp=df_traintest2[\"Fatalities\"][df_traintest2[\"place_id\"]==place].values\n    tmp[1:]=tmp[1:]-tmp[:-1]\n    df_traintest2[\"fatal/day\"][df_traintest2[\"place_id\"]==place]=tmp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_traintest2[df_traintest2[\"place_id\"]==\"China/Hubei\"].head() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import copy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def df_aggregation(df,col,mean_range):\n    df_new=copy.deepcopy(df)  \n    col_new='{}-({}-{})'.format(col,mean_range[0],mean_range[1])###############\n    df_new[col_new]=0\n    tmp=df_new[col].rolling(mean_range[1]-mean_range[0]+1).mean() #################都是每7天滚动求一次均值\n    df_new[col_new][mean_range[0]:]=tmp[:-(mean_range[0])]    ##################手动延后时间序列\n    df_new[col_new][pd.isna(df_new[col_new])]=0       \n    return df_new[[col_new]].reset_index(drop=True)  #####################完全把原来的索引丢弃掉，设立新的数字索引","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def do_aggregations(df):\n    df=(pd.concat([df,df_aggregation(df,'cases/day',[1,1])],axis=1)).reset_index(drop=True)\n    df=(pd.concat([df,df_aggregation(df,'cases/day',[1,7])],axis=1)).reset_index(drop=True)\n    df=(pd.concat([df,df_aggregation(df,'cases/day',[8,14])],axis=1)).reset_index(drop=True)\n    df=(pd.concat([df,df_aggregation(df,'cases/day',[15,21])],axis=1)).reset_index(drop=True)\n    df=(pd.concat([df,df_aggregation(df,'fatal/day',[1,1])],axis=1)).reset_index(drop=True)\n    df=(pd.concat([df,df_aggregation(df,'fatal/day',[1,7])],axis=1)).reset_index(drop=True)\n    df=(pd.concat([df,df_aggregation(df,'fatal/day',[8,14])],axis=1)).reset_index(drop=True)\n    df=(pd.concat([df,df_aggregation(df,'fatal/day',[15,21])],axis=1)).reset_index(drop=True)\n    for threshold in[1,10,100]: ################设立不同的阈值求和\n        days_under_threshold=(df['ConfirmedCases']<threshold).sum()\n        tmp=df[\"day\"]-22-days_under_threshold\n        tmp[tmp<0]=0                            ###########照顾到22号之前已经爆发的地区，比如中国湖北\n        df[\"days_since_{}cases\".format(threshold)]=tmp\n    for threshold in [1, 10, 100]:\n        days_under_threshold = (df['Fatalities']<threshold).sum()\n        tmp = df['day'].values - 22 - days_under_threshold\n        tmp[tmp<=0] = 0\n        df['days_since_{}fatal'.format(threshold)] = tmp\n    if df['place_id'][0]=='China/Hubei':             #################湖北爆发时间比其他地区早，需要特别调整\n        df['days_since_1cases'] += 35 # 2019/12/8\n        df['days_since_10cases'] += 35-13 # 2019/12/8-2020/1/2 assume 2019/12/8+13\n        df['days_since_100cases'] += 4 # 2020/1/18\n        df['days_since_1fatal'] += 13 # 2020/1/9\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## 生成趋势数据\ndf_traintest3=[]\nfor place in places:\n    df_tmp=df_traintest2[df_traintest2[\"place_id\"]==place].reset_index(drop=True)\n    df_tmp=do_aggregations(df_tmp)\n    df_traintest3.append(df_tmp)\ndf_traintest3=pd.concat(df_traintest3).reset_index(drop=True)\ndf_traintest3[df_traintest3[\"place_id\"]==\"China/Hubei\"].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 引入国家信息数据","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_countryinfo=pd.read_csv(\"../input/countryinfo/covid19countryinfo.csv\")[[\"country\",\"pop\",\"tests\",\"testpop\",\"density\",\"medianage\",\"urbanpop\",\"quarantine\",\"schools\",\"hospibed\",\"smokers\",\"sex0\",\"sex14\",\"sex25\",\"sex54\",\"sex64\",\"sex65plus\",\"sexratio\",\"lung\",\"femalelung\",\"malelung\"]]\ndf_countryinfo.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_countryinfo[\"femalelung\"][df_countryinfo[\"country\"]==\"China\"] ####################中国，中国香港，中国澳门，湖北特别疫情地区","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_countryinfo[\"femalelung\"][df_countryinfo[\"country\"]==\"China\"]=56.35\ndf_countryinfo[\"femalelung\"][df_countryinfo[\"country\"]==\"China\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_countryinfo[\"Country_Region\"]=df_countryinfo[\"country\"]\ndf_countryinfo=df_countryinfo[df_countryinfo[\"country\"].duplicated()==False]    ############把不同的称呼的同一地区合并","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df_countryinfo[df_countryinfo['country'].duplicated()].shape)    ############确认无重复\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_traintest4=pd.merge(df_traintest3,df_countryinfo.drop([\"country\",\"testpop\",\"tests\"],axis=1),on=[\"Country_Region\"],how=\"left\")\nprint(df_traintest4.shape)\ndf_traintest4.head()\ndf_traintest4[df_traintest4[\"Country_Region\"]==\"China\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## 引入吸烟数据","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"smoke_path=r\"../input/smokingstats/share-of-adults-who-smoke.csv\"\ndf_smoking=pd.read_csv(smoke_path)\ndf_smoking.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_smoking_recent=df_smoking.sort_values(\"Year\",ascending=False).reset_index(drop=True) \n####################同个地区多个年份的数据重复\ndf_smoking_recent=df_smoking_recent[df_smoking_recent[\"Entity\"].duplicated()==False]\n###########################改了两列的名字,没什么大变动,方便之后的连接\ndf_smoking_recent['Country_Region'] = df_smoking_recent['Entity']\ndf_smoking_recent['SmokingRate'] = df_smoking_recent['Smoking prevalence, total (ages 15+) (% of adults)']\ndf_smoking_recent.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_traintest5 = pd.merge(df_traintest4,df_smoking_recent[[\"Country_Region\",\"SmokingRate\"]],on=\"Country_Region\",how=\"left\")\nprint(df_traintest5.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_traintest5[df_traintest4[\"place_id\"]==\"China/Hubei\"].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##使用世界平均值填补空值的抽烟率\nSmokingRate=df_smoking_recent[\"SmokingRate\"][df_smoking_recent[\"Entity\"]==\"World\"].values[0]\ndf_traintest5[\"SmokingRate\"][pd.isna(df_traintest5[\"SmokingRate\"])]=SmokingRate\ndf_traintest5.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## 引入国家经济水平数据","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"smoke_path=r\"../input/smokingstats/WEO.csv\"\ndf_weo=pd.read_csv(smoke_path)\ndf_weo.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df_weo['Subject Descriptor'].unique()) ## 查看包含的经济描述项目","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## 取国家2019的经济数据然后将数据横竖变换\nsubs=df_weo[\"Subject Descriptor\"].unique()[:-1]  ###########去掉最后一个空缺的经济指标\ndf_weo_agg=df_weo[[\"Country\"]][df_weo[\"Country\"].duplicated()==False].reset_index(drop=True)\nfor sub in subs[:]:\n    df_tmp=df_weo[[\"Country\",\"2019\"]][df_weo[\"Subject Descriptor\"]==sub].reset_index(drop=True) \n    df_tmp=df_tmp[df_tmp[\"Country\"].duplicated()==False].reset_index(drop=True)\n    df_tmp.columns=[\"Country\",sub]          ##############把表头的2019改了\n    df_weo_agg=df_weo_agg.merge(df_tmp,on=\"Country\",how=\"left\")\ndf_weo_agg.columns=[\"\".join(c if c.isalnum() else \"_\" for c in str(x))for x in df_weo_agg.columns]\ndf_weo_agg.columns\ndf_weo_agg['Country_Region'] = df_weo_agg['Country']\ndf_weo_agg.head()        #####################各个经济指标的数据","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_traintest6 = pd.merge(df_traintest5, df_weo_agg, on='Country_Region', how='left')\nprint(df_traintest6.shape)\ndf_traintest6.head()      ##################### 将经济数据与上一版本数据合并","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## 引入平均寿命数据","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"life_path=r\"../input/smokingstats/Life expectancy at birth.csv\"\ndf_life=pd.read_csv(life_path)\ndf_life.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tmp=df_life.iloc[:,1].values.tolist()\ndf_life=df_life[[\"Country\",\"2018\"]]\ndef func(x):\n    try:\n        x_new=float(x.replace(\",\",\"\"))\n    except:\n        print(x)\n        x_new=np.nan \n    return x_new\ndf_life[\"2018\"]=df_life[\"2018\"].apply(lambda x:func(x))\ndf_life.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_life = df_life[['Country', '2018']]\ndf_life.columns = ['Country_Region', 'LifeExpectancy']  #############表头转换","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_traintest7 = pd.merge(df_traintest6, df_life, on='Country_Region', how='left') ##再次合并\nprint(len(df_traintest7))\ndf_traintest7.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def encode_label(df, col, freq_limit=0):\n    df[col][pd.isna(df[col])] = 'nan'\n    tmp = df[col].value_counts()\n    cols = tmp.index.values               ######################cols是索引值\n    freq = tmp.values                   ######################freq是对应的值\n    num_cols = (freq>=freq_limit).sum()\n    print(\"col: {}, num_cat: {}, num_reduced: {}\".format(col, len(cols), num_cols))\n    col_new = '{}_le'.format(col)\n    df_new = pd.DataFrame(np.ones(len(df), np.int16)*(num_cols-1), columns=[col_new])\n    for i, item in enumerate(cols[:num_cols]):\n        df_new[col_new][df[col]==item] = i\n    return df_new\n\ndef get_df_le(df, col_index, col_cat):\n    df_new = df[[col_index]]\n    for col in col_cat:\n        df_tmp = encode_label(df, col)\n        df_new = pd.concat([df_new, df_tmp], axis=1)\n    return df_new\n\ndf_traintest7['id'] = np.arange(len(df_traintest7))\ndf_le = get_df_le(df_traintest7, 'id', ['Country_Region', 'Province_State'])\ndf_traintest8 = pd.merge(df_traintest7, df_le, on='id', how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_traintest8['cases/day'] = df_traintest8['cases/day'].astype(np.float)\ndf_traintest8['fatal/day'] = df_traintest8['fatal/day'].astype(np.float)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 转换数据类型从对象至float\ndef func(x):\n    x_new = 0\n    try:\n        x_new = float(x.replace(\",\", \"\"))\n    except:\n        x_new = np.nan\n    return x_new\ncols = [\n    'Gross_domestic_product__constant_prices', \n    'Gross_domestic_product__current_prices', \n    'Gross_domestic_product__deflator', \n    'Gross_domestic_product_per_capita__constant_prices', \n    'Gross_domestic_product_per_capita__current_prices', \n    'Output_gap_in_percent_of_potential_GDP', \n    'Gross_domestic_product_based_on_purchasing_power_parity__PPP__valuation_of_country_GDP', \n    'Gross_domestic_product_based_on_purchasing_power_parity__PPP__per_capita_GDP', \n    'Gross_domestic_product_based_on_purchasing_power_parity__PPP__share_of_world_total', \n    'Implied_PPP_conversion_rate', 'Total_investment', \n    'Gross_national_savings', 'Inflation__average_consumer_prices', \n    'Inflation__end_of_period_consumer_prices', \n    'Six_month_London_interbank_offered_rate__LIBOR_', \n    'Volume_of_imports_of_goods_and_services', \n    'Volume_of_Imports_of_goods', \n    'Volume_of_exports_of_goods_and_services', \n    'Volume_of_exports_of_goods', 'Unemployment_rate', 'Employment', 'Population', \n    'General_government_revenue', 'General_government_total_expenditure', \n    'General_government_net_lending_borrowing', 'General_government_structural_balance', \n    'General_government_primary_net_lending_borrowing', 'General_government_net_debt', \n    'General_government_gross_debt', 'Gross_domestic_product_corresponding_to_fiscal_year__current_prices', \n    'Current_account_balance', 'pop'\n]\nfor col in cols:\n    df_traintest8[col] = df_traintest8[col].apply(lambda x: func(x))  \nprint(df_traintest8['pop'].dtype)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## 补充给地区标签改成数值类型\ni=0\ndf_traintest8['place_label']=0\n\nplaces=df_traintest8['place_id'].unique()\nfor place in places:\n    df_traintest8['place_label'][df_traintest8['place_id']==place]=i\n    i=i+1\nprint(df_traintest8['place_label'].unique())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_traintest8[df_traintest8['place_id']=='China/Hubei'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_list=[    'Gross_domestic_product__constant_prices',\n    'Gross_domestic_product__current_prices',\n    'Gross_domestic_product__deflator',\n    'Gross_domestic_product_per_capita__constant_prices',\n    'Gross_domestic_product_per_capita__current_prices',\n    'Output_gap_in_percent_of_potential_GDP',\n    'Gross_domestic_product_based_on_purchasing_power_parity__PPP__valuation_of_country_GDP',\n    'Gross_domestic_product_based_on_purchasing_power_parity__PPP__per_capita_GDP',\n    'Gross_domestic_product_based_on_purchasing_power_parity__PPP__share_of_world_total',\n    'Implied_PPP_conversion_rate', 'Total_investment',\n    'Gross_national_savings', 'Inflation__average_consumer_prices',\n    'Inflation__end_of_period_consumer_prices',\n    'Six_month_London_interbank_offered_rate__LIBOR_',\n    'Volume_of_imports_of_goods_and_services', 'Volume_of_Imports_of_goods',\n    'Volume_of_exports_of_goods_and_services', 'Volume_of_exports_of_goods',\n    'Unemployment_rate',                            ###############失业率\n    'Employment', 'Population',\n    'General_government_revenue', 'General_government_total_expenditure',\n    'General_government_net_lending_borrowing',\n    'General_government_structural_balance',\n    'General_government_primary_net_lending_borrowing',\n    'General_government_net_debt', 'General_government_gross_debt',\n    'Gross_domestic_product_corresponding_to_fiscal_year__current_prices',\n    'Current_account_balance', \n    'LifeExpectancy',\n    'pop',\n    'density', \n    'medianage', \n    'urbanpop', \n    'hospibed', 'smokers']\n## 将df_train中空的且是数值类型的数据用中位数填补\ntemp1=[]\n\n\nfor column in temp_list:\n    try:\n        mean=df_traintest8[column].mean()\n        temp1.append(column)\n    except:\n        print(column+\"不是数值类型\")\n\nfor col in temp1:\n    df_traintest8[col].fillna(df_traintest8[col].mean(),inplace=True)\n    \ndf_traintest8[temp_list].isnull()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_traintest8[temp_list].isnull().sum() ##仍有两各属性存在空值，之后不使用这两各属性","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## 输出预测用数据\ndf_traintest8.to_csv(\"data_prepared.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 模型训练","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# 模型参数：class sklearn.ensemble.RandomForestRegressor(n_estimators=10, criterion='mse', \n# max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, \n# max_features='auto',max_leaf_nodes=None, bootstrap=True, oob_score=False, n_jobs=1, \n# random_state=None, verbose=0, warm_start=False)\n\n# 一些重要参数：\n# n_estimators:森林中树木的数量——越多越好\n# criterion: 分裂时候的决策算法：mse:均方误差。只支持mse。如果是RandomForestClassifier还支持gini等\n# max_features: 选取特征时候抽取的数量。可以是（int,float,string）可以是一个数目，或者sqrt表示所有特征取方根，auto表示max_features=n_features\n# min_samples_leaf: 树木叶子节点最少包含的样本数目\n# n_jobs: 表示并行的进程数目\n\n# 更详尽的连接：\n#http://lijiancheng0614.github.io/scikit-learn/modules/generated/sklearn.ensemble.RandomForestRegressor.html","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_traintest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train model to predict fatalities/day\ncol_target = 'fatal/day'\ncol_var = ['day',\n     'place_label',                 #####保留地区标签？\n    'days_since_1cases', \n    'days_since_10cases', \n    'days_since_100cases',\n    'days_since_1fatal', \n    'days_since_10fatal', 'days_since_100fatal',\n ################################下面是一组十分重要的变量，我认为他在拟合不同的传染阶段，人为调整爆发期\n    'cases/day-(1-1)',  ####################第一天的确诊人数\n    'cases/day-(1-7)',  ################从第七天开始统计的滚动确诊每日平均数目\n    'cases/day-(8-14)',   #################从第八天开始统计的滚动确诊每日平均数目\n    'cases/day-(15-21)',  ##################同理\n    'fatal/day-(1-1)', \n    'fatal/day-(1-7)',         #########同理\n    'fatal/day-(8-14)', \n    'fatal/day-(15-21)', \n    'SmokingRate',\n    'Gross_domestic_product__constant_prices',\n    'Gross_domestic_product__current_prices',\n    'Gross_domestic_product__deflator',\n    'Gross_domestic_product_per_capita__constant_prices',\n    'Gross_domestic_product_per_capita__current_prices',\n    'Output_gap_in_percent_of_potential_GDP',\n    'Gross_domestic_product_based_on_purchasing_power_parity__PPP__valuation_of_country_GDP',\n    'Gross_domestic_product_based_on_purchasing_power_parity__PPP__per_capita_GDP',\n    'Gross_domestic_product_based_on_purchasing_power_parity__PPP__share_of_world_total',\n    'Implied_PPP_conversion_rate', 'Total_investment',\n    'Gross_national_savings', 'Inflation__average_consumer_prices',\n    'Inflation__end_of_period_consumer_prices',\n#     'Six_month_London_interbank_offered_rate__LIBOR_', ##数据质量过差，不适用抛弃\n    'Volume_of_imports_of_goods_and_services', 'Volume_of_Imports_of_goods',\n    'Volume_of_exports_of_goods_and_services', 'Volume_of_exports_of_goods',\n    'Unemployment_rate',                            ###############失业率\n#     'Employment',  ##数据质量过差，不适用抛弃\n    'Population',\n    'General_government_revenue', 'General_government_total_expenditure',\n    'General_government_net_lending_borrowing',\n    'General_government_structural_balance',\n    'General_government_primary_net_lending_borrowing',\n    'General_government_net_debt', 'General_government_gross_debt',\n    'Gross_domestic_product_corresponding_to_fiscal_year__current_prices',\n    'Current_account_balance', \n    'LifeExpectancy',\n    'pop',\n    'density', \n    'medianage', \n    'urbanpop', \n    'hospibed', 'smokers'\n]\ncol_cat = []\n#####################当然是划分训练集和验证集\n#####################日期序号为93天前的数据作为训练集合\n#####################日期学号为92-107天的数据作为验证集合\ndf_train = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']<93)] #4/2之前\ndf_valid = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']>92) & (df_traintest8['day']<107)]\ndf_test = df_traintest8[pd.isna(df_traintest8['ForecastId'])==False]\nX_train = df_train[col_var]\nX_valid = df_valid[col_var]\n#######################为什么要取对数：取对数可以缩小特别大特别小的数据之间的差别，比如1和e10,相当于排除了特殊点？\n#######################最后得到的最佳参数肯定是不变的\ny_train = np.log(df_train[col_target].values.clip(0, 1e10)+1)\ny_valid = np.log(df_valid[col_target].values.clip(0, 1e10)+1)\n\n\n\nrf=RandomForestRegressor(n_estimators=600)\nrf.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tmp = pd.DataFrame() ## 发现重要的是趋势数据\ntmp[\"feature\"] = col_var\ntmp[\"importance\"] = rf.feature_importances_\ntmp = tmp.sort_values('importance', ascending=False)\ntmp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntmp[0:13].sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## 评分良好暂时不需要调参\nprint(rf.score(X_train,y_train))\nprint(rf.score(X_valid,y_valid))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##根据特征显著性数量级保留前10样特征重新训练","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col_var2=tmp['feature'][0:13].tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col_var2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train model to predict caes/day\n\ncol_cat = []\n#####################当然是划分训练集和验证集\n#####################日期序号为93天前的数据作为训练集合\n#####################日期学号为92-107天的数据作为验证集合\ndf_train = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']<93)] #4/2之前\ndf_valid = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']>92) & (df_traintest8['day']<107)]\ndf_test = df_traintest8[pd.isna(df_traintest8['ForecastId'])==False]\nX_train = df_train[col_var2]\nX_valid = df_valid[col_var2]\n#######################为什么要取对数：取对数可以缩小特别大特别小的数据之间的差别，比如1和e10,相当于排除了特殊点？\n#######################最后得到的最佳参数肯定是不变的\ny_train = np.log(df_train[col_target].values.clip(0, 1e10)+1)\ny_valid = np.log(df_valid[col_target].values.clip(0, 1e10)+1)\n\n\n\nrf2=RandomForestRegressor(n_estimators=600)\nrf2.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## 评分良好暂时不需要调参\nprint(rf2.score(X_train,y_train))\nprint(rf2.score(X_valid,y_valid))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tmp2 = pd.DataFrame() ## 发现重要的是趋势数据\ntmp2[\"feature\"] = col_var2\ntmp2[\"importance\"] = rf2.feature_importances_\ntmp2 = tmp2.sort_values('importance', ascending=False)\ntmp2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## 预测case/day","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train model to predict fatalities/day\ncol_target = 'cases/day'\ncol_var = ['day',\n     'place_label',                 #####保留地区标签？\n    'days_since_1cases', \n    'days_since_10cases', \n    'days_since_100cases',\n    'days_since_1fatal', \n    'days_since_10fatal', 'days_since_100fatal',\n ################################下面是一组十分重要的变量，我认为他在拟合不同的传染阶段，人为调整爆发期\n    'cases/day-(1-1)',  ####################第一天的确诊人数\n    'cases/day-(1-7)',  ################从第七天开始统计的滚动确诊每日平均数目\n    'cases/day-(8-14)',   #################从第八天开始统计的滚动确诊每日平均数目\n    'cases/day-(15-21)',  ##################同理\n    'fatal/day-(1-1)', \n    'fatal/day-(1-7)',         #########同理\n    'fatal/day-(8-14)', \n    'fatal/day-(15-21)', \n    'SmokingRate',\n    'Gross_domestic_product__constant_prices',\n    'Gross_domestic_product__current_prices',\n    'Gross_domestic_product__deflator',\n    'Gross_domestic_product_per_capita__constant_prices',\n    'Gross_domestic_product_per_capita__current_prices',\n    'Output_gap_in_percent_of_potential_GDP',\n    'Gross_domestic_product_based_on_purchasing_power_parity__PPP__valuation_of_country_GDP',\n    'Gross_domestic_product_based_on_purchasing_power_parity__PPP__per_capita_GDP',\n    'Gross_domestic_product_based_on_purchasing_power_parity__PPP__share_of_world_total',\n    'Implied_PPP_conversion_rate', 'Total_investment',\n    'Gross_national_savings', 'Inflation__average_consumer_prices',\n    'Inflation__end_of_period_consumer_prices',\n#     'Six_month_London_interbank_offered_rate__LIBOR_', ##数据质量过差，不适用抛弃\n    'Volume_of_imports_of_goods_and_services', 'Volume_of_Imports_of_goods',\n    'Volume_of_exports_of_goods_and_services', 'Volume_of_exports_of_goods',\n    'Unemployment_rate',                            ###############失业率\n#     'Employment',  ##数据质量过差，不适用抛弃\n    'Population',\n    'General_government_revenue', 'General_government_total_expenditure',\n    'General_government_net_lending_borrowing',\n    'General_government_structural_balance',\n    'General_government_primary_net_lending_borrowing',\n    'General_government_net_debt', 'General_government_gross_debt',\n    'Gross_domestic_product_corresponding_to_fiscal_year__current_prices',\n    'Current_account_balance', \n    'LifeExpectancy',\n    'pop',\n    'density', \n    'medianage', \n    'urbanpop', \n    'hospibed', 'smokers'\n]\ncol_cat = []\n#####################当然是划分训练集和验证集\n#####################日期序号为93天前的数据作为训练集合\n#####################日期学号为92-107天的数据作为验证集合\ndf_train = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']<93)] #4/2之前\ndf_valid = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']>92) & (df_traintest8['day']<107)]\ndf_test = df_traintest8[pd.isna(df_traintest8['ForecastId'])==False]\nX_train = df_train[col_var]\nX_valid = df_valid[col_var]\n#######################为什么要取对数：取对数可以缩小特别大特别小的数据之间的差别，比如1和e10,相当于排除了特殊点？\n#######################最后得到的最佳参数肯定是不变的\ny_train = np.log(df_train[col_target].values.clip(0, 1e10)+1)\ny_valid = np.log(df_valid[col_target].values.clip(0, 1e10)+1)\n#######################lgb封装自身的训练集\n# train_data = lgb.Dataset(X_train, label=y_train, categorical_feature=col_cat)\n# valid_data = lgb.Dataset(X_valid, label=y_valid, categorical_feature=col_cat)\n# num_round = 15000\n# model = lgb.train(params, train_data, num_round, valid_sets=[train_data, valid_data],\n#                   verbose_eval=100, ###########每100次输出一次评测结果\n#                   early_stopping_rounds=150,)      #############如果连续150轮都无法优化，那么就提前停下\n\n# best_itr = model.best_iteration\n\n\nrf3=RandomForestRegressor(n_estimators=600)\nrf3.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## 评分良好暂时不需要调参\nprint(rf3.score(X_train,y_train))\nprint(rf3.score(X_valid,y_valid))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tmp3 = pd.DataFrame() ## 发现重要的是趋势数据\ntmp3[\"feature\"] = col_var\ntmp3[\"importance\"] = rf3.feature_importances_\ntmp3 = tmp3.sort_values('importance', ascending=False)\ntmp3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tmp3[0:13].sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## 保留前30的特征\ncol_var2=tmp3['feature'][0:13].tolist()\ncol_var2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#重新训练","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train model to predict caess/day again\ncol_target = 'cases/day'\ncol_cat = []\n#####################当然是划分训练集和验证集\n#####################日期序号为93天前的数据作为训练集合\n#####################日期学号为92-107天的数据作为验证集合\ndf_train = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']<93)] #4/2之前\ndf_valid = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']>92) & (df_traintest8['day']<107)]\ndf_test = df_traintest8[pd.isna(df_traintest8['ForecastId'])==False]\nX_train = df_train[col_var2]\nX_valid = df_valid[col_var2]\n#######################为什么要取对数：取对数可以缩小特别大特别小的数据之间的差别，比如1和e10,相当于排除了特殊点？\n#######################最后得到的最佳参数肯定是不变的\ny_train = np.log(df_train[col_target].values.clip(0, 1e10)+1)\ny_valid = np.log(df_valid[col_target].values.clip(0, 1e10)+1)\n\n\n\nrf4=RandomForestRegressor(n_estimators=600)\nrf4.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## 评分良好暂时不需要调参\nprint(rf4.score(X_train,y_train))\nprint(rf4.score(X_valid,y_valid))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_traintest8[df_traintest8['day']==93]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 开始预测","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_tmp = df_traintest8[(df_traintest8['day']<93) | (pd.isna(df_traintest8['ForecastId'])==False)].reset_index(drop=True)\n################################删除之前聚合的数据项\ndf_tmp = df_tmp.drop([\n    'cases/day-(1-1)', 'cases/day-(1-7)', 'cases/day-(8-14)', 'cases/day-(15-21)', \n    'fatal/day-(1-1)', 'fatal/day-(1-7)', 'fatal/day-(8-14)', 'fatal/day-(15-21)',\n    'days_since_1cases', 'days_since_10cases', 'days_since_100cases',\n    'days_since_1fatal', 'days_since_10fatal', 'days_since_100fatal',\n                               ],  axis=1)\ndf_traintest9 = []\n######重新聚合\nfor i, place in enumerate(places[:]):\n    df_tmp2 = df_tmp[df_tmp['place_id']==place].reset_index(drop=True)\n    df_tmp2 = do_aggregations(df_tmp2)\n    df_traintest9.append(df_tmp2)\ndf_traintest9 = pd.concat(df_traintest9).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_tmp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col_target1='fatal/day'\ncol_target='cases/day'\n\ncol_var1=tmp['feature'][0:13].tolist()\ncol_var2=tmp3['feature'][0:13].tolist()\n\nprint(col_var1)\nprint(col_var2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###############注意啊，这是什么模型的曲线?\n#############这当然是确诊数目的曲线，但是确诊我训练了两个模型\n########这是哪个模型？\n#########这是我们拟合72天内的模型啊\n##########可以看到72天内效果及其优秀，72天后呵呵\nplace = 'Japan'\ndf_interest_base = df_traintest9[df_traintest9['place_id']==place].reset_index(drop=True)\ndf_interest = copy.deepcopy(df_interest_base)\ndf_interest['ConfirmedCases'] = df_interest['ConfirmedCases'].astype(np.float)\ndf_interest['cases/day'] = df_interest['cases/day'].astype(np.float)\ndf_interest['fatal/day'] = df_interest['fatal/day'].astype(np.float)\ndf_interest['Fatalities'] = df_interest['Fatalities'].astype(np.float)\ndf_interest['cases/day'][df_interest['day']>92] = -1\ndf_interest['fatal/day'][df_interest['day']>92] = -1\nlen_known = (df_interest['cases/day']!=-1).sum()\nlen_unknown = (df_interest['cases/day']==-1).sum()\nprint(\"len train: {}, len prediction: {}\".format(len_known, len_unknown))\nX_valid = df_interest[col_var1][df_interest['day']>92]\n\nX_valid2 = df_interest[col_var2][df_interest['day']>92]\npred_f =  np.exp(rf2.predict(X_valid))-1\npred_c = np.exp(rf4.predict(X_valid2))-1\ndf_interest['fatal/day'][df_interest['day']>92] = pred_f.clip(0, 1e10)\ndf_interest['cases/day'][df_interest['day']>92] = pred_c.clip(0, 1e10)\ndf_interest['Fatalities'] = np.cumsum(df_interest['fatal/day'].values)\ndf_interest['ConfirmedCases'] = np.cumsum(df_interest['cases/day'].values)\nfor j in range(len_unknown): # use predicted cases and fatal for next days' prediction\n    X_valid =np.array(df_interest[col_var1].iloc[j+len_known]).reshape(1,-1)\n    X_valid2 = np.array(df_interest[col_var2].iloc[j+len_known]).reshape(1,-1) #就算只有一个样本也要转换成2d矩阵\n    pred_f = rf2.predict(X_valid)\n    pred_c = rf4.predict(X_valid2)\n    pred_c = (np.exp(pred_c)-1).clip(0, 1e10)\n    pred_f = (np.exp(pred_f)-1).clip(0, 1e10)\n    df_interest['fatal/day'][j+len_known] = pred_f\n    df_interest['cases/day'][j+len_known] = pred_c\n    df_interest['Fatalities'][j+len_known] = df_interest['Fatalities'][j+len_known-1] + pred_f\n    df_interest['ConfirmedCases'][j+len_known] = df_interest['ConfirmedCases'][j+len_known-1] + pred_c\n    df_interest = df_interest.drop([\n        'cases/day-(1-1)', 'cases/day-(1-7)', 'cases/day-(8-14)', 'cases/day-(15-21)', \n        'fatal/day-(1-1)', 'fatal/day-(1-7)', 'fatal/day-(8-14)', 'fatal/day-(15-21)', \n        'days_since_1cases', 'days_since_10cases', 'days_since_100cases',\n        'days_since_1fatal', 'days_since_10fatal', 'days_since_100fatal',], axis=1)\n    df_interest = do_aggregations(df_interest.reset_index(drop=True))\n# visualize\n\ntmp = df_interest['cases/day'].values\ntmp = np.cumsum(tmp)\nsns.lineplot(x=df_interest_base['day'], y=tmp, label='pred')\ntmp = df_traintest8['ConfirmedCases'][(df_traintest8['place_id']==place)& (pd.isna(df_traintest8['ForecastId']))].values\nprint(len(tmp), tmp)\nsns.lineplot(x=df_traintest8['day'][(df_traintest8['place_id']==place)& (pd.isna(df_traintest8['ForecastId']))].values,\n             y=tmp, label='true')\nprint(place)\nplt.title(place)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## 预测文件生成","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predict test data in public\nday_before_public = 92\ndf_preds = []\nfor i, place in enumerate(places[:]):\n#     if place!='Japan' and place!='Afghanistan' :continue\n    df_interest = copy.deepcopy(df_traintest9[df_traintest9['place_id']==place].reset_index(drop=True))\n    df_interest['cases/day'][(pd.isna(df_interest['ForecastId']))==False] = -1\n    df_interest['fatal/day'][(pd.isna(df_interest['ForecastId']))==False] = -1\n    len_known = (df_interest['day']<=day_before_public).sum()\n    len_unknown = (day_before_public<df_interest['day']).sum()\n    for j in range(len_unknown): # use predicted cases and fatal for next days' prediction\n        X_valid = np.array(df_interest[col_var1].iloc[j+len_known]).reshape(1,-1)\n        X_valid2 = np.array(df_interest[col_var2].iloc[j+len_known]).reshape(1,-1)\n        pred_f = rf2.predict(X_valid)\n        pred_c = rf4.predict(X_valid2)\n        pred_c = (np.exp(pred_c)-1).clip(0, 1e10)\n        pred_f = (np.exp(pred_f)-1).clip(0, 1e10)\n        df_interest['fatal/day'][j+len_known] = pred_f\n        df_interest['cases/day'][j+len_known] = pred_c\n        df_interest['Fatalities'][j+len_known] = df_interest['Fatalities'][j+len_known-1] + pred_f\n        df_interest['ConfirmedCases'][j+len_known] = df_interest['ConfirmedCases'][j+len_known-1] + pred_c\n        df_interest = df_interest.drop([\n            'cases/day-(1-1)', 'cases/day-(1-7)', 'cases/day-(8-14)', 'cases/day-(15-21)', \n            'fatal/day-(1-1)', 'fatal/day-(1-7)', 'fatal/day-(8-14)', 'fatal/day-(15-21)',\n            'days_since_1cases', 'days_since_10cases', 'days_since_100cases',\n            'days_since_1fatal', 'days_since_10fatal', 'days_since_100fatal',\n\n                                       ],  axis=1)\n        df_interest = do_aggregations(df_interest)\n    if (i+1)%10==0:\n        print(\"{:3d}/{}  {}, len known: {}, len unknown: {}\".format(i+1, len(places), place, len_known, len_unknown), df_interest.shape)\n    df_interest['fatal_pred'] = np.cumsum(df_interest['fatal/day'].values)\n    df_interest['cases_pred'] = np.cumsum(df_interest['cases/day'].values)\n    df_preds.append(df_interest)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# concat prediction\ndf_preds= pd.concat(df_preds)\ndf_preds = df_preds.sort_values('day')\ncol_tmp = ['place_id', 'ForecastId', 'day', 'cases/day', 'cases_pred', 'fatal/day', 'fatal_pred',]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_preds.to_csv(\"df_preds.csv\", index=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load sample submission\ndf_sub = pd.read_csv(\"../input/covid19-global-forecasting-week-4/submission.csv\")\nprint(len(df_sub))\ndf_sub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# merge prediction with sub\ndf_sub = pd.merge(df_sub, df_traintest3[['ForecastId', 'place_id', 'day']])\ndf_sub = pd.merge(df_sub, df_preds[['place_id', 'day', 'cases_pred', 'fatal_pred']], on=['place_id', 'day',], how='left')\ndf_sub.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sub['ConfirmedCases'] = df_sub['cases_pred']\ndf_sub['Fatalities'] = df_sub['fatal_pred']\ndf_sub = df_sub[['ForecastId', 'ConfirmedCases', 'Fatalities']]\ndf_sub.to_csv(\"submission.csv\", index=None)\ndf_sub.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}