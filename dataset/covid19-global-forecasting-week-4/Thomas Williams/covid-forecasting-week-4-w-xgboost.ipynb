{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 0. Discussion of approach"},{"metadata":{},"cell_type":"markdown","source":"# 1. Import required libraries"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import datetime\nimport pdb\nimport xgboost as xgb\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.preprocessing import MinMaxScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split, cross_val_score, KFold\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Define functions to simplify processing"},{"metadata":{"trusted":true},"cell_type":"code","source":"verbose = False\nloop_logic = True    # loop on country/state in main program\nscale_data = False   # scale data with MinMaxScaler\nuse_base_model = True  # use base model throughout\none_hot_encode = False  # use one-hot encoding or label to integer encoding\nestimators=5000\n# only submit predictions up to last date in test set during public leaderboard period\npublic_leaderboard_end_date = None # run for final submission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fix_data_issues(df):\n    df['Province_State'] = np.where(df['Province_State'].isnull(), df['Country_Region'], df['Province_State']) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_test_train_for_country_state(one_hot_encode_flag, df_train, df_test, country, state):\n    if one_hot_encode_flag == True:\n        cs_train = df_train[(df_train['Country_Region_'+country] == 1) & (df_train['Province_State_'+state] == 1) ]\n        cs_test = df_test[(df_test['Country_Region_'+country] == 1) & (df_test['Province_State_'+state] == 1) ]\n    else:\n        cs_train = df_train[(df_train['Country_Region'] == country) & (df_train['Province_State'] == state) ]\n        cs_test = df_test[(df_test['Country_Region'] == country) & (df_test['Province_State'] == state) ]\n    \n    return (cs_train, cs_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def transform_dates(df):\n    dates = pd.to_datetime(df['Date']) \n    min_dates = dates.min()\n#    df['Date_Days_Since_Pandemic_Start'] = (dates - min_dates).dt.days\n    df['Date_Year'] = dates.dt.year\n    df['Date_Month'] = dates.dt.month\n    df['Date_Day'] = dates.dt.day\n#    df['Date_Week'] = dates.dt.week\n#    df['Date_DayofWeek'] = dates.dt.dayofweek\n#    df['Date_DayofYear'] = dates.dt.dayofyear\n#    df['Date_WeekofYear'] = dates.dt.weekofyear\n#    df['Date_Quarter'] = dates.dt.quarter\n    df.drop(['Date'], axis=1, inplace=True)   # remove the date column, no longer needed","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def setup_df_encode_and_dates(df, encode_flag, dummy_cols, target_cols=[]):\n    # move country in front of province/state\n    enc_df = df.copy()\n    enc_df = enc_df[[enc_df.columns[0], enc_df.columns[2], enc_df.columns[1],enc_df.columns[3]]]  # 1st column named differently in train vs test\n    \n    if encode_flag == True:\n        enc_df = pd.get_dummies(enc_df, columns=dummy_cols)  # one-hot encoding\n    else:\n        le = LabelEncoder()\n        for dum_col in dummy_cols:\n            enc_df[dum_col] = le.fit_transform(enc_df[dum_col])   # label encoding\n\n    # extract date parts / date descriptors (week, quarter, etc.).  Remove original date variable as it can't be used by NN\n    transform_dates(enc_df)\n\n    for col in target_cols:\n        enc_df[col] = df[col]\n\n    return(enc_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def prepare_train_set(df_train):\n    # break out main body of train set and separate the target variables out\n    train_x, train_target1, train_target2 = df_train.iloc[:, :-2], df_train.iloc[:, -2], df_train.iloc[:, -1]\n#    pdb.set_trace()\n    return(train_x, train_target1, train_target2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def prepare_submission(preds):\n    preds['ForecastId'] = preds['ForecastId'].fillna(0.0).astype('int32')\n    preds['Fatalities'] = preds['Fatalities'].fillna(0.0).astype('int32')\n    preds['ConfirmedCases'] = preds['ConfirmedCases'].fillna(0.0).astype('int32')\n    preds.clip(lower=0, inplace=True)\n    preds.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def model_and_predict(model, X, y, test, estimators=5000):\n    if verbose == True:\n        print(\"Initial model ID in model_and_predict: {0}\".format(id(model)))\n    if model != None:\n        run_model = model\n        if verbose == True:\n            print(\"Running with model id #{0}\".format(id(model)))\n\n    else:\n        run_model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators= estimators)\n        if verbose == True:\n            print(\"Running with new model\")\n\n    if verbose == True:\n        print(\"Model ID in model_and_predict: {0}\".format(id(run_model)))\n    #initial training on 80%/20% train/test split \n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=12345)\n\n#    model = model.fit(X_train, y_train)\n    run_model.fit(X_train, y_train)\n\n#    y_train_pred = model.predict(X_train)\n#    y_test_pred = model.predict(X_test)\n    y_train_pred = run_model.predict(X_train)\n    y_test_pred = run_model.predict(X_test)\n    \n#    print(\"R2: {0:.2f}\".format(r2_score(y_train_pred, y_train)))\n#    pdb.set_trace()\n\n    # now predict using the trained model on all of the test rows\n#    y_pred = model.predict(test)  \n    y_pred = run_model.predict(test)  \n\n    y_pred[y_pred < 0] = 0\n    \n    r2 = r2_score(y_train_pred, y_train, multioutput='variance_weighted')\n\n    return(y_pred, r2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_results(model):\n    # Code based on \"Selecting Optimal Parameters for XGBoost Model Training\" by Andrej Baranovskij (Medium)\n    results = model.evals_result()\n    epochs = len(results['validation_0']['error'])\n    x_axis = range(0, epochs)\n    # plot log loss\n    fig, ax = plt.subplots()\n    ax.plot(x_axis, results['validation_0']['logloss'], label='Train')\n    ax.plot(x_axis, results['validation_1']['logloss'], label='Test')\n    ax.legend()\n    plt.ylabel('Log Loss')\n    plt.title('XGBoost Log Loss')\n    plt.show()\n    # plot classification error\n    fig, ax = pyplot.subplots()\n    ax.plot(x_axis, results['validation_0']['error'], label='Train')\n    ax.plot(x_axis, results['validation_1']['error'], label='Test')\n    ax.legend()\n    plt.ylabel('Classification Error')\n    plt.title('XGBoost Classification Error')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fit_models_and_train(country, state, model, train, test):\n    X, y_cases, y_fatal = prepare_train_set(train)\n    X = X.drop(['Id'], axis=1)    # remove the Id column from the training set to avoid leakage\n\n    forecast_IDs = test.iloc[:,0]  # save the ForecastId column\n\n    test_no_id = test.iloc[:, 1:]   # use the rest of the test set without the ForecastId column\n\n    \n    # apply scaling to train and test set\n    if scale_data == True:\n        scaler = MinMaxScaler()\n        X = scaler.fit_transform(X.values)\n        test_no_id = scaler.transform(test_no_id.values)\n\n    y_cases_pred, cases_r2 = model_and_predict(model, X, y_cases, test_no_id)   # prior version: estimators = 10000, trying default of 2000\n    if verbose == True:\n        print(\"Country {0}, state {1}: cases R2 score: {2:0.2f}.\".format(country, state, cases_r2))\n\n#   pdb.set_trace()\n    \n#    X_train, X_test, y_train, y_test = train_test_split(X, y_fatal, test_size=0.2, random_state=12345)\n    y_fatal_pred, fatal_r2 = model_and_predict(model, X, y_fatal, test_no_id)\n    if verbose == True:\n        print(\"Country {0}, state {1}: fatalities R2 score: {2:0.2f}.\".format(country, state, fatal_r2))\n\n    preds = pd.DataFrame(forecast_IDs)\n    preds['ConfirmedCases'] = y_cases_pred\n    preds['Fatalities'] = y_fatal_pred\n\n    return(preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def cv_model(country, state, train, test):\n    X, y_cases, y_fatal = prepare_train_set(train)\n    X = X.drop(['Id'], axis=1)    # remove the Id column from the training set to avoid leakage\n\n#    forecast_IDs = test.iloc[:,0]  # save the ForecastId column\n\n    X_test = test.iloc[:, 1:]   # use the rest of the test set without the ForecastId column\n\n    data_train_cases_matrix = xgb.DMatrix(data=X, label=y_cases)\n    data_train_fatal_matrix = xgb.DMatrix(data=X, label=y_fatal)\n    \n#    scores = cross_val_score(model, X, y_cases,cv=5, scoring='accuracy')\n#    print(\"Country {0}, state {1}: cases mean cross-validation score: {2:0.2f}.\".format(country, state, scores.mean()))\n\n    cv_results_cases = xgb.cv(dtrain=data_train_cases_matrix, params=parms, nfold=3, num_boost_round=50,\n                   early_stopping_rounds=50,metrics=\"rmse\",as_pandas=True,seed=12345)\n    \n    print(\"Cases RMSE: {0:.2f}.\".format(cv_results_cases['test-rmse-mean'].tail(1).values[0]))\n    \n    cv_results_fatal = xgb.cv(dtrain=data_train_fatal_matrix, params=parms, nfold=3, num_boost_round=50,\n                   early_stopping_rounds=50,metrics=\"rmse\",as_pandas=True,seed=12345)        \n\n    print(\"Fatalities RMSE: {0:.2f}.\".format(cv_results_fatal['test-rmse-mean'].tail(1).values[0]))\n\n#    scores = cross_val_score(model, X, y_fatal,cv=5, scoring='accuracy')\n#    print(\"Country {0}, state {1}: fatalities mean cross-validation score: {2:0.2f}.\".format(country, state, scores.mean()))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Obtain data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get the training data\ndf_train = pd.read_csv('../input/covid19-global-forecasting-week-4/train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get the test data\ndf_test = pd.read_csv('../input/covid19-global-forecasting-week-4/test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Perform Exploratory Data Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Exploratory data analysis\ndf_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fix any known data issues in train and test sets\nfix_data_issues(df_train)\nfix_data_issues(df_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Save original train/test data in case we need it later\ndf_train_original = df_train.copy()\ndf_test_original = df_test.copy()\ndf_train_original['Datetime'] = pd.to_datetime(df_train_original['Date'])\ndf_test_original['Datetime'] = pd.to_datetime(df_test_original['Date'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove overlap dates from train set\ndate_filter = df_train[df_train.Date > \"4/1/2020\"].index\nif not (public_leaderboard_end_date is None):\n    df_train.drop(date_filter, inplace=True)  # remove for final submissions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train[df_train.Date > '2020/04/01']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_original.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Define base model and parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"parms = {'objective' :'reg:squarederror', 'colsample_bytree' : 0.4, 'learning_rate' : 0.01,\n                'max_depth' : 5, 'reg_alpha' : 0.3, 'n_estimators' : 2000 }\nnum_round = 2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# define base XGBoost parameters and model for predictions\n#base_model = xgb.XGBRegressor(objective='reg:squarederror', \n#                         colsample_bytree=0.4, \n#                         learning_rate=0.01,\n#                         max_depth=15, \n#                         reg_alpha=0.3,\n#                         n_estimators= estimators)\n\nbase_model = xgb.XGBRegressor(n_estimators=estimators, random_state=12345, max_depth=15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Model ID: {0}\".format(id(base_model)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 6. Train models, predict outcomes"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Logic influenced by Anshul Sharma's \"COVID19-Explained through Visualizations\" notebook,\n# RanjitKS's \"20 lines; XGBoost; No Leaks; Best Score\" and others:\n\n# Set up one-hot encoding to avoid possible leakage from LabelEncoder values (alphabetical ordering of geographies, etc.)\n\n# Possible improvements:\n#  - try time lags and other time-series adjustments\n#  - try geog, political, transportation, cultural data to enhance model fit\n\n# get country / state list. If one-hot encoded, Train dataframe will have one column per country/state combination\n# If label encoded, original columns will have a numeric value instead of text country/state name\n\nif one_hot_encode == True:\n    country_groups = df_train_original.groupby(['Country_Region', 'Province_State']).groups\n    df_country_list = pd.DataFrame.from_dict(list(country_groups))\n    train_country_list = df_country_list[0].unique()\n\n#pdb.set_trace()\n\ndf_train_dd = setup_df_encode_and_dates(df_train, one_hot_encode, ['Country_Region', 'Province_State'], ['ConfirmedCases', 'Fatalities'])\ndf_test_dd = setup_df_encode_and_dates(df_test, one_hot_encode, ['Country_Region', 'Province_State'])\n\n\nif one_hot_encode == False:\n    country_groups = df_train_dd.groupby(['Country_Region', 'Province_State']).groups\n    df_country_list = pd.DataFrame.from_dict(list(country_groups))\n    train_country_list = df_country_list[0].unique()\n\n#pdb.set_trace()\n\ndf_preds = pd.DataFrame({'ForecastId':[],  'ConfirmedCases': [],  'Fatalities': []})\n\nif (loop_logic == True): \n    # loop over states within countries\n    print(\"Starting forecasting for {0} countries.\".format(len(train_country_list)))\n    for country in train_country_list:\n        print(\"Starting country {0}.\".format(country))\n\n        # Get list of states/provinces (if any) for the current country \n        country_states = df_country_list[(df_country_list[0] == country)][1].values\n\n        for state in country_states:\n    #        pdb.set_trace()\n            # get train / test data for current state/province\n            curr_cs_train, curr_cs_test = get_test_train_for_country_state(one_hot_encode, df_train_dd, df_test_dd, country, state)\n\n            # train model for each state/province combination\n            # predict state's values (if country values not broken out by state/province, state == country)\n            preds = fit_models_and_train(country, state, base_model if use_base_model==True else None, curr_cs_train, curr_cs_test)\n\n            preds = preds.round(5)  # round predictions to 5 decimal places\n\n            # add results to list of predictions\n            df_preds = pd.concat([df_preds, preds], axis=0)\n\n    #        show_results(base_model)\n        print(\"Country {0} complete.\".format(country))\nelse:\n    print(\"Starting forecasting for all {0} countries.\".format(len(train_country_list)))\n    preds = fit_models_and_train(\"All\", \"All\", base_model if use_base_model==True else None, df_train_dd, df_test_dd)\n    df_preds = pd.concat([df_preds, preds], axis=0)\nprint(\"All countries complete.\")\n\nif not (public_leaderboard_end_date is None):\n    # Set predictions to 1 beyond public leaderboard cut-off date if still in pu\n    df_preds.loc[(df_test_original.Datetime > pd.to_datetime(public_leaderboard_end_date)), 'ConfirmedCases'] = 1\n    df_preds.loc[(df_test_original.Datetime > pd.to_datetime(public_leaderboard_end_date)), 'Fatalities'] = 1\n    df_preds[(df_test_original.Datetime > pd.to_datetime(public_leaderboard_end_date))].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(not (public_leaderboard_end_date is None))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test_dd.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test_dd.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_dd.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#show_results(base_model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 7. Analyze results to improve model"},{"metadata":{"trusted":true},"cell_type":"code","source":"if loop_logic == False:\n    xgb.plot_importance(base_model)\n    plt.rcParams['figure.figsize'] = [40,40]\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 8. Prepare submission file for Kaggle"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_preds.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prepare_submission(df_preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"}},"nbformat":4,"nbformat_minor":4}