{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"\n\nimport pandas as pd                #Pandas library for data analysis\nimport numpy as np                 #For numerical analysis of data\nimport matplotlib.pyplot as plt    #Python's plotting \n\n\nimport plotly.express as px       #Plotly for plotting the COVID-19 Spread.\nimport plotly.offline as py       #Plotly for plotting the COVID-19 Spread.\nimport seaborn as sns             #Seaborn for data plotting\nimport plotly.graph_objects as go #Plotlygo for plotting\n\nfrom plotly.subplots import make_subplots\n\n\nimport glob                       #For assigning the path\nimport os                         #OS Library for implementing the functions.\n\nimport warnings\nwarnings.filterwarnings('ignore') \n\n#Selcting the other essential libraries for data manipulation\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import MinMaxScaler\nimport datetime as dt\nimport xgboost as xgb\nfrom xgboost import XGBRegressor\n\n\n\n#Reading the cumulative cases dataset\ncovid_cases = pd.read_csv('../input/novel-corona-virus-2019-dataset/covid_19_data.csv')\ntraining_data = pd.read_csv(\"../input/covid19-global-forecasting-week-4/train.csv\")\ntesting_data = pd.read_csv(\"../input/covid19-global-forecasting-week-4/test.csv\")\ntraining_data['Province_State'].fillna(\"\",inplace = True)\ntesting_data['Province_State'].fillna(\"\",inplace = True)\ncountry_list = covid_cases['Country/Region'].unique()\n\ncountry_grouped_covid = covid_cases[0:1]\n\nfor country in country_list:\n    test_data = covid_cases['Country/Region'] == country   \n    test_data = covid_cases[test_data]\n    country_grouped_covid = pd.concat([country_grouped_covid, test_data], axis=0)\n    \ncountry_grouped_covid.reset_index(drop=True)\ncountry_grouped_covid.head()\n\n#Dropping of the column Last Update\ncountry_grouped_covid.drop('Last Update', axis=1, inplace=True)\n\n#Replacing NaN Values in Province/State with a string \"Not Reported\"\ncountry_grouped_covid['Province/State'].replace(np.nan, \"Not Reported\", inplace=True)\nlatest_data = country_grouped_covid['ObservationDate'] == '04/10/2020'\ncountry_data = country_grouped_covid[latest_data]\n\n#The total number of reported Countries\ncountry_list = country_data['Country/Region'].unique()\nprint(\"The total number of countries with COVID-19 Confirmed cases = {}\".format(country_list.size))\nunique_dates = country_grouped_covid['ObservationDate'].unique()\nconfirmed_cases = []\nrecovered = []\ndeaths = []\n\nfor date in unique_dates:\n    date_wise = country_grouped_covid['ObservationDate'] == date  \n    test_data = country_grouped_covid[date_wise]\n    \n    confirmed_cases.append(test_data['Confirmed'].sum())\n    deaths.append(test_data['Deaths'].sum())\n    recovered.append(test_data['Recovered'].sum())\n    \n#Converting the lists to a pandas dataframe.\n\ncountry_dataset = {'Date' : unique_dates, 'Confirmed' : confirmed_cases, 'Recovered' : recovered, 'Deaths' : deaths}\ncountry_dataset = pd.DataFrame(country_dataset)\n\n#Plotting the Graph of Cases vs Deaths Globally.\n\nfig = go.Figure()\nfig.add_trace(go.Bar(x=country_dataset['Date'], y=country_dataset['Confirmed'], name='Confirmed Cases of COVID-19', marker_color='rgb(55, 83, 109)'))\nfig.add_trace(go.Bar(x=country_dataset['Date'],y=country_dataset['Deaths'],name='Total Deaths because of COVID-19',marker_color='rgb(26, 118, 255)'))\n\nfig.update_layout(title='Confirmed Cases and Deaths from COVID-19',xaxis_tickfont_size=14,\n                  yaxis=dict(title='Reported Numbers',titlefont_size=16,tickfont_size=14,),\n    legend=dict(x=0,y=1.0,bgcolor='rgba(255, 255, 255, 0)',bordercolor='rgba(255, 255, 255, 0)'),barmode='group',bargap=0.15, bargroupgap=0.1)\nfig.show()\n\n\nfig = go.Figure()\nfig.add_trace(go.Bar(x=country_dataset['Date'], y=country_dataset['Confirmed'], name='Confirmed Cases of COVID-19', marker_color='rgb(55, 83, 109)'))\nfig.add_trace(go.Bar(x=country_dataset['Date'],y=country_dataset['Recovered'],name='Total Recoveries because of COVID-19',marker_color='rgb(26, 118, 255)'))\n\nfig.update_layout(title='Confirmed Cases and Recoveries from COVID-19',xaxis_tickfont_size=14,\n                  yaxis=dict(title='Reported Numbers',titlefont_size=16,tickfont_size=14,),\n    legend=dict(x=0,y=1.0,bgcolor='rgba(255, 255, 255, 0)',bordercolor='rgba(255, 255, 255, 0)'),\n    barmode='group',bargap=0.15, bargroupgap=0.1)\nfig.show()\nfolder_name = '../input/covcsd-covid19-countries-statistical-dataset'\nfile_type = 'csv'\nseperator =','\ndataframe = pd.concat([pd.read_csv(f, sep=seperator) for f in glob.glob(folder_name + \"/*.\"+file_type)],ignore_index=True,sort=False)\n\n#Selecting the columns that are required as is essential for the data-wrangling task\n\ncovid_data = dataframe[['Date', 'State', 'Country', 'Cumulative_cases', 'Cumulative_death',\n       'Daily_cases', 'Daily_death', 'Latitude', 'Longitude', 'Temperature',\n       'Min_temperature', 'Max_temperature', 'Wind_speed', 'Precipitation',\n       'Fog_Presence', 'Population', 'Population Density/km', 'Median_Age',\n       'Sex_Ratio', 'Age%_65+', 'Hospital Beds/1000', 'Available Beds/1000',\n       'Confirmed Cases/1000', 'Lung Patients (F)', 'Lung Patients (M)',\n       'Life Expectancy (M)', 'Life Expectancy (F)', 'Total_tests_conducted',\n       'Out_Travels (mill.)', 'In_travels(mill.)', 'Domestic_Travels (mill.)']]\n\ntraining_data['Country_Region'] = training_data['Country_Region'] + ' ' + training_data['Province_State']\ntesting_data['Country_Region'] = testing_data['Country_Region'] + ' ' + testing_data['Province_State']\ndel training_data['Province_State']\ndel testing_data['Province_State']\n\n#Creating a function to split-date\n\ndef split_date(date):\n    date = date.split('-')\n    date[0] = int(date[0])\n    if(date[1][0] == '0'):\n        date[1] = int(date[1][1])\n    else:\n        date[1] = int(date[1])\n    if(date[2][0] == '0'):\n        date[2] = int(date[2][1])\n    else:\n        date[2] = int(date[2])    \n    return date\n\ntraining_data.Date = training_data.Date.apply(split_date)\ntesting_data.Date = testing_data.Date.apply(split_date)\n\n#Manipulation of columns for both training dataset\n\nyear = []\nmonth = []\nday = []\n\nfor i in training_data.Date:\n    year.append(i[0])\n    month.append(i[1])\n    day.append(i[2])\n    \ntraining_data['Year'] = year\ntraining_data['Month'] = month\ntraining_data['Day'] = day\ndel training_data['Date']\n\n#Manipulation of columns for both testing dataset\n\nyear = []\nmonth = []\nday = []\nfor i in testing_data.Date:\n    year.append(i[0])\n    month.append(i[1])\n    day.append(i[2])\n    \ntesting_data['Year'] = year\ntesting_data['Month'] = month\ntesting_data['Day'] = day\ndel testing_data['Date']\ndel training_data['Id']\ndel testing_data['ForecastId']\ndel testing_data['Year']\ndel training_data['Year']\n\n#Filtering of the dataset to view the latest contents (as of 30-03-2020)\nlatest_data = covid_data['Date'] == '30-03-2020'\ncountry_data_detailed = covid_data[latest_data]\n\n#Dropping off unecssary columns from the country_data_detailed dataset\ncountry_data_detailed.drop(['Daily_cases','Daily_death','Latitude','Longitude'],axis=1,inplace=True)\ncountry_data_detailed.replace('Not Reported',np.nan,inplace=True)\ncountry_data_detailed.replace('N/A',np.nan,inplace=True)\n\n\ncountry_data_detailed['Lung Patients (F)'].replace('Not reported',np.nan,inplace=True)\ncountry_data_detailed['Lung Patients (F)'] = country_data_detailed['Lung Patients (F)'].astype(\"float\")\n\n#Getting the dataset to check the correlation \ncorr_data = country_data_detailed.drop(['Date','State','Country','Min_temperature','Max_temperature','Out_Travels (mill.)',\n                                        'In_travels(mill.)','Domestic_Travels (mill.)','Total_tests_conducted','Age%_65+'], axis=1)\n\n#Converting the dataset to the correlation function\ncorr = corr_data.corr()\n\ndef heatmap(x, y, size,color):\n    fig, ax = plt.subplots(figsize=(20,3))\n    \n    # Mapping from column names to integer coordinates\n    x_labels = corr_data.columns\n    y_labels = ['Cumulative_cases', 'Cumulative_death']\n    x_to_num = {p[1]:p[0] for p in enumerate(x_labels)} \n    y_to_num = {p[1]:p[0] for p in enumerate(y_labels)} \n    \n    n_colors = 256 # Use 256 colors for the diverging color palette\n    palette = sns.cubehelix_palette(n_colors) # Create the palette\n    color_min, color_max = [-1, 1] # Range of values that will be mapped to the palette, i.e. min and max possible correlation\n\n    def value_to_color(val):\n        val_position = float((val - color_min)) / (color_max - color_min) # position of value in the input range, relative to the length of the input range\n        ind = int(val_position * (n_colors - 1)) # target index in the color palette\n        return palette[ind]\n\n    \n    ax.scatter(\n    x=x.map(x_to_num),\n    y=y.map(y_to_num),\n    s=size * 1000,\n    c=color.apply(value_to_color), # Vector of square color values, mapped to color palette\n    marker='s'\n)\n    \n    # Show column labels on the axes\n    ax.set_xticks([x_to_num[v] for v in x_labels])\n    ax.set_xticklabels(x_labels, rotation=30, horizontalalignment='right')\n    ax.set_yticks([y_to_num[v] for v in y_labels])\n    ax.set_yticklabels(y_labels)\n    \n    \n    ax.set_xticks([t + 0.5 for t in ax.get_xticks()], minor=True)\n    ax.set_yticks([t + 0.5 for t in ax.get_yticks()], minor=True)\n    \n    ax.set_xlim([-0.5, max([v for v in x_to_num.values()]) + 0.5]) \n    ax.set_ylim([-0.5, max([v for v in y_to_num.values()]) + 0.5])\n    \ncorr = pd.melt(corr.reset_index(), id_vars='index') \ncorr.columns = ['x', 'y', 'value']\nheatmap(x=corr['x'],y=corr['y'],size=corr['value'].abs(),color=corr['value'])\n\n\n#Reading the temperature data file\ntemperature_data = pd.read_csv('../input/covcsd-covid19-countries-statistical-dataset/temperature_data.csv')\n\n#Viewing the dataset\ntemperature_data.head()\n\nunique_temp = temperature_data['Temperature'].unique()\nconfirmed_cases = []\ndeaths = []\n\nfor temp in unique_temp:\n    temp_wise = temperature_data['Temperature'] == temp\n    test_data = temperature_data[temp_wise]\n    \n    confirmed_cases.append(test_data['Daily_cases'].sum())\n    deaths.append(test_data['Daily_death'].sum())\n    \n#Converting the lists to a pandas dataframe.\n\ntemperature_dataset = {'Temperature' : unique_temp, 'Confirmed' : confirmed_cases, 'Deaths' : deaths}\ntemperature_dataset = pd.DataFrame(temperature_dataset)\n\n\n\n#Plotting a scatter plot for cases vs. Temperature\n\nfig = make_subplots(specs=[[{\"secondary_y\": True}]])\n\nfig.add_trace(go.Scattergl(x = temperature_dataset['Temperature'],y = temperature_dataset['Confirmed'], mode='markers',\n                                  marker=dict(color=np.random.randn(10000),colorscale='Viridis',line_width=1)),secondary_y=False)\n\nfig.add_trace(go.Box(x=temperature_dataset['Temperature']),secondary_y=True)\n\nfig.update_layout(title='Daily Confirmed Cases (COVID-19) vs. Temperature (Celcius) : Global Figures - January 22 - March 30 2020',\n                  yaxis=dict(title='Reported Numbers'),xaxis=dict(title='Temperature in Celcius'))\n\nfig.update_yaxes(title_text=\"BoxPlot Range \", secondary_y=True)\ntraining_data['ConfirmedCases'] = training_data['ConfirmedCases'].apply(int)\ntraining_data['Fatalities'] = training_data['Fatalities'].apply(int)\n\ncases = training_data.ConfirmedCases\nfatalities = training_data.Fatalities\ndel training_data['ConfirmedCases']\ndel training_data['Fatalities']\n\nlb = LabelEncoder()\ntraining_data['Country_Region'] = lb.fit_transform(training_data['Country_Region'])\ntesting_data['Country_Region'] = lb.transform(testing_data['Country_Region'])\n\nscaler = MinMaxScaler()\nx_train = scaler.fit_transform(training_data.values)\nx_test = scaler.transform(testing_data.values)\nfrom xgboost import XGBRegressor\n\nrf = XGBRegressor(n_estimators = 1500 , max_depth = 10, learning_rate=0.1)\nrf.fit(x_train,cases)\ncases_pred = rf.predict(x_test)\n\nrf = XGBRegressor(n_estimators = 1500 , max_depth = 10, learning_rate=0.1)\nrf.fit(x_train,fatalities)\nfatalities_pred = rf.predict(x_test)\n\n#Rouding off the prediction values and converting negatives to zero\ncases_pred = np.around(cases_pred)\nfatalities_pred = np.around(fatalities_pred)\n\ncases_pred[cases_pred < 0] = 0\nfatalities_pred[fatalities_pred < 0] = 0\n\nsubmission_dataset = pd.read_csv(\"../input/covid19-global-forecasting-week-4/submission.csv\")\n\n#Adding results to the dataset\nsubmission_dataset['ConfirmedCases'] = cases_pred\nsubmission_dataset['Fatalities'] = fatalities_pred\n\nsubmission_dataset.head()\nsubmission_dataset.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom scipy.optimize import curve_fit\n\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\nimport matplotlib.pyplot as plt\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import OneHotEncoder\n\nimport xgboost as xgb\nimport tensorflow as tf\n\nfrom tensorflow.keras.optimizers import Nadam\nfrom sklearn.metrics import mean_squared_error\nimport tensorflow.keras.layers as KL\nfrom datetime import timedelta\nimport numpy as np\nimport pandas as pd\n\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression, Ridge\n\nimport datetime\nimport gc\nfrom tqdm import tqdm\n\ndef get_cpmp_sub(save_oof=False, save_public_test=False):\n    train = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-4/train.csv')\n    train['Province_State'].fillna('', inplace=True)\n    train['Date'] = pd.to_datetime(train['Date'])\n    train['day'] = train.Date.dt.dayofyear\n    #train = train[train.day <= 85]\n    train['geo'] = ['_'.join(x) for x in zip(train['Country_Region'], train['Province_State'])]\n    train\n\n    test = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-4/test.csv')\n    test['Province_State'].fillna('', inplace=True)\n    test['Date'] = pd.to_datetime(test['Date'])\n    test['day'] = test.Date.dt.dayofyear\n    test['geo'] = ['_'.join(x) for x in zip(test['Country_Region'], test['Province_State'])]\n    test\n\n    day_min = train['day'].min()\n    train['day'] -= day_min\n    test['day'] -= day_min\n\n    min_test_val_day = test.day.min()\n    max_test_val_day = train.day.max()\n    max_test_day = test.day.max()\n    num_days = max_test_day + 1\n\n    min_test_val_day, max_test_val_day, num_days\n\n    train['ForecastId'] = -1\n    test['Id'] = -1\n    test['ConfirmedCases'] = 0\n    test['Fatalities'] = 0\n\n    debug = False\n\n    data = pd.concat([train,\n                      test[test.day > max_test_val_day][train.columns]\n                     ]).reset_index(drop=True)\n    if debug:\n        data = data[data['geo'] >= 'France_'].reset_index(drop=True)\n    #del train, test\n    gc.collect()\n\n    dates = data[data['geo'] == 'France_'].Date.values\n\n    if 0:\n        gr = data.groupby('geo')\n        data['ConfirmedCases'] = gr.ConfirmedCases.transform('cummax')\n        data['Fatalities'] = gr.Fatalities.transform('cummax')\n\n    geo_data = data.pivot(index='geo', columns='day', values='ForecastId')\n    num_geo = geo_data.shape[0]\n    geo_data\n\n    geo_id = {}\n    for i,g in enumerate(geo_data.index):\n        geo_id[g] = i\n\n\n    ConfirmedCases = data.pivot(index='geo', columns='day', values='ConfirmedCases')\n    Fatalities = data.pivot(index='geo', columns='day', values='Fatalities')\n\n    if debug:\n        cases = ConfirmedCases.values\n        deaths = Fatalities.values\n    else:\n        cases = np.log1p(ConfirmedCases.values)\n        deaths = np.log1p(Fatalities.values)\n\n\n    def get_dataset(start_pred, num_train, lag_period):\n        days = np.arange( start_pred - num_train + 1, start_pred + 1)\n        lag_cases = np.vstack([cases[:, d - lag_period : d] for d in days])\n        lag_deaths = np.vstack([deaths[:, d - lag_period : d] for d in days])\n        target_cases = np.vstack([cases[:, d : d + 1] for d in days])\n        target_deaths = np.vstack([deaths[:, d : d + 1] for d in days])\n        geo_ids = np.vstack([geo_ids_base for d in days])\n        country_ids = np.vstack([country_ids_base for d in days])\n        return lag_cases, lag_deaths, target_cases, target_deaths, geo_ids, country_ids, days\n\n    def update_valid_dataset(data, pred_death, pred_case):\n        lag_cases, lag_deaths, target_cases, target_deaths, geo_ids, country_ids, days = data\n        day = days[-1] + 1\n        new_lag_cases = np.hstack([lag_cases[:, 1:], pred_case])\n        new_lag_deaths = np.hstack([lag_deaths[:, 1:], pred_death]) \n        new_target_cases = cases[:, day:day+1]\n        new_target_deaths = deaths[:, day:day+1] \n        new_geo_ids = geo_ids  \n        new_country_ids = country_ids  \n        new_days = 1 + days\n        return new_lag_cases, new_lag_deaths, new_target_cases, new_target_deaths, new_geo_ids, new_country_ids, new_days\n\n    def fit_eval(lr_death, lr_case, data, start_lag_death, end_lag_death, num_lag_case, fit, score):\n        lag_cases, lag_deaths, target_cases, target_deaths, geo_ids, country_ids, days = data\n\n        X_death = np.hstack([lag_cases[:, -start_lag_death:-end_lag_death], country_ids])\n        X_death = np.hstack([lag_deaths[:, -num_lag_case:], country_ids])\n        X_death = np.hstack([lag_cases[:, -start_lag_death:-end_lag_death], lag_deaths[:, -num_lag_case:], country_ids])\n        y_death = target_deaths\n        y_death_prev = lag_deaths[:, -1:]\n        if fit:\n            if 0:\n                keep = (y_death > 0).ravel()\n                X_death = X_death[keep]\n                y_death = y_death[keep]\n                y_death_prev = y_death_prev[keep]\n            lr_death.fit(X_death, y_death)\n        y_pred_death = lr_death.predict(X_death)\n        y_pred_death = np.maximum(y_pred_death, y_death_prev)\n\n        X_case = np.hstack([lag_cases[:, -num_lag_case:], geo_ids])\n        X_case = lag_cases[:, -num_lag_case:]\n        y_case = target_cases\n        y_case_prev = lag_cases[:, -1:]\n        if fit:\n            lr_case.fit(X_case, y_case)\n        y_pred_case = lr_case.predict(X_case)\n        y_pred_case = np.maximum(y_pred_case, y_case_prev)\n\n        if score:\n            death_score = val_score(y_death, y_pred_death)\n            case_score = val_score(y_case, y_pred_case)\n        else:\n            death_score = 0\n            case_score = 0\n\n        return death_score, case_score, y_pred_death, y_pred_case\n\n    def train_model(train, valid, start_lag_death, end_lag_death, num_lag_case, num_val, score=True):\n        alpha = 2\n        lr_death = Ridge(alpha=alpha, fit_intercept=False)\n        lr_case = Ridge(alpha=alpha, fit_intercept=True)\n\n        (train_death_score, train_case_score, train_pred_death, train_pred_case,\n        ) = fit_eval(lr_death, lr_case, train, start_lag_death, end_lag_death, num_lag_case, fit=True, score=score)\n\n        death_scores = []\n        case_scores = []\n\n        death_pred = []\n        case_pred = []\n\n        for i in range(num_val):\n\n            (valid_death_score, valid_case_score, valid_pred_death, valid_pred_case,\n            ) = fit_eval(lr_death, lr_case, valid, start_lag_death, end_lag_death, num_lag_case, fit=False, score=score)\n\n            death_scores.append(valid_death_score)\n            case_scores.append(valid_case_score)\n            death_pred.append(valid_pred_death)\n            case_pred.append(valid_pred_case)\n\n            if 0:\n                print('val death: %0.3f' %  valid_death_score,\n                      'val case: %0.3f' %  valid_case_score,\n                      'val : %0.3f' %  np.mean([valid_death_score, valid_case_score]),\n                      flush=True)\n            valid = update_valid_dataset(valid, valid_pred_death, valid_pred_case)\n\n        if score:\n            death_scores = np.sqrt(np.mean([s**2 for s in death_scores]))\n            case_scores = np.sqrt(np.mean([s**2 for s in case_scores]))\n            if 0:\n                print('train death: %0.3f' %  train_death_score,\n                      'train case: %0.3f' %  train_case_score,\n                      'val death: %0.3f' %  death_scores,\n                      'val case: %0.3f' %  case_scores,\n                      'val : %0.3f' % ( (death_scores + case_scores) / 2),\n                      flush=True)\n            else:\n                print('%0.4f' %  case_scores,\n                      ', %0.4f' %  death_scores,\n                      '= %0.4f' % ( (death_scores + case_scores) / 2),\n                      flush=True)\n        death_pred = np.hstack(death_pred)\n        case_pred = np.hstack(case_pred)\n        return death_scores, case_scores, death_pred, case_pred\n\n    countries = [g.split('_')[0] for g in geo_data.index]\n    countries = pd.factorize(countries)[0]\n\n    country_ids_base = countries.reshape((-1, 1))\n    ohe = OneHotEncoder(sparse=False)\n    country_ids_base = 0.2 * ohe.fit_transform(country_ids_base)\n    country_ids_base.shape\n\n    geo_ids_base = np.arange(num_geo).reshape((-1, 1))\n    ohe = OneHotEncoder(sparse=False)\n    geo_ids_base = 0.1 * ohe.fit_transform(geo_ids_base)\n    geo_ids_base.shape\n\n    def val_score(true, pred):\n        pred = np.log1p(np.round(np.expm1(pred) - 0.2))\n        return np.sqrt(mean_squared_error(true.ravel(), pred.ravel()))\n\n    def val_score(true, pred):\n        return np.sqrt(mean_squared_error(true.ravel(), pred.ravel()))\n\n\n\n    start_lag_death, end_lag_death = 14, 6,\n    num_train = 6\n    num_lag_case = 14\n    lag_period = max(start_lag_death, num_lag_case)\n\n    def get_oof(start_val_delta=0):   \n        start_val = min_test_val_day + start_val_delta\n        last_train = start_val - 1\n        num_val = max_test_val_day - start_val + 1\n        print(dates[start_val], start_val, num_val)\n        train_data = get_dataset(last_train, num_train, lag_period)\n        valid_data = get_dataset(start_val, 1, lag_period)\n        _, _, val_death_preds, val_case_preds = train_model(train_data, valid_data, \n                                                            start_lag_death, end_lag_death, num_lag_case, num_val)\n\n        pred_deaths = Fatalities.iloc[:, start_val:start_val+num_val].copy()\n        pred_deaths.iloc[:, :] = np.expm1(val_death_preds)\n        pred_deaths = pred_deaths.stack().reset_index()\n        pred_deaths.columns = ['geo', 'day', 'Fatalities']\n        pred_deaths\n\n        pred_cases = ConfirmedCases.iloc[:, start_val:start_val+num_val].copy()\n        pred_cases.iloc[:, :] = np.expm1(val_case_preds)\n        pred_cases = pred_cases.stack().reset_index()\n        pred_cases.columns = ['geo', 'day', 'ConfirmedCases']\n        pred_cases\n\n        sub = train[['Date', 'Id', 'geo', 'day']]\n        sub = sub.merge(pred_cases, how='left', on=['geo', 'day'])\n        sub = sub.merge(pred_deaths, how='left', on=['geo', 'day'])\n        #sub = sub.fillna(0)\n        sub = sub[sub.day >= start_val]\n        sub = sub[['Id', 'ConfirmedCases', 'Fatalities']].copy()\n        return sub\n\n\n    if save_oof:\n        for start_val_delta, date in zip(range(3, -8, -3),\n                                  ['2020-03-22', '2020-03-19', '2020-03-16', '2020-03-13']):\n            print(date, end=' ')\n            oof = get_oof(start_val_delta)\n            oof.to_csv('../submissions/cpmp-%s.csv' % date, index=None)\n\n    def get_sub(start_val_delta=0):   \n        start_val = min_test_val_day + start_val_delta\n        last_train = start_val - 1\n        num_val = max_test_val_day - start_val + 1\n        print(dates[last_train], start_val, num_val)\n        num_lag_case = 14\n        train_data = get_dataset(last_train, num_train, lag_period)\n        valid_data = get_dataset(start_val, 1, lag_period)\n        _, _, val_death_preds, val_case_preds = train_model(train_data, valid_data, \n                                                            start_lag_death, end_lag_death, num_lag_case, num_val)\n\n        pred_deaths = Fatalities.iloc[:, start_val:start_val+num_val].copy()\n        pred_deaths.iloc[:, :] = np.expm1(val_death_preds)\n        pred_deaths = pred_deaths.stack().reset_index()\n        pred_deaths.columns = ['geo', 'day', 'Fatalities']\n        pred_deaths\n\n        pred_cases = ConfirmedCases.iloc[:, start_val:start_val+num_val].copy()\n        pred_cases.iloc[:, :] = np.expm1(val_case_preds)\n        pred_cases = pred_cases.stack().reset_index()\n        pred_cases.columns = ['geo', 'day', 'ConfirmedCases']\n        pred_cases\n\n        sub = test[['Date', 'ForecastId', 'geo', 'day']]\n        sub = sub.merge(pred_cases, how='left', on=['geo', 'day'])\n        sub = sub.merge(pred_deaths, how='left', on=['geo', 'day'])\n        sub = sub.fillna(0)\n        sub = sub[['ForecastId', 'ConfirmedCases', 'Fatalities']]\n        return sub\n        return sub\n\n\n    known_test = train[['geo', 'day', 'ConfirmedCases', 'Fatalities']\n              ].merge(test[['geo', 'day', 'ForecastId']], how='left', on=['geo', 'day'])\n    known_test = known_test[['ForecastId', 'ConfirmedCases', 'Fatalities']][known_test.ForecastId.notnull()].copy()\n    known_test\n\n    unknow_test = test[test.day > max_test_val_day]\n    unknow_test\n\n    def get_final_sub():   \n        start_val = max_test_val_day + 1\n        last_train = start_val - 1\n        num_val = max_test_day - start_val + 1\n        print(dates[last_train], start_val, num_val)\n        num_lag_case = num_val + 3\n        train_data = get_dataset(last_train, num_train, lag_period)\n        valid_data = get_dataset(start_val, 1, lag_period)\n        (_, _, val_death_preds, val_case_preds\n        ) = train_model(train_data, valid_data, start_lag_death, end_lag_death, num_lag_case, num_val, score=False)\n\n        pred_deaths = Fatalities.iloc[:, start_val:start_val+num_val].copy()\n        pred_deaths.iloc[:, :] = np.expm1(val_death_preds)\n        pred_deaths = pred_deaths.stack().reset_index()\n        pred_deaths.columns = ['geo', 'day', 'Fatalities']\n        pred_deaths\n\n        pred_cases = ConfirmedCases.iloc[:, start_val:start_val+num_val].copy()\n        pred_cases.iloc[:, :] = np.expm1(val_case_preds)\n        pred_cases = pred_cases.stack().reset_index()\n        pred_cases.columns = ['geo', 'day', 'ConfirmedCases']\n        pred_cases\n        print(unknow_test.shape, pred_deaths.shape, pred_cases.shape)\n\n        sub = unknow_test[['Date', 'ForecastId', 'geo', 'day']]\n        sub = sub.merge(pred_cases, how='left', on=['geo', 'day'])\n        sub = sub.merge(pred_deaths, how='left', on=['geo', 'day'])\n        #sub = sub.fillna(0)\n        sub = sub[['ForecastId', 'ConfirmedCases', 'Fatalities']]\n        sub = pd.concat([known_test, sub])\n        return sub\n\n    if save_public_test:\n        sub = get_sub()\n    else:\n        sub = get_final_sub()\n    return sub\n\ndef get_nn_sub():\n    df = pd.read_csv(\"../input/covid19-global-forecasting-week-4/train.csv\")\n    sub_df = pd.read_csv(\"../input/covid19-global-forecasting-week-4/test.csv\")\n\n    coo_df = pd.read_csv(\"../input/week11/train.csv\").rename(columns={\"Country/Region\": \"Country_Region\"})\n    coo_df = coo_df.groupby(\"Country_Region\")[[\"Lat\", \"Long\"]].mean().reset_index()\n    coo_df = coo_df[coo_df[\"Country_Region\"].notnull()]\n\n    loc_group = [\"Province_State\", \"Country_Region\"]\n\n\n    def preprocess(df):\n        df[\"Date\"] = df[\"Date\"].astype(\"datetime64[ms]\")\n        df[\"days\"] = (df[\"Date\"] - pd.to_datetime(\"2020-01-01\")).dt.days\n        df[\"weekend\"] = df[\"Date\"].dt.dayofweek//5\n\n        df = df.merge(coo_df, how=\"left\", on=\"Country_Region\")\n        df[\"Lat\"] = (df[\"Lat\"] // 30).astype(np.float32).fillna(0)\n        df[\"Long\"] = (df[\"Long\"] // 60).astype(np.float32).fillna(0)\n\n        for col in loc_group:\n            df[col].fillna(\"none\", inplace=True)\n        return df\n\n    df = preprocess(df)\n    sub_df = preprocess(sub_df)\n\n    print(df.shape)\n\n    TARGETS = [\"ConfirmedCases\", \"Fatalities\"]\n\n    for col in TARGETS:\n        df[col] = np.log1p(df[col])\n\n    NUM_SHIFT = 5\n\n    features = [\"Lat\", \"Long\"]\n\n    for s in range(1, NUM_SHIFT+1):\n        for col in TARGETS:\n            df[\"prev_{}_{}\".format(col, s)] = df.groupby(loc_group)[col].shift(s)\n            features.append(\"prev_{}_{}\".format(col, s))\n\n    df = df[df[\"Date\"] >= df[\"Date\"].min() + timedelta(days=NUM_SHIFT)].copy()\n\n    TEST_FIRST = sub_df[\"Date\"].min() # pd.to_datetime(\"2020-03-13\") #\n    TEST_DAYS = (df[\"Date\"].max() - TEST_FIRST).days + 1\n\n    dev_df, test_df = df[df[\"Date\"] < TEST_FIRST].copy(), df[df[\"Date\"] >= TEST_FIRST].copy()\n\n    def nn_block(input_layer, size, dropout_rate, activation):\n        out_layer = KL.Dense(size, activation=None)(input_layer)\n        #out_layer = KL.BatchNormalization()(out_layer)\n        out_layer = KL.Activation(activation)(out_layer)\n        out_layer = KL.Dropout(dropout_rate)(out_layer)\n        return out_layer\n\n\n    def get_model():\n        inp = KL.Input(shape=(len(features),))\n\n        hidden_layer = nn_block(inp, 622, 0.0, \"relu\")\n        gate_layer = nn_block(hidden_layer, 311, 0.0, \"hard_sigmoid\")\n        hidden_layer = nn_block(hidden_layer, 311, 0.0, \"relu\")\n        hidden_layer = KL.multiply([hidden_layer, gate_layer])\n\n        out = KL.Dense(len(TARGETS), activation=\"linear\")(hidden_layer)\n\n        model = tf.keras.models.Model(inputs=[inp], outputs=out)\n        return model\n\n    get_model().summary()\n\n    def get_input(df):\n        return [df[features]]\n\n    NUM_MODELS = 100\n\n\n    def train_models(df, save=False):\n        models = []\n        for i in range(NUM_MODELS):\n            model = get_model()\n            model.compile(loss=\"mean_squared_error\", optimizer=Nadam(lr=1e-4))\n            hist = model.fit(get_input(df), df[TARGETS],\n                             batch_size=2250, epochs=1000, verbose=0, shuffle=True)\n            if save:\n                model.save_weights(\"model{}.h5\".format(i))\n            models.append(model)\n        return models\n\n    models = train_models(dev_df)\n\n\n    prev_targets = ['prev_ConfirmedCases_1', 'prev_Fatalities_1']\n\n    def predict_one(df, models):\n        pred = np.zeros((df.shape[0], 2))\n        for model in models:\n            pred += model.predict(get_input(df))/len(models)\n        pred = np.maximum(pred, df[prev_targets].values)\n        pred[:, 0] = np.log1p(np.expm1(pred[:, 0]) + 0.1)\n        pred[:, 1] = np.log1p(np.expm1(pred[:, 1]) + 0.01)\n        return np.clip(pred, None, 15)\n\n    print([mean_squared_error(dev_df[TARGETS[i]], predict_one(dev_df, models)[:, i]) for i in range(len(TARGETS))])\n\n\n    def rmse(y_true, y_pred):\n        return np.sqrt(mean_squared_error(y_true, y_pred))\n\n    def evaluate(df):\n        error = 0\n        for col in TARGETS:\n            error += rmse(df[col].values, df[\"pred_{}\".format(col)].values)\n        return np.round(error/len(TARGETS), 5)\n\n\n    def predict(test_df, first_day, num_days, models, val=False):\n        temp_df = test_df.loc[test_df[\"Date\"] == first_day].copy()\n        y_pred = predict_one(temp_df, models)\n\n        for i, col in enumerate(TARGETS):\n            test_df[\"pred_{}\".format(col)] = 0\n            test_df.loc[test_df[\"Date\"] == first_day, \"pred_{}\".format(col)] = y_pred[:, i]\n\n        print(first_day, np.isnan(y_pred).sum(), y_pred.min(), y_pred.max())\n        if val:\n            print(evaluate(test_df[test_df[\"Date\"] == first_day]))\n\n\n        y_prevs = [None]*NUM_SHIFT\n\n        for i in range(1, NUM_SHIFT):\n            y_prevs[i] = temp_df[['prev_ConfirmedCases_{}'.format(i), 'prev_Fatalities_{}'.format(i)]].values\n\n        for d in range(1, num_days):\n            date = first_day + timedelta(days=d)\n            print(date, np.isnan(y_pred).sum(), y_pred.min(), y_pred.max())\n\n            temp_df = test_df.loc[test_df[\"Date\"] == date].copy()\n            temp_df[prev_targets] = y_pred\n            for i in range(2, NUM_SHIFT+1):\n                temp_df[['prev_ConfirmedCases_{}'.format(i), 'prev_Fatalities_{}'.format(i)]] = y_prevs[i-1]\n\n            y_pred, y_prevs = predict_one(temp_df, models), [None, y_pred] + y_prevs[1:-1]\n\n\n            for i, col in enumerate(TARGETS):\n                test_df.loc[test_df[\"Date\"] == date, \"pred_{}\".format(col)] = y_pred[:, i]\n\n            if val:\n                print(evaluate(test_df[test_df[\"Date\"] == date]))\n\n        return test_df\n\n    test_df = predict(test_df, TEST_FIRST, TEST_DAYS, models, val=True)\n    print(evaluate(test_df))\n\n    for col in TARGETS:\n        test_df[col] = np.expm1(test_df[col])\n        test_df[\"pred_{}\".format(col)] = np.expm1(test_df[\"pred_{}\".format(col)])\n\n    models = train_models(df, save=True)\n\n    sub_df_public = sub_df[sub_df[\"Date\"] <= df[\"Date\"].max()].copy()\n    sub_df_private = sub_df[sub_df[\"Date\"] > df[\"Date\"].max()].copy()\n\n    pred_cols = [\"pred_{}\".format(col) for col in TARGETS]\n    #sub_df_public = sub_df_public.merge(test_df[[\"Date\"] + loc_group + pred_cols].rename(columns={col: col[5:] for col in pred_cols}), \n    #                                    how=\"left\", on=[\"Date\"] + loc_group)\n    sub_df_public = sub_df_public.merge(test_df[[\"Date\"] + loc_group + TARGETS], how=\"left\", on=[\"Date\"] + loc_group)\n\n    SUB_FIRST = sub_df_private[\"Date\"].min()\n    SUB_DAYS = (sub_df_private[\"Date\"].max() - sub_df_private[\"Date\"].min()).days + 1\n\n    sub_df_private = df.append(sub_df_private, sort=False)\n\n    for s in range(1, NUM_SHIFT+1):\n        for col in TARGETS:\n            sub_df_private[\"prev_{}_{}\".format(col, s)] = sub_df_private.groupby(loc_group)[col].shift(s)\n\n    sub_df_private = sub_df_private[sub_df_private[\"Date\"] >= SUB_FIRST].copy()\n\n    sub_df_private = predict(sub_df_private, SUB_FIRST, SUB_DAYS, models)\n\n    for col in TARGETS:\n        sub_df_private[col] = np.expm1(sub_df_private[\"pred_{}\".format(col)])\n\n    sub_df = sub_df_public.append(sub_df_private, sort=False)\n    sub_df[\"ForecastId\"] = sub_df[\"ForecastId\"].astype(np.int16)\n\n    return sub_df[[\"ForecastId\"] + TARGETS]\n\nsub1 = get_cpmp_sub()\nsub1['ForecastId'] = sub1['ForecastId'].astype('int')\nsub2 = get_nn_sub()\n\nsub1.sort_values(\"ForecastId\", inplace=True)\nsub2.sort_values(\"ForecastId\", inplace=True)\n\n\nfrom sklearn.metrics import mean_squared_error\n\nTARGETS = [\"ConfirmedCases\", \"Fatalities\"]\n\n[np.sqrt(mean_squared_error(np.log1p(sub1[t].values), np.log1p(sub2[t].values))) for t in TARGETS]\n\nsub_df = sub1.copy()\nfor t in TARGETS:\n    sub_df[t] = np.expm1(np.log1p(sub1[t].values)*0.5 + np.log1p(sub2[t].values)*0.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nfrom pathlib import Path\nfrom pandas_profiling import ProfileReport\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import LabelEncoder\nimport datetime\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import cross_val_score\n\ndataset_path = Path('/kaggle/input/covid19-global-forecasting-week-4')\n\ntrain = pd.read_csv(dataset_path/'train.csv')\ntest = pd.read_csv(dataset_path/'test.csv')\nsubmission = pd.read_csv(dataset_path/'submission.csv')\n\ndef fill_state(state,country):\n    if pd.isna(state) : return country\n    return state\n\ntrain['Province_State'] = train.loc[:, ['Province_State', 'Country_Region']].apply(lambda x : fill_state(x['Province_State'], x['Country_Region']), axis=1)\ntest['Province_State'] = test.loc[:, ['Province_State', 'Country_Region']].apply(lambda x : fill_state(x['Province_State'], x['Country_Region']), axis=1)\n\ntrain['Date'] = pd.to_datetime(train['Date'],infer_datetime_format=True)\ntest['Date'] = pd.to_datetime(test['Date'],infer_datetime_format=True)\n\ntrain['Day_of_Week'] = train['Date'].dt.dayofweek\ntest['Day_of_Week'] = test['Date'].dt.dayofweek\n\ntrain['Month'] = train['Date'].dt.month\ntest['Month'] = test['Date'].dt.month\n\ntrain['Day'] = train['Date'].dt.day\ntest['Day'] = test['Date'].dt.day\n\ntrain['Day_of_Year'] = train['Date'].dt.dayofyear\ntest['Day_of_Year'] = test['Date'].dt.dayofyear\n\ntrain['Week_of_Year'] = train['Date'].dt.weekofyear\ntest['Week_of_Year'] = test['Date'].dt.weekofyear\n\ntrain['Quarter'] = train['Date'].dt.quarter  \ntest['Quarter'] = test['Date'].dt.quarter  \n\ntrain.drop('Date',1,inplace=True)\ntest.drop('Date',1,inplace=True)\n\nsubmission=pd.DataFrame(columns=submission.columns)\n\nl1=LabelEncoder()\nl2=LabelEncoder()\n\nl1.fit(train['Country_Region'])\nl2.fit(train['Province_State'])\n\ncountries=train['Country_Region'].unique()\nfor country in countries:\n    country_df=train[train['Country_Region']==country]\n    provinces=country_df['Province_State'].unique()\n    for province in provinces:\n            train_df=country_df[country_df['Province_State']==province]\n            train_df.pop('Id')\n            x=train_df[['Province_State','Country_Region','Day_of_Week','Month','Day','Day_of_Year','Week_of_Year','Quarter']]\n            x['Country_Region']=l1.transform(x['Country_Region'])\n            x['Province_State']=l2.transform(x['Province_State'])\n            y1=train_df[['ConfirmedCases']]\n            y2=train_df[['Fatalities']]\n            model_1=DecisionTreeClassifier()\n            model_2=DecisionTreeClassifier()\n            model_1.fit(x,y1)\n            model_2.fit(x,y2)\n            test_df=test.query('Province_State==@province & Country_Region==@country')\n            test_id=test_df['ForecastId'].values.tolist()\n            test_df.pop('ForecastId')\n            test_x=test_df[['Province_State','Country_Region','Day_of_Week','Month','Day','Day_of_Year','Week_of_Year','Quarter']]\n            test_x['Country_Region']=l1.transform(test_x['Country_Region'])\n            test_x['Province_State']=l2.transform(test_x['Province_State'])\n            test_y1=model_1.predict(test_x)\n            test_y2=model_2.predict(test_x)\n            test_res=pd.DataFrame(columns=submission.columns)\n            test_res['ForecastId']=test_id\n            test_res['ConfirmedCases']=test_y1\n            test_res['Fatalities']=test_y2\n            submission=submission.append(test_res)\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\\","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lcc=submission[\"ConfirmedCases\"]\nlf=submission[\"Fatalities\"]\n\nbuscc = submission_dataset[\"ConfirmedCases\"]\nbusf = submission_dataset[\"Fatalities\"]\nsdfcc = sub_df[\"ConfirmedCases\"]\nsdff = sub_df[\"Fatalities\"]\nsub_df[\"ConfirmedCases\"] = 0.1 * buscc.values +  0.70 * sdfcc.values + 0.20 *lcc.values\nsub_df[\"Fatalities\"] = 0.1 * busf.values  +  0.70 * sdff.values + 0.20  * lf.values\nsub_df.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}