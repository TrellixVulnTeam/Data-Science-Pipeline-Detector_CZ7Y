{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"# Load data"},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_data():\n    train = pd.read_csv('../input/covid19-global-forecasting-week-4/train.csv', index_col='Id', parse_dates=['Date']).set_index('Date')\n    test = pd.read_csv('../input/covid19-global-forecasting-week-4/test.csv', index_col='ForecastId', parse_dates=['Date'])\n    \n    return train, test\n\ntrain, test = load_data()\nsubmission = pd.read_csv('../input/covid19-global-forecasting-week-4/submission.csv')\nsubmission.set_index('ForecastId', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TEST_START_DATE = pd.Timestamp('2020-04-02')\nGROUND_TRUTH_END_DATE = pd.Timestamp(train.index.max())\nPRIVATE_START_DATE = pd.Timestamp('2020-04-16')\nTEST_END_DATE = pd.Timestamp('2020-05-14')\n\npredict_length = (TEST_END_DATE - TEST_START_DATE).days + 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train, test = load_data()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocess"},{"metadata":{},"cell_type":"markdown","source":"1. replace NaNs to `NA`"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Province_State'].fillna('NA', inplace=True)\ntest['Province_State'].fillna('NA', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For what number of Country and Province we must predict?"},{"metadata":{"trusted":true},"cell_type":"code","source":"test.drop_duplicates(subset=['Country_Region', 'Province_State'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We must predict **313** Country and Province"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Utility Function\n# Simply plot feature\ndef plot_feature(country_region, province_state='NA', column='ConfirmedCases', ax=None, figwidth=12):\n    if ax is None:\n        fig, ax = plt.subplots()\n        fig.set_figwidth(12)\n    plot_data = train.loc[\n        (train['Country_Region'] == country_region) & (train['Province_State'] == province_state),\n        column\n    ]\n    if len(plot_data) == 0:\n        raise Exception(f'country - province pair ({country_region}, {province_state}) you provided was not found.')\n    l = ax.plot(plot_data.index, plot_data[column], label=f'{column} - {country_region} / {province_state}')\n    ax.grid(True)\n    return ax\n\n# Strip preceding and trailing NaNs\ndef strip_nan(data):\n    notnan_index = data.index[~(data.isna())]\n    min_index = notnan_index[0]\n    max_index = notnan_index[-1]\n    \n    return data[(min_index <= data.index) & (data.index <= max_index)]\n\n# Strip preceding zeros\ndef strip_preceding_zeros(data):\n    nonzero_index = data.index[data != 0]\n    min_index = nonzero_index.min()\n    return data[(min_index <= data.index)]\n\n\ndef strip_preceding_values(data, value=0):\n    nonzero_index = data.index[data != value]\n    min_index = nonzero_index.min()\n    return data[(min_index <= data.index)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"country_province_index = train[['Country_Region', 'Province_State']].drop_duplicates().values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create difference features"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['ConfirmedCasesIncrement'] = 0\ntrain['FatalitiesIncrement'] = 0\nfor country_region, province_state in country_province_index:\n    targeted = train.loc[\n        (train['Country_Region'] == country_region) & (train['Province_State'] == province_state),\n        ['ConfirmedCases', 'Fatalities']\n    ]\n    preprocessed = targeted.diff(1).values + 1\n    train.loc[\n        (train['Country_Region'] == country_region) & (train['Province_State'] == province_state),\n        ['ConfirmedCasesIncrement', 'FatalitiesIncrement']\n    ] = preprocessed","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Try Exponential Smoothing"},{"metadata":{"trusted":true},"cell_type":"code","source":"import itertools\nfrom statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt\nfrom statsmodels.tsa import ar_model, stattools","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import ipywidgets as widgets\n\n\nprogressbar = widgets.IntProgress(\n    value=0,\n    min=0,\n    max=len(country_province_index),\n    step=1,\n    description='Predicting:',\n    bar_style='success', \n    orientation='horizontal'\n)\n\nprogresstext = widgets.Label(value='0 / {}'.format(len(country_province_index)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"widgets.HBox([progressbar, progresstext])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.simplefilter('ignore')\n\nadf_significant_level = 0.1\n\nfor i, (country_region, province_state) in enumerate(country_province_index):\n    for feature in ['ConfirmedCases', 'Fatalities']:\n\n        ground_truth = train.loc[\n            (train['Country_Region'] == country_region) & (train['Province_State'] == province_state),\n            feature + 'Increment'\n        ].dropna()\n\n        ground_truth = strip_preceding_values(ground_truth, value=1)\n        # Split into training and validation\n        train_data = ground_truth[ground_truth.index < TEST_START_DATE]\n        validation_data = ground_truth[(TEST_START_DATE <= ground_truth.index) & (ground_truth.index < PRIVATE_START_DATE)]\n\n        if len(train_data) < 10:\n            continue\n\n\n        # get best alpha and beta for Holt\n        alpha_grid = np.arange(0.5, 1.0, 0.05)\n        beta_grid = np.arange(0.5, 1.0, 0.05)\n        scores = []\n        for alpha, beta in itertools.product(alpha_grid, beta_grid):\n            fitted = Holt(train_data).fit(smoothing_level=alpha, smoothing_slope=beta)\n            scores.append(((alpha, beta), fitted.aic))\n        alpha, beta = sorted(scores, key=lambda k: k[1])[0][0]\n\n        holt_result = Holt(train_data).fit(smoothing_level=alpha, smoothing_slope=beta)\n        holt_fcast = holt_result.forecast(predict_length) # This is assumed as Trend component\n        \n        holt_fcast[holt_fcast < 1] = 1\n\n        noises = pd.Series(train_data.values.ravel() / holt_result.fittedvalues.values, index=train_data.index)\n\n        trends_cdt = []\n        try:\n            ct = stattools.adfuller(noises, regression='ct')[1]\n            trends_cdt.append(('ct', ct))\n        except:\n            pass\n        try:\n            c = stattools.adfuller(noises, regression='c')[1]\n            trends_cdt.append(('c', c))\n        except:\n            pass\n        try:\n            nc = stattools.adfuller(noises, regression='nc')[1]\n            trends_cdt.append(('nc', nc))\n        except:\n            pass\n        \n        if len(trends_cdt) == 0:\n            trend = 'urp'\n        else:\n            trend = sorted(trends_cdt, key=lambda x:x[1])[0][0]\n\n        # If unit root process\n        if trend == 'urp':\n            noise_prediction = pd.Series(\n                data=np.ones((predict_length,)) * noises[-1], \n                index=pd.date_range(TEST_START_DATE, TEST_END_DATE)\n            )\n        else:\n            # Search best lags\n            aics = []\n            for lags in np.arange(min(len(train_data), 30)):\n                try:\n                    noise_model = ar_model.AutoReg(noises.values.ravel(), lags=lags, trend=trend)\n                    noise_result = noise_model.fit()\n                    aics.append(noise_result.aic)\n                except:\n                    pass\n            \n            if len(aics) == 0:\n                noise_prediction = pd.Series(\n                    data=np.ones((predict_length,)) * noises[-1], \n                    index=pd.date_range(TEST_START_DATE, TEST_END_DATE)\n                )\n            else:\n                lags = np.argmin(aics)\n\n                # Build AR(lags_best) model\n                noise_model = ar_model.AutoReg(noises.values.ravel(), lags=lags, trend=trend)\n                noise_result = noise_model.fit()\n\n                noise_prediction = pd.Series(\n                    data=noise_result.predict(len(noises), len(noises) + predict_length - 1, dynamic=True),\n                    index=pd.date_range(TEST_START_DATE, TEST_END_DATE)\n                )\n            noise_prediction[noise_prediction < 0] = 0\n        # Calculate ConfirmedCases from ConfirmedCases Increment\n        predicted_confirmed_cases= pd.concat([train_data, noise_prediction * (holt_fcast - 1)]).cumsum()\n\n        #predicted_confirmed_cases = pd.concat([train_data, holt_fcast]).cumsum()\n        # Set values to submission DF\n        indices = test.loc[\n            (test['Country_Region'] == country_region) & (test['Province_State'] == province_state)\n        ]\n        submission.loc[indices.index, feature] = predicted_confirmed_cases[indices['Date']].values\n\n\n\n        progressbar.value = i + 1\n        progresstext.value = '{} / {}'.format(i +1 , len(country_province_index))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nrows = 53\nncols = 6\nfigwidth = 18\nfig, axes = plt.subplots(nrows, ncols, figsize=(figwidth, figwidth * nrows / ncols))\nfor i, (country_region, province_state) in enumerate(country_province_index):\n    \n    predicted = submission.loc[(test['Country_Region'] == country_region) & (test['Province_State'] == province_state), 'ConfirmedCases']\n    row = i // ncols\n    col = i % ncols\n    axes[row][col].plot(predicted)\n    axes[row][col].set_title(f'{country_region} - {province_state}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nrows = 53\nncols = 6\nfigwidth = 18\nfig, axes = plt.subplots(nrows, ncols, figsize=(figwidth, figwidth * nrows / ncols))\nfor i, (country_region, province_state) in enumerate(country_province_index):\n    \n    predicted = submission.loc[(test['Country_Region'] == country_region) & (test['Province_State'] == province_state), 'Fatalities']\n    row = i // ncols\n    col = i % ncols\n    axes[row][col].plot(predicted)\n    axes[row][col].set_title(f'{country_region} - {province_state}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}