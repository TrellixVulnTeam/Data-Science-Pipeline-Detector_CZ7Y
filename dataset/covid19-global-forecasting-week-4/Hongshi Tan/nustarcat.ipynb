{"cells":[{"metadata":{"id":"S72TNZYlGYQC"},"cell_type":"markdown","source":"# 1.Introduction"},{"metadata":{"id":"vl7japWP9kGH"},"cell_type":"markdown","source":"In this report, we analyze the spreading and evolution of COVID-19 over 312 regions around the world and forecast the confirmed cases and fatalities between April 15 and May 14 using the previous observations. Our submission is publicly available in [Kaggle](https://www.kaggle.com/hongshitan/nustarcat)\n\nIn the first part of the report, we explore the data set in terms of structure and features, our data cleaning method is also illustrated in this part. The overviews of the features are given in the following section. In the third part, we provide the details of the selected model for modelling the growth of COVID-19 cases and approaches to estimate the corresponding parameters. We also conduct a comparison of our modelling approach with the naive linear regression method, which has been widely used and achieved desired performance. Due to the diversity of the training data set which is sampled globally, there are some corner cases and outliners during the parameter estimation that introduce negative influence on the prediction results, we adopt several outlier handling strategies, which are available in the fourth section. In the final part, we discuss the disadvantages of our model and the further work we aim to."},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"id":"1tEKv2-o4T22","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"id":"FopPHzPisRQs","trusted":true},"cell_type":"code","source":"import plotly.graph_objects as go\nimport plotly.express as px \n\nfrom sklearn import linear_model\nfrom sklearn import preprocessing\nfrom plotly.subplots import make_subplots\n\n#import plotly.io as pio\n#pio.renderers.default = \"notebook+pdf\" ","execution_count":null,"outputs":[]},{"metadata":{"id":"kZga4j22IAsm","trusted":true},"cell_type":"code","source":"path = \"../input/covid19-global-forecasting-week-4/\"\ntrain_df = pd.read_csv(path + \"train.csv\")\ntest_df = pd.read_csv(path + \"test.csv\")\nsubmission = pd.read_csv(path + \"submission.csv\")\n","execution_count":null,"outputs":[]},{"metadata":{"id":"T2fhuHvF3Iig"},"cell_type":"markdown","source":"# 2. Dataset Description\n\n\n"},{"metadata":{"id":"Cge6IY1fdqbY"},"cell_type":"markdown","source":"Our goal is to forecast confirmed cases and fatalities between April 15 and May 14 by region.\n\n\n- **a. What is the source of your data? What is it about?**  \nThe train dataset contains the confirmed cases and fatalites of COVID-19 by region between January 22 and May 15 2020, which is provided by JHU CSSE.\nThe test dataset includes the same region with train set with date between April 2 and May 14 2020.\n- **b. How many features and datapoints does it contain?**  \nThe `train_df` dataframe has a shape of (35995, 6), which contains daily number of cases and deaths of COVID-19 from 184 countries and 312 states. As is mentioned above, for each region the data is collected in 115 consecutive days.\n- **c. List a few (at most 10) features and describe them.**  \nSadly, there are only 6 features (technically only 5 are meaningful), which are `State,\tCountry,\tDate,\tConfirmedCases,\tFatalities`. In the following part of this section, the visualization of these features is given. "},{"metadata":{"id":"KEh5v4bmqoJm","trusted":true},"cell_type":"code","source":"#train_df.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"Jov8WbPxq2Z7","trusted":true},"cell_type":"code","source":"#test_df.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"mgWtbYXcpbeG","outputId":"cd7405fc-76f6-4a07-deaf-45bd5d0c5c72","trusted":true},"cell_type":"code","source":"train_df.groupby(['Province_State','Country_Region']).ConfirmedCases.describe()","execution_count":null,"outputs":[]},{"metadata":{"id":"36jm92_U9NLi"},"cell_type":"markdown","source":"## 2.1 Data Set Structure\n\n\nThis task includes two data sets: training data set and test data set.\nThe training data set conclude six column, the description of each column is as follows.\n\n* Id: Unique index of the observation\n* Province_state: Provinces and states of a specific country\n* Country_region: Name of Country\n* Date: Timestamp for the corresponding data\n* ConfirmedCases: The number of confirmed infected cases of COVID-19\n* Fatalities: The number of deaths caused by COVID-19\n\nThe train dataset contains the confirmed cases and fatalites of COVID-19 by region between January 22 and May 15 2020. The test dataset includes the same region with train set with date between April 2 and May 14 2020. \nThere is an overlapping bewteen the training dataset and the test dateset.Therefore, we remove the overlapping data in training dataset to avoid the data leakage."},{"metadata":{"id":"SJGAyb9n5gTX","trusted":true},"cell_type":"code","source":"def count_missing_values(df):\n    missing_values = 0\n    for column in df:\n        missing_values += df[column].isna().sum()\n    return missing_values\n\ndef get_col_have_missing_values(df):\n    col = []\n    for column in df:\n        \n        missing_values = df[column].isna().sum()\n        if not( missing_values == 0):\n            col += [column]\n    return col\n\ndef fill_missing_state(state, country):\n    if pd.isna(state):\n        return country\n    else :\n        return state","execution_count":null,"outputs":[]},{"metadata":{"id":"mqyGgpU33_0n"},"cell_type":"markdown","source":"## 2.2 Fill Missing Values\n\nBecause different countries have different policies on the statistic and release of the COVID-19 related data, the details of each province/state for some country, such as Germany, is not publicly avalible, and it casuses nan in the data set. Therefore, before the data analytics, we fill the missing province/state by its country name."},{"metadata":{"id":"vPU3soz4ObKS"},"cell_type":"markdown","source":"- **a. Are there missing values in your dataset?**  \n"},{"metadata":{"id":"zBMGwJHlOaOU","outputId":"73dca954-91df-4fc9-d0e6-23564446f12a","trusted":true},"cell_type":"code","source":"if  count_missing_values(train_df) == 0:\n    print('No')\nelse:\n    print('Yes, the following table shows the missing values intuitively.')\n    ","execution_count":null,"outputs":[]},{"metadata":{"id":"XZA5wnFYSufD","outputId":"6901c0d1-203c-4e54-853d-b86efe9d8294","trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"vx5h9Ds9OzEP"},"cell_type":"markdown","source":"- **b. If yes, please provide the statistics (such has how many, which features, etc.) of the missing values.**  "},{"metadata":{"id":"h1Q0afMdvdhQ","outputId":"32cc745f-c6ea-4b01-d9c5-d5f1e323f982","trusted":true},"cell_type":"code","source":"print(f\"The total number of missing value is {count_missing_values(train_df)}.\")","execution_count":null,"outputs":[]},{"metadata":{"id":"xLcnsaQZQjSz","outputId":"d2729080-3c3e-4f6c-b316-bee2af55d6f6","trusted":true},"cell_type":"code","source":"missing_cols = get_col_have_missing_values(train_df)\nprint('The incomplete feature is:')\nfor col in missing_cols:\n    print('\\t'+col)\n  ","execution_count":null,"outputs":[]},{"metadata":{"id":"D-3sS-WGd-lh","outputId":"c0f9a345-8cce-45a4-9a87-e864d31a94ea","trusted":true},"cell_type":"code","source":"print('All the missing values are State data from some countries (without state-specific data).')","execution_count":null,"outputs":[]},{"metadata":{"id":"kwSiOd18RFF6"},"cell_type":"markdown","source":"- **c. Do you want to remove such data-points from the dataset? Why?**  \nNo. Since for some large countries, like US or China, the population density distribution, human traffic and transmission efficiency vary with different provinces. And that's why we have more detailed data in a specific state to analyze and predict the future data of COVID-19. For those countries without `states` feature, we take the  country's data as a whole to predict future cases and fatalities, instead of removing these data_points.  "},{"metadata":{"id":"dY6syw3NRYdf"},"cell_type":"markdown","source":"- **d. Do you perform imputation to fill in missing values? What technique would you use?**  \nWe can fill the data just with the country name to replace NAN because the `State` represent geographic information similar to `country` but more specific. When there is no `State` value, we can just use the `Country` value to represent the Place."},{"metadata":{"id":"vqAhtTT04i1k","trusted":true},"cell_type":"code","source":"\n# modify col name and fill in missing value in 'Province_State' with 'Country_Region'\ntest_df.rename(columns={'Province_State':'State','Country_Region':'Country'}, inplace=True)\ntest_df['State'] = test_df.apply(lambda df: fill_missing_state(df['State'],df['Country']),axis=1)\n\ntrain_df.rename(columns={'Province_State':'State','Country_Region':'Country'}, inplace=True)\ntrain_df['State'] = train_df.apply(lambda df: fill_missing_state(df['State'], df['Country']) ,axis=1)","execution_count":null,"outputs":[]},{"metadata":{"id":"R62j7YQ66V5A"},"cell_type":"markdown","source":"## 2.3 Data Analytics\n\nWe conduct several explorative data analysis which includes:\n-  A TOP 20 ranking of confirmed cases/fatalities grouped by countries. \n- A visualization of global cases/fatalities grouped by countries. "},{"metadata":{"id":"4h7SnfX068Or","trusted":true},"cell_type":"code","source":"# constant\nplot_days = 14\ncases = 20\npower = 1\nalpha = 0.3\nrmsle_cal_days = 28","execution_count":null,"outputs":[]},{"metadata":{"id":"fb6YEeDW37F0","trusted":true},"cell_type":"code","source":"def global_overview(data):\n    plot = train_df.loc[(train_df['Country'].isin(countries))].groupby(['Date', 'Country', 'State']).max().groupby(['Date', 'Country']).sum().sort_values(by=data, ascending=False).reset_index()\n    plot2 = train_df.groupby(['Date'])[data].sum().reset_index()\n    fig = px.bar(plot, x=\"Date\", y=data, color=\"Country\", barmode=\"stack\")\n    fig.add_scatter(x=plot2['Date'], y=plot2[data],name='Global Trend') \n    fig.update_layout(title=data,width=1000,\n    height=500,)\n    \n    fig.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"TqF7X85f52CG","outputId":"0834ead6-0ab6-4645-e6cd-da0d18ef8b5b","trusted":true},"cell_type":"code","source":"# A sorted dataframe according to countries. \ndisplay_df = train_df.drop(['Fatalities'], axis= 1)\ndf_confirmedcases = display_df.groupby(['Country','State']).max().groupby('Country').sum().sort_values(by='ConfirmedCases', ascending=False).reset_index().drop(columns='Id')\ncountries = df_confirmedcases[:10]['Country'].unique().tolist()\ndf_confirmedcases[:10].set_index('Country').transpose()\n","execution_count":null,"outputs":[]},{"metadata":{"id":"j-prakY7WN01"},"cell_type":"markdown","source":"The above table shows that, before May 15, 2020, the United States has the most COVID-19 confirmed infections. The Russia holds the second place in trems of confirmed cases."},{"metadata":{"id":"lIWNX4Ix49Hn","outputId":"0ce8af48-3921-424f-d457-3e81a148c7af","trusted":true},"cell_type":"code","source":"global_overview('ConfirmedCases')","execution_count":null,"outputs":[]},{"metadata":{"id":"HaXRvkoPaC_o"},"cell_type":"markdown","source":"The above figure depicts the global trend of COVID-19 confirmed cases, of which the growth after March 29 is almost linear. The number of infected had exceeded 3 million by the end of April.\n"},{"metadata":{"id":"jbU6wpKxYfhS","outputId":"9aa77f9c-f994-4a76-84de-5af118b74de0","trusted":true},"cell_type":"code","source":"display_df = train_df.drop(['ConfirmedCases'], axis= 1)\ndf_confirmedcases = display_df.groupby(['Country','State']).max().groupby('Country').sum().sort_values(by='Fatalities', ascending=False).reset_index().drop(columns='Id')\ncountries = df_confirmedcases[:10]['Country'].unique().tolist()\ndf_confirmedcases[:10].set_index('Country').transpose()\n","execution_count":null,"outputs":[]},{"metadata":{"id":"PIx-jeXwYab2"},"cell_type":"markdown","source":"The top 10 fatalities countries are listed in this table. Due to a large amount of confrimed cases, the United States also has the most fatalites, and the United Kingdom is the second one. "},{"metadata":{"id":"HYJBVadJ8f31","outputId":"cc2139c4-8c82-43a6-915f-a20e364c2991","trusted":true},"cell_type":"code","source":"global_overview('Fatalities')","execution_count":null,"outputs":[]},{"metadata":{"id":"bth5HGJ1bMKK"},"cell_type":"markdown","source":"The above figure depicts the global trend of COVID-19 death cases. The growth ratio of fatalities declined slightly. The top 10 countries make up the majority of fatalities"},{"metadata":{"id":"lbsr0I6iHy8u"},"cell_type":"markdown","source":"# 3. Feature Engineering"},{"metadata":{"id":"VkfKz6UQfvld"},"cell_type":"markdown","source":"In this section, we clean up the data set and prepare the data for the constuction of our model.\n\nWe remove the leakage existing in training data set. The training data set contains the observation from 2020-04-02 to 2020-05-15, which is the period needs to predict, and we eliminate them by setting zeros.\n\nThere are only five features in the orignal data set, which are State, Country, Date, ConfirmedCases, Fatalities. As the `Province_State` and `Country_Region` columns are string type which is not friendly during the data process, we transform the values in these columns into unique numerical indexes and create corrsponding remapping dictionarys. Then, we separate `Date` column into `Day`, `Week`, `Month`, `DayOfWeek` for futher exploration.\n\n\n\n"},{"metadata":{"id":"bX9e10KlcnrB","trusted":true},"cell_type":"code","source":"data_leak = pd.merge(train_df,test_df, how='inner', on='Date')['Date'].unique().tolist()\ndata_leak.append('2020-05-15')\ndata_leak.sort()\n","execution_count":null,"outputs":[]},{"metadata":{"id":"7C9bAfYdMxb8","trusted":true},"cell_type":"code","source":"train_df_fix = train_df.loc[~train_df['Date'].isin(data_leak)]\ndf_all = pd.concat([train_df_fix, test_df], axis = 0, sort=False)\n\n\ndf_all['ConfirmedCases'].fillna(0, inplace=True)\ndf_all['Fatalities'].fillna(0, inplace=True)\ndf_all['Id'].fillna(-999, inplace=True)\ndf_all['ForecastId'].fillna(-999, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"56syOsRSMxe8","trusted":true},"cell_type":"code","source":"def create_features(df_in):\n    df = df_in.copy()\n    df['Day_num'] = le.fit_transform(df['Date'])\n    df['Date'] = pd.to_datetime(df['Date'])\n    df['Day'] = df['Date'].dt.day\n    df['Week'] = df['Date'].dt.isocalendar().week\n    df['Month'] = df['Date'].dt.month\n    #df['Year'] = df['Date'].dt.year\n    df['DayOfWeek'] = df['Date'].dt.dayofweek\n    \n    df['Country'] = le.fit_transform(df['Country'])\n    country_dict = dict(zip(le.inverse_transform(df['Country']), df['Country'])) \n    \n    df['State'] = le.fit_transform(df['State'])\n    state_dict = dict(zip(le.inverse_transform(df['State']), df['State']))\n    \n    return df, country_dict, state_dict","execution_count":null,"outputs":[]},{"metadata":{"id":"ZDI0Qpb0M71C","trusted":true},"cell_type":"code","source":"le = preprocessing.LabelEncoder()\n\ndf_all, country_dict, state_dict = create_features(df_all)\n\ngolden_df,golden_country_dict, golden_state_dict = create_features(train_df)\n\ninv_country_dict = {v: k for k, v in country_dict.items()}\ninv_state_dict = {v: k for k, v in state_dict.items()}","execution_count":null,"outputs":[]},{"metadata":{"id":"QztzRADmx0oT"},"cell_type":"markdown","source":"# 4. Modeling"},{"metadata":{"id":"H4-Z2n7jQxUi"},"cell_type":"markdown","source":"## 4.1 Logistic Curve Regression\nA logistic function is a common S-shaped curve (also called sigmoid curve) with equation:\n\n$$f(x) = \\frac{L}{1+e^{-k(x-x_0)}}$$\nwhere:\n* $x_0$ is the $x$ value of the sigmoid's midpoint;  \n* $L$, the curve's maximum value;  \n* $k$, the logistic growth rate or steepness of the curve.  \n\nA logistic function, or related functions (e.g. the Gompertz function) are usually used in a descriptive or phenomenological manner because they fit well not only to the early exponential rise, but to the eventual levelling off of the pandemic as the population develops a herd immunity. the curve can be used to model a pandemic [[1]](https://en.wikipedia.org/wiki/Logistic_function#In_medicine:_modeling_of_a_pandemic).\nTherefore, we choose a logistic function as the model for the increase in the number of confirmed COVID-19 cases and deaths over time. Generally, three coefficients: the growth ratio $k$, the maximum value $L$ and the midpoint $x_0$ need to be estimated. \n\nThe growth ratio $k$ is influenced by many factors, such as social distance government policy and population density, which can be different among countries, even the provinces in the same country. Therefore, we fit the coefficients of each place independently, and the propagation cross the place is not considered.\n\nWe use the following ways to fit the growth ratio $k$ and the maximum value $L$. Firstly, from the derivative of $f(x)$:\n\n$$\n\\begin{aligned}\n\\frac{d}{dt}f(x) &= L \\cdot k \\cdot e^{-k (x - x_0)} \\cdot \\left(1 + e^{-k  (x - x_0)} \\right)^{-2} \\\\\n &=  k \\cdot \\frac{L }{1 + e^{-k(x-x_0)}} \\cdot  \\left( \\frac{ e^{-k (x-x_0)}}{1 + e^{-k (x-x_0)}} \\right) \\\\\n &=  k \\cdot f(x) \\cdot \\left( 1 - \\frac{f(x)}{L} \\right) \\\\\n\\frac{\\frac{d}{dt}f(x)}{f(x)} &= k \\cdot \\left( 1 - \\frac{f(x)}{L} \\right) \n\\end{aligned}\n$$\n\nWe notice that the proportional growth ratio $\\frac{\\frac{d}{dt}f(x)}{f(x)}$ has linear relation with the growth $f(x)$. Hence, we adopt the linear regression on the training dataset to fit the above equation. Finally, $k$  and $L$ can be calculated by the coefficients from linear regression.\n\nThe midpoint $x_0$ can be treated as a translation transformation of the logistic curve on the time axis ($x$ axis). We construct an error function $h(x_0)$: \n\n$$h(x_0) = \\sum_{i \\in T}{\\left( f(i,x_0) - g(i) \\right) }$$\nwhere:\n* $T$ is the set of time points in training dataset;\n* $g(i)$ is the ground truth of confirm cases or fatalites at time $i$ (from training dataset);\n* $f(i,x_0)$ is the output of logistic function with midpoint $x_0$ at time $i$;\n\n\nThe optimal $x_0$ holds the equation $h(x_0) = 0$. Hence, we adopt the Newton's method for solving this equation to get the optimal $x_0$.\n\n"},{"metadata":{"id":"nLVKSJsYmRR0"},"cell_type":"markdown","source":"           \nIn this task, we choose Root Mean Squared Logarithmic Error (RMSLE) to evaluate the estimation.\nSince the result of RMSE tends to be dominated by some large values when the range of predicted values is large. In this way, even if the model predicts a lot of small values accurately, the RMSE may be large because one very large value is not accurate. On the contrary, if another poor algorithm is more accurate for large values, but not good at many small values, the RMSE may be smaller than the previous one.             \nThis problem can be improved by taking logarithm first and then computing RMSE. RMSLE penalizes under-prediction more than over-prediction."},{"metadata":{"id":"Rp6qQsKa8oYQ","trusted":true},"cell_type":"code","source":"def RMSLE(pred,actual):\n    return np.sqrt(np.mean(np.power((np.log(pred+1)-np.log(actual+1)),2)))","execution_count":null,"outputs":[]},{"metadata":{"id":"6dRSd0GeQkKS","trusted":true},"cell_type":"code","source":"def filter_df_state(df ,country_dictionary,state_dictionary,country_name,state_name):\n    df_country = df.copy()\n    df_country = df_country.loc[df_country['Day_num'] >= 0]\n    if type(country_name) == type('str'):\n        df_country = df_country.loc[df_country['Country'] == country_dictionary[country_name]]\n        df_country = df_country.loc[df_country['State'] == state_dictionary[state_name]]\n    else:\n        df_country = df_country.loc[df_country['Country'] == country_name]\n        df_country = df_country.loc[df_country['State'] == state_name]\n    features = ['Id', 'State', 'Country','ConfirmedCases', 'Fatalities', 'Day_num']\n    df_country = df_country[features]\n    return df_country","execution_count":null,"outputs":[]},{"metadata":{"id":"pwahL4_aoBjN","trusted":true},"cell_type":"code","source":"def error_function(L, k, groundtruth, tn):\n    logis = L / (1. + np.exp(-k * (np.arange(len(groundtruth)) - tn)))\n    max_value =  max([max(logis), max(groundtruth.values)])\n    normed_pred = [i/max_value for i in logis]\n    normed_real = [i/max_value for i in groundtruth.values]\n    error = sum(np.array(normed_pred) - np.array(normed_real))/len(logis)\n    return error","execution_count":null,"outputs":[]},{"metadata":{"id":"SWYckDYtoGs3","outputId":"4bade393-486d-484e-e9d4-183698b30c83","trusted":true},"cell_type":"code","source":"start = df_all[df_all['Id']==-999].Day_num.min()\nend = df_all[df_all['Id']==-999].Day_num.max()\ntotal_day = end + 1\nprint('prediction start day: {} end day: {}'.format(start, end))","execution_count":null,"outputs":[]},{"metadata":{"id":"ULCmIXMwMDnT"},"cell_type":"markdown","source":"\nFunction description:\n- Dataset preparation: function `prepare_training`\n- error function: `error = sum(y_truth - y_hat)`\n- predict with the sigmoid function regression with `country_calculation_logistic` function\n\nParameter description:\n- method 0: Ridge Regression\n- method 1: Linear Regression\n- method 2: Ransac                                           \n\nUse one of these three methods to compute the appropriated coefficients for logistic curve.                            \nD(t) is the number of confirmed cases/fatalities of day t, and Ratios = (D(t+1)-D(t-1))/(2*D(t)).                      \nWhen y_hat = 0, x is the maximum of ConfirmedCases/fatalities of `country`, that is, `L`. And the intercept of linear regression is the approximated value growth rate `k`. So we can compute the coefficients for logistic curve from outputs `a` and `b`.                             \nIn this way, we transform the logistic curve problem into a linear regression problem, which is easier to detect outlier by ransacRegressor.                                 \nFor the last coefficient `t0`, we use Newton's method to iteratively get `tn` and reset `res_t0` = `tn`."},{"metadata":{"id":"TNqMOJAOoKxH","trusted":true},"cell_type":"code","source":"def ratios_calculation(country, all_case, method, t0=0, plot=0, min_case = 1, start_day = 0,cut_ratio = 0.3):\n    '''\n    params:\n    country: cases data for a single region\n    method: choose one method to get the \n    t0: the day of the inflexion\n    plot: decide to draw the plot or not\n    min_case: the minimum number of cases\n    start_day: prediction start date \n    cut_ratio:\n    return: L,k,res_t0, error, pd.Series(data=prediction)\n    '''\n    ground_truth = country.copy()\n    country  =  country[country >= min_case]\n    if (len(country) >= 10):\n        cut_ratio = 0.1\n    slopes = 0.5 * ((country.diff(1) - country.diff(-1)))\n    ratios = slopes / country\n    x1 = country.values[1:-1]\n    y1 = ratios.values[1:-1]\n    x, y = [],[]\n    \n    for i in range(len(x1)):\n        if not ((x1[i] <= x1.max()* cut_ratio )\n                and ((y1[i] <= y1.max()* cut_ratio) )):\n           # if (y1[i] < y1.max()  * 0.9):\n            x += [x1[i]]\n            y += [y1[i]]\n\n    x, y = np.array(x), np.array(y)\n    X = x.reshape(-1, 1) \n   \n    try:\n        if method == 0:\n            reg = linear_model.Ridge(fit_intercept=True, normalize=True)\n            reg.fit(X, y)\n            a = reg.coef_[0]\n            b = reg.intercept_\n        elif method == 1:\n            reg = linear_model.LinearRegression(fit_intercept=True, normalize=True)\n            reg.fit(X, y)\n            a = reg.coef_[0]\n            b = reg.intercept_\n        else: # RANSAC\n            try :\n                ransac = linear_model.RANSACRegressor()\n                reg = ransac.fit(X, y)\n                reg.fit(X, y)\n                a = reg.estimator_.coef_[0]\n                b = reg.estimator_.intercept_\n            except:\n                print(\"RANSAC failed\")\n                #reg = linear_model.LinearRegression(fit_intercept=True, normalize=True)\n                reg = linear_model.Ridge(fit_intercept=True, normalize=True)\n                reg.fit(X, y)\n                a = reg.coef_[0]\n                b = reg.intercept_\n\n        #print(method)\n    except:\n        if plot >=4:\n            print(\"[DEBUG] unexpected error, using the default value {}\".format(len(country)))\n        L,k, res_t0,error= 0,0,0,0\n        prediction = pd.Series(\n            data=[ ground_truth.values[i] if i < len(ground_truth) else ground_truth.max() for i in range(total_day +1)])\n        return L,k,res_t0, error, prediction\n   \n    L = -b / a \n    if (a > 0) or (np.isnan(L)):\n        if plot >=4:\n            print(\"[DEBUG] unexpected regression results: a {} L {}\".format(a, L))\n        L,k, res_t0,error= 0,0,0,0\n        prediction = pd.Series(\n            data=[ ground_truth.values[i] if i < len(ground_truth) else ground_truth.max() for i in range(total_day +1)])\n        return L,k,res_t0, error, prediction\n\n    k = b\n    L = -b / a \n    y_hat = a * x + b \n    \n   \n        \n    tn = t0\n    epsilon  = 0.01\n    max_iteration = 100\n\n    # Newton's method to calculate t0 \n    for n in range(0, max_iteration):\n        error  = error_function(L, k, ground_truth,tn)\n        derror = (error_function(L, k, ground_truth,tn + 1) - error_function(L, k, ground_truth,tn -1))/2\n        if plot >= 4:\n            print('[DEBUG] error: {}, derror: {}'.format(error,derror))\n        if (abs(error) <  epsilon):\n            break\n        if (abs(derror) < epsilon): # accepct\n            break\n        tn = tn - error/derror\n\n    if np.isnan(tn):\n        res_t0 = t0\n        #print('nan detected')\n    else:\n        res_t0 = tn\n\n    #res_t0 += start_day\n    prediction= [L / (1. + np.exp(-k * (t - res_t0))) for t in range(0 ,total_day + 1)]\n    \n    if plot >= 1:\n        fig = go.Figure()\n        fig = make_subplots()\n        fig.add_trace(go.Scatter(x=x, y=y, mode=\"markers\", name='Proportional growth ratio'))\n        fig.add_scatter(x=x, y=y_hat, \n                        name='Linear regression')\n\n        fig.update_layout(title='Linear Regression on the Proportional Growth Ratio and Number of Growth',\n                       xaxis_title='Number of Growth',\n                       yaxis_title='Proportional Growth Ratio')\n        fig.show()\n        \n    if plot >= 2:\n        logis = L / (1. + np.exp(-k * (np.arange(len(ground_truth)) - res_t0)))\n        basic_xaixs = list(range(len(logis)))\n        fig = go.Figure()\n        fig = make_subplots()\n        fig.add_scatter(x=basic_xaixs, y=ground_truth.values, \n                        name='Ground Truth', \n                        line=dict(  color=\"MediumPurple\",\n                                    width=4,\n                                    dash=\"dot\",)\n                       )\n        fig.add_scatter(x=basic_xaixs, y=logis, \n                        name='Proposed Model')\n\n        fig.update_layout(title='Our Model versus the Ground Truth ',\n                       xaxis_title='Days',\n                       yaxis_title='Number of Cases')\n        fig.show()\n\n    return L,k,res_t0, error, pd.Series(data=prediction)","execution_count":null,"outputs":[]},{"metadata":{"id":"iGyOeJCcs-Am"},"cell_type":"markdown","source":"Use the coefficients computed by `ratios_calculation` to finish logistic curve regression."},{"metadata":{"id":"6g8d5lqKoK-J","trusted":true},"cell_type":"code","source":"def fit_logistic_curve(country,state, t0, method = None, plot = 1, target = 'ConfirmedCases', res_df = None,cut_ratio = 0.3):\n    '''\n    params:\n    country: input the country feature\n    state: input the state feature\n    t0: the day of the inflexion\n    method: choose a method mentioned above to compute the coefficients of logistic cuurve, default value = None\n    plot:\n    target: predict confirmed cases or fatalities, default value = 'ConfirmedCases'\n    res_df: \n    cut_ratio:\n    return: res[selected_index]\n    '''\n    df_country = filter_df_state(df_all,country_dict, state_dict, country,state)\n    df_golden = filter_df_state(golden_df,golden_country_dict,golden_state_dict,country,state)\n    start_day = df_country[df_country[target] >= 1]['Day_num'].min()\n    #print(start_day)\n\n    min_case = 1\n    test_country = df_country[df_country['Day_num'] < start][target]\n    all_case_country = df_golden[target]\n    \n    num_of_tests = 5\n    rmsle = [None] * num_of_tests\n    res = [None] * num_of_tests\n    methods = [None] * num_of_tests\n    \n      \n \n    if test_country.max() < 1:\n        prediction_res = pd.Series(data=[0 for i in range(total_day +1)])\n        #fake \n        selected_index = 0\n        methods[selected_index] = method \n        res[selected_index] = prediction_res\n        rmsle[selected_index] = RMSLE(prediction_res[start:start + rmsle_cal_days],\n                                      all_case_country.values[start: start+ rmsle_cal_days])\n        L,k = 0,0 \n        \n    else:\n        if method == None: #debug only\n            for tests in range(len(res)):\n                L_array,k_array,t_array, error_array = [None]*3,[None]*3,[None]*3,[None]*3\n                for i  in range(3):\n                    L, k,t, error, prediction_res \\\n                        = ratios_calculation(test_country,\n                                             all_case_country, \n                                             i, \n                                             t0=t0,\n                                             plot=0,\n                                             start_day=start_day,\n                                             cut_ratio=cut_ratio)\n                    error_array[i] = error\n                #print(error)\n                error_array = np.abs(error_array)\n                index_min = np.argmin(error_array)\n                res[tests] = prediction_res \n                rmsle[tests] = RMSLE(prediction_res[start:start + rmsle_cal_days],\n                                     all_case_country.values[start: start+ rmsle_cal_days])\n                methods[tests] = index_min\n            selected_index = np.argmin(rmsle)\n        else:\n            L, k,t, error, prediction_res  \\\n                    = ratios_calculation(test_country,\n                                         all_case_country, \n                                         method, \n                                         t0=t0,\n                                         plot=plot,\n                                         start_day=start_day,\n                                         cut_ratio=cut_ratio)\n            selected_index = 0\n            methods[selected_index] = method\n            res[selected_index] = prediction_res\n            rmsle[selected_index] = RMSLE(prediction_res[start:start + rmsle_cal_days],\n                                          all_case_country.values[start: start+ rmsle_cal_days])\n\n   \n\n    if plot >= 3:\n        basic_xaixs = list(range(len(all_case_country)))\n        train_xaixs = list(range(len(test_country)))\n        fig = go.Figure()\n        fig = make_subplots()\n        fig.add_trace(go.Scatter(x=train_xaixs, y=test_country.values, mode=\"markers\", name='Training data'))\n        fig.add_trace(go.Scatter(x=basic_xaixs[len(train_xaixs):], \n                                 y=all_case_country.values[len(train_xaixs):],\n                                 mode=\"markers\", name='Ground Truth'))\n        fig.add_scatter(x=basic_xaixs, y=res[selected_index], \n                        name='Prediction')\n        max_y = max([max(res[selected_index]),max(test_country.values),max(all_case_country.values)])\n        fig.add_annotation( x=start, y=-0.1 * max_y, xref=\"x\", yref=\"y\", text=\"Start of Prediction\", showarrow=True, \n            font=dict(size=12),\n            align=\"center\",  arrowhead=0, arrowsize=1, arrowwidth=2, ax=-0, ay=-345, borderwidth=2, borderpad=4, opacity=1\n            )\n        fig.update_layout(title='Prediction',\n                       xaxis_title='Days',\n                       yaxis_title='Number of Cases')\n        fig.show()\n\n    if plot >=1:\n        print(\"RMSLE: {} by method {}\".format(rmsle[selected_index],methods[selected_index]))\n    if plot >= 4:\n        print(\"[DEBUG] L:{} k:{}\".format(L, k))\n\n    return res[selected_index]\n","execution_count":null,"outputs":[]},{"metadata":{"id":"sx14SItdoLH7","outputId":"537e4b4a-7c0e-4f39-8dea-e0f3814c9228","trusted":true},"cell_type":"code","source":"res = fit_logistic_curve('Australia','New South Wales',15,plot=3 )","execution_count":null,"outputs":[]},{"metadata":{"id":"beE_cfVAsNsY","outputId":"d88bb7a5-50ae-4506-fb55-262167ad72ee","trusted":true},"cell_type":"code","source":"res = fit_logistic_curve('China','Hubei',15, plot=3)","execution_count":null,"outputs":[]},{"metadata":{"id":"aHRQ36OssXpr"},"cell_type":"markdown","source":"## 4.2 Comparision with Linear Regression"},{"metadata":{"id":"tDvELDZgukgp","trusted":true},"cell_type":"code","source":"def linear_regression_prepare_training(df,d,day,filtered_cols,target):\n    '''\n    params:\n    df: data frame\n    d: the higher bound of day_num for training df\n    day: the lower bound of day_num\n    filtered_cols: choose cases or fatalities\n    return: tuple of training x, y and test_x\n    '''\n    df=df.loc[df['Day_num'] >= day]\n    df_train = df.loc[df['Day_num'] < d]\n    x = df_train\n    y = df_train[target]\n    x = x.drop(columns=filtered_cols).drop(columns=target)\n    res = df.loc[df['Day_num'] == d]\n    test_x = res\n    test_x = test_x.drop(columns=filtered_cols).drop(columns=target) \n    x.drop('Id', inplace=True, errors='ignore', axis=1)\n    x.drop('ForecastId', inplace=True, errors='ignore', axis=1)\n    test_x.drop('Id', inplace=True, errors='ignore', axis=1)\n    test_x.drop('ForecastId', inplace=True, errors='ignore', axis=1)\n    return x,y,test_x","execution_count":null,"outputs":[]},{"metadata":{"id":"fsD4Xy68uXrs","trusted":true},"cell_type":"code","source":"def linear_regression_calculation(df_all,country,date,day):\n    def lin_reg(X_train, Y_train, x_test):\n        regr = linear_model.Ridge()\n        regr.fit(X_train, Y_train)\n        pred = regr.predict(x_test)\n        return regr, pred\n    def lag_feature(df,target,lags):\n        for lag in lags:\n            lag_col = target + \"_{}\".format(lag)\n            df[lag_col] = df.groupby(['Country','State'])[target].shift(lag, fill_value=0)\n        return df\n    targets = ['ConfirmedCases', 'Fatalities']\n    lags = [40 , 20]\n    df_country = df_all.copy()\n    df_country = df_country.loc[df_country['Date'] >= date]\n    df_country = df_country.loc[df_country['Country'] == country_dict[country]]\n    features = ['Id', 'State', 'Country','ConfirmedCases', 'Fatalities', 'Day_num']\n    df_country = df_country[features]\n    \n    for i in range(len(targets)):\n        df_country = lag_feature(df_country, targets[i], range(1, lags[i]))\n\n    filter_col_confirmed = [col for col in df_country if col.startswith('Confirmed')]\n    filter_col_fatalities= [col for col in df_country if col.startswith('Fataliti')]\n    filter_col = np.append(filter_col_confirmed, filter_col_fatalities)    \n    filtered_cols = [filter_col_fatalities, filter_col_confirmed]\n\n    df_country[filter_col] = df_country[filter_col].apply(lambda x: np.float_power(np.log1p(x), power))\n\n    for d in range(start,start + 28+ 1):\n        df_country.replace([np.inf, -np.inf], 0, inplace=True)\n        df_country.fillna(0, inplace=True)\n        for i in range(len(targets)):\n            \n            X_train_1,Y_train_1,x_test_1 = linear_regression_prepare_training(df_country,d,day,\n                                                                              filtered_cols[i],targets[i])\n            regr_1, pred_1 = lin_reg(X_train_1, Y_train_1, x_test_1)\n            df_country.loc[(df_country['Day_num'] == d) & (df_country['Country'] == country_dict[country]), targets[i]] = pred_1[0]\n            df_country = lag_feature(df_country, targets[i], range(1, lags[i]))   \n    #print(\"Calculation done.\")\n    return df_country","execution_count":null,"outputs":[]},{"metadata":{"id":"kZdzAw3AtjU7","trusted":true},"cell_type":"code","source":"def linear_regression_plot(df,test_con):\n    groundtruth = train_df[(train_df['Country'] == test_con) & (train_df['Date'] >= '2020-03-10')].reset_index()\n    df = df[df['Day_num'] >= 48]\n    df = df[df['Day_num'] <= start + 14]\n    df = df[:len(groundtruth)].reset_index()\n    df['GroundTruthConfirmedcases'] = groundtruth['ConfirmedCases']\n    df['GroundTruthFatalities'] = groundtruth['Fatalities']\n    df['ConfirmedCases'] = df['ConfirmedCases'].apply(lambda x: np.float_power(np.expm1(x), 1/power))\n    df['Fatalities'] = df['Fatalities'].apply(lambda x: np.float_power(np.expm1(x), 1/power))\n\n    fig = go.Figure()\n    fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n    fig.add_scatter(x=df['Day_num'], y=df['ConfirmedCases'], name='Confirmedcases - Prediction')\n    fig.add_scatter(x=df['Day_num'], y=df['GroundTruthConfirmedcases'], name='Confirmedcases')\n    fig.add_scatter(x=df['Day_num'], y=df['Fatalities'], name='Fatalities - Prediction', secondary_y=True)\n    fig.add_scatter(x=df['Day_num'], y=df['GroundTruthFatalities'], name='Fatalities ', secondary_y=True)\n    max_y = max([max(df['ConfirmedCases']),max(df['GroundTruthConfirmedcases']),max(df['Fatalities']), max(df['GroundTruthFatalities'])])\n    fig.add_annotation( x=start, y=-0.1 * max_y, xref=\"x\", yref=\"y\", text=\"Start of Prediction\", showarrow=True, \n            font=dict(size=12),\n            align=\"center\",  arrowhead=0, arrowsize=1, arrowwidth=2, ax=-0, ay=-345, borderwidth=2, borderpad=4, opacity=1\n            )\n\n    fig.update_layout(title='Prediction of '+ test_con,\n                       xaxis_title='#Days ',\n                       yaxis_title='Confirmed Cases')\n\n    fig.update_yaxes(title_text=\"Confirmed Cases\", secondary_y=False)\n    fig.update_yaxes(title_text=\"Fatalities\", secondary_y=True)\n    \n    return fig.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"j-DKiHkXt9Gq","trusted":true},"cell_type":"code","source":"def linear_regression_plot_country(country):\n    df_check = linear_regression_calculation(df_all, country, '2020-03-10', 48)\n    linear_regression_plot(df_check,country)","execution_count":null,"outputs":[]},{"metadata":{"id":"y9ixhr7VuhBk","outputId":"bac3c6b6-5554-4bea-ccb1-0a558242496e","trusted":true},"cell_type":"code","source":"linear_regression_plot_country('Germany')","execution_count":null,"outputs":[]},{"metadata":{"id":"wykWPwwFMJe9"},"cell_type":"markdown","source":"We also make a comparsion with the naive linear regression approaches, of which the asssumption is the exponential growth. The above figure shows the ground truth and the prediction of confirmed cases and fatalites in Germany. The perdiction fit well in the training data set, however it is easy to fail in long term perdiction. The reason why some of the naive linear regression based submission can also achieve a good result is that they do not handle the leakage of test data."},{"metadata":{"id":"tADHLZYeD0uL"},"cell_type":"markdown","source":"# 5. Outliers.\n- **a. Have you found any outliers in your data? How have you found them?**   \nSince the data is from the statistics all over the world, and the situation of different places are different, it is hard to determine ***whether it is an outlier or an incident-related/policy-related change***. So we plot some figures to see the trend of the data and try to find some unusual things in the visualization. There are some discoveries but we can't assure any outliers without further information.  \n- **b. Do you plan to remove them or keep them in your data? Why?**\nDetails are as follows  \n"},{"metadata":{"id":"134Tup2Y8hG4"},"cell_type":"markdown","source":"## 5.1 Outliers in Data Set"},{"metadata":{"id":"hVnshdeQ97hV","trusted":true},"cell_type":"code","source":"df_exp = train_df.groupby(['State']).sum()\ndf_exp = df_exp.reset_index()\nstate_exp = df_exp.sort_values(by='ConfirmedCases', ascending=False)[:20]['State'].to_list()\ndf_exp = train_df.groupby(['State', 'Date']).sum().reset_index()","execution_count":null,"outputs":[]},{"metadata":{"id":"8V5g1TNOD0Gc","outputId":"4cd0fe8e-9615-41dd-cf62-07ca0498f12f","trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\nimport seaborn as sns\nfig, axes = plt.subplots(5,4, figsize=(25,20), sharex=True, sharey=True)\nfor i, state in enumerate(state_exp):\n    a = i//4\n    b = i % 4\n    df_x = df_exp[df_exp.State==state]\n    sns.scatterplot(df_x.Date, df_x.ConfirmedCases, ax=axes[a][b])\n    # axes[i].title.set_text('First Plot')\n","execution_count":null,"outputs":[]},{"metadata":{"id":"JzNl9KEiqR-m"},"cell_type":"markdown","source":"We can see from the figure that:\n- figure(0, 3) , (1,0), (1,1)"},{"metadata":{"id":"OC1SR313DzUF","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"taPnq4ro8DBA"},"cell_type":"markdown","source":"## 5.2 Outliers in Model Fiting"},{"metadata":{"id":"tP72D-OgvKes","outputId":"be98273d-67bd-44ca-a3a9-0977c6242fa2","trusted":true},"cell_type":"code","source":"res = fit_logistic_curve('West Bank and Gaza', 'West Bank and Gaza',15,0,plot = 4,target = 'Fatalities')","execution_count":null,"outputs":[]},{"metadata":{"id":"HbSvUXy57HFV"},"cell_type":"markdown","source":"For the region only has a few number of confirm cases or fatalities before the start of prediction, it lacks enough vaild data to fit the logistic curve. For example, West Bank and Gaza, which only have one vailed point in terms of proportional growth ratio (the 63rd day in above figure), fails the regression. This is a typical pathological case during the modelling. Currently, we directly use the latest available observation and give a constant prediction in the future. "},{"metadata":{"id":"As9-KhiAvQqk","outputId":"7e890901-3dd2-4ce9-e4f6-d2ea8eea768b","trusted":true},"cell_type":"code","source":"res = fit_logistic_curve('China','Hubei',15,1, plot= 2)","execution_count":null,"outputs":[]},{"metadata":{"id":"djsyZnUi_QaD"},"cell_type":"markdown","source":"In idea case, the change of proportional growth ratio over the time is constant. However, there are many factors in real world influencing the growth ratio. Discontinuity factors, such as the change of the standard for judging the infection or government regulation, introduce ourliers and cause unexpected prediction result. For example, in Hube, China, we can see there is a jump on the 21st day, which is because of a change in diagnositc criteria. This data is an outlier in the regression, and we adopt random sample consensus (RANSAC) algorithm to deal with outliers. RANSAC is a learning technique to estimate parameters of a model by random sampling of observed data.  Given a dataset whose data elements contain both inliers and outliers, RANSAC uses the voting scheme to find the optimal fitting result and remove the outliers [[7]](https://en.wikipedia.org/wiki/Random_sample_consensus). "},{"metadata":{"id":"ETkDGafWvWGb","outputId":"b4176e48-67b6-43e1-ffa1-640fb7f58cfe","trusted":true},"cell_type":"code","source":"res = fit_logistic_curve('China','Hubei',15,2, plot= 2)","execution_count":null,"outputs":[]},{"metadata":{"id":"DEjMuQfPH4gj"},"cell_type":"markdown","source":"The above figures are the result of adopting RANSAC. In terms of RMSLE, we can see an order of magnitude improvement is brought by the RANSAC, that is from 0.02 to 0.003. After the 40th day, the RANSAC-enabled estimations perfectly fit with the ground truth, while the results from the original method have clear biases."},{"metadata":{"id":"yPooVgl3u41o"},"cell_type":"markdown","source":"# 6. Conclusion"},{"metadata":{"id":"XhBmRY__QoV-"},"cell_type":"markdown","source":"We choose the logistic curve fitting as our baseline modeling approach, and combining with the RANSAC, our approach achieve an acceptable result both in short-range prediction and long-range predicition. However, our model lacks the consideration of the cross region propagation and other potential factors may have influence on the spread of virus, such as the temperature, population and average age etc. We will further explore these aspects and pursue better results"},{"metadata":{"id":"R0TiHGP2u7R3","trusted":true},"cell_type":"code","source":"res_df = df_all.copy()\n\n\n\nfor country in res_df['Country'].unique():\n\n    state_list = res_df[res_df['Country']==country]['State'].unique()\n\n    for state in state_list:\n        #print('country')\n        #print(str(country) + ' , ' + str(state))\n        targets = ['ConfirmedCases', 'Fatalities']\n        \n        for target in targets:\n            res = fit_logistic_curve(country,state,15,0,plot = 0,target = target)\n            \n            for i in range(start, end + 1):\n                res_df.loc[(res_df['Day_num'] == i) & (res_df['Country'] == country) & (res_df['State'] == state), \\\n                       target] = res[i]\n        \n\nprint(\"done\")","execution_count":null,"outputs":[]},{"metadata":{"id":"t9Um0FcJu_Ha","trusted":true},"cell_type":"code","source":"results_df_submit = res_df.copy()\nsubmission_data = pd.DataFrame()\nsubmission_data['ForecastId']= results_df_submit[results_df_submit['ForecastId'] >=0 ]['ForecastId'].astype(int)\nsubmission_data['ConfirmedCases'] = results_df_submit[results_df_submit['ForecastId'] >=0 ]['ConfirmedCases'].replace([np.inf, -np.inf], 0)\nsubmission_data['Fatalities'] = results_df_submit[results_df_submit['ForecastId'] >=0 ]['Fatalities'].replace([np.inf, -np.inf], 0)\nsubmission_data['ConfirmedCases']  = submission_data['ConfirmedCases'].apply(lambda x: int(0) if (x < 0 or np.isnan(x)) else int(x))\nsubmission_data['Fatalities']  = submission_data['Fatalities'].apply(lambda x: int(0) if (x < 0 or np.isnan(x)) else int(x))\n\nsubmission_data.columns = ['ForecastId','ConfirmedCases','Fatalities']\nsubmission_data.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"id":"vi1kXf5pvEXf","outputId":"bd8a4428-dc51-46fa-db9d-53325914eea7","trusted":true},"cell_type":"code","source":"ground_truth = train_df.loc[train_df['Date'].isin(data_leak)]\n\ntruth = pd.concat([ground_truth, test_df], axis = 0, sort=False)\ntruth = pd.merge(train_df,test_df, how='inner', on=['Date','State','Country'] ) \n\nprediction = submission_data.copy()\nprediction.rename(columns={'ConfirmedCases':'ConfirmedCasesHat','Fatalities':'FatalitiesHat'}, inplace=True)\n\nall_res = pd.merge(truth,prediction, how='inner', on=['ForecastId']) \n\neva_date = '2020-04-15'\na = all_res[all_res['Date'] < eva_date]['ConfirmedCasesHat']\nb = all_res[all_res['Date'] < eva_date]['ConfirmedCases']\n\nprint(RMSLE(a,b))\n\na = all_res[all_res['Date'] < eva_date]['FatalitiesHat']\nb = all_res[all_res['Date'] < eva_date]['Fatalities']\n\nprint(RMSLE(a,b))\n","execution_count":null,"outputs":[]},{"metadata":{"id":"2JgRqdgATU2S"},"cell_type":"markdown","source":"# Reference"},{"metadata":{"id":"rXi_P5SQxLjj"},"cell_type":"markdown","source":"[1] [*Logistic Function*](https://en.wikipedia.org/wiki/Logistic_function#In_medicine:_modeling_of_a_pandemic). (2021, Feburary 24). Wikipedia. \n\n[2] [*Linear Regression Model*](https://www.kaggle.com/abhijithchandradas/linearregressionmodel). (2021, Feburary 24). Kaggle Inc. \n\n[3] [*COVID-19 Logistic Curve Prediction*](https://www.kaggle.com/orianao/covid-19-logistic-curve-prediction). (2021, Feburary 24). Kaggle Inc. \n\n[4] [*EDA and Forcast Polynomial & Linear Regression*](https://www.kaggle.com/abhijithchandradas/eda-and-forcast-polynomial-linear-regression). (2021, Feburary 24). Kaggle Inc. \n\n[5] [*Linear Regression Is All you Need*](https://www.kaggle.com/c/covid19-global-forecasting-week-5/discussion/151461\n). (2021, Feburary 24). Kaggle Inc.\n\n[6] [*Bayesian Model for COVID-19 Spread Prediction*](https://www.kaggle.com/bpavlyshenko/bayesian-model-for-covid-19-spread-prediction). Kaggle Inc. \n\n[7] [*Random Sample Consensus*](https://en.wikipedia.org/wiki/Random_sample_consensus). (2021, Feburary 26). Wikipedia. \n\n[8] [*COVID-19 - Logistic Curve Fitting and Correlation*](https://www.kaggle.com/diamondsnake/covid-19-logistic-curve-fitting-and-correlation). (2021, Feburary 26). Kaggle Inc. \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}