{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n#import os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#importing Libraries\nimport re\nimport string\nimport pandas as pd\nfrom pickle import dump\nfrom pickle import load\nfrom numpy import array\nfrom datetime import datetime, timedelta\nfrom unicodedata import normalize\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import figure\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\nfrom keras.utils.vis_utils import plot_model\nfrom keras.models import Sequential\nfrom keras.layers import LSTM\nfrom keras.layers import Dense\nfrom keras.layers import Embedding\nfrom keras.layers import RepeatVector\nfrom keras.layers import TimeDistributed\nfrom keras.callbacks import ModelCheckpoint\n#importing libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\nfrom math import sqrt\n#Initiallizing RNN\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers import Dropout\nfrom keras.optimizers import Adam","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#importing dataset\ndataset = pd.read_csv(\"/kaggle/input/covid19-global-forecasting-week-4/train.csv\")\ntestset = pd.read_csv(\"/kaggle/input/covid19-global-forecasting-week-4/test.csv\")\nsample = pd.read_csv(\"/kaggle/input/covid19-global-forecasting-week-4/submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#replacing null State will \"\"\nindex = dataset[dataset[\"Province_State\"].isnull()==True].index\ndataset.loc[index, \"Province_State\"] = \"\"\ndataset[\"location\"] = dataset[\"Country_Region\"] + \" | \" + dataset[\"Province_State\"]\n\n\nindex = testset[testset[\"Province_State\"].isnull()==True].index\ntestset.loc[index, \"Province_State\"] = \"\"\ntestset[\"location\"] = testset[\"Country_Region\"] + \" | \" + testset[\"Province_State\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#dataset Year, Month and day\ndataset[\"year\"] = dataset[\"Date\"].apply(lambda x:x.split(\"-\")[0])\ndataset[\"month\"] = dataset[\"Date\"].apply(lambda x:x.split(\"-\")[1])\ndataset[\"day\"] = dataset[\"Date\"].apply(lambda x:x.split(\"-\")[2])\n\n\ntestset[\"year\"] = testset[\"Date\"].apply(lambda x:x.split(\"-\")[0])\ntestset[\"month\"] = testset[\"Date\"].apply(lambda x:x.split(\"-\")[1])\ntestset[\"day\"] = testset[\"Date\"].apply(lambda x:x.split(\"-\")[2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#location which had more than 0 Confirmed Cases on 2020-01-22\nnonzero_location = dataset[(dataset[\"ConfirmedCases\"]>0) & (dataset[\"Date\"]==\"2020-01-22\")][\"location\"].unique()\n\n\ndataset[\"days_from_start\"] = 0\nindex = dataset[dataset[\"location\"].isin(nonzero_location)].index\ndataset.loc[index, \"days_from_start\"] = dataset.loc[index, \"Date\"].apply(lambda x:(datetime.strptime(x, \"%Y-%m-%d\") - datetime.strptime(\"2020-01-22\", \"%Y-%m-%d\")).days)\n\n\nlocation_startdate = {}\nfor location in dataset[dataset[\"location\"].isin(nonzero_location)==False][\"location\"].unique():\n    location_startdate[location] = (dataset[(dataset[\"location\"]==location)&(dataset[\"ConfirmedCases\"]==0)][\"Date\"].iloc[-1])\n\nfor location in dataset[dataset[\"location\"].isin(nonzero_location)==False][\"location\"].unique():\n    index = (dataset[(dataset[\"location\"]==location) & (dataset[\"Date\"]>location_startdate[location])].index)\n    dataset.loc[index, \"days_from_start\"] = dataset.loc[index, \"Date\"].apply(lambda x:(datetime.strptime(x, \"%Y-%m-%d\") - datetime.strptime(location_startdate[location], \"%Y-%m-%d\")).days)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"toit = dataset.copy()\nlocation_map = {}\ni = 0\nfor location in dataset[\"location\"].unique():\n    location_map[location] = i\n    i = i + 1\ndataset[\"location\"] = dataset[\"location\"].apply(lambda x:location_map[x])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"window = 8\nX = []\ny = []\nfor location in dataset[\"location\"].unique():\n    temp = dataset[dataset[\"location\"] == location].reset_index()\n    for row in temp.loc[0:len(temp)-window-2, :].index:\n        X.append(temp.loc[row:row+window-1, [\"location\", \"days_from_start\", \"ConfirmedCases\"]].values)\n        y.append(temp.loc[row+window,\"ConfirmedCases\"])\n        \ny = np.array(y)\nX = np.array(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"window = 8\nP = []\nq = []\nfor location in dataset[\"location\"].unique():\n    temp = dataset[dataset[\"location\"] == location].reset_index()\n    for row in temp.loc[0:len(temp)-window-2, :].index:\n        P.append(temp.loc[row:row+window-1, [\"location\", \"days_from_start\", \"Fatalities\"]].values)\n        q.append(temp.loc[row+window,\"Fatalities\"])\n        \nq = np.array(q)\nP = np.array(P)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n = int(0.7 * len(X))\nX_train = X[:n]\nX_test = X[n:]\ny_train = y[:n]\ny_test = y[n:]\n\nX_train .shape, X_test .shape, y_train .shape, y_test .shape\n\nn = int(0.7 * len(P))\nP_train = P[:n]\nP_test = P[n:]\nq_train = q[:n]\nq_test = q[n:]\n\nP_train.shape, P_test.shape, q_train.shape, q_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"regressor1 = Sequential()\n\n#Adding first LSTM layer and some Dropout regularization to avoid Overfitting\nregressor1.add(LSTM(units=200,return_sequences=True,input_shape=(X_train.shape[1],3)))\nregressor1.add(Dropout(0.2))\n\n#Adding second and third LSTM layer\nregressor1.add(LSTM(units=200,return_sequences=True))\nregressor1.add(Dropout(0.2))\n#regressor.add(LSTM(units=200,return_sequences=True))\n#regressor.add(Dropout(0.2))\n#Adding 1RNN Layer\nregressor1.add(LSTM(units=200,return_sequences=False))\nregressor1.add(Dropout(0.2))\n\n\n#Adding 1NN Layer\nregressor1.add(Dense(units=100))\n#Adding output layer\nregressor1.add(Dense(units=1))\n\n#Compiling RNN\nregressor1.compile(optimizer='rmsprop',loss='mae')#Adam(lr=0.003) #\n\n#Fitting RNN to the training set\nregressor1.fit(X_train,y_train,epochs=10,batch_size=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"regressor2 = Sequential()\n\n#Adding first LSTM layer and some Dropout regularization to avoid Overfitting\nregressor2.add(LSTM(units=200,return_sequences=True,input_shape=(X_train.shape[1],3)))\nregressor2.add(Dropout(0.2))\n\n#Adding second and third LSTM layer\nregressor2.add(LSTM(units=200,return_sequences=True))\nregressor2.add(Dropout(0.2))\n#regressor.add(LSTM(units=200,return_sequences=True))\n#regressor.add(Dropout(0.2))\n#Adding 1RNN Layer\nregressor2.add(LSTM(units=200,return_sequences=False))\nregressor2.add(Dropout(0.2))\n\n\n#Adding 1NN Layer\nregressor2.add(Dense(units=100))\n#Adding output layer\nregressor2.add(Dense(units=1))\n\n#Compiling RNN\nregressor2.compile(optimizer='rmsprop',loss='mae')#Adam(lr=0.003) #\n\n#Fitting RNN to the training set\nregressor2.fit(P_train,q_train,epochs=10,batch_size=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result1 = []\nfor location in dataset[\"location\"].unique():\n    temp = (dataset[dataset[\"location\"] == location].tail(8)).reset_index()\n    temp = temp[[\"location\", \"days_from_start\", \"ConfirmedCases\"]].values\n    result1.extend(temp.tolist())\n    for days in range(60):\n        prediction = regressor1.predict(np.reshape(temp,(1,temp.shape[0], temp.shape[1])))\n        last_record = np.resize(np.array([location, temp[7][1]+1, int(prediction[0][0])]), (1,temp.shape[1]))\n        temp = (np.append(temp,  last_record, axis=0))[1:]\n        result1.extend([[location, temp[7][1], int(prediction[0][0])]])\nresult1 = pd.DataFrame(result1)\nresult1.columns = ([\"location\", \"days_from_start\", \"ConfirmedCases\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result2 = []\nfor location in dataset[\"location\"].unique():\n    temp = (dataset[dataset[\"location\"] == location].tail(8)).reset_index()\n    temp = temp[[\"location\", \"days_from_start\", \"Fatalities\"]].values\n    result2.extend(temp.tolist())\n    for days in range(60):\n        prediction = regressor2.predict(np.reshape(temp,(1,temp.shape[0], temp.shape[1])))\n        last_record = np.resize(np.array([location, temp[7][1]+1, int(prediction[0][0])]), (1,temp.shape[1]))\n        temp = (np.append(temp,  last_record, axis=0))[1:]\n        result2.extend([[location, temp[7][1], int(prediction[0][0])]])\nresult2 = pd.DataFrame(result2)\nresult2.columns = ([\"location\", \"days_from_start\", \"Fatalities\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result1[\"Country_Region\"] = result1[\"location\"].apply(lambda x:list(location_map.keys())[list(location_map.values()).index(x)].split(\" | \")[0])\nresult1[\"Province_State\"] = result1[\"location\"].apply(lambda x:list(location_map.keys())[list(location_map.values()).index(x)].split(\" | \")[1])\nresult1[\"location\"] = result1[\"location\"].apply(lambda x:list(location_map.keys())[list(location_map.values()).index(x)])\nresult1[\"Date\"] = \"\"\nfor location in result1[\"location\"].unique():\n    index  = result1[result1[\"location\"]==location].index\n    result1.loc[index, \"start_date\"] = result1.loc[index, \"location\"].apply(lambda x:location_startdate[x] if x in list(location_startdate.keys()) else \"2020-01-22\")\n\n    \nresult2[\"Country_Region\"] = result2[\"location\"].apply(lambda x:list(location_map.keys())[list(location_map.values()).index(x)].split(\" | \")[0])\nresult2[\"Province_State\"] = result2[\"location\"].apply(lambda x:list(location_map.keys())[list(location_map.values()).index(x)].split(\" | \")[1])\nresult2[\"location\"] = result2[\"location\"].apply(lambda x:list(location_map.keys())[list(location_map.values()).index(x)])\nresult2[\"Date\"] = \"\"\nfor location in result2[\"location\"].unique():\n    index  = result2[result2[\"location\"]==location].index\n    result2.loc[index, \"start_date\"] = result2.loc[index, \"location\"].apply(lambda x:location_startdate[x] if x in list(location_startdate.keys()) else \"2020-01-22\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for row in result1.index:\n    days_from_start = result1.loc[row, \"days_from_start\"]\n    start_date = result1.loc[row, \"start_date\"]\n    result1.loc[row, \"Date\"] = datetime.strftime((datetime.strptime(start_date, \"%Y-%m-%d\").date()+timedelta(days=days_from_start)), \"%Y-%m-%d\")\n\n\nfor row in result2.index:\n    days_from_start = result2.loc[row, \"days_from_start\"]\n    start_date = result2.loc[row, \"start_date\"]\n    result2.loc[row, \"Date\"] = datetime.strftime((datetime.strptime(start_date, \"%Y-%m-%d\").date()+timedelta(days=days_from_start)), \"%Y-%m-%d\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final1 = pd.merge(result1, testset, right_on = [\"location\", \"Date\"], left_on = [\"location\", \"Date\"])\nfinal2 = pd.merge(result2, testset, right_on = [\"location\", \"Date\"], left_on = [\"location\", \"Date\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_ids = list(set(testset.ForecastId) - set(final1.ForecastId))\nsample[\"ConfirmedCases\"] = sample[\"ForecastId\"].apply(lambda x:final1[final1[\"ForecastId\"] == x][\"ConfirmedCases\"].values[0] if x not in missing_ids else 0.0)\nsample[\"Fatalities\"] = sample[\"ForecastId\"].apply(lambda x:final2[final2[\"ForecastId\"] == x][\"Fatalities\"].values[0] if x not in missing_ids else 0.0)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}