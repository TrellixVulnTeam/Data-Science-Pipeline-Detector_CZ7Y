{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom datetime import date\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\ntrain = pd.read_csv(\"/kaggle/input/covid19-global-forecasting-week-4/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/covid19-global-forecasting-week-4/test.csv\")\nsubmit = pd.read_csv(\"/kaggle/input/covid19-global-forecasting-week-4/submission.csv\")\n#weather = pd.read_csv(\"/kaggle/input/weather-data/training_data_with_weather_info_week_4.csv\")\n#country = pd.read_csv(\"/kaggle/input/countryinfo/covid19countryinfo.csv\")\n#population = pd.read_csv(\"/kaggle/input/population-sizes-worldwide/population_sizes.csv\")\nc2 = pd.read_csv(\"/kaggle/input/covid19-forecasting-metadata/region_metadata.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"# Data Transformation\n1.  Combine the 'Country_Region' and 'Province_State' columns into 'country_province'.\n2.  Calculate the cumulative cases and fatalities for each country_province as 'cumCases' and 'cumDeath'\n3.  Chagnge the datatype of 'Date' to datetime  \n4.  Log transform the cases and deaths\n5.  Add 'prevCases' and 'prevDeath' for train data\n6.  Add 'Days' since the first cases in each region"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Steps 1 to 6\ndef dataTrans(df):\n    # 1. Combine the Country_Region and Province_State columns into country_province.\n    df.Province_State[df['Province_State'].isnull()] = '' # change the null to empty string\n    df['country_province'] = df.apply(lambda x: x.Country_Region+'-' if x.Province_State == '' else x.Country_Region+'-'+x.Province_State, axis = 1)\n    # 3. Chagnge the datatype of Date to datetime\n    df['Date'] = pd.to_datetime(df['Date'], infer_datetime_format=True)    \n    # 4. Log transform the cases and deaths.\n    df['log_ConfirmedCases'] = df['ConfirmedCases'].apply(lambda x: np.log(x) if x > 0 else 0)\n    df['log_Fatalities'] = df['Fatalities'].apply(lambda x: np.log(x) if x > 0 else 0)\n    \n    ##################################################################################################\n    # 2. Calculate the cumulative cases and fatalities for each country_province for the train data\n    # 5.  Add column for prevCases and prevDeath\n    cumCases = pd.Series()\n    cumDeath = pd.Series()\n    prevCases = pd.Series()\n    prevDeath = pd.Series()\n    \n    for region in df.country_province.unique():\n        cases = df.log_ConfirmedCases[df.country_province==region].cumsum() # cumulative sum of log cases\n        death = df.log_Fatalities[df.country_province==region].cumsum() # cumulative sum of log deaths\n        cumCases = pd.concat([cumCases,cases])\n        cumDeath = pd.concat([cumDeath,death])\n        prevCases = pd.concat([prevCases,pd.Series([0]),cases.iloc[:len(cases)-1]])\n        prevDeath = pd.concat([prevDeath,pd.Series([0]),death.iloc[:len(death)-1]])\n        \n        # 7.  Add a column for the days since the first cases\n        date_firstCase = df.iloc[df.ConfirmedCases.to_numpy().nonzero()[0][0]].Date # date of the first cases\n        df[\"DaySinceFirstCase\"] = (df.Date - date_firstCase).dt.days # calculates the day since the first case\n        df[\"DaySinceFirstCase\"] = df['DaySinceFirstCase'].apply(lambda x: x if x>0 else 0) # change the negative values to zero\n\n    prevCases = prevCases.reset_index(drop=True)\n    prevDeath = prevDeath.reset_index(drop=True)\n    #print(len(cumCases), len(cumDeath), df.shape[0])\n    df_cum = pd.concat([df,cumCases,cumDeath,prevCases,prevDeath], axis=1)\n    #print(df.shape, df_cum.shape)\n    df_cum = df_cum.rename(columns={0:'log_cumCases', 1:'log_cumDeath', 2:'log_prevCases', 3:'log_prevDeath'})\n    return df_cum\n###############################################################################################################################################\n\ndf_train = dataTrans(train)\ndf_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Additional Dataset\n1. weather\n2. country info\n3. population"},{"metadata":{},"cell_type":"markdown","source":"## Additional dataset: Weather\nHere is the weather data:\nThe weather data is only available until April-11-2020\n\n- temp: Mean temperature for the day in degrees Fahrenheit to tenths.\n- max: Maximum temperature reported during the day in Fahrenheit to tenths--time of max temp report varies by country and region, so this will sometimes not be the max for the calendar day.\n- min: Minimum temperature reported during the day in Fahrenheit to tenths--time of min temp report varies by country and region, so this will sometimes not be the min for the calendar day.\n- stp: Mean station pressure for the day in millibars to tenths.\n- slp: Mean sea level pressure for the day in millibars to tenths.\n- dewp: Mean dew point for the day in degrees Fahrenheit to tenths.\n- rh: relative humidity as ratio between actual vapour pressure (computed from dewpoint temperature)\n- ah: absolute humidity from the gas law of vapour calcuated from the actual vapour pressure (in pascals). (ah = mass / volume = pressure / (constant * temperature))\n- wdsp: Mean wind speed for the day in knots to tenths.\n- prcp: Total precipitation (rain and/or melted snow) reported during the day in inches and hundredths; will usually not end with the midnight observation--i.e., may include latter part of previous day. .00 indicates no measurable precipitation (includes a trace).\n- fog: Indicators (1 = yes, 0 = no/not reported) for the occurrence during the day"},{"metadata":{"trusted":true},"cell_type":"code","source":"# add north america and south america in the continents\nnorthamerica = ['Antigua and Barbuda','Bahamas','Barbados','Belize','Canada','Costa Rica','Cuba', 'Dominica', \\\n                'Dominican Republic','El Salvador', 'Grenada', 'Guatemala','Haiti','Honduras', 'Jamaica', \\\n                'Mexico', 'Nicaragua', 'Panama','Saint Kitts and Nevis', 'Saint Lucia','Saint Vincent and the Grenadines',\\\n                'Trinidad and Tobago', 'US']\nsouthamerica = ['Argentina','Bolivia', 'Brazil','Chile', 'Colombia', 'Ecuador','Guyana','Paraguay','Peru',\\\n                'Suriname','Uruguay', 'Venezuela']\n#print(len(c2[c2.continent == \"Americas\"].Country_Region.unique()) == (len(northamerica)+len(southamerica)))\nfor country in c2.Country_Region.unique():\n    #print(country)\n    if country in northamerica:\n        c2.loc[c2.Country_Region==country,['continent']] = 'North_America'\n    elif country in southamerica:\n        c2.loc[c2.Country_Region==country,['continent']] = 'South_America'\nprint(c2.continent.unique())\n\n# change the continents to codes\nc2.continent = pd.Categorical(c2.continent)\nc2['continent_code'] = c2.continent.cat.codes \n\n# add a column for country_province\nc2.Province_State[c2['Province_State'].isnull()] = '' # change the null to empty string\nc2['country_province'] = c2.apply(lambda x: x.Country_Region+'-' if x.Province_State == '' else x.Country_Region+'-'+x.Province_State, axis = 1)\nc2.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"markdown","source":"# Clean out the filler values for NAs and outliers in particular columns\nweather['ah'] = weather['ah'].apply(lambda x: np.nan if x==np.inf else x)\nweather['wdsp'] = weather['wdsp'].apply(lambda x: np.nan if x==999.9 else x)\nweather['prcp'] = weather['prcp'].apply(lambda x: np.nan if x==99.99 else x)\n\n# replace np.nan with mean\nfrom sklearn.impute import SimpleImputer\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\nweather.loc[:,'temp':'prcp']=imputer.fit_transform(weather.loc[:,'temp':'prcp'])\n\n# check that there is no outliers in the weather dataset\nfor col in weather.columns:\n    if col in ['temp','min','max','stp','slp','dewp','rh', 'ah','wdsp','prcp','fog']:\n        #print(col,sorted(weather[col].unique(),reverse=True)[:5])\n        print(col,sum(weather[col].isna()))"},{"metadata":{},"cell_type":"markdown","source":"## Additional data: Country info\nIt currently contains:\n\n1. Population (2020)\n2. Density: The number of people who lives per square meter. (2020)\n3. Median age (2020)\n4. Urban population: the % of the population who lives in urban areas. (2020)\n5. Hospital beds per 1K people: I assume that the higher this number is, the lower the fatalities number would be. (2020, 2018)\n6. Forced quarantine policy initial date: I believe that a couple of weeks after this specific date, we can assume there would be a reduction of the infection rate. (updated on a daily basis)\n7. School closure policy initial date: Same as (6). (updated on a daily basis)\n8. Public places (bars, restaurants, movie theatres, etc.) closure policy initial date (4/3/2020)\n9. The maximum amount of people allowed in gatherings and the initial date of the policy (4/3/2020)\n10. Non-essential house leaving - initial date of the restriction (4/3/2020)\n11. Sex ratio grouped by age groups (amount of males per female). (2020)\n12. Lung disease death rate per 100k people, separated by sex. (2020)\n13. % of smokers within the population: The higher this number is, the higher the fatalities number would be. (2019)\n14. Amount of COVID detection test made per day: I collected this information for about 50 countries, missing 120 more. (3/22/2020)\n15. GDP-nominal (2019)\n16. Health expenses in international USD (2019, 2017, 2015)\n17. Health expenses divided by population (2020 - population), (2019, 2017, 2015 - health expenses)\n18. Average amount of children per woman - I find it as an important feature when it comes in interaction with density and school restriction variables. (2017)\n19. First patient detection date\n20. Total confirmed cases (4/3/2020)\n21. Total active cases (4/3/2020)\n22. New confirmed cases (4/3/2020)\n23. Total deaths (4/3/2020)\n24. New deaths (4/3/2020)\n25. Total recovered (4/3/2020)\n26. Amount of patients in critical situation (4/3/2020)\n27. Total cases / 1 million population (4/3/2020)\n28. Total deaths / 1 million population (4/3/2020)\n29. Average temperature (Celsius) measured between January and April. (2020)\n30. Average percentage of humidity measured between January and April. (2020)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# merge columns from the additional datasets to the training set and testing set\n#weather.rename(columns={'country+province':'country_province'}, inplace=True)\n\ndf_train = pd.merge(df_train, c2[['country_province','lat', 'lon', 'continent_code','population', 'area', 'density']], on='country_province', how='outer')\ndf_train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# making the testdata the same\ndef dataTrans_test(df):\n    # 1. Combine the Country_Region and Province_State columns into country_province.\n    df.Province_State[df['Province_State'].isnull()] = '' # change the null to empty string\n    df['country_province'] = df.apply(lambda x: x.Country_Region+'-' if x.Province_State == '' else x.Country_Region+'-'+x.Province_State, axis = 1)\n    # 3. Chagnge the datatype of Date to datetime\n    df['Date'] = pd.to_datetime(df['Date'], infer_datetime_format=True)    \n    return df\n\ndf_test = dataTrans_test(test)\ndf_test = pd.merge(df_test, c2[['country_province','lat', 'lon', 'continent_code','population', 'area', 'density']], on='country_province', how='outer')\ndf_test = df_test.merge(df_train[['Date', 'ConfirmedCases',\n                                   'Fatalities', 'country_province', 'log_ConfirmedCases',\n                                   'log_Fatalities', 'DaySinceFirstCase', 'log_cumCases', 'log_cumDeath',\n                                   'log_prevCases', 'log_prevDeath']], on=['country_province','Date'],how='left')\n\n# calculate the DaySinceFirstCase column\ns = df_test.groupby('country_province').DaySinceFirstCase.cumcount()\ns1 = (df_test.DaySinceFirstCase - s).groupby(df_test.country_province).transform('first')\ndf_test['DaySinceFirstCase'] = s1 + s\n\ndf_test.columns\ndf_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = df_train.columns.tolist()\ncolumns = ['ForecastId'] + columns[1:]\ncolumns\ndf_test[columns]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(df_train), len(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(df_test), len(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"df_train_weather = df_train[df_train.Date <= pd.Timestamp('2020-04-11 00:00:00')]\ndf_train_weather.info()"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# check the temp, min, and max of one country\ndf = df_train_weather[df_train_weather.country_province == \"Germany-\"]\nfigsize = (20,12)\ndf.temp.plot(figsize=figsize)\ndf['min'].plot(figsize=figsize)\ndf['max'].plot(figsize=figsize)\nplt.legend()"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"figsize = (10,7)\ndf_train_weather.prcp.plot(figsize=figsize) "},{"metadata":{"trusted":true},"cell_type":"markdown","source":"df_train_weather.wdsp.plot(figsize=(15,8))"},{"metadata":{},"cell_type":"markdown","source":"for region in df_test.country_province.unique():\n    df = df_test[(df_test.country_province==region)]#.log_cumCases\n    print(df)\n    value = int(df.loc['log_cumCases'][~df.loc['log_cumCases'].isna()][-1])\n    print(value)\n    break"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test = df_test.fillna(method='ffill')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#from sklearn import model_selection\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn import metrics\nmodel = RandomForestRegressor(n_estimators=100)\n\npredictors =['DaySinceFirstCase','log_prevCases','log_prevDeath', 'lat', 'lon', 'continent_code', 'population', 'area', 'density']\n#predictors_deaths = ['DaySinceFirstCase','log_prevCases','log_prevDeath','lat', 'lon', 'continent', 'population', 'area', 'density']\ntargets = ['log_cumCases','log_cumDeath']\n\n# train the models separately for each region\nfor region in df_train.country_province.unique():\n    # train the models separately for log_cumCases and log_cumDeath\n    for target in targets:\n        X_train = df_train[df_train.country_province == region][predictors]\n        y_train = df_train[df_train.country_province == region][target]\n\n        X_test = df_test[(df_test.country_province == region)][predictors]\n        y_test = df_test[(df_test.country_province == region)][target]\n\n        model.fit(X_train, y_train)\n        df_test.loc[df_test.country_province == region,[target]] = model.predict(X_test)\n        \n# change the predicted outcome from log scale to linear scale\ndf_test['cumCases'] = np.exp(df_test['log_cumCases'])\ndf_test['cumDeaths'] = np.exp(df_test['log_cumDeath'])\n# calculate the daily new cases\ndf_test['ConfirmedCases'] = df_test.groupby('country_province').cumCases.diff()\ndf_test['Fatalities'] = df_test.groupby('country_province').cumDeaths.diff()\n    \n    # Default metric is R2 for regression, which can be accessed by score()\n    #print('Score of training model:',model.score(X_test, y_test))\n    #print('Mean Squared Error:',metrics.mean_squared_error(y_test, y_pred))\n    #print('R2:',metrics.r2_score(y_test, y_pred))\n\n\ndf_test.head(50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = df_test[['ForecastId', 'ConfirmedCases', 'Fatalities']]\nsubmission = submission.fillna(method='bfill')\nsubmission.head(50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = submission.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}