{"cells":[{"metadata":{},"cell_type":"markdown","source":" This kernel is based on:\n [Covid-19 Forecasting with an RNN](https://www.kaggle.com/frlemarchand/covid-19-forecasting-with-an-rnn)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport plotly.graph_objects as go\nimport plotly.express as px\nimport plotly.io as pio\npio.templates.default = \"plotly_dark\"\nfrom plotly.subplots import make_subplots\n\nimport geopandas as gpd\nfrom shapely.geometry import Point\nimport tensorflow as tf\nfrom tqdm import tqdm\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import mean_squared_log_error\n\nfrom datetime import datetime\nfrom datetime import timedelta\n\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import Input\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Enriched Dataset week 4"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = gpd.read_file('/kaggle/input/covid19-global-forecasting-week-4/train.csv')\ntrain_data = train_data.rename(columns={'Province_State': 'State', 'Country_Region': 'Country', 'ConfirmedCases': 'Confirmed'})\ntrain_data[\"Confirmed\"] = train_data[\"Confirmed\"].astype(\"float\")\ntrain_data[\"Fatalities\"] = train_data[\"Fatalities\"].astype(\"float\")\n#The country_region got modified in the enriched dataset by @optimo, \n# so we have to apply the same change to this Dataframe to facilitate the merge.\ntrain_data[\"Country\"] = [row.Country.replace(\"'\",\"\").strip(\" \") if row.State==\"\" else str(row.Country+\"_\"+row.State).replace(\"'\",\"\").strip(\" \") for idx,row in train_data.iterrows()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Still using the enriched data from week 2 as there is everything required for the model's training\nextra_data_df = gpd.read_file(\"/kaggle/input/covid-19-enriched-dataset-week-2/enriched_covid_19_week_2.csv\")\nextra_data_df = extra_data_df.rename(columns={'Province_State': 'State', \n                                              'Country_Region': 'Country', \n                                              'ConfirmedCases': 'Confirmed'})\nextra_data_df[\"Country\"] = [country_name.replace(\"'\",\"\") for country_name in extra_data_df[\"Country\"]]\nextra_data_df[\"restrictions\"] = extra_data_df[\"restrictions\"].astype(\"int\")\nextra_data_df[\"quarantine\"] = extra_data_df[\"quarantine\"].astype(\"int\")\nextra_data_df[\"schools\"] = extra_data_df[\"schools\"].astype(\"int\")\nextra_data_df[\"total_pop\"] = extra_data_df[\"total_pop\"].astype(\"float\")\nextra_data_df[\"density\"] = extra_data_df[\"density\"].astype(\"float\")\nextra_data_df[\"hospibed\"] = extra_data_df[\"hospibed\"].astype(\"float\")\nextra_data_df[\"lung\"] = extra_data_df[\"lung\"].astype(\"float\")\nextra_data_df[\"total_pop\"] = extra_data_df[\"total_pop\"]/max(extra_data_df[\"total_pop\"])\nextra_data_df[\"density\"] = extra_data_df[\"density\"]/max(extra_data_df[\"density\"])\nextra_data_df[\"hospibed\"] = extra_data_df[\"hospibed\"]/max(extra_data_df[\"hospibed\"])\nextra_data_df[\"lung\"] = extra_data_df[\"lung\"]/max(extra_data_df[\"lung\"])\nextra_data_df[\"age_100+\"] = extra_data_df[\"age_100+\"].astype(\"float\")\nextra_data_df[\"age_100+\"] = extra_data_df[\"age_100+\"]/max(extra_data_df[\"age_100+\"])\n\nextra_data_df = extra_data_df[[\"Country\",\"Date\",\"restrictions\",\"quarantine\",\"schools\",\"hospibed\",\"lung\",\"total_pop\",\"density\",\"age_100+\"]]\nextra_data_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = train_data.merge(extra_data_df, how=\"left\", on=['Country','Date']).drop_duplicates()\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Still need to complete part of the data for dates past the 25th of March as the enriched dataset didn't go that far."},{"metadata":{"trusted":true},"cell_type":"code","source":"for country_region in train_data.Country.unique():\n    query_df = train_data.query(\"Country=='\"+country_region+\"' and Date=='2020-03-25'\")\n    train_data.loc[(train_data[\"Country\"]==country_region) & (train_data[\"Date\"]>\"2020-03-25\"),\"total_pop\"] = query_df.total_pop.values[0]\n    train_data.loc[(train_data[\"Country\"]==country_region) & (train_data[\"Date\"]>\"2020-03-25\"),\"hospibed\"] = query_df.hospibed.values[0]\n    train_data.loc[(train_data[\"Country\"]==country_region) & (train_data[\"Date\"]>\"2020-03-25\"),\"density\"] = query_df.density.values[0]\n    train_data.loc[(train_data[\"Country\"]==country_region) & (train_data[\"Date\"]>\"2020-03-25\"),\"lung\"] = query_df.lung.values[0]\n    train_data.loc[(train_data[\"Country\"]==country_region) & (train_data[\"Date\"]>\"2020-03-25\"),\"age_100+\"] = query_df[\"age_100+\"].values[0]\n    train_data.loc[(train_data[\"Country\"]==country_region) & (train_data[\"Date\"]>\"2020-03-25\"),\"restrictions\"] = query_df.restrictions.values[0]\n    train_data.loc[(train_data[\"Country\"]==country_region) & (train_data[\"Date\"]>\"2020-03-25\"),\"quarantine\"] = query_df.quarantine.values[0]\n    train_data.loc[(train_data[\"Country\"]==country_region) & (train_data[\"Date\"]>\"2020-03-25\"),\"schools\"] = query_df.schools.values[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As I was using an enriched dataset during the Week 2 competition, I have to add the new countries to my dataframe and fill the missing data with median values."},{"metadata":{"trusted":true},"cell_type":"code","source":"median_pop = np.median(extra_data_df.total_pop)\nmedian_hospibed = np.median(extra_data_df.hospibed)\nmedian_density = np.median(extra_data_df.density)\nmedian_lung = np.median(extra_data_df.lung)\nmedian_centenarian_pop = np.median(extra_data_df[\"age_100+\"])\n#need to replace that with a joint using Pandas\nprint(\"The missing countries/region are:\")\nfor country_region in train_data.Country.unique():\n    if extra_data_df.query(\"Country=='\"+country_region+\"'\").empty:\n        print(country_region)\n        \n        train_data.loc[train_data[\"Country\"]==country_region,\"total_pop\"] = median_pop\n        train_data.loc[train_data[\"Country\"]==country_region,\"hospibed\"] = median_hospibed\n        train_data.loc[train_data[\"Country\"]==country_region,\"density\"] = median_density\n        train_data.loc[train_data[\"Country\"]==country_region,\"lung\"] = median_lung\n        train_data.loc[train_data[\"Country\"]==country_region,\"age_100+\"] = median_centenarian_pop\n        train_data.loc[train_data[\"Country\"]==country_region,\"restrictions\"] = 0\n        train_data.loc[train_data[\"Country\"]==country_region,\"quarantine\"] = 0\n        train_data.loc[train_data[\"Country\"]==country_region,\"schools\"] = 0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Add Smokers Percentages By Country"},{"metadata":{"trusted":true},"cell_type":"code","source":"# #https://ourworldindata.org/smoking#prevalence-of-smoking-across-the-world\n# smokers = pd.read_csv('/kaggle/input/smokingstats/share-of-adults-who-smoke.csv')\n# smokers = smokers[smokers.Year == 2016].reset_index(drop=True)\n\n# smokers_country_dict = {'North America' : \"US\",\n#  'Gambia' : \"The Gambia\",\n#  'Bahamas': \"The Bahamas\",\n#  \"'South Korea'\" : \"Korea, South\",\n# 'Papua New Guinea' : \"Guinea\",\n#  \"'Czech Republic'\" : \"Czechia\",\n#  'Congo' : \"Congo (Brazzaville)\"}\n\n# smokers['Entity'] = smokers.Entity.apply(lambda x : rename_countries(x, smokers_country_dict))\n\n# no_datas_smoker = []\n# for country in df['Country'].unique():\n#     if country not in smokers.Entity.unique():\n#         mean_score = smokers[['Smoking prevalence, total (ages 15+) (% of adults)']].mean().to_dict()\n#         mean_score['Entity'] = country\n#         no_datas_smoker.append(mean_score)\n# no_data_smoker_df = pd.DataFrame(no_datas_smoker)   \n# clean_smoke_data = pd.concat([smokers, no_data_smoker_df], axis=0)[['Entity','Smoking prevalence, total (ages 15+) (% of adults)']]\n# clean_smoke_data.rename(columns={\"Entity\": \"Country\",\n#                                   \"Smoking prevalence, total (ages 15+) (% of adults)\" : \"smokers_perc\"}, inplace=True)\n\n# df = df.merge(clean_smoke_data, on=\"Country\", how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## ICU Beds per Country"},{"metadata":{},"cell_type":"markdown","source":"We wish to further inspect the ratio of ICU beds per 1000 people that every country has readily available. Therefore we load the dataset from: [hospital-beds-by-country](https://www.kaggle.com/hamzael1/hospital-beds-by-country)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# icu_df = pd.read_csv(\"../input/hospital-beds-by-country/API_SH.MED.BEDS.ZS_DS2_en_csv_v2_887506.csv\")\n# icu_df['Country Name'] = icu_df['Country Name'].replace('United States', 'US')\n# icu_df['Country Name'] = icu_df['Country Name'].replace('Russian Federation', 'Russia')\n# icu_df['Country Name'] = icu_df['Country Name'].replace('Iran, Islamic Rep.', 'Iran')\n# icu_df['Country Name'] = icu_df['Country Name'].replace('Egypt, Arab Rep.', 'Egypt')\n# icu_df['Country Name'] = icu_df['Country Name'].replace('Venezuela, RB', 'Venezuela')\n# data['Country'] = data['Country'].replace('Czechia', 'Czech Republic')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# icu_cleaned = pd.DataFrame()\n# icu_cleaned[\"Country\"] = icu_df[\"Country Name\"]\n# icu_cleaned[\"ICU\"] = np.nan\n\n# for year in range(1960, 2020):\n#     year_df = icu_df[str(year)].dropna()\n#     icu_cleaned[\"ICU\"].loc[year_df.index] = year_df.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# data = pd.merge(data, icu_cleaned, on='Country')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# data['State'] = data['State'].fillna('')\n# temp = data[[col for col in data.columns if col != 'State']]\n\n# latest = temp[temp['Date'] == max(temp['Date'])].reset_index()\n# latest_grouped = latest.groupby('Country')['ICU'].mean().reset_index()\n\n\n# fig = px.bar(latest_grouped.sort_values('ICU', ascending=False)[:12][::-1], \n#              x='ICU', y='Country',\n#              title='Ratio of ICU Beds per 1000 People', text='ICU', orientation='h',color_discrete_sequence=['green'] )\n# fig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trend_df = pd.DataFrame(columns={\"infection_trend\",\n                                 \"fatality_trend\",\n                                 \"quarantine_trend\",\n                                 \"school_trend\",\n                                 \"total_population\",\n                                 \"expected_cases\",\n                                 \"expected_fatalities\"})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Just getting rid of the first days to have a multiple of 7\n#Makes it easier to generate the sequences\ntrain_data = train_data.query(\"Date>'2020-01-22'and Date<'2020-04-01'\")\ndays_in_sequence = 21\n\ntrend_list = []\n\nwith tqdm(total=len(list(train_data.Country.unique()))) as pbar:\n    for country in train_data.Country.unique():\n        for province in train_data.query(f\"Country=='{country}'\").State.unique():\n            province_df = train_data.query(f\"Country=='{country}' and State=='{province}'\")\n            \n            #I added a quick hack to double the number of sequences\n            #Warning: This will later create a minor leakage from the \n            # training set into the validation set.\n            for i in range(0,len(province_df),int(days_in_sequence/3)):\n                if i+days_in_sequence<=len(province_df):\n                    #prepare all the temporal inputs\n                    infection_trend = [float(x) for x in province_df[i:i+days_in_sequence-1].Confirmed.values]\n                    fatality_trend = [float(x) for x in province_df[i:i+days_in_sequence-1].Fatalities.values]\n                    restriction_trend = [float(x) for x in province_df[i:i+days_in_sequence-1].restrictions.values]\n                    quarantine_trend = [float(x) for x in province_df[i:i+days_in_sequence-1].quarantine.values]\n                    school_trend = [float(x) for x in province_df[i:i+days_in_sequence-1].schools.values]\n\n                    #preparing all the demographic inputs\n                    total_population = float(province_df.iloc[i].total_pop)\n                    density = float(province_df.iloc[i].density)\n                    hospibed = float(province_df.iloc[i].hospibed)\n                    lung = float(province_df.iloc[i].lung)\n                    centenarian_pop = float(province_df.iloc[i][\"age_100+\"])\n\n                    expected_cases = float(province_df.iloc[i+days_in_sequence-1].Confirmed)\n                    expected_fatalities = float(province_df.iloc[i+days_in_sequence-1].Fatalities)\n\n                    trend_list.append({\"infection_trend\":infection_trend,\n                                     \"fatality_trend\":fatality_trend,\n                                     \"restriction_trend\":restriction_trend,\n                                     \"quarantine_trend\":quarantine_trend,\n                                     \"school_trend\":school_trend,\n                                     \"demographic_inputs\":[total_population,density,hospibed,lung,centenarian_pop],\n                                     \"expected_cases\":expected_cases,\n                                     \"expected_fatalities\":expected_fatalities})\n        pbar.update(1)\ntrend_df = pd.DataFrame(trend_list)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Preparing the inputs and shuffling the dataframe to make sure we have a bit of everything in our training and validation set."},{"metadata":{"trusted":true},"cell_type":"code","source":"trend_df[\"temporal_inputs\"] = [np.asarray([trends[\"infection_trend\"],trends[\"fatality_trend\"],trends[\"restriction_trend\"],trends[\"quarantine_trend\"],trends[\"school_trend\"]]) for idx,trends in trend_df.iterrows()]\n\ntrend_df = shuffle(trend_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trend_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Only keeping 25 sequences where the number of cases stays at 0, as there were way too many of these samples in our dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"i=0\ntemp_df = pd.DataFrame()\nfor idx,row in trend_df.iterrows():\n    if sum(row.infection_trend)>0:\n        temp_df = temp_df.append(row)\n    else:\n        if i<25:\n            temp_df = temp_df.append(row)\n            i+=1\ntrend_df = temp_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trend_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Splitting my dataset with 90% for training and 10% for validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"sequence_length = 20\ntraining_percentage = 0.9","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_item_count = int(len(trend_df)*training_percentage)\nvalidation_item_count = len(trend_df)-int(len(trend_df)*training_percentage)\ntraining_df = trend_df[:training_item_count]\nvalidation_df = trend_df[training_item_count:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_temporal_train = np.asarray(np.transpose(np.reshape(np.asarray([np.asarray(x) for x in training_df[\"temporal_inputs\"].values]),(training_item_count,5,sequence_length)),(0,2,1) )).astype(np.float32)\nX_demographic_train = np.asarray([np.asarray(x) for x in training_df[\"demographic_inputs\"]]).astype(np.float32)\nY_cases_train = np.asarray([np.asarray(x) for x in training_df[\"expected_cases\"]]).astype(np.float32)\nY_fatalities_train = np.asarray([np.asarray(x) for x in training_df[\"expected_fatalities\"]]).astype(np.float32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_temporal_test = np.asarray(np.transpose(np.reshape(np.asarray([np.asarray(x) for x in validation_df[\"temporal_inputs\"]]),(validation_item_count,5,sequence_length)),(0,2,1)) ).astype(np.float32)\nX_demographic_test = np.asarray([np.asarray(x) for x in validation_df[\"demographic_inputs\"]]).astype(np.float32)\nY_cases_test = np.asarray([np.asarray(x) for x in validation_df[\"expected_cases\"]]).astype(np.float32)\nY_fatalities_test = np.asarray([np.asarray(x) for x in validation_df[\"expected_fatalities\"]]).astype(np.float32)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Build the model"},{"metadata":{},"cell_type":"markdown","source":"The model is very simple in terms of architecture. The only difference from what could traditionally be seen is that it has two outputs so we can have two different losses (one for the expected number of cases and for the expected number of fatalities)."},{"metadata":{"trusted":true},"cell_type":"code","source":"#temporal input branch\ntemporal_input_layer = Input(shape=(sequence_length, 5))\nmain_rnn_layer = layers.LSTM(64, return_sequences=True, recurrent_dropout=0.2)(temporal_input_layer)\n\n#demographic input branch\ndemographic_input_layer = Input(shape=(5))\ndemographic_dense = layers.Dense(32)(demographic_input_layer)\ndemographic_dropout = layers.Dropout(0.2)(demographic_dense)\n\n#cases output branch\nrnn_c = layers.LSTM(64)(main_rnn_layer)\nmerge_c = layers.Concatenate(axis=-1)([rnn_c,demographic_dropout])\ndense_c = layers.Dense(256)(merge_c)\ndropout_c = layers.Dropout(0.2)(dense_c)\ncases = layers.Dense(1, activation=layers.LeakyReLU(alpha=0.1),name=\"cases\")(dropout_c)\n\n#fatality output branch\nrnn_f = layers.LSTM(64)(main_rnn_layer)\nmerge_f = layers.Concatenate(axis=-1)([rnn_f,demographic_dropout])\ndense_f = layers.Dense(256)(merge_f)\ndropout_f = layers.Dropout(0.2)(dense_f)\nfatalities = layers.Dense(1, activation=layers.LeakyReLU(alpha=0.1), name=\"fatalities\")(dropout_f)\n\n\nmodel = Model([temporal_input_layer,demographic_input_layer], [cases,fatalities])\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"callbacks = [ReduceLROnPlateau(monitor='val_loss', patience=4, verbose=1, factor=0.6),\n             EarlyStopping(monitor='val_loss', patience=20),\n             ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)]\nmodel.compile(loss=[tf.keras.losses.MeanSquaredLogarithmicError(),tf.keras.losses.MeanSquaredLogarithmicError()], optimizer=\"adam\")\nhistory = model.fit([X_temporal_train,X_demographic_train], [Y_cases_train, Y_fatalities_train], \n          epochs = 250, \n          batch_size = 16, \n          validation_data=([X_temporal_test,X_demographic_test],  [Y_cases_test, Y_fatalities_test]),\n                   callbacks=callbacks)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Performance during training"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Loss over epochs')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='best')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history.history['cases_loss'])\nplt.plot(history.history['val_cases_loss'])\nplt.title('Loss over epochs for the number of cases')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='best')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history.history['fatalities_loss'])\nplt.plot(history.history['val_fatalities_loss'])\nplt.title('Loss over epochs for the number of fatalities')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='best')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Generate predictions using the model"},{"metadata":{},"cell_type":"markdown","source":"We can quickly check the quality of the predictions... One thing is clear, there is room for improvement!"},{"metadata":{"trusted":true},"cell_type":"code","source":"model.load_weights(\"best_model.h5\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = model.predict([X_temporal_test,X_demographic_test])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I only display what I call \"temporal\" inputs as we're simply trying to have a feeling of how well our model is fitting the trends."},{"metadata":{"trusted":true},"cell_type":"code","source":"display_limit = 30\nfor inputs, pred_cases, exp_cases, pred_fatalities, exp_fatalities in zip(X_temporal_test,predictions[0][:display_limit], Y_cases_test[:display_limit], predictions[1][:display_limit], Y_fatalities_test[:display_limit]):\n    print(\"================================================\")\n    print(inputs)\n    print(\"Expected cases:\", exp_cases, \" Prediction:\", pred_cases[0], \"Expected fatalities:\", exp_fatalities, \" Prediction:\", pred_fatalities[0] )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Apply the model to predict future trends"},{"metadata":{},"cell_type":"markdown","source":"The following functions will be used to get the 13 previous days from a given date and demographic information, predict the number of cases and fatalities, before iterating again. Therefore, it will use the prediction for the next day as part of the data for the one afterwards."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Will retrieve the number of cases and fatalities for the past 6 days from the given date\ndef build_inputs_for_date(country, province, date, df):\n    start_date = date - timedelta(days=20)\n    end_date = date - timedelta(days=1)\n    \n    str_start_date = start_date.strftime(\"%Y-%m-%d\")\n    str_end_date = end_date.strftime(\"%Y-%m-%d\")\n    df = df.query(\"Country=='\"+country+\"' and State=='\"+province+\"' and Date>='\"+str_start_date+\"' and Date<='\"+str_end_date+\"'\")\n    \n    #preparing the temporal inputs\n    temporal_input_data = np.transpose(np.reshape(np.asarray([df[\"Confirmed\"],\n                                                 df[\"Fatalities\"],\n                                                 df[\"restrictions\"],\n                                                 df[\"quarantine\"],\n                                                 df[\"schools\"]]),\n                                     (5,sequence_length)), (1,0) ).astype(np.float32)\n    \n    #preparing all the demographic inputs\n    total_population = float(province_df.iloc[i].total_pop)\n    density = float(province_df.iloc[i].density)\n    hospibed = float(province_df.iloc[i].hospibed)\n    lung = float(province_df.iloc[i].lung)\n    centenarian_pop = float(province_df.iloc[i][\"age_100+\"])\n    demographic_input_data = [total_population,density,hospibed,lung,centenarian_pop]\n    \n    return [np.array([temporal_input_data]), np.array([demographic_input_data])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Take a dataframe in input, will do the predictions and return the dataframe with extra rows\n#containing the predictions\ndef predict_for_region(country, province, df):\n    begin_prediction = \"2020-04-01\"\n    start_date = datetime.strptime(begin_prediction,\"%Y-%m-%d\")\n    end_prediction = \"2020-05-14\"\n    end_date = datetime.strptime(end_prediction,\"%Y-%m-%d\")\n    \n    date_list = [start_date + timedelta(days=x) for x in range((end_date-start_date).days+1)]\n    for date in date_list:\n        input_data = build_inputs_for_date(country, province, date, df)\n        result = model.predict(input_data)\n        \n        #just ensuring that the outputs is\n        #higher than the previous counts\n        result[0] = np.round(result[0])\n        if result[0]<input_data[0][0][-1][0]:\n            result[0]=np.array([[input_data[0][0][-1][0]]])\n        \n        result[1] = np.round(result[1])\n        if result[1]<input_data[0][0][-1][1]:\n            result[1]=np.array([[input_data[0][0][-1][1]]])\n        \n        #We assign the quarantine and school status\n        #depending on previous values\n        #e.g Once a country is locked, it will stay locked until the end\n        df = df.append({\"Country\":country, \n                        \"State\":province, \n                        \"Date\":date.strftime(\"%Y-%m-%d\"), \n                        \"restrictions\": 1 if any(input_data[0][0][2]) else 0,\n                        \"quarantine\": 1 if any(input_data[0][0][3]) else 0,\n                        \"schools\": 1 if any(input_data[0][0][4]) else 0,\n                        \"total_pop\": input_data[1][0],\n                        \"density\": input_data[1][0][1],\n                        \"hospibed\": input_data[1][0][2],\n                        \"lung\": input_data[1][0][3],\n                        \"age_100+\": input_data[1][0][4],\n                        \"Confirmed\":round(result[0][0][0]),\t\n                        \"Fatalities\":round(result[1][0][0])},\n                       ignore_index=True)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#The functions that are called here need to optimise, sorry about that!\ncopy_df = train_data\nwith tqdm(total=len(list(copy_df.Country.unique()))) as pbar:\n    for country in copy_df.Country.unique():\n        for province in copy_df.query(\"Country=='\"+country+\"'\").State.unique():\n            copy_df = predict_for_region(country, province, copy_df)\n        pbar.update(1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Check the model's performance for the beginning of April"},{"metadata":{"trusted":true},"cell_type":"code","source":"groundtruth_df = gpd.read_file(\"/kaggle/input/covid19-global-forecasting-week-4/train.csv\")\ngroundtruth_df = groundtruth_df.rename(columns={'Province_State': 'State', \n                                              'Country_Region': 'Country', \n                                              'ConfirmedCases': 'Confirmed'})\ngroundtruth_df[\"Confirmed\"] = groundtruth_df[\"Confirmed\"].astype(\"float\")\ngroundtruth_df[\"Fatalities\"] = groundtruth_df[\"Fatalities\"].astype(\"float\")\n#The country_region got modifying in the enriched dataset by @optimo, \n# so we have to apply the same change to this Dataframe.\ngroundtruth_df[\"Country\"] = [ row.Country.replace(\"'\",\"\").strip(\" \") if row.State==\"\" else str(row.Country+\"_\"+row.State).replace(\"'\",\"\").strip(\" \") for idx,row in groundtruth_df.iterrows()]\n\nlast_date = groundtruth_df.Date.unique()[-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#to remove annoying warnings from pandas\npd.options.mode.chained_assignment = None\n\ndef get_RMSLE_per_region(region, groundtruth_df, display_only=False):\n    groundtruth_df[\"Confirmed\"] = groundtruth_df[\"Confirmed\"].astype(\"float\")\n    groundtruth_df[\"Fatalities\"] = groundtruth_df[\"Fatalities\"].astype(\"float\")\n    \n    #we only take data until the 30th of March 2020 as the groundtruth was not available for later dates.\n    groundtruth = groundtruth_df.query(\"Country=='\"+region+\"' and Date>='2020-04-01' and Date<='\"+last_date+\"'\")\n    predictions = copy_df.query(\"Country=='\"+region+\"' and Date>='2020-04-01' and Date<='\"+last_date+\"'\")\n    \n    RMSLE_cases = np.sqrt(mean_squared_log_error( groundtruth.Confirmed.values, predictions.Confirmed.values ))\n    RMSLE_fatalities = np.sqrt(mean_squared_log_error( groundtruth.Fatalities.values, predictions.Fatalities.values ))\n    \n    if display_only:\n        print(region)\n        print(\"RMSLE on cases:\",np.mean(RMSLE_cases))\n        print(\"RMSLE on fatalities:\",np.mean(RMSLE_fatalities))\n    else:\n        return RMSLE_cases, RMSLE_fatalities","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_RMSLE_for_all_regions(groundtruth_df):\n    RMSLE_cases_list = []\n    RMSLE_fatalities_list = []\n    for region in groundtruth_df.Country.unique():\n        RMSLE_cases, RMSLE_fatalities = get_RMSLE_per_region(region, groundtruth_df, False)\n        RMSLE_cases_list.append(RMSLE_cases)\n        RMSLE_fatalities_list.append(RMSLE_fatalities)\n    print(\"RMSLE on cases:\",np.mean(RMSLE_cases_list))\n    print(\"RMSLE on fatalities:\",np.mean(RMSLE_fatalities_list))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can first check whether one of the outputs is globally harder to predict than the other."},{"metadata":{"trusted":true},"cell_type":"code","source":"get_RMSLE_for_all_regions(groundtruth_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, let's check how we are performing on two groups:\n\n* Countries known to have an outbreak\n* Countries with relatively \"few\" cases"},{"metadata":{"trusted":true},"cell_type":"code","source":"badly_affected_countries = [\"France\",\"Italy\",\"United Kingdom\",\"Spain\",\"Iran\",\"Germany\", \"Turkey\"]\nfor country in badly_affected_countries:\n    get_RMSLE_per_region(country, groundtruth_df, display_only=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"healthy_countries = [\"Taiwan*\",\"Singapore\",\"Kenya\",\"Slovenia\",\"Portugal\", \"Israel\"]\nfor country in healthy_countries:\n    get_RMSLE_per_region(country, groundtruth_df, display_only=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Generating the submission file"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data = gpd.read_file(\"/kaggle/input/covid19-global-forecasting-week-4/test.csv\")\n#The country_region got modifying in the enriched dataset by @optimo, \n# so we have to apply the same change to the test Dataframe.\ntest_data[\"Country_Region\"] = [ row.Country_Region if row.Province_State==\"\" else row.Country_Region+\"_\"+row.Province_State for idx,row in test_data.iterrows() ]\ntest_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Just need to do this little trick to extract the relevant date and the forecastId and add that to the submission file."},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df = pd.DataFrame(columns=[\"ForecastId\",\"ConfirmedCases\",\"Fatalities\"])\nwith tqdm(total=len(test_data)) as pbar:\n    for idx, row in test_data.iterrows():\n        #Had to remove single quotes because of countries like Cote D'Ivoire for example\n        country_region = row.Country_Region.replace(\"'\",\"\").strip(\" \")\n        province_state = row.Province_State.replace(\"'\",\"\").strip(\" \")\n        item = copy_df.query(\"Country=='\"+country_region+\"' and State=='\"+province_state+\"' and Date=='\"+row.Date+\"'\")\n        submission_df = submission_df.append({\"ForecastId\":row.ForecastId,\n                                              \"ConfirmedCases\":int(item.Confirmed.values[0]),\n                                              \"Fatalities\":int(item.Fatalities.values[0])},\n                                             ignore_index=True)\n        pbar.update(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df.sample(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"copy_df.to_csv('covid_19_week_4_data.csv', index=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df.to_csv(\"submission.csv\",index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}