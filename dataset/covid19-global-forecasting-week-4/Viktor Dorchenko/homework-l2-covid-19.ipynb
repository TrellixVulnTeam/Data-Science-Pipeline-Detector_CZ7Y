{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import lightgbm as lgb\nimport numpy as np\nimport pandas as pd\npd.set_option(\"max_columns\", 500)\npd.set_option(\"max_rows\", 300)\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set(style=\"white\")\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import LabelEncoder\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom bokeh.models import Panel, Tabs\nfrom bokeh.io import output_notebook, show\nfrom bokeh.plotting import figure","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## defining constants\nPATH_TRAIN = \"/kaggle/input/covid19-global-forecasting-week-4/train.csv\"\nPATH_TEST = \"/kaggle/input/covid19-global-forecasting-week-4/test.csv\"\n\nPATH_SUBMISSION = \"submission.csv\"\nPATH_OUTPUT = \"output.csv\"\n\nPATH_REGION_METADATA = \"/kaggle/input/covid19-forecasting-metadata/region_metadata.csv\"\nPATH_REGION_DATE_METADATA = \"/kaggle/input/covid19-forecasting-metadata/region_date_metadata.csv\" # кол-во выздоровевших по датам\n\nVAL_DAYS = 7 # дней валидации\nMAD_FACTOR = 0.5 # гиперпараметр второго алгоритма\nDAYS_SINCE_CASES = [1, 10, 50, 100, 500, 1000, 5000, 10000] # сколько дней прошло с момента, когда было зарегистрировано N случаев заражения\n\nSEED = 2357\n\n# гиперпараметры для бустинга\nLGB_PARAMS = {\"objective\": \"regression\", \n              \"num_leaves\": 5, # максимальное количество листьев\n              \"learning_rate\": 0.013,\n              \"bagging_fraction\": 0.91, # сэмплируем данные\n              \"feature_fraction\": 0.81, # сэмплируем факторы\n              \"reg_alpha\": 0.13, # коэффициент L1 регуляризации\n              \"reg_lambda\": 0.13, # коэффициент L2 регуляризации\n              \"metric\": \"rmse\", # оптимизируемая метрика\n              \"seed\": SEED}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## reading data\ntrain = pd.read_csv(PATH_TRAIN)\ntest = pd.read_csv(PATH_TEST)\n\ntrain[\"Date\"] = pd.to_datetime(train[\"Date\"])\ntest[\"Date\"] = pd.to_datetime(test[\"Date\"])\n\nregion_metadata = pd.read_csv(PATH_REGION_METADATA)\nregion_date_metadata = pd.read_csv(PATH_REGION_DATE_METADATA)\n\nregion_date_metadata[\"Date\"] = pd.to_datetime(region_date_metadata[\"Date\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output_notebook()\n\ntab_list = []\nfor country in [\"Italy\", \"Russia\", \"Ukraine\"]:\n    v = figure(plot_width=800, plot_height=400, x_axis_type=\"datetime\", title=\"Covid-19 Confirmed Cases over time\")\n    v.line(train[train[\"Country_Region\"] == country][\"Date\"], train[train[\"Country_Region\"] == country][\"ConfirmedCases\"], color=\"green\", legend_label=\"CC\")\n    v.legend.location = \"top_left\"\n    tab = Panel(child=v, title=country)\n    tab_list.append(tab)\n\ntabs = Tabs(tabs=tab_list)\nshow(tabs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## preparing data\ntrain = train.merge(test[[\"ForecastId\", \"Province_State\", \"Country_Region\", \"Date\"]], on=[\"Province_State\", \"Country_Region\", \"Date\"], how=\"left\")\ntest = test[~test[\"Date\"].isin(train[\"Date\"].unique())]\n\ndf_panel = pd.concat([train, test], sort=False)\n\n# combining state and country into 'geography'\ndf_panel[\"geography\"] = df_panel[\"Country_Region\"].astype(str) + \": \" + df_panel[\"Province_State\"].astype(str)\ndf_panel.loc[df_panel[\"Province_State\"].isna(), \"geography\"] = df_panel[\"Country_Region\"]\n\n# fixing data issues with cummax\ndf_panel[\"ConfirmedCases\"] = df_panel.groupby(\"geography\")[\"ConfirmedCases\"].cummax()\ndf_panel[\"Fatalities\"] = df_panel.groupby(\"geography\")[\"Fatalities\"].cummax()\n\n# merging external metadata\ndf_panel = df_panel.merge(region_metadata, on=[\"Country_Region\", \"Province_State\"])\ndf_panel = df_panel.merge(region_date_metadata, on=[\"Country_Region\", \"Province_State\", \"Date\"], how=\"left\")\n\n# label encoding continent\ndf_panel[\"continent\"] = LabelEncoder().fit_transform(df_panel[\"continent\"])\ndf_panel[\"Date\"] = pd.to_datetime(df_panel[\"Date\"], format=\"%Y-%m-%d\")\n\ndf_panel.sort_values([\"geography\", \"Date\"], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## feature engineering\nmin_date_train = np.min(df_panel[~df_panel[\"Id\"].isna()][\"Date\"])\nmax_date_train = np.max(df_panel[~df_panel[\"Id\"].isna()][\"Date\"])\n\nmin_date_test = np.min(df_panel[~df_panel[\"ForecastId\"].isna()][\"Date\"])\nmax_date_test = np.max(df_panel[~df_panel[\"ForecastId\"].isna()][\"Date\"])\n\nn_dates_test = len(df_panel[~df_panel[\"ForecastId\"].isna()][\"Date\"].unique())\n\nprint(\"Train date range:\", str(min_date_train), \" - \", str(max_date_train))\nprint(\"Test date range:\", str(min_date_test), \" - \", str(max_date_test))\n\n# creating lag features\nfor lag in range(1, 41):\n    df_panel[f\"lag_{lag}_cc\"] = df_panel.groupby(\"geography\")[\"ConfirmedCases\"].shift(lag)\n    df_panel[f\"lag_{lag}_ft\"] = df_panel.groupby(\"geography\")[\"Fatalities\"].shift(lag)\n    df_panel[f\"lag_{lag}_rc\"] = df_panel.groupby(\"geography\")[\"Recoveries\"].shift(lag)\n\nfor case in DAYS_SINCE_CASES:\n    df_panel = df_panel.merge(df_panel[df_panel[\"ConfirmedCases\"] >= case].groupby(\"geography\")[\"Date\"].min().reset_index().rename(\n        columns={\"Date\": f\"case_{case}_date\"}), on=\"geography\", how=\"left\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## function for preparing features\ndef prepare_features(df, gap):\n\n    df[\"perc_1_ac\"] = np.around((df[f\"lag_{gap}_cc\"] - df[f\"lag_{gap}_ft\"] - df[f\"lag_{gap}_rc\"]) / df[f\"lag_{gap}_cc\"], 4)\n    df[\"perc_1_cc\"] = np.around(df[f\"lag_{gap}_cc\"] / df.population, 4)\n\n    for i in range(1, 4):\n        df[f\"diff_{i}_cc\"] = df[f\"lag_{gap + i - 1}_cc\"] - df[f\"lag_{gap + i}_cc\"]\n        df[f\"diff_{i}_ft\"] = df[f\"lag_{gap + i - 1}_ft\"] - df[f\"lag_{gap + i}_ft\"]\n\n    df[\"diff_123_cc\"] = np.around((df[f\"lag_{gap}_cc\"] - df[f\"lag_{gap + 3}_cc\"]) / 3, 4)\n    df[\"diff_123_ft\"] = np.around((df[f\"lag_{gap}_ft\"] - df[f\"lag_{gap + 3}_ft\"]) / 3, 4)\n\n    df[\"diff_change_1_cc\"] = np.around(df[\"diff_1_cc\"] / df[\"diff_2_cc\"], 4)\n    df[\"diff_change_2_cc\"] = np.around(df[\"diff_2_cc\"] / df[\"diff_3_cc\"], 4)\n    \n    df[\"diff_change_1_ft\"] = np.around(df[\"diff_1_ft\"] / df[\"diff_2_ft\"], 4)\n    df[\"diff_change_2_ft\"] = np.around(df[\"diff_2_ft\"] / df[\"diff_3_ft\"], 4)\n\n    df[\"diff_change_12_cc\"] = np.around((df[\"diff_change_1_cc\"] + df[\"diff_change_2_cc\"]) / 2, 4)\n    df[\"diff_change_12_ft\"] = np.around((df[\"diff_change_1_ft\"] + df[\"diff_change_2_ft\"]) / 2, 4)\n\n    for i in range(1, 4):\n        df[f\"change_{i}_cc\"] = df[f\"lag_{gap + i - 1}_cc\"] / df[f\"lag_{gap + i}_cc\"]\n        df[f\"change_{i}_ft\"] = df[f\"lag_{gap + i - 1}_ft\"] / df[f\"lag_{gap + i}_ft\"]\n\n    df[\"change_123_cc\"] = np.around(df[f\"lag_{gap}_cc\"] / df[f\"lag_{gap + 3}_cc\"], 4)\n    df[\"change_123_ft\"] = np.around(df[f\"lag_{gap}_ft\"] / df[f\"lag_{gap + 3}_ft\"], 4)\n\n    for case in DAYS_SINCE_CASES:\n        df[f\"days_since_{case}_case\"] = (df[\"Date\"] - df[f\"case_{case}_date\"]) / np.timedelta64(1, \"D\")\n        df.loc[df[f\"days_since_{case}_case\"] < gap, f\"days_since_{case}_case\"] = np.nan\n\n    df[\"country_flag\"] = df[\"Province_State\"].isna().astype(np.int64)\n    df[\"density\"] = np.around(df[\"population\"] / df[\"area\"], 4)\n\n    # target variable is log of change from last known value\n    df[\"target_cc\"] = np.log1p(df[\"ConfirmedCases\"]) - np.log1p(df[f\"lag_{gap}_cc\"])\n    df[\"target_ft\"] = np.log1p(df[\"Fatalities\"]) - np.log1p(df[f\"lag_{gap}_ft\"])\n\n    features = [f\"lag_{gap}_cc\", f\"lag_{gap}_ft\", f\"lag_{gap}_rc\",\n                \"perc_1_ac\", \"perc_1_cc\",\n                \"diff_1_cc\", \"diff_2_cc\", \"diff_3_cc\",\n                \"diff_1_ft\", \"diff_2_ft\", \"diff_3_ft\",\n                \"diff_123_cc\", \"diff_123_ft\",\n                \"diff_change_1_cc\", \"diff_change_2_cc\",\n                \"diff_change_1_ft\", \"diff_change_2_ft\",\n                \"diff_change_12_cc\", \"diff_change_12_ft\",\n                \"change_1_cc\", \"change_2_cc\", \"change_3_cc\",\n                \"change_1_ft\", \"change_2_ft\", \"change_3_ft\",\n                \"change_123_cc\", \"change_123_ft\",\n                \"days_since_1_case\",\n                \"days_since_10_case\",\n                \"days_since_50_case\",\n                \"days_since_100_case\",\n                \"days_since_500_case\",\n                \"days_since_1000_case\",\n                \"days_since_5000_case\",\n                \"days_since_10000_case\",\n                \"country_flag\",\n                \"lat\",\n                \"lon\",\n                \"continent\",\n                \"population\",\n                \"area\",\n                \"density\",\n                \"target_cc\",\n                \"target_ft\"]\n\n    return df[features]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### LGB Model\n\n* Таргет - это логарифмическая разница между текущим значением ConfirmedCases / Fatalities и последним известным значением (предыдущий день)\n\n* Одна модель строится для каждого дня в отдельности в тестовой выборке.\n\n* Единый набор параметров с небольшим числом листьев и регуляризацией используется для предотвращения переобучения."},{"metadata":{"trusted":true},"cell_type":"code","source":"## function for building and predicting using LGBM model\ndef build_predict_lgbm(df_train, df_test, gap):\n\n    df_train.dropna(subset=[\"target_cc\", \"target_ft\", f\"lag_{gap}_cc\", f\"lag_{gap}_ft\"], inplace=True)\n\n    target_cc = df_train[\"target_cc\"]\n    target_ft = df_train[\"target_ft\"]\n\n    test_lag_cc = df_test[f\"lag_{gap}_cc\"].values\n    test_lag_ft = df_test[f\"lag_{gap}_ft\"].values\n    \n    df_train.drop([\"target_cc\", \"target_ft\"], axis=1, inplace=True)\n    df_test.drop([\"target_cc\", \"target_ft\"], axis=1, inplace=True)\n\n    categorical_features = [\"continent\"]\n\n    dtrain_cc = lgb.Dataset(df_train, label=target_cc, categorical_feature=categorical_features)\n    dtrain_ft = lgb.Dataset(df_train, label=target_ft, categorical_feature=categorical_features)\n\n    model_cc = lgb.train(LGB_PARAMS, train_set=dtrain_cc, num_boost_round=200)\n    model_ft = lgb.train(LGB_PARAMS, train_set=dtrain_ft, num_boost_round=200)\n\n    # inverse transform from log of change from last known value\n    y_pred_cc = np.expm1(model_cc.predict(df_test, num_boost_round=200) + np.log1p(test_lag_cc))\n    y_pred_ft = np.expm1(model_ft.predict(df_test, num_boost_round=200) + np.log1p(test_lag_ft))\n\n    return y_pred_cc, y_pred_ft, model_cc, model_ft","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### MAD Model\n\n* This Moving Average with Decay (MAD) model is a simple heuristic using historic values that decays with time.\n\n* It is structured based on my EDA and intuitive feeling of how the Covid-19 trend is likely to move."},{"metadata":{"trusted":true},"cell_type":"code","source":"## function for predicting moving average decay model\ndef predict_mad(df_test, gap, val=False):\n\n    df_test[\"avg_diff_cc\"] = np.around((df_test[f\"lag_{gap}_cc\"] - df_test[f\"lag_{gap + 3}_cc\"]) / 3, 4)\n    df_test[\"avg_diff_ft\"] = np.around((df_test[f\"lag_{gap}_ft\"] - df_test[f\"lag_{gap + 3}_ft\"]) / 3, 4)\n\n    if val:\n        y_pred_cc = df_test[f\"lag_{gap}_cc\"] + gap * df_test[\"avg_diff_cc\"] - (1 - MAD_FACTOR) * df_test[\"avg_diff_cc\"] * np.sum(list(range(gap))) / VAL_DAYS\n        y_pred_ft = df_test[f\"lag_{gap}_ft\"] + gap * df_test[\"avg_diff_ft\"] - (1 - MAD_FACTOR) * df_test[\"avg_diff_ft\"] * np.sum(list(range(gap))) / VAL_DAYS\n    else:\n        y_pred_cc = df_test[f\"lag_{gap}_cc\"] + gap * df_test[\"avg_diff_cc\"] - (1 - MAD_FACTOR) * df_test[\"avg_diff_cc\"] * np.sum(list(range(gap))) / n_dates_test\n        y_pred_ft = df_test[f\"lag_{gap}_ft\"] + gap * df_test[\"avg_diff_ft\"] - (1 - MAD_FACTOR) * df_test[\"avg_diff_ft\"] * np.sum(list(range(gap))) / n_dates_test\n\n    return y_pred_cc, y_pred_ft","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Modelling\n\n* Одна модель на каждую дату.\n\n* Для дат, которые уже присутствуют в train модель не обучаем.\n\n* Проверяем модели аналогичным фреймворком"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test_full = df_panel[~df_panel[\"ForecastId\"].isna()].reset_index(drop=True)\ndf_train = df_panel[df_panel[\"ForecastId\"].isna()].reset_index(drop=True)\n\nmax_date_train = pd.Timestamp(\"2020-04-15\")\n\nprint(f\"Test shape: {df_test_full.shape}\")\nprint(f\"Train shape: {df_train.shape}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for date in df_test_full[\"Date\"].unique():\n    print(\"Processing date:\", date)\n\n    if date in df_train[\"Date\"].values:\n        print(\"already exists\")\n    else:\n        df_test = df_test_full[df_test_full[\"Date\"] == date]\n\n        gap = (pd.Timestamp(date) - max_date_train).days\n        print(pd.Timestamp(date).strftime(\"%Y-%m-%d\"), max_date_train.strftime(\"%Y-%m-%d\"), gap)\n        break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_date_train - pd.Timedelta(VAL_DAYS, \"D\") + pd.Timedelta(gap, \"D\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## building lag x-days models\ndf_train = df_panel[df_panel[\"Date\"] <= max_date_train]\ndf_test_full = df_panel[(df_panel[\"Date\"] > max_date_train) & (~df_panel[\"ForecastId\"].isna())]\n\ndf_preds_val = []\ndf_preds_test = []\n\nfor date in df_test_full[\"Date\"].unique():\n\n    print(\"Processing date:\", date)\n    \n    # ignore date already present in train data\n    if date in df_train[\"Date\"].values:\n        df_pred_test = df_test_full.loc[df_test_full[\"Date\"] == date, [\"ForecastId\", \"ConfirmedCases\", \"Fatalities\"]].rename(\n            columns={\"ConfirmedCases\": \"ConfirmedCases_test\", \"Fatalities\": \"Fatalities_test\"})\n\n    else:\n        df_test = df_test_full[df_test_full[\"Date\"] == date]\n        gap = (pd.Timestamp(date) - max_date_train).days\n\n        if gap <= VAL_DAYS:\n            val_date = max_date_train - pd.Timedelta(VAL_DAYS, \"D\") + pd.Timedelta(gap, \"D\")\n\n            df_build = df_train[df_train[\"Date\"] < val_date]\n            df_val = df_train[df_train[\"Date\"] == val_date]\n\n            X_build = prepare_features(df_build, gap)\n            X_val = prepare_features(df_val, gap)\n\n            y_val_cc_lgb, y_val_ft_lgb, _, _ = build_predict_lgbm(X_build, X_val, gap)\n            y_val_cc_mad, y_val_ft_mad = predict_mad(df_val, gap, val = True)\n\n            df_pred_val = pd.DataFrame({\"Id\": df_val[\"Id\"].values,\n                                        \"ConfirmedCases_val_lgb\": y_val_cc_lgb,\n                                        \"Fatalities_val_lgb\": y_val_ft_lgb,\n                                        \"ConfirmedCases_val_mad\": y_val_cc_mad,\n                                        \"Fatalities_val_mad\": y_val_ft_mad})\n\n            df_preds_val.append(df_pred_val)\n\n        X_train = prepare_features(df_train, gap)\n        X_test = prepare_features(df_test, gap)\n\n        y_test_cc_lgb, y_test_ft_lgb, model_cc, model_ft = build_predict_lgbm(X_train, X_test, gap)\n        y_test_cc_mad, y_test_ft_mad = predict_mad(df_test, gap)\n        \n        if gap == 1:\n            model_1_cc = model_cc\n            model_1_ft = model_ft\n            features_1 = X_train.columns.values\n        elif gap == 14:\n            model_14_cc = model_cc\n            model_14_ft = model_ft\n            features_14 = X_train.columns.values\n        elif gap == 28:\n            model_28_cc = model_cc\n            model_28_ft = model_ft\n            features_28 = X_train.columns.values\n\n        df_pred_test = pd.DataFrame({\"ForecastId\": df_test.ForecastId.values,\n                                     \"ConfirmedCases_test_lgb\": y_test_cc_lgb,\n                                     \"Fatalities_test_lgb\": y_test_ft_lgb,\n                                     \"ConfirmedCases_test_mad\": y_test_cc_mad,\n                                     \"Fatalities_test_mad\": y_test_ft_mad})\n\n    df_preds_test.append(df_pred_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Validation\n\n* Валидируем модели LGB и MAD модели используя RMSLE метрику.\n\n* Визуализируем feature importance для 1, 14 и 28 модели, чтобы определить, что влияет на предсказания ближайшего периода, среднего и долгосрочного периодов."},{"metadata":{"trusted":true},"cell_type":"code","source":"## validation score\ndf_panel = df_panel.merge(pd.concat(df_preds_val, sort=False), on=\"Id\", how=\"left\")\ndf_panel = df_panel.merge(pd.concat(df_preds_test, sort=False), on=\"ForecastId\", how=\"left\")\n\nrmsle_cc_lgb = np.sqrt(mean_squared_error(np.log1p(df_panel[~df_panel[\"ConfirmedCases_val_lgb\"].isna()][\"ConfirmedCases\"]),\n                                          np.log1p(df_panel[~df_panel[\"ConfirmedCases_val_lgb\"].isna()][\"ConfirmedCases_val_lgb\"])))\n\nrmsle_ft_lgb = np.sqrt(mean_squared_error(np.log1p(df_panel[~df_panel[\"Fatalities_val_lgb\"].isna()][\"Fatalities\"]),\n                                          np.log1p(df_panel[~df_panel[\"Fatalities_val_lgb\"].isna()][\"Fatalities_val_lgb\"])))\n\nrmsle_cc_mad = np.sqrt(mean_squared_error(np.log1p(df_panel[~df_panel[\"ConfirmedCases_val_mad\"].isna()][\"ConfirmedCases\"]),\n                                          np.log1p(df_panel[~df_panel[\"ConfirmedCases_val_mad\"].isna()][\"ConfirmedCases_val_mad\"])))\n\nrmsle_ft_mad = np.sqrt(mean_squared_error(np.log1p(df_panel[~df_panel[\"Fatalities_val_mad\"].isna()][\"Fatalities\"]),\n                                          np.log1p(df_panel[~df_panel[\"Fatalities_val_mad\"].isna()][\"Fatalities_val_mad\"])))\n\n\nprint(\"LGB CC RMSLE Val of\", VAL_DAYS, \"days for CC:\", round(rmsle_cc_lgb, 2))\nprint(\"LGB FT RMSLE Val of\", VAL_DAYS, \"days for FT:\", round(rmsle_ft_lgb, 2))\nprint(\"LGB Overall RMSLE Val of\", VAL_DAYS, \"days:\", round((rmsle_cc_lgb + rmsle_ft_lgb) / 2, 2))\nprint(\"\\n\")\nprint(\"MAD CC RMSLE Val of\", VAL_DAYS, \"days for CC:\", round(rmsle_cc_mad, 2))\nprint(\"MAD FT RMSLE Val of\", VAL_DAYS, \"days for FT:\", round(rmsle_ft_mad, 2))\nprint(\"MAD Overall RMSLE Val of\", VAL_DAYS, \"days:\", round((rmsle_cc_mad + rmsle_ft_mad) / 2, 2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# feature importance\nfrom bokeh.io import output_notebook, show\nfrom bokeh.layouts import column\nfrom bokeh.palettes import Spectral3\nfrom bokeh.plotting import figure\n\noutput_notebook()\n\ndf_fimp_1_cc = pd.DataFrame({\"feature\": features_1, \"importance\": model_1_cc.feature_importance(), \"model\": \"m01\"})\ndf_fimp_14_cc = pd.DataFrame({\"feature\": features_14, \"importance\": model_14_cc.feature_importance(), \"model\": \"m14\"})\ndf_fimp_28_cc = pd.DataFrame({\"feature\": features_28, \"importance\": model_28_cc.feature_importance(), \"model\": \"m28\"})\n\ndf_fimp_1_cc.sort_values(\"importance\", ascending=False, inplace=True)\ndf_fimp_14_cc.sort_values(\"importance\", ascending=False, inplace=True)\ndf_fimp_28_cc.sort_values(\"importance\", ascending=False, inplace=True)\n\nv1 = figure(plot_width=800, plot_height=400, x_range=df_fimp_1_cc[\"feature\"], title=\"Feature Importance of LGB Model 1\")\nv1.vbar(x=df_fimp_1_cc[\"feature\"], top=df_fimp_1_cc[\"importance\"], width=1)\nv1.xaxis.major_label_orientation = 1.3\n\nv14 = figure(plot_width=800, plot_height=400, x_range=df_fimp_14_cc[\"feature\"], title=\"Feature Importance of LGB Model 14\")\nv14.vbar(x=df_fimp_14_cc[\"feature\"], top=df_fimp_14_cc[\"importance\"], width=1)\nv14.xaxis.major_label_orientation = 1.3\n\nv28 = figure(plot_width=800, plot_height=400, x_range=df_fimp_28_cc[\"feature\"], title=\"Feature Importance of LGB Model 28\")\nv28.vbar(x=df_fimp_28_cc[\"feature\"], top=df_fimp_28_cc[\"importance\"], width=1)\nv28.xaxis.major_label_orientation = 1.3\n\nv = column(v1, v14, v28)\n\nshow(v)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualizing Predictions\n\n* Отображаем текущие, валидационные и тестовые данные для каждой страны по выявленным случаям и смертям."},{"metadata":{"trusted":true},"cell_type":"code","source":"## visualizing ConfirmedCases\nfrom bokeh.models import Panel, Tabs\nfrom bokeh.io import output_notebook, show\nfrom bokeh.plotting import figure\n\noutput_notebook()\n\ntab_list = []\nfor geography in df_panel[\"geography\"].unique():\n    df_geography = df_panel[df_panel[\"geography\"] == geography]\n    v = figure(plot_width=800, plot_height=400, x_axis_type=\"datetime\", title=\"Covid-19 ConfirmedCases over time\")\n\n    v.line(df_geography[\"Date\"], df_geography[\"ConfirmedCases\"], color=\"green\", legend_label=\"CC (Train)\")\n    v.line(df_geography[\"Date\"], df_geography[\"ConfirmedCases_val_lgb\"], color=\"blue\", legend_label=\"CC LGB (Val)\")\n    v.line(df_geography[\"Date\"], df_geography[\"ConfirmedCases_val_mad\"], color=\"purple\", legend_label=\"CC MAD (Val)\")\n\n    v.line(df_geography[\"Date\"][df_geography[\"Date\"] > max_date_train], df_geography[\"ConfirmedCases_test_lgb\"][df_geography[\"Date\"] > max_date_train],\n           color=\"red\", legend_label=\"CC LGB (Test)\")\n\n    v.line(df_geography[\"Date\"][df_geography[\"Date\"] > max_date_train], df_geography[\"ConfirmedCases_test_mad\"][df_geography[\"Date\"] > max_date_train],\n           color=\"orange\", legend_label=\"CC MAD (Test)\")\n\n    v.legend.location = \"top_left\"\n    tab = Panel(child=v, title=geography)\n    tab_list.append(tab)\n\ntabs = Tabs(tabs=tab_list)\nshow(tabs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## visualizing Fatalities\nfrom bokeh.models import Panel, Tabs\nfrom bokeh.io import output_notebook, show\nfrom bokeh.plotting import figure\n\noutput_notebook()\n\ntab_list = []\nfor geography in df_panel[\"geography\"].unique():\n    df_geography = df_panel[df_panel[\"geography\"] == geography]\n    v = figure(plot_width=800, plot_height=400, x_axis_type=\"datetime\", title=\"Covid-19 Fatalities over time\")\n\n    v.line(df_geography[\"Date\"], df_geography[\"Fatalities\"], color=\"green\", legend_label=\"FT (Train)\")\n    v.line(df_geography[\"Date\"], df_geography[\"Fatalities_val_lgb\"], color=\"blue\", legend_label=\"FT LGB (Val)\")\n    v.line(df_geography[\"Date\"], df_geography[\"Fatalities_val_mad\"], color=\"purple\", legend_label=\"FT MAD (Val)\")\n\n    v.line(df_geography[\"Date\"][df_geography[\"Date\"] > max_date_train], df_geography[\"Fatalities_test_lgb\"][df_geography[\"Date\"] > max_date_train],\n           color=\"red\", legend_label=\"FT LGB (Test)\")\n\n    v.line(df_geography[\"Date\"][df_geography[\"Date\"] > max_date_train], df_geography[\"Fatalities_test_mad\"][df_geography[\"Date\"] > max_date_train],\n           color=\"orange\", legend_label=\"FT MAD (Test)\")\n\n    v.legend.location = \"top_left\"\n    tab = Panel(child=v, title=geography)\n    tab_list.append(tab)\n\ntabs = Tabs(tabs=tab_list)\nshow(tabs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Submission\n\n* Combining the two aapproaches using weights for final submission.\n\n* LGB models don't seem to perform well for certain geographies and are replaced by MAD predictions."},{"metadata":{"trusted":true},"cell_type":"code","source":"## preparing submission file\ndf_test = df_panel.loc[~df_panel[\"ForecastId\"].isna(), [\"ForecastId\", \"Country_Region\", \"Province_State\", \"Date\",\n                                                        \"ConfirmedCases_test_lgb\", \"ConfirmedCases_test_mad\",\n                                                        \"Fatalities_test_lgb\", \"Fatalities_test_mad\"]].reset_index(drop=True)\n\ndf_test[\"ConfirmedCases\"] = 0.15 * df_test[\"ConfirmedCases_test_lgb\"] + 0.85 * df_test[\"ConfirmedCases_test_mad\"]\ndf_test[\"Fatalities\"] = 0.15 * df_test[\"Fatalities_test_lgb\"] + 0.85 * df_test[\"Fatalities_test_mad\"]\n\ndf_submission = df_test[[\"ForecastId\", \"ConfirmedCases\", \"Fatalities\"]]\ndf_submission[\"ForecastId\"] = df_submission[\"ForecastId\"].astype(np.int64)\n\n## writing final submission and complete output\ndf_submission.to_csv(PATH_SUBMISSION, index=False)\ndf_test.to_csv(PATH_OUTPUT, index=False)\n\ndf_submission.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1. Подбор MAD фактора"},{"metadata":{"trusted":true},"cell_type":"code","source":"## function for predicting moving average decay model\ndef predict_mad(df_test, gap, MAD_FACTOR, val=False):\n\n    df_test[\"avg_diff_cc\"] = np.around((df_test[f\"lag_{gap}_cc\"] - df_test[f\"lag_{gap + 3}_cc\"]) / 3, 4)\n    df_test[\"avg_diff_ft\"] = np.around((df_test[f\"lag_{gap}_ft\"] - df_test[f\"lag_{gap + 3}_ft\"]) / 3, 4)\n\n    if val:\n        y_pred_cc = df_test[f\"lag_{gap}_cc\"] + gap * df_test[\"avg_diff_cc\"] - (1 - MAD_FACTOR) * df_test[\"avg_diff_cc\"] * np.sum(list(range(gap))) / VAL_DAYS\n        y_pred_ft = df_test[f\"lag_{gap}_ft\"] + gap * df_test[\"avg_diff_ft\"] - (1 - MAD_FACTOR) * df_test[\"avg_diff_ft\"] * np.sum(list(range(gap))) / VAL_DAYS\n    else:\n        y_pred_cc = df_test[f\"lag_{gap}_cc\"] + gap * df_test[\"avg_diff_cc\"] - (1 - MAD_FACTOR) * df_test[\"avg_diff_cc\"] * np.sum(list(range(gap))) / n_dates_test\n        y_pred_ft = df_test[f\"lag_{gap}_ft\"] + gap * df_test[\"avg_diff_ft\"] - (1 - MAD_FACTOR) * df_test[\"avg_diff_ft\"] * np.sum(list(range(gap))) / n_dates_test\n\n    return y_pred_cc, y_pred_ft","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\n\ndef get_script_time(script_time):\n    hours = script_time // 3600\n    minutes = (script_time % 3600) // 60\n    seconds = script_time % 60\n    result = (f\"{hours}h \" if hours > 0 else \"\") + (f\"{minutes}m \" if minutes > 0 else \"\") + f\"{seconds}s\"\n    return result\n\nt_start = time.time()\n\ngap = 13\nmad_factors = np.arange(0, 10.01, .01)\nscores_mad_df = pd.DataFrame(data={})\n\nfor mad_factor in mad_factors:\n    predicts_cc_mad, predicts_ft_mad = predict_mad(df_test_full, gap, mad_factor, val=True)\n    rmsle_cc_mad = np.around(np.sqrt(mean_squared_error(np.log1p(df_test_full[\"ConfirmedCases\"].values), np.log1p(predicts_cc_mad))), 5)\n    rmsle_ft_mad = np.around(np.sqrt(mean_squared_error(np.log1p(df_test_full[\"Fatalities\"].values), np.log1p(predicts_ft_mad))), 5)\n    scores_mad_df = pd.concat([scores_mad_df, pd.DataFrame(data={\"mad_factor\": mad_factor,\n                                                                 \"rmsle_cc_mad\": rmsle_cc_mad,\n                                                                 \"rmsle_ft_mad\": rmsle_ft_mad}, index=[0])], axis=0, ignore_index=True)\n\nscores_mad_df[\"mean_rmsle\"] = scores_mad_df[[\"rmsle_cc_mad\", \"rmsle_ft_mad\"]].mean(axis=1).round(5)\nscores_mad_df = scores_mad_df.sort_values(by=\"mean_rmsle\", ascending=True).reset_index(drop=True)\n\nprint(f\"Done in {get_script_time(int(time.time() - t_start))}\")\nscores_mad_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Лучший скор на валидации MAD выдает при mad-факторе 1.07"},{"metadata":{"trusted":true},"cell_type":"code","source":"top_countries = df_test_full[df_test_full[\"geography\"].apply(lambda x: not x.startswith(\"China\"))].groupby(\"geography\")[\"population\"].max().reset_index().sort_values(\n    by=\"population\", ascending=False).reset_index(drop=True)\ntop_countries = top_countries.head(20)[\"geography\"].tolist()\n\ntop_countries","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t_start = time.time()\n\nscores_by_country_mad_df = pd.DataFrame(data={})\n\nfor country in top_countries:\n    for mad_factor in mad_factors:\n        predicts_cc_mad, predicts_ft_mad = predict_mad(df_test_full[df_test_full[\"geography\"] == country], gap, mad_factor, val=True)\n        rmsle_cc_mad = np.around(np.sqrt(mean_squared_error(np.log1p(df_test_full[df_test_full[\"geography\"] == country][\"ConfirmedCases\"].fillna(-1)), np.log1p(predicts_cc_mad))), 5)\n        rmsle_ft_mad = np.around(np.sqrt(mean_squared_error(np.log1p(df_test_full[df_test_full[\"geography\"] == country][\"Fatalities\"].fillna(-1)), np.log1p(predicts_ft_mad))), 5)\n        scores_by_country_mad_df = pd.concat([scores_by_country_mad_df, pd.DataFrame(data={\"country\": country,\n                                                                                           \"mad_factor\": mad_factor,\n                                                                                           \"rmsle_cc_mad\": rmsle_cc_mad,\n                                                                                           \"rmsle_ft_mad\": rmsle_ft_mad}, index=[0])], axis=0, ignore_index=True)\n\nscores_by_country_mad_df[\"mean_rmsle\"] = scores_by_country_mad_df[[\"rmsle_cc_mad\", \"rmsle_ft_mad\"]].mean(axis=1).round(5)\nscores_by_country_mad_df = scores_by_country_mad_df.sort_values(by=[\"country\", \"mean_rmsle\"], ascending=[False, True]).reset_index(drop=True)\nscores_by_country_mad_df = scores_by_country_mad_df.drop_duplicates(subset=[\"country\"], keep=\"first\").sort_values(by=\"mean_rmsle\", ascending=True).reset_index(drop=True)\n\nprint(f\"Done in {get_script_time(int(time.time() - t_start))}\")\nscores_by_country_mad_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. Feature importance"},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm\nimport shap\n\ndef get_shap_values(estimator, X, plot_summary=True):\n    explainer = shap.TreeExplainer(estimator)\n    shap_values = explainer.shap_values(X)\n\n    if plot_summary:\n        shap.summary_plot(shap_values, X, plot_type=\"bar\")\n        shap.summary_plot(shap_values, X)\n\n    df_shap = pd.DataFrame(abs(shap_values).mean(axis=0), X.columns.values).reset_index()\n    df_shap.sort_values(by=0, ascending=False, inplace=True)\n    df_shap.columns = [\"feature\", \"shap_value\"]\n    return df_shap\n\n\nfor gap in [1, 14, 28]:\n    train_data = prepare_features(df_train, gap)\n\n    estimator = lightgbm.LGBMRegressor(**LGB_PARAMS)\n    dont_use_cols = [col for col in df_train.columns if \"target\" in col]\n    estimator.fit(train_data.drop(dont_use_cols, axis=1), train_data[\"target_cc\"])\n\n    print(f\"Gap: {gap}\", \"\\n\")\n    get_shap_values(estimator, train_data.drop(dont_use_cols, axis=1), plot_summary=True)\n    print(\"\\n\" + \"\".join([\"=\" for _ in range(200)]) + \"\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Все топовые фичи выглядят логично\n\n* change_123_cc - отношение ConfirmedCases сегодня к ConfirmedCases 3 дня назад хорошо описывает динамику распространения эпидемии\n* diff_123_cc - аналогично среднее между ConfirmedCases сегодня и 3 дня назад\n* days_since_1_case - важная информация, когда началась эпидемия\n* Интересно, что модель выделила широту, скорее всего выделяя гораздо большие значения заражений в странах севернее экватора\n* Логично, что для моделей на 14 дней и 28 дней в топе фичи lag_14_cc и lag_28_cc (ConfirmedCases 14 и 28 дней назад соответственно)"},{"metadata":{},"cell_type":"markdown","source":"### 3. Кластеризация стран"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = prepare_features(df_panel.drop(df_panel.columns.tolist()[-8:], axis=1), gap=1)\n\ndata = pd.concat([df_panel[[\"Id\", \"geography\", \"Date\", \"ConfirmedCases\", \"Fatalities\", \"Recoveries\"]], data], axis=1)\n\nneed_countries = data[(data[\"Date\"] == pd.to_datetime(\"2020-05-01\")) & (data[\"ConfirmedCases\"] > 1000)][\"geography\"].unique()\n\ndata = data[data[\"geography\"].isin(need_countries)].reset_index(drop=True)\n\n# Кол-во выявленных случаев за день\ndata[\"cc_by_day\"] = 0\ndata.loc[data[\"geography\"] == data[\"geography\"].shift(), \"cc_by_day\"] = data[\"ConfirmedCases\"] - data[\"ConfirmedCases\"].shift()\n\n# Максимум выявленных случаев в день по стране\ndata[\"cc_by_day_max\"] = data.groupby([\"geography\"])[\"cc_by_day\"].transform(np.max)\n\n# Отношение кол-ва выявленных случаев в текущий день к максимуму выявленных случаев\ndata[\"cc_by_day_ratio_to_max\"] = np.around(data[\"cc_by_day\"] / data[\"cc_by_day_max\"], 4)\ndata.loc[data[\"cc_by_day_max\"] == 0, \"cc_by_day_ratio_to_max\"] = 0\n\n# Среднее кол-во выявленных случаев за последние 3 дня (не включая сегодня)\ndata[\"cc_by_day_mean_in_last_3d\"] = data.groupby([\"geography\"])[\"cc_by_day\"].apply(lambda x: x.rolling(3, min_periods=1).mean().round(0)).shift()\n\n# Среднее кол-во выявленных случаев за следующие 3 дня (не включая сегодня)\ndata = data.sort_values(by=[\"geography\", \"Date\"], ascending=[True, False]).reset_index(drop=True)\ndata[\"cc_by_day_mean_in_next_3d\"] = data.groupby([\"geography\"])[\"cc_by_day\"].apply(lambda x: x.rolling(3, min_periods=1).mean().round(0)).shift()\n\n# Отношение кол-ва выявленных случаев сегодня к среднему кол-ву выявленных случаев за 3 дня до этого\ndata = data.sort_values(by=[\"geography\", \"Date\"], ascending=[True, True]).reset_index(drop=True)\ndata[\"cc_by_day_dynamic_to_past\"] = np.around(data[\"cc_by_day\"] / data[\"cc_by_day_mean_in_last_3d\"], 4)\ndata.loc[data[\"cc_by_day_dynamic_to_past\"] == np.inf, \"cc_by_day_dynamic_to_past\"] = 1\n\n# Отношение кол-ва выявленных случаев сегодня к среднему кол-ву выявленных случаев за следующие 3 дня\ndata[\"cc_by_day_dynamic_to_future\"] = np.around(data[\"cc_by_day\"] / data[\"cc_by_day_mean_in_next_3d\"], 4)\ndata.loc[data[\"cc_by_day_dynamic_to_future\"] == np.inf, \"cc_by_day_dynamic_to_future\"] = 1\n\ndata[\"cc_by_day_max_flag\"] = (data[\"cc_by_day_ratio_to_max\"] > .95).astype(np.int64)\ndata[\"cluster\"] = data.groupby([\"geography\"])[\"cc_by_day_max_flag\"].cumsum()\n\ndata[\"cluster\"] = data[\"cluster\"].apply(lambda x: \"До пика\" if x == 0 else \"\")\ndata.loc[(data[\"cluster\"] == \"\") & (data[\"cc_by_day_max_flag\"] == 1), \"cluster\"] = \"Пик\"\ndata.loc[data[\"cluster\"] == \"\", \"cluster\"] = \"После пика\"\n\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nrows, ncols = 2, 4\n\nfig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=[8 * ncols, 6 * nrows])\n\nfor idx, feature in enumerate([\"diff_1_cc\", \"diff_123_cc\", \"diff_change_1_cc\", \"diff_change_12_cc\", \"change_1_cc\", \"change_123_cc\", \"perc_1_ac\", \"perc_1_cc\"]):\n    i, j = int(idx / ncols), int(idx % ncols)\n    quan1 = np.quantile(data[feature].fillna(1).replace(np.inf, 1), .025)\n    quan2 = np.quantile(data[feature].fillna(1).replace(np.inf, 1), .975)\n    values = data[(data[feature] > quan1) & (data[feature] < quan2)][feature].apply(lambda x: 1 if not np.isfinite(x) or np.isnan(x) else x)\n    sns.distplot(values, bins=30, kde=False, ax=axes[i, j])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[data[\"Date\"] == pd.to_datetime(\"2020-05-01\")][\"cluster\"].value_counts(dropna=False).reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cluster_df = data[data[\"Date\"] == pd.to_datetime(\"2020-05-01\")][[\"geography\", \"cluster\"]].reset_index(drop=True)\n\ncluster_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## visualizing ConfirmedCases by day\nfrom bokeh.models import Panel, Tabs\nfrom bokeh.io import output_notebook, show\nfrom bokeh.plotting import figure\n\noutput_notebook()\n\ntab_list = []\nfor geography in data[\"geography\"].unique():\n    df_geography = data[data[\"geography\"] == geography]\n    v = figure(plot_width=800, plot_height=400, x_axis_type=\"datetime\", title=\"Covid-19 ConfirmedCases by day\")\n\n    cluster = df_geography[data[\"Date\"] == pd.to_datetime(\"2020-05-01\")][\"cluster\"].values[0]\n    v.line(df_geography[\"Date\"], df_geography[\"cc_by_day\"], color=\"green\", legend_label=cluster)\n\n    v.legend.location = \"top_left\"\n    tab = Panel(child=v, title=geography)\n    tab_list.append(tab)\n\ntabs = Tabs(tabs=tab_list)\nshow(tabs)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}