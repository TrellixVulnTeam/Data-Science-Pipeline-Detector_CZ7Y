{"cells":[{"metadata":{"_uuid":"67d5707e-0cf3-470b-82d5-914147fe6778","_cell_guid":"d69b47bc-b7ba-4c51-bedb-6408468e0a2b","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\np1 = \"/kaggle/input/covid19-global-forecasting-week-4/\"\np2 = \"/kaggle/input/world-bank-wdi-212-health-systems/\"\np3 = \"/kaggle/input/covid19inf/\"\ntrain = pd.read_csv(p1 + \"train.csv\")\ntest =  pd.read_csv(p1 + \"test.csv\")\nsubmission =  pd.read_csv(p1 + \"submission.csv\")\nhealth = pd.read_csv(p2 + \"2.12_Health_systems.csv\")\ncountry = pd.read_csv(p3 + \"covid19countryinfo.csv\")\npollution = pd.read_csv(p3 + \"region_pollution.csv\")\n\ncountry.drop(columns= country.columns[range(22,54)], inplace=True)\ncountry[\"pop\"] = country[\"pop\"].str.replace(\",\", \"\").astype('float64')\ncountry[\"quarantine\"] = pd.to_datetime(country.quarantine)\ncountry[\"schools\"] = pd.to_datetime(country.schools)\ncountry[\"restrictions\"] = pd.to_datetime(country.restrictions)\n\ntrain[\"Date\"] = pd.to_datetime(train.Date)\ntrain[\"country_province\"] = train[\"Province_State\"]\ntrain.country_province.fillna(train[\"Country_Region\"], inplace=True)\ntest[\"Date\"] = pd.to_datetime(test.Date)\ntest[\"country_province\"] = test[\"Province_State\"]\ntest.country_province.fillna(test[\"Country_Region\"], inplace=True)\ntrain = train.merge(country, how='left', left_on = [\"country_province\"], right_on = [\"country\"])\ntrain = train.merge(pollution, how='left', left_on = [\"country_province\"], right_on = [\"Region\"])\ntrain = train.merge(health, how='left', left_on = [\"Country_Region\", \"Province_State\"], right_on = [\"Country_Region\", \"Province_State\"])\ntest = test.merge(country, how='left', left_on = [\"country_province\"], right_on = [\"country\"])\ntest = test.merge(pollution, how='left', left_on = [\"country_province\"], right_on = [\"Region\"])\ntest = test.merge(health, how='left', left_on = [\"Country_Region\", \"Province_State\"], right_on = [\"Country_Region\", \"Province_State\"])\ntrain[\"days\"] = (train.Date - train.Date[0]).dt.days\ntest[\"days\"] = (test.Date - train.Date[0]).dt.days\n\n#The columns with region/state names in the different csvs are not longer needed\ncolumns_to_drop = [\"country\", \"Region\", \"Province_State\", \"Country_Region\", \"World_Bank_Name\"]\n\ntrain[\"in_quarantine\"] = 0\ntrain[\"in_schools\"] = 0\ntrain[\"in_restrictions\"] = 0\ntest[\"in_quarantine\"] = 0\ntest[\"in_schools\"] = 0\ntest[\"in_restrictions\"] = 0\nfor cp in train.country_province.unique():\n    quarantine = country.loc[country.country == cp, \"quarantine\"]\n    schools = country.loc[country.country == cp, \"schools\"]\n    restrictions = country.loc[country.country == cp, \"restrictions\"]\n    if (len(quarantine) > 0) and (quarantine.values[0] is not np.nan):\n        date1 = pd.to_datetime(quarantine.values[0])\n        train.loc[(train.country_province == cp) & (train.Date > date1), \"in_quarantine\"] = (train.Date - date1).dt.days\n        test.loc[(test.country_province == cp) & (test.Date > date1), \"in_quarantine\"] = (test.Date - date1).dt.days\n        \n    if (len(schools) > 0) and (schools.values[0] is not np.nan):\n        date1 = pd.to_datetime(schools.values[0])\n        train.loc[(train.country_province == cp) & (train.Date > date1), \"in_schools\"] = (train.Date - date1).dt.days\n        test.loc[(test.country_province == cp) & (test.Date > date1), \"in_schools\"] = (test.Date - date1).dt.days\n\n    if (len(restrictions) > 0) and (restrictions.values[0] is not np.nan):\n        date1 = pd.to_datetime(restrictions.values[0])\n        train.loc[(train.country_province == cp) & (train.Date > date1), \"in_restrictions\"] = (train.Date - date1).dt.days\n        test.loc[(test.country_province == cp) & (test.Date > date1), \"in_restrictions\"] = (test.Date - date1).dt.days\n\ncolumns_to_drop += [\"quarantine\", \"schools\", \"restrictions\"]\n\nfrom sklearn.preprocessing import LabelEncoder\nlb = LabelEncoder()\ntrain[\"country_province\"] = lb.fit_transform(train.country_province)\ntest[\"country_province\"] = lb.transform(test.country_province)\n\ntrain[\"days_from_first_case\"] = 0\ntest[\"days_from_first_case\"] = 0\ntrain[\"days_from_first_death\"] = 0\ntrain[\"days_from_case_100\"] = 0\ntest[\"days_from_case_100\"] = 0\ntest[\"days_from_first_death\"] = 0\n\ndates = list(train.Date.unique())\nfor province in tqdm(train.country_province.unique()):\n    print(province)\n    mask1 = train.country_province == province\n    mask2 = train.ConfirmedCases > 1.0\n    mask3 = train.ConfirmedCases > 100.0\n    mask4 = train.Fatalities > 1.0\n    try:\n        idx1 = train.loc[mask1 & mask2 ,[\"ConfirmedCases\"]].idxmin()[0]\n        dateidx1 = train.iloc[idx1][\"Date\"]\n    except:\n        dateidx1 = test.Date.max()\n        pass\n    #print(dateidx1)\n    train.loc[mask1 & (train.Date >= dateidx1), \"days_from_first_case\"] = (train.Date - dateidx1).dt.days\n    test.loc[mask1 & (test.Date >= dateidx1), \"days_from_first_case\"] = (test.Date - dateidx1).dt.days\n    \n    try:\n        idx1 = train.loc[mask1 & mask3 ,[\"ConfirmedCases\"]].idxmin()[0]\n        dateidx1 = train.iloc[idx1][\"Date\"]\n    except:\n        dateidx1 = test.Date.max()\n        pass\n    train.loc[mask1 & (train.Date >= dateidx1), \"days_from_case_100\"] = (train.Date - dateidx1).dt.days\n    test.loc[mask1 & (test.Date >= dateidx1), \"days_from_case_100\"] = (test.Date - dateidx1).dt.days    \n\n        \n    try:\n        idx1 = train.loc[mask1 & mask4 ,[\"Fatalities\"]].idxmin()[0]\n        dateidx1 = train.iloc[idx1][\"Date\"]\n    except:\n        dateidx1 = test.Date.max()\n        pass\n    train.loc[mask1 & (train.Date >= dateidx1), \"days_from_first_death\"] = (train.Date - dateidx1).dt.days\n    test.loc[mask1 & (test.Date >= dateidx1), \"days_from_first_death\"] = (test.Date - dateidx1).dt.days    \n\ntrain.fillna(value = 0, inplace = True)\ntest.fillna(value = 0, inplace = True)  \n\n#Construction of laged variables \nlag_number = 3\nfor lag in range(1, lag_number + 1):\n    var_name = \"cases_lag%d\" % lag\n    train[var_name] = train.ConfirmedCases.shift(periods = lag)\n    train.loc[train.Date <= train.Date[lag - 1] , var_name] = 0\n    var_name = \"fatalities_lag%d\" % lag\n    train[var_name] = train.Fatalities.shift(periods = 1)\n    train.loc[train.Date <= train.Date[lag - 1] , var_name] = 0\n    \n#Days that coincide in train and test\nprint(train.loc[train.Date.isin(test.Date.unique()), \"Date\"].unique())\n# The smallest of those days will be the separation between train and validation\nsep_date = train.loc[train.Date.isin(test.Date.unique()), \"Date\"].unique().min()\n\nresult_columns = [\"ConfirmedCases\", \"Fatalities\"]\nX_train = train.loc[(train.Date<sep_date),].drop(columns = columns_to_drop + [\"Id\", \"Date\"] + result_columns)\ny_train_cases = train.loc[(train.Date<sep_date),].ConfirmedCases\ny_train_fatalities = train.loc[(train.Date<sep_date),].Fatalities\n\nX_val = train.loc[(train.Date>=sep_date),].drop(columns = columns_to_drop + [\"Id\", \"Date\"] + result_columns)\ny_val_cases = train.loc[(train.Date>=sep_date),].ConfirmedCases\ny_val_fatalities = train.loc[(train.Date>=sep_date),].Fatalities\n\nX_test = test.drop(columns = columns_to_drop + [\"ForecastId\", \"Date\"])\n\nfrom sklearn.metrics import mean_squared_error\ndef validate_models(model_cases, model_fatalities):\n    predict_train_cases = model_cases.predict(X_train)\n    predict_val_cases = model_cases.predict(X_val)\n    print(\"RMSE in train detected cases: \", np.sqrt(mean_squared_error(y_train_cases, predict_train_cases)))\n    print(\"RMSE in validation detected cases: \", np.sqrt(mean_squared_error(y_val_cases, predict_val_cases)))\n    predict_train_fatalities = model_fatalities.predict(X_train)\n    predict_val_fatalities = model_fatalities.predict(X_val)\n    print(\"RMSE in train fatalities: \", np.sqrt(mean_squared_error(y_train_fatalities, predict_train_fatalities)))\n    print(\"RMSE in validation fatalities: \", np.sqrt(mean_squared_error(y_val_fatalities, predict_val_fatalities)))\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nlm_cases = LinearRegression()\nlm_cases.fit(X_train, y_train_cases)\n\nlm_fatalities = LinearRegression()\nlm_fatalities.fit(X_train, y_train_fatalities)\n\nvalidate_models(lm_cases, lm_fatalities)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#RandomForest\nfrom sklearn.ensemble import RandomForestRegressor\nrf_cases = RandomForestRegressor(n_estimators= 300, max_depth=6, random_state=0, verbose=0, n_jobs=-1)\nrf_cases.fit(X_train, y_train_cases)\n\nrf_fatalities = RandomForestRegressor(n_estimators= 400, max_depth=6, random_state=0, verbose=0, n_jobs=-1)\nrf_fatalities.fit(X_train, y_train_fatalities)\n\nvalidate_models(rf_cases, rf_fatalities)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV, GridSearchCV","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost\nrf2_cases=xgboost.XGBRegressor()\nrf2_cases.fit(X_train, y_train_cases)\n\nrf2_fatalities=xgboost.XGBRegressor()\nrf2_fatalities.fit(X_train, y_train_cases)\n\nvalidate_models(rf2_cases, rf2_fatalities)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params={\n \"learning_rate\"    : [0.05, 0.10, 0.20, 0.25, 0.30,0.50] ,\n \"max_depth\"        : [ 3, 4, 5, 6, 8, 10, 12, 15],\n \"min_child_weight\" : [ 1, 3, 5, 7 ],\n \"gamma\"            : [ 0.0, 0.1, 0.2 , 0.3, 0.4 ],\n \"colsample_bytree\" : [ 0.3, 0.4, 0.5 , 0.7 ],\n \"n_estimators\"     : [100,300,500,700,800,1000]\n    \n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_search1=RandomizedSearchCV(rf2_cases,param_distributions=params,n_iter=5,n_jobs=-1,cv=5,verbose=3)\nrandom_search1.fit(X_train, y_train_cases)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_search2=RandomizedSearchCV(rf2_fatalities,param_distributions=params,n_iter=5,n_jobs=-1,cv=5,verbose=3)\nrandom_search2.fit(X_train, y_train_cases)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost\nrf1_cases=xgboost.XGBRFRegressor(n_estimators=700 ,n_jobs=-1, random_state=42 ,learning_rate=0.75)\nrf1_cases.fit(X_train, y_train_cases)\n\nrf1_fatalities=xgboost.XGBRFRegressor()\nrf1_fatalities.fit(X_train, y_train_cases)\n\nvalidate_models(rf1_cases, rf1_fatalities)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgb\nlgb_params = {\n               'feature_fraction': 0.8,\n               'metric': 'rmse',\n               'nthread':-1, \n               'min_data_in_leaf': 2**4,\n               'bagging_fraction': 0.75, \n               'learning_rate': 0.5, \n               'objective': 'mse', \n               'bagging_seed': 2**5, \n               'num_leaves': 2**6,\n               'bagging_freq':1,\n               'verbose':0 \n              }\nlgbm_cases = lgb.train(lgb_params, \n                       train_set=lgb.Dataset(X_train, label=y_train_cases), \n                       valid_sets=lgb.Dataset(X_val, label=y_val_cases), \n                       num_boost_round=500)\nlgbm_fatalities = lgb.train(lgb_params, \n                            train_set=lgb.Dataset(X_train, label=y_train_fatalities), \n                            valid_sets=lgb.Dataset(X_val, label=y_val_fatalities), \n                            num_boost_round=500)\nvalidate_models(lgbm_cases, lgbm_fatalities)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVR\nlm_cases = SVR(C=1.0, epsilon=0.2)\nlm_cases.fit(X_train, y_train_cases)\n\nlm_fatalities = SVR(C=1.0, epsilon=0.2)\nlm_fatalities.fit(X_train, y_train_fatalities)\n\nvalidate_models(lm_cases, lm_fatalities)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nlags = {}\npredict_test_cases, predict_test_fatalities = [] , []\ntest_min_day = test.days.min()\nfor i in range(1, lag_number + 1):\n    lags[\"caseslag%d\" % i] = 0\n    lags[\"fatalitieslag%d\" % i] = 0\n    \nfor ind in range(len(X_test)):\n    print(\"case: {} of {}\".format(ind, len(X_test)))\n    #First lag data are obtained either from previous calculations or train data\n    if X_test.iloc[ind].days == test_min_day:\n        #print(test_min_day)\n        for i in range(1, lag_number + 1):\n            mask1 = train.days == (test_min_day - i)\n            mask2 = train.country_province == X_test.iloc[ind].country_province\n            lags[\"caseslag%d\" % i] = train.loc[mask1 & mask2, \"ConfirmedCases\"].values[0]\n            lags[\"fatalitieslag%d\" % i] = train.loc[mask1 & mask2, \"Fatalities\"].values[0]\n    else:\n        lags[\"caseslag1\"] = pred_cases\n        lags[\"fatalitieslag1\"] = pred_fatalities\n        for i in range(2, lag_number + 1):\n            lags[\"caseslag%d\" % i] = lags[\"caseslag%d\" % (i-1)]\n            lags[\"fatalitieslag%d\" % i] = lags[\"fatalitieslag%d\" % (i-1)]\n    x_test = X_test.iloc[ind].copy()\n    x_test =pd.DataFrame(x_test).transpose()\n    for i in range(1, lag_number + 1):\n        x_test[\"cases_lag%d\" % i] = lags[\"caseslag%d\" % i]\n        x_test[\"fatalities_lag%d\" % i] = lags[\"fatalitieslag%d\" % i]\n        \n    pred_cases = lm_cases.predict(x_test)[0]\n    pred_fatalities = lm_fatalities.predict(x_test)[0]\n    predict_test_cases.append(pred_cases)\n    predict_test_fatalities.append(pred_fatalities)\n    \nsubmission.ConfirmedCases = predict_test_cases\nsubmission.Fatalities = predict_test_fatalities\nsubmission.to_csv(\"submission.csv\", index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":4}