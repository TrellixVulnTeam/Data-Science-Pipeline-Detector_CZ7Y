{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\npd.options.display.max_rows = 500\npd.options.display.max_columns = 500\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom scipy.signal import savgol_filter\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.isotonic import IsotonicRegression\nfrom sklearn.metrics import mean_squared_error, mean_squared_log_error\nfrom sklearn.linear_model import LinearRegression, Ridge\n\nimport datetime\nimport gc\nfrom tqdm import tqdm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/covid19-global-forecasting-week-4/train.csv')\nus_before = pd.read_csv('../input/jhu-covid19-data-with-us-state-data-prior-to-mar-9/covid19_train_data_us_states_before_march_09_new.csv')\nupdate = (train['Country_Region'] == 'US') & (train.Date <= '2020-03-09')\ndf = train[update]\nus_before = us_before[(us_before['Country.Region'] == 'US') & (us_before.Date <= '2020-03-09')]\nus_before = us_before[['Province.State', 'Country.Region', 'Date', 'ConfirmedCases', 'Fatalities']]\nus_before.columns = ['Province_State', 'Country_Region', 'Date', 'ConfirmedCases_new', 'Fatalities_new']\ndf.shape, us_before.shape\ndf = df.merge(us_before, how='left', on=['Province_State', 'Country_Region', 'Date']).fillna(0)\ntrain.loc[update, 'ConfirmedCases'] = df['ConfirmedCases_new'].values\ntrain.loc[update, 'Fatalities'] = df['Fatalities_new'].values\n\ntrain['Province_State'].fillna('', inplace=True)\ntrain['Date'] = pd.to_datetime(train['Date'])\ntrain['day'] = train.Date.dt.dayofyear\n#train = train[train.day <= 85]\ntrain['geo'] = ['_'.join(x) for x in zip(train['Country_Region'], train['Province_State'])]\ntrain\n\ndelta = (train[train.geo == 'France_'].ConfirmedCases.diff().max() ) / 2\ntrain.loc[10405:10414, 'ConfirmedCases'] = train.loc[10405:10414, 'ConfirmedCases'] - delta\n\ntest = pd.read_csv('../input/covid19-global-forecasting-week-4/test.csv')\ntest['Province_State'].fillna('', inplace=True)\ntest['Date'] = pd.to_datetime(test['Date'])\ntest['day'] = test.Date.dt.dayofyear\ntest['geo'] = ['_'.join(x) for x in zip(test['Country_Region'], test['Province_State'])]\ntest\n\nday_min = train['day'].min()\ntrain['day'] -= day_min\ntest['day'] -= day_min\n\nmin_test_val_day = test.day.min()\nmax_test_val_day = train.day.max()\nmax_test_day = test.day.max()\nnum_days = max_test_day + 1\n\nmin_test_val_day, max_test_val_day, num_days\n\ntrain['ForecastId'] = -1\ntest['Id'] = -1\ntest['ConfirmedCases'] = 0\ntest['Fatalities'] = 0\n\ndata = pd.concat([train,\n                  test[test.day > max_test_val_day][train.columns]\n                 ]).reset_index(drop=True)\n\n\ndates = data[data['geo'] == 'France_'].Date.values\n\nregion_meta = pd.read_csv('../input/covid19-forecasting-metadata/region_metadata.csv')\nregion_meta['Province_State'].fillna('', inplace=True)\nregion_meta['geo'] = ['_'.join(x) for x in zip(region_meta['Country_Region'], region_meta['Province_State'], )]\nregion_meta\n\nset(data.geo.unique()) - set(region_meta.geo.unique())\n\nregion_meta = data[['geo']].merge(region_meta, how='left', on='geo') #.fillna(0)\nregion_meta = region_meta.groupby('geo').first()\n\n\npopulation = np.log1p(region_meta[['population']])\n#_ = plt.hist(population.population, bins=100)\n\n#population = population.pivot(index='geo', columns='day', values='population').values\npopulation = population[['population']].values\npopulation.shape\n\nlockdown_date = pd.read_csv('../input/covid19-lockdown-dates-by-country/countryLockdowndates.csv')\nlockdown_date['Province'].fillna('', inplace=True)\nlockdown_date['Date'] = pd.to_datetime(lockdown_date['Date'], dayfirst=True)\nlockdown_date['lock_day'] = lockdown_date.Date.dt.dayofyear\nlockdown_date['lock_day'] -= day_min\nlockdown_date['geo'] = ['_'.join(x) for x in zip(lockdown_date['Country/Region'], lockdown_date['Province'])]\nlockdown_date\n\nlockdown_date = data[['geo', 'day']].merge(lockdown_date[['geo', 'lock_day']], how='left', on=['geo'])\nlockdown_date['locked'] = 1 * (lockdown_date['day'] > lockdown_date['lock_day'])\nlockdown_date = lockdown_date.pivot(index='geo', columns='day', values='locked').values\nlockdown_date\n\ngeo_data = data.pivot(index='geo', columns='day', values='ForecastId')\nnum_geo = geo_data.shape[0]\ngeo_data\n\ngeo_id = {}\nfor i,g in enumerate(geo_data.index):\n    geo_id[g] = i\nnum_geo\n\nConfirmedCases = data.pivot(index='geo', columns='day', values='ConfirmedCases')\nFatalities = data.pivot(index='geo', columns='day', values='Fatalities')\n\ncases = np.log1p(ConfirmedCases.values)\ndeaths = np.log1p(Fatalities.values)\n\ndef get_dataset(start_pred, num_train, lag_period, continents_ids_base, country_ids_base, cases, deaths, \n                 population, time_cases, time_deaths, lockdown_date):\n    days = np.arange( start_pred - num_train + 1, start_pred + 1)\n    lag_cases = np.vstack([cases[:, d - lag_period : d] for d in days])\n    lag_deaths = np.vstack([deaths[:, d - lag_period : d] for d in days])\n    target_cases = np.vstack([cases[:, d : d + 1] for d in days])\n    target_deaths = np.vstack([deaths[:, d : d + 1] for d in days])\n    continents_ids = np.vstack([continents_ids_base for d in days])\n    country_ids = np.vstack([country_ids_base for d in days])\n    population = np.vstack([population for d in days])\n    time_case = np.vstack([time_cases[:, d - 1: d ] for d in days])\n    time_death = np.vstack([time_deaths[:, d - 1 : d ] for d in days])\n    lockdown = [get_lockdown(lockdown_date, d) for d in days]\n    lockdown_case = np.vstack([l[0] for l in lockdown])\n    lockdown_death = np.vstack([l[1] for l in lockdown])\n    start_pred = np.hstack([d * np.ones((cases.shape[0], )) for d in days]).astype('int')\n    return (lag_cases, lag_deaths, target_cases, target_deaths, \n            continents_ids, country_ids, population, time_case, time_death, \n            lockdown_case, lockdown_death, start_pred, days)\n\ndef update_valid_dataset(data, pred_death, pred_case, cases, deaths):\n    (lag_cases, lag_deaths, target_cases, target_deaths, \n     continents_ids, country_ids, population, time_case, time_death, \n     lockdown_case, lockdown_death, start_pred, days) = data\n    day = days[-1] + 1\n    new_lag_cases = np.hstack([lag_cases[:, 1:], pred_case])\n    new_lag_deaths = np.hstack([lag_deaths[:, 1:], pred_death]) \n    new_target_cases = cases[:, day:day+1]\n    new_target_deaths = deaths[:, day:day+1] \n    new_continents_ids = continents_ids  \n    new_country_ids = country_ids  \n    new_population = population  \n    new_time_death, new_time_case = update_time(time_death, time_case, pred_death, pred_case)\n    new_lockdown_case = lockdown_case\n    new_lockdown_death = lockdown_death\n    new_start_pred = 1 + start_pred\n    new_days = 1 + days\n    return (new_lag_cases, new_lag_deaths, new_target_cases, new_target_deaths, \n            new_continents_ids, new_country_ids, new_population, \n            new_time_case, new_time_death, new_lockdown_case, new_lockdown_death, \n            new_start_pred, new_days)\n\ndef train_model(train, valid, start_lag_death, end_lag_death, num_lag_case, num_val, score, cases, deaths,):\n    alpha = 3\n    lr_death = Ridge(alpha=alpha, fit_intercept=True)\n    lr_case = Ridge(alpha=alpha, fit_intercept=False)\n        \n    (train_death_score, train_case_score, train_pred_death, train_pred_case,\n    ) = fit_eval(lr_death, lr_case, train, start_lag_death, end_lag_death, num_lag_case, fit=True, score=score,\n                 cases=cases, deaths=deaths)\n    \n    death_scores = []\n    case_scores = []\n    \n    death_pred = []\n    case_pred = []\n    \n    for i in range(num_val):\n\n        (valid_death_score, valid_case_score, valid_pred_death, valid_pred_case,\n        ) = fit_eval(lr_death, lr_case, valid, start_lag_death, end_lag_death, num_lag_case, \n                     fit=False, score=score,\n                 cases=cases, deaths=deaths)\n        death_scores.append(valid_death_score)\n        case_scores.append(valid_case_score)\n        death_pred.append(valid_pred_death)\n        case_pred.append(valid_pred_case)\n        \n        if 0:\n            print('val death: %0.3f' %  valid_death_score,\n                  'val case: %0.3f' %  valid_case_score,\n                  'val : %0.3f' %  np.mean([valid_death_score, valid_case_score]),\n                  flush=True)\n        valid = update_valid_dataset(valid, valid_pred_death, valid_pred_case, cases, deaths)\n    \n    if score:\n        death_scores = np.sqrt(np.mean([s**2 for s in death_scores]))\n        case_scores = np.sqrt(np.mean([s**2 for s in case_scores]))\n        if 0:\n            print('train death: %0.3f' %  train_death_score,\n                  'train case: %0.3f' %  train_case_score,\n                  'val death: %0.3f' %  death_scores,\n                  'val case: %0.3f' %  case_scores,\n                  'val : %0.3f' % ( (death_scores + case_scores) / 2),\n                  flush=True)\n        else:\n            print('%0.4f' %  case_scores,\n                  ', %0.4f' %  death_scores,\n                  '= %0.4f' % ( (death_scores + case_scores) / 2),\n                  flush=True)\n    death_pred = np.hstack(death_pred)\n    case_pred = np.hstack(case_pred)\n    return death_scores, case_scores, death_pred, case_pred\n\ndef get_country_ids(last_train, case_threshold):\n    countries = [g.split('_')[0] for g in geo_data.index]\n    countries = pd.factorize(countries)[0]\n    countries[cases[:, :last_train+1].max(axis=1) < np.log1p(case_threshold)] = -1\n    countries = pd.factorize(countries)[0]\n    \n\n    country_ids_base = countries.reshape((-1, 1))\n    ohe = OneHotEncoder(sparse=False)\n    country_ids_base = 0.2 * ohe.fit_transform(country_ids_base)\n    return country_ids_base\n\ndef get_continent_ids():\n    continents = region_meta['continent']\n    continents = pd.factorize(continents)[0]\n    continents_ids_base = continents.reshape((-1, 1))\n    ohe = OneHotEncoder(sparse=False)\n    continents_ids_base = ohe.fit_transform(continents_ids_base)\n    return continents_ids_base\n\ndef val_score(true, pred):\n    pred = np.log1p(np.round(np.expm1(pred) - 0.2))\n    return np.sqrt(mean_squared_error(true.ravel(), pred.ravel()))\n\ndef val_score(true, pred):\n    return np.sqrt(mean_squared_error(true.ravel(), pred.ravel()))\n\ndef predict(tdata, num_lag, eps, w, window):\n    num_pred = 1\n    num_geo = tdata.shape[0]\n    days = np.arange(num_lag)\n    pred = np.zeros((num_geo, num_pred))\n    start_pred = tdata.shape[1]\n    x0 = np.arange(start_pred).reshape((-1, 1))\n    w_delta = (window - 1)/2\n    x1 = np.arange(start_pred + w_delta, start_pred + 1 + w_delta).reshape((-1, 1))\n    w0 = np.array([w** i for i in range(num_lag)])\n    w0 = w0 / w0.mean()\n    for i in range(num_geo):\n        y0 = pd.Series(tdata[i, :])\n        y = y0.rolling(window=window).mean().fillna(method='bfill')\n        y = y.diff().fillna(0)\n        y = np.clip(y.values, 0, 1e6)\n        if y.max() == 0:\n            #print(0, geo)\n            pred[i, :] = 0 #tdata[i, -1]\n            continue\n        x = x0\n        filter_ = (y > 0)\n        x = x[y > 0]\n        y = y[y > 0]\n        y = y[-num_lag:]\n        x = x[-num_lag:]\n        pad = num_lag - len(y)\n        if pad > 0:\n            y = np.hstack((np.zeros(pad), y))\n            xmin = x.min()\n            x = np.vstack((np.arange(xmin - pad, xmin).reshape((-1, 1)), x))\n        w = w0[-len(y):]\n        if len(y) < num_lag:\n            #print(1, geo, y)\n            pred[i, :] = 0 #tdata[i, -1]\n            continue\n        y = np.log(y + eps)\n        lr = Ridge(fit_intercept=True)\n        #lr = HuberRegressor()\n        #print(w)\n        lr.fit(x, y, w)\n        if 0 and lr.coef_[0] > 0:\n            print('up', lr.coef_, lr.intercept_)\n            #lr.coef_[0] = 0\n            #lr.intercept_ = y.mean()\n        y_pred = lr.predict(x1)\n        #print(y, y.mean(), y_pred)\n\n        y_pred = np.clip(y_pred, -10, np.average(y, weights=w))\n        y_pred = np.exp(y_pred) - eps\n        y_pred = np.clip(y_pred, 0, 5)\n        y_pred = np.cumsum(y_pred)\n        pred[i, :] = y_pred #+ tdata[i, -1]\n    return pred\n\ndef fit_eval(lr_death, lr_case, data, start_lag_death, end_lag_death, num_lag_case, fit, score, cases, deaths):\n    (lag_cases, lag_deaths, target_cases, target_deaths, \n     continents_ids, country_ids, population, time_case, time_death, \n     lockdown_case, lockdown_death, start_pred, days)  = data\n    #lag_cases = lag_cases[:, -lag_period:]\n    #lag_deaths = lag_deaths[:, -lag_period:]\n    idx = np.arange(lag_cases.shape[0])\n    X_trend0 = predict(lag_deaths[:, -lag_period:], num_lag_death_trend, eps_trend, w_trend, window_trend)\n    X_trend = predict(lag_cases[:, -lag_period-lag_case_death:-lag_case_death], num_lag_death_trend, eps_trend, w_trend, window_trend)\n    y_death_prev = lag_deaths[:, -1:]\n    X_trend0 = X_trend0 + y_death_prev\n    X_trend = X_trend + y_death_prev\n    w_t = 0.4 * (lag_deaths[:, -num_lag_death_trend:1-num_lag_death_trend]  > np.log1p(death_trend_thr))\n    X_trend = w_t * X_trend0 + (1 - w_t) * X_trend    \n    y_death = target_deaths\n    if 1:\n        X_death = np.hstack([lag_cases[:, -start_lag_death:-end_lag_death], \n                             lag_deaths[:, -num_lag_death:], \n                             country_ids,\n                             continents_ids,\n                              population,\n                             #time_death,\n                             #lockdown_death[idx, start_pred].reshape((-1, 1)),\n                            ])\n        if fit:\n            lr_death.fit(X_death, y_death)\n        y_pred_death = lr_death.predict(X_death)\n        if 0:\n            w_t = lag_deaths[:, -1:]  \n            w_t = (0 + w_t) / (0 + np.max(w_t))\n            w_t = w_t**0.7\n            w_t = 0.6 * w_t * (lag_deaths[:, -num_lag_death_trend:1-num_lag_death_trend]  > np.log1p(death_trend_thr))\n        w_t = 0.4 * (lag_deaths[:, -num_lag_death_trend:1-num_lag_death_trend]  > np.log1p(death_trend_thr))\n        w_t = 0.6\n        y_pred_death = ((1 - w_t) *  y_pred_death + w_t * X_trend)\n    \n    else:\n        y_pred_death = X_trend\n        y_pred_death = np.maximum(y_pred_death, y_death_prev)\n    \n    \n    y_case_prev = lag_cases[:, -1:]\n    X_trend = predict(lag_cases[:, -lag_period:], num_lag_trend, eps_trend, w_trend, window_trend)\n    X_trend = X_trend + y_case_prev\n    X_case = np.hstack([lag_cases[:, -num_lag_case:], \n                        country_ids, \n                        continents_ids,\n                        population,\n                        time_case,\n                        lockdown_case[idx, start_pred].reshape((-1, 1)),\n                       ])\n    #print(X_case.shape, start_pred.shape, lockdown_case.shape, lockdown_case[idx, start_pred].shape)\n    y_case = target_cases\n    if fit:\n        lr_case.fit(X_case, y_case)\n    y_pred_case = lr_case.predict(X_case)\n    w_t = lag_cases[:, -1:]  \n    w_t = (0 + w_t) / (0 + np.max(w_t))\n    w_t = np.clip(1.1 * w_t**0.7, 0, 1)\n    \n    #w_t = w_t * (lag_cases[:, -num_lag_trend:1-num_lag_trend]  > np.log1p(case_trend_thr))\n    \n    y_pred_case = ((1 - w_t) *  y_pred_case + w_t * X_trend)\n    y_pred_case = np.maximum(y_pred_case, y_case_prev)\n    \n    if score:\n        death_score = val_score(y_death, y_pred_death)\n        case_score = val_score(y_case, y_pred_case)\n    else:\n        death_score = 0\n        case_score = 0\n        \n    return death_score, case_score, y_pred_death, y_pred_case\n\ndef update_time(time_death, time_case, pred_death, pred_case):\n    new_time_death = np.expm1(time_death) + c_death * (pred_death >= np.log1p(t_death))\n    new_time_death = 1 *np.log1p(new_time_death) \n    new_time_case = np.expm1(time_case) + c_case * (pred_case >= np.log1p(t_case))\n    new_time_case = 1 *np.log1p(new_time_case) \n    return new_time_death, new_time_case\n\nc_case = 1\nt_case = 1000\nc_death = 1\nt_death = 100\n\ntime_cases = c_case * (cases >= np.log1p(t_case)) \ntime_cases = np.cumsum(time_cases, axis=1)\ntime_cases = 1 * np.log1p(time_cases) \ntime_cases.shape\n\ntime_deaths = c_death * (cases >= np.log1p(t_death))\ntime_deaths = np.cumsum(time_deaths, axis=1)\ntime_deaths = 1 *np.log1p(time_deaths) \ntime_deaths.shape\n\ndef get_lockdown(lockdown_date, day):\n    lockdown = lockdown_date.copy()\n    # only consider lockdown kinown before val period\n    lockdown[:, day:] = 0\n    time_since_lockdown = np.cumsum(lockdown, axis=1)\n    lockdown_cases = np.log1p(1e1*np.clip(time_since_lockdown - 12, 0, 100))\n    lockdown_deaths = np.log1p(1e1*np.clip(time_since_lockdown- 14, 0, 100))\n    return lockdown_cases, lockdown_deaths\n\nstart_lag_death, end_lag_death = 15, 5,\nnum_train = 7\nnum_lag_death = 6\nnum_lag_case = 12\nlag_period = 15\nlag_all_period = 30\ncase_threshold = 30\nlag_case_death = 7\n\n(num_lag_trend, eps_trend, w_trend, window_trend) = (12, 1e-5, 1.2, 5)\nnum_lag_death_trend = 12\ndeath_trend_thr = 1\ncase_trend_thr = 1\nscores = []\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef get_sub(start_val_delta=0):   \n    start_val = min_test_val_day + start_val_delta\n    last_train = start_val - 1\n    num_val = max_test_val_day - start_val + 1\n    first_train = last_train + 1 - (num_train ) \n    #num_lag_case = num_val\n    keep_cases = cases\n    keep_deaths = deaths\n    print(dates[last_train], '%3d %3d' % (start_val, num_val), end=' ')\n    country_ids_base = get_country_ids(last_train, case_threshold)\n    continents_ids_base = get_continent_ids()\n    train_data = get_dataset(last_train, num_train, lag_period, \n                             continents_ids_base, country_ids_base, keep_cases, keep_deaths, \n                             population, time_cases, time_deaths, lockdown_date)\n    valid_data = get_dataset(start_val, 1, lag_period, \n                             continents_ids_base, country_ids_base, keep_cases, keep_deaths, \n                             population, time_cases, time_deaths, lockdown_date)\n    _, _, val_death_preds, val_case_preds = train_model(train_data, valid_data, start_lag_death, end_lag_death, \n                                                  num_lag_case, num_val, True, keep_cases, keep_deaths,\n                                                  )\n\n    pred_deaths = Fatalities.iloc[:, start_val:start_val+num_val].copy()\n    pred_deaths.iloc[:, :] = np.expm1(val_death_preds)\n    pred_deaths = pred_deaths.stack().reset_index()\n    pred_deaths.columns = ['geo', 'day', 'Fatalities']\n    pred_deaths\n\n    pred_cases = ConfirmedCases.iloc[:, start_val:start_val+num_val].copy()\n    pred_cases.iloc[:, :] = np.expm1(val_case_preds)\n    pred_cases = pred_cases.stack().reset_index()\n    pred_cases.columns = ['geo', 'day', 'ConfirmedCases']\n    pred_cases\n\n    sub = test[['Date', 'ForecastId', 'geo', 'day']]\n    sub = sub.merge(pred_cases, how='left', on=['geo', 'day'])\n    sub = sub.merge(pred_deaths, how='left', on=['geo', 'day'])\n    sub = sub.fillna(0)\n    sub = sub[['ForecastId', 'ConfirmedCases', 'Fatalities']]\n    return sub\n\n#sub = get_sub()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sub","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sub.to_csv('submission.csv', index=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"known_test = train[['geo', 'day', 'ConfirmedCases', 'Fatalities']\n          ].merge(test[['geo', 'day', 'ForecastId']], how='left', on=['geo', 'day'])\nknown_test = known_test[['ForecastId', 'ConfirmedCases', 'Fatalities']][known_test.ForecastId.notnull()].copy()\n\nknown_test\n\nunknow_test = test[test.day > max_test_val_day]\nunknow_test\n\ndef get_final_sub():   \n    start_val = max_test_val_day + 1\n    last_train = start_val - 1\n    num_val = max_test_day - start_val + 1\n    first_train = last_train + 1 - (num_train ) \n    #num_lag_case = num_val\n    keep_cases = cases\n    keep_deaths = deaths\n    print(dates[last_train], '%3d %3d' % (start_val, num_val), end=' ')\n    country_ids_base = get_country_ids(last_train, case_threshold)\n    continents_ids_base = get_continent_ids()\n    train_data = get_dataset(last_train, num_train, lag_period, \n                             continents_ids_base, country_ids_base, keep_cases, keep_deaths, \n                             population, time_cases, time_deaths, lockdown_date)\n    valid_data = get_dataset(start_val, 1, lag_period, \n                             continents_ids_base, country_ids_base, keep_cases, keep_deaths, \n                             population, time_cases, time_deaths, lockdown_date)\n    _, _, val_death_preds, val_case_preds = train_model(train_data, valid_data, start_lag_death, end_lag_death, \n                                                  num_lag_case, num_val, True, keep_cases, keep_deaths,\n                                                  )\n\n    pred_deaths = Fatalities.iloc[:, start_val:start_val+num_val].copy()\n    pred_deaths.iloc[:, :] = np.expm1(val_death_preds)\n    pred_deaths = pred_deaths.stack().reset_index()\n    pred_deaths.columns = ['geo', 'day', 'Fatalities']\n    pred_deaths\n\n    pred_cases = ConfirmedCases.iloc[:, start_val:start_val+num_val].copy()\n    pred_cases.iloc[:, :] = np.expm1(val_case_preds)\n    pred_cases = pred_cases.stack().reset_index()\n    pred_cases.columns = ['geo', 'day', 'ConfirmedCases']\n    pred_cases\n    print(unknow_test.shape, pred_deaths.shape, pred_cases.shape)\n\n    sub = unknow_test[['Date', 'ForecastId', 'geo', 'day']]\n    sub = sub.merge(pred_cases, how='left', on=['geo', 'day'])\n    sub = sub.merge(pred_deaths, how='left', on=['geo', 'day'])\n    #sub = sub.fillna(0)\n    sub = sub[['ForecastId', 'ConfirmedCases', 'Fatalities']]\n    sub = pd.concat([known_test, sub])\n    sub['ForecastId'] = sub['ForecastId'] .astype('int')\n    return sub\n\nsub = get_final_sub()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.to_csv('submission.csv', index=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df = sub.merge(test[['ForecastId', 'geo', 'day']], how='left', on='ForecastId')\nsub_df.ConfirmedCases = np.log1p(sub_df.ConfirmedCases)\nsub_df.Fatalities = np.log1p(sub_df.Fatalities)\nTEST_FIRST = 84","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df[sub_df[\"geo\"] == \"France_\"][[\"day\", \"ConfirmedCases\", \"Fatalities\"]].plot(x=\"day\")\nplt.axvline(TEST_FIRST, color='r', linestyle='--', lw=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df[sub_df[\"geo\"] == \"Brazil_\"][[\"day\", \"ConfirmedCases\", \"Fatalities\"]].plot(x=\"day\")\nplt.axvline(TEST_FIRST, color='r', linestyle='--', lw=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df[sub_df[\"geo\"] == \"Turkey_\"][[\"day\", \"ConfirmedCases\", \"Fatalities\"]].plot(x=\"day\")\nplt.axvline(TEST_FIRST, color='r', linestyle='--', lw=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}