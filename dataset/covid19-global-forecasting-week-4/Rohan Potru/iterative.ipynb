{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Team Name: Jonathan Li\n# Team Members: Rohan Potru, Jonathan Li, Arnav Iyer\n\n### Description:\nThe following notebook was largely written from scratch. The basic method is to use the dates, in order, as features and the places as rows. Neural network models are fit separately for confirmed cases and fatalities. Each sample is normalized with min-max normalization. The input size of the models (the number of features) is equal to the number of unique dates in the train.csv file (N) minus 1. The value at the final date in the train.csv file is the predicted value. Thus, the models perform a regression. After predicting the value at date D, the model predicts on the last N-1 dates to predict the value at D+1, and so on. Thus, predictions are iteratively produced. Notice that the submitted predictions will include the relevant data given in the train.csv file (since there is an overlap).\n\nA custom loss function was introduced to penalize undersetimates more than overestimates. It accomplishes this by adding log(actual/predicted) to the mean absolute error loss. To further ensure that predictions would be non-decreasing, the true prediction is the max of the model's prediction and the value at the previous date.\n\nAdditionally, k-fold cross validation was performed to determine the optimal number of epochs for fitting the model.\n\nFinally, in the case of fatalities, if the model predicted 0 fatalities, a factor of the number of confirmedcases for that place 14 days ago is added to encourage the prediction to rise.\n\n### Resources:\nhttps://www.tensorflow.org/tutorials/keras/regression\nhttps://towardsdatascience.com/advanced-keras-constructing-complex-custom-losses-and-metrics-c07ca130a618"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport tensorflow as tf\nimport math\nfrom tensorflow import keras\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import cross_val_score\nimport keras.backend as kb\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nfilepath = {}\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        filepath[filename[:-4]] = os.path.join(dirname, filename)\n\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The following cell transforms the data into two datframes - one for confirmed cases and one for fatalities - such that each sample is a place and each column is a date."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"inputData = pd.read_csv(filepath[\"train\"])\n\ninputData = inputData.rename(columns = {\"Country_Region\" : \"Place\"})\n\n# len(inputData['Province_State'])\n# len(inputData['Place'])\nfor i in range(inputData.shape[0]):\n    if str(inputData['Province_State'][i]) != \"nan\":\n        inputData['Place'][i] = inputData['Province_State'][i] + \", \" + inputData[\"Place\"][i]\n\n\nplaces = inputData.Place.unique()\ninDates = inputData.Date.unique()\n\nnplaces = len(places)\nnInDates = len(inDates)\n\ncaseData = {}\nfatalityData = {}\n\nfor date in inDates:\n    caseData[date] = []\n    fatalityData[date] = []\n    \na = 0\nb = nInDates\nfor i in range(313):    \n    for j in range(a, b):\n        caseData[inputData['Date'][j]].append(inputData['ConfirmedCases'][j])\n        fatalityData[inputData['Date'][j]].append(inputData['Fatalities'][j])   \n    a+=nInDates\n    b+=nInDates\n\n\ncasesdf = pd.DataFrame(caseData, index = [i for i in range(nplaces)])\nfataldf = pd.DataFrame(fatalityData, index = [i for i in range(nplaces)])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some code to store the dates for submission, the dates given, the dates that need to be predicted, etc."},{"metadata":{"trusted":true},"cell_type":"code","source":"outputData = pd.read_csv(filepath['test'])\noutputDates = outputData.Date.unique()\n\nbeginOutDate = outputDates[0]\nendOutDate = outputDates[-1]\nendInDate = inDates[-1]\n\noutputDates = outputDates[np.where(outputDates == endInDate)[0][0]+1:]\noutputDates","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Min-Max normalization. The normed values will be inversed to generate the actual predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"mins = []\nmaxes = []\n\ndef norm(series):\n    if np.min(series.values) != np.max(series.values):\n        return np.min(series.values), np.max(series.values), (series-np.min(series.values))/(np.max(series.values)-np.min(series.values))\n    return 0,1,series\n\ndef invnorm(x, mini, maxi):\n    return x*(maxi-mini) + mini","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Custom loss function and function to get compiled neural network model"},{"metadata":{"trusted":true},"cell_type":"code","source":"def custom_loss(actual, predicted):\n    return kb.maximum(0.0, kb.abs(actual-predicted)+kb.log(actual/predicted))\n\ndef get_model():\n    model = keras.Sequential([\n    keras.layers.Dense(64, activation='relu', input_shape=[nInDates-1]),\n    keras.layers.Dense(32, activation='relu'),\n    keras.layers.Dense(1)\n    ])\n\n    optimizer = tf.keras.optimizers.RMSprop(0.001)\n\n    model.compile(loss=custom_loss, optimizer = optimizer)\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1. Confirmed Cases"},{"metadata":{},"cell_type":"markdown","source":"Normalization"},{"metadata":{"trusted":true},"cell_type":"code","source":"normed_casesdf = casesdf.copy()\n\nfor i in range(313):\n    mini, maxi, normed = norm(casesdf.loc[i,:])\n    mins.append(mini)\n    maxes.append(maxi)\n    for j in range(nInDates):\n        normed_casesdf[inDates[j]][i] = normed[j]\n\nnormed_casesdf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"k-fold cross validation for number of epochs. 10 folds, epochs tested by 10s from 10 to 100."},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = [(i+1)*10 for i in range(10)]\n\ncvscores = []\n\nfor epochsize in epochs:    \n    totalloss = 0\n    \n    print(epochsize)\n    \n    for i in range(10):\n        model = get_model()\n        \n        validation_df = normed_casesdf.iloc[(31*i):(31*(i+1))]\n        training_df = normed_casesdf.copy()\n        training_df.drop(training_df.index[(31*i):(31*(i+1))])\n        \n        model.fit(training_df[training_df.columns[:-1]], training_df[training_df.columns[-1]], epochs = epochsize, verbose=0)\n        \n        loss = model.evaluate(validation_df[validation_df.columns[:-1]], validation_df[validation_df.columns[-1]])\n        \n        totalloss+=loss\n        \n    \n    avg = totalloss/10\n    cvscores.append(avg)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cvscores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimalepochsize = epochs[cvscores.index(min(cvscores))]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Iteratively produce predictions. Fit on given dates and then walk forward with predictions."},{"metadata":{"trusted":true},"cell_type":"code","source":"model = get_model()\n\nnormed_train_X = normed_casesdf[normed_casesdf.columns[:-1]]\nnormed_train_y = normed_casesdf[normed_casesdf.columns[-1]]\n\nmodel.fit(normed_train_X, normed_train_y, epochs = optimalepochsize)\n\nfor i in range(len(outputDates)):\n    normed_X = normed_casesdf[normed_casesdf.columns[i+1:]]\n    normed_predictions = model.predict(normed_X).reshape(nplaces)\n      \n    for j in range(nplaces):\n        normed_predictions[j] = max(normed_predictions[j], normed_casesdf[normed_casesdf.columns[-1]][j])    #ensure non-decreasing predictions\n \n    unnormed_predictions = [invnorm(normed_predictions[j], mins[j], maxes[j]) for j in range(nplaces)]\n    \n    casesdf[outputDates[i]] = unnormed_predictions              #add predictions directly to dataframe which stores all confirmed cases values\n    normed_casesdf[outputDates[i]] = normed_predictions","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Fatalities"},{"metadata":{},"cell_type":"markdown","source":"Normalize again"},{"metadata":{"trusted":true},"cell_type":"code","source":"mins = []\nmaxes = []\n\nnormed_fataldf = fataldf.copy()\n\nfor i in range(313):\n    mini, maxi, normed = norm(fataldf.loc[i,:])\n    mins.append(mini)\n    maxes.append(maxi)\n    for j in range(nInDates):\n        normed_fataldf[inDates[j]][i] = normed[j]\n\nnormed_fataldf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Predict fatalities. "},{"metadata":{"trusted":true},"cell_type":"code","source":"model2 = get_model()\n\nnormed_train_X = normed_fataldf[inDates[:-1]]\nnormed_train_y = normed_fataldf[inDates[-1]]\n\nmodel2.fit(normed_train_X, normed_train_y, epochs = optimalepochsize)\n\nfor i in range(len(outputDates)):\n    normed_train_X = normed_fataldf[normed_fataldf.columns[i+1:]]\n    normed_predictions = model.predict(normed_train_X).reshape(nplaces)\n    \n    unnormed_predictions = []\n    \n    for j in range(nplaces):\n        normed_predictions[j] = max(normed_predictions[j], normed_fataldf[normed_fataldf.columns[-1]][j])\n        \n        unnormed_prediction = invnorm(normed_predictions[j], mins[j], maxes[j])      #different from confirmed cases code\n        if unnormed_prediction == 0:                                                 #ensures fatalities do not stay at 0 by considering cases\n            unnormed_prediction = fataldf[fataldf.columns[-14]][j]*.05\n        unnormed_predictions.append(unnormed_prediction)\n    \n    fataldf[outputDates[i]] = unnormed_predictions\n    normed_fataldf[outputDates[i]] = normed_predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"casesdf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fataldf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The rest of the cells use the dataframes (which have values from January to May) to prepare the submission file."},{"metadata":{"trusted":true},"cell_type":"code","source":"inDatesTail = inDates[np.where(inDates == beginOutDate)[0][0]:]\nsubmissionDates = [i for i in inDatesTail] + [i for i in outputDates]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"caseSub = casesdf[submissionDates]\nfatalSub = fataldf[submissionDates]\n\ncaseSub","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submissiondf = pd.read_csv(filepath['submission'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for place in range(nplaces):\n    for date in range(len(submissionDates)):\n        submissiondf['ConfirmedCases'][place*len(submissionDates) + date] = caseSub[submissionDates[date]][place]\n        submissiondf['Fatalities'][place*len(submissionDates) + date] = fatalSub[submissionDates[date]][place]\n\nsubmissiondf.set_index(['ForecastId'])\n\nsubmissiondf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submissiondf.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}