{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":false},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"code","source":"import math\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import mean_absolute_error\n\ndef rmsle(y, y_pred):\n        assert len(y) == len(y_pred)\n        terms_to_sum = [(math.log(y_pred[i] + 1) - math.log(y[i] + 1)) ** 2.0 for i,pred in enumerate(y_pred)]\n        return (sum(terms_to_sum) * (1.0/len(y))) ** 0.5\n    \n\ndef fix_target(frame, key, target, new_target_name=\"target\"):\n    import numpy as np\n\n    corrections = 0\n    group_keys = frame[ key].values.tolist()\n    target = frame[target].values.tolist()\n\n    for i in range(1, len(group_keys) - 1):\n        previous_group = group_keys[i - 1]\n        current_group = group_keys[i]\n\n        previous_value = target[i - 1]\n        current_value = target[i]\n        if current_group == previous_group:\n                if current_value<previous_value:\n                    current_value=previous_value\n                    target[i] =current_value\n\n\n        target[i] =max(0,target[i] )#correct negative values\n\n    frame[new_target_name] = np.array(target)\n    \n    \ndef rate(frame, key, target, new_target_name=\"rate\"):\n    import numpy as np\n\n\n    corrections = 0\n    group_keys = frame[ key].values.tolist()\n    target = frame[target].values.tolist()\n    rate=[1.0 for k in range (len(target))]\n\n    for i in range(1, len(group_keys) - 1):\n        previous_group = group_keys[i - 1]\n        current_group = group_keys[i]\n\n        previous_value = target[i - 1]\n        current_value = target[i]\n         \n        if current_group == previous_group:\n                if previous_value!=0.0:\n                     rate[i]=current_value/previous_value\n\n                 \n        rate[i] =max(1,rate[i] )#correct negative values\n\n    frame[new_target_name] = np.array(rate)\n    \ndef get_data_by_key(dataframe, key, key_value, fields=None):\n    mini_frame=dataframe[dataframe[key]==key_value]\n    if not fields is None:                \n        mini_frame=mini_frame[fields].values\n        \n    return mini_frame\n\ndirectory=\"/kaggle/input/covid19-global-forecasting-week-4/\"\n\ntrain=pd.read_csv(directory + \"train.csv\", parse_dates=[\"Date\"] , engine=\"python\")\ntest=pd.read_csv(directory + \"test.csv\", parse_dates=[\"Date\"], engine=\"python\")\n\ntrain[\"key\"]=train[[\"Province_State\",\"Country_Region\"]].apply(lambda row: str(row[0]) + \"_\" + str(row[1]),axis=1)\ntest[\"key\"]=test[[\"Province_State\",\"Country_Region\"]].apply(lambda row: str(row[0]) + \"_\" + str(row[1]),axis=1)\n\n#last day in train\nmax_train_date=train[\"Date\"].max()\nmax_test_date=test[\"Date\"].max()\nhorizon=  (max_test_date-max_train_date).days\nprint (\"horizon\", int(horizon))\n\n\n#test_new=pd.merge(test,train, how=\"left\", left_on=[\"key\",\"Date\"], right_on=[\"key\",\"Date\"] )\n#train.to_csv(directory + \"transfomed.csv\")\n\ntarget1=\"ConfirmedCases\"\ntarget2=\"Fatalities\"\n\nkey=\"key\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"fix_target(train, key, target1, new_target_name=target1)\nfix_target(train, key, target2, new_target_name=target2)\n\nrate(train, key, target1, new_target_name=\"rate_\" +target1 )\nrate(train, key, target2, new_target_name=\"rate_\" +target2 )\nunique_keys=train[key].unique()\nprint(len(unique_keys))\n\n\ntrain","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"\n\ndef get_lags(rate_array, current_index, size=20):\n    lag_confirmed_rate=[-1 for k in range(size)]\n    for j in range (0, size):\n        if current_index-j>=0:\n            lag_confirmed_rate[j]=rate_array[current_index-j]\n        else :\n            break\n    return lag_confirmed_rate\n\ndef days_ago_thresold_hit(full_array, indx, thresold):\n        days_ago_confirmed_count_10=-1\n        if full_array[indx]>thresold: # if currently the count of confirmed is more than 10\n            for j in range (indx,-1,-1):\n                entered=False\n                if full_array[j]<=thresold:\n                    days_ago_confirmed_count_10=abs(j-indx)\n                    entered=True\n                    break\n                if entered==False:\n                    days_ago_confirmed_count_10=100 #this value would we don;t know it cross 0      \n        return days_ago_confirmed_count_10 \n    \n    \ndef ewma_vectorized(data, alpha):\n    sums=sum([ (alpha**(k+1))*data[k] for  k in range(len(data)) ])\n    counts=sum([ (alpha**(k+1)) for  k in range(len(data)) ])\n    return sums/counts\n\ndef generate_ma_std_window(rate_array, current_index, size=20, window=3):\n    ma_rate_confirmed=[-1 for k in range(size)]\n    std_rate_confirmed=[-1 for k in range(size)] \n    \n    for j in range (0, size):\n        if current_index-j>=0:\n            ma_rate_confirmed[j]=np.mean(rate_array[max(0,current_index-j-window+1 ):current_index-j+1])\n            std_rate_confirmed[j]=np.std(rate_array[max(0,current_index-j-window+1 ):current_index-j+1])           \n        else :\n            break\n    return ma_rate_confirmed, std_rate_confirmed\n\ndef generate_ewma_window(rate_array, current_index, size=20, window=3, alpha=0.05):\n    ewma_rate_confirmed=[-1 for k in range(size)]\n\n    \n    for j in range (0, size):\n        if current_index-j>=0:\n            ewma_rate_confirmed[j]=ewma_vectorized(rate_array[max(0,current_index-j-window+1 ):current_index-j+1, ], alpha)           \n        else :\n            break\n    \n    #print(ewma_rate_confirmed)\n    return ewma_rate_confirmed\n\n\ndef get_target(rate_col, indx, horizon=33, average=3, use_hard_rule=False):\n    target_values=[-1 for k in range(horizon)]\n    cou=0\n    for j in range(indx+1, indx+1+horizon):\n        if j<len(rate_col):\n            if average==1:\n                target_values[cou]=rate_col[j]\n            else :\n                if use_hard_rule and j +average <=len(rate_col) :\n                     target_values[cou]=np.mean(rate_col[j:j +average])\n                else :\n                    target_values[cou]=np.mean(rate_col[j:min(len(rate_col),j +average)])\n                   \n            cou+=1\n        else :\n            break\n    return target_values\n\n\ndef dereive_features(frame, confirmed, fatalities, rate_confirmed, rate_fatalities, \n                     horizon ,size=20, windows=[3,7], days_back_confimed=[1,10,100], days_back_fatalities=[1,2,10]):\n    targets=[]\n    \n    names=[\"lag_confirmed_rate\" + str(k+1) for k in range (size)]\n    for day in days_back_confimed:\n        names+=[\"days_ago_confirmed_count_\" + str(day) ]\n    for window in windows:        \n        names+=[\"ma\" + str(window) + \"_rate_confirmed\" + str(k+1) for k in range (size)]\n        names+=[\"std\" + str(window) + \"_rate_confirmed\" + str(k+1) for k in range (size)] \n        names+=[\"ewma\" + str(window) + \"_rate_confirmed\" + str(k+1) for k in range (size)]         \n        \n        \n    names+=[\"lag_fatalities_rate\" + str(k+1) for k in range (size)]\n    for day in days_back_fatalities:\n        names+=[\"days_ago_fatalitiescount_\" + str(day) ]    \n    for window in windows:        \n        names+=[\"ma\" + str(window) + \"_rate_fatalities\" + str(k+1) for k in range (size)]\n        names+=[\"std\" + str(window) + \"_rate_fatalities\" + str(k+1) for k in range (size)]  \n        names+=[\"ewma\" + str(window) + \"_rate_fatalities\" + str(k+1) for k in range (size)]        \n    names+=[\"confirmed_level\"]\n    names+=[\"fatalities_level\"]    \n    \n    names+=[\"confirmed_plus\" + str(k+1) for k in range (horizon)]    \n    names+=[\"fatalities_plus\" + str(k+1) for k in range (horizon)]  \n    \n    #names+=[\"current_confirmed\"]\n    #names+=[\"current_fatalities\"]    \n    \n    features=[]\n    for i in range (len(confirmed)):\n        row_features=[]\n        #####################lag_confirmed_rate       \n        lag_confirmed_rate=get_lags(rate_confirmed, i, size=size)\n        row_features+=lag_confirmed_rate\n        #####################days_ago_confirmed_count_10\n        for day in days_back_confimed:\n            days_ago_confirmed_count_10=days_ago_thresold_hit(confirmed, i, day)               \n            row_features+=[days_ago_confirmed_count_10] \n        #####################ma_rate_confirmed       \n        #####################std_rate_confirmed \n        for window in windows:\n            ma3_rate_confirmed,std3_rate_confirmed= generate_ma_std_window(rate_confirmed, i, size=size, window=window)\n            row_features+= ma3_rate_confirmed   \n            row_features+= std3_rate_confirmed          \n            ewma3_rate_confirmed=generate_ewma_window(rate_confirmed, i, size=size, window=window, alpha=0.05)\n            row_features+= ewma3_rate_confirmed              \n        #####################lag_fatalities_rate   \n        lag_fatalities_rate=get_lags(rate_fatalities, i, size=size)\n        row_features+=lag_fatalities_rate\n        #####################days_ago_confirmed_count_10\n        for day in days_back_fatalities:\n            days_ago_fatalitiescount_2=days_ago_thresold_hit(fatalities, i, day)               \n            row_features+=[days_ago_fatalitiescount_2]     \n        #####################ma_rate_fatalities       \n        #####################std_rate_fatalities \n        for window in windows:        \n            ma3_rate_fatalities,std3_rate_fatalities= generate_ma_std_window(rate_fatalities, i, size=size, window=window)\n            row_features+= ma3_rate_fatalities   \n            row_features+= std3_rate_fatalities  \n            ewma3_rate_fatalities=generate_ewma_window(rate_fatalities, i, size=size, window=window, alpha=0.05)\n            row_features+= ewma3_rate_fatalities                  \n        ##################confirmed_level\n        confirmed_level=0\n        \n        \"\"\"\n        if confirmed[i]>0 and confirmed[i]<1000:\n            confirmed_level= confirmed[i]\n        else :\n            confirmed_level=2000\n        \"\"\"   \n        confirmed_level= confirmed[i]\n        row_features+=[confirmed_level]\n        ##################fatalities_is_level\n        fatalities_is_level=0\n        \"\"\"\n        if fatalities[i]>0 and fatalities[i]<100:\n            fatalities_is_level= fatalities[i]\n        else :\n            fatalities_is_level=200            \n        \"\"\"\n        fatalities_is_level= fatalities[i]\n        \n        row_features+=[fatalities_is_level]              \n            \n        #######################confirmed_plus target\n        confirmed_plus=get_target(rate_confirmed, i, horizon=horizon)\n        row_features+= confirmed_plus          \n        #######################fatalities_plus target\n        fatalities_plus=get_target(rate_fatalities, i, horizon=horizon)\n        row_features+= fatalities_plus \n        ##################current_confirmed\n        #row_features+=[confirmed[i]]\n        ##################current_fatalities\n        #row_features+=[fatalities[i]]        \n        \n          \n\n        \n        features.append(row_features)\n        \n    new_frame=pd.DataFrame(data=features, columns=names).reset_index(drop=True)\n    frame=frame.reset_index(drop=True)\n    frame=pd.concat([frame, new_frame], axis=1)\n    #print(frame.shape)\n    return frame\n    \n    \ndef feature_engineering_for_single_key(frame, group, key, horizon=33, size=20, windows=[3,7], \n                                       days_back_confimed=[1,10,100], days_back_fatalities=[1,2,10]):\n    mini_frame=get_data_by_key(frame, group, key, fields=None)\n    \n    mini_frame_with_features=dereive_features(mini_frame, mini_frame[\"ConfirmedCases\"].values,\n                                              mini_frame[\"Fatalities\"].values, mini_frame[\"rate_ConfirmedCases\"].values, \n                                               mini_frame[\"rate_Fatalities\"].values, horizon ,size=size, windows=windows,\n                                              days_back_confimed=days_back_confimed, days_back_fatalities=days_back_fatalities)\n    #print (mini_frame_with_features.shape[0])\n    return mini_frame_with_features\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from tqdm import tqdm\ntrain_frame=[]\nsize=20\nwindows=[3,5,7]\ndays_back_confimed=[1,10,100]\ndays_back_fatalities=[1,2,10]\n#print (len(train['key'].unique()))\nfor unique_k in tqdm(unique_keys):\n    mini_frame=feature_engineering_for_single_key(train, key, unique_k, horizon=horizon, size=size, \n                                                  windows=windows, days_back_confimed=days_back_confimed,\n                                                  days_back_fatalities=days_back_fatalities).reset_index(drop=True) \n    #print (mini_frame.shape[0])\n    train_frame.append(mini_frame)\n    \ntrain_frame = pd.concat(train_frame, axis=0).reset_index(drop=True)\n#train_frame.to_csv(directory +\"all\" + \".csv\", index=False)\nnew_unique_keys=train_frame['key'].unique()\nfor kee in new_unique_keys:\n    if kee not in unique_keys:\n        print (kee , \" is not there \")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"import lightgbm as lgb\nfrom sklearn.linear_model import Ridge\nfrom sklearn.externals import joblib\n\ndef bagged_set_train(X_ts,y_cs,wts, seed, estimators,xtest, xt=None,yt=None, output_name=None):\n   #print (type(yt))\n   # create array object to hold predictions \n  \n   baggedpred=np.array([ 0.0 for d in range(0, xtest.shape[0])]) \n   #print (y_cs[:10])\n   #print (yt[:10])  \n\n   #loop for as many times as we want bags\n   for n in range (0, estimators):\n       \n       params = {'objective': 'rmse',\n                'metric': 'rmse',\n                'boosting': 'gbdt',\n                'learning_rate': 0.01, #change here    \n                'drop_rate':0.01,\n                #'alpha': 0.99, \n                'skip_drop':0.6,\n                'uniform_drop':True,               \n                'verbose': -1,    \n                'num_leaves': 30, # ~18    \n                'bagging_fraction': 0.9,    \n                'bagging_freq': 1,    \n                'bagging_seed': seed + n,    \n                'feature_fraction': 0.8,    \n                'feature_fraction_seed': seed + n,    \n                'min_data_in_leaf': 10, #30, #56, # 10-50    \n                'max_bin': 100, # maybe useful with overfit problem    \n                'max_depth':20,                   \n                #'reg_lambda': 10,    \n                'reg_alpha':1,    \n                'lambda_l2': 10,\n                #'categorical_feature':'2', # because training data is extremely unbalanced                     \n                'num_threads':6\n                }\n       d_train = lgb.Dataset(X_ts,y_cs, weight=wts, free_raw_data=False)#np.log1p(\n       if not type(yt) is type(None):           \n           d_cv = lgb.Dataset(xt,yt, free_raw_data=False, reference=d_train)#, reference=d_train\n           model = lgb.train(params,d_train,num_boost_round=500,\n                             valid_sets=d_cv,\n\n                             verbose_eval=50 ) #1000                        \n           \n       else :\n           #d_cv = lgb.Dataset(xt, free_raw_data=False, categorical_feature=\"2\")  \n           model = lgb.train(params,d_train,num_boost_round=500) #1000                              \n           #importances=model.feature_importance('gain')\n           #print(importances)\n       preds=model.predict(xtest)               \n       # update bag's array\n       baggedpred+=preds\n       #np.savetxt(\"preds_lgb\" + str(n)+ \".csv\",baggedpred)   \n       #if n%5==0:\n           #print(\"completed: \" + str(n)  )                 \n\n   if not output_name is None:\n        joblib.dump((model), output_name)\n   \"\"\"\n   model=Ridge(normalize=True, alpha=0.1, random_state=1)\n   model.fit(X_ts,y_cs)\n   preds=model.predict(xtest)\n   baggedpred+=preds\n   \"\"\"\n   # divide with number of bags to create an average estimate  \n   baggedpred/= estimators\n     \n   return baggedpred","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"\nnames=[\"lag_confirmed_rate\" + str(k+1) for k in range (size)]\nfor day in days_back_confimed:\n    names+=[\"days_ago_confirmed_count_\" + str(day) ]\nfor window in windows:        \n    names+=[\"ma\" + str(window) + \"_rate_confirmed\" + str(k+1) for k in range (size)]\n    names+=[\"std\" + str(window) + \"_rate_confirmed\" + str(k+1) for k in range (size)] \n    names+=[\"ewma\" + str(window) + \"_rate_confirmed\" + str(k+1) for k in range (size)]         \n\n\nnames+=[\"lag_fatalities_rate\" + str(k+1) for k in range (size)]\nfor day in days_back_fatalities:\n    names+=[\"days_ago_fatalitiescount_\" + str(day) ]    \nfor window in windows:        \n    names+=[\"ma\" + str(window) + \"_rate_fatalities\" + str(k+1) for k in range (size)]\n    names+=[\"std\" + str(window) + \"_rate_fatalities\" + str(k+1) for k in range (size)]  \n    names+=[\"ewma\" + str(window) + \"_rate_fatalities\" + str(k+1) for k in range (size)]        \nnames+=[\"confirmed_level\"]\nnames+=[\"fatalities_level\"]      \n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"number_of_folds=5\nseed=1412\n\ntarget_confirmed=[\"confirmed_plus\" + str(k+1) for k in range (horizon)]    \ntarget_fatalities=[\"fatalities_plus\" + str(k+1) for k in range (horizon)]  \nrmsle_metric_confirmed=0.0\nrmsle_metric_fatalities=0.0      \nprint (train_frame[target1].mean())\nprint (len(train_frame['key'].unique()))\nnumber_of_folds=0\n\n\n#################Full model\n\n#### scoring \ndef decay_4_first_10_then_1_f(array):\n    arr=[1.0 for k in range(len(array))]\n    for j in range(len(array)):\n        if j<10:\n            arr[j]=1. + (max(1,array[j])-1.)/4.\n        else :\n            arr[j]=1.\n    return arr\n            \ndef decay_2_f(array):\n    arr=[1.0 for k in range(len(array))]    \n    for j in range(len(array)):\n            arr[j]=1. + (max(1,array[j])-1.)/2.\n    return arr            \n\ndef decay_1_5_f(array):\n    arr=[1.0 for k in range(len(array))]    \n    for j in range(len(array)):\n            arr[j]=1. + (max(1,array[j])-1.)/1.5\n    return arr            \n         \n         \ndef stay_same_f(array):\n    arr=[1.0 for k in range(len(array))]      \n    for j in range(len(array)):\n        arr[j]=1.\n    return arr   \n\ndef decay_2_last_12_linear_inter_f(array):\n    arr=[1.0 for k in range(len(array))]\n    for j in range(len(array)):\n        arr[j]=1. + (max(1,array[j])-1.)/2.\n    arr12= (max(1,arr[-12])-1.)/12. \n\n    for j in range(0, 12):\n        arr[len(arr)-12 +j]= max(1, 1 + ( (arr12*12) - (j+1)*arr12 ))\n    return arr\n\ndef linear_last_12_f(array):\n    arr=[1.0 for k in range(len(array))]\n    for j in range(len(array)):\n        arr[j]=max(1,array[j])\n    arr12= (max(1,arr[-12])-1.)/12. \n    \n    for j in range(0, 12):\n        arr[len(arr)-12 +j]= max(1, 1 + ( (arr12*12) - (j+1)*arr12 ))\n    return arr\n    \ndecay_4_first_10_then_1 =[\"Beijing_China\",\"Fujian_China\",\"Guangdong_China\", \"Hong Kong_China\",\n\"Inner Mongolia_China\",\"Jiangsu_China\",\"Liaoning_China\",\"Macau_China\",\"Shandong_China\",\"Tianjin_China\",\n\"Yunnan_China\",\"Zhejiang_China\",\"Northern Territory_Australia\",\"nan_Bahamas\",\n\"nan_Belize\",\"nan_Benin\",\"nan_Bhutan\",\"nan_Seychelles\"]\n\ndecay_2 =[\"Shanghai_China\" , \"nan_Afghanistan\",\"nan_Andorra\",\"Australian Capital Territory_Australia\",\n\"South Australia_Australia\",\"Tasmania_Australia\",\"nan_Azerbaijan\",\"nan_Bahrain\",\"nan_Bangladesh\",\"nan_Belarus\"\n\"nan_Belgium\",\"nan_Bolivia\",\"nan_Cameroon\",\"Manitoba_Canada\",\"New Brunswick_Canada\",\"Newfoundland and Labrador_Canada\",\n\"Saskatchewan_Canada\",\"nan_Central African Republic\",\"nan_Congo (Kinshasa)\",\"nan_Cote d'Ivoire\",\"nan_Cuba\",\"Mayotte_France\",\n\"nan_Honduras\"]\n\n\ndecay_2_last_12_linear_inter=[\"nan_Angola\" , \"nan_Barbados\",\"nan_Cabo Verde\" ,\"Prince Edward Island_Canada\",\"nan_Chad\",\n\"nan_Congo (Brazzaville)\",\"Greenland_Denmark\",\"nan_Djibouti\",\"nan_Dominica\",\"nan_El Salvador\",\"nan_Equatorial Guinea\",\n\"nan_Eritrea\",\"nan_Eswatini\",\"nan_Fiji\",\"French Guiana_France\",\"French Polynesia_France\",\"New Caledonia_France\",\n\"Saint Barthelemy_France\",\"St Martin_France\",\"nan_Gabon\",\"nan_Gambia\",\"nan_Grenada\",\"nan_Guinea\",\"nan_Guinea-Bissau\",\n\"nan_Guyana\",\"nan_Haiti\",\"nan_Holy See\",\"nan_Kyrgyzstan\",\"nan_Laos\",\"nan_Liberia\",\"nan_Libya\",\"nan_Madagascar\",\n\"nan_Maldives\",\"nan_Mali\",\"nan_Mauritania\",\"nan_Mauritius\",\"nan_Mongolia\",\"nan_Mozambique\",\"nan_Namibia\",\"nan_Nepal\",\n\"Aruba_Netherlands\",\"Curacao_Netherlands\",\"Sint Maarten_Netherlands\",\"nan_Nicaragua\",\"nan_Niger\",\"nan_Papua New Guinea\",\n\"nan_Saint Kitts and Nevis\",\"nan_Saint Lucia\",\"nan_Somalia\",\"nan_Sudan\",\"nan_Suriname\",\"nan_Syria\",\"nan_Tanzania\",\n\"nan_Togo\",\"Virgin Islands_US\",\"Bermuda_United Kingdom\",\"Cayman Islands_United Kingdom\",\"Channel Islands_United Kingdom\",\n\"Gibraltar_United Kingdom\",\"Isle of Man_United Kingdom\",\"nan_Zimbabwe\"]\n\n\ndecay_1_5 =[\"nan_Dominican Republic\",\"nan_Kazakhstan\",\"nan_Tunisia\",\n\"Alabama_US\", \"Alaska_US\",\"Arizona_US\",\"Colorado_US\",\"Florida_US\",\"Montana_US\",\n\"Nebraska_US\",\"Nevada_US\",\"New Hampshire_US\",\"New Mexico_US\",\n\"Puerto Rico_US\",\"nan_Ukraine\",\"nan_Uzbekistan\"] #\"nan_Philippines\",\"nan_Romania\"\n       \n\n\nlinear_last_12=[\"nan_Uganda\",\"nan_Zambia\"]\n\nstay_same=[ \"nan_Antigua and Barbuda\",\"nan_Diamond Princess\",\"nan_Saint Vincent and the Grenadines\",\"nan_Timor-Leste\",\"Montserrat_United Kingdom\"]\n\n#\"China\",\n\n\n\n\ntr_frame=train_frame\n\ntarget_confirmed_train=tr_frame[target_confirmed].values\nprint (\"  original shape of train is {}  \".format( target_confirmed_train.shape) )\n\ntarget_fatalities_train=tr_frame[target_fatalities].values\nfeatures_train=tr_frame[names].values   \n\nstandard_confirmed_train=tr_frame[\"ConfirmedCases\"].values\nstandard_fatalities_train=tr_frame[\"Fatalities\"].values\ncurrent_confirmed_train=tr_frame[\"ConfirmedCases\"].values\n\n     \n\nfeatures_cv=[]\nname_cv=[]\nstandard_confirmed_cv=[]\nstandard_fatalities_cv=[]\nnames_=tr_frame[\"key\"].values\ntraining_horizon=int(features_train.shape[0]/len(unique_keys)) \nprint(\"training horizon = \",training_horizon)\nfor dd in range(training_horizon-1,features_train.shape[0],training_horizon):\n    features_cv.append(features_train[dd])\n    name_cv.append(names_[dd])\n    standard_confirmed_cv.append(standard_confirmed_train[dd])\n    standard_fatalities_cv.append(standard_fatalities_train[dd])\n    print (name_cv[-1], standard_confirmed_cv[-1], standard_fatalities_cv[-1])\n    \n \n    \ncurrent_confirmed_train=[k for k in range(len(current_confirmed_train)) if current_confirmed_train[k]>0]\ntarget_confirmed_train=target_confirmed_train[current_confirmed_train]\ntarget_fatalities_train=target_fatalities_train[current_confirmed_train]        \nfeatures_train=features_train[current_confirmed_train]         \nstandard_confirmed_train=standard_confirmed_train[current_confirmed_train]\nstandard_fatalities_train=standard_fatalities_train[current_confirmed_train]  \n    \nfeatures_cv=np.array(features_cv)\npreds_confirmed_cv=np.zeros((features_cv.shape[0],horizon))\npreds_confirmed_standard_cv=np.zeros((features_cv.shape[0],horizon))\n\npreds_fatalities_cv=np.zeros((features_cv.shape[0],horizon))\npreds_fatalities_standard_cv=np.zeros((features_cv.shape[0],horizon))\n\noveral_rmsle_metric_confirmed=0.0\n\nfor j in range (preds_confirmed_cv.shape[1]):\n    this_target=target_confirmed_train[:,j]\n    index_positive=[k for k in range(len(this_target)) if this_target[k]!=-1]\n    this_features=features_train[index_positive]\n    this_target=this_target[index_positive]\n    this_weight=np.log(standard_confirmed_train[index_positive]+2.)\n    this_features_cv=features_cv                          \n\n    preds=bagged_set_train(this_features,this_target,this_weight, seed, 1,features_cv, xt=None,yt=None, output_name=None)#model_directory +\"confirmed\"+ str(j)\n    preds_confirmed_cv[:,j]=preds\n    print (\" modelling confirmed, case %d, original train %d, and after %d, original cv %d and after %d \"%(\n    j,target_confirmed_train.shape[0],this_target.shape[0],this_features_cv.shape[0],this_features_cv.shape[0])) \n\npredictions=[]\nfor ii in range (preds_confirmed_cv.shape[0]):\n    current_prediction=standard_confirmed_cv[ii]\n    if current_prediction==0 :\n        current_prediction=0.1    \n    this_preds=preds_confirmed_cv[ii].tolist()\n    name=name_cv[ii]\n    #overrides\n    if name in decay_4_first_10_then_1:\n        this_preds=decay_4_first_10_then_1_f(this_preds)\n        \n    elif name in decay_2:\n        this_preds=decay_2_f(this_preds)\n        \n    elif name in decay_2_last_12_linear_inter:\n        this_preds=decay_2_last_12_linear_inter_f(this_preds)\n        \n    elif name in decay_1_5:\n        this_preds=decay_1_5_f(this_preds)        \n        \n    elif name in linear_last_12:\n        this_preds=linear_last_12_f(this_preds)      \n        \n    elif name in stay_same or  \"China\" in name:\n        this_preds=stay_same_f(this_preds)      \n\n    for j in range (preds_confirmed_cv.shape[1]):\n                current_prediction*=max(1,this_preds[j])\n                preds_confirmed_standard_cv[ii][j]=current_prediction\n\n\n\n\nfor j in range (preds_confirmed_cv.shape[1]):\n    this_target=target_fatalities_train[:,j]\n    index_positive=[k for k in range(len(this_target)) if this_target[k]!=-1]\n    this_features=features_train[index_positive]\n    this_target=this_target[index_positive]\n    this_weight=np.log(standard_confirmed_train[index_positive]+2.)\n\n    this_features_cv=features_cv\n                             \n    preds=bagged_set_train(this_features,this_target,this_weight, seed, 1,features_cv, xt=None,yt=None, output_name=None)#model_directory +\"fatal\"+ str(j)\n    preds_fatalities_cv[:,j]=preds\n    print (\" modelling fatalities, case %d, original train %d, and after %d, original cv %d and after %d \"%(\n    j,target_confirmed_train.shape[0],this_target.shape[0],this_features_cv.shape[0],this_features_cv.shape[0])) \n\npredictions=[]\nfor ii in range (preds_fatalities_cv.shape[0]):\n    current_prediction=standard_fatalities_cv[ii]\n    if current_prediction==0 and standard_confirmed_cv[ii]>400:\n        current_prediction=0.1\n    this_preds=preds_fatalities_cv[ii].tolist()\n    name=name_cv[ii]\n    #overrides\n    if name in decay_4_first_10_then_1:\n        this_preds=decay_4_first_10_then_1_f(this_preds)\n        \n    elif name in decay_2:\n        this_preds=decay_2_f(this_preds)\n        \n    elif name in decay_2_last_12_linear_inter:\n        this_preds=decay_2_last_12_linear_inter_f(this_preds)\n        \n    elif name in decay_1_5:\n        this_preds=decay_1_5_f(this_preds)        \n        \n    elif name in linear_last_12:\n        this_preds=linear_last_12_f(this_preds)      \n        \n    elif name in stay_same or  \"China\" in name:\n        this_preds=stay_same_f(this_preds)         \n        \n    for j in range (preds_fatalities_cv.shape[1]):\n                if current_prediction==0 and  preds_confirmed_standard_cv[ii][j]>400:\n                    current_prediction=1.\n                current_prediction*=max(1,this_preds[j])\n                preds_fatalities_standard_cv[ii][j]=current_prediction\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"key_to_confirmed_rate={}\nkey_to_fatality_rate={}\nkey_to_confirmed={}\nkey_to_fatality={}\nprint(len(features_cv), len(name_cv),len(standard_confirmed_cv),len(standard_fatalities_cv)) \nprint(preds_confirmed_cv.shape,preds_confirmed_standard_cv.shape,preds_fatalities_cv.shape,preds_fatalities_standard_cv.shape) \n\nfor j in range (len(name_cv)):\n    \n    key_to_confirmed_rate[name_cv[j]]=preds_confirmed_cv[j,:].tolist()\n    #print(key_to_confirmed_rate[name_cv[j]])\n    key_to_fatality_rate[name_cv[j]]=preds_fatalities_cv[j,:].tolist()\n    key_to_confirmed[name_cv[j]]  =preds_confirmed_standard_cv[j,:].tolist()  \n    key_to_fatality[name_cv[j]]=preds_fatalities_standard_cv[j,:].tolist()  \n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_new=train[[\"Date\",\"ConfirmedCases\",\"Fatalities\",\"key\",\"rate_ConfirmedCases\",\"rate_Fatalities\"]]\n\ntest_new=pd.merge(test,train_new, how=\"left\", left_on=[\"key\",\"Date\"], right_on=[\"key\",\"Date\"] ).reset_index(drop=True)\ntest_new","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def fillin_columns(frame,key_column, original_name, training_horizon, test_horizon, unique_values, key_to_values):\n    keys=frame[key_column].values\n    original_values=frame[original_name].values.tolist()\n    print(len(keys), len(original_values), training_horizon ,test_horizon,len(key_to_values))\n    \n    for j in range(unique_values):\n        current_index=(j * (training_horizon +test_horizon )) +training_horizon \n        current_key=keys[current_index]\n        values=key_to_values[current_key]\n        co=0\n        for g in range(current_index, current_index + test_horizon):\n            original_values[g]=values[co]\n            co+=1\n    \n    frame[original_name]=original_values\n \n\nall_days=int(test_new.shape[0]/len(unique_keys))\n\ntr_horizon=all_days-horizon\nprint(all_days,tr_horizon, horizon )\n\nfillin_columns(test_new,\"key\", 'ConfirmedCases', tr_horizon, horizon, len(unique_keys), key_to_confirmed)    \nfillin_columns(test_new,\"key\", 'Fatalities', tr_horizon, horizon, len(unique_keys), key_to_fatality)   \nsubmission=test_new[[\"ForecastId\",\"ConfirmedCases\",\"Fatalities\"]]\n\nsubmission.to_csv( \"submission.csv\", index=False)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"submission","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":4}