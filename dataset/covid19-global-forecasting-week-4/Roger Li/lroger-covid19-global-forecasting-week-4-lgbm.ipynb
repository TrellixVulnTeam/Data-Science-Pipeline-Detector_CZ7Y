{"cells":[{"metadata":{},"cell_type":"markdown","source":"Reference: https://www.kaggle.com/osciiart/covid-19-lightgbm-no-leak/notebook"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","scrolled":true,"trusted":true},"cell_type":"code","source":"import os, gc, pickle, copy, datetime, warnings\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom xgboost import plot_importance, plot_tree\nimport pandas_profiling\nfrom sklearn.preprocessing import MinMaxScaler, RobustScaler\nfrom sklearn import metrics\npd.set_option('display.max_columns', 100)\npd.set_option('display.max_rows', 500)\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Loading"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"df_train = pd.read_csv(\"../input/covid19-global-forecasting-week-4/train.csv\")\nprint(df_train.shape)\nprint(df_train.Date.min(), df_train.Date.max())\ndf_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_min_date, train_max_date = df_train.Date.min(), df_train.Date.max()\ntrain_min_dayofyear, train_max_dayofyear = (pd.to_datetime(train_min_date)).dayofyear, (pd.to_datetime(train_max_date)).dayofyear\nprint(train_min_dayofyear, train_max_dayofyear)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"train_valid_cutoff_dayofyear = train_min_dayofyear + ( train_max_dayofyear - train_min_dayofyear ) // 3 * 2\ntrain_valid_cutoff_dayofyear","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"df_test = pd.read_csv(\"../input/covid19-global-forecasting-week-4/test.csv\")\nprint(df_test.shape)\ntest_min_date, test_max_date = df_test.Date.min(), df_test.Date.max()\nprint(test_min_date, test_max_date)\ndf_test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preprocessing"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# concat train and test\ndf_traintest = pd.concat([df_train, df_test])\nprint(df_train.shape, df_test.shape, df_traintest.shape)\ndf_traintest.head()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# concat Country/Region and Province/State\ndef concat_country_province(x):\n    try:\n        x_new = x['Country_Region'] + \"/\" + x['Province_State']\n    except:\n        x_new = x['Country_Region']\n    return x_new\n        \ndf_traintest['place_id'] = df_traintest.apply(lambda x: concat_country_province(x), axis=1)\ntmp = np.sort(df_traintest['place_id'].unique())\nprint(\"num unique places: {}\".format(len(tmp)))\nprint(tmp[:10])","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# process date\n# df_traintest['Date'] = pd.to_datetime(df_traintest['Date'])\n# df_traintest['day'] = df_traintest['Date'].apply(lambda x: x.dayofyear).astype(np.int16)\n# df_traintest['dayofmonth'] = df_traintest['Date'].apply(lambda x: x.day).astype(np.int16)\n# df_traintest['dayofweek'] = df_traintest['Date'].apply(lambda x: x.dayofweek).astype(np.int16)\n# df_traintest.head()\n\n#     # time features\ndf_traintest['Date'] = pd.to_datetime(df_traintest['Date'])\ntime_cols = [\n#     \"year\", \"quarter\", \n    \"month\", \n    \"week\", \n    \"day\", \n    \"dayofyear\", \n    \"dayofweek\", \n#     \"is_year_end\", \"is_year_start\", \"is_quarter_end\", \"is_quarter_start\", \n#     \"is_month_end\",\"is_month_start\",\n]\n\nfor attr in time_cols:\n    dtype = np.int if attr == \"year\" else np.int8\n#     df_traintest[attr] = getattr(df_traintest['Date'].dt, attr).astype(dtype)\n    df_traintest[attr] = getattr(df_traintest['Date'].dt, attr)\n# df_traintest[\"is_weekend\"] = df_traintest[\"dayofweek\"].isin([5, 6]).astype(np.int8)\n# time_cols += [\"is_weekend\"]\nprint(time_cols)\ndf_traintest.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"day_before_valid = train_valid_cutoff_dayofyear\nday_before_public = 92 #2020-04-01\nday_before_private = df_traintest['dayofyear'][pd.isna(df_traintest['ForecastId'])].max() # last day of train\nprint(df_traintest['Date'][df_traintest['dayofyear']==day_before_valid].values[0])\nprint(df_traintest['Date'][df_traintest['dayofyear']==day_before_public].values[0])\nprint(df_traintest['Date'][df_traintest['dayofyear']==day_before_private].values[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"( df_traintest[pd.isna(df_traintest['ForecastId'])].groupby('place_id')['Date'].max() ).min()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# calc cases and fatalities per day\ndf_traintest['cases/day'] = 0\ndf_traintest['fatal/day'] = 0\nplaces = np.sort(df_traintest['place_id'].unique())\nfor place in places:\n    tmp = df_traintest['ConfirmedCases'][df_traintest['place_id']==place].values\n    tmp[1:] -= tmp[:-1]\n    df_traintest['cases/day'][df_traintest['place_id']==place] = tmp\n    tmp = df_traintest['Fatalities'][df_traintest['place_id']==place].values\n    tmp[1:] -= tmp[:-1]\n    df_traintest['fatal/day'][df_traintest['place_id']==place] = tmp\n    \ndf_traintest[df_traintest['place_id']=='China/Hubei']","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# aggregate cases and fatalities\ndef do_aggregation(df, col, mean_range, method='mean', val_cols=[]):\n    df_new = copy.deepcopy(df)\n    col_new = '{}_{}_({}-{})'.format(col, method, mean_range[0], mean_range[1])\n    val_cols.append(col_new)\n    df_new[col_new] = 0\n    if method=='mean':\n        tmp = df_new[col].rolling(mean_range[1]-mean_range[0]+1).mean()\n    elif method=='std':\n        tmp = df_new[col].rolling(mean_range[1]-mean_range[0]+1).std()\n    df_new[col_new][mean_range[0]:] = tmp[:-(mean_range[0])]\n    df_new[col_new][pd.isna(df_new[col_new])] = 0\n    return df_new[[col_new]].reset_index(drop=True)\n\n# def do_aggregations(df):\n#     for method in ['mean']:\n#         df = pd.concat([df, do_aggregation(df, 'cases/day', [1,1], method).reset_index(drop=True)], axis=1)\n#         df = pd.concat([df, do_aggregation(df, 'cases/day', [1,7], method).reset_index(drop=True)], axis=1)\n#         df = pd.concat([df, do_aggregation(df, 'cases/day', [8,14], method).reset_index(drop=True)], axis=1)\n#         df = pd.concat([df, do_aggregation(df, 'fatal/day', [1,1], method).reset_index(drop=True)], axis=1)\n#         df = pd.concat([df, do_aggregation(df, 'fatal/day', [1,7], method).reset_index(drop=True)], axis=1)\n#         df = pd.concat([df, do_aggregation(df, 'fatal/day', [8,14], method).reset_index(drop=True)], axis=1)\n#     return df\n\ndef do_aggregations(df, roll_ranges=[[1,1], [1,7], [8,14]], val_cols=[]):\n    for method in ['mean']:\n        for roll_range in roll_ranges:\n            df = pd.concat([df, do_aggregation(df, 'cases/day', roll_range, method, val_cols).reset_index(drop=True)], axis=1)\n            df = pd.concat([df, do_aggregation(df, 'fatal/day', roll_range, method, val_cols).reset_index(drop=True)], axis=1)\n            \n    for threshold in [1, 10, 100]:\n        days_under_threshold = (df['ConfirmedCases']<threshold).sum()\n        tmp = df['dayofyear'].values - 22 - days_under_threshold\n        tmp[tmp<=0] = 0\n        df['days_since_{}cases'.format(threshold)] = tmp\n        val_cols.append('days_since_{}cases'.format(threshold))\n            \n    for threshold in [1, 10, 100]:\n        days_under_threshold = (df['Fatalities']<threshold).sum()\n        tmp = df['dayofyear'].values - 22 - days_under_threshold\n        tmp[tmp<=0] = 0\n        df['days_since_{}fatal'.format(threshold)] = tmp\n        val_cols.append('days_since_{}fatal'.format(threshold))\n    \n    # process China/Hubei\n    if df['place_id'][0]=='China/Hubei':\n        df['days_since_1cases'] += 35 # 2019/12/8\n        df['days_since_10cases'] += 35-13 # 2019/12/8-2020/1/2 assume 2019/12/8+13\n        df['days_since_100cases'] += 4 # 2020/1/18\n        df['days_since_1fatal'] += 13 # 2020/1/9\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_traintest[df_traintest['dayofyear']<0]","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"df_traintest2 = []\nval_cols = []\nroll_ranges = [[i,i] for i in range(1,15)]\nroll_ranges += [[1,7], [8,14], [15,21]]\n\nfor place in places[:]:\n    df_tmp = df_traintest[df_traintest['place_id']==place].reset_index(drop=True)\n    df_tmp = do_aggregations(df_tmp, roll_ranges=roll_ranges, val_cols=val_cols)\n    df_traintest2.append(df_tmp)\ndf_traintest2 = pd.concat(df_traintest2).reset_index(drop=True)\n\nval_cols = list(set(val_cols))\nprint(val_cols)\ndf_traintest2[df_traintest2['place_id']=='China/Hubei'].head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"roll_ranges","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Adding Smoking Rate Data"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# add Smoking rate per country\n# data of smoking rate is obtained from https://ourworldindata.org/smoking\ndf_smoking = pd.read_csv(\"../input/shareofadultswhosmoke/adults-smoking-2000-2016.csv\")\nprint(np.sort(df_smoking['Entity'].unique())[:10])\ndf_smoking.head()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# extract newest data\ndf_smoking_recent = df_smoking.sort_values('Year', ascending=False).reset_index(drop=True)\ndf_smoking_recent = df_smoking_recent[df_smoking_recent['Entity'].duplicated()==False]\ndf_smoking_recent['Country/Region'] = df_smoking_recent['Entity']\ndf_smoking_recent['SmokingRate'] = df_smoking_recent['Share of adults who smoke (%)']\n\ndf_smoking_recent[\"Country/Region\"] = df_smoking_recent[\"Country/Region\"].str.replace(\"South Korea\", \"Korea, South\")\ndf_smoking_recent[\"Country/Region\"] = df_smoking_recent[\"Country/Region\"].str.replace(\"United States\", \"US\")\n\ndf_smoking_recent.head()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# merge\ndf_traintest3 = pd.merge(df_traintest2, df_smoking_recent[['Country/Region', 'SmokingRate']], left_on='Country_Region', right_on='Country/Region', how='left')\ndf_traintest3.drop('Country/Region', axis=1, inplace=True)\ndf_traintest3.head()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"## fill na with world smoking rate\nSmokingRate = df_smoking_recent['SmokingRate'][df_smoking_recent['Entity']=='World'].values[0]\nprint(\"Smoking rate of the world: {:.6f}\".format(SmokingRate))\ndf_traintest3['SmokingRate'][pd.isna(df_traintest3['SmokingRate'])] = SmokingRate\ndf_traintest3.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Add World Bank Dataset"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"world_happiness_index = pd.read_csv(\"../input/world-bank-datasets/World_Happiness_Index.csv\")\nworld_happiness_grouped = world_happiness_index.groupby('Country name').nth(-1)\nworld_happiness_grouped.drop(\"Year\", axis=1, inplace=True)\n\nworld_happiness_grouped.dropna(axis=1, how='all', inplace=True)\n\nprint(world_happiness_grouped.shape)\n\nworld_happiness_grouped.index = world_happiness_grouped.index.str.replace(\"Taiwan Province of China\", \"Taiwan*\")\nworld_happiness_grouped.index = world_happiness_grouped.index.str.replace(\"United States\", \"US\")\nworld_happiness_grouped.index = world_happiness_grouped.index.str.replace(\"South Korea\", \"Korea, South\")\nworld_happiness_grouped.index = world_happiness_grouped.index.str.replace(\"Ivory Coast\", \"Cote d'Ivoire\")\n\ndf_traintest3 = pd.merge(left=df_traintest3, right=world_happiness_grouped, how='left', left_on='Country_Region', right_on='Country name')","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"wh_cols = world_happiness_grouped.columns.to_list()\nprint(wh_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for wh_col in wh_cols:\n    df_traintest3[wh_col][pd.isna(df_traintest3[wh_col])] = world_happiness_grouped[wh_col].mean()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"malaria_world_health = pd.read_csv(\"../input/world-bank-datasets/Malaria_World_Health_Organization.csv\")\n\ndf_traintest3 = pd.merge(left=df_traintest3, right=malaria_world_health, how='left', left_on='Country_Region', right_on='Country')\ndf_traintest3.drop(\"Country\", axis=1, inplace=True)\n\nmwh_cols = [ col for col in malaria_world_health.columns.to_list() if col != \"Country\" ]\nprint(mwh_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_traintest3[['Country_Region','Estimated number of malaria cases']][pd.isna(df_traintest3['Estimated number of malaria cases'])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_traintest3[['Estimated number of malaria cases']].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"human_development_index = pd.read_csv(\"../input/world-bank-datasets/Human_Development_Index.csv\")\nhuman_development_index.drop([\"Gross national income (GNI) per capita 2018\"], axis=1, inplace=True)\n\nhuman_development_index['Country'] = human_development_index['Country'].str.replace(\"South Korea\", \"Korea, South\")\nhuman_development_index['Country'] = human_development_index['Country'].str.replace(\"United States\", \"US\")\n\ndf_traintest3 = pd.merge(left=df_traintest3, right=human_development_index, how='left', left_on='Country_Region', right_on='Country')\ndf_traintest3.drop(\"Country\", axis=1, inplace=True)\n\nhdi_cols = [ col for col in human_development_index.columns.to_list() if col != \"Country\" ]\nprint(hdi_cols)\n\nfor hdi_col in hdi_cols:\n    df_traintest3[hdi_col][pd.isna(df_traintest3[hdi_col])] = human_development_index[hdi_col].mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Add Country Info Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# add additional info from countryinfo dataset\ndf_country = pd.read_csv(\"../input/countryinfo/covid19countryinfo.csv\", thousands=',')\ndf_country = df_country[df_country['country'].duplicated()==False]\nprint(df_country.shape)\ndf_country.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"country_info_cols = ['density', 'pop', 'fertility']\nprint(country_info_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_country[country_info_cols].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_traintest3 = pd.merge(left=df_traintest3, \n                         right=df_country[['country']+country_info_cols], \n                         left_on=['Country_Region'], right_on=['country'], how='left')\ndf_traintest3.drop('country', axis=1, inplace=True)\n\ndf_traintest3['density'][df_traintest3['place_id']==\"South Sudan\"] = 18\ndf_traintest3['density'][df_traintest3['place_id']==\"Angola\"] = 14.8\ndf_traintest3['density'][df_traintest3['place_id']==\"Botswana\"] = 3\ndf_traintest3['density'][df_traintest3['place_id']==\"Burma\"] = 83\ndf_traintest3['density'][df_traintest3['place_id']==\"Burundi\"] = 463\ndf_traintest3['density'][df_traintest3['place_id']==\"Malawi\"] = 129\ndf_traintest3['density'][df_traintest3['place_id']==\"Papua New Guinea\"] = 17.8\ndf_traintest3['density'][df_traintest3['place_id']==\"Sao Tome and Principe\"] = 228\ndf_traintest3['density'][df_traintest3['place_id']==\"Sierra Leone\"] = 105\ndf_traintest3['density'][df_traintest3['place_id']==\"West Bank and Gaza\"] = 758.98\ndf_traintest3['density'][df_traintest3['place_id']==\"Western Sahara\"] = 2\ndf_traintest3['density'][df_traintest3['place_id']==\"MS Zaandam\"] = 1432.0+615.0\n\ndf_traintest3['pop'][df_traintest3['place_id']==\"South Sudan\"] = 10.98e6\ndf_traintest3['pop'][df_traintest3['place_id']==\"Angola\"] = 30.81e6\ndf_traintest3['pop'][df_traintest3['place_id']==\"Botswana\"] = 2.254e6\ndf_traintest3['pop'][df_traintest3['place_id']==\"Burma\"] = 53.71e6\ndf_traintest3['pop'][df_traintest3['place_id']==\"Burundi\"] = 11.18e6\ndf_traintest3['pop'][df_traintest3['place_id']==\"Malawi\"] = 18.14e6\ndf_traintest3['pop'][df_traintest3['place_id']==\"Papua New Guinea\"] = 8.606e6\ndf_traintest3['pop'][df_traintest3['place_id']==\"Sao Tome and Principe\"] = 211028.0\ndf_traintest3['pop'][df_traintest3['place_id']==\"Sierra Leone\"] = 7.65e6\ndf_traintest3['pop'][df_traintest3['place_id']==\"West Bank and Gaza\"] = 4.569e6\ndf_traintest3['pop'][df_traintest3['place_id']==\"Western Sahara\"] = 567402.0\ndf_traintest3['pop'][df_traintest3['place_id']==\"MS Zaandam\"] = 1432.0+615.0\n\ndf_traintest3['fertility'][df_traintest3['place_id']==\"South Sudan\"] = 4.78\ndf_traintest3['fertility'][df_traintest3['place_id']==\"Angola\"] = 5.60\ndf_traintest3['fertility'][df_traintest3['place_id']==\"Botswana\"] = 2.91\ndf_traintest3['fertility'][df_traintest3['place_id']==\"Burma\"] = 2.17\ndf_traintest3['fertility'][df_traintest3['place_id']==\"Burundi\"] = 5.50\ndf_traintest3['fertility'][df_traintest3['place_id']==\"Malawi\"] = 4.30\ndf_traintest3['fertility'][df_traintest3['place_id']==\"Papua New Guinea\"] = 3.61\ndf_traintest3['fertility'][df_traintest3['place_id']==\"Sao Tome and Principe\"] = 4.37\ndf_traintest3['fertility'][df_traintest3['place_id']==\"Sierra Leone\"] = 4.36\ndf_traintest3['fertility'][df_traintest3['place_id']==\"West Bank and Gaza\"] = 3.74\ndf_traintest3['fertility'][df_traintest3['place_id']==\"Western Sahara\"] = 3.79\n# df_traintest3['fertility'][df_traintest3['place_id']==\"MS Zaandam\"] = 0.0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_traintest3['place_id'][pd.isna(df_traintest3['density'])].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_traintest3['place_id'][pd.isna(df_traintest3['pop'])].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_traintest3['place_id'][pd.isna(df_traintest3['fertility'])].unique()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# df_lat_long = pd.concat( [ pd.read_csv(\"../input/covid19-global-forecasting-week-1/train.csv\"), pd.read_csv(\"../input/covid19-global-forecasting-week-4/test.csv\") ] )\n# df_lat_long = df_lat_long[['Country/Region', 'Province/State', 'Lat', 'Long']].drop_duplicates()\n# df_lat_long = df_lat_long.rename(columns={'Country/Region': 'Country_Region', 'Province/State': 'Province_State'})\n# df_lat_long['place_id'] = df_lat_long.apply(lambda x: concat_country_province(x), axis=1)\n# df_lat_long.drop([\"Country_Region\", 'Province_State'], axis=1, inplace=True)\n\n# df_traintest3 = pd.merge(left=df_traintest3, right=df_lat_long, how='left', on='place_id')","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# df_lat_long = pd.concat( [ pd.read_csv(\"../input/covid19-global-forecasting-week-1/train.csv\"), pd.read_csv(\"../input/covid19-global-forecasting-week-4/test.csv\") ] )\n# df_lat_long = df_lat_long[['Country/Region', 'Province/State', 'Lat', 'Long']].drop_duplicates()\n# df_lat_long = df_lat_long.rename(columns={'Country/Region': 'Country_Region', 'Province/State': 'Province_State'})\n# df_lat_long.to_csv(\"lat_long.csv\", index=None)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# df_lat_long = pd.read_csv(\"../input/lat-long/lat_long.csv\")\n# df_lat_long['place_id'] = df_lat_long.apply(lambda x: concat_country_province(x), axis=1)\n# df_lat_long.drop([\"Country_Region\", 'Province_State'], axis=1, inplace=True)\n\n# df_traintest3 = pd.merge(left=df_traintest3, right=df_lat_long, how='left', on='place_id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df_lat_long.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# tmp = df_lat_long['place_id'].unique()\n# print(\"num unique places: {}\".format(len(tmp)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_lat_long2 = pd.read_csv(\"../input/coronavirus-2019ncov/covid-19-all.csv\")\ndf_lat_long2 = df_lat_long2[[\"Country/Region\", \"Province/State\", \"Latitude\", \"Longitude\"]].drop_duplicates()\ndf_lat_long2 = df_lat_long2.rename(columns = {\"Country/Region\":\"Country_Region\", \n                                              \"Province/State\":\"Province_State\", \n                                              \"Latitude\":\"Lat\", \n                                              \"Longitude\":\"Long\"})\n\ndf_lat_long2['place_id'] = df_lat_long2.apply(lambda x: concat_country_province(x), axis=1)\ndf_lat_long2.drop([\"Country_Region\", 'Province_State'], axis=1, inplace=True)\n\ndf_lat_long2[\"place_id\"] = df_lat_long2[\"place_id\"].str.replace(\"Ivory Coast\", \"Cote d'Ivoire\")\ndf_lat_long2[\"place_id\"] = df_lat_long2[\"place_id\"].str.replace(\"South Korea\", \"Korea, South\")\ndf_lat_long2[\"place_id\"] = df_lat_long2[\"place_id\"].str.replace(\"Taiwan\", \"Taiwan*\")\ndf_lat_long2[\"place_id\"] = df_lat_long2[\"place_id\"].str.replace(\"Vatican City\", \"Holy See\")\n\ndf_lat_long2 = pd.concat([df_lat_long2,\n          pd.DataFrame({\"place_id\": ['Czechia', 'Dominica', 'Niger'],\n             \"Lat\": [49.8175, 15.4150, 17.6078],\n             \"Long\": [15.4730, 61.3710, 8.0817]})])\n\ndf_lat_long2 = df_lat_long2[(pd.isna(df_lat_long2[\"Lat\"])==False) & (pd.isna(df_lat_long2[\"Long\"])==False)]\n# df_lat_long2 = df_lat_long2.groupby([\"Country_Region\", \"Province_State\"]).mean().reset_index()\ndf_lat_long2 = df_lat_long2.groupby([\"place_id\"]).mean().reset_index()\nprint(df_lat_long2.shape)\ndf_lat_long2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_traintest3 = pd.merge(left=df_traintest3, right=df_lat_long2, how='left', on=[\"place_id\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"assert(len(df_traintest3[(pd.isna(df_traintest3[\"Long\"])) | (pd.isna(df_traintest3[\"Lat\"]))])==0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def encode_label(df, col, freq_limit=0):\n    df[col][pd.isna(df[col])] = 'nan'\n    tmp = df[col].value_counts()\n    cols = tmp.index.values\n    freq = tmp.values\n    num_cols = (freq>=freq_limit).sum()\n    print(\"col: {}, num_cat: {}, num_reduced: {}\".format(col, len(cols), num_cols))\n\n    col_new = '{}_le'.format(col)\n    df_new = pd.DataFrame(np.ones(len(df), np.int16)*(num_cols-1), columns=[col_new])\n    for i, item in enumerate(cols[:num_cols]):\n        df_new[col_new][df[col]==item] = i\n\n    return df_new\n\ndef get_df_le(df, col_index, col_cat):\n    df_new = df[[col_index]]\n    for col in col_cat:\n        df_tmp = encode_label(df, col)\n        df_new = pd.concat([df_new, df_tmp], axis=1)\n    return df_new\n\ndf_traintest3['id_le'] = np.arange(len(df_traintest3))\ndf_le = get_df_le(df_traintest3, 'id_le', ['Country_Region', 'Province_State'])\ndf_traintest3 = pd.merge(df_traintest3, df_le, on='id_le', how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"le_cols = [\"Country_Region_le\", \"Province_State_le\"]\nprint(le_cols)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# df_tmp = pd.get_dummies(df_traintest3['Province_State'], prefix='ps')\n# ps_cols = df_tmp.columns.to_list()\n# print(ps_cols)\n# df_traintest3 = pd.concat([df_traintest3,df_tmp],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# df_tmp = pd.get_dummies(df_traintest3['Country_Region'], prefix='cr')\n# cr_cols = df_tmp.columns.to_list()\n# print(cr_cols)\n# df_traintest3 = pd.concat([df_traintest3,df_tmp],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_wk3 = pd.read_csv(\"../input/covid19-country-data-wk3-release/Data Join - RELEASE.csv\")\ndf_wk3['place_id'] = df_wk3.apply(lambda x: concat_country_province(x), axis=1)\ndf_wk3['Personality_uai'][df_wk3['Personality_uai']=='#NULL!'] = np.nan\ndf_wk3['Personality_uai'] = df_wk3['Personality_uai'].astype(np.float64)\nprint(df_wk3.shape)\ndf_wk3.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_wk3[['Personality_uai']].info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_wk3['Personality_uai'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wk3_cols = ['Personality_uai']\nprint(wk3_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_wk3[wk3_cols].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_traintest3 = pd.merge(left=df_traintest3, right=df_wk3[['place_id']+wk3_cols], how='left', on=[\"place_id\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_traintest3[\"place_id\"][pd.isna(df_traintest3['Personality_uai'])].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_traintest3[['Personality_uai']].info()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"df_traintest3[df_traintest3['place_id']=='China/Hubei']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"def calc_score(y_true, y_pred):\n    y_true[y_true<0] = 0\n    score = metrics.mean_squared_error(np.log(y_true.clip(0, 1e10)+1), np.log(y_pred[:]+1))**0.5\n    return score","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# params\nSEED = 42\nparams = {'num_leaves': 8,\n          'min_data_in_leaf': 5,  # 42,\n          'objective': 'regression',\n          'max_depth': 8,\n          'learning_rate': 0.02,\n          'boosting': 'gbdt',\n          'bagging_freq': 5,  # 5\n          'bagging_fraction': 0.8,  # 0.5,\n          'feature_fraction': 0.8201,\n          'bagging_seed': SEED,\n          'reg_alpha': 1,  # 1.728910519108444,\n          'reg_lambda': 4.9847051755586085,\n          'random_state': SEED,\n          'metric': 'mse',\n          'verbosity': 100,\n          'min_gain_to_split': 0.02,  # 0.01077313523861969,\n          'min_child_weight': 5,  # 19.428902804238373,\n          'num_threads': 6,\n          }","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"df_traintest3.info()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# train model to predict fatalities/day\ncol_target = 'fatal/day'\ncol_var = [\n    'Lat', 'Long',\n#    'cases/day_mean_(1-1)', 'cases/day_mean_(1-7)', 'cases/day_mean_(8-14)', \n#      'fatal/day_mean_(1-1)', 'fatal/day_mean_(1-7)', 'fatal/day_mean_(8-14)',\n#    'cases/day_std_(1-1)', 'cases/day_std_(1-7)', 'cases/day_std_(8-14)', \n#      'fatal/day_std_(1-1)', 'fatal/day_std_(1-7)', 'fatal/day_std_(8-14)',\n    'SmokingRate',\n#     'dayofyear',\n#     'day',\n#     'dayofweek',\n]\ncol_var += val_cols\ncol_var += time_cols\n# extra_cols = wh_cols + mwh_cols + hdi_cols + ps_cols + cr_cols\nextra_cols = wh_cols + mwh_cols + hdi_cols\ncol_var += extra_cols\ncol_var += country_info_cols\ncol_var += le_cols\ncol_var += wk3_cols\n\n# df_train = df_traintest3[(pd.isna(df_traintest3['ForecastId'])) & (df_traintest3['dayofyear']<train_valid_cutoff_dayofyear)]\n# df_valid = df_traintest3[(pd.isna(df_traintest3['ForecastId'])) & (df_traintest3['dayofyear']>=train_valid_cutoff_dayofyear)]\ndf_train = df_traintest3[(pd.isna(df_traintest3['ForecastId'])) & (df_traintest3['dayofyear']<=day_before_valid)]\ndf_valid = df_traintest3[(pd.isna(df_traintest3['ForecastId'])) & (day_before_valid<df_traintest3['dayofyear']) & (df_traintest3['dayofyear']<=day_before_public)]\n\ndf_test = df_traintest3[pd.isna(df_traintest3['ForecastId'])==False]\nX_train = df_train[col_var].values\nX_valid = df_valid[col_var].values\nprint(len(X_train), len(X_valid))\n\n# scaler = MinMaxScaler()\n# scaler = RobustScaler()\n# X_train = scaler.fit_transform(X_train)\n# X_valid = scaler.transform(X_valid)\n\n# y_train = df_train[col_target].values\n# y_valid = df_valid[col_target].values\ny_train = np.log(df_train[col_target].values.clip(0, 1e10)+1)\ny_valid = np.log(df_valid[col_target].values.clip(0, 1e10)+1)\n\ntrain_data = lgb.Dataset(X_train, label=y_train)\nvalid_data = lgb.Dataset(X_valid, label=y_valid)\nnum_round = 15000\nmodel = lgb.train(params, train_data, num_round, valid_sets=[train_data, valid_data],\n                  verbose_eval=100,\n                  early_stopping_rounds=150,)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_true = df_valid['fatal/day'].values\ny_pred = np.exp(model.predict(X_valid))-1\nscore = calc_score(y_true, y_pred)\nprint(\"{:.6f}\".format(score))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_features = pd.merge( left=(pd.DataFrame(model.feature_importance(), index=col_var, columns=[\"importance\"])).sort_values('importance', ascending=False),\n                      right=df_train[col_var].isnull().sum().to_frame(name='count_null'),\n                      how='left', left_index=True, right_index=True)\ndf_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"important_features = df_features.index[df_features['importance']>=10].to_list()\nprint(len(important_features))\nimportant_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df_train_profile = df_train[col_var].profile_report(title='Pandas Profile Report:Train Data')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df_train_profile","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# rejected_var = df_train_profile.get_rejected_variables()\n# rejected_var","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"col_var = important_features\n\nX_train = df_train[col_var].values\nX_valid = df_valid[col_var].values\n\n# scaler = MinMaxScaler()\n# scaler = RobustScaler()\n# X_train = scaler.fit_transform(X_train)\n# X_valid = scaler.transform(X_valid)\n\n# y_train = df_train[col_target].values\n# y_valid = df_valid[col_target].values\ny_train = np.log(df_train[col_target].values.clip(0, 1e10)+1)\ny_valid = np.log(df_valid[col_target].values.clip(0, 1e10)+1)\n\ntrain_data = lgb.Dataset(X_train, label=y_train)\nvalid_data = lgb.Dataset(X_valid, label=y_valid)\nnum_round = 15000\nmodel = lgb.train(params, train_data, num_round, valid_sets=[train_data, valid_data],\n                  verbose_eval=100,\n                  early_stopping_rounds=150,)\nbest_itr = model.best_iteration","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_true = df_valid['fatal/day'].values\ny_pred = np.exp(model.predict(X_valid))-1\nscore = calc_score(y_true, y_pred)\nprint(\"{:.6f}\".format(score))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train with all data before public\ndf_train = df_traintest3[(pd.isna(df_traintest3['ForecastId'])) & (df_traintest3['dayofyear']<=day_before_public)]\ndf_valid = df_traintest3[(pd.isna(df_traintest3['ForecastId'])) & (df_traintest3['dayofyear']<=day_before_public)]\ndf_test = df_traintest3[pd.isna(df_traintest3['ForecastId'])==False]\nX_train = df_train[col_var].values\nX_valid = df_valid[col_var].values\ny_train = np.log(df_train[col_target].values.clip(0, 1e10)+1)\ny_valid = np.log(df_valid[col_target].values.clip(0, 1e10)+1)\ntrain_data = lgb.Dataset(X_train, label=y_train)\nvalid_data = lgb.Dataset(X_valid, label=y_valid)\nmodel_pub = lgb.train(params, train_data, best_itr, valid_sets=[train_data, valid_data],\n                  verbose_eval=100,\n                  early_stopping_rounds=150,)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from lightgbm import LGBMRegressor\n# lgb_reg = LGBMRegressor(random_state=17)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# %%time\n# lgb_reg.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from sklearn.metrics import mean_squared_error\n\n# mean_squared_error(y_valid, lgb_reg.predict(X_valid))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**1 stage of hyper-param tuning: tuning model complexity**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# param_grid = {'num_leaves': [7, 15, 31, 63], \n#               'max_depth': [3, 4, 5, 6, -1]}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from sklearn.model_selection import train_test_split, GridSearchCV\n\n# grid_searcher = GridSearchCV(estimator=lgb_reg, param_grid=param_grid, \n#                              cv=5, verbose=1, n_jobs=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# grid_searcher.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# grid_searcher.best_params_, grid_searcher.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# mean_squared_error(y_valid, grid_searcher.predict(X_valid))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**2 stage of hyper-param tuning: convergence**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# num_iterations = 500\n# lgb_reg2 = LGBMRegressor(random_state=17, max_depth=3, \n#                           num_leaves=7, n_estimators=num_iterations,\n#                           n_jobs=1)\n\n# param_grid2 = {'learning_rate': np.logspace(-3, 0, 10)}\n# grid_searcher2 = GridSearchCV(estimator=lgb_reg2, param_grid=param_grid2,\n#                                cv=5, verbose=1, n_jobs=4)\n# grid_searcher2.fit(X_train, y_train)\n# print(grid_searcher2.best_params_, grid_searcher2.best_score_)\n# print(mean_squared_error(y_valid, grid_searcher2.predict(X_valid)))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# model = xgb.XGBRegressor(n_estimators=1000)\n# eval_set = [(df_valid[col_var], df_valid[col_target])]\n# model.fit(df_train[col_var], df_train[col_target], eval_metric=\"rmse\", eval_set=eval_set, verbose=True)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# 19.30146**2","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# plot = plot_importance(model, height=0.9, max_num_features=20)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# train model to predict cases/day\ncol_target2 = 'cases/day'\ncol_var2 = [\n    'Lat', 'Long',\n#    'cases/day_mean_(1-1)', 'cases/day_mean_(1-7)', 'cases/day_mean_(8-14)', \n#      'fatal/day_mean_(1-1)', 'fatal/day_mean_(1-7)', 'fatal/day_mean_(8-14)',\n#    'cases/day_std_(1-1)', 'cases/day_std_(1-7)', 'cases/day_std_(8-14)', \n#      'fatal/day_std_(1-1)', 'fatal/day_std_(1-7)', 'fatal/day_std_(8-14)',\n    'SmokingRate',\n#     'day',\n#     'dayofmonth',\n#     'dayofweek'\n]\ncol_var2 += val_cols\ncol_var2 += time_cols\n# col_var2 += ps_cols\n# col_var2 += cr_cols\ncol_var2 += extra_cols\ncol_var2 += country_info_cols\ncol_var2 += le_cols\ncol_var2 += wk3_cols\n\ndf_train = df_traintest3[(pd.isna(df_traintest3['ForecastId'])) & (df_traintest3['dayofyear']<=day_before_valid)]\ndf_valid = df_traintest3[(pd.isna(df_traintest3['ForecastId'])) & (day_before_valid<df_traintest3['dayofyear']) & (df_traintest3['dayofyear']<=day_before_public)]\n\nX_train = df_train[col_var2].values\nX_valid = df_valid[col_var2].values\nprint(len(X_train), len(X_valid))\n\n# scaler = MinMaxScaler()\n# scaler = RobustScaler()\n# X_train = scaler.fit_transform(X_train)\n# X_valid = scaler.transform(X_valid)\n\n# y_train = df_train[col_target2].values\n# y_valid = df_valid[col_target2].values\ny_train = np.log(df_train[col_target2].values.clip(0, 1e10)+1)\ny_valid = np.log(df_valid[col_target2].values.clip(0, 1e10)+1)\n\ntrain_data = lgb.Dataset(X_train, label=y_train)\nvalid_data = lgb.Dataset(X_valid, label=y_valid)\nnum_round = 15000\nmodel2 = lgb.train(params, train_data, num_round, valid_sets=[train_data, valid_data],\n                  verbose_eval=100,\n                  early_stopping_rounds=150,)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_true = df_valid['cases/day'].values\ny_pred = np.exp(model2.predict(X_valid))-1\nscore = calc_score(y_true, y_pred)\nprint(\"{:.6f}\".format(score))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_features2 = pd.merge( left=(pd.DataFrame(model2.feature_importance(), index=col_var2, columns=[\"importance\"])).sort_values('importance', ascending=False),\n                      right=df_train[col_var2].isnull().sum().to_frame(name='count_null'),\n                      how='left', left_index=True, right_index=True)\ndf_features2","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"important_features2 = df_features2.index[df_features2['importance']>=18].to_list()\nprint(len(important_features2))\nimportant_features2","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"col_var2 = important_features2\n\nX_train = df_train[col_var2].values\nX_valid = df_valid[col_var2].values\n\n# scaler = MinMaxScaler()\n# scaler = RobustScaler()\n# X_train = scaler.fit_transform(X_train)\n# X_valid = scaler.transform(X_valid)\n\n# y_train = df_train[col_target2].values\n# y_valid = df_valid[col_target2].values\ny_train = np.log(df_train[col_target2].values.clip(0, 1e10)+1)\ny_valid = np.log(df_valid[col_target2].values.clip(0, 1e10)+1)\n\ntrain_data = lgb.Dataset(X_train, label=y_train)\nvalid_data = lgb.Dataset(X_valid, label=y_valid)\nnum_round = 15000\nmodel2 = lgb.train(params, train_data, num_round, valid_sets=[train_data, valid_data],\n                  verbose_eval=100,\n                  early_stopping_rounds=150,)\nbest_itr2 = model2.best_iteration","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_true = df_valid['cases/day'].values\ny_pred = np.exp(model2.predict(X_valid))-1\nscore = calc_score(y_true, y_pred)\nprint(\"{:.6f}\".format(score))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = df_traintest3[(pd.isna(df_traintest3['ForecastId'])) & (df_traintest3['dayofyear']<=day_before_public)]\ndf_valid = df_traintest3[(pd.isna(df_traintest3['ForecastId'])) & (df_traintest3['dayofyear']<=day_before_public)]\nX_train = df_train[col_var2].values\nX_valid = df_valid[col_var2].values\ny_train = np.log(df_train[col_target2].values.clip(0, 1e10)+1)\ny_valid = np.log(df_valid[col_target2].values.clip(0, 1e10)+1)\ntrain_data = lgb.Dataset(X_train, label=y_train)\nvalid_data = lgb.Dataset(X_valid, label=y_valid)\nmodel2_pub = lgb.train(params, train_data, best_itr2, valid_sets=[train_data, valid_data],\n                  verbose_eval=100,\n                  early_stopping_rounds=150,)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Train a model for private LB**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# train model to predict fatalities/day\ndf_train = df_traintest3[(pd.isna(df_traintest3['ForecastId'])) & (df_traintest3['dayofyear']<=day_before_public)]\ndf_valid = df_traintest3[(pd.isna(df_traintest3['ForecastId'])) & (day_before_public<df_traintest3['dayofyear'])]\ndf_test = df_traintest3[pd.isna(df_traintest3['ForecastId'])==False]\nX_train = df_train[col_var].values\nX_valid = df_valid[col_var].values\ny_train = np.log(df_train[col_target].values.clip(0, 1e10)+1)\ny_valid = np.log(df_valid[col_target].values.clip(0, 1e10)+1)\ntrain_data = lgb.Dataset(X_train, label=y_train)\nvalid_data = lgb.Dataset(X_valid, label=y_valid)\nnum_round = 15000\nmodel = lgb.train(params, train_data, num_round, valid_sets=[train_data, valid_data],\n                  verbose_eval=100,\n                  early_stopping_rounds=150,)\n\nbest_itr = model.best_iteration","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train with all data\ndf_train = df_traintest3[(pd.isna(df_traintest3['ForecastId']))]\ndf_valid = df_traintest3[(pd.isna(df_traintest3['ForecastId']))]\nX_train = df_train[col_var].values\nX_valid = df_valid[col_var].values\ny_train = np.log(df_train[col_target].values.clip(0, 1e10)+1)\ny_valid = np.log(df_valid[col_target].values.clip(0, 1e10)+1)\ntrain_data = lgb.Dataset(X_train, label=y_train)\nvalid_data = lgb.Dataset(X_valid, label=y_valid)\nmodel_pri = lgb.train(params, train_data, best_itr, valid_sets=[train_data, valid_data],\n                  verbose_eval=100,\n                  early_stopping_rounds=150,)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train model to predict cases/day\ndf_train = df_traintest3[(pd.isna(df_traintest3['ForecastId'])) & (df_traintest3['dayofyear']<=day_before_public)]\ndf_valid = df_traintest3[(pd.isna(df_traintest3['ForecastId'])) & (day_before_public<df_traintest3['dayofyear'])]\nX_train = df_train[col_var2].values\nX_valid = df_valid[col_var2].values\ny_train = np.log(df_train[col_target2].values.clip(0, 1e10)+1)\ny_valid = np.log(df_valid[col_target2].values.clip(0, 1e10)+1)\ntrain_data = lgb.Dataset(X_train, label=y_train)\nvalid_data = lgb.Dataset(X_valid, label=y_valid)\nmodel2 = lgb.train(params, train_data, num_round, valid_sets=[train_data, valid_data],\n                  verbose_eval=100,\n                  early_stopping_rounds=150,)\nbest_itr2 = model2.best_iteration","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train with all data\ndf_train = df_traintest3[(pd.isna(df_traintest3['ForecastId']))]\ndf_valid = df_traintest3[(pd.isna(df_traintest3['ForecastId']))]\nX_train = df_train[col_var2].values\nX_valid = df_valid[col_var2].values\ny_train = np.log(df_train[col_target2].values.clip(0, 1e10)+1)\ny_valid = np.log(df_valid[col_target2].values.clip(0, 1e10)+1)\ntrain_data = lgb.Dataset(X_train, label=y_train)\nvalid_data = lgb.Dataset(X_valid, label=y_valid)\nmodel2_pri = lgb.train(params, train_data, best_itr2, valid_sets=[train_data, valid_data],\n                  verbose_eval=100,\n                  early_stopping_rounds=150,)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# model2 = xgb.XGBRegressor(n_estimators=1000)\n# eval_set = [(df_valid[col_var2], df_valid[col_target2])]\n# model.fit(df_train[col_var2], df_train[col_target2], eval_metric=\"rmse\", eval_set=eval_set, verbose=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df_traintest['Date'][df_traintest['dayofyear']==day_before_valid].values[0])\nprint(df_traintest['Date'][df_traintest['dayofyear']==day_before_public].values[0])\nprint(df_traintest['Date'][df_traintest['dayofyear']==day_before_private].values[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove overlap for public LB prediction\ndf_tmp = df_traintest3[\n    ((df_traintest3['dayofyear']<=day_before_public)  & (pd.isna(df_traintest3['ForecastId'])))\n    | ((day_before_public<df_traintest3['dayofyear']) & (pd.isna(df_traintest3['ForecastId'])==False))].reset_index(drop=True)\n# df_tmp = df_tmp.drop([\n#     'cases/day_(1-1)', 'cases/day_(1-7)', 'cases/day_(8-14)', 'cases/day_(15-21)', \n#     'fatal/day_(1-1)', 'fatal/day_(1-7)', 'fatal/day_(8-14)', 'fatal/day_(15-21)',\n#     'days_since_1cases', 'days_since_10cases', 'days_since_100cases',\n#     'days_since_1fatal', 'days_since_10fatal', 'days_since_100fatal',\n#                                ],  axis=1)\ndf_tmp = df_tmp.drop(val_cols, axis=1)\ndf_traintest9 = []\nfor i, place in enumerate(places[:]):\n    df_tmp2 = df_tmp[df_tmp['place_id']==place].reset_index(drop=True)\n    df_tmp2 = do_aggregations(df_tmp2, roll_ranges=roll_ranges)\n    df_traintest9.append(df_tmp2)\ndf_traintest9 = pd.concat(df_traintest9).reset_index(drop=True)\ndf_traintest9[df_traintest9['dayofyear']>day_before_public-2].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove overlap for private LB prediction\ndf_tmp = df_traintest3[\n    ((df_traintest3['dayofyear']<=day_before_private)  & (pd.isna(df_traintest3['ForecastId'])))\n    | ((day_before_private<df_traintest3['dayofyear']) & (pd.isna(df_traintest3['ForecastId'])==False))].reset_index(drop=True)\n# df_tmp = df_tmp.drop([\n#     'cases/day_(1-1)', 'cases/day_(1-7)', 'cases/day_(8-14)', 'cases/day_(15-21)', \n#     'fatal/day_(1-1)', 'fatal/day_(1-7)', 'fatal/day_(8-14)', 'fatal/day_(15-21)',\n#     'days_since_1cases', 'days_since_10cases', 'days_since_100cases',\n#     'days_since_1fatal', 'days_since_10fatal', 'days_since_100fatal',\n#                                ],  axis=1)\ndf_tmp = df_tmp.drop(val_cols, axis=1)\ndf_traintest10 = []\nfor i, place in enumerate(places[:]):\n    df_tmp2 = df_tmp[df_tmp['place_id']==place].reset_index(drop=True)\n    df_tmp2 = do_aggregations(df_tmp2, roll_ranges=roll_ranges)\n    df_traintest10.append(df_tmp2)\ndf_traintest10 = pd.concat(df_traintest10).reset_index(drop=True)\ndf_traintest10[df_traintest10['dayofyear']>day_before_private-2].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predict test data in public\n# predict the cases and fatatilites one day at a time and use the predicts as next day's feature recursively.\ndf_preds = []\nfor i, place in enumerate(places[:]):\n    df_interest = copy.deepcopy(df_traintest9[df_traintest9['place_id']==place].reset_index(drop=True))\n    df_interest['cases/day'][(pd.isna(df_interest['ForecastId']))==False] = -1\n    df_interest['fatal/day'][(pd.isna(df_interest['ForecastId']))==False] = -1\n    len_known = (df_interest['dayofyear']<=day_before_public).sum()\n    len_unknown = (day_before_public<df_interest['dayofyear']).sum()\n    for j in range(len_unknown): # use predicted cases and fatal for next days' prediction\n        X_valid = df_interest[col_var].iloc[j+len_known]\n        X_valid2 = df_interest[col_var2].iloc[j+len_known]\n        pred_f = model_pub.predict(X_valid)\n        pred_c = model2_pub.predict(X_valid2)\n        pred_c = (np.exp(pred_c)-1).clip(0, 1e10)\n        pred_f = (np.exp(pred_f)-1).clip(0, 1e10)\n        df_interest['fatal/day'][j+len_known] = pred_f\n        df_interest['cases/day'][j+len_known] = pred_c\n        df_interest['Fatalities'][j+len_known] = df_interest['Fatalities'][j+len_known-1] + pred_f\n        df_interest['ConfirmedCases'][j+len_known] = df_interest['ConfirmedCases'][j+len_known-1] + pred_c\n#         print(df_interest['ConfirmedCases'][j+len_known-1], df_interest['ConfirmedCases'][j+len_known], pred_c)\n#         df_interest = df_interest.drop([\n#             'cases/day_(1-1)', 'cases/day_(1-7)', 'cases/day_(8-14)', 'cases/day_(15-21)', \n#             'fatal/day_(1-1)', 'fatal/day_(1-7)', 'fatal/day_(8-14)', 'fatal/day_(15-21)',\n#             'days_since_1cases', 'days_since_10cases', 'days_since_100cases',\n#             'days_since_1fatal', 'days_since_10fatal', 'days_since_100fatal',\n\n#                                        ],  axis=1)\n        df_interest = df_interest.drop(val_cols,  axis=1)\n        df_interest = do_aggregations(df_interest, roll_ranges=roll_ranges)\n    if (i+1)%10==0:\n        print(\"{:3d}/{}  {}, len known: {}, len unknown: {}\".format(i+1, len(places), place, len_known, len_unknown), df_interest.shape)\n    df_interest['fatal_pred'] = np.cumsum(df_interest['fatal/day'].values)\n    df_interest['cases_pred'] = np.cumsum(df_interest['cases/day'].values)\n    df_preds.append(df_interest)\ndf_preds = pd.concat(df_preds)\ndf_preds.to_csv(\"df_preds.csv\", index=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predict test data in private\ndf_preds_pri = []\nfor i, place in enumerate(places[:]):\n    df_interest = copy.deepcopy(df_traintest10[df_traintest10['place_id']==place].reset_index(drop=True))\n    df_interest['cases/day'][(pd.isna(df_interest['ForecastId']))==False] = -1\n    df_interest['fatal/day'][(pd.isna(df_interest['ForecastId']))==False] = -1\n    len_known = (df_interest['dayofyear']<=day_before_private).sum()\n    len_unknown = (day_before_private<df_interest['dayofyear']).sum()\n    for j in range(len_unknown): # use predicted cases and fatal for next days' prediction\n        X_valid = df_interest[col_var].iloc[j+len_known]\n        X_valid2 = df_interest[col_var2].iloc[j+len_known]\n        pred_f = model_pri.predict(X_valid)\n        pred_c = model2_pri.predict(X_valid2)\n        pred_c = (np.exp(pred_c)-1).clip(0, 1e10)\n        pred_f = (np.exp(pred_f)-1).clip(0, 1e10)\n        df_interest['fatal/day'][j+len_known] = pred_f\n        df_interest['cases/day'][j+len_known] = pred_c\n        df_interest['Fatalities'][j+len_known] = df_interest['Fatalities'][j+len_known-1] + pred_f\n        df_interest['ConfirmedCases'][j+len_known] = df_interest['ConfirmedCases'][j+len_known-1] + pred_c\n#         print(df_interest['ConfirmedCases'][j+len_known-1], df_interest['ConfirmedCases'][j+len_known], pred_c)\n#         df_interest = df_interest.drop([\n#             'cases/day_(1-1)', 'cases/day_(1-7)', 'cases/day_(8-14)', 'cases/day_(15-21)', \n#             'fatal/day_(1-1)', 'fatal/day_(1-7)', 'fatal/day_(8-14)', 'fatal/day_(15-21)',\n#             'days_since_1cases', 'days_since_10cases', 'days_since_100cases',\n#             'days_since_1fatal', 'days_since_10fatal', 'days_since_100fatal',\n\n#                                        ],  axis=1)\n        df_interest = df_interest.drop(val_cols,  axis=1)\n        df_interest = do_aggregations(df_interest, roll_ranges=roll_ranges)\n    if (i+1)%10==0:\n        print(\"{:3d}/{}  {}, len known: {}, len unknown: {}\".format(i+1, len(places), place, len_known, len_unknown), df_interest.shape)\n    df_interest['fatal_pred'] = np.cumsum(df_interest['fatal/day'].values)\n    df_interest['cases_pred'] = np.cumsum(df_interest['cases/day'].values)\n    df_preds_pri.append(df_interest)\ndf_preds_pri = pd.concat(df_preds_pri)\ndf_preds_pri.to_csv(\"df_preds_pri.csv\", index=None)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Visualize prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"places_sort = df_traintest10[['place_id', 'ConfirmedCases']][df_traintest10['dayofyear']==day_before_private]\nplaces_sort = places_sort.sort_values('ConfirmedCases', ascending=False).reset_index(drop=True)['place_id'].values\nprint(len(places_sort))\nplaces_sort[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Fatalities / Public\")\nplt.figure(figsize=(30,30))\nfor i in range(30):\n    plt.subplot(5,6,i+1)\n    idx = i * 10\n    df_interest = df_preds[df_preds['place_id']==places_sort[idx]].reset_index(drop=True)\n    tmp = df_interest['fatal/day'].values\n    tmp = np.cumsum(tmp)\n#     print(len(tmp), places_sort[idx])\n    sns.lineplot(x=df_interest['dayofyear'], y=tmp, label='pred')\n    df_interest2 = df_traintest10[(df_traintest10['place_id']==places_sort[idx]) & (df_traintest10['dayofyear']<=day_before_private)].reset_index(drop=True)\n    sns.lineplot(x=df_interest2['dayofyear'].values, y=df_interest2['Fatalities'].values, label='true')\n    plt.title(places_sort[idx])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Confirmed Cases / Public\")\nplt.figure(figsize=(30,30))\nfor i in range(30):\n    plt.subplot(5,6,i+1)\n    idx = i * 10\n    df_interest = df_preds[df_preds['place_id']==places_sort[idx]].reset_index(drop=True)\n    tmp = df_interest['cases/day'].values\n    tmp = np.cumsum(tmp)\n    sns.lineplot(x=df_interest['dayofyear'], y=tmp, label='pred')\n    df_interest2 = df_traintest10[(df_traintest10['place_id']==places_sort[idx]) & (df_traintest10['dayofyear']<=day_before_private)].reset_index(drop=True)\n    sns.lineplot(x=df_interest2['dayofyear'].values, y=df_interest2['ConfirmedCases'].values, label='true')\n    plt.title(places_sort[idx])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Fatalities / Private\")\nplt.figure(figsize=(30,30))\nfor i in range(30):\n    plt.subplot(5,6,i+1)\n    idx = i * 10\n    df_interest = df_preds_pri[df_preds_pri['place_id']==places_sort[idx]].reset_index(drop=True)\n    tmp = df_interest['fatal/day'].values\n    tmp = np.cumsum(tmp)\n    sns.lineplot(x=df_interest['dayofyear'], y=tmp, label='pred')\n    df_interest2 = df_traintest10[(df_traintest10['place_id']==places_sort[idx]) & (df_traintest10['dayofyear']<=day_before_private)].reset_index(drop=True)\n    sns.lineplot(x=df_interest2['dayofyear'].values, y=df_interest2['Fatalities'].values, label='true')\n    plt.title(places_sort[idx])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"ConfirmedCases / Private\")\nplt.figure(figsize=(30,30))\nfor i in range(30):\n    plt.subplot(5,6,i+1)\n    idx = i * 10\n    df_interest = df_preds_pri[df_preds_pri['place_id']==places_sort[idx]].reset_index(drop=True)\n    tmp = df_interest['cases/day'].values\n    tmp = np.cumsum(tmp)\n    sns.lineplot(x=df_interest['dayofyear'], y=tmp, label='pred')\n    df_interest2 = df_traintest10[(df_traintest10['place_id']==places_sort[idx]) & (df_traintest10['dayofyear']<=day_before_private)].reset_index(drop=True)\n    sns.lineplot(x=df_interest2['dayofyear'].values, y=df_interest2['ConfirmedCases'].values, label='true')\n    plt.title(places_sort[idx])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# # remove overlaps between train and test\n# df_traintest4 = copy.deepcopy(df_traintest3)\n# df_traintest4['unique'] = df_traintest4.apply(lambda x: x['place_id'] + str(x['dayofyear']), axis=1)\n# print(len(df_traintest4))\n# df_traintest4 = df_traintest4[df_traintest4['unique'].duplicated()==False]\n# print(len(df_traintest4))\n# df_traintest4[(df_traintest4['place_id']=='China/Hubei') & (df_traintest4['dayofyear']>75)].head() #2020-03-15","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# # count the fatalities per place until Feb.\n# df_tmp = df_traintest[pd.isna(df_traintest['Fatalities'])==False]\n# df_tmp = df_tmp[df_tmp['dayofyear']<61]\n# df_agg = df_tmp.groupby('place_id')['Fatalities'].agg('max').reset_index()\n# df_agg = df_agg.sort_values('Fatalities', ascending=False)\n# df_agg.head()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"print(len(col_var), len(col_var2))\ncol_var, col_var2","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# # Check the predictions of some hot areas.\n# place = 'China/Hubei'\n# # place = 'Iran'\n# df_interest_base = df_traintest4[df_traintest4['place_id']==place].reset_index(drop=True)\n# df_interest = copy.deepcopy(df_interest_base)\n# df_interest['cases/day'] = df_interest['cases/day'].astype(np.float)\n# df_interest['fatal/day'] = df_interest['fatal/day'].astype(np.float)\n# df_interest['cases/day'][df_interest['dayofyear']>=train_valid_cutoff_dayofyear] = -1\n# df_interest['fatal/day'][df_interest['dayofyear']>=train_valid_cutoff_dayofyear] = -1\n# len_known = (df_interest['cases/day']!=-1).sum()\n# len_unknown = (df_interest['cases/day']==-1).sum()\n# print(\"len train: {}, len prediction: {}\".format(len_known, len_unknown))\n# for i in range(len_unknown): # use predicted cases and fatal for next days' prediction\n# #     print(i)\n#     X_valid = df_interest[col_var].iloc[i+len_known]\n#     X_valid2 = df_interest[col_var2].iloc[i+len_known]\n# #     print(X_valid.shape)\n#     pred_f = model.predict(X_valid)\n#     pred_c = model2.predict(X_valid2)\n#     df_interest['fatal/day'][i+len_known] = pred_f\n#     df_interest['cases/day'][i+len_known] = pred_c\n#     df_interest = df_interest[['cases/day', 'fatal/day', 'Long', 'Lat', 'SmokingRate']+time_cols+extra_cols+country_info_cols+le_cols+wk3_cols]\n#     df_interest = do_aggregations(df_interest, roll_ranges=roll_ranges)\n\n# # visualize\n# tmp = df_interest_base['fatal/day'].values\n# tmp = np.cumsum(tmp)\n# sns.lineplot(x=df_interest_base['dayofyear'][pd.isna(df_interest_base['Fatalities'])==False],\n#              y=tmp[pd.isna(df_interest_base['Fatalities'])==False], label='true')\n# tmp = df_interest['fatal/day'].values\n# tmp = np.cumsum(tmp)\n# sns.lineplot(x=df_interest_base['dayofyear'], y=tmp, label='pred')\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# place = 'Iran'\n# df_interest_base = df_traintest4[df_traintest4['place_id']==place].reset_index(drop=True)\n# df_interest = copy.deepcopy(df_interest_base)\n# df_interest['cases/day'] = df_interest['cases/day'].astype(np.float)\n# df_interest['fatal/day'] = df_interest['fatal/day'].astype(np.float)\n# df_interest['cases/day'][df_interest['dayofyear']>=train_valid_cutoff_dayofyear] = -1\n# df_interest['fatal/day'][df_interest['dayofyear']>=train_valid_cutoff_dayofyear] = -1\n# len_known = (df_interest['cases/day']!=-1).sum()\n# len_unknown = (df_interest['cases/day']==-1).sum()\n# print(\"len train: {}, len prediction: {}\".format(len_known, len_unknown))\n# for i in range(len_unknown): # use predicted cases and fatal for next days' prediction\n#     X_valid = df_interest[col_var].iloc[i+len_known]\n#     X_valid2 = df_interest[col_var2].iloc[i+len_known]\n# #     print(X_valid.shape)\n#     pred_f = model.predict(X_valid)\n#     pred_c = model2.predict(X_valid2)\n#     df_interest['fatal/day'][i+len_known] = pred_f\n#     df_interest['cases/day'][i+len_known] = pred_c\n#     df_interest = df_interest[['cases/day', 'fatal/day', 'Long', 'Lat', 'SmokingRate']+time_cols+extra_cols+country_info_cols+le_cols+wk3_cols]\n#     df_interest = do_aggregations(df_interest, roll_ranges=roll_ranges)\n\n# # visualize\n# tmp = df_interest_base['fatal/day'].values\n# tmp = np.cumsum(tmp)\n# sns.lineplot(x=df_interest_base['dayofyear'][pd.isna(df_interest_base['Fatalities'])==False],\n#              y=tmp[pd.isna(df_interest_base['Fatalities'])==False], label='true')\n# tmp = df_interest['fatal/day'].values\n# tmp = np.cumsum(tmp)\n# sns.lineplot(x=df_interest_base['dayofyear'], y=tmp, label='pred')\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# place = 'Italy'\n# df_interest_base = df_traintest4[df_traintest4['place_id']==place].reset_index(drop=True)\n# df_interest = copy.deepcopy(df_interest_base)\n# df_interest['cases/day'] = df_interest['cases/day'].astype(np.float)\n# df_interest['fatal/day'] = df_interest['fatal/day'].astype(np.float)\n# df_interest['cases/day'][df_interest['dayofyear']>=train_valid_cutoff_dayofyear] = -1\n# df_interest['fatal/day'][df_interest['dayofyear']>=train_valid_cutoff_dayofyear] = -1\n# len_known = (df_interest['cases/day']!=-1).sum()\n# len_unknown = (df_interest['cases/day']==-1).sum()\n# print(\"len train: {}, len prediction: {}\".format(len_known, len_unknown))\n# for i in range(len_unknown): # use predicted cases and fatal for next days' prediction\n#     X_valid = df_interest[col_var].iloc[i+len_known]\n#     X_valid2 = df_interest[col_var2].iloc[i+len_known]\n# #     print(X_valid.shape)\n#     pred_f = model.predict(X_valid)\n#     pred_c = model2.predict(X_valid2)\n#     df_interest['fatal/day'][i+len_known] = pred_f\n#     df_interest['cases/day'][i+len_known] = pred_c\n#     df_interest = df_interest[['cases/day', 'fatal/day', 'Long', 'Lat', 'SmokingRate']+time_cols+extra_cols+country_info_cols+le_cols+wk3_cols]\n#     df_interest = do_aggregations(df_interest, roll_ranges=roll_ranges)\n\n# # visualize\n# tmp = df_interest_base['fatal/day'].values\n# tmp = np.cumsum(tmp)\n# sns.lineplot(x=df_interest_base['dayofyear'][pd.isna(df_interest_base['Fatalities'])==False],\n#              y=tmp[pd.isna(df_interest_base['Fatalities'])==False], label='true')\n# tmp = df_interest['fatal/day'].values\n# tmp = np.cumsum(tmp)\n# sns.lineplot(x=df_interest_base['dayofyear'], y=tmp, label='pred')\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# # train model to predict fatalities/day\n# # col_target = 'fatal/day'\n# # col_var = [\n# #     'Lat', 'Long',\n# # #     'cases/day_(1-1)', 'cases/day_(1-7)', 'cases/day_(8-14)', \n# #     'fatal/day_(1-1)', 'fatal/day_(1-7)', 'fatal/day_(8-14)',\n# #     'SmokingRate',\n# # #     'day'\n# # ]\n# df_train = df_traintest3[(pd.isna(df_traintest3['ForecastId']))]\n# X_train = df_train[col_var].values\n# X_valid = df_train[col_var].values\n# y_train = df_train[col_target].values\n# y_valid = df_train[col_target].values\n# train_data = lgb.Dataset(X_train, label=y_train)\n# valid_data = lgb.Dataset(X_valid, label=y_valid)\n# # num_round = 575\n# num_round = 15000\n# model = lgb.train(params, train_data, num_round, valid_sets=[train_data, valid_data], verbose_eval=100)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# # train model to predict cases/day\n# df_train = df_traintest3[(pd.isna(df_traintest3['ForecastId']))]\n# X_train = df_train[col_var2].values\n# X_valid = df_train[col_var2].values\n# y_train = df_train[col_target2].values\n# y_valid = df_train[col_target2].values\n# train_data = lgb.Dataset(X_train, label=y_train)\n# valid_data = lgb.Dataset(X_valid, label=y_valid)\n# # num_round = 225\n# num_round = 15000\n# model2 = lgb.train(params, train_data, num_round, valid_sets=[train_data, valid_data], verbose_eval=100,)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Make Submission"},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"# # predict test data\n# df_preds = []\n# for i, place in enumerate(places[:]):\n#     df_interest = copy.deepcopy(df_traintest4[df_traintest4['place_id']==place].reset_index(drop=True))\n#     df_interest['cases/day'] = df_interest['cases/day'].astype(np.float)\n#     df_interest['fatal/day'] = df_interest['fatal/day'].astype(np.float)\n#     df_interest['cases/day'][(pd.isna(df_interest['ForecastId']))==False] = -1\n#     df_interest['fatal/day'][(pd.isna(df_interest['ForecastId']))==False] = -1\n#     len_known = (df_interest['cases/day']!=-1).sum()\n#     len_unknown = (df_interest['cases/day']==-1).sum()\n#     if (i+1)%10==0:\n#         print(\"{:3d}/{}  {}, len known: {}, len unknown: {}\".format(i+1, len(places), place, len_known, len_unknown), df_interest.shape)\n#     for j in range(len_unknown): # use predicted cases and fatal for next days' prediction\n#         X_valid = df_interest[col_var].iloc[j+len_known]\n#         X_valid2 = df_interest[col_var2].iloc[j+len_known]\n# #         print(X_valid.shape)\n#         pred_f = model.predict(X_valid)\n#         pred_c = model2.predict(X_valid2)\n# #         print(pred_f, pred_c)\n#         df_interest['fatal/day'][j+len_known] = pred_f\n#         df_interest['cases/day'][j+len_known] = pred_c\n#         df_interest = df_interest[['cases/day', 'fatal/day', 'Long', 'Lat', 'SmokingRate', 'ForecastId', 'place_id']+time_cols+extra_cols+country_info_cols+le_cols+wk3_cols]\n#         df_interest = do_aggregations(df_interest, roll_ranges=roll_ranges)\n#     df_interest['fatal_pred'] = np.cumsum(df_interest['fatal/day'].values)\n#     df_interest['cases_pred'] = np.cumsum(df_interest['cases/day'].values)\n#     df_preds.append(df_interest)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(df_preds), len(df_preds_pri)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# merge 2 preds\ndf_preds[df_preds['dayofyear']>day_before_private] = df_preds_pri[df_preds['dayofyear']>day_before_private]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_preds.to_csv(\"df_preds2.csv\", index=None)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"# # concat prediction\n# df_preds= pd.concat(df_preds)\n# df_preds = df_preds.sort_values('dayofyear')\n# col_tmp = ['place_id', 'ForecastId', 'dayofyear', 'cases/day', 'cases_pred', 'fatal/day', 'fatal_pred',]\n# df_preds[col_tmp][(df_preds['place_id']=='Afghanistan') & (df_preds['dayofyear']>75)].head(10)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"# load sample submission\ndf_sub = pd.read_csv(\"../input/covid19-global-forecasting-week-4/submission.csv\")\nprint(len(df_sub))\ndf_sub.head()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"# merge prediction with sub\ndf_sub = pd.merge(df_sub, df_traintest3[['ForecastId', 'place_id', 'dayofyear']])\ndf_sub = pd.merge(df_sub, df_preds[['place_id', 'dayofyear', 'cases_pred', 'fatal_pred']], on=['place_id', 'dayofyear',], how='left')\ndf_sub.head(10)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"# save\ndf_sub['ConfirmedCases'] = df_sub['cases_pred']\ndf_sub['Fatalities'] = df_sub['fatal_pred']\ndf_sub = df_sub[['ForecastId', 'ConfirmedCases', 'Fatalities']]\ndf_sub.to_csv(\"submission.csv\", index=None)\ndf_sub.head(10)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":4}