{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import os","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm.notebook import tqdm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(\"../input/covid19-global-forecasting-week-4/train.csv\")\ntest_df = pd.read_csv(\"../input/covid19-global-forecasting-week-4/test.csv\")\nsubmission_df = pd.read_csv(\"../input/covid19-global-forecasting-week-4/submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"region_metadata = pd.read_csv(\"../input/covid19-forecasting-metadata/region_metadata.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def transform_geo_location(df):\n    lat = df.lat.values\n    lon = df.lon.values\n    \n    df[\"lat_sin\"] = np.sin(2 * np.pi * lat / 360)\n    df[\"lat_cos\"] = np.cos(2 * np.pi * lat / 360)\n    df[\"lon_sin\"] = np.sin(2 * np.pi * lon / 360)\n    df[\"lon_cos\"] = np.cos(2 * np.pi * lon / 360)\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"region_metadata = transform_geo_location(region_metadata)\ngeo_location_columns = [\"lat_sin\", \"lat_cos\", \"lon_sin\", \"lon_cos\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"country_data = pd.read_csv(\"../input/countries-of-the-world/countries of the world.csv\")\ncountry_data.Country = country_data.Country.apply(lambda c: c.rstrip(\" \"))\ncountry_data.rename(\n    columns={\n        \"Country\": \"Country_Region\",\n    },\n    inplace=True\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"country_meta_columns = [\n    'Coastline (coast/area ratio)',\n    'Net migration',\n    'Infant mortality (per 1000 births)',\n    'GDP ($ per capita)',\n    'Literacy (%)',\n    'Phones (per 1000)',\n    'Arable (%)',\n    'Crops (%)',\n    'Other (%)',\n    'Climate',\n    'Birthrate',\n    'Deathrate',\n    'Agriculture',\n    'Industry',\n    'Service'\n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in country_meta_columns:\n    # country_data[col].fillna(-1e6, inplace=True)\n    country_data[col] = (\n        country_data[col]\n        .fillna(-1000)\n        .apply(lambda v: v.replace(\",\", \".\") if isinstance(v, str) else v)\n        .astype(np.float32)\n    )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lockdown_meta_df = pd.read_csv(\"../input/covid19-lockdown-dates-by-country/countryLockdowndates.csv\")\nlockdown_meta_df.rename(\n    columns={\n        \"Country/Region\": \"Country_Region\",\n        \"Province\": \"Province_State\",\n        \"Date\": \"LockdownDate\",\n            \"Type\": \"LockdownType\"\n    },\n    inplace=True\n)\nlockdown_meta_df.drop(\"Reference\", axis=1, inplace=True)\n\ndef convert_lockdown_date(d):\n    if isinstance(d, str):\n        return \"-\".join(reversed(d.split(\"/\")))\n    else:\n        return \"2020-12-31\"\n\nlockdown_meta_df.LockdownDate = lockdown_meta_df.LockdownDate.apply(convert_lockdown_date)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = train_df.merge(lockdown_meta_df, on=[\"Province_State\", \"Country_Region\"], how=\"left\")\ntrain_df = train_df.merge(country_data, on=[\"Country_Region\"], how=\"left\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[\"days_since_lockdown\"] = np.clip(\n    (pd.to_datetime(train_df.Date) - pd.to_datetime(train_df.LockdownDate)).dt.days,\n    a_min=-1,\n    a_max=None\n)\ntrain_df[\"lockdown_type\"] = [\n    t if d >= 0 else \"None\" for t, d in zip (train_df.LockdownType, train_df.days_since_lockdown)\n]\nlockdown_type_encoder = LabelEncoder().fit(train_df.lockdown_type)\ntrain_df[\"lockdown_type\"] = lockdown_type_encoder.transform(train_df.lockdown_type)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[\"ConfirmedCases\"] = np.log1p(train_df.ConfirmedCases)\ntrain_df[\"Fatalities\"] = np.log1p(train_df.Fatalities)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def extract_region(df):\n    return (\n        df.Country_Region +\n        df.Province_State.fillna(\"\").apply(lambda s: \" + \" + s if s else s)\n    )\n    \nregion_encoder = LabelEncoder().fit(extract_region(train_df))\ntrain_df[\"region_id\"] = region_encoder.transform(extract_region(train_df))\ntest_df[\"region_id\"] = region_encoder.transform(extract_region(test_df))\n\nregion_metadata[\"region_id\"] = region_encoder.transform(extract_region(region_metadata))\nregion_metadata.drop([\"Province_State\", \"Country_Region\"], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stats_df = train_df[[\"Fatalities\", \"ConfirmedCases\", \"region_id\"]].groupby(\"region_id\").sum()\nstats_df[\"fatalities_to_cases\"] = stats_df.Fatalities - stats_df.ConfirmedCases\nstats_df.drop([\"Fatalities\", \"ConfirmedCases\"], axis=1, inplace=True)\ntrain_df = train_df.merge(stats_df, on=\"region_id\", how=\"left\")\nstats_columns = [\"fatalities_to_cases\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def extract_country_from_region(region):\n    if \"+\" in region:\n        country, state = region.split(\" + \")\n    else:\n        country = region\n        \n    return country\n\nregion_to_country = dict(\n    zip(\n        range(len(region_encoder.classes_)),\n        map(extract_country_from_region, region_encoder.classes_)\n    )\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = train_df.merge(region_metadata, on=\"region_id\", how=\"left\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"meta_columns = [\n    \"density\",\n    \"population\",\n    \"area\"\n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_columns = [\n    \"region_id\",\n    \"lockdown_type\"\n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def compute_historical_features(df, target_column, past_horizon, create_target=True):\n    \n    features = dict()\n    \n    for col in meta_columns + country_meta_columns:\n        features[col] = df[col].unique().item()\n        \n    for col in geo_location_columns:\n        features[col] = df[col].unique().item()\n        \n    for col in stats_columns:\n        features[col] = df[col].unique().item()\n        \n    features[\"region_id\"] = df[\"region_id\"].unique().item()\n\n    last = -1 if create_target else len(df)\n    \n    target_values = df[target_column].values\n        \n    for lag in range(past_horizon):\n        features[\"value_{}_{}\".format(target_column, lag)] = target_values[last-lag-1]\n        \n    features[\"days_since_lockdown\"] = df.days_since_lockdown.max()\n    features[\"lockdown_type\"] = df.lockdown_type.values[last-1]\n    \n    if create_target:\n        features[target_column] = df[target_column].values[-1]\n        \n    return features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def prepare_datasets(\n    train_df,\n    last_train_date,\n    target_column,\n    train_start_offset=50,\n    step=1,\n):\n\n    dates = train_df.Date.unique()\n    train_dates = dates[dates <= last_train_date]\n    val_dates = dates[dates > last_train_date]\n\n    train_features = []\n\n    train_subdf = train_df[train_df.Date <= last_train_date]\n    val_subdf = train_df[train_df.Date > last_train_date]\n\n    for region_id in tqdm(sorted(train_df.region_id.unique()), desc=\"Making train features\"):\n\n        subdf = train_subdf[train_subdf.region_id == region_id]\n\n        for date in train_dates[train_start_offset::step]:\n            features = compute_historical_features(\n                subdf[subdf.Date <= date],\n                target_column=target_column,\n                past_horizon=train_start_offset,\n                create_target=True\n            )\n            train_features.append(features)\n            \n    return pd.DataFrame(train_features), train_subdf, val_subdf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def autoregressive_predict(model, dates, df, target_column, past_horizon):\n    \n    n_regions = len(region_encoder.classes_)\n    \n    original_size = len(df)\n    future_df = pd.DataFrame({\n        \"Date\": np.repeat(dates, repeats=n_regions),\n        \"region_id\": np.tile(np.arange(n_regions), len(dates))\n    })\n    future_df[\"Country_Region\"] = future_df.region_id.map(region_to_country)\n    \n    combined_df = pd.concat([\n        df.drop(meta_columns + country_meta_columns + geo_location_columns + stats_columns, axis=1),\n        future_df\n    ], sort=True)\n    combined_df = combined_df.merge(region_metadata, on=\"region_id\", how=\"left\")\n    combined_df = combined_df.merge(country_data, on=[\"Country_Region\"], how=\"left\")\n    combined_df = combined_df.merge(stats_df, on=\"region_id\", how=\"left\")\n\n    for date in tqdm(dates, \"Prediction\"):\n        date_features = []\n        for region_id in range(len(region_encoder.classes_)):\n            cond = (combined_df.Date < date) & (combined_df.region_id == region_id)\n            features = compute_historical_features(\n                combined_df[cond],\n                target_column=target_column,\n                past_horizon=past_horizon,\n                create_target=False\n            )\n            date_features.append(features)\n            \n        date_predictions = model.predict(pd.DataFrame(date_features))\n        combined_df.loc[combined_df.Date == date, target_column] = date_predictions\n        \n    return combined_df[[\"Date\", \"region_id\", target_column]].loc[original_size:].reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def target_metric(actual, predicted):\n    return np.sqrt(((actual - predicted) ** 2).mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_booster(\n    train_features_df,\n    target_column,\n    categorical_columns,\n    epochs=20,\n    train_start_offset=50,\n    val_df=None\n):\n\n    train_dataset = lgb.Dataset(\n        train_features_df.drop([target_column], axis=1),\n        train_features_df[target_column],\n        free_raw_data=False,\n        categorical_feature=categorical_columns\n    )\n\n    params = {\n        'boosting_type': 'gbdt',\n        'objective': 'rmse',\n        'num_leaves': 31,\n        'max_depth': 7,\n        'learning_rate': 0.01,\n        'feature_fraction': 0.5,\n        'num_threads': os.cpu_count()\n    }\n\n    rounds_per_epoch = 300\n\n    model = None\n\n    for epoch in range(epochs):\n\n        model = lgb.train(\n            params,\n            train_dataset,\n            num_boost_round=rounds_per_epoch,\n            valid_sets=[train_dataset],\n            valid_names=[\"train\"],\n            verbose_eval=rounds_per_epoch // 5,\n            init_model=model\n        )\n\n        if val_df is not None:\n            \n            val_dates = val_df.Date.unique()\n            \n            val_predictions = autoregressive_predict(\n                model,\n                dates=val_dates,\n                df=train_subdf,\n                past_horizon=train_start_offset,\n                target_column=target_column\n            )\n\n            error = target_metric(\n                val_subdf.sort_values(by=[\"Date\", \"region_id\"])[target_column].values,\n                val_predictions.sort_values(by=[\"Date\", \"region_id\"])[target_column].values\n            )\n\n            print(\"Epoch\", epoch, \"error:\", error)\n            print()\n        \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_start_offset = 70\nepochs = 10\ntest_dates = test_df.Date[test_df.Date > train_df.Date.max()].unique()\nmerge_columns = [\"Date\", \"region_id\"]\n\nfor target_column in [\"ConfirmedCases\", \"Fatalities\"]:\n    \n    print()\n    print(\"Working on column\", target_column)\n    print()\n    \n    train_features_df, train_subdf, val_subdf = prepare_datasets(\n        train_df,\n        # no validation <._.>\n        last_train_date=train_df.Date.max(),\n        target_column=target_column,\n        train_start_offset=train_start_offset\n    )\n\n    model = train_booster(\n        train_features_df,\n        target_column=target_column,\n        categorical_columns=categorical_columns,\n        epochs=epochs,\n        train_start_offset=train_start_offset,\n        val_df=None\n    )\n\n    test_predictions = autoregressive_predict(\n        model,\n        dates=test_dates,\n        df=train_subdf,\n        past_horizon=train_start_offset,\n        target_column=target_column\n    )\n\n    predicted_test_df = test_df.merge(\n        pd.concat([\n            train_df[merge_columns + [target_column]],\n            test_predictions[merge_columns + [target_column]]\n        ], sort=True),\n        on=merge_columns,\n        how=\"left\"\n    )\n\n    submission_df = (\n        submission_df\n        .drop([target_column], axis=1)\n        .merge(\n            predicted_test_df[[target_column, \"ForecastId\"]],\n            on=[\"ForecastId\"],\n            how=\"left\"\n        )\n    )\n    \n    submission_df[target_column] = np.expm1(submission_df[target_column])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"}},"nbformat":4,"nbformat_minor":4}