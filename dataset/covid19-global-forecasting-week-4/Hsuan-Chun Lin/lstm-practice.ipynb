{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data_train = pd.read_csv(\"/kaggle/input/covid19-global-forecasting-week-4/train.csv\", parse_dates = [\"Date\"])\ndata_test = pd.read_csv(\"/kaggle/input/covid19-global-forecasting-week-4/test.csv\", parse_dates = [\"Date\"])\nsubmission = pd.read_csv(\"/kaggle/input/covid19-global-forecasting-week-4/submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train.Province_State = data_train.Province_State.fillna(\"N\")\ndata_test.Province_State = data_test.Province_State.fillna(\"N\")\ninteraction = data_train[\"Province_State\"] + \"_\" + data_train[\"Country_Region\"]\ninteraction_test = data_test[\"Province_State\"] + \"_\" + data_test[\"Country_Region\"]\ndata_train[\"Interaction\"] = interaction\ndata_test[\"Interaction\"] = interaction_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import LabelEncoder\ncl_case = MinMaxScaler()\ncl_fate = MinMaxScaler()\nle = LabelEncoder()\ndata_train[\"ConfirmedCases_norm\"] = 0\ndata_train[\"Fatalities_norm\"] = 0\ndata_train[\"ConfirmedCases_norm\"] = cl_case.fit_transform(data_train[\"ConfirmedCases\"].values.reshape(-1,1))\ndata_train[\"Fatalities_norm\"] = cl_fate.fit_transform(data_train[\"Fatalities_norm\"].values.reshape(-1,1))\ndata_train[\"CR_label\"] = le.fit_transform(data_train.Interaction)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating windows\ndef create_window(interval, prediction_day,data, column):\n    X = []\n    y = []\n    groupby_Interaction = data.groupby(\"Interaction\")\n    Interaction_list = data.Interaction.unique()\n    for Interaction_ind in Interaction_list:\n        county = groupby_Interaction.get_group(Interaction_ind)\n        county = county[[\"ConfirmedCases\", \"Fatalities\", \"CR_label\"]]\n        for i in range(county.shape[0]-interval-prediction_day):\n            X.append(county.iloc[i:i+interval].values)\n            y.append(county.iloc[i+interval:i+interval+prediction_day, column].values)\n    return np.array(X), np.array(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"interval = 5\nprediction_day = 1\ndata_case_X, data_case_y = create_window(interval, prediction_day, data_train, 0)\ndata_f_X, data_f_y = create_window(interval, prediction_day, data_train, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_case_y.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_c_train, X_c_test, y_c_train, y_c_test = train_test_split(data_case_X, data_case_y, test_size=0.33, random_state=42)\nX_f_train, X_f_test, y_f_train, y_f_test = train_test_split(data_f_X, data_f_y, test_size=0.33, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_c_train = X_c_train.reshape((X_c_train.shape[0],X_c_train.shape[1],3))\nX_c_test = X_c_test.reshape((X_c_test.shape[0],X_c_test.shape[1],3))\nX_f_train = X_f_train.reshape((X_f_train.shape[0],X_f_train.shape[1],3))\nX_f_test = X_f_test.reshape((X_f_test.shape[0],X_f_test.shape[1],3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.losses import MeanSquaredLogarithmicError\nfrom keras import Sequential\nfrom keras import layers\nfrom keras import Input\nfrom keras.utils import plot_model\nfrom keras.models import Model\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_c = Sequential()\nmodel_c.add(layers.LSTM(130,input_shape=(interval,3)))\nmodel_c.add(layers.Dense(65))\nmodel_c.add(layers.Dropout(rate = 0.2))\nmodel_c.add(layers.Dense(32))\nmodel_c.add(layers.Dense(prediction_day))\n\nmodel_f = Sequential()\nmodel_f.add(layers.LSTM(130,input_shape=(interval,3)))\nmodel_f.add(layers.Dense(65))\nmodel_f.add(layers.Dropout(rate = 0.2))\nmodel_f.add(layers.Dense(32))\nmodel_f.add(layers.Dense(prediction_day))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"callbacks = [ReduceLROnPlateau(monitor='val_loss', patience=4, verbose=1, factor=0.6),\n             EarlyStopping(monitor='val_loss', patience=20),\n             ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)]\nmodel_c.compile(loss=[MeanSquaredLogarithmicError()], optimizer=\"adam\")\nmodel_f.compile(loss=[MeanSquaredLogarithmicError()], optimizer=\"adam\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history_c = model_c.fit(X_c_train, y_c_train, \n          epochs = 100, \n          batch_size = 1000, \n          validation_data=(X_c_test,  y_c_test), \n          callbacks=callbacks)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history_f = model_f.fit(X_f_train, y_f_train, \n          epochs = 100, \n          batch_size = 1000, \n          validation_data=(X_f_test,  y_f_test), \n          callbacks=callbacks)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history_c.history['loss'])\nplt.plot(history_c.history['val_loss'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history_f.history['loss'])\nplt.plot(history_f.history['val_loss'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"country_ = data_test.Interaction.unique()\n#first 7 days\npredict_c = []\npredict_f = []\nold_cases = data_train.groupby('Interaction').get_group(\"N_Afghanistan\")\ntest_df = old_cases[[\"ConfirmedCases\", \"Fatalities\", \"CR_label\"]]\nfor repeat in range(1):\n    trial = test_df.iloc[-interval:]\n    input_x = trial.values.reshape(1,interval,3)\n    pd_value_c= model_c.predict(input_x).reshape(1)\n    pd_value_f= model_f.predict(input_x).reshape(1) \n    predict_c.extend(pd_value_c)\n    predict_f.extend(pd_value_f)\n    new_df = pd.DataFrame({\"ConfirmedCases\":pd_value_c, \"Fatalities\": pd_value_f})\n    new_df[\"CR_label\"] = trial.iloc[-1,2]\n    test_df = pd.concat([test_df, new_df], axis = 0)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"country_ = data_test.Interaction.unique()\nCp = []\nFp = []\nfor ct in country_:\n    predict_c = []\n    predict_f = []\n    old_cases = data_train.groupby('Interaction').get_group(ct)\n    test_df = old_cases[[\"ConfirmedCases\", \"Fatalities\",  \"CR_label\"]]\n    for repeat in range(44):\n        trial = test_df.iloc[-interval:]\n        input_x = trial.values.reshape(1,interval,3)\n        pd_value_c= model_c.predict(input_x).reshape(1)\n        pd_value_f= model_f.predict(input_x).reshape(1)\n        \n        predict_c.extend(pd_value_c)\n        predict_f.extend(pd_value_f)\n        \n        new_df = pd.DataFrame({\"ConfirmedCases\":pd_value_c, \"Fatalities\": pd_value_f})\n        new_df[\"CR_label\"] = trial.iloc[-1,2]\n        test_df = pd.concat([test_df, new_df], axis = 0)\n    Cp.extend(predict_c[0:43])\n    Fp.extend(predict_f[0:43])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission[\"ConfirmedCases\"] = Cp\nsubmission[\"Fatalities\"] = Fp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv(\"submission.csv\",index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}