{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Wed Apr 15 18:19:39 2020\n\n@author: Jaehoon Cha\n\n@email: chajaehoon79@gmail.com\n\"\"\"\nimport pandas as pd\nimport numpy as np\nimport pickle\nimport os\n\n'''\ncall data\n'''\ndef make_data():   \n    train_path = os.path.join(\"covid19-global-forecasting-week4\", \"train.csv\")\n    test_path = os.path.join(\"covid19-global-forecasting-week4\", \"test.csv\")\n    \n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n    \n    \n    train.columns.values[2] = 'Country'\n    train.columns.values[1] = 'Subplace'\n    train.fillna(\"\", inplace = True)\n    \n    test.columns.values[2] = 'Country'\n    test.columns.values[1] = 'Subplace'\n    test.fillna(\"\", inplace = True)\n    \n    with open('loc2popv4.pickle', 'rb') as f:\n        loc2pop = pickle.load(f)\n\n\n    train.reset_index(drop = True, inplace = True)    \n    \n    \n    train['Location'] = train[['Subplace', 'Country']].apply(lambda x: '_'.join(x), axis=1)\n    for i in range(len(train)):\n        if i%100 == 0:\n            print(i/len(train))\n        if train.loc[i, 'Subplace'] == \"\":\n            train.loc[i, 'Location'] = train.loc[i, 'Country']\n        elif train.loc[i, 'Subplace'] == train.loc[i, 'Country']:\n            train.loc[i, 'Location'] = train.loc[i, 'Country']\n        else:\n            train.loc[i, 'Location'] = '_'.join([train.loc[i, 'Country'], train.loc[i, 'Subplace']])\n        train.loc[i, 'Population'] = loc2pop[train.loc[i, 'Location']]\n    \n    \n    test.reset_index(drop = True, inplace = True)    \n    \n    \n    test['Location'] = test[['Subplace', 'Country']].apply(lambda x: '_'.join(x), axis=1)\n    for i in range(len(test)):\n        if i%100 == 0:\n            print(i/len(test))\n        if test.loc[i, 'Subplace'] == \"\":\n            test.loc[i, 'Location'] = test.loc[i, 'Country']\n        elif test.loc[i, 'Subplace'] == test.loc[i, 'Country']:\n            test.loc[i, 'Location'] = test.loc[i, 'Country']\n        else:\n            test.loc[i, 'Location'] = '_'.join([test.loc[i, 'Country'], test.loc[i, 'Subplace']])\n        test.loc[i, 'Population'] = loc2pop[test.loc[i, 'Location']]\n    \n    for d in [6, 7, 8, 9]:\n        train.loc[(train.Location == 'Australia_Northern Territory') & (train.Date == '2020-03-{:02}'.format(d)), 'ConfirmedCases'] = 1.0   \n    for d in [31]:\n        train.loc[(train.Location == 'Australia_Queensland') & (train.Date == '2020-01-{:02}'.format(d)), 'ConfirmedCases'] = 3.0   \n    for d in [2, 3]:\n        train.loc[(train.Location == 'Australia_Queensland') & (train.Date == '2020-02-{:02}'.format(d)), 'ConfirmedCases'] = 3.0  \n    for d in [25]:\n        train.loc[(train.Location == 'Canada_Alberta') & (train.Date == '2020-03-{:02}'.format(d)), 'ConfirmedCases'] = 433.0  \n    for d in [17]:\n        train.loc[(train.Location == 'China_Guizhou') & (train.Date == '2020-03-{:02}'.format(d)), 'ConfirmedCases'] = 146.0  \n    for d in [9, 10, 11, 12, 13, 14, 15]:\n        train.loc[(train.Location == 'France_Saint Barthelemy') & (train.Date == '2020-03-{:02}'.format(d)), 'ConfirmedCases'] = 3.0  \n    for d in [17, 18, 19, 20, 21, 22, 23, 24, 25,26]:\n        train.loc[(train.Location == 'Guyana') & (train.Date == '2020-03-{:02}'.format(d)), 'ConfirmedCases'] = 5.0  \n    for d in [14]:\n        train.loc[(train.Location == 'US_Alaska') & (train.Date == '2020-03-{:02}'.format(d)), 'ConfirmedCases'] = 1.0  \n    for d in [18]:\n        train.loc[(train.Location == 'US_Nevada') & (train.Date == '2020-03-{:02}'.format(d)), 'ConfirmedCases'] = 76.0  \n    for d in [20]:\n        train.loc[(train.Location == 'US_Utah') & (train.Date == '2020-03-{:02}'.format(d)), 'ConfirmedCases'] = 108.0  \n    for d in [20, 21]:\n        train.loc[(train.Location == 'US_Virgin Islands') & (train.Date == '2020-03-{:02}'.format(d)), 'ConfirmedCases'] = 3.0  \n    for d in [29, 30]:\n        train.loc[(train.Location == 'US_Virgin Islands') & (train.Date == '2020-03-{:02}'.format(d)), 'ConfirmedCases'] = 22.0  \n    for d in [18]:\n        train.loc[(train.Location == 'US_Washington') & (train.Date == '2020-03-{:02}'.format(d)), 'ConfirmedCases'] = 1226.0  \n    for d in [6]:\n        train.loc[(train.Location == 'Canada_Ontario') & (train.Date == '2020-04-{:02}'.format(d)), 'ConfirmedCases'] = 4540.0\n    for d in [4]:\n        train.loc[(train.Location == 'France_New Caledonia') & (train.Date == '2020-04-{:02}'.format(d)), 'ConfirmedCases'] = 18.0\n    for d in [9]:\n        train.loc[(train.Location == 'US_Arizona') & (train.Date == '2020-04-{:02}'.format(d)), 'ConfirmedCases'] = 3074.0\n    for d in [22]:\n        train.loc[(train.Location == 'US_District of Columbia') & (train.Date == '2020-03-{:02}'.format(d)), 'ConfirmedCases'] = 109.0\n    for d in [2]:\n        train.loc[(train.Location == 'US_New Hampshire') & (train.Date == '2020-04-{:02}'.format(d)), 'ConfirmedCases'] = 423.0\n    for d in [9]:\n        train.loc[(train.Location == 'US_New Mexico') & (train.Date == '2020-04-{:02}'.format(d)), 'ConfirmedCases'] = 991.0\n    for d in [1]:\n        train.loc[(train.Location == 'United Kingdom_Turks and Caicos Islands') & (train.Date == '2020-04-{:02}'.format(d)), 'ConfirmedCases'] = 5.0\n\n\n\n    for d in [21]:\n        train.loc[(train.Location == 'Canada_Quebec') & (train.Date == '2020-03-{:02}'.format(d)), 'Fatalities'] = 4.0  \n    for d in [15]:\n        train.loc[(train.Location == 'Iceland') & (train.Date == '2020-03-{:02}'.format(d)), 'Fatalities'] = 0.0  \n    for d in [20]:\n        train.loc[(train.Location == 'Iceland') & (train.Date == '2020-03-{:02}'.format(d)), 'Fatalities'] = 1.0  \n    for d in [20]:\n        train.loc[(train.Location == 'India') & (train.Date == '2020-03-{:02}'.format(d)), 'Fatalities'] = 4.0  \n    for d in [20]:\n        train.loc[(train.Location == 'Kazakhstan') & (train.Date == '2020-03-{:02}'.format(d)), 'Fatalities'] = 0.0  \n    for d in [18]:\n        train.loc[(train.Location == 'Philippines') & (train.Date == '2020-03-{:02}'.format(d)), 'Fatalities'] = 14.0  \n    for d in [18, 19, 20, 21]:\n        train.loc[(train.Location == 'Slovakia') & (train.Date == '2020-03-{:02}'.format(d)), 'Fatalities'] = 0.0  \n    for d in [24]:\n        train.loc[(train.Location == 'US_Hawaii') & (train.Date == '2020-03-{:02}'.format(d)), 'Fatalities'] = 0.0  \n    for d in [26, 27]:\n        train.loc[(train.Location == 'Serbia') & (train.Date == '2020-03-{:02}'.format(d)), 'Fatalities'] = 4.0  \n    for d in [30]:\n        train.loc[(train.Location == 'US_Virginia') & (train.Date == '2020-03-{:02}'.format(d)), 'Fatalities'] = 25.0  \n    for d in [8]:\n        train.loc[(train.Location == 'Canada_Prince Edward Island') & (train.Date == '2020-04-{:02}'.format(d)), 'Fatalities'] = 0.0  \n    for d in [5,6,7,8,9,10]:\n        train.loc[(train.Location == 'Cyprus') & (train.Date == '2020-04-{:02}'.format(d)), 'Fatalities'] = 11.0\n    for d in [6]:\n        train.loc[(train.Location == 'Finland') & (train.Date == '2020-04-{:02}'.format(d)), 'Fatalities'] = 28.0\n    for d in [4]:\n        train.loc[(train.Location == 'Kazakhstan') & (train.Date == '2020-04-{:02}'.format(d)), 'Fatalities'] = 6.0\n    for d in [7]:\n        train.loc[(train.Location == 'US_District of Columbia') & (train.Date == '2020-04-{:02}'.format(d)), 'Fatalities'] = 24.0\n    for d in [3]:\n        train.loc[(train.Location == 'US_Montana') & (train.Date == '2020-04-{:02}'.format(d)), 'Fatalities'] = 6.0\n       \n       \n    with open('train_dfv4.csv', 'wb') as f:\n        pickle.dump(train, f)\n    \n    with open('test_dfv4.csv', 'wb') as f:\n        pickle.dump(test, f)   \n\n# make_data()\n\nmother_path = \"/kaggle/input/covid19week4\"\ntrain_path = os.path.join(mother_path, \"train_dfv4.csv\")\ntest_path = os.path.join(mother_path, \"test_dfv4.csv\")\nloc_path = os.path.join(mother_path, \"loc2popv4.pickle\")\nsubmit_path = os.path.join(\"/kaggle/input/covid19-global-forecasting-week-4\", \"submission.csv\")\n\nwith open(train_path, 'rb') as f:\n    train = pickle.load(f)\n\ntrain = train[train.Location != 'Diamond Princess']\ntrain = train[train.Location != 'MS Zaandam']\n\n\nsubmit = pd.read_csv(submit_path)\n\nwith open(test_path, 'rb') as f:\n    test = pickle.load(f)    \n    \nwith open(loc_path, 'rb') as f:\n    loc2pop = pickle.load(f)\n\nLocations = train.Location.unique()\n\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense, Concatenate\nfrom tensorflow.keras.models import Model\n\ntf.random.set_seed(0)\ntf.keras.backend.set_floatx('float32')\n\n\nbatch_size = 32\ntotal_epochs = 10000\nepoch_log = 100\n'''\nmake model\n'''\n      \nclass FC(Model):\n    def __init__(self, input_dims):\n        super(FC, self).__init__()\n        self.input_dims = input_dims\n        \n        self.inputs = Input(shape=self.input_dims)\n        self.dense1 = Dense(200, activation = 'softplus', name = 'dense1')\n        self.subdense1 = Dense(50, activation = 'softplus', name = 'sub_dense1')\n        self.subdense2 = Dense(1, activation = 'softplus', name = 'super_class')\n        self.subdense3 = Dense(50, activation = 'softplus', name = 'sub_dense3')\n        \n        self.dense2 = Dense(50, activation = 'softplus', name = 'dense2')\n       \n        self.logit = Dense(1, activation ='softplus', name = 'logit')\n        \n\n    def build(self):\n        x = self.inputs\n        f = self.dense1(x)\n        s  = self.subdense1(f)\n        self.lat = self.subdense2(s)\n        s = self.subdense3(self.lat)\n        f = self.dense2(f)\n        x = Concatenate()([f, s])\n        logit = self.logit(x)\n        return tf.keras.Model(inputs=self.inputs, outputs= [logit, self.lat])\n        \n\ndef loss_fn(X, LOGITS):\n    y, s = X\n    y_logit, s_logit  = LOGITS\n    y_loss = tf.keras.losses.MeanSquaredError()(y, y_logit)\n    s_loss = tf.keras.losses.MeanSquaredError()(s, s_logit)\n    return tf.reduce_mean(y_loss + s_loss)\n\ndef allround(n):\n    if n > 0:\n        integer, decimal = int(np.log10(n)), np.log10(n) - int(np.log10(n))\n        return int(10**(decimal)) * 10**integer\n    else:\n        return 0\n\n\n\n'''\nconfirmed case\n'''\n\ndef run_confirmed():\n    dic = {}\n    for loc in Locations:\n        indexNames = train[train.Location == loc].index\n        dic[loc] = np.array(train.loc[indexNames, \"ConfirmedCases\"])\n    \n\n    #make trainset   \n    trainset = []\n    testset = []\n    \n    \n    weight_pop = 100000\n    def extract_feature(sig, pop):\n        diff = np.diff(sig)\n        diff = np.array([allround(n) for n in diff]).astype(np.float32)\n        diff_ratio = weight_pop*diff/pop\n        return diff_ratio\n        \n    def target(sig, pop):\n        diff = np.diff(sig)\n        diff = np.array([allround(n) for n in diff]).astype(np.float32)\n        diff_ratio = weight_pop*diff/pop\n        return diff_ratio[0]\n        \n      \n    period = 35\n    for i, loc in enumerate(Locations):\n        if loc in ['Korea, South', 'China_Hubei']:\n            loc_df =  dic[loc].copy()\n            pop = loc2pop[loc]\n            pop = allround(pop)\n            try:\n                k = np.where(loc_df != 0)[0][0]\n            except:\n                k = len(indexNames)-1\n            try:\n                critical_point = np.where(loc_df/pop>1e-07)[0][0]\n            except:\n                critical_point = len(indexNames)-1\n            if k + period < len(indexNames)-1:\n                while k+period < len(indexNames)-1:\n                    tmp = loc_df[k:k+period+1]\n                    diff = extract_feature(tmp[:-1], pop)\n                    y_tmp = target(tmp[-2:], pop)\n                    days = 1/(np.maximum(1, k - critical_point-14))\n                    trainset.append([diff, days, y_tmp])\n                    k+=1\n                tmp = loc_df[k:k+period+1]\n                diff = extract_feature(tmp[:-1], pop)\n                y_tmp = target(tmp[-2:], pop)\n                days = 1/(np.maximum(1, k - critical_point-14))\n                testset.append([diff, days, y_tmp])\n        \n       \n    \n    train_x = np.stack([trainset[i][0] for i in range(len(trainset))], axis = 0).astype(np.float32)\n    train_s = np.stack([trainset[i][1] for i in range(len(trainset))], axis = 0).astype(np.float32)\n    train_y = np.array([trainset[i][2] for i in range(len(trainset))]).astype(np.float32)\n    test_x = np.stack([testset[i][0] for i in range(len(testset))], axis = 0).astype(np.float32)\n    test_s = np.stack([testset[i][1] for i in range(len(testset))], axis = 0).astype(np.float32)\n    test_y = np.array([testset[i][2] for i in range(len(testset))]).astype(np.float32)\n    \n    \n    n_train_samples = len(train_x)\n    n_test_samples = len(test_x)\n    \n        \n    train_ds = tf.data.Dataset.from_tensor_slices(\n        (train_x, train_y, train_s)).shuffle(n_train_samples).batch(batch_size)\n    test_ds = tf.data.Dataset.from_tensor_slices((test_x, test_y, test_s)).batch(n_test_samples)\n\n\n\n    model= FC(train_x.shape[1:]).build()    \n                \n    compute_loss = loss_fn\n    \n    optimizer = tf.keras.optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n    \n    \n    def train_step(inputs):\n        X, Y, S = inputs\n        with tf.GradientTape() as tape:\n            logits = model(X)\n            loss = compute_loss([Y, S], logits)\n        gradients = tape.gradient(loss, model.trainable_variables)\n        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n        train_loss(loss)  \n\n    def test_step(inputs):\n        X, Y, S = inputs\n        logits = model(X)\n        loss = compute_loss([Y, S], logits)\n        test_loss(loss)\n\n    train_loss = tf.keras.metrics.Mean(name='train_loss')\n    test_loss = tf.keras.metrics.Mean(name='test_loss')\n\n    for epoch in range(total_epochs):\n        for epoch_x, epoch_y, epoch_s in train_ds:\n            inputs = [epoch_x, epoch_y, epoch_s]\n            train_step(inputs)\n\n        for epoch_x, epoch_y, epoch_s in test_ds:\n            test_step([epoch_x, epoch_y, epoch_s])\n        template = 'epoch: {}, train_loss: {}, test_loss: {}'\n        if epoch % epoch_log == 0:\n            print(template.format(epoch+1,\n                                  train_loss.result(),\n                                  test_loss.result()))    \n\n\n    Result = []    \n    for loc in Locations: \n        future = []\n        loc_df =  dic[loc].copy()\n        pop = loc2pop[loc]\n        pop = allround(pop)\n        try:\n            critical_point = np.where(loc_df/pop>1e-07)[0][0]\n        except:\n            critical_point = len(indexNames)-1\n        k = len(indexNames)-1-period\n        #Think here\n        tmp = loc_df[-period:]\n        diff = extract_feature(tmp, pop)\n        last = tmp[-1]\n        for i in range(45):            \n            days = 1/(np.maximum(1, k - critical_point-14))\n            diff = np.expand_dims(diff, 0).astype(np.float32)\n            y = model(diff)\n            y_tmp = y[0].numpy()[0][0]\n            last = int(pop*y_tmp/weight_pop) + last\n            diff = np.append(diff[0][1:],[y_tmp])\n            future.append(last)\n        Result.append([loc,tmp,future])\n        \n\n    for i in range(len(Result)):\n        tmp = np.append(Result[i][1][-13:], Result[i][2][:30])\n        loc_idx = test[test.Location == Result[i][0]].index\n        test.loc[loc_idx, 'ConfirmedCases'] = tmp     \n         \n    loc_idx = test[test.Location == 'Diamond Princess'].index\n    test.loc[loc_idx, 'ConfirmedCases'] = 712\n\n    loc_idx = test[test.Location == 'MS Zaandam'].index\n    test.loc[loc_idx, 'ConfirmedCases'] = 9\n    \n                  \n    return test   \n                \n\n    \n\ntest = run_confirmed()\n\ntf.keras.backend.clear_session()\n\n'''\nfatalities\n'''\n\ndef run_fatalities():\n    dic_fat = {}\n    for loc in Locations:\n        indexNames = train[train.Location == loc].index\n        dic_fat[loc] = np.array(train.loc[indexNames, \"Fatalities\"])\n\n    dic = {}\n    for loc in Locations:\n        indexNames = train[train.Location == loc].index\n        dic[loc] = np.array(train.loc[indexNames, \"ConfirmedCases\"])\n    \n\n    dic_for = {}\n    for loc in Locations:\n        indexNames = test[test.Location == loc].index\n        dic_for[loc] = np.array(test.loc[indexNames, \"ConfirmedCases\"])\n        \n     \n    trainset = []\n    testset = []\n    \n    weight_con = 100\n    period = 35\n\n    def extract_feature(fat, con):\n        diff = np.diff(fat)\n        con_1 = np.array([allround(n) for n in con[:-1]]).astype(np.float32)\n        ratio = diff/con_1\n        ratio[np.where(con_1 == 0)[0]] = 0\n        return weight_con*ratio\n        \n    def target(fat, con):\n        diff = np.diff(fat)\n        con_1 = np.array([allround(n) for n in con[:-1]]).astype(np.float32)\n        ratio = diff/con_1\n        ratio[np.where(con_1 == 0)[0]] = 0\n        return weight_con*ratio[0]     \n      \n        \n    for i, loc in enumerate(Locations):\n        if loc in ['Korea, South', 'China_Hubei']:\n            loc_df =  dic[loc].copy()\n            loc_fat_df = dic_fat[loc].copy()\n            pop = loc2pop[loc]\n            pop = allround(pop)\n            try:\n                k = np.where(loc_df != 0)[0][0]\n            except:\n                k = len(indexNames)-1\n            try:\n                critical_point = np.where(loc_df/pop>1e-07)[0][0]\n            except:\n                critical_point = len(indexNames)-1\n            #Think here\n            if k + period < len(indexNames)-1:\n                # print(k)\n                while k+period < len(indexNames)-1:\n                    tmp_fat = loc_fat_df[k:k+period+1]\n                    tmp = loc_df[k:k+period+1]\n                    diff = extract_feature(tmp_fat[:-1], tmp[:-1])\n                    y_tmp = target(tmp_fat[-2:],tmp[-2:])\n                    days = 1/(np.maximum(1, k - critical_point-14))\n                    trainset.append([diff, days, y_tmp])\n                    k+=1\n                tmp_fat = loc_fat_df[k:k+period+1]\n                tmp = loc_df[k:k+period+1]\n                diff = extract_feature(tmp_fat[:-1], tmp[:-1])\n                y_tmp = target(tmp_fat[-2:],tmp[-2:])\n                days = 1/(np.maximum(1, k - critical_point-14))\n                testset.append([diff, days, y_tmp])\n    \n    \n    \n    train_x = np.stack([trainset[i][0] for i in range(len(trainset))], axis = 0).astype(np.float32)\n    train_s = np.stack([trainset[i][1] for i in range(len(trainset))], axis = 0).astype(np.float32)\n    train_y = np.array([trainset[i][2] for i in range(len(trainset))]).astype(np.float32)\n    test_x = np.stack([testset[i][0] for i in range(len(testset))], axis = 0).astype(np.float32)\n    test_s = np.stack([testset[i][1] for i in range(len(testset))], axis = 0).astype(np.float32)\n    test_y = np.array([testset[i][2] for i in range(len(testset))]).astype(np.float32)\n    \n    \n    n_train_samples = len(train_x)\n    n_test_samples = len(test_x)\n    \n    \n    train_ds = tf.data.Dataset.from_tensor_slices(\n        (train_x, train_y, train_s)).shuffle(n_train_samples).batch(batch_size)\n    test_ds = tf.data.Dataset.from_tensor_slices((test_x, test_y, test_s)).batch(n_test_samples)\n    \n    model= FC(train_x.shape[1:]).build()    \n                \n    compute_loss = loss_fn\n    \n    optimizer = tf.keras.optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n    train_loss = tf.keras.metrics.Mean(name='train_loss')\n    test_loss = tf.keras.metrics.Mean(name='test_loss')\n    \n    \n    def train_step(inputs):\n        X, Y, S = inputs\n        with tf.GradientTape() as tape:\n            logits = model(X)\n            loss = compute_loss([Y, S], logits)\n        gradients = tape.gradient(loss, model.trainable_variables)\n        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n        train_loss(loss)  \n\n    def test_step(inputs):\n        X, Y, S = inputs\n        logits = model(X)\n        loss = compute_loss([Y, S], logits)\n        test_loss(loss)\n        \n        \n    for epoch in range(total_epochs):\n        for epoch_x, epoch_y, epoch_s in train_ds:\n            inputs = [epoch_x, epoch_y, epoch_s]\n            train_step(inputs)\n\n        for epoch_x, epoch_y, epoch_s in test_ds:\n            test_step([epoch_x, epoch_y, epoch_s])\n        template = 'epoch: {}, train_loss: {}, test_loss: {}'\n        if epoch % epoch_log == 0:\n            print(template.format(epoch+1,\n                                  train_loss.result(),\n                                  test_loss.result()))    \n\n\n\n    Result = []    \n    for loc in Locations: \n        future = []\n        loc_df =  dic[loc].copy()\n        loc_fat_df = dic_fat[loc].copy()\n        confromdf = dic_for[loc].copy()\n        pop = loc2pop[loc]\n        pop = allround(pop)\n        try:\n            critical_point = np.where(loc_df/pop>1e-07)[0][0]\n        except:\n            critical_point = len(indexNames)-1\n        k = len(indexNames)-1-period\n        tmp = loc_df[-period:]\n        tmp_fat = loc_fat_df[-period:]\n        diff = extract_feature(tmp_fat, tmp)\n        last_fat = tmp_fat[-1]\n        last = tmp[-1]\n        con_idx = 13\n        for i in range(30):            \n            days = 1/(np.maximum(1, k - critical_point-14))\n            diff = np.expand_dims(diff, 0).astype(np.float32)\n            y = model(diff)\n            y_tmp = y[0].numpy()[0][0]\n            last_fat = int(last*y_tmp/weight_con) + last_fat\n            diff = np.append(diff[0][1:],[y_tmp])\n            future.append(last_fat)\n            last = confromdf[con_idx]\n            con_idx += 1\n        Result.append([loc,tmp_fat,future])\n\n    for i in range(len(Result)):\n        tmp = np.append(Result[i][1][-13:], Result[i][2][:30])\n        loc_idx = test[test.Location == Result[i][0]].index\n        test.loc[loc_idx, 'Fatalities'] = tmp\n\n    loc_idx = test[test.Location == 'Diamond Princess'].index\n    test.loc[loc_idx, 'Fatalities'] = 11\n\n    loc_idx = test[test.Location == 'MS Zaandam'].index\n    test.loc[loc_idx, 'Fatalities'] = 2\n\n    return test   \n    \n                  \ntest = run_fatalities()\ntf.keras.backend.clear_session()\n\nsubmit.ConfirmedCases = test.ConfirmedCases\nsubmit.Fatalities = test.Fatalities\n\nsubmit.to_csv('/kaggle/working/submission.csv',index=False)\n\nprint('complete')\n\n\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}