{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\ntrain = pd.read_csv('../input/covid19-global-forecasting-week-4/train.csv')\ntrain['Date'] = pd.to_datetime( train['Date'] )\ntrain['Date'].max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv('../input/covid19-global-forecasting-week-4/test.csv')\ntest['Date'] = pd.to_datetime( test['Date'] )\ntest['Date'].max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def BLEND_WEEK4_1():\n    from tensorflow.keras.optimizers import Nadam\n    from sklearn.metrics import mean_squared_error\n    import tensorflow as tf\n    import tensorflow.keras.layers as KL\n    from datetime import timedelta\n    import numpy as np\n    import pandas as pd\n    import tensorflow.keras.backend as K\n\n    import os\n    pd.options.display.max_rows = 500\n    pd.options.display.max_columns = 500\n\n    import matplotlib.pyplot as plt\n    %matplotlib inline\n\n    from scipy.signal import savgol_filter\n    from sklearn.preprocessing import StandardScaler, OneHotEncoder\n    from sklearn.isotonic import IsotonicRegression\n    from sklearn.metrics import mean_squared_error, mean_squared_log_error\n    from sklearn.linear_model import LinearRegression, Ridge\n\n    import datetime\n    import gc\n    from tqdm import tqdm\n\n    import xgboost as xgb\n\n    def rmse( yt, yp ):\n        return np.sqrt( np.mean( (yt-yp)**2 ) )\n\n\n\n\n    class CovidModel:\n        def __init__(self):\n            pass\n\n        def predict_first_day(self, date):\n            return None\n\n        def predict_next_day(self, yesterday_pred_df):\n            return None\n\n\n    class CovidModelAhmet(CovidModel):\n        def preprocess(self, df, meta_df):\n            df[\"Date\"] = pd.to_datetime(df['Date'])\n\n            df = df.merge(meta_df, on=self.loc_group, how=\"left\")\n            df[\"lat\"] = (df[\"lat\"] // 30).astype(np.float32).fillna(0)\n            df[\"lon\"] = (df[\"lon\"] // 60).astype(np.float32).fillna(0)\n\n            df[\"population\"] = np.log1p(df[\"population\"]).fillna(-1)\n            df[\"area\"] = np.log1p(df[\"area\"]).fillna(-1)\n\n            for col in self.loc_group:\n                df[col].fillna(\"\", inplace=True)\n\n            df['day'] = df.Date.dt.dayofyear\n            df['geo'] = ['_'.join(x) for x in zip(df['Country_Region'], df['Province_State'])]\n            return df\n\n        def get_model(self):\n\n            def nn_block(input_layer, size, dropout_rate, activation):\n                out_layer = KL.Dense(size, activation=None)(input_layer)\n                out_layer = KL.Activation(activation)(out_layer)\n                out_layer = KL.Dropout(dropout_rate)(out_layer)\n                return out_layer\n\n            ts_inp = KL.Input(shape=(len(self.ts_features),))\n            global_inp = KL.Input(shape=(len(self.global_features),))\n\n            inp = KL.concatenate([global_inp, ts_inp])\n            hidden_layer = nn_block(inp, 64, 0.0, \"relu\")\n            gate_layer = nn_block(hidden_layer, 32, 0.0, \"sigmoid\")\n            hidden_layer = nn_block(hidden_layer, 32, 0.0, \"relu\")\n            hidden_layer = KL.multiply([hidden_layer, gate_layer])\n\n            out = KL.Dense(len(self.TARGETS), activation=\"linear\")(hidden_layer)\n\n            model = tf.keras.models.Model(inputs=[global_inp, ts_inp], outputs=out)\n            return model\n\n        def get_input(self, df):\n            return [df[self.global_features], df[self.ts_features]]\n\n        def train_models(self, df, num_models=20, save=False):\n\n            def custom_loss(y_true, y_pred):\n                return K.sum(K.sqrt(K.sum(K.square(y_true - y_pred), axis=0, keepdims=True)))/len(self.TARGETS)\n\n            models = []\n            for i in range(num_models):\n                model = self.get_model()\n                model.compile(loss=custom_loss, optimizer=Nadam(lr=1e-4))\n                hist = model.fit(self.get_input(df), df[self.TARGETS],\n                                 batch_size=2048, epochs=200, verbose=0, shuffle=True)\n                if save:\n                    model.save_weights(\"model{}.h5\".format(i))\n                models.append(model)\n            return models\n\n\n        def predict_one(self, df):\n\n            pred = np.zeros((df.shape[0], 2))\n            for model in self.models:\n                pred += model.predict(self.get_input(df))/len(self.models)\n            pred = np.maximum(pred, df[self.prev_targets].values)\n            pred[:, 0] = np.log1p(np.expm1(pred[:, 0]) + 0.1)\n            pred[:, 1] = np.log1p(np.expm1(pred[:, 1]) + 0.01)\n            return np.clip(pred, None, 15)\n\n\n        def __init__(self):\n            df = pd.read_csv(\"../input/covid19-global-forecasting-week-4/train.csv\")\n            sub_df = pd.read_csv(\"../input/covid19-global-forecasting-week-4/test.csv\")\n\n            meta_df = pd.read_csv(\"../input/covid19-forecasting-metadata/region_metadata.csv\")\n\n            self.loc_group = [\"Province_State\", \"Country_Region\"]\n\n            df = self.preprocess(df, meta_df)\n            sub_df = self.preprocess(sub_df, meta_df)\n\n            df = df.merge(sub_df[[\"ForecastId\", \"Date\", \"geo\"]], how=\"left\", on=[\"Date\", \"geo\"])\n            df = df.append(sub_df[sub_df[\"Date\"] > df[\"Date\"].max()], sort=False)\n\n            df[\"day\"] = df[\"day\"] - df[\"day\"].min()\n\n            self.TARGETS = [\"ConfirmedCases\", \"Fatalities\"]\n            self.prev_targets = ['prev_ConfirmedCases_1', 'prev_Fatalities_1']\n\n            for col in self.TARGETS:\n                df[col] = np.log1p(df[col])\n\n            self.NUM_SHIFT = 7\n\n            self.global_features = [\"lat\", \"lon\", \"population\", \"area\"]\n            self.ts_features = []\n\n            for s in range(1, self.NUM_SHIFT+1):\n                for col in self.TARGETS:\n                    df[\"prev_{}_{}\".format(col, s)] = df.groupby(self.loc_group)[col].shift(s)\n                    self.ts_features.append(\"prev_{}_{}\".format(col, s))\n\n            self.df = df[df[\"Date\"] >= df[\"Date\"].min() + timedelta(days=self.NUM_SHIFT)].copy()\n\n\n        def predict_first_day(self, day):\n            self.models = self.train_models(self.df[self.df[\"day\"] < day])\n\n            temp_df = self.df.loc[self.df[\"day\"] == day].copy()\n            y_pred = self.predict_one(temp_df)\n\n            self.y_prevs = [None]*self.NUM_SHIFT\n\n            for i in range(1, self.NUM_SHIFT):\n                self.y_prevs[i] = temp_df[['prev_ConfirmedCases_{}'.format(i), 'prev_Fatalities_{}'.format(i)]].values\n\n            temp_df[self.TARGETS] = y_pred\n            self.day = day\n            return temp_df[[\"geo\", \"day\"] + self.TARGETS]\n\n\n        def predict_next_day(self, yesterday_pred_df):\n            self.day = self.day + 1\n\n            temp_df = self.df.loc[self.df[\"day\"] == self.day].copy()\n\n            yesterday_pred_df = temp_df[[\"geo\"]].merge(yesterday_pred_df[[\"geo\"] + self.TARGETS], on=\"geo\", how=\"left\")\n            temp_df[self.prev_targets] = yesterday_pred_df[self.TARGETS].values\n\n            for i in range(2, self.NUM_SHIFT+1):\n                temp_df[['prev_ConfirmedCases_{}'.format(i), 'prev_Fatalities_{}'.format(i)]] = self.y_prevs[i-1]\n\n            y_pred, self.y_prevs = self.predict_one(temp_df), [None, temp_df[self.prev_targets].values] + self.y_prevs[1:-1]\n\n            temp_df[self.TARGETS] = y_pred\n            return temp_df[[\"geo\", \"day\"] + self.TARGETS]\n\n\n    class CovidModelCPMP(CovidModel):\n\n        def __init__(self):\n            train = pd.read_csv('../input/covid19-global-forecasting-week-4/train.csv')\n            train['Province_State'].fillna('', inplace=True)\n            train['Date'] = pd.to_datetime(train['Date'])\n            train['day'] = train.Date.dt.dayofyear\n            train['geo'] = ['_'.join(x) for x in zip(train['Country_Region'], train['Province_State'])]\n            test = pd.read_csv('../input/covid19-global-forecasting-week-4/test.csv')\n            test['Province_State'].fillna('', inplace=True)\n            test['Date'] = pd.to_datetime(test['Date'])\n            test['day'] = test.Date.dt.dayofyear\n            test['geo'] = ['_'.join(x) for x in zip(test['Country_Region'], test['Province_State'])]\n            day_min = train['day'].min()\n            train['day'] -= day_min\n            test['day'] -= day_min  \n            self.min_test_val_day = test.day.min()\n            self.max_test_val_day = train.day.max()\n            self.max_test_day = test.day.max()\n\n            train['ForecastId'] = -1\n            test['Id'] = -1\n            test['ConfirmedCases'] = 0\n            test['Fatalities'] = 0    \n            data = pd.concat([train,\n                      test[test.day > self.max_test_val_day][train.columns]\n                     ]).reset_index(drop=True)\n            self.data = data\n            self.train = train\n            self.test = test\n            self.dates = data[data['geo'] == 'France_'].Date.values\n            region_meta = pd.read_csv('../input/covid19-forecasting-metadata/region_metadata.csv')\n            region_meta['Province_State'].fillna('', inplace=True)\n            region_meta['geo'] = ['_'.join(x) for x in zip(region_meta['Country_Region'], region_meta['Province_State'], )]\n            population = data[['geo']].merge(region_meta, how='left', on='geo').fillna(0)\n            population = population.groupby('geo')[['population']].first()\n            population['population'] = np.log1p(population['population'])\n            self.population = population[['population']].values\n            continents = region_meta['continent']\n            continents = pd.factorize(continents)[0]\n            continents_ids_base = continents.reshape((-1, 1))\n            ohe = OneHotEncoder(sparse=False)\n            self.continents_ids_base = ohe.fit_transform(continents_ids_base)\n\n            self.geo_data = data.pivot(index='geo', columns='day', values='ForecastId')\n            self.num_geo = self.geo_data.shape[0]\n            self.ConfirmedCases = data.pivot(index='geo', columns='day', values='ConfirmedCases')\n            self.Fatalities = data.pivot(index='geo', columns='day', values='Fatalities')\n            self.cases = np.log1p(self.ConfirmedCases.values)\n            self.deaths = np.log1p(self.Fatalities.values)\n            self.case_threshold = 30\n\n            self.c_case = 10\n            self.t_case = 100\n            self.c_death = 10\n            self.t_death = 5\n\n            time_cases = self.c_case * (self.cases >= np.log1p(self.t_case)) \n            time_cases = np.cumsum(time_cases, axis=1)\n            self.time_cases = 1 * np.log1p(time_cases) \n\n            time_deaths = self.c_death * (self.deaths >= np.log1p(self.t_death))\n            time_deaths = np.cumsum(time_deaths, axis=1)\n            self.time_deaths = 1 *np.log1p(time_deaths) \n\n            countries = [g.split('_')[0] for g in self.geo_data.index]\n            countries = pd.factorize(countries)[0]\n            country_ids_base = countries.reshape((-1, 1))\n            ohe = OneHotEncoder(sparse=False)\n            self.country_ids_base = 0.2 * ohe.fit_transform(country_ids_base)\n\n            self.start_lag_death = 13\n            self.end_lag_death = 5\n            self.num_train = 5\n            self.num_lag_case = 14\n            self.lag_period = max(self.start_lag_death, self.num_lag_case)\n\n            # For tetsing purpose       \n            self.df = train[['geo', 'day', 'ConfirmedCases', 'Fatalities']].copy()\n            self.df.ConfirmedCases = np.log1p(self.df.ConfirmedCases)\n            self.df.Fatalities = np.log1p(self.df.Fatalities)\n\n        def get_country_ids(self):\n            countries = [g.split('_')[0] for g in self.geo_data.index]\n            countries = pd.factorize(countries)[0]\n            countries[self.cases[:, :self.last_train+1].max(axis=1) < np.log1p(self.case_threshold)] = -1\n            countries = pd.factorize(countries)[0]\n\n\n            country_ids_base = countries.reshape((-1, 1))\n            ohe = OneHotEncoder(sparse=False)\n            country_ids_base = 0.2 * ohe.fit_transform(country_ids_base)\n            return country_ids_base\n\n        def val_score(self, true, pred):\n            return np.sqrt(mean_squared_error(true.ravel(), pred.ravel()))\n\n        def get_dataset(self, start_pred, num_train):\n            days = np.arange( start_pred - num_train + 1, start_pred + 1)\n            lag_cases = np.vstack([self.cases[:, d - self.lag_period : d] for d in days])\n            lag_deaths = np.vstack([self.deaths[:, d - self.lag_period : d] for d in days])\n            target_cases = np.vstack([self.cases[:, d : d + 1] for d in days])\n            target_deaths = np.vstack([self.deaths[:, d : d + 1] for d in days])\n            continents_ids = np.vstack([self.continents_ids_base for d in days])\n            country_ids = np.vstack([self.country_ids_base for d in days])\n            population = np.vstack([self.population for d in days])\n            time_case = np.vstack([self.time_cases[:, d - 1: d ] for d in days])\n            time_death = np.vstack([self.time_deaths[:, d - 1 : d ] for d in days])\n            return (lag_cases, lag_deaths, target_cases, target_deaths, \n                continents_ids, country_ids, population, time_case, time_death, days)\n\n        def update_time(self, time_death, time_case, pred_death, pred_case):\n            new_time_death = np.expm1(time_death) + self.c_death * (pred_death >= np.log1p(self.t_death))\n            new_time_death = 1 *np.log1p(new_time_death) \n            new_time_case = np.expm1(time_case) + self.c_case * (pred_case >= np.log1p(self.t_case))\n            new_time_case = 1 *np.log1p(new_time_case) \n            return new_time_death, new_time_case\n\n        def update_valid_dataset(self, dataset, pred_death, pred_case, pred_day):\n            (lag_cases, lag_deaths, target_cases, target_deaths, \n             continents_ids, country_ids, population, time_case, time_death, days) = dataset\n            if pred_day != days[-1]:\n                print('error', pred_day, days[-1])\n                return None\n            day = days[-1] + 1\n            new_lag_cases = np.hstack([lag_cases[:, 1:], pred_case])\n            new_lag_deaths = np.hstack([lag_deaths[:, 1:], pred_death]) \n            new_target_cases = self.cases[:, day:day+1]\n            new_target_deaths = self.deaths[:, day:day+1] \n            new_continents_ids = continents_ids  \n            new_country_ids = country_ids  \n            new_population = population  \n            new_time_death, new_time_case = self.update_time(time_death, time_case, pred_death, pred_case)\n            new_days = 1 + days\n            return (new_lag_cases, new_lag_deaths, new_target_cases, new_target_deaths, \n                new_continents_ids, new_country_ids, new_population, \n                    new_time_case, new_time_death, new_days)\n\n        def fit_eval(self, dataset, fit):\n            (lag_cases, lag_deaths, target_cases, target_deaths, \n             continents_ids, country_ids, population, \n             time_case, time_death, days) = dataset\n\n            X_death = np.hstack([lag_cases[:, -self.start_lag_death:-self.end_lag_death], \n                                 lag_deaths[:, -self.num_lag_case:], \n                                 country_ids,\n                                 #continents_ids,\n                                  population,\n                                 time_case,\n                                 time_death,\n                                ])\n            y_death = target_deaths\n            y_death_prev = lag_deaths[:, -1:]\n            if fit:\n                 self.lr_death.fit(X_death, y_death)\n            y_pred_death = self.lr_death.predict(X_death)\n            y_pred_death = np.maximum(y_pred_death, y_death_prev)\n\n            X_case = np.hstack([lag_cases[:, -self.num_lag_case:], \n                                country_ids, \n                                #continents_ids,\n                                population,\n                                 time_case,\n                                 #time_death,\n                               ])\n            y_case = target_cases\n            y_case_prev = lag_cases[:, -1:]\n            if fit:\n                self.lr_case.fit(X_case, y_case)\n            y_pred_case = self.lr_case.predict(X_case)\n            y_pred_case = np.maximum(y_pred_case, y_case_prev)\n\n            return y_pred_death, y_pred_case\n\n        def get_pred_df(self, val_death_preds, val_case_preds, ):\n            pred_deaths = self.Fatalities.iloc[:, self.start_val:self.start_val+self.num_val].copy()\n            #pred_deaths.iloc[:, :] = np.expm1(val_death_preds)\n            pred_deaths.iloc[:, :] = val_death_preds\n            pred_deaths = pred_deaths.stack().reset_index()\n            pred_deaths.columns = ['geo', 'day', 'Fatalities']\n            pred_deaths\n\n            pred_cases = self.ConfirmedCases.iloc[:, self.start_val:self.start_val+self.num_val].copy()\n            #pred_cases.iloc[:, :] = np.expm1(val_case_preds)\n            pred_cases.iloc[:, :] = val_case_preds\n            pred_cases = pred_cases.stack().reset_index()\n            pred_cases.columns = ['geo', 'day', 'ConfirmedCases']\n            pred_cases\n\n            sub = self.data[['geo', 'day']]\n            sub = sub[sub.day == self.start_val]\n            sub = sub.merge(pred_cases, how='left', on=['geo', 'day'])\n            sub = sub.merge(pred_deaths, how='left', on=['geo', 'day'])\n            sub = sub[(sub.day >= self.start_val) & (sub.day <= self.end_val)]\n            return sub\n\n        def predict_first_day(self, day):\n            self.start_val = day\n            self.end_val = day + 1\n            self.num_val = self.end_val - self.start_val + 1\n            score = True\n            self.last_train = self.start_val - 1\n            print(self.dates[self.last_train], self.start_val, self.num_val)\n            self.country_ids_base = self.get_country_ids()\n            train_data = self.get_dataset(self.last_train, self.num_train)\n            alpha = 3\n            self.lr_death = Ridge(alpha=alpha, fit_intercept=True)\n            self.lr_case = Ridge(alpha=alpha, fit_intercept=True)\n            _ = self.fit_eval(train_data, fit=True)\n\n            self.valid_data = self.get_dataset(self.start_val, 1)\n            val_death_preds, val_case_preds = self.fit_eval(self.valid_data, fit=False)\n            df = self.get_pred_df(val_death_preds, val_case_preds)\n            return df\n\n        def predict_next_day(self, yesterday_pred_df):\n            yesterday_pred_df = yesterday_pred_df.sort_values(by='geo').reset_index(drop=True)\n            if yesterday_pred_df.day.nunique() != 1:\n                print('error', yesterday_pred_df.day.unique())\n                return None\n            pred_death = yesterday_pred_df[['Fatalities']].values\n            pred_case = yesterday_pred_df[['ConfirmedCases']].values\n            pred_day = yesterday_pred_df.day.unique()[0]\n\n            new_valid_data = self. update_valid_dataset(self.valid_data, \n                                                        pred_death, pred_case, pred_day)\n            if len(new_valid_data) > 0:\n                self.valid_data = new_valid_data\n            self.start_val = pred_day + 1\n            self.end_val = pred_day + 2\n            val_death_preds, val_case_preds = self.fit_eval(self.valid_data, fit=False)\n            df = self.get_pred_df(val_death_preds, val_case_preds)\n\n            return df\n\n\n\n    class CovidModel:\n        def __init__(self):\n            pass\n\n        def predict_first_day(self, date):\n            return None\n\n        def predict_next_day(self, yesterday_pred_df):\n            return None\n\n\n    class CovidModelGIBA(CovidModel):\n        def __init__(self, lag=1, seed=1 ):\n\n            self.lag  = lag\n            self.seed = seed\n            print( 'Lag:', lag, 'Seed:', seed )\n\n            train = pd.read_csv('../input/covid19-global-forecasting-week-4/train.csv')\n            train['Date'] = pd.to_datetime( train['Date'] )\n            self.maxdate  = str(train['Date'].max())[:10]\n            self.testdate = str( train['Date'].max() + pd.Timedelta(days=1) )[:10]\n            print( 'Last Date in Train:',self.maxdate, 'Test first Date:',self.testdate )\n            train['Province_State'].fillna('', inplace=True)\n            train['day'] = train.Date.dt.dayofyear\n            self.day_min = train['day'].min()\n            train['day'] -= self.day_min\n            train['geo'] = ['_'.join(x) for x in zip(train['Country_Region'], train['Province_State'])]\n\n            test  = pd.read_csv('../input/covid19-global-forecasting-week-4/test.csv')\n            test['Date'] = pd.to_datetime( test['Date'] )\n            test['Province_State'].fillna('', inplace=True)\n            test['day'] = test.Date.dt.dayofyear\n            test['day'] -= self.day_min\n            test['geo'] = ['_'.join(x) for x in zip(test['Country_Region'], test['Province_State'])]\n            test['Id'] = -1\n            test['ConfirmedCases'] = 0\n            test['Fatalities'] = 0\n\n            self.trainmaxday  = train['day'].max()\n            self.testday1 = train['day'].max() + 1\n            self.testdayN = test['day'].max()\n\n            publictest = test.loc[ test.Date > train.Date.max() ].copy()\n            train = pd.concat( (train, publictest ), sort=False )\n            train.sort_values( ['Country_Region','Province_State','Date'], inplace=True )\n            train = train.reset_index(drop=True)\n\n            train['ForecastId'] = pd.merge( train, test, on=['Country_Region','Province_State','Date'], how='left' )['ForecastId_y'].values\n\n            train['cid'] = train['Country_Region'] + '_' + train['Province_State']\n\n            train['log0'] = np.log1p( train['ConfirmedCases'] )\n            train['log1'] = np.log1p( train['Fatalities'] )\n\n            train = train.loc[ (train.log0 > 0) | (train.ForecastId.notnull()) | (train.Date >= '2020-03-17') ].copy()\n            train = train.reset_index(drop=True)\n\n            train['days_since_1case'] = train.groupby('cid')['Id'].cumcount()\n\n            dt = pd.read_csv('../input/covid19-lockdown-dates-by-country/countryLockdowndates.csv')\n            dt.columns = ['Country_Region','Province_State','Date','Type','Reference']\n            dt = dt.loc[ dt.Date == dt.Date ]\n            dt['Province_State'] = dt['Province_State'].fillna('')\n            dt['Date'] = pd.to_datetime( dt['Date'] )\n            dt['Date'] = dt['Date'] + pd.Timedelta(days=8)\n            dt['Type'] = pd.factorize( dt['Type'] )[0]\n            dt['cid'] = dt['Country_Region'] + '_' + dt['Province_State']\n            del dt['Reference'], dt['Country_Region'], dt['Province_State']\n            train = pd.merge( train, dt, on=['cid','Date'], how='left' )\n            train['Type'] = train.groupby('cid')['Type'].fillna( method='ffill' )\n\n            train['target0'] = np.log1p( train['ConfirmedCases'] )\n            train['target1'] = np.log1p( train['Fatalities'] )\n            # dt = pd.read_csv('../input/covid19-country-data-wk3-release/Data Join - RELEASE.csv')\n            # dt['Province_State'] = dt['Province_State'].fillna('')\n            # dt['Country_Region'] = dt['Country_Region'].fillna('')\n            # train = pd.merge( train, dt, on=['Country_Region','Province_State'], how='left' )\n            # #Fix\n\n            #print( train.head(4) )\n            #print( train.shape ) \n\n            self.train = train.copy()\n\n\n        def create_features( self, df, valid_day ):\n\n            #df = df.loc[ df.day<=valid_day ].copy()\n            df = df.loc[ df.day>=(valid_day-50) ].copy()\n\n            df['lag0_1'] = df.groupby('cid')['target0'].shift(self.lag)\n            df['lag0_1'] = df.groupby('cid')['lag0_1'].fillna( method='bfill' )\n\n            df['lag0_8'] = df.groupby('cid')['target0'].shift(8)\n            df['lag0_8'] = df.groupby('cid')['lag0_8'].fillna( method='bfill' )\n\n            df['lag1_1'] = df.groupby('cid')['target1'].shift(self.lag)\n            df['lag1_1'] = df.groupby('cid')['lag1_1'].fillna( method='bfill' )\n\n            df['m0'] = df.groupby('cid')['lag0_1'].rolling(2).mean().values\n            df['m1'] = df.groupby('cid')['lag0_1'].rolling(3).mean().values\n            df['m2'] = df.groupby('cid')['lag0_1'].rolling(4).mean().values\n            df['m3'] = df.groupby('cid')['lag0_1'].rolling(5).mean().values\n            df['m4'] = df.groupby('cid')['lag0_1'].rolling(7).mean().values\n            df['m5'] = df.groupby('cid')['lag0_1'].rolling(10).mean().values\n            df['m6'] = df.groupby('cid')['lag0_1'].rolling(12).mean().values\n            df['m7'] = df.groupby('cid')['lag0_1'].rolling(16).mean().values\n            df['m8'] = df.groupby('cid')['lag0_1'].rolling(20).mean().values\n            df['m9'] = df.groupby('cid')['lag0_1'].rolling(25).mean().values\n\n            df['n0'] = df.groupby('cid')['lag1_1'].rolling(2).mean().values\n            df['n1'] = df.groupby('cid')['lag1_1'].rolling(3).mean().values\n            df['n2'] = df.groupby('cid')['lag1_1'].rolling(4).mean().values\n            df['n3'] = df.groupby('cid')['lag1_1'].rolling(5).mean().values\n            df['n4'] = df.groupby('cid')['lag1_1'].rolling(7).mean().values\n            df['n5'] = df.groupby('cid')['lag1_1'].rolling(10).mean().values\n            df['n6'] = df.groupby('cid')['lag1_1'].rolling(12).mean().values\n            df['n7'] = df.groupby('cid')['lag1_1'].rolling(16).mean().values\n            df['n8'] = df.groupby('cid')['lag1_1'].rolling(20).mean().values\n\n\n            df['m0'] = df.groupby('cid')['m0'].fillna( method='bfill' )\n            df['m1'] = df.groupby('cid')['m1'].fillna( method='bfill' )\n            df['m2'] = df.groupby('cid')['m2'].fillna( method='bfill' )\n            df['m3'] = df.groupby('cid')['m3'].fillna( method='bfill' )\n            df['m4'] = df.groupby('cid')['m4'].fillna( method='bfill' )\n            df['m5'] = df.groupby('cid')['m5'].fillna( method='bfill' )\n            df['m6'] = df.groupby('cid')['m6'].fillna( method='bfill' )\n            df['m7'] = df.groupby('cid')['m7'].fillna( method='bfill' )\n            df['m8'] = df.groupby('cid')['m8'].fillna( method='bfill' )\n            df['m9'] = df.groupby('cid')['m9'].fillna( method='bfill' )\n\n            df['n0'] = df.groupby('cid')['n0'].fillna( method='bfill' )\n            df['n1'] = df.groupby('cid')['n1'].fillna( method='bfill' )\n            df['n2'] = df.groupby('cid')['n2'].fillna( method='bfill' )\n            df['n3'] = df.groupby('cid')['n3'].fillna( method='bfill' )\n            df['n4'] = df.groupby('cid')['n4'].fillna( method='bfill' )\n            df['n5'] = df.groupby('cid')['n5'].fillna( method='bfill' )\n            df['n6'] = df.groupby('cid')['n6'].fillna( method='bfill' )\n            df['n7'] = df.groupby('cid')['n7'].fillna( method='bfill' )\n            df['n8'] = df.groupby('cid')['n8'].fillna( method='bfill' )\n\n            df['flag_China'] = 1*(df['Country_Region'] == 'China')\n            #df['flag_Italy'] = 1*(df['Country_Region'] == 'Italy')\n            #df['flag_Spain'] = 1*(df['Country_Region'] == 'Spain')\n            df['flag_US']    = 1*(df['Country_Region'] == 'US')\n            #df['flag_Brazil']= 1*(df['Country_Region'] == 'Brazil')\n\n            df['flag_Kosovo_']   = 1*(df['cid'] == 'Kosovo_')\n            df['flag_Korea']     = 1*(df['cid'] == 'Korea, South_')\n            df['flag_Nepal_']    = 1*(df['cid'] == 'Nepal_')\n            df['flag_Holy See_'] = 1*(df['cid'] == 'Holy See_')\n            df['flag_Suriname_'] = 1*(df['cid'] == 'Suriname_')\n            df['flag_Ghana_']    = 1*(df['cid'] == 'Ghana_')\n            df['flag_Togo_']     = 1*(df['cid'] == 'Togo_')\n            df['flag_Malaysia_'] = 1*(df['cid'] == 'Malaysia_')\n            df['flag_US_Rhode']  = 1*(df['cid'] == 'US_Rhode Island')\n            df['flag_Bolivia_']  = 1*(df['cid'] == 'Bolivia_')\n            df['flag_China_Tib'] = 1*(df['cid'] == 'China_Tibet')\n            df['flag_Bahrain_']  = 1*(df['cid'] == 'Bahrain_')\n            df['flag_Honduras_'] = 1*(df['cid'] == 'Honduras_')\n            df['flag_Bangladesh']= 1*(df['cid'] == 'Bangladesh_')\n            df['flag_Paraguay_'] = 1*(df['cid'] == 'Paraguay_')\n\n            tr = df.loc[ df.day  < valid_day ].copy()\n            vl = df.loc[ df.day == valid_day ].copy()\n\n            tr = tr.loc[ tr.lag0_1 > 0 ].copy()\n\n            maptarget0 = tr.groupby('cid')['target0'].agg( log0_max='max' ).reset_index()\n            maptarget1 = tr.groupby('cid')['target1'].agg( log1_max='max' ).reset_index()\n            vl['log0_max'] = pd.merge( vl, maptarget0, on='cid' , how='left' )['log0_max'].values\n            vl['log1_max'] = pd.merge( vl, maptarget1, on='cid' , how='left' )['log1_max'].values\n            vl['log0_max'] = vl['log0_max'].fillna(0)\n            vl['log1_max'] = vl['log1_max'].fillna(0)\n\n            return tr, vl\n\n\n        def train_models(self, valid_day = 10 ):\n\n            train = self.train.copy()\n\n            #Fix some anomalities:\n            train.loc[ (train.cid=='China_Guizhou') & (train.Date=='2020-03-17') , 'target0' ] = np.log1p( 146 )\n            train.loc[ (train.cid=='Guyana_')&(train.Date>='2020-03-22')&(train.Date<='2020-03-30') , 'target0' ] = np.log1p( 12 )\n            train.loc[ (train.cid=='US_Virgin Islands')&(train.Date>='2020-03-29')&(train.Date<='2020-03-29') , 'target0' ] = np.log1p( 24 )\n            train.loc[ (train.cid=='US_Virgin Islands')&(train.Date>='2020-03-30')&(train.Date<='2020-03-30') , 'target0' ] = np.log1p( 27 )\n\n            train.loc[ (train.cid=='Iceland_')&(train.Date>='2020-03-15')&(train.Date<='2020-03-15') , 'target1' ] = np.log1p( 0 )\n            train.loc[ (train.cid=='Kazakhstan_')&(train.Date>='2020-03-20')&(train.Date<='2020-03-20') , 'target1' ] = np.log1p( 0 )\n            train.loc[ (train.cid=='Serbia_')&(train.Date>='2020-03-26')&(train.Date<='2020-03-26') , 'target1' ] = np.log1p( 5 )\n            train.loc[ (train.cid=='Serbia_')&(train.Date>='2020-03-27')&(train.Date<='2020-03-27') , 'target1' ] = np.log1p( 6 )\n            train.loc[ (train.cid=='Slovakia_')&(train.Date>='2020-03-22')&(train.Date<='2020-03-31') , 'target1' ] = np.log1p( 1 )\n            train.loc[ (train.cid=='US_Hawaii')&(train.Date>='2020-03-25')&(train.Date<='2020-03-31') , 'target1' ] = np.log1p( 1 )\n\n            param = {\n                'subsample': 0.950,\n                'colsample_bytree': 0.85,\n                'max_depth': 6,\n                'gamma': 0.000,\n                'learning_rate': 0.010,\n                'min_child_weight': 5.00,\n                'reg_alpha': 0.000,\n                'reg_lambda': 0.400,\n                'silent':1,\n                'objective':'reg:squarederror',\n                #'booster':'dart',\n                #'tree_method': 'gpu_hist',\n                'nthread': 12,#-1,\n                'seed': self.seed\n                }    \n\n            tr, vl = self.create_features( train.copy(), valid_day )\n            #Features for Cases\n            features = [f for f in tr.columns if f not in [\n                #'flag_China','flag_US',\n                #'flag_Kosovo_','flag_Korea','flag_Nepal_','flag_Holy See_','flag_Suriname_','flag_Ghana_','flag_Togo_','flag_Malaysia_','flag_US_Rhode','flag_Bolivia_','flag_China_Tib','flag_Bahrain_','flag_Honduras_','flag_Bangladesh','flag_Paraguay_',\n                'lag0_8',\n                'Id','ConfirmedCases','Fatalities','log0','log1','target0','target1','ypred0','ypred1','Province_State','Country_Region','Date','ForecastId','cid','geo','day',\n                'GDP_region','TRUE POPULATION','pct_in_largest_city',' TFR ',' Avg_age ','latitude','longitude','abs_latitude','temperature', 'humidity',\n                'Personality_pdi','Personality_idv','Personality_mas','Personality_uai','Personality_ltowvs','Personality_assertive','personality_perform','personality_agreeableness',\n                'murder','High_rises','max_high_rises','AIR_CITIES','AIR_AVG','continent_gdp_pc','continent_happiness','continent_generosity','continent_corruption','continent_Life_expectancy'        \n            ] ]\n            self.features0 = features\n            #Features for Fatalities\n            features = [f for f in tr.columns if f not in [\n                'm0','m1','m2','m3',\n                #'flag_China','flag_US',\n                #'flag_Kosovo_','flag_Korea','flag_Nepal_','flag_Holy See_','flag_Suriname_','flag_Ghana_','flag_Togo_','flag_Malaysia_','flag_US_Rhode','flag_Bolivia_','flag_China_Tib','flag_Bahrain_','flag_Honduras_','flag_Bangladesh','flag_Paraguay_',\n                'Id','ConfirmedCases','Fatalities','log0','log1','target0','target1','ypred0','ypred1','Province_State','Country_Region','Date','ForecastId','cid','geo','day',\n                'GDP_region','TRUE POPULATION','pct_in_largest_city',' TFR ',' Avg_age ','latitude','longitude','abs_latitude','temperature', 'humidity',\n                'Personality_pdi','Personality_idv','Personality_mas','Personality_uai','Personality_ltowvs','Personality_assertive','personality_perform','personality_agreeableness',\n                'murder','High_rises','max_high_rises','AIR_CITIES','AIR_AVG','continent_gdp_pc','continent_happiness','continent_generosity','continent_corruption','continent_Life_expectancy'        \n            ] ]\n            self.features1 = features\n\n\n            nrounds0 = 680\n            nrounds1 = 630\n             #lag 1###############################################################\n            dtrain = xgb.DMatrix( tr[self.features0], tr['target0'] )\n            param['seed'] = self.seed\n            self.model0 = xgb.train( param, dtrain, nrounds0, verbose_eval=0 )\n            param['seed'] = self.seed+1\n            self.model1 = xgb.train( param, dtrain, nrounds0, verbose_eval=0 )\n\n            dtrain = xgb.DMatrix( tr[self.features1], tr['target1'] )\n            param['seed'] = self.seed\n            self.model2 = xgb.train( param, dtrain, nrounds1, verbose_eval=0 ) \n            param['seed'] = self.seed+1\n            self.model3 = xgb.train( param, dtrain, nrounds1, verbose_eval=0 )\n\n            self.vl = vl\n\n            return 1\n\n\n        def predict_first_day(self, day ):\n\n            self.day = day\n            self.train_models( day )\n\n            dvalid = xgb.DMatrix( self.vl[self.features0] )\n            ypred0 = ( self.model0.predict( dvalid ) + self.model1.predict( dvalid )  ) / 2\n            dvalid = xgb.DMatrix( self.vl[self.features1] )\n            ypred1 = ( self.model2.predict( dvalid ) + self.model3.predict( dvalid )  ) / 2\n\n            self.vl['ypred0'] = ypred0\n            self.vl['ypred1'] = ypred1\n            self.vl.loc[ self.vl.ypred0<self.vl.log0_max, 'ypred0'] =  self.vl.loc[ self.vl.ypred0<self.vl.log0_max, 'log0_max']\n            self.vl.loc[ self.vl.ypred1<self.vl.log1_max, 'ypred1'] =  self.vl.loc[ self.vl.ypred1<self.vl.log1_max, 'log1_max']\n\n            VALID = self.vl[[\"geo\", \"day\", 'ypred0', 'ypred1']].copy()\n            VALID.columns = [\"geo\", \"day\", 'ConfirmedCases', 'Fatalities']        \n            return VALID.reset_index(drop=True)\n\n\n        def predict_next_day(self, yesterday ):\n\n            self.day += 1\n\n            feats = ['geo','day']        \n            self.train[ 'ypred0' ] = pd.merge( self.train[feats], yesterday[feats+['ConfirmedCases']], on=feats, how='left' )['ConfirmedCases'].values\n            self.train.loc[ self.train.ypred0.notnull(), 'target0'] = self.train.loc[ self.train.ypred0.notnull() , 'ypred0']\n\n            self.train[ 'ypred1' ] = pd.merge( self.train[feats], yesterday[feats+['Fatalities']], on=feats, how='left' )['Fatalities'].values\n            self.train.loc[ self.train.ypred1.notnull(), 'target1'] = self.train.loc[ self.train.ypred1.notnull() , 'ypred1']\n            del self.train['ypred0'], self.train['ypred1']\n\n            tr, vl = self.create_features( self.train.copy(), self.day )        \n            dvalid = xgb.DMatrix( vl[self.features0] )\n            ypred0 = (self.model0.predict( dvalid ) + self.model1.predict( dvalid ) )/2\n            dvalid = xgb.DMatrix( vl[self.features1] )\n            ypred1 = (self.model2.predict( dvalid ) + self.model3.predict( dvalid ) )/2\n\n            vl['ypred0'] = ypred0\n            vl['ypred1'] = ypred1\n            vl.loc[ vl.ypred0<vl.log0_max, 'ypred0'] =  vl.loc[ vl.ypred0<vl.log0_max, 'log0_max']\n            vl.loc[ vl.ypred1<vl.log1_max, 'ypred1'] =  vl.loc[ vl.ypred1<vl.log1_max, 'log1_max']\n\n            self.vl = vl\n            VALID = vl[[\"geo\", \"day\", 'ypred0', 'ypred1']].copy()\n            VALID.columns = [\"geo\", \"day\", 'ConfirmedCases', 'Fatalities']        \n            return VALID.reset_index(drop=True)\n\n\n    TARGETS = [\"ConfirmedCases\", \"Fatalities\"]\n\n\n    def rmse(y_true, y_pred):\n        return np.sqrt(mean_squared_error(y_true, y_pred))\n\n    df = pd.read_csv(\"../input/covid19-global-forecasting-week-4/train.csv\")\n    df[TARGETS] = np.log1p(df[TARGETS].values)\n    sub_df = pd.read_csv(\"../input/covid19-global-forecasting-week-4/test.csv\")\n\n    def preprocess(df):\n        for col in [\"Country_Region\", \"Province_State\"]:\n            df[col].fillna(\"\", inplace=True)\n\n        df[\"Date\"] = pd.to_datetime(df['Date'])\n        df['day'] = df.Date.dt.dayofyear\n        df['geo'] = ['_'.join(x) for x in zip(df['Country_Region'], df['Province_State'])]\n        return df\n\n    df = preprocess(df)\n    sub_df = preprocess(sub_df)\n\n    sub_df[\"day\"] -= df[\"day\"].min()\n    df[\"day\"] -= df[\"day\"].min()\n\n\n\n    TEST_FIRST = sub_df[sub_df[\"Date\"] > df[\"Date\"].max()][\"Date\"].min()\n    print(TEST_FIRST)\n    TEST_DAYS = (sub_df[\"Date\"].max() - TEST_FIRST).days + 1\n    TEST_FIRST = (TEST_FIRST - df[\"Date\"].min()).days\n\n    print(TEST_FIRST, TEST_DAYS)\n\n\n    def get_blend(pred_dfs, weights, verbose=True):\n        if verbose:\n            for n1, n2 in [(\"cpmp\", \"giba1\"),(\"cpmp\", \"giba2\"), (\"cpmp\", \"ahmet\"), (\"giba1\", \"ahmet\"), (\"giba2\", \"ahmet\")]:\n                print(n1, n2, np.round(rmse(pred_dfs[n1][TARGETS[0]], pred_dfs[n2][TARGETS[0]]), 4), np.round(rmse(pred_dfs[n1][TARGETS[1]], pred_dfs[n2][TARGETS[1]]), 4))\n\n        blend_df = pred_dfs[\"cpmp\"].copy()\n        blend_df[TARGETS] = 0\n        for name, pred_df in pred_dfs.items():\n            blend_df[TARGETS] += weights[name]*pred_df[TARGETS].values\n\n        return blend_df\n\n\n    cov_models = {\"ahmet\": CovidModelAhmet(), \"cpmp\": CovidModelCPMP(), 'giba1': CovidModelGIBA(lag=1), 'giba2': CovidModelGIBA(lag=2)}\n    weights = {\"ahmet\": 0.35, \"cpmp\": 0.30, \"giba1\": 0.175, \"giba2\": 0.175}\n    pred_dfs = {name: cm.predict_first_day(TEST_FIRST).sort_values(\"geo\") for name, cm in cov_models.items()}\n\n\n    blend_df = get_blend(pred_dfs, weights)\n    eval_df = blend_df.copy()\n\n    for d in range(1, TEST_DAYS):\n        pred_dfs = {name: cm.predict_next_day(blend_df).sort_values(\"geo\") for name, cm in cov_models.items()}\n        blend_df = get_blend(pred_dfs, weights)\n        eval_df = eval_df.append(blend_df)\n        print(d, eval_df.shape, flush=True)\n\n\n\n\n    print( eval_df.head() )\n\n    print(sub_df.shape)\n    sub_df = sub_df.merge(df.append(eval_df, sort=False), on=[\"geo\", \"day\"], how=\"left\")\n    print(sub_df.shape)\n    print(sub_df[TARGETS].isnull().mean())\n\n    sub_df[sub_df[\"geo\"] == \"France_\"][[\"day\"] +TARGETS].plot(x=\"day\")\n    plt.axvline(TEST_FIRST, color='r', linestyle='--', lw=2)\n\n    sub_df[sub_df[\"geo\"] == \"Brazil_\"][[\"day\"] +TARGETS].plot(x=\"day\")\n    plt.axvline(TEST_FIRST, color='r', linestyle='--', lw=2)\n\n    sub_df[sub_df[\"geo\"] == \"Turkey_\"][[\"day\"] +TARGETS].plot(x=\"day\")\n    plt.axvline(TEST_FIRST, color='r', linestyle='--', lw=2)\n\n    sub_df[TARGETS] = np.expm1(sub_df[TARGETS].values)\n    sub_df.sort_values(\"ForecastId\", inplace=True)\n    \n    sub_df.to_csv(\"submission1.csv\", index=False, columns=[\"ForecastId\"] + TARGETS)\n    sub_df.head()    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BLEND_WEEK4_1()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def BLEND_WEEK4_2():\n    import pandas as pd\n    import numpy as np\n    from scipy.optimize import curve_fit\n\n    %matplotlib inline\n    %config InlineBackend.figure_format = 'retina'\n    import matplotlib.pyplot as plt\n\n    from sklearn.linear_model import LinearRegression\n    from sklearn.metrics import log_loss\n    from sklearn.preprocessing import OneHotEncoder\n\n    import xgboost as xgb\n\n    from tensorflow.keras.optimizers import Nadam\n    from sklearn.metrics import mean_squared_error\n    import tensorflow as tf\n    import tensorflow.keras.layers as KL\n    from datetime import timedelta\n    import numpy as np\n    import pandas as pd\n\n\n    def giba_model():\n        def exponential(x, a, k, b):\n            return a*np.exp(x*k) + b\n\n        def rmse( yt, yp ):\n            return np.sqrt( np.mean( (yt-yp)**2 ) )  \n\n        train = pd.read_csv('../input/covid19-global-forecasting-week-4/train.csv')\n        train['Date'] = pd.to_datetime( train['Date'] )\n        train['Province_State'] = train['Province_State'].fillna('')\n\n        test  = pd.read_csv('../input/covid19-global-forecasting-week-4/test.csv')\n        test['Date'] = pd.to_datetime( test['Date'] )\n        test['Province_State'] = test['Province_State'].fillna('')\n        test['Id'] = -1\n        test['ConfirmedCases'] = 0\n        test['Fatalities'] = 0\n\n        publictest = test.loc[ test.Date > train.Date.max() ].copy()\n        train = pd.concat( (train, publictest ) )\n\n        train['ForecastId'] = pd.merge( train, test, on=['Country_Region','Province_State','Date'], how='left' )['ForecastId_y'].values\n\n        train.sort_values( ['Country_Region','Province_State','Date'], inplace=True )\n        train = train.reset_index(drop=True)\n\n        train['cid'] = train['Country_Region'] + '_' + train['Province_State']\n\n\n        train['log0'] = np.log1p( train['ConfirmedCases'] )\n        train['log1'] = np.log1p( train['Fatalities'] )\n\n        train['log0'] = train.groupby('cid')['log0'].cummax()\n        train['log1'] = train.groupby('cid')['log1'].cummax()\n\n        train = train.loc[ (train.log0 > 0) | (train.ForecastId.notnull()) ].copy()\n        train = train.reset_index(drop=True)\n\n        train['day'] = train.groupby('cid')['Id'].cumcount()\n\n\n        def create_features( df, traindate, lag=1 ):\n            df['lag0_1'] = df.groupby('cid')['target0'].shift(lag)\n            df['lag1_1'] = df.groupby('cid')['target1'].shift(lag)\n            df['lag0_1'] = df.groupby('cid')['lag0_1'].fillna( method='bfill' )\n            df['lag1_1'] = df.groupby('cid')['lag1_1'].fillna( method='bfill' )\n\n            df['m0'] = df.groupby('cid')['lag0_1'].rolling(2).mean().values\n            df['m1'] = df.groupby('cid')['lag0_1'].rolling(3).mean().values\n            df['m2'] = df.groupby('cid')['lag0_1'].rolling(4).mean().values\n            df['m3'] = df.groupby('cid')['lag0_1'].rolling(5).mean().values\n            df['m4'] = df.groupby('cid')['lag0_1'].rolling(7).mean().values\n            df['m5'] = df.groupby('cid')['lag0_1'].rolling(10).mean().values\n            df['m6'] = df.groupby('cid')['lag0_1'].rolling(12).mean().values\n            df['m7'] = df.groupby('cid')['lag0_1'].rolling(16).mean().values\n            df['m8'] = df.groupby('cid')['lag0_1'].rolling(20).mean().values\n\n            df['n0'] = df.groupby('cid')['lag1_1'].rolling(2).mean().values\n            df['n1'] = df.groupby('cid')['lag1_1'].rolling(3).mean().values\n            df['n2'] = df.groupby('cid')['lag1_1'].rolling(4).mean().values\n            df['n3'] = df.groupby('cid')['lag1_1'].rolling(5).mean().values\n            df['n4'] = df.groupby('cid')['lag1_1'].rolling(7).mean().values\n            df['n5'] = df.groupby('cid')['lag1_1'].rolling(10).mean().values\n            df['n6'] = df.groupby('cid')['lag1_1'].rolling(12).mean().values\n            df['n7'] = df.groupby('cid')['lag1_1'].rolling(16).mean().values\n            df['n8'] = df.groupby('cid')['lag1_1'].rolling(20).mean().values\n\n            df['m0'] = df.groupby('cid')['m0'].fillna( method='bfill' )\n            df['m1'] = df.groupby('cid')['m1'].fillna( method='bfill' )\n            df['m2'] = df.groupby('cid')['m2'].fillna( method='bfill' )\n            df['m3'] = df.groupby('cid')['m3'].fillna( method='bfill' )\n            df['m4'] = df.groupby('cid')['m4'].fillna( method='bfill' )\n            df['m5'] = df.groupby('cid')['m5'].fillna( method='bfill' )\n            df['m6'] = df.groupby('cid')['m6'].fillna( method='bfill' )\n            df['m7'] = df.groupby('cid')['m7'].fillna( method='bfill' )\n            df['m8'] = df.groupby('cid')['m8'].fillna( method='bfill' )\n\n            df['n0'] = df.groupby('cid')['n0'].fillna( method='bfill' )\n            df['n1'] = df.groupby('cid')['n1'].fillna( method='bfill' )\n            df['n2'] = df.groupby('cid')['n2'].fillna( method='bfill' )\n            df['n3'] = df.groupby('cid')['n3'].fillna( method='bfill' )\n            df['n4'] = df.groupby('cid')['n4'].fillna( method='bfill' )\n            df['n5'] = df.groupby('cid')['n5'].fillna( method='bfill' )\n            df['n6'] = df.groupby('cid')['n6'].fillna( method='bfill' )\n            df['n7'] = df.groupby('cid')['n7'].fillna( method='bfill' )\n            df['n8'] = df.groupby('cid')['n8'].fillna( method='bfill' )\n\n            df['flag_China'] = 1*(df['Country_Region'] == 'China')\n            df['flag_Italy'] = 1*(df['Country_Region'] == 'Italy')\n            df['flag_Spain'] = 1*(df['Country_Region'] == 'Spain')\n            df['flag_US']    = 1*(df['Country_Region'] == 'US')\n            df['flag_Brazil']= 1*(df['Country_Region'] == 'Brazil')\n\n        #     ohe = OneHotEncoder(sparse=False)\n        #     country_ohe = ohe.fit_transform( df[['cid']] )\n        #     country_ohe = pd.DataFrame( country_ohe )\n        #     country_ohe.columns = df['cid'].unique().tolist()\n\n        #     df = pd.concat( ( df, country_ohe ), axis=1, sort=False )\n\n            tr = df.loc[ df.Date  < traindate ].copy()\n            vl = df.loc[ df.Date == traindate ].copy()\n\n            tr = tr.loc[ tr.lag0_1 > 0 ]\n\n            return tr, vl    \n\n        def train_period( \n                        train, \n                        valid_days = ['2020-03-13'],\n                        lag = 1,\n                        seed = 1,\n                        ):\n\n            train['target0'] = np.log1p( train['ConfirmedCases'] )\n            train['target1'] = np.log1p( train['Fatalities'] )\n\n            param = {\n                'subsample': 0.80,\n                'colsample_bytree': 0.85,\n                'max_depth': 7,\n                'gamma': 0.000,\n                'learning_rate': 0.01,\n                'min_child_weight': 5.00,\n                'reg_alpha': 0.000,\n                'reg_lambda': 0.400,\n                'silent':1,\n                'objective':'reg:squarederror',\n                #'booster':'dart',\n                #'tree_method': 'gpu_hist',\n                'nthread': -1,\n                'seed': seed,\n                }\n\n            tr, vl = create_features( train.copy(), valid_days[0] , lag=lag )\n            features = [f for f in tr.columns if f not in [\n                'Id',\n                'ConfirmedCases',\n                'Fatalities',\n                'log0',\n                'log1',\n                'target0',\n                'target1',\n                'Province_State',\n                'Country_Region',\n                'Date',\n                'ForecastId',\n                'cid',\n                #'day',\n            ] ]\n\n            dtrain = xgb.DMatrix( tr[features], tr['target0'] )\n            dvalid = xgb.DMatrix( vl[features], vl['target0'] )\n            watchlist = [(dvalid, 'eval')]\n            model0 = xgb.train( param, dtrain, 767, watchlist , verbose_eval=0 )#, early_stopping_rounds=25 )\n\n            dtrain = xgb.DMatrix( tr[features], tr['target1'] )\n            dvalid = xgb.DMatrix( vl[features], vl['target1'] )\n            watchlist = [(dvalid, 'eval')]\n            model1 = xgb.train( param, dtrain, 767, watchlist , verbose_eval=0 )#, early_stopping_rounds=25 )\n\n            ypred0 = model0.predict( dvalid )\n            ypred1 = model1.predict( dvalid )\n            vl['ypred0'] = ypred0\n            vl['ypred1'] = ypred1\n\n            #walkforwarding scoring all dates\n            feats = ['Province_State','Country_Region','Date']\n            decay = 1.\n            decayK = 1.002\n            for day in valid_days:\n                tr, vl = create_features( train.copy(), day, lag=2 )\n                dvalid = xgb.DMatrix( vl[features] )\n                ypred0 = model0.predict( dvalid )\n                ypred1 = model1.predict( dvalid )\n                vl['ypred0'] = ypred0\n                vl['ypred1'] = ypred1\n\n                train[ 'ypred0' ] = pd.merge( train[feats], vl[feats+['ypred0']], on=feats, how='left' )['ypred0'].values * decay\n                train.loc[ train.ypred0<0, 'ypred0'] = 0\n                train.loc[ train.ypred0.notnull(), 'target0'] = train.loc[ train.ypred0.notnull() , 'ypred0']\n\n                train[ 'ypred1' ] = pd.merge( train[feats], vl[feats+['ypred1']], on=feats, how='left' )['ypred1'].values * decay\n                train.loc[ train.ypred1<0, 'ypred1'] = 0\n                train.loc[ train.ypred1.notnull(), 'target1'] = train.loc[ train.ypred1.notnull() , 'ypred1']\n\n                px = np.where( (train.Date==day ) )[0]\n                print( day, rmse( train['log0'].iloc[px], train['target0'].iloc[px] ), rmse( train['log1'].iloc[px], train['target1'].iloc[px] )  )\n                decay = decay*decayK\n                decayK = decayK*0.99995\n\n            VALID = train.loc[ (train.Date>=valid_days[0])&(train.Date<=valid_days[-1]) ].copy() \n            del VALID['ypred0'],VALID['ypred1']\n\n            sc0 = rmse( VALID['log0'], VALID['target0'] )\n            sc1 = rmse( VALID['log1'], VALID['target1'] )\n            print( sc0, sc1, (sc0+sc1)/2 )\n\n            return VALID.copy()    \n\n\n        VALID0 = train_period( train, \n                              valid_days = ['2020-03-24','2020-03-25','2020-03-26','2020-03-27','2020-03-28','2020-03-29','2020-03-30','2020-03-31','2020-04-01','2020-04-02','2020-04-03','2020-04-04','2020-04-05','2020-04-06','2020-04-07','2020-04-08','2020-04-09','2020-04-10','2020-04-11','2020-04-12','2020-04-13','2020-04-14'],\n                              lag = 1,\n                              seed = 1 )    \n\n        VALID1 = train_period( train, \n                              valid_days = ['2020-03-27','2020-03-28','2020-03-29','2020-03-30','2020-03-31','2020-04-01','2020-04-02','2020-04-03','2020-04-04','2020-04-05','2020-04-06','2020-04-07','2020-04-08','2020-04-09','2020-04-10','2020-04-11','2020-04-12','2020-04-13','2020-04-14'],\n                              lag = 1,\n                              seed = 2 )    \n\n        VALID2 = train_period( train, \n                              valid_days = ['2020-03-30','2020-03-31','2020-04-01','2020-04-02','2020-04-03','2020-04-04','2020-04-05','2020-04-06','2020-04-07','2020-04-08','2020-04-09','2020-04-10','2020-04-11','2020-04-12','2020-04-13','2020-04-14'],\n                              lag = 1,\n                              seed = 3 )    \n\n        VALID3 = train_period( train, \n                              valid_days = ['2020-04-02','2020-04-03','2020-04-04','2020-04-05','2020-04-06','2020-04-07','2020-04-08','2020-04-09','2020-04-10','2020-04-11','2020-04-12','2020-04-13','2020-04-14'],\n                              lag = 1,\n                              seed = 4 )   \n\n        sa0 = rmse( VALID0['log0'], VALID0['target0'] )\n        sa1 = rmse( VALID1['log0'], VALID1['target0'] )\n        sa2 = rmse( VALID2['log0'], VALID2['target0'] )\n        sa3 = rmse( VALID3['log0'], VALID3['target0'] )\n\n        sb0 = rmse( VALID0['log1'], VALID0['target1'] )\n        sb1 = rmse( VALID1['log1'], VALID1['target1'] )\n        sb2 = rmse( VALID2['log1'], VALID2['target1'] )\n        sb3 = rmse( VALID3['log1'], VALID3['target1'] )\n\n\n        print('24-11: ' + str(sa0)[:6] + ', ' + str(sb0)[:6] + ' = ' + str(0.5*sa0+0.5*sb0)[:6]  )\n        print('27-11: ' + str(sa1)[:6] + ', ' + str(sb1)[:6] + ' = ' + str(0.5*sa1+0.5*sb1)[:6]  )\n        print('30-11: ' + str(sa2)[:6] + ', ' + str(sb2)[:6] + ' = ' + str(0.5*sa2+0.5*sb2)[:6]  )\n        print('02-11: ' + str(sa3)[:6] + ', ' + str(sb3)[:6] + ' = ' + str(0.5*sa3+0.5*sb3)[:6]  )\n\n        print( 'Avg: ',  (sa0+sb0+sa1+sb1+sa2+sb2+sa3+sb3) / 8 )\n\n        TEST  = train_period( train, \n                              valid_days = ['2020-04-15','2020-04-16','2020-04-17','2020-04-18','2020-04-19','2020-04-20',\n                                            '2020-04-21','2020-04-22','2020-04-23','2020-04-24','2020-04-25','2020-04-26','2020-04-27','2020-04-28','2020-04-29','2020-04-30',\n                                            '2020-05-01','2020-05-02','2020-05-03','2020-05-04','2020-05-05','2020-05-06','2020-05-07','2020-05-08','2020-05-09','2020-05-10',\n                                            '2020-05-11','2020-05-12','2020-05-13','2020-05-14'],\n                              lag = 1,\n                              seed = 1 )    \n\n        VALID2_sub = VALID3.copy()\n        VALID2_sub['target0'] = np.log1p( VALID2_sub['ConfirmedCases']  )\n        VALID2_sub['target1'] = np.log1p( VALID2_sub['Fatalities']  )\n        sub = pd.concat( (VALID2_sub,TEST.loc[ TEST.Date>='2020-04-15' ] ) )\n\n        sub = sub[['ForecastId','target0','target1']]\n        sub.columns = ['ForecastId','ConfirmedCases','Fatalities']\n        sub['ForecastId'] = sub['ForecastId'].astype( np.int )\n        sub['ConfirmedCases'] = np.expm1( sub['ConfirmedCases'] )\n        sub['Fatalities'] = np.expm1( sub['Fatalities'] )\n        print( sub.describe()  )\n\n        # :21 / 22:31\n        # :18 / 19:31\n        # :15 / 16:31\n        # :12 / 13:31\n        VALID0.to_csv('fold-24-14.csv', index=False)\n        VALID1.to_csv('fold-27-14.csv', index=False)\n        VALID2.to_csv('fold-30-14.csv', index=False)\n        VALID3.to_csv('fold-02-14.csv', index=False)\n        TEST.to_csv('fold-submission.csv', index=False)    \n\n        return sub\n\n\n\n    def get_nn_sub():\n        df = pd.read_csv(\"../input/covid19-global-forecasting-week-4/train.csv\")\n        sub_df = pd.read_csv(\"../input/covid19-global-forecasting-week-4/test.csv\")\n\n        coo_df = pd.read_csv(\"../input/covid19week1/train.csv\").rename(columns={\"Country/Region\": \"Country_Region\"})\n        coo_df = coo_df.groupby(\"Country_Region\")[[\"Lat\", \"Long\"]].mean().reset_index()\n        coo_df = coo_df[coo_df[\"Country_Region\"].notnull()]\n\n        loc_group = [\"Province_State\", \"Country_Region\"]\n\n\n        def preprocess(df):\n            df[\"Date\"] = df[\"Date\"].astype(\"datetime64[ms]\")\n            df[\"days\"] = (df[\"Date\"] - pd.to_datetime(\"2020-01-01\")).dt.days\n            df[\"weekend\"] = df[\"Date\"].dt.dayofweek//5\n\n            df = df.merge(coo_df, how=\"left\", on=\"Country_Region\")\n            df[\"Lat\"] = (df[\"Lat\"] // 30).astype(np.float32).fillna(0)\n            df[\"Long\"] = (df[\"Long\"] // 60).astype(np.float32).fillna(0)\n\n            for col in loc_group:\n                df[col].fillna(\"none\", inplace=True)\n            return df\n\n        df = preprocess(df)\n        sub_df = preprocess(sub_df)\n\n        print(df.shape)\n\n        TARGETS = [\"ConfirmedCases\", \"Fatalities\"]\n\n        for col in TARGETS:\n            df[col] = np.log1p(df[col])\n\n        NUM_SHIFT = 5\n\n        features = [\"Lat\", \"Long\"]\n\n        for s in range(1, NUM_SHIFT+1):\n            for col in TARGETS:\n                df[\"prev_{}_{}\".format(col, s)] = df.groupby(loc_group)[col].shift(s)\n                features.append(\"prev_{}_{}\".format(col, s))\n\n        df = df[df[\"Date\"] >= df[\"Date\"].min() + timedelta(days=NUM_SHIFT)].copy()\n\n        TEST_FIRST = sub_df[\"Date\"].min() # pd.to_datetime(\"2020-03-13\") #\n        TEST_DAYS = (df[\"Date\"].max() - TEST_FIRST).days + 1\n\n        dev_df, test_df = df[df[\"Date\"] < TEST_FIRST].copy(), df[df[\"Date\"] >= TEST_FIRST].copy()\n\n        def nn_block(input_layer, size, dropout_rate, activation):\n            out_layer = KL.Dense(size, activation=None)(input_layer)\n            #out_layer = KL.BatchNormalization()(out_layer)\n            out_layer = KL.Activation(activation)(out_layer)\n            out_layer = KL.Dropout(dropout_rate)(out_layer)\n            return out_layer\n\n\n        def get_model():\n            inp = KL.Input(shape=(len(features),))\n\n            hidden_layer = nn_block(inp, 64, 0.0, \"relu\")\n            gate_layer = nn_block(hidden_layer, 32, 0.0, \"sigmoid\")\n            hidden_layer = nn_block(hidden_layer, 32, 0.0, \"relu\")\n            hidden_layer = KL.multiply([hidden_layer, gate_layer])\n\n            out = KL.Dense(len(TARGETS), activation=\"linear\")(hidden_layer)\n\n            model = tf.keras.models.Model(inputs=[inp], outputs=out)\n            return model\n\n        get_model().summary()\n\n        def get_input(df):\n            return [df[features]]\n\n        NUM_MODELS = 10\n\n\n        def train_models(df, save=False):\n            models = []\n            for i in range(NUM_MODELS):\n                model = get_model()\n                model.compile(loss=\"mean_squared_error\", optimizer=Nadam(lr=1e-4))\n                hist = model.fit(get_input(df), df[TARGETS],\n                                 batch_size=2048, epochs=500, verbose=0, shuffle=True)\n                if save:\n                    model.save_weights(\"model{}.h5\".format(i))\n                models.append(model)\n            return models\n\n        models = train_models(dev_df)\n\n\n        prev_targets = ['prev_ConfirmedCases_1', 'prev_Fatalities_1']\n\n        def predict_one(df, models):\n            pred = np.zeros((df.shape[0], 2))\n            for model in models:\n                pred += model.predict(get_input(df))/len(models)\n            pred = np.maximum(pred, df[prev_targets].values)\n            pred[:, 0] = np.log1p(np.expm1(pred[:, 0]) + 0.1)\n            pred[:, 1] = np.log1p(np.expm1(pred[:, 1]) + 0.01)\n            return np.clip(pred, None, 15)\n\n        print([mean_squared_error(dev_df[TARGETS[i]], predict_one(dev_df, models)[:, i]) for i in range(len(TARGETS))])\n\n\n        def rmse(y_true, y_pred):\n            return np.sqrt(mean_squared_error(y_true, y_pred))\n\n        def evaluate(df):\n            error = 0\n            for col in TARGETS:\n                error += rmse(df[col].values, df[\"pred_{}\".format(col)].values)\n            return np.round(error/len(TARGETS), 5)\n\n\n        def predict(test_df, first_day, num_days, models, val=False):\n            temp_df = test_df.loc[test_df[\"Date\"] == first_day].copy()\n            y_pred = predict_one(temp_df, models)\n\n            for i, col in enumerate(TARGETS):\n                test_df[\"pred_{}\".format(col)] = 0\n                test_df.loc[test_df[\"Date\"] == first_day, \"pred_{}\".format(col)] = y_pred[:, i]\n\n            print(first_day, np.isnan(y_pred).sum(), y_pred.min(), y_pred.max())\n            if val:\n                print(evaluate(test_df[test_df[\"Date\"] == first_day]))\n\n\n            y_prevs = [None]*NUM_SHIFT\n\n            for i in range(1, NUM_SHIFT):\n                y_prevs[i] = temp_df[['prev_ConfirmedCases_{}'.format(i), 'prev_Fatalities_{}'.format(i)]].values\n\n            for d in range(1, num_days):\n                date = first_day + timedelta(days=d)\n                print(date, np.isnan(y_pred).sum(), y_pred.min(), y_pred.max())\n\n                temp_df = test_df.loc[test_df[\"Date\"] == date].copy()\n                temp_df[prev_targets] = y_pred\n                for i in range(2, NUM_SHIFT+1):\n                    temp_df[['prev_ConfirmedCases_{}'.format(i), 'prev_Fatalities_{}'.format(i)]] = y_prevs[i-1]\n\n                y_pred, y_prevs = predict_one(temp_df, models), [None, y_pred] + y_prevs[1:-1]\n\n\n                for i, col in enumerate(TARGETS):\n                    test_df.loc[test_df[\"Date\"] == date, \"pred_{}\".format(col)] = y_pred[:, i]\n\n                if val:\n                    print(evaluate(test_df[test_df[\"Date\"] == date]))\n\n            return test_df\n\n        test_df = predict(test_df, TEST_FIRST, TEST_DAYS, models, val=True)\n        print(evaluate(test_df))\n\n        for col in TARGETS:\n            test_df[col] = np.expm1(test_df[col])\n            test_df[\"pred_{}\".format(col)] = np.expm1(test_df[\"pred_{}\".format(col)])\n\n        models = train_models(df, save=True)\n\n        sub_df_public  = sub_df[sub_df[\"Date\"] <= df[\"Date\"].max()].copy()\n        sub_df_private = sub_df[sub_df[\"Date\"] > df[\"Date\"].max()].copy()\n\n        pred_cols = [\"pred_{}\".format(col) for col in TARGETS]\n        #sub_df_public = sub_df_public.merge(test_df[[\"Date\"] + loc_group + pred_cols].rename(columns={col: col[5:] for col in pred_cols}), \n        #                                    how=\"left\", on=[\"Date\"] + loc_group)\n        sub_df_public = sub_df_public.merge(test_df[[\"Date\"] + loc_group + TARGETS], how=\"left\", on=[\"Date\"] + loc_group)\n\n        SUB_FIRST = sub_df_private[\"Date\"].min()\n        SUB_DAYS = (sub_df_private[\"Date\"].max() - sub_df_private[\"Date\"].min()).days + 1\n\n        sub_df_private = df.append(sub_df_private, sort=False)\n\n        for s in range(1, NUM_SHIFT+1):\n            for col in TARGETS:\n                sub_df_private[\"prev_{}_{}\".format(col, s)] = sub_df_private.groupby(loc_group)[col].shift(s)\n\n        sub_df_private = sub_df_private[sub_df_private[\"Date\"] >= SUB_FIRST].copy()\n\n        sub_df_private = predict(sub_df_private, SUB_FIRST, SUB_DAYS, models)\n\n        for col in TARGETS:\n            sub_df_private[col] = np.expm1(sub_df_private[\"pred_{}\".format(col)])\n\n        sub_df = sub_df_public.append(sub_df_private, sort=False)\n        sub_df[\"ForecastId\"] = sub_df[\"ForecastId\"].astype(np.int16)\n\n        return sub_df[[\"ForecastId\"] + TARGETS]\n\n\n\n    sub1 = giba_model()\n    sub2 = get_nn_sub()\n\n    sub1.sort_values(\"ForecastId\", inplace=True)\n    sub2.sort_values(\"ForecastId\", inplace=True)\n\n    from sklearn.metrics import mean_squared_error\n    TARGETS = [\"ConfirmedCases\", \"Fatalities\"]\n    [np.sqrt(mean_squared_error(np.log1p(sub1[t].values), np.log1p(sub2[t].values))) for t in TARGETS]\n\n    sub_df = sub1.copy()\n    for t in TARGETS:\n        sub_df[t] = np.expm1(np.log1p(sub1[t].values)*0.5 + np.log1p(sub2[t].values)*0.5)\n\n    sub_df.to_csv(\"submission2.csv\", index=False)\n\n\n\n    test = pd.read_csv('../input/covid19-global-forecasting-week-4/test.csv')\n    test['Date'] = pd.to_datetime( test['Date'] )\n    test = test.merge( sub_df, on='ForecastId', how='left' )\n    print( test.head(10) )\n\n    test.loc[ test.Country_Region=='Spain' ].plot( x='Date', y='ConfirmedCases' )\n    test.loc[ test.Country_Region=='Spain' ].plot( x='Date', y='Fatalities' )\n\n    test.loc[ test.Country_Region=='Brazil' ].plot( x='Date', y='ConfirmedCases' )\n    test.loc[ test.Country_Region=='Brazil' ].plot( x='Date', y='Fatalities' )\n\n    test.loc[ test.Country_Region=='Italy' ].plot( x='Date', y='ConfirmedCases' )\n    test.loc[ test.Country_Region=='Italy' ].plot( x='Date', y='Fatalities' )\n\n    test.loc[ test.Country_Region=='Iran' ].plot( x='Date', y='ConfirmedCases' )\n    test.loc[ test.Country_Region=='Iran' ].plot( x='Date', y='Fatalities' )\n\n    tmp = test.groupby('Date')[['ConfirmedCases','Fatalities']].agg('sum').reset_index()\n    tmp['ConfirmedCases'] = np.log1p( tmp['ConfirmedCases'] )\n    tmp['Fatalities'] = np.log1p( tmp['Fatalities'] )\n    tmp.plot( x='Date', y='ConfirmedCases' )\n    tmp.plot( x='Date', y='Fatalities' )\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BLEND_WEEK4_2()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def model_simple_decay():\n    \n    import pandas as pd\n    import numpy as np\n    from scipy.optimize import curve_fit\n\n    %matplotlib inline\n    %config InlineBackend.figure_format = 'retina'\n    import matplotlib.pyplot as plt\n\n    from sklearn.linear_model import LinearRegression\n    from sklearn.metrics import log_loss\n\n    import xgboost as xgb\n\n    from tqdm.notebook import tqdm\n\n\n    def rmse( yt, yp ):\n        return np.sqrt( np.mean( (yt-yp)**2 ) )\n\n    def funclog(x, a, b,c):\n        return a*np.log(b+x)+c\n\n    def ewa( var, alpha=0.5 ):\n        x    = var.copy()\n        for i in range( 1,len(var) ):\n            x[i] = x[i]*alpha + x[i-1]*(1-alpha)\n        return x\n\n\n    train = pd.read_csv('../input/covid19-global-forecasting-week-4/train.csv')\n    train['Date'] = pd.to_datetime( train['Date'] )\n    maxdate  = str(train['Date'].max())[:10]\n    testdate = str( train['Date'].max() + pd.Timedelta(days=1) )[:10]\n    print( maxdate, testdate )\n    train['Province_State'] = train['Province_State'].fillna('')\n\n    test  = pd.read_csv('../input/covid19-global-forecasting-week-4/test.csv')\n    test['Date'] = pd.to_datetime( test['Date'] )\n    test['Province_State'] = test['Province_State'].fillna('')\n    test['Id'] = -1\n    test['ConfirmedCases'] = 0\n    test['Fatalities'] = 0\n\n    publictest = test.loc[ test.Date > train.Date.max() ].copy()\n    train = pd.concat( (train, publictest ), sort=False )\n    train.sort_values( ['Country_Region','Province_State','Date'], inplace=True )\n    train = train.reset_index(drop=True)\n\n    train['ForecastId'] = pd.merge( train, test, on=['Country_Region','Province_State','Date'], how='left' )['ForecastId_y'].values\n\n    train['cid'] = train['Country_Region'] + '_' + train['Province_State']\n\n\n    #Fix some anomalities:\n    train.loc[ (train.cid=='China_Guizhou') & (train.Date=='2020-03-17') , 'ConfirmedCases' ] = 146\n    train.loc[ (train.cid=='Guyana_')&(train.Date>='2020-03-22')&(train.Date<='2020-03-30') , 'ConfirmedCases' ] = 12\n    train.loc[ (train.cid=='US_Virgin Islands')&(train.Date>='2020-03-29')&(train.Date<='2020-03-29') , 'ConfirmedCases' ] = 24\n    train.loc[ (train.cid=='US_Virgin Islands')&(train.Date>='2020-03-30')&(train.Date<='2020-03-30') , 'ConfirmedCases' ] = 27\n\n    train.loc[ (train.cid=='Iceland_')&(train.Date>='2020-03-15')&(train.Date<='2020-03-15') , 'Fatalities' ] = 0\n    train.loc[ (train.cid=='Kazakhstan_')&(train.Date>='2020-03-20')&(train.Date<='2020-03-20') , 'Fatalities' ] = 0\n    train.loc[ (train.cid=='Serbia_')&(train.Date>='2020-03-26')&(train.Date<='2020-03-26') , 'Fatalities' ] = 5\n    train.loc[ (train.cid=='Serbia_')&(train.Date>='2020-03-27')&(train.Date<='2020-03-27') , 'Fatalities' ] = 6\n    train.loc[ (train.cid=='Slovakia_')&(train.Date>='2020-03-22')&(train.Date<='2020-03-31') , 'Fatalities' ] = 1\n    train.loc[ (train.cid=='US_Hawaii')&(train.Date>='2020-03-25')&(train.Date<='2020-03-31') , 'Fatalities' ] = 1\n\n    train['log0'] = np.log1p( train['ConfirmedCases'] )\n    train['log1'] = np.log1p( train['Fatalities'] )\n\n    train = train.loc[ (train.log0 > 0) | (train.ForecastId.notnull()) | (train.Date >= '2020-03-17') ].copy()\n    train = train.reset_index(drop=True)\n\n    train['day'] = train.groupby('cid')['Id'].cumcount()\n\n    #Fix\n\n    print( train.head(4) )\n    print(train.shape)\n\n\n\n    cid = train['cid'].unique().tolist()\n    train['dif0'] = train['log0'] - train.groupby('cid')['log0'].shift(1)\n    train['dif0'].fillna(0, inplace=True)\n\n\n    #K = [ 0.00814423,  0.54210745, -1.36271011,  0.29145165,  6.90285755,-1.70264593]\n    K = [ -3.61052304e-03,  1.97547109e-01,  1.98486015e+00,  9.03304453e-02,4.54479999e+00,  2.49382214e+00]\n\n    thr = '2020-04-01'\n    VAL = []\n    for c in cid:\n        tmp = train.loc[ (train.cid==c) & (train.Date<thr) ].copy()\n        tmp['dif1'] = K[0]*ewa( tmp['dif0'].values, K[1] ) + K[2]*ewa( tmp['dif0'].values[::-1], K[3] )[::-1]\n        tmp['dif1'] = tmp['dif1'] * (tmp['log0'].values[-1]/(tmp['dif1'].sum()+0.000001))\n\n        tmp['dif2'] = tmp['dif1'] - tmp['dif1'].shift(1)\n        tmp['dif2'].fillna(0, inplace=True)\n        decay = tmp['dif2'].values[-tmp.shape[0]//10:].mean()\n        lastv = tmp['log0'].values[-1]\n\n        tmp2 = train.loc[ (train.cid==c) & (train.Date >= thr) ].copy()\n        tmp2['dif1'] = [ tmp['dif1'].values[-1]*(1-(K[4]*i/tmp2.shape[0])) - decay*(1-(K[5]*i/tmp2.shape[0])) for i in range(tmp2.shape[0]) ]\n        tmp2['dif2'] = 0\n\n        ts = pd.concat( (tmp, tmp2) )\n        ts['dif2'] = ts['dif1'].cumsum() \n        ts.loc[ (ts.dif2<lastv)&(ts.Date>=thr), 'dif2'] = lastv\n        ts.loc[ (ts.dif2>13), 'dif2'] = 13\n        ts['dif2'] = ts['dif2'].cummax()\n        VAL.append( ts )\n\n    VAL = pd.concat( VAL, sort=False )\n\n    tmp = VAL.loc[ (VAL.Date>=thr)&(VAL.Date<='2020-04-14')  ].copy()\n    sc = rmse( tmp.log0, tmp.dif2 )\n    print(sc)\n\n\n\n    K = [ -3.61052304e-03,  1.97547109e-01,  1.98486015e+00,  9.03304453e-02,4.54479999e+00,  2.49382214e+00]\n\n    thr = '2020-04-15'\n    VAL = []\n    for c in cid:\n        tmp = train.loc[ (train.cid==c) & (train.Date<thr) ].copy()\n        tmp['dif1'] = K[0]*ewa( tmp['dif0'].values, K[1] ) + K[2]*ewa( tmp['dif0'].values[::-1], K[3] )[::-1]\n        tmp['dif1'] = tmp['dif1'] * (tmp['log0'].values[-1]/(tmp['dif1'].sum()+0.000001))\n\n        tmp['dif2'] = tmp['dif1'] - tmp['dif1'].shift(1)\n        tmp['dif2'].fillna(0, inplace=True)\n        decay = tmp['dif2'].values[-tmp.shape[0]//10:].mean()\n        lastv = tmp['log0'].values[-1]\n\n        tmp2 = train.loc[ (train.cid==c) & (train.Date >= thr) ].copy()\n        tmp2['dif1'] = [ tmp['dif1'].values[-1]*(1-(K[4]*i/tmp2.shape[0])) - decay*(1-(K[5]*i/tmp2.shape[0])) for i in range(tmp2.shape[0]) ]\n        tmp2['dif2'] = 0\n\n        ts = pd.concat( (tmp, tmp2) )\n        ts['dif2'] = ts['dif1'].cumsum() \n        ts.loc[ (ts.dif2<lastv)&(ts.Date>=thr), 'dif2'] = lastv\n        ts.loc[ (ts.dif2>13), 'dif2'] = 13\n        ts['dif2'] = ts['dif2'].cummax()\n        VAL.append( ts )\n\n    VAL1 = pd.concat( VAL, sort=False )\n\n\n    K = [ 0.00814423,  0.54210745, -1.36271011,  0.29145165,  6.90285755,-1.70264593]\n\n    thr = '2020-04-15'\n    VAL = []\n    for c in cid:\n        tmp = train.loc[ (train.cid==c) & (train.Date<thr) ].copy()\n        tmp['dif1'] = K[0]*ewa( tmp['dif0'].values, K[1] ) + K[2]*ewa( tmp['dif0'].values[::-1], K[3] )[::-1]\n        tmp['dif1'] = tmp['dif1'] * (tmp['log0'].values[-1]/(tmp['dif1'].sum()+0.000001))\n\n        tmp['dif2'] = tmp['dif1'] - tmp['dif1'].shift(1)\n        tmp['dif2'].fillna(0, inplace=True)\n        decay = tmp['dif2'].values[-tmp.shape[0]//10:].mean()\n        lastv = tmp['log0'].values[-1]\n\n        tmp2 = train.loc[ (train.cid==c) & (train.Date >= thr) ].copy()\n        tmp2['dif1'] = [ tmp['dif1'].values[-1]*(1-(K[4]*i/tmp2.shape[0])) - decay*(1-(K[5]*i/tmp2.shape[0])) for i in range(tmp2.shape[0]) ]\n        tmp2['dif2'] = 0\n\n        ts = pd.concat( (tmp, tmp2) )\n        ts['dif2'] = ts['dif1'].cumsum() \n        ts.loc[ (ts.dif2<lastv)&(ts.Date>=thr), 'dif2'] = lastv\n        ts.loc[ (ts.dif2>13), 'dif2'] = 13\n        ts['dif2'] = ts['dif2'].cummax()\n        VAL.append( ts )\n\n    VAL2 = pd.concat( VAL, sort=False )\n\n\n\n    tmp = VAL1.loc[ VAL1.cid == 'US_New York' ].copy()\n    tmp[['Date','log0','dif2']].plot(x='Date')\n\n    tmp = VAL1.loc[ VAL1.cid == 'US_California' ].copy()\n    tmp[['Date','log0','dif2']].plot(x='Date')\n\n    tmp = VAL1.loc[ VAL1.cid == 'Brazil_' ].copy()\n    tmp[['Date','log0','dif2']].plot(x='Date')\n\n    tmp = VAL1.loc[ VAL1.cid == 'Spain_' ].copy()\n    tmp[['Date','log0','dif2']].plot(x='Date')\n\n    tmp = VAL1.loc[ VAL1.cid == 'Italy_' ].copy()\n    tmp[['Date','log0','dif2']].plot(x='Date')\n\n    tmp = VAL1.loc[ VAL1.cid == 'Iran_' ].copy()\n    tmp[['Date','log0','dif2']].plot(x='Date')\n\n    tmp = VAL1.loc[ VAL1.cid == 'Chile_' ].copy()\n    tmp[['Date','log0','dif2']].plot(x='Date')\n\n    tmp = VAL1.loc[ VAL1.cid == 'Congo (Brazzaville)_' ].copy()\n    tmp[['Date','log0','dif2']].plot(x='Date')\n\n    tmp = VAL1.loc[ VAL1.cid == 'India_' ].copy()\n    tmp[['Date','log0','dif2']].plot(x='Date')\n\n    tmp = VAL1.loc[ VAL1.cid == 'Peru_' ].copy()\n    tmp[['Date','log0','dif2']].plot(x='Date')\n\n    tmp = VAL1.loc[ VAL1.cid == 'Japan_' ].copy()\n    tmp[['Date','log0','dif2']].plot(x='Date')\n\n    tmp = VAL1.loc[ VAL1.cid == 'US_Washington' ].copy()\n    tmp[['Date','log0','dif2']].plot(x='Date')\n\n    tmp = VAL1.loc[ VAL1.cid == 'US_Maryland' ].copy()\n    tmp[['Date','log0','dif2']].plot(x='Date')\n\n    tmp = VAL1.loc[ VAL1.cid == 'Trinidad and Tobago_' ].copy()\n    tmp[['Date','log0','dif2']].plot(x='Date')\n\n    tmp = VAL1.loc[ VAL1.cid == 'Kosovo_' ].copy()\n    tmp[['Date','log0','dif2']].plot(x='Date')\n\n\n    tmp = VAL1.groupby(['Date'])['dif2'].agg('sum').reset_index()\n    tmp[['Date','dif2']].plot(x='Date')\n\n    tmp = VAL2.groupby(['Date'])['dif2'].agg('sum').reset_index()\n    tmp[['Date','dif2']].plot(x='Date')\n\n    VAL1.sort_values( ['cid','Date','Id','ForecastId'], inplace=True )\n    VAL2.sort_values( ['cid','Date','Id','ForecastId'], inplace=True )\n\n    VAL1 = VAL1.loc[ VAL1.Date >='2020-04-02' ].copy()\n    VAL2 = VAL2.loc[ VAL2.Date >='2020-04-02' ].copy()\n\n\n    VAL1['ConfirmedCases'] = np.expm1( 0.5*VAL1['dif2'] + 0.5*VAL2['dif2'] )\n\n\n\n\n\n    K = [1.34183856e-03, 6.74294944e-01, 1.25931206e+00, 6.75211842e-02, 4.20000395e-01, 2.20018025e+00]\n    thr = '2020-04-15'\n    VAL = []\n    for c in cid:\n        tmp = train.loc[ (train.cid==c) & (train.Date<thr) ].copy()\n        tmp['dif1'] = K[0]*ewa( tmp['dif0'].values, K[1] ) + K[2]*ewa( tmp['dif0'].values[::-1], K[3] )[::-1]\n        tmp['dif1'] = tmp['dif1'] * (tmp['log1'].values[-1]/(tmp['dif1'].sum()+0.000001))\n\n        tmp['dif2'] = tmp['dif1'] - tmp['dif1'].shift(1)\n        tmp['dif2'].fillna(0, inplace=True)\n        decay = tmp['dif2'].values[-tmp.shape[0]//10:].mean()\n        lastv = tmp['log1'].values[-1]\n\n        tmp2 = train.loc[ (train.cid==c) & (train.Date >= thr) ].copy()\n        tmp2['dif1'] = [ tmp['dif1'].values[-1]*(1-(K[4]*i/tmp2.shape[0])) - decay*(1-(K[5]*i/tmp2.shape[0])) for i in range(tmp2.shape[0]) ]\n        tmp2['dif2'] = 0\n\n        ts = pd.concat( (tmp, tmp2) )\n        ts['dif2'] = ts['dif1'].cumsum() \n        ts.loc[ (ts.dif2<lastv)&(ts.Date>=thr), 'dif2'] = lastv\n        ts['dif2'] = ts['dif2'].cummax()\n        VAL.append( ts )\n\n    VAL3 = pd.concat( VAL, sort=False )\n\n\n    K = [0.00524396, 0.29401264, 1.58642304, 0.0290667 , 2.45286858, 0.34968403]\n\n    thr = '2020-04-15'\n    VAL = []\n    for c in cid:\n        tmp = train.loc[ (train.cid==c) & (train.Date<thr) ].copy()\n        tmp['dif1'] = K[0]*ewa( tmp['dif0'].values, K[1] ) + K[2]*ewa( tmp['dif0'].values[::-1], K[3] )[::-1]\n        tmp['dif1'] = tmp['dif1'] * (tmp['log1'].values[-1]/(tmp['dif1'].sum()+0.000001))\n\n        tmp['dif2'] = tmp['dif1'] - tmp['dif1'].shift(1)\n        tmp['dif2'].fillna(0, inplace=True)\n        decay = tmp['dif2'].values[-tmp.shape[0]//10:].mean()\n        lastv = tmp['log1'].values[-1]\n\n        tmp2 = train.loc[ (train.cid==c) & (train.Date >= thr) ].copy()\n        tmp2['dif1'] = [ tmp['dif1'].values[-1]*(1-(K[4]*i/tmp2.shape[0])) - decay*(1-(K[5]*i/tmp2.shape[0])) for i in range(tmp2.shape[0]) ]\n        tmp2['dif2'] = 0\n\n        ts = pd.concat( (tmp, tmp2) )\n        ts['dif2'] = ts['dif1'].cumsum() \n        ts.loc[ (ts.dif2<lastv)&(ts.Date>=thr), 'dif2'] = lastv\n        ts['dif2'] = ts['dif2'].cummax()\n        VAL.append( ts )\n\n    VAL4 = pd.concat( VAL, sort=False )\n\n\n\n    tmp = VAL3.groupby(['Date'])['dif2'].agg('sum').reset_index()\n    tmp[['Date','dif2']].plot(x='Date')\n\n    tmp = VAL4.groupby(['Date'])['dif2'].agg('sum').reset_index()\n    tmp[['Date','dif2']].plot(x='Date')\n\n    VAL3.sort_values( ['cid','Date','Id','ForecastId'], inplace=True )\n    VAL4.sort_values( ['cid','Date','Id','ForecastId'], inplace=True )\n\n    VAL3 = VAL3.loc[ VAL3.Date >='2020-04-02' ].copy()\n    VAL4 = VAL4.loc[ VAL4.Date >='2020-04-02' ].copy()\n\n    VAL1['Fatalities'] = np.expm1( 0.5*VAL3['dif2'] + 0.5*VAL4['dif2'] )\n\n    VAL1['dif1'] = np.log1p( VAL1['ConfirmedCases'] )\n    VAL1['dif2'] = np.log1p( VAL1['Fatalities'] )\n\n    tmp = VAL1.groupby(['Date'])['dif1'].agg('sum').reset_index()\n    tmp[['Date','dif1']].plot(x='Date')\n\n    tmp = VAL1.groupby(['Date'])['dif2'].agg('sum').reset_index()\n    tmp[['Date','dif2']].plot(x='Date')\n\n\n    tmp = VAL1.groupby(['Date'])['ConfirmedCases'].agg('sum').reset_index()\n    tmp[['Date','ConfirmedCases']].plot(x='Date')\n\n    tmp = VAL1.groupby(['Date'])['Fatalities'].agg('sum').reset_index()\n    tmp[['Date','Fatalities']].plot(x='Date')\n\n\n    tmp = VAL1.loc[ VAL1.cid == 'US_New York' ].copy()\n    tmp[['Date','ConfirmedCases','Fatalities']].plot(x='Date')\n\n    tmp = VAL1.loc[ VAL1.cid == 'US_California' ].copy()\n    tmp[['Date','ConfirmedCases','Fatalities']].plot(x='Date')\n\n    tmp = VAL1.loc[ VAL1.cid == 'Italy_' ].copy()\n    tmp[['Date','ConfirmedCases','Fatalities']].plot(x='Date')\n\n    tmp = VAL1.loc[ VAL1.cid == 'Spain_' ].copy()\n    tmp[['Date','ConfirmedCases','Fatalities']].plot(x='Date')\n\n    tmp = VAL1.loc[ VAL1.cid == 'Brazil_' ].copy()\n    tmp[['Date','ConfirmedCases','Fatalities']].plot(x='Date')\n\n    sub = VAL1[['ForecastId','ConfirmedCases','Fatalities']].copy()\n    sub['ForecastId'] = sub['ForecastId'].astype(int)\n    \n    sub.sort_values(\"ForecastId\", inplace=True)\n\n    sub.to_csv( 'submission3.csv', index=False )\n\n    return ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_simple_decay()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub1 = pd.read_csv(\"submission1.csv\")\nsub2 = pd.read_csv(\"submission2.csv\")\nsub3 = pd.read_csv(\"submission3.csv\")\n\nprint( np.corrcoef( sub1['ConfirmedCases'], sub2['ConfirmedCases'] )[0][1] )\nprint( np.corrcoef( sub1['Fatalities']    , sub2['Fatalities'] )[0][1] )\nprint( np.corrcoef( sub1['ConfirmedCases'], sub3['ConfirmedCases'] )[0][1] )\nprint( np.corrcoef( sub1['Fatalities']    , sub3['Fatalities'] )[0][1] )\n\nsub1['ConfirmedCases'] = 0.35*sub1['ConfirmedCases'] + 0.25*sub2['ConfirmedCases'] + 0.40*sub3['ConfirmedCases']\nsub1['Fatalities']     = 0.35*sub1['Fatalities']     + 0.25*sub2['Fatalities']     + 0.40*sub3['Fatalities']\n\nsub1.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv('../input/covid19-global-forecasting-week-4/test.csv')\ntest['Date'] = pd.to_datetime( test['Date'] )\ntest = test.merge( sub1, on='ForecastId', how='left' )\nprint( test.head(10) )\n\nprint('Spain')\ntest.loc[ test.Country_Region=='Spain' ].plot( x='Date', y='ConfirmedCases' )\ntest.loc[ test.Country_Region=='Spain' ].plot( x='Date', y='Fatalities' )\n\nprint('Brazil')\ntest.loc[ test.Country_Region=='Brazil' ].plot( x='Date', y='ConfirmedCases' )\ntest.loc[ test.Country_Region=='Brazil' ].plot( x='Date', y='Fatalities' )\n\nprint('Italy')\ntest.loc[ test.Country_Region=='Italy' ].plot( x='Date', y='ConfirmedCases' )\ntest.loc[ test.Country_Region=='Italy' ].plot( x='Date', y='Fatalities' )\n\nprint('Iran')\ntest.loc[ test.Country_Region=='Iran' ].plot( x='Date', y='ConfirmedCases' )\ntest.loc[ test.Country_Region=='Iran' ].plot( x='Date', y='Fatalities' )\n\nprint('Global')\ntmp = test.groupby('Date')[['ConfirmedCases','Fatalities']].agg('sum').reset_index()\ntmp['ConfirmedCases'] = np.log1p( tmp['ConfirmedCases'] )\ntmp['Fatalities'] = np.log1p( tmp['Fatalities'] )\ntmp.plot( x='Date', y='ConfirmedCases' )\ntmp.plot( x='Date', y='Fatalities' )\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":4}