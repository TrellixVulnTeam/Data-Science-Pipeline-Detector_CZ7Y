{"cells":[{"metadata":{},"cell_type":"markdown","source":"# COVID19 Global Forecasting (week 4)\n\n### Data used:\n* ConfirmedCases, Fatalities per Country/Region, Province/State, Date - competition data\n* Population data per Country/Region, Province/State - collected from different sources:\n    * population\n    * population yearly change\n    * population density\n    * land area\n    * median age\n    * urban population\n* Selected additional data per Country/Region, Province/State - collected mainly from World By Map portal\n    * labor force\n    * labor force per capita\n    * death rate\n    * air traffic passengers total\n    * air traffic passengers per capita\n    * hospital bed density\n    * obesity\n    * old people\n    * physicians density\n* Cigarettes and alcohol consumption per capita (data originally from WHO, taken from Wikipedia)\n* ConfirmedCasesDelta and FatalitiesDelta are used as output for prediction\n\n### Feature engineering\n- Adding for every day features containing data from N=50 previous days:\n    + ConfirmedCases, Fatalities\n    + ConfirmedCases delta, Fatalities delta\n- Adding extra day counters starting from the selected moments of epidemy for every Country and Region:\n    + first 1, 50, 200, 500, 1000, 5000, 20000, 50000, 100000 cases reported\n    + first 1, 25, 50, 100, 150, 200, 500, 1000, 2000, 5000, 10000 fatalities reported\n- Adding 'ExposedDensity' feature, indicating how many people haven't gone yet through COVID-19 per square kilometer\n- Scaling with PowerTransformer, MinMaxScaling, StandardScaling\n- Filling NaNs with mean values\n\n### Model training\n- Two deep neural networks (massive exeriments done before choosing final parameters that are far from optimum):\n    + for ConfirmedCasesDelta\n    + for FatalitiesDelta\n- Training, Validation and Test datasets, splitted by days\n    + training dataset contains days from day0 to dayK\n    + validaton dataset contains days from dayK+1 to dayK+N\n    + test dataset contains days from dayK+N+1 to dayLast\n- Activation functions: 'swish', 'elu'\n- Adam optimizer\n- Loss function 'mean_absolute_error'\n- Metric 'mae'\n\n### Prediction\n- Adding all the features (like in the training dataset)\n- Scaling with scalers from the training phase\n- Day by day prediction to build proper features with data from previous days\n- Predicting deltas and calculating CC and F"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nnp.random.seed(1337) # for reproducibility\n\nimport seaborn as sns\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nfrom os import listdir, remove\nfrom os.path import isfile, join\n\nbase_folder = '/kaggle/input/'\nplt.rcParams['figure.figsize'] = [15, 7]\n\n# let's define different sets of features\next_cols = ['LaborForceTotal', 'LaborForcePerCapita', 'DeathRate', 'AirTrafficPassengersTotal',\n            'AirTrafficPassengersPerCapita', 'HospitalBedDensity', 'Obesity', 'OldPeople',\n            'PhysiciansDensity', 'AlcoholConsumptionPerCapita', 'CigaretteConsumptionPerCapita']\n# ext_cols = ['LaborForcePerCapita', 'DeathRate', 'AirTrafficPassengersPerCapita', 'HospitalBedDensity',\n#             'Obesity', 'OldPeople', 'PhysiciansDensity']\npop_cols = ['Yearly change', 'Density', 'Land Area', 'Med. Age', 'Urban Pop', 'Population']\n# pop_cols = ['Yearly change', 'Med. Age', 'Urban Pop', 'Density']\nadd_cols = ['DayNum', 'PreviousDay-0ExposedDensity']\n\n# loading day-by-day data (based on hopkins datasets) prepared by Kaggle\n# (it contains 'ConfirmedCases' and 'Fatalities')\ndata_base = base_folder + 'covid19-global-forecasting-week-4/'\ndf = pd.read_csv(data_base + 'train.csv').drop(columns=['Id'])\ndf.rename(columns={'Province_State': 'Province/State', 'Country_Region': 'Country/Region'}, inplace=True)\n\n# fill empty Province\ndf['Province/State'].fillna('entire country', inplace=True)\n\n# add Delta features\ndf['ConfirmedCasesDelta'] = df.groupby(['Country/Region', 'Province/State'])['ConfirmedCases'].diff().fillna(0)\ndf['FatalitiesDelta'] = df.groupby(['Country/Region', 'Province/State'])['Fatalities'].diff().fillna(0)\n\n\n# set the proper type for Date column and calculate DayNum\ndf['Date'] = pd.to_datetime(df['Date']).dt.date\nday_zero = min(df['Date'])\ndf['DayNum'] = (df['Date'] - day_zero).apply(lambda x: int(x.days))\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def add_extra_features_from_previous_days(data_fr, tail_size=5):\n    cols_tmp = []\n    col_prefix = 'PreviousDay'\n    for i in range (0, tail_size):\n        col_cc = '{}-{}ConfirmedCases'.format(col_prefix, i)\n        col_f  = '{}-{}Fatalities'.format(col_prefix, i)\n\n        data_fr[col_cc] = data_fr.groupby(['Country/Region', 'Province/State'])['ConfirmedCases'].shift(periods=i+1, fill_value=0)\n        data_fr[col_f] = data_fr.groupby(['Country/Region', 'Province/State'])['Fatalities'].shift(periods=i+1, fill_value=0)\n        data_fr[col_cc + 'Delta'] = data_fr.groupby(['Country/Region', 'Province/State'])[col_cc].diff().fillna(0)\n        data_fr[col_f + 'Delta'] = data_fr.groupby(['Country/Region', 'Province/State'])[col_f].diff().fillna(0)\n        cols_tmp += [col_cc, col_f, col_cc + 'Delta', col_f + 'Delta']\n    # df['PreviousDay-0ConfirmedCases'] = df.groupby(['Country/Region', 'Province/State'])['ConfirmedCases'].shift(periods=1, fill_value=0)\n    return  cols_tmp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# creating extra features from the history: previous day, previous day -1, previous day -2 ...\nTAIL = 50\nprevious_days_cols = add_extra_features_from_previous_days(df, TAIL)\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['PreviousDay-0ConfirmedCases'].max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def special_day_CC(org_df, number_of_cc):\n    # calculating for every country days from the day a patient # was confirmed\n    print('calculating for every Country & Province days passed from the first day when ConfirmedCases >= {}'.format(number_of_cc))\n    col_final = 'Day_CC{}'.format(number_of_cc)\n    col = 'Day_CC{}_zero'.format(number_of_cc)\n    org_df[col] = org_df.where(\n        (org_df['ConfirmedCases'] >= number_of_cc) &\n        ((org_df['PreviousDay-0ConfirmedCases'] < number_of_cc)|(org_df['Date'] == day_zero))\n    )['DayNum']\n    org_df[col] = org_df.groupby(['Country/Region', 'Province/State'])[col].ffill()\n    org_df[col] = org_df.groupby(['Country/Region', 'Province/State'])[col].bfill()\n    # calculating real DayNum counted from the day \"zero\"\n    day_num = org_df['DayNum'] - org_df[col] + 1\n    org_df[col_final] = (day_num - day_num.where(day_num<0).fillna(0)).fillna(0)\n    return col, col_final\n\ndef special_day_F(org_df, number_of_f):\n    # calculating for every country days from the day a patient # was confirmed\n    print('calculating for every Country & Province days passed from the first day when Fatalities >= {}'.format(number_of_f))\n    col_final = 'Day_F{}'.format(number_of_f)\n    col = 'Day_F{}_zero'.format(number_of_f)\n    org_df[col] = org_df.where(\n        (org_df['Fatalities'] >= number_of_f) & \n        ((org_df['PreviousDay-0Fatalities'] < number_of_f)|(org_df['Date'] == day_zero))\n    )['DayNum']\n    org_df[col] = org_df.groupby(['Country/Region', 'Province/State'])[col].ffill()\n    org_df[col] = org_df.groupby(['Country/Region', 'Province/State'])[col].bfill()\n    # calculating real DayNum counted from the day \"zero\"\n    day_num = org_df['DayNum'] - org_df[col] + 1\n    org_df[col_final] = (day_num - day_num.where(day_num<0).fillna(0)).fillna(0)\n    return col, col_final\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"special_cols1 = []\nspecial_cols2 = []\n\nc1, c2 = special_day_CC(df, 1)\nspecial_cols1.append(c1)\nspecial_cols2.append(c2)\nc1, c2 = special_day_CC(df, 50)\nspecial_cols1.append(c1)\nspecial_cols2.append(c2)\nc1, c2 = special_day_CC(df, 200)\nspecial_cols1.append(c1)\nspecial_cols2.append(c2)\nc1, c2 = special_day_CC(df, 500)\nspecial_cols1.append(c1)\nspecial_cols2.append(c2)\nc1, c2 = special_day_CC(df, 1000)\nspecial_cols1.append(c1)\nspecial_cols2.append(c2)\nc1, c2 = special_day_CC(df, 5000)\nspecial_cols1.append(c1)\nspecial_cols2.append(c2)\nc1, c2 = special_day_CC(df, 20000)\nspecial_cols1.append(c1)\nspecial_cols2.append(c2)\nc1, c2 = special_day_CC(df, 50000)\nspecial_cols1.append(c1)\nspecial_cols2.append(c2)\nc1, c2 = special_day_CC(df, 100000)\nspecial_cols1.append(c1)\nspecial_cols2.append(c2)\n\nc1, c2 = special_day_F(df, 1)\nspecial_cols1.append(c1)\nspecial_cols2.append(c2)\nc1, c2 = special_day_F(df, 25)\nspecial_cols1.append(c1)\nspecial_cols2.append(c2)\nc1, c2 = special_day_F(df, 50)\nspecial_cols1.append(c1)\nspecial_cols2.append(c2)\nc1, c2 = special_day_F(df, 200)\nspecial_cols1.append(c1)\nspecial_cols2.append(c2)\nc1, c2 = special_day_F(df, 500)\nspecial_cols1.append(c1)\nspecial_cols2.append(c2)\nc1, c2 = special_day_F(df, 1000)\nspecial_cols1.append(c1)\nspecial_cols2.append(c2)\nc1, c2 = special_day_F(df, 2000)\nspecial_cols1.append(c1)\nspecial_cols2.append(c2)\nc1, c2 = special_day_F(df, 5000)\nspecial_cols1.append(c1)\nspecial_cols2.append(c2)\nc1, c2 = special_day_F(df, 10000)\nspecial_cols1.append(c1)\nspecial_cols2.append(c2)\n\nspecial_cols2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# loading population data \ndf_population = pd.read_csv(base_folder + 'worldpopulaton-ver2/all_population.csv', delimiter=';', decimal=',', na_values='N.A.')\n# urban population: NaNs with 100% (it's a good estimation!)\ndf_population['Urban Pop'] = df_population['Urban Pop'].fillna(100.0)\n# OHE for a continent\ndf_population = pd.get_dummies(df_population, columns=['Continent'])\n# let's remember new columns for continents\ncontinent_columns = []\nfor c in df_population.columns:\n    if 'Continent_' in c:\n        continent_columns.append(c)\ndf_population","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Countries names map between World By Map and Hopking datasets\ncountries_to_replace = [\n    ('Czech Republic', 'Czechia'),\n    ('United States of America', 'US'),\n    ('Côte d\\'Ivoire (Ivory Coast)', 'Côte d\\'Ivoire'),\n    ('Korea (South)', 'Korea, South'),\n    ('Swaziland', 'Eswatini'),\n    ('Myanmar (Burma)', 'Burma'),\n    ('East Timor', 'Timor-Leste'),\n    ('Macedonia', 'North Macedonia'),\n    ('Cape Verde', 'Cabo Verde'),\n    ('Congo (Republic)', 'Congo (Brazzaville)'),\n    ('Congo (Democratic Republic)', 'Congo (Kinshasa)'),\n    ('Palestinian Territories', 'West Bank and Gaza'),\n    ('United Kingdom of Great Britain and Northern Ireland', 'United Kingdom'),\n    ('Vatican City', 'Holy See'),\n    ('Sao Tome & Principe', 'Sao Tome and Principe')\n]\n\ncountry_state_pairs_to_replace = [\n    (('Greenland', 'entire country'), ('Denmark', 'Greenland')),\n    (('Anguilla', 'entire country'), ('United Kingdom', 'Anguilla')),\n    (('Bermuda', 'entire country'), ('United Kingdom', 'Bermuda')),\n    (('British Virgin Islands', 'entire country'), ('United Kingdom', 'British Virgin Islands')),\n    (('Isle of Man', 'entire country'), ('United Kingdom', 'Isle of Man')),\n    (('Turks and Caicos Islands', 'entire country'), ('United Kingdom', 'Turks and Caicos Islands')),\n    (('Sint Maarten', 'entire country'), ('Netherlands', 'Sint Maarten')),\n    (('Saint Pierre & Miquelon', 'entire country'), ('France', 'Saint Pierre and Miquelon')),\n    (('Falkland Islands', 'entire country'), ('United Kingdom', 'Falkland Islands (Malvinas)'))\n    \n]\n# loading different datasets from World By Map\ncsv_dir = base_folder + 'worldbymap-ver2/'\nfiles = [\n    'labor_force',\n    'death_rate',\n    'air_traffic_passengers',\n    'hospital_bed_density',\n    'obesity',\n    'old_people',\n    'physicians_density',\n    'cigarettes',\n    'alcohol'\n]\nwbm = {}\nfor f in files:\n    wbm[f] = pd.read_csv(csv_dir + f + '.csv', delimiter=';', decimal=',', na_values='N.A.')\n    for ctr in countries_to_replace:\n        wbm[f]['Country'] = wbm[f]['Country'].replace(ctr[0], ctr[1])\n    for pair in country_state_pairs_to_replace:\n        ind = (wbm[f]['Country'] == pair[0][0]) & (wbm[f]['State'] == pair[0][1])\n        wbm[f].loc[ind, 'Country'] = pair[1][0]\n        wbm[f].loc[ind, 'State'] = pair[1][1]\nwbm[files[0]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_add = pd.DataFrame()\nfor dataset in wbm.keys():\n    if df_add.shape == (0, 0):\n        df_add = wbm[dataset].copy()\n    else:\n        df_add = pd.merge(df_add, wbm[dataset], on=['Country', 'State'], how='left')\ndf_add.rename(columns={\"Country\": \"Country/Region\", \"State\": \"Province/State\"}, inplace=True)\ndf_add","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# merging available external data\ndf_external = pd.merge(df_population, df_add, on=['Country/Region', 'Province/State'], how='left')\n\ndef fill_missing_percapita_values(dfr, feature_total, feature_percapita):\n    cond = (dfr[feature_percapita].isna()) & (df_external[feature_total].notna()) & (df_external['Population'].notna())\n    ind = df_external[cond].index\n    df_external.loc[ind, feature_percapita] = df_external.loc[ind, feature_total] / df_external.loc[ind, 'Population'] * 100.0\n\n# now we have to fill LaborForcePerCapita and AirTrafficPassengersPerCapita for some regions where total values have been given only  \nfill_missing_percapita_values(df_external, 'LaborForceTotal', 'LaborForcePerCapita')\nfill_missing_percapita_values(df_external, 'AirTrafficPassengersTotal', 'AirTrafficPassengersPerCapita')\n\n# filling NaNs in external data with column means\ndf_external[pop_cols+ext_cols] = df_external[pop_cols+ext_cols].apply(lambda x: x.fillna(x.mean()),axis=0)\n\n# changing names of some countries to\n\ndf_external","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# merging covid dataset with additional external data\ndf_pop = pd.merge(df, df_external, on=['Country/Region', 'Province/State'], how='left')\ndf_pop","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's see some charts showing COVID-19 spread in the past"},{"metadata":{"trusted":true},"cell_type":"code","source":"cond_ctry = [\n#     ((df['Country/Region']=='Poland') & (df['Province/State']=='entire country'), 'red'),\n    ((df['Country/Region']=='Germany') & (df['Province/State']=='entire country'), 'blue'),\n    ((df['Country/Region']=='China') & (df['Province/State']=='Hubei'), 'green'),\n    ((df['Country/Region']=='Italy') & (df['Province/State']=='entire country'), 'cyan'),\n    ((df['Country/Region']=='Spain') & (df['Province/State']=='entire country'), 'magenta'),\n    ((df['Country/Region']=='Korea, South') & (df['Province/State']=='entire country'), 'gray'),\n]\nstart_day = 'Day_F25'\ncond_day = df[start_day]>0\nfeature = 'Fatalities'\n\nfor i, cnd in enumerate(cond_ctry):\n    my_df = df_pop[(cnd[0]) & (cond_day)] \n    my_x = my_df[start_day]\n    my_y = my_df[feature]/my_df['PreviousDay-11ConfirmedCases']\n\n    chart_data = pd.DataFrame({\n        'x': my_x,\n        'y': my_y})\n\n    sns.lineplot(x='x', y='y', data=chart_data, color=cnd[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# With new datasets coming from Hopkins, some country names may change.\n# Uncomment 2 lines below and check if there are countries with missing population data.\nccc = pop_cols + ext_cols\ndf_pop[df_pop[ccc].isnull().any(axis=1)][['Country/Region', 'Province/State'] + ccc].drop_duplicates(subset=['Country/Region', 'Province/State'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# adding column \"ExposedDensity\" - population still exposed to covid per km2\n# 1.43 is my own factor for the exponential function indicating hidden spread of COVID (people having COVID but never diagnosed)\ndf_pop['ExposedDensity'] = (df_pop['Population'] - np.power(df_pop['ConfirmedCases'], 1.43))/df_pop['Land Area']\ndensity = df_pop.groupby(['Country/Region', 'Province/State'])['Density']\ndf_pop['PreviousDay-0ExposedDensity'] = df_pop.groupby(['Country/Region', 'Province/State'])['ExposedDensity'].shift(periods=1, fill_value=np.nan)\ndf_pop['PreviousDay-0ExposedDensity'] = df_pop.apply(\n    lambda row: row['Density'] if np.isnan(row['PreviousDay-0ExposedDensity']) else row['PreviousDay-0ExposedDensity'],\n    axis=1\n)\ndf_pop","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error, mean_squared_log_error\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, PowerTransformer\nfrom scipy import stats\n\nmodel_x_columns_without_dummies = add_cols + pop_cols + ext_cols + previous_days_cols + special_cols2\nmodel_x_columns = model_x_columns_without_dummies + continent_columns\n\ndef train_test_split(X, y, test_size=0.3, random_state=0):\n    day_first = min(X['DayNum'])\n    day_last = max(X['DayNum'])\n    number_of_days_for_train = int(round((day_last-day_first+1)*(1-test_size),0))\n    last_day_for_training = number_of_days_for_train + day_first - 1\n    X_tr = X[X['DayNum']<=last_day_for_training].copy()\n    y_tr = y[X['DayNum']<=last_day_for_training].copy()\n    X_te = X[X['DayNum']>last_day_for_training].copy()\n    y_te = y[X['DayNum']>last_day_for_training].copy()\n    return X_tr, X_te, y_tr, y_te\n\n# let's define an evaluation metric\ndef rmsle(ytrue, ypred):\n    return np.sqrt(mean_squared_log_error(ytrue, ypred))\n\ndef mae(ytrue, ypred):\n    return mean_absolute_error(ytrue, ypred)\n\ndef mse(ytrue, ypred):\n    return mean_squarred_error(ytrue, ypred)\n\ndef analyse3(tr_true, tr_pred, val_true, val_pred, test_true, test_pred):\n    chart_data0 = pd.DataFrame({\n        'x0': tr_true.flatten(),\n        'x1': tr_pred.flatten(),\n        'y': tr_true.flatten()-tr_pred.flatten()})\n\n    chart_data1 = pd.DataFrame({\n        'x0': val_true.flatten(),\n        'x1': val_pred.flatten(),\n        'y': val_true.flatten()-val_pred.flatten()})\n    \n    chart_data2 = pd.DataFrame({\n        'x0': test_true.flatten(),\n        'x1': test_pred.flatten(),\n        'y': test_true.flatten()-test_pred.flatten()})\n    \n    fig, ax =plt.subplots(1,3)\n    sns.scatterplot(x='x0', y='y', data=chart_data0, color='black', ax=ax[0])\n    sns.scatterplot(x='x1', y='y', data=chart_data0, color='red', ax=ax[0])\n    sns.scatterplot(x='x0', y='y', data=chart_data1, color='black', ax=ax[1])\n    sns.scatterplot(x='x1', y='y', data=chart_data1, color='red', ax=ax[1])\n    sns.scatterplot(x='x0', y='y', data=chart_data2, color='black', ax=ax[2])\n    sns.scatterplot(x='x1', y='y', data=chart_data2, color='red', ax=ax[2])\n    \n    print('MAE train: {}'.format(round(rmsle(tr_true, tr_pred), 6)))\n    print('MAE val:  {}'.format(round(rmsle(val_true, val_pred), 6)))\n    print('MAE test:  {}'.format(round(rmsle(test_true, test_pred), 6)))\n\ndef prepare_data(df, what_to_predict, test_size=0.3, dropna=False):\n    df_tmp = df.copy()\n    \n    if dropna:\n        df_tmp.dropna(inplace=True)\n    \n    df_tmp.loc[df_tmp[what_to_predict]<0, what_to_predict] = 0\n    # preparing X and y datasets for output model training\n    data_X = df_tmp[model_x_columns+['Country/Region']]\n    data_y = df_tmp[what_to_predict].values.flatten()\n    # splitting data to train and test\n    return train_test_split(data_X, data_y, test_size=test_size, random_state=42)\n    \ndef predict_output(input_data, model):\n    y_pred = np.abs(model.predict(input_data))\n    return y_pred\n\ndef expm1_relu(y):\n    tmp = np.expm1(y)\n    tmp[tmp<0]=0    \n    return np.around(tmp)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"scaler0 = None\nscaler1 = None\nscaler2 = None\n\ndef scale_data(data):\n    global scaler0, scaler1, scaler2\n    data_bis = data.copy()\n    daynum = data_bis['DayNum'].copy()\n    memory = dict()\n    for c in special_cols2:\n        memory[c] = data_bis[c].copy()\n        \n    if scaler1:\n        data_bis[model_x_columns_without_dummies] = scaler0.transform(data[model_x_columns_without_dummies])\n        data_bis[model_x_columns_without_dummies] = scaler1.transform(data[model_x_columns_without_dummies])\n        data_bis[model_x_columns_without_dummies] = scaler2.transform(data_bis[model_x_columns_without_dummies])\n\n    else:\n        scaler0 = PowerTransformer()\n        scaler1 = MinMaxScaler()\n        scaler2 = StandardScaler()\n        data_bis[model_x_columns_without_dummies] = scaler0.fit_transform(data[model_x_columns_without_dummies])\n        data_bis[model_x_columns_without_dummies] = scaler1.fit_transform(data[model_x_columns_without_dummies])\n        data_bis[model_x_columns_without_dummies] = scaler2.fit_transform(data_bis[model_x_columns_without_dummies])\n\n    for c in memory.keys():\n        data_bis[c] = memory[c]\n    data_bis['DayNum'] = daynum\n    \n    return data_bis\n\ndf_pop_bis = scale_data(df_pop)\ndf_pop_bis\n\n# ccc = pop_cols + ext_cols + special_cols2\n# df_pop[df_pop[ccc].isnull().any(axis=1)][['Country/Region', 'Province/State'] + pop_cols + ext_cols]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# neural network\nfrom keras.models import Sequential, load_model\nfrom keras.layers import Dense, Dropout, Activation, ELU\nfrom keras.metrics import mean_squared_error, mean_absolute_error, accuracy\nfrom keras.optimizers import Adam, SGD, RMSprop\nfrom keras.callbacks import ModelCheckpoint\n# from keras import regularizers\nfrom keras.regularizers import l1, l2, l1_l2\n\nmodel_path = join('.')\nmodel_file_f = join(model_path, 'nn_model_f.h5')\nmodel_file_cc = join(model_path, 'nn_model_cc.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# swish activation function\nfrom keras import backend as K\nfrom keras.utils.generic_utils import get_custom_objects\nfrom keras.activations import sigmoid\n\nclass Swish(Activation):\n    \n    def __init__(self, activation, **kwargs):\n        super(Swish, self).__init__(activation, **kwargs)\n        self.__name__ = 'swish'\n\ndef swish(x, beta = 0.6):\n    return (x * sigmoid(beta * x))\n\nget_custom_objects().update({'swish': Swish(swish)})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's create and train the model for 'FatalitiesDelta' prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_f = Sequential()\nmodel_f.add(Dense(50, input_dim=len(model_x_columns)-1, activation='swish'))\nmodel_f.add(Dropout(0.2))\nmodel_f.add(Dense(15, activation='elu'))\nmodel_f.add(Dropout(0.2))\nmodel_f.add(Dense(1, activation='elu'))\nopt_f = Adam(learning_rate=0.001, beta_1=0.94, beta_2=0.99, amsgrad=False)\nmodel_f.compile(loss='mean_absolute_error', optimizer=opt_f, metrics=['mae'])\n\nsave_best_only_callback_f = ModelCheckpoint(\n    filepath=model_file_f,\n    monitor='val_loss',\n    verbose=1,\n    save_best_only=True,\n    save_weights_only=False,\n    mode='min',\n    period=1\n)\n\ndata_X_tr, data_X_rest, data_y_tr, data_y_rest = prepare_data(df_pop_bis, 'FatalitiesDelta', test_size=0.3, dropna=False)\ndata_X_val, data_X_test, data_y_val, data_y_test = train_test_split(data_X_rest, data_y_rest, test_size=0.5, random_state=111)\n\nhistory = model_f.fit(x=data_X_tr[model_x_columns].drop(columns=['DayNum']), y=data_y_tr,\n                      validation_data=(data_X_val[model_x_columns].drop(columns=['DayNum']), data_y_val),\n                      epochs=170, batch_size=128, verbose=1, callbacks=[save_best_only_callback_f])\n\n# summarize history for loss\nplt.plot(history.history['loss'][0:])\nplt.plot(history.history['val_loss'][0:])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_f = load_model(model_file_f)\n\ntr_pred = predict_output(data_X_tr[model_x_columns].drop(columns=['DayNum']), model_f)\nval_pred = predict_output(data_X_val[model_x_columns].drop(columns=['DayNum']), model_f)\ntest_pred = predict_output(data_X_test[model_x_columns].drop(columns=['DayNum']), model_f)\n# print(data_y_test)\nanalyse3(data_y_tr, tr_pred,\n         data_y_val, val_pred,\n         data_y_test, test_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's create and train the model for 'ConfirmedCasesDelta' prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_cc = Sequential()\nmodel_cc.add(Dense(28, input_dim=len(model_x_columns)-1, activation='swish'))\nmodel_cc.add(Dropout(0.0))\nmodel_cc.add(Dense(15, activation='elu'))\nmodel_cc.add(Dropout(0.0))\nmodel_cc.add(Dense(1, activation='swish'))\n\nopt_cc = Adam(learning_rate=0.0001, beta_1=0.988, beta_2=0.99, amsgrad=False)\n\nmodel_cc.compile(loss='mean_absolute_error', optimizer=opt_cc, metrics=['mae'])\n\nsave_best_only_callback_cc = ModelCheckpoint(\n    filepath=model_file_cc,\n    monitor='val_loss',\n    verbose=1,\n    save_best_only=True,\n    save_weights_only=False,\n    mode='min',\n    period=1\n)\n\ndata_X_tr, data_X_rest, data_y_tr, data_y_rest = prepare_data(df_pop_bis, 'ConfirmedCasesDelta', test_size=0.3, dropna=False)\ndata_X_val, data_X_test, data_y_val, data_y_test = train_test_split(data_X_rest, data_y_rest, test_size=0.5, random_state=111)\n\nhistory = model_cc.fit(x=data_X_tr[model_x_columns].drop(columns=['DayNum']), y=data_y_tr,\n                       validation_data=(data_X_val[model_x_columns].drop(columns=['DayNum']), data_y_val),\n                       epochs=770, batch_size=128, verbose=1, callbacks=[save_best_only_callback_cc])\n\n# summarize history for loss\nplt.plot(history.history['loss'][0:])\nplt.plot(history.history['val_loss'][0:])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_cc = load_model(model_file_cc)\n\ntr_pred = predict_output(data_X_tr[model_x_columns].drop(columns=['DayNum']), model_cc)\nval_pred = predict_output(data_X_val[model_x_columns].drop(columns=['DayNum']), model_cc)\ntest_pred = predict_output(data_X_test[model_x_columns].drop(columns=['DayNum']), model_cc)\n# print(data_y_test)\nanalyse3((data_y_tr), (tr_pred),\n         (data_y_val), (val_pred),\n         (data_y_test), (test_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### A SHORT BREAK AFTER INTENSIVE TRAINING\n\n#### WHAT DO WE REALLY KNOW ABOUT THE SPREAD OF COVID-19 IN INDIVIDUAL COUNTRIES AND REGIONS?\n\nNot much. \n\n'ConfirmedCases' can be very confusing. Some countries may have had a \"day zero\" long before the first case was revealed. There are many people getting through the COVID-19 without even knowing it, and some never get examined for this desease. There are many factors influencing the accuracy of CC feature.\n\n'Fatalities' - this is probably the most informative feature. It is hardly imaginable that someone passes away and doctors don't know it was COVID-19 or not.\n\nTherefore when comparing the pandemy between countries, I like to use 'Fatalities' to compare the spread pace."},{"metadata":{"trusted":true},"cell_type":"code","source":"aaa = dict()\nchart_data = dict()\ncountries = ['US', 'France', 'Korea, South', 'Spain', 'Poland', 'Italy', 'Japan', 'Iran']\ncolors = ['red', 'blue', 'orange', 'black', 'cyan', 'brown', 'grey', 'purple']\n\nfor ind, (c, color) in enumerate(zip(countries, colors)):\n    tmp = df_pop_bis.where(df_pop_bis['Country/Region']==c).dropna(subset=['DayNum'])\n    aaa[c] = tmp[tmp['Day_CC1']>0]\n    chart_data[c] = pd.DataFrame({\n        'x': aaa[c]['Day_CC1'],\n        'y': np.log1p(aaa[c]['Fatalities'])})\n\n    sns.lineplot(x='x', y='y', data=chart_data[c], color=color)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Reading test.csv, feature engineering on the test subset, predicting."},{"metadata":{"trusted":true},"cell_type":"code","source":"# prepare test data\ndf_test = pd.read_csv(data_base + 'test.csv')\ndf_test.rename(columns={'Province_State': 'Province/State', 'Country_Region': 'Country/Region'}, inplace=True)\n\n# replace empty province\ndf_test['Province/State'].fillna('entire country', inplace=True)\n\n# set proper type for the Date column and calculate DayNum\ndf_test['Date'] = pd.to_datetime(df_test['Date']).dt.date\ndf_test['DayNum'] = (df_test['Date'] - day_zero).apply(lambda x: int(x.days))\n\n# get countries' special days from df train dataset, join them with the test dataset and set the counter for each such special day\nfor c1, c2 in zip(special_cols1, special_cols2):\n    zero_days = pd.DataFrame(df.groupby(['Country/Region', 'Province/State', c1]).size().reset_index()[['Country/Region', 'Province/State', c1]])\n    zero_days.drop_duplicates(subset=['Country/Region', 'Province/State'], keep='last', inplace=True)\n    df_test = df_test.merge(zero_days, on=['Country/Region', 'Province/State'], how='left')\n    real_day_num = df_test['DayNum'] - df_test[c1] + 1\n    df_test[c2] = (real_day_num - real_day_num.where(real_day_num<0).fillna(0)).fillna(0)\ndf_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# merging df_test with population data\ndf_test_pop = pd.merge(df_test, df_external, on=['Country/Region', 'Province/State'], how='left')\ndf_test_pop","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Let's start day by day prediction."},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's take available data from training dataset (overlap with test dataset)\noutput_columns = ['ConfirmedCases', 'Fatalities']\ntmp_output_columns = ['ConfirmedCases_y', 'Fatalities_y']\n\nlast_training_day = df['DayNum'].max()\nfirst_test_day = df_test['DayNum'].min()\ntrain_test_keys = ['Country/Region', 'Province/State', 'DayNum']\ndf_test_pop_train = pd.merge(df_test_pop, df_pop[df_pop['DayNum']>=first_test_day][train_test_keys + ['PreviousDay-0ExposedDensity', 'ExposedDensity'] + previous_days_cols + output_columns],\n                             on=train_test_keys, how='left')\ndf_test_pop_train\n\n# ccc = pop_cols + ext_cols + special_cols2\n# let's check if there are some missing data now\n# df_test_pop[df_test_pop[ccc].isnull().any(axis=1)][train_test_keys + pop_cols + ext_cols]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's stick training and test datasets (we need it to have previous days info)\ndf_test_final = pd.concat([df_pop[df_pop['DayNum']<first_test_day], df_test_pop_train]).reset_index(drop=True)\ndf_test_final[(df_test_final['Country/Region']=='Poland')&(df_test_final['DayNum']<=last_training_day+1)&(df_test_final['DayNum']>last_training_day-10)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we need do keep some feature without scaling to calculation ExposedDensity\ndf_test_final['PopulationOrg'] = df_test_final['Population'].copy()\ndf_test_final['Land Area Org'] = df_test_final['Land Area'].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# final loop to predict every day separately and to make feature engineering on-the-fly\nmodel_cc = load_model(model_file_cc)\nmodel_f = load_model(model_file_f)\n\nlast_test_day = df_test['DayNum'].max()\nfor day in range(last_training_day+1, last_test_day+1):\n    print('predicting day {} ({} to go)'.format(day, last_test_day-day))\n    # calculate columns for previous days\n    add_extra_features_from_previous_days(df_test_final, TAIL)\n    # keep unscaled previous day output data\n    df_test_final['PreviousDay-0ConfirmedCases_NotScaled'] = df_test_final['PreviousDay-0ConfirmedCases'].copy()\n    df_test_final['PreviousDay-0Fatalities_NotScaled'] = df_test_final['PreviousDay-0Fatalities'].copy()\n    # add ExposedDensity\n    df_test_final['PreviousDay-0ExposedDensity'] = df_test_final.groupby(['Country/Region', 'Province/State'])['ExposedDensity'].shift(periods=1, fill_value=np.nan)\n    df_test_final['PreviousDay-0ExposedDensity'] = df_test_final.apply(\n        lambda row: row['Density'] if np.isnan(row['PreviousDay-0ExposedDensity']) else row['PreviousDay-0ExposedDensity'],\n        axis=1\n    ) \n    # get current day only\n    current_day = df_test_final[df_test_final['DayNum']==day].copy()\n    # scale data\n    current_day_scaled = scale_data(current_day)\n    # predict output for the current day\n    current_day_scaled['ConfirmedCasesDelta'] = predict_output(current_day_scaled[model_x_columns].drop(columns=['DayNum']), model_cc)\n    current_day_scaled['FatalitiesDelta'] = predict_output(current_day_scaled[model_x_columns].drop(columns=['DayNum']), model_f)\n    current_day_scaled['ConfirmedCases'] = current_day_scaled['PreviousDay-0ConfirmedCases_NotScaled'] + current_day_scaled['ConfirmedCasesDelta']\n    current_day_scaled['Fatalities'] = current_day_scaled['PreviousDay-0Fatalities_NotScaled'] + current_day_scaled['FatalitiesDelta']\n    # fill ExposedDensity\n    current_day_scaled['ExposedDensity'] = (current_day_scaled['PopulationOrg'] - np.power(current_day_scaled['ConfirmedCases'], 1.43))/current_day_scaled['Land Area Org']\n\n    # fill df_test with current day predictions\n    cond = df_test_final['DayNum']==day\n    df_test_final.loc[cond, output_columns+['ExposedDensity']] = current_day_scaled[output_columns+['ExposedDensity']].copy()\n    df_test_final.loc[(cond)&(df_test_final[cond]['ConfirmedCases']<current_day_scaled['ConfirmedCases']), 'ConfirmedCases'] = df_test_final['PreviousDay-0ConfirmedCases']\n    df_test_final.loc[(cond)&(df_test_final[cond]['Fatalities']<current_day_scaled['Fatalities']), 'Fatalities'] = df_test_final['PreviousDay-0Fatalities']\n\ndf_test_final.columns\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_columns = ['ForecastId', 'ConfirmedCases', 'Fatalities']\n# convert to int\ndf_test_final.loc[df_test_final['ForecastId'].isna(), 'ForecastId'] = 0\ndf_test_final[submission_columns] = df_test_final[submission_columns].astype(int)\n# save submission\ndf_test_final[df_test_final['DayNum']>=first_test_day][submission_columns].to_csv('submission.csv', index=False)\n# submission view\ndf_test_final[df_test_final['DayNum']>=last_training_day][submission_columns]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test_final['ConfirmedCases'].max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test_final['Fatalities'].max()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":4}