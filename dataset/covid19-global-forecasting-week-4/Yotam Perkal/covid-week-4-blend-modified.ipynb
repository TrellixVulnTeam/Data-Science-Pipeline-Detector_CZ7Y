{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This kernel is a simple modification of the amazing covid19-w3-submission-blend-4-models kernel."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"code","source":"from tensorflow.keras.optimizers import Nadam\nfrom sklearn.metrics import mean_squared_error\nimport tensorflow as tf\nimport tensorflow.keras.layers as KL\nfrom datetime import timedelta\nimport numpy as np\nimport pandas as pd\nimport tensorflow.keras.backend as K\n\nimport os\npd.options.display.max_rows = 500\npd.options.display.max_columns = 500\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom scipy.signal import savgol_filter\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.isotonic import IsotonicRegression\nfrom sklearn.metrics import mean_squared_error, mean_squared_log_error\nfrom sklearn.linear_model import LinearRegression, Ridge\n\nimport datetime\nimport gc\nfrom tqdm import tqdm\n\nimport xgboost as xgb\n\ndef rmse( yt, yp ):\n    return np.sqrt( np.mean( (yt-yp)**2 ) )\n\n\n\n\nclass CovidModel:\n    def __init__(self):\n        pass\n    \n    def predict_first_day(self, date):\n        return None\n    \n    def predict_next_day(self, yesterday_pred_df):\n        return None\n\n\nclass CovidModelAhmet(CovidModel):\n    def preprocess(self, df, meta_df):\n        df[\"Date\"] = pd.to_datetime(df['Date'])\n\n        df = df.merge(meta_df, on=self.loc_group, how=\"left\")\n        df[\"lat\"] = (df[\"lat\"] // 30).astype(np.float32).fillna(0)\n        df[\"lon\"] = (df[\"lon\"] // 60).astype(np.float32).fillna(0)\n\n        df[\"population\"] = np.log1p(df[\"population\"]).fillna(-1)\n        df[\"area\"] = np.log1p(df[\"area\"]).fillna(-1)\n\n        for col in self.loc_group:\n            df[col].fillna(\"\", inplace=True)\n            \n        df['day'] = df.Date.dt.dayofyear\n        df['geo'] = ['_'.join(x) for x in zip(df['Country_Region'], df['Province_State'])]\n        return df\n\n    def get_model(self):\n        \n        def nn_block(input_layer, size, dropout_rate, activation):\n            out_layer = KL.Dense(size, activation=None)(input_layer)\n            out_layer = KL.Activation(activation)(out_layer)\n            out_layer = KL.Dropout(dropout_rate)(out_layer)\n            return out_layer\n    \n        ts_inp = KL.Input(shape=(len(self.ts_features),))\n        global_inp = KL.Input(shape=(len(self.global_features),))\n\n        inp = KL.concatenate([global_inp, ts_inp])\n        hidden_layer = nn_block(inp, 64, 0.0, \"relu\")\n        gate_layer = nn_block(hidden_layer, 32, 0.0, \"sigmoid\")\n        hidden_layer = nn_block(hidden_layer, 32, 0.0, \"relu\")\n        hidden_layer = KL.multiply([hidden_layer, gate_layer])\n\n        out = KL.Dense(len(self.TARGETS), activation=\"linear\")(hidden_layer)\n\n        model = tf.keras.models.Model(inputs=[global_inp, ts_inp], outputs=out)\n        return model\n    \n    def get_input(self, df):\n        return [df[self.global_features], df[self.ts_features]]\n        \n    def train_models(self, df, num_models=20, save=False):\n        \n        def custom_loss(y_true, y_pred):\n            return K.sum(K.sqrt(K.sum(K.square(y_true - y_pred), axis=0, keepdims=True)))/len(self.TARGETS)\n    \n        models = []\n        for i in range(num_models):\n            model = self.get_model()\n            model.compile(loss=custom_loss, optimizer=Nadam(lr=1e-4))\n            hist = model.fit(self.get_input(df), df[self.TARGETS],\n                             batch_size=2048, epochs=200, verbose=0, shuffle=True)\n            if save:\n                model.save_weights(\"model{}.h5\".format(i))\n            models.append(model)\n        return models\n    \n    \n    def predict_one(self, df):\n        \n        pred = np.zeros((df.shape[0], 2))\n        for model in self.models:\n            pred += model.predict(self.get_input(df))/len(self.models)\n        pred = np.maximum(pred, df[self.prev_targets].values)\n        pred[:, 0] = np.log1p(np.expm1(pred[:, 0]) + 0.1)\n        pred[:, 1] = np.log1p(np.expm1(pred[:, 1]) + 0.01)\n        return np.clip(pred, None, 15)\n    \n\n    def __init__(self):\n        df = pd.read_csv(\"../input/covid19-global-forecasting-week-4/train.csv\")\n        sub_df = pd.read_csv(\"../input/covid19-global-forecasting-week-4/test.csv\")\n\n        meta_df = pd.read_csv(\"../input/covid19-forecasting-metadata/region_metadata.csv\")\n\n        self.loc_group = [\"Province_State\", \"Country_Region\"]\n\n        df = self.preprocess(df, meta_df)\n        sub_df = self.preprocess(sub_df, meta_df)\n        \n        df = df.merge(sub_df[[\"ForecastId\", \"Date\", \"geo\"]], how=\"left\", on=[\"Date\", \"geo\"])\n        df = df.append(sub_df[sub_df[\"Date\"] > df[\"Date\"].max()], sort=False)\n        \n        df[\"day\"] = df[\"day\"] - df[\"day\"].min()\n\n        self.TARGETS = [\"ConfirmedCases\", \"Fatalities\"]\n        self.prev_targets = ['prev_ConfirmedCases_1', 'prev_Fatalities_1']\n\n        for col in self.TARGETS:\n            df[col] = np.log1p(df[col])\n\n        self.NUM_SHIFT = 7\n\n        self.global_features = [\"lat\", \"lon\", \"population\", \"area\"]\n        self.ts_features = []\n\n        for s in range(1, self.NUM_SHIFT+1):\n            for col in self.TARGETS:\n                df[\"prev_{}_{}\".format(col, s)] = df.groupby(self.loc_group)[col].shift(s)\n                self.ts_features.append(\"prev_{}_{}\".format(col, s))\n\n        self.df = df[df[\"Date\"] >= df[\"Date\"].min() + timedelta(days=self.NUM_SHIFT)].copy()\n\n        \n    def predict_first_day(self, day):\n        self.models = self.train_models(self.df[self.df[\"day\"] < day])\n        \n        temp_df = self.df.loc[self.df[\"day\"] == day].copy()\n        y_pred = self.predict_one(temp_df)\n            \n        self.y_prevs = [None]*self.NUM_SHIFT\n\n        for i in range(1, self.NUM_SHIFT):\n            self.y_prevs[i] = temp_df[['prev_ConfirmedCases_{}'.format(i), 'prev_Fatalities_{}'.format(i)]].values\n            \n        temp_df[self.TARGETS] = y_pred\n        self.day = day\n        return temp_df[[\"geo\", \"day\"] + self.TARGETS]\n    \n    \n    def predict_next_day(self, yesterday_pred_df):\n        self.day = self.day + 1\n\n        temp_df = self.df.loc[self.df[\"day\"] == self.day].copy()\n        \n        yesterday_pred_df = temp_df[[\"geo\"]].merge(yesterday_pred_df[[\"geo\"] + self.TARGETS], on=\"geo\", how=\"left\")\n        temp_df[self.prev_targets] = yesterday_pred_df[self.TARGETS].values\n\n        for i in range(2, self.NUM_SHIFT+1):\n            temp_df[['prev_ConfirmedCases_{}'.format(i), 'prev_Fatalities_{}'.format(i)]] = self.y_prevs[i-1]\n\n        y_pred, self.y_prevs = self.predict_one(temp_df), [None, temp_df[self.prev_targets].values] + self.y_prevs[1:-1]\n\n        temp_df[self.TARGETS] = y_pred\n        return temp_df[[\"geo\", \"day\"] + self.TARGETS]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class CovidModel:\n    def __init__(self):\n        pass\n    \n    def predict_first_day(self, date):\n        return None\n    \n    def predict_next_day(self, yesterday_pred_df):\n        return None\n    \n\nclass CovidModelGIBA(CovidModel):\n    def __init__(self, lag=1, seed=1 ):\n\n        self.lag  = lag\n        self.seed = seed\n        print( 'Lag:', lag, 'Seed:', seed )\n        \n        train = pd.read_csv('../input/covid19-global-forecasting-week-4/train.csv')\n        train['Date'] = pd.to_datetime( train['Date'] )\n        self.maxdate  = str(train['Date'].max())[:10]\n        self.testdate = str( train['Date'].max() + pd.Timedelta(days=1) )[:10]\n        print( 'Last Date in Train:',self.maxdate, 'Test first Date:',self.testdate )\n        train['Province_State'].fillna('', inplace=True)\n        train['day'] = train.Date.dt.dayofyear\n        self.day_min = train['day'].min()\n        train['day'] -= self.day_min\n        train['geo'] = ['_'.join(x) for x in zip(train['Country_Region'], train['Province_State'])]\n\n        test  = pd.read_csv('../input/covid19-global-forecasting-week-4/test.csv')\n        test['Date'] = pd.to_datetime( test['Date'] )\n        test['Province_State'].fillna('', inplace=True)\n        test['day'] = test.Date.dt.dayofyear\n        test['day'] -= self.day_min\n        test['geo'] = ['_'.join(x) for x in zip(test['Country_Region'], test['Province_State'])]\n        test['Id'] = -1\n        test['ConfirmedCases'] = 0\n        test['Fatalities'] = 0\n\n        self.trainmaxday  = train['day'].max()\n        self.testday1 = train['day'].max() + 1\n        self.testdayN = test['day'].max()\n        \n        publictest = test.loc[ test.Date > train.Date.max() ].copy()\n        train = pd.concat( (train, publictest ), sort=False )\n        train.sort_values( ['Country_Region','Province_State','Date'], inplace=True )\n        train = train.reset_index(drop=True)\n\n        train['ForecastId'] = pd.merge( train, test, on=['Country_Region','Province_State','Date'], how='left' )['ForecastId_y'].values\n\n        train['cid'] = train['Country_Region'] + '_' + train['Province_State']\n\n        train['log0'] = np.log1p( train['ConfirmedCases'] )\n        train['log1'] = np.log1p( train['Fatalities'] )\n\n        train = train.loc[ (train.log0 > 0) | (train.ForecastId.notnull()) | (train.Date >= '2020-03-17') ].copy()\n        train = train.reset_index(drop=True)\n\n        train['days_since_1case'] = train.groupby('cid')['Id'].cumcount()\n\n        dt = pd.read_csv('../input/covid19-lockdown-dates-by-country/countryLockdowndates.csv')\n        dt.columns = ['Country_Region','Province_State','Date','Type','Reference']\n        dt = dt.loc[ dt.Date == dt.Date ]\n        dt['Province_State'] = dt['Province_State'].fillna('')\n        dt['Date'] = pd.to_datetime( dt['Date'] )\n        dt['Date'] = dt['Date'] + pd.Timedelta(days=8)\n        dt['Type'] = pd.factorize( dt['Type'] )[0]\n        dt['cid'] = dt['Country_Region'] + '_' + dt['Province_State']\n        del dt['Reference'], dt['Country_Region'], dt['Province_State']\n        train = pd.merge( train, dt, on=['cid','Date'], how='left' )\n        train['Type'] = train.groupby('cid')['Type'].fillna( method='ffill' )\n\n        train['target0'] = np.log1p( train['ConfirmedCases'] )\n        train['target1'] = np.log1p( train['Fatalities'] )\n\n        \n        self.train = train.copy()\n   \n    \n    def create_features( self, df, valid_day ):\n\n        #df = df.loc[ df.day<=valid_day ].copy()\n        df = df.loc[ df.day>=(valid_day-50) ].copy()\n        \n        df['lag0_1'] = df.groupby('cid')['target0'].shift(self.lag)\n        df['lag0_1'] = df.groupby('cid')['lag0_1'].fillna( method='bfill' )\n\n        df['lag0_8'] = df.groupby('cid')['target0'].shift(8)\n        df['lag0_8'] = df.groupby('cid')['lag0_8'].fillna( method='bfill' )\n        \n        df['lag1_1'] = df.groupby('cid')['target1'].shift(self.lag)\n        df['lag1_1'] = df.groupby('cid')['lag1_1'].fillna( method='bfill' )\n\n        df['m0'] = df.groupby('cid')['lag0_1'].rolling(2).mean().values\n        df['m1'] = df.groupby('cid')['lag0_1'].rolling(3).mean().values\n        df['m2'] = df.groupby('cid')['lag0_1'].rolling(4).mean().values\n        df['m3'] = df.groupby('cid')['lag0_1'].rolling(5).mean().values\n        df['m4'] = df.groupby('cid')['lag0_1'].rolling(7).mean().values\n        df['m5'] = df.groupby('cid')['lag0_1'].rolling(10).mean().values\n        df['m6'] = df.groupby('cid')['lag0_1'].rolling(12).mean().values\n        df['m7'] = df.groupby('cid')['lag0_1'].rolling(16).mean().values\n        df['m8'] = df.groupby('cid')['lag0_1'].rolling(20).mean().values\n        df['m9'] = df.groupby('cid')['lag0_1'].rolling(25).mean().values\n\n        df['n0'] = df.groupby('cid')['lag1_1'].rolling(2).mean().values\n        df['n1'] = df.groupby('cid')['lag1_1'].rolling(3).mean().values\n        df['n2'] = df.groupby('cid')['lag1_1'].rolling(4).mean().values\n        df['n3'] = df.groupby('cid')['lag1_1'].rolling(5).mean().values\n        df['n4'] = df.groupby('cid')['lag1_1'].rolling(7).mean().values\n        df['n5'] = df.groupby('cid')['lag1_1'].rolling(10).mean().values\n        df['n6'] = df.groupby('cid')['lag1_1'].rolling(12).mean().values\n        df['n7'] = df.groupby('cid')['lag1_1'].rolling(16).mean().values\n        df['n8'] = df.groupby('cid')['lag1_1'].rolling(20).mean().values\n\n\n        df['m0'] = df.groupby('cid')['m0'].fillna( method='bfill' )\n        df['m1'] = df.groupby('cid')['m1'].fillna( method='bfill' )\n        df['m2'] = df.groupby('cid')['m2'].fillna( method='bfill' )\n        df['m3'] = df.groupby('cid')['m3'].fillna( method='bfill' )\n        df['m4'] = df.groupby('cid')['m4'].fillna( method='bfill' )\n        df['m5'] = df.groupby('cid')['m5'].fillna( method='bfill' )\n        df['m6'] = df.groupby('cid')['m6'].fillna( method='bfill' )\n        df['m7'] = df.groupby('cid')['m7'].fillna( method='bfill' )\n        df['m8'] = df.groupby('cid')['m8'].fillna( method='bfill' )\n        df['m9'] = df.groupby('cid')['m9'].fillna( method='bfill' )\n\n        df['n0'] = df.groupby('cid')['n0'].fillna( method='bfill' )\n        df['n1'] = df.groupby('cid')['n1'].fillna( method='bfill' )\n        df['n2'] = df.groupby('cid')['n2'].fillna( method='bfill' )\n        df['n3'] = df.groupby('cid')['n3'].fillna( method='bfill' )\n        df['n4'] = df.groupby('cid')['n4'].fillna( method='bfill' )\n        df['n5'] = df.groupby('cid')['n5'].fillna( method='bfill' )\n        df['n6'] = df.groupby('cid')['n6'].fillna( method='bfill' )\n        df['n7'] = df.groupby('cid')['n7'].fillna( method='bfill' )\n        df['n8'] = df.groupby('cid')['n8'].fillna( method='bfill' )\n\n        df['flag_China'] = 1*(df['Country_Region'] == 'China')\n        #df['flag_Italy'] = 1*(df['Country_Region'] == 'Italy')\n        #df['flag_Spain'] = 1*(df['Country_Region'] == 'Spain')\n        df['flag_US']    = 1*(df['Country_Region'] == 'US')\n        #df['flag_Brazil']= 1*(df['Country_Region'] == 'Brazil')\n        \n        df['flag_Kosovo_']   = 1*(df['cid'] == 'Kosovo_')\n        df['flag_Korea']     = 1*(df['cid'] == 'Korea, South_')\n        df['flag_Nepal_']    = 1*(df['cid'] == 'Nepal_')\n        df['flag_Holy See_'] = 1*(df['cid'] == 'Holy See_')\n        df['flag_Suriname_'] = 1*(df['cid'] == 'Suriname_')\n        df['flag_Ghana_']    = 1*(df['cid'] == 'Ghana_')\n        df['flag_Togo_']     = 1*(df['cid'] == 'Togo_')\n        df['flag_Malaysia_'] = 1*(df['cid'] == 'Malaysia_')\n        df['flag_US_Rhode']  = 1*(df['cid'] == 'US_Rhode Island')\n        df['flag_Bolivia_']  = 1*(df['cid'] == 'Bolivia_')\n        df['flag_China_Tib'] = 1*(df['cid'] == 'China_Tibet')\n        df['flag_Bahrain_']  = 1*(df['cid'] == 'Bahrain_')\n        df['flag_Honduras_'] = 1*(df['cid'] == 'Honduras_')\n        df['flag_Bangladesh']= 1*(df['cid'] == 'Bangladesh_')\n        df['flag_Paraguay_'] = 1*(df['cid'] == 'Paraguay_')\n\n        tr = df.loc[ df.day  < valid_day ].copy()\n        vl = df.loc[ df.day == valid_day ].copy()\n\n        tr = tr.loc[ tr.lag0_1 > 0 ].copy()\n\n        maptarget0 = tr.groupby('cid')['target0'].agg( log0_max='max' ).reset_index()\n        maptarget1 = tr.groupby('cid')['target1'].agg( log1_max='max' ).reset_index()\n        vl['log0_max'] = pd.merge( vl, maptarget0, on='cid' , how='left' )['log0_max'].values\n        vl['log1_max'] = pd.merge( vl, maptarget1, on='cid' , how='left' )['log1_max'].values\n        vl['log0_max'] = vl['log0_max'].fillna(0)\n        vl['log1_max'] = vl['log1_max'].fillna(0)\n\n        return tr, vl\n    \n\n    def train_models(self, valid_day = 10 ):\n\n        train = self.train.copy()\n\n        #Fix some anomalities:\n        train.loc[ (train.cid=='China_Guizhou') & (train.Date=='2020-03-17') , 'target0' ] = np.log1p( 146 )\n        train.loc[ (train.cid=='Guyana_')&(train.Date>='2020-03-22')&(train.Date<='2020-03-30') , 'target0' ] = np.log1p( 12 )\n        train.loc[ (train.cid=='US_Virgin Islands')&(train.Date>='2020-03-29')&(train.Date<='2020-03-29') , 'target0' ] = np.log1p( 24 )\n        train.loc[ (train.cid=='US_Virgin Islands')&(train.Date>='2020-03-30')&(train.Date<='2020-03-30') , 'target0' ] = np.log1p( 27 )\n\n        train.loc[ (train.cid=='Iceland_')&(train.Date>='2020-03-15')&(train.Date<='2020-03-15') , 'target1' ] = np.log1p( 0 )\n        train.loc[ (train.cid=='Kazakhstan_')&(train.Date>='2020-03-20')&(train.Date<='2020-03-20') , 'target1' ] = np.log1p( 0 )\n        train.loc[ (train.cid=='Serbia_')&(train.Date>='2020-03-26')&(train.Date<='2020-03-26') , 'target1' ] = np.log1p( 5 )\n        train.loc[ (train.cid=='Serbia_')&(train.Date>='2020-03-27')&(train.Date<='2020-03-27') , 'target1' ] = np.log1p( 6 )\n        train.loc[ (train.cid=='Slovakia_')&(train.Date>='2020-03-22')&(train.Date<='2020-03-31') , 'target1' ] = np.log1p( 1 )\n        train.loc[ (train.cid=='US_Hawaii')&(train.Date>='2020-03-25')&(train.Date<='2020-03-31') , 'target1' ] = np.log1p( 1 )\n\n        param = {\n            'subsample': 1.000,\n            'colsample_bytree': 0.85,\n            'max_depth': 5,\n            'gamma': 0.000,\n            'learning_rate': 0.010,\n            'min_child_weight': 6.00,\n            'reg_alpha': 0.000,\n            'reg_lambda': 0.400,\n            'silent':1,\n            'objective':'reg:squarederror',\n            #'booster':'dart',\n            #'tree_method': 'gpu_hist',\n            'nthread': 12,#-1,\n            'seed': self.seed\n            }    \n        \n        tr, vl = self.create_features( train.copy(), valid_day )\n        #Features for Cases\n        features = [f for f in tr.columns if f not in [\n            #'flag_China','flag_US',\n            #'flag_Kosovo_','flag_Korea','flag_Nepal_','flag_Holy See_','flag_Suriname_','flag_Ghana_','flag_Togo_','flag_Malaysia_','flag_US_Rhode','flag_Bolivia_','flag_China_Tib','flag_Bahrain_','flag_Honduras_','flag_Bangladesh','flag_Paraguay_',\n            'lag0_8',\n            'Id','ConfirmedCases','Fatalities','log0','log1','target0','target1','ypred0','ypred1','Province_State','Country_Region','Date','ForecastId','cid','geo','day',\n            'GDP_region','TRUE POPULATION','pct_in_largest_city',' TFR ',' Avg_age ','latitude','longitude','abs_latitude','temperature', 'humidity',\n            'Personality_pdi','Personality_idv','Personality_mas','Personality_uai','Personality_ltowvs','Personality_assertive','personality_perform','personality_agreeableness',\n            'murder','High_rises','max_high_rises','AIR_CITIES','AIR_AVG','continent_gdp_pc','continent_happiness','continent_generosity','continent_corruption','continent_Life_expectancy'        \n        ] ]\n        self.features0 = features\n        #Features for Fatalities\n        features = [f for f in tr.columns if f not in [\n            'm0','m1','m2','m3',\n            #'flag_China','flag_US',\n            #'flag_Kosovo_','flag_Korea','flag_Nepal_','flag_Holy See_','flag_Suriname_','flag_Ghana_','flag_Togo_','flag_Malaysia_','flag_US_Rhode','flag_Bolivia_','flag_China_Tib','flag_Bahrain_','flag_Honduras_','flag_Bangladesh','flag_Paraguay_',\n            'Id','ConfirmedCases','Fatalities','log0','log1','target0','target1','ypred0','ypred1','Province_State','Country_Region','Date','ForecastId','cid','geo','day',\n            'GDP_region','TRUE POPULATION','pct_in_largest_city',' TFR ',' Avg_age ','latitude','longitude','abs_latitude','temperature', 'humidity',\n            'Personality_pdi','Personality_idv','Personality_mas','Personality_uai','Personality_ltowvs','Personality_assertive','personality_perform','personality_agreeableness',\n            'murder','High_rises','max_high_rises','AIR_CITIES','AIR_AVG','continent_gdp_pc','continent_happiness','continent_generosity','continent_corruption','continent_Life_expectancy'        \n        ] ]\n        self.features1 = features\n        \n\n        nrounds0 = 680\n        nrounds1 = 630\n         #lag 1###############################################################\n        dtrain = xgb.DMatrix( tr[self.features0], tr['target0'] )\n        param['seed'] = self.seed\n        self.model0 = xgb.train( param, dtrain, nrounds0, verbose_eval=0 )\n        param['seed'] = self.seed+1\n        self.model1 = xgb.train( param, dtrain, nrounds0, verbose_eval=0 )\n        \n        dtrain = xgb.DMatrix( tr[self.features1], tr['target1'] )\n        param['seed'] = self.seed\n        self.model2 = xgb.train( param, dtrain, nrounds1, verbose_eval=0 ) \n        param['seed'] = self.seed+1\n        self.model3 = xgb.train( param, dtrain, nrounds1, verbose_eval=0 )\n        \n        self.vl = vl\n        \n        return 1\n    \n        \n    def predict_first_day(self, day ):\n        \n        self.day = day\n        self.train_models( day )\n        \n        dvalid = xgb.DMatrix( self.vl[self.features0] )\n        ypred0 = ( self.model0.predict( dvalid ) + self.model1.predict( dvalid )  ) / 2\n        dvalid = xgb.DMatrix( self.vl[self.features1] )\n        ypred1 = ( self.model2.predict( dvalid ) + self.model3.predict( dvalid )  ) / 2\n        \n        self.vl['ypred0'] = ypred0\n        self.vl['ypred1'] = ypred1\n        self.vl.loc[ self.vl.ypred0<self.vl.log0_max, 'ypred0'] =  self.vl.loc[ self.vl.ypred0<self.vl.log0_max, 'log0_max']\n        self.vl.loc[ self.vl.ypred1<self.vl.log1_max, 'ypred1'] =  self.vl.loc[ self.vl.ypred1<self.vl.log1_max, 'log1_max']\n        \n        VALID = self.vl[[\"geo\", \"day\", 'ypred0', 'ypred1']].copy()\n        VALID.columns = [\"geo\", \"day\", 'ConfirmedCases', 'Fatalities']        \n        return VALID.reset_index(drop=True)\n    \n    \n    def predict_next_day(self, yesterday ):\n\n        self.day += 1\n        \n        feats = ['geo','day']        \n        self.train[ 'ypred0' ] = pd.merge( self.train[feats], yesterday[feats+['ConfirmedCases']], on=feats, how='left' )['ConfirmedCases'].values\n        self.train.loc[ self.train.ypred0.notnull(), 'target0'] = self.train.loc[ self.train.ypred0.notnull() , 'ypred0']\n\n        self.train[ 'ypred1' ] = pd.merge( self.train[feats], yesterday[feats+['Fatalities']], on=feats, how='left' )['Fatalities'].values\n        self.train.loc[ self.train.ypred1.notnull(), 'target1'] = self.train.loc[ self.train.ypred1.notnull() , 'ypred1']\n        del self.train['ypred0'], self.train['ypred1']\n        \n        tr, vl = self.create_features( self.train.copy(), self.day )        \n        dvalid = xgb.DMatrix( vl[self.features0] )\n        ypred0 = (self.model0.predict( dvalid ) + self.model1.predict( dvalid ) )/2\n        dvalid = xgb.DMatrix( vl[self.features1] )\n        ypred1 = (self.model2.predict( dvalid ) + self.model3.predict( dvalid ) )/2\n    \n        vl['ypred0'] = ypred0\n        vl['ypred1'] = ypred1\n        vl.loc[ vl.ypred0<vl.log0_max, 'ypred0'] =  vl.loc[ vl.ypred0<vl.log0_max, 'log0_max']\n        vl.loc[ vl.ypred1<vl.log1_max, 'ypred1'] =  vl.loc[ vl.ypred1<vl.log1_max, 'log1_max']\n        \n        self.vl = vl\n        VALID = vl[[\"geo\", \"day\", 'ypred0', 'ypred1']].copy()\n        VALID.columns = [\"geo\", \"day\", 'ConfirmedCases', 'Fatalities']        \n        return VALID.reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TARGETS = [\"ConfirmedCases\", \"Fatalities\"]\n\n\ndef rmse(y_true, y_pred):\n    return np.sqrt(mean_squared_error(y_true, y_pred))\n\ndf = pd.read_csv(\"../input/covid19-global-forecasting-week-4/train.csv\")\ndf[TARGETS] = np.log1p(df[TARGETS].values)\nsub_df = pd.read_csv(\"../input/covid19-global-forecasting-week-4/test.csv\")\n\ndef preprocess(df):\n    for col in [\"Country_Region\", \"Province_State\"]:\n        df[col].fillna(\"\", inplace=True)\n\n    df[\"Date\"] = pd.to_datetime(df['Date'])\n    df['day'] = df.Date.dt.dayofyear\n    df['geo'] = ['_'.join(x) for x in zip(df['Country_Region'], df['Province_State'])]\n    return df\n\ndf = preprocess(df)\nsub_df = preprocess(sub_df)\n\nsub_df[\"day\"] -= df[\"day\"].min()\ndf[\"day\"] -= df[\"day\"].min()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TEST_FIRST = sub_df[sub_df[\"Date\"] > df[\"Date\"].max()][\"Date\"].min()\nprint(TEST_FIRST)\nTEST_DAYS = (sub_df[\"Date\"].max() - TEST_FIRST).days + 1\nTEST_FIRST = (TEST_FIRST - df[\"Date\"].min()).days\n\nprint(TEST_FIRST, TEST_DAYS)\n\n\ndef get_blend(pred_dfs, weights, verbose=True):\n    if verbose:\n#         for n1, n2 in [(\"cpmp\", \"giba1\"),(\"cpmp\", \"giba2\"), (\"cpmp\", \"ahmet\"), (\"giba1\", \"ahmet\"), (\"giba2\", \"ahmet\")]:\n        for n1, n2 in [(\"giba1\", \"ahmet\"), (\"giba2\", \"ahmet\")]:\n            print(n1, n2, np.round(rmse(pred_dfs[n1][TARGETS[0]], pred_dfs[n2][TARGETS[0]]), 4), np.round(rmse(pred_dfs[n1][TARGETS[1]], pred_dfs[n2][TARGETS[1]]), 4))\n    \n#     blend_df = pred_dfs[\"cpmp\"].copy()\n    blend_df = pred_dfs[\"giba1\"].copy()\n    blend_df[TARGETS] = 0\n    for name, pred_df in pred_dfs.items():\n        blend_df[TARGETS] += weights[name]*pred_df[TARGETS].values\n        \n    return blend_df\n\n\n# cov_models = {\"ahmet\": CovidModelAhmet(), \"cpmp\": CovidModelCPMP(), 'giba1': CovidModelGIBA(lag=1), 'giba2': CovidModelGIBA(lag=2)}\ncov_models = {\"ahmet\": CovidModelAhmet(), 'giba1': CovidModelGIBA(lag=1), 'giba2': CovidModelGIBA(lag=2)}\n# weights = {\"ahmet\": 0.35, \"cpmp\": 0.30, \"giba1\": 0.175, \"giba2\": 0.175}\nweights = {\"ahmet\": 0.45, \"giba1\": 0.275, \"giba2\": 0.275}\n\npred_dfs = {name: cm.predict_first_day(TEST_FIRST).sort_values(\"geo\") for name, cm in cov_models.items()}\n\n\nblend_df = get_blend(pred_dfs, weights)\neval_df = blend_df.copy()\n\nfor d in range(1, TEST_DAYS):\n    pred_dfs = {name: cm.predict_next_day(blend_df).sort_values(\"geo\") for name, cm in cov_models.items()}\n    blend_df = get_blend(pred_dfs, weights)\n    eval_df = eval_df.append(blend_df)\n    print(d, eval_df.shape, flush=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eval_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(sub_df.shape)\nsub_df = sub_df.merge(df.append(eval_df, sort=False), on=[\"geo\", \"day\"], how=\"left\")\nprint(sub_df.shape)\nprint(sub_df[TARGETS].isnull().mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"flat = [\n            'China_Anhui',\n            'China_Beijing',\n            'China_Chongqing',\n            'China_Fujian',\n            'China_Gansu',\n            'China_Guangdong',\n            'China_Guangxi',\n            'China_Guizhou',\n            'China_Hainan',\n            'China_Hebei',\n            'China_Heilongjiang',\n            'China_Henan',\n            'China_Hubei',\n            'China_Hunan',\n            'China_Jiangsu',\n            'China_Jiangxi',\n            'China_Jilin',\n            'China_Liaoning',\n            'China_Ningxia',\n            'China_Qinghai',\n            'China_Shaanxi',\n            'China_Shandong',\n            'China_Shanxi',\n            'China_Sichuan',\n            'China_Tibet',\n            'China_Xinjiang',\n            'China_Yunnan',\n            'China_Zhejiang',\n            'Diamond Princess_',\n            'Holy See_', \n        ]     ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df[sub_df[\"geo\"] == \"France_\"][[\"day\"] +TARGETS].plot(x=\"day\")\nplt.axvline(TEST_FIRST, color='r', linestyle='--', lw=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df[sub_df[\"geo\"] == \"Brazil_\"][[\"day\"] +TARGETS].plot(x=\"day\")\nplt.axvline(TEST_FIRST, color='r', linestyle='--', lw=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df[sub_df[\"geo\"] == flat[0]][[\"day\"] +TARGETS].plot(x=\"day\")\nplt.axvline(TEST_FIRST, color='r', linestyle='--', lw=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df[sub_df[\"geo\"] == flat[1]][[\"day\"] +TARGETS].plot(x=\"day\")\nplt.axvline(TEST_FIRST, color='r', linestyle='--', lw=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df[sub_df[\"geo\"] == flat[2]][[\"day\"] +TARGETS].plot(x=\"day\")\nplt.axvline(TEST_FIRST, color='r', linestyle='--', lw=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df[sub_df[\"geo\"] == flat[-1]][[\"day\"] +TARGETS].plot(x=\"day\")\nplt.axvline(TEST_FIRST, color='r', linestyle='--', lw=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df[sub_df[\"geo\"] == flat[-2]][[\"day\"] +TARGETS].plot(x=\"day\")\nplt.axvline(TEST_FIRST, color='r', linestyle='--', lw=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dt = sub_df.loc[ sub_df.Date_x == \"2020-04-13\"  ].copy()\ndt = dt.loc[ dt.geo.isin(flat)  ].copy()\ndt = dt[['geo','Date_x','day','ConfirmedCases','Fatalities']].copy()\ndt = dt.reset_index(drop=True)\ndt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df['ow0'] = pd.merge( sub_df, dt, on='geo', how='left' )['ConfirmedCases_y'].values\nsub_df['ow1'] = pd.merge( sub_df, dt, on='geo', how='left' )['Fatalities_y'].values\nsub_df.tail(60)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df.loc[ sub_df.geo.isin(flat) ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df.loc[ sub_df.ow0.notnull() & (sub_df.Date_x >= '2020-04-14') , 'ConfirmedCases'  ] = sub_df.loc[ sub_df.ow0.notnull() & (sub_df.Date_x >= '2020-04-14') , 'ow0'  ]\nsub_df.loc[ sub_df.ow1.notnull() & (sub_df.Date_x >= '2020-04-14') , 'Fatalities'  ]     = sub_df.loc[ sub_df.ow1.notnull() & (sub_df.Date_x >= '2020-04-14') , 'ow1'  ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df.loc[ sub_df.geo.isin(flat) ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df[sub_df[\"geo\"] == flat[0]][[\"day\"] +TARGETS].plot(x=\"day\")\nplt.axvline(TEST_FIRST, color='r', linestyle='--', lw=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df[TARGETS] = np.expm1(sub_df[TARGETS].values)\nsub_df.to_csv(\"submission.csv\", index=False, columns=[\"ForecastId\"] + TARGETS)\nsub_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df.shape","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}