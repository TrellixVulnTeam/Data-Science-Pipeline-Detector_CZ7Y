{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"##### decorators.py #####\n\nimport functools\nimport time\n\n\nclass DecoratorInputOutputExceptionTime(object):\n    def __call__(self, fn):\n        @functools.wraps(fn)\n        def decorated(*args, **kwargs):\n            try:\n                print(\"{} ({} - {})\".format(fn.__name__, args, kwargs))\n                tic = time.time()\n                result = fn(*args, **kwargs)\n                toc = time.time()\n                print(\"Result: {} [in {:.4f}s]\".format(result, toc-tic))\n                return result\n            except Exception as ex:\n                print(\"Exception {0}\".format(ex))\n                raise ex\n        return decorated\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##### myio.py #####\nimport pathlib\nimport os\nimport pandas as pd\nimport pickle\n\ntrain_file_path = '/kaggle/input/covid19-global-forecasting-week-4/train.csv'\ntest_file_path = '/kaggle/input/covid19-global-forecasting-week-4/test.csv'\nenrichment_file_path = '/kaggle/input/covid-19-enriched-dataset-week-2/enriched_covid_19_week_2.csv'\n\n\ndef cast_columns(df: pd.DataFrame) -> pd.DataFrame:\n    df['location'] = df.Country_Region + '_' + df.Province_State\n    df.location = df.location.astype('category')\n    df = df.drop(columns=['Country_Region', 'Province_State'])\n    if 'Id' in df.columns:\n        df = df.drop(columns=['Id'])\n    df.Date = pd.to_datetime(df.Date)\n    return df\n\n\ndef read_enrichment_file() -> pd.DataFrame:\n    df = pd.read_csv(enrichment_file_path, na_filter=False)\n    # Fix the representation of Country_Region colunn\n    df['Country_Region'] = df['Country_Region'].apply(lambda val: val.split('_')[0])\n    df = cast_columns(df)\n\n    enriched_df = df.sort_values('Date').groupby('location').last().reset_index().dropna()\n    enriched_df = enriched_df.drop(columns=[\"ConfirmedCases\", \"Fatalities\", \"Date\", 'restrictions', 'quarantine', 'schools'])\n\n    # Fill the missing rows with avg values\n    avg_row = enriched_df.mean().to_dict()\n    train_df = read_train_file()\n    train_df_locations = train_df.groupby('location').last().reset_index().dropna()['location']\n\n    joined_df = pd.merge(train_df_locations, enriched_df['location'], indicator=True, on='location', how='left')\n    for _, row in joined_df[joined_df._merge != 'both'].iterrows():\n        avg_row['location'] = row.location\n        enriched_df = enriched_df.append(pd.Series(avg_row), ignore_index=True)\n    assert len(enriched_df) == len(train_df_locations)\n    return enriched_df\n\n\ndef read_train_file() -> pd.DataFrame:\n    df = pd.read_csv(train_file_path, na_filter=False)\n    df = cast_columns(df)\n    return df\n\n\ndef read_test_file() -> pd.DataFrame:\n    df = pd.read_csv(test_file_path, na_filter=False)\n    df = cast_columns(df)\n    return df\n\n\ndef get_train_subset() -> pd.DataFrame:\n    # before 2020-04-01\n    df = read_train_file()\n    df = df[df.Date < \"2020-04-01\"].reset_index(drop=True).copy(deep=True)\n    return df\n\n\ndef get_validation_subset() -> pd.DataFrame:\n    # 2020-04-01 to 2020-04-15 (included)\n    df = read_train_file()\n    df = df[(df.Date >= \"2020-04-01\") & (df.Date <= \"2020-04-15\")].reset_index(drop=True).copy(deep=True)\n    return df\n\n\ndef save_pickle(obj: object, file_path: str) -> None:\n    with open(file_path, \"wb\") as pk:\n        pickle.dump(obj, pk)\n\n\ndef load_pickle(file_path: str) -> object:\n    # May rise IOError if the file is not there\n    with open(file_path, \"rb\") as pk:\n        return pickle.load(pk)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##### featurization.py #####\n\nimport random\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\nfrom typing import List, Optional, Tuple, Dict\n\n\ndef build_point_feats_label(df: pd.DataFrame, lookback: int) -> Tuple[List[Tuple[float, float]], Tuple[float, float]]:\n    assert len(df) == lookback+1\n    points = [(row.ConfirmedCases, row.Fatalities,) for _, row in df.iterrows()]\n    return points[:-1], points[-1]\n\n\ndef build_point_feats(df: pd.DataFrame) -> List[Tuple[float, float]]:\n    return [(row.ConfirmedCases, row.Fatalities,) for _, row in df.iterrows()]\n\n\ndef create_timelines(dataset: pd.DataFrame, labels_from: datetime, lookback: int) -> Dict[str, List]:\n    timelines = {}\n    for loc in dataset.location.unique():\n        sorted_points_for_location = dataset[dataset.location == loc].sort_values(by='Date')\n        timelines[loc] = []\n        for label_idx in range(len(sorted_points_for_location)-1, lookback+1, -1):\n            label_date = sorted_points_for_location.iloc[label_idx].Date\n            if label_date < labels_from:\n                break\n            timeline = sorted_points_for_location.iloc[label_idx-lookback: label_idx+1]\n            point = build_point_feats_label(timeline, lookback)\n\n            timelines[loc].append(point)\n    return timelines\n\n\ndef create_start_timelines_for_val_test(train_dataset: pd.DataFrame, test_val_dataset: pd.DataFrame,\n                                        lookback: int) -> Dict[str, List[Tuple[float, float]]]:\n    # 1 starting point for each country\n    start_timeline = {}\n    for loc in test_val_dataset.location.unique():\n        first_date = test_val_dataset[test_val_dataset.location == loc].sort_values(by='Date').iloc[0].Date\n        sorted_points_for_location = train_dataset[train_dataset.location == loc].sort_values(by='Date')\n        point_feats = build_point_feats(sorted_points_for_location.iloc[-lookback-1:-1])\n        start_timeline[loc] = point_feats\n    return start_timeline\n\n\ndef create_contextual_features(enrichment_df: pd.DataFrame) -> Dict[str, List]:\n    feats = {}\n    for _, row in enrichment_df.iterrows():\n        feat = row.tolist()\n        feats[row.location] = feat[1:]\n    return feats\n\n\ndef location_label_encoder_dict(list_of_locations: List[str]) -> Dict[str, int]:\n    return {loc: idx for idx, loc in enumerate(list_of_locations)}\n\n\ndef generate_features(df: pd.DataFrame) -> pd.DataFrame:\n    for col in df.columns:\n        df[str(col) + '_sqrt'] = df[col].pow(1./2)\n        df[str(col) + '_log2'] = np.log2(df[col].values + 1)\n    return df\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##### modeling.py #####\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom tensorflow import keras\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.layers import Embedding, Dense, Input, Concatenate, Dropout, Reshape, BatchNormalization\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\nfrom tensorflow.keras.optimizers import Adam, RMSprop, Nadam\nfrom typing import List, Optional, Tuple, Dict\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.utils import shuffle\n\n\ndef create_model(cat_df: pd.DataFrame, ctx_df: pd.DataFrame, tl_df: pd.DataFrame, labels_df: pd.DataFrame) ->\\\n        Tuple[keras.Model, StandardScaler, StandardScaler]:\n\n    ctx_ss = StandardScaler()\n    tl_ss = StandardScaler()\n\n    X_train_cat = shuffle(cat_df.values, random_state=0)\n    X_train_ctx = shuffle(ctx_ss.fit_transform(ctx_df.values), random_state=0)\n    X_train_tl = shuffle(tl_ss.fit_transform(tl_df.values), random_state=0)\n    Y_train = shuffle(labels_df.values, random_state=0)\n\n    cat_inputs = Input(shape=(X_train_cat.shape[1], ))\n    cat_embeddings = Embedding(len(np.unique(X_train_cat)), 32)(cat_inputs)\n    modifier_cat_embeddings = Reshape(target_shape=(32,))(cat_embeddings)\n    cat_output = Dense(64, activation='relu')(modifier_cat_embeddings)\n\n    ctx_inputs = Input(shape=(X_train_ctx.shape[1], ))\n    ctx_dense = Dense(512, activation='relu')(ctx_inputs)\n\n    tl_inputs = Input(shape=(X_train_tl.shape[1],))\n    tl_dense = Dense(512, activation='relu')(tl_inputs)\n\n    merge_layer = Concatenate(axis=-1)([cat_output, ctx_dense, tl_dense])\n    normalization_layer = BatchNormalization()(merge_layer)\n    merge_dropout_layer = Dropout(0.2)(normalization_layer)\n    dense_layer = Dense(512, activation='relu')(merge_dropout_layer)\n    normalization_layer_2 = BatchNormalization()(dense_layer)\n    dropout_layer = Dropout(0.2)(normalization_layer_2)\n    embedding_layer = Dense(256, activation='relu')(dropout_layer)\n\n    cases = Dense(1, activation='relu')(embedding_layer)\n    fatalities = Dense(1, activation='relu')(embedding_layer)\n\n    model = Model(inputs=[cat_inputs, ctx_inputs, tl_inputs], outputs=[cases, fatalities])\n    model.compile(loss=[tf.keras.losses.MeanSquaredLogarithmicError(),\n                        tf.keras.losses.MeanSquaredLogarithmicError()],\n                  optimizer=Nadam())\n\n    print(model.summary())\n    callbacks = [ReduceLROnPlateau(monitor='val_loss', patience=20, verbose=1, factor=0.8),\n                 EarlyStopping(monitor='val_loss', patience=20),\n                 ModelCheckpoint(filepath='/tmp/keras_covid_w4_best_model.h5', monitor='val_loss', save_best_only=True)]\n\n    train_history = model.fit([X_train_cat, X_train_ctx, X_train_tl], [Y_train[:,0], Y_train[:,1]],\n                              epochs=500,\n                              batch_size=1024,  # with BatchNormalization, should be large enough\n                              validation_split=0.1,\n                              callbacks=callbacks,\n                              shuffle=True,\n                              use_multiprocessing=True)\n\n    plt.plot(train_history.history['loss'])\n    plt.plot(train_history.history['val_loss'])\n    plt.title('loss per epoch')\n    plt.ylabel('overall loss')\n    plt.xlabel('# epochs')\n    plt.legend(['train', 'val'])\n    plt.show()\n    return model, ctx_ss, tl_ss\n\ndef apply_model(cat_df: pd.DataFrame, ctx_df: pd.DataFrame, tl_df: pd.DataFrame, model: keras.Model,\n                ctx_ss: StandardScaler, tl_ss: StandardScaler) -> pd.DataFrame:\n\n    X_cat = cat_df.values\n    X_ctx = ctx_ss.transform(ctx_df.values)\n    X_tl = tl_ss.transform(tl_df.values)\n\n    Y_pred = model.predict([X_cat, X_ctx, X_tl])\n    return pd.DataFrame(list(zip(Y_pred[0].ravel().tolist(), Y_pred[1].ravel().tolist())), columns=['ConfirmedCases', 'Fatalities'])\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##### main.py #####\n\nimport datetime\nimport numpy as np\nimport pandas as pd\nfrom typing import List, Optional, Tuple, Dict\n# from myio import get_train_subset, get_validation_subset, read_test_file, read_enrichment_file\n# from featurization import (create_timelines, create_start_timelines_for_val_test, create_contextual_features,\n#                            location_label_encoder_dict, generate_features)\n# from modeling import create_model, apply_model\npd.options.display.max_columns = 20\npd.set_option('max_colwidth', 100)\n\n\ndef create_features_labels(train_tl: Dict[str, list], contextual_feats: Dict[str, list], le: Dict[str, int]) -> List[pd.DataFrame]:\n    cat_features, ctx_features, tl_feats, labels = [], [], [], []\n    for location, train_label_tl in train_tl.items():\n        le_location = le[location]\n        ctx_feats = contextual_feats[location]\n        for (feat, label) in train_label_tl:\n            cat_features.append([le_location])\n            ctx_features.append(ctx_feats)\n            tl_feats.append([f for ft in feat for f in ft])\n            labels.append([label[0], label[1]])\n\n    return [pd.DataFrame.from_records(cat_features),\n            generate_features(pd.DataFrame.from_records(ctx_features)),\n            generate_features(pd.DataFrame.from_records(tl_feats)),\n            pd.DataFrame.from_records(labels)]\n\n\ndef create_val_test_start_features(val_test_tl: Dict[str, list], contextual_feats: Dict[str, list], le: Dict[str, int]) -> List[pd.DataFrame]:\n    cat_features, ctx_features, tl_feats = [], [], []\n    for location, feat in val_test_tl.items():\n        cat_features.append([le[location]])\n        ctx_features.append(contextual_feats[location])\n        tl_feats.append([f for ft in feat for f in ft])\n    return [pd.DataFrame.from_records(cat_features),\n            generate_features(pd.DataFrame.from_records(ctx_features)),\n            generate_features(pd.DataFrame.from_records(tl_feats))]\n\n\ndef add_new_point_to_tl(val_test_tl: Dict[str, list], prediction: pd.DataFrame) -> Dict[str, list]:\n    for idx, location in enumerate(val_test_tl):\n        feat = val_test_tl[location]\n        new_point = (np.around(prediction.iloc[idx].ConfirmedCases), np.around(prediction.iloc[idx].Fatalities))\n        new_feat = feat[1:] + [new_point]\n        val_test_tl[location] = new_feat\n    return val_test_tl\n\n\ndef msle(y_true: np.array, y_pred: np.array):\n    y_pred_clip = y_pred.clip(min=0.0)\n    return np.mean((np.log(y_true + 1) - np.log(y_pred_clip + 1)) ** 2)\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_dataset = get_train_subset()\nvalidation_dataset = get_validation_subset()\ntest_dataset = read_test_file()\nenrichment_dataset = read_enrichment_file()\nLOOKBACK = 28\n\ntrain_tl = create_timelines(train_dataset, datetime.datetime(2020, 3, 10), LOOKBACK)\nval_tl = create_start_timelines_for_val_test(train_dataset, validation_dataset, LOOKBACK)\ntest_tl = create_start_timelines_for_val_test(train_dataset, test_dataset, LOOKBACK)\ncontextual_feats = create_contextual_features(enrichment_dataset)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"location_le = location_label_encoder_dict(sorted(list(train_tl.keys())))\n\ncat_features_train, ctx_features_train, tl_feats_train, labels_train = create_features_labels(train_tl, contextual_feats, location_le)\nmodel, ctx_ss, tl_ss = create_model(cat_features_train, ctx_features_train, tl_feats_train, labels_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nvalidation_dataset[\"predict_ConfirmedCases\"] = np.nan\nvalidation_dataset[\"predict_Fatalities\"] = np.nan\ncat_features_val, ctx_features_val, tl_feats_val = create_val_test_start_features(val_tl, contextual_feats, location_le)\nfor date in validation_dataset.Date.unique():\n    print(date)\n    y_pred = apply_model(cat_features_val, ctx_features_val, tl_feats_val, model, ctx_ss, tl_ss)\n    val_tl = add_new_point_to_tl(val_tl, y_pred)\n    cat_features_val, ctx_features_val, tl_feats_val = create_val_test_start_features(val_tl, contextual_feats, location_le)\n    for idx, location in enumerate(validation_dataset.location.unique()):\n        validation_dataset.loc[(validation_dataset.Date == date) & (validation_dataset.location == location), 'predict_ConfirmedCases'] = y_pred.iloc[idx].ConfirmedCases\n        validation_dataset.loc[(validation_dataset.Date == date) & (validation_dataset.location == location), 'predict_Fatalities'] = y_pred.iloc[idx].Fatalities\n\n\nval_MSLE_confirmed_cases = msle(validation_dataset.ConfirmedCases.values, validation_dataset.predict_ConfirmedCases.values)\nval_MSLE_fatalities = msle(validation_dataset.Fatalities.values, validation_dataset.predict_Fatalities.values)\nprint('L1={:.3f}, L2={:.3f} -> L1+L2={:.3f}'.format(val_MSLE_confirmed_cases, val_MSLE_fatalities, val_MSLE_confirmed_cases+val_MSLE_fatalities))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntest_dataset[\"predict_ConfirmedCases\"] = np.nan\ntest_dataset[\"predict_Fatalities\"] = np.nan\ncat_features_test, ctx_features_test, tl_feats_test = create_val_test_start_features(test_tl, contextual_feats, location_le)\nfor date in test_dataset.Date.unique():\n    print(date)\n    y_pred = apply_model(cat_features_test, ctx_features_test, tl_feats_test, model, ctx_ss, tl_ss)\n    test_tl = add_new_point_to_tl(test_tl, y_pred)\n    cat_features_test, ctx_features_test, tl_feats_test = create_val_test_start_features(test_tl, contextual_feats, location_le)\n    for idx, location in enumerate(test_dataset.location.unique()):\n        test_dataset.loc[(test_dataset.Date == date) & (test_dataset.location == location), 'predict_ConfirmedCases'] = y_pred.iloc[idx].ConfirmedCases\n        test_dataset.loc[(test_dataset.Date == date) & (test_dataset.location == location), 'predict_Fatalities'] = y_pred.iloc[idx].Fatalities\n\nsubmission_df = test_dataset.rename(columns={\"predict_ConfirmedCases\": \"ConfirmedCases\", \"predict_Fatalities\": \"Fatalities\"})[['ForecastId', 'ConfirmedCases', 'Fatalities']]\nsubmission_df.to_csv('submission.csv', header=True, index=False)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}