{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import preprocessing\nimport time\nfrom datetime import datetime\nfrom scipy import integrate, optimize\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# ML libraries\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn import linear_model\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold\n\nfrom tensorflow.python.ops import variables\nfrom tensorflow.python.framework import ops\nimport tensorflow.compat.v1 as tf\ntf.disable_v2_behavior()\n\nfrom fancyimpute import KNN \n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Helper functions for managing the data\n\ndef get_place(row):\n    place = row[2]\n    if isinstance(row[1], str):\n        place = row[1]\n    return place\n\n# Returns a dictionary, keyed by places, of their data\ndef separate_by_place(data):\n    place_data = {}\n    for row in data:\n        place = get_place(row)\n        if place in place_data:\n            place_data[place].append(row)\n        else:\n            place_data[place] = [row]\n    return place_data\n\ndef rmsle(y_test, predictions):\n    return np.sqrt(mean_squared_log_error(y_test, predictions))\n\n# Get dict of [cases_dom, deaths_dom, cases_int, deaths_int] for each country\ndef current_day_info(dataset, day):\n    day_data = {}\n    indices = np.where(dataset[:, 3] == day)\n    total_cases = np.sum(dataset[indices, 4])\n    total_deaths = np.sum(dataset[indices, 5])\n    for row in dataset[indices]:\n        place = get_place(row)\n        day_data[place] = [row[4], row[5], total_cases, total_deaths]\n    return day_data\n\n# Add previous day total cases, deaths domestically and internationally\ndef make_nn_train_data(dataset):\n    # Create the columns to fill\n    added_data = np.c_[dataset, np.zeros(len(dataset))]\n    added_data = np.c_[added_data, np.zeros(len(dataset))]\n    added_data = np.c_[added_data, np.zeros(len(dataset))]\n    added_data = np.c_[added_data, np.zeros(len(dataset))]\n    \n    # For each day in data set, and each country, grab [cases_dom, deaths_dom, cases_int, deaths_int]\n    data_day_place = []\n    num_days = np.amax(dataset[:, 3])\n    for day in range(int(num_days)):\n        data_day_place.append(current_day_info(dataset, day))\n        \n    # Now insert into the dataset\n    for index in range(len(added_data)):\n        row = dataset[index]\n        place = get_place(row)\n        prev_day = int(row[3] - 1)\n        if prev_day >= 0:\n            added_data[index, [-4, -3, -2, -1]] = data_day_place[prev_day][place]\n        else:\n            added_data[index, [-4, -3, -2, -1]] = [0.0, 0.0, 0.0, 0.0]\n            \n    x_indices = [3] + [i for i in range(6, len(added_data[0]))]\n    y_indices = [4, 5]\n    id_indices = [0, 1, 2]\n    \n    train_x = added_data[:, x_indices]\n    train_y = added_data[:, y_indices]\n    train_id = added_data[:, id_indices]\n    \n    # Change y to be delta cases and deaths. Spots -4, -3 of x are already domestic cases, deaths previously.\n    # And y is how many occur by the end of the day, so take difference\n    train_y = train_y - train_x[:, [-4, -3]]\n    \n    return train_x, train_y, train_id","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load Data\ntrain = pd.read_csv(\"../input/covid19-global-forecasting-week-4/train.csv\")\ntest_norm = pd.read_csv(\"../input/covid19-global-forecasting-week-4/test.csv\")\n\n# First, add some data (first WDI obtained here https://www.kaggle.com/sambitmukherjee/covid-19-data-adding-world-development-indicators/output)\nwdi_info = pd.read_csv(\"../input/wdi-data-covid19/wdi_data.csv\")\ntrain_wdi = pd.merge(train, wdi_info,  how='left', on=['Province_State','Country_Region'])\n\n# Now add health system data\nhealth_info = pd.read_csv(\"../input/world-bank-wdi-212-health-systems/2.12_Health_systems.csv\")\ntrain_wdi = pd.merge(train_wdi, health_info,  how='left', on=['Province_State','Country_Region'])\n\n# Add personality info\npersonality_info = pd.read_csv(\"../input/covid19-country-data-wk3-release/Data Join - RELEASE.csv\")\npersonality_info = personality_info.rename(columns={\"TRUE POPULATION\": \"TRUE_POPULATION\"})\npersonality_info.pct_in_largest_city = personality_info.pct_in_largest_city.apply(lambda x: x.replace('%', ''))\npersonality_info.TRUE_POPULATION = personality_info.TRUE_POPULATION.apply(lambda x: x.replace(',', ''))\ntrain_wdi = pd.merge(train_wdi, personality_info,  how='left', on=['Province_State','Country_Region'])\n\n# Add leader info https://www.kaggle.com/lunatics/global-politcs-and-governance-data-apr-2020\nleader_info = pd.read_csv(\"../input/politics/politics_apr2020.csv\")\ntrain_wdi = pd.merge(train_wdi, leader_info,  how='left', on=['Country_Region'])\n\n# Add immunization coverage https://www.kaggle.com/lsind18/who-immunization-coverage\nfor filename in os.listdir(\"../input/who-immunization-coverage\"):\n    immun_info = pd.read_csv(\"../input/who-immunization-coverage/\" + filename).iloc[:,0:2]\n    immun_info = immun_info.rename(columns={\"Country\": \"Country_Region\", \"2018\": filename})\n    train_wdi = pd.merge(train_wdi, immun_info,  how='left', on=['Country_Region'])\n\n# Replace bad data with nan\ntrain_wdi = train_wdi.apply(lambda x: x.replace('#NULL!', np.nan))\ntrain_wdi = train_wdi.apply(lambda x: x.replace('#DIV/0!', np.nan))\ntrain_wdi = train_wdi.apply(lambda x: x.replace('#N/A', np.nan))\ntrain_wdi = train_wdi.apply(lambda x: x.replace('N.A.', np.nan))\ntrain_wdi = train_wdi.drop(['World_Bank_Name'], axis=1)\n\n# Convert dates to integers, starting from 0\ntest_norm[\"Date\"] = (pd.to_datetime(test_norm['Date']) - pd.to_datetime(min(train_wdi['Date']))).dt.days\ntrain_wdi[\"Date\"] = (pd.to_datetime(train_wdi['Date']) - pd.to_datetime(min(train_wdi['Date']))).dt.days\ntrain[\"Date\"] = (pd.to_datetime(train['Date']) - pd.to_datetime(min(train['Date']))).dt.days\n\ntrain = train.to_numpy()\ntrain_wdi = train_wdi.to_numpy()\ntest_norm = test_norm.to_numpy()\n\n# Cast to float\nindices = [i for i in range(3, len(train_wdi[0]))]\ntrain_wdi[:, indices] = train_wdi[:, indices].astype('float64') ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Apply k nearest neighbors to obtain data for nan\ntrain_wdi[:, indices] = KNN(k=5).fit_transform(train_wdi[:, indices])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create training sets\ntrain_x, train_y, train_info = make_nn_train_data(train_wdi)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Helper function to view gradients for debugging purposes\ndef debug_grads(sess, feed_dict):\n    var_list = (variables.trainable_variables() + ops.get_collection(\n        ops.GraphKeys.TRAINABLE_RESOURCE_VARIABLES))\n    print('variables')\n    for v in var_list:\n        print('  ', v.name)\n    # get all gradients\n    grads_and_vars = optimizer.compute_gradients(loss)\n    # train_op = optimizer.apply_gradients(grads_and_vars)\n\n    zipped_val = sess.run(grads_and_vars, feed_dict=feed_dict)\n\n    for rsl, tensor in zip(zipped_val, grads_and_vars):\n        print('-----------------------------------------')\n        print('name', tensor[0].name.replace('/tuple/control_dependency_1:0', '').replace('gradients/', ''))\n        print('gradient', rsl[0])\n        print('value', rsl[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create graph\n\ndef fcn(num_in, num_out, X, name):\n    with tf.name_scope(name):\n        W = tf.get_variable(name + 'W', shape=(num_in, num_out), initializer=tf.keras.initializers.glorot_normal())\n        b = tf.Variable(tf.zeros((num_out,)), trainable=True)\n        X = tf.add(tf.matmul(X, W), b)\n        X = tf.layers.batch_normalization(X)\n        X = tf.nn.leaky_relu(X)\n    return X\n\ntf.reset_default_graph()\ngraph = tf.Graph()\n\nlearning_rate = 0.007\nNUM_FEATURES = train_x.shape[1]\nDEPTH = 20\n\nwith graph.as_default():\n    X = tf.placeholder(tf.float32, (None, NUM_FEATURES))\n    labels = tf.placeholder(tf.float32, (None, 2))\n    X_es = [X]\n    for i in range(DEPTH):\n        num_in = 150\n        num_out = 150\n        if i == 0:\n            num_in = NUM_FEATURES\n        if i == DEPTH - 1:\n            num_out = 2\n        X_es.append(fcn(num_in, num_out, X_es[-1], \"fcn\" + str(i)))\n    predictions = X_es[-1]\n    loss = tf.losses.huber_loss(labels, predictions)\n\n    optimizer = tf.train.AdamOptimizer(learning_rate)\n    train_op = optimizer.minimize(loss)\n    init = tf.global_variables_initializer()\n    saver = tf.train.Saver()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train\n\nNUM_EPOCHS = 500\nsave_freq = 100\nDEBUG = False\nlearning_rate = 0.007\nrestore = False\nsave = True\nload_path = \"../input/pretrained-covid19/model.ckpt\"\nbatch_size = 16\ntest_size = 0.01\n\n# SKLearn scalers\nx_scaler = StandardScaler()\nx_scaler.fit(train_x)\ny_scaler = StandardScaler()\ny_scaler.fit(train_y)\ntransformed_x = x_scaler.transform(train_x)\ntransformed_y = y_scaler.transform(train_y)\n\nwith tf.Session(graph=graph) as sess:\n    if restore:\n        saver.restore(sess, load_path)\n        NUM_EPOCHS = 0\n    else:\n        init.run()\n        \n    indices = [i for i in range(len(train_x))]\n    for epoch in range(NUM_EPOCHS):\n        # Shuffle and split data set\n        np.random.shuffle(indices)\n        split_index = int((1 - test_size) * (len(indices)))\n        train_indices = indices[:split_index]\n        cv_indices = indices[split_index:]\n        # Initialize some variables\n        avg_loss = 0\n        batch_index = 0\n\n        # Mini batches\n        while batch_index < len(train_indices) - batch_size:\n            batch_indices = train_indices[batch_index: batch_index + batch_size]\n            batch_x = transformed_x[batch_indices]\n            batch_y = transformed_y[batch_indices]\n            batch_index += batch_size\n\n            if DEBUG:\n                debug_grads(sess, feed_dict)\n                \n            feed_dict = {X: batch_x, labels: batch_y}\n            _, loss_val, outs = sess.run([train_op, loss, predictions], feed_dict=feed_dict)\n            avg_loss += loss_val\n            \n        cv_x = transformed_x[cv_indices]\n        cv_y = transformed_y[cv_indices]\n        feed_dict = {X: cv_x, labels: cv_y}\n        cv_loss = sess.run([loss], feed_dict=feed_dict)[0]\n        print(epoch, \"Avg Train Loss\", avg_loss/batch_index, \"Avg CV Loss\", cv_loss/len(cv_indices))\n        \n        # Save\n        if save and (epoch % save_freq == 0): \n            save_str = \"tmp/model\" + str(epoch) + \".ckpt\"\n            save_path = saver.save(sess, save_str)\n    \n    # Save final weights\n    save_path = saver.save(sess, \"tmp/model.ckpt\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"days_to_extend = 60\n\ndef row_to_nn(row, prev_day_data):\n    place = get_place(row)\n    new_row = np.copy(row)\n    new_row = np.append(new_row, prev_day_data[place])\n    x_indices = [3] + [i for i in range(6, len(new_row))]\n    return new_row[x_indices]\n\nlonger_train = np.copy(train_wdi)\n\nwith tf.Session(graph=graph) as sess:\n    saver.restore(sess, \"tmp/model.ckpt\")\n    num_days = int(np.amax(longer_train[:, 3]))\n    # x_indices = [3] + [i for i in range(6, len(added_data[0]))]\n    for day in range(num_days, num_days + days_to_extend):\n        print(day)\n        prev_day_data = current_day_info(longer_train, day)\n        indices = np.where(longer_train[:, 3] == day)\n        for row in longer_train[indices]:\n            # turn each item into nn data format\n            row_x = np.asarray([row_to_nn(row, prev_day_data)])\n            # Run through NN\n            standardized_x = x_scaler.transform(row_x)\n            feed_dict = {X: standardized_x}\n            outs = sess.run(predictions, feed_dict=feed_dict)\n            inverse_outs = y_scaler.inverse_transform(outs)[0]\n            # Floor at 0\n            if inverse_outs[0] < 0:\n                inverse_outs[0] = 0.0\n            if inverse_outs[1] < 0:\n                inverse_outs[1] = 0.0\n            # Create new row\n            new_row = np.copy(row)\n            new_row[3] += 1\n            new_row[4] += inverse_outs[0]\n            new_row[5] += inverse_outs[1]\n            longer_train = np.append(longer_train, [new_row], axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert date back\ntrain = pd.read_csv(\"../input/covid19-global-forecasting-week-4/train.csv\")\nstart_date = np.datetime64(np.min(train['Date']))\nnew_dates = []\nfor index in range(len(longer_train)):\n    new_date = start_date + np.timedelta64(int(longer_train[index][3]), 'D')\n    new_dates.append(new_date)\n\nconv_predictions = np.copy(longer_train)\nconv_predictions[:, 3] = new_dates\n\n# Save predictions as a file\nmy_columns = [\"ForecastId\", \"Province_State\", \"Country_Region\", \"Date\", \"ProjectedCases\", \"Fatalities\"]\noutputs = conv_predictions[:, [0, 1, 2, 3, 4, 5]]\ndf = pd.DataFrame(outputs, columns=my_columns) \ndf.to_csv('predictions.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create submission file\nsubmission = pd.read_csv(\"../input/covid19-global-forecasting-week-4/test.csv\")\nsubmission[\"Date\"] = pd.to_datetime(submission['Date'])\nsubmission = pd.merge(submission, df,  how='left', on=['Province_State', 'Country_Region', 'Date'])\nsubmission = submission[['ForecastId_x', 'ProjectedCases', 'Fatalities']]\nsubmission = submission.rename(columns={\"ForecastId_x\": \"ForecastId\", \"ProjectedCases\": \"ConfirmedCases\"})\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}