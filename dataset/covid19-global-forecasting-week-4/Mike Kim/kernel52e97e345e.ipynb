{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\n\nfrom sklearn.linear_model import LinearRegression, BayesianRidge\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import make_pipeline\nfrom joblib import Parallel, delayed\n\nimport numpy as np\nimport pandas as pd\nfrom datetime import timedelta, datetime\nfrom collections import defaultdict\nfrom xgboost import XGBRegressor\n\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\nPREFIX = dirname + \"/\"\nprint(\"\")\nprint(PREFIX)\n#\"/kaggle/input/covid19-global-forecasting-week-3/\"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"code","source":"train_df = pd.read_csv(PREFIX+\"train.csv\", parse_dates=[\"Date\"])\ntest_df = pd.read_csv(PREFIX+\"test.csv\", parse_dates=[\"Date\"])\nsub_df = pd.read_csv(PREFIX+\"submission.csv\")\n\n\n# In[3]:\n\n\nLAG_DAYS = 28\n\n\nMIN_DATE_TRAIN = train_df[\"Date\"].min()\nMID_DATE_TRAIN = train_df[\"Date\"].min()  + timedelta(days=LAG_DAYS)\nMAX_DATE_TRAIN = train_df[\"Date\"].max()\n\nSTART_DATE_TRAIN = MID_DATE_TRAIN + timedelta(days=1)\n\n\nMIN_DATE_TEST = test_df[\"Date\"].min()\nMAX_DATE_TEST = test_df[\"Date\"].max()\nprint(MIN_DATE_TRAIN)\nprint(MID_DATE_TRAIN)\n\nprint(MAX_DATE_TRAIN)\nprint(MIN_DATE_TEST)\nprint(MAX_DATE_TEST)\nprint('done')\n\n\n# In[4]:\n\n\nboth_df = pd.concat([train_df,test_df[test_df[\"Date\"]>MAX_DATE_TRAIN]],axis=0)\nprint(train_df.shape)\nprint(test_df.shape)\nprint(both_df.shape)\nboth_df.head(3)\n\n\n# In[5]:\n\n\noof_df = train_df[train_df[\"Date\"]<MID_DATE_TRAIN]\ndef create_oof_dict(oof_df):\n    first_death_list = []\n    first_case_list = []\n    country_list = oof_df[\"Country_Region\"].unique().tolist()\n    for country in country_list:\n        df = oof_df[oof_df['Country_Region']==country]\n        df1 = df[df[\"ConfirmedCases\"]>0]\n        if df1.shape[0] > 0:\n            first_case_list.append(df1[\"Date\"].min())\n        else:\n            first_case_list.append(MAX_DATE_TEST)\n        df2 = df[df[\"Fatalities\"]>0]\n        if df2.shape[0] > 0:\n            first_death_list.append(df2[\"Date\"].min())\n        else:\n            first_death_list.append(MAX_DATE_TEST)\n        \n\n    first_df = pd.DataFrame()\n    first_df['country'] = country_list\n    first_df['first_death_date'] = first_death_list\n    first_df['first_case_date'] = first_case_list\n\n    first_df['first_death_date'] = (first_df['first_death_date']-MIN_DATE_TRAIN).dt.days\n    first_df['first_case_date'] = (first_df['first_case_date']-MIN_DATE_TRAIN).dt.days\n\n    death_map = dict(zip(first_df['country'].tolist(),first_df['first_death_date'].tolist()))\n    case_map = dict(zip(first_df['country'].tolist(),first_df['first_case_date'].tolist()))\n    return case_map, death_map\n\n\ncase_map, death_map = create_oof_dict(oof_df)\nprint(\"done\")\n\n\n# In[6]:\n\n\n\n\ndef replace_state_nan(df):\n    df = df.copy()\n    bool_index = df['Province_State'].isnull()\n    df.loc[bool_index, 'Province_State'] = df.loc[bool_index,'Country_Region'] + \"_NaN\"\n    return df\n\ndef create_days(df):\n    df = df.copy()\n    df['Days'] = (df[\"Date\"]-MIN_DATE_TRAIN).dt.days.astype(int)\n    \n    df['weekday'] = df[\"Date\"].dt.dayofweek\n    \n    return df\n\ndef create_CFR(df):\n    df = df.copy()\n    df[\"CFR\"] = df[\"Fatalities\"]/df[\"ConfirmedCases\"]\n    df[\"CFR\"].fillna(0, inplace=True)\n    return df\n\nkorea_dummy_dict = defaultdict(int)\nkorea_dummy_dict['Korea, South'] =  1\n\nhubei_dummy_dict = defaultdict(int)\nhubei_dummy_dict['Hubei'] =  1\n\niran_dummy_dict = defaultdict(int)\niran_dummy_dict['Iran'] =  1\n\nus_dummy_dict = defaultdict(int)\nus_dummy_dict['US'] =  1\n\nitaly_dummy_dict = defaultdict(int)\nitaly_dummy_dict['Italy'] =  1\n\n\nCOUNTRY_VC = train_df[\"Country_Region\"].value_counts()\ndef hash_country(df):\n    df = df.copy()\n    df['country_hash'] = pd.util.hash_array(df['Country_Region'])\n    df['country_hash'] = df['country_hash'].rank(pct=True)\n    \n    df['country_count'] = df['Country_Region'].map(COUNTRY_VC)\n    \n    df['state_hash'] = pd.util.hash_array(df['Province_State'])\n    df['state_hash'] = df['Province_State'].rank(pct=True)\n\n    df['first_death'] = df[\"Country_Region\"].map(death_map)\n    df['first_case'] = df[\"Country_Region\"].map(case_map)\n    \n    df['dummy_korea'] = df[\"Country_Region\"].map(korea_dummy_dict)\n    df['dummy_hubei'] = df[\"Province_State\"].map(hubei_dummy_dict)\n    df['dummy_iran'] = df[\"Country_Region\"].map(iran_dummy_dict)\n    df['dummy_italy'] = df[\"Country_Region\"].map(italy_dummy_dict)\n    \n    return df\n\n\n\ndef create_features(df):\n    df = replace_state_nan(df)\n    df = hash_country(df)\n    df = create_days(df)\n    #df = create_CFR(df)\n    return df\n\n\n\n# In[7]:\n\n\nboth_df = create_features(both_df)\nboth_df['state_date_key'] = both_df[\"Province_State\"].astype(str) + \"_\" + both_df[\"Date\"].astype(str)\n\ncase_dict = dict(zip(both_df['state_date_key'].tolist(), both_df['ConfirmedCases'].tolist()))\nfatal_dict = dict(zip(both_df['state_date_key'].tolist(), both_df['Fatalities'].tolist()))\n\nboth_df.head(3)\n\n\n# In[8]:\n\n\ntarget_to_features_dict = {}\nSTATIC_FEATURES_LIST = [\"country_hash\",\"country_count\",\"state_hash\",\n                        \"first_death\",\n                        \"first_case\",\n                        \"Days\",\"weekday\",\n                        \"dummy_korea\",\"dummy_hubei\",\"dummy_iran\"]\nprovince_list = both_df[\"Province_State\"].unique().tolist()\ndate_array =  both_df[(both_df[\"Date\"]>START_DATE_TRAIN) & (both_df[\"Date\"]<=MAX_DATE_TRAIN)][\"Date\"].unique()\n\nboth_df[\"CFR\"] = both_df[\"Fatalities\"]/both_df[\"ConfirmedCases\"]\nboth_df[\"CFR\"].fillna(0, inplace=True)\n\n\ndef update_target_dict(both_df, current_date, current_province_state):\n        features_list = []\n        target_df = both_df[(both_df[\"Province_State\"]==current_province_state) & (both_df[\"Date\"] == current_date)]\n        cases = target_df[\"ConfirmedCases\"].values[0]\n        fatals = target_df[\"Fatalities\"].values[0]\n        current_key = target_df[\"state_date_key\"].values[0]\n        \n        prior_df = both_df[(both_df[\"Province_State\"]==current_province_state) & \n                           (both_df[\"Date\"] < current_date)].copy()\n        prior_df.sort_values(\"Date\",inplace=True)\n        prior_df = prior_df.tail(LAG_DAYS).copy()\n        prior_df.reset_index(drop=True, inplace=True)\n        \n\n        features_list += [prior_df[\"Fatalities\"].max()]\n        features_list += [prior_df[\"ConfirmedCases\"].max()]\n        features_list += [prior_df[\"CFR\"].max()]        \n        features_list += [(prior_df[\"Fatalities\"]*prior_df[\"ConfirmedCases\"]).max()] # good\n        features_list += [prior_df[\"Fatalities\"].diff().abs().max()]\n        features_list += [prior_df[\"ConfirmedCases\"].diff().abs().max()]\n        features_list += [prior_df[\"CFR\"].diff().abs().max()]\n\n        features_list += target_df[STATIC_FEATURES_LIST].values.tolist()[0]\n\n        for f in [\"ConfirmedCases\",\"Fatalities\",\"CFR\"]: \n            features_list += prior_df[f].tolist()\n            \n        for f in [\"ConfirmedCases\",\"Fatalities\"]: \n            features_list += (prior_df[f].diff(2)/(1+prior_df[f])).dropna().tolist() #-1,3worse, 1.2 good, \n    \n        features_list += (-prior_df[\"ConfirmedCases\"]+prior_df[\"Fatalities\"]).tolist()\n\n        #################### new\n        model = make_pipeline(PolynomialFeatures(2), BayesianRidge()) #Ridge() worse\n        model.fit(prior_df[[\"ConfirmedCases\",\"Fatalities\",\"Days\"]].shift(periods=2).dropna(),\n                  prior_df[[\"ConfirmedCases\"]].shift(periods=-2).dropna().values.reshape(-1,))  \n        preds = model.predict(prior_df[[\"ConfirmedCases\",\"Fatalities\",\"Days\"]])\n        features_list += [preds[-1]]\n        \n        model.fit(prior_df[[\"ConfirmedCases\",\"Fatalities\",\"Days\"]].shift(periods=2).dropna(),\n                  prior_df[[\"Fatalities\"]].shift(periods=-2).dropna().values.reshape(-1,))  \n        preds = model.predict(prior_df[[\"ConfirmedCases\",\"Fatalities\",\"Days\"]])\n        features_list += [preds[-1]]\n        \n        #\n        model = make_pipeline(PolynomialFeatures(2), BayesianRidge()) #Ridge() worse\n        model.fit(prior_df[[\"ConfirmedCases\",\"Fatalities\",\"Days\"]].diff(periods=1).dropna(), \n                  prior_df[[\"ConfirmedCases\"]].diff(periods=-1).dropna().values.reshape(-1,))  \n        preds = model.predict(prior_df[[\"ConfirmedCases\",\"Fatalities\",\"Days\"]].diff(1).dropna())\n        features_list += [preds[-1]]\n        \n        model.fit(prior_df[[\"ConfirmedCases\",\"Fatalities\",\"Days\"]].diff(periods=1).dropna(),\n                  prior_df[[\"Fatalities\"]].diff(periods=-1).dropna().values.reshape(-1,))  \n        preds = model.predict(prior_df[[\"ConfirmedCases\",\"Fatalities\",\"Days\"]].diff(1).dropna())\n        features_list += [preds[-1]]     \n        #\n        features_list += [(prior_df[\"Days\"]-prior_df[\"first_death\"]).max()]\n        features_list += [(prior_df[\"Days\"]-prior_df[\"first_case\"]).max()]\n        \n        target_to_features_dict[(current_province_state, current_date, current_key)] = [cases,fatals,features_list]\n\n\n\nfor current_province_state in province_list:\n    for current_date in date_array:\n        update_target_dict(both_df, current_date, current_province_state)\n\nprint(\"done\")\n\n\n# In[9]:\n\n\ndef list_to_matrix(x):\n    return np.array([np.array(xi) for xi in x])\n\ndef create_train_X_y(target_to_features_dict):\n    X_train = []\n    y_key = []\n    y_train_case = []\n    y_train_fatal = []\n    for k,v in target_to_features_dict.items():\n        X_train.append(v[2])\n        y_train_case.append(v[0])\n        y_train_fatal.append(v[1])\n        y_key.append(k[2])\n    X_train = list_to_matrix(X_train)\n    return X_train, y_train_case, y_train_fatal, y_key\n\nX_train, y_train_case, y_train_fatal, y_key = create_train_X_y(target_to_features_dict)\nprint(X_train.shape)\nprint(\"done\")\n\n\n# In[10]:\n\n\ndef create_test_X_y(target_to_features_dict, date_array_0):\n    X_test = []\n    y_key = []\n    y_test_case = []\n    y_test_fatal = []\n    for k,v in target_to_features_dict.items():\n        if k[1] == date_array_0: #date_array[0]:\n            X_test.append(v[2])\n            y_test_case.append(v[0])\n            y_test_fatal.append(v[1])\n            y_key.append(k[2])\n    X_test = list_to_matrix(X_test)\n    return X_test, y_test_case, y_test_fatal, y_key\n\n\n# In[11]:\n\n\nnp.random.seed(2)\n\ndef rmsle(y_true, y_pred):\n    return 'RMSLE', np.sqrt(np.mean(np.power(np.log1p(y_pred) - np.log1p(y_true), 2))), False\n\nnum_features = X_train.shape[1]\nmc = [\"0\"]*(num_features-4)\nMC_STR = \"(1,1,1,1,\" + \",\".join(mc)  + \")\" \n\ndef xgb_reg():\n    return XGBRegressor(n_estimators=2000,\n                   booster=\"gbtree\", \n                   tree_method = \"exact\",\n                   eta= 0.003, \n                   max_depth=6, \n                   subsample=0.9,\n                   colsample_bytree=0.9,\n                   colsample_bylevel=0.9,\n                   min_child_weight = 0,\n                   objective = \"reg:squarederror\",\n                   reg_alpha=0.1, \n                   reg_lambda=0.95)\n\n\ndef dual_xgb_fit(X_train, y_train_case, y_train_fatal):\n    gbm_case = xgb_reg()\n    gbm_case.fit(X_train, np.log1p(y_train_case))\n\n    gbm_fatal = xgb_reg()\n    gbm_fatal.fit(X_train, np.log1p(y_train_fatal))\n    return gbm_case, gbm_fatal\n    \n\ngbm_case, gbm_fatal = dual_xgb_fit(X_train, y_train_case, y_train_fatal)\n\n\nprint(\"done\")\n\n\n# In[12]:\n\n\n\nprint(both_df[\"state_date_key\"].nunique())\nprint(both_df.shape)\n\n\n# In[13]:\n\n\nprint(both_df[\"Date\"][both_df[\"Fatalities\"].isnull()].min())\n\n\n# In[14]:\n\n\ndate_array =  [both_df[\"Date\"][both_df[\"Fatalities\"].isnull()].min()]\nprint(date_array)\nboth_df[\"CFR\"] = both_df[\"Fatalities\"]/both_df[\"ConfirmedCases\"]\nboth_df[\"CFR\"].fillna(0, inplace=True)\n\nfor current_province_state in province_list:\n    for current_date in date_array:\n        update_target_dict(both_df, current_date, current_province_state)\n\nprint(\"done\")\n\n\n# In[15]:\n\n\nX_test, y_test_case, y_test_fatal, y_key = create_test_X_y(target_to_features_dict, date_array[0])\nprint(X_test.shape)\n\n\n# In[16]:\n\n\ny_pred_case = np.expm1(gbm_case.predict(X_test))\ny_pred_fatal = np.expm1(gbm_fatal.predict(X_test))\nprint(\"done\")\n\n\n# In[17]:\n\n\nfor j in range(len(y_key)):\n    key_val = y_key[j]\n    case_dict[key_val] = y_pred_case[j]  \n    fatal_dict[key_val] = y_pred_fatal[j]\n    \nprint(\"done\")\n\n\n# In[18]:\n\n\nboth_df[\"ConfirmedCases\"] = both_df[\"state_date_key\"].map(case_dict)\nboth_df[\"Fatalities\"] = both_df[\"state_date_key\"].map(fatal_dict)\n\nboth_df[\"CFR\"] = both_df[\"Fatalities\"]/both_df[\"ConfirmedCases\"]\nboth_df[\"CFR\"].fillna(0, inplace=True)\n\nprint(\"done\")\n\n\n# In[19]:\n\n\n# repeat loop\n\n#date_array =  [both_df[\"Date\"][both_df[\"Fatalities\"].isnull()].min()]\n# keep prior date, now filled with prediction\nprint(date_array)\nboth_df[\"CFR\"] = both_df[\"Fatalities\"]/both_df[\"ConfirmedCases\"]\nboth_df[\"CFR\"].fillna(0, inplace=True)\n\nfor current_province_state in province_list:\n    for current_date in date_array:\n        update_target_dict(both_df, current_date, current_province_state)\n\nprint(\"done\")\n\n\n# In[20]:\n\n\nX_train, y_train_case, y_train_fatal, y_key = create_train_X_y(target_to_features_dict)\nprint(X_train.shape)\nprint(\"done\")\n\n\n# In[21]:\n\n\n\ngbm_case, gbm_fatal = dual_xgb_fit(X_train, y_train_case, y_train_fatal)\nprint(\"done\")\n\n\n# In[22]:\n\n\ndate_array =  [both_df[\"Date\"][both_df[\"Fatalities\"].isnull()].min()]\nprint(date_array)\nboth_df[\"CFR\"] = both_df[\"Fatalities\"]/both_df[\"ConfirmedCases\"]\nboth_df[\"CFR\"].fillna(0, inplace=True)\n\nfor current_province_state in province_list:\n    for current_date in date_array:\n        update_target_dict(both_df, current_date, current_province_state)\n\nprint(\"done\")\n\n\n# In[23]:\n\n\nX_test, y_test_case, y_test_fatal, y_key = create_test_X_y(target_to_features_dict, date_array[0])\nprint(X_test.shape)\n\n\n# In[24]:\n\n\ny_pred_case = np.expm1(gbm_case.predict(X_test))\ny_pred_fatal = np.expm1(gbm_fatal.predict(X_test))\nprint(\"done\")\n\n\n# In[25]:\n\n\nfor j in range(len(y_key)):\n    key_val = y_key[j]\n    case_dict[key_val] = y_pred_case[j]  \n    fatal_dict[key_val] = y_pred_fatal[j]\n    \nprint(len(case_dict.keys()))\nprint(\"done\")\n\n\n# In[26]:\n\n\nboth_df[\"ConfirmedCases\"] = both_df[\"state_date_key\"].map(case_dict)\nboth_df[\"Fatalities\"] = both_df[\"state_date_key\"].map(fatal_dict)\n\n#both_df[\"CFR\"] = both_df[\"Fatalities\"]/both_df[\"ConfirmedCases\"]\n#both_df[\"CFR\"].fillna(0, inplace=True)\n\nprint(\"done\")\n\n\n# In[ ]:\n\n\nnull_count = both_df[\"Fatalities\"].isnull().sum()\nprint(null_count)\n\nwhile null_count > 0:\n    print(null_count)\n    \n    both_df[\"CFR\"] = both_df[\"Fatalities\"]/both_df[\"ConfirmedCases\"]\n    both_df[\"CFR\"].fillna(0, inplace=True)\n    \n    for current_province_state in province_list:\n        for current_date in date_array:\n            update_target_dict(both_df, current_date, current_province_state)\n\n\n    X_train, y_train_case, y_train_fatal, y_key = create_train_X_y(target_to_features_dict)\n    \n    gbm_case, gbm_fatal = dual_xgb_fit(X_train, y_train_case, y_train_fatal)\n\n\n\n    date_array =  [both_df[\"Date\"][both_df[\"Fatalities\"].isnull()].min()]\n    print(date_array)\n\n    for current_province_state in province_list:\n        for current_date in date_array:\n            update_target_dict(both_df, current_date, current_province_state)\n\n    X_test, y_test_case, y_test_fatal, y_key = create_test_X_y(target_to_features_dict, date_array[0])\n    print(X_test.shape)\n\n    y_pred_case = np.expm1(gbm_case.predict(X_test))\n    y_pred_fatal = np.expm1(gbm_fatal.predict(X_test))\n\n    for j in range(len(y_key)):\n        key_val = y_key[j]\n        case_dict[key_val] = y_pred_case[j]  \n        fatal_dict[key_val] = y_pred_fatal[j]\n\n    print(len(case_dict.keys()))\n\n\n    both_df[\"ConfirmedCases\"] = both_df[\"state_date_key\"].map(case_dict)\n    both_df[\"Fatalities\"] = both_df[\"state_date_key\"].map(fatal_dict)\n    null_count = both_df[\"Fatalities\"].isnull().sum()\n\nprint(\"done\")\n\n\n# In[ ]:\n\n\nboth_df.tail(10)\n\n\n# In[ ]:\n\n\ntest_df = pd.read_csv(PREFIX+\"test.csv\", parse_dates=[\"Date\"])\ntest_df = replace_state_nan(test_df)\ntest_df.head(3)\nprint(test_df.shape)\nmerge_df = both_df[[\"Province_State\",\"Date\",\"ConfirmedCases\",\"Fatalities\"]].copy()\nmerge_df.sort_values([\"Province_State\",\"Date\"], inplace=True)\nprint(merge_df.head(5))\nmerge_df[\"ConfirmedCases\"] = merge_df.groupby('Province_State')[\"ConfirmedCases\"].cummax()\nmerge_df[\"Fatalities\"] = merge_df.groupby('Province_State')[\"Fatalities\"].cummax()\ntest_df = test_df.merge(merge_df, how=\"inner\",\n                        on=[\"Province_State\",\"Date\"])\nprint(test_df.shape)\n\n\n# In[ ]:\n\n\ntest_df[[\"ForecastId\",\"ConfirmedCases\",\"Fatalities\"]].to_csv(\"submission.csv\", index=False)\nprint(\"done\")\n\n\n# In[ ]:\n\n\n# END HERE","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}