{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport nltk\nfrom sklearn.preprocessing import LabelBinarizer,LabelEncoder,StandardScaler,MinMaxScaler, OrdinalEncoder\nfrom sklearn.linear_model import LogisticRegression,SGDClassifier,LinearRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom xgboost import XGBRegressor\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import VotingRegressor\nfrom sklearn.ensemble import BaggingRegressor\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score\nfrom sklearn.model_selection import train_test_split\nimport keras\nfrom keras.wrappers.scikit_learn import KerasRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import LinearSVR\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_absolute_error\nimport datetime\nfrom dateutil.relativedelta import relativedelta\nfrom sklearn.model_selection import train_test_split\nfrom time import time\n\nfrom sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.preprocessing import PolynomialFeatures\n\nimport plotly_express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n%matplotlib inline\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(\"../input/covid19-global-forecasting-week-4/train.csv\")\ntest_df = pd.read_csv(\"../input/covid19-global-forecasting-week-4/test.csv\")\nsubmission = pd.read_csv(\"../input/covid19-global-forecasting-week-4/submission.csv\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Get the top 10 countries\nlast_date = train_df.Date.max()\ndf_countries = train_df[train_df['Date']==last_date]\ndf_countries = df_countries.groupby('Country_Region', as_index=False)['ConfirmedCases','Fatalities'].sum()\ndf_countries = df_countries.nlargest(10,'ConfirmedCases')\n#Get the trend for top 10 countries\ndf_trend = train_df.groupby(['Date','Country_Region'], as_index=False)['ConfirmedCases','Fatalities'].sum()\ndf_trend = df_trend.merge(df_countries, on='Country_Region')\ndf_trend.drop(['ConfirmedCases_y','Fatalities_y'],axis=1, inplace=True)\ndf_trend.rename(columns={'Country_Region':'Country', 'ConfirmedCases_x':'Cases', 'Fatalities_x':'Deaths'}, inplace=True)\n#Add columns for studying logarithmic trends\ndf_trend['ln(Cases)'] = np.log(df_trend['Cases']+1)# Added 1 to remove error due to log(0).\ndf_trend['ln(Deaths)'] = np.log(df_trend['Deaths']+1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"px.line(df_trend, x='Date', y='Cases', color='Country', title='COVID19 Total Cases growth for top 10 worst affected countries')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Get the top 10 countries\nlast_date = train_df.Date.max()\ndf_countries = train_df[train_df['Date']==last_date]\ndf_countries = df_countries.groupby('Country_Region', as_index=False)['ConfirmedCases','Fatalities'].sum()\ndf_countries = df_countries.nsmallest(10,'ConfirmedCases')\n#Get the trend for top 10 countries\ndf_trend = train_df.groupby(['Date','Country_Region'], as_index=False)['ConfirmedCases','Fatalities'].sum()\ndf_trend = df_trend.merge(df_countries, on='Country_Region')\ndf_trend.drop(['ConfirmedCases_y','Fatalities_y'],axis=1, inplace=True)\ndf_trend.rename(columns={'Country_Region':'Country', 'ConfirmedCases_x':'Cases', 'Fatalities_x':'Deaths'}, inplace=True)\n#Add columns for studying logarithmic trends\ndf_trend['ln(Cases)'] = np.log(df_trend['Cases']+1)# Added 1 to remove error due to log(0).\ndf_trend['ln(Deaths)'] = np.log(df_trend['Deaths']+1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"px.line(df_trend, x='Date', y='Cases', color='Country', title='COVID19 Total Cases growth for least 10 worst affected countries')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = ['ln(No_of_Days)', 'Country_Region','ConfirmedCases','Fatalities']\ntest_set_columns = ['ln(No_of_Days)', 'Country_Region']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def extract_features(df):\n    \n    #df['No_of_Days'] = df['Date'].apply(lambda x: relativedelta(datetime.datetime.strptime(x, '%Y-%m-%d'), datetime.datetime.strptime(min(df['Date']), '%Y-%m-%d')).days)\n    df['month'] = df['Date'].apply(lambda x: int(x.split(' ')[0].split('-')[1]))\n   \n    df['day'] = df['Date'].apply(lambda x: int(x.split(' ')[0].split('-')[2]))\n    df['is_weekend'] = ((df.Date.astype('datetime64[ns]').dt.dayofweek) // 4 == 1).astype(float)\n    df['weekday'] = df.Date.astype('datetime64[ns]').dt.dayofweek\n    df['is_holyday'] = df.apply(lambda row: 1 if (row['month']==1 and row['day']==1) or (row['month']==7 and row['day']==4) or (row['month']==11 and row['day']==11) or (row['month']==12 and row['day']==25) or (row['month']==1 and row['day'] >= 15 and row['day'] <= 21 and row['weekday'] == 0) or (row['month']==2 and row['day'] >= 15 and row['day'] <= 21 and row['weekday'] == 0) or (row['month']==5 and row['day'] >= 25 and row['day'] <= 31 and row['weekday'] == 0) or (row['month']==9 and row['day'] >= 1 and row['day'] <= 7 and row['weekday'] == 0) or (row['month']==10 and row['day'] >= 8 and row['day'] <= 14 and row['weekday'] == 0) or (row['month']==11 and row['day'] >= 22 and row['day'] <= 28 and row['weekday'] == 3) else 0, axis=1)\n    df['is_day_before_holyday'] = df.apply(lambda row: 1 if (row['month']==12 and row['day']==31) or (row['month']==7 and row['day']==3) or (row['month']==11 and row['day']==10) or (row['month']==12 and row['day']==24) or (row['month']==1 and row['day'] >= 14 and row['day'] <= 20 and row['weekday'] == 6) or (row['month']==2 and row['day'] >= 14 and row['day'] <= 20 and row['weekday'] == 6) or (row['month']==5 and row['day'] >= 24 and row['day'] <= 30 and row['weekday'] == 6) or ((row['month']==9 and row['day'] >= 1 and row['day'] <= 6) or (row['month']==8 and row['day'] == 31) and row['weekday'] == 6) or (row['month']==10 and row['day'] >= 7 and row['day'] <= 13 and row['weekday'] == 6) or (row['month']==11 and row['day'] >= 21 and row['day'] <= 27 and row['weekday'] == 2) else 0, axis=1)\n    #df.drop('Date', axis=1, inplace=True)\n    #df.drop('month', axis=1, inplace=True)\n    #df.drop('day', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['Province_State'].fillna(\"\",inplace = True)\ntest_df['Province_State'].fillna(\"\",inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"extract_features(train_df)\nextract_features(test_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(len(train_df)):\n    if train_df[\"Province_State\"][i] != '':\n        train_df[\"Country_Region\"][i] = train_df[\"Province_State\"][i] + \" (\" + str(train_df[\"Country_Region\"][i]) + \")\"\n    \nfor i in range(len(test_df)):\n    if test_df[\"Province_State\"][i] != '':\n        test_df[\"Country_Region\"][i] = test_df[\"Province_State\"][i] + \" (\" + str(test_df[\"Country_Region\"][i]) + \")\"\n        \ntrain_df.drop(columns = \"Province_State\", inplace=True)\ntest_df.drop(columns = \"Province_State\", inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"i = 0\nfor value in train_df[\"Country_Region\"].unique():\n    if i < len(train_df):\n        j = 1\n        while(train_df[\"Country_Region\"][i] == value):\n            train_df[\"day\"][i] = j\n            j += 1; i += 1\n            if i == len(train_df):\n                break\n\ni = 0\nfor value in test_df[\"Country_Region\"].unique():\n    if i < len(test_df):\n        j = 72\n        while(test_df[\"Country_Region\"][i] == value):\n            test_df[\"day\"][i] = j\n            j += 1; i += 1\n            if i == len(test_df):\n                break\n                \ntrain_df.rename(columns = {\"day\" : \"No_of_Days\"}, inplace = True)\ntest_df.rename(columns = {\"day\" : \"No_of_Days\"}, inplace = True)\n\ntrain_df['ln(No_of_Days)'] = np.log1p(train_df['No_of_Days'])\ntest_df['ln(No_of_Days)'] = np.log1p(test_df['No_of_Days'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Get the top 10 countries\nlast_date = train_df.No_of_Days.max()\ndf_countries = train_df[train_df['No_of_Days']==last_date]\ndf_countries = df_countries.groupby('Country_Region', as_index=False)['ConfirmedCases','Fatalities'].sum()\ndf_countries = df_countries.nlargest(10,'ConfirmedCases')\n#Get the trend for top 10 countries\ndf_trend = train_df.groupby(['No_of_Days','Country_Region'], as_index=False)['ConfirmedCases','Fatalities'].sum()\ndf_trend = df_trend.merge(df_countries, on='Country_Region')\ndf_trend.drop(['ConfirmedCases_y','Fatalities_y'],axis=1, inplace=True)\ndf_trend.rename(columns={'Country_Region':'Country', 'ConfirmedCases_x':'Cases', 'Fatalities_x':'Deaths'}, inplace=True)\n#Add columns for studying logarithmic trends\ndf_trend['ln(Cases)'] = np.log1p(df_trend['Cases'])\ndf_trend['ln(Deaths)'] = np.log1p(df_trend['Deaths'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"px.line(df_trend, x='No_of_Days', y='Cases', color='Country', title='COVID19 Total Cases growth for top 10 worst affected countries')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['ConfirmedCases'] = np.log1p(train_df['ConfirmedCases'])\ntrain_df['Fatalities'] = np.log1p(train_df['Fatalities'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"train_df['ConfirmedCases'] = train_df['ConfirmedCases'].apply(int)\ntrain_df['Fatalities'] = train_df['Fatalities'].apply(int)"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"grid = {\n    'C': np.linspace(0.01, 10),\n    'epsilon': np.linspace(0.01, 10)\n}\n    \nsvr_gridsearch =  LinearSVR(fit_intercept=True, max_iter=10000)\ngrid_svr = GridSearchCV(svr_gridsearch, grid, scoring='neg_mean_absolute_error', cv=5)\ngrid_svr.fit(x_train, cases)"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"best_grid_svr_mae = grid_svr.best_estimator_\nbest_grid_svr_mae.fit(x_train, cases)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n\nbest_grid_svr_mae.predict(x_test)"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndf_train = train_df[columns]\ndf_test = test_df[test_set_columns]\n#df_dev = df_dev[columns]\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = []\n#Loop through all the unique countries\nfor country in df_train.Country_Region.unique():\n    #Filter on the basis of country\n    df_train1 = df_train[df_train[\"Country_Region\"]==country]\n    cases = np.array(df_train1.ConfirmedCases)\n    fatalities = np.array(df_train1.Fatalities)\n    del df_train1['ConfirmedCases']\n    del df_train1['Fatalities']  \n    \n    lb = LabelEncoder()\n    df_train1['Country_Region'] = lb.fit_transform(df_train1['Country_Region'])\n  \n    #Convert to numpy array for training\n    #train = df_train1.values\n    #sc = StandardScaler()\n    scaler = MinMaxScaler()\n    train = scaler.fit_transform(df_train1.values)\n    #train = sc.fit_transform(train)\n    #Separate the features and labels\n    X_train, y_train = train, cases\n    #model1 for predicting Confirmed Cases\n    param_dict = {\n              'n_estimators': [1500, 2000, 2200, 2500, 3000],\n              'max_depth': [10,15,20, 25, 30],\n              'min_child_weight': [1,2,3,4, 5, 6, 7, 8, 9, 10],\n              'learning_rate': [0.05, 0.1,0.15, 0.2, 0.3, 0.5],\n              'gamma': [0.0, 0.1, 0.2, 0.3, 0.4]\n    }\n    \n    model_xgb_c = XGBRegressor(gamma = 0.4, learning_rate = 0.05, max_depth = 10, min_child_weight= 7, n_estimators = 1500, random_state = 37 ) \n    polynomial_features_c= PolynomialFeatures(degree=2)\n    X_train_poly = polynomial_features_c.fit_transform(X_train)\n    \n    #steps1 = [('poly_features', PolynomialFeatures(3, interaction_only=True)),\n    #     ('XGBoost', XGBRegressor(gamma = 0.4, learning_rate = 0.05, max_depth = 10, min_child_weight= 7, n_estimators = 1500, random_state = 37 ))]\n    steps1 = [('poly_features', PolynomialFeatures(4, interaction_only=True)),\n         ('lg', LinearRegression())]\n\n    pipeline1 = Pipeline(steps=steps1)\n\n    #start = time()\n    #grid_search1 = GridSearchCV(model1, param_dict)\n    #grid_search1.fit(X_train, y_train)\n    #print(\"GridSearch took %.2f seconds to complete.\" % (time()-start))\n    #display(grid_search1.best_params_)\n    \n    #l1 = LinearRegression()\n    #r2 = RandomForestRegressor(n_estimators=150, random_state=37)\n    #model1 = BaggingRegressor(base_estimator=x1, n_estimators=500, random_state=37, verbose=1, n_jobs= -1)\n    #model1 = VotingRegressor([('xgb', x1), ('lr', l1), ('rf', r2)], n_jobs = -1)\n    pipeline1.fit(X_train, y_train)\n\n    #model2 for predicting Fatalities\n\n    x_train_cas = []\n    for i in range(len(X_train)):\n        x = list(X_train[i])\n        x.append(y_train[i])\n        x_train_cas.append(x)\n    x_train_cas[0]\n\n    model2 = XGBRegressor(gamma = 0.4,\n                             learning_rate = 0.05,\n                             max_depth = 10,\n                             min_child_weight= 7,\n                             n_estimators = 1500, random_state = 37 )\n    \n    steps2 = [('poly_features', PolynomialFeatures(4, interaction_only=True)),\n         ('XGBoost', LinearRegression())]\n\n    pipeline2 = Pipeline(steps=steps2)\n\n    #start = time()\n    #grid_search2 = GridSearchCV(model2, param_dict)\n    #grid_search2.fit(np.array(x_train_cas), fatalities)\n    #print(\"GridSearch took %.2f seconds to complete.\" % (time()-start))\n    #display(grid_search2.best_params_)\n    \n    \n    \n    \n    #l21 = LinearRegression()\n    #r22 = RandomForestRegressor(n_estimators=150, random_state=37)\n    #model2 = BaggingRegressor(base_estimator=x21, n_estimators=500, random_state=37, verbose=1, n_jobs= -1)\n    #model2 = VotingRegressor([('xgb', x21), ('lr', l21), ('rf', r22)], n_jobs = -1)\n    \n    #polynomial_features_f= PolynomialFeatures(degree=2)\n    #x_train_cas_poly = polynomial_features_f.fit_transform(np.array(x_train_cas))\n    pipeline2.fit(np.array(x_train_cas), fatalities)\n\n    #Get the test data for that particular country and state\n    df_test1 = test_df[(test_df[\"Country_Region\"]==country)]\n    df_test1['Country_Region'] = lb.transform(df_test1['Country_Region'])\n    #Store the ForecastId separately\n    ForecastId = df_test1.ForecastId.values\n    #Remove the unwanted columns\n    df_test1 = df_test1[test_set_columns]\n    #df_test2 = sc.transform(df_test2)\n    #Get the predictions\n    \n    df_test1_scaled = scaler.transform(df_test1.values)\n    cases_pred = pipeline1.predict(df_test1_scaled)\n    \n    cases_pred[cases_pred < 0] = 0\n    \n    x_test_cas = []\n    for i in range(len(df_test1_scaled)):\n        x = list(df_test1_scaled[i])\n        x.append(cases_pred[i])\n        x_test_cas.append(x)\n    x_test_cas[0]\n    \n    fatalities_pred = pipeline2.predict(np.array(x_test_cas))\n    \n    fatalities_pred[fatalities_pred < 0] = 0\n    #Append the predicted values to submission list\n    for i in range(len(cases_pred)):\n        d = {'ForecastId':ForecastId[i], 'ConfirmedCases':np.around(np.expm1(cases_pred[i]),decimals = 0), 'Fatalities':np.around(np.expm1(fatalities_pred[i]),decimals = 0)}\n        submission.append(d)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_submit = pd.DataFrame(submission)\ndf_submit.to_csv(r'submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output_df = pd.merge(test_df, df_submit, on='ForecastId')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output_df['Country_Region'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output_df[output_df['Country_Region'] == 'India']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Get the top 10 countries\nlast_date = output_df.Date.max()\ndf_countries = output_df[output_df['Date']==last_date]\ndf_countries = df_countries.groupby('Country_Region', as_index=False)['ConfirmedCases','Fatalities'].sum()\ndf_countries = df_countries.nlargest(20,'ConfirmedCases')\n#Get the trend for top 10 countries\ndf_trend = output_df.groupby(['Date','Country_Region'], as_index=False)['ConfirmedCases','Fatalities'].sum()\ndf_trend = df_trend.merge(df_countries, on='Country_Region')\ndf_trend.drop(['ConfirmedCases_y','Fatalities_y'],axis=1, inplace=True)\ndf_trend.rename(columns={'Country_Region':'Country', 'ConfirmedCases_x':'Cases', 'Fatalities_x':'Deaths'}, inplace=True)\n#Add columns for studying logarithmic trends\ndf_trend['ln(Cases)'] = np.log(df_trend['Cases']+1)# Added 1 to remove error due to log(0).\ndf_trend['ln(Deaths)'] = np.log(df_trend['Deaths']+1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"px.line(df_trend, x='Date', y='Cases', color='Country', title='COVID19 Total Cases growth for top 10 worst affected countries')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}