{"cells":[{"metadata":{},"cell_type":"markdown","source":"- Set the number of CC and Fatalities per every day\n\n- Country list with coordinates  https://developers.google.com/public-data/docs/canonical/countries_csv\n- States of US https://developers.google.com/public-data/docs/canonical/states_csv"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Read the data"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"df_train =pd.read_csv('/kaggle/input/covid19-global-forecasting-week-4/train.csv')\ndf_test =pd.read_csv('/kaggle/input/covid19-global-forecasting-week-4/test.csv')\ndf_ss =pd.read_csv('/kaggle/input/covid19-global-forecasting-week-4/submission.csv')\n\n# df_geo = pd.read_csv('/kaggle/input/corona-virus-report/covid_19_clean_complete.csv')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Any missing sample in training set:\",df_train.isnull().values.any())\nprint(\"Any missing sample in test set:\",df_test.isnull().values.any(), \"\\n\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Check the Country list in train and testing data: \",(df_test.Country_Region.unique() == df_train.Country_Region.unique()).all())\n# print(\"Check the Country list in train and clean_  data: \",(df_geo['Country/Region'].unique() == df_train.Country_Region.unique()).all())\n# countries_geo = df_geo['Country/Region'].unique()\n# states_geo = df_geo['Province/State'].unique()\n# # df_train.Country_Region.unique() ==df_geo['Country/Region'].unique()\n# # print(\"Check the Province list in train and clean_  data: \",(df_geo['Province/State'].unique() == df_train.Province_State.unique()).all())\n# for i, country in enumerate(df_train.Country_Region.unique()):\n#     if (country not in countries_geo):\n#         print(i, country)\n# for i, country in enumerate(countries_geo):\n#     if 'Korea' in country:\n#         print(country)\n        \n# for i, state in enumerate(df_train.Province_State.unique()):\n#     if (state not in states_geo):\n#         print(i, state)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport matplotlib.pyplot as plt\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport plotly.graph_objects as go\nimport plotly.express as px\nimport plotly.io as pio\n# pio.templates.default = \"plotly_dark\"\nfrom plotly.subplots import make_subplots\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeRegressor\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingClassifier, RandomForestClassifier, GradientBoostingRegressor\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.tree import DecisionTreeRegressor  \nfrom sklearn.preprocessing import LabelEncoder\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"renames = {'Province_State':'state','Country_Region':'country','ConfirmedCases':'cc','Fatalities':'deaths','Date':'date','ForecastId':'id','Id':'id'}\ndf_train.rename(columns=renames,inplace=True)\ndf_test.rename(columns=renames,inplace=True)\n# df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fill the empty states in countries\ndf_train.state.fillna(df_train.country,inplace=True)\ndf_test.state.fillna(df_test.country,inplace=True)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"convert Date to year,month ..."},{"metadata":{"trusted":true},"cell_type":"code","source":"def date_convert(df):\n    date = pd.DatetimeIndex(df['date'])\n    df['year'] = date.year\n    df['month'] = date.month\n    df['dayofmonth'] = date.day\n    df['weekofyear'] = date.weekofyear\n    df['dayofweek'] = date.dayofweek\n    df['dayofyear'] = date.dayofyear\ndate_convert(df_train)\ndate_convert(df_test)\ndf_train.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Check the countries with states and cumulative data. "},{"metadata":{"trusted":true},"cell_type":"code","source":"countries_train = df_train[df_train['country']!=df_train['state']]['country'].unique().tolist()\nprint(countries_train)\nstates_train = df_train['state'].unique().tolist()\nfor country in countries_train:\n    if country in states_train:\n        print(country)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"countries_test = df_test[df_test['country']!=df_test['state']]['country'].unique().tolist()\nprint(countries_test)\nstates_test = df_test['state'].unique().tolist()\nfor country in countries_test:\n    if country in states_test:\n        print(country)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t = (df_train['country']==country) &(df_train['state']!=country)\ncntr = df_train[df_train['state']==country][['cc']].reset_index().copy()\ncntr['cum'] = 0.0\ncntr[['cum','cc']]\nfor i,state in enumerate(df_train[t]['state'].unique().tolist()):\n    print(i,df_train[df_train['state'] == state]['cc'].max())\n    cntr['cum_{}'.format(i)] = df_train[df_train['state'] == state]['cc'].values\n# a = df_train[t].groupby(by=['state','dayofyear'])['cc'].sum(axis=1).plot()\n# df_train[(df_train['country']==country) &(df_train['state']==country)].plot(x='dayofyear',y='cc')\ncntr[['cum_{}'.format(i) for i,state in enumerate(df_train[t]['state'].unique().tolist())]].sum(axis=1)\n# cntr['cc','cum']\ncntr.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Label the countries and states\nlabel_countries = LabelEncoder()\nlabel_states = LabelEncoder()\n\nfor i in ['state', 'country']:\n    df_train['label_{}'.format(i)] = label_countries.fit_transform(df_train[i])\n    df_test['label_{}'.format(i)] = label_countries.transform(df_test[i])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# set the first day of first case\n\nday_first_case_state = df_train[df_train.cc>0].groupby('state').first()['dayofyear']\nday_first_case_country = df_train[df_train.cc>0].groupby('country').first()['dayofyear']\n# df_train['day1_state'] = np.zeros(df_train.state.size)\n# df_train['day1_country'] = np.zeros(df_train.country.size)\n\n# for day in day_first_case_state.keys():\n#     cnt = df_train[df_train['state']==day].loc[:,'day1_state'].size\n#     df_train[df_train['state']==day].loc[:,'day1_state'] = np.ones(cnt) * day_first_case_state[day]\n# for day in day_first_case_country.keys():\n#     cnt = df_train[df_train['state']==day].loc[:,'day1_country'].size\n#     df_train[df_train['state']==day].loc[:,'day1_country'] = np.ones(cnt) * day_first_case_country[day] \n\ndf_train['day1_state'] = day_first_case_state[df_train['state']].values\ndf_train['day1_country'] = day_first_case_country[df_train['country']].values\n\ndf_test['day1_state'] = day_first_case_state[df_test['state']].values\ndf_test['day1_country'] = day_first_case_country[df_test['country']].values\n\nprint(\"set the log1p for cc and deaths\")\ndf_train['log_cc'] = np.log1p(df_train.cc)\ndf_train['log_deaths'] = np.log1p(df_train.deaths)\n\ndf_train['cc_day'] = 0.\ndf_train['deaths_day'] = 0.\n\n\nfor i, c_state in enumerate(df_train.state.unique()[:]):\n    t = (df_train.state == c_state)\n#     df_train[t].apply(,axis=1)\n    isize = df_train[t].cc.size\n#     print(t.size, isize)\n#     df_train[t].loc[:,'cc_day'] = df_train[t].cc.diff().fillna(0)\n    cnt = 0\n    for i, ind in enumerate(df_train[t].index):\n        if(df_train.loc[ind,'cc'] < cnt):\n            df_train.loc[ind,'cc'] = df_train.loc[ind-1,'cc']\n        cnt = df_train.loc[ind,'cc']\n    cnt = 0\n    for i, ind in enumerate(df_train[t].index):\n        if(df_train.loc[ind,'deaths'] < cnt):\n            df_train.loc[ind,'deaths'] = df_train.loc[ind-1,'deaths']\n        cnt = df_train.loc[ind,'deaths']\n        \n    df_train.loc[t,'cc_day'] = df_train[t].cc.diff().fillna(0).values\n    df_train.loc[t,'deaths_day'] = df_train[t].deaths.diff().fillna(0).values\n#     print(c_state,df_train.loc[t,['cc','deaths_day']].min().values)\n#Set lag of every day diff\ndf_train.loc[df_train['cc_day']<0 , ['cc_day']] = 0\ndf_train.loc[(df_train['deaths_day']<0) , ['deaths_day']] = 0\nprint(\"Set lag of every day diff \", df_train['cc_day'].min(),df_train['deaths_day'].min())\ndf_train['log_cc_day'] = np.log1p(df_train['cc_day'])\ndf_train['log_deaths_day'] = np.log1p(df_train['deaths_day'])\nprint(\"Check Inf :\",df_train['log_cc_day'].notnull().values.all(),df_train['log_deaths_day'].notnull().values.all(),df_train['log_cc_day'].isna().any())\ndf_train['log_cc_day'].replace([np.inf, -np.inf], 0,inplace=True)\ndf_train['log_deaths_day'].replace([np.inf, -np.inf], 0,inplace=True)\nprint(\"Check Inf after replace :\",df_train['log_cc_day'].notnull().values.any(),df_train['log_deaths_day'].notnull().values.any(),df_train['log_cc_day'].isna().any())\n\n# print(df_train.is\n\n\ndf_train['log_cc_diff'] = 0.\ndf_train['log_deaths_diff'] = 0.\n# get log_diff\nprint(\"get log_diff\")\nfor i, c_state in enumerate(df_train.state.unique()[:]):\n    t = (df_train.state == c_state)\n    df_train.loc[t,'log_cc_diff'] = (df_train[t]['log_cc'] - df_train[t]['log_cc'].shift()) #.fillna(0)\n    df_train.loc[t,'log_deaths_diff'] = (df_train[t]['log_deaths'] - df_train[t]['log_deaths'].shift()) #.fillna(0)\n\ndf_train.loc[df_train['log_cc_diff']<0 , ['log_cc_diff']] = 0\ndf_train.loc[(df_train['log_deaths_diff']<0) , ['log_deaths_diff']] = 0\nprint(\"Check the negative vals \",df_train['log_cc_diff'].min(),df_train['log_deaths_diff'].min())\ndf_train['log_cc_diff'] = np.log1p(df_train['log_cc_diff'])\ndf_train['log_deaths_diff'] = np.log1p(df_train['log_deaths_diff'])\n\ndf_train['log_cc_diff'].fillna(0,inplace=True)\ndf_train['log_deaths_diff'].fillna(0,inplace=True)\ndf_train['log_cc_diff'].replace([np.inf, -np.inf], 0,inplace=True)\ndf_train['log_deaths_diff'].replace([np.inf, -np.inf], 0,inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train[df_train['state']=='France'].plot(x='dayofyear',y='cc')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t = df_train.country=='Sweden'\ndf_train[t][['log_cc_diff','log_deaths_diff']].fillna(0)\ndf_train[['log_cc_diff','log_deaths_diff']].isna().any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df_train[t].plot(x='dayofyear',y=['log_cc_diff','log_deaths_diff'])\nplt.plot(df_train[t]['dayofyear'],df_train[t]['log_cc'].ewm(5).mean())\nplt.plot(df_train[t]['dayofyear'],df_train[t]['log_cc'].rolling(5).mean())\nplt.plot(df_train[t]['dayofyear'],df_train[t]['log_cc'])\n# pd.(df_train[t]['cc'],5)\n# df_train[t]['cc'].ewm(5).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_corr = df_train.drop(['year','id','country','state','month','weekofyear'\n                         ,'date','deaths','cc','log_cc','cc_day','log_deaths'\n                         ,'log_cc_day', 'deaths_day', 'log_deaths_day'\n                        ],axis=1) .corr()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nsns.heatmap(df_corr, annot = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.country.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_drop = ['year','id','country','state','month','weekofyear','date','day1_state','day1_country','dayofmonth']\ncols_pred = ['cc','deaths']\n# cols_drop += cols_pred\ncols_pred_day = ['cc_day','deaths_day']\ncols_pred_log = ['log_cc','log_deaths']\ncols_pred_log_day = ['log_cc_day','log_deaths_day']\ncols_pred_log_diff = ['log_cc_diff','log_deaths_diff']\n\ncol_drop = cols_drop+cols_pred + cols_pred_log + cols_pred_day + cols_pred_log_day + cols_pred_log_diff\n\nX = df_train[df_train.cc>0].drop(col_drop,axis=1)\ny = df_train[df_train.cc>0][cols_pred]\ny_log = df_train[df_train.cc>0][cols_pred_log]\ny_day = df_train[df_train.cc>0][cols_pred_day]\ny_dif = df_train[df_train.cc>0][cols_pred_log_diff + cols_pred_log]\n\nX_test = df_test.drop(cols_drop,axis=1)\n# y_test = df_test[['cc','deaths']]\n\nday_end_train = df_train.dayofyear.max()\nday_start_test = df_test.dayofyear.min()\nprint('Testing starting from day: {} , {}'.format(day_start_test, day_end_train))\n\nX_train = X[X.dayofyear< day_start_test]\ny_train = y[X.dayofyear< day_start_test]\ny_train_log = y_log[X.dayofyear< day_start_test]\ny_train_day = y_day[X.dayofyear< day_start_test]\ny_train_dif = y_dif[X.dayofyear< day_start_test]\n\nX_valid = X[X.dayofyear>= day_start_test]\ny_valid = y[X.dayofyear>= day_start_test]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check the same columns in train and test data\n(X_train.columns == X_test.columns).all()\n# print(X_train.columns.size,X_test.columns.size)\n# print(X_train.columns,X_test.columns)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ‘neg_mean_squared_log_error’\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.svm import SVC, SVR\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.decomposition import PCA\n\nmodel_dtr = DecisionTreeRegressor(random_state = 0) \nmodel_rfr = RandomForestRegressor(n_estimators=110, random_state=0)\n\n# Multiply by -1 since sklearn calculates *negative* MAE\ndef get_score(model,X_tr, y_tr,n_est=50,n_cv=5):\n    pipeline = Pipeline(steps=[\n        ('preprocessor', SimpleImputer()),\n        ('model', model)\n])\n\n    scores = -1 * cross_val_score(pipeline, X_tr, y_tr,\n                              cv=n_cv,\n#                               scoring='neg_mean_squared_log_error')\n                              scoring='neg_mean_absolute_error')\n    return scores.mean()\n\n\nprint(\"Average NMSLE score RandForestRegre :\", get_score(model_rfr,X_train, y_train))\nprint(\"Average NMSLE score DecisTreeRegres:\", get_score(model_dtr,X_train, y_train))\nprint(\"Average NMSLE score DecisTreeClass:\", get_score(DecisionTreeClassifier(random_state=0),X_train, y_train))\n# print(\"Average NMSLE score GBC:\", get_score(GradientBoostingClassifier(random_state=0),X_train, y_train))\n# print(\"Average NMSLE score GBR:\", get_score(GradientBoostingRegressor(random_state=0),X_train, y_train))\n# # print(\"Average NMSLE score:\", get_score(model_dtr,X_train, y_train))\n# print(\"Average NMSLE score:\", get_score(model_rfr,X_train, y_train))\n\n# results = {i: get_score(model_rfr,X_train, y_train,n_est=i,n_cv=5) for i in range(50,450,50)} # Your code here\n# for i in [50,100,500,5000]:\n#     print(i,\" Average NMSLE score:\", get_score(model_rfr,X_train, y_train_dif,n_est=i,n_cv=5))\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for i in range(50,500,50):\n#     print(i,\" Num of Est MSE score:\", get_score(model_rfr,X_train, y_train,n_est=i,n_cv=5))\n    \n# 50  Num of Est MSE score: 0.31316054949126604\n# 100  Num of Est MSE score: 0.31269114946680254\n# 150  Num of Est MSE score: 0.31290359325866357\n# 200  Num of Est MSE score: 0.3127087613582483\n# 250  Num of Est MSE score: 0.3131997694520491\n# 300  Num of Est MSE score: 0.31319708740038366\n# 350  Num of Est MSE score: 0.3127921127791045\n# 400  Num of Est MSE score: 0.31244946171626103\n# 450  Num of Est MSE score: 0.3128081193774369","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for i in [2,3,4,5]:\n#     print(i,\" Average NMSLE score:\", get_score(model_rfr,X_train, y_train,n_est=350,n_cv=i))\n# for i in [6,8,10]:\n#     print(i,\" Average NMSLE score:\", get_score(model_rfr,X_train, y_train,n_est=350,n_cv=i))\n    \n# set the number of CV\n# 2  Average NMSLE score: 0.013467870703916035\n# 3  Average NMSLE score: 0.008843653417933703\n# 4  Average NMSLE score: 0.006452329486737336\n# 5  Average NMSLE score: 0.006024922098354479\n# 6  Average NMSLE score: 0.007329168527967208\n# 8  Average NMSLE score: 0.006077414852762886\n# 10  Average NMSLE score: 0.005814285650045135","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# X_train.shape, y_train.shape\n# sss  = SVR()\n# sss.fit(X_train,y_train_log['log_cc'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# y_predict_log = sss.predict(X_valid)\n# y_pred = np.exp(y_predict_log) - 1\n# # y_pred = (y_pred).astype(int)\n# # score =  mean_squared_log_error(y_predict, y_valid)\n# score_log =  mean_squared_log_error(y_pred, y_valid['cc'])\n# print(\"MAE result: \", score_log)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get_score(model_rfr,X_train, y_train,n_est=350,n_cv=10)\nsteps = [('scaler', StandardScaler()), \n         ('SVM', SVC()),        \n#          ('model', RandomForestRegressor(n_estimators=400, random_state=0)\n          ]\npipeline = Pipeline(steps=[\n    ('preprocessor', SimpleImputer()),\n    ('model', DecisionTreeRegressor(random_state = 0))\n#     ('model', RandomForestRegressor(n_estimators=100, max_depth=2, random_state=0))\n])\n\npipeline0 = Pipeline(steps=[\n    ('preprocessor', SimpleImputer()),\n#     ('model', DecisionTreeRegressor(random_state = 0)),\n    ('gbm', GradientBoostingRegressor(n_estimators=300, learning_rate = 0.01,alpha=0.9,random_state = 0))\n])\npipeline1 = Pipeline(steps=[\n    ('preprocessor', SimpleImputer()),\n#     ('model', DecisionTreeRegressor(random_state = 0)),\n    ('gbm', GradientBoostingRegressor(n_estimators=300, learning_rate = 0.01,alpha=0.9,random_state = 0))\n])\n\npipeline_log = Pipeline(steps=[\n    ('scaler', StandardScaler()),\n    ('preprocessor', SimpleImputer()),\n    ('model', RandomForestRegressor(n_estimators=100, max_depth=2, random_state=0))\n#     ('gdm', GradientBoostingRegressor(n_estimators=300, learning_rate = 0.01,alpha=0.9,random_state = 0))\n])\npipeline_svm = Pipeline(steps=steps)\n\n\n# scores = -1 * cross_val_score(pipeline, X_train, y_train,\n#                           cv=10,\n# #                               scoring='neg_mean_squared_log_error')\n#                           scoring='neg_mean_absolute_error')\nparameters_svm = {'SVM__C':[0.001,0.1,10], 'SVM__gamma':[0.1,0.01]}\n# parameters_log = {'model__n_estimators': [100,300,450], 'model__max_depth': [None,3]} #, 'model__min_samples_split': [1, 2, 3]}\nparameters_log = {'model__n_estimators': [450], 'model__max_depth': [None]} #, 'model__min_samples_split': [1, 2, 3]}\nparameters = {'model__max_depth': [None, 1, 2, 3],'model__max_features':[None,'log2','auto']} #, 'model__min_samples_split': [1, 2, 3]}\n# parameters = {'model__n_estimators': [100,300,500, 700, 1000]}\ngrid = GridSearchCV(pipeline_log, \n                    param_grid=parameters_log, \n                    scoring='r2', \n#                     scoring='neg_mean_squared_log_error',\n                    verbose=True,\n                    cv=5)\n# grid = GridSearchCV(pipeline_svm, \n#                     param_grid=parameters_svm, \n# #                     scoring='r2', \n#                     scoring='neg_mean_squared_log_error',\n#                     verbose=True,\n#                     cv=5)\ngrid0 = GridSearchCV(pipeline0, \n                    param_grid=parameters, \n#                     scoring='r2', \n                    scoring='neg_mean_squared_log_error',\n                    cv=5)\ngrid1 = GridSearchCV(pipeline1, \n                    param_grid=parameters, \n#                     scoring='r2', \n                    scoring='neg_mean_squared_log_error',\n                    cv=5)\ngrid2 = GridSearchCV(pipeline_log, \n                    param_grid=parameters_log, \n                    scoring='r2', \n#                     scoring='neg_mean_squared_log_error',\n                    verbose=True,\n                    cv=5)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\npipeline.fit(X_train,y_train)\n\n# grid0.fit(X_train,y_train['cc'])\n# grid1.fit(X_train,y_train['deaths'])\n\ngrid.fit(X_train,y_train_log)\n\ngrid2.fit(X_train,y_train_dif)\n\n# grid.fit(X_train,y_train['cc'])\n\n# pipeline.fit(X_train,y_train_day['cc_day'])\n\ny_predict = pipeline.predict(X_valid)\n\n# y_predict_cc = grid0.predict(X_valid)\n# y_predict_cc = np.expm1(y_predict_cc)\n# y_predict_deaths = grid1.predict(X_valid)\n# y_predict_deaths = np.expm1(y_predict_deaths)\n\n# y_predict_log = pipeline_log.predict(X_valid)\ny_predict_log = grid.predict(X_valid)\n\ny_predict_dif = grid2.predict(X_valid)\n\ny_pred = np.exp(y_predict_log) - 1\n# y_pred_dif = np.exp(y_predict_dif) - 1\n# y_pred = (y_predict_log)\n\n# y_pred = np.round(y_pred).astype(int)\n# score_cc =  mean_squared_log_error(y_predict_cc, y_valid['cc'])\n# score_deaths =  mean_squared_log_error(y_predict_deaths, y_valid['deaths'])\nscore =  mean_squared_log_error(y_predict, y_valid)\nscore_log =  mean_squared_log_error(y_pred, y_valid)\nprint(\"MAE result: \",score, score_log ) #, score_cc, score_deaths)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_predict_dif","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.dayofyear.max(),X_valid.dayofyear.min(),y_train.loc[X_train.dayofyear == X_train.dayofyear.max(),'cc']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_predicted_diff = y_valid.copy()\n# y_predicted_diff\ny_pred_diff_res = y_valid.copy()\ny_pred_diff_res['log_cc_pred_diff'] = y_predict_dif[:,0]\ny_pred_diff_res['log_dd_pred_diff'] = y_predict_dif[:,1]\ny_pred_diff_res['log_cc_pred'] = y_predict_dif[:,2]\ny_pred_diff_res['log_dd_pred'] = y_predict_dif[:,3]\ny_pred_diff_res['log_cc_cumsum'] = 0.0\ny_pred_diff_res['log_dd_cumsum'] = 0.0\n\nfor i, c_state in enumerate(X_valid.label_state.unique()[:]):\n    t = (X_valid.label_state == c_state)\n    y_pred_diff_res.loc[t,'log_cc_cumsum'] = y_pred_diff_res[t]['log_cc_pred_diff'].cumsum() #.fillna(0)\n    y_pred_diff_res.loc[t,'log_dd_cumsum'] = y_pred_diff_res[t]['log_dd_pred_diff'].cumsum() #.fillna(0)\n    \n#     tt = (X_train.label_state == c_state) & (X_train.dayofyear==X_train.dayofyear.max())\n#     cc = np.log1p(y_train.loc[tt,'cc'])\n#     dd = np.log1p(y_train.loc[tt,'deaths'])\n#     y_pred_diff_res.loc[t,'log_cc_cumsum'] +=cc\n#     y_pred_diff_res.loc[t,'log_dd_cumsum'] +=dd\n#     print(c_state,cc.values, dd.values)\n    \ny_pred_diff_res['log_cc_cumsum'] += (y_pred_diff_res['log_cc_pred'])\ny_pred_diff_res['log_dd_cumsum'] += (y_pred_diff_res['log_dd_pred'])\ny_pred_diff_res['cc_cumsum'] = np.expm1(y_pred_diff_res['log_cc_cumsum'])\ny_pred_diff_res['dd_cumsum'] = np.expm1(y_pred_diff_res['log_dd_cumsum']) \n\ny_pred_diff_res.head()\nscore =  mean_squared_log_error(y_pred_diff_res[['cc_cumsum','dd_cumsum']], y_valid)\nprint(\"MAE result: \",score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_diff_res.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t_tr = X_train.label_state == 261\nt_v = X_valid.label_state == 261\nf, ax = plt.subplots()\n# y_train[t_tr].plot(ax=ax)\n# y_valid[t_v].plot(y='deaths',ax=ax)\ny_valid[t_v].plot(y=['cc','deaths'],ax=ax)\n# y_pred_diff_res[t_v].plot(y=['dd_cumsum'],ax=ax)\ny_pred_diff_res[t_v].plot(y=['cc_cumsum','dd_cumsum'],ax=ax)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df_chk = pd.DataFrame({'cc_log':y_pred[:,0],'cc_pl':y_predict[:,0],'cc_cc':y_predict_cc,'cc_valid':y_valid['cc'],'deaths_log':y_pred[:,1]})\n# df_chk\n# # y_valid[X_valid['state']=='Jamaica']\n# # X_valid\n# for i, state in enumerate(X_valid.label_state.unique()[:]):\n#     t = X_valid.label_state == state\n#     vals = df_chk.loc[t,'cc_log'].values\n#     df_chk.loc[t,'cc_log'] = [vals[:j+1].max() for j in range(vals.size)]\n#     vals = df_chk.loc[t,'deaths_log'].values\n#     df_chk.loc[t,'deaths_log'] = [vals[:j+1].max() for j in range(vals.size)]\n# #     print([vals[:j+1].max() for j in range(vals.size)])\n# score_log =  mean_squared_log_error(df_chk[['cc_log','deaths_log']], y_valid)\n# score_log1 =  mean_squared_log_error(np.round(df_chk[['cc_log','deaths_log']]).astype('int'), y_valid)\n# print(\"MAE result: \",score, score_log, score_log1, score_deaths)\n# df_chk","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid2.best_params_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Update the model for all data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# grid.fit(X,y_log)\ngrid2.fit(X,y_dif)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# my_pipeline.fit(X,y)\ny_test_predict = pipeline.predict(X_test) \n# y_test_predict_log = pipeline_log.predict(X_test) \ny_test_predict_log = grid.predict(X_test) \ny_test_predict_log = np.expm1(y_test_predict_log)\n# y_test_predict_log = np.round(y_test_predict_log).astype('int')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test_predict_dif = grid2.predict(X_test) \n\ny_pred_diff_res = X_test[['label_state']].copy()\ny_pred_diff_res['log_cc_pred_diff'] = y_test_predict_dif[:,0]\ny_pred_diff_res['log_dd_pred_diff'] = y_test_predict_dif[:,1]\ny_pred_diff_res['log_cc_pred'] = y_test_predict_dif[:,2]\ny_pred_diff_res['log_dd_pred'] = y_test_predict_dif[:,3]\ny_pred_diff_res['log_cc_cumsum'] = 0.0\ny_pred_diff_res['log_dd_cumsum'] = 0.0\nfor i, c_state in enumerate(X_test.label_state.unique()[:]):\n    t = (X_test.label_state == c_state)\n    y_pred_diff_res.loc[t,'log_cc_cumsum'] = y_pred_diff_res[t]['log_cc_pred_diff'].cumsum() #.fillna(0)\n    y_pred_diff_res.loc[t,'log_dd_cumsum'] = y_pred_diff_res[t]['log_dd_pred_diff'].cumsum() #.fillna(0)\ny_pred_diff_res['log_cc_cumsum'] += (y_pred_diff_res['log_cc_pred'])\ny_pred_diff_res['log_dd_cumsum'] += (y_pred_diff_res['log_dd_pred'])\ny_pred_diff_res['cc_cumsum'] = np.expm1(y_pred_diff_res['log_cc_cumsum'])\ny_pred_diff_res['dd_cumsum'] = np.expm1(y_pred_diff_res['log_dd_cumsum']) \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_chk = pd.DataFrame({'st':y_test_predict[:,0],'cc_log':y_test_predict_log[:,0],'deaths_log':y_test_predict_log[:,1]})\nfor i, state in enumerate(X_valid.label_state.unique()[:]):\n    t = X_test.label_state == state\n    vals = df_chk.loc[t,'cc_log'].values\n    df_chk.loc[t,'cc_log'] = [vals[:j+1].max() for j in range(vals.size)]\n    vals = df_chk.loc[t,'deaths_log'].values\n    df_chk.loc[t,'deaths_log'] = [vals[:j+1].max() for j in range(vals.size)]\ndf_chk['cc_log'] = df_chk['cc_log'].apply(lambda x: np.round(x).astype('int'))\ndf_chk['deaths_log'] = df_chk['deaths_log'].apply(lambda x: np.round(x).astype('int'))\n# y_valid.values[:,0].size\ndf_chk","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df_ss[['ConfirmedCases','Fatalities']] = y_test_predict_log\n\ndf_ss[['ConfirmedCases','Fatalities']] = df_chk[['cc_log','deaths_log']]\n\n# df_ss[['ConfirmedCases','Fatalities']] = y_pred_diff_res[['cc_cumsum','dd_cumsum']]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_ss.to_csv('submission.csv',index=False)\n# # preds = my_pipeline.predict(X_test)\n# test_preds0 = my_pipeline0.predict(X_test)\n# test_preds1 = my_pipeline1.predict(X_test)\n\n# t0 = np.round(test_preds0).astype(int)\n# t1 = np.round(test_preds1).astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_ss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check for inconsistencies in daily new cases, cumulative count should only increase or remain equal\ndf_train[df_train['cc_day'] < 0].sort_values('cc_day')\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":4}